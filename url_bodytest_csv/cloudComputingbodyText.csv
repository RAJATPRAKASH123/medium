story_url,bodyText
https://medium.com/free-code-camp/why-learning-to-code-alexa-skills-is-the-gateway-to-a-cloud-computing-job-fa13c1c0c853?source=search_post---------0,"There are currently no responses for this story.
Be the first to respond.
Top highlight
There are radical economic shifts underway. Society is moving from commodity-based capital toward intellectual capital. Not only will repetitive manufacturing jobs will be wiped-out — mundane white-collar work will also be eliminated.
The jobs of the future will be those which can’t be done by robots or replaced by artificial intelligence.
“We have an opportunity to fill the hundreds of thousands of vacant American jobs that aren’t being automated. These are the jobs that involve telling those machines what to do. The programming jobs.” — Quincy Larson
So where will these programming jobs exist? For those willing to invest in developing the intellectual capital necessary to compete in the new economy, all trends point to the public cloud.
In 2017, an estimated $122.5 billion will be invested in public cloud services. The cloud segment is growing fast — with spend expected to increase 30% annually to $203 billion by 2020.
Expectations are most bullish for cloud applications and cloud infrastructure to over perform. Customer demand for IaaS and SaaS cloud components will accelerate as they pursue digital business strategies.
All of this spending on public cloud is creating a massive demand for skills. In CompTIA’s recent outlook, they noted IDC’s anticipation of 7 million cloud-related jobs — many from existing positions which will be re-engineered with a heavy cloud focus.
But cloud talent is in short supply — and the scarcity of cloud computing skills is now the #1 impediment for companies with major cloud initiatives. As a result, cloud-fluent developers are now commanding some of the highest paying jobs.
In 2016, the top paying IT certification was cloud-related Amazon Web Services, with an average salary of $125,871.
“Many people in the U.S. and around the world lack the education and skills required to participate in the great new companies coming out of the software revolution. There’s no way through this problem other than education, and we have a long way to go.” — Marc Andreessen
“Today, another shift is taking place, to event driven functions, because the underlying constraints have changed, costs have reduced, and radical improvements in time to value are possible.” — Adrian Cockcroft
The cloud is also going through a radical change — one that mimics the economic shift from commodity-based capital toward intellectual capital. Repetitive virtual machines will be wiped out, and mundane white-collar maintenance scripts will be eliminated. The cloud of the future will be driven by “functions as a service” that can’t be performed by server-based infrastructure or platform services.
A simple and engaging way to explore cloud computing services and the new concept of event driven “serverless” functions is by developing a custom Alexa skill. It’s a fun and engaging entry point into the API-driven world of cloud computing and emerging serverless architecture patterns.
While Alexa is still in the early stages of adoption, it’s clear that voice interaction is a breakout technology. Since Amazon opened Alexa to developers, over 10,000 skills have been developed for the service.
Underlying the growth of Alexa is a key innovation by Amazon Web Services that is fueling a majority of the custom skills — a service called Lambda.
AWS Lambda is an event-driven, serverless computing platform provided by Amazon as a part of the Amazon Web Services. It is a compute service that runs code in response to events and automatically manages the compute resources required by that code.
When a user invokes an Amazon Alexa skill, it triggers an event via an API call. AWS Lambda makes it easy to execute a function in response to the event. And since AWS Lambda currently supports functions written in Node.js, Python, C#, and Java — it’s easy for developers that are already familiar with those common languages to build functions.
To get started with Amazon Alexa and AWS Lambda, I encourage you take a few easy steps toward building marketable cloud skills.
acloud.guru
After publishing your skill, just fill out a simple form to get a free Alexa swag. Amazon offers a new promotion each and every month.
Thanks for reading! If you like what you read, hold the clap button below so that others may find this. You can follow me on Twitter.
We’ve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
1.4K 
15
1.4K claps
1.4K 
15
Written by
migrating talent to the cloud at acloud.guru
We’ve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
Written by
migrating talent to the cloud at acloud.guru
We’ve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@nnilesh7756/what-are-cloud-computing-services-iaas-caas-paas-faas-saas-ac0f6022d36e?source=search_post---------1,"Sign in
There are currently no responses for this story.
Be the first to respond.
Top highlight
Nilesh Suryavanshi
Nov 8, 2017·2 min read
Today, everyone are moving towards Cloud World (AWS/GCP/Azure/PCF/VMC). It might be a public cloud, a private cloud or a hybrid cloud.
But are you aware of what are Services Cloud Computing provides to us ????
Majorly there are three categories of Cloud Computing Services:
a) Infrastructure as a Service (IaaS) : It provides only a base infrastructure (Virtual machine, Software Define Network, Storage attached). End user have to configure and manage platform and environment, deploy applications on it.
AWS (EC2), GCP (CE), Microsoft Azure (VM) are examples of Iaas.
b) Software as a Service (SaaS) : It is sometimes called to as “on-demand software”. Typically accessed by users using a thin client via a web browser. In SaaS everything can be managed by vendors: applications, runtime, data, middleware, OSes, virtualization, servers, storage and networking, End users have to use it.
GMAIL is Best example of SaaS. Google team managing everything just we have to use the application through any of client or in browsers. Other examples SAP, Salesforce .
c) Platform as a Service (PaaS): It provides a platform allowing end user to develop, run, and manage applications without the complexity of building and maintaining the infrastructure.
Google App Engine, CloudFoundry, Heroku, AWS (Beanstalk) are some examples of PaaS.
Below fig 1.0 while give you more idea on it.
d) Container as a Service (CaaS): Is a form of container-based virtualization in which container engines, orchestration and the underlying compute resources are delivered to users as a service from a cloud provider.
Google Container Engine(GKE), AWS (ECS), Azure (ACS) and Pivotal (PKS) are some examples of CaaS.
e) Function as a Service (FaaS): It provides a platform allowing customers to develop, run, and manage application functionalities without the complexity of building and maintaining the infrastructure.
AWS (Lamda), Google Cloud Function are some examples of Faas
Hope this gives you clear idea on what are Cloud Computing Services provided in market!!!!!!
@nilesh7756 | VMware NSX-T | Kubernetes | Openshift | PCF/PKS | Docker | AWS | GCE/GKE | DevOps
See all (25)
935 
7
935 claps
935 
7
@nilesh7756 | VMware NSX-T | Kubernetes | Openshift | PCF/PKS | Docker | AWS | GCE/GKE | DevOps
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/hackernoon/cloud-computing-for-beginners-85d168959afb?source=search_post---------2,"There are currently no responses for this story.
Be the first to respond.
Introduction to Cloud
Following is an excerpt from the book “Cloud Is a Piece of Cake”. Learn more about the book here.
Consider you wanted to host a movie booking service. Before cloud, you had to buy a physical server and host it. So whenever a customer wants to book a ticket, he sends a request from his computer to your server. The server processes the request and sends data back to the client (customer’s computer). If the server and the client were having a conversation, it would look like this.
Client: I need tickets for Captain America: Civil War.
Server: Here are the available seats for the movie {P1, P2, P4, A3 …}
Client: I am booking P1, P2
Server: Blocking the seats, awaiting payment
Client: Here are my payment details {CardNo: XXXXXX, CVV…}
Server: Congrats, they are booked.
The customer’s computer and the server communicate with packets of data.
Your server would be able to handle 1000’s of such requests based on its configuration.
However, what happens when the demands exceed the server’s limit? Say 10k — 20k requests? It is the same case when ten to twenty people are pinging you on Whatsapp. You will attempt to answer them all one by one. However, you would take a lot of time. However, on the internet this delay is unacceptable.
Why can’t we add more servers?
Yes, that’s a solution. Say you buy or rent ten more servers. Now they can work as a team and share the load. That works well. However, your service won’t have such requests all the time. Compare the first day of the Civil War Movie to the fiftieth day. The number of requests drops down. The additional servers you bought are now jobless. You will be still paying for their electricity and bandwidth. In some situations, you can never predict when requests are at peak. So it’s necessary for the servers to be ready. When the next peak time arrives, your servers may be outdated. Hosting it physically has a lot of other problems like power outages, maintenance cost, etc. Since this involves wastage of resources, this isn’t an effective solution. The cloud was developed to tackle this.
How can the cloud help?
In the cloud, we have a shared pool of computer resources (servers, storage, applications, etc.) at our disposal. When you need more resources, all you need is to ask. Provisioning resources immediately is a piece of cake for the cloud. You can free resources when they are not needed. In this way, you only pay for what you use. Your cloud provider will take care of all the maintenance.
Where is the cloud?
The shared pool of computer resources exists in a physical location called data centers. Your cloud providers have multiple data centers around the world. So your data is replicated at multiple sites. Even if a data center goes down because of a natural calamity, it’s still safe in another location.
What are IaaS, PaaS and SaaS?
In IaaS (Infrastructure as a Service) you are given materials like cement, bricks, sheets, etc. to build a house. Similarly, here you get to choose the hardware you want to make the cloud service. You have got the flexibility to make it in the way you want. Ex: Amazon Web Services, Microsoft Azure, Google Compute Engine, etc.
In PaaS (Platform as a Service) the house is built for you, you only need to furnish it. Similarly, here you are provided preconfigured hardware. So it can only run applications it supports. You don’t get the flexibility when compared with IAAS. Ex: Heroku, Google App Engine, etc.
SaaS (Software as a Service) all you need to occupy. Here you are offered software on a subscription basis. Ex: Gmail, Yahoo, etc.
Enjoyed reading this? Do you want to learn to create virtual machines, build a cloud app, secure and scale it? The above is just an excerpt from the book Cloud Is a Piece of Cake .
Hacker Noon is how hackers start their afternoons. We’re a part of the @AMI family. We are now accepting submissions and happy to discuss advertising & sponsorship opportunities.
If you enjoyed this story, we recommend reading our latest tech stories and trending tech stories. Until next time, don’t take the realities of the world for granted!
#BlackLivesMatter
830 
10
830 claps
830 
10
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
Author of Cloud Is a Piece of Cake
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
"
https://medium.com/coincentral-staff/grid-computing-the-powers-of-distributed-cloud-computing-7098c11e673f?source=search_post---------3,"There are currently no responses for this story.
Be the first to respond.
Grid computing, a descendant of the cloud and big brother to distributed computing.
Think of grid computing as the intersection of two core systems of organization: cloud computing and public utilities like electricity. At this intersection, grid computing is enabling you to tap into computational resources, centralized and not. Just like you would tap into the nearby energy lines for some of those glorious electrons that we rely on.
A modern power grid will have many sources of input. Power plants, for example, contribute a lot to the power grid but burgeoning technologies, such as solar panels and windmills, are democratizing power production.
Independent and artisanal power producers can contribute to the power grid and receive compensation. In some cases, this is excess energy.
Farmers, for example, may have solar panels to generate cheaper electricity locally. However, the farmer cannot store any unused electrons for future use, so they may choose to route that surplus energy back to the energy grid, where others can use it. One person’s wasted electrons are another’s fully charged Tesla.
Grid computing is much like the electricity grid. Contributors, big and small, can add to the grid. Users can tap into the computational grid and access services independent of the contributor.
To better understand what grid computing is and its nuanced differences from distributed computing, it will be easier to first understand the barrier and limitations that grid computing is able to overcome. In other words, seeing the problems grid computing can solve will help us better understand what grid computing is.
Grid computing is a subset or extension of cloud computing. In a nutshell, cloud computing is the outsourcing of computational functions. A common cloud service, like cloud data storage from Google Drive or Dropbox, lets a customer store their data with those companies.
Someone looking to use cloud data storage chooses between providers like Google Drive, Dropbox and iCloud. The company they go with would then be their provider of cloud storage. Customer support, troubleshooting, billing, networking infrastructure, and all aspects to providing the cloud service to the customer would then come directly and solely from the company they choose.
Pretty straightforward, right? One customer, one provider. However, we are looking for the limitations of cloud computing. Where do the perks of cloud computing fall short and leave room for other organizational structures like grid computing?
Common Criticisms of Cloud Computing:
Keeping in mind the parallels that grid computing has with a public utility grid, this type of computational organization can alleviate some of the common criticisms limiting cloud computing.
Let’s look over each of these claims and examine how a grid system could be more beneficial for a user over a traditional cloud service.
Cloud Limitation #1: User resources are committed to a single symmetric multiprocessing (SMP) system.
I’ll use a really basic example to showcase this pain point. There is a neural scientist looking to crunch two data sets (Set A and Set B). These data sets are huge and she’ll need to outsource the task to a cloud service.
The cloud service will have no problem running these data sets and she happily rents one machine from them to process her datasets. Remember that her datasets are exclusive to each other and need to be processed separately.
This means that the single SMP machine she leased will run Set A followed by Set B. Her single machine is unable to process both data sets simultaneously.
No big deal though, the cloud machines she leased are heavy duty and tear through the massive data sets in less than a few hours each. Processing the data will take less time than a full nights sleep for the scientist.
Now, what happens if she needs to do the same processing but for 100 data sets. Her budget still only gives her enough funding to access one cloud SMP machine. Being a person of science, she quickly does the math and discovers that it will take nearly two weeks to process all that data!
Grid Advantage: The same scientist with two data sets (Set A and Set B) could instead tap into a grid service. Instead of the scientist renting a single SMP machine from a cloud service, she would access the computing grid and rent the necessary computational power required.
The two data sets get processed at the same time. Perhaps by two machines, each dedicated to either data set, or it could be thousands of machines each fractionally processing the data sets. Regardless, the data is being processed parallel to each other. What took six hours before in two batches, now takes three hours in a single batch.
One hundred data sets? In theory, this would still only take three hours as each data set is processed side by side.
Cloud Limitation #2: Unused computing resources sit idle and are locked into a single task until it is complete.
Expanding on the above example of a neural scientist, the cloud service she leased independently processed her datasets, one after the other.
While processing either data set, the scientist noticed her rented hardware is only operating at 80 percent of it’s capacity. The remaining 20 percent is not enough to process the second data set, instead, it sits idly waiting for the next task.
Grid Advantage: The commodification of processing power allows a single task to be conducted across multiple machines. In the case of the scientist’s datasets, a grid system could process the data in a range of combinations between machines.
For example, the two datasets are allocated to two machines in the grid, each using 80 percent of the machine they are being processed on. The remaining 20 percent wouldn’t sit idly, instead, another user of the grid captures it. This use of idle capacity is an important component of the strengths of grid computing.
Cloud Limitation #3: Relatively limited scalability
There’s no denying that the capabilities of cloud computing are exponentially larger than most localized machines. The multiple layers to the cloud stack have enabled many more participants to the entire field than ever before possible.
Furthermore, cloud computing has many scaling benefits compared to self-custodianship of these same services. So to say that cloud computing is also limited in scalability may seem a touch paradoxical.
However, relative to cloud computing, scaling on a grid is even more achievable. This is in part due to the modularity of grid computing in addition to the more efficient use of idle resources.
Grid Advantage: Regardless if you are contributing to it or using it, scaling your task in a grid computing system can be as easy as installing a grid client on additional machines.
In the case of the neural scientist, she was able to scale her needs from two data sets to 100 data sets in the same timeframe, under the same budget.
Both! Well, sort of.
In conversation, it’s pretty common to use grid and distributed interchangeably. Fundamentally, both terms refer to fairly similar concepts. They are both systems for organizing and networking computational resources.
However, if you really want to split hairs, grid computing is the overall collection of distributed networks. Grid computing itself is a distributed network of distributed networks. Meta enough for you?
This has been a very macro understanding of grid computing. In actuality, is a multifaceted system for organizing a range of dynamic and individual parts, in order to get the most out them. Each component of the computing grid is layered with complexity and utility, not unlike the multiple pieces required in a public power grid.
Similar to a public utility, how it works is a beast of its own. However, the real impact is the overall accessibility. Because, like a public utility, grid computing is increasingly becoming a plug-and-play service.
The next evolution of grid computing is likely in the blockchain. Grid computing relies on multiple stakeholders trusting each other. Already, projects like Cosmos Network are creating decentralized grid systems that foster network interoperability and leverage the powers of a grid computing network.
Originally published at coincentral.com on December 4, 2018.
Follow us on twitter @realcoincentralFollow me on twitter @marshalletaylor
Curated editorials from the friendly decentralized…
844 
1
844 claps
844 
1
Curated editorials from the friendly decentralized CoinCentral journalists.
Written by
Canadian Creative and Professional Content Creator/Marketer | Armchair Economist | Travel Enthusiast
Curated editorials from the friendly decentralized CoinCentral journalists.
"
https://medium.com/iotforall/cloud-computing-vs-fog-computing-aa94cbc4b827?source=search_post---------4,"There are currently no responses for this story.
Be the first to respond.
What’s best for IoT?
The real business value enabled by the Internet of Things is derived not really from the data, but from insights that facilitate real-time actions that increase asset efficiency, reliability and utilization.
That value takes many forms with IoT use cases that range from supply chain management and manufacturing automation to parking and waste management solutions.
But, to actually save time and money with IoT, the data insight has to come from somewhere — typically centralized, scalable cloud computing platforms tailored for the device, connectivity and data management needs of the Internet of Things.
At a basic level, cloud computing is a way for businesses to use the internet to connect to off-premise storage and compute infrastructure. In the context of the Internet of Things, the cloud provides a scalable way for companies to manage all aspects of an IoT deployment including device location and management, billing, security protocols, data analysis and more.
Cloud services also allow developers to leverage powerful tools to create IoT applications and deliver services quickly. On-demand scalability is key here given the grand vision of IoT; a world saturated with smart, connected objects.
Many major technology players have brought cloud-as-a-service offerings to market for IoT. Microsoft has its Azure suite, Amazon Web Services, a giant in cloud services, has an IoT-specific play, IBM offers access to the Watson platform via its Bluemix cloud, and the list goes on and on.
Regardless of the specific product, the commonality is the ability to access flexible IT resources without having to make big investments into hardware and software and the management that comes with it.
However, for services and applications that require very low latency or have a limited “pipe” through which to pipe data, there are some downsides to the cloud that are better addressed at the edge.
Monica Paolini, president of Senza Fili Consulting, wrote on LinkedIn, “In recent years, there has been a strong push to move everything to a centralized cloud, enabled by virtualization and driven by the need to cut costs, reduce the time to market for new services, and increase flexibility. In the process, we lost sight of how important the location of functionality is to performance, efficient use of network resources and subscriber experience. Physical distance inevitably increases latency.”
The OpenFog Consortium was organized to develop a cross-industry approach to enabling end-to-end IoT deployments by creating a reference architecture to drive interoperability in connecting the edge and the cloud. The group has identified numerous IoT use cases that require edge computing including smart buildings, drone-based delivery services, real-time subsurface imaging, traffic congestion management and video surveillance. The group released a fog computing reference architecture in February 2017.
Helder Antunes, OpenFog Consortium chairman and senior director of the corporate strategic innovation group at Cisco, said the release will drive IoT adoption by providing a “universal framework. While fog computing is starting to be rolled out in smart cities, connected cars, drones and more, it needs a common, interoperable platform to turbocharge the tremendous opportunity in digital transformation.”
Another group that was formed to drive edge interoperability is the Edge X Foundry, an open source consortia approach managed by The Linux Foundation and seeded with some 125,000 lines of code developed internally by Dell Technologies.
If you want to dive deeper into how open source initiatives like the Edge X Foundry are impacting the internet of things, you can check out our primer, “Open Source and the IoT: Innovation through Collaboration.”
Let’s consider autonomous driving. Cellular networks will connect vehicles, equipped with advanced LiDAR, image processing, and other self-driving technologies with other vehicles, pedestrians, smart infrastructure, and an array of cloud-based services to support in-car entertainment, predictive maintenance, remote diagnostics, and the like.
It’s fine for your car to access your cloud-based Netflix account or maintain operational and maintenance logs, but the cloud isn’t necessarily the best place for mission critical decisions that could help a vehicle avoid a collision on the highway — given the time (latency) demands, this type of processing is best handled at the network edge.
To facilitate this type of hybrid approach, Cisco and Microsoft have integrated the former’s Fog Data Services with the latter’s Azure IoT cloud platform. The combo joins edge analytics, security, control, and data management with centralized connectivity, policy, security, analytics, app development and more.
In a recent blog post, Cisco Head of IoT Strategy Macario Namie noted, “One of the beautiful outputs of connecting ‘things’ is unlocking access to real-time data. Next is turning that data into information and, more importantly, actions that drive business value. In trying to do so, companies are finding themselves deluged with data.
So much so that demand for vast compute and storage needs have arisen, very nicely handled by public cloud providers. But the cost of transport and speed of processing has also increased, which is challenging for many uses cases such as mission-critical services…As a result, many IoT initiatives are now distributing this computing power across the edge network, data centers, and public cloud.”
Originally published at www.link-labs.com
Expert analysis, simple explanations, and the latest…
207 
4
207 claps
207 
4
Expert analysis, simple explanations, and the latest advances in IoT, AR/VR/MR, AI & ML and beyond! To publish with us please email: contribute@iotforall.com
Written by
Entreprenuer, wireless nerd, Making the Things of Internet of Things. Founder of Link Labs. Dad. Sailor. Former Submarine Officer, IC, USNA, Oxford.
Expert analysis, simple explanations, and the latest advances in IoT, AR/VR/MR, AI & ML and beyond! To publish with us please email: contribute@iotforall.com
"
https://medium.com/javarevisited/top-10-courses-to-learn-amazon-web-services-aws-cloud-in-2020-best-and-free-317f10d7c21d?source=search_post---------5,"There are currently no responses for this story.
Be the first to respond.
Hello guys, if you want to learn Amazon Web Services, popularly known as AWS, and looking for some excellent resources like books, courses, and tutorials then you have come to the right place.
In this article, I am going to share some of the best Amazon Web Services or AWS courses which will help you to learn this revolutionary and valuable technology free of cost.
Unlike many other free courses you will find on the internet, these are genuine free AWS courses which are made free by their authors and instructors for the promotional and educational purpose.
You just need to enroll with them and then you can learn AWS at any time, at any place, and on your own schedule.
But, if you are completely new to the AWS domain or Cloud let me give you a brief overview of Amazon Web Services and its benefits over traditional infrastructure setup.
The AWS is nothing but a cloud service provider by Amazon. It’s a revolutionary change because it allows you to develop an application without worrying about hardware, network, database and other physical infrastructure you need to run your application.
For example, if you want to develop an online application for your business, you need a lot of servers, databases, and other infrastructure.
You need to rent a data center, buy servers, routers, databases, and other stuff to get the start, which is pain and pose a big hurdle for many entrepreneurs. AWS solves that problem by renting their infrastructure and servers with a nominal cost of what you incur in setting up yourself.
Amazon has built many data center around the world to support its core business i.e. the E-Commerce business which powers Amazon.com and AWS just emerge from that.
AWS allows Amazon to monetize its massive infrastructure by renting out to people and business who needs that.
It created the phenomenon of Infrastructure as Service because now you only need to pay for infrastructure you are actually using.
For example, if you set up your own data center and buy 10 servers and 10 databases but end up using only 5 of them then remaining are waste of money and they also cost in terms of maintenance. With Amazon Web Service, you can quickly get rid of them.
Similarly, you can scale pretty quickly if you are hosting your application on the cloud-like on Amazon Web Service.
If you see that your traffic is increasing then you can quickly order new servers and boom your new infrastructure is ready in hours unlike days and months with the traditional approach.
You also don’t need to hire UNIX admins, Database Administrator, Network admins, Storage guys, etc, All that is done by Amazon and because Amazon is doing it in a scale, it can offer the same service at a much lower cost.
In short, Amazon Web Service gives birth to the concept of Cloud which allows you to bring your business online without worrying about hardware and infrastructure which powers them.
Now that we know what is AWS and what are the benefits it offers in terms of Infrastructure as service, it’s time to learn different Amazon service in-depth and that’s where these courses will help you.
You can join these courses if you want to learn about AWS and Cloud in general or you are preparing for various AWS certifications like AWS Solutions Architect, AWS SysOps Admin, or AWS Developer (associate). These courses will help you to kick-start your journey to the beautiful world of AWS.
Though, if you are preparing for the AWS Solution Architect exam, I highly recommend you to join the AWS Certified Solutions Architect — Associate 2021 course on Udemy by Cloud Guru Ryan Kroonenburg, one of the best course on AWS. It’s not free but it’s completely worthy of your money.
Anyway, here is the list of free online courses to learn AWS or Amazon Web Service.
This is one of the best courses to learn Amazon Web Service and it’s FREE. It follows the principle of learning by example and real-world scenarios and that reflects in their course.
This is a short course, having just 2 hours worth of material but it’s power-packed and intense. There is no-nonsense talk or flipping, the instructor Dhruv Bias always means business.
Even if you check the preview chapter you will learn a lot about what is AWS and what problem it solves, what benefits it offers and why should you learn it.
Here is the link to join this course — Amazon Web Services — Learning and Implementing AWS Solution
The course is divided into 5 sections, in the first section, you will get an Introduction of AWS and Overview of the course while remaining section focus on different Amazon Web Service offering like Amazon S3(Simple Storage Service), Amazon AWS EC2 (Elastic Cloud Compute) and Databases like AWS DynamoDB or RDB.
Overall a great free course to learn what is AWS and its different services. I highly recommend this course to any programmer who wants to learn about Cloud and Amazon Web Service (AWS).
Btw, if you are preparing for the Amazon Web Service Solution Architect exam (code SAA- C01) I highly recommend Cloud Guru Ryan’s AWS certification course as well.
This is another awesome free course to learn Amazon Web Service on Udemy. It’s from LinuxAcademy and taught by instructor Thomas Haslet.
The series is actually divided into 2 courses: AWS Concepts and AWS Essentials.
This is the first part while the next course, which is also free is the second part of this series. In this course, you will learn the concepts of Cloud Computing and Amazon Web Service from instructor Thomas Haslet who is also a certified AWS developer.
He holds all three AWS certification for the associate level like
This course is for the absolute beginner, someone who has never heard about Cloud or AWS but understands what is hardware, server, database and why you need them. In this course, you will not only learn essential concepts but also build your vocabulary.
You will find answers to all of your basic AWS question e.g. what is Cloud? What is AWS? What is AWS Core Services? What is the benefit of AWS? Why should you use it? in this course.
In short, a perfect course if you are new to the cloud. You will learn about VPC, EC2, S3, RDS and other Cloud terminology in simple language, and if you are preparing for AWS certifications like Solution architect associate level one, I also recommend to check Whizlab’s AWS Solution Architect Practice Test + Course along with Ryan’s course, both are a very valuable resource to pass this prestigious exam.
This is the second part of the free AWS courses by LinuxAcademy on Udemy. If you have not read the first part, AWS Concepts then you should finish that first before joining this course, though it’s not mandatory.
This course goes into a little bit more details into AWS Core Services then the previous one. It also has a lot of materials with around 50 lectures covering different cloud and AWS concepts.
The course is divided into 14 section and each one covering a key AWS concept e.g. Identity and Access Management (IAM), Virtual Private Cloud (VPC), Simple Storage Service (S3), Elastic Compute Cloud (EC2), Database, Simple Notification Service (SNS), Auto Scaling, Route 53, Serverless Lambdas, etc.
In short, one of the most comprehensive AWS course which is also free. More than 70 thousand students have already enrolled in this course and learning AWS and I also highly recommend this one to anyone interested in Cloud and AWS.
This is another useful and exciting free AWS course you will love to join on Udemy. In this course instructor Mike Chambers, an early adopter of Cloud and AWS explains the basics of Amazon Web Services.
The course is also very hands-on, you will start up signing up to AWS, creating your account, and then using the command-line interface to control AWS.
You will also learn to navigate around the AWS console, build Windows and Linux Servers and create a Wordpress website in 5 minutes which demonstrates how you can leverage Cloud for your database, server, and storage requirement.
The course also teaches you how to build a simple AWS serverless system.
The course not only focuses on AWS technology and terminology but also teaches you basics e.g. the true definition of Cloud Computing and How AWS fits into the Cloud model. You will also get some real pictures to find where is AWS located in the world.
But, most importantly you will gain some hands-on experience in essential AWS services like
In short, one of the best free course to learn Amazon Web Service and Cloud Computing basics.
This is another short but truly hands-on AWS course that will teach you how to perform a common task on the AWS interface. In just 2 hours’ time, you will learn how to launch a Wordpress website based on Amazon EC2 service.
You will also learn how to create a NodeJS based web application, sending an email with AWS SES, uploading a file to AWS S3, the storage solution provided by Amazon, and finally, learn to create and connect to an AWS relational database server.
In short, a great course if you want to use AWS for hosting your application or want to learn how you can leverage Cloud to host your application and most importantly it’s FREE.
This is a free AWS course by Andrew Brown of ExamPro.com. This is a course for early startups and people who want a practical way to apply AWS, instead of going for just AWS Certification!
It covers some AWS Services people normally overlook but are incredibly useful. You will learn how to get started with AWS. This is a practical guide for early-stage startups interested in using AWS. You will learn how to create an AWS account, deploy an application, budget scaling applications, and more!
It's completely free and you can watch this on @Youtube on freeCodeCamp youtube channel.
This is another free course to learn AWS by Andrew Brown to prepare for AWS Certified Solution Architect but you can also use this to learn about AWS.
This Study Course is designed to help you pass the AWS Solutions Architect Associate Certification. It was recorded at the end of 2019 so if you’re in 2021 you can absolutely use this course for study.
All Videos are part of a 275+ Video Youtube Playlist
If you want to start earning some developer certifications, the AWS Certified Cloud Practitioner may be a good starting point for you, and this free course can help you achieve that.
For starters, it’s not super technical. You can earn this certification without knowing how to code. The official certification description recommends you have some basic IT knowledge and “six months of experience with the AWS Cloud in any role, including technical, managerial, sales, purchasing, or financial.”
But if you watch this course and follow along on your own computer, you should be OK. The course will break down most of the important concepts in detail for you.
Again this is completely free and you can watch freely on freeCodeCamp Youtube Channel.
This is an excellent course to learn AWS Fundamentals and Cloud Computing fundamentals with a focus on developing Cloud-Native applications. It will introduce you to AWS core services and infrastructure and the best thing is that its offered by the AWS itself.
It’s a hands-on course with a demonstration and you will how the AWS cloud infrastructure is built, understand Amazon Elastic Compute Cloud (Amazon EC2) and Amazon Lightsail compute services. You will also learn about networking on AWS, including how to set up Amazon Virtual Public Cloud (VPC) and different cloud storage options, including Amazon Elastic Block Storage (EBS), Amazon Simple Storage Service (S3) and Amazon Elastic File Service (EFS).
Later in the course, you’ll learn about AWS Database services, such as Amazon Relational Database Service (RDS) and Amazon DynomoDB. Your instructors will also walk you through how to monitor and scale your application on AWS using Amazon CloudWatch and Amazon EC2 Elastic Load Balancing (ELB) and Auto Scaling.
Lastly, you’ll learn about security on AWS, as well as how to manage costs when using the AWS cloud platform. While on cost, this is a free-to-audit course on Coursera which means it's free for learning but you need to pay if you need a certificate and access to quizzes and assessments.
coursera.com
This is a four-week course that focuses on migrating workloads to AWS. You will focus on analyzing your current environment, planning your migration, AWS services that are commonly used during your migration, and the actual migration steps.
There are also Hands-on labs, though not required for this class. Access to the labs is limited to paid enrolled students. You can audit this course without taking the labs.
If you are new to AWS, we strongly suggest that you take the “AWS Fundamentals: Going Cloud-Native” course available on Coursera to provide an introduction to AWS concepts and services.
coursera.com
And, if you find Coursera courses useful, which they are because they are created by reputed companies and universities around the world, I suggest you join the Coursera Plus, a subscription plan from Coursera which gives you unlimited access to their most popular courses, specialization, professional certificate, and guided projects. It cost around $399/year but its complete worth of your money as you get unlimited certificates.
This is an interactive course to learn about the core AWS’s services like compute, storage, networking services and how they work with other services like Identity, Mobile, Routing, and Security.
This course has been designed by three AWS Solution Certified Architects who have a combined industry experience of 17 years. We aim to provide you with just the right depth of knowledge you need to have.
If you don’t know Educative is another online learning platform that is gaining a lot of traction for its text-based, interactive learning courses. Reading is generally faster than watching and If you prefer reading text than watching videos then this is the platform to checkout.
It has some of the best courses to prepare for coding interviews like Grokking the Coding Interview: Patterns for Coding Questions and Grokking the system design interview.
It also has a lot of free resources like this free JavaScript tutorial to learn essential technologies. You can register for this course for free but if you like to take full advantage of the platform, I suggest you buy its membership which costs $18 (50% discount now) monthly, completely worth it for a programmer and software engineers where continuous learning is required.
www.educative.io
This is one of the best courses to learn practical AWS you will find online. This course is created by a former Amazon engineer with 15 years of experience working on AWS.
This is not your typical AWS reference course. You won’t find most of the knowledge that’s shared here in the AWS docs. The goal here is to help you realize which AWS features you’d be foolish not to use — features that have passed the test of time by being at the backbone of most things on the Internet.
In this course, you’ll learn a technique used to help make reliable technical choices without getting paralyzed in the face of so many options. You’ll start by going through the most fundamental services AWS offers such as DynamoDB, S3, EC2. Each section breaks down how it’s used, the pros and cons, why you should (or shouldn’t) be using it, and more.
Here is the link to sign up for this course — The Good Parts of AWS: Cutting Through the Clutter
You can either buy this course or you can get and Educative membership to access this course. If you ask me, I suggest you get Educative Subscription which costs $17(50% discount now) monthly, completely worth it for a programmer and software engineers where continuous learning is required. Thanks to The Educative Team for this awesome course.
This course is also available as e-Book on Gumroad, if you like to read books then you can also check out The Good Parts of AWS eBook. There is a 20% discount available if you buy the book using this link.
gumroad.com
That’s all about some of the best and free courses to learn Amazon Web Services or AWS. These are absolutely free courses on Udemy but you need to keep that in mind that sometimes instructor converts their free course to paid course once they achieve their promotional target.
This means you should check the price of the course before you join and if possible join early so that you can get the course free. Once you are enrolled in the course it’s free for life and you can learn at any time from anywhere.
I generally join the course immediately even if I am not going to learn AWS currently. This way I can get access to the course and I can start learning once I have some time or priority changes.
Other Free Programming Resources you may like to explore:
Top 5 Courses to Crack AWS Solution Architect Exam5 Free JavaScript Courses for Web Developers5 Free Courses to Learn React JS for JavaScript Developers5 Free Courses to Learn Core Spring, Spring Boot, and Spring MVC5 Free Docker Courses for Java and DevOps Engineer5 Courses to learn Maven And Jenkins for Java Developers3 Books and Courses to Learn RESTful Web Services in Java5 Courses to Learn Blockchain Technology for FREE7 Free Selenium Webdriver courses for Java and C# developers5 Free course to learn Servlet, JSP, and JDBC5 Free TypeScript Courses for Web Developers5 Free Big Data Courses to Learn Hadoop and Spark
Thanks for reading this article so far. If you like these AWS courses then please share with your friends and colleagues. If you have any questions or feedback then please drop a note.
P. S. — If you are preparing for the AWS Solution Architect — Associate Exam (SAA-C01), I strongly recommend you to go through AWS Certified Solutions Architect — Associate 2021 course on Udemy by Cloud Guru Ryan Kroonenburg. It’s not free but it’s completely worth your money.
click.linksynergy.com
Medium’s largest Java publication, followed by 14630+ programmers. Follow to join our community.
528 
4
528 claps
528 
4
A humble place to learn Java and Programming better.
Written by
I am Java programmer, blogger, working on Java, J2EE, UNIX, FIX Protocol. I share Java tips on http://javarevisited.blogspot.com and http://java67.com
A humble place to learn Java and Programming better.
"
https://read.acloud.guru/the-true-cost-of-cloud-a-comparison-of-two-development-teams-edc77d3dc6dc?source=search_post---------6,"Over the last few months, I’ve talked with a number people who are concerned about the average cost of public cloud services in comparison to the price of traditional on-premises infrastructure. To provide some insights from the discussion, let’s follow two development teams within an enterprise — and compare how they would approach building a similar service.
The first team will deploy their application using traditional on-premises infrastructure, while the second will leverage some of the public cloud services available on AWS.
The two teams are being asked to develop a new service for a global enterprise company that currently serves millions of consumers worldwide. This new service will need to meet theses basic requirements:
As far as new services go, this seems to be a fairly standard set of requirements — nothing that would intrinsically favor either traditional on-premises infrastructure over the public cloud.
When it comes to scalability, this new service needs to scale to meet the variable demands of consumers. We can’t build a service that might drop requests and cost our company money and cause reputational risk.
The Traditional TeamWith on-premises infrastructure, the architectural approach dictates that compute capacity needs to be sized to match your peak data demands. For services that features a variable workload, this will leave you with a lot of excess and expensive compute capacity in times of low utilization.
This approach is wasteful — and the capital expense will eat into your profits. In addition, there is a heavy operational costs with maintaining your fleet of underutilized servers. This is a cost that is often overlooked — and I cannot emphasize enough how much money and time will be wasted supporting a rack of bare-metal servers for a single service.
The Cloud TeamWith a cloud-based autoscaling solution, your application scales up and down in-line with demand. This means that you’re only paying for the compute resources that you consume.
A well-architected cloud-based application enables the act of scaling up and down to be seamless — and automatic. The development team defines auto-scaling groups that spin up more instances of your application based on high-CPU utilization, or a large number of requests per second, and you can customize these rules to your heart’s content.
When it comes to resiliency, hosting a service on infrastructure that resides within the same four walls isn’t an option. If your application resides within a single datacenter — then you are stuffed when (not if) something fails.
The Traditional TeamTo meet basic resiliency criteria for an on-premises solution, this team would a minimum of two servers for local resiliency — replicated in a second data center for geographic redundancy.
The development team will need to identify a load balancing solution that automatically redirects traffic between sites in the event of saturation or failure — and ensure that the mirror site is continually synchronized with the entire stack.
The Cloud TeamWithin each of their 50 regions worldwide, AWS provides multiple availability zones. Each zone consists of one of more fault-tolerant data centers — with automated failover capabilities that can seamlessly transition AWS services to other zones within the region.
Defining your infrastructure as code within a CloudFormation template ensures your infrastructure resources remain consistent across the zones during autoscaling events — and the AWS load balancer service requires minimal effort to setup and manage the traffic flow.
Security is be a fundamental requirement of any system being developed within an organization. You really don’t want to be one of the unfortunate companies dealing with fallout from a security breach.
The Traditional TeamThe traditional team will incur the ongoing costs of ensuring that the bare-metal servers that’s running their services is secure. This means investing in a team that is trying to monitor, identify, and patch security threats across multiple vendor solutions from a variety of unique data sources.
The Cloud TeamLeveraging the public cloud does not exempt yourself from security. The cloud team still has to remain vigilant, but doesn’t have to worry about patching the underlying infrastructure. AWS actively works combat against zero-day exploits — most recently with Spectre and Meltdown.
Leveraging the identify management and encryption security services from AWS allows the cloud team to focus on their application — and not the undifferentiated security management. The API calls to AWS services are fully audited using CloudTrail, which provides transparent monitoring.
Every infrastructure and application service being deployed need to be closely monitored with aggregated realtime data. The teams should have access to dashboards that provide alerts when thresholds are exceeded, and offer the ability to leverage logs for event correlation for troubleshooting.
The Traditional TeamFor traditional infrastructure, you will have to set up monitoring and alerting solutions across disparate vendors and snowflake solutions. Setting this up takes a hell of a lot of time — and effort and getting it right is incredibly difficult.
For many applications deployed on-premises, you may find yourself searching through log files stored on your server’s file-system in order to make sense of why your application is crashing. A lot of time will be wasted as teams need to ssh into the server, navigate to the unique directory of log files, and then grep through potentially hundreds of files. Having done this for an application that was deployed across 60 servers — I can tell you that this isn’t a pretty solution.
The Cloud TeamNative AWS services such as CloudWatch and CloudTrail make monitoring cloud applications an absolute breeze. Without much setup, the development team can monitor a wide variety of different metrics for each of the deployed services — making the process of debugging issues an absolute dream.
With traditional infrastructure, teams need build their own solution, and configure their REST API or service to push log files to an aggregator. Getting this ‘out-of-the-box’ is an insane improvement fo productivity.
The ability to accelerate time-to-market is increasingly important in today’s business environment. The lost opportunity costs for delayed implementations can have a major impact on bottom-line profits.
The Traditional TeamFor most organizations, it takes a long time to purchase, configure and deploy hardware needed for new projects — and procuring extra capacity in advance leads to massive waste due to poor forecasting.
Most likely, the traditional development team will spend months working across the myriad of silos and hand-offs to create the services. Each step of the project will require a distinct work request to database, system, security, and network administrators.
The Cloud TeamWhen it comes to developing new features in a timely manner, having a massive suite of production-ready services at the disposal of your keyboard is a developer’s paradise. Each of the AWS services is typically well documented and can be accessed programmatically via your language of choice.
With new cloud architectures such a serverless, development teams can build and deploy a scalable solution with minimal friction. For example, in just a couple of days I recently built a serverless clone of Imgur that features image recognition, a production-ready monitoring/logging solution built-in, and is incredibly resilient.
If I had to engineer the resiliency and scalability myself, I can guarantee you that I would still be developing this project — and the final product would have been nowhere near as good as it currently is.
Using serverless architecture, I experimented and delivered the solution in a less time that it takes to provision hardware in most companies. I simply glued together a series of AWS services with Lambda functions — and ta-da! I focused on developing the solution, while the undifferentiated scalability and resiliency is handled for me by AWS.
When it comes to scalability, the cloud team is the clear winner with demand-drive elasticity — thus only paying for compute power that they need. The team no longer needs dedicated resources devoted to maintaining and patching the underlying physical infrastructure.
The cloud also provides development teams a resilient architecture with multiple availability zones, security features built into each service, consistent tools for logging and monitoring, pay-as-you-go services, and low-cost experimentation for accelerated delivery.
More often than not, the absolute cost of cloud will amount to less than the cost of buying, supporting, maintaining, and designing the on-premises infrastructure needed for your application to run — with minimal fuss.
By leveraging cloud we can move faster and with the minimal upfront capital investment needed. Overall, the economics of the cloud will actively work in your favor when developing and deploying your business services.
There will always be a couple of niche examples where the cost of cloud is prohibitively more expensive than traditional infrastructure, and a few situations where you end up forgetting that you have left some incredibly expensive test boxes running over the weekend.
Dropbox saved almost $75 million over two years by building its own tech infrastructure — After making the decision to roll its own infrastructure and reduce its dependence on Amazon Web Services, Dropbox… — www.geekwire.com
However, these cases remain few and far between. Not to mention that Dropbox initially started out life on AWS — and it was only after they had reached a critical mass that they decided to migrate off the platform. Even now, they are overflowing into the cloud and retain 40% of their infrastructure on AWS and GCP.
The idea of comparing cloud services to traditional infrastructure-based of a single “cost” metric is incredibly naive — it blatantly disregards some of the major advantages the cloud offers development teams and your business.
In the rare cases when cloud services result in a greater absolute cost than your more traditional infrastructure offering — it still represents a better value in terms of developer productivity, speed, and innovation.
Master modern tech skills, get certified, and level up your career. Whether you’re starting out or a seasoned pro, you can learn by doing and advance your career in cloud with ACG.
I’m very interested in hearing your own experiences and feedback related to the true cost of developing in cloud! Please drop a comment below, on Twitter at @Elliot_F, or connect with me directly on LinkedIn.
Get more insights, news, and assorted awesomeness around all things cloud learning.
"
https://javascript.plainenglish.io/unikernel-vs-container-vs-operating-system-side-by-side-comparison-3b8d6d93665d?source=search_post---------7,"You have 2 free member-only stories left this month. Sign up for Medium and get an extra one
The process of deploying software to production evolves constantly. Just a few decades ago, everyone used virtual machines to host and manage the infrastructure. Lately, the industry shifted towards using containers with systems such as Docker and Kubernetes. The next logical step in this progression is Unikernels, which combines the best of virtualization and containerization.
The reason for the existence of all these tools is the problems that arise while deploying software to production. Some of the common issues that need to be addressed are the following:
This list is not exhaustive, but these are some common problems that had no easy solution until recently.
Before containers came around, the virtualization stack was the to-go tool for large-scale application deployment. To understand the difference, you first have to understand what an OS is and what Kernel is.
Operating System is system software that manages hardware and controls the running software. Most importantly, it gives (and takes away) hardware access to processes.
A kernel is the main component of an OS. The rest of the OS is there to load the kernel. It resides constantly in memory and has complete control of both hardware and software.
Now, virtualization is a way to run multiple operating systems on shared hardware. One OS, called host, runs a hypervisor, which lets guest operating systems safely access the hardware. Sometimes there is no host OS. A Type-1 hypervisor runs directly on hardware and loads virtual machines.
A typical deployment stack consisted of a number of virtual machines running on multiple servers. You can see a simplified diagram of such a stack above. Sometimes it is a per-app VM, per-client VM, or per-concept VM (one for database, one for cache, etc.). You can notice how much resources goes to waste: every VM has to run its own instance of an operating system (Linux, most often), resulting in it a duplication of responsibility. You can also imagine how hard it is to manage such an infrastructure: you can effectively end up with 100+ servers (for example, a SaaS platform) which are all independent virtual machines.
The deployment process for a VM-based approach can go one of two ways. The build system can produce a complete image of a VM with software built-in and the VM just reboots once the update arrives. Or, the build system produces only the software bundle, which is uploaded to servers using a collection of scripts. The downside of both these approaches is complex setup and, eventually, inconsistencies between VMs. A real nightmare starts when you need to update the OS version. You would have to do it twice: firstly, the host OS, and then the guest OS.
This approach has its upsides, however. This way you get complete control of the environment for every aspect of the system and can configure it to suit your needs perfectly. It also simplifies debugging, as you can directly connect to a VM and use it as a regular workstation.
With the emergence of container runtimes such as Docker and container orchestration systems such as Kubernetes, a new era in software infrastructure manifested itself. You may be familiar with containers, and some of you even used them, so now I will explain how they work.
Containers try to achieve the same concept as virtual machines but eliminating duplication of effort between machines. Do you remember the concept of kernel from the previous section? Well, instead of loading an entire operating system for an app, Docker lets containers use the kernel of the host OS while allowing them to sideload app-specific libraries and programs. Since Linux kernel is so common and does not really change across distributions, complete platform independence is achieved. By tweaking the container and its image you can fine-tune the specific libraries and configuration your app will use, resulting in performance gains without the overhead of running an entire OS. A typical container stack can look like this:
CliveLess overhead from running an entire OS means you can run more containers (more apps) on the same hardware. It is important to note that Docker only works with Linux kernels, due to their robust namespace support. Namespaces standardize the available resources and can control what process has access to what resources. Thus, every container gets exactly the amount of CPU time, memory, storage, and networking as it needs. When you run Docker on macOS/Windows, it spins up a Linux VM under the hood, that is why it is so much slower on these platforms.
Of course, the container-based approach has its downsides. The software has to be adapted for usage in containers (containerized), and this can get tricky, especially with legacy codebases. Also, since Docker uses the Linux kernel, you cannot use it to run legacy software designed for Windows Server (before .NET Core was introduced). Containers have many more configurations for resource allocation and interop capabilities, so it is very easy to screw up setting up a moderate infrastructure. Lastly, since Docker containers share the same kernel, you cannot use a custom one (for example, a Realtime Linux kernel for transaction-based software).
One of the advantages of containers is the ability to easily run them on your development machine. Developers like this very much, so let’s keep that. The deployment process itself is also much easier, you just upload pre-built containers to a container repository and your production servers pull the updated version.
While containers are quickly becoming industry standard, unikernels still have a lot to go through. Unikernels try to push the concepts of containers even further, eliminating the need for an OS altogether. Unikernels achieve this by using library operating systems. Library operating systems provide similarly (but limited to single-user, single address space) features to the regular OS, but in the form of libraries that your app uses. So, instead of maintaining a resident kernel in memory, everything is managed through pre-built binary libraries. Unikernels do not handle resource allocation, though, so they still require a hypervisor:
You can get your head around it by thinking of hypervisor as a regular OS, and unikernels as processes running on it. The difference is that all app-specific system calls are pushed as close to the app as possible, while hypervisor handles only direct hardware interop.
As you can imagine, unikernels have even less overhead than containers and should be more performant. Also, by eliminating the use of a multi-user, multiple address space kernel, the security is improved drastically. Unikernels are a truly amazing technology, but they are far from being production-ready yet. Some of the issues that need to be addressed first are these:
Some notable Unikernel projects include ClickOS, runtime.js, and Clive.
Thank you for reading, I hope I got you interested in the subject of Unikernels. Stay tuned for more posts on this revolutionary technology!
Show some love by subscribing to our YouTube channel!
New JavaScript and Web Development content every day. Follow to join our +2M monthly readers.
232 
232 claps
232 
Written by
Software Developer | Writer | Open Source Evangelist
New JavaScript and Web Development content every day. Follow to join our +2M monthly readers.
Written by
Software Developer | Writer | Open Source Evangelist
New JavaScript and Web Development content every day. Follow to join our +2M monthly readers.
"
https://medium.com/aelfblockchain/aelf-a-decentralized-platform-for-cloud-computing-78165a5ec79d?source=search_post---------8,"There are currently no responses for this story.
Be the first to respond.
aelf has been going strong since being introduced into the crypto world in the second half of 2017. Over the last three months this project has undergone some astounding growth, major updates, partnerships and a strong following spreading across multiple social media platforms. As such, it is high time for a review overview. This is aelf — Year in review.
Project Summary — What is aelf?
Before I go any further, let me give a brief introduction to the aelf project. aelf, for all intents and purposes, is creating a new blockchain which will resolve 3 main blockchain problems which has been restricting this technology from being adopted in more mainstream uses. These problems are limited performance, lack of resource segregation, and a properly working governance model. By resolving these issues, aelf will be bringing blockchain technology into business scenarios with full force.
In response to this, they have created a blockchain which can be integrated to side chain specific business scenarios. The aelf blockchain and sidechains will possess the following qualities:
Rebrand — Out with the Old, In with the New
aelf was originally launched as Grid, however due to confusion with a similar project called Grid+, it was rebranded as aelf late November 2017. Another confusion which was cleared up early in aelf’s life was the complete separation from the Polkadot project.
From Presentations to Investors
Throughout the Christmas and New Year season, Co-Founder and COO, Zhuling Chen led the worldwide charge of aelf presentations at multiple conferences. One of the first of these was the presentation at Consensus Invest 2017. aelf’s vision presented by Zhuling was to establish a ‘Central Business District’ where different industry applications have dedicated chains to serve their needs. Thanks to these impressive presentations; aelf’s private sale, to a focused investor group, oversubscribed by more than 6 times their cap within 2 weeks. These investors included 1kx Capital, Alphabit, BlockTower, FBG Capital, Galaxy Partners, Hashed, Hyperchain, LinkVC, and Signam Capital. aelf has attracted investors from all around the world which indicates that it’s looking to branch it’s operations and outreach globally instead of focusing on one region.
Candy, Candy for Everyone!
An exceptional marketing scheme developed by the aelf team was the Candy Airdrop Program. This airdrop project rewards users for completing simple tasks like re-tweeting an aelf tweet or logging onto the Candy dashboard daily. As of March 4th 2018, the Candy system’s success is demonstrated below:
Partnerships — They all want to be One
Another example of the combined marketing success and clear real world application for the aelf project is the continuous partnership announcements. aelf is targeting existing companies that are looking for Blockchain solutions. Companies that already have a customer base which will help increase awareness among customers that are not yet familiar with Blockchain. Over the last two months, partnerships and agreements have come from Decent, Youlive, Merry Merkle Tree, Wachsman, Theta, U Network, DATx and CNN.
Social Media — An Army has been Raised
Similarly to the business partnerships, the social media and community component to this project has not been overlooked either. Within a few months:
Behind the scenes — What Keeps this Machine Running
Such huge efforts on the frontline at aelf is just an indication to the work ethic of the whole team. Behind the front lines, the development team have been working tirelessly to improve and refine the overall aelf system. This includes things like SDK encapsulation and test work for asset management in relation to the Asset Chain, Smart Contract invoking tests and Chain initialization tests.
One Small Step for Man, One Giant Leap for Mankind
2018 is proving to be a busy year for the aelf team. Not only are they ramping up their marketing and online presence, they are also aiming to release minor/major upgrades every few months. By the start of 2019, aelf plans on officially launching their main product into multiple business scenarios.
I’m hooked, how do I get involved?
If you would like to hear the aelf team present, their next calendar event will be on Friday 23rd of March in Berlin, Germany. aelf will be presenting at ZK0x01 — The Zero Knowledge Summit at 9:30am. Alternatively, if you would like to invest in aelf, the tokens are currently listed on multiple exchanges including Huobi, Binance, OKEx, Bibox, Bancor Network, Kucoin and Gate.io. You can also get involved by following aelf on twitter and facebook, and joining the Telegram and WeChat groups.
Mappo has been investing and trading in fiat currencies since 2013. He has recently moved into the crypto world spreading his portfolio over long term coin investments, ICOs and day trading.
#Cryptocurrency #ICO #exchange #bitcoin #Crypto #blockchain #aelf #Mappo
Tomorrow is running on ælf.
904 
8
904 claps
904 
8
ælf, the next breakthrough in Blockchain.
Written by
ælf, the next breakthrough in Blockchain.
ælf, the next breakthrough in Blockchain.
"
https://medium.com/hackernoon/how-to-choose-a-cloud-computing-technology-for-your-startup-2527c0d68ce5?source=search_post---------9,"There are currently no responses for this story.
Be the first to respond.
Cloud computing technology becomes a standard when talking about developing applications nowadays. A few years ago, companies were enforced to have dedicated teams for configuring, running and maintaining server rooms which made it extremely difficult to scale up easily and offer a sustainable product. For small startups, it was even more difficult due to the lack of human resource as well as funding.
In present days, not only there are cloud computing technologies for almost every architecture you might imagine, but the cloud vendors also compete nonstop about our (the developers) attention. Most of the largest tech companies, like Google, Amazon and IBM launched cloud services in the past few years. They advertise, offer free tiers, present in tech conferences and conduct free-of-charge workshops for experiencing with their cloud solutions. They are aware that once you fall in love with their services, it will most likely be your favorite choice in every project for years to come.
So what is a cloud provider anyway? A cloud provider is an entity that offers cloud services for operating your application. Operating may include running servers, serving your application, hosting static files, providing database solutions, handling networking between servers, managing DNS and much more. Different cloud vendors offer different levels of abstractions in their services, usually defined as IaaS vs. PaaS.
IaaS, or infrastructure-as-a-service, refers to a low-level solution, like providing a Linux Ubuntu server with nothing installed on it. This kind of solutions is suitable for more advanced developers who have experience with designing, configuring and securing servers infrastructure in all aspects. IaaS services provide you with flexibility and scalability down the road, and this will most likely be the way to go when designing application for scale. This approach requires, as already mentioned before, at least one developer in your startup who has this skill-set, otherwise, your product will turn into a big mess sooner than later.
PaaS, or platform-as-a-service, refers to a fully-maintained and managed environment that is hidden under a layer of abstraction you should not even care of. The cloud vendor takes care of maintaining the servers needed for the operations for you, and you get high-level databases for storing your data, services for user authentication, endpoints for client side applications etc. This approach is much easier and faster to get up and running with, and typically satisfies most of the basic applications. You should take into consideration though, that for more complex architectures it might not be enough.
Generally speaking, both IaaS and PaaS are huge time-savers when dealing with deploying and serving applications. You are able to run a server with a click-of-a-button and usually pay per use. Scaling your servers can be done manually or even automatically using APIs when a peak in traffic suddenly occurs. You can be sure that you’re in a good company (as long as you choose wisely) and whatever you can imagine, you can basically create.
In early-stage startups, using cloud computing technologies became a standard because of the flexibility, the pricing models and the accessibility. Choosing the best cloud service for your startup is an essential task every technological entrepreneur must perform. As the head of development in your company, you should know the differences between the main alternatives, and choose the one that suits your product best.
Technical debts may stack up in a case of a bad decision. In addition, migrating an entire architecture from one cloud provider to another is not considered to be a trivial task at all. Therefore, you should be able to know the differences, experiment with each of the main alternatives and make a wise decision.
After examining and experiencing the best cloud providers out there:
and using them in a wide variety of project, I’ll take my top two: AWS and DigitalOcean and compare them using a set of parameters.
I’ve chosen these two cloud providers to be my best choice after grading each of them using the most important parameters when building a startup from the ground up:
How wide is the range of services offered?
Amazon Web Services: AWS has by far the widest range of services and it comes to offerings. If you don’t find a cloud computing technology under AWS manifest, you’ll most likely not find it anywhere else. AWS has many different IaaS and PaaS services dedicated to every task needed to be performed by a server, divided into organized categories. When using AWS you can be sure that your startup scalability is potentially endless. On the other hand, the offering might sometimes be confusing for beginners as it makes the getting started process a little longer. If your application has many custom components that are not trivial, AWS might be the cloud provider you should consider.
Grade: 5/5
DigitalOcean: DigitalOcean offers a relatively narrow range of services. As for IaaS, you can find droplets (servers), data storage units, networking and monitoring services. As for PaaS, you can easily deploy apps with zero configuration needed, like Node.js, Redis, Docker etc. Although the offering is very concise, I find it to be exactly what you need for more than 80% of the applications. In addition to the standard droplets, high CPU and high memory droplets are available for custom use, as well as backups and snapshots for each droplet. DigitalOcean team is working nonstop on increasing their offering based on the community requests. As a developer who uses DigitalOcean for quite a lot of time now, I can admit that their desire to satisfy their community is highly appreciated.
Grade: 4/5
Pricing models available and transparency
Amazon Web Services: AWS is based on a pay per use pricing model. Every cloud computing technology has its own unique pricing and a pricing calculator is available for trying to estimate your costs upfront. You might find this calculator a bit complex if you haven’t used AWS before. In order to estimate your costs up front you need to translate your servers architecture design into AWS terms, and then try and estimate by choosing the appropriate services from the sidebar. I find the wide range of offering sometimes overshadows the costs estimations, so I find it useful sometimes to start firing up services and tracking the costs inside the dashboard using pricing alerts. On the other hand, AWS offers a very useful free tier for 12 months that can help early-stage startups get up and running.
Grade: 3.5/5
DigitalOcean: DigitalOcean extremely transparent pricing models exist in two different yet similar approaches: pay per hour and pay per month. When using DigitalOcean you have no surprises. You can calculate the exact amount that will be charged, due to fixed prices for each droplet unit. Starting at $5/month for a 512MB droplet, DigitalOcean is suitable also for tiny side projects. Besides droplets and data storage units that are charged according to the resources allocated to you, networking, monitoring, alerts, DNS management and more, are completely free of charge. Bottom line, you pay only for the allocated resources, and you get a lot of useful extra components as a free of charge service.
Grade: 4.5/5
How easy is it to get up and running as well as to iterate
Amazon Web Services: AWS dashboard is quite comfortable once you get used to it. Because of the large amounts of services, you might find it a bit overcrowded in comparison to the other alternatives presented here. You can use the default settings for your services and then to get up and running relatively quickly, but if you’d like to dive deeper into details (also for reducing costs) you might find yourself spending quite a lot of time on configurations using AWS dashboard. On the other hand, in large-scaled applications, you can find the additional features available for each service extremely useful and necessary.
Grade: 4/5
DigitalOcean: DigitalOcean is branded for a reason as “Cloud computing, designed for developers”. As developers, we have so many things to take care of, especially when in charge of the end-to-end technological stack of our startup. Therefore, we need our cloud provider to be as simple as possible to setup. DigitalOcean’s user interface is the best I’ve used. It’s intuitive and let you get up and running in minutes even when using it for the first time. You don’t need to explore and scroll over too many features and options, just choose your Linux distribution, plan and geographic location, and you’re up and running in no time.
Grade: 5/5
Available resources and support team
Amazon Web Services: AWS has a very useful tutorials library. There are many tutorials, but ones sometimes seem to be less detailed and user-friendly than others. You need to be experienced with servers infrastructure design before accessing many of the AWS tutorials. So, it might take you some time to explore their library before you’ll be able to actually find what you’re looking for. On the other hand, their customer support team is extraordinary. AWS support agents are super responsive and sensitive and will answer your questions in a professional way.
Grade: 4/5
DigitalOcean: The tutorials library of DigitalOcean is endless. In almost every Google search about a topic related to servers or cloud infrastructure, you’ll find results from DigitalOcean tutorials library. The tutorials are well-written and cover important principals alongside with the technicalities of how to achieve your goal. In addition to accomplishing your task, you’re actually learning new things when following DigitalOcean’s tutorials. The support team is very responsive and professional, and free of charge virtual meetings are available with cloud specialists to help you design the architecture of your server.
Grade: 5/5
Amazon Web Services: AWS is by far the leading cloud provider when it comes to offering, scalability and features. On the other hand, its learning curve is moderate, so if you haven’t experienced with AWS before, it might take you some time to get up and running with properly.
Final startup grade: 4.5/5
DigitalOcean: I like comparing DigitalOcean to a boutique hotel. When using their cloud computing technologies you feel like you’re part of a family and treated like one. DigitalOcean covers everything you need as an early-stage startup, it is easy to use and provides expected convenient pricing models.
Final startup grade: 5/5
The most important thing about cloud provider is to have one. In our world, it’s much better to have your application deployed in a little smaller cloud provider than keep arguing about which cloud provider is better when you have no idea where your application will be 6 months from now.
If you’re familiar with one of the cloud vendors, use it for your main startup unless you’re sure it will not meet your requirements.
When developing side projects, I highly encourage you to try and play with new cloud providers. Who knows, maybe you’ll fall in love with another.
Try DigitalOcean with $10 credit | Try AWS free tier
Find more great tips for technological entrepreneurs at CodingStartups.
Published originally here.
#BlackLivesMatter
501 
9
how hackers start their afternoons. the real shit is on hackernoon.com. Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
501 claps
501 
9
Written by
CEO, Co-founder @ DataGen | https://datagen.tech/
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
CEO, Co-founder @ DataGen | https://datagen.tech/
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://scientya.com/a-beginners-guide-to-the-basics-of-what-cloud-computing-is-about-e8b3b7f25a30?source=search_post---------10,"Forget about definitions and technical explanations. Not everyone is a highly skilled DevOps Engineer and Cloud Architect. Salesforce CEO and Chairman Marc Benioff brings it straight to the point. Cloud computing is a better way to run your business according to him. The possibility of improving your business is something which is appealing to every decision maker across the globe. Who thought that the cloud would one day be considered as the digitization tool par excellence as n’cloud.swiss Founder and Chairman André Matter emphasizes again and again. The year 2000 will always be remembered as the milenium of the more and more oustanding pace of technological progress. New and old technologies are as close together as never before. The almost uncontrollable increase in human knowledge leads to endless incremental innovations. The hunt for the next big thing seems to be never ending.
Even though cloud computing still plays that role, one might think that it is being pushed aside by the likes of artificial intelligence and blockchain. But wait… most new technologies are distinguished by a huge number of data and intelligence. In other words, “the many disparate servers which are part of cloud technology hold the data which an AI can access and use to make decisions and learn things like how to hold a conversation. But as the AI learns this, it can impart this new data back to the cloud, which can thus help other AIs learn as well” as noticed Gary Eastwood from the IDG Contributor Network. Same goes for blockchain and other data intensive technology. Cloud computing is not only the digitization tool par excellence, it is omnipresent and plays undoubtedly a key role in today’s technological progress.
Providing IT resources and on-demand applications over the Internet at usage-based prices
“Simply put, cloud computing is the delivery of computing services — servers, storage, databases, networking, software, analytics and more — over the Internet (“the cloud”). Companies offering these computing services are called cloud providers and typically charge for cloud computing services based on usage, similar to how you are billed for water or electricity at home.” (Microsoft Azure)
Whether you run apps that share photos with millions of mobile users or support critical business operations in your organization, the cloud is a technology providing quick access to flexible and cost-effective IT resources. When it comes to cloud computing, you do not have to invest in hardware in advance or spend a lot of time managing it. Instead, you can provide the exact type and size of computing resources you need to implement your latest breakthrough idea or operate your IT department. You can access as many resources as you need almost immediately by paying only for what you use. Cloud computing provides an easy way to access servers, storage, databases and a full range of application services over the Internet. Cloud providers such as Amazon Web Services, Microsoft Azure, Google Cloud Platform or “Swiss made” n’cloud.swiss operate and manage the network-attached hardware needed for these application services, providing and using the resources you need through a web application.
Advantages of cloud computing
The cloud has become a technology that influences everyone’s daily life. The adoption of solutions and services in the cloud present a number of advantages and benefits, among others:
Types of cloud services: IaaS, PaaS, SaaS
Cloud computing consists of three main types, commonly referred to as Infrastructure as a Service (IaaS), Platform as a Service (PaaS), and Software as a Service (SaaS). Choosing the right cloud computing type consists in knowing your needs to achieve an optimal level of control with worrying about unnecessary tasks. Microsoft defines these types as follows:
Infrastructure-as-a-service (IaaS): The most basic category of cloud computing services. With IaaS, you rent IT infrastructure — servers and virtual machines (VMs), storage, networks, operating systems — from a cloud provider on a pay-as-you-go basis.
Platform as a service (PaaS): Platform-as-a-service (PaaS) refers to cloud computing services that supply an on-demand environment for developing, testing, delivering and managing software applications. PaaS is designed to make it easier for developers to quickly create web or mobile apps, without worrying about setting up or managing the underlying infrastructure of servers, storage, network and databases needed for development.
Software as a service (SaaS): Software-as-a-service (SaaS) is a method for delivering software applications over the Internet, on demand and typically on a subscription basis. With SaaS, cloud providers host and manage the software application and underlying infrastructure and handle any maintenance, like software upgrades and security patching. Users connect to the application over the Internet, usually with a web browser on their phone, tablet or PC.
Cloud deployments: public, private, hybrid
There are three different ways to deploy cloud computing resources. These are public cloud, private cloud and hybrid cloud.
Public clouds are owned and operated by a third-party cloud service provider, and deliver computing resources like servers and storage over the Internet using a web browser. Today’s leading cloud providers Amazon AWS or Microsoft Azure are examples of a public cloud.
A private or on-prem cloud refers to cloud computing resources used in-house and exclusively by a single business or organisation. The particularity here is that a private cloud can be physically located on the company’s on-site datacenter.
A combination of both public and private clouds leads to what we call hybrid cloud. The advantage here is that a hybrid cloud allows data and applications to be shared between them. By allowing data and applications to move between private and public clouds, customers enjoy greater flexibility and more deployment options.
After which criteria do customers decide for a cloud provider?
Simply put, customers do not decide for one single cloud provider. Many companies pursue a multi cloud strategy to maintain the ability and flexibility to select different cloud services from different providers. Single multi-cloud infrastructure vendors like n’cloud.swiss are rare. Praised as a Swiss and European alternative to the major cloud providers, n’cloud.swiss is a cloud platform running in one of the world’s most secure data centers in Switzerland. The idea is to enable customers to design a cloud according to their specific requirements with the same product, either as a service model, an on-prem version in existing IT environments or as a hybrid variant. In addition, all cloud service models from Infrastructure as a Service (IaaS), over Platform as a Service (PaaS) to Software as a Service (SaaS) are part of the platform. All this is surrounded by an innovative internal n’cloud.swiss application catalogue. Within the latter, n’cloud.swiss offers more than 142 applications from 30 different IT categories “free and ready to go” as well as the opportunity to upload also other development applications and tools easily. In addition, personal support and competitive pricing models, API connectivity for easy and fast transfers of existing developments from or to other major cloud platforms award this Swiss cloud platform a unique selling point and a competitive advantage.
Overview of n’cloud.swiss products und services
UNMANAGED PRODUCTS
MANAGED PRODUCTS
A cloud solution in minutesTry n’cloud.swiss for free1. Create an n’cloud.swiss account2. Start a virtual machine3. Join the world of n’cloud
The digital world publication
2.8K 
3
2.8K claps
2.8K 
3
Written by
Dynamic and data-driven marketing leader. Author, Mentor & Speaker. Top 50 Global Thought Leader. See www.yahyamohamedmao.com
Scientya is all about creating and sharing knowledge in the digital world. Switzerland’s fastest growing Medium publication  covers various topics including Blockchain, Artificial Intelligence, Cloud Computing, Health Care, Marketing and more.
Written by
Dynamic and data-driven marketing leader. Author, Mentor & Speaker. Top 50 Global Thought Leader. See www.yahyamohamedmao.com
Scientya is all about creating and sharing knowledge in the digital world. Switzerland’s fastest growing Medium publication  covers various topics including Blockchain, Artificial Intelligence, Cloud Computing, Health Care, Marketing and more.
"
https://medium.com/seedify/seedify-integrates-phala-network-for-trustless-and-secure-cloud-computing-power-to-its-customers-5bf6bd814ad7?source=search_post---------11,"There are currently no responses for this story.
Be the first to respond.
Hello, Seedify Community!
Today’s announcement has us up on ‘cloud nine’. We are proud and thrilled to share our new partnership with Phala Network!
Cloud computing has played a transformational role in the development of new tech projects. Having access to existing computer system resources, such as; data storage, computing power and speed, and analytics, has afforded developers the ability to build and launch applications and software with reduced time to market. Further benefits also include less initial capital requirement and having the ability to scale without the extensive cost of upgrading infrastructure. Although this innovation has been undeniably advantageous, it also comes at a price, with large cloud services providers like AWS and Google reaping significant financial benefits through selling users’ private data. This is an issue that Phala Network’s forward-thinking technology solves.
Phala Network is a Web 3.0 cloud computing platform that supports data privacy while remaining trustless. Unlike centralized cloud services, Phala doesn’t own any server or data center. Anyone can provide permissionless servers into the Phala Network, and because of a clever combination of blockchain and trusted execution environments (TEE), Phala can ensure that there are no ‘evil’ servers in the network, even when they are in an edge network situation. Together, this creates the infrastructure for a robust, secure, and scalable trustless computing cloud. Phala Network is also developing high-performance smart contracts that will offer a decentralized alternative to Google Analytics.
Through this strategic alliance, Seedify will direct new blockchain game projects towards utilizing Phala’s decentralized cloud computing resources. At the same time, Phala Network will introduce GameFi projects using its services to Seedify’s incubator and launchpad platform.
Phala Network is a marriage between the best of Web 2.0 and Web 3.0 and will allow new GameFi projects to gain access to crucial infrastructure in a decentralized setting. Through Phala’s platform, gaming projects will have the ability to employ cloud resources with computing speeds comparable to large centralized cloud services providers like AWS and Alibaba Cloud without compromising the privacy and security of their community’s private data.
Phala Network also features interoperability. Built on Substrate and being a Polkadot para chain, it has the ability to serve the entire Polkadot para chain ecosystem while also serving other blockchain developers.
GameFi projects introduced to Seedify through Phala Network will have access to the full benefits of the Seedify Gaming Incubator and Launchpad ecosystem. This includes; various early-stage funding opportunities, incubation, solutions partners, launchpad and all other platform services, which will be made available, depending on each project’s specific needs.
Through Phala Network’s game-changing ability to separate computation and consensus by utilizing TEE tech and blockchain, they can provide a 2-fold solution, bringing decentralization and user privacy to cloud computing, a space that has offered neither of these features up until now. We are excited to partner with Phala, which will allow us to provide cloud computing solutions to the projects we work with while simultaneously protecting our community’s private data.
Levent Cem Aydan — CEO and Founder, Seedify
We are glad to partner with Seedify to explore the potential integration of gaming projects into the Phala universe. We hope to provide a powerful, secure, and scalable trustless computing cloud to all the Seedify users as well as enhance the private data security for GameFi projects.
Marvin Tong — Co-founder, Phala Network
Through this partnership, Phala Network and Seedify have the ability to work together, engaging GameFi and decentralization to usher in a new era in cloud computing, where the integrity of users’ private data is a vital part of the resources and solutions that cloud computing platforms provide.
To learn more about Phala Network, follow the links:
About Seedify
Seedify is a blockchain gaming-focused incubator and launchpad ecosystem, empowering innovators and project developers through access to funding, community and partnership building, and a full support system to help drive the future of gaming and blockchain.
To learn more about Seedify, follow the links:
Blockchain Gaming Incubator & Launchpad
310 
310 claps
310 
Seedify is an incubator & launchpad platform for blockchain games and gamers. Through holding SFUND, become eligible to participate in Initial Game Offerings.
Written by
Digital Marketing Manager at Seedify
Seedify is an incubator & launchpad platform for blockchain games and gamers. Through holding SFUND, become eligible to participate in Initial Game Offerings.
"
https://medium.com/bloomberg/amazons-cloud-computing-empire-faces-threat-from-edge-of-the-network-e8815f5bd015?source=search_post---------12,"There are currently no responses for this story.
Be the first to respond.
By Olga Kharif
Three companies — Amazon.com Inc., Microsoft Corp. and Alphabet Inc. — quietly dominate the world of cloud computing.
"
https://medium.com/aws-enterprise-collection/the-fast-and-the-furious-how-the-evolution-of-cloud-computing-is-accelerating-builder-velocity-d4282d74a370?source=search_post---------13,"There are currently no responses for this story.
Be the first to respond.
In my role as Head of Enterprise Strategy for AWS, I have a lot to be grateful for. After having led a large-scale business transformation using the cloud as the CIO of Dow Jones, I now have a front-row seat to watch some of the largest companies in the world (News Corp, Capital One, GE) transform their business using the cloud. This seat affords me the opportunity to learn from some of the brightest and most innovative minds in the industry — both from our customers and from within AWS. As often happens in life, I sometimes run into ideas from others that “if I only knew then,” I would have done things a lot differently. One such set of ideas was recently gifted to me by Ilya Epshteyn, one of AWS’ well-tenured Solution Architects. More below…….
(Note: these best practices, and a number of others, are now available in my book Ahead in the Cloud: Best Practices for Navigating the Future of Enterprise IT).
In a world of unprecedented market disruption, where barriers to entry are crumbling and a great user experience means more than a hundred-year-old brand name, CEOs are coming to expect a different conversation with IT. CIOs and CTOs are trying to change the conversation from that of an IT Supplier to that of a Business Partner. An IT Supplier conversation (Business waiting on IT) sounds familiar: “when will the infrastructure be ready, when will you deliver X capability, how can you reduce my budget by X?” By contrast, a Business Partner conversation (IT waits on Business) sounds very different: “feel free to start building what you need when you need to by provisioning on demand; here is a new security API you can leverage; here are the results of a pilot we ran; and by the way, we cut your costs by X this month.” The latter is initiated by IT with a single goal in mind: bringing products to market. And accelerating innovation and product development hinges on increasing the velocity of your builders.
To make this happen, IT needs to focus on the tasks that differentiate the business and enable builders to move faster. IT tasks that do not differentiate the business should be automated and offloaded to a platform that provides as much of that functionality as possible out-of-the-box. This paradigm shift requires a transformation, and, as Stephen often points out, it’s often more about the people and your organization than the underlying technology. For many of our customers, this is a multi-year journey, one that began long before cloud came along — whether they realize it or not.
In the pre-virtualization era, infrastructure deployment was manual. Infrastructure took months to be provisioned, racked, stacked, wired, installed, and configured. Most applications were monolithic, with tight inter-dependencies and manual deployment. Installation and configuration guides commonly ran dozens, if not hundreds, of pages long. Data center efficiency was also a challenge. With such long provisioning cycles, businesses would often provision 25–40% more than what was needed during peak usage. With so much wasted capacity, utilization rates were often less than 10%. In this model, the development, infrastructure and operations teams all operated in silos, requiring weeks or months of planning for every change. Operations itself became a major challenge since everything was managed and operated manually, with little standardization across environments.
Virtualization and private cloud promised a better way. They sought to improve server efficiency, shrink infrastructure footprint, enable automation and new service delivery models, and, most importantly, bring agility to business.
In reality, while server virtualization had positive impacts on power and cooling consumption and even allowed organizations to consolidate and/or rationalize some data centers, many of the promised benefits have not been fully achieved. Server provisioning time has decreased and servers can often be spun up in minutes, but the reality is that provisioning and capacity planning has not significantly improved. Builders are still forced to procure capacity required for peak based on anticipated (and elusive) usage patterns of their product. In some cases, builders then have to double that to accommodate for disaster recovery (DR, or n-1) scenarios. Business cases to spread this across multiple business units are developed 3–5 years out to justify the capital expenditure, and in most cases we’ve found that they don’t end up paying off.
The infrastructure teams have started to take advantage of the automation that virtualization enables, but for the most part this capability has not been extended to the development teams. “Self-service” delivery models are still mostly manual, often requiring days or weeks of approvals using limited automation. Changes are hard since teams are still operating in silos, and orchestrating changes across these silos often comes with a lot of bureaucratic overhead that is rarely managed well. Ultimately, very little has been achieved in terms of builder velocity. Builders are still frustrated by limited automation, which impacts their productivity. It still takes too long to get things done for the business.
The good news is that organizations that have taken steps to virtualize their environments or pursue a private cloud strategy are able to move to the next phase of this evolution at a faster pace than customers that have not already made that shift. Virtualization not only simplifies the actual migration of VMs, but it’s also a reflection of an organization’s ability to transform, adapt to business needs, and upskill its IT workforce.
Completing the shift to the cloud helps customers realize the unfulfilled promises of virtualization. The on-demand provisioning of network, compute, storage, database, and other resources in a pay-as-you-go model provides unprecedented agility and helps accelerate the velocity of the development teams. But even in the cloud, this transformation is not instantaneous. Rather, the velocity of the development teams accelerates as the customer journeys through various stages of adoption.
During the first three stages of cloud adoption, where organizations 1) start with a few projects to learn the benefits, 2) lay the foundation for organizational transformation through a cloud center of excellence (CCoE) team, and 3) execute mass migrations, we start to see realization of several key factors that directly accelerate the velocity of your builders.
There is no doubt that the velocity of the dev teams is greatly improved once the customer moves through the initial stages of adoption. But in most cases, the opportunity to optimize does not end with the Migration stage. Rarely do customers have the opportunity or resources to re-architect all of their applications to be cloud native as part of the migration (see Cloud-Native vs. Lift-and-Shift). This creates an opportunity for constant reinvention to further accelerate builder velocity. Re-architecting the applications often entails decoupling and decomposing the monolith into smaller services with APIs while maximizing reusability. As part of this process, customers are looking to offload the undifferentiated portions of the application to the AWS platform and laser focus on the business logic instead.
During the Reinvention phase, organizations typically opt for more fully managed services such as Amazon Kinesis for ingestion of data, AWS Lambda for real time processing, Amazon Aurora and Amazon DynamoDB for relational and NoSQL databases, and Amazon Redshift for data warehousing, so that builders can spend maximum amount of time on the business differentiators. Developing the best queuing, messaging or API management solution is unlikely to move the needle for your business. Rather, it’s your algorithms, business workflows, and real time analytics that will delight your customers and help grow your business. At this stage, we also see a more focused effort to transform Operations to a true DevOps model. The Cloud COE also focuses more on developing reference architectures, governance, and compliance frameworks and allowing the development team more autonomy to deploy both infrastructure and applications through a unified CI/CD pipeline. And the security teams are accelerating their velocity by embracing DevSecOps methodologies and exposing security capabilities via APIs.
To put this into perspective, let’s look at how this evolution has played out in the areas of compute and big data in the cloud.
The first major compute evolution in recent times was from physical servers to virtual servers in data centers. This stage brought about higher utilization, uniformed environments, hardware independence, and new DR capabilities. The next phase was virtual servers in the cloud. This brought about on-demand resources, greater scalability and agility, and improved availability and fault tolerance. But from a builder velocity perspective, there is still room for improvement. You still need to worry about high availability and DR, need to manage golden images and patch the servers, and need to right size the instances to your workload. From a builder perspective, all you want to do is focus on the business logic and have it run on schedule or in response to an event.
This is where AWS Lambda and the transition to serverless computing come in. With AWS Lambda, there are no servers to manage or patch. The builder simply writes his or her function while the Lambda service automatically manages execution, high availability, and scalability. VidRoll, for example, uses AWS Lambda to power its business logic for real time ad bidding and to transcode videos in real time. With Lambda, VidRoll can have 2–3 engineers doing the work of 8–10 engineers, a direct result of code reusability and not having to understand or worry about the infrastructure.
Another similar example is in the evolution of big data services on AWS. Customers may run a self-managed Hadoop cluster on EC2 and EBS. In fact, they may see some initial benefits in terms of on-demand provision of resources, pay-as-you-go model, many different instance types, and more. But many of the challenges with such architectures on premise may continue to persist in the cloud. For example, because of the coupled nature of compute and storage, your cluster will be over-utilized during peak hours and underutilized at other times. You can’t shut down the cluster easily during off peak since you need to persist the data in HDFS, and you constantly need to move large amounts of data to local HDFS before you can even run a query.
Amazon EMR addresses these issues by decoupling your compute and storage and leveraging S3 as your persisted data lake. FINRA, for example, is able to launch a new HBase cluster on EMR and accept queries in less than 30 minutes because the data stays on S3, as compared to two days on FINRA’s self-managed EC2 cluster. Leveraging S3 for data storage has also reduced FINRA’s cost and enabled FINRA to right-size the cluster for the workload. The builders and data engineers are also no longer locked into long-term technology decisions, but can evolve the analytics platform and experiment with new tools as business demands. And what if the data scientists don’t want to manage any cluster at all? Amazon Athena offers a completely Serverless option. With zero spin time and transparent upgrades, the data scientist can simply write an SQL query and have Presto engine execute it immediately.
With any transformation, it can be hard to know how to measure success and keep the end goal in sight. Focusing on the velocity of your dev teams provides a great yardstick for cloud success, and can help guide your decisions as you become a Business Partner and continue to evolve and reinvent with AWS.
Tales of AWS in the Enterprise
118 
2
118 claps
118 
2
Tales of AWS in the Enterprise
Written by
Husband to Meghan, father to Harper and Finley. GM of AWS Data Exchange (formerly Head of Enterprise Strategy for AWS). Author of “Ahead in the Cloud”.
Tales of AWS in the Enterprise
"
https://medium.com/hackernoon/multi-tenancy-after-10-years-of-cloud-computing-19de782ef899?source=search_post---------14,"There are currently no responses for this story.
Be the first to respond.
It has been ten years since Amazon launched EC2. Cloud is very much real. Thousands of systems run in the Cloud.
Multi-tenancy is a key concept tied to cloud from its inception. It let multiple untrusting parties share resources while giving them the illusion of his own space. The best example of this idea is an apartment complex, which gives each tenant his own space while sharing resources like plumbing, shared spaces, security, maintenance etc. For multi-tenancy to be useful, the resource cost per tenant should be less than the cost of owning his own resource (house) exclusively.
A multi-tenant App in the Cloud needs to manage three types of resources: OLTP executions, data storage, and OLAP batch executions (analytics). For an example, think of an App that has a web app or a mobile app. The App will have a set of services that provide the backend for the app, a database, and a system to process its analytics. Since the state of the art architectures is built using stateless servers, separate databases, and separate analytics systems, we can tackle multi-tenancy separately for each aspect data, execution, and analytics. Let’s explore each of these aspects and discuss techniques for supporting them.
Cloud can implement sharing at different levels.
As going down the list, each gets more resource sharing, but less isolation (security). IaaS ( Infrastructure as a Service) platforms provide level two in the above list. PaaS and SaaS providers should choose among 2–3.
One of the key advantages of the Cloud, if it is properly done, is that a user only “Pay for what she used”. Basically, there are no costs for being available, rather you pay only if it gets used. For IaaS such as AWS, this comes almost for free as it shares hardware across users through VMs.
However, “Pay for what you use” does not come free for PaaS and SaaS. Let’s assume a Cloud provider “XCloud” that has 1 million users, who have deployed their PaaS/SaaS apps in XCloud.
At a given time, only a few hundred to thousands of those Apps will be active. However, we do not know which Apps. So when a user arrived at XCloud for AppY, out of the blue, we need to be able to serve him. We have two choices.
Option one is really wasteful. Think AppY is Gmail, and Google does not want to keep running a VM per each Gmail account all the time. If they keep a once instance for each user running, it is hard for them to let users pay only when there App used.
This is the difference between VMs vs. containers or in process multi-tenancy. VMs take seconds to minutes to boot, and unless your app is really simple, there is no way you can boot it up fast enough to serve the user who just came. Typically, you need to get back to the user in less than ten seconds.
Containers like docker solve this problem. They are lightweight VMs and can boot up in milliseconds. Thinking about the Gmail example, it is no wonder containers came from Google as they have the same problem. It is worth noting that using docker does not guarantee that your app will start fast, and you can very well mess it up at your App’s code. However, with careful coding, you can often hit the mark. Hence, containers remove a major overhead added by infrastructure.
Another interesting point to note is that this is the reason microservices and docker based systems are so interested in the start time. If your system can boots-up fast enough to serve the user, that will lead to major cost saving as you do not need to keep an instance running all the time per App. Moreover, it will also significantly simplify the autoscaling algorithms as then the algorithm does not need to predict the load and keep a buffer of instances running to counter for startup time.
Comparing containers vs. in process multi-tenancy (level 4 in our list), the latter can be even more efficient. We have done a lot of work in the area [1,2]. However, sharing a process look scary in retrospect as you need to trust the programmer of the platform of not having made a mistake. Also, performance isolation is tricky with in-process multi-tenancy.
It seems Cloud computing has settled with containers based multi-tenancy for executions, the middle ground. Containers provide acceptable performance and isolation. In retrospect, a sensible choice.
Finally, there is a much simpler solution that works in some cases.
It is worth noting that, if your servers are stateless, which is the case in most enterprise deployments, then you can do without multi-tenancy at all. If server is stateless and it can look at the request, figure out the tenant, and just in time do what is required for that tenant, then you do not need multi-tenancy. Instead you can run a pool of servers common to all tenants and assign one to each app as users arrives. For data, you can use a multi-tenant data base as described in the next section. If this solution possible, it will reduce the complexity of your architecture significantly.
After a restart, both VMs and containers lose their disks. Hence, multi-tenanting stateful servers such as databases, message broker etc are much more complicated than running them in a container. One option is to use the mounted disks such as S3 or block devices. However, this could be slow. For most applications, we need to use a database system that runs on top of proper hardware.
Even on top of hardware, we need to share storage across tenants. There are several choices. The article Multi-Tenant Data Architecture provides a great outline of these choices except for the following choice #5, which did not exist at the time of writing.
The approach #1 and #2, sharing the database server or the database, is acceptable in an IaaS setup, but prohibitively expensive for PaaS or SaaS setup. For example, it is not practical to keep a million databases one per each tenant.
The approach #3, giving a table per tenant, is better than #1 and #2, but still prohibitive if we are talking about million tenants.
The approach #4, having one table shared between many tenants, provides the necessary performance. However, all tenants need to share the same schema with this method, which is acceptable for most PaaS and SaaS scenarios. Just like in-process multi-tenancy, for isolation, we need to trust the programmers of the database system to not have made any mistakes. However, it is easier to verify the SQL filtering rather than verifying Java or C++ code used with the in-process multi-tenancy.
The fifth approach (e.g. Oracle now have a multi-tenant database server) will provide the best of all worlds. If carefully implemented, it can provide #4 level or better performance and isolation at the database level. Although we would have to trust the RDBMS developer to not make a mistake, I believe that within the RDBMS there is a better chance of handling isolation. Moreover, database developers are likely to understand the domain much better, which will let them do a better job.
Thanks to Big data, everything must have analytics. For most SaaS applications, analytics has become a competitive advantage. Multi-tenancy requirements for Analytics are different from OLTP use cases that we have considered so far. Implementing them in a PaaS or a SaaS environment needs answers for several challenges.
Analytics includes data collection, data storage, running analysis, and provide controlled access to results. Cloud providers can do following to facilitate data collection.
Each method should track tenant and user information with each event.
While handling the rest, solutions for storage, analysis, and data collection are intertwined with each other. In my opinion, the answer depends on several questions about what user needs.
Based on these requirements, solutions change. Let’s look at each solution.
If the PaaS control the data generation, data analysis, and presentation of the final results, then the system can push data into a super-tenant store and handle permissions itself. Although this might be a boring solution, this is a common use case for most SaaS apps, which gives their user’s pre-cooked analytics and nothing else. If users want to do their own analysis, the system can offer a way to export their data.
Just like with the data storage, if all users can share the same set of tables (schema), then we can store data by just adding a user and tenant columns to the table. This works pretty well. Assigning ownership to the results of the data calculated by analysis, however, has some complexity. Hence the analysis logic (e.g. Hadoop Jobs) have to resolve the ownership of each record by adding ownership data as part of the data record. Another advantage is that with this method, PaaS can run a single set of analytics queries once on all the data partitioned using “group by” operators.
Providing a private space for each tenant is the most flexible. It provides most of the control to the end user. Each tenant can have his own schema and his own analytics queries.
As usual, this flexibility needs to be paid for. It is expensive due to several reasons.
It is possible to do a hybrid solution where SaaS provides a shared Schema for shared data and provide a tenant’s own space for user-defined data. PaaS can charge additional cost from the users for private space and thereby limit the number of users. In either case, the cost would be proportional to the data generated by the tenant and cost structure need to be adjusted accordingly.
After ten years of Cloud computing, there are thousands of systems that run in the Cloud. This article explores multi-tenancy, a key idea in the cloud. Multi-tenancy is the ability to run multiple tenants (users) within the same system. While realizing multi-tenancy, a Cloud may choose between multiple levels of isolation ranging from sharing same hardware using VMs to sharing the same process through clever programming. We explored how to build a multi-tenant App. A multi-tenant App in the Cloud needs to manage three types of resources: executions, data storage, and OLAP batch executions ( analytics). We explored each and saw that the executions are converging towards containers and databases towards multi-tenant databases. Furthermore, the article discussed several solutions to support multi-tenant analytics.
Hope this was useful if you enjoyed this post you might also find the following interesting.
medium.com
hackernoon.com
Hacker Noon is how hackers start their afternoons. We’re a part of the @AMIfamily. We are now accepting submissions and happy to discuss advertising &sponsorship opportunities.
To learn more, read our about page, like/message us on Facebook, or simply, tweet/DM @HackerNoon.
If you enjoyed this story, we recommend reading our latest tech stories and trending tech stories. Until next time, don’t take the realities of the world for granted!
#BlackLivesMatter
228 
2
Some rights reserved

228 claps
228 
2
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
A scientist, software architect, author, Apache member and distributed systems programmer for 15y. Designed Apache Axis2, WSO2 Stream Processor... Works @WSO2
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
"
https://blog.signals.network/were-partnering-up-with-iexec-to-give-cryptotraders-power-of-blockchain-based-cloud-computing-2419d2fa9080?source=search_post---------15,"At Signals we want to make complex trading algorithms accessible for the crypto community, and that requires a powerful computational infrastructure. Therefore, we cannot be more excited to announce the beginning of a collaboration and a partnership with a blockchain-based cloud computing platform iExec!
This French company’s team with a strong scientific background has been among the first ones to disrupt cloud computing. Now, they are building an innovative market network where everyone can monetize their applications, servers, and data-sets.
Big data storages keep taking more and more of the world’s data distribution and therefore keeping it centralized. In fact, their private infrastructures are so huge, that they can easily dictate prices — which are much higher than necessary. With iExec, every household can profit from their free data storage and computing power and together with others, compete against big corps.
The technology of iExec is based on the same software which is used by scientists to reveal mysteries in high energy physics. In connection with Signals, this technology will serve cryptotraders for analyzing patterns in vast amounts of historical data.
At Signals, in order to help users in the process of assembling the strategy and setting it correctly, we provide them with features based on machine learning. Cutting through the noise of unstructured data for generating profitable trading signals requires state-of-the-art machine learning algorithms together with a powerful infrastructure capable of solving complex computational problems in reasonable time.
By integrating with the iExec decentralized cloud, we will be able to process big data computations in reasonable time while still making it affordable for common users. That is the foundation of the new partnership between Signals and iExec. The future is in decentralization and blockchain technology and together with iExec, we are going to revolutionize the crypto trading industry.
From day trader to data scientist — Signals lets you…
534 
1
534 claps
534 
1
Written by
Signals is a futuristic marketplace where you can discover, create and monetize cryptocurrency trading strategies driven by machine learning.
From day trader to data scientist — Signals lets you create, test and monetize automated trading bots. Access trusted strategies with no technical knowledge
Written by
Signals is a futuristic marketplace where you can discover, create and monetize cryptocurrency trading strategies driven by machine learning.
From day trader to data scientist — Signals lets you create, test and monetize automated trading bots. Access trusted strategies with no technical knowledge
"
https://medium.com/hackernoon/dadi-a-decentralized-web-service-designed-to-revolutionize-cloud-computing-c2c05386c301?source=search_post---------16,"There are currently no responses for this story.
Be the first to respond.
Information About DADI
DADI is a decentralized web service that utilizes blockchain with the hopes of revolutionizing cloud computing. It aims to reduce the grasp and control of the handful of powerful corporations that have risen to control much of the Internet, and allocates that power amongst the DADI participants, all of whom can become beneficiaries of revenues earned.
Corporations such as Google, Facebook and Amazon have grained a large percentage of the market share in their respective fields, and whilst web services are extremely helpful, their foothold in the industry smothers competitors, which leaves them free to charge extremely high commission fees for a simple online service.
DADI aims to change all that by introducing a decentralized platform that promises users benefits such as a reduction of up to 90% in costs, increased security and rate flexibility.
The new cloud computing service is built on the Ethereum blockchain to enable peer-to-peer (P2P) collaboration on a global scale. It focuses on the provision of web services to help bring a product to market, and allows any online device to connect to the platform as a miner and sell the computational power required by these services.
The potential for a decentralized global platform such as DADI is a great as can be. Since every business functioning on the online is powered by web services, each and every one of these businesses can leverage DADI’s technology to improve efficiency, lower costs and provide a securer way for its customers to purchase their services.
Unlike most other platforms being developed for, or making the switch to, a blockchain-based architecture, the technologies that power DADI services have already been launched and are being used to power products such as Empire Online, Monocle and Virgin Limited Edition.
Technology Behind DADI
DADI incorporates a series of open web services that function on a container layer built upon already-established P2P technologies. These open source web services provide users with streamlined, user-friendly solutions to carryout common tasks and help them build, scale and grow their digital products.
Currently, to set up an API the provider must first develop and API wrapper, acquire an instance, install the necessary software and integrate their code before beginning the maintenance phase. Then when it comes to scaling the product, things become even more complex. With DADI, all the user is required to is obtain a DADI API. There are 10 currently in production, 5 of which are already on the market. DADI APIs have been designed to scale with ease allowing developers to modify their technology to meet growing demand.
DADI APIs
DADI API (Complete)
DADI CDN (Under construction)
DADI Identity (Under construction)
DADI Match (Under construction)
DADI Predict (Under construction)
DADI Publish (Complete)
DADI Queue (Complete)
DADI Track (Under construction)
DADI Visualize (Under construction)
DADI Web (Complete)
Through the implementation of DADI APIs, users will be able to offer all kinds of services including, but not limited to:
DADI web services are organized around a series of micro services that provide intelligent applications for creating digital products. These ready-made solutions are designed to meet common requirements and provide an out of the box solution for almost every use case.
DADI API
A high-performance RESTful API layer designed in support of API-first development and the principles of COPE.
DADI PUBLISH
A writer’s window to the world of content creation. Flexible interfaces designed to optimize editorial workflow.
DADI WEB
A schemaless templating layer that can operate as a standalone platform or with API as a full stack web application.
DADI CDN
A just-in-time asset manipulation and delivery layer designed as a modern content distribution solution.
DADI STORE
A cloud storage solution for all types of data, with built-in security, privacy and redundancy.
DADI QUEUE
A lightweight queue processing system powered by Redis, featuring simple task routing and throttling.
DADI IDENTITY
Guarantees uniqueness of individuals — and powers segmentation — for anonymous and known users.
DADI TRACK
A real-time, streaming data layer providing accurate metrics at individual and product level.
DADI VISUALIZE
A data visualization interface for Identity and Track, capable of taking data feeds from virtually any source.
DADI MATCH
A taxonomic framework for automated content classification through machine learning, which plugs into Publish.
DADI PREDICT
A machine-learning layer that predicts user behavior at an individual level based on past interactions.
DADI CLI
DADI CLI is a command-line tool to help with the installation and customization of the various products of the DADI platform.
Users of the network are categorized the following:
Hosts: The network node owners who contribute computational power. DADI’s web services run inside a container within host environments. Hosts are incentivized for their participation in the network, they announce themselves by locked a small amount of DADI tokens, and are rewarded in DADI tokens for their work. The availability of the Hosts is managed through a proof of work system that also updates a reputation system to identify the most favorable Hosts.
Gateways: Network node owners who contribute bandwidth. These are the entry points to the network for Host and provide the domain name system that makes Host resources addressable. In essence they are the cache for Host processed content. Gateways are also rewarded with DADI tokens; a percentage of the tokens from all the Hosts connected to them. To become a Gateway the user must lock in a large amount of tokens, much more than the Hosts. This facilitates the reputation system and mitigates the risk of Sybil attacks.
Stargates: The provide vital functions in the network in the form of domain name mapping and Consumer-DADI contract negotiation. They are intended for extremely high bandwidth environments. Stargates are also rewarded with DADI tokens; a percentage of all Consumer tokens, and must lock in the most amount of tokens to announce themselves.
Consumers: Users of the network. These may either be businesses, governments, or even individuals, and their customers.
The DADI Token
The DADI Token is an ERC20 token for the Ethereum blockchain. All transactions on the network will be carried out using the DADI token with the largest slice of the tokens going to Hosts, a percentage of that being paid to the Gateways they are attached to, and then a smaller percentage is retained to support the network, and a similar percentage paid to the develop team.
The team will develop a wallet front-end interface that will allow Consumers to purchase services in the currency of their choice.
Valuable Information
Coin Name : DADI Cloud (DADI)
Total Supply : — 100,000,000 DADI
Circulating Supply : —
Market Cap : —
Pre Ico Price : — 1 DADI = 0.4 USD
Ico Price : — 1 DADI = 0.5 USD
Tokens/Money Raised Through ICO : — $4,000,000 OF $29,000,000 (14%)
Social Media Information
Facebook
URL : — https://www.facebook.com/daditechnology
Likes : 967 likes and 1,003 followers
How Many Posts / Updates In The Last 7 Days : — post
Twitter
URL : https://twitter.com/dadi
Followers :12.4k followers
How Many Tweets / Retweets In The Last 7 Days : 14 posts
Telegram
URL : — https://t.me/dadichat
Users : 34 659 members
Telegram Admins
User Name: @NoHexBot
User name: @Whytecleon
User Name: @bjay2200
User Name: @arthurmingard
User Name: @paulregan
Reddit
URL : — https://www.reddit.com/r/DADI/
Readers :1,859 readers
Youtube
URL : — https://www.youtube.com/channel/UCPwoSL3xPOvRBhMz-A-Z2PA
Subscribers : — 2k subscribers
Medium
URL : — https://medium.com/@daditech
Followers : 1k followers
How Many Updates In The Last 7 Days : 4 post
LinkedIn
URL : — https://www.linkedin.com/company/dadi/?originalSubdomain=ph
Followers : 349 followers
How Many Updates In The Last 7 Days : 1 post
Steemit
URL : —
Followers : — followers
How Many Updates In The Last 7 Days : —
GitHub
URL : — https://github.com/dadi
How Many Updates In The Last 7 Days :4 posts
Extra
Discord
URL: https://discordapp.com/invite/3sEvuYJ
Staff / Team Info
Name : Joseph Denne
Title : Founder & CEO
Bio : — Joseph is the Founder & CEO of DADI, and the visionary behind DADI’s decentralized architecture and web services. He is an expert in multi-agent and blockchain technologies as well as big data and machine learning. He was responsible for Symphony CMS and has 20+ years experience developing data and content platforms. Joseph was previously Group Technical Director for the Leo Burnett Group, the Founder of Airlock (a multi-award winning technology company), the Technical Director at Chime Communications and a member of the technology board at the BBC. His work is known across the industry and has been recognized with multiple Webby, Lovie, Emmy, Sony and Bafta awards.
LinkedIn : — https://www.linkedin.com/in/josephdenne/
Extra Links : — https://twitter.com/josephdenne/
https://josephdenne.com/
Name : Chris Mair
Title : Founder/Architect
Bio : — Chris is a strategist and an early blockchain enthusiast, having first invested in Bitcoin in 2011. He is a founder of DADI and is the key architect behind the decentralized business strategy. Prior to DADI he was a partner at London based technology company Airlock, where he headed the strategy division. Before joining Airlock, Chris was the Global Head of Digital Technology for fashion brand Diesel where he was responsible for the development and implementation of the brand’s digital strategy across 33 markets worldwide.
LinkedIn : — https://www.linkedin.com/in/chrismair/
Extra Links : — https://twitter.com/chrismair/
Name : Will Lebens
Title : Partnerships and the strategic implementation of DADI technology
Bio : — Will has been founding tech companies since 1998, specializing in content and data management. He leads the solutions and support teams at DADI, focusing on partnerships and the strategic implementation of DADI technology. Will was previously Managing Director for technology company Airlock, where he oversaw all client relationships — and sat on the Senior Management Council at Leo Burnett Group. He was also a Non-Executive Director for Symphony CMS.
LinkedIn : — https://www.linkedin.com/in/willlebens/
Extra Links : — https://twitter.com/WillLebens/
Name : Paul Kingsley
Title : Chief Operating Officer
Bio : — Paul is Chief Operating Officer at DADI, bringing COO and CFO experience from portfolio of digital, marketing and media companies and projects. He is responsible for operations and financial management at DADI. Paul also holds a variety of senior and consulting positions across a portfolio of digital and tech organizations including Code and Theory and Bright Analytics.
LinkedIn : — https://www.linkedin.com/in/paul-kingsley-838bb2/
Extra Links : — https://twitter.com/PaulDKingsley/
Name : James Lambie
Title : Technical Director
Bio : — James is a Technical Director at DADI and is an experienced software/data architect. He has extensive experience in blockchain technologies, load balancing (nginx, nginx modules), containerization (Docker) and multi-agent tech (with a focus on Node.js, Go, C#, ASP.Net and PHP). He is responsible for the development of DADI’s decentralized cloud and web services layer. James was previously the Lead Developer for BBC Worldwide, a Lead Developer for Barclays bank (working on deep system integration and distributed data warehousing) and the Senior Developer at Synergy International (working in the networking space for NGOs).
LinkedIn : — https://www.linkedin.com/in/jameslambie/
Extra Links : — https://twitter.com/jimlambie/
Name : Francesco Iannuzzelli
Title : Technical Director
Bio : — Francesco is a Technical Director at DADI with over 25 years of engineering experience. He is a systems security expert with experience across the stack, including in cloud and distributed networking. He specializes in network architecture design and oversees much of the core DADI product. Francesco previously held senior systems and consultancy roles with many organizations and was the technical architect for several multi-channel solutions in various sectors, such as e-commerce, retail (including tesco.com), broadcasting, tourism, education, media and publishing.
LinkedIn : — https://www.linkedin.com/in/iannuzzelli/
Extra Links : — https://twitter.com/iannuzzelli/
Name : Jo Biddulph
Title : Technology Implementation Director
Bio : — Jo is the Technology Implementation Director at DADI and is responsible for the support of the growing portfolio of enterprise clients using the platform. She brings over 15 years product management, operational support and account handling experience to the role and previously led the delivery of Renault TV, a globally distributed platform covering 126 territories, delivering over 200,000 broadcast hours of content. At DADI she has led the delivery of 200+ products for Bauer Media. Jo was DADI’s second employee.
LinkedIn : — https://www.linkedin.com/in/jobiddulph/
Extra Links : — https://twitter.com/jobiddulph/
Name : Paul Regan
Title : Product Director
Bio : — Paul is the Product Director at DADI, responsible for the strategic development of the web services stack. He has many years experience of digital and content strategy for publishers and broadcasters, specializing in the application of discrete web services to enable real time, personalized experiences — a key feature of DADI technology. Paul joined DADI after working as a client to the business over many years, for brands including BBC Worldwide, Discovery Channel, Monocle, Haymarket and the BBC.
LinkedIn : — https://www.linkedin.com/in/paulrgn/
Extra Links : — https://twitter.com/paulrgn/
http://paulrgn.com/
Name : David Longworth
Title : Design Director
Bio : — David is the Design Director at DADI, responsible for user experience and design across the stack. His skillset blends design and front-end development and he prefers to ‘sketch in code’ rather than work with static creative — as such he is a capable engineer with expertise in HTML, CSS and JS. Before joining DADI, David worked at production agency Stink Studios and was responsible for design work for Redbull, Google, Chanel, Ray-Ban and Samsung among other high profile brands. His work has won many awards and has featured more than once in Apple keynote presentations.
LinkedIn : — https://www.linkedin.com/in/davelongworth/
Extra Links : — https://twitter.com/abovedave/
https://davidlongworth.com/
Name : Viktor Fero
Title : Principal Engineer
Bio : — Viktor is a Principal Engineer at DADI, focused on the development of the web services layer of the stack. He has extensive experience with microservice architectures, blockchain technologies, load balancing (nginx, nginx modules), containerization (Docker) and deployment methodologies. Viktor was previously the Technical Lead for Nike and oversaw the development of distributed video delivery networks for Renault TV and the London 2012 Olympics during his time at Airlock. Viktor was DADI’s first employee.
LinkedIn : — https://www.linkedin.com/in/viktorfero/
Extra Links : — https://twitter.com/viktorfero/
Name : Arthur Mingard
Title : Principal Engineer
Bio : — Arthur is a Principal Engineer at DADI, focused on machine learning technologies in decentralized environments. He is an expert in multi-agent technologies (with a focus on Node.js, Python, PHP and Go), and has extensive experience in machine learning algorithm development. He has also worked with creative technologies, including real-time physical data capture and analytics. Arthur was previously the Technical Lead for Monocle, where he was responsible for the implementation of broadcast services including live playout radio, podcast distribution and video management platforms, delivering a distributed infrastructure with points of presence in every major market.
LinkedIn : — https://www.linkedin.com/in/arthurmingard/
Extra Links : — https://twitter.com/ArthurMingard/
https://arthurmingard.com//
Name : Eduardo Bouças
Title : Senior Engineer
Bio : — Eduardo is a Senior Engineer at DADI with a passion for API-first content management for publishers. He has carved a niche for himself in the industry, speaking at many events on the benefits of COPE, ‘headless’ CMS and discrete web services. He plays a key role in the development of many DADI products. Before joining DADI, Eduardo managed the development and delivery of several products across a portfolio of 50 publications at Time Inc, including Wallpaper, NME and Uncut.
LinkedIn : — https://www.linkedin.com/in/eduardoboucas/
Extra Links : — https://twitter.com/eduardoboucas/
https://eduardoboucas.com/
Name : Adam K Dean
Title : Senior Engineer
Bio : — Adam is a Senior Engineer with a background in full stack software development & systems administration, and a keen interest in distributed computing & automation. He has been involved with containerization since 2013 and has participated in numerous community projects including Docker, Deis, and Rancher. He has extensive experience across a range of relevant disciplines including virtualization, containerization, network topology, as well as common networking protocols such as UDP, TCP, DNS, HTTP, AMQP, and ZMTP. He has a solid understanding of site reliability, having led devops efforts in a high-traffic commercial environment. He is well versed in an array of programming languages, multiple database technologies (SQL/NoSQL), and blockchain technology. Adam joined DADI after leading development & devops for a major UK retailer.
LinkedIn : — https://www.linkedin.com/in/adamkdean/
Extra Links : — https://twitter.com/imdsm/
http://akd.sh/
Name : Jean-Luc Thiebaut
Title : Senior Engineer
Bio : — Jean-Luc is a Senior Engineer at DADI and has spent many years working with the founders of the company. He has extensive experience with distributed and technologies including load balancing (nginx, nginx modules), containerization (Docker) and multi-agent tech (with a focus on Node.js, Go, C#, and PHP).He was the Technical Manager at Airlock, leading the development of large-scale platforms, most notably for the BBC’s coverage of Wimbledon and the 2012 London Olympics. Jean-Luc has also successfully launched a whisky trading platform (Whisky Marketplace) and a social network for whisky connoisseurs (Connosr).
LinkedIn : — https://www.linkedin.com/in/jean-luc-thiebaut/
Extra Links : — https://twitter.com/jeanylucky/
Name : Robert Stanford
Title : Senior Engineer
Bio : — Rob is a Senior Engineer at DADI focused on content management technologies and distributed publishing models. His 20 years’ in software engineering brings experience with Node.js, Ruby, Redis and the usual frontend suspects. Rob joined DADI after working as the Development Lead for Comic Relief. He was previously a Senior Engineer for MTV, Virgin and the BBC, where he led the migration of traditional technologies to discrete web services designed for performance at scale.
LinkedIn : — https://www.linkedin.com/in/orinocai/
Extra Links : — https://twitter.com/orinocai/
Name : Peishan Chen
Title : Senior Project Manager
Bio : — Peishan is a Senior Project Manager at DADI, working to support the development of DADI technology. She brings to the role an interest in digital products that leverage machine learning and big data to personalize CRM — and has worked to deliver DADI-powered solutions that do just that. Peishan joined DADI three years ago from a position with a creative agency, where she looked after accounts for Facebook, WWF, Threadless and W Hotels.
LinkedIn : — https://www.linkedin.com/in/peishanchen/
Extra Links : — https://twitter.com/shelfconscious/
Name : Robert Belgrave
Title : Advisor/ Wirehive CEO
Bio : — DADI is delighted to welcome Wirehive CEO Robert Belgrave as an advisor to its cloud computing product. Robert brings experience in cloud services having been part of the founding team at Wirehive and instrumental in its success in the five years since. He is also chair of BIMA South, co-creator of the Alexa Stop! podcast and founder of Omnitude — an ecommerce ecosystem built on the blockchainRobert’s unique background in cloud consultancy and blockchain services makes him an ideal fit as an advisor to DADI as its decentralized peer-to-peer hosting network develops.
LinkedIn : — https://www.linkedin.com/in/robertbelgrave/
Extra Links : — https://twitter.com/robertbelgrave/
https://about.me/robertbelgrave/
Company Address
London, UK
#BlackLivesMatter
174 
1
174 claps
174 
1
Written by
I do token research, if you don't have time to find promising projects yourself, just follow my posts, it's my job to do this.
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
I do token research, if you don't have time to find promising projects yourself, just follow my posts, it's my job to do this.
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
"
https://towardsdatascience.com/you-need-to-move-from-cloud-computing-to-edge-computing-now-e8759eb9690f?source=search_post---------17,"Sign in
Sabina Pokhrel
Nov 20, 2019·6 min read
In this decade there has been a transformational movement from on-premise Computing to Cloud Computing, allowing systems to be centralised and accessible and increasing security and collaboration. Today, at the precipice of a new decade, we are witnessing a shift from Cloud Computing to Edge Computing.
Edge Computing refers to the computations that take place at the ‘outside Edge’ of the internet, as opposed to Cloud Computing, where computation happens at a central location. Edge Computing typically executes close to the data source, for example onboard or adjacent to a connected camera.
A self-driving car is a perfect example of Edge Computing. In order for cars to drive down any road safely, it must observe the road in real-time and stop if a person walks in front of the car. In such a case, processing visual information and making a decision is done at the Edge, using Edge Computing.
This example highlights one key motivation for Edge Computing — speed. Centralized Cloud systems provide ease of access and collaboration, but the centralization of servers means they are remote from data sources. Data transmission introduces delays caused by network latency. For a self-driving car, it is critical to have the shortest possible time from collecting data through sensors to making a decision and then acting on it.
Edge Computing market size is expected to reach USD 29 billion by 2025. — Grand View Research
All such real-time applications demand Edge Computing. Market Research Future (MRFR) study shows that by 2024, the market size of Edge Computing is expected to reach USD 22.4 billion, and USD 29 billion by 2025 according to Grand View Research, Inc. Top tier VCs like Andreessen Horowitz are making big bets. Edge Computing is already being used in various applications ranging from self-driving cars and drones to home automation systems and retail, and with this rate of its popularity, we can only imagine the future applications of it.
In Cloud Computing, Edge devices collect data and send it to the Cloud for further processing and analysis. Devices at the Edge play a limited role, sending raw information to, and receiving processed information from, the Cloud. All of the real work is done in the Cloud.
This type of infrastructure may be suitable for applications where users can afford to wait for 2 or 3 seconds to get a response. However, this is unsuitable for applications that need to respond faster, and especially those which are looking for real-time action such as in a self-driving car. But even in a more mundane example, like basic website interactivity, developers deploy JavaScript to detect a user’s action and respond to it in the user’s browser. Where the response time is critical to the success of an application, interpreting input data close to the source is the better option if it is available. When both input and output happen at the same location, such as in an IoT device, Edge Computing removes the network latency, and real-time becomes possible.
Computing at the Edge means less data transmission as most of the heavy-lifting is done by the Edge devices. Instead of sending the raw data to the Cloud, most of the processing is done at the Edge, and only the result is sent to the Cloud, thus requiring less data transmission bandwidth.
Let’s take an example of a smart parking system which uses Cloud Computing infrastructure to find out how many parking spaces are available. Live video feed or still images of let’s say, 1080p every few seconds may be sent to the Cloud. Imagine the network bandwidth and data transmission cost required every hour for this solution, with continuous transmission of voluminous raw data over the network:
In comparison, if the smart parking system uses Edge Computing, it would only have to send an integer of how many parking spaces are available every few seconds to the Cloud, thus resulting in reduced bandwidth, and leading reduced data transmission cost.
IoT means more systems, more systems means more bandwidth and more load on centralized servers. In Cloud Computing architecture, adding one IoT device results in an increase in bandwidth requirement and the Cloud compute power.
Let’s use the above example of the smart parking system, streaming live 1080p video feed to the Cloud. The customer wants to install this system in 10 additional parking lots. In order to facilitate this, they need to increase their network bandwidth and need approximately 10 times more compute power and Cloud storage space as the load on the centralized servers increases with incoming data from 10 additional cameras. This increases network traffic and the uplink bandwidth becomes a bottleneck. Thus, scaling is costly as the network traffic, bandwidth and use of Cloud resources increase with every additional device.
In contrast, in Edge Computing, adding one IoT means per unit cost of the device increases. Bandwidth and Cloud compute power do not need to increase per device, as most of the processing is done at the Edge. With Edge Computing architecture, adding 10 additional IoT devices in the parking lot system seems less daunting because there is no need to increase the Cloud compute power or network bandwidth. Therefore, Edge Computing architecture is much more scalable.
From on-premise Computing to Cloud Computing and now to Edge Computing, software architectures have evolved as we demand better performance and more innovation from our Computing systems. As we outgrow our current Cloud-based architectures, the market of Edge Computing is growing driven by demand for real-time applications and the cost pressures of IoT, among others. This is a trend that will color the software industry for the 2020’s. Welcome to the future.
Are you already using Edge Computing? What are some of the other benefits of Edge Computing? Leave your thoughts as comments below.
Originally published in www.xailient.com/blog.
More stories:
Challenges of Running Deep Learning Computer Vision on Computationally Limited Devices
Struggles of Running Object Detection on a Raspberry Pi
About the author
Sabina Pokhrel works at Xailient, a computer-vision start-up that has build the world’s fastest Edge-optimized object detector.
References:
https://www.marketresearchfuture.com/reports/Edge-Computing-market-3239
https://www.grandviewresearch.com/press-release/global-Edge-Computing-market
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6539964/
https://www.theverge.com/circuitbreaker/2018/5/7/17327584/Edge-Computing-Cloud-google-microsoft-apple-amazon
https://www.salesforce.com/products/platform/best-practices/benefits-of-Cloud-Computing/#
https://www.controleng.com/articles/key-drivers-and-benefits-of-edge-computing-for-smart-manufacturing/
https://ces.eetimes.com/the-advantages-of-edge-computing/
AI Specialist | Machine Learning Engineer | Writer and former Editorial Associate at Towards Data Science
See all (336)
197 
3
Every Thursday, the Variable delivers the very best of Towards Data Science: from hands-on tutorials and cutting-edge research to original features you don't want to miss. Take a look.
197 claps
197 
3
Your home for data science. A Medium publication sharing concepts, ideas and codes.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/threat-intel/cloud-computing-e5e746b282f5?source=search_post---------18,"There are currently no responses for this story.
Be the first to respond.
What exactly is cloud computing? This is something that, no doubt, most people have wondered in recent times, as more and more of the services we use have migrated to the semi-mythical “cloud”.
One dictionary definition of cloud computing defines it as: “Internet-based computing in which large groups of remote servers are networked so as to allow sharing of data-processing tasks, centralized data storage, and online access to computer services or resources.” Users no longer need vast local services to access storage or carry out certain tasks, they can do it all “in the cloud”, which essentially means over the internet.
If we go back to the very beginning, we can trace cloud computing’s origins all the way back to the 1950s, and the concept of time-sharing. At that time, computers were both huge and hugely expensive, so not every company could afford to have one. To tackle this, users would “time-share” a computer. Basically, they would rent the right to use the computer’s computational power, and share the cost of running it. In a lot of ways, that remains the basic concept of cloud computing today.
In the 1970s, the creation of “virtual machines” took the time-share model to a new level. This development allowed multiple computing environments to be housed in one physical environment. This was a key development that made it possible for the cloud computing we know today to develop.
Professor Ramnath Chellappa is often credited with being the person who coined the term “cloud computing” in its modern context, at a lecture he delivered in 1997. He defined it then as a “computing paradigm where the boundaries of computing will be determined by economic rationale rather than technical limits alone.” However, some months before this, in 1996, a business plan created by a group of technologists at Compaq also used the term when discussing the “evolution” of computing. So, while the source of the expression might be in dispute, it is clear that the modern “cloud” was something that was being seriously thought about by those in the IT industry in the mid ’90s — 20 years ago.
In 2006, Amazon launched Amazon Web Services (AWS), which provided services such as computing and storage in the cloud. Back then, you could rent computer power or storage from Amazon by the hour. Nowadays, you can rent more than 70 services, including analytics, software and mobile services. Its S3 storage service holds reams of data and services millions of requests every second. Amazon Web Services is used by more than one million customers in 190 countries. Massive companies including Netflix and AOL don’t have their own data centers but exclusively use AWS. Its projected revenue for 2017 was $18 billion.
While the other major tech players, such as Microsoft Azure, did subsequently launch their own cloud offerings to compete with AWS, it dominates the cloud infrastructure market; according to recent reports, at the end of 2017 it held a 62 percent market share of the public cloud business, with Microsoft Azure holding 20 percent, and Google 12 percent. While AWS is still way ahead of its rivals in this space, it is interesting to note that its market share did drop since the previous year, while both Microsoft and Google’s market share grew.
While AWS dominates in the enterprise space, when it comes to consumers, they are probably most familiar with services like Dropbox, iCloud and Google Drive, which they use to store back-ups of photos, documents, and more. The increased use by people of mobile devices with smaller storage capacities increased the need for cloud-based storage among consumers. While they may lack understanding about what exactly the cloud is, it is likely that most consumers are using at least one cloud-based service. The cloud has allowed for the growth of the mobile economy, in many ways, allowing for the development of apps that may not have been possible in the absence of a cloud infrastructure.
In organizations, the numbers using cloud services is even larger. The Symantec ISTR 2017 showed that the average enterprise has 928 cloud apps in use, though many businesses don’t realize that their employees are actually using so many cloud services.
However, while there are many advantages to cloud computing, and many reasons why companies and individuals use cloud services, it does present some security concerns. One of the appeals of information stored on the cloud is that it can be accessed remotely, however, if inadequate security protocols are in place, this is also one of its weaknesses. There have been many stories in the news about Amazon S3 buckets being left on the web unsecured and revealing personal information about people. However, as it seems unlikely that cloud computing is going anywhere, the answer to these kinds of issues is more likely to be improving people’s cyber security practices to ensure they protect data stored online with strong passwords and other forms of authentication, such as two-factor and encryption.
The adoption of cloud was almost inevitable in our hyper-connected world. The need for computing power and storage simply became too expensive and too much for many businesses and individuals to tackle, meaning they needed to farm out these tasks to cloud services. As the move to mobile continually escalates, and as the Internet of Things (IoT) continues to grow as a sector, cloud computing is set to continue its growth.
It may have started out as a marketing term, but cloud computing is an important reality in today’s IT world.
Check out the Security Response blog and follow Threat Intel on Twitter to keep up-to-date with the latest happenings in the world of threat intelligence and cybersecurity.
Like this story? Recommend it by hitting the heart button so others on Medium see it, and follow Threat Intel on Medium for more great content.
Insights into the world of threat intelligence, cybercrime…
190 
190 claps
190 
Insights into the world of threat intelligence, cybercrime and IT security. Brought to you by researchers at Symantec.
Written by
Symantec’s Threat Hunter team brings you the latest threat intelligence from the IT security world.
Insights into the world of threat intelligence, cybercrime and IT security. Brought to you by researchers at Symantec.
"
https://read.acloud.guru/iaas-paas-serverless-the-next-big-deal-in-cloud-computing-34b8198c98a2?source=search_post---------19,"Cloud migration. What does it mean? Cloud migration is the process of moving digital assets — like data, workloads, IT resources, or applications — to cloud infrastructure. Cloud migration commonly refers to moving tools and data from old, legacy infrastructure or an on-premises* data center to the cloud.
*On-premises is sometimes shortened to “on-prem” and commonly — though incorrectly (from a grammar point-of-view) — called “on-premise.” 
Though “cloud migration” typically refers to moving things from on-premises to the cloud, it can also refer to moving from one cloud to another cloud. A migration may involve moving all or just some assets. It also involves a whole lot of other things. That’s why we’ve complied this guide to cover all things cloud migration.
Looking to learn more about cloud migration? This beginner’s guide to cloud migration covers everything you need to go from knowing nothing about cloud migration to knowing something about cloud migration. We’ll start with an explain-it-like-I’m-five intro to cloud basics and take you all the way to cloud migration strategies and migration tools and services in this high-level look at successfully accomplishing a cloud migration. Plus, you’ll get handy resources to take you from getting the gist to getting it down pat.

The cloud — often used as shorthand for “cloud computing” — refers to a pool of computer services accessed over the internet. This pool is accessible on-demand and self-service, giving instant access to services without setup. With the cloud, computers that businesses once had to have on-premises can be stored in massive data centers* around the world.
*“Data center” is a term used to describe the space dedicated to housing computer systems and their necessary accouterments. 
Unlike a server* room or server closet, the computer-systems-storing units you might find behind a (hopefully) locked door in any given office, data centers are dedicated primarily to housing computer equipment. They’re normally huge and designed specifically for the express purpose of keeping a ton of tech running in optimal conditions.
*“Tap the brakes. What are servers?” you may be asking. Servers are basically heavy-duty computers designed to run all the time and typically dedicated to a single task, like storing, sending, or processing data.
 Learn server security in a non-technical environment!
These data centers are managed by companies focused on running data centers, reducing the need for local IT resources — computers, that is, not people. (You’ll still probably want some people, particularly people who know how to use cloud.)
Interested in upscaling or beginning your journey with Cloud Data? A Cloud Guru’s AWS Data Learning Paths offers custom courses fit for beginners and advanced gurus!
These computer services don’t actually exist in the sky, of course. (You probably knew that, but just in case . . . ) The name “cloud” actually comes from the symbol used in conceptual network diagrams from the 1970s. 
The cloud is sometimes used interchangeably with the term “internet,” but, technically, the cloud isn’t the internet. Rather, the cloud uses the internet to deliver resources. But when people are doing things on the internet, they’re probably using cloud services — from checking Gmail to uploading a dog pics to Instagram to streaming Adam Sandler movies on Netflix.
So, what can you do with the cloud? Almost anything you can do with a computer or a server is available as a service in the cloud.
Services vary in complexity, from storing the digital pieces that compose a webpage to employing beefy hardware to perform resource-intensive tasks, like using artificial intelligence (A.I.) and machine learning (M.L.) to analyze business data for insights.
The reasons businesses migrate to the cloud are plentiful and varied. But one big reason is that working in the cloud gives you access to virtually limitless computer resources. 
An easy thought exercise is to think of this nearly bottomless pool of computing power and storage a bit like electricity. Sure, you could produce electricity on your own by running a generator, but the upfront cost for the hardware is expensive. Then, you’ve got to keep it running, which requires a level of expertise and time spent on ongoing maintenance. And if it goes down, you’re out of luck — unless you’ve really splurged and have a second generator just sitting around ready to go at a moment’s notice.
But like the way you (probably) consume electricity, if you and others around you all need electricity, it makes more sense to outsource the setup and day-to-day management of electricity production. Then, you and all your neighbors who need electricity can tap into electricity with the flip of a switch. You’re free to use what you need and pay for only what you use. Got a bunch of people coming to visit for a holiday? You can have all the electricity they need, no problem. The electricity company has you covered. They focus on delivering you electricity quickly and efficiently, so you’re free to focus on what you need to do. (If analogies aren’t your thing, just re-read the last two paragraphs and replace “electricity” with “cloud.”) 
The plain and simple of it is this: Cloud abstracts and serves up IT on tap. You get servers and computer services without the need to buy, maintain, and manage hardware or dedicate employees to the low-level admin work that comes with that.
If you think about cloud like a utility (like electricity), you’re basically thinking about public cloud, which is just one type of cloud deployment model. But what’s the difference between public and private cloud? And what’s hybrid cloud? Funny you ask.
What’s the difference between SaaS, PaaS, and IaaS? They’re three different kinds of cloud service models, cloud service categories, or the types of cloud computing — all just different terms for the same three funny-looking acronyms. Whatever you want to call them, understanding these is a solid step to wrapping your head around what cloud can do.
“aaS” stands for “as a service.” In its simplest form, this means moving that piece of the tech stack to the cloud. For example, software as a service means moving software that would typically reside on a computer to the cloud. 
Are you an IT Manager? Admin? IT Team Leadership? A Cloud Guru’s AWS Executive Learning Paths offers enterprise continuous learning with custom courses fit for beginners and advanced gurus!
At a basic level, the benefits of cloud are often around efficiency, achieving the maximum results with minimal expense. For organizations that pull off a successful cloud migration, the rewards reaped can include increased scalability, lower costs, and security. 
For simplicity’s sake, we’ll focus specifically on public cloud benefits, though some of these benefits apply to other cloud deployment models, too. Here are seven big benefits of migrating to the cloud.
Cloud allows for improved scalability, giving organizations the ability to almost instantaneously add or take away resources on an as-needed basis or to match demand. Elasticity goes hand in hand with scalability and refers to the ability to quickly expand or decrease computer processing, memory, and storage resources (data storage optimization) to meet changing demands without concerning yourself with cloud capacity planning. Elasticity enables scalability.
Think about it this way: An influx of users can eat up a server’s resources. With traditional self-hosted hardware, requests would then be denied. But the cloud has a practically infinite set of resources. That means servers can scale up to handle the rush without a hitch.
The process of scaling can be done automatically (called autoscaling), based on, for example, the time of day or the amount of processor resources being used.
Cloud gives you the ability to only pay for the resources you use. (Think: taking an Uber instead of buying a car.) This gives you access to resources that would cost way too much time and money to keep up yourself, which ties back into scalability. 
A traditional IT approach to scaling up is costly. It takes planning; it can take months, requires hardware at a big upfront cost, electricity to keep it all operating and cool, and skilled IT staff capable of getting (and keeping) it all up and running. With cloud, that’s all done nearly instantly by your cloud provider.
But where cloud scalability really leaves on-prem scalability in the dust is around decreasing resources. Traditional IT infrastructure can require having enough resources to handle peak demand. (For example, a retailer having to pay for the data center resources to be ready for a rush of Black Friday shoppers, even though they’re not half as busy the other 364 days of the year.) Cloud infrastructure can scale up and down with the peaks and valleys of a year, month, day, or hour. 
Keep in mind, an organization’s cloud spend can get out of control without the proper planning and execution, and cloud isn’t inherently a money-saving move if done improperly. It does however undoubtedly allow an organization to . . . 
Cloud shifts tech systems from a capital expenditure (CapEx) to an operating expenditure (OpEx) — or from an investment in something you hold onto for a few years that will depreciate in value (like a bunch of expensive hardware gathering cobwebs) to a regular, ongoing cost for running the business (like paying for internet). That’s great news for businesses that like to hang onto as much cash as possible. (You can talk to your finance department to confirm, but that’s probably anywhere anyone works.)
Agility can mean a couple of different things around the cloud. “Cloud agility” is often used to describe the ability to quickly develop, test, and launch business applications. But cloud also gives you the agility to respond quickly as needs change. 
Even small businesses get access to the same powerful tools that the biggest enterprises use. New services can be accessed with a few mouse clicks, so when a new need, challenge, or opportunity arises, it’s possible to respond immediately.
And cloud can also make it easier for organizations with multiple offices — eliminating the need to set up infrastructure in each location. Just turn on the power and the internet and your people are ready to go. It also makes it possible for your workforce to suddenly switch to, say, working from home (looking at you, 2020).
The big cloud service providers run a worldwide, world-class network of facilities packed with cutting-edge tech. This ensures everything from keeping network latency low to delivering near unparalleled data backup and disaster recovery. No matter where the people who need access to your tools are, the cloud typically brings them closer.
Most cloud providers are big companies with big companies relying on them. That’s why they go out of the way to consider security and compliance, which includes staying on top of updates and trends that will ensure your sensitive data is safe in the cloud.
Public cloud providers typically bring to the table policies, tech, and controls that are a huge step up from the average organization’s security practices. This is paired with considerations for almost any industry-specific compliance needs.
Plus, keeping data in the cloud rather than on your hard drive can also keep data from getting compromised if a device is stolen or misplaced.
Maintaining computer hardware and software is a full-time job. Literally. With public cloud, you don’t have to have employees spending time dedicated to the tedious maintenance of equipment that doesn’t directly contribute to business objectives. Your cloud service provider ensures the infrastructure is in place, so your tech wizards can focus on driving business outcomes.
Create a culture of cloud innovation with accelerate cloud success with hands-on learning at scale. Upskill or reskill 10 or 10,000 with the most comprehensive and up-to-date learning library, assessments and sandbox software.
Without the right strategy, you can hurt your chances of achieving your desired cloud benefits. There are many common mistakes when companies go cloud, and a botched migration can mean hampered performance and increased costs. It’s regularly reported that more than half of companies don’t see the cloud benefits they expected. This is commonly due to faults in migration strategy and a lack of the necessary cloud talent.
If you’re looking for a guide to cloud migration, here are the three fundamentals to consider no matter what type of cloud and what cloud service models you’ll use:
At a more detailed level, it’s all about figuring out the migration process and giving plenty of thought to planning.
Need an accurate read of cloud competency across your organization? Start with a cloud training needs analysis to identify skills gaps. Try our Skills Assessment to position your team for further cloud success.
Ideally, you won’t be winging it when migrating to the cloud. Keep tabs on what’s done and what’s next to ensure all the moving pieces end up where they should. Need a cloud migration template? Here some steps to include on your cloud migration checklist.
Why are you moving to the cloud? This calls for a formal business case! It’s important for leadership to be clear on the purpose of the migration — and set aggressive goals to drive the organization forward. This includes creating a baseline of your current IT infrastructure and forming some cloud migration key performance indicators (KPIs). These will make it possible to measure your cloud migration success.
Take stock of what’s in your environment, noting any interdependencies, then figure out what you’ll migrate first and how you’ll migrate it. Look at which applications can be moved as they are, which ones will need some (or A LOT of) reworking, and what tools are out there to simplify migration of those trickier workloads. Pick your cloud deployment models and the various tools and services out there (see below). You’ll ideally want to figure out ROI (return on investment) for things you’ll be migrating and how long that might take to achieve. 
Time to roll those sleeves up and get your hands cloudy. When you’re ready to start migrating, it’s typically best to start with something not overly complex or business-critical. With any luck, that quick win will boost excitement and teach you some things along the way. 
Applications should be designed, migrated, and validated using one of the migration strategies covered below. After making the move, you’ll want to test everything out and decommission your old systems. This can mean running two environments for a time, but you can keep this time in limbo from lasting too long by ensuring your cloud leaders are ready to confirm all systems are go and are able to measure your cloud success.
As you migrate applications, keep hammering away to figure out your new operating model, turn off those legacy systems, and keep pushing forward.
There are three basic types or patterns of cloud migration. In the order below, they run from easiest and fastest (with some drawbacks) to more difficult (with bigger benefits). 
Let’s dig in a bit more into the types of cloud migration strategies and the pros and cons of each.
What are the advantages of a lift and shift (also called “rehosting) approach? It’s quick and requires minimal refactoring. But (and this is a big but) the downsides of lift-and-shift include the fact that you may miss out on benefits you’d see if going cloud native because you’re performing the bare minimum changes needed. This means you could end up paying for the speed and the ease of your migration in the long run — at least when compared to a more thorough approach.
Lift-and-shift can be used for simple, low-impact workloads, particularly by organizations that are still far from cloud maturity. If your current setup includes loads of virtual machines, it’s relatively straightforward. There are even products from vendors that claim to help you automate your migration (but doing the process manually can be a great learning exercise). The good news is even if you’ve hastily made a lift-and-shift, applications are easier to rework and optimize once they’re running in cloud.
The move-and-improve (or replatforming) approach to migration includes making some modern updates to your application — like, say, introducing scaling or automation — without throwing the whole thing out. This happy-medium approach can seem like the superior option at a glance, but it can result in migrations where you keep all your technical debt and get none of the cloud-native benefits.
This approach (also called refactoring or re-architecting) means rebuilding your workload from scratch to be “cloud-native.” It takes an investment in time and skills development (particularly reskilling and upskilling your existing talent), but it pays out with the maximum benefits available in the cloud. 
While every organization and workload is unique, if the reason for moving to the cloud is to take advantage of its awesome benefits and capabilities, embracing cloud-native design principles should be your approach. This means taking the time to plan and do things right: ensuring your people have the necessary skills to make the move and refactoring your code.
Big public cloud providers like AWS, GCP, and Azure want you to move to their slice of the cloud (go figure), so they throw plenty of tools your way to make migrations as pain-free as possible.
Of course, they’re also happy to take your money if you want to just throw it their way. Cost savings are a potential benefit of moving to cloud, but cloud costs can also easily get out of control. That’s why it’s important to use all the tools at your disposal to plan for what you need and adjust processes. Things that worked fine on-premises can be costly mistakes in the cloud.
Cloud cost calculators can help you sneak a peek at the cost of your setup before making the move. Check out the one your public cloud provider offers. Examples include the AWS Pricing Calculator, the Azure Pricing Calculator, and the (equally surprisingly named) Google Cloud Pricing Calculator.
Other tools to check out early on include (for AWS) AWS Trusted Advisor and (for Azure) Microsoft Azure Advisor. These give you real-time guidance around cloud best practices and can also help with cost optimization, as well as security and performance.
What cloud migration tools does AWS offer? If you’re looking for Amazon cloud migration services, the cloud juggernaut has a range of solutions — including plenty of free ones — to help you kick off your migration.
There are also several tools for migrating data and files, including AWS Snowball, AWS Snowball Edge, AWS Snowmobile, AWS DataSync, and AWS Transfer for SFTP. You can get more details on these and other services from AWS.
Need Azure cloud migration tools? Azure offers plenty of resources, including tools, tutorials, and videos. (We also have a few tips for those looking to migrate servers to Azure.)
GCP has two different paths for simplifying cloud migration planning. The first (and newer option) is called Google Cloud Rapid Assessment & Migration Program (RAMP), which the company describes as a “holistic, end-to-end migration program.” The second option is The Google Cloud Adoption Framework (a whitepaper you can download) and the 15-minute Cloud Maturity Assessment.
Ready to start migrating to the cloud that Google built? The company has its own laundry list of migration services.
Other Google Cloud migration tools can be viewed on Google’s migration center site.
Anticipating potential issues is a key part of your cloud migration strategy. What are some common cloud migration challenges?
Hungry for more cloud knowledge? Need self paced practice labs for a true contextual learning experience? A Cloud Guru makes it easy to get up to speed in the cloud — whether you’re an individual looking to learn more or part of an organization looking to upskill thousands.
Start a free trial for yourself, view plans for your organization, or peruse our rotating lineup of free cloud courses and start learning cloud by doing.

Get more insights, news, and assorted awesomeness around all things cloud learning.
"
https://medium.com/@zackbloom/isolates-are-the-future-of-cloud-computing-cf7ab91c6142?source=search_post---------20,"Sign in
There are currently no responses for this story.
Be the first to respond.
Zack Bloom
Oct 18, 2018·7 min read
I work on a cloud computing platform. Unlike essentially every other cloud computing platform I know, it doesn’t use containers or virtual machines. I think that is the future of Serverless and cloud computing in general, and I’ll try to convince you why.
(I’m a part of Cloudflare, where we have our Workers platform, making me incredibly biased. Please discount what I say appropriately in that light, and feel free to leave comments here or on Hacker News where you disagree.)
Almost two years ago we set out to find a way to let people write code on our servers deployed around the world (we had a little over a hundred data centers then, it’s 154 as of this writing). We were dramatically limited in how many features and options we could build in-house and needed a way for customers to be able to build for themselves. We needed a way to run untrusted code securely, with low overhead. It also had to run very very quickly, as we sit in front of ten million sites and process millions and millions of requests per second.
The Lua we had used previously didn’t run in a sandbox, meaning customers couldn’t write their own code without our supervision. Traditional virtualization and container technologies like Kubernetes would have been exceptionally expensive for everyone involved. Running thousands of Kubernetes pods in a single location would be resource intensive, much less 154. Scaling them would be easier than without a management system, but far from trivial.
What we ended up settling on was a technology built by the Google Chrome team to power the Javascript engine in that browser, V8: Isolates.
Isolates are lightweight contexts which group variables with the code allowed to mutate them. Most importantly, a single process can run hundreds or thousands of Isolates, seamlessly switching between them. They make it possible to run untrusted code from many different customers within a single operating-system process. They’re designed to start very quickly (several had to start in your web browser just for you to load this web page), and to not allow one Isolate to access the memory of another.
We pay the overhead of a Javascript runtime once, and then are able to run essentially limitless scripts with almost no individual overhead. Any given Isolate can start around a hundred times faster than I can get a Node process to start on my machine. Even more importantly, they consume an order of magnitude less memory than that process.
They have all the lovely function-as-a-service ergonomics of getting to just write code and not worry how it runs or scales. Simultaneously, they don’t use a virtual machine or a container, which means you are actually running _closer_ to the metal than any other form of cloud computing I’m aware of. I believe it’s possible with this model to get close to the economics of running code on bare metal, but in an entirely Serverless environment.
Not everyone fully understands how a traditional Serverless platform like Lambda works. It spins up a containerized process for your code. It isn’t running your code in any environment more lightweight than running Node on your own machines. What it does do is auto-scale those processes (somewhat clumsily). That auto-scaling creates cold-starts.
A cold-start is what happens when a new copy of your code has to be started on a machine. In the Lambda world this amounts to spinning up a new containerized process, and it can take between 500ms and 10 seconds. This means any requests you get will be left hanging for as much as ten seconds, a terrible user experience. As a Lambda can only process a single request at a time, a new Lambda must be cold-started every time you get an additional concurrent request. This means that laggy request can happen over and over. If your Lambda doesn’t get a request soon enough, it will be shut down and it all starts again. Whenever you deploy new code it all happens again as every Lambda has to be redeployed. This has been correctly cited as a reason Serverless is not all it’s cracked up to be.
Because we don’t have to start a process, Isolates start in 5ms, a duration which is imperceptible. Isolates similarly scale and deploy just as quickly, entirely eliminating this issue with existing Serverless technologies.
One of the key features of an operating system is it gives you the ability to run many processes at once. It transparently switches between the various processes which would like to run code at any given time. To accomplish that it does what’s called a ‘context switch’, moving all of the memory required for one process out, and the memory required for the next in.
That context switch can take as much as 100uS. When multiplied by all the Node, Python or Go processes running on your average Lambda server this creates a heavy overhead which means not all of the CPUs power can actually be devoted to running the customer’s code; it’s spent switching between them.
As an Isolate-based system runs all of the code in a single process and uses its own mechanisms to ensure safe memory access, there are no context switches.
The Node or Python runtimes were meant to be ran by individual people on their own servers. They were never intended to be ran in a multi-tenant environment with thousands of other people’s code and strict memory requirements. A basic Node Lambda running no real code consumes 35 MB of memory. When you can share the runtime between all of the Isolates as we do, that drops to around 3 MB.
Memory is often the highest cost of running a customer’s code (even higher than the CPU), lowering it by an order of magnitude dramatically changes the economics.
Fundamentally V8 was designed to be multi-tenant. It was designed to run the code from the many tabs in your browser in isolated environments within a single process. Node and runtimes like it were not, and it shows in the multi-tenant systems which are built atop it.
Running multiple customer’s code within the same process is something which should, obviously, be done with a careful attention paid to security. I don’t think it would ever have been productive or efficient for us to build that isolation layer ourselves. The amount of testing, fuzzing, penetration testing, and bounties required to end up with something truly secure is astronomical.
The only reason this was possible at all is the open-source nature of V8, and it’s standing as perhaps the most well security tested piece of software on earth. We also have a few layers of security built on our end, including various protections against timing attacks, but V8 is the real wonder that makes this compute model possible.
This is not meant to be a referendum on AWS billing, but it’s worth a quick mention as the economics are interesting. Lambda’s are billed based on how long they run for. That billing is rounded up to the nearest 100ms, meaning people are overpaying for an average of 50ms every execution. Worse, they bill you for the entire time the Lambda is running, even if it’s just waiting for an external request to complete. As external requests can take hundreds or thousands of ms, you can end up paying ridiculous amounts at scale.
Isolates have such a small memory footprint that we, at least, can afford to only bill you while your code is actually executing.
In our case, due to the lower overhead, Workers end up being around 3x cheaper per CPU-cycle. A Worker offering 50ms of CPU is $0.50 per million requests, the equivalent Lambda is $1.84 per million.
Amazon has a product called Lambda@Edge which is deployed to their CDN data centers. I’m sure they’re doing their best with the technology they have to make it compelling, but it’s three times more expensive than traditional Lambda, and it takes 30 minutes to deploy a code change. I don’t think there’s a compelling reason to use it.
Conversely, as I mentioned, with Isolates we are able to deploy every source file to 154 data centers at better economics than Amazon can do it to one. It might actually be cheaper to run 154 Isolates than a single container, or perhaps Amazon is charging what the market will bear and it’s much higher than their costs. I don’t know Amazon’s economics, I do know we’re very comfortable with ours.
It long-ago became clear that to have a truly reliable system it must be deployed to more than one place on earth. A Lambda runs in a single availability zone, in a single region, in a single data center.
No technology is magical, every transition comes with disadvantages. The biggest is an Isolate-based system can’t run arbitrary compiled code. In a world where you have process-level isolation your Lambda can spin up any binary it might need. In an Isolate universe you have to either write your code in Javascript (we use a lot of TypeScript), or a language which targets WebAssembly like Go or Rust.
If you can’t recompile your processes, you can’t run them in an Isolate. This might mean Isolate-based Serverless is something which is only for newer, more modern, applications in the immediate future. It also might mean legacy applications get only their most latency-sensitive components moved into an Isolate initially. The community may also find new and better ways to transpile existing applications into WebAssembly, rendering the issue moot.
I would love for you to try Workers and let us and the community know what your experience is like. There is still a lot for us to build, we could use your feedback.
We also need engineers and product managers who think this is interesting and want to take it in new directions. If you’re in San Francisco, Austin, or London, please reach out.
Product Strategy at Cloudflare
455 
6
455 claps
455 
6
Product Strategy at Cloudflare
"
https://faun.pub/an-introduction-to-microsoft-azure-and-cloud-computing-54a7ebac0287?source=search_post---------21,"You have 1 free member-only story left this month. Sign up for Medium and get an extra one
You have heard lots of things about the cloud. Let’s understand each component of the cloud on this story. Also, let’s understand how the organizations are implementing various cloud models using Microsoft Azure. Before going further let’s know a bit about what is cloud computing is all about. As per the general definition, we can see
Cloud computing is a model for enabling convenient, on-demand network access to a shared pool of configurable computing resources (e.g., networks, servers, storage, applications, and services) that can be rapidly provisioned and released with minimal management effort or service provider interaction.
How many types of cloud computing are there?
The thing that differentiates cloud and On-Premise is that cloud service must be accessible over the network (Internet) and can be accessed using different types of clients (like PC, smartphone, or tablet).
Cloud Computing typically has the following characteristics:
Now all the fuzz about Cloud computing and all is done. The main point now comes is what is the model to work on that.
Is everything related to internet and server is Cloud or is there a different approach to it altogether.
Let's have a look into Cloud Deployment Models
The most commonly used cloud deployment model is public.
A public cloud means the service is run by an organization that is not a part of the organization to which the consumer belongs. The business objective of a public cloud provider, in most cases, is to make money. Another characteristic of a public cloud is that it is open tomultiple consumers. This so-called multitenant usage is offered in data centers that are only accessible to employees working for the operator of the service.
A private cloud is the opposite of a public cloud. Services offered in a private cloud are typically consumed by a single organization. The infrastructure can be located either on-premise or in a data center owned and operated by a service provider. The provider of the private cloud service is the IT department. It is also possible that the cloud management is outsourced to a vendor while the IT department handles the governance. A private cloud, in most cases, exists in large organizations that have frequent demands for new IT services. Organizations with a lot of software developers are use cases for private cloud, as developers have frequent requests for new virtual machines.
A Hybrid cloud is a combination of public cloud services and private clouds. It is not necessary to have a private cloud in order to use the hybrid cloud. A hybrid cloud can be a combination of virtualized, on-premises data centers and public cloud services as well. Hybrid cloud can be done on the IaaS or SaaS level. Hybrid cloud can also be seen as a bridge between the public and private clouds, which enables moving workloads between those deployments based on policy, costs, and so on.
Now everything said, how can we use Cloud Computing and how can it affect us in a good way. We need our Power Tools.
Azure is one of the top players when it comes to Cloud Computing Market. Other players in the fields are.
Few of the names are:-
Microsoft Azure is an open and flexible cloud platform that serves as the development, data storing, service hosting and service management environment. Microsoft Azure provides on-demand compute and storage to host, scale, and manage web applications on the internet through Microsoft data centers. Even, you can integrate your public cloud applications with your existing IT environment.
The following are some important aspects wherein Azure scores over any other Cloud Providers out there:
Source: Indeed.com
The demand for Azure Solutions Architect and Developers as can be seen in the trends are increasing, and hence it makes sense for you to upgrade yourself to be the master of the cloud.
Join our community Slack and read our weekly Faun topics ⬇
The Must-Read Publication for Creative Developers & DevOps Enthusiasts
297 
1
297 claps
297 
1
Written by
I am a professional Backend and DevOps Developer I mainly work on NodeJS and Python based Backend. On Cloud I have my forte on AWS and Azure.
The Must-Read Publication for Creative Developers & DevOps Enthusiasts. Medium’s largest DevOps publication.
Written by
I am a professional Backend and DevOps Developer I mainly work on NodeJS and Python based Backend. On Cloud I have my forte on AWS and Azure.
The Must-Read Publication for Creative Developers & DevOps Enthusiasts. Medium’s largest DevOps publication.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/technology-hits/pandemic-has-changed-the-future-of-cloud-computing-1155e94d97d?source=search_post---------22,"There are currently no responses for this story.
Be the first to respond.
This is your last free member-only story this month. Sign up for Medium and get an extra one
Top highlight
Cloud computing is an internet-based service. This provides on-demand computing services like data storage, database, networking, server, analytics, and intelligence through the Internet.
It offers services as per our requirement for those we need to pay. We need to pay for the services we are using which are cost-effective. We can set up a virtual office and operate our business from anywhere and anytime. We do not need to buy a local storage device or any networking infrastructure for our office.
These days it has become a popular option because it is cost savings, increase productivity, speed and efficiency, performance, and security.
Technology has a big impact on personal life and the business profession. After the Covid-19 in 2020 requirement for the digital process has increased rapidly. Almost every big country went through a lockdown.
Most of the works is being organized from home. The demand for the internet has been noticed in every area. These are especially in the corporate, education sectors, and e-commerce activities.
In 2019, before the pandemic, the global cloud computing market size was valued at USD 266.0 billion, and the Compound Annual Growth Rate (CAGR) was expected 14.9% from 2020–2027. But, after the pandemic, the expected CAGR is 17.5% by 2025 only. It is showing rapid demand for cloud computing worldwide.
Now, we will understand the types of cloud computing and also why Hybrid cloud is the best option. There are 4 types of cloud computing are available to fulfill most of the requirements of organizations.
These are Public, Private, Hybrid, and Community Cloud. The public cloud is open to all. Computing resources are managed and operated by the cloud service provider and available for any users.
In the private cloud, resources are not available publically and are managed by the organization itself, and are dedicated to a single business only.
A hybrid cloud is a combination of public and private clouds. This is the best method for any business organization. It facilitates both public and private services.
Services on public cloud computing can be accessed by anyone, and in private cloud computing, services can be accessed by specified users of the organization only. Therefore, an organization can decide what services they want to perform through public or private cloud computing.
Hybrid cloud computing is mainly required for those where a high engagement is noticed from outsides the organization. Therefore, it is important to give separate access to users such as public information for anyone and while internal and critical information only for users of the organizations. Insurance, healthcare, education sectors are an example of Hybrid cloud computing.
Three main services are available in cloud computing. These are Infrastructure as a Service (IaaS), Software as a Service (SaaS), and Platform as a Service (PaaS). These all services are used for different purposes.
Many major cloud computing service providers play a big role. IaaS, PaaS, and SaaS all the essential services which are required for operating a business efficiently. It is important to understand which services are more important. It depends on our business process and activities.
If we compare Microsoft Azure models, we will know IaaS provides visualization, servers, storage, and networking. PaaS is available for visualization, servers, storage, networking, runtime, middleware, and O/S. In the last, SaaS provides all services mentioned in PaaS including application and data.
The top 5 major players of cloud computing in IaaS are the following:
Apart from the above, there is various service provider available worldwide.
These are the following main reason for the increasing demand for cloud computing in the business.
Cloud computing is a game-changer in the business. We can adopt this because we do not need to pay for computer accessories or hardware, IT staff, maintenance. It provides the easiest way to access data and information that save our time and money.
Cloud computing charges for the services we decide. It is up to us to define and refine our requirements. We can get additional features if it is needed.
We can take as an example, if we have a requirement of 100 TB storage, we need to pay for that only. Similarly, if we are small enterprises, we have the choice to pay for small types of networking processes. It depends on us to choose the features and manage our company cost liability.
Data storage and backup are the biggest challenges for every businessman. Data collection from different sources and keep it safe for a long time are very important for all businessmen. Cloud data storage allows store data in the cloud which are easily accessible and usable.
These data can be secured and protected from any types of theft because cloud services also provided an end-to-end data encryption facility. When data is on the cloud, it can not be damaged because of natural disasters. These data are easily recoverable.
Most of the companies are adopting digital processes and they want to reduce paperwork. Cloud computing allows mobile access to data easily on smartphones and devices. We can do all the activities from home and even we are traveling by car or train.
Cloud technology provides an excellent enhancement in its internal communication. It has made the system more efficient compare to traditional methods.
These days video conferencing is arranged, discussed and relevant data are shared in one click. All the users may get the facility to use data. They do not need to depend on other members. These days it is maintaining a good work-life balance.
In cloud computing, we access and store all the data on the cloud’s server. Therefore if our computers, laptop, smartphones, hard drives, and things get physically damaged, we can retrieve data easily from the server. Cloud computing services are essential business tools, and therefore we have a feeling what if our data is not secure.
Cloud service providers are much aware of these types of threats. They maintain all security provisions and ensure their client not worry as these cloud services are already integrated with powerful firewalls, security protocols, antivirus, or data security.
We do not need to think about software up-gradation. They provide a high-end encryption policy as well as data and network security. These all are to secure data from cybercrime.
Any types of data and information are sensitive and critical for any business. Therefore, it is the responsibility of internet security engineers to keep engage themselves on day to day basis to work on data safety and security. Cloud computing service providers have sufficient experts and scientists to work on this and be accountable for user’s data.
The future of cloud computing is now everywhere and for everyone. These days the organizations that are still using trading infrastructure have started thinking that cloud computing is their immediate need.
Many internet services are already integrated with cloud computing, and physical infrastructure will not be very successful because it will not sustain big corporate organizations. Cloud computing is thinking as a major part of business innovation.
As per Innovecs.com, many organizations have already adopted cloud computing services for their workloads. Around 80% of big corporations are using cloud computing, and it is expected that the number will increase by 90% up to 2024.
“If someone asks me what cloud computing is, I try not to get bogged down with definitions. I tell them that, simply put, cloud computing is a better way to run your business.” - Marc Benioff, Founder, CEO and Chairman, Salesforce
Please also read the related blog of Dr. Preeti Singh on Cloud Computing.
medium.com
My previous blogs on technology:
medium.com
medium.com
medium.com
medium.com
Important, informing, and engaging stories on technology
1.5K 
9
Featured stories, tech news, industry trends, useful tips Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
1.5K claps
1.5K 
9
Written by
A manager, interested in writing on technology, life style, health, work and happiness, thinker, reader, follower, techy.
Important, high-impact, informative, and engaging stories on all aspects of technology.
Written by
A manager, interested in writing on technology, life style, health, work and happiness, thinker, reader, follower, techy.
Important, high-impact, informative, and engaging stories on all aspects of technology.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/hackernoon/real-world-applications-of-cryptocurrencies-cloud-computing-59745dd95145?source=search_post---------23,"There are currently no responses for this story.
Be the first to respond.
As part of my series “Real World Applications of Cryptocurrencies”, and follow-up from my previous post on Payment Systems & the Request Network, which you can find here, I will be discussing how the Cloud Computing industry will be disrupted by the emergence of cryptocurrencies.
Cloud computing has become increasingly popular for carrying out heavy-computational tasks but there is one major issue — it’s extremely centralized. This leads to a number of drawbacks, including:
Enter Golem (GNT).
Golem is an accessible-to-everyone, distributed, supercomputer that reduces costs [sometimes by over 10x] of scientific research, big data analysis, graphics rendering, machine learning, AI… just about anything that requires heavy computational power. All while bringing CPU cycles into the sharing economy so anyone with a computer can connect it to the Golem Network when they’re not using it and make an automated side income.
Golem connects computers in a peer-to-peer network, enabling both application owners and individual users (“requestors”) to rent resources of other users’ (“providers”) machines. These resources can be used to complete tasks requiring any amount of computation time and capacity. I will describe a simple example below:
Alice is a requestor on the Golem network. Bob, Charlie and many others are providers; renting out their idle PC resources for a passive income. Alice has created a CGI animation and wants to render it using Golem.
The animation can be divided into frames. Each of these frames will then be rendered individually using resources provided by Bob, Charlie and others on the network. Therefore, Alice is essentially paying Bob, Charlie to carry out the complex rendering out for her.
Golem aims to solve most of these issues and even improve upon them in the following ways:
D̵u̵e̵ ̵t̵o̵ ̵h̵o̵w̵ ̵t̵e̵c̵h̵n̵i̵c̵a̵l̵l̵y̵ ̵c̵o̵m̵p̵l̵i̵c̵a̵t̵e̵d̵ ̵b̵u̵i̵l̵d̵i̵n̵g̵ ̵s̵o̵m̵e̵t̵h̵i̵n̵g̵ ̵l̵i̵k̵e̵ ̵t̵h̵i̵s̵ ̵i̵s̵,̵ ̵t̵h̵e̵ ̵G̵o̵l̵e̵m̵ ̵t̵e̵a̵m̵ ̵h̵a̵s̵ ̵b̵e̵e̵n̵ ̵h̵i̵t̵ ̵b̵y̵ ̵a̵ ̵n̵u̵m̵b̵e̵r̵ ̵o̵f̵ ̵s̵e̵t̵-̵b̵a̵c̵k̵s̵.̵ ̵T̵h̵e̵i̵r̵ ̵f̵i̵r̵s̵t̵ ̵r̵e̵l̵e̵a̵s̵e̵ ̵(̵n̵a̵m̵e̵d̵ ̵B̵r̵a̵s̵s̵ ̵G̵o̵l̵e̵m̵ ̵ — ̵ ̵s̵p̵e̵c̵i̵a̵l̵i̵z̵i̵n̵g̵ ̵i̵n̵ ̵C̵G̵I̵ ̵r̵e̵n̵d̵e̵r̵i̵n̵g̵)̵ ̵w̵a̵s̵ ̵i̵n̵i̵t̵i̵a̵l̵l̵y̵ ̵p̵l̵a̵n̵n̵e̵d̵ ̵t̵o̵ ̵b̵e̵ ̵r̵e̵l̵e̵a̵s̵e̵d̵ ̵i̵n̵ ̵Q̵2̵ ̵2̵0̵1̵7̵.̵ ̵T̵h̵i̵s̵ ̵h̵a̵s̵ ̵s̵i̵n̵c̵e̵ ̵b̵e̵e̵n̵ ̵p̵u̵s̵h̵e̵d̵ ̵t̵o̵ ̵Q̵1̵ ̵2̵0̵1̵8̵,̵ ̵h̵o̵w̵e̵v̵e̵r̵,̵ ̵a̵n̵ ̵a̵l̵p̵h̵a̵ ̵v̵e̵r̵s̵i̵o̵n̵ ̵i̵s̵ ̵a̵l̵r̵e̵a̵d̵y̵ ̵r̵u̵n̵n̵i̵n̵g̵ ̵o̵n̵ ̵t̵h̵e̵ ̵t̵e̵s̵t̵-̵n̵e̵t̵.̵ ̵W̵e̵ ̵a̵r̵e̵ ̵s̵t̵i̵l̵l̵ ̵i̵n̵ ̵e̵a̵r̵l̵y̵ ̵s̵t̵a̵g̵e̵s̵ ̵b̵u̵t̵ ̵t̵h̵e̵ ̵p̵o̵t̵e̵n̵t̵i̵a̵l̵ ̵f̵o̵r̵ ̵t̵h̵e̵ ̵p̵r̵o̵j̵e̵c̵t̵ ̵i̵s̵ ̵g̵r̵e̵a̵t̵.̵
Golem’s first release (codenamed Brass) has finally reached the Ethereum main net! You can find more information, here.
Below you can find an excellent short video created by the Golem team, giving a brief introduction on what GNT is and how it will work.
Unlike other projects, the use for GNT tokens is very simple. From Golem’s whitepaper: Payments from requestors to providers for resource usage, and remuneration for software developers is going to be exclusively conducted in GNT.
I would also like to point out that GNT is not the only project out there trying to tackle these problems, including, DeepBrain Child (DBC), Elastic (XEL), GridCoin (GRC), iExec RLC (RLC) and SONM (SNM). If you would like to see a post comparing and contrasting these projects, feel free to drop me a comment below or a tweet.
GNT is available for purchase on a variety of exchanges, including, Bittrex, Bitfinex and many more.
You can follow me on Twitter @ermos_k to keep up with the latest blog posts.
twitter.com
Want to support my work and see more? Show your support by donating here :ETH/GNT: 0x4c7195E074cf0Ab6F77Bdb7C97Fd2567066Bb712
Disclaimer : All information and data on this blog post is for informational purposes only . I make no representations as to the accuracy, completeness, suitability, or validity, of any information. I will not be liable for any errors, omissions, or any losses, or damages arising from its display or use. All information is provided as is with no warranties, and confers no rights.
#BlackLivesMatter
645 
4
how hackers start their afternoons. the real shit is on hackernoon.com. Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
645 claps
645 
4
Written by
Cryptocurrency Enthusiast expert circa 2012. Visit my website at https://ermos.io or reach out via e-mail on contact@ermos.io
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
Cryptocurrency Enthusiast expert circa 2012. Visit my website at https://ermos.io or reach out via e-mail on contact@ermos.io
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/bloomberg/inside-googles-quest-to-become-a-cloud-computing-giant-18e417c49f1c?source=search_post---------24,"There are currently no responses for this story.
Be the first to respond.
By Nico Grant
Google Cloud employees were baffled when they learned that their next boss would be Thomas Kurian, an executive who struggled to shepherd…
"
https://towardsdatascience.com/cloud-computing-aws-1-introduction-ec2-instance-creation-via-console-5177a2b43359?source=search_post---------25,"Sign in
There are currently no responses for this story.
Be the first to respond.
Ilyas Habeeb
Feb 26, 2018·10 min read
Hello, folks! Welcome to the AWS Tutorials. If you don’t know me already, I am a graduate student at NYU who has ventured into the world of writing to share his (limited) knowledge with you all. I am currently undertaking a ‘Cloud Computing’ course in NYU and thought it would be worthwhile to share what I am learning. I am also writing about Machine Learning, so check those articles out as well!
In this series, I will be mostly concentrating on the implementation part and less on the theory part. I understand that there are various articles related to AWS but they are scattered across various articles and I felt that there is a need to ‘tie’ them all together. Also, I plan to write about creating web applications and chatbots using AWS in the upcoming articles.
This article is inspired from a variety of materials, including, but not limited to:
Let us start with the official definition from NIST (National Institute of Standards and Technology):
Cloud computing is a model for enabling ubiquitous, convenient, on-demand network access to a shared pool of configurable computing resources (e.g., networks, servers, storage, applications, and services) that can be rapidly provisioned and released with minimal management effort or service provider interaction.
Went right over your head, didn’t it? Just remember this:
Cloud computing gives access to on-demand resources, such as computing power, database storage, applications, and several other IT resources through web interfaces via the Internet.
For a detailed definition, refer this.
Clouds can be categorized into three types:
Cloud services can be categorized into three types as well:
There are several reasons:
There are primarily five steps involved in the sign-up process:
Go to https://aws.amazon.com, and click the Create a Free Account/Create an AWS Account button. Follow the steps on the screen to create your account.
2. Provide your Contact Credentials
Just fill your contact information in the required fields.
3. Provide your Payment Details
Yes, you have to provide your payment details in order to create an AWS account. Bear note that AWS supports MasterCard and Visa.
4. Verify your Identity
After you complete the above steps, you will receive a call from AWS. A pin number will be displayed on your screen and you have to provide this pin to the robotic voice over the phone.
5. Choose your Support Plan
Choose the Basic plan as it’s free.
Congratulations! You managed to create an AWS account. You can now sign in to your AWS Management Console at https://console.aws.amazon.com
Note: It is highly recommended that you set up a billing alarm as advised by AWS: http://mng.bz/M7Sj
Creating an EC2 Instance is akin to writing a “Hello World” program in a programming language. It is crucial that you understand the various processes involved in creating this instance.
EC2 is short for Elastic Compute Cloud. It is a service which provides us with a virtual server called an instance. With an Amazon EC2, you can set up and configure the OS and install applications on this instance.
The architecture of launching an Amazon EC2 instance looks something like this:
The Instance is an Amazon EBS-backed instance, i.e., root is an EBS volume. Amazon EBS (Elastic Block Storage) provides persistent block storage volumes for use to the instance. Amazon EBS provides the following:
You can either specify the Availability Zone for your instance (where it should run), or AWS does that automatically for you.
To launch your instance, you need to secure it by specify a key pair and a security group.
A key pair consists of a private key and a public key. The public key will be uploaded to the AWS and inserted into the virtual server. You are suppose to keep the private key; it acts like a password but is much more secure. When you want to connect to an instance, you need to verify your identity. This is done by authentication using the private key.
Security groups are a fundamental service of AWS to control network traffic like a firewall. When an instance is launched, one or more security groups is associated with it. For more information related to security groups, refer the official AWS documentation: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-network-security.html
Enough with the theory! Now, we will look into launching an instance in two ways:
We will create an EC2 instance for Linux since it is easy to do so. There are three main steps involved in this process:
There are two ways you can connect to your instance:
The official AWS documentation provides guidelines for connecting the instance through the browser. However, we will connect our instance through a standalone SSH. Also note that, I will be connecting the instance through a Mac but I will try to provide instructions for Window users wherever I can.
For Linux (and Mac) Users:
Downloads/my-key-pair.pem
ssh -i [Path of private key] user_name@[public_dns_name or IPv4_address]
Congratulations! You are now connected to the Amazon Linux AMI.
For Window Users:
Window users have it a bit rough. They don’t have an SSH client by default, so they need to install PuTTY which is a free SSH client for Windows. If you have an older version of PuTTy, it is recommended to install the latest version. I will not provide step-by-step instructions; rather, I will redirect you to links where you can do this on your own.
For the last three instructions, refer the AWS official documentation: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/putty.html
It is important that you terminate your instance after you use it. Since you are using the AWS Free Tier version of the instance, if you just let your instance sit idle, you will incur expenses. Here are the steps to do it:
Phewh! That was a handful! We have finally learned how to create an instance, access the instance through an SSH, and terminate it.
That’s it for this folks! Next week, we will learn to do the same thing,i.e., create an Amazon Linux instance, programmatically. Stay tuned!
Graduate Student at NYU. Data Scientist in the Making.
163 
1
Every Thursday, the Variable delivers the very best of Towards Data Science: from hands-on tutorials and cutting-edge research to original features you don't want to miss. Take a look.
163 claps
163 
1
Your home for data science. A Medium publication sharing concepts, ideas and codes.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/financial-times/does-cloud-computing-mean-gameover-for-xbox-and-playstation-8e13b0a2418c?source=search_post---------26,"There are currently no responses for this story.
Be the first to respond.
By Leo Lewis
Home entertainment has always been a ruthless business, but games consoles’ 50-year territorial war for the living room has…
"
https://medium.com/hackernoon/the-future-of-cloud-computing-aws-is-the-system-architecture-of-the-web-whats-next-17aaa6077f47?source=search_post---------27,"There are currently no responses for this story.
Be the first to respond.
It’s that time of year — AWS re:Invent. The time when tens of thousands of developers, product managers, operations engineers, venture capitalists and industry experts congregate in Las Vegas to discuss the future of the cloud computing behemoth that is Amazon.
I was fortunate enough to present center stage at the Venetian on Tuesday morning with Paul Underwood, an AWS Solutions Architect who works closely with young and innovative startups. We shared a vision of the serverless future, how the transition ought to happen, and how what we’re building at Polybit — stdlib: A Function as a Service Software Library — helps developers and companies build, maintain, organize, share and scale services in this new frontier.
In the same way that JavaScript developers don’t worry about the memory addresses of the functions they write, future cloud developers in a serverless world will never have to worry about the physical locations, geography, VMs or containers in which their code executes. They will build services that just work. Period. Scale will cease to be a concern. Some of these services will be shared within an organization, others with the world as a whole.
In a sense, you can consider providers like AWS as system architectures of the web. Software-configurable infrastructure provides us with a wide array of options to accomplish a specific task. The breadth of these infrastructure offerings presents a large opportunity for opinionated software to help create an abstraction layer on top that drastically simplifies large, at-scale software development.
Viewing cloud infrastructure from this perspective, we can draw parallels to the evolution of IBM and Microsoft for a guideline as to how creating abstraction layers (such as an operating system) on top of a compute layer impacts the progression of technology within the industry. I would argue that the future of cloud computation is not rooted in offering more and more specialized infrastructure services and not about the configuration of cloud-based systems in any combination you desire. The future is reductive, in a sense— it’s about enabling developers to forget about the underlying infrastructure implementation details altogether. No hardware, no metaphor for hardware like containers, just code.
Over the next decade, cloud compute volume (and competition between AWS, Azure, and other players) will continue to increase dramatically, but spending on cloud services will shift up the value chain to the application layer, in the same way we previously observed with Microsoft on top of IBM — focus moved towards user-friendly operating systems that enabled businesses to move faster instead of the computer layer itself. Margins and profits on infrastructure as a service will trend towards zero as computation becomes commoditized, with industry spend rewarding players focused on enabling usability over raw system configuration.
The shift in focus towards user experience on standalone systems in the past can be used as a map to help us predict how the cloud compute market will change. Infrastructure providers simply do not have the vision and maneuverability to execute on building the next layer of abstraction. It is a challenging task for larger players to shift up the value chain alone, because the design-by-committee mantra that dominates established corporations is not a suitable environment to foster the growth of the first iterations of these “operating system of the cloud” abstractions.
As the serverless space matures, we will see startups, driven by talented teams focused on building simple developer products, dominate the field. This has been, and will continue to be, the status quo of the emerging technology market. Think of the successes of Dropbox or Stripe, but applied to cloud computation as a whole instead of a specific business vertical.
The infrastructure providers that pave the way for these startups to flourish will ultimately set themselves the best for success in the future. Providers that spread themselves too thin trying to dominate both the commodity and usability layers are in for a rude awakening — a death by a thousand needles, built atop their competitors. AWS, though more than capable of outspending startups, has historically shown an awareness of this, with a certain willingness to “play ball” and encourage innovation on top of their platform — Heroku being one of the largest successes.
Companies like Firebase and Parse acted as pioneers in the cloud services space, focused on developer experience with infrastructure as a commodity. I believe we will look back on them as the early indicators for the upcoming “Cambrian explosion” of developer-focused ease-of-use tooling and abstractions in the cloud infrastructure market.
Our data certainly suggests this is the case. Within weeks of launching the latest version of stdlib we saw developer adoption (measured by compute time on platform) skyrocket. After months of iteration and nailing down the best serverless workflows, our growth chart, as measured by platform usage, looks like this (Y-Axis labels obfuscated);
Thousands of developers have registered for our platform, with hundreds actively creating services in just the past few weeks. This is only the beginning. The usage of the serverless developer tools and frameworks today is just a drop in the bucket of industry-wide transition that’s underway.
Keep pushing forward. Infrastructure management is old hat. Feature delivery without operational management is where the industry is heading. Maintain a key focus on developer experience as you build and experiment with new technology.
The shift to serverless architectures is the first step in a much larger transition in how we think about interfacing with the cloud. Our vision for the future is relatively straightforward — the computational resources of our planet will be accessible, seamlessly, to any developer worldwide, without needing domain-specific infrastructure expertise. This will not only have a fundamentally disruptive impact on how people think about developing software, but also how people think about forming business entities in the new “API economy” altogether.
At Polybit, we’re building this future. We’re excited about what’s coming next, and you should be, too. We’re thankful for our wonderful developer community, the kindness of AWS to give us a stage in which to share our ideas and vision, and the reception we’ve received. The act of creating gigantic, globally-scaled platforms in the cloud is about to become an order of magnitude easier.
Keith Horwood is the founder and CEO of Polybit, where he’s building stdlib: A Standard Library for the Web. The team includes talented employees, investors, advisors, and mentors from AngelPad, YC, Google, Heroku, and AWS.
You can follow StdLib on Twitter, @StdLibHQ, or follow Keith directly, @keithwhor.
Hacker Noon is how hackers start their afternoons. We’re a part of the @AMI family. We are now accepting submissions and happy to discuss advertising & sponsorship opportunities.
If you enjoyed this story, we recommend reading our latest tech stories and trending tech stories. Until next time, don’t take the realities of the world for granted!
#BlackLivesMatter
74 
4
74 claps
74 
4
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
Solving problems since 1988.
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
"
https://medium.com/javarevisited/5-best-cloud-computing-courses-to-learn-in-2020-f5f091159401?source=search_post---------28,"There are currently no responses for this story.
Be the first to respond.
Hello folks, Cloud Computing is becoming an essential skill for any Software developer or IT professional today as more and more companies are moving towards Cloud for their infrastructure and service needs.
It pays to learn Cloud Computing today as the most likely application will be solely building and deploy in the cloud in the future.
No matter what your role is, if you are working in technology, cloud computing is going to affect you, one way or another.
Anyway, if you want to learn cloud computing then you have come to the right place, in this article, I am going to share some of the best courses to learn Cloud computing basics in general and with AWS and GCP in particular. To be honest, I have had a tough time finding an excellent course to learn Cloud Computing, after trying a handful of courses on Udemy, Pluralsight, Educative, and Coursera, I have finally zeroed down into following five directions to learn the basics of Cloud Computing. These courses provide a broad introduction to all aspects of cloud computing, but before going to see these courses let’s first understand what is cloud and what benefit it provides? What problem does cloud solve, and why is there so much buzz about it? Well, Cloud has a different meaning for different people and is also used to refer to different things in a different context, but in general, the cloud means a host of services that are ready to use for your application. There are many Cloud service provider companies like AWS (Amazon Web Service), GCP (Google Cloud Platform), and Microsoft Azure, which provides these services. Now, what are those services? Well, these could be servers, networks, storage, computing power, and other infra-related services commonly offered and referred to as IaaS (Infrastructure as service).
There are other cloud models as well, like PaaS (Platform as Service) and SaaS (Software as service) which is nothing but Cloud application and very popular for building online products and startups, but let’s focus on IaaS first to understand the benefits provided by Cloud. Not long ago, in order to host a real-world E-commerce application, you need to buy servers, networks, storage and need to set them up to your requirements like installing the required software and operating system. Most of the companies have their own Data Center where these servers are kept, and there was a high cost involved to keep those data center running. It was just initial time and cost for setting up the server, but also their utilization was poor; for example, secondary servers are often idle just wasting that precious CPU and memory power. The effect was more evident on big companies which own thousands of servers across multiple data center on the world. Amazon was one such company, which quickly realize that by using virtualization, they can use the computing power of their infrastructure, and that gives birth to the cloud.
In the cloud, you can spin up a server, database, network very quickly with just some clicks and you will only be charged for whatever you use. So this solves the problem of setup, utilization, and scalability.
Now that you know what cloud is and what benefits it offers in terms of cost, ease of setup, efficiency, and scalability, it’s time to deep dive and goes through these online training courses, which will help you to learn Cloud Computing fundamentals better.
When I started learning about Cloud, it was a very abstract thing for me; I wasn’t able to understand what the cloud is and why everyone is talking about the cloud. It was like a person without a face, but when I learned about AWS, I could co-relate various cloud concepts very quickly. So, AWS is like putting a face on Cloud, and that’s why I recommend this course for anyone starting to learn about Cloud. This course explains key concepts of clouds like Iaas, PaaS, and Saas with examples from AWS, which makes it easy to learn what Cloud is and what benefits it offer. Knowing AWS is also a plus because it’s one of the most popular Cloud platforms, and your experience with AWS will also boost your CV for any AWS job or for a developer job where AWS skills are desired.
Here is the link to join this course — Introduction to Cloud Computing with Amazon Web Services
Whether you want to learn Cloud Computing basics or want to prepare for the Azure Fundamentals AZ-900 exam, this course is perfect to start your Cloud computing journey.
This course is offered by Microsoft itself and it's perfect to launch your career in cloud computing and prepare for the Azure Fundamental exam.
This Microsoft certification course consists of four courses that will act as a bedrock of fundamental knowledge to prepare you for the AZ-900 certification exam and for a career in the cloud.
This program will provide foundational level knowledge on Microsoft Azure concepts; core Microsoft Azure services; core solutions and management tools; general security and network security; governance, privacy, and compliance features; Microsoft Azure cost management, and service level agreements.
Overall, an ideal cloud computing and Azure course for IT personnel just beginning to work with Microsoft Azure or anyone wanting to learn about it.
Here is the link to join this program — Microsoft Azure Fundamentals AZ-900 Exam Prep Specialization
Some of you might know that I also have Pluralsight membership and it's one of my favorite places for online learning.
So, when I started learning about Cloud Computing in general and AWS in particular, I looked into Pluralsight for some learning material and get hooked to this course. This course provides a good overview of cloud platforms, including Amazon Web Services and Microsoft Azure, as well as private clouds (bringing cloud technology on-premises). By the time you’re done, you’ll know what cloud computing is all about and be ready to start exploring specific implementations.
Here is the link to join this course — Cloud Computing: The Big Picture By David Chappell
So, if you have a Pluralsight membership, this course is for you. If you don’t have a membership, it’s worth getting as you get access to almost 6000+ top-quality courses for just $29/month or $299/year. If you want to try, you can also get access to this course for free by taking their 10-day free trial.
This is another beginner-level Udemy course on Cloud Computing, which I often recommend to my readers.
This course will provide you with a fundamental understanding of what cloud computing is and explains the essential characteristics of cloud computing. It also explains 3 main services models like IaaS (Infrastructure as Service), SaaS (Software as Service), and PaaS (Platform as Service), along with 4 cloud deployment models like private, public, hybrid, and community model. On top of that, you will learn the key benefits of public cloud services like AWS, GCP, and Microsoft Azure. Overall an excellent Cloud computing beginner course for anyone who is just starting off.
Here is the link to join this course — Getting Started with Cloud Computing — Level 1
This is one of the best online courses to learn about Cloud Computing on Coursera. I strongly recommend this course to anyone who wants to learn the basics of cloud computing. The course material is excellent, and instructor Indranil Gupta is phenomenal. He is exceptionally thorough, and his delivery is excellent as well, which makes learning a joyful experience. On top of this, exams are marvelous and help you to retain the knowledge you have learned. This course is also part of Coursera’s Cloud Computing Specialization, which is nothing but a collection of some individual sessions to learn different areas of Cloud computing.
Here is the link to join this course — Cloud Computing Concepts by Coursera
This course is also part of the 100% online Master in Computer Science degree from the University of Illinois at Urbana-Champaign. If you have joined the full program, your course will also count towards your degree learning as well.
For online learners, this specialization provides a self-paced learning option, practice quizzes, graded assignments with peer feedback, graded quizzes, and sharable certificates, which you can put on your LinkedIn profile.
To be honest with you, this is the course that teaches you most about cloud and cloud computing to me.
I learned most when I started preparing for the AWS Certified Solution Architect exam; unfortunately, I couldn’t carry on and need to drop in the middle as I didn’t have any time to progress further with exams, but taking this course was the best decision for me. The instructor Ryan Kroonenburg and Faye Ellis are some of the well-known Cloud experts and run a site with the name CloudGuru which is quite right given their extensive experience and in-depth knowledge of AWS Cloud. They also hold most of the AWS cloud certification you can think of.
Here is the link to join this course — AWS Certified Solutions Architect — Associate
This is the best course to pass the AWS Solutions Architect — Associate Exam, but it also teaches you a lot of small details about cloud computing with AWS. You learn how the services you use on a daily basis on the cloud works like S3, IAM, etc. Talking about the social proof, this course has, on average, 4.5 ratings from 132,775 ratings and 417,354 students enrolled, which is massive for any stretch of the imagination. No doubt that this course is the best-seller on Udemy, and most of the AWS-certified people I have met have taken this course.
If you are looking for a Coursera course to learn Cloud Computing with Amazon Web Service then this AWS Fundamentals Specialisation, offered by AWS itself is a great program to start with.
This Coursera certification program provides a nice overview of the features, benefits, and capabilities of Amazon Web Services (AWS).
This Coursera AWS Specizliation contains four online courses to give you a detailed understanding of core AWS services, key AWS security concepts, strategies for migrating from on-premises to AWS, and the basics of building serverless applications with AWS.
Additionally, you will have opportunities to practice whatever you have learned by completing labs and exercises developed by AWS technical instructors. This makes it the best AWS course for beginners on Coursera, and I highly recommend it to people who want to learn Cloud Computing with AWS.
Here is the link to join this course — AWS Fundamentals Specialization
By the way, if you find Coursera courses and specialization useful then you should also join the Coursera Plus, a subscription plan from Coursera which provides you unlimited access to their most popular courses, specialization, professional certificate, and guided projects. It cost around $399/year but its complete worth your money as you get unlimited certificates
coursera.com
That’s all about some of the best online courses to learn Cloud Computing. There is no better time to learn Cloud computing than today, and I recommend every programmer to learn and get familiar with Cloud computing because no matter what you are doing, your job will be affected by clouds and if you don’t pay attention now, you will be left behind in future. Just knowing about public cloud platforms like AWS, GCP, and Microsoft Azure will also help you to get a job quickly as more and companies are looking for professionals who can operate on these public clouds. In other words, Cloud Computing is a handy skill for IT professionals, and you should pay attention to it.   Other IT and Cloud Certification Articles you may like:
Thanks for reading this article so far. If you like these Cloud Computing online courses, then please share them with your friends and colleagues. If you have any questions or feedback, then please drop a note.
P. S. — If you are new to the world of Cloud and AWS and looking for some free courses to learn Amazon Web Service, then you can also check this list of Free AWS Courses for Beginners.
medium.com
Medium’s largest Java publication, followed by 14630+ programmers. Follow to join our community.
174 
3
174 claps
174 
3
A humble place to learn Java and Programming better.
Written by
I am Java programmer, blogger, working on Java, J2EE, UNIX, FIX Protocol. I share Java tips on http://javarevisited.blogspot.com and http://java67.com
A humble place to learn Java and Programming better.
"
https://medium.com/@the_economist/ibm-lags-in-cloud-computing-and-ai-can-techs-great-survivor-recover-c81639d639c8?source=search_post---------29,"Sign in
There are currently no responses for this story.
Be the first to respond.
The Economist
Oct 26, 2017·7 min read
Technology giants are a bit like dinosaurs. Most do not adapt successfully to a new age — a “platform shift” in the lingo. A few make it through two and even three. But only a single company spans them…
"
https://medium.com/coinmonks/dapps-are-just-censorship-resistant-cloud-computing-a44aad777ab7?source=search_post---------30,"There are currently no responses for this story.
Be the first to respond.
Top highlight
You could say that misunderstandings about the nature of dapps have resulted in the misappropriation of about $13 billion dollars between 2017 and 2018. While some of these ICOs may one day lead to something which merits the initial capital investment, it’s hard to argue that this will be the case for many (if any at all). The technology has been improperly leveraged and overcapitalized (unsuccessfully) in an effort to overcome fundamental limitations. Here I explain how the upsides and downsides of dapps compared to contemporary web services to show how to successfully leverage dapps.
The alternative architecture offered gives hope to dapps finding a use-case considering that dapps will have a hard time competing with web services.
Web services use the client-server architecture with centralized computing. The client-server architecture means that the end-user will make use of a remote computer to use the service. This is typically done for a number of reasons, but mainly because it’s scalable. This allows for a single entity to leverage economies of scale and share that computing power / data storage with users for free (usually in exchange for their data or to show them ads). Adding new users is easy because all that’s required to access the service is a lightweight, internet-connected computing device, and most people already own personal computers and cell phones. You could say this architecture exchanges security from hostile adversaries for better efficiency. Typically security is largely outsourced to government anyway (e.g. common law), so these architectures can focus strictly on efficiency.
Blockchains use a peer-to-peer network and each user validates the entire blockchain. Validation typically requires downloading the entire blockchain and checking that every transaction conforms to the protocol to compute the final state. Adding new users is harder because it requires users owning hardware capable of validating the entire blockchain within a reasonable period of time. Owning personal computers is common, but requiring several hundred GB of data and days of syncing is a UX anti-pattern. The highly distributed blockchain data makes the network more resilient to attacks. You could say this architecture exchanges efficiency for better security. Unlike web services, blockchain networks do not outsource any of their security, even to governments. More often, governments are considered an adversary in the threat model.
Cloud services are just server hardware rented out as a product. Basically web hosting. In today’s centralized cloud services, the service provider can revoke access at their discretion. In contrast, any code uploaded to the blockchain must stay there indefinitely. This is because the nature of blockchain validation requires that all data be present, so objectionable or politically sensitive data cannot be excised without preventing the ability to perform full validation. As long as full validation is a priority, data on the blockchain is censorship-resistant.
All full nodes contain the current state as well as the entire blockchain. This also means that they have a copy of each dapp and the contract code. If someone wanted to, they could offer access to blockchain data as a service. In other words, users have two options for accessing dapps:
Blockchain networks are only censorship-resistant because copies of the blockchain are highly distributed. However, that high level of distribution means intensive on-chain computation or high-capacity data storage are prohibited. This is because the hardware requirements must be kept reasonably low for the full node count to stay reasonably high. The exact number of full nodes necessary for censorship-resistance is not well understood, but for the time being any service which wishes to retain this property should be conservative. Once a service is centralized there is no going back. This means:
Today’s censorship-resistant services offer this important feature in one of two ways:
The unspoken reality of smart contract platforms is that they are a consequence of taking a highly distributed data store and adding scalable business logic capabilities. That is, if we embedded user-created APIs (smart contracts), a virtual machine, a formal means to triggering those API calls (transactions), and a way of ordering transactions in a highly distributed data store (a blockchain), we get a smart contract platform.
This is great news. It means that dapps offer something compelling and unique that both web services cannot offer and neither can existing censorship-resistant services.
The biggest feature difference offered by dapps is that they lack human and machine points of failure. The big question really comes down to how scalable they can be made. With the client-server model, this is a real possibility.
Dapps as censorship-resistant cloud-computing is more-or-less the same as its conception as an unstoppable (read: censorship-resistant) world computer (read: cloud computer) back in 2014. So if they had it right the first time, what happened?
Gambling, adult classifieds, and anonymous markets are the three most profitable categories which require some degree of censorship resistance. Only in the first case was there any attempt made at pursuing products which could offer something on the Ethereum platform. It would be fair to say that the Ethereum team was composed of technologists without any clear sense of product-market fit.
Instead, the Ethereum community targeted inappropriate applications which rarely needed censorship-resistance at all. It seems that their primary target was services which used some trusted intermediary, and the reality is that the additional rent imposed by the trusted intermediary was nowhere near the additional costs that would be incurred by a decentralized architecture. The largest markups products usually see is something like 50x (e.g. pharmaceuticals), but the inefficiency of dapps (at least on Ethereum) is orders of magnitude larger on most (all?) metrics.
It was pretty clear from the start that blockchains wouldn’t offer anything cost-effective relative to conventional cloud computing platforms, such as Amazon web services. Given that they were targeting the wrong (or at best, suboptimal) markets, they really should have paid more attention to how cost-effective their solution was.
Ethereum was not a solution which started with a real-world problem. It was a generalization of a technology that itself was solving a very specialized problem (trust-minimized wealth transfer). Given this, they never optimized the architecture to solve any problem in particular, and were caught off guard when they found that Ethereum was too inefficient and costly for every problem domain they targeted.
Dapps are just code on an immutable cloud (the blockchain). The highly distributed blockchain data makes this code censorship-resistant. Dapps are absent both hardware and human points of failure which is a unique feature set that does not exist in other censorship-resistant technologies. Anything which doesn’t require this censorship-resistance may be better off using a completely centralized alternative. Ethereum was right when they called it an unstoppable world computer, but they failed to properly analyze the costs or target the correct markets.
Get Best Software Deals Directly In Your Inbox
Coinmonks is a non-profit Crypto educational publication.
221 
4
A newsletter that brings you day's best crypto news, Technical analysis, Discount Offers, and MEMEs directly in your inbox, by CoinCodeCap.com Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
221 claps
221 
4
Written by
http://thedevilscompiler.com/
Coinmonks is a non-profit Crypto educational publication. Follow us on Twitter @coinmonks Our other project - https://coincodecap.com
Written by
http://thedevilscompiler.com/
Coinmonks is a non-profit Crypto educational publication. Follow us on Twitter @coinmonks Our other project - https://coincodecap.com
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@aditi.chaudhry92/what-is-cloud-computing-59d0d5570332?source=search_post---------31,"Sign in
There are currently no responses for this story.
Be the first to respond.
Aditi Chaudhry
Jul 1, 2017·4 min read
Cloud Computing is still a very popular buzzword in the tech industry. But what does it actually mean? How did it come about? Here’s a quick 5 minutes high-level overview of cloud.
The idea of Cloud Computing started in 1996 by Sean O’Sullivan and George Favaloro. These two men had a vision that business software would not just move to the web, but consumer file storage on the Internet would become much more common. The idea didn’t really catch on until the mid-2000’s, but to understand why it didn’t catch on, we have to go back to the 80's.
In the 80’s, you could have thousands of visitors in a day and your app would be considered to be extremely successful. Now, there are millions of people across the world on the internet who can access your app in one part of the day. You can’t really plan for that, so scalability has become an issue. One server can’t handle millions of people, but having hundreds of servers during one part of the day is not an ideal solution.
This leads us to our current definition of cloud computing. Cloud computing is essentially a large number of computers connected through the Internet. It’s not just the computers storing info on their hard-drive. It’s computers storing and accessing data through the Internet. Essentially, it’s an extra layer of abstraction.
Why and how is this useful to our lives? Let’s walk through an example. Imagine if Bob was launching an ecommerce application. In the old days, he would have to buy the right hardware and software for his business and every time he hires more people, or the application becomes more popular, he would have to buy more resources. Bob would be spending a lot of money. But now, using cloud computing, Bob’s employees can log into web-based services that host all the programs that they need. Bob will save money because he will only pay for what he needs. He can start one server and then overnight become famous and scale up to a whole datacenter. When it fails, he can scale back to one server. And that’s why cloud computing is so cool!
But let’s take a second to talk about the key characteristics of cloud computing. The major characteristic is that cloud computing is on-demand. A consumer can unilaterally provision computing capabilities such as server time and network storage. They can do so without any human interaction with the service provider.
The next characteristic is that cloud computing provides broad network access. Capabilities are available over the network and accessed through standard mechanisms that promote use by thin or thick client platforms. A thin or thick client can be a mobile phone, tablet, laptop, workstation, etc.
The third characteristic is resource pooling. This characteristic, in my opinion, is super awesome. The provider has huge clusters that have a lot of resources pooled together. The user however, is given a small slice of that resource pool. The user sees nothing except for the resources they are given and the provider sees thousands of users, utilizing their resource pool.
The next characteristic is rapid elasticity which means that resources can be elastically provisioned and released. In the eyes of the user, resources that are available for provisioning often appear to be unlimited and can be appropriated in any quantity at any time.
The last characteristic, measured service is probably the most beneficial for users on a budget. Finally, computing power is available the same way electricity is. Very simply, you pay for what you use. Resource usage can be monitored, controlled and reported. This provides transparency for the provider and the user.
At this point, you may be thinking, this is all useful information, but so what? Why is cloud computing so useful for applications. Here is the TL;DR version of the benefits of cloud computing:
1. It lowers the cost of access to computing power
2. It provides virtual access to many different computers and operating systems
3. It makes it easier to build customer trust on established cloud systems
4. It makes it faster for companies to develop applications
5. It allows business to scale quickly and be ‘elastic’
6. It allows small companies to compete in a bigger market than they would have otherwise
7. The quick development allows quick pilots of solutions without long term capital cost commitments
I hope this quick overview was useful in establishing the benefits of cloud computing. There are many cloud computing providers so I encourage you to pick one and play with the services it offers to learn more about the power of cloud computing! Here are some additional resources:
· https://aws.amazon.com/what-is-cloud-computing/
· https://www.ibm.com/cloud-computing/learn-more/what-is-cloud-computing/
· https://azure.microsoft.com/en-us/overview/what-is-cloud-computing/
· http://www.pcmag.com/article2/0,2817,2372163,00.asp
I majored in Computer Engineering at UVA (Wahoowa!) and now work as a CyberSecurity Engineer! Follow me @aditichaudhry92
70 
1
70 
70 
1
I majored in Computer Engineering at UVA (Wahoowa!) and now work as a CyberSecurity Engineer! Follow me @aditichaudhry92
"
https://medium.com/occam-finance/occamrazer-to-host-ido-of-decentralised-cloud-computing-provider-iagon-d4afe874b08d?source=search_post---------32,"There are currently no responses for this story.
Be the first to respond.
ZUG, 24 MAY 2021 — The Occam Association is pleased to announce that the OccamRazer platform will be hosting the IAGON IDO. A decentralised cloud computing platform, IAGON aims to leverage unused processing power to build a distributed network of nodes, forming a decentralised supercomputer.
By leveraging unused storage capacity in data centers, computers, and smart devices, IAGON aims to create a massive distributed data network that provides an unmatched level of security. In order to achieve this level of security, IAGON uses its own encryption technology, based on SHA256 encryption and blockchain technology, in combination with its proprietary and patented “Secure Lake” technology.
Combined, these technologies ensure that even when information systems are breached, data and files cannot be accessed, deleted, or otherwise modified — not even by IAGON itself. Through their innovative solutions, IAGON will become the new standard in handling sensitive information.
As one of the most decentralised and academically proven blockchain protocols developed to date, IAGON is implementing Cardano’s components and features for future integrations.
Navjit Dhaliwal, CEO at IAGON, says:
“We are delighted that we can kick start our relationship with the Cardano ecosystem through Occam.fi. Our decentralised cloud computing infrastructure, long in development, has been a labour of love by the IAGON team. We have poured our time and attention into making IAGON a market-leading and fully transparent cloud computing platform; eschewing quick gains for steady and tangible progress. As a result, we see many parallels with the Cardano ecosystem and its community.”
IAGON has also received grant approval from Innovation Norway, a government-funded innovation program tailored for the nation’s most promising tech start-ups.
Mark Berger, President of the Occam Association, says:
“Having satisfied OccamRazer’s stringent due diligence requirements and requirement to build for the good of the Cardano ecosystem, the Occam Association is delighted to welcome IAGON to our platform. IAGON’s vision for truly decentralised cloud computing infrastructure, and their relentless pursuit of developing real solutions for our interconnected world, is closely aligned with the wider vision and mission of the Cardano ecosystem.”
Like many, IAGON initially launched in the midst of a changeable blockchain market, through the Dragonchain Incubator in March 2018. Following difficulties with Dragonchain’s platform, the IAGON team went into full-scale development and deployment mode, working diligently to deliver on their vision for truly decentralised cloud computing infrastructure.
To reiterate its commitment to full transparency, the IAGON team recently released a detailed Medium article, explaining its new positioning in the market and its commitment to protecting its community.
Having satisfied Occam.fi’s stringent due diligence requirements, and evidenced a clear ready-for-market use case, the Occam Association is delighted to welcome their IDO to the OccamRazer platform.
Follow IAGON’s social media channels to stay up to date with the latest news:
Follow Occam.fi’s social media channels to stay up to date with the latest news:
About Occam.fi: The Occam.fi ecosystem is managed and maintained by the Occam Association, a Switzerland-based entity operating from the crypto-friendly Canton of Zug. When the Occam.fi ecosystem grows sufficiently, Occam.fi will be governed and steered by a carefully designed decentralized autonomous organization. Occam.fi is the first decentralized launchpad designed for the Cardano ecosystem, built with expertise and care from a veteran team of blockchain entrepreneurs and professionals. Learn more at www.occam.fi.
About IAGON.com: IAGON is a Norway-based blockchain company that aims to revolutionize the cloud and web services market by offering a decentralized grid of storage and processing power to connect users that demand storage capacities for Big Data and large processing capabilities for Artificial Intelligence computations with unique patented technology. Learn more at www.iagon.com.
All-in-one DeFi layer that will jump start @Cardano ‘s #DeFi ecosystem.
698 
3
698 claps
698 
3
The Occam.fi ecosystem encompasses the OccamX DEX, the OccamRazer decentralized launchpad, and the Occelerator Incubator. Backed by IOHK through cFund and EMURGO, Occam.fi brings DeFi to Cardano.
Written by
Official PR account of www.occam.fi, Twitter: https://twitter.com/OccamFi
The Occam.fi ecosystem encompasses the OccamX DEX, the OccamRazer decentralized launchpad, and the Occelerator Incubator. Backed by IOHK through cFund and EMURGO, Occam.fi brings DeFi to Cardano.
"
https://towardsaws.com/a-comprehensive-guide-to-cloud-computing-with-aws-9904fd3d99fc?source=search_post---------33,"Sign in
Kisan Tamang
Nov 28, 2020·4 min read
According to Research and market, the cloud computing industry is to grow from $371.4B in 2020 to $832.1B by 2025 at the rate of 17.5%. The growth is unstoppable and the demand for certified professionals has increased more than ever.
The cloud computing services cover a vast range of options — from computing power, databases, networking to storage — typically over the internet…
"
https://medium.com/design-ibm/ibm-cloud-wins-a-2019-stratus-cloud-award-for-cloud-computing-79e58bd9c1af?source=search_post---------34,"There are currently no responses for this story.
Be the first to respond.
Our team at IBM Cloud is excited to announce that we’ve recently been awarded a 2019 Stratus Award for Cloud Computing for the IBM Cloud Platform Unification Project in the Hybrid Cloud Provider category. We are thrilled for our team to be recognized in these awards, for our diligent efforts in making our cloud products more consumable to our users.
“The 33 winners we recognize today are changing the way we all communicate, conduct business and interact with the world. We are honored and proud to reward these leaders in business.” — Maria Jimenez, Chief Nominations Officer of Business Intelligence Group
The IBM Cloud Platform Unification Project (PUP) was an effort across a cross functional team of designers, developers, and product offering managers to unify all of IBM’s cloud offerings into one platform, with a cohesive and easy-to-use user experience. IBM Cloud users enjoyed the hundreds of products and services that they had access to, however they reported that it was difficult having to access them through different portals. IBM PUP created a central platform for users that made their applications more accessible and easier to navigate, and allowed users to search for and find all the different cloud offerings under one URL.
The platform interface offers a sleek UI that engages users and helps them accomplish tasks faster. The IBM Cloud UI was also updated to include bold graphics and data visualizations, as well as an accessible voice and tone of the UI content, down to even the server error pages. These product updates were also designed with a cohesive global application of design language and principles, so that the user experience extended from the IBM Cloud platform landing page to the actual products themselves, helping to give users a sense of familiarity while they were using the platform.
The target users of this project were IBM Cloud clients who were constantly navigating between different applications and needed an easier way to view and find all of their products. The IBM Cloud products were all designed to be user-centered and intuitive, however users had to adjust to a new environment as they worked within each product. Users also needed to access customer and technical support, manage their billing, and discover new products in an easier way. The design team behind the IBM Cloud Platform Unification Project wanted to give users a seamless and focused path to accomplish all of these things.
Following the principles of IBM Enterprise Design Thinking framework (Observe > Reflect > Make), the team created a platform that offered a more cohesive user experience across the IBM Cloud portfolio. Users could go to one location where they could find everything related to IBM Cloud products, from discovering new applications to troubleshooting their software. The team also emphasized on visual as well as content design focus, to give IBM Cloud a stronger brand and a user community. To ensure that the new designs met users needs and assisted in the task completion of their jobs to be done, the design team spent a lot of time and effort in performing user research to be able to optimize the user flows and journey. The resulting impact is a completely new way for users to experience IBM Cloud. They can now easily manage their IT systems, access customer and technical support, manage their billing, find and order new products and more — all from a single holistic experience.
The Business Intelligence Group launched its crowd-sourced industry awards programs to recognize authentic talent in the business world. The organization’s scoring system selectively measures performance across multiple business domains and rewards companies with outstanding achievements throughout various areas of businesses and technology.
Congrats to the IBM Cloud Platform team for all their hard work in making this a great product for our users.
Arin Bhowmick (@arinbhowmick) is Vice President, Design at IBM based in San Francisco, California. The above article is personal and does not necessarily represent IBM’s positions, strategies or opinions.
Stories from the practice of design at IBM
219 
1
219 claps
219 
1
Written by
Global VP Design | Chief Design Officer, @IBM |Cloud, Data and AI I UX Leadership| UX Strategy| Usability & User Research| Product Design
Stories from the practice of design at IBM
Written by
Global VP Design | Chief Design Officer, @IBM |Cloud, Data and AI I UX Leadership| UX Strategy| Usability & User Research| Product Design
Stories from the practice of design at IBM
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/azure-na-pratica/aprendendo-cloud-computing-na-faixa-artigos-v%C3%ADdeos-canais-comunidades-222bc1596d55?source=search_post---------35,"There are currently no responses for this story.
Be the first to respond.
Neste post estou agrupando referências gratuitas (artigos, projetos de exemplo, vídeos, links para canais e comunidades…) com foco em Cloud Computing, sobretudo de serviços e soluções voltadas ao Microsoft Azure (parte importante de minha atuação profissional hoje). Meu objetivo sempre que possível será manter este guia de referência atualizado com novos conteúdos que eu mesmo vier a produzir, bem como refereciando também outras iniciativas gratuitas.
Neste ano de 2021 mais uma vez o Canal .NET promoverá o Azure Tech Nights, um evento completamente ONLINE e GRATUITO com sessões durante Fevereiro/2021 e Março/2021 para levar mais conhecimento sobre nuvem e tecnologias Microsoft a todos aqueles interessados em aprender mais.
As seguintes apresentações já estão agendadas (sempre às 21:00 - horário de Brasília), basta clicar em cada um dos links para efetuar sua inscrição:
O canal Azure na Prática também já conta com um evento ONLINE e GRATUITO programado para este mês de Fevereiro (também às 21:00 - horário de Brasília):
Com foco em cloud computing e nuvem Microsoft, a comunidade Azure na Prática vem promovendo minicursos online gratuitos com temas relacionados a DevOps e que já foram acompanhados por milhares de pessoas de todo o Brasil e até mesmo fora dele (Portugal, Angola, Itália, Reino Unido, EUA, Argentina, Uruguai, Irlanda, …).
A seguir estão links contendo as gravações, materiais de apoio e informações gerais sobre todos os minicursos já realizados:
Azure na Prática Gratuito #1 — Desenvolvimento Web: saiba como foi + conteúdos gratuitos
Azure na Prática Gratuito #2 — Docker: saiba como foi + conteúdos gratuitos
Azure na Prática Gratuito #3 — Azure DevOps: saiba como foi + conteúdos gratuitos
Azure na Prática Gratuito #4 — Azure Functions: saiba como foi + conteúdos gratuitos
Azure na Prática Gratuito #5 — Infraestrutura na nuvem: saiba como foi + conteúdos gratuitos
Azure na Prática Gratuito #6 — Kubernetes na Nuvem: saiba como foi + conteúdos gratuitos
Azure na Prática Gratuito #7 — Banco de Dados no Azure: saiba como foi + conteúdos gratuitos
MasterClass GitHub Actions: saiba como foi, participe gratuitamente e receba um certificado
Os links a seguir reunem artigos, vídeos e projetos de exemplo cobrindo diversas tecnologias:
Os slides a seguir trazem ainda uma visão geral de diversos serviços oferecidos pelo Microsoft Azure:
Para assistir no YouTube clique aqui.
Tecnologias abordadas: Azurite, Azure Storage, Azure SQL, Logic Apps, Slack, Docker, Azure Functions, Linux, Visual Studio Code
Para assistir no YouTube clique aqui.
Tecnologias abordadas: GitHub Actions, PowerShell, Azure Functions, Azure Storage, Azure DevOps, Microsoft Teams, Visual Studio Code
Para assistir no YouTube clique aqui.
Para ficar por dentro de lives e eventos gratuitos siga as seguintes comunidades no Meetup:
Devshow Podcast
E para concluir deixo aqui links de alguns vídeos recentes de eventos em que fui organizador ou palestrante e nos quais se abordaram temas voltados à área de Cloud Computing…
Blog do Azure na Prática
133 
133 claps
133 
Blog do Azure na Prática
Written by
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
Blog do Azure na Prática
"
https://medium.com/capital-one-tech/cloud-computing-one-experts-view-on-what-to-expect-in-2019-cf1b30191a34?source=search_post---------36,"There are currently no responses for this story.
Be the first to respond.
Cloud computing has come a long, long way. I first began working with cloud computing in 2006, when Amazon Web Services launched what it called Infrastructure-as-a-Service; the phrase cloud computing hadn’t even been coined then.
From my first exposure to AWS, I recognized that cloud computing would become the next major IT platform, with profound implications for the entire technology industry, vendors and users alike.
The manifest benefits of cloud computing — immediate resource access, easy scalability, usage-based pricing — made traditional IT practices obsolete overnight.
However, that didn’t mean adoption occurred overnight. Far from it. Skepticism borne of doubt regarding cloud security, reliability, and cost advantages hindered adoption. And that went on for years.
But it’s all changed now. Cloud computing has not only reached but passed a tipping point; it’s clearly on the path to platform dominance I envisioned twelve years ago. Which means we’re getting to the interesting stage of cloud computing — what happens when cloud computing is the default choice for application deployment.
Next year promises to be a banner year for cloud computing. You can expect to see significant developments in the cloud ecosystem; I’ve captured what you’ll see as the biggest topics in 2019.
You’re probably saying, well, duh. If cloud computing has passed its tipping point, if it’s now the default deployment choice, of course enterprise IT will make it a strategic priority.
True.
But there’s a whole list of interesting implications associated with that. CIOs will direct their staffs to deliver cloud-based yearly goals. Procurement staffs will shift budget from traditional legacy vendors to cloud alternatives. Architecture groups will develop new cloud-based designs that incorporate both core infrastructure and higher-level managed services. Corporate training will develop new curriculum to build workforce skills. Legal groups will learn the ins and outs of CSP contracts. What’s more, enterprise IT will require focus and attention at all levels from the Board of Directors down to the developers, with more companies likely to list “cloud computing” as a material risk in their 10-Ks, the annual filing submitted by publicly traded companies to the Securities and Exchange Commission.
It’s not overstating it to say that 2019 will witness a shakeup in enterprise IT business-as-usual as most shops digest what it means to make a wholesale platform shift to the cloud.
Well, if cloud is now the default deployment platform, and enterprise procurement groups are shifting budget to cloud providers, what will that mean?
A lot of money. Flowing into the coffers of cloud providers. And in the US, that really means into the coffers of three companies: Amazon, Microsoft, and Google, which I dub AMG.
The three providers have had scorching numbers for the past few years, and 2019 will be no exception. AWS’s growth rate, which I regard as the most important financial number in the tech industry will remain above 40% throughout the year.
If you don’t pay attention closely, what this means is easy to overlook. AWS is likely to exit 2019 at a $35 billion run rate, growing at 40%+ per year. And Microsoft and Google, albeit on smaller numbers, will be growing at even faster rates. It’s no exaggeration to say that AMG are poised to become the dominant presence in the tech vendor space. The implications of this fact will play out over the next few years, but I don’t think it takes a genius to connect the dots. Additionally, AMG are expanding into areas previously unassociated with cloud computing by challenging other tech companies beyond the traditional “platform” companies (e.g., mainframe, server, database).
SDLC (Software Development Lifecycle) is a venerable term in IT and, in my experience, one that has fallen out of favor, replaced by CI/CD or DevOps. I was somewhat surprised, then, when I heard it used in a couple of different advanced development tools conferences over the past month.
I asked the organizer of one of these conferences (NewOps) why the term surfaced at his event. His explanation: too much of the recent past has focused on developers, without looking at longer-term application operability.
I welcome the comeback of SDLC, as I think it captures the totality of what it takes to bring a software release from inspiration to production, with an emphasis on the entire process rather than one stage of it.
You can expect to see SDLC emerge as a big issue in 2019. IT shops will complete their initial move to the cloud and then confront an unpalatable truth: cloud computing doesn’t mean fast SDLC. In fact, using cloud highlights inefficiencies in the application development value chain and acts as a forcing function to improve each stage along with inter-stage handoffs.
Part of using cloud computing and streamlining the SDLC is restructuring application architectures, both in terms of execution environments and architecture.
In terms of execution environments, containers provide benefits over traditional virtual machines — faster launch times, easier updates, and greater portability.
Commonly going hand-in-hand with the move to containers is adopting microservice architectures — functionally decomposing monolithic applications from single large binaries to small, single function components that communicate via APIs. Microservice applications allow easier scalability and facilitate more rapid application evolution.
The only thing is, coordinating all those containers, making sure they’re installed properly, have the correct security implemented, can communicate across the network, is pretty hard.
Which leads us to Kubernetes. Kubernetes enables declarative descriptions of applications, including access controls, automatic scaling, and network connectivity.
I’ve heard many describe Kubernetes as making container-based applications easy to deploy and manage. Enthusiasm for Kubernetes grew to epidemic-like proportions over the past couple of years.
Next year will bring home a truth: While Kubernetes-based applications may be easy to run, Kubernetes itself is no picnic to operate. It is an early stage product which is serving as the foundation of an ecosystem of supplementary products (e.g., Istio) all of which must be installed, configured, and managed correctly to make applications run properly.
In a recent podcast, I heard one participant state that with Kubernetes and Istio, traffic would just magically get routed to the right resource, without noting that someone would have to work really, really hard to make that magic happen.
Next year will be a “back to the future” time as IT organizations (re)learn that managing infrastructure for cloud-native applications is not easy. It’s hard.
However, the overall sense that containers and Kubernetes is a natural foundation for cloud-native applications is not misguided. So expect to see a further enthusiasm for consuming Kubernetes from one of the big cloud providers; they specialize in running complex infrastructure at scale, and most IT organizations will conclude they should let the specialists take over.
As I hope these predictions make clear, we are in the midst of an IT transformation. Every part of IT is changing as part of a drive to deliver applications better suited to the digital economy. This will require a major upgrade in talent, as organizations seek out the skills needed to build the more complex applications typical of cloud computing.
One approach, of course — and a good one — is to improve the skills of people already on staff. They’re smart, they’re motivated, and they have lots of domain knowledge of a company and its industry. I always advocate education as the right first step.
However, most companies will also have to step into the recruiting fray to hire from the outside. There’s just no way to meet the demand for cloud-native skills solely from existing staff.
And those companies will find this kind of talent is in short supply. Next year, you can expect to see many articles about how hard it is to find cloud-capable candidates, and how expensive they are.
Get used to it. The imbalance of cloud talent supply and demand will be constant over the next few years and will result in a salary increase spiral.
If ever you thought of IT as a cost center to be managed through budget squeezes and infrequent salary increases, banish those thoughts.
Technology adoption commonly follows what is referred to as an “S-curve.” Early on, adoption of a new technology is slow, as people struggle to learn about it and envision how it can be applied. Once a critical mass of proof points and knowledge is hit, adoption accelerates dramatically.
Cloud computing has now hit the steep part of its s-curve. It is now the de facto platform for new applications, and, increasingly, legacy applications are migrating to the cloud.
Next year will see the implications of this shift hit home, as different aspects of IT incorporate cloud computing and change their established practices. There will be much more discussion about the best way to take advantage of the cloud’s immediate resource access, easy scalability, usage-based pricing, and much less about its supposed drawbacks and shortcomings.
Get ready for a very, very interesting year.
DISCLOSURE STATEMENT: These opinions are those of the author. Unless noted otherwise in this post, Capital One is not affiliated with, nor is it endorsed by, any of the companies mentioned. All trademarks and other intellectual property used or displayed are the ownership of their respective owners. This article is © 2018 Capital One.
The low down on our high tech from the engineering experts…
132 
1
132 claps
132 
1
The low down on our high tech from the engineering experts at Capital One. Learn about the solutions, ideas and stories driving our tech transformation.
Written by
Named by Wired.com as one of the ten most influential persons in cloud computing. Learn more at bernardgolden.com
The low down on our high tech from the engineering experts at Capital One. Learn about the solutions, ideas and stories driving our tech transformation.
"
https://medium.com/open-consensus/7-oss-will-eat-all-of-cloud-computing-40fdbf9c1da0?source=search_post---------37,"There are currently no responses for this story.
Be the first to respond.
Dear reader: we are moving off of Medium to coss.media, a new dedicated website where we will continue providing actionable knowledge to help Commercial Open Source Software (COSS) founders build companies. Please visit and subscribe to coss.media for future posts.
Open source software represents the biggest paradigm shift in technology since the rise of the internet and will disrupt major industries like cloud computing and create far larger new ones as a result.
Extending Marc Andreessen’s “software is eating the world” axiom, this fun “fish food chain” analogy by Alexis Richardson visualizes how OSS is eating software (I strongly agree) and subsequently how Cloud is eating OSS (I fairly strongly disagree, after thinking through this more)…
Bassam Tabbara followed up with the above continuation saying “Multi-Cloud will eat Cloud”.
Ultimately, I think an improved way to describe what is happening is by looking at OSS as the fundamental enabling lever for all new technology paradigms, platforms, companies and patterns of the future which actually supersede all of software and cloud computing (including multi-cloud which, IMO, eventually becomes the modus operandi of running all distributed systems on the internet — curiously powered by OSS). 👀
It is abundantly clear, to me, that OSS will disrupt all existing software and tech industries eventually (over a highly heterogeneous transition/bridge from here — a mostly prorpietary world — to there — a vastly OSS world) and form entirely new technology categories as a result/in the midst of major new growth areas across the global industry landscape.
In short, software may be eating the world, but we’ve since mostly past that transition and moved on to open source software (OSS) really primed to eat everything (including “software” which implies closed-source proprietary products, in my mind).
To examine why one specific and exciting new (only ~10 year old) industry is on the cusp of getting seriously shaken up by OSS, let’s dig more into cloud computing…!
In order to appreciate just how fast the world changes in 5 year increments, let’s examine what the top three cloud providers looked like 5 years ago along a few distinct dimensions, what they look like today and what they might look like 5 years from now!
5 years ago, the cloud computing market started to hit full stride, but was nascent in terms of more stable abstractions and platform services:
It is clear that OSS will unlock the future potential of cloud computing in a world that looks very different from the one we are in today.
This has already started to happen… EC2 is a black-box indirect commercialization of Xen (OSS hypervisor) where AWS captured lots of value around the business value of spinning up a compute instance more quickly and ephemerally than ever before, unlocking Jevons paradox. However, we now see Kubernetes basically replacing EC2 as the abstraction for compute (Google’s superior OSS-first strategy I alluded to before.. explicitly chosen to speed the commoditization of the more fundamental proprietary IaaS/PaaS services their competitors have assumed massive network effects around). K8s has now forced a funguble and liquid compute dynamic across ALL the leading clouds — AWS/Azure/GCP. This is just the beginning.
What about storage? Things like Rook eat EBS, S3, EFS… things like Kafka eat Kinesis. Thinks like Rook+Vitess eat RDS.
What about networking? Things like Envoy/Istio eat AWS ALB, ELB, NLB, API Gateway, etc.
OSS eventually eats all of cloud computing. This happens faster than cloud computing can transition another $2–300B from on-prem IT (5–10 years).
Does OSS eating cloud computing mean the clouds are commoditized to zero? No, but it speeds that effect up dramatically. Google is doing this intentionally as a strategy to leapfrog ahead to #1 above AWS and it is smart. Interestingly, though, an OSS-first cloud *aaS service development strategy offers FAR faster innovation cycles than clouds themselves can build on internally even with millions of customers. That is because OSS has billions of customers.
Next, we’ll dig into why, after or a bit later than OSS eating cloud (probably 7-10+ years out), OSS will then eat all of SaaS.
Data-Driven Perspectives on COSS (Commercial Open Source…
156 
156 claps
156 
Written by

Data-Driven Perspectives on COSS (Commercial Open Source Software)
Written by

Data-Driven Perspectives on COSS (Commercial Open Source Software)
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/bz-notes/edge-computing-the-end-of-cloud-computing-as-we-know-it-d7809ed9fd18?source=search_post---------38,"There are currently no responses for this story.
Be the first to respond.
Just watched a great overview of Edge Computing by Peter Levine of A16Z. He is absolutely right, this is the future of computing. And all of us investing in deeptech today can attest that this massive shift in computing paradigm is happening as we speak.
https://www.youtube.com/watch?v=l9tOd6fHR-U&feature=share
Many of our portfolio companies at Lux are both enabling edge computing, and also benefitting from it. And it happening across all sorts of industries, from traditional IT and mobile computing, to consumer tech, commercial & industrial verticals, and even government/military.
These are some of the key elements of an edge-cloud computing system.
This shift to edge computing is easily observed in emerging platforms like autonomous car, commercial and consumer drones, physical security and anti-terrorism systems, home automation systems, responsive consumer electronics, and/or AR/VR systems. It is starting to emerge also in medical devices and wearables, in order to provide more real-time analysis, inference, and support in critical decision-making.
As Peter points out, this field presents new challenges & opportunities for technologists and entrepreneurs. From massive networking challenges to network and data security at such distributed scale, development of low power GPUs/FPGAs, power management systems and advanced batteries…to name a few.
I am excited for all our portfolio companies hat are at the cutting edge of this. From Hangar and AIRMAP to HappiestBaby, Zoox, Drone Racing League, Survios, AltSpace, Evolv, Cala, AIRA, Isocline, CYPHY Works, Sonic Sensory, LightForm, Latch, Locoroll, Auris and so many others…
I am an early stage VC, partner at Lux Capital.
44 
2
44 claps
44 
2
I am an early stage VC, partner at Lux Capital. I am attracted to startups that solve real, practical problems with technically ambitious solutions. Passionate about tech, entrepreneurship and social good. I like tacos and café lattes.
Written by
Partner at Lux Capital. Investing in entrepreneurs inventing the future. I like tacos and café lattes. bz at luxcapital.com. @bznotes
I am an early stage VC, partner at Lux Capital. I am attracted to startups that solve real, practical problems with technically ambitious solutions. Passionate about tech, entrepreneurship and social good. I like tacos and café lattes.
"
https://medium.com/javarevisited/10-free-courses-to-learn-cloud-computing-for-beginners-4f3cd984ddb1?source=search_post---------39,"There are currently no responses for this story.
Be the first to respond.
Hello there, if you want to learn Cloud computing in 2021 (which is really great!!) and looking for the best free resources like online courses to kick start your Cloud Computing journey then you have come to the right place. Earlier, I have shared free courses to learn AWS, Google Cloud, and Microsoft Azure, and today, I am going to share free Cloud Computing courses for beginners.
Cloud Computing provides access to computing resources like CPU, storage, networking, security, analytics, and other software platforms over the internet and allows anyone to deploy applications to take advantage of these services on an on-demand basis at a much lower cost due to economies of scale.
This easier access to computing resources is powering innovation and more and more companies are moving to Cloud. Ultimately, everyone wants to focus on their business leaving infrastructure and computing to someone like Google, Microsoft, and Amazon to manage, who can certainly do a better job.
Cloud Computing is an essential skill for the modern developer as most of the new development is happening on Cloud and will happen in the cloud.
If you are not familiar with how the cloud works and with essential Cloud concepts and terminology like IaaS, PaaS, SaaS, compute, storage, network, etc then you are at risk of left behind.
It’s imperative now for every IT professional to learn Cloud computing, its more important for full-stack developers, system administrator, DevOps engineer and other people who work closely with Infrastructure and code but equally important for a project manager, solution architect, business analytics and salespeople, and that’s where these free cloud computing courses will help you.
If you are still now sure what is Cloud computing then just think of it as Amazon or Microsoft’s Computer where you run your application. It’s the provider’s data center that you rent for running your services.
In more technical term, Cloud computing is the combination of the software and hardware offered by companies such as Amazon web service ( AWS), Google Cloud, and Microsoft Azure to host the files and data for a company that wants to deploy their solutions or online services and can use that data anywhere anytime.  Cloud computing engineers are highly in-demand and you literally learn these skills of running cloud computing managing them, securing your data, and much more only by taking some online courses on platforms such as Udemy, Coursera, Pluralsight, edX, and more. This article has come up with the best free courses to learn cloud computing in 2021 so stay tuned.
Btw, if you are a Java developer who wants to learn more about Distributed Systems and Cloud Computing from a Java developer perspective, then I also recommend you to join Distributed Systems & Cloud Computing with Java course by Michael Pogrebinsky on Udemy. It’s not free but you can get it for just $9.9 on Udemy sale.
udemy.com
Without wasting any more of your time, here is a list of free online courses to learn Cloud computing in 2021. The list includes courses to learn Cloud Computing with AWS, GCP, and Microsoft Azure, three of the most popular cloud platform.
These free courses are picked from Udemy, Coursera, and Pluralsight. They are created by experts and made free for educational and marketing purposes, while they are not very comprehensive, they are very good for beginners to start learning essential Cloud Computing concepts and how cloud computing works in general.
Let’s start this list with the amazon AWS course which is considered the most popular cloud service provider used among big companies such as Netflix, Linkedin, and Twitch. This free course has almost 3 hours of video content and more than 5k enrollment.   The course starts with an introduction to AWS services and then moves to the practical lessons where you will learn how to create an EC2 instance used to host your web application. Then you will learn about the S3 cloud storage where you put your files and data in the cloud.
You will also learn managing users and groups, configuring the cloud infrastructure, using amazon SES to send emails, and more.
Here is the link to join this course — Cloud Computing With Amazon Web Services
If you just want to learn cloud computing theoretically not practically then you may need to see this IBM introduction course about cloud computing on the Coursera platform. The course has almost 15k student enrollment and it is a part series of courses.   The course starts by understanding the evolution of cloud computing as well as the different types of deployment models on the cloud such as Saas, Iaas, Paas.
Then you will see the cloud computing architecture and virtualization like virtual machines and also the serverless computing and microservices. Finally, some lessons about cloud security, like identity, access management, and cloud encryption.
Here is the link to join this free course — Introduction to Cloud Computing
Another very famous cloud computing service is called Google Cloud Platform is known as well as GCP and big companies such as Spotify and Snapchat. This course is probably the best free one on udemy with 6 hours of video content as well as more than 41k student enrollment which is pretty good.   Starting by understanding the basics of the Google Cloud Platform such as the storage services as well as databases and compute services.
Then you will start exploring the GCP interface like the shell, SDK, console, and more. Next, you will start using the compute service and creating an instance as well as the cloud storage and SQL databases, networking services, and platform management.
Here is the link to join this course for FREE — Google Cloud Platform Concepts
As I mentioned earlier that there are many cloud service that has a good reputation and one of them known as Microsoft Azure. This free and small course is an introduction to how to use this service and get familiar with its services and infrastructure.   The course is really small with just one hour of video content and will introduce you first to this cloud service and what you need to know about Microsoft Azure before start using it.
Then you will understand the data centers and regions as well as some practical lessons such as creating SQL database service and SQL Geo-replication. You will also create windows virtual machine and configuring it.
Here is the link to join this free Azure course — Learning Microsoft Azure
If you really want to learn cloud computing theoretically and learn the cloud technologies that run on clouds such as AWS, GCP, Azure then you need to see this Pluralsight course.
The course is designed to be for beginners and around 2 hours of video content. Starting by understanding some of the fundamentals like the benefits of using cloud computing as well as the risks behind this technology.
Here are the main things you will learn in this course:
You will also learn the deployment model types such as infrastructure as a service Iaas and its components like virtualization and pricing models. Then you will learn the storage in the cloud as well as security and encryption.
Here is the link to join this Cloud course — Fundamentals of Cloud Computing
By the way, you would need a Pluralsight membership to join this course which costs around $29 per month or $299 per year (14% discount). I highly recommend this subscription to all programmers as it provides instant access to more than 7000+ online courses to learn any tech skill. Alternatively, you can also use their 10-day-free-pass to watch this course for FREE.
pluralsight.pxf.io
Btw, all Pluralsight courses are free this month as they are running their free April offer, which means the best time to check out Pluralsight courses.
This is another free Udemy course to learn about Cloud Computing from scratch. Unlike other courses, this one gives you a practical introduction to Cloud Computing. You will learn accurately what is Cloud and what is not.
This course is designed to clear up the many misunderstandings about Cloud Computing and to give you a crystal clear and easy-to-understand explanation of exactly what it is, how it works, the different options available, the advantages provided, and how much it’s going to cost.
This 2.5-hour free cloud computing course is divided into several modules each focusing on key knowledge areas.
here are things you will learn in this course:
Overall a great free course to learn Cloud Computing, suitable for any decision-maker or IT professional who needs an easy-to-understand explanation of Cloud Computing.
Here is the link to join this course — A Practical Introduction to Cloud Computing
This is a short free online course to learn Cloud Computing on AWS on Udemy. Created by Alan Rodrigues, a Software Engineering Evangelist and one of my favorite Udemy instructors, holder of many cloud certifications this is perfect to get an overview of both AWS and Cloud Computing.
This course is designed to give an introduction to students on Amazon web services. Here are the things you will learn in this course
Overall a great free course for IT professionals who want to have an introduction to AWS Cloud and want to learn about some of the services available in AWS
Here is the link to join this free course — Introduction to AWS Cloud Computing
This is another awesome course to learn Cloud Computing basics from a technical point of view and it's completely free on Udemy.
Here are things you will learn in this course :
This course teaches you the technical basics which are required before learning about Cloud Computing. It also gives you an idea of Cloud Computing and the services of cloud computing.
Overall a nice course for both students and professionals who have never worked on Cloud Computing.
Here is the link to join this course for FREE — Cloud Computing: The Technical essentials
This is a free, comprehensive, 10-hour long course to learn Cloud Computing for Beginners from Edureka, a leading online learning platform specialized in online classroom training.
The course is freely available on Youtube. In this Cloud Computing, Full Course will take you deeper into concepts like Cloud Service and deployment models and also cover the fundamentals of AWS, Microsoft Azure, and GCP with Practical implementation. Following pointers will be covered in this
You can watch it on Youtube or right here -
This is another free course to learn Cloud Computing but with the Microsoft Azure Platform. Though, I advise you to first learn Cloud Computing Concepts before going on Azure or AWS.
Once you will get a solid understanding of Cloud or Cloud Computing concepts and start working on AWS, Azure, or Google Cloud you will get the real gist of features like Auto-scaling, Elastic Load Balancers, Regions, and Availability Zones.
In this free course, you will learn about cloud computing basic and core concepts which are very essential before you learn and work on any cloud platform.
Here are things you will learn in this free course:
Overall, an engaging course for anyone who wants to learn about cloud computing concepts.
Here is the link to learn more about this course — Zero to Hero in Cloud computing Essentials With Azure
That’s all about the free Cloud Computing Courses for beginners to learn in 2021. These free online courses are probably the best ones to start your career in the cloud computing industry or just if you are to learn theoretically how these technologies work and their benefits to the nowadays digital world.
After going through one of these courses you will be familiar with essential Cloud concerts and get an understanding of how the cloud works and how to use different cloud services to build and deploy your solution.
The Cloud is here to stay. It’s important for every IT professional to understand cloud computing.
All the best with your Cloud computing journey.
Other Cloud Computing Resources You may like
Thanks for reading this article so far. If you like these free Cloud Computing Courses, then please share them with your friends and colleagues. If you have any questions or feedback, then please drop a note. P.S. — If you are serious about learning Cloud Computing in-depth and don’t mind spending a few bucks then I also suggest you join this Introduction to Cloud Computing with AWS course by Neal Davis on Udemy. It’s one of the comprehensive courses with 8 hours of content to learn Cloud Computing essentials.
udemy.com
Medium’s largest Java publication, followed by 14630+ programmers. Follow to join our community.
226 
226 claps
226 
A humble place to learn Java and Programming better.
Written by
I am Java programmer, blogger, working on Java, J2EE, UNIX, FIX Protocol. I share Java tips on http://javarevisited.blogspot.com and http://java67.com
A humble place to learn Java and Programming better.
"
https://medium.com/@unfoldlabs/8-trends-in-cloud-computing-for-2018-d893be2d8989?source=search_post---------40,"Sign in
There are currently no responses for this story.
Be the first to respond.
UnfoldLabs
Nov 7, 2017·10 min read
Business and IT executives are no longer looking at the Cloud solely as a tool; now the focus has shifted towards finding the right way to use it so they can accomplish their 2018 business goals. The advent of the Cloud has created significant changes to organizations in the past few years. Cloud Computing has provided Big Data with a way to store and retrieve an immense amount of information. It has evolved from personal cloud storage to entire organizations moving all of their data to the cloud.
Although the Cloud has brought so many benefits, larger organizations are still hesitant to transfer their information to the cloud, and that is mainly because of security concerns. But, even with the security concerns, the adoption of Cloud services continues to rise due to the improved usage of cloud-based services including, mobility, increased efficiency, cost-effectiveness, streamlined collaboration, and speed of connectivity.
Here are some data points for us to look into:
Amazon leads ahead of Google and Microsoft in cloud computing. SiliconANGLE reports that the research firm, Wikibon predicts Amazon Web Services (AWS) will reach $43B in revenue by 2022.
Microsoft on the other hand, though arriving late in the cloud computing industry, is still going strong in the game with its enterprise cloud, Azure. It is ready with its ability to compete head-to-head with Amazon.
The advantages of using cloud computing that will influence the market’s growth potential, include:
Cloud Computing empowers developers & companies to innovate faster and cheaper. It’s impressive that you can spin up any server, any operating system and stack in minutes to get rolling. It is far more secure than the traditional on premise systems and it is a better way to run a business.
- Asokan Ashok, CEO — UnfoldLabs Inc
“When making the move to the cloud, it is important for enterprises to understand that while cloud providers are responsible for securing the cloud, only enterprises can secure how they use the cloud. This requires a new security mindset. Simply “lifting and shifting” an on-premise security stack is certain to fail.”
- Jason Guesman, Board Advisor — Lacework
With cloud computing on the rise, it’s only natural that the cloud services and solutions will also grow.
Bain & Company predicts that Software as a Service (SaaS), where software is licensed on a subscription basis and is centrally hosted, will grow at an 18% CAGR by 2020. The likes of Google Apps, Salesforce, and Citrix GoToMeeting will most likely continue to represent the largest cloud market.
According to KPMG, Platform as a Service (PaaS) adoption is predicted to be the fastest-growing sector of cloud platforms — growing from 32% in 2017 to 56% adoption in 2020. PaaS solutions provide a platform that allows customers to develop, launch, and manage apps in a way that is much simpler than having to build and maintain the infrastructure.
According to Statista, Infrastructure-as-a-Service (IaaS), which provides virtualized computing resources over the internet, will have a market that is predicted to reach $17.5B worldwide in 2018. Amazon holds the largest IaaS market share with Amazon Web Services (AWS) and will be competing with other cloud infrastructure services including Microsoft Azure and Google Compute Engine (GCE).
2018 will be the year where Adoption of Enterprise Cloud Services improves; Consumer Cloud Services will skyrocket, Cloud-based File Sharing Services will Increase, Collaboration Services will become more familiar, Social Media Services will get democratized and get the highest adoption.
Cloud to Cloud Connectivity — Some businesses are not particularly fond of being tied to a single cloud provider, which is why multiple cloud providers are opening up APIs on Platforms for connecting multiple solutions. Opening up APIs is necessary to sync cross-functional and multi-disciplinary process and data management as well as integrating and connecting to systems and tools.
Cloud to On-Premise Connectivity — Most enterprises will keep their on-premise solutions and also connect to cloud-based solutions with heavy customization that will best fit their business needs.
Two main reasons for this phenomenon:
Cloud storage is getting cheaper — It’s the economics of supply and demand — the higher the supply and the lower the demand, the price goes down. However, with cloud storage, not only is there a significant supply, but a high demand too; therefore, cloud storage is not just cheap, but offered for free from certain cloud providers so they can gain market share and collect valuable user data.
Crowd Sourced Storage -Instead of using expensive, slow, and sometimes insecure traditional cloud storage, crowdsourced storage will become an option for people who want to keep the cost low, but still, want to take full advantage of the benefits of the cloud.
Sharing your friends or a strangers’ storage will become a common practice and will be used in place of applications like Google Drive or DropBox. Crowd-sourced cloud storage platforms will be used in building and maintaining large scale applications.
Crowd Sourced Data — Major Cloud players like Google and Amazon are giving away Cloud Storage for free to crowdsource data for big data /analytics and artificial intelligence applications.
Cloud Cost Containment — Cost containment is essential for keeping costs down to only necessary expenses to remain within financial targets. The growth of the cloud accelerates as more organizations adopt cloud-based strategies to cut costs. Cloud computing is a long-term cost-cutting IT strategy that reduces infrastructure expenditures and increases ROI by lowering costs while expanding accessibility and productivity.
Cloud Cost Wars — The cloud pricing war between Google and Amazon is based on each organization’s attempt to provide the cheapest service and dominate the cloud market. AWS announced its lower prices early this year, and Google introduced Committed Use Discounts (CUD), or as defined by Google, “the ability to purchase committed use contracts in return for deeply discounted prices for VM usage.”
Security breaches are on the rise. According to the Identity Theft Resource Center, the number of U.S. data breaches tracked through June 30, 2017, hit a half-year record high of 791, a significant jump of 29% from the same period in 2016. The ITRC anticipates that at this rate, there could be a 37% annual increase of breaches in 2017 compared to 2016.
It’s no surprise that security was and continues to be an issue with technology; Cloud solutions are not an exception. The data breach of credit reporting agency, Equifax, resulted in the exposure of millions of sensitive personal including people’s names, Social Security numbers, birth dates, driver’s license numbers, and credit card numbers. Bank breaches are also a common occurrence, mostly due to adopting open banking which leaves the user more vulnerable than ever.
Google’s taken some security measures with their security key fob for logging into devices that includes a 2-step verification process. This key fob involves a code that is sent to your smartphone in addition to your fixed password to ensure the safety of your account.
Gartner expects worldwide information security spending to reach $93B in 2018 compared to $86.4B in 2017, and IDC expects the global revenue for security technology to reach $101.6B in 2020.
Security issues will continue to prevail in 2018, which means we will see more cybersecurity companies coming up with new cloud security options.
In order to succeed in the cloud, IT and security teams need to embrace a new operating model. This is exactly what startups such as Lacework are focused on. By bringing automation, speed, and integration with cloud security services, the company redefines how to approach cloud security for success. CSO Online took a deep look at the Lacework solution this summer (Top Security Software, 2017; Lacework unmasks hidden attackers amid data center and cloud chaos).
Most IoT devices rely on the cloud to work, especially with connected devices working together. IoT connected devices like household appliances, cars, and electronics, have a cloud-based back end as a means to communicate and store information. The cloud supports these devices, and as we see more IoT devices being made and sold, the cloud usage will continue to increase as a result.
Serverless Cloud Computing that allows developers to build, run applications and services without worrying about managing/operating servers, will increase cloud usage, and cloud use cases. In addition to not having to manage any infrastructure, Serverless Cloud Computing also improves efficiency by allowing developers to connect and extend cloud services to easily address their applications and multiple use cases. Serverless Cloud Computing requires less time and effort, and it simplifies the release of new updates.
Edge computing, or performing data processing at the edge of the network to optimize cloud computing, will also be on the rise. It is a result of increased usage of internet-connected devices. Edge is necessary and will be on the rise in 2018 because it will be required to run the real-time services as it streamlines the flow of traffic from IoT devices and provides real-time local data analysis and analytics.
Cloud containers as a service will become mainstream because it can provide a better infrastructure security. Also, cloud-based container systems are an alternative to virtual machines and allow for apps to be deployed in a quick, reliable, consistent and straightforward manner — allowing for faster releases of new features and software to run reliably. Furthermore, cloud providers can offer hosted container management services as well as differentiate their platforms from one another through cloud container systems.
Adoption of the Cloud will continue to proliferate in the Consumer and SMB/Mid-Market segments; CIO’s will take strategic plans to take advantage of the latest offerings from cloud providers. The tools for a visualization of cloud data and processing market will continue to improve. Security, Performance, and Management of cloud-based solutions will be the issues that Cloud providers will have to overcome.
Cloud adoption will be fast in most segments of the market, but the growth in the enterprise segment will be slow due to security issues being the prime factor in CXO’s taking a cautious approach with Hybrid Cloud adoption. Some enterprises may see a Private Cloud as a solution as well.
As mobility enhances, the cloud will get more global. Cloud providers will succeed when they can adequately mine the data to build better analytics, proactive monitoring and decision making, data mining and artificial intelligence. Cloud technology will continue to grow in the years to come, and organizations must actively participate in its adoption, development and security.
Do you agree with these trends?
OR
Do you see any other trends for 2018?
This post was written by Asokan Ashok, the CEO of UnfoldLabs. Ashok is an expert in driving customer insights into thriving businesses and commercializing products for scale. As a leading strategist in the technology industry, he is great at recommending strategies to address technology & market trends. Highly analytical and an industry visionary, Ashok is a sought after global high-tech industry thought leader and trusted strategic advisor by companies.
Entrepreneur. Inventor. Product Ideation. Strategist. Visionary. Evangelist. Architect.
For any comments or discussions, please feel free to reach out to Ashok or UnfoldLabs at “marketing-at-unfoldlabs-dot-com”.
Innovative Technology Product/Services company. Makers of cool next-gen products. Guide to Mobile, BigData, Cloud, IoT, VR, Wearables, Telematics, 5G.
96 
1
96 
96 
1
Innovative Technology Product/Services company. Makers of cool next-gen products. Guide to Mobile, BigData, Cloud, IoT, VR, Wearables, Telematics, 5G.
"
https://medium.com/future-vision/what-is-cloud-computing-713bd6c3b105?source=search_post---------41,"There are currently no responses for this story.
Be the first to respond.
Cloud will be synonymous with the word Web. Can you think of a business today that’s not on the internet? If you don’t get on the cloud your company could be falling behind quickly.
This article will help you to get what the cloud is even if you don’t know much about technology.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://blog.heron.me/cloud-computing%E9%80%99%E5%A0%82%E8%AA%B2-101b31f9d306?source=search_post---------42,"終於把累死人的Cloud Computing這堂課些修完了！
幾年來，我認知中CMU網路與系統類的神課有四堂：Introduction to Computer Systems、Distributed Systems、Cloud Computing與Operating System Design and Implementation。神課的定義表示它是多位超厲害老師投資多年經驗所設計的課，而更通常是表示這些課有很重的Work Load。它們常常好評壞評兼具，如Cloud Computing（簡稱CC）在統計上一週要花學生30到35個小時，有人認為沒有必要去這樣犧牲掉做其他事情的時間，或是犧牲自己的睡覺時間；但也有人覺得浴火能成為鳳凰，越是花時間投資經歷的東西，最終帶走的越多。
網路上不少關於這堂課兩極的看法：
約莫三年前我就聽過朋友談論過這堂課。那時，有位新認識的朋友在學期初說他修了Cloud Computing，然後，直到學期末結束前，再也沒有看過他了。只聽說他一直都在圖書館裡頭做Cloud Computing的作業。於是乎，直到這學期我才認為已經準備差不多了，可以安心去選來修。
要準備的事情有兩項。其一，確定有足夠的時間，因為作業需要跑各式各樣的服務或運算，會持續花掉不少時間，例如一個沒效率的算法讓Scala程式跑了20小時，又或是Load一份資料到資料庫可能也花掉一兩小時，於是乎選在不是太繁忙的學期修課變得很重要。其二，有相關的經驗再修課比較合適，不然全部都是新知識的話會很崩潰。
所以Cloud Computing到底是怎麼樣的一堂課呢？
它是不用上課的，而是線上出Project給我們做的一堂課。從學期第一天到最後一天，一週一個Project，每個Project會分成2到4個左右的Task，透過自動評分系統打分數，而Deadline設在每週日晚上，沒有遲交的額度，且每週有固定預算，若超過便扣分。
這些Project是在Amazon AWS、Google Cloud Platform和Microsoft Azure這幾個平台上完成的，每個Project有個特定學習目標，實際上的做法就是先閱讀很長很長的Writeup，然後跟著敘述學習某個Programming Model或Service後，實作Writeup中想要的功能，接著在以上幾個平台中順利跑起來，最後送交自動評分系統評分。一般花了我最短兩天最長五個整天來拿到全部的分數。
細節可以參考老師放在網路上公開的Syllabus。
所以一週內要完成的是像是怎樣的Project呢？讓我簡單描述幾個印象比較深刻的。
主題是Database的那一週，問題中他描述了各種Database使用上的好壞處，於是他想要架一個網站使用到多種Database。那次，我們用MySQL寫登入的部分、HBase弄追蹤者的功能、MongoDB負責留言的紀錄、S3存影像，各種Database同時跑起來拿去測試算分數。
又或是學習Iterative Processing的那週，用一小段時間學個新語言Scala後，寫出助教們改版過後的PageRank功能，接著再把一度關係延伸成兩度關係的算法，加分題則是改進效率讓問題可以在30分鐘內跑出來（我的第一版本跑了20小時沒有跑出來）。雖然實際要寫的Code不多，但不熟悉Functional Programming Language的我花了不少時間去表達腦中的邏輯。
相較於一般的個人Project有給明確的方法與提示，其中三週的Team Project需要我們自己設計解法。首先Writeup中會描述需要做出來的Query功能、預算限制和可以使用的Database種類等限制，接著會有一個目標Throughput值，在Live Test達到該Throughput值就能拿到滿分。
我的隊友同我一樣曾是Google實習生，之後也會到Google做正職工作，他擅長分析與撰寫算法多的程式。我們組的設計裡面自己寫了很多的In-memory Cache來加速，然而因為同一個機器要跑多種服務，我們甚至寫了C++的Socket程式管理Memory的使用。然而我們雖然在兩次的Live Test之前都當了排行榜的第一名，但實際Live Test的時候分數卻不是最高的，估計我們的方法還有一些瑕疵之處，值得好好想想。
想起往年在交大一學期修六七堂課，最終什麼也沒學好；相反的，我很喜歡這一整個學期專注在這麼一堂課上面的感覺，期末回頭看我們真學習了、也實作了很多東西：EMR、GCP Dataproc、HBase、Kubernetes、Spark、Kafka、Samza等等，寫了Python、Java、JS還有C/C++跟Scala。學期結束前老師寄信問有沒有興趣當下一個學期的TA時，我也欣然地報名了，下學期又會是另一種Cloud Computing的體驗啦！
Solve Problem Systematically | 軟體系統、美國生活
68 
68 claps
68 
Written by
Solve Problem Systematically | www.heron.me | Software Engineer at Google
Solve Problem Systematically | 軟體系統、美國生活
Written by
Solve Problem Systematically | www.heron.me | Software Engineer at Google
Solve Problem Systematically | 軟體系統、美國生活
"
https://towardsdatascience.com/read-this-if-youre-considering-a-career-in-cloud-computing-1eca26239c51?source=search_post---------43,"Sign in
There are currently no responses for this story.
Be the first to respond.
Arunn Thevapalan
Jun 28, 2021·7 min read
“I don’t want to waste time on cloud computing; I’m focussed on data science.”
"
https://blog.qtum.org/wireline-and-qtum-to-pioneer-the-next-generation-of-cloud-computing-9db9f330c63a?source=search_post---------44,"Wireline, the cloud application marketplace and pioneer of serverless architecture, is today announcing it will work with Qtum, the open-source blockchain project developed by the Singapore-based Qtum Foundation. The cooperation is a major step in creating the largest open-source ecosystem that will enable enterprises to consume microservices at scale using blockchain at its core.
Microservices are revolutionizing the delivery of software; they offer the ability to rapidly create APIs without having to manage the underlying hardware and software infrastructure. Wireline is building an ecosystem based on a microservices exchange; it is a marketplace for enterprises to discover, test and integrate open-source apps, enabling them to build bespoke IT systems.
Wireline is seeking to create the largest open-source developer fund, whereby developers building software critical for serverless architecture and the blockchain ecosystem are rewarded. The cooperation opens up new possibilities for both Qtum and Wireline developers for connecting distributed applications.
“Corporate IT is a huge market with global spend approaching $1.5 trillion, but until now there’s been little reward for developing open-source software because of stifling competition from industry giants,” says Lucas Geiger, CEO at Wireline. “With Qtum we want to change this. Through the combination of both our efforts through the Qtum Foundation and Wireline Developer Fund, we will create an opportunity to monetize existing apps as well as support the creation of new ones.”
Qtum aims to establish a platform designed to bridge the still-existing gap between blockchains and the business world. Qtum’s strategy includes providing a toolset to standardize the workflow of businesses and a hub of tested and verified smart contract templates that address various specialized business-use cases.
“We’re excited to collaborate with Wireline as we have a shared mission — to decentralize the application market and create a platform to facilitate the use of blockchain in business,” says Patrick Dai, co-founder and chairman of the Qtum Foundation. “We think this is beneficial to our developer community and will dramatically reduce the time to market for many of their apps.”
Qtum — Defining the Blockchain Economy
84 
84 claps
84 
Written by

Qtum — Defining the Blockchain Economy
Written by

Qtum — Defining the Blockchain Economy
"
https://medium.com/illumination/why-cloud-computing-is-beneficial-to-anyone-be9015109c70?source=search_post---------45,"There are currently no responses for this story.
Be the first to respond.
Plenty of people associate cloud computing for helping businesses, but it is also useful in the day to day activities of individuals. You can choose from a wide range of cloud service providers, such as Amazon Web Services, Microsoft…
"
https://medium.com/thipwriteblog/%E0%B8%AA%E0%B8%A3%E0%B8%B8%E0%B8%9B%E0%B8%9B%E0%B8%A3%E0%B8%B0%E0%B9%80%E0%B8%A0%E0%B8%97%E0%B8%82%E0%B8%AD%E0%B8%87-cloud-computing-service-%E0%B8%A1%E0%B8%B5%E0%B8%AD%E0%B8%B0%E0%B9%84%E0%B8%A3%E0%B8%9A%E0%B9%89%E0%B8%B2%E0%B8%87-544e2c6a8c85?source=search_post---------46,"There are currently no responses for this story.
Be the first to respond.
ในปัจจุบันมีผู้ให้บริการคลาวด์ หรือที่เรียกกันว่า Cloud Provider อยู่มากกว่า 20 เจ้า แต่ที่เราคุ้นหูคุ้นตากันดีก็คงเป็นสามเจ้ายักษ์ใหญ่ ที่ติดอันดับเป็น “Big Three” ของวงการคลาวด์ นั่นก็คือ Amazon Web Service (AWS), Google Cloud Platform (GCP) และ Microsoft Azure ซึ่งแต่ละเจ้าก็จะให้บริการคลาวด์ที่แตกต่างกันไป
อ้อ! ขอย้ำว่า นี่เรากำลังพูดถึง “ประเภทบริการคลาวด์” ไม่ใช่ “ประเภทของคลาวด์” นะ
ทีนี้เรามาทำความรู้จักประเภทของ Cloud Service กันดีกว่า ซึ่งบางครั้งพวกฝรั่งเขาก็เรียกกันว่า Cloud Computing Stack เผื่อใครเอา keyword ไปใช้ research ต่อ โดยทั่วไปจะแบ่งประเภทบริการคลาวด์ออกเป็น 3+1 ประเภท
ทำไมต้องบวกหนึ่ง?ก็เพราะว่า.. ถ้าเอาแบบหลักๆ จริงๆ ที่คนพูดถึงบ่อยๆ จะมีแค่ 3 ประเภทแรก ส่วนข้อสี่ที่บวกเพิ่มมานั้นเหมือนมันเป็นตัวย่อยแยกออกมาทีหลังนั่นเอง
Step ถัดไป เพื่อให้สมองเรียบเรียงเรื่องยากให้เป็นเรื่องง่าย เราต้องสร้างความคุ้นเคยกับชื่อ types ทั้งหมดก่อน โดยที่ยังไม่ต้องสนใจว่าแต่ละชื่อมันคืออะไร เริ่มค่ะ!!
อ่ะ! ก็ยังดูยาวๆ จำยากๆ อยู่ดีใช่มะ งั้นดูปาก thip นะคะ แล้วท่องตามวนไปค่ะ…
“ แอส-แพส-แซส-แฟส ”
“ แอส-แพส-แซส-แฟส ”
“ แอส-แพส-แซส-แฟส ”
Infra — Platform — Software — Function
บริการทั้งหมดนี้ ผู้ให้บริการคลาวด์บางเจ้า อาจจะเปิดทุก service ครอบคลุมทั้งสามสี่อย่างนี้ หรือบางเจ้าก็จะให้บริการแค่บางประเภท อันนี้ก็แล้วแต่เรา ว่าจะเลือกใช้บริการอะไร ของเจ้าไหน
ต่อไป สายย่อจะขอบรีฟใจความสำคัญสั้นๆ ของคลาวด์แต่ละประเภท ให้พอมองภาพออก
โดย cloud provider แต่ละเจ้าก็จะมี products ยิบย่อยและเยอะมากกก ชื่อผลิตภัณฑ์แต่ละเจ้าก็จะแตกต่างกัน ตัวอย่างเช่น Web Server ของ AWS จะชื่อ “EC2” แต่ถ้าเป็นของ Microsoft Azure จะชื่อ “App Service” ประมาณนี้
สำหรับใครที่อ่านจบแล้ว แต่รู้สึกยังจำอะไรไม่ได้เท่าไหร่ เราแนะนำให้กลับมาอ่านวนไปวันละ 1 รอบ แล้วเลือกจำทำความเข้าใจแค่ครั้งละ 1 service
แต่ถ้าใครอ่านรอบเดียวแล้วรู้เรื่องงง นั่นแปลว่าเราบรีฟได้ดีมากกก555+ ไม่ต้องอ่านซ้ำแล้วก็ด่ะ แต่ก่อนออกไปกด clapsss ให้เรารู้หน่อย จะได้หาอะไรมาบรีฟบ่อยๆ เนอะ!
อ่านบทความที่เกี่ยวข้อง
medium.com
Programming, Technology, Work Life, Finance, Storyteller, Lifestyle, Content Creator, Podcast
32 
Some rights reserved

32 claps
32 
พื้นที่รวมเรื่องราว เรื่องเล่า ของผู้หญิงคนหนึ่งที่ชอบอ่าน ชอบเขียน ชอบเรียนรู้ และอยากแบ่งปัน หากชื่นชอบกันก็กด Follow ไว้นะ ❤
Written by
Software Engineer & Freelance Writer | thipwriteblog@gmail.com
พื้นที่รวมเรื่องราว เรื่องเล่า ของผู้หญิงคนหนึ่งที่ชอบอ่าน ชอบเขียน ชอบเรียนรู้ และอยากแบ่งปัน หากชื่นชอบกันก็กด Follow ไว้นะ ❤
"
https://medium.com/technology-hits/free-ai-cognitive-data-science-programming-and-cloud-learning-for-2020-50070eabc6a7?source=search_post---------47,"There are currently no responses for this story.
Be the first to respond.
In this post, I want to introduce free learning programs by industry experts and industry skill badges…
"
https://medium.com/illumination/how-cloud-computing-saves-your-business-money-77cf2c3c9152?source=search_post---------48,"There are currently no responses for this story.
Be the first to respond.
Switching to the cloud is an excellent option for many small business owners that do not have the extra resources for an in-house IT team. Cloud computing is a cost-effective investment that offers many benefits, whether employees are…
"
https://read.acloud.guru/from-cloud-computing-to-edge-computing-5c639b1796da?source=search_post---------49,NA
https://medium.com/sonm/fog-computing-can-beat-amazons-cloud-computing-services-on-price-speed-and-convenience-5904d1dd14e8?source=search_post---------50,"There are currently no responses for this story.
Be the first to respond.
Many people look at Amazon as the reference point for accessible cloud computing. However, the company maintains a centralized approach, which drives up costs and can lead to lower performance. A decentralized solution is direly needed to alleviate all of these concerns. SONM is providing this change by using decentralized fog computing to the masses.
There are many advantages to fog computing that need to be taken into account. Any device in the world can be a part of the fog “network” and generate computing power to be used by others. Whether it is a computer, smartphone, or even a supercomputer, everything can be used to provide fog computing power. This is quite different from how Amazon provides computing services, as the company only uses proprietary servers. Moreover, the average consumer cannot contribute to Amazon’s infrastructure by any means.
With so many devices that can make up the fog network, the possibilities are virtually limitless. SONM’s fog solutions can provide calculations capable of performing CGI rendering tasks or even processing of scientific data. Those are two of the primary use cases for cloud computing as a whole. This creates a win-win situation for everyone who is part of the fog. The people supplying unused computer power can sell it to the highest bidder. On the other hand, everyone can request additional computing power on demand, regardless of their location.
To put this into perspective, the SONM network can distribute larger tasks over different computing devices all over the world. Moreover, the tasks can be completed in a matter of minutes, rather than hours or days. This is especially useful when it comes to CGI rendering, as the SONM infrastructure is incredibly flexible and can scale to accommodate all needs.
Conducting a traditional CGI processing project on Amazon’s K80 NVIDIA unit would take about 10 hours to complete. Using the SONM fog network, that process can be completed in 10 minutes. This is made possible thanks to SOMM allowing customers to rent as much computing power as they need at affordable prices. Making cloud computing more efficient is of utmost importance. Moreover, SONM provides parallel computing, which is unlike all other cloud computing services in existence today.
Fog computing provides other major benefits as well. When using the services provided by Amazon, there is a question of data safety, additional expenses, and who controls the data in question. Since information is hosted on Amazon’s servers, the company retains a right to copy all data stored on their servers. Fog computing, on the other hand, does not suffer from any of these drawbacks whatsoever. There is no intermediary involved in the process of sharing and buying computing power.
Speaking of renting computing power, buyers can select any rental time or computer architecture they need for their project. Lower prices are achieved due to decentralization and no need for administrator. SONM is also quicker compared to competitors, as there are no data centers involved. In the end, the fog computing solution is more convenient for all parties involved. Plus, it creates a swath of commercial opportunities for both buyers and sellers of computing power.
Join the ICO on June 15th : ico.sonm.io
Sonm Website: http://sonm.io/Bitcointalk ANN: https://bitcointalk.org/index.php?topic=1845114.0Bitcointalk Bounty Thread: https://bitcointalk.org/index.php?topic=1881191.0Medium: https://medium.com/@sonm/Slack: https://sonmio.signup.team/Telegram (English) @sonm_engTelegram (Russian) @sonm_rus
SONM is a global fog computing platform for general purpose…
127 
127 claps
127 
SONM is a global fog computing platform for general purpose computing from website hosting to scientific calculations. SONM company is an effective way to solve a worldwide problem - creating a multi-purpose decentralized computational power market.
Written by
Global Fog Computing Platform
SONM is a global fog computing platform for general purpose computing from website hosting to scientific calculations. SONM company is an effective way to solve a worldwide problem - creating a multi-purpose decentralized computational power market.
"
https://medium.com/@erickschonfeld/mobile-cloud-computing-is-the-return-of-client-server-fe1ac1e6e2f5?source=search_post---------51,"Sign in
There are currently no responses for this story.
Be the first to respond.
Erick Schonfeld
Apr 10, 2015·3 min read
Everything comes around. In the days before the Internet, we used to have something called client-server computing. Applications took advantage of the growing power of desktop computers to run many tasks locally, while tapping into powerful, newly-networked servers to handle more intense tasks and coordinate between all the desktop clients.
With the Internet, most of the computing tasks shifted over to the servers, and the only “client” we need now is a lightweight browser. This definition is fluid — modern browsers can handle many more tasks locally than before, but most of the heavy lifting still happens on the servers on the web.
But with mobile, we are seeing a return of the old client-server model. Except this time, we are calling it mobile-cloud computing. Peter Levine, a partner at Andreessen Horowitz who will be speaking at my next conference, DEMO Traction, April 22, 2015 in San Francisco (tickets), pointed this out to me on a recent call. I’d never thought of it that way before, but it’s obvious.
In mobile, apps rule. And apps take advantage of the growing computing power of our phones to offload as many tasks as possible to the device. This is because the bottleneck is often bandwidth. Apps need to be able to work in offline mode, and communicate with the cloud as efficiently as possible. All that communication drains the battery as well, another reason apps try to do as much as they can on the device.
Just think about your own desktop vs mobile computing habits. How much of your desktop time is spent in the browser? For me, it’s got to be 90%. Yes, there are plenty of desktop apps like Word, Excel, and Photoshop, but I barely use them anymore. On mobile, it’s the opposite. The browser is still important, but it is one of many apps. I use my mobile browser less than 50% of the time I am on my phone or iPad. (Based on my battery usage, it’s only 4%, but I open my browser inside other apps like Twitter all the time).
Mobile apps are just the return of client-server computing. (It never really went away. The desktop web was just more server than client, and now the pendulum is swinging back again with mobile). The servers in the cloud are still super-important, especially for coordinating and communicating between clients. Messaging apps are perhaps the most important type of apps, and they are useless without the network. But apps do as much work as they can on your phone before sending information to the cloud, or tapping into more powerful servers and databases which reside there.
Mobile and the cloud are two sides of the same coin, with mobile becoming the new end-points or “clients.” Think about the apps you use on your phone, whether they are games or taking photos. You can shoot a video and edit it on your phone without even being connected to the network. It’s only when you have to send the finished product that you need the cloud, and of course for sharing. But most of the computing power it took to make that video happened on your phone.
Maybe as mobile bandwidth improves, everything moves back into the cloud. But I doubt it. We like our apps, and how they tap into the computing power resident on our phones. They are here to stay. Welcome back, client-server.
Gentleman blogger
51 
51 
51 
Gentleman blogger
"
https://levelup.gitconnected.com/cloud-computing-101-intro-to-cloud-services-and-concepts-2bc89bce2de?source=search_post---------52,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Cloud computing has gone from a technology limited to big billion-dollar corporations to one that is accessible to anyone. Most developers probably have forgotten about an EC2 instance running an AWS occasionally because, at the end of the…
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/cloudflare-blog/cloud-computing-without-containers-abd0176e1816?source=search_post---------53,"There are currently no responses for this story.
Be the first to respond.
by Zack Bloom
Cloudflare has a cloud computing platform called Workers. Unlike essentially every other cloud computing platform I know of, it doesn’t use containers or virtual machines. We believe that is the future of Serverless and cloud computing in general, and I’ll try to convince you why.
Two years ago we had a problem. We were limited in how many features and options we could build in-house, we needed a way for customers to be able to build for themselves. We set out to find a way to let people write code on our servers deployed around the world (we had a little over a hundred data centers then, 155 as of this writing). Our system needed to run untrusted code securely, with low overhead. We sit in front of ten million sites and process millions and millions of requests per second, it also had to run very very quickly.
The Lua we had used previously didn’t run in a sandbox; customers couldn’t write their own code without our supervision. Traditional virtualization and container technologies like Kubernetes would have been exceptionally expensive for everyone involved. Running thousands of Kubernetes pods in a single location would be resource intensive, doing it in 155 locations would be even worse. Scaling them would be easier than without a management system, but far from trivial.
What we ended up settling on was a technology built by the Google Chrome team to power the Javascript engine in that browser, V8: Isolates.
Isolates are lightweight contexts that group variables with the code allowed to mutate them. Most importantly, a single process can run hundreds or thousands of Isolates, seamlessly switching between them. They make it possible to run untrusted code from many different customers within a single operating system process. They’re designed to start very quickly (several had to start in your web browser just for you to load this web page), and to not allow one Isolate to access the memory of another.
We pay the overhead of a Javascript runtime once, and then are able to run essentially limitless scripts with almost no individual overhead. Any given Isolate can start around a hundred times faster than I can get a Node process to start on my machine. Even more importantly, they consume an order of magnitude less memory than that process.
They have all the lovely function-as-a-service ergonomics of getting to just write code and not worry how it runs or scales. Simultaneously, they don’t use a virtual machine or a container, which means you are actually running closer to the metal than any other form of cloud computing I’m aware of. I believe it’s possible with this model to get close to the economics of running code on bare metal, but in an entirely Serverless environment.
This is not meant to be an ad for Workers, but I do want to show you a chart to reflect just how stark the difference is, to showcase why I believe this is not iterative improvement but an actual paradigm shift:
Not everyone fully understands how a traditional Serverless platform like Lambda works. It spins up a containerized process for your code. It isn’t running your code in any environment more lightweight than running Node on your own machines. What it does do is auto-scale those processes (somewhat clumsily). That auto-scaling creates cold-starts.
A cold-start is what happens when a new copy of your code has to be started on a machine. In the Lambda world this amounts to spinning up a new containerized process which can take between 500 milliseconds and 10 seconds. Any requests you get will be left hanging for as much as ten seconds, a terrible user experience. As a Lambda can only process a single request at a time, a new Lambda must be cold-started every time you get an additional concurrent request. This means that laggy request can happen over and over. If your Lambda doesn’t get a request soon enough, it will be shut down and it all starts again. Whenever you deploy new code it all happens again as every Lambda has to be redeployed. This has been correctly cited as a reason Serverless is not all it’s cracked up to be.
Because Workers don’t have to start a process, Isolates start in 5 milliseconds, a duration which is imperceptible. Isolates similarly scale and deploy just as quickly, entirely eliminating this issue with existing Serverless technologies.
One key feature of an operating system is it gives you the ability to run many processes at once. It transparently switches between the various processes which would like to run code at any given time. To accomplish that it does what’s called a ‘context switch’: moving all of the memory required for one process out, and the memory required for the next in.
That context switch can take as much as 100 microseconds. When multiplied by all the Node, Python or Go processes running on your average Lambda server this creates a heavy overhead which means not all of the CPUs power can actually be devoted to running the customer’s code; it’s spent switching between them.
An Isolate-based system runs all of the code in a single process and uses its own mechanisms to ensure safe memory access. This means there are no expensive context switches, the machine spends virtually all of its time running your code.
The Node or Python runtimes were meant to be run by individual people on their own servers. They were never intended to be run in a multi-tenant environment with thousands of other people’s code and strict memory requirements. A basic Node Lambda running no real code consumes 35 MB of memory. When you can share the runtime between all of the Isolates as we do, that drops to around 3 MB.
Memory is often the highest cost of running a customer’s code (even higher than the CPU), lowering it by an order of magnitude dramatically changes the economics.
Fundamentally V8 was designed to be multi-tenant. It was designed to run the code from the many tabs in your browser in isolated environments within a single process. Node and similar runtimes were not, and it shows in the multi-tenant systems which are built atop it.
Running multiple customers’ code within the same process obviously requires paying careful attention to security. It wouldn’t have been productive or efficient for Cloudflare to build that isolation layer ourselves. It takes an astronomical amount of testing, fuzzing, penetration testing, and bounties required to build a truly secure system of that complexity.
The only reason this was possible at all is the open-source nature of V8, and its standing as perhaps the most well security tested piece of software on earth. We also have a few layers of security built on our end, including various protections against timing attacks, but V8 is the real wonder that makes this compute model possible.
This is not meant to be a referendum on AWS billing, but it’s worth a quick mention as the economics are interesting. Lambdas are billed based on how long they run for. That billing is rounded up to the nearest 100 milliseconds, meaning people are overpaying for an average of 50 milliseconds every execution. Worse, they bill you for the entire time the Lambda is running, even if it’s just waiting for an external request to complete. As external requests can take hundreds or thousands of ms, you can end up paying ridiculous amounts at scale.
Isolates have such a small memory footprint that we, at least, can afford to only bill you while your code is actually executing.
In our case, due to the lower overhead, Workers end up being around 3x cheaper per CPU-cycle. A Worker offering 50 milliseconds of CPU is $0.50 per million requests, the equivalent Lambda is $1.84 per million. I believe lowering costs by 3x is a strong enough motivator that it alone will motivate companies to make the switch to Isolate-based providers.
Amazon has a product called Lambda@Edge which is deployed to their CDN data centers. I’m sure they’re doing their best with the technology they have to make it compelling, but it’s three times more expensive than traditional Lambda, and it takes 30 minutes to deploy a code change. I don’t think there’s a compelling reason to use it.
Conversely, as I mentioned, with Isolates we are able to deploy every source file to 155 data centers at better economics than Amazon can do it to one. It might actually be cheaper to run 155 Isolates than a single container, or perhaps Amazon is charging what the market will bear and it’s much higher than their costs. I don’t know Amazon’s economics, I do know we’re very comfortable with ours.
Long-ago it became clear that to have a truly reliable system it must be deployed to more than one place on earth. A Lambda runs in a single availability zone, in a single region, in a single data center.
No technology is magical, every transition comes with disadvantages. An Isolate-based system can’t run arbitrary compiled code. Process-level isolation allows your Lambda to spin up any binary it might need. In an Isolate universe you have to either write your code in Javascript (we use a lot of TypeScript), or a language which targets WebAssembly like Go or Rust.
If you can’t recompile your processes, you can’t run them in an Isolate. This might mean Isolate-based Serverless is only for newer, more modern, applications in the immediate future. It also might mean legacy applications get only their most latency-sensitive components moved into an Isolate initially. The community may also find new and better ways to transpile existing applications into WebAssembly, rendering the issue moot.
I would love for you to try Workers and let us and the community know about your experience. There is still a lot for us to build, we could use your feedback.
We also need engineers and product managers who think this is interesting and want to take it in new directions. If you’re in San Francisco, Austin, or London, please reach out.
Originally published at blog.cloudflare.com on November 9, 2018.
Select highlights from blog.cloudflare.com
45 
3
45 claps
45 
3
Select highlights from blog.cloudflare.com
Written by
The Official Account of Cloudflare
Select highlights from blog.cloudflare.com
"
https://medium.com/ethex-market/built-on-ethereum-decentralized-cloud-computing-projects-b47bf2a4df43?source=search_post---------54,"There are currently no responses for this story.
Be the first to respond.
At Ethex.market, we are interested in innovative projects built on Ethereum that have the potential to disrupt various industries. Here is an overview of four Ethereum-based projects that are changing the field of cloud computing. They all have a few common characteristics. For example, they have already made significant progress at developing working market solutions and have successfully launched on the Ethereum mainnet. Moreover, each is actively working on improving their existing infrastructures as well as launching new and innovative applications.
Founded: April 2017
Funds Raised: ~$12,000,000
Market Thesis: Many organizations have on-premises or cloud-based computing resources that are not in permanent use. At the same time, other organizations have short-term requirements for computing power. iExec’s vision is to bring the two sides together and create a cloud-based marketplace for executing computations. The iExec team is creating a new ecosystem of companies offering storage, computer farms, data providers, web hosting, and SaaS applications through a mature, solid, and open-source Desktop Grid software as well its Proof-of-Contribution (PoCo) protocol.
Mainnet Launch Date: On December 21, 2017, for the first time, an actual RLC token was used to trigger an off-chain computation for the FIGlet application.
Live Applications: iExec Marketplace, iExec DappStore, iExec SDK
Alibaba: Alibaba Cloud provides Encrypted Computing, powered by Intel® SGX, to support the iExec Trusted Execution Environment (TEE) solution. This iExec TEE solution provides end-to-end protection of data for blockchain-based computing. On the Marketplace, iExec has deployed multiple ‘worker pools’, giving requesters of computation the choice on which machines power their applications and services. The iExec Trusted Execution Environment (TEE) solution can be run on the Alibaba Cloud trusted worker pool.
Intel: In May 2018 at Consensus, iExec presented alongside Intel to demonstrate how its proof-of-concept Intel® SGX enclaves can ensure that, although the applications were running on decentralized nodes, sensitive data can’t be inspected or altered with by malicious attackers on the network.
IBM: Thanks to a partnership with IBM, iExec provides a high degree of security and privacy to the enterprises running code on its cloud and can provide a zero-trust architecture. Empowered by the unique IBM Cloud approach to cloud security, iExec is extending the value of cloud by helping enterprises run even their most sensitive workloads on shared hardware at much lower risk. An IBM workerpool has been made available on the iExec network.
Data Wallet will allow data providers to monetize their datasets on the marketplace with the highest level of security. This new paradigm, which will be implemented in iExec V3 (release in May 2019), fuels disruptive ecosystems and offers companies new and innovative business models.
iExec End-to-End Trusted Execution with Intel SGX: Intends to be used as an industry reference to enhance the overall security of decentralized cloud computing. This new Intel® SGX solution, combined with blockchain, allows for an unmatched level of trust for dApps and execution/data processing on decentralized nodes.
Open Decentralized Brokering: 0x-like protocol that aims to address marketplace scalability challenges and reduce gas consumption. In the future, it will include more complex order book management (i.e. OTC, discounts, subscriptions, etc.).
Domain Specific Sidechain (DSS): Stores information needed to establish consensus on a sidechain with PoA. It bridges mainchain/sidechain with Parity bridge. Later, DSS will include a Parity substrate + Polkadot relay.
Founded: November 2016
Funds Raised: ~$8,000,000
Market Thesis: The Golem network is made up of the combined power of users’ machines, from PCs to entire data centers. It’s designed to handle a wide variety of tasks, from CGI rendering to machine learning and scientific computing. This project doesn’t take or plan on taking any transaction fees from the users.
Mainnet Launch Date: On April 10, 2018, Golem announced the launch of Brass Beta into mainnet.
Live Applications: Golem (Brass Beta) mainnet
Streamr: This partnership is strategically important because Streamr provides low-overhead power, which Golem needs to assign tasks, validate results, manage peer reputation, hone pricing mechanisms, and accomplish resource-intensive tasks that the decentralized network isn’t entirely capable of handling.
Friend Software Corporation: This company offers a user-friendly operating system called FriendUP, which has ambitions to be decentralized. Golem donated $750,000 of its fundraising earnings to Friend in support of this mission and lends its power and advice in exchange for use of the FriendUP OS in its own ecosystem.
Marketplace: This feature launched in February 2019 with the release of Brass Golem Beta 0.19. In previous versions of Golem, the prices for cloud computing services were basically set by the requestors. On the other end, providers were only able to set the lowest amount per task they would be willing to work for. Then, the requestors would just accept any and all offers that made it to them first. With the marketplace, the accent is shifted to the providers. With marketplace, providers are given the option to compete with others with both their prices and reputations.
Clay: Set to launch sometime during the first half of 2019, a core component of Clay will be an early version of the following APIs. Task API: creates and manages computations. Market API: chooses the best offers. Net API: Golem’s networking layer. Onboarding API: smoother onboarding process. Once all the initial development of the APIs is complete, the Golem team will start developing an SDK.
Later releases of Golem will include (but not limited to) the following:
Founded: June 2017
Funds Raised: ~$42,000,000
Market Thesis: The main idea of fog computing, the technology used by SONM, is the use of a decentralized network formed from disparate individual computing devices, as opposed to a single data center structure. Compared to other Ethereum-based cloud projects, SONM appears to have more advanced solutions that are in active use. These include infrastructure for blockchain applications, machine learning, video rendering on CPU/GPU, and even CDN and video streaming.
Mainnet Launch Date: July 1, 2018
Live Applications: SONM Market (mainnet), SONM OS (mining equipment rental marketplace)
POSTKINO FX: This studio deals with the creation of computer graphics for cinematography and TV. It is one of the first companies to use SONM to render tasks. One task included rendering graphics in a video (shown below) for MEGAFON, the second-largest mobile phone operator and the third-largest telecom operator in Russia.
AION: Through this partnership, SONM supports Aion’s technical roadmap by providing its community with the ability to rapidly deploy Aion’s kernel operating system. In addition, SONM also acts as an early adopter of Aion mining.
Dbrain: SONM provides greater computing power to Dbrain, which is a tool to collectively build AI (i.e. neural network training). SONM enables data to be processed quicker and cheaper than traditional providers.
Hacken: This group of enterprises was created to maintain links between blockchain and cybersecurity communities, the promotion of hacker ethics, and the encouragement of legitimate research on computer networks and software. Hacken provides security solutions and professional knowledge to SONM.
GPU cloud platform: Expected to be released in the first half of 2019, this platform will be designed for building, training, and deploying machine learning models. The application will be aimed at data scientists, novice AI specialists, and nontechnical professionals who don’t have access to the expensive computing infrastructure that’s necessary for machine learning. The idea is to provide them with a set of tools for exploring data, training neural networks, and running GPU compute tasks. It will even provide a solution to run custom containers on the required devices, with specified network bandwidth, in the country of the customer’s choice.
‘Plug & play’ service for video rendering: This will help freelancers and small studios to render scenes in just a few clicks. The service is aimed at customers who want to reduce rendering costs or need additional computing resources at those moments when rendering farms are busy and can’t perform the required task.
Next version of SONM OS: This is designed to not only help suppliers deploy a worker but also to prevent the device owner from decrypting and copying customer data. It will include a fully-managed enterprise GPU cloud platform with a TEE (trusted execution environment) for securing computations on consumer devices.
The above-mentioned development projects are all expected to launch in 2019. A full list of detailed, long-term development objectives and future applications (through 2022) can be found here.
Founded: January 2018
Funds Raised: ~$28,750,000
Market Thesis: DADI’s Edge network connects together discrete devices to deliver faster and more efficient digital services. DADI is owned and run by everyone, sharing its revenue to help build a fairer internet. The DADI network offers improved efficiency and performance, saving up to 90% compared with traditional cloud services.
Mainnet Launch Date: On June 28, 2018, DADI launched its CDN on the Ethereum mainnet. This is the first of the project’s dApps to be ‘network-ready’.
Live Applications: DADI CDN, DADI Marketplace (links to all DADI apps)
Verasity: VeraPlayer is integrated with the DADI network, enabling platform customers to easily publish content to the Verasity ecosystem and provide a full micropayment capability for their content. Verasity’s video toolkit (vDaf) allows existing online video services to access the benefits of blockchain technology. Broadcasters, media platforms, OTT services, and video publishers can use Verasity’s blockchain enabled commerce, analytics, verification, and crowdfunding systems to enhance their online video offering.
Agorai: This blockchain-based marketplace offers fair access to artificial intelligence software and data assets. The Agorai marketplace will run on the DADI network. Additionally, DADI will work with their team to further develop AI capability within its technology. Integrating with DADI will provide Agorai users with a much-needed choice of how they interact with the platform, keeping costs low, distributing compute power, and ensuring anyone can access Agorai’s platform to make use of data assets or to access AI tools and applications.
Wirehive: This company delivers expert infrastructure consultancy and support for a broad portfolio of clients including Vodafone, Honda and IT. Wirehive offers DADI’s network to clients as a decentralized alternative to AWS, Microsoft Azur, and Google Cloud.
London Block Exchange: LBX is a crypto exchange that provides a fiat on/off ramp for customers of the DADI network. DADI makes its dApps (including API, Web and CDN) available for use on the LBX website.
DADI Visualize: RESTful API layer designed in support of API-first development and COPE.
DADI Predict: Machine learning layer that predicts in-session user behavior.
DADI Marketplace: First iteration of DADI application marketplace, will be open to third-party applications complementary to the scope of the DADI suite of microservices.
Edge Compute Beta: Like a virtual machine, but hyper-localized. This containerized app layer enables you to distribute your applications directly to the Edge network, with a real-time distribution matrix that automatically deploys and scales based on demand.
All of the above projects are scheduled to be completed in 2019. The full DADI roadmap can be found here.
Useful Tokens. Trustless Trading. https://ethex.market
273 
1
273 claps
273 
1
Written by
Writing and reading about startups, SaaS, blockchain, e-commerce, real estate, and more.
Useful Tokens. Trustless Trading. https://ethex.market
Written by
Writing and reading about startups, SaaS, blockchain, e-commerce, real estate, and more.
Useful Tokens. Trustless Trading. https://ethex.market
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://codeburst.io/why-investing-in-cloud-computing-skills-is-worth-it-100-e4e957ae89b9?source=search_post---------55,"Cloud Computing is here…yesterday I mean.
For most of us more or less Cloud Computing now can be as easy as issuing one command line and your application gets cloudified immediately or magically.
According to the State of Cloud Computing survey from Rightscale, companies now run 79% of workloads in cloud, with 41% of workloads in public cloud and 38% in private cloud. The highest concerns were on security, spend, and lack of expertise, Docker is the top DevOps tool.
This means that Cloud Computing is here to stay. It makes sense to realize that there is accumulated maturity and it’s not just an early adoption trend. For the rest of us though, like me, as curious little brainiacs or geeks or whatever, it feels dull. There is a challenge ahead waiting to be explored. A challenge of new exploration. Like landing on a new planet and wanting to explore the whole terrain.
When in Rome, do as the Romans do
As a curious little lizard, I would like to know more. To understand how a Service Mesh works or how Software Defined Networking glues services together or what are the Security Considerations of Cloud Applications. How Kubernetes does its magic or maybe create your own Orchestration platform yourself.
As part of my Continuous learning process and to prevent my self in scattering my focus all over the place I would like to broaden my existing T-Shaped skills with something cool and useful. It’s not out of my current exposure anyway as I’ve used several services like Docker or Gcloud before.
I’ve assembled my own plan to achieve practical expertise in Cloud technologies for the next year or so. I would like to gain a good mixture of accredited certifications and real-world contributions. While I may or may not experience that in excess I would ideally like to be Cloud Savvy and fairly opinionated. You might get an idea of how you would like to plan your own skills investment strategy.
Those are the learning paths and materials with the associate study duration that I would like to work on. They form an initial body of knowledge and motivation and they are good for strengthening the core skill buildup:
While learning and gaining certifications is a good thing, I also need to apply what I’ve learned in practice so I can understand the theory in practice. Here is the list of possible contributions:
In addition, to open sourcing, I would also like to blog about my experiences write tutorials in Tech.io and if I have time attend some meetups. That will give me some valuable feedback of my current level.
I hope that I gave you at least some motivational hints of why Cloud Computing is worth to invest in. It gives great satisfaction in the long run.
Learning new things to broaden your horizon and is an exciting thing. It can also be daunting and time-consuming. I believe with the right plan and passion one can achieve what he really wants. Cloud technologies are here to make computing accessible and efficient. I believe that it will be a good challenge if not of a mind blast.
Bursts of code to power through your day.
49 
1
49 claps
49 
1
Written by
Senior Software Engineer @teckro, Experienced mentor @codeinstitute, @hack.hands, Technical Writer @fixate.io Tweet me at @nerdokto.
Bursts of code to power through your day. Web Development articles, tutorials, and news.
Written by
Senior Software Engineer @teckro, Experienced mentor @codeinstitute, @hack.hands, Technical Writer @fixate.io Tweet me at @nerdokto.
Bursts of code to power through your day. Web Development articles, tutorials, and news.
"
https://medium.com/hackernoon/city-brain-cloud-computing-and-data-technology-help-our-city-think-deeply-382e72f7f7f?source=search_post---------56,"There are currently no responses for this story.
Be the first to respond.
It takes HUMAN 100 years to process these data. It takes TECHNOLOGY one second to make our city a better place.
On November 9, 2017, Dr. Wang Jian, Chairman of Alibaba Group’s Technology Committee, published an article in China’s mainstream media on the “City Brain”. In it, he explains that in the future, data resources will be more important than land resources in the field of urban development through the creation of ‘City Brains’. The following is a revised excerpt of that article.
The Internet was a revolutionary technology that changed the world. However, we are now entering a new technological era that some are saying will be the ‘death of the Internet’. And that new era is being born through AI. However, AI will not be the death of the Internet, because it is the large amounts of data created by the Internet that is spurring the rapid development of AI and its ability to help us.
When artificial intelligence was first proposed in the 1950s, scientists struggled to make computers successfully simulate human intelligence. However, with the power of the Internet and computing technology today, we have not only been able to simulate human intelligence but are well on our way to creating a whole new kind of intelligence. Now, using the vast amounts of data created daily, we are able to use computers to solve problems that cannot be solved by human intelligence alone. Just as in the past when humans used tools and machines to build things where human physical strength was lacking, we are now starting to use the intelligence of machines where our intelligence is lacking. In this way, it may be more accurate to refer to artificial intelligence as machine intelligence. It is not so much artificial ‘human’ intelligence, but a different kind of intelligence altogether. And this makes it very powerful.
Today, many cities around the world face issues to do with sustainable development. Without further breakthroughs in technological innovation, cities will all face even greater challenges going forward. So, to tackle these challenges, machine intelligence, fueled by data from the internet, is being utilized to solve many urban development issues, such as solutions to traffic management. This is a golden opportunity for a new generation of technologies surrounding machine intelligence to mature and for the rate at which smart industries are developing to increase. And this situation is where the creation of ‘City Brains’ began.
In the past 20 years, China has continuously invested in the development of urban informatization. In particular, the public security and traffic police departments have always been at the forefront of data accumulation. But without the help of machine intelligence there has always been limitations in the use of these data. For example, the amount of video recorded by traffic cameras in a single city in one day would take a single human over 100 years to watch, which means most of these data is either deleted or stored, never to be used.
When I first heard this statistic, I felt like the farthest distance in the world was not from Antarctica to the North Pole, but from traffic lights to traffic cameras. They are both on the same pole but they do not share data, meaning that traffic operations cannot be optimized using the traffic information recorded by cameras. This is a clear waste of urban data resources and increases the cost of urban operations and development. It was in answer to situations like this that on the Computing Conference in Hangzhou last year, the world’s first City Brain was announced to be in operation, in the same city.
A City Brain will utilize the Internet as its infrastructure and make use of the wealth of urban data being recorded every second to make global real-time analyses of cities. This means that cities will be able to effectively leverage urban data to allocate public resources, constantly improve social governance, and promote sustainable urban development. It is in this way that in the future, urban data resources will be more important than land resources in urban development. This idea is the driving force behind the development of the City Brains.
In Hangzhou (where Alibaba’s headquarters is located; population: 8.1 million), the City Brain receives instant traffic flow information from the city’s video cameras. For the first time, the city’s traffic lights have optimized the time distribution of intersections to improve traffic efficiency based on real-time traffic. The City Brain utilizes each traffic camera on the road for instant traffic analysis, just like having traffic police patrolling every junction, all year round, controlling the lights.
In October this year, at the Cloud Conscious General Assembly, data from the past years operation of Hangzhou’s City Brain was shared. It showed that the optimization of time distribution for the 128 junctions connected to traffic data reduced the overall passage time by 15.3%. Also, in the center of the city, the automatic alarm for criminal activity was raised 500 times daily, with an accuracy rate of 92%, greatly improving the direction of law enforcement. Currently, the Hangzhou traffic police division is using the City Brain to optimize traffic lights in the main metropolitan area and provide immediate dispatching decisions.
In addition, Hangzhou’s Xiaoshan District was able to give priority to special vehicles, including over 120 ambulances. Once an emergency call had been received, the City Brain automatically optimized a traffic light route from the ambulance to the accident, using traffic data, while still reducing the impact on other traffic. On average, this cut ambulance arrival time in half.
Traffic control is only the beginning. Data is starting to solve many urban development problems that cannot be solved by the humans. Just as it was difficult to build things before the invention of mechanical equipment, it is currently very difficult to optimize urban development without AI’s help. There are 3 major ways that City Brains will assist us.
Firstly, there will be a breakthrough in urban governance. Using urban data, including that of social structure, social environment, and social activities, we can determine how to best utilize physical resources and solve outstanding problems facing different cities, achieving an innovative and humanized governance modal.
Secondly, there will be a breakthrough in urban services. City Brain will become the most important tool we have to improve the lives of the public. Using City Brain, enterprises and individuals will be served more accurately and efficiently. For example, public services, such as transportation, will enter an era of perfectly balanced and efficient.
Thirdly, there will be a breakthrough in urban industrial development. Urban data will become the foundation of the transformation of traditional industries and the development of new industries, just as the invention of the semiconductor was the foundation of the development of the computer industry.
For example, in Suzhou (60 miles west to Shanghai; population: 10.6 million), it has recently been decided that there will be a focus on traffic management using the City Brain framework. The public security department will pilot the joining of the transportation, tourism, rail transportation, and the city appearance and municipal administration departments to bring massive amounts of data together from different sources to be analyzed by the City’s Brain s to help the city operate efficiently and safely. As urban brain systems expand into all areas of social governance, Suzhou will have the opportunity to become a model for the successful harnessing of value from urban data resources for urban governance, social governance, and industrial governance.
Today, no other country in the world is treating City Brain as a necessity. City Brain first appeared in China thanks to China’s well-developed Internet infrastructure and China’s willingness to quickly adopt new technology, as can be seen in China’s widespread use of mobile payments. This difference has given China a competitive edge in the City Brain field. The accumulation of urban data in China will be faster than that in any other country in the world. This gives China an important opportunity to solve the problems of urban development in a more advanced way than other countries.
City Brain is not only of great significance for the development of China’s cities, but also provides a huge platform for the development of China’s technological innovation. With unprecedented computing power and data resources, City Brain will not only benefit people but also become the most important research platform for machine intelligence for the next 10 years.
Ten years ago, Alibaba was the first to invest in cloud computing research and development with the goal of enabling young people to possess the computing power of multinational giants. Now, City Brains will also give people advantages in the upcoming era of global intelligence.
Every technological revolution carries urban civilization a step forward. In the steam engine era, cities developed roads and railways; in the electricity era, cities developed lights and power grids. Now, in the Internet age, City Brains will once again elevate civilization. Like the first subway construction in London 160 years ago and Manhattan’s first power grid construction 135 years ago, City Brains will become a brand-new part of urban infrastructure and an important step forward that China will give to the world.
First hand, detailed, and in-depth information about Alibaba’s latest technology. Follow us: www.facebook.com/AlibabaTechnology
#BlackLivesMatter
227 
1
how hackers start their afternoons. the real shit is on hackernoon.com. Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
227 claps
227 
1
Written by
First-hand & in-depth information about Alibaba's tech innovation in Artificial Intelligence, Big Data & Computer Engineering. Follow us on Facebook!
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
First-hand & in-depth information about Alibaba's tech innovation in Artificial Intelligence, Big Data & Computer Engineering. Follow us on Facebook!
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/ankr-network/ankr-distributed-cloud-computing-network-dccn-illustration-a74c3f5bbbfd?source=search_post---------57,"There are currently no responses for this story.
Be the first to respond.
1. Dashboard — The landing page that appears when users first log in to DCCN web app. This page displays the high-level information of the following aspects
*some data points remain as placeholders as indicated in the screens below
2. Marketplace — This section offers users recommendations on the best data centers for their tasks to run on. With the first version, all available data centers will be listed for users to choose from. In the next two months, Ankr will complete the design phase and release the recommendation system.
3. Task/App Store — The task section enables users to manage and deploy docker containers. One important note: the “image” field requires following docker’s naming convention (docker name:tag)
4. Billing/Wallet — Before the mainnet launch event, the public-facing wallet related functions will be temporarily disabled. However, users will be able to interact with the UI to gain a better understanding of future functions.
5. Profile/Settings — As of now, only the data that users input during registration will be displayed, but in a few days, the backend APIs that our engineering team is working on will be ready.
We are excited for you all to trying out Ankr’s test platform. It’s not yet finished or finalized, but it’s a big step towards the distributed cloud powered by idle resources across the world. Definitely send us feedbacks!
Link: app.ankr.network
The official blog of Ankr
318 
318 claps
318 
Written by
Blockchain infrastructure and staking DeFi
The official blog of Ankr
Written by
Blockchain infrastructure and staking DeFi
The official blog of Ankr
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://tales.codepicnic.com/cloud-computing-easier-than-ever-with-codepicnic-cli-7fca6ce8828c?source=search_post---------58,NA
https://medium.com/javarevisited/5-best-cloud-computing-books-for-beginners-53267098a3e3?source=search_post---------59,"There are currently no responses for this story.
Be the first to respond.
Hello guys, if you want to learn Cloud Computing like AWS, Google Cloud Platform, or Microsoft Azure, or any Cloud computing platform and looking for the best resources then you have come to the right place.
Earlier, I have shared both the best Cloud Computing Courses for beginners as well as some free cloud computing courses to start with. And in this article, I am going to share the best books to learn about Cloud Computing for Beginners.
The list not only includes books to learn Cloud Computing fundamentals and basic concepts like IaaS, Pass, Saas, etc but also book which covers popular Cloud computing platform like AWS.
If you are a complete beginner, Cloud computing is the field of using a network of remote servers, which are powerful computers that work twenty-four hours in a data center somewhere in the world. You can host your website, data, files on that computer and use it remotely.  The benefits of using cloud computing have made many businesses use this technology for hosting their web application as well as it’s cost-effective instead of hosting their services in the localhost servers and spending much money managing them.  There are other benefits of using cloud computing like its high speed in transforming data, the ability to restore and back up your data and files, helps employees in different locations to collaborate together, and many other benefits. Some of the best cloud computing are Amazon AWS, Google Cloud Platform, and Microsoft Azure.  If you are trying to get started in the field of cloud computing, then I’ve listed here in this article five of the best cloud computing books, and they are for beginners as well as experts.
I have also shared a few online courses along the way to get the best of both worlds as online courses help you to learn quicker and books generally help you to learn deeper.
Without wasting any more of your time, here is a list of the best Cloud computing books to learn fundamental and essential Cloud computing concepts about computing, storage, networking, billing as well as popular public cloud platform like AWS.
As the title said, you will learn cloud computing as a beginner starting from scratch. This book is really a helpful resource to start your career in cloud computing with a lot of pictures and examples to learn as you are 10 years old, and it costs $9.99 as a kindle book on amazon.  You will cover tons of interesting topics, starting with a definition of the cloud and how it got this name and what it means when some say as a service like software as a service, and if the weather really affects cloud computing as most people think.
If you need a course, I highly recommend you to combine this book with the Introduction to Cloud Computing on Amazon AWS for Beginners course by Neal Davis, one of the authorities on teaching AWS on Udemy. This is probably the best and most up-to-date course to learn AWS and Cloud computing from scratch in a single course.
Another interesting book to learn the basics of cloud computing for beginners is this book called cloud computing for dummies for the authors Judith S. Hurwitz and Daniel Kirsch, who wrote this second edition and make it $21 as a kindle book on amazon.  You will learn about cloud computing and the security concerns like weaknesses and risks of using this technology.
You will also understand the cloud models such as software as a service and the other models Iaas and Paas, as well as managing in a multi-cloud world, developing your cloud strategy by integrating data in the cloud, promoting cloud security, and more.
You can also combine this book with the Cloud Computing for Beginners — Infrastructure as a Service course from Udemy to mix some active learning. this will help you to learn cloud computing concepts better.
If you search on Amazon for cloud computing, you will get this book on the first results suggestion of amazon which means that this book is one of the top-selling on the platform.
This cloud computing book is created by the author Erl Thomas and costs $229.26 as a kindle book, quite expensive, isn’t it? but, once you read the book you will see it’s worth it.  After you complete reading the book, you will have a solid understanding of the fundamentals of this industry known as cloud computing as well as its architecture and models, understand the business and economic factors that make the world-leading companies use this technology to host their services and websites, and more.
IF you need a course to go along with this book, Introduction to Cloud Computing course by IBM on Coursera is a nice one to start with. It’s also free to audit.
Kief Morris has been in cloud computing for more than twenty years, managing and deploying IT infrastructure services. He has created a book called Infrastructure as a code, teaching you the skills needed to land a job as a system administrator or an infrastructure engineer.   The nice I like about this book is that the author has made it practical as well, not only theoretically like most of the online books, and you will learn about the tools that configure the cloud infrastructure as well as writing code to automate tasks, explore services for managing the infrastructure, and much more.
If you need a course to join along with this book, checkout Infrastructure as Code, Master AWS Cloud Development Kit CDK courses on Udemy.
As I’ve mentioned earlier, one of the best cloud service providers is Amazon AWS (Amazon Web Services), and most companies nowadays are using this service. This book will give you the foundation the Amazon AWS and how to use its different services and infrastructure.  In this practical guide, you will learn what cloud computing is and why most businesses choose AWS instead of the other cloud providers.
Then you will see how you can store your data and files on the Amazon S3, as well as how to use the hardware and software cloud resources to deploy your web application using the Amazon EC2, secure your data, and more.
And, if you need a course to go along with this book, I I highly recommend you to check out AWS Fundamentals Specialization from Coursera, which is created and offered by AWS itself and more than 20K people have already joined this course.
That’s all about the best books to learn about Cloud Computing and AWS for Beginners. The above books that I have listed with you are created by authors who have been in the industry for years, and they have shared what you will need to know if you are trying to involve in this industry, and they will definitely help you learn the skills needed to start your journey in the field of cloud computing.
At the moment, the list doesn’t have any book on Google Cloud Platform or Azure but I will update it in the figure to include those.
Other IT and Cloud Certification Articles you may like:
Thanks for reading this article so far. If you like these best Cloud Computing online courses, then please share them with your friends and colleagues. If you have any questions or feedback, then please drop a note.
P. S. — If you are new to the world of Cloud and AWS and looking for the best online courses to learn Amazon Web Service, then I highly recommend you to check out AWS Fundamentals Specialization from Coursera, which is created and offered by AWS itself and more than 20K people have already joined this course.
coursera.com
Medium’s largest Java publication, followed by 14630+ programmers. Follow to join our community.
148 

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
148 claps
148 
Written by
I am Java programmer, blogger, working on Java, J2EE, UNIX, FIX Protocol. I share Java tips on http://javarevisited.blogspot.com and http://java67.com
A humble place to learn Java and Programming better.
Written by
I am Java programmer, blogger, working on Java, J2EE, UNIX, FIX Protocol. I share Java tips on http://javarevisited.blogspot.com and http://java67.com
A humble place to learn Java and Programming better.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/dreamforce-15/dreamforce-the-super-bowl-of-cloud-computing-86aa280be723?source=search_post---------60,"There are currently no responses for this story.
Be the first to respond.
By Lynn Vojvodich
Every fall, San Francisco is home to Dreamforce, the largest software conference on the planet. It’s the Super Bowl of cloud computing, with more than 135,000 registered attendees and five million people tuning in to watch the live stream. And, like any Super Bowl, it is filled with energy, excitement and, of course, some amazing entertainment. Foremost, Dreamforce is an incredible gathering of innovators and a launch pad for new technologies and products from the entire Salesforce ecosystem.
Dreamforce ’14 (#DF14) will be our best Dreamforce ever — packed with 26 inspiring keynotes, more than 1,450 breakout sessions, over 500 interactive product demos, and more than a few groundbreaking product introductions. But what makes Dreamforce so special — and keeps people coming back year after year — is the unique Dreamforce combination of innovation, fun, and giving back.
This year’s program features an amazing mix of creative thinkers and game-changing technologies. Our keynote speakers range from Hillary Clinton and Tony Robbins to Marc Andreessen and Tony Prophet. Seven-time Grammy award winner will.i.am and rock legend Neil Young will unveil exciting new products. Of course, Salesforce — recognized by Forbes as the world’s most innovative company four years running — will announce revolutionary new technologies and products, too. Don’t miss the big reveals in Marc Benioff’s keynote on Tuesday at 1:30pm PDT!
Salesforce has a long-standing commitment to customer success. At Dreamforce, we invite our customers to share their stories so we can all learn from them. This year, more than 2,000 customers and partners will showcase their Salesforce solutions and business transformations. We’re also bringing back the hugely successful $1 Million Hackathon, in which developers build mobile apps on the Salesforce1 Platform to compete for $1 million in cash prizes (more info here). It’s my fourth Dreamforce, and I can’t wait to be inspired by the innovations of our Salesforce community.
Don’t forget your dancing shoes! Live entertainment and casual networking opportunities have been part of the Dreamforce experience from the beginning. This year, we’ve lined up more than 20 bands to perform live at Dreamforce, including our benefit concert featuring Billboard’s 2013 Artist of the Year Bruno Mars and alternative rock band Cake. Keep an eye out for SaaSy, who’s got some sweet dance moves and is never far from the fun.
Howard Street in downtown San Francisco will become Dreamforce Plaza — an outdoor space designed to cultivate connections and idea-sharing while you soak up the sun. With its creative culture and startup-rich economy, San Francisco is the perfect backdrop for relationship-building — whether at the hundreds of partner-organized events during Dreamforce or through the active #DF14 social and digital channels.
At Dreamforce, we go big when it comes to giving back. This year, we’ve got four days of giving, with four main philanthropic causes — all part of our #DFGives initiative (learn more at dfgives.org).
On Monday, we’ll focus on fighting hunger with our food drive for One Million Meals, benefiting the San Francisco/Marin Food Bank and anti-hunger partners around the world. Please bring a can to donate, or if you can’t attend in person, contribute to the cause online. On Tuesday, we are supporting children’s health and education. Volunteers will assemble 2,000 bags of educational materials, and the Bruno Mars concert will raise more than $8 million to benefit UCSF Benioff Children’s Hospitals.
Wednesday’s focus is on helping military veterans transition back into civilian life. More than 200 vets will join us at Dreamforce for hands-on Salesforce training and networking with fellow vets and potential employers. And on Thursday, we’re encouraging other companies to take the 1–1–1 pledge and give back to their communities just as tech leaders like Box, Google, and Yelp have already done.
In the end, Dreamforce isn’t about us — it’s about you. It is our chance to create a transformative experience for our customers, partners, and Salesforce communities. We hope you will be inspired, create new connections, gain practical knowledge, give back to the community and, of course, have an amazing time.
I look forward to seeing you at Dreamforce ’14! And if you can’t make it to San Francisco, be sure to join us online at the Dreamforce LIVE stream (salesforce.com/live).
Lynn Vojvodich is Executive Vice President and Chief Marketing Officer at salesforce.com. This post originally appeared on salesforce.com.
Four days of innovation, fun, and giving back
17 
17 claps
17 
Four days of innovation, fun, and giving back
Written by
Connect to Your Customers in a Whole New Way
Four days of innovation, fun, and giving back
"
https://medium.com/@womenwhocode/a-cloud-guru-partners-with-wwcode-to-advance-more-women-into-cloud-computing-e089ffa919dc?source=search_post---------61,"Sign in
There are currently no responses for this story.
Be the first to respond.
Women Who Code
Dec 13, 2017·2 min read
The Disruptive Cloud Computing Education Tech Startup to Provide Training Resources and Scholarships to Members of World’s Largest Women in Tech Nonprofit
San Francisco — October 3rd, 2017 — A Cloud Guru and Women Who Code (WWCode) announce a partnership to provide more women access to Amazon Web Services (AWS) education and certification training, which offers pathways into high-demand careers in cloud computing.
The partnership supports women in technology by providing opportunities for participation in A Cloud Guru events and scholarships for online training. Over $25,000 in scholarships were awarded to WWCode to attend A Cloud Guru’s premier cloud computing conference, ServerlessConf, being held October 8 -11 in NYC. As an official WWCode training partner, A Cloud Guru is also providing members full and partial educational scholarships to their leading online cloud computing education and AWS certification training.
Drew Firment, Managing Partner at A Cloud Guru, spoke about the partnership, “The technology industry needs more women leaders of tomorrow. With so many cloud computing job openings, the time is now to level the playing field in tech. By empowering others to achieve their career goals, WWCode is having a dramatic impact on the effort to improve diversity and inclusion.”
Women Who Code’s Vice President of Partnerships and Growth, Jennifer Tacheff said, “The generosity and support of our sponsors and partners is vital to the work that Women Who Code is doing. It allows us to make opportunities available to our members, provide services that they might not otherwise have access to, and continue to celebrate and inspire them to change the world.”
About A Cloud Guru
A Cloud Guru delivers cloud computing training to over 250,000 students across 160 countries and 6 continents. Our platform was developed by experts in building scalable cloud solutions that don’t rely on servers or traditional infrastructure. To learn more about A Cloud Guru, visit https://acloud.guru or like us on Facebook: https://www.facebook.com/acloudguru or follow us on Twitter @acloudguru.
MEDIA: Danielle Bechtel — danielle@acloud.guru | 804.263.3852 | https://acloud.guru
Originally published at www.womenwhocode.com.
We are a 501(c)(3) non-profit organization dedicated to inspiring women to excel in technology careers. https://www.womenwhocode.com/
239 
239 
239 
We are a 501(c)(3) non-profit organization dedicated to inspiring women to excel in technology careers. https://www.womenwhocode.com/
"
https://medium.com/iex-ec/a-new-era-of-cloud-computing-580bbaf1041d?source=search_post---------62,"There are currently no responses for this story.
Be the first to respond.
iExec is a new decentralized cloud computing platform that is blockchain-based. It aims to tackle the current limitations of centralized cloud computing that are holding business and innovation back. A decentralized cloud connects users to one another, whether server providers, application providers, data providers or end users. All manner of business can be executed on the iExec cloud. At the same time the system is fast, efficient and secure.
‘Having a decentralized and globally accessible marketplace to trade computing resources is an invaluable step forward for the cloud computing industry as a whole’
Despite the vast scale of computing resources available to enterprise, the demand for more powerful solutions continues to increase. iExec combines blockchain technology with distributed computing so that consumers and enterprises from all over the world will be able to pool together computing resources, as well as exchange services. Rather than forcing businesses to invest in more expensive infrastructure, iExec provides on-demand, low cost computing resources. Anyone who needs to complete a demanding task, such as analysing large amount of data, performing compute intensive simulations, orrunning next AI algorithm, will be able to use additional computer resources whenever the need arises.
Affordable High Performance Computing For Everyone
iExec uses blockchain to open up access to unlimited computing resources in a safe manner. Cloud users will be able to buy as many or as little resources as they need from one another, depending on the project they are working on. This is made possible thanks to the Ethereum blockchain that will underpin the iExec network, allowing users to monetise their contributions. The iExec platform will enable a new marketplace to grow until its capabilities will be accessible by every company, everywhere, at an affordable price.
iExec’s cloud removes a central broker or distributor. With traditional cloud providers there are central gatekeepers to the data and services they provide. In contrast, iExec decentralizes the entire business of cloud computing to make sure fair and full access can be provided for everyone. On the iExec platform anyone, anywhere can use the computing resources they rent for their own projects, without having to force or approve the process. This effectively removes all barriers to entry when it comes to using the network.
An Emerging Business tool
iExec addresses the cost and security concerns of cloud computing among enterprise users. With a decentralized solution such as iExec, users will always retain control over their data. Moreover, the platform addresses any scalability and reliability concern clients may have, as there is is no centralized infrastructure to deal with. This also eliminates centralised points of failure.
Businesses also find cloud computing rather expensive but iExec’s highly competitive solution aims to make computing resources affordable for everyone. Since everyone in the world can rent their services through the iExec network, the costs are driven down significantly. This leads to better energy efficiency for everyone who makes use of this blockchain-based cloud computing platform. This has caught the attention of eco-friendly server providers Stimergy who will be the first service to operate on the iExec platform.
A Tool for the Emerging Decentralized Economy
Several industries will reap the rewards from this decentralized blockchain-based cloud computing platform. The Internet of Things, which aims to connect billions of consumer devices to the internet, will eventually show a growing demand for distributed computing. Through the iExec decentralized platform, access to distributed computing will only be a few clicks away. Moreover, participants can choose to rent resources nearest to their location for an optimal user experience. In fact, IoT devices will be able to trade resources between one another in real-time.
Decentralized applications represent another industry that will benefit from hassle-free cloud computing. Decentralized applications, known as DApps, are often created on top of the Ethereum blockchain which also powers the iExec service. At the moment, the Ethereum blockchain offers very few capacity to the Dapps for their execution: storage is limited to few Bytes, and the computation is very slow and expensive. All of these applications require significant computing resources and data, both of which will be accommodated by the iExec platform. By allowing Dapps to perform off chain computations iExec will considerably extend the application domain of the Dapps. We can now imagine Dapps in any domain of IT industry: artificial intelligence, machine learning etc.
iExec combines blockchain technology and smart contracts to ensure renting computing resources is fully automated and transparent. Both technologies rely on peer-to-peer connectivity, which effectively eliminates any intermediaries. Moreover, this technology also ensures computation is located close to the source of where data is generated. Having a decentralized and globally accessible marketplace to trade computing resources is an invaluable step forward for the cloud computing industry as a whole.
Funding The iExec Platform Through A Crowdsale
As one would expect, developing such a powerful distributed cloud computing platform is not cheap. iExec will host a crowdsale to raise the necessary funds for developing the platform. Participants in the crowdsale will receive RLC tokens, the native currency of the iExec platform. The crowdsale launches on April 19th, 2017 at 13:00 PM UTC. The team aims to raise a minimum of USD $2m to cover the costs of developing and bringing to market the blockchain-based cloud computing platform. Contributions can be made with Bitcoin (BTC) and Ethereum tokens (ETH).
Many thanks to Jean-Pierre Buntinx and freya stevens for this article!
Blockchain-based Decentralized Marketplace for Computing Assets
193 
1
Thanks to Gilles Fedak. 
193 claps
193 
1
Written by
CEO and co-founder of Āto | Founder of Strat | Volonteer at Emmaüs Connect | Contrib at Concord
Blockchain-based Decentralized Marketplace for Computing Assets
Written by
CEO and co-founder of Āto | Founder of Strat | Volonteer at Emmaüs Connect | Contrib at Concord
Blockchain-based Decentralized Marketplace for Computing Assets
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@nutanix/the-rise-of-anything-as-a-service-xaas-the-new-hulk-of-cloud-computing-5eca37c2ff02?source=search_post---------63,"Sign in
There are currently no responses for this story.
Be the first to respond.
Nutanix
Apr 26, 2017·3 min read
Cloud Computing, as we see it today, has seen a tremendous evolution in the service segments — right from the dawn of Software-as-a-Service (SaaS) to Infrastructure-as-a-Service (IaaS) and Platform-as-a-Service (PaaS), and now Anything-as-a-Service (XaaS).
Analysts forecasted that the global XaaS market will grow at a CAGR of 38.22% between 2016–2020. Besides the typical SaaS, IaaS, and PaaS offerings discussed, there are other ‘As-a-Service(aaS)’ offerings too. For instance, Database-as-a-Service, Storage-as-a-Service, Windows-as-a-Service, and even Malware-as-a-Service.
No doubt the ‘Cloud-driven aaS’ era is clearly upon us and cloud computing remains the top catalyst for all these services’ growth. The converse holds true too.
In the words of Amarkant Singh, Head of Product, Botmetric, “The persuasive wave of cloud computing is affecting every industry and every vertical we can think of. Thanks to all of its fundamental models — IaaS, PaaS, and SaaS plus the latest XaaS, cloud has brought in democratization of infrastructure for businesses. Talking about XaaS. It is the new hulk of the cloud computing and is ushering in more of ready-made, do-it-yourself components and drag-and-drop development.”
The XaaS model was born as a result of the elasticity that the cloud offers. More so, the XaaS provides an ever-increasing range of solutions that ultimately gives businesses the extreme flexibility to choose exactly what they want tailored for their business, irrespective of size/vertical.
Recently, Stratoscale asked 32 IT experts to share their insights on the differences between IaaS, PaaS and SaaS and compiled an exhaustive Op-Ed report IaaS/PaaS/SaaS: The Good, the Bad and the Ugly[1]. Among these experts, Amarkant too has penned few lines for the report.
Here are the excerpts from the article:
“More companies across the spectrum have gained trust in cloud infrastructure services, pioneered by AWS. While IaaS provides a high degree of control over the cloud infrastructure, it is very-capital intensive and has geographic limitations. On the other hand, PaaS comes with decreased costs but offers limited scalability.
With its roots strongly tied to virtualization, SOA and utility/grid computing, SaaS is gaining more popularity. More so, it is gaining traction due to its scalability, resilience, and cost-effectiveness.
According to a recent survey by IDC, 45% of the budget organizations allocate for IT cloud computing is spent on SaaS.
As organizations move more of their IT infrastructure and operations to the cloud, they are willing to embrace a serverless/NoOps model. This marks the gradual move towards the XaaS model (Anything as a Service), which cannot be ignored.
XaaS is the new hulk of the cloud computing. Born due to elasticity offered by the cloud, XaaS can provide an ever-increasing range of solutions, allowing businesses to choose exactly the solution they want, tailored for their business, irrespective of size/vertical. Additionally, since these services are delivered through either hybrid clouds or one or more of the IaaS/PaaS/SaaS models, XaaS has tremendous potential to lower costs. It can also offer low-risk infrastructure for building a new product or focusing on further innovation. XaaS embracement has already gained traction, so the day is not far when XaaS will be the new norm. But at the end of the day, it all matters on how cloud-ready a company is for XaaS adoption.”
Each expert has an idiosyncratic perspective to what, where, when, and why XaaS. For few, it stands for everything-as-a-service and refers to the increasing number of services delivered through cloud over the Internet. For few it is anything-as-a-service. Techopedia quotes it as a broad category of services related to cloud computing and remote access where businesses can cut costs and get specific kinds of personal resources. Different perspective, different views, but one goal: Putting cloud in perspective.
Read what other experts are deliberating on XaaS on Stratoscale’s Op-Ed article ‘IaaS/PaaS/SaaS — the Good, the Bad and the Ugly.’[1]
Share your thoughts in the comment section below or give us a shout out on either Facebook, Twitter, or LinkedIn. We would love to hear what’s your take on XaaS.
[1] Stratoscale, 2017, “IaaS/PaaS/SaaS — the Good, the Bad and the Ugly.”
We make infrastructure invisible, elevating IT to focus on the applications and services that power their business.
21 
1
21 
21 
1
We make infrastructure invisible, elevating IT to focus on the applications and services that power their business.
"
https://medium.com/design-ibm/ibm-design-soars-in-cloud-computing-competition-a456d56e8739?source=search_post---------64,"There are currently no responses for this story.
Be the first to respond.
IBM Virtual Private Cloud Infrastructure with Interconnectivity and Red Hat Marketplace recognized as 2021 Stratus Award Winners
I’m thrilled to announce that two IBM products were recognized for their experience excellence in Cloud Computing. IBM Virtual Private Cloud (VPC) Infrastructure with Interconnectivity and Red Hat Marketplace, operated by IBM, both received a 2021 Stratus Award. IBM VPC Infrastructure with Interconnectivity won the Infrastructure as a Service Category award and Red Hat Marketplace won under the Cloud Disruptor Category.
The jury panel for the 2021 Stratus Awards, conducted by Business Intelligence Group, is comprised of volunteer industry experts. The panel selects companies that have differentiated themselves as leaders in the cloud space. It’s exciting that out of all the cloud technology currently on the market, two IBM products were recognized.
“We now rely on the cloud for everything from entertainment to productivity, so we are proud to recognize all of our winners. Each and every one is helping in their own way to make our lives richer everyday. We are honored and proud to reward these leaders in business”, said Maria Jimenez, Chief Nominations Officer of Business Intelligence Group.
I’m so impressed by the dedication I’ve seen each design team put into creating these exceptional user experiences. This increase in demand for cloud has made IBM’s products even more indispensable. Our design team’s commitment to IBM’s Enterprise Design Thinking framework and thorough User Research has led them to craft industry-leading products.
The global pandemic and shift to remote work illuminated the benefits for companies to adopt cloud technologies. While the desire to make the shift has grown, there are a few common obstacles facing customers.
Through rigorous user research, the IBM Design team for Red Hat Marketplace was able to gain insight into the problems many customers are facing while shifting over to cloud. Many users are forced to work with outdated ecosystems, which are running on a specific cloud platform, rather than working across diverse cloud environments. These customers are also commonly locked-in to a particular vendor, making it expensive to switch and even harder to innovate. Once customers do find a cloud solution, monitoring their IT costs and return on investment is an incredibly manual process.
The Red Hat Marketplace design team responded to the increased demand for cloud technology by fundamentally changing the way enterprise cloud software is bought. Leveraging IBM’s Enterprise Design Thinking framework, the design team was able to create an all-in-one platform experience. Customers are now able to try, buy, and deploy container-based software on the cloud of their choosing: public, private, on-premise, and even the clouds of IBM’s own competitors.
With a robust catalog of third-party cloud software and transparent cost management capabilities, the Marketplace alleviates the common obstacles consumers run into while supporting the demand for a smooth shift into cloud-based technology. A persistent dedication to improving the user’s experience allowed our design team to make what used to be a frustrating and inefficient process into an easy online shopping experience.
While Red Hat Marketplace supports an increased demand to convert to cloud technologies, the IBM Design team for IBM Virtual Private Cloud product suite responded with innovative product updates to meet user needs related to virtual servers.
The design team met with clients, tracked analytics, and conducted user testing. This in-depth research allowed the them to better understand the product suite’s current capabilities, which led to the insight that our users were seeking less friction in their ordering experience and better ease of use regardless of their starting point.
Over the past year, the team responded by redesigning the user experience to delight both a first time single user and massive enterprise scale clients. They’ve updated IBM’s Virtual Private Cloud product suite’s digital presence to make first time use exceptionally easy. They created smart defaults for networking to get new users up and running their first virtual machine as quickly as possible and reduced the complexity of a first time experience by adding guided tours for new users.
Acknowledging the user’s needs and anticipating the best response to any other future pain points have led to an increase in customer success and ease of use. Upon experiencing the updated VPC suite, a user remarked at the success of their new experience.
“I have to say congratulations because I think you have developed an interface that is very easy to follow & understand which is something hard to come across with these cloud providers. Sometimes these things get too technical and they seem to forget the essence of what a good interface is. An interface should be good for anyone, not just someone with an engineering degree.”- IBM’s Virtual Private Cloud User
All of these features have been designed for maximum ease of use, transforming what was once a painful series of calls, emails, and support tickets into an automated system built to ensure user delight.
Red Hat Marketplace, Operated by IBM: Shelby Aranyi, Amina El-Ashry, Eleanor Bartosh, Shannon DeCock, Bethany Doan, Neil Everette, Justin Gier, Sarah Hardison, Charlie Hill, Ashley Johnson, Danielle Justilien, Corey Keller, Chase Kettl, Willow Lafone, Scott McCall, Alisha Moore, Matthew Reinhold, Aaron Sickler, Robert Uthe, Sarah Walter
Thank you to the contributors from the Red Hat team: Nick Burns, Dan Caryll, Mike Esser, Libby Levi,
Virtual Private Cloud Infrastructure With Interconnectivity: Philip Begel, Alissa Chan, Stephanie Cree, Austin Edwards, Isabelle Encela, Bill Grady, Lisa Kaiser, Elijah King, Tracey King, Jenny Lanier, JP Pollard, Ty Tyner, Michelle Yang
Arin Bhowmick (@arinbhowmick) is Vice President and Chief Design Officer, IBM Products, based in San Francisco, California. The above article is personal and does not necessarily represent IBM’s positions, strategies or opinions.
Stories from the practice of design at IBM
127 
2
127 claps
127 
2
Stories from the practice of design at IBM
Written by
Global VP Design | Chief Design Officer, @IBM |Cloud, Data and AI I UX Leadership| UX Strategy| Usability & User Research| Product Design
Stories from the practice of design at IBM
"
https://medium.com/bit-country/phalas-metaverse-its-web3-0-cloud-computing-dd46fcf14488?source=search_post---------65,"There are currently no responses for this story.
Be the first to respond.
EXCITING ANNOUNCEMENT
Phala and Bit.Country & Metaverse Network Announce their NEW Partnership. We believe that Phala’s endorsement along with its next-generation cloud service to the Web and beyond as well as computing power bar none will help sustain and protect the vision of Bit.Country & Metaverse Network of a decentralized, user-friendly, immersive metaverse experience.
The Bit.Country & Metaverse Network is delighted to announce its official partnership with Phala, an alma mater of the Polkadot cross-chain ecosystem, and innovative creators of the most powerful cloud computing network.
Metaverse will become the best practice of XCM based on Polkadot’s interoperability.
Phala will not only build the Phala World metaverse on Bit.Country for their community, but also work with Ray Lu’s innovative team to establish a game mode that goes beyond the metaverse as we know it.
*More to be announced in the near future.
Phala’s powerful Web3.0 computational power will provide game server and derivative services in the truly decentralized multi-metaverse.” — quote from Phala
We look forward to working with Phala and providing the best performing metaverse gaming and ultimate customer metaverse experience while everything is owned and governed by users.
Quote from Ray, CEO & Co-Founder of Bit.Country Metaverse Network — “I am excited to see the biggest metaverse developers are coming in to add value across metaverses and provide web3.0 infrastructure to ensure web3 paradigm is executed.”
Phala will bring immense computing power to Bit.Country while protecting its data layer, and allowing for privacy-protected DeFi trading positions and more.
Phala acts as a go-between, with the blockchain keeping transactions, between users and computing operations, in order, and ensuring privacy as well as security whilst allowing for massive trustless data exchanges between parties.
“Since 2019, Phala has firmly believed in the importance of Web3 computing facilities. The emergence of the metaverse has allowed us to see the infinite possibilities of the crypto world.
The next generation of operating systems will be born from platforms like Bit.Country & Metaverse Network.
Phala Network tackles the issue of trust in the computation cloud.
This blockchain is a trustless computation platform that enables massive cloud processing without sacrificing data confidentiality. Built around TEE-based privacy technology already embedded into modern processors, Phala Network’s distributed computing cloud is versatile and confidential. By separating the consensus mechanism from computation, Phala ensures processing power is highly scalable. Together, this creates the infrastructure for a powerful, secure, and scalable trustless computing cloud.
As a member parachain of the Polkadot cross-chain ecosystem, Phala will be able to provide computing power to other blockchain applications while protecting the data layer, enabling possibilities like privacy-protected DeFi trading positions and transaction history, co-computing DID confidential data, developing light-node cross-chain bridges, and more.
On-chain services currently being developed on Phala Network include Web3 Analytics: high-performance smart contracts from Phala enable highly concurrent mass data analytics with privacy, paving the way for an alternative to Google Analytics that inherently respects individual confidentiality.
🍽 — Subscribe Us | Website | Twitter | Github🥤 — Discord | Forum | Telegram
Bit.Country & Metaverse Network is an application framework and blockchain for user-created metaverses and games. Bit.Country enables everyone to start their own metaverse for their social groups using the 3D world, NFTs, play-to-earn & build communities to earn, and takes community engagement to a new decentralized dimension on Web3.0.
Bit.Country & Metaverse Network is disrupting the current creators’ value system built by web2.0, as traditional social platforms are harvesting the value from the community’s time, only distributing a portion of the profits to the creators while members are only being entertained, but not rewarded.
Bit.Country Pioneer Network (Application & Blockchain for Kusama), the pioneering network is the sister and canary platform for Kusama. Just as Bit.Country will serve as the de-facto metaverse portal of Polkadot, Bit.Country Pioneer will serve as the metaverse portal of Kusama.
Need more information about Bit.Country Metaverse Network？
Website | Twitter | Medium | LinkedIn | Facebook | Telegram | Discord | Documentation
Start your own metaverse
210 

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
210 claps
210 
Written by
Start your own metaverse for your people.
Bit.Country & Metaverse.Network is a platform & blockchain ecosystem for user-created metaverses, games, and dApps. Community owners, influencers, KOLs, and individuals can launch their own metaverses for their members, fans, followers, families, and friends.
Written by
Start your own metaverse for your people.
Bit.Country & Metaverse.Network is a platform & blockchain ecosystem for user-created metaverses, games, and dApps. Community owners, influencers, KOLs, and individuals can launch their own metaverses for their members, fans, followers, families, and friends.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@highwayman1991/cloud-computing-in-a-modern-world-distributed-cloud-computing-by-hivenet-f6f2f96294d5?source=search_post---------66,"Sign in
There are currently no responses for this story.
Be the first to respond.
Алексей Филатов
Sep 19, 2019·4 min read
The Internet and information technology play a great role in modern human’s life. Today we can observe a new era in the development of social media where we deal with a variety of electronic devices, big storage capacities and high speed of information response. There is an important innovative in this world of new technologies which is called cloud computing. It can bring absolutely new advantages to people from different professional groups.
Cloud computing represents a system which serves as an internet-based information centre. The customers can get an access to all kind of files and software safely through some different devices. This opportunity doesn’t depend on the location of customers: they can use it from any place on Earth where it possible to have an access to Wi-Fi.
Cloud computing is an excellent instrument for organizations and individuals seeking an easy way to store and access media from one device to another. It enables people to run software programs without installing and develop and test programs without necessarily having servers. And it has a wide list of advantages for business projects. Let’s consider them in a more detailed way.
There are several positive features in the usage of cloud computing in business. Here are some of them.
Global access. Thanks to a remote server, employees will be able to share files and calendars with ease. Such opportunities help to improve planning and time management within your company.
Comfortable data sharing. Thanks to a remote cluster of servers, you won’t get lost important information. If there will be mistakes with your concrete files or all of your data, it will be able to be sent to another device.
Economic solutions. Cloud computing allows you to avoid additional expenses for licensing fees, data storage costs, payments for software updates and salary for special managers.
Safety and security. Databases in cloud computing are difficult to penetrate for hackers. And developers strive to find new techniques to protect the system from hackers better.
Large storage. In comparison with a personal computer, cloud computing gives your company an opportunity to store much more information.
Device independence. You can get an access to cloud computing software everywhere and anywhere. You just need to have a device and an access to Wi-Fi.
HiveNet represents a Distributed Cloud Computing Network. It connects devices from different countries in order to perform valuable computing tasks. HiveNet helps customers and computer owners to come into contacts with each other.
HiveNet is responsible for task assignment and validation and fair payment distribution. Computer owners offer computing power to customers and receive payments from them. Let’s see what advantages different sides in this project have.
Computer owners. Usual people can earn during using computer. Some profit will be got for the fulfillment of simple tasks by your computer’s power. You just need a device with an internet connection. So, this is a real perspective way to get passive income that doesn’t require special abilities from you. And you can get profit even when you sleep.
Customers. HiveNet allows to use already available computers, so there is no need for companies to spend money on new devices, housing, etc. That’s why this instrument is much cheaper in comparison with usual cloud computing.
Crypto traders. People from this group will have nice short- and long-term opportunities thanks to using HiveCoin. These tokens are used for the payments for computing power within the HiveNet.
Nature. Thanks to the increase of the usage of available devices, there will be no need to build many new ones. That’s why there will be less electronic garbage. And it will be better for environment.
The project team consists of specialists in different fields. You can get to know more about them here.
Presale of tokens will be started on 24th September, so you will have a chance to purchase perspective HiveCoins for good value. You will be able to buy tokens for the cost 0.06 USD/HNT from 24th September to 7th October. Then the price will have risen gradually. With the start of IEO / public sale, tokens will be sold for the cost 0.08 USD/HNT.
Here are the links to resources where you will be able to know more about the project:
Website: https://www.hivenet.cloud/
White Paper and Light Paper: https://www.hivenet.cloud/resources
Facebook: https://www.facebook.com/HiveNetCloud
Twitter: https://twitter.com/HiveNetCloud
LinkedIn: https://www.linkedin.com/company/hivenetcloud
Medium: https://medium.com/official-hivenet-blog
Youtube: https://www.youtube.com/c/HiveNet
Telegram: https://t.me/hivenet
The article was written and published by Arcadio707 (https://bitcointalk.org/index.php?action=profile;u=2426003)
Crypto enthusiast
213 
213 
213 
Crypto enthusiast
"
https://medium.com/syncedreview/cambricon-unveils-its-first-ai-chip-for-cloud-computing-d3f7acdb4076?source=search_post---------67,"There are currently no responses for this story.
Be the first to respond.
Cambricon today unveiled its Cambricon 1M chip for edge computing, and the MLU100, the first in a new chip series for cloud computing.
Cambricon 1M is the company’s third generation AI chip for edge devices. With its TSMC 7nm technology, the AI chip provides efficiency of 5 TOPS/Watt for 8-bit computing. The 1M chip is available in 2, 4, and 8 TOPS versions to support a range of AI applications.
Like the company’s 1H and 1A chips, Cambricon 1M supports deep learning models such as CNN, RNN, SOM; and supports SVM, k-NN, k-Means, decision tree and other algorithms. This is also the world’s first AI processor supporting local machine learning training.
Cambricon MLU100 is the first generation of Cambricon’s new product series supporting cloud computing. MLU100 adopts Cambricon’s latest MLUv01 architecture and TSMC 16nm technology. This processor has the capability of providing 128 TFLOPS/166.4 TFLOPS in balance mode and high-performance mode respectively.
Cambricon and its partners demonstrated several use cases: Lenovo’s new ThinkSystem SR650 server is based on the MLU100, as is Sugon’s new PHANERON series of products. iFlytek has also announced a collaboration with Cambricon.
* * *
Author: Victor Lu| Editor: Tony Peng, Michael Sarazen
* * *
* * *
The ATEC Artificial Intelligence Competition is a fintech algorithm competition hosted by Ant Financial for top global data algorithm developers. It focuses on highly-important industry-level fintech issues, provides a prize pool worth millions. Register now!
We produce professional, authoritative, and…
45 
45 claps
45 
We produce professional, authoritative, and thought-provoking content relating to artificial intelligence, machine intelligence, emerging technologies and industrial insights.
Written by
AI Technology & Industry Review — syncedreview.com | Newsletter: http://bit.ly/2IYL6Y2 | Share My Research http://bit.ly/2TrUPMI | Twitter: @Synced_Global
We produce professional, authoritative, and thought-provoking content relating to artificial intelligence, machine intelligence, emerging technologies and industrial insights.
"
https://medium.com/tomochain/tomochain-threefold-joining-forces-to-decentralize-cloud-computing-tomochain-masternodes-12004bfdf14f?source=search_post---------68,"There are currently no responses for this story.
Be the first to respond.
The ThreeFold Grid’s peer-to-peer nature comes with many advantages for blockchains as it truly decentralizes their IT infrastructures and, enabling better performance, security, affordability, and sustainability, as compared to current centralized cloud alternatives.
TomoChain will invite their developer community and anyone interested in contributing to the TomoChain Blockchain, to connect and use the Threefold Grid. By deploying their Masternodes and workloads on the ThreeFold Grid, their developers will benefit from a more decentralized and affordable cloud offering and benefit from the autonomous execution of their workloads via ThreeFold’s Smart Contract for IT.
Anyone interested in being part of the TomoChain node ecosystem using the ThreeFold Grid would be highly encouraged to get started in a few steps following the documentation.
Furthermore, with the recent announcement of TomoChain’s DEX Protocol, TomoX, released, both organizations will also test the ThreeFold P2P Cloud solution to enable developers to build DEXs on top of TomoChain using ThreeFold’s truly decentralized IT infrastructure.
ThreeFold has developed the largest and most advanced peer-to-peer Internet grid in the world. They launched the ThreeFold Grid V2.0 in April 2020 — a more performant, private & secure, affordable, and sustainable solution than centralized cloud and internet providers.
With a collective and incentivized ecosystem of Farmers (independent Internet capacity providers) around the globe, the ThreeFold Grid is now open for developers to build on top of it, and users can now benefit from all the advantages of the ThreeFold Grid.
The ThreeFold Foundation aims to empower a more equal, autonomous (decentralized) and sustainable world, and does so by providing the necessary peer-to-peer technologies and infrastructure to the world, while incentivizing growth in developing regions.
TomoChain is a scalable blockchain-powered via Proof-of-Stake Voting consensus and used commercially by companies globally.
Our mission is to accelerate the onboarding of millions of users by empowering today’s applications with technology that masks the friction of Blockchain, all while retaining its underlying benefits.
TomoChain’s technology and DeFi-focused flagship products include:
Find us on
Website: tomochain.com
Telegram: t.me/tomochain
Twitter: twitter.com/TomoChainANN
Welcome to TomoChain blog.
316 
316 claps
316 
Welcome to TomoChain blog. Follow us to get the official news and good reads from TOMO team. Clap along if you feel like a room without a roof!
Written by
Work @ tomochain.com
Welcome to TomoChain blog. Follow us to get the official news and good reads from TOMO team. Clap along if you feel like a room without a roof!
"
https://medium.com/@fedakv/cloud-computing-architecture-layers-of-cloud-pyramid-b2e70f877c91?source=search_post---------69,"Sign in
There are currently no responses for this story.
Be the first to respond.
Vladimir Fedak
Jul 19, 2019·6 min read
Nowadays cloud computing services are no longer only a trend but a part of everyday life. We use them everywhere: when checking our emails and social media in the morning, using banking applications, eLearning platforms, etc. You can start to watch a video on your laptop and continue on your phone. All these things are available because of cloud computing.
The majority of people use it but not everyone knows what it is and how it works.
Today we will describe what is cloud computing, what does it consist of and what it can offer for your business.
We can simply describe cloud computing as interaction with different configurable services (virtual machines, data storage, servers, etc.) via the Internet. All services could be quickly provided and decommissioned without big effort.
A lot of people perceive cloud computing and data centers as similar things. If you have a business you might hear you need something of the listed. So are they alike or not?
Actually, both cloud and dedicated data centers serve and storage a big amount of data. Common features end. What’s the difference?
If you want to reduce the expenses of maintaining servers in on-prem data centers, you can rent the equipment from dedicated data centers, which are placed in different places on different distances. When you need to send some data or launch the application, the signal will move through different points in the different cities and countries. Until it gets to the finish, you might lose time and resources. Also, don’t forget that the data center equipment has limited capacity. This is one of the crucial moments when we are talking about large numbers of users. The more users you have, the greater the response delay will be.
Opposite to that, cloud computing allows you to use almost unlimited recourses. Amazon Web Services Availability Zones (AWS AZ), for example, cover a huge territory around AWS data centers. This allows to minimize latency remarkably. In this case, you do not need to wait for the signal to hop to the remote data center. The route to the nearest cloud data center will be created automatically and it will be as short as it can be.
As you can see, the cloud provides high-availability, while data centers can’t do the same things.
For a better understanding of cloud computing structure imagine a pyramid consisting of three layers:
If we imagine cloud structure as a pyramid, we can call SaaS the pyramid head. This is also called “on-demand software”, as end users have ready-to-use software. The Managed Service Provider (MSP) manages all the services, users have only access to the software via the website or mobile application.
The common benefit of SaaS is no installation, maintenance, update and other costs. Users pay only for usage and do not need to worry about all the technical moments. The payment model in SaaS looks like renting — you pay for software only if you use it. Thus, you can pause usage and do not overpay for this period.
SaaS model provides the next features:
PaaS is a central layer of the cloud architecture pyramid. PaaS is a model that provides access to cloud-based platforms like operating systems, database management systems, instruments for software development and testing. Provider in SaaS delivers maintenance for all the IT infrastructure (servers, networks, databases) and decides what resources to provide. The payment n PaaS depends on the level of usage. You might pay for the time of use, the volume of processed information or network traffic.
PaaS model provides the next features:
Infrastructure-as-a-Service is the bottom layer of the cloud architecture pyramid, it provides access to computing resources. The provider handles administration and hardware issues. All the settings of the operating system and application the customer has to manage on his own.
IaaS model provides the next features:
There is a well-known example of cloud infrastructure and all layers — Pizza as a Service. To ensure the complete understanding of the whole pyramid, let’s have a look at this example.
SaaS is a pizza in the restaurant. You have all the services — served table, menu with different pizzas, waiter and pizza itself and you don’t need to cook or prepare something on your own. It looks like a list of applications you can choose, the application itself and technical support.
PaaS is like pizza delivery. You have a menu, you can choose and receive your pizza, but the table and service are up to you. The same works with software — you have the environment for software development.
IaaS, in this case, is homemade pizza — you are supplied with products and need to cook and serve the pizza. In the development area, you have instruments and can make your own software with them. Your instruments are computing resources and access to the hardware, they allow you to make any “recipe” you need.
As you can see, the architecture of cloud computing is quite simple for understanding and can be used by different customers with different needs. In addition to the described pyramid, you can make your cloud private, public or hybrid. It means, you can make cloud only for your company or use one of the public clouds, or combine both approaches.
Cloud computing, in general, provides a huge level of customization. You can use the ready application with technical support from the provider and without needing to access the software source code or vice versa, have access to the settings and customize the environment for your needs, or even make your own environment and software with access to the necessary resources.
Resources in the cloud are delivered by Managed Service Providers, which can support your cloud projects, implement cloud computing to your company, make cloud strategy, etc. Cloud computing can propose for your business high-availability, flexibility, scalability and, of course, cost reduction. In the cloud, you pay by subscription, which you can cancel if you need, or by Pay-As-You-Go model, where you pay only for used supplies.
Scalability allows you to increase and lower the allocated resources, as a result, you have a very stable but flexible system.
Thus, cloud computing can upgrade your business or startup and make it more competitive. You just need to choose the right pyramid layer and strategy. If you have doubts about what’s better for your project, refer to the MSP which always can give some good advice.
DevOps & Big Data lover
See all (63)
15 
1
15 claps
15 
1
DevOps & Big Data lover
About
Write
Help
Legal
Get the Medium app
"
https://scientya.com/combining-cloud-computing-and-artificial-intelligence-ai-9db02226c7e3?source=search_post---------70,"There are currently no responses for this story.
Be the first to respond.
Originally published on LinkedIn in November 2017
Today’s technological progress and the wealth of modern economies have reached an impressive level. While in the early period of human history, the pace of technological progress was relatively slow, nowadays time to market and being state-of-the-art are key factors for competitiveness. Consequently, companies have to be quick and predict successfully the development of new and old technologies. Amazon did very well to concentrate on the development of their cloud services in 2006 already. Today, the cloud computing world is dominated by Amazon AWS who enjoy a seemingly monopolistic market share of over 30 percent. A serious competition only comes from Microsoft Azure, Google, IBM and Alibaba. According to Forrester’s predictions for 2018, the total global public cloud market will be $178B in 2018, up from $146B in 2017, and will continue to grow at a 22% compound annual growth rate. From this point of view, the cloud seems to be a driving force for economic growth.
Artificial intelligence will become a factor of production
However, a recent study by Accenture drew attention to the role of Artificial Intelligence (AI). The latter is believed capable to double the gross domestic product growth of many developed economies by 2035. The findings of this study are extremely impressive and will certainly exert pressure on every future-oriented decision maker. As Accenture’s figure beneath shows, AI will become a factor of production.
A conference on Artificial Intelligence in Strasbourg by Frederic Wickert, Technical Evangelist at Microsoft, came as if called. This conference was organised by Hacking Health and Med’Advice on 15th November 2017. Med’Advice is a junior enterprise specialised in the pharmacy sector. Hacking Health on the other hand works together with all actors of the health sector (entrepreneurs, companies, SMEs etc.) and organises around 300 conferences, 10 workshops and 50 projects with over 400 participants. The objective is to bring together digitization and health industry.
With the experience of over 10 years as developer at Microsoft, Frederic Wickert works on the development of cloud software solutions and now more and more on artificial intelligence. As he points out, the fear towards artificial intelligence always is related to the fear of men to being replaced by robots. However, we are at the very beginning of artificial intelligence. Frederic Wickert underlines that the fear towards AI is not justified. The reality is that we are not aware of the importance and advantages of AI. In fact, AI is not coming to replace anybody.
What exactly is artificial intelligence and what is meant by “artificial”?
First of all, we have to define what is „intelligence“ before understanding AI. Different forms of human intelligence exist (interpersonal intelligence, intrapersonal, spiritual, existential intelligence etc.). Intelligence is not just human, it is „living” (linguistic intelligence, collective intelligence etc.) Artificial intelligence also consists in adapting. Every system that is able of adapting to give a perfect reply is „intelligent“.
Different types and levels of AI have to be taken into consideration. The first level is known as Artificial Superior Intelligence (ASI). The second level is the level of Artificial General Intelligence (AGI) known to be a general system without any expert capabilities. When it comes to the third level of AI, we approach human intelligence. At this point, we speak of Artificial Narrow Intelligence (ANI). Today’s AI systems are situated between AGI and ANI according to Wickert.
Why AI today?
In fact, AI has existed for long time. What changed over time is the definition. The first time AI appeared as a term was in 1950. The reason why we speak more and more about AI is because of Data, Cloud and Intelligence/Algorithm. A modern application has to take into consideration these three as Wickert points out. Systems of Intelligence transform products, engage customers, optimize operations and empower employees. Therefore, AI is present everywhere across all sectors.
AI and cloud computing?
A strong conclusion we can make from this instructive conference is that cloud computing and artificial intelligence will melt into one another. Frederic Wickert also referred to the role of machine and deep learning in the field of AI. Nowadays, we are used to intelligent assistants like Cortana, Siri, Alexa and Google Assistant. Gary Eastwood from the IDG Contributor Network predicts an AI and cloud fusion in the future. According to him “the many, disparate servers which are part of cloud technology hold the data which an AI can access and use to make decisions and learn things like how to hold a conversation. But as the AI learns this, it can impart this new data back to the cloud, which can thus help other AIs learn as well.” In fact, with the cloud, we reach a calculation power and the capability of treating a number of data and intelligence.
n’cloud.swiss — A serious “Swiss made” alternative to AWS, Azure & Co.
Artificial intelligence plays an increasingly important role at n’cloud.swiss AG. With n’cloud.swiss we have developed a “Swiss made” alternative to the big 5 (Amazon, Microsoft, Google, IBM and Alibaba). The idea behind n’cloud.swiss is to enable customers to design a cloud according to their own taste with the same product, either as a service model, as an ON-PREMISE application in existing IT environments or even as a hybrid variant. The ON-PREM-VERSION can be used centrally or as a multi-cloud variant in a satellite network for large companies. Moreover it is compatible with all leading server, storage and / or network manufacturers and can be purchased as a license model with personal support from n’cloud.swiss AG. The customer decides whether the administration of the cloud platform is taken over by n’cloud.swiss AG, through its own IT department or in a mix of both. Flexibility and simplicity in managing all workloads and resources, via a single development and production platform characterises n’cloud.swiss. The main focus hereby is that a cloud platform is freed from all known limitations by hardware vendors.
About n’cloud.swiss AG
As a Swiss cloud computing specialist, n’cloud.swiss AG is one of the cloud pioneers in Europe and was initiated by Netkom IT Services GmbH. Through various advisory mandates and IT projects at and for well-known large companies from home and abroad such as Citibank Switzerland AG, Schindler AG, ABB Turbo Systems and many more, the company developed early as a specialist in IT strategy and its implementation in the operational IT infrastructures and organization of the respective customers. Numerous projects in the classic server / client area for SMEs made it possible to acquire further core competencies in the areas of transformation and digitization.
The digital world publication
258 
1
258 claps
258 
1
Scientya is all about creating and sharing knowledge in the digital world. Switzerland’s fastest growing Medium publication  covers various topics including Blockchain, Artificial Intelligence, Cloud Computing, Health Care, Marketing and more.
Written by
Dynamic and data-driven marketing leader. Author, Mentor & Speaker. Top 50 Global Thought Leader. See www.yahyamohamedmao.com
Scientya is all about creating and sharing knowledge in the digital world. Switzerland’s fastest growing Medium publication  covers various topics including Blockchain, Artificial Intelligence, Cloud Computing, Health Care, Marketing and more.
"
https://medium.com/quick-code/top-tutorials-to-learn-microsoft-azure-for-cloud-computing-258affcd2c5d?source=search_post---------71,"There are currently no responses for this story.
Be the first to respond.
Microsoft Azure is the fastest-growing cloud platform in the world. It is a cloud computing service for building, testing, deploying, and managing applications and services through a global network of Microsoft-managed datacenters.
This course covers :
This course in an introduction to Microsoft Azure services. Students will gain familiarity with core Azure topics and practice implementation of infrastructure components.
In this course, you will learn:
The seemingly infinite number of paths you can take to learn Azure can be overwhelming.
The course includes:
The course focuses on the fundamentals, covering what you — the developer — need to know to start building, deploying, and managing applications using the popular cloud platform.
It provides a thorough look at the capabilities and offerings of the Azure ecosystem, from storage options to Kubernetes Service scaling strategies.
Along the way, you will be provided with information you need to go for a deeper dive on the topics that most interest you.
Whether you need to learn Azure for your job or you’re just curious about how it stacks up against other ecosystems, this course can help you get started.
Analyze your data in the cloud in real time with Azure Stream Analytics. Get insights from data in real time at scale.
The course includes:
This course teaches you how to design, deploy, configure and manage your real time scalable data analytics in the Azure cloud resources with Azure Stream Analytics.
The course will start with basics of ASA and query setup, and then moves deeper into details about ASA and its other integrated services so you can make the most out of the functionalities you have available in this tool.
In this course, you will gain a conceptual understanding of the various services available in Azure, and how to use them in your solutions.
The course includes:
You will learn foundational knowledge to begin planning solutions using Microsoft Azure.
First, you will learn about cloud computing and the different ways to run your application code. Next, you will discover the data storage, processing, and analysis capabilities in Auzre.
Finally, you will explore how to create networks; integrate, manage, and secure your applications; and develop for Azure.
When you’re finished with this course, you will have the skills and knowledge of Microsoft Azure needed to begin working on your cloud solutions.
This entry-level course introduces to Microsoft Azure to learn basic tasks in Microsoft Azure.
You will understand what Azure offers and will also know how to perform basic tasks, such as deploying resource groups, dashboards, virtual machines, network security groups, and virtual networks. The section on PowerShell walks through the process of installing the AzureRM module and hwo to use it to connect to Azure via PowerShell.
In this course learn about how to deploy Azure Resource Groups, deploy Azure dashboards, deploy Azure virtual machines via the portal, deploy Azure virtual machines via powershell, deploy azure virtual networks via the portal, deploy azure virtual networks via PowerShell, deploy network security groups, deploy Public IP Addresses in Azure, deploy firewall rules in Azure.
Learn about the key building blocks of the Microsoft Azure cloud using several theory lectures and lab demonstrations.
In this course:
This course provides an overview of app service building blocks, creating and managing web apps, mobile apps, API apps. Auto scaling of app services, Securing and monitoring of App services. High level overview of Azure notification hub, mobile engagement, content delivery network, media services and Azure search. You will deep dive into following layers of Azure architecture with several hands on exercises.
This course contains both theory lectures and a significant number of hands-on demos that helps you in gaining hands-on experience in key Azure services. This course help you in laying strong basic foundation in preparation of Microsoft Azure certifications 70–532, 70–533, 70–534/5.
Pass your Microsoft DP-203 certification exam — Includes 200 practice questions.
In this course, you will learn:
Here, you will learn about the various Azure services that pertain to Data Engineering. Some of the important aspects that you will learn are the purpose of an Azure Data Lake Gen 2 storage account, the basics of Transact-SQL commands, etc.
Next, you will learn how to work with Azure Synapse. This will include building a data warehouse into a dedicated SQL Pool. Also, you will learn how to build an ETL pipeline with the help of Azure Data Factory. There will be various scenarios on how to create mapping data flows.
Finally, The different security measures and monitoring aspects to consider when working with Azure services.
With this course, know how to implement solutions for the Microsoft Azure platform, pass the Microsoft 70–533 Implementing Microsoft Azure Solutions test, Become Microsoft MCP Certified, understand the main concepts of Azure.
This course goes through all of the skills needed to take and pass the Microsoft certification exam, 70–533: Implementing Microsoft Azure Infrastructure Solutions. This course teaches all of the requirements for this exam, one by one. Each of the things that Microsoft tests for will be covered in this course.
The most complete course on the Microsoft Azure updated developer exam and certification. With this course, pass the Microsoft 70–532 Developing Microsoft Azure Solutions test, master the main concepts of Azure, become Microsoft Specialist: Developing Azure Solutions certified
This course goes through all of the requirements of the Microsoft exam 70–532: Developing Microsoft Azure Solutions. This course will cover each core section in detail. These videos will guide you through the Azure interface, and through your own practice and experience with the platform.
Get Azure certified as an architect in one of Microsoft’s newest certifications on the hottest cloud platform Azure.
In this course:
This course is the complete study preparation course for 70–535: Architecting Microsoft Azure Solutions. This course goes through all of the requirements of the Microsoft exam 70–535. Multiple videos are devoted to each sub-objective, and it covers the topic thoroughly.
Get a comprehensive overview of Azure cloud services in this 9-course bundle
In this course:
In the Introduction to Azure course will introduce you to the Cloud, types of Cloud services, Azure services, and Azure PowerShell.
The Deploying Virtual Machines course is designed to instruct on Azure as it pertains to Virtual Networks, Virtual Machines, and Storage capabilities.
In the Azure Active Directory course, you will gain an understanding of directory service options, use a custom domain, manage users and groups, use multi-factor authentication, work with application access, and add and access applications.
The Deploying Websites course covers Azure as it pertains to Virtual Networks, Virtual Machines, and Storage capabilities. you will learn about PaaS Cloud Services, Understand the deployment environment, and create and configure PaaS Cloud Service.
In the Azure SQL course, you will overview and provision Azure SQL. You will also connect to Azure SQL DB, Migrate DB to Azure, work with SQL security and metrics, Configure SQL DB auditing, copy and export a database, use DB Self-Service Restore, and Use SQL DB Geo-Replication.
The Azure Networking course will teach about IP address space and DNS in Azure VNets, creating a VNet, configuring a point to site and site to site VPN, and VNet peering.
The Azure Storage course will teach about storage accounts and access and using blob storage and file storage, file sync, content delivery network, and backup and recovery.
The Azure Containers course will teach about using Docker, deploying containers, and multi-container applications, container registry, clustering options, and installing ACS.
The Azure Automation and Log Analytics course will teach about creating automation accounts and creating runbooks, creating OMS workspaces and using the Azure Security Center.
Thank you for reading this. We have curated top tutorials on more subjects, you would like to see them:
medium.com
tutorials.botsfloor.com
medium.com
Disclosure: We are affiliated with some of the resources mentioned in this article. We may get a small commission if you buy a course through links on this page. Thank you.
Find the best tutorials and courses for the web, mobile…
19 
19 claps
19 
Find the best tutorials and courses for the web, mobile, chatbot, AR/VR development, database management, data science, web design and cryptocurrency. Practice in JavaScript, Java, Python, R, Android, Swift, Objective-C, React, Node Js, Ember, C++, SQL & more.
Written by
A list of best courses to learn programming, web, mobile, chatbot, AR/VR development, database management, data science, web design and cryptocurrency.
Find the best tutorials and courses for the web, mobile, chatbot, AR/VR development, database management, data science, web design and cryptocurrency. Practice in JavaScript, Java, Python, R, Android, Swift, Objective-C, React, Node Js, Ember, C++, SQL & more.
"
https://medium.com/@urban-institute/how-we-make-accessing-cloud-computing-power-simple-ad8f3bfc086f?source=search_post---------72,"Sign in
There are currently no responses for this story.
Be the first to respond.
Data@Urban
Oct 30, 2018·7 min read
You might think the only organizations that take advantage of this age of “big data” are financial firms, technology firms, health organizations, and other groups that collect and use real-time information. But at the Urban Institute, our researchers analyze large datasets and run computationally intensive models. These data and models can sometimes run up against the limitations of our in-house servers, which is a problem for producing timely, policy-relevant research. Models that take days or weeks to complete hamper our ability to produce quick-turnaround, high-impact policy analysis.
Cloud computing platforms such as Amazon Web Services (AWS) Elastic Compute Cloud (EC2) provide easy access to powerful and cost-efficient computational resources. Renting a large, state-of-the-art computer or computing cluster in the cloud for a few hours is more cost-efficient than buying and maintaining a large server on-site.
People who have spun up their own EC2 instance from the AWS console or other cloud providers know that the process can be tedious and confusing. And even if researchers manage to make it through the spin-up process, they still need to figure out how to transfer and access the data, which typically sit on a personal computer or shared drive.
At Urban, we wanted to give researchers access to an EC2 environment for their computationally intensive data analysis projects without the hassle of getting them up to speed with storage, network access, authentication, installation, and the myriad options for instance sizes in AWS. To do so, we built our own in-house solution for Windows and Linux operating systems, and our researchers have seen significant gains.
Speed increases are significant
Before we talk about how the system works, let’s talk about the benefits of cloud computing and when it should be used. The on-demand cloud computing environment is most useful for projects that have datasets too large to read into memory on a standard desktop (i.e., they cause your computer to freeze) and for long-running independent jobs that can be parallelized (simultaneously run in pieces across a computer’s cores).
One team at Urban had a project that involved a 50-gigabyte dataset that included text from more than 65 million Twitter observations. Using an x1.16xlarge instance, we could load hundreds of gigabytes of data into R and then parallelize the analysis across all 64 virtual cores. A job that was initially estimated to take 30 hours, but would often freeze even our most powerful on-site machines, ultimately ran in less than 30 minutes in the cloud. A cluster analysis project, which originally took 500 hours to run on a single thread on our on-site servers, took just 4 hours when parallelized using the 128 virtual core x1.32xlarge instances on AWS.
Accessing the instances
We created a simple web submission form using R Shiny, a package that makes it easy to create a web application within R. Researchers see a web form with a few inputs, text explaining their options, and a tool for estimating which instance size they need. Researchers simply need to understand the size of the dataset, and our form recommends the size of the instance to select.
From there, users need only to choose which operating system they want to use and the size of the computer (EC2 instance) they need. We use the web form to subset the numerous combinations of instance type and size down to a list of six that are most suitable for our research needs, providing researchers only the newest, fastest compute instances.
All the other AWS details that are typically confusing to first-time cloud users — such as the image, subnet, IAM (identity and access management) role, and security group — are abstracted away from researchers and programmatically handled on the back end. When they have made their choices, they click Submit to request their instance.
How it works
When the instance request form is submitted, the details are saved as a JSON file to a bucket in Simple Storage Service (S3), an AWS service that allows you to save data remotely in the cloud. Saving a file to S3 triggers a Python script in AWS Lambda (a serverless computing platform) that uses boto3 (a Python package that allows developers to access AWS services programmatically) to read in the submission parameters and spin up the properly sized and formatted instance. We use Lambda because it allows us to run functions without having to maintain a server; we pay only for the brief time the boto3 script runs to spin up the instance.
Once the instance is ready for use, a second Lambda function (again powered by boto3 and written in Python) is triggered that sends an email notification to the researcher with instructions on how to access the instance.
A researcher sees none of this back-end infrastructure and instead simply receives this email 30 seconds after submission for Windows users or 3 to 5 minutes after submission for Linux users. A researcher simply needs to follow the emailed instructions to access the instance. Emails are sent every few hours while the instance is running to show researchers the current total cost of use and a reminder to terminate the instance once they are finished to reduce costs. These reminders are powered by time-triggered Lambda functions.
When researchers are done with their work, the same submission form is used to terminate the instance. Entering the necessary information and clicking “Terminate My Instance” writes a second file to S3 and triggers another Lambda function that uses boto3 to properly shut the machine down. A final confirmation email is sent with the total cost of use.
How the instance works: Windows
A Windows instance is set up with a custom Amazon Machine Image that includes access to R and RStudio, Python, and Stata for data analysis; the Microsoft Office suite; and git and GitHub Desktop for version control. For our R installation, we use the Microsoft Open R distribution. This version of R comes precompiled with the Intel Math Kernel library, which increases the speed of matrix algebra operations and many modeling tasks.
Windows instances are accessed using Windows Remote Desktop Connection. Thanks to AWS Directory Service, users can log in using the same credentials they use for their personal work computers.
For data storage, we employ AWS Storage Gateway. This allows Urban Institute researchers to easily move their files to the cloud without having to know anything about S3. The file share is mapped as a network drive on the user’s local machine and on the EC2 instance. Users simply copy their data into the mapped drive, and they are immediately available for use within any EC2 instance. All data processing can be completed on the shared drive, where the data are stored even when the instance is shut down and can be downloaded locally whenever necessary.
For Windows instances, we initially had issues with start-up times for applications on their initial launch. Simply opening programs, such as RStudio and Spyder, would take a couple of minutes to load. To overcome this bottleneck, we decided to employ a queue of Windows instances that have had their root device volumes “prewarmed” with the dd program. This queue is stopped, resized, and started as requested, which cuts down on instance initialization time. As a result, our whole process, from form submission to the email indicating the instance is ready, typically takes less than 30 seconds.
How the instance works: Linux
The configuration of Linux instances is primarily handled with a bash script that is run on instance launch. We start with an Ubuntu image and install the Docker program.
To give users access to RStudio Server, we run a tidyverse container courtesy of the Rocker Project. This speeds up the configuration process and installation time, as Rocker already handles the installation of R and RStudio and the compilation of the tidyverse packages. With daily builds of the Rocker images, Urban researchers have seamless access to the latest versions of all software and packages with no effort on our part. Similarly, the scipy-notebook image from Project Jupyter allows us to easily give users access to a Jupyter Notebook server running Anaconda3 with the full stack of popular scientific libraries installed for Python.
This process is fully elastic and does not depend on a queue of prelaunched instances, unlike the Windows process, but because it uses Linux, users are typically more advanced data science practitioners. Researchers must be familiar with S3, and start-up times for the instance range from 3 to 5 minutes, on average.
More accessible tools for the big data age
As Urban researchers use larger datasets and more computationally intensive modeling techniques, our data science team needs to deliver easy-to-use, secure, and powerful services. Translating new technical capabilities, such as scalable cloud computing services, to a broader research audience will ensure that Urban can conduct new, more timely research and leverage emerging big and unstructured data sources to inform policy in real time.
-Kyle Ueyama
Want to learn more? Sign up for the Data@Urban newsletter.
Data@Urban is a place to explore the code, data, products, and processes that bring Urban Institute research to life.
20 
20 
20 
Data@Urban is a place to explore the code, data, products, and processes that bring Urban Institute research to life.
"
https://medium.com/monolith/monolith-spotlights-iexec-the-blockchain-cloud-computing-solution-8d0bbb79962d?source=search_post---------73,"There are currently no responses for this story.
Be the first to respond.
Ethereum is on the way to creating the next iteration of the Internet. As the evolution of Web3 seeks to solve many of the problems found with relying on centralised intermediaries, a number of promising solutions leveraging the power of blockchain have emerged in recent years. One example is iExec, a blockchain cloud computing solution running on Ethereum. For our latest Monolith Spotlights feature, we examine the project in detail.
iExec explained
One problem with the Internet in its current form is the amount of computing power that many applications require to run smoothly. Since we started to rely on the benefits of a global data sharing network in our everyday lives, we’ve become increasingly dependent on large-scale centralised cloud computing services such as Amazon Web Service. But this presents security problems, and it can mean that there’s only a single point of failure preventing an outage. When major companies rely on centralised cloud computing services, the costs of downtime if the network fails can be enormous.
iExec aims to solve this issue.
iExec is a blockchain network that’s best described as a decentralised cloud.
Running on Ethereum, the iExec network is made up of computing resource providers. Computing power providers can connect their machines and be rewarded in RLC tokens for contributing resources to the network. In addition to computing power, data is another resource that can be securely monetized through iExec. It seeks to be an alternative to the current system, in which many companies heavily rely on bigger centralised services such as Amazon Web Service.
It could potentially improve efficiency by running a network that anyone with spare computation power and an Ethereum address can participate in and earn rewards.
As off-chain computations are brought back on-chain for validation and blockchain-level consensus, the network ensures security, confidentiality and transparency between the parties involved in each transaction.
iExec runs a Dapp store, where anyone can list their decentralised application powered by iExec off-chain computer. It also runs its own Marketplace for cloud computing resources (computing power, applications or datasets), connecting resource providers with requesters.
One key use of the network is allowing builders to run price data oracles to provide off-chain price feeds, something that’s become integral to the Ethereum ecosystem.
In 2021, iExec launched its V5 protocol. The updates focussed on interoperability, and introduced ERC-721 NFT tokens representing computing resources on its marketplace, compatibility with Ethereum Name Service to support readable Ethereum addresses, and a Trusted Execution Environment solution aiming to preserve privacy of data-in-use.
While the team has focussed on collaborating with decentralised projects, their collaborations include computing giant Intel. With them, they’re working on building the future of Artificial Intelligence as part of Intel’s AI Builder Program.
iExec believes that the future of the world will include smart cities, and its hope is to become an integral part of the change.
The RLC token
iExec’s native crypto token is RLC. Providers of cloud computing resources (computing power, applications or datasets) receive RLC in exchange for contributing to the network. It’s an ERC-20 token, which means it’s compatible with Ethereum. It’s also available in the Monolith app: our users can send, swap and store RLC tokens, and they can also be used to top up our Visa debit card.
To learn more about the project and its plans for the future, we spoke to Blair Maclennan from the iExec marketing and community team.
The crypto space obviously experienced huge growth in 2020. How were you affected by the growth of the space?
We are happy to have seen an increased interest in blockchain recently, but not surprised. What was most interesting for us to see was that even long after the crypto craze of 2017, we’ve observed with our own eyes the blockchain ecosystem has been consistently improving, year after year. So of course, this time around is even more exciting, given that we have more useful smart contract use cases than ever before. The emerging DeFi industry for example, and more accessible cryptocurrency products like Monolith.
One of last year’s talking points was the problems associated with certain oracles, particularly in combination with flash loans. How does iExec aim to improve the current landscape of oracle solutions?
The tech behind iExec Decentralized Oracles has existed since 2016. The consensus protocol that iExec runs on, which validates off-chain computation, works perfectly for decentralised oracles. iExec Decentralized Oracles work by the nodes executing off-chain logic before returning certification is achieved back on the blockchain. The unique value added by iExec comes from integrating Confidential Computing or “trusted computing” with decentralised oracles. This works by processing results in a highly secured environment (Intel® SGX hardware enclaves), meaning no one can access or change them, ensuring that data connecting Web2 and Web3 is treated in the most secure way possible. You can try out a sandbox demonstration of trusted iExec oracles here.
This year marked the launch of iExec V5. Can you give an overview of what the updates bring to the platform?
iExec V5 was a very technical release, integrating building blocks and features for interoperability and scalability. Over the past few years, these releases have established strong foundations, and iExec is now pursuing adoption from blockchain, crypto and DeFi projects, meaning that why can use iExec for trusted off-chain computing (be it anything from secure data sharing to secure decentralised oracles). Our adoption strategy is starting off with some basic products initially, based on the iExec stack that will be easily understood and usable by the greatest number of users/developers possible: oracle, private data (NFTs), workers, etc. The aim is to have an easy-to-understand and easy-to-use interface that can be used by a greater number of users and developers. We look forward to showing this off soon. To get an idea of what iExec can offer, test some of the iExec demo interfaces here.
Can you give a step-by-step process on how a data provider interacts with the platform?
iExec is a decentralised infrastructure for cloud computing resources, with a complete solution to run privacy-preserving executions of any application.
Developers and enterprises can use the iExec tools to monetize data. Data providers can list their valuable datasets or AI models, and through iExec, these can be used with an application combined with remote computing power. The providers of the datasets, applications, and computing power are all rewarded through the decentralised iExec Marketplace.
We recently released an iExec private data/Confidential Computing demo, showing how it is possible to send you an email without ever knowing your email address… amazing right?
RLC is available via Monolith. What are the benefits and use cases of the token for our users?
The iExec RLC token was created as a method of exchange used for buying and selling cloud computing resources. Solutions such as Monolith allow for those already exchanging on the iExec marketplace to take their asset even further — paying for a coffee directly from your cloud computing resources is a brilliant concept!
How will iExec make an impact on the future of Artificial Intelligence technology?
For the most part, the recent rapid development of AI is thanks to enormous amounts of datasets becoming available. Some of these datasets are so large and complex that even data scientists are pulling their hair out. We are also seeing a further increase in the resistance of citizens and companies sharing their data. This is where Confidential Computing through iExec comes in — enabling secure and reassuring solutions addressing the upstream movement stage of data generation. No AI model will be satisfied if it is not being fed with valuable and certified large data sets.
As we head towards a smart city world, how will iExec be a part of the evolution?
At iExec, we’re building the infrastructure needed to address the next digital era. If we imagine 5G-connected smart cities of the future, we imagine “machine-to-machine” or “edge computing” economies. But for this, we will need a layer for interoperability, in order to allow these different technologies to work together. We know that smart cities will be based on millions of connected devices, built and owned by thousands of different companies. But, we know we will not have just one hardware standard, nor software standard. So we need this solution to ensure that the fragmented infrastructure and networks will be able to work together. That’s what iExec is building. A use case that iExec showcased at the Intel® booth at Mobile World Congress 2019 is a perfect example of this.
Learn more about iExec here.Download the Monolith app here.
Monolith: everything you would expect from a bank, built…
228 
228 claps
228 
Written by
Monolith is the world’s first DeFi wallet and accompanying Visa debit card made for spending crypto assets anywhere.
Monolith: everything you would expect from a bank, built for the DeFi economy.
Written by
Monolith is the world’s first DeFi wallet and accompanying Visa debit card made for spending crypto assets anywhere.
Monolith: everything you would expect from a bank, built for the DeFi economy.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/blockchain-digital-transformation/george-gilder-forget-cloud-computing-blockchain-is-the-future-e5796cbd7a84?source=search_post---------74,"There are currently no responses for this story.
Be the first to respond.
“The freer an economy is, the more this human diversity of knowledge will be manifested. By…
"
https://faun.pub/investing-in-cloud-computing-top-7-stocks-to-buy-d1794e53c83c?source=search_post---------75,"There are currently no responses for this story.
Be the first to respond.
This article was originally published on The Chief I/O: Investing in Cloud Computing: Top 7 Stocks to Buy
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://blog.bluzelle.com/bluzelle-webinar-edge-vs-cloud-computing-624d521b0b23?source=search_post---------76,"Most of you may have heard about “Cloud” but are new to the term “Edge”. Edge computing is the practice of bringing data near the edge of your network, where the data is being generated and needed, instead of in a centralized data-processing warehouse.
Do you want to learn about why edge computing is critical to the next generation of consumer applications? Join Bluzelle CTO Neeraj Murarka for a live webinar on YouTube!
In this webinar you will learn about:
Date & Time: 20th June 6pm PDT | 21st June 9am SGT (add to calendar)
Link to Youtube Live: https://youtu.be/JlSNDjjScqg (subscribe to our channel)
Bluzelle is a decentralized storage network for the creator economy.
447 
447 claps
447 
Written by
Bluzelle is a decentralized data network for dapps to manage data in a secure, tamper-proof, and highly scalable manner.
Bluzelle delivers high security, unmatched availability, and is censorship-resistant. Whether you are an artist, musician, scientist, publisher, or developer, Bluzelle protects the intellectual property of all creators.
Written by
Bluzelle is a decentralized data network for dapps to manage data in a secure, tamper-proof, and highly scalable manner.
Bluzelle delivers high security, unmatched availability, and is censorship-resistant. Whether you are an artist, musician, scientist, publisher, or developer, Bluzelle protects the intellectual property of all creators.
"
https://medium.com/geekculture/three-strategic-actions-from-cloud-computing-e6031898c3cf?source=search_post---------77,"There are currently no responses for this story.
Be the first to respond.
This may be because you and the guidelines do not have a fairly active role or ensure coverage on the main director of the Information Director (COIO) and the Head of Technology (CTO). Together, the…
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/frankuslife/%E9%9B%B2%E7%AB%AF%E9%81%8B%E7%AE%97%E5%B9%B3%E5%8F%B0%E7%9A%84%E7%87%9F%E6%94%B6%E6%88%90%E9%95%B7%E6%AF%94%E8%BC%83-cloud-computing-service-platform-aws-vs-azure-vs-google-cloud-c5fadeffd20?source=search_post---------78,"There are currently no responses for this story.
Be the first to respond.
近期指標性的科技企業都接連公布的各自的2020/Q1的財報，考量我們投資的SaaS主要都是提供雲端軟體服務，因此其租用的雲端運算平台成長情形可以當作我們度量整體市場大小的一個參考。眾所皆知大部分的企業都是租用Amazon Web Service(AWS)/Microsoft Azure(Azure)/Google…
"
https://medium.com/javarevisited/10-best-aws-google-cloud-and-azure-courses-and-certification-from-coursera-to-join-in-2021-5c5e2029a8e7?source=search_post---------79,"There are currently no responses for this story.
Be the first to respond.
Hello guys, if you are keen to learn Cloud Computing and essential Cloud platforms like AWS, Google Cloud, Microsoft Azure, and looking for the best Coursera certifications, courses, specializations, and projects then you have come to the right place.
Earlier, I have shared the best Coursera courses and certifications to learn Artificial Intelligence, Python, Software Development, and Web Development, and, today I am going to share the best Coursera courses for Cloud computing skill like AWS, GCP, and Azure from reputed universities like Illionis and tech companies like Amazon, Google Cloud, etc.
Cloud Computing is an in-demand skill for programmers, Software Developers, and any IT professionals including support engineers, sysadmins, and even QA and business analysts.
In the last few years, more and more companies have moved to the cloud and most of the technical development happening there, hence Cloud computing has become an essential skill to service the tech world. If you are wan to become a Cloud developer or administrator or just want to know about Cloud computing then you have come to the right place.  In the past, I have shared the best AWS, GCP, and Microsoft Azure online courses, and today, I will share the best Coursera courses to learn Cloud Computing with AWS and Google Cloud Platform. Coursera is one of the most popular online learning websites which allows you to learn from top universities of the world and top-class IT companies. The courses in this list are created by companies like IBM, Google, and Amazon itself. This means you will be learning from the most authoritative sources. Cloud computing is one of the big industries that have millions of users and attract investors from anywhere in the world and generate an unbelievable amount of revenue compared to the other services or industries in the IT area since all the organization needs to host their websites and services to engage and connect with their customers. The certification can make the difference between getting hired by your employees and have a professional career or stay away from the competition so for that, many platforms come to the real-world and offer some online courses for many industries and Cloud computing is one of them.  Today you will see in this article several cloud computing courses from Coursera with the ability to get certified after completing their classes and covers many cloud services such as Amazon AWS and Google cloud.
Here is the list of the best Coursera courses, certifications, specialization, and guided projects you can join in 2021 to learn Cloud Computing with AWS and Google Cloud Platform.
These courses have been offered by reputed universities and companies like IBM, Google, and AWS itself. Most of the courses are free for audit which means you can join them for FREE to learn but you need to pay for certifications, quizzes, and assessments.
The best specialization to learn Amazon web services offered from amazon itself starting as a beginner such as how the AWS infrastructure builts and works then addressing the security issues and how to secure your applications then learn how to migrate your work to the cloud then introducing you to the AWS serverless applications.
Here is the link to join this course —AWS Fundamentals
One of the best training courses for learning Google cloud architecture offered from Google itself starting y learning the core infrastructure of their cloud services then understanding the various components such as networking as well as some other services scaling and automation and more.
Here is the link to join this course —Cloud Architecture with Google Cloud
Another great course offered from IBM to learn cloud computing in general starting with the definition and revolution of the cloud services then the cloud computing models such as Saas Paas and Laas as well as the cloud components like virtualization, security, and encryption to protect your apps.
Here is the link to join this course —Introduction to Cloud Computing
A part of a master’s degree in cloud computing offered by the Illinois university targeting intermediate people starting by some concepts such as NoSQL databases as well as the infrastructure, Big data, and data analytics then moving to the cloud networking then a capstone project at the end.
Here is the link to join this course —Cloud Computing
This project-based course will show you how to deploy a Node.js website to Amazon AWS Lightsail using the SSH client so you create an amazon Lightsail instance then configure your SSH client protocol then deploy or upload your website files to the server using the Apache webserver and much more.
Here is the link to join this course —AWS: Publish a NodeJS Website from Scratch
Another good course to learn the fundamentals of cloud computing starting with the definition of the cloud and its infrastructure then the levels of managed services such as Laas, Paas, Saas as well as the different deployment models public vs private and you will a comparison between many cloud providers.
Here is the link to join this course — Cloud Computing Basics (Cloud 101)
The best training course for learning security in the cloud you start by learning the fundamentals of cloud then understanding the DDoS attacks and how to defend yourself against them as well as the privacy inside the TOR network and how securely access your web documents and network resources.
Here is the link to join this course — Advanced System Security Design
A great course for people that have some background in networking and want to move to the cloud and you will start by understanding the core of Google cloud services then exploring google cloud networking such as Google virtual private and cloud VPC as well as firewalls sharing networks and more.
Here is the link to join this course — Networking in Google Cloud
The best course for entrepreneurs and people how are going to start their online business so you start by understanding what the cloud is as well as how it can transform your business and build a transformational solution using the cloud and the security risks behind that and many more topics included in this course.
Here is the link to join this course — Business Transformation with Google Cloud
This course is about the security in Google cloud starting as usual by understanding the cloud infrastructure and different services then moving to manage security in the platform such as firewalls and security keys and how to protect yourself from being hacked as well as mitigating security vulnerabilities in Google cloud.
Here is the link to join this course — Security in Google Cloud Platform
That’s all about the best Coursera Courses and Certifications to learn Cloud Computing with AWS and Google Cloud Platform. Those certifications are one of the best to start a professional career in the cloud computing field either specializing in Amazon AWS or Google cloud platform and learn the fundamentals as well as the networking and security of this amazing industry. By the way, if you find Coursera courses and certifications useful, which they are because they are provided by reputed companies and universities around the world then I highly recommend you join Coursera Plus, a subscription plan from Coursera.
click.linksynergy.com
This Coursera subscription plan gives you unlimited access to their most popular courses, specialization, professional certificate, and guided projects. It cost around $399/year but it's completely worth your money as you get unlimited certificates and in fact, it’s cost-effective if you take multiple specializations and certifications.  Other Coursera and Programming Articles you may like
Thanks for reading this article. If you like these best Cloud Computing, AWS, and Google Cloud Platform courses from Coursera then please share it with your friends and colleagues. If you have any questions or feedback then please drop a note.
P. S. — If you are looking for the best Udemy courses to learn Cloud Computing and AWS then you can also check out my list of best Online Courses to learn Cloud Computing in 2021. It contains the best Udemy courses to learn Cloud Computing from scratch.
medium.com
Medium’s largest Java publication, followed by 14630+ programmers. Follow to join our community.
136 
1
136 claps
136 
1
A humble place to learn Java and Programming better.
Written by
I am Java programmer, blogger, working on Java, J2EE, UNIX, FIX Protocol. I share Java tips on http://javarevisited.blogspot.com and http://java67.com
A humble place to learn Java and Programming better.
"
https://medium.com/@cere-network/cloud-computing-enters-the-age-of-blockchain-and-decentralization-with-the-launch-of-cere-networks-367c69bed602?source=search_post---------80,"Sign in
There are currently no responses for this story.
Be the first to respond.
Cere Network
Dec 10, 2021·3 min read
At this point, there is no industry that has been left untouched by the cloud.
The cloud computing market size in 2021 was worth a staggering $445.3 billion, which is just the tip of the iceberg, as some studies expect a compound annual growth rate of over 16.3% through 2026.
Snowflake, a cloud data warehousing firm, had the biggest software IPO EVER in 2020. After pricing their initial public offering (IPO) at $120 a share, the stock price more than doubled on its first day of trading. Today (as of this writing), just a little over one year after their IPO, Snowflake stock is trading at $366.
It was only a matter of time before blockchain and its concepts of “Web3” and “decentralization” took a stab at the future of cloud computing.
Cere Network (Cere), a company that has raised over $41 million from some of the blockchain industry’s top VCs and brokers, today announced the launch of the first decentralized data cloud. Cere calls their decentralized data cloud, Cere Decentralized Data Cloud (DDC) — naturally — which uses blockchain identity and data encryption to power its data ecosystem.
It took two years of being in stealth mode, and six more years for Snowflake to have an IPO — backed by a who’s who of Silicon Valley titans, including Sequoia Capital, Salesforce, and Altimeter, amongst others. Similarly, Cere has been in development mode for just over two-and-a-half years, before recently announcing the launch of their data cloud this week. Cere’s who’s who of investors including Binance Labs, Arrington XRP Capital, Republic, and Polygon, amongst others — all of who are brand names in the blockchain/crypto ecosystem.
“Cere’s Decentralized Data Cloud takes models like Snowflake’s and makes it much easier for dApps to integrate with decentralized datasets,” said Sandeep Nailwal, Co-founder and COO of Polygon. “We have been working closely with the Cere team, and we are excited that developers, companies, and individuals in our ecosystem can leverage smart contracts deployed on Polygon to include trustless data transfers on top of value transfers via Cere DDC.”
Polygon, an Ethereum Layer-2 scaling solution, which had been in development for two years, was framed as ‘destroying everyone else’ (outside of Bitcoin and Ethereum) by Mark Cuban, who invested in the company in May of 2021. Cere is positioned closely with Polygon during launch, from a technical perspective, but has much more in the works.
Cere has built its own blockchain which provides the necessary security, trust, and utility to power the Cere DDC. As Cere’s innovation stack extends Substrate components, it not only supports any Polkadot ecosystem blockchain, but can already be easily integrated with any other EVM-compatible blockchain such as Ethereum, Polygon, Cosmos, and Solana. This interoperable approach allows for cross-chain data, including Non-Fungible Token (NFT)-backed data assets to be transferred along with NFTs via smart contracts.
Next to its DDC, Cere is also releasing a set of innovative toolkits that can automate decentralized data into highly customizable and interoperable virtual datasets, which are directly accessible in near real-time by all business units, partners/vendors, and machine-learning processes.
“As blockchain-based companies continue to see massive growth, it’s imperative that decentralized teams and the companies they work with are able to leverage data that aren’t behind walled-gardens,” said Fred Jin, Co-founder of Cere Network. “We’re very excited to be able to work with Polkadot and Polygon ecosystem projects to harness their datasets and take their businesses to the next iteration of what blockchain and cryptocurrency are to become — and that’s mainstream adoption.”
Like consumers have grown used to using iCloud or Google Cloud, and Web 2.0 tech companies have utilized companies like Snowflake to improve their bottom line, Cere’s Decentralized Data Cloud may soon be the underpinning of all major blockchain ecosystem projects.
Cere Network is the first blockchain CRM ecosystem platform optimized for customer data integration and collaboration.
125 
125 
125 
Cere Network is the first blockchain CRM ecosystem platform optimized for customer data integration and collaboration.
"
https://architecht.io/the-golden-opportunities-in-ai-and-cloud-computing-are-outside-the-us-and-outside-of-tech-a2cc97c0c877?source=search_post---------81,"This is a reprint (more or less) of the ARCHITECHT newsletter from April 12, 2018. Sign up here to get new issues delivered to your inbox.
If there’s one consistent trend we’ve been seeing over the past few years, it’s the shift away from Silicon Valley (and, indeed, the United States, in general) as the world’s only hub of software innovation and the world’s only real consumer of advanced technology. There are many answers as to why this is happening, including that other countries are simply catching up in terms of education, entrepreneurship and venture capital infrastructure (and sometimes government assistance).
But another driving factor appears to be that we’re reaching peak web, if not in terms of usage then at least in terms of what we want, need and expect from web companies. Mark Zuckerberg’s testimony to Congress this week was a pretty stark reminder that something’s got to give when it comes to the primary business models of Facebook and its peers. Maybe the web we have (flawed as it is in certain ways) is good enough for the most part. Maybe we don’t want or need better search engines or more social networking platforms, or constant changes in the user experience — especially if it means more chances for security flaws or more of our data being harvested.
As I noted last week, the web drove a lot of innovation in areas such as big data systems and artificial intelligence, but there’s no guarantee that must continue as people and governments rethink the terms of this data-for-services bargain. Indeed, if you watch the AI space, you still see a lot of research innovation coming out of places like Google, but the really cool applications are coming in fields like agriculture, manufacturing and energy — and, of course, autonomous cars. And as adoption in these fields heats up and mixes with the general move toward open source general openness in tech, it’s not inconceivable they could be new hotbeds of innovation. (Although some applications, like surveillance, can get a little sketchy.)
A few examples just from this week:
The fact that a lot of these stories happen to be based overseas certainly does not mean there’s not room for massive adoption and innovation in the United States, but I do think they remind us that the rest of the world is doing business, too, and they’re sometimes free of certain structural hurdles and assumptions that hinder experimentation in countries like the United States. And those stories are just about AI adoption. If we look even broader, at the expanding appetite for cloud computing around the globe, the opportunities are even bigger.
If regular readers think I’m at risk of beating a dead horse by pointing out every time Alibaba Cloud grows, I actually think I’m not calling enough attention to it. This week, for example, the company announced it’s opening a region in Turkey, and also highlighted a new relational database service it’s building. If you go read the Alibaba Cloud blog, you’ll see that it’s cranking away on lots of new products across the board, from databases to AI.
None of this means Alibaba Cloud is bigger or more innovative than Amazon Web Services, Google Cloud or Microsoft Azure. But it does mean that Alibaba is actively investing in its platform and targeting geographic regions that U.S. providers can’t/won’t/aren’t yet targeting in earnest. If AWS helped fuel the startup boom in the U.S. and western Europe over the past decade, it’s conceivable that Alibaba could do the same in other parts of the world. And the problems those companies are solving for could be quite distinct from the types of things startups are trying to solve in places like the United States.
Again, this isn’t an indictment; (nor a suggestion that China will “win” some perceived AI war); just a reminder that there’s now officially a global market for things that until recently felt quite concentrated in certain sectors and geographic regions. For people and companies that know where to look, there are some golden opportunities to make a real difference and also make some real money.
Also, not a plug, but LinuxCon is holding an event in Beijing in June. Regardless whether the event itself is good or bad, I feel confident saying that attendance will be huge.
ARCHITECHT
architecht.io
Amplify Partners is an early-stage venture capital firm investing in high-potential startups in machine intelligence, enterprise infrastructure, and cyber-security. After half a decade of partnering with university researchers to find real-world applications for their work, Amplify is now deepening its ties with undergraduate and graduate communities through its University Associate and Fellowship program. They’re looking for students excited about exploring how technology will transform business and society.
medium.com
That’s a commendable thing to do, but I bet it’s easier to do with smaller deals doing clearly unethical things. It’s where money and gray areas come into play that things get trickier.
geekwire.com
The part of this that received a lot of attention is this passage: “We are committed to providing public goods that help society navigate the path to AGI. Today this includes publishing most of our AI research, but we expect that safety and security concerns will reduce our traditional publishing in the future, while increasing the importance of sharing safety, policy, and standards research.” I’m still torn on whether less openness in AI research is net good or bad.
openai.com
Satellite imagery is another great opportunity for AI, probably with greater potential than analyzing selfies.
cnn.com
Nothing in this issue is meant as a slight against Google, which continues to drive AI research and applied AI. And if someone can actually nail speech-to-text, it will be a game-changer. I don’t think we’re there yet.
googleblog.com
More cool work from Google. This model is able to focus on speech from only one speaker, even when multiple people are talking. Low-hanging fruit here includes closed captioning and, yes, speech-to-text transcription.
googleblog.com
We probably don’t need cameras in everything, but that’s not going to stop chipmakers from pushing that future. Qualcomm is hardly alone in pursuing this vision.
zdnet.com
architecht.io
This is a good analysis of what really matters in health care, at least in the U.S. Diagnosing disease is great, but the effects are actually mitigated by a bloated, inefficient system overall.
venturebeat.com
Unfortunately, you have to register to download this entire survey, but here are the highlights. There’s some decent info, but nothing too surprising. If anything, I was impressed by the commanding lead TensorFlow has in terms of adoption among DL libraries.
oreilly.com
IBM is helping to foster quantum computing startups, and perhaps get them through a long stretch where revenue will be hard to come by. This, of course, will benefit IBM, as well. Outside of this group, the most notable quantum computing startup is probably Rigetti Computing.
ieee.org
This is cool as hell and, notably, doesn’t involve any sort of “thinking” or business insights. I truly believe the biggest benefits of deep learning will come from added efficiency and capabilities in discrete applications that can unlock humans to do a lot more.
ucla.edu
Speaking of AI and ethics, how do we feel about Uber stepping up its AI research game? I must say, it’s doing some pretty interesting stuff.
uber.com
If there’s a takeaway here for non-researchers, it’s that Google is working on methods to increase performance of neural networks in situations where adding more machines/workers becomes inefficient.
arxiv.org
architecht.io
Good news for Pivotal, but we’ll have to see what this means for the burgeoning “cloud-native” market. A richer Pivotal could become a great and powerful partner, but also ramp up its investment in places like containers and serverless and eat a lot of that revenue.
fortune.com
Speaking of the container market, here’s Amazon CTO Werner Vogels explaining the AWS Fargate service, which might have been overlooked during a slew of announcements at re:Invent last year.
allthingsdistributed.com
This is not an entirely crazy idea. You know how when you start talking about how something will kill something else in tech, someone always points out that mainframes are still big business? It’s true.
venturebeat.com
This is based on the annual RightScale State of the Cloud report, which is usually interesting but obviously is a relatively small sample size. In this case, respondents cited a 7 percent year-over-year increase in single-cloud plans versus multi-cloud plans.
zdnet.com
architecht.io
Smart folks tackling a potentially huge market. Here are the founders on my podcast last year.
stackrox.com
FYI if you follow the container space very closely. It’s not clear to me there’s a big opportunity for too many container distributions, but a company like Red Hat (which now owns CoreOS) could probably use one to its benefit. You can learn more about Red Hat’s container strategy in this recent podcast interview.
coreos.com
A total FYI, as I don’t follow this space too closely. But … the Linux Foundation is taking on an awful lot of new projects.
datacenterknowledge.com
Google and Netflix teamed up on a tool for ensuring code pushed via CI/CD pipelines is working as planned. This is integrated with Spinnaker, the CD platform developed by Netflix and really embraced by Google lately. You have to wonder if the Netflix-AWS relationship isn’t cooling (for some obvious business reasons) …
googleblog.com
This seems like a lot to do with data protection re: privacy rather than security.
theregister.co.uk
The future of server sales might very well be in workloads that need acceleration, from AI to crypto-mining. And Intel needs to blunt the use of GPUs for those things.
nextplatform.com
architecht.io
AWS acquired a PostgreSQL specialist, which the author suggests is not good news for Oracle or other companies trying to sell databases. When you extrapolate this acquisition and look at the bigger picture, he’s probably right. The big wildcard, though, is always lock-in.
infoworld.com
Some fair things to think about in this post. However, one big benefit to using cloud-provider serverless tools might very well be the extra reliance on them for security.
datacenterknowledge.com
A good analysis of what major cloud providers are spending on infrastructure and, as a result, what it takes to truly compete with them.
platformonomics.com
Again, this is about offsetting “dirty” energy usage as well as directly powering Apple facilities with clean energy.
techcrunch.com
This is a serious concern, and I’m not just saying that because I work for a company that helps SaaS companies take their applications behind the firewall. As SaaS footprints grow, so does the attack surface and the liability exposure, especially as laws like GDPR come online.
zdnet.com
Good advice from Kolton Andrus from Gremlin (and Amazon and Netflix before that). I spoke with him for the podcast this week, and it should air in a couple weeks.
infoworld.com
architecht.io
This is definitely the trend for Hortonworks, Cloudera and MapR. A good line here is that “nobody’s taking a victory lap” as they grind away at making the transition from Hadoop to broader data platforms and real-time, AI, etc.
siliconangle.com
Speaking of evolving to handle broader (or, in this case, more specific) use cases.
techcrunch.com
Probably true, but I really think “data” is going to have a different meaning going forward than it has historically thanks to the newfound attention to privacy. Interesting that Cloudera, Hortonworks and MongoDB aren’t on this list of data investment opportunities.
bloomberg.com
architecht.io
This is a fair, if not entirely fleshed out, argument. One of my more eye-opening podcast interviews last year was with the CEO of Demisto, who explained how it’s using machine learning in its platform, but the real promise is helping security teams bring order to their operations rather than claiming to automate away all their problems.
gartner.com
Venice is actually a distributed key-value store, meant to replace (or at least improve upon) the Voldemort KV store that LinkedIn developed about a decade ago. It’s also a city in Italy … and California.
linkedin.com
It’s probably good to see some separation in these roles. For too long, it seemed as if data scientists were expected to handle all things data.
oreilly.com
This is a Microsoft project combining its Cognitive Toolkit for AI with Apache Spark. If I had to bet, I would say Spark is going to be an integral part of many, many future AI workloads, despite all the alternatives that are emerging. It’s widely adopted and it works, and enterprises don’t like rushing into the next big thing all the time.
arxiv.org
architecht.io
Enterprise IT interviews and analysis: AI, cloud-native, startups, and more
34 
34 claps
34 
Written by
Founder/editor/writer of ARCHITECHT. Day job is at Pivotal. You might know me from Gigaom - way back in the day, now.
Once a site about next-gen enterprise IT and the people building it; now a place where Derrick Harris occasionally blogs about tech-related things.
Written by
Founder/editor/writer of ARCHITECHT. Day job is at Pivotal. You might know me from Gigaom - way back in the day, now.
Once a site about next-gen enterprise IT and the people building it; now a place where Derrick Harris occasionally blogs about tech-related things.
"
https://towardsdatascience.com/types-of-cloud-computing-952ae75e07c9?source=search_post---------82,"Sign in
There are currently no responses for this story.
Be the first to respond.
Giorgos Myrianthous
Nov 6, 2021·4 min read
In one of my recent articles I discussed about the deployment models in Cloud Computing, namely private, public and hybrid models. Now apart from deployment models, it’s equally important to know the three different types of cloud…
"
https://aws.plainenglish.io/5-ways-to-get-into-cloud-computing-e2333680b7dd?source=search_post---------83,"Whether you are a system admin, a cybersecurity expert, or a developer, one thing holds. The cloud is ramping up very, very high. We can see this in different statistics based on revenue coming in from the cloud, traction towards the cloud, and all the different types of services that are available in the cloud. So, what can you do to kind of start…
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/hackernoon/alibaba-unveils-new-distributed-file-system-6bade3ad0413?source=search_post---------84,"There are currently no responses for this story.
Be the first to respond.
This article is part of the Academic Alibaba series and is taken from the paper entitled “PolarFS: An Ultra-low Latency and Failure Resilient Distributed File System for Shared Storage Cloud Databases” by Wei Cao, Zhenjun Liu, Peng Wang, Sen Chen, Caifeng Zhu, Song Zheng, Yuhui Wang, and Guoqing Ma, accepted by VLDB 2018. The full paper can be read here.
Recently, cloud computing services are increasingly looking to decouple storage from compute functions. However, while there are a number benefits to this, it also poses some key challenges related to the use of distributed file systems with emerging hardware methods like RDMA, NVMe, and SPDK. In response to this, the Alibaba tech team have developed a new bespoke distributed file system called PolarFS to fully tap into the potential of these emerging technologies.
Decoupled cloud architecture is more flexible and allows for the exploitation of shared storage capabilities, which has several benefits:
1. Compute and storage can be customized independently and use different types of server hardware.
2. Disks on storage node clusters can form a single storage pool, reducing the risk of fragmentation, disk-usage imbalances, and space-wasting.
3. The capacity and throughput of a storage cluster can be scaled out transparently.
4. Compute nodes do not have local persistent state, as all data is stored on the storage cluster. As a result, data reliability is improved through the use of data replication and other high-availability features from the underlying distributed storage system.
Cloud database services also benefit from decoupling. Decoupling allows databases to build on more secure and easily-scalable environments based on virtualization techniques such as Xen, KVM, or Docker. Additionally, key features of databases, such as multiple read-only instances and checkpoints, can be enhanced with the support of back-end storage clusters that provide fast I/O, data sharing, and snapshots.
As data storage technology continues developing at a rapid pace, the current leading cloud platforms are struggling to take full advantage of emerging hardware methods like RDMA, NVMe, and SPDK. Certain widely-used open-source distributed file systems, such as HDFS and CEPH, have much higher latency than local disks. When used with current PCIe SSDs, this performance gap is increased dramatically. Moreover, the performance of relational databases like MySQL running directly on these storage systems is significantly worse than those on local PCIe SSDs with identical CPUs and memory configurations.
As a solution, vendors like Google Cloud, AWS, and Alibaba Cloud use a system called instance store. Instance store uses a local SSD and high I/O VM instance to meet customer requirements for high performance databases. Unfortunately, this system has several drawbacks:
1. Limited capacity that is unsuitable for large database service.
2. Databases must manage data replication by themselves to ensure data reliability.
3. General-purpose file systems (e.g. ext4 or XFS) compromise I/O throughput because the passing cost between kernel space and user space when used with low I/O latency hardware (e.g. RDMA, PCIe SSD).
4. Does not support share-everything database architecture, a key feature of advanced cloud database service.
In response to the issues with instance store, the Alibaba Tech team has designed and implemented the PolarFS distributed file system, which meets customer requirements for low-latency and high throughput in several ways:
1. Uses emerging hardware like RDMA and NVMe SSD with a lightweight network stack and an I/O stack in user space to avoid trapping into kernel and dealing with kernel locks.
2. Provides a file system API that is similar to POSIX that allows the entire I/O path to be kept in user space.
3. The data plane I/O mode eliminates locks, avoids context switches on the critical path, and eliminates all unnecessary memory copies so the DMA can be utilized for transferring data between main memory and RDMA NIC/NVMe disks.
These features combine to drastically reduce the end-to-end latency of PolarFS, making it function similar to a local file system on an SSD.
Despite the improved performance achieved by PolarFS, its I/O scalability was impeded when using low latency hardware under distributed systems developed with Raft. To meet this challenge, the Alibaba Tech team proposed ParallelRaft, an enhanced consensus protocol. Based on Raft, this new protocol allows out-of-order log acknowledging, committing and applying, while letting PolarFS comply with traditional I/O schematics, significantly improving its parallel I/O concurrency
As an addendum to this solution, the Tech team implemented a relational database system called POLARDB, which was modified from AliSQL, a newly available database service on the Alibaba cloud computing platform. POLARDB uses shared storage architecture and supports a multiple read-only instances feature. Its databases node are divided into primary nodes and read-only (RO) nodes. The primary node handles read and write queries, while the RO node provides read service. Both types share redo log files and data files under the same database directory in in PolarFS.
In addition, PolarFS supports POLARDB with the following features:
1. Synchronized modification of file meta-data from primary nodes to RO nodes.
2. Ensures concurrent modifications to file metadata are serialized so the file system is consistent across database nodes.
3. In the event of a network partition, prevents data corruption by ensuring that only the real primary node is served successfully.
Polar FS and POLARDB provide significant benefits to cloud service vendors looking to decouple storage and compute architecture. As detailed in this article, PolarFS can provide:
· Ultra-low latency
· Maximize I/O throughput
· Decrease the potential for failures and other errors
For more information about PolarFS and its supporting systems, the full paper can be read here.
First hand and in-depth information about Alibaba’s latest technology → Facebook: “Alibaba Tech”. Twitter: “AlibabaTech”.
#BlackLivesMatter
38 
38 claps
38 
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
First-hand & in-depth information about Alibaba's tech innovation in Artificial Intelligence, Big Data & Computer Engineering. Follow us on Facebook!
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
"
https://medium.com/aws-enterprise-collection/be-deliberate-in-your-approach-to-cloud-computing-66eae5ee70d2?source=search_post---------85,"There are currently no responses for this story.
Be the first to respond.
If there was no intentional walks, the guy would just walk him anyway, unintentionally intentionally walk him. You see a lot more of that than what meets the eye. -George Brett
I often write about how I believe businesses can be transformed using the cloud, and that doing so is a Journey that takes time. While every enterprise Journey will be unique, the ones I’ve seen transform the fastest are deliberate and methodical in their approach. This posts outlines some thoughts from one AWS’ Enterprise Solution Architects, Peter Buonora, on this topic. Without further ado…
(Note: these best practices, and a number of others, are now available in my book Ahead in the Cloud: Best Practices for Navigating the Future of Enterprise IT).
As organizations of all sizes move toward cloud computing, we find that the organizations who take a deliberate approach create value faster than those that don’t. This means looking at how your solutions are planned and architected and breaking through misconceptions others in your company might have by establishing valuable first-hand experiences.
To save yourself a lot of back-and-forth on how the shift to cloud computing will happen in your company, here are a few steps you might take:
1. Analyze your people, process and talent base. Running a cloud environment and deploying applications in the cloud is different than on-premises. Those who get the most out of the cloud take an API-driven and automated approach that is hard to replicate on-premises. For example, adding 50 terabytes to your data center requires that you purchase trays of disks, coordinate the installation, align and calculate proper performance, and so on, versus making a single API call that specifies the performance characteristics required. Many companies are spinning up entire application environments using automation, turning them off when capacity is no longer needed, and only paying for the time it was utilized.
You also need to think about how you’re deploying your own talent: Do you want them spending time dealing with the heavy lifting of managing a data warehouse, or thinking of ways to differentiate the overall business and better serve your customers? The other part of this is empowering your existing talent to accelerate the move toward cloud computing by minimizing the fear, uncertainty and doubt within the organization. It may be surprising, but You Already Have the People You need to Succeed with the Cloud! You just need to tap into their desire for learning and curiosity by providing the right level of education and motivation to keep things moving forward.
2. Assess your attitude toward security. As good as your internal security may be, many CIOs are realizing that you can actually be more secure in a cloud environment because of the visibility you have across every service or infrastructure component you deploy. If something changes, such as a firewall or configuration, you will know who did it, when they did it, and be notified/respond accordingly. AWS offers the choice of holding your own encryption keys in a tamper-resistant piece of hardware in the cloud, in an on premise hardware security module called AWS Cloud HSM, or using a fully managed service called AWS Key Management Service (KMS).
I would add compliance to this picture as well: have you done all the things required for your infrastructure to be certified for HIPAA, PCI or other compliance frameworks? AWS goes through the all of the rigorous steps required for compliance of our services, greatly simplifying the process for customers by narrowing their scope and allowing them to focus on the application layers.
AWS has already achieved a number of internationally recognized certifications and accreditations, demonstrating compliance with 3rd party assurance frameworks, such as ISO27001, PCI 3.2, SOC 1, 2 & 3, HIPAA, FedRAMP at the Moderate level for Federal government systems, as well as Level 2 and 4 for DoD systems.
3. Determine the cloud maturity of your organization. This includes an understanding of how much you’ve done with cloud computing to date and how you might build an experience base quickly through training and experimenting with dev/test in the cloud.
It also includes updating people’s perceptions based on what’s been shared in your culture about cloud over the years. It’s important to actually sit down and go through the concerns, but also the goals you’re trying to accomplish and how you might do that with your current capabilities. These will provide a valuable foundation of experience that are part of Your Enterprises’ Flywheel to the Cloud. Take advantage of the ability to quickly test and iterate on new ideas within the organization that could lead to tremendous breakthroughs or business opportunities; Perfect for lean product development methodologies.
4. Be Well-Architected from the start. You may be facing an executive mandate to move as quickly as possible to the cloud and just lift and shift everything from existing data centers in an “as-is” fashion. While this typically will lead to operational cost savings, you may also want to consider how to optimize parts of your applications during the migration. Look for services that might be more cost effective than re-using all of the existing software from your current datacenter.
Overall, we look at four pillars to determine if an application is well architected and we have established the AWS Well Architected Framework as a reference for customers. These pillars are: Security, Reliability, Performance, and Cost Optimization. It is critical that this framework is properly established within your organization from the start. Get guidance and help from your AWS Solutions Architect, AWS Professional Services or our AWS partners to accelerate your migration and the use of this framework.
“Establishing and Empowering Your Cloud Team” will be covered in my next guest post as this is another critical step in providing the internal guidance needed to being well-architected on AWS.
-Peter Buonora
Enterprise Solutions ArchitectAmazon Web Servicespbuonora@amazon.comTwitter: @pbuonorahttps://www.linkedin.com/in/pbuonora
Tales of AWS in the Enterprise
9 
9 claps
9 
Tales of AWS in the Enterprise
Written by
Husband to Meghan, father to Harper and Finley. GM of AWS Data Exchange (formerly Head of Enterprise Strategy for AWS). Author of “Ahead in the Cloud”.
Tales of AWS in the Enterprise
"
https://becominghuman.ai/machine-learnings-impact-on-cloud-computing-71c5d3a15477?source=search_post---------86,"People have been talking about machine learning for a few years now, since it promises vast benefits that could impact every aspect of human life. Efforts are also being made to develop machine learning to a stage where there will be no human intervention necessary. This section of Artificial Intelligence which uses machine learning models to learn from data, is seen as the bright future of science — machine learning being the next level of evolution in automation.
When coupled with the power of cloud computing, machine learning could be even more beneficial. This amalgamation is termed ‘the intelligent cloud.’The current usage of cloud involves computing, storage and networking. But with the feature of machine learning infused in the cloud, the capabilities of the cloud will increase vastly. The intelligent cloud becomes capable of learning from the vast amount of data stored in the cloud, to build up predictions and analyse situations. This will serve as an intelligent platform to perform tasks much efficiently.
www.eventbrite.com
Cloud computing provides two basic prerequisites for running an AI system efficiently and cost effectively — scalable and low cost resources (computing and storage mainly) and processing power to crunch huge amounts of data. First, it provides scalable, low cost computing and secondly, it is a great way to store and process large volumes of data. Therefore, the amalgamation of cloud with machine learning benefits both these disciplines. The impact of machine learning on cloud is greatest in the following aspects:
Machine learning in the cloud does exactly this. The large amounts of data stored in the cloud provides a source of information for the machine learning process. With millions of people using the cloud for computing, storage and networking, the already existing data, the millions of processes that happen every day, all provide a source of information for the machine to learn from. The whole process will provide applications in the cloud with sensory capabilities. The applications will be able to perform cognitive functions and make decisions.Some examples of cognitive computing in the current market have made remarkable progress in the field of artificial intelligence. IBM Watson, AWS IA and Microsoft Cognitive APIs have been notable cases in the industry.Cognitive computing systems in existence today are more at an experimental stage and are given tasks of minimal importance. Over time, we can expect these systems to take over healthcare and hospitality, business and personal lives even.
Personal assistants have made life easier for individuals. Products like Apple Siri ,Google Allo, or Microsoft Cortana are pre-coded voice recognition systems that give a feel of human touch to machines. But these personal digital assistants have limited capabilities. The responses are pre-programmed into the system and most of the queries are more general than personal.
With the mass data on the cloud, the learning capabilities of machine learning, and its cognitive computing feature as mentioned above, personal assistance can almost replace any form of human interaction. Fantasies of owning computer systems like those in science fiction or super-hero movies can become a reality.
Such capabilities can be of great assistance in running huge businesses. Imagine a computer that can access all the information on your past transactions, analyse the present sales and make predictions on future profits! Plus, it also tells you where the functioning is at fault and what can be done to correct it. The control still lies in the hands of humans. All the computer does is to process all the information and give out possible solutions.Chatbots have risen to the limelight in the customer service industry with their inception in providing solutions through chat. Also, with the rising use of chat apps, this becomes easier, completely eliminating human interaction, hence reducing any errors.Implementing machine learning will increase the cognitive capabilities of these chatbots, giving them a human touch. These chatbots can learn from past conversations and provide better assistance. Not just that, instead of a plain, question-answer session between the customer and the chatbot, a real conversation can take place with the chatbot. The chatbot can initiate queries about previous problems or additional suggestions for the problem at hand. The main aim is to make these chatbots as human and personal as possible to make customers feel important.
Standing alone, the cloud is on its way to become an essential computing commodity in many fields. But the integration of machine learning will increase the need for intelligent clouds in the market. With all the capabilities provided by the intelligent cloud, it is definitely the most disruptive technological change in the market. With ever-increasing competition, the intelligent cloud will become a core necessity in managing big companies and help them stay on top of the competition.The need for intelligent clouds will take over the world.As this IBM article states “Digital transformation has become an ongoing process rather than a one-time goal, with market-attuned companies continually on the hunt for the next big technology shift that gives them a competitive advantage. That next big shift is the fusion of artificial intelligence and cloud computing, which promises to be both a source of innovation and a means to accelerate change.”The need for an intelligent cloud in fields like healthcare cannot be over-estimated. It would not act s a replacement for doctors or their procedures. Rather, it can act as a virtual assistant to decide the right methods to be used in the treatment of the patients. The machine can gather years of information on a particular case, make comparisons and recommend new approaches to treatment to make the process easier on the doctors.Fields like banking, investments, education, etc. could also make use of the intelligent cloud capabilities and make human lives simpler and more efficient.Business intelligenceBusiness Intelligence can become smarter with the introduction of machine learning. Figuring out real time anomalies, identifying and rectifying faults as they are happening and predicting future are some ways that machine learning could help.The need for proactive analytics and real-time dashboards is currently high. The demand for advanced, predictive analytics that processes previously collected data, makes real time suggestions or even future predictions is the kind of business intelligence systems the market needs. The integration of Machine learning into cloud computing will help business intelligence systems get better at what they do.Businesses need their BI to be proactive and not just crunch up numbers. Predictions from current trends and suggestions for actions should be generated by the BI to make things easier on leaders. Machine learning helps Business intelligence reach that goal.
The opportunities for IoT are endless. From self-driven cars to smart homes to real time accident predictions, IoT is working towards connecting everything in one web. As the connections and interconnectivity grow, a massive amount of data will be produced. Stored in the cloud, IoT will work better with machine learning.The Internet of Things will only get better. The system, through machine learning, will be able to identify and rectify problems with systems even before the users will. Warnings about any malfunctioning device can be given out before the defective pieces affect the entire system. Also, some of the processes could be automated based on previous actions, eliminating inconvenience in services.For example, an internet of self-driving cars will know where exactly the other cars in their immediate surroundings are. This will help them keep distance from each other so as to avoid collision. In case there is a collision, the other cars can avoid those roads to avoid traffic inconvenience.The need for such technology is fast growing and the developments made in the IoT, machine learning and cloud computing are looking positive in this light.
AI is being provided as a Platform (AIaaS) by cloud providers via open source platforms. This provides users with a chunk of AI tools for necessary functions. AIaaS is said to have potential to be a delivery model which provides fast and cost-effective AI solutions rather than consulting many AI experts to complete a task.AI as a platform service makes the process of intelligent automation easier on users who do not want to be involved in the complexities of the process. This will further increase the capabilities of cloud computing, in return increasing demand for the cloud.
An intelligent cloud is the future. The interdependency of cloud computing and artificial intelligence (and humans!) will be the essence of any systems or applications developed in the future.The risk of computers taking over and the fear of robot apocalypse will keep the human involved and in charge of the machines, hence hindering full automation of machines. After the Facebook AI incident (where two AI systems started communicating with each other in a language unknown to the programmers), the need for control over the interaction between the machines has increased.But the interdependency will exist as long as humans need technology to make their lives easier. The cloud can help provide AI with the information which they need to learn, while the AI can provide more information, automate and make the cloud better — an intelligent one.
With great strides happening in the development of both machine learning and the cloud, their future seems increasingly tied together. Cloud computing becomes much easier to handle, scale and protect with machine learning. Not just that, the wider the business initiatives get on the cloud, the more the cloud will need machine learning to be integrated, to make it more efficient. There will be a point of time where no cloud will exist without machine learning.
Have thoughts? Please share with us.
Latest News, Info and Tutorials on Artificial Intelligence…
20 
20 claps
20 
Written by
We make infrastructure invisible, elevating IT to focus on the applications and services that power their business.
Latest News, Info and Tutorials on Artificial Intelligence, Machine Learning, Deep Learning, Big Data and what it means for Humanity.
Written by
We make infrastructure invisible, elevating IT to focus on the applications and services that power their business.
Latest News, Info and Tutorials on Artificial Intelligence, Machine Learning, Deep Learning, Big Data and what it means for Humanity.
"
https://medium.com/@harryfk/the-inevitable-future-of-wearable-cloud-computing-b2cd4bcb3fa5?source=search_post---------87,"Sign in
There are currently no responses for this story.
Be the first to respond.
Harry Keller
May 16, 2013·5 min read
A lot has been written about Google Glass in the past weeks. Now that the first devices are in the hands of eager technology enthusiasts, the internet is exploding with opinions on whether it is a powerful augmentation of our body or a non-starter. This article doesn’t take either side, it isn’t even about Google Glass specifically; it is about the future Glass hints at and its inevitability.
… or have we already? This article was written on an iPhone in the palm of my hand, while calming down from a 6.05km run with an average pace of 4'40”/km. I collected 1329 fuel points with my Nike+ Fuelband. You can follow my run here. Meanwhile this text is stored in Apple’s iCloud service and probably being scanned, interpreted and indexed by algorithms as I type.
The point being: Technology is already around and with us at all times. Right here, right now we measure and augment our lives digitally. Be it in our pockets in the form of the smartphone, on our laps as a laptop or glimpsing at watches on our wrists. Our physical presence has already become inseparable from its digital counterpart – Google Glass, a piece of technology worn on your head, is just the latest advancement in form factors.
Google Glass sits on top of your ears and nose, like glasses, and is equipped with a camera. Via speech recognition you can instruct it to take a photo or capture video clips and when doing so, it sees what you see. It also searches the web for you, fetches your email and can provide directions on how to get from A to B. The information is presented in a heads up display, a hologram-like screen floating right in front of your eye. Judging from the early reviews its overall performance is disappointing: weak battery life, the need for constant adjustment to fit comfortably and its dependence on a smartphone make it quite underwhelming.
Despite the many snarky reviews one should not forget though: In its current form Google Glass is a prototype, a proof of concept, clearly not ready for mass market release. And yet this piece of technology makes people unusually nervous, causing quite a stir before even being released to the public. People who have actually tried Glass say it makes others uncomfortable to be around you, because you could be filming them at any given time, without them knowing. But there’s more to that: Glass makes people nervous because it presents a leap into a post-privacy future. Judging from the current pace of tech evolution, this future suddenly came a lot closer and therefore appears more daunting. Also, we know we will be a part of this connected future, voluntarily or not.
Google Glass in its current form admittedly seems to be pretty useless, even downright ridiculous: The battery lasts only a couple of hours, videos are short, audio is hard to understand and it relies on wifi or smartphone connectivity. As if this wasn’t enough it makes you look like a dork. One could argue these are striking points against its adoption, but these are technical, not conceptual issues and will be ironed out over time. In future iterations the hardware will become smaller, the battery will last longer, wireless internet will be faster, the camera will gain multiple megapixels and the voice recognition will eventually work flawlessly. It’s easy to imagine manufacturers like Ray Ban offering their products in a regular and a Glass-enabled version, similar to what Nike is doing with Nike+.
Fast forward a few years and a Glass successor will sit on your head, recording video and audio 24 hours a day, 7 days a week. It will be unnoticeably small, uploading a live video stream of your everyday life to the cloud, where the data will be analyzed. Speech recognition technology (e.g. Siri is a public beta test in this regard) will index every word we hear and say, digging for meaning and relevance. In a web dashboard every reallife conversation we ever have will be searchable, just like your present-day email inbox. You will be able to rewind your life and re-live every minute. You won’t worry about when to pull out your smartphone to take a picture, because everything you see is already being captured automatically.
Today we have apps that plug into our Twitter, Facebook or Instagram feeds and extract information like discovering connections between our friends or where we like to take pictures. Glass apps will plug into our lives and analyze what we see, whom we talk to and what we pay attention to. What brands and products do I notice over the course of the day? Which one of my friends haven’t I seen in 10 days? How many words have I spoken today? How many have I read? When was the last time I ate an apple? What was I working on three days ago? Am I having enough sleep? What was the longest time I ever held eye-contact with someone? These are just some of the possible statistics that will be distilled from this data.
We’ll be able to see a digest of our day, including social interactions, time spent alone, with maps outlining where all these actions took place. We’ll have our eyesight augmented with additional information such as on-the-fly translation of text written in languages we couldn’t comprehend otherwise. All this data will be cross-matched with other Glass owners, tracking where we are, what we see and hear, essentially borrowing our eyes and ears to whoever has access to our data, similar to the cellphone grid pictured in the movie “The Dark Knight”. And there will be many with access to our data. No doubt Google will happily store our digital lives for free, indexed for free, browsable for free. Just as they do with Gmail. We shove in the data, they make sense of it – that’s the deal. Of course this is nothing new: Our digital lives already belong to global corporations, all the indexing and cross-referencing is already happening. Glass will be neither an exception nor a novelty in that regard.
Side note: I am by no means arguing that all this is a good thing. I am just observing that people usually choose convenience over privacy; right now millions of internet users happily feed their digital self into the cloud whenever they like something on Facebook, write an email in Gmail or upload an image to Instagram.
In the beginning of the 20th century being photographed was scary. Photographs from that time rarely picture smiling, relaxed people, instead everyone looks serious, almost stressed by the camera freezing a moment in time on paper. Today, a hundred years later, being photographed has become completely normal, it happens to all of us dozens of times each day. Imagine Facebook figured out face detection reliably: You would suddenly pop up in tourist’s photographs from all over the world. Does that bother you? Probably not … most likely you are shrugging or laughing at that thought.
The same indifference will manifest itself towards Glass’ constant video capture – but this time it won’t take a 100 years.
Grand millennial with a teenage mind: always curious, mostly optimistic, annoyingly idealistic. Developer and partner at @diesdasdigital.
15 
15 
15 
Grand millennial with a teenage mind: always curious, mostly optimistic, annoyingly idealistic. Developer and partner at @diesdasdigital.
"
https://medium.com/@LindaVivah/what-is-cloud-computing-in-60-seconds-7a4e468d8649?source=search_post---------88,"Sign in
There are currently no responses for this story.
Be the first to respond.
Linda Vivah
Jan 25, 2020·2 min read
Ready to learn what cloud computing is in 60 seconds? Let’s Go!
SLIDE 1: What is Cloud Computing?
SLIDE 2: Advantages of Cloud Computing
SLIDE 3: 3 Types of Cloud Computing
SLIDE 4: Some of the Leading Infrastructure Cloud Providers
** These 60 second lessons are originally shared on my Instagram
Hope this was helpful! Stay tuned for more mini lessons posted both on IG & Medium.
Happy Coding!
Web Developer | Tech & Lifestyle Blogger | Founder CodingCrystals.com | IG @LindaVivah
See all (176)
23 
23 claps
23 
Web Developer | Tech & Lifestyle Blogger | Founder CodingCrystals.com | IG @LindaVivah
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@cloud-opinion/today-s-cloud-computing-news-digest-6073cfb779e7?source=search_post---------89,"Sign in
There are currently no responses for this story.
Be the first to respond.
.Cloud Opinion
Apr 1, 2016·3 min read
Apr 1, 2016, Dispatches from C-Cloud Field Office, Cloud City
Your summary of all the Cloud news from around the market:
The Magic Quadrant for Serverless Cloud Infrastructure was released today. IBM and Oracle are the only vendors in the Leader quadrant, based on their oxymoronic claims to offer cloud computing services without actually having deployed the necessary server infrastructure, much less the millions of servers needed to be competitive in the hyper-scale cloud infrastructure market. Gartner analyst Darth Gideon said “IBM and Oracle are the clear leaders in serverless cloud. Their ability to entirely substitute marketing for a global infrastructure and operations footprint cannot be matched by any other cloud vendors.”
As part of the company’s new-found if not completely thought-out desire to woo the enterprise for Google Cloud, the company is bringing one of its moonshot efforts to bear in the battle for the enterprise cloud. Google’s Loon balloons have been deployed to hover ominously above various enterprise prospects. The company plans to leave them there until the companies become customers. A Google product manager added “We’re going to turn every stogey enterprise into a moonshot whether they like it or not.” One customer, a defense contractor, said they were not amused and were contemplating a demonstration of their own products to “give Google a real taste of NoOps”.
Countering Google’s airborne cloud threat, AWS today hinted it would soon be deploying its own drone air force to deploy hybrid cloud computing resources to customers and counter Google’s aerial cloud strategy. Customers can request delivery of hybrid cloud resources to their campus by saying “Alexa, I need to hug a server”. Whether the drones will use the Fire brand is still being discussed at the company. One insider commented on the debate: “Evoking flaming aerial objects definitely has interesting brand connotations so we’re still working that decision.”
Facebook, Box.net and Intel today announced they were partnering up with the Federal Bureau of Investigation to streamline law enforcement access to data residing in the cloud. The parties explained their division of labor as one where Facebook has all the data, Box can’t resist anything involving PR, Intel is just happy to be asked to participate, and the FBI would be in everyone’s business. In related news, another government agency, the US Department of Agriculture, was hoping Under Armour, Salesforce, DropBox and Apple would join their cloud effort, code-named “Cow Cloud”.
Unable to resolve their differences over which was the first to become a Cloud Broker, Rackspace and HPE have agreed to take the battle to MGM Grand Garden Arena. Each company will send ten marketing representatives to convince other company’s marketing team that they were the first to throw in the towel as a real Cloud provider and instead focus on being a “broker”. Las Vegas police have issued an advisory to citizens to avoid surrounding areas of MGM Grand Garden Arena this weekend due to high risk of narcolepsy.
In an effort to update and glamorize the traditionally thankless and low margin reseller business for the cloud era, fierce competitors (see previous story) and aspiring resellers of other people’s clouds Hewlett Packard Enterprise and RackSpace today announced they were joining forces to pay industry publication CRN to change its name to Cloud Broker News.
Several vendors have formed Yet Another Cybersecurity Alliance (YACA) to secure the nation’s information infrastructure by building a huge wall around China. Sir Scares-Alot, a spokesperson for YACA, said, “We have to take the battle to the hackers and we know that these hackers are located in China, so we are going to build a wall around China”. Whether YACA will leverage existing portions of the Great Wall of China or the Great Firewall of China in this effort is unknown at this time.
With help from Canonical, Microsoft is adding support for the BASH shell to Windows 10, allowing Linux command line applications to run on Windows. Former Microsoft Windows executive Jim Allchin, when reached for comment, said “This must be some kind of April Fools gag.”
That’s all folks — there is never a dull day in the Cloud market. To get your company news covered in c-cloud news digest, please call 1800-PoundSand
Parody + Tech commentary.
9 
9 
9 
Parody + Tech commentary.
"
https://medium.com/@fedakv/how-cloud-computing-and-storage-can-help-startups-bf8b30cc0bc0?source=search_post---------90,"Sign in
There are currently no responses for this story.
Be the first to respond.
Vladimir Fedak
Jun 6, 2019·7 min read
Every day, startups are becoming more popular. Unlike a regular business, a startup has an innovative basis, which means, there was no such business before. Usually, it is a young team, which has a great idea and a plan for its implementation — on the one hand, and the lack of money, resources and, sometimes, experience — on the other.
All processes of the project, like logistics, marketing, infrastructure, and others must be built from scratch. Such processes require investments, which the young company simply does not have. So, most parts of the startups need to cut all expenses, until they have the main investor. Another big problem of startups is the team members’ location. You can build a startup with people from a few cities or counties, so you need some system to coordinate each member’s work. Both of these problems can be solved by cloud infrastructure.
Cloud computing is the delivery of computing services like software, databases, storage, servers, analytics, networking, and others using the cloud services for startups like AWS, GCP, Azure, DigitalOcean, etc. Cloud computing platforms provide quick and flexible resources, which have their uses in many areas: from private storages to the administration of big business processes. This model of network access is very popular because of its benefits for different organizations. Cloud computing is used for startups, small-to-medium business (SMB) and big organizations.
To better understand the cloud infrastructure, let us find out about its models. There are deployment and service models of cloud computing.
There are three data storage models: public, private or hybrid.
Cloud storage solutions can be classified into three layers:
Serverless computing. This model allows you to work quickly with a large amount of short-term data, for example, exchanges between microservices of one big monolith application. Earlier, you needed additional servers for this work, which operated much longer, than necessary. Now, with serverless computing, you have a huge economy of resources, because you pay only for data processing you use.
Now you have a common comprehension of cloud infrastructure. But will it be useful for your startup? Startup remains quite a risky business. It is considered that only about 5% of startups survive and become successful. All parties of the startup have certain risks: founders, investors, customers, and partners. However, with a reasonable approach, you can avoid the most part of risks.
Cloud technologies can solve the majority of the startup’s problems and protect them from risks. As we said above, at the first time, each startup faces such problems:
There are some major benefits of cloud computing for startups.
Cost planning. Transparent tariffs of cloud services allow you to plan and calculate your expenses on any stage of the project and Pay-As-You-Go (PAYG).
Like any other solution, the cloud infrastructure is not so flawless. What challenges can you face when using cloud technologies?
Despite the fact that cloud computing is supposed to cut your company’s expenses, cloud products like web services from AWS can increase spending. A common analogy for cloud infrastructure is utilities in your home: you pay only for what you use. But, if you leave the light on and forget about it, you will pay for it. With a similar problem, you may face in the cloud infrastructure and get overpayment for resources that, in fact, were not used. To solve this problem you need to fight with cloud waste in your project. Literally, you need to switch off unused servers. This can be done manually using the cloud platform dashboard or with special services (e.g. Cloud Timing, Cloudyn, Turbonomic).
The second most common problem — the transition to cloud technology may require additional resources to manage the project. Fortunately, these problems can be solved if you carefully prepare for the transition to the cloud. There are two ways of transition: lift-and-shift and cloud-native.
In the first case, your data and application will migrate to the cloud without big changes. This method is often compared with moving a houseplant from one pot to another. Applications are being lifted from their environment and shifted as-is to the cloud. Unfortunately, lift-and-shift doesn’t suit all projects. This approach can be useful for you if you have a relatively small system.
Cloud-native migration gives you more flexibility. It’s a more complicated approach, but you’ll get better results. In this case, you need to build all the systems in the cloud. This will require copying the data from the existing databases to their cloud analogs, or making dumps of the data if the direct migration is impossible and manually correcting the missing details. Despite the complexity, this method gives you advantages in fully using service models and other features of cloud infrastructure.
As you can see, cloud infrastructure could be quite efficient for startups. It helps reduce the expenses of your business, increase safety, scale your project and more. Of course, there are some minuses, that can make you doubt, but all of them are leveled by finding a reliable and experienced contractor, like IT Svit.
Our company has 13 years of experience and successfully implements cloud technologies for many projects. You can trust us with the implementation of the cloud infrastructure and be sure of the high quality of the results.
Contact us and we will answer all your questions!
DevOps & Big Data lover
22 
22 
22 
DevOps & Big Data lover
"
https://medium.com/cuelogic-technologies/saas-paas-iaas-decoding-the-3-cloud-computing-service-models-25407ee1a568?source=search_post---------91,"There are currently no responses for this story.
Be the first to respond.
Over the years, this technology paradigm has evolved through multiple phases. The earlier forms of computing that preceded modern cloud computing included grid, utility, and on-demand computing. The earliest forms of modern cloud computing that include Software (SaaS), Platform (PaaS) and Infrastructure (IaaS) emerged as a technological outcome that attended the dipping costs of computer and server hardware. Users could purchase individual servers to power their computing requirements.
The cloud paradigm emerged when software makers and hardware vendors combined multiple servers in a concerted bid to harness the immense computing power generated by a grid (or network) of connected servers. Concurrently, the evolution of digital connectivity technologies that underlie the World Wide Web in recent years formally brought about the modern concept of “cloud computing.” In recent times, the purveyors of technology have parlayed cloud-computing systems into multiple tiers of service, variously labeled as SaaS, PaaS, and IaaS.
(SaaS) is a software licensing and delivery model that has gained a significant presence in a wide range of modern corporate, business, scientific, and commercial applications.
SaaS technologies allow users to license proprietary software on a subscription basis — monthly or annual. As providers of an ‘on-demand’ service, SaaS service providers host the software on the cloud to which users connect through a browser and an Internet connection. As a hugely cost-effective alternative to on-premise software installations and packages, the SaaS model seamlessly delivers a variety of applications that pertain to enterprise resource planning programs, office, and communications software, payroll and accounting packages, human resources management, mobile applications, etc.
This computing paradigm remains vulnerable to unauthorized access and malevolent hacking expeditions in online domains. Digital miscreants have targeted businesses that operate on the cloud by blocking customer access to critical online systems. This poses real risks that can translate into an erosion of market value for SaaS service providers. In response, service providers must continuously invest in improving security and authentication processes on the cloud, thereby delivering incremental assurances to their clients and customers.
Additional challenges that figure in the development of SaaS-powered products and services include custom third-party payment integration, safe and well-defined database access compliant with GDPR norms, guaranteeing zero-downtime deployment,managing the subscription lifecycle, and building a fully customizable SaaS system.
PaaS is “a category of cloud services that provide a platform allowing customers to develop, run, and manage applications without the complexity of building and maintaining the infrastructure typically associated with developing and launching an app.”
PaaS providers host the hardware and software on their infrastructure, thereby releasing customers from any obligation to install in-house hardware and software to develop or run a new application.
PaaS technologies pose particular problems and risks for service providers. These challenges include balancing control, cost, and capacity of a PaaS-based service, providing full multi-tenancy support, designing role-based access controls, creating audit trails, and integrating third-party services into modern PaaS platforms. Additional challenges may emerge in the form of virtualization management, fine-tuning the PaaS compute architecture, designing inter-operability with other cloud services, and creating technically sound fault tolerance parameters.
IaaS operates by traditional cloud architecture. Per the IaaS cloud-computing paradigm, service providers host the infrastructure such as servers, storage units, networking hardware, virtualization or hypervisor layer, etc. This model negates the legacy business case for investing in on-premise data center infrastructure and equipment. Modern IaaS service providers also offer policy-driven services to clients and customers. These services include monitor service performance, detailed billing of customer services, log access, digital security, load balancing, backup, replication, and recovery, etc.
IaaS represents “the virtual delivery of computing resources in the form of hardware, networking, and storage services. It may also include the delivery of operating systems and virtualization technology to manage the resources. Rather than buying and installing the required resources in their own data center, companies rent these resources as needed,” according to popular definitions of IaaS.
A raft of business challenges have emerged to face service providers that offer IaaS services to clients and customers. These may include subscriber expectation management, defining support systems that handle different forms of payments from customers, accommodating the need for custom analytics that measures customer profitability and usage, managing the service value chain, and controlling business with multiple partners. Also, service operators must remain agile in terms of experimenting with product prices, product features, the configuration of service packages, and navigating the intricacies of customer licenses. They must also work to actively manage customer expectations and refine the concept of datacentre ‘in-the-sky.’
The business case for cloud computing technologies and frameworks will continue to burnish its relevance and utility years and decades into the future. The large and incrementally enormous volumes of data generated by modern scientific, commercial, and technological enterprises will require larger data centers powered by innovative technologies. These may form the central planks of local, regional, and national economies in the future. That said, the central role of the cloud may morph into differentiated expressions, marshaled by real-time processing technologies and a deeper engagement with refined versions of civilizational requirements.
Source: Cuelogic Blog
Tech for leaders & developers
69 
69 claps
69 
Written by
Global organizations partner with us to leverage our engineering excellence and product thinking to build Cloud Native & Data-Driven applications.
Tech for leaders & developers
Written by
Global organizations partner with us to leverage our engineering excellence and product thinking to build Cloud Native & Data-Driven applications.
Tech for leaders & developers
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/ankr-network/introduction-to-technology-cloud-computing-bcb8135471b4?source=search_post---------92,"There are currently no responses for this story.
Be the first to respond.
“Cloud computing is a model for enabling ubiquitous, convenient, on-demand network access to a shared pool of configurable computing resources (e.g., networks, servers, storage, applications, and services) that can be rapidly provisioned and released with minimal management effort or service provider interaction.” — NIST
In this series we would like to explain the basics of cloud computing in an elementary way. We will look to answer some basic questions such as what cloud computing really is, and why it is used by countless enterprises around the globe.
Let’s start with the most common definition; cloud computing is a model that provides marketable and measurable computing resources remotely. For more understanding, we will explain the five main characteristics, service models and common deployment models of cloud computing technology.
Basically, cloud computing technology allows consumers to rent as much computing resources as they need, access them whenever and wherever they want, and pay for the amount of resources they actually use. And these resources can be used in a limited way, including infrastructure layer, platform software layer and application software.
There are various commonly defined business drivers including demand driven, profit driven, trust driven etc., these drivers inspire and expand the use of a new generation of technologies. But what are the business drivers of cloud computing technology?
Many companies currently benefit of the cloud computing model by implementing a multi-cloud management platform. How does this work?
A company that uses the shared economic model, integrates many computing resource suppliers (such as Amazon AWS or Microsoft Azure) of idle computing resources, for consumers around the world to provide cost-effective hardware infrastructure (network, server and storage), platform (machine learning research and development platform) and application (application market), and other services.
By using cloud-native technology, the company provides consumers with a unified view of resources to manage, schedule, and monitor resources distributed globally. Some of the goals include: to make full use of idle global resources, to provide cost-effective and easy-to-use machine learning platform for consumers, and to build the App market to provide an opportunity for App developers to promote around the world.
Please stay tuned for more upcoming articles in this series to learn about the mechanics and use cases of cloud computing.
The official blog of Ankr
66 
66 claps
66 
Written by
Blockchain infrastructure and staking DeFi
The official blog of Ankr
Written by
Blockchain infrastructure and staking DeFi
The official blog of Ankr
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/hackernoon/3-types-of-cloud-computing-and-what-fits-small-or-medium-business-smb-best-46cad297c7a8?source=search_post---------93,"There are currently no responses for this story.
Be the first to respond.
Every business has to make a decision regarding transitioning to the cloud at some point.
While startups often choose the cloud-native approach from the very beginning, many established business have to go from on-prem legacy infrastructure or using clusters of dedicated servers to deploying their software ecosystem to the cloud. This article depicts the possible types of cloud computing and how to choose the type that fits your small or medium business (SMB) best.
Cloud computing is the practice of using highly scalable infrastructure for storing, managing and processing data over the Internet, as opposed to using on-prem or dedicated servers.
The main benefits of going for the cloud are the following:
The main flaw of cloud computing is the lack of direct control over the data flow, storage and backup, which arises certain security concerns. To address this issue, three types of cloud computing are used:
How to choose the type of cloud computing and the service provider that fits your SMB best, then? While all companies say they offer the full range of services, there is a list of really important features they should provide:
Thus said, while choosing a trusted service provider and going for cloud-native deployment is natural for a startup, many SMB’s or enterprises have to make a tough choice. An expert piece of advice and consultation might make the difference between making or breaking the project, and IT Svit is glad to provide such service. Contact us with any questions and concerns regarding leveraging various types of cloud computing. Want to find out if we fulfill our promises? Ask our satisfied customers or partners for feedback!
Originally published at https://itsvit.com/blog/, fell free to follow and contact us
#BlackLivesMatter
38 
1
38 claps
38 
1
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
DevOps & Big Data lover
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
"
https://servian.dev/how-football-team-data-scientists-and-solution-architects-utilised-cloud-computing-at-aws-5c2de581ce14?source=search_post---------94,"Sign in
What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Perth Ngarmtrakulchol
Sep 4, 2019·7 min read
Most people see Amazon as a giant e-Commerce website where we can get the same-day delivery on anything. But, for people who work in the IT industry, we see Amazon as one of the biggest cloud computing services providers in the world.
Amazon Web Services (AWS) is the subsidiary of Amazon which is launched in 2006. AWS has been expanding into a wide range of services from simple message queue to sophisticated machine learning platform.
I recently went to the AWS Community Day Melbourne with my colleagues at Servian. There were 2 tracks of 30-minute sessions as well as various activities such as Alexa Skills Workshop, AWS DeepRacer, and DevBytes lightning talks. In this event, I also had a chance to meet the legendary Jeff Barr who is the chief evangelist and VP at AWS.
I was impressed to see many sessions focusing on how the AWS community has greatly strengthened their businesses and personal projects by leveraging AWS services. AWS services are effective and also have a low cost for what it could do.
One of the speakers (Kris Howard) mentioned she only spends $10 a year to host her serverless Meetup calendar application.
In this blog, I would like to share several AWS use cases I learned from this event. Hopefully, these use cases will give you ideas on how you could also use AWS to improve your business or side projects.
This session was delivered by the Kate Cohen, national football team analyst who utilised the power of cloud analytics to prepare the Australian women football team for 2019 FIFA World Cup.
The challenges for the team was that the coaches and athletes were not in the same geographic location. Some coaches were based in Melbourne, while some were in Sydney. Many athletes were also playing in different leagues around the world. They would come together in a camp to train for a few weeks before the World Cup. Before this time, they needed the central platform to connect everyone.
The platform they chose is Hudl, which is the sports analytics platform running on AWS. Hudl allowed the team to upload the full-length game videos including practice, scout, and match games. The coaches and athletes can then review the game as well as the strategy behind each play.
Some notable use cases on sports analytics for the Australia’s FIFA Women’s World Cup were:
One game video includes 3 different views (tactical wide, tactical behind, and broadcast views) which required ~ 22 GB of storage per game. Therefore, the team needed to use fast upload connection, which was provided by Hudl through Amazon S3 Transfer Acceleration.
I loved this session because it was an eye-opening opportunity for me to learn about how the modern sport teams utilise data to their advantages.
I had a chance to play with Amazon DeepRacer at re:Invent 2018. It was really interesting to learn reinforcement learning through the physical (toy) car.
This talk was delivered by a game developer Paris Buttfield-Addison and a computer scientist Mars Geldard.
Instead of having the physical toy car like DeepRacer, they decided to simulate the environment using the popular game development software Unity and its powerful physics engine.
They mentioned “A game engine is controlled, self-contained spatial, physical environment that can (closely) replicate (enough of) the real world (to be useful)”, which was the reason why they chose to simulate the race track to train self-driving car.
The virtual car was training reinforcement learning through Tensorflow on AWS powerful servers to drive as efficient as possible on the track. Same as DeepRacer, the only input the car would have at any time is the track in front of it, which makes this problem very challenging.
The heart of reinforcement learning on this self-driving car is to gain a reward (+0.05) when the car drove well, and take away the reward (-1.0) when the car collided with the sides of the track. The final model is the car that can run with the highest reward.
I liked this session because the way they combined game development tool with the power of the cloud was remarkable. The simulated environment was also well built.
“There is no server better than no server”
— Werner Vogels, CTO and Vice President of Amazon
About a year ago when I started my AWS journey, I was amazed to see A Cloud Guru runs everything on serverless to keep their costs tiny. It was also the first time I learned how serverless computing is a huge game-changer for any developer.
In a nutshell, serverless computing is a way to execute any code without having to set up the environment ourselves e.g. if we want to execute any Python code, previously we would need to get a server, install Linux, and install Python. But serverless computing allows us to just upload the code to execute anything.
“Serverless is like cooking food without the need to clean the kitchen”
— Kris Howard, the manager of solution architects at AWS
Furthermore, the cost of serverless computing is almost nothing. The serverless computing service on AWS (AWS Lambda) charges $0.000016 per second while the code is running. We pay nothing while the code is not running.
This session was delivered by Kris Howard, the manager of solution architects at AWS. She talked about her pain points where she wanted to efficiently spend her time by visiting the tech meetups on the day she is available.
Her issue was that there was no central place to see which meetup is coming up on each day. The website meetup.com does provide the event calendar, but the calendar is only available if you are a group member.
So, she built the small application called MUGicalPHP using PHP to aggregate event calendar from each group, and store it into the central calendar she created.
However, running PHP application requires the PHP server. She was asking her private sysadmin (a.k.a. her husband) to set up the server in her garage. But, the server requires maintenance which she grew tired of fixing it after a while.
That was when she decided to move her program to serverless computing on AWS. This is her first architecture, which utilised AWS CloudWatch Event to trigger the Lambda once a day:
By converting her PHP application to serverless, she can stop worrying about the servers. Her code runs smoothly without the need for her or her private sysadmin to do any maintenance work. Noice!
Serverless computing is also evolving at an incredible speed. Over the year, she revised her architecture 3 times to solve different challenges such as API rate limit. In the latest architecture, she created a workflow using AWS Step Function.
Not only is her application now fully serverless, it also only costs her $10 per year to run the whole application. This cost included AWS Route 53 and CloudFront cost for distributing the central calendar. Amazing!
I like this session because it showed the benefits of serverless computing. This session convinced me to wholeheartedly believed “there is no server better than no server”.
The single takeaway I had from the event is this: we can utilise AWS cloud computing from small stuff like Kris’s side projects or for bigger stuff like winning a World Cup.
And you don’t have to be a legendary programmer to get started with AWS. All the great sessions in this AWS Community Day showcased many exciting projects from AWS users with a different background. Anyone can easily build powerful projects using AWS cloud.
Hope this article will give you some ideas on how you could also find suitable AWS services from 100+ available services to improve your work. Feel free to let me know if you need any support!
At Servian, we design, deliver, and manage innovative data & analytics, digital, customer engagement and cloud solutions that help our customers sustain competitive advantage. If you need any help from us in these areas, please reach out!
Data Consultant at Servian | Monash Data Science Alumni | Front-end Developer based in Melbourne, Australia | LinkedIn: http://bit.ly/lkdn-perth
See all (779)
27 
1
27 claps
27 
1
At Servian, we design, deliver and manage innovative data & analytics, digital, customer engagement and cloud solutions that help you sustain competitive advantage.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@neal-davis/what-is-cloud-computing-in-simple-terms-f143209250c0?source=search_post---------95,"Sign in
There are currently no responses for this story.
Be the first to respond.
Neal Davis
Mar 1, 2021·7 min read
Many explanations can be complex and full of jargon so we’re here to break things down using simple language and explain the core concepts around cloud computing and help you to understand the terminology that is used to describe this computing paradigm.
You’ve probably heard about cloud computing in one form or another. Cloud computing has become the best way to deliver enterprise applications — and the ideal solution for businesses that want to launch new innovations or extend their infrastructure. It’s also used for many of the consumer applications you know and love such as Uber, Airbnb, Gmail, YouTube and Netflix.
Jobs in cloud computing pay well and offer exciting work with cutting edge technology and huge career potential. At Digital Cloud Training we provide training courses to get you certified on Amazon Web Services (AWS) so you can find your way into this amazing industry. But before we get to that, let’s start at the basics.
Cloud computing is basically the running of workloads remotely over the web in a commercial provider’s data center, popularly known as the “public cloud” model. Some of the most popular cloud models include Microsoft Azure, Salesforce’s CRM system, and Amazon Web Services (AWS). To enhance performance and stay ahead of the competition, most modern organisation’s take a ‘multi-cloud’ approach, which means they use multiple public cloud services.
There are several cloud computing deployment models which include:
Public clouds are operated by third-party cloud service providers. These service providers deliver various computing resources such as servers and storage over the web. Amazon Web Services (AWS) and Microsoft Azure are examples of public clouds. With this type of cloud computing, all the software, hardware and supporting infrastructure is owned, operated and managed by the cloud provider. All you need is to create an account, and you will access and manage all services through your web browser.
Private cloud is the direct opposite of the public cloud. It refers to cloud computing resources that are owned and used exclusively by a single organization or business. A private cloud can be physically located within the organization’s datacenter. However, there are those that pay third-party service providers to host private cloud on their servers. All the resources, services and infrastructure in a private cloud are managed on a private network.
A Hybrid cloud is a cloud that combines both private and public clouds. On a hybrid network, these two clouds are bound together by technology, which allows applications and data to be conveniently shared between them. By allowing data to be flawlessly shared between public and private clouds, hybrid cloud gives businesses more deployment options, greater flexibility and helps them optimize their existing infrastructure, compliance and security.
It is worth noting that these cloud computing types are not made equal. There is no one type that is right for everyone. They are all designed to help offer the right solution based on your unique needs. Therefore, it is critical for business owners to understand each type and determine which type of cloud computing architecture their cloud services will be implemented on.
There are different types of cloud computing services. Generally, they are known as cloud computing ‘stack’ because they are created on top of each other. Understanding what they are and how they work makes it easy for those who want to incorporate cloud computing to achieve their desired goals.
The main types of cloud computing services include:
This is the most basic type of cloud computing service. With it, you rent all the IT infrastructure, including virtual machines (VMs), servers, networks, operating systems, storage and more from a third-party cloud provider on a pay-as-you-go basis.
This is a way of delivering software applications over the web on-demand and on a subscription basis. In SaaS, third-party cloud providers host and manage the software and handle any required maintenance such as security patching and software upgrades. Businesses connect to the application over the web, mostly through web browsers across various devices.
PaaS refers to cloud computing services that provide an on-demand environment for creating, testing, managing and delivering applications. This service is specially made to make it easy for developers to create mobile or web apps without stressing about investing and managing the underlying infrastructure of databases, networks, storage and servers required for the development.
FaaS adds a layer of abstraction to PaaS so that developers are protected from everything in the stack below their code. Instead of working around virtual servers, application runtimes, and containers, developers upload functional blocks of code and set them to be triggered by particular events such as uploaded files or form admission. All major clouds like Google Cloud Functions, Microsoft Azure and AWS provide FaaS on top of IaaS.
Serverless computing is focused on enhancing app functionality without spending much time constantly managing services and any other infrastructure needed to do so. The third-party cloud provider is the one that manages the server, setup and capacity planning. Serverless architectures are event-driven and scalable and only use resources when triggered by a certain event.
Instead of owning data centers and computing infrastructure, business owners can rent access to servers, storage, applications and other infrastructure from third-party cloud providers. With cloud computing, companies avoid upfront fees and the complexity of owning and maintaining their own IT infrastructure. They simply pay for the resources they use.
Cloud computing creates a win-win environment for both business owners and cloud computing providers. The former enjoys a blend of convenience and affordability, while the latter makes the most out of the significant economies of scale by offering similar services to a myriad of customers.
Cloud computing has a wide range of applications. In fact, you are probably using cloud computing right now, but you don’t realize it! For instance, if you use a particular online service to edit documents, stream movies or watch TV, play games, send emails, store files, or listen to music — cloud computing is playing an integral role behind the curtains.
To understand cloud computing better, below are some of the possible uses of cloud computing:
Cloud computing rides along with a plethora of benefits, including:
Cloud computing offers an extensive set of technologies, controls and policies that enhance the overall security of your systems. It provides military-grade protection to your apps, infrastructure, apps and more from potential threats posed by hackers and snoopers.
Cloud computing is highly reliable. With it, data is mirrored on multiple redundant platforms on the cloud provider’s network. This makes data backup and recovery, as well as business continuity, easier and affordable.
The best cloud computing services operate on a worldwide network of safe datacenters that are regularly upgraded to cutting-edge computing hardware. This enhances speed, efficiency, reduces network latency for applications, and leads to greater economies of scale.
If you choose to have an on-site datacenter, be prepared to incur costs and spend a lot of time doing software patching, hardware setup and other IT management roles. Cloud computing eliminates these tasks so that your team can focus on achieving your short and long-term business goals.
Cloud computing is in relatively early stages of adoption despite having a long history. With more and more companies getting comfortable with their data being on the cloud, cloud computing practitioners have many opportunities to make a decent living. Now that you know what cloud computing is, how it is used and its unrivaled benefits, there is no reason why you shouldn’t get started with AWS Certification. Check out the Introduction to Cloud Computing video course for Beginners from Digital Cloud Training so that one day you get to join the league of the most celebrated cloud computing experts of all time. Good luck!
Contact us here
Founder of Digital Cloud Training, IT instructor and Cloud Solutions Architect with 20+ year of IT industry experience. Passionate about empowering his students
8 
8 claps
8 
Founder of Digital Cloud Training, IT instructor and Cloud Solutions Architect with 20+ year of IT industry experience. Passionate about empowering his students
About
Write
Help
Legal
Get the Medium app
"
https://itnext.io/how-to-harness-the-power-of-cloud-computing-d0bd73a34f7b?source=search_post---------96,NA
https://medium.com/@mentormate/how-healthcare-can-thrive-with-cloud-computing-c8f529ecb948?source=search_post---------97,"Sign in
There are currently no responses for this story.
Be the first to respond.
MentorMate
Mar 21, 2017·5 min read
Cloud computing offers efficiency and the ability for deep, predictive learning. Both could play a substantial role in helping reduce administration and increase the personalization of healthcare. But, what inherent characteristics of cloud computing must we overcome before we can get there?
When analyzing characteristics of cloud computing, efficiency is oft-mentioned. The “cloud” is the only economically practical place for the intermittent yet intensive workloads to be performed. The good news is that the “Big Data” tools that have emerged over the last decade to support such computing tasks. For example, ADAM, an open-source genomics analysis platform based on Apache Spark, is available from multiple public cloud providers.
This support includes not only hosting the tools themselves but providing auto-scaling computational nodes that start up as needed to support the tasks you give them and shut down when idle, effectively guaranteeing that the compute you need does not cost any more than it needs to at current commodity rates.
With Amazon Web Services (AWS), you can even use the spot market to reduce those costs further by only using spare compute power available on the spot market. At last year’s AWS re:Invent conference, a couple of different organizations explored dramatic cost reductions that could be gained using AWS for genomic processing compared to dedicated in-house data centers. Reductions can also be gained in calendar time due to the ability to flex up to a larger machine count or flex down without machines sitting idle during periods of transition.
Privacy can be among the tougher characteristics of cloud computing to reconcile from a technical point of view, but we are not without a few options to consider.
De-Identification. Let’s start by dismissing the idea that we can use de-identification as a solution to the challenge of privacy in healthcare cloud data. Typically, this is the approach taken: Reduce the amount of information shared to make sure data could apply to any individual in a large enough group. Unfortunately, there is no more unique and specific way to describe person than their full genome. The only real exception to this is the case of identical twins, and even then you are down to one of two people the data could belong to.
Encryption. Encrypting the information does not help, because any system that wants to process the data has to decrypt it first. For that to happen, the system must have the key to do so. Decrypting data to process it means it is also vulnerable to being copied, stored or shared by whatever system was able to decrypt it. That kind of transitive trust at a large scale is unprecedented.
Bitcoin. We can imagine a couple of possible ways that technology can help. First, new technologies like blockchains (used by Bitcoin to enable financial transactions) may be able to secure information in a way that uses for shared information can be traced. While this does not by itself guarantee the security of the information, it would, in principle, allow you to trace what happened to your information up to the point it was leaked and therefore who leaked it. This could be key to giving organizations the accountability needed to build consumer confidence.
Distributed Computing. The more practical solution that delivers true privacy is a distributed computing model able to process, yet not disclose, the data. The data in our example would live in a single system used by a trusted individual where processing algorithms could be run on that data to answer questions. This is tricky to execute because any processing code that will operate on the data must itself be proven secure. Thus, there will be significant performance impacts in coordinating distributed computing this way. At least it is possible to provide a solution that can protect each contributor’s data but allow their data to be included in larger cohorts for processing.
Alternately, we could hope that cultural norms change so the sensitivity around disclosing this kind of information evolves — though, we can’t count on that happening as fast as needed to eliminate the problem.
Since we all use the Internet, we know that there is already more information available to us than we can ever possibly consume. Tools like search engines have made it more possible to get relevant information faster on demand and to create learning systems that get better over time at showing us relevant search results.
A recent joint announcement by IBM and Illumina suggests at least one creative approach that could go well beyond making it possible to simply locate information relevant to a patient’s condition. What if your sequencing system or lab could not only provide you the raw genomic data but combine that with knowledge of the patient’s clinical history and genomic information go beyond the location of all recent or relevant publications that might apply? What if it could also summarize it for you in a report that is available within minutes?
The Watson for Genomics software adds data from 10,000 scientific articles and 100 new clinical trials every month to improve the odds that an oncologist can have at their fingertips the most precise and current information possible related to observed genomic alterations related to the patient’s tumor. While there will never be a substitute for a skilled human mind in making a diagnostic decision, we can at least make sure physicians have the help they need to stay on top of the ever-increasing rate of precise knowledge required to make informed decisions.
Loose ends remain before the use of cloud systems and AI in healthcare can be more fully implemented. How many of the advertised capabilities are as fully-baked as they appear? How safe are public clouds? How has our ability to process and store data improved? As the healthcare and tech worlds continue to co-mingle, the opportunities to shave costs and increase precision will only grow.
[Original Post Here]
Innovate with us! Click here to access all of our free resources. Authored by Craig Knighton.
Trusted guidance, global expertise, secure integration. We design and develop custom software solutions that deliver digital transformation at scale.
See all (1,229)
7 
7 claps
7 
Trusted guidance, global expertise, secure integration. We design and develop custom software solutions that deliver digital transformation at scale.
About
Write
Help
Legal
Get the Medium app
"
https://medium.datadriveninvestor.com/how-to-get-the-cheapest-cloud-computing-eaf6463806b6?source=search_post---------98,"Are you looking for the cheapest cloud computing available? Depending on your current situation, there are a few ways you might find the least expensive cloud offering that fits your needs.
If you don’t currently use the public cloud, or if you’re willing to have infrastructure in multiple clouds, you’re probably looking for the cheapest cloud provider. If you have existing infrastructure, there are a few approaches you can take to minimize costs and ensure they don’t spiral out of control.
www.datadriveninvestor.com
There are a variety of small cloud providers that attempt to compete by dropping their prices. If you work for small business and prefer a no-frills experience, perhaps one of these is right for you.
However, there’s a reason that the “big three” cloud providers — Amazon Web Services (AWS), Microsoft Azure, and Google Cloud — dominate the market. They offer a wide range of product lines and are continually innovating. They have a low frequency of outages, and their scale requires a straightforward onboarding process and plenty of documentation.
Whatever provider you decide on, ensure that you’ll have access to all the services you need — is there a computing product, storage, databases? If you want to use containers or have the option for serverless, how do those products fit in? How good is the customer support? Does your company directly compete with the provider — for example, with Amazon’s retail arm? (You may not care, but some companies definitely do.)
While there is no one “cheapest” cloud provider among the major options, you should still compare to ensure you’re getting the best cloud prices for the services you’ll use most. For more information about the three major providers’ pricing, please see the following cloud computing cost comparisons:
A note on the idea of vendor lock-in: if you are already purchasing cloud services from a cloud service provider, you may be worried that you’re “locked-in” to that provider. What we see in practice is a little different: with on-demand flexibility and more opportunity than ever to practice multi-cloud, companies shouldn’t really worry about vendor lock-in when it comes to public cloud.
Of course, whether or not you’re concerned about vendor lock-in, you should ensure that you’re getting the most efficient cloud computing cost available to you. That means optimizing your options for the products you use most.
Here’s a brief rundown of things you should do to ensure you’re getting the cheapest cloud computing possible from your current provider.
All of the major cloud providers offer a pricing option for Reserved Instances — that is, if you commit to use capacity over time, you can pay a discounted price. Reserved instances can save money — as long as you use them the right way. It’s important to focus on workloads with 24×7 demand — i.e., production workloads — for Reserved Instances. You will get the best price for the longest commitment. Of course, each cloud provider structures this option differently. Here are our guides to each:
There are a few common ways that users inadvertently waste money and throw away the option for the cheapest public cloud bill, such as using larger instances than they need, and running development/testing instances 24/7 rather than only when they’re needed. To pay for what you need, ensure that all of your instances are “rightsized” to the size that best matches the workload. You should also use on/off schedules so your non-production resources used for development, testing, and staging are turned off nights and weekends.
ParkMyCloud makes it easy to automate both of those things and reduce wasted cloud spend — try it out.
There are a number of other discounted pricing and purchasing options offered by the major cloud providers to help you get the cheapest cloud services.
It never hurts to contact your provider and ask if there’s anything you could be doing to get a cheaper price. If you use Microsoft Azure, you may want to sign up for an Enterprise License Agreement, or if you’re on AWS, ask your rep about the Enterprise Discount Program. Or maybe you qualify for AWS startup credits.
While finding the cheapest cloud computing is, of course, beneficial to your organization’s common good, there’s no need to let your work in spending reduction go unnoticed. Make sure that you track your organization’s spending and show your team where you are reducing spend.
ParkMyCloud users have a straightforward way to do this. You can not only create and customize reports of your cloud spending and savings, but you can also schedule these reports to be emailed out. Users are already putting this to work by having savings reports automatically emailed to their bosses and department heads, to ensure that leadership is aware of the cost savings gained… and so users can get credit for their efforts.
Originally published at www.parkmycloud.com on October 15, 2019.
empowerment through data, knowledge, and expertise.
69 
69 claps
69 
Written by
CEO of ParkMyCloud
empowerment through data, knowledge, and expertise. subscribe to DDIntel at https://ddintel.datadriveninvestor.com
Written by
CEO of ParkMyCloud
empowerment through data, knowledge, and expertise. subscribe to DDIntel at https://ddintel.datadriveninvestor.com
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/next-level-german-engineering/how-cloud-computing-changes-the-way-of-business-at-porsche-5597e4313572?source=search_post---------99,"There are currently no responses for this story.
Be the first to respond.
Andy Grau, Innovation Manager at Porsche AG and Nils Kreft, Product Owner Cloud Enablement at Porsche AG, reflect on the first Cloud Innovation Day @ Porsche and how the Cloud changes the way Porsche makes use of IT.
When I joined Porsche in 2016, cloud computing was still classified as an emerging technology by Gartner Inc., one of the world’s leading research and advisory companies. Today, cloud computing is no longer hype. According to Gartner’s 2019 Hype Cycle, the technology has reached the “slope of enlightenment.” Cloud computing is here to stay and is going to play an increasingly important role in the automotive world.
The Cloud is a powerful and radical enabler that offers access to a plethora of virtual services and applications such as data storage, networking and analytics through the internet or dedicated networks. And yet, we have only begun to scratch the surface of the Cloud’s potential. Just to give you an idea: In 2022, the cloud market is expected to reach market revenue of $350 billion.
To better understand the benefits and challenges that cloud computing presents for our organization, our industry and our jobs — and to harness its potential — Porsche brought together leading IT experts in Stuttgart Zuffenhausen. Together with our colleagues from Cloud Enablement & Innovation Management at Porsche, we organized the first Cloud Innovation Day @ Porsche, a one-of-a-kind in-house exhibition.
Together with our technology partners SAP, Microsoft, Amazon Web Services (AWS), MHP — A Porsche Company, MongoDB, Rackspace, Cloudreach, HashiCorp, GitHub, we put the spotlight on cloud computing and discussed the latest advances, how the Cloud changes the way we do business, and what lies ahead for Porsche.
Indeed, the Cloud Innovation Day @ Porsche, which took place last week at the Porsche Museum in Stuttgart-Zuffenhausen, was a huge success. It was conceived as a forum for innovators, experts and peers to share, learn, and network.
Around 750 registered Porsche visitors showed up to the event, and another 1.500 people at their offices watched our live-stream online. Furthermore, sixty-eight special guests took part in the VIP-Walk, including four board members and four managing directors of Porsche subsidiaries. It was great to see that so many people are interested in this topic!
The Volkswagen Group’s cooperation with Microsoft and AWS in the field of automotive and industrial cloud underscores the organization’s commitment to this technology. It has and will be a key enabler of automotive and industry use cases as well as digital business transformation. At Porsche, the deployment of cloud technology has increased sixfold since 2016, and it has been incremental to advancing a flexible innovation infrastructure.
The Cloud not only changes the way we use IT and think about our operations, but it also changes the way we interact with our customers. For example, it enables us to collect, process and visualize large volumes of data in real-time, so that sales representatives can share relevant information everywhere in the world at any given moment. Cloud services enable us to reach a new level of customer experience.
Cloudification is not only about technology, although that’s clearly important. It’s also about adjusting and improving organizational structures and processes. It’s about governance, finance, innovation, culture and people. Rather than focusing solely on technology, we also attempted to consider the Cloud against the background of organizational change. Thanks to the Cloud, different departments can communicate and co-operate in real-time.
With the Porsche Innovation Day last week, it was also our aim to give stakeholders the opportunity to present their solution portfolio to the Porsche Board of Management as well as to all IT interested Porsche employees. The presentations that were given by our partners were truly fascinating and highlighted how our departments and employees can embrace and benefit from cloud services. Given its transformational potential, it’s important that we have a robust understanding of the technology and its potential. At Porsche, we want to be one step ahead when it comes to digitization. There is no time to waste, as IT is becoming a shaper of future innovation.
It was a pleasure to organize the Cloud Innovation Day and we’re looking forward to doing this again next year!
Andy Grau is Innovation Manager at Porsche AG. Nils Kreft is Product Owner Cloud Enablement at Porsche AG.
About this publication: Where innovation meets tradition. There’s more to Porsche than sports cars — we’re tackling new challenges, develop digital products and think digital with focus on the customer. On our Medium blog, we tell these stories. It´s about our #nextvisions, smart technologies and the people that drive our digital journey. Please follow us on Twitter (Porsche Digital, Next Visions), Instagram (Porsche Digital, Next Visions, Porsche Newsroom) and LinkedIn (Porsche AG, Porsche Digital) for more.
Next Level German Engineering: Where innovation meets…
9 
9 claps
9 
Written by
Official Medium Account of Porsche AG | #NextLevelGermanEngineering #createtomorrow | More: newsroom.porsche.com |
Next Level German Engineering: Where innovation meets tradition. The Porsche technology hub to create tomorrow.
Written by
Official Medium Account of Porsche AG | #NextLevelGermanEngineering #createtomorrow | More: newsroom.porsche.com |
Next Level German Engineering: Where innovation meets tradition. The Porsche technology hub to create tomorrow.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/hackernoon/is-cloud-computing-insidious-without-a-cloud-exit-strategy-c53e61641209?source=search_post---------100,"There are currently no responses for this story.
Be the first to respond.
Today’s IT of a company has the role of a real service department. It also covers issues such as cost-efficient IT operations, data security, failure security, BYOD (Bring Your Own Device), mobility and much more. Consequently and logically of course, aging IT infrastructures turn out to be a thorn in the eyes of every decision-maker with a future-oriented thinking. The more modern IT infrastructures are, the more they count as a competitive advantage. The idea is to apply a strategic and innovative approach to technology. The solutions must therefore be perfectly tailored to business requirements.
However, as business development progresses, the infrastructure must be able to cope with new modern initiatives and steady growth. Cloud computing with its role as the digitization par excellence is playing an increasingly important role in the IT world. As the “next big thing”, the cloud stands for a changing world. Almost like victorious gladiators from ancient Rome, cloud-based software solutions expel long-held beliefs and products from apparently impenetrable IT landscapes. Decision-makers from companies, whether small, medium-sized or large, are more than ever interested in equipping their companies with cloud-based technologies.
Surprising is the fact that only a few years ago the majority of companies were still very closed towards cloud computing. In the 2016 report “The Business-Technology Performance Connection for SMBs”, the SMB Group has shown the shift towards cloud computing. This development is explained by the fact that more and more companies see the cloud as the most cost-efficient, flexible and best solution for providing a future-oriented IT infrastructure. In fact, today’s decision-makers deal with issues such as digitization, Big Data, Internet of Things (IoT) and, in general, the need for innovation to remain competitive. At the same time, however, costs must be kept as low as possible. The opportunity of having a modern IT infrastructure without the need for substantial capital investments and personnel increases, which also meets the high standards of services and products is a chance every decision maker wants to take.
Due to the rise of the cloud computing industry and many competing companies shifting to the cloud, there is definitely a high risk of making quick shot decisions. Gartner, Inc. predicts that over the next five years more than one trillion dollars of IT spending will be directly or indirectly affected by the shift to the cloud. Forbes goes even further by making clear that the risk of remaining closed towards the cloud is more harmful for one’s business than moving towards it. However, adopting a new technology is one thing, the question or risk of making irreversible investment is another thing, especially in the case of cloud computing. Companies seem to be over-excited towards cloud computing to think about a cloud exit strategy. Without a clear cloud exit strategy, one may end up locked into a single cloud provider. The latter may have the advantage of making the price at will and the customer might have no opportunity to leave anymore since time, effort and money to exit the service are considered too high.
Unsurprisingly, the majority of companies regardless their size, have a preference for well-known cloud providers, because established ones like Amazon AWS, Microsoft Azure, Google Cloud Platform, IBM Softlayer and Alibabacloud are believed to respect their promises and to stay in business. Today Amazon, Microsoft and Google lead in Gartner’s Magic Quadrant. Together they share a market with a value of billions of dollars. However, is choosing a big name the right strategy?
This is indeed a question that allows new entrants to this promising market such as n’cloud.swiss to convince customers by pointing out several advantages. The cloud platform which is paving its way with an aggressive expansion strategy as the “Swiss made” alternative to the major cloud providers, scores with a number of advantages for customers that range from agility, flexibility and adaptability to a maximum amount of freedom to build their cloud according to their needs. The Cloud Act seems thereby to play in their cards given the fact that the cry for a non American cloud provider and a serious alternative to Amazon AWS, Microsoft Azure & Co. is louder than ever. Global IT service provider Luxoft has recognized the potential of a Swiss alternative and is planning according to Business Insider to create the first customizable blockchain-based e-voting system in Switzerland by deploying the platform with the help of not only Amazon AWS but with the help of n’cloud.swiss as well.
All things considered, the entry into the world of cloud computing seems to be easy — however, the exit can turn out to be financially disastrous. American giant Apple is known for making it hard for customers to turn their back to Apple’s World. Dan Tynan once summarized Apple’s strategy as follows: “Once you enter the Big Tent of Apple, it’s exceedingly hard to find the exit.” The question now is: How should cloud providers and their customers deal with the issue surrounding the importance of having a cloud exit strategy?
#BlackLivesMatter
145 
how hackers start their afternoons. the real shit is on hackernoon.com. Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
145 claps
145 
Written by
Dynamic and data-driven marketing leader. Author, Mentor & Speaker. Top 50 Global Thought Leader. See www.yahyamohamedmao.com
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
Dynamic and data-driven marketing leader. Author, Mentor & Speaker. Top 50 Global Thought Leader. See www.yahyamohamedmao.com
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/decentralize-today/with-bletchley-microsoft-brings-cloud-computing-to-the-blockchain-35c36bb52cc0?source=search_post---------101,"There are currently no responses for this story.
Be the first to respond.
Industry observers acknowledge that today’s Microsoft is vastly different than it was a few short years ago. One of the most obvious differences is in its Azure cloud offering. In the past, Azure seemed to suffer from stop-and-start management attention and, consequently, lagged in functionality, awareness, and customer adoption.
That state of affairs has changed dramatically since Satya Nadella became CEO in early 2014. Today, Azure is widely considered a strong number two in the cloud industry and its functionality is excellent. Nevertheless, Microsoft has struggled to gain ground on AWS, the industry leader. In part, that’s because it’s hard to leapfrog someone running at full steam. It’s also because doing what everyone else does as good as they do or even a bit better isn’t enough to gain the crown of number one. And in this respect, Microsoft faces perhaps the biggest challenge in the industry: AWS is so innovative, it’s hard to find an area in which it isn’t the pioneer. Just look at what it’s done in the past 18 months with Lambda and Echo.
Today, however, Microsoft has found, perhaps, its first uncontested, innovative, important territory that it can lay claim to. Project Bletchley addresses the white-hot blockchain market and delivers functionality that addresses shortcomings in the current offerings on the market (Here is a Microsoft github document that discusses Bletchley in more detail). It positions Azure to become the computing fabric for a potentially enormous revolution in financial transactions and asset ownership. To give you an idea of the scale of the potential impact of blockchain, consider that the 2015 total size of the IT industry worldwide, according to Gartner, was around $3.5 trillion; financial services, on the other hand, McKinsey (with some help from Investopedia) estimates was $13.1 trillion in 2014 — nearly four times as large.
Starting with Blockchain-as-a-Service
Microsoft began its blockchain support last November, when it launched its Blockchain-as-a-Service (which I’ll refer to as BaaS for the rest of this post to save my fingers from a ton of typing), supporting the Ethereum blockchain. Ethereum offers a different blockchain than the one that underpins Bitcoin; it provides a full (i.e., Turing complete) scripting language designed to enable smart contracts that are richer and more flexible than the capability of the Bitcoin blockchain.
Want to understand smart contract pros and cons? Learn the four benefits and three drawbacks of smart contracts in this white paper.
The BaaS offering was a smart move on Microsoft’s part. There’s intense interest in blockchain potential throughout the financial services industry which is centered around consortia that have been formed to facilitate experimentation. Clearly this requires a testbed; just as clearly, consortia are not really set up to build out computing infrastructure. Azure can supply this easily and with great flexibility. The consortia win by getting to avoid a lot of plumbing work and starting experimenting quickly; Microsoft wins because it has a new customer that will consume lots of computing resources. The offering also plays to Microsoft’s strengths in the enterprise market, making it a trusted partner for companies delicately exploring a technology that, while perhaps offering tremendous efficiencies, also holds the potential to be enormously disruptive.
Project Bletchley: Blockchain Innovation
Project Bletchley, however, is an offering of a different order — and must be recognized as a bold and innovative step by Microsoft. It represents MIcrosoft’s attempt to become a core part of the emerging nexus of blockchain and fintech — that is, Microsoft’s objective is clearly to become a fundamental part of every organization’s blockchain adoption. Given the size of the financial services market observed above, if it achieves that, Azure could grow to gigantic proportions.
Microsoft has put three elements into Bletchley to wrap core blockchain functionality and to surround blockchain-based applications (aka smart contracts) with critical capabilities necessary for them to address real-world use cases. Here they are in ascending order of importance:
Azure Marketplace
Azure will provide a place for organizations to place and retrieve smart contracts. This makes sense since most organizations won’t or can’t write their own smart contracts, which can be very challenging to get right (as the recent DAO hack demonstrated). Rather than create their own smart contract, organizations will take one from the marketplace, use it as-is or perhaps modify to suit their specific situation.
The benefit of a smart contracts marketplace is significant. For users, being able to leverage an existing artifact rather than attempt to create one from scratch makes it easier to use smart contracts. From Microsoft’s perspective, anything that reduces adoption friction grows usage and revenue. More importantly, it ties users more tightly to the Azure service; once they’ve begun using the Azure smart contract templates and have them running on BaaS, there’s less reason to explore another provider’s blockchain service, should one come to market. It’s easy to overlook the power of reducing friction, but look at AWS: it has built a $10 billion business by providing easy-to-use computing.
Blockchain Middleware
Much more strategic, and much stickier in terms of customer use, is the Azure Blockchain Middleware. For all the discussion about the revolutionary nature of blockchain and smart contracts, it’s important not to overlook one central truism: this economic activity will occur within an existing ecosystem of industry players, technology solutions, legacy applications, and established practices.
Blockchain Middleware’s aim is to make it easier to embed smart contracts inside that ecosystem.
Microsoft aims to ease integration of blockchain smart contracts with Azure Active Directory and Key Management services, along with off-chain data communication with audit applications, machine learning, etc. Moreover, the Middleware offering will include an encryption service so that fields and transactions can be encrypted to prevent viewing by undesired parties (e.g., competitors to one of the parties creating a smart contract).
These services make it easier for users to surround standalone smart contracts with extended functionality, and, of course, this ease further wraps users in Azure’s embrace. These additional middleware services create powerful lock-in; organizations considering moving off of Azure’s BaaS would need to figure out how to recreate (and operate) the additional middleware services with which they integrate their smart contract applications. One can make two observations: (1) Blockchain Middleware presents an opportunity for Microsoft to make it as unthinkable of moving a smart contract to another environment as AWS’s middleware services (e.g., Simple Queue Service) make it unthinkable to migrate applications out of that environment; and (2) while people decry lock-in, users embrace lock-in when it provides significant benefit vis a vis the other options.
Barely mentioned in a sort of sotto voce fashion is the fact that Blockchain Middleware will support blockchain software other than Ethereum, including Hyperledger. In other words, Microsoft plans to work with all variants of blockchain that gain market traction.
Cryptlets
The most intriguing — and powerful — part of Bletchley, to my mind, is Cryptlets. What is a cryptlet? Microsoft describes them as
Cryptlets are off-chain code components that are written in any language, execute within a secure, trusted container and communicated with using secure channels. Cryptlets can be used in SmartContracts and UTXO systems when additional functionality or information is needed and provided via a “CryptoDelegate” or adapter.
Microsoft provides this image to illustrate Cryptlet functionality:
One of the key challenges of smart contracts is that they are supposed to help you with real-world agreements. For example, someone wants to sell a future contract against the price of wheat six months from now. To price that future during the ensuing time period, the spot price of wheat is a critical input (i.e., as the price of wheat varies, so too does the price of the future).
How can the current price be input into the futures smart contract, and how can all parties trust that the price is honest? After all, each party to the contract has an incentive to adjust the price. It’s critical that this kind of real-world event information be accurate. This challenge, called the oracle problem, is enormous, as without trustworthy information, the entire futures exercise is pointless.
Cryptlets are designed to provide a secure, trustworthy way to serve as an oracle to a smart contract. Of course, wheat futures are only one example. Another might be a rental payment for a temporary lodging (e.g., an Airbnb apartment). How can the smart contract know the lodging has been vacated so that final payment can be charged to the renter? A cryptlet. Microsoft envisions a whole range of what it calls utility cryptlets — timestamps, authentication, etc.
Another use for cryplets is as a smart contract surrogate — i.e., code that captures an agreement. Some blockchains do not provide as rich a scripting language as Ethereum, making the ability to create smart contracts offchain very useful (and indicating why Microsoft’s middleware ambition extends beyond Ethereum). In this variant, a cryptlet would contain the smart contract and execute it on behalf of the transaction contained within the blockchain.
While Microsoft indicates the cryptlets will be released as open source so that they can be run anywhere, this is, again, an enticement to stay within the Azure ecosystem. Why go to the work of porting the cryptlet source code and running the service oneself, when Azure will take care of all the work for you — allowing you to focus on the business opportunity that the cryptlet implements.
Cryptlets offer incredibly powerful functionality and reduce the risk associated with smart contracts significantly. Microsoft certainly agrees, referring to this new capability as Blockchain 3.0 (and here you were thinking that we were still struggling toward Blockchain 2.0!).
Conclusion
With Bletchley, Microsoft marks out its ambition to set the pace in an emerging area of cloud computing — one in which it can break free of the pack and create an entire new category of functionality.
It was surprising to me that most of the technical press addressed this announcement in a rather perfunctory manner — parroting Microsoft’s talking points without, seemingly, understanding their implications. I attribute this to the fact that, for most people, blockchain is something they’ve heard of but can’t quite figure out — something exotic and not very important.
Learn the four benefits and three drawbacks of smart contracts in this white paper.
One could have made the same observation about the Amazon Echo mid-last year — how important could the ability to order Amazon products by voice be? In short order Amazon opened up the Echo to developers and they started creating “skills” to connect it to all kinds of things — home lighting, front door locks, and even opening a garage and backing a car out of it. Echo is rapidly becoming the de facto home automation center of gravity. Amazon talks about Echo as its next billion dollar business. With Project Bletchley, Microsoft suggests its ambition to become the blockchain center of gravity — but with more zeroes.
Your Daily Dose of Decentralization
10 
10 claps
10 
Your Daily Dose of Decentralization
Written by
Named by Wired.com as one of the ten most influential persons in cloud computing. Learn more at bernardgolden.com
Your Daily Dose of Decentralization
"
https://cloudrumblings.io/mentoring-a-millenial-4e9a137cfa88?source=search_post---------102,"Winston Frick is currently in his second year as computer science major at the University of Virginia. Over the summer, I shared my passion for cloud computing with him as part of an internship — that has now evolved into a longer-term mentorship.
Along the course of the summer, we sought opportunities to gain hands-on experience by hacking drones, building Alexa skills, and exploring the writings of thought leaders connected to cloud computing.
The internship turned into a mentorship that continues today, and goes beyond sharing technical knowledge and industry experience. I’ve invited Winston to share his insights as a guest blogger. In return, he promised to invite me to a tailgating party at UVA, and to consider me for a position after he graduates.
When I first arrived at my internship I was eager to learn, and more than a bit nervous. I had just completed my first year as a Computer Engineering major at the University of Virginia, and my experience was limited to building my own computers and taking a semester of Python programming.
I was about to intern with the a major technology company that is known for their cloud engineering expertise, but I had never heard of “AWS”.
After a summer of immersive training and mentorship, my life has changed dramatically.
As a result of the mentorship, I learned three additional programming languages and switched my major to Computer Science. So many doors have been opened based on my experience:
Throughout the experience, I’m fortunate to have an mentor that abandoned traditional training in favor of interactive DIY projects, A Cloud Guru self-paced training, and encouraged the development of practical skills. The more I learned about cloud computing over the summer, the more excited I became to pursue a software engineering career.
As part of the mentorship, Drew suggested that I keep a daily blog using the following categories to capture my experiences:
Each morning, we would spend a few minutes reviewing my previous day’s experience. The daily diary provided us a chance to talk what I learned, my plans for the day, and then discuss anything that was confusing.
My initials blog entries were heavy on the last category.
To help provide the business context of cloud computing, Drew encouraged me to start by reading Steve Case’s “The Third Wave,” and “The Big Switch” by Nicholas Carr. Those books help me quickly realized that the future of business is about developing in the cloud, and not circuits of a motherboard. So I switched my major from Computing Engineering to Computer Science.
Within the first week, Amazon Web Services (AWS) became a technology that I used daily as the basis for my all projects. I am still amazed at all of the services that are available on AWS that allow me to quickly deploy servers, storage, and functions. It’s easy to see how companies can combine AWS with Agile and DevOps to quickly design and deliver new products to their customers.
Besides learning new technical skills, I also experienced the culture of a leading technology company. I really like the open office layout, and the campus had everything you can imagine including a bakery, frozen yogurt, a bike share program — and a tree fort.
Even more impressive is the willingness of other engineers to help me troubleshoot and share their insights and experiences. In particular, I was given a masters-level engineering education from Terren Peterson. He is a vice president of cloud engineering teams migrating applications to the cloud, yet took the time to coach me on programming languages and cloud architecture.
Even though it’s a big company, it seemed like everyone knows each other. One morning, I was waiting with Drew in line at the campus Starbucks for our morning coffee. We started a conversation with a co-worker who talked about his son’s upcoming graduation from UVA. He asked me about my classes and my internship experience. After he left, I was informed that I had been talking to the CIO of Capital One!
I am very grateful to all the people I met and their willingness to share their knowledge and expertise. The mentorship has created so many new opportunities and paths — it is definitely a life-changer.
Follow me on twitter @Winston_Frick to see where this journey takes me next!.
cloud adoption and talent transformation | engage → educate…
13 
13 claps
13 
Written by
migrating talent to the cloud at acloud.guru
cloud adoption and talent transformation | engage → educate → evolve
Written by
migrating talent to the cloud at acloud.guru
cloud adoption and talent transformation | engage → educate → evolve
"
https://medium.com/high-alpha/a-cloud-computing-recap-of-mary-meekers-internet-trends-2017-530aaa974e?source=search_post---------103,"There are currently no responses for this story.
Be the first to respond.
Mary Meeker gave her Internet Trends 2017 talk last week, and as usual there was a ton of meaningful and valuable content in the 355 slides. She covered a ton of interesting industries such as gaming/e-sports (a personal favorite of mine), healthcare, and media.
Out of all of the big sections, however, I was most intrigued by the cloud computing section—naturally, since I work at a B2B SaaS venture studio. I wanted to take a few moments to highlight some of the big takeaways from this section of the report.
Unsurprisingly, cloud solutions are becoming much more popular in infrastructure spend compared to traditional data centers. The ratio has gone from around 25% cloud to almost 40%. With this increase, the Public Cloud is a battleground for data center and cloud service providers. AWS dominates the space with Azure and Google Cloud (to a lesser degree) making up some ground in the past few years. This has the making for a duopoly going forward which has significant ramifications on the pricing and availability of services.
One of the most interesting data points in the public cloud section is the concerns of purchasers of cloud products. Data Security maintained it’s place as the biggest concern (although it has dropped significantly), but the significant drop-off in cost savings and control has paved the way for compliance/governance and lock-in to come in as big concerns. This demonstrates the shift in attitude toward cloud computing as a “new solution” to something that is much more highly regarded. The concern shift shows how the cloud has moved from an investment decision based on cost and security to more about implementation.
This has bode well for the new economy of companies that are attempting to create the modern version of solutions such as BI, transaction management, data infrastructure, and healthcare in the cloud.
Over the past 15 years, the consumer expectation of business software has significantly changed. The bar for an MVP has gone up massively, and the expectation of customer service and performance has grown exponentially. We are now treating business software in the same way that we treat our own personal purchases. While our expectations of companies such as Amazon to provide free shipping, more content, and faster fulfillment increase, so do our expectations of cloud software. In the past 17 years, the mechanism of how we interact with software has changed; from the delivery (on-premise to cloud), pricing (perpetual license to subscription), UX (generic to personalized), intelligence (simple function to AI/ML), and so much more.
And with this, enterprise cloud has shifted to a customer focus, which puts an increasing onus on design and user interface. Thus, the ratio of designers to developers has massively increased in some of the most important cloud software giants out there.
Cloud applications are on the rise in the enterprise, however, most of the applications being adopted are not actually ready from a security standpoint to be used at the enterprise level.
At the same time, spam (with and without malicious attachments) has grown over 350% year over year which has directly correlated to a big rise in the amount of cyberthreats and security breaches. Bots again have started increase their share of the internet traffic, which is making the cloud an area where threats are increasing. This has major implications on how cybersecurity and data management works in the future. Investment is going up in these spaces and it is becoming an increasingly important sector in the cloud computing world.
I highly recommend checking out the rest of the deck below for other great content. All of the slides were taken from the KPCB deck, so thank you to KPCB for the data and resources.
High Alpha is a venture studio pioneering a new model for entrepreneurship that unites company building and venture capital. To learn more, visit highalpha.com or subscribe to our newsletter on the future of enterprise cloud.
Stories, resources, and ideas on the future of SaaS
9 
9 claps
9 
Stories, resources, and ideas on the future of B2B SaaS from the High Alpha network and team. Learn more at www.highalpha.com.
Written by
Research Analyst at High Alpha
Stories, resources, and ideas on the future of B2B SaaS from the High Alpha network and team. Learn more at www.highalpha.com.
"
https://medium.com/@ericgarland/how-the-nsa-took-the-heaven-out-of-cloud-computing-9b3e514c9514?source=search_post---------105,"Sign in
There are currently no responses for this story.
Be the first to respond.
Eric Garland
Jul 23, 2013·4 min read
We are all prisoners of metaphor when it comes to describing the world. English is a hodge-podge of Greco-Roman references and grunty Germanic loan words, so it should come as no surprise that precision is difficult to achieve through language. We are forced to analyze everything through a series of blurry images and loaded connotations.
Now, imagine trying to understand enterprise-scale information security using such brutish tools. Take the notion of “cloud computing.” We describe a massive global infrastructure of networks, servers and protocols with a term reserved for condensed water vapor in the sky. This makes no sense until you consider one of the additional uses of the image. Christian mythology has made the clouds synonymous with heaven, the supposed locale of Paradise. Now the reason behind the choice of metaphor becomes clearer. Clever marketers use a term loaded with dreams of eternal peace. Your data is in…THE CLOUD! If you close your eyes, you imagine your data dressed in a clean white tunic, seated comfortably on a fluffy white Ottoman, perhaps listening to soothing harp music, kept in the loving embrace of the Almighty for Eternity.
Now, open your eyes. Your data is on a server in Kentucky. Or maybe Hong Kong. Or maybe it was duplicated onto “backup” server in China. Maybe Mongolia. You don’t know.
Your data is not up in the sky, it is housed by the physical assets of a private corporation operating under the aegis of a nation-state, one that likely makes its digital signals available to relevant intelligence and police services. That harp music is dying away, replaced by the clicks and whirrs of a Borg-like complex of disk drives in some strange country.
In theory, cloud computing should be an ideal fit for the needs of businesses large and small. Using the cloud, companies can outsource management and maintenance of servers to specialized contractors who can lower costs while increasing reliability. Moreover, cloud computing should lower the risk of catastrophic data loss because the management of servers is handled by dedicated professionals.
This nonchalant comfort with the use of transnational digital service providers shows just how far the world has come since the Cold War. There is a level of trust between organizations that would have seemed insane fifteen years ago. Companies that compete for marketshare in several sectors of heavy industry think nothing of having the same software providers for critical functions such as finance and accounting. Boeing, a major defense contractor to the U.S. Government among other things, actually explored the idea of moving its headquarters to China. Companies all over the world use the banking systems of Ireland, Mauritius, and the Caymans with the express intent of depriving their nation-states of tax revenue — and yet this is not seen as an act of war. In 2008, the U.S. Federal Reserve, backed by U.S. taxpayers, sprang into action to recapitalize banks in Europe as well as those in the United States. So if normally competitive countries are so chummy these days, it should be no big deal to use a cloud computing service from anywhere…right?
Into the fray comes Edward Snowden, that unexpected new resident of Sheremetyevo Airport who deftly tore the mask of bonhomie from the neoliberal international system. A couple of articles in the Guardian later, and the world looks the Kissingerian image of realpolitik. Instead of a bunch of relaxed, cheerful partners, the world is divided once more into nation-states: The U.S. China. Russia. France. It’s back to great power politics, and if you have the juice, then you bug the G8 meeting, or all of Brazil.
Oh my God, where is our data again? Is it in…gasp…a competitor nation? And now the weakness of metaphor is exposed. Instead of a velvet painting of clouds and angels, “cloud computing” looks like a balance sheet, a trade-off of risks and opportunities. Should a company foot the bill for its own data services, or outsource it to some company with dodgy owners in a country that isn’t exactly friendly? Digital strategy looks complex once more.
We perceive ourselves to be living in a Golden Age of digital peace, where anything tech related has seemed to transcend geopolitical risk. If you read history, that kind of peace does pop up from time to time…but it is illusory. For there is no “heaven on Earth,” and your data doesn’t sit on a “cloud.” We thus require some new metaphors, so that we might come by a clearer and more rational view of the role of information technology plays in a geopolitical dynamic that resembles the twentieth century more than we believed.
>>>>>
I provide strategy and competitive analysis to executives from business and government here and I’ve been known to rock a microphone here.
Expert in strategic trends and decision making. Author. Linguist. Bassist.
8 
8 
8 
Expert in strategic trends and decision making. Author. Linguist. Bassist.
"
https://medium.com/cudos/blockchain-transforming-cloud-computing-94a76af89354?source=search_post---------106,"There are currently no responses for this story.
Be the first to respond.
The world of cloud computing is evolving every day, however despite the growth in terms of innovation the world of cloud computing is starting to become increasingly siloed. When deciding which cloud service to go with the choices that businesses have seem to be decreasing, with large corporations contributing to an oligopolistic market structure.
According to the seventh annual Cisco Global Cloud Index (2016–2021) globally, cloud data centre traffic will represent 95% of total data centre traffic by 2021. In the event there was any market power abuse or distortions, there would definitely be ripple effects across a range of industries. This is where blockchain steps in.
The intrinsic characteristics of blockchain technology make it possible to address the unfavourable market dynamics in the cloud sector mentioned previously, reduce supply-side inefficiencies, enhance scalability, and reduce total costs for businesses.
The decentralized nature of blockchain technology lends itself to improving the overall security of cloud computing systems. As well as the benefits to security activating and distributing unused compute power around the world enables cloud capacity to be leveraged to the user and their devices, democratising access to cloud-based compute power which in turn contributes towards creating a healthier and more dynamic market structure.
Blockchain technology opens up the marketplace for companies and individuals that have surplus compute power by allowing them to become suppliers of cloud capacity, and companies/users in need of cloud capacity will have access to their cloud capacity needs at a fraction of the prices offered by the major cloud service providers. The outcome is highly efficient utilization of compute power that may have gone unused, as well as a significantly lower market price for every Gig of cloud computing capacity.
With the emergence of blockchain-powered cloud offerings, the future is bright, here at CUDOS we are working hard to support a more secure and flexible cloud infrastructure, and provide a high-quality service at significantly lower costs when compared to the top 4 cloud service providers. Find out more about what we’re doing here at CUDOS by visiting www.cudos.org or clicking on one of the links below.
Next Generation Cloud
51 
51 claps
51 
Written by
Next Generation Cloud
Next Generation Cloud
Written by
Next Generation Cloud
Next Generation Cloud
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.datadriveninvestor.com/boost-your-career-with-cloud-computing-da2879f2216d?source=search_post---------107,"There are currently no responses for this story.
Be the first to respond.
A couple of weeks ago I had the privilege to speak in a setting which was unfamiliar to me. Usually, I speak about professional topics — Software Architecture, Cloud Computing, etc., but this time it was different. This time I was asked to speak about myself. A good friend of mine told me that he was organizing an event about how developers and IT professionals can grow their career by getting more into cloud computing. He asked me if I can talk about why is it important to understand the cloud, and how I personally got into the field. While I’m not usually the person who likes to speak about himself, I believed that the cause was crucial — getting more developers and IT pros to be familiar with the cloud. The cloud is the modern platform which drives the entire industry forward and has the power to transform one’s career. So…
Basically, the cloud can be seen as someone else’s big machine. You input your credit card number, and you get a whole lot of computing, storage and networking capacity without having to manage the intricate details of building and maintaining a data center yourself. However, this is a rather simplified point of view; the cloud offers much more than simple Virtual Machines for those who seek to take advantage of it. Services like Serverless Compute, Database-as-a-Service (DaaS), Machine Learning as a Service and others rapidly change the way we build systems. These services are making the process much easier and faster on the one hand but much more complex on the other, with complexity rising from one day to the other as new services are introduced. This leads me to the next analogy…
www.datadriveninvestor.com
I like to view the cloud as a pile of Lego bricks; each brick has it’s own shape and color and you connect the bricks in various ways to create new creations. This is the job of the software architect and I find it as one of the most enjoyable aspects of my work. Let's say, for example, that you want to build a “standard” web application consisting of a modern SPA front-end, a back-end API and a relational database. Before the modern cloud, you had to think (among other things) about how to host the application, how to scale it, how to support disaster recovery (DR), how to update the host OS, etc..
With the cloud, you focus on your own application code and use the appropriate bricks to handle the other concerns. In our example, we can use a BLOB store for the SPA, some Serverless platform for the API and a DaaS for the DB. These services all provide inherent solutions for scale, DR, infrastructure maintenance — so that you won’t have to come up with a solution yourself. You need to keep in mind, though, that these services come with their own bag of concerns and issues that we need to deal with — but who said that building Lego art is an easy thing. 😉
When I teach Cloud Computing to developers and IT pros, I often ask them the following question — “At you company, how long does it take from the moment you need a computing resource and until you finally get it?”. The answers I get for this question are varied. Some quote times of up to a day. Others mention several days. However, some participants always quote an answer in the range of months. Months! Imagine yourself in an environment where it can take days to months to receive a required computing resource? At companies which do not utilize the cloud, this can often be the case. This, in turn, leads to reduced productivity and lower developer happiness overall.
Enter the cloud. In the cloud, the need for a computing resource translates to an actual computing resource within minutes. Not months, not days, but minutes with a click of a button or script execution. The world will never be the same again…
Serving your application to millions of users around the globe is not an easy task; neither from the software perspective nor from the infrastructure point-of-view. Unless your business is building data centers, managing and maintaining huge data centers is probably not your specialty or your company’s specialty.
Using the cloud, you can achieve high scale with (again) a click of a button. When I say “scale” I refer to it in terms of both computing power and geographic scale. Of course, scaling the applications to cover multiple geographies and millions of users is still complex on the software level. But at least with the cloud, you can focus on your own business instead of the infrastructure running it and that is a power multiplier for any business.
Cloud Computing can often be cheaper than maintaining your own data-center since public cloud vendors utilize economies of scale. However, practitioners sometimes calculate and find that the cloud increases their expense. While this can be the case for very specific scenarios, it contains a couple of oversights more often than not.
Comparing Apples to Oranges
When comparing the cost of the cloud to the cost of maintaining your data center you need to make sure to compare comparable things. Cloud computing costs take into account two important factors. One factor is the fact that most cloud services provide a Service Level Agreement (SLA) for availability and/or durability. Maintaining similar 99.9%, 99.99% or even 99.999% availability SLAs on-premise is not an easy task and one which costs quite a lot of money. Most production applications require an SLA.
Another factor is the cost of maintenance. Managing and maintaining an on-premise data center requires a lot of IT personnel while cloud services cost fold that cost inside the service. It’s true that you still need IT professionals when utilizing the cloud, but their tasks are significantly different in such a scenario and you’ll usually need less when comparing both scenarios side-by-side.
Auto-Scale Economy
Another (positive) issue with cloud economics is the auto-scale economy. When you plan an on-premise data center, you usually need to plan the capacity to match the maximum required performance level. However, as most workloads are seasonal, it makes no sense to pay for full capacity all of the time. Examples of such seasonality are daytime vs. nighttime and weekday vs. weekend where there is simply no justification to pay for all the computing resources all the time.
This is where the cloud really shines — scale out your computing resources during peak times and scale down during the lows. And the best part — pay for only what you use, when you use it. From my experience, utilizing such a tactic can reduce your computing costs by as much as 60% depending on your specific workload.
In 2008 I was a software architect at a government organization where public or private cloud options were almost never even heard of. Most of my production systems at the time ran on top of physical machines, and virtual machines were not considered a real option for production. I still remember the time when the IT person called me and said that “the servers crashed” meaning the servers physically crashed from the rack… 🤣
The situation in the world was not much different. AWS was launched two years before, and Azure was announced at the end of the year. Cloud still seemed very far away, as can be seen in the following Gartner Hype Cycle for Emerging Technologies from 2008.
In 2010 I co-founded CodeValue, which is an expert software services company. Cloud Computing was spoken about a lot, but real customers almost didn’t want to hear about it.
At that phase, only young startups agreed to consider the cloud, and the cloud (consultancy) business was quite small at the time.
In 2012, big customers still didn’t want to hear about the cloud. Maturity and security were still a big concern, while Virtual Machines and on-premise servers were well understood. Like every emerging technology, cloud computing hype was reduced and was spoken about less. Sure, it was still innovative at IT/Dev conferences, but other than that we still didn’t know if it will catch on.
For me personally, 2014 was a big turning point for the Cloud. For many customers, the Cloud has suddenly reached maturity. Starting with this point in time, more and more use-cases were found as suitable for the Cloud. Customers were more open to hearing about the Cloud, and its spread increased from day to day.
Today, when approaching customers with new projects the Cloud is the default choice. No one even thinks about running a system on-premise unless they have a specific reason (such as regulation) to run on-premise, and its advantages are widely accepted.
This is a true revolution, and this is the main reason why I believe that every IT Pro and developer should master the Cloud.
All cloud platforms are advancing at a very high pace. The sheer amount of technologies and services that each professional needs to be aware of is huge. It already happened to me once that I was sure that something was not possible using a specific cloud service, only to be shown my mistake after it actually added support for that scenario in the previous week.
Staying on top of things in the Cloud era requires passion and dedication. My recommendation to those who want to get into the Cloud and promote their career is to start small and progress one step at a time. Don’t get overwhelmed by all the services various vendors are offering. Choose something that excites you and start from that. Build a real project with that service. From my experience, learning is done best by doing.
In addition, choose a learning accessory according to your personal learning style. Whether you like reading books or blog posts, watching online courses or asking at various online communities — find the one or two things which can promote you and act, so that you won’t get left behind.
The slide deck from my talk can be found below. Enjoy!
Originally published at https://stiller.blog on July 8, 2019.
empowerment through data, knowledge, and expertise.
97 
97 claps
97 
empowerment through data, knowledge, and expertise. subscribe to DDIntel at https://ddintel.datadriveninvestor.com
Written by
A software architect, consultant, and instructor. CodeValue founder & CTO. Microsoft Regional Director (MRD) & MVP on Azure. (https://stiller.blog)
empowerment through data, knowledge, and expertise. subscribe to DDIntel at https://ddintel.datadriveninvestor.com
"
https://medium.com/@jameshamann/cloud-computing-service-types-3da6998a7a11?source=search_post---------108,"Sign in
There are currently no responses for this story.
Be the first to respond.
James Hamann
Nov 24, 2017·3 min read
These days most people refer to things being in The Cloud but what does this actually mean? A general, high-level definition is the delivery of hosted services over the internet. Email, calendars, todo lists, photos, pretty much everything fits into that description. Think Google Drive backing your photos up or iCloud backing your iPhone up, both of these keep your data in stored in the cloud. This makes it easy to access your stuff across all of your devices. There are, though, a few different levels to cloud computing.
This is the outermost layer of the types, typically targeted at the end customer. The provider makes their application available through a web-browser, mobile app or dedicated desktop app, which is powered by cloud infrastructure. Lets use Google Sheets as an example here. The benefits provided to the consumer include the ability to create, edit and update spreadsheets from anywhere, with multiple users able to edit at one time.
This is the middle layer of our pyramid, typically used by developers. Here a developer can deploy their app, written in whatever language they’ve chosen, to a pre-configured cloud infrastructure. The developer doesn’t deal with the configuration of the servers, databases or operating systems. Instead they are able to deploy their app with ease and speed. Let’s use Heroku as an example here. The benefits provided to the developer is quick, easy deployment with minimal hassle. You can push a rails app live within minutes, it allows for rapid prototyping and gives developers the ability to deploy an app without needing to worry about things like server configuration.
This is the final layer, where all the nuts and bolts lie. This level is typically used by sysadmins who would be charge of provisioning servers and deploying application builds. From here you’re able to completely configure everything, from operating systems and network settings. Lets use AWS’s EC2 instances as an example here. From the EC2 console when deploying a new instance you’re able to choose from a wide range of AMI images. When your instance is setup, you can SSH in and configure it in anyway you see fit. This gives the user huge levels of customisation and flexibility, which is important if you’re running apps that require very specific needs.
The diagram below provides a visual representation of each level.
Beyond this there are further abstractions like DaaS (Data/Desktop as a Service), STaaS (Storage as a Service) and SECaaS (Security as a Service). These are quite specific, focused in particular sectors and not as commonly known as the main three mentioned above.
As always, thanks for reading, hit 👏 if you like what you read and be sure to follow to keep up to date with future posts.
Software Developer https://jameshamann.com
11 
11 claps
11 
Software Developer https://jameshamann.com
About
Write
Help
Legal
Get the Medium app
"
https://architecht.io/the-convergence-of-cloud-computing-alibaba-google-and-china-2b490b51cfb0?source=search_post---------109,"This is a reprint (more or less) of the ARCHITECHT newsletter from Aug. 6, 2018. Sign up here to get new issues delivered to your inbox.
This is a reprint (more or less) of the ARCHITECHT newsletter from Aug. 2, 2018. Sign up here to get new issues delivered to your inbox.I really couldn’t think of a more compelling headline for this issue, which is based on a handful of stories I’ve read over the past few days — ranging from a summary of Gartner’s Magic Quadrant for Public Cloud Storage to the U.S. Senate’s letter to Google over its rumored censored search engine for China. Here they all are:
I suggest you read them all, but let me quick recap in a somewhat stream-of-consciousness style.
There are two highlights of Gartner’s Magic Quadrant (for public cloud storage), in my opinion. One is that Google Cloud and Microsoft Azure are both moving closer to AWS (which actually slipped) in the top-right corner. The other is that Alibaba Cloud is moving toward becoming the fourth cloud provider to enter that quadrant. As I’ve written before, Alibaba should be taken seriously as a cloud provider, especially because it has some inherent advantages in establishing footholds in markets — including, possibly, India — where U.S.-based providers might struggle.
However, reports that Google is looking to partner with Tencent and/or Inspur to build out its cloud business in mainland China could make things a little more interesting — at least in china. AWS and Microsoft already have partnered with Chinese companies to sell their cloud services there (Chinese law requires such partnerships, and requires data to stay in the country), but none as large as Tencent. Furthermore, Google and Tencent already have a patent-sharing partnership, and Tencent is battling with Alibaba and Baidu for digital dominance in China, so Tencent could definitely benefit from this relationship, too.
I’m not sure if server-manufacturer Inspur already builds servers for Google, but that’s also an interesting angle. Google building out its footprint in China could catapult Inspur into the data center business, while ensuring it has a buyer to fill up those facilities with its gear. The major takeaway here is that while AWS and Microsoft went a more traditional route by partnering with more traditional telcos, Google is looking at major web companies and server makers, which might give it more control over operations while also helping it establish a foothold in the greater Chinese economy.
Which is something Google appears to want, based on reports that it’s building a censored search engine to comply with Chinese laws. I think the U.S. senators who wrote the letter to Google CEO Sundar Pichai make some very good points and ask some very good questions, and they could do the same about artificial intelligence if they haven’t already. It strikes me as odd that while Google employees fight against military deals in the U.S. and the company publishes its ethical guidelines for AI, it’s also establishing deeper connections with Chinese institutions that presumably are not entirely free from government/military control.
I’ve never really believed that the United States is in danger of losing an AI technology battle against China, but geopolitics are a different thing altogether. While it’s fascinating to watch the world’s biggest companies fight to stake their claim in what might very well become the world’s most important economy (if it’s not already), you have to wonder how much they’re willing to put up with in order to do it, or if they really care as long as the money is flowing. You also have to wonder how long the U.S. government will sit on the sidelines before it decides it needs to act, and whether it should at all.
architecht.io
This is one of those stories that reaffirms the optimistic viewpoint on automation, which is that AI will take over monotonous tasks and leave companies free to turn employees (and even hire more of them) on more meaningful tasks. Of course, the not-so-silver lining is the potential skills gap between the jobs automated and the new jobs created.
nytimes.com
This is an absolutely, 100-percent commendable effort by AWS. And, my inner cynic believes, an attempt to counter those recent ACLU claims with an example of AWS and facial recognition doing good.
amazon.com
If you’ve ever had to track down quality images, you know what a pain this can be. This is a simple, but useful example of deep learning at work.
adweek.com
Pretty sure this isn’t the first company to take on this challenge, which I would argue is difficult in part because industrial robots are usually designed to do a single thing, which makes proprietary software a little easier to work with.
techcrunch.com
This is pretty impressive work, and it highlights a couple of important things: (1) that it’s good to have access to a boatload of cloud computing resources for training, and (2) that it’s not so much “intelligence” that makes the system better, but its ability to instantly access information.
theregister.co.uk
Don’t believe the hype. Some of these business-focused pieces are almost the exact same arguments made during the big data / data science boom. And if you don’t have those systems and processes in place yet, there will be no benefit from AI, either.
hbr.org
At the national level, I’d argue any trust deficit is more about convincing citizens you’re spending those huge AI investments wisely. That certainly includes planning for any economic upheaval, but also includes things like not blowing resources on vaporware or pie-in-the-sky ideas.
theglobeandmail.com
I don’t know how many jobs quantum computing can directly create, beyond what’s natural as part of any shift in computer science and IT. Although, perhaps the field itself will open up some new opportunities that we can’t yet predict.
wired.com
Research from DeepMind to build a system that can correlate images with their related sounds. This could be potentially very useful, for search if for nothing else.
deepmind.com
architecht.io
I linked to most of this in one way or another last week, but here’s a roundup of a few different cloud market predictions all in one place. Long story, short: AWS is very large.
geekwire.com
FYI, if you’ve been following this lawsuit. This is the kind of ugliness we’ve been fortunate to avoid (mostly) in the current open source era, but something tells me patent litigation will rear its ugly head as money and stakes ramp up.
zdnet.com
451 Research has found that a fair number of companies are looking to bring some workloads back into private data centers, or at least into hybrid and multi-cloud environments. That seems natural as everyone gets more experience with what works best and what doesn’t, and on-prem options continue to improve.
451research.com
Google explains its new feature to help ensure your cloud instances aren’t infected with boot- or firmware-level malware. And in this post it explains more about the TPM technology that helps Shielded VMs do their job.
google.com
A couple of VMware engineers explain how to go from a Kubernetes test to a production Kubernetes cluster. You might say, “Well, of course folks from VMware are pushing VM usage as part of this,” but from what I’ve seen that’s par for the course. This is a good primer.
kubernetes.io
Some highlights from the recent GeekWire cloud conference serverless track, including videos from engineers at AWS and Google.
geekwire.com
So, naturally, he started a company to build a better database. This is really a story of FaunaDB, a startup pushing what it calls a “serverless database.”
venturebeat.com
I agree that there’s something here when you look at everything that’s possible beyond just pushing code faster. But, yeah, it’s a lot to expect everyone to embrace new term after new term as their practices evolve.
redmonk.com
architecht.io
As I’ve pointed out many times before, Uber is becoming quite the engineering shop. Although, I can’t help but wonder if it’s spending an inordinate amount of time building tools and managing heavy-duty data and infrastructure systems. Perhaps as a result of its being founded in 2009, before cloud was really mature and when “big data” was just taking off.
uber.com
This is good work from Facebook to take advantage of its global ubiquity, and also a good explanation of how it’s gathering the data it’s presenting.
fb.com
Big data vendors continue to embrace the cloud, while cloud providers continue to make sure they’re not leaving any workloads behind. The whole world will never run on cloud-provider-managed services.
hortonworks.com
Enterprise IT interviews and analysis: AI, cloud-native, startups, and more
22 
22 claps
22 
Written by
Founder/editor/writer of ARCHITECHT. Day job is at Pivotal. You might know me from Gigaom - way back in the day, now.
Once a site about next-gen enterprise IT and the people building it; now a place where Derrick Harris occasionally blogs about tech-related things.
Written by
Founder/editor/writer of ARCHITECHT. Day job is at Pivotal. You might know me from Gigaom - way back in the day, now.
Once a site about next-gen enterprise IT and the people building it; now a place where Derrick Harris occasionally blogs about tech-related things.
"
https://medium.com/iotforall/8-things-you-didnt-know-about-wifi-unlicensed-lte-explained-and-cloud-computing-vs-1fe7e3d271bd?source=search_post---------110,"There are currently no responses for this story.
Be the first to respond.
This is an email from IoT For All Newsletter, a newsletter by IoT For All.
Check out the latest posts from IoT For All! Click the titles or pictures to read the full stories.
Ah, good ‘ole WiFi. From helping us to keep our phone bill low (except March, goddammit) to allowing our laptops/tablets/etc. to connect to the internet, WiFi has been a ubiquitous companion that we’ve all come to know and love.
WiFi is also useful for some IoT applications, such as building and home automation or in-house energy management. For many other IoT applications, WiFi is absolutely useless.
Given the importance of WiFi to our everyday lives and to certain IoT applications, here are 8 interesting things about WiFi that you didn’t know…
The CBRS Alliance is looking more and more powerful every day. The group, which advocates for LTE services in the 3.5 GHz Citizens Broadband Radio Service (CBRS) spectrum, now boasts all four major US cellular carriers (AT&T, Verizon, T-Mobile, and Sprint), cable giants Comcast and Charter, as well as Google, Intel, Nokia, and Qualcomm.
It’s easy to see why the cellular carriers would be interested in more available spectrum, but what’s driving Google and other cable companies’ interests in the band?
This post will examine what CBRS spectrum is, why it’s an attractive deployment option for cellular and non-wireless operators alike, and how this affects IoT business strategy moving forward…
By now, you’ve probably read through Mary Meeker’s 2017 Internet Trendsreport and confirmed that mobile is still eating the world. Mobile internet usage is growing steadily with average US adults spending 3 hours per day on the internet via mobile devices. As demands for data traffic continues to grow, wireless providers have responded by making adjustments to their LTE technology.
If you own one of the newer phones equipped with Qualcomm’s X16 LTE modem (e.g. Galaxy S7/S7 Edge, LG V20, Pixel), your phone may soon run on a new technology called LTE-U or LTE Unlicensed. With T-Mobile already launching LTE-U service in select areas and Verizon not far behind, it’s time to understand what the hype and the technology is all about…
Predictive analytics is at the heart of getting the best payback from IoT. Connecting devices and adding a few sensors or meters will quickly generate more information than your staff can handle.
In fact, your team can be overwhelmed when IoT data starts rolling in faster than you can digest it, even with alerts and other messaging. That’s one of the reasons why, according Vernon Turner, senior vice president and research fellow at IDC, less than 1 percent of data generated today is being analyzed. You need predictive analytics to help you sort and understand what’s coming in, so you can take intelligent actions…
For IoT security to be successful, humans must trust the security, safety, and privacy of this massive transformation of the world. Most importantly, “ordinary people,” whether they are consumers or workers, must be able to safely, reliably, and intuitively interact with vast, complex, interconnected systems of IoT devices.
This poses the question, “How can we get back to a place of relative simplicity of function, where the average user has a reasonable understanding of the integrity of their connected devices?”…
The real business value enabled by the Internet of Things is derived not really from the data, but from insights that facilitate real-time actions that increase asset efficiency, reliability and utilization.
That value takes many forms with IoT use cases that range from supply chain management and manufacturing automation to parking and waste management solutions.
But, to actually save time and money with IoT, the data insight has to come from somewhere — typically centralized, scalable cloud computing platforms tailored for the device, connectivity and data management needs of the Internet of Things…
IoT For All is brought to you by the curious engineers at Leverege. If you liked this week’s roundup, please recommend or share with someone you think would enjoy it! Thank You!
Expert analysis, simple explanations, and the latest…
12 
1
12 claps
12 
1
Expert analysis, simple explanations, and the latest advances in IoT, AR/VR/MR, AI & ML and beyond! To publish with us please email: contribute@iotforall.com
Written by
Director of Projects @Leverege. Striving to change myself and the world for the better. I value active living, life-long learning, and keeping an open mind.
Expert analysis, simple explanations, and the latest advances in IoT, AR/VR/MR, AI & ML and beyond! To publish with us please email: contribute@iotforall.com
"
https://medium.com/@alibabatech/could-go-defeat-java-in-the-cloud-computing-era-945d216e0b99?source=search_post---------111,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Tech
Apr 27, 2020·13 min read
The Java platform provides a large number of class libraries and frameworks, helping developers quickly build applications. Most of the Java framework class libraries run concurrently based on the thread pool and blocking mechanism, mainly for the following reasons:
1) The Java language provides powerful concurrency capabilities in the core class library, and multi-thread applications help achieve good performance.2) Some Java EE standards are used for thread-level blocking modes (such as Java Database Connectivity (JDBC)).3) Quick application development based on the blocking modes.
However, with the emergence of many new asynchronous frameworks and languages (such as Go) that support coroutine, the thread scheduling of operating systems (OSs) has become a performance bottleneck in many scenarios. People now wonder whether Java is still applicable to the latest cloud scenarios. Four years ago, the Java Virtual Machine (JVM) team at Alibaba independently developed Wireless Internet Service Provider 2 (WISP 2), bringing the coroutine capability of Go into the Java community. WISP 2 not only allows users to enjoy the rich resources of the Java ecosystem but also supports asynchronous programs, keeping the Java platform up to date.
WISP 2 is mainly designed for I/O-intensive server scenarios, where most companies use online services (offline applications are mostly used for computation, so they are not applicable). WISP 2 is a benchmark for Java coroutine functionality and is now an ideal product in terms of product format, performance, and stability. To date, hundreds of applications, and tens of thousands of containers have already been deployed on WISP 1 or WISP 2. The WISP coroutine is fully compatible with the code for multi-thread blocking. You only need to add JVM parameters to enable the coroutine. The coroutine models of core Alibaba Cloud e-commerce applications have been put to the test during two Double 11 Shopping Festivals. These models not only enjoy the rich resources of the Java ecosystem but also support asynchronous programs.
WISP 2 focuses on performance and compatibility with existing code. In short, the existing multi-thread I/O-intensive performance of Java applications may improve asynchronously simply by adding the JVM parameters of WISP 2.
In the following example, the stress test results are compared between the message middleware proxy, Message Queue (MQ) and Distributed Relational Database Service (DRDS) after JVM parameters are added without modifying the code.
According to the table, the context switching and sys CPU usage are significantly reduced, the response time (RT) is reduced by 11.45%, and queries per second (QPS) is increased by 18.13%.
WISP 2 is completely compatible with the existing Java code and, therefore, easy to use.
For any standard online application which uses /home/admin/$ APP_NAME/setenv.sh to configure parameters, run the following command in the admin user to enable WISP 2:
Otherwise, manually update the JDK and Java parameters using the commands below.
Run the yum command to install the latest JDK.
Use the WISP parameters to start the Java application. Then, run the jstack command to check whether the coroutine is enabled. The Carrier thread is used to schedule a Coroutine. In the following figure:
The following figure shows the top-H of the DRDS stress test on Elastic Compute Service (ECS). According to the figure, hundreds of application threads are hosted by eight Carrier threads and distributed evenly on several CPU core threads to be run. Some Java threads are GC threads.
The following snippet shows a test program.
The probability of context switching is very low when this program is running. In fact, the preceding I/O system calls are not blocked. Therefore, the kernel does not need to suspend the thread or switch the context. In fact, it is the user or kernel-mode that is switched.
When the preceding program is tested on the ECS Bare Metal Instance server, each pipe operation only takes about 334 ns.
Essentially, both user-mode and kernel-mode context switching are very lightweight operations and support some hardware commands. For example, PUSHA is used to store general-purpose registers. Threads in the same process share the page table. Therefore, context switching overhead is generally only caused by either storing registers or switching SPs. The call command automatically stacks PCs, and the switch is completed in dozens of commands.
Since kernel switching and context switching are fast, it’s crucial to understand what produces multithreading overhead.
Let’s take a look at the hotspot distribution of the blocked system call futex.
As shown in the above figure, the hotspot produces a large amount of scheduling overhead. Let’s look at the process.
1) Call system calls (which may need to be blocked).2) System calls need to be blocked. The kernel needs to determine the next thread to be run or scheduled.3) Switch the context.
Therefore, the two preceding misunderstandings have a certain causal relationship with multithreading overhead, but the actual overhead comes from thread blocking and wake-up scheduling.
In conclusion, use a thread model to improve web server performance according to the following principles:
1) The number of active threads is approximately equal to the number of CPUs.2) Each thread does not have to be blocked.
This article will introduce these two principles in the following sections.
To meet these two conditions, an ideal method is eventloop + asynchronous callback.
For simplicity, let’s take a Netty write operation on an asynchronous server as an example (write operations may also be blocked).
Here, sync() blocks the thread, which does not meet expectations. Netty itself is an asynchronous framework, so let’s introduce a callback.
Note that writeQuery returns the result after the asynchronous write operation is called. Therefore, if the code to be run after write must logically be contained in the callback, “write” must be in the last row of the function. This is the simplest case. If the function has other callers, CPS conversion is required.
Constantly extract the “lower part” (continuation part.) of the program. This requires some thinking and hence, let’s introduce the kotlin coroutine to simplify the program.
In this example, suspendCoroutine is introduced to obtain a reference to the current Continuation, run a segment of code, and ultimately suspend the current coroutine. Continuation represents a continuation of the current calculation. Use Continuation.resume() to resume the execution context. Therefore, just call cont.resume(0) back when the write operation is completed. The program status is returned to the execution status (including caller writeQuery) at suspendCoroutine. The system continues to run the program and then runs a log after the code result is returned. With writeQuery, perform asynchronous operations by using the synchronous write method. After a coroutine is switched by suspendCoroutine, the system schedules other executable coroutines to run the thread and thus the thread is not actually blocked. Hence, this improves performance.
From this point of view, there is only a need for a mechanism to save or resume the execution context and requirement to use the non-blocking thread + callback method in the blocking library function to release or resume the coroutine. This helps to run the programs written in the synchronous mode in the asynchronous mode.
Theoretically, as long as one library encapsulates all JDK blocking methods, it is easy to write asynchronous programs. The rewritten blocking library function itself needs to be widely used in many programs. Also, the kotlin support of vert.x has already encapsulated all JDK blocking methods.
Despite its wide application, vert.x cannot balance the legacy code and lock blocking logic in the code. Therefore, vert.x is not the most common choice. In fact, all Java programs must use the library JDK. WISP supports coroutine scheduling by using non-blocking methods and event recovery coroutines in all blocking calls in JDK. While providing users with the greatest convenience, this ensures compatibility with the existing code.
With the preceding method, each thread does not need to be blocked, and WISP converts the thread into a coroutine at Thread.start() to make the number of active threads approximately equal to the number of CPUs. Therefore, just use the WISP coroutine to enable all the existing Java multi-thread code to implement asynchronous performance.
For applications based on traditional programming models, given the logical integrity, the convenience of exception handling, and the compatibility with existing libraries, it is costly to transform these applications to run asynchronously. WISP has obvious advantages over the asynchronous programming mode.
Next, let’s discuss how to select technology for new application programming only considering the performance.
To write a new application, we usually use common protocols or components such as JDBC, Dubbo, and Jedis. If the library uses the blocking mode and does not expose the callback interface, it’s not possible to write asynchronous applications based on these libraries (unless the thread pool is packed, but this is putting the cart before the horse). Assume that all the libraries we depend on, such as Dubbo, support callbacks.
(1) Assume using Netty to accept requests. We call this the entry eventLoop. The received requests might be processed in the Netty handler, or in the service thread pool for real-time I/O.(2) Assume that Dubbo needs to be called during request processing. Since we did not write Dubbo and it contains its own Netty eventLoop, we process I/O requests in the Netty eventLoop inside Dubbo and wait until the backend responds to initiate a callback.(3) Dubbo eventLoop calls callback in the eventLoop or callback thread pool after receiving the response.(4) The subsequent logic is continued in the callback thread pool or the original service thread pool.(5) The response to the client is always ultimately written back by eventLoop at the entry.
As eventLoops are separated in this encapsulation mode, even if the complete callback is used, the request must be transmitted between multiple eventLoops or thread pools, but each thread is not fully run, resulting in frequent OS scheduling. This is contrary to the preceding principle that each thread does not need to be blocked. Therefore, with this method, although the number of threads is reduced and memory is saved, the performance benefits are very limited.
For a new application with limited functions (for example, an NGINX application that only supports HTTP and mail protocols), rewrite the application without relying on existing components. For example, write a database proxy server based on Netty to share the same eventLoop with the client connections and the real backend database connections. In this way, applications that precisely control the thread model generally have good performance.
Generally, their performance is superior to coroutines that are converted from non-asynchronous programs for the following reasons:
However, coroutine still has an advantage as WISP correctly switches the scheduling of ubiquitous synchronized blocks in JDK.
Based on the preceding background information, note that WISP and other coroutines are suitable for I/O-intensive Java programs. Otherwise, the threads are not switched and only need to run on the CPU without much intervention from the OS. This is a typical scenario for offline or scientific computing.
Online applications usually need to access Remote Procedure Call (RPC), databases, caches, and messages, which are blocked. Therefore, WISP allows for improving the performance of these applications.
The earliest version, WISP 1, was deeply customized for these scenarios. For example, requests received by HSF are automatically processed in a coroutine instead of a thread pool. It allows to set the number of I/O threads to 1 and then call epoll_wait(1ms) to replace selector.wakeup(). Therefore, one of the main challenges is deciding whether WISP is only suitable for Alibaba workloads.
This is proved by using the techempower benchmark set, the most authoritative in the web field. We chose common blocking tests such as com.sun.net.httpserver and Servlet (the performance is not optimal, but is closest to the performance of common users' devices and has some room for improvement) to verify the performance of WISP 2 under common open source components. The test results show that, under high pressure, QPS and RT are improved by 10% to 20%.
Project Loom is a standard coroutine implementation on OpenJDK. But, should Java developers use Project Loom? Let’s, first compare WISP and Project Loom.
(1) Project Loom serializes the context and then save it, which saves memory but reduces the switching efficiency.(2) Similar to GO, WISP uses an independent stack. For coroutine switching, it only requires switching registers. This operation is highly efficient but consumes memory.(3) Project Loom does not support ObectMonitor, but WISP supports it.
(4) WISP supports switching (such as reflection) when a native function is installed on a stack, but Project Loom does not.
In our view, it will take at least two years for Project Loom to achieve stability and improve its functions. WISP features excellent performance, provides more comprehensive functions, and is a much more mature product. As an Oracle project, Project Loom might be included in the Java standard. We are also actively contributing some feature implementations of WISP to the community.
Besides, WISP is currently fully compatible with the Fiber API of Project Loom. If our users’ program is based on the Fiber API, we ensure that the code behaves exactly the same on Project Loom and WISP.
Coroutines are suitable for I/O-intensive scenarios, which means that, generally, a task is blocked for I/O after it is performed for a short period of time and then is scheduled. In this case, as long as the system’s CPU is not used up, the first-in-first-out scheduling policy basically ensures fair scheduling. Besides, the lock-free scheduling implementation greatly reduces the scheduling overhead compared to kernel implementation.
ForkJoinPool is excellent, but it is not suitable for WISP 2 scenarios. For ease of understanding, view a coroutine wake-up operation as an Executor.execute() operation. ForkJoinPool supports task stealing, but the execute() operation is performed by a random thread or a thread in the thread queue (depending on whether asynchronous mode is used). As a result, the thread on which the coroutine is woken up is also random.
At the underlying layer of WISP, the cost of a steal operation is high. Therefore, there is a need for an affinity to bind the coroutine to a fixed thread as far as possible. In this way, work-stealing occurs only when the thread is busy. We have implemented our own workStealingPool to support this feature. It is basically comparable with ForkJoinPool in terms of scheduling overhead and latency.
To support the M and P mechanisms similar to those of Go, we need to force the thread blocked by the coroutine out of the scheduler. But these features cannot be added in ForkJoinPool.
The Reactive programming model has been widely accepted and is an important technology trend. It is difficult to completely avoid Java code blocking. We believe that coroutines can be used as an underlying worker mechanism to support Reactive programming. In this way, the Reactive programming model is retained, and avoid situations where the entire system is blocked due to user code blocking.
This idea is taken from a recent speech by Ron Pressler, the developer of Quasar and Project Loom. He clearly pointed out that the callback model will pose many challenges to current programming modes.
The four-year R&D process of WISP is divided into the following phases:
(1) WISP 1 did not support objectMonitor and parallel class loading but could run some simple applications.(2) WISP 1 supported objectMonitor and was deployed with core e-commerce services, but did not support workStealing. Therefore, only some short tasks could be converted to coroutines (otherwise the workload would be uneven). Netty threads were still threads and required complex and tricky configurations.(3) WISP 2 supports workStealing and therefore converts all threads into coroutines. The preceding Netty problems were also solved.
Currently, blocked JNI calls are not supported. WISP inserts hook in JDK to schedule calls before they are blocked. Hook cannot be inserted for customized JNI calls.
The most common scenario is to use Netty’s EpollEventLoop.
(1) This feature is enabled for bolt components of Ant Finance by default and can be disabled through -Dbolt.netty.epoll.switch=false. This has little impact on performance.(2) Use -Dio.netty.noUnsafe=true to disable this feature, but other unsafe functions may be affected.(3) For Netty 4.1.25 and later versions, use -Dio.netty.transport.noNative=true to disable only jni epoll. We recommend using this method.
(Original article by Liang Xi梁希)
First hand and in-depth information about Alibaba’s latest technology → Facebook: “Alibaba Tech”. Twitter: “AlibabaTech”.
First-hand & in-depth information about Alibaba's tech innovation in Artificial Intelligence, Big Data & Computer Engineering. Follow us on Facebook!
1 
1
1 
1 
1
First-hand & in-depth information about Alibaba's tech innovation in Artificial Intelligence, Big Data & Computer Engineering. Follow us on Facebook!
"
https://medium.com/@gregsramblings/let-s-stop-trying-to-visualize-cloud-computing-as-physical-hardware-705084248f57?source=search_post---------112,"Sign in
There are currently no responses for this story.
Be the first to respond.
Greg Wilson
May 30, 2015·6 min read
Until recently, I thought of cloud computing as the virtualization of physical hardware. I thought of virtual machines as servers, cloud storage as a collection of Internet-connected disks, and so on. To show how deep this mental model went for me, take a look at the screenshot below — it’s from a tablet app (iPad and Android) called “Greg’s Toolkit” that I conceived in 2011 and built with two friends. The app would introspect the user’s Amazon Web Services account and create a visual computer room — racks filled with servers that represented my EC2 instances, disks that represented my S3 buckets and attached devices, and on and on. Our pitch was that it made the virtual world familiar again.
The app did OK, but it never gained the traction we had hoped for, so after a few months, we shut it down and moved on.
When I conceived Greg’s Toolkit, cloud computing was mostly a hobby for me — the back end of several apps and websites I created. About a year ago, I joined the Google Cloud Platform team and my fun hobby suddenly became my new day job! During the first few months at Google, I still thought of cloud computing as infrastructure in terms of physical entities such as “servers”, “machines”, “CPUs”, and “disks”. When I started learning about Google App Engine, however, I began to see flaws in my thinking. App Engine is a true platform-as-a-service (PaaS) that enables you to deploy your code without giving any thought to what type of hardware it’s implemented on. Important needs, including auto-scaling, security, and authentication, are taken care of for you. So it doesn’t really work in my screenshot above that mapped to a physical environment. My mental model was quickly falling apart.
Soon after, I started learning about containers (including Docker and Kubernetes). Containers are similar to VMs in some ways, but unlike VMs, you’re not virtualizing an entire server. Containers are very lightweight compared to VMs because the OS is not part of the container itself. You can even run multiple containers on the same host, each of which has its application isolation from a user point of view. Obviously my mental model was now fully invalidated. This epiphany has me looking at the future of cloud computing in a much broader way.
With my new expanded, non-physically-constrained view of cloud computing, I’ve been thinking about what the future might look like. Today, major cloud vendors still use many physical terms in the titles and description of services.
Let’s take a look at storage. When you need virtual disks with high IOPS, you select “SSD-based” storage. The fact that SSDs are the actual devices used to provide the desired high disk performance is an implementation detail. The fact that the type of physical hardware being used is exposed to the user actually contradicts the real value proposition of cloud computing, which is that you don’t have to be concerned with the implementation details. I suspect that “SSD” is used because in today’s world, SSDs are universally known to be faster than standard disks, so it’s a bit of a marketing play. Looking toward the future, it’s easy to imagine a world where you choose the performance requirements, durability requirements, and access models you need for your storage without the use of any physical (legacy?) terms. Behind the scenes, the storage solution will be implemented by various means including SSDs, high speed memory, or some other technology based on our needs, but we will no longer be concerned about what actual hardware implemented the solution. We’ve already seen some progress here from the major cloud vendors. For example, Google Cloud Bigtable provides a familiar interface (HBase) with very high reliability and extreme Google-level scalability, and Google Cloud Bigtable does it without exposing the underlying details on how it’s implemented. It’s crazy fast, it’s super reliable, and it uses an established interface, and that’s all we should care about.
Now let’s talk about the compute side of things. When looking at the various offerings of the major cloud vendors, you’ll quickly run into references to physical hardware and other implementation details. There are references to the type of CPUs (e.g. Ivy Bridge, Haswell, etc.), number of cores, and CPU clock speeds. Are these the details we need in the future of cloud computing, or do we need a new vocabulary that is more closely related to how we will use the computing power? Most of us struggle as it is to relate actual performance to the concepts of clock speed, multi-core processing, and hyper-threading. For example, which MacBook Pro is faster? — the model with “2.9GHz dual-core Intel Core i5 Turbo Boost up to 3.3GHz“, or the model with “2.2GHz quad-core Intel Core i7 Turbo Boost up to 3.4GHz“? Is the quad core model faster than the dual core for all types of computing tasks? Is the i5 GHz value directly comparable to the i7 GHz value? You have to do a lot of studying to understand the differences, and it gets very technical very fast. For most, these specs mean very little. We simply care how fast the stuff we do will get done. Most of us just take the wine approach and assume that the more expensive one must be better.
During my college years, I briefly pursued a degree in Computer Engineering until I realized that I’m more of a software guy than a hardware guy (whew!). The curriculum had several required electrical engineering courses where I learned that electricity is extremely complicated… more complicated that many of us care to understand! However, most applications of electricity have been abstracted to relatively simple plumbing concepts where we think of the “flowing” of current measured in amps — a relatively simple metaphor that most of us can understand. This simplified abstraction provides an adequate level of understanding for most common applications without requiring an understanding of how electricity actually works at the atomic level.
Cloud computing needs similar abstractions that let us focus on how computing power is applied rather than how it’s implemented. This got me thinking about how we would need to define our computing needs when configuring our future cloud services. When you boil it down to the ones and zeros, we have a series of instructions that need executing with various inputs and outputs. How will we distinguish the need for extremely fast execution of a single stream of instructions and instructions that can be parallelized across multiple CPUs? Will we need to specify that a specialized CPU is needed for certain types of instructions (e.g. GPU), or will that be a detail that is determined by the service automatically? I’m now over my head, but hopefully you get my point. We need to start thinking of cloud computing in different ways using a different vocabulary.
All of this has me pondering a few things:
If you look at how things have changed in the past few years in cloud computing and extrapolate how things could look in the near future, it’s super exciting.
Originally published at gregsramblings.com on May 30, 2015.
Google Developer Relations — Living in San Francisco https://gregsramblings.com | https://cloud.google.com | http://instagram.com/gregsimages
6 
6 
6 
Google Developer Relations — Living in San Francisco https://gregsramblings.com | https://cloud.google.com | http://instagram.com/gregsimages
"
https://medium.com/@neal-davis/10-tips-on-how-to-enter-the-cloud-computing-industry-7aaf7b427975?source=search_post---------113,"Sign in
There are currently no responses for this story.
Be the first to respond.
Neal Davis
Sep 2, 2021·8 min read
So you’ve decided to take the plunge into the cloud computing industry. Great for you! It is an exciting field with opportunities everywhere, but it can also be difficult to get your foot in the door without experience or connections. If that sounds like where you are at this moment, this article is for you!
It is no secret that the cloud industry is booming. With an estimated $312 billion in revenue, this sector continues to grow and offer new opportunities for cloud enthusiasts. Whether you are just at the start of your professional career or want to explore some different options, consider jumping into the world of cloud computing! In this blog post, we will outline 10 ways to jump-start your cloud career.
Choosing the right cloud computing certification will help you make the most of your time and money. There are plenty of options, so best to identify the provider that matches your skillset or interests.
The major cloud certification providers include:
AWS is, without a doubt, one of the best cloud providers. They offer a wide variety of certifications and specialization courses, which are perfect for those looking to work in the AWS ecosystem. There are 4 levels of certifications, which have different scopes and requirements.
The Microsoft cloud ecosystem is a great place to start your career in the world of computing. They offer two different types of certification programs: associate and professional. Associate-level certifications train you how to get started with the platform while professional-level courses provide knowledge for more experienced IT pros who want to do network management or advanced system administration tasks like Windows Server 2012 clustering over virtual machines (SCVMM).
Google Cloud Platform has a lot to offer. They have two different certification programs: Professional-level and Associate. They are both excellent options for those looking to start their careers in cloud computing or advance existing ones. GCP also offers specialization courses that train things like building IoT solutions with Google Cloud services as well as create custom machine learning models using TensorFlow.
Factually, there is no cloud platform that is better than the other. The one you choose should depend on your goals and what you hope to achieve in the long run. Also, consider getting multi-cloud skills because the cloud industry is moving fast, and having skills in multiple platforms will make you flexible and allow you to grab different opportunities.
While certifications do not automatically result in a job, they are a great way to learn technologies and demonstrate expertise to employers. There are a lot of free resources online to help you learn more about cloud computing. You can sign up for individual classes or explore multi-part certifications that will show off your skills.
When choosing a certification course, look for training that includes hands-on exercises. This way you can put what you learn straight away into action. Also, make sure to practice with cloud labs in a risk-free environment.
Keep in mind that future employers want to know that you are dedicated to your field. These certifications will demonstrate your enthusiasm for Cloud Computing and demonstrate your willingness to keep up with emerging technologies.
Nothing beats practical experience. When working on projects related to the type of job you want to pursue, you get to develop relevant skills and gain hands-on experience. The IT field is a constantly evolving industry that is always looking for eager professionals to fill the need for skilled workers. One of the best ways to get a foot in the door is by offering services as a freelancer on sites like Fiverr and Upwork.
Volunteering can also be a really great way to gain experience in the field and meet new people. There are many companies out there that need an extra hand from talented individuals. So don’t shy away from accepting an unpaid internship (as long as it’s temporary).
The best thing about taking on an actual project is that you’ll soon find out whether this new career path is for you. Meanwhile, the skills you’re learning will help build your portfolio (and resume) and make future interviews more successful.
Building open-source projects using the tools and services offered by one of the major cloud providers is another way to gain practical experience. You don’t have to be a software developer to contribute to cloud computing projects on GitHub. It’s a fantastic opportunity to demonstrate value, build trust, and gain skills in a way that contributes to the greater good and builds your public reputation.
Portfolios are crucial for any type of developer or a computer-related field. Having a portfolio will show your potential employers what you can do and help them decide if you would be the perfect fit for their company.
In your portfolio, always include links to projects that demonstrate skills from all aspects of the cloud: data, application development, cloud architecture, and more. You may also include articles related to topics such as security standards, trends on emerging technologies, and more.
There are a few things that should always appear on your portfolio:
If you want to kick start your cloud career, having good references can go a long way in making you stick out. Reach out to people who have seen your work and ask them if they would be willing to serve as your reference. This will help boost the chance of getting an interview at any company because it shows that professionals in the know think highly of you. The best referees are those who are experts in the field and are familiar with your work.
In the information age, there is no excuse not to stay up-to-date with all that’s happening in your field of expertise. Having knowledge about what’s going on can vastly improve your chances of being successful because you are always aware of new innovations or changes to regulations in the industry. It can be tiring following so many different sites. Luckily there are plenty of aggregators out there who do this job for you! Some popular ones include ZDNet, Computerworld, The Verge, InfoWorld.
Keeping up-to-date on developments within the industry will help improve your skillset so you can remain competitive in any market.
The best way to build a reputation for yourself is by sharing what you know about cloud computing, whether it’s through blog posts or videos. That builds up an audience who will likely follow you as well as share your content with others. You can also create profiles on sites like LinkedIn, Twitter, GitHub — anywhere that will allow people in the industry to see what you’re all about.
If you see something on a social media site related to cloud computing, don’t hesitate to reach out and ask that person what they do for work because it could lead to an opportunity in the future. You never know who might have connections at companies looking for someone with your skillset.
Learning from the experience of others who have been in your shoes helps you achieve your goals in a smarter way.
Networking is one of the most important things you can do to keep your skills sharp and find a job. Be sure to connect with people who are in positions that require cloud computing. This will give you connections and open doors that may otherwise not be available to you. Make sure you network with people from other fields as well to broaden your horizons.
LinkedIn is one of the best ways to network with professionals from all different fields. Join groups related to cloud computing or just general IT topics where employers are more likely to look through profiles on this platform when hiring someone for their team. This will also help you establish yourself as an expert in cloud computing, which will make it easier when applying for jobs.
For those who are brand new to the Cloud Computing industry, it’s important to realize that getting your foot into the door is an effort and will take some time. If you are a fresher with no practical experience of working in the cloud environment, you need to set realistic expectations and goals. This may translate into getting a good understanding of the different roles and expectations of the industry. So it may be best to first apply for an entry-level or internship position that matches your level of experience.
Most important — stay positive and don’t give up! Keep in mind that the opportunities are endless. So go out there and make use of them!
Cloud Computing is a fascinating and ever-changing career field that provides a great future. There are many ways to become a successful IT professional in the cloud computing industry. The above list is a good place to start. Make sure you follow each one of these tips and you will become successful in this ever-booming industry in no time.
Good luck!
Check out our catalog of courses now to get started with your AWS certification journey. We provide the highest quality AWS training courses in the market at affordable prices. Our training options include:
This post originally appeared on https://digitalcloud.training/10-tips-on-how-to-enter-the-cloud-computing-industry/
Founder of Digital Cloud Training, IT instructor and Cloud Solutions Architect with 20+ year of IT industry experience. Passionate about empowering his students
6 
1
6 
6 
1
Founder of Digital Cloud Training, IT instructor and Cloud Solutions Architect with 20+ year of IT industry experience. Passionate about empowering his students
"
https://towardsdatascience.com/deployment-models-cloud-computing-b17fe1e58d44?source=search_post---------114,"Sign in
There are currently no responses for this story.
Be the first to respond.
Giorgos Myrianthous
Nov 5, 2021·3 min read
The term Cloud Computing refers to the on-demand access to a set of resources such as storage, applications, compute power and other IT services. Cloud users can have instant access to the provisioned services that are scaled based on the…
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/codelit/learn-what-cloud-computing-is-in-10-minutes-4651c5aa5ddf?source=search_post---------115,"There are currently no responses for this story.
Be the first to respond.
The following video will lay proper foundations about cloud technologies and how it can be useful to solo developers and enterprises.
I hope the video was able to address your basic queries about Cloud Computing, but if you need more clarifications about certain terms or concepts used in the video, please leave a comment on this post and I’ll try to address your questions, to the best of my knowledge.
Notes on Software Engineering, DevOps & Cloud
56 
56 claps
56 
Notes on Software Engineering, DevOps & Cloud
Written by
Software Engineer; Builds Internet Based Businesses & Apps; Solopreneur
Notes on Software Engineering, DevOps & Cloud
"
https://medium.com/@marutitech/maruti-techlabs-lalit-bhatt-on-cloud-computing-opportunities-and-challenges-d7c6e931ff90?source=search_post---------116,"Sign in
There are currently no responses for this story.
Be the first to respond.
Maruti Techlabs
Sep 9, 2016·6 min read
We recently caught up with Lalit Bhatt, Project Leader at Maruti Techlabs. We were keen to learn more about his profile at Maruti Techlabs, how cloud computing has been an integral part of his work and benefits of cloud systems for the clients.
Hi Lalit, firstly thank you for the interview. Let’s start with your background and how you became interested in cloud computing.
Rohit Akiwatkar: What is your short bio? And as a project leader what are your day to day activities?
Lalit Bhatt: Before joining Maruti Techlabs in 2010, I was working at a leading Chip Design and Software Development Company as a software developer. It is difficult to explain my role at Maruti Techlabs because I literally hopscotch between different roles and technologies. If I am working on client’s cloud migration this week, next week I might be mentoring the junior developers or improving the infrastructure aspects. Currently, I am working on improving the backend system for our US based clients.
That’s a very interesting job profile. Thank you for sharing. Now let’s talk about your work in Cloud computing.
RA: How did you got involved in cloud computing? Or How did you start paying attention to cloud computing and what was your initial thoughts? How have your views evolved?
Lalit Bhatt: We have always advised our clients to adopt cloud computing and migrate their data to cloud for better management. Thus, since joining Maruti Techlabs, I was involved with cloud technologies. Cloud computing is a revolution in IT industry. For an in-house server, I would require setting up a local environment including backups, CPU usage monitoring and trigger notifications. Whereas in the cloud, you are free from such hassles. We have been providing cloud strategy and migration of all the major cloud providers — AWS, MS Azure, Rackspace, Google Cloud platform. But I prefer Amazon Web Services (AWS) think it’s leading this race.
RA: Why do you prefer AWS? Any particular instance that you can share.
Lalit Bhatt: One of our long-term clients were using Rackspace’s dedicated servers but owing to the dynamic business growth and scalability, wanted a solution which would provide cost-effective scalability in terms of IT infrastructure. We explored Amazon Web Services, performed the detailed comparison in terms of pricing, scalability & performance and completed migration in less than a month. Implementing AWS led to a reduction in hosting cost by 50%. The customer got an additional benefit of separate web server and database instances compared to existing setup. It provides single click scale up and down instance configurations without human support. Also, AWS has availability of unique subscription options (reserved instances, spot instances, etc.) to choose from at any moment. We use Amazon EC2, RDS, Route53, Amazon S3, CloudFront, CloudWatch, ElastiCache for software development.
RA: That sounds fantastic! As you have explained about the benefits of cloud, I want to talk about possible concern about cloud systems.
Lalit Bhatt: In terms of Infrastructure as a Service (IaaS), most cloud providers give hardware virtual machine (HVM). When talking about HVM, many will immediately get concerned with resource sharing, multi-tenancy, compliance, and security. As the servers are shared there is but a negligible security threat because the cloud providers regularly update their software and have a dedicated team for troubleshooting.
RA: What industries could benefit most from using Cloud Computing?
Lalit Bhatt: Generally, all the sectors can benefit from cloud technology. But it’s easy to realize gains in startups and small firms. Purchasing a server class machine is costly. Further, it requires regular maintenance and backup. Thus for a startup, it’s not possible to allocate a dedicated team/time for server maintenance. Also if the startup becomes an overnight success and requires more space it can easily increase in the cloud. Large organizations use redundant instances and autoscaling to keep their businesses safe in case of natural calamities. In an effort to do everything from offer better in-store customer service to fully leverage advances in manufacturing, companies from even most traditional and change-resistant sectors are seeing the writing on the wall: Cloud technology strategies cut cost and risk.
RA: Apart from the basic understanding of IaaS, PaaS and SaaS models, can you share latest technology trends in cloud computing?
Lalit Bhatt: I think cloud analytics, rich API integrations and hybrid clouds are the latest trends in cloud computing. Cloud analytics solutions that allow for digging into both usage and billing data will give IT leaders the power to quickly spot potentially costly services and prevent budget overruns. SaaS, PaaS and IaaS vendors will be pressured to provide rich sets of APIs, enabling security vendors and application vendors to provide value added services. Hybrid clouds i.e. a good mix of on-premise and cloud infrastructure makes cloud adoption easier. In-short whatever that makes utilization of cloud-based system better — monitor, manage and govern would be next trends.
RA: As you mentioned Hybrid Clouds as one of the latest trends. Is security the major concern for adopting this model?
Lalit Bhatt: Partly yes as legacy solutions, compliance and a host of issues can keep a portion of an IT roadmap anchored on premise. But achieving development speed is the main reason behind hybrid clouds. For example, a software firm having three servers — development, staging and production will adopt a hybrid model. Development server will be hosted on premise and other two on the cloud. Software development requires frequent communication with the server for committing the code multiple times. But using cloud servers for running queries frequently will require higher internet bandwidth so it’s advisable to uses in premise servers for development.
RA: For organizations that aren’t yet using the cloud, what’s the first step they should take?
Lalit Bhatt: Cloud is an enabler for technology transition for the next 5 years. How to move entire company’s data and applications into the cloud; keeping the privacy and security intact is the question. Obviously moving the entire system overnight without messing up is not possible. The first step would be to identify present cloud requirements and go for the cloud with help of cloud consultants and gradually migrating the whole system in parts.
RA: Moving to the last question — what do you think is the next big disruptive technology in conjunction with cloud computing?
Lalit Bhatt: I think incorporating Machine Learning (ML) and Artificial Intelligence (AI) is next big challenge for cloud providers. Once your models are ready, Machine Learning makes it easy to obtain predictions for your application using simple APIs, without having to implement custom prediction generation code, or manage any infrastructure. Major players have started taking steps to provide APIs; Google cloud platform, Microsoft API for Azure and AWS Machine Learning technologies are already available for the cloud users. AWS ML is at a nascent stage compared to Google ML and Microsoft Azure ML.
Lalit, Thank you so much for your time! Really enjoyed learning more about your background, how the world of cloud computing is evolving and what you are working on at Maruti Techlabs. You can read more about Maruti Techlabs here.
If you resonated with this article, please subscribe to our newsletter. You will get a free copy of our Case Study on SMS-Bot Powered by Artificial Intelligence.
We are a digital product development company and your guide on the digital transformation journey.
11 
11 
11 
We are a digital product development company and your guide on the digital transformation journey.
"
https://medium.com/partech-ventures-team-publications/from-cloud-computing-to-fog-computing-a65236a6b53b?source=search_post---------117,"There are currently no responses for this story.
Be the first to respond.
By Reza Malekzadeh, General Partner at Partech
The need to solve storage problems
Today, more and more devices connect to the internet: smarter sensors are used in many industries to improve output and increase safety, cities all over the world are installing surveillance cameras on every corner and in every car park… New distributed applications and the Internet of Things (IoT) are generating an unprecedented volume and variety of data.
Moving all these exabytes of data from the Internet of Things to the cloud for analysis purposes won’t be feasible for much longer as this calls for vast amounts of bandwidth which is far too costly. Also, latency is increased as by the time the data makes its way from the network edge to the data center for analysis, the opportunity to act on it might have passed. This is particularly true for time-sensitive decisions such as security issues or when dealing with manufacturing line shutdown for example.
So there is a real need for new infrastructure capable of handling the enormous volumes of data produced each day. Current cloud computing models were not designed for the volume and variety of data generated by the IoT. The first wave of storage was hardware-based and very costly. With companies such as EMC selling storage arrays for over a million dollars. Then we experienced a second wave with the development of software defined storage technologies leveraging more and more commodity hardware and being better adapted to cloud architectures and centralized applications. We are now entering a third phase of intelligent storage capable of handling larger volumes and varieties of data. Instead of sending all the data to a data center site far from the end-point, we need to be capable of storing and analyzing most data close to the point-of-collection device. This new computing model, Fog computing, allows compute at the edge of the cloud — one could think of it as a collection of multiple proximity clouds. Because it doesn’t need to send all the data to a centralized data center, fog computing reduces data management time and improves quality of service. Additionally, most data is now compressed and encrypted at the source, making it impossible to optimize data transfer and footprint efficiency. By analyzing and assigning the correct value of data at the edge, it is now possible to improve overall infrastructure efficiency and send to the cloud only valuable pieces of information, enabling to take better decisions faster.
We believe that Fog computing calls for the next generation of intelligent object storage, capable of providing all the benefits of traditional storage while running applications at the edge, directly on the storage infrastructure.
OpenIO will take object storage to a new level
Fog computing is still a relatively new market but it is expected to grow exponentially in line with connected objects. We are very excited about our latest investment in OpenIO as we believe that they are one of the best-positioned players present on the pioneer fog computing market. OpenIO is a Lille-based startup specialized in object storage and server-less computing. We were very impressed by their technology, their team and their achievements. OpenIO offers the most flexible and cost-effective object storage solution to solve the scale-out challenges faced by businesses with thousands of Petabytes. Their open-source solution delivers a new way to build application back-end services, enabling companies to combine storage with integrated data processing on a single platform.
The quality of the founding team played a key role in our investment decision. OpenIO was founded by a small team of experts, engineers, and former intrapreneurs at Worldline, a €1.5 billion Atos company — they are engineers with real field experience and a true understanding of their clients’ challenges and expectations. Their technology does not come out of a research lab but instead came from working with customers and delivering to them a product that met their needs.
We are also very impressed by their recent contract with Dailymotion, the European leader in user-generated and premium video content. Dailymotion chose a small startup which had previously never raised any funds to replace the incumbent EMC. This is proof of the OpenIO excellent execution capability.
We are very happy to welcome OpenIO to the Partech family. We believe that OpenIO will play a key role in solving the data storage challenges of the future. Their total addressable market is projected to reach $4.95 billion. We will be at their side to help them take object storage to a new level.
Ideas and articles from the Partech team
55 
55 claps
55 
Written by
A global investment platform for tech and digital companies with offices in #SanFrancisco, #Paris, #Berlin & #Dakar
Ideas and articles from the Partech team
Written by
A global investment platform for tech and digital companies with offices in #SanFrancisco, #Paris, #Berlin & #Dakar
Ideas and articles from the Partech team
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/artificialciti/cloud-computing-powers-smart-cities-5f1ea08c8549?source=search_post---------118,"There are currently no responses for this story.
Be the first to respond.
If you think of computer systems, computing devices as landmasses, then cloud computing infrastructure is the waterway that connects these land masses and powers it in our smart cities. Cloud computing with its immense power, elasticity, and stability has the power to transform our…
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/dataseries/5-straight-answers-about-cloud-computing-c6f906686c44?source=search_post---------119,"There are currently no responses for this story.
Be the first to respond.
Of the last twenty years, no single technology has had a greater effect on modern business than cloud computing. Whether we’re talking about small to medium-sized businesses or solo entrepreneurs, the cloud has transformed the way many not only do business but how they approach productivity.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.datadriveninvestor.com/a-look-ahead-at-2020-tech-trends-for-cloud-computing-f12b66210568?source=search_post---------120,"There are currently no responses for this story.
Be the first to respond.
As the end of the year approaches, and we look ahead at what the 2020 tech trends promise to have in store for the cloud, we can’t help but also reflect on what the past years’ trends have foretold and given us thus far. As we enter 2020 we are not only entering a new year, but also a new decade, so it’s doubly interesting (and fun) to sit back and ponder on what the year and decade ahead might hold.
Before summoning the oracle and thinking ahead specifically on the future of cloud management, it’s worthwhile looking back at the big picture over the last decade to give us some sense of what we have to look forward to. Let’s start with the proverbial ‘gorilla’. AWS was founded in 2006 and had reached annual revenues by 2010 of ~$500MM. Not bad growth from a standing start and a growth trend which continued throughout the decade. With Wall St. estimates of some $50B in revenue for 2020, this means 100x growth. That is quite simply incredible and with growth last year at 35% year-on-year, this AWS growth doesn’t look like it will stop..
2010 Cloud Prediction
Amazon Cloud Revenue Could Exceed $500 Million In 2010, CRN (2010)
Growth among Microsoft Azure and Google Cloud Platform has also not been too shabby but AWS has held (and in many ways strengthened) its dominant position over the last decade.
www.datadriveninvestor.com
Due to the wonderful archival powers of the internet, finding white papers on the future of cloud from a decade ago is no more than a few clicks away. Rather than try and summarize them here, it’s worth reviewing yourself — one that’s worth taking a look at is Microsoft’s The Economics of the Cloud (2010). Some parts were right, some wrong, but the key point was that: “cloud services will enable IT groups to focus more on innovation while leaving non-differentiating activities to reliable and cost-effective providers”.
On this particular point, it’s hard to argue that as a result new companies and new business models have been realized in ways previously not imaginable. Be it in the world of the sharing economy, Uber, Lyft, Airbnb, etc or the myriad of other cloud powered unicorns which have been built over the last decade, cloud infrastructure has been a huge enabler of growth.
As interesting as it would be to speculate on where the cloud industry might be headed to by 2030, (AI-Cloud, IOT, Blockchain, Space Cloud Computing, etc) and so that we keep our feet firmly on the ground, the trends we at ParkMyCloud feel qualified to comment on are somewhat more modest and closer to home.
We have always enjoyed the quote ‘never make predictions, especially about the future.’ Nevertheless, entering a new year and a new decade it’s hard not to. We think the predictions above are fairly safe bets but equally, we are sure the speed and scale of change will likely be faster than we predicted.
Afternote
In doing this research our favorite headline from 2010 was “Airbnb Founder Eats His Own Dogfood, Goes ‘Homeless’ For Months”. Enjoy. Season’s greetings and happy 2020.
Originally published at www.parkmycloud.com on December 10, 2019.
empowerment through data, knowledge, and expertise.
99 
99 claps
99 
Written by
CEO of ParkMyCloud
empowerment through data, knowledge, and expertise. subscribe to DDIntel at https://ddintel.datadriveninvestor.com
Written by
CEO of ParkMyCloud
empowerment through data, knowledge, and expertise. subscribe to DDIntel at https://ddintel.datadriveninvestor.com
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/swlh/top-5-reasons-for-financial-services-to-opt-for-cloud-computing-941a707e77bc?source=search_post---------121,"There are currently no responses for this story.
Be the first to respond.
Cloud computing is the de-facto standard for app deployment nowadays. However, what fits small startups quite often seems not so useful to financial services bodies. Below are 5 reasons to go to the cloud.
What is the main difference between the cloud computing and the on-prem server infrastructure? Cost of operations. To install an app to an on-prem dedicated server the business must pay the following expenses:
It’s obvious the upfront investments (CAPEX) in this case are huge, and ongoing infrastructure maintenance (OPEX) is also going to cost quite a sum. The benefits of this approach are also quite tangible:
Keep in mind that the hardware updates and software upgrades, as well as disaster recovery and possible service downtime after some faulty update — all of these nightmares of the pre-cloud era are still there. And for financial services time is money, more than for anybody else.
Cloud transition is essentially a decision to lift that CAPEX and responsibility for disaster recovery, software and hardware updates, DDoS protection — and to let the cloud computing service provider handle that! Thus said, one of the main gains after the cloud transition is the safety of mind. But wait, what about the security? Major cloud data leaks are frequently making it to the headlines in the media!
Well, surprisingly enough, according to a Bloomberg report, 25 of 38 world’s leading financial organizations are already in the cloud or in the process of transitioning to it. Perhaps, they don’t look at the naked celebrities photos leaked from iCloud, and instead, trust AWS and Microsoft Azure will take good care of the security issues? There is a solid ground for such assumptions, as the US Department of Defense and CIA has signed a series of contracts on sums of up to $600 million with AWS to provide the cloud computing services for their classified data. In addition, the Microsoft Azure’s home page clearly shows 90% of Fortune 500 companies host their data with Azure.
Below are the top 5 reasons for financial services to opt for cloud computing:
The customers today are increasingly tech-savvy, and failing to meet their needs will result in the loss of crucial customer audiences.
There are certain downsides to moving the IT infrastructure of financial entities to the cloud, of course:
These are the reasons many companies decide to go back from the public cloud to on-prem cloud solutions like OpenStack or Oracle Private Cloud solutions. However, this decision should be made once the monthly OPEX of cloud operations become higher than building and operating a private cloud data center. The best part of it all is your business will enjoy
As you can see, there are tangible reasons for financial services to opt for cloud computing solutions. This might seem a daunting perspective, yet it is the only viable way to remain competitive in the fast-evolving landscape of the modern business. Moving the corporate workloads to the cloud helps cut the operational expenses, ensure service continuity and scalability, provide robust security features, increased efficiency, and operational flexibility.
The only problem is finding a reliable contractor to make such digital transformation a reality. IT Svit would be glad to assist with this endeavor. We are constantly listed among top-10 Managed Services Providers worldwide and top-3 IT Outsourcing companies in Ukraine, according to international technology agency Clutch. Our expertise gained over 13 years of providing cloud infrastructure support and more than 600 completed projects are at your disposal — contact IT Svit to leverage all the benefits of the transition to the cloud for your business!
Originally published at itsvit.com on August 21, 2018.
Get smarter at building your thing. Join The Startup’s +750K followers.
56 
56 claps
56 
Written by
DevOps & Big Data lover
Get smarter at building your thing. Follow to join The Startup’s +8 million monthly readers & +750K followers.
Written by
DevOps & Big Data lover
Get smarter at building your thing. Follow to join The Startup’s +8 million monthly readers & +750K followers.
"
https://medium.com/@jaychapel/multi-cloud-hybrid-cloud-and-cloud-spend-statistics-on-cloud-computing-ba4c194d2e10?source=search_post---------122,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Mar 20, 2019·4 min read
The latest statistics on cloud computing all point to multi-cloud and hybrid cloud as the reality for most companies. This is confirmed by what we see in our customers’ environments, as well as by what industry experts and analysts report. At last week’s CloudHealth Connect18 in Boston we heard from Dave Bartoletti, VP and Principal Analyst at Forrester Research, who broke down multi-cloud and hybrid cloud by the numbers:
More often than not, public cloud users and enterprises have adopted a multi-cloud or hybrid cloud strategy to meet their cloud computing needs. Taking advantage of features and capabilities from different cloud providers can be a great way to get the most out of the benefits that cloud services can offer, but if not used optimally, these strategies can also result in wasted time, money, and computing capacity.
The data is telling — but we won’t stop there. For more insight on the rise of multi-cloud and hybrid cloud strategies, and to demonstrate the impact on cloud spend (and waste) — we have compiled a few more statistics on cloud computing.
The statistics on cloud computing show that companies not only use multiple clouds today, but they have plans to expand multi- and hybrid cloud use in the future:
As enterprises’ cloud footprints expand, so too does their spending:
“Cloud is an inexpensive and easily accessible technology. People consume more, thereby spending more, and forget to control or limit their consumption. With ease of access, inevitably some resources get orphaned with no ownership; these continue to incur costs. Some resources are overprovisioned to provide extra capacity as a ‘just in case’ solution. Unexpected line items, such as bandwidth, are consumed. The IT department has limited visibility or control of these items.”
We’ve noticed some interesting patterns in the cloud platforms adopted by ParkMyCloud users as well, which highlight the multi-cloud trends discussed above as well as correlations between the types of companies that are attracted to each of the major public clouds. We observed:
Upon examining these statistics on cloud computing, it’s clear that multi-cloud and hybrid cloud approaches are not just the future, they’re the current state of affairs. While this offers plenty of advantages to organizations looking to benefit from different cloud capabilities, using more than one CSP complicates governance, cost optimization, and cloud management further as native CSP tools are not multi-cloud. As cloud costs remain a primary concern, it’s crucial for organizations to stay ahead with insight into cloud usage trends to manage spend (and prevent waste). To keep costs in check for a multi-cloud or hybrid cloud environment, optimization tools that can track usage and spend across different cloud providers are a CIO’s best friend.
Originally published at www.parkmycloud.com on September 18, 2018
CEO of ParkMyCloud
See all (317)
12 
12 claps
12 
CEO of ParkMyCloud
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@jaychapel/wasted-cloud-spend-to-exceed-17-6-billion-in-2020-fueled-by-cloud-computing-growth-7c8f81d5c616?source=search_post---------123,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Mar 20, 2020·4 min read
More than 90% of organizations will use public cloud services this year, fueled by record cloud computing growth. In fact, public cloud customers will spend more than $50 billion on Infrastructure as a Service (IaaS) from providers like AWS, Azure, and Google. While this growth is due in large part to wider adoption of public cloud services, much of it is also due to growth of infrastructure within existing customers’ accounts. Unfortunately, the growth in spending often exceeds the growth in business. That’s because a huge portion of what companies are spending on cloud is wasted.
Before we get to the waste, let’s look a little closer at that growth in the cloud market. Gartner recently predicted that cloud services spending will grow 17% in 2020, to reach $266.4 billion.
While Software as a Service (SaaS) makes up the largest market segment at $116 billion, the fastest growing portion of cloud spend will continue to be Infrastructure as a Service (IaaS), growing 24% year-over-year to reach $50 billion in 2020.
Typically, we find that about ⅔ of enterprise’s average public cloud bill is spent on compute, which means about $33.3 billion this year will be spent on compute resources.
Unfortunately, this portion of a cloud bill is particularly vulnerable to wasted spend.
As cloud computing growth continues and cloud users mature, you might hope that this $50 billion is being put to optimal use. While we do find that cloud customers are more aware of the potential for wasted spending than they were just a few years ago, this does not seem to be correlated with cost optimized infrastructure from the beginning — it’s simply not a default human behavior. We frequently run potential savings reports for companies interested in using ParkMyCloud, to find out whether or not they will benefit from using the product. Invariably, we find wasted spend in these customers’ accounts. For example, one healthcare IT provider was found to be wasting up to $5.24 million annually on their cloud spend, an average of more than $1,000 per resource per year.
Here’s where the total waste is coming from:
Idle resources are VMs and instances being paid for by the hour, minute, or second, that are not actually being used 24×7. Typically, these are non-production resources being used for development, staging, testing, and QA. Based on data collected from our users, about 44% of their compute spend is on non-production resources. Most non-production resources are only used during a 40-hour work week, and do not need to run 24/7. That means that for the other 128 hours of the week (76%), the resources sit idle, but are still paid for.
So, we find the following wasted spend from idle resources:
$33.3 billion in compute spend * 0.44 non-production * 0.76 of week idle = $11 billion wasted on idle cloud resources in 2020.
Another source of wasted cloud spend is overprovisioned infrastructure — that is, paying for resources are larger in capacity than needed. That means you’re paying for resource capacity you’re rarely, or never, using.
About 40% of instances are sized at least one size larger than needed for their workloads. Just by reducing an instance by one size, the cost is reduced by 50%. Downsizing by two sizes saves 75%.
The data we see in ParkMyCloud’s users’ infrastructure confirms this, and in the problem may be even larger. Infrastructure managed in our platform has an average CPU utilization of 4.9%. Of course, this could be skewed by the fact that resources managed in ParkMyCloud are more commonly for non-production resources. However, it still paints a picture of gross underutilization, ripe for rightsizing and optimization.
If we take a conservative estimate of 40% of resources oversized by just one size, we find the following:
$33 billion in compute spend * 0.4 oversized * 0.5 overspend per oversized resource = $6.6 billion wasted on oversized resources in 2020.
Between idle and overprovisioned resources alone, that’s $17.6 billion in cloud spend that will be completely wasted this year. And the potential is even higher. Other sources of waste include orphaned volumes, inefficient containerization, underutilized databases, instances running on legacy resource types, unused reserved instances, and more. Some of these result in significant one-off savings (such as deleting unattached volumes and old snapshots) whereas others can deliver regular monthly savings.
That’s a minimum of about $5 million wasted per day, every day this year, that could be reallocated toward other areas of the business.
It’s time to end wasted cloud spend. Join ParkMyCloud in taking a stand against it today.
Originally published at www.parkmycloud.com on March 10, 2020.
CEO of ParkMyCloud
See all (317)
26 
26 claps
26 
CEO of ParkMyCloud
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/intertrust/how-mobile-devices-and-cloud-computing-changed-security-9343b4f962e7?source=search_post---------124,"There are currently no responses for this story.
Be the first to respond.
Prior to the advent of mobile computing, security was limited to corporate IT assets that were often physically secured in facilities owned and managed by the company. According to a recent SANS Institute study, organizations spend as much as 12 percent of their IT budget on security.
In a Ponemon Institute study, it was found that organizations have a 27.7 percent probability of having a material data breach in the next 24 months at an average cost of $3.62M.
Meanwhile, the world of computing has changed. Security is not just about physically secure data centers and corporate controlled computing assets. Instead, end users have gone mobile, connecting to cloud enabled services, often with their own personal devices. And with the rise of the Internet of Things, there will be billions of connected computing devices on the planet in the next several years.
The primary consequences of applications getting hacked include financial loss, destroyed brand reputation, exposure to liability, and regulatory risk. Over 7 billion identities have been stolen in data breaches over the last eight years equal to one data breach for every person on the planet. Meanwhile, mobile’s rapid expansion has introduced a complicated and potentially hostile environment that is difficult to manage and protect.
64 percent of security practitioners said they were very concerned about the use of insecure mobile applications in the workplace with an average of 472 mobile applications reported as actively used in organizations.
Prior to the advent of mobile computing, security was limited to corporate IT assets that were often physically secured in facilities owned and managed by the company, on a network behind a managed firewall, and possibly in a datacenter with multi-factor access, physical security, and armed guards. Because the company owned those assets, they were able to dictate what applications could run on those machines, and actively manage and monitor them, providing the latest patches, endpoint security, and other controls dictated by corporate IT. Assets located in such places were implicitly trusted.
Today, the situation has changed. Mobile devices dominate the market, often as the primary or only way users access the Internet and the many cloud services available. These devices also have very little, if any, physical security. It is a well-worn path hackers use to access such devices to reverse engineer or tamper with the applications running on them, often through rooting, jailbreaking or hoodwinking the user.
This shift has created all sorts of new business models to take advantage of the popularity of mobile devices.
By 2025, the total global worth of IoT technology will reach USD 6.2 trillion with the most value coming from health care devices (USD 2.5 trillion) and manufacturing (USD 2.3 trillion). Meanwhile, we see a persistent lack of IoT security investment with 67 percent of medical device makers expecting an attack on their devices while only 17 percent taking measures to prevent an attack. These numbers are staggering when you consider U.S. hospitals have an average of 10 to 15 connected devices per bed with some hospitals registering 5,000 beds — totaling 50,000 connected devices per hospital.
Furthermore, traditional security solutions do not port well to the IoT world, due to differences in system architectures and resource constraints. Therefore, IoT security solutions have not evolved enough and are prone to numerous vulnerabilities.
Download the Intertrust Code Protection white paper to learn more about the methods hackers use such as reverse engineering, tampering with code and exploited design flaws.
SANS Institute, IT Security Spending TrendsPonemon Institute, 2017 Cost of a Data Breach StudySymantec, Internet Threat Security ReportPonemon Institute, State of Mobile and IoT Application Security StudyCSO Online, Hackers Found 47 Vulnerabilities in 23 IoT Devices
Building Trust for a Connected World
21 
21 claps
21 
Written by
Senior Product Evangelist in data and security. All things #startups #mobile, #data #security and #IoT. Snowboarder, book worm.
Building Trust for a Connected World
Written by
Senior Product Evangelist in data and security. All things #startups #mobile, #data #security and #IoT. Snowboarder, book worm.
Building Trust for a Connected World
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://architecht.io/below-the-surface-ces-is-becoming-the-next-cloud-computing-battleground-842c4a2c8dad?source=search_post---------125,"By all accounts (including this one), CES 2017 attendees are being bombarded by the Internet of Things—countless devices promising great new experiences thanks to technological advances such as voice control and computer vision. If robots, “smart” hairbrushes and Amazon Echo clones continue to replace high-end TVs as the stars of CES, then the untold billions in consumer spending the show represents will increasingly be linked to large cloud platform providers such as Amazon, Google and Microsoft.
IoT and “smart” consumer experiences depend heavily on technologies such as big data and artificial intelligence, at least if device makers want to deliver an enjoyable and useful user experience. As it happens, these also are areas where cloud providers presently hold nearly all the cards, and where they’re already girding their loins for battle with each other. To look back at their activities over the past couple years is to watch the groundwork being laid for the coming fight over who controls the Internet of Things.
Here’s the quick and dirty version of where we stand today:
All of this stuff can be very difficult to master, by the way. Many companies still struggle to build respectable mobile apps, deploy Hadoop in any meaningful way or wrap their heads around microservices. It’s a tall order to expect L’Oréal or a small smart kitchen startup, for example, to suddenly gain the expertise to train AI models and manage real-time data pipelines.
Subscribe to the ArchiTECHt newsletter for daily news and analysis! Want to sponsor the newsletter or podcast? Email advertise@architecht.io.
Given this, it’s arguable the really important consumer conference now happens several weeks earlier than CES, just a few blocks west of the Las Vegas Convention Center where CES is centered. That’s when, at some point in October or November (at least for the past few years), the Amazon Web Services re:Invent conference rolls into town.
The 2016 incarnation of re:Invent served as a warning shot for what the world would ultimately see as CES, as well as a healthy reminder of just how much control cloud providers have in the world of IoT. Amazon rolled out a new set of developer tools, based on what is has already built for the Echo and its Alexa digital assistant, to help developers incorporate AI-based voice recognition and computer vision into their own applications. A year earlier at re:Invent, the company announced AWS IoT, a one-stop shop for all the advanced compute and data functions that IoT applications require.
A new component of AWS IoT, announced in 2016: a service called Greengrass to let connected devices, according to AWS, “run AWS Lambda functions, keep device data in sync, and communicate with other devices securely — even when not connected to the Internet.” Yes, AWS wants developers to push simple functions down to the device while saving the cloud for higher-latency and compute-intensive workloads such as analytics and long-term storage.
It’s also worth considering that Amazon, Google and Microsoft will all be ramping up their own IoT and smart home efforts over the coming years. The more data they get from users of their own IoT platforms, and from external apps integrating with them (possibly also running on their clouds and using their SDKs), the smarter their AI models—and, thus, the ones they ultimately productize—will get. The better the companies will get at understanding how to deliver prepackaged IoT services via their cloud divisions.
It’s a virtuous cycle that’s already in full swing for Amazon. Various reporters have deemed Amazon Alexa the champion of CES this year, meaning more integrations, more data, more cloud users, better Alexa, better AWS … But Google and Microsoft will not go down without a fight.
There will always be good reasons for companies to host their own infrastructure and use open source tools from independent communities, but in a world where consumer expectations and technological advances are both advancing at breakneck speed, there’s also a lot to be said for leaving the hard work to the experts. Whatever your thoughts on the matter, it’s a message on which the cloud provider community will certainly turn up the volume over the next couple years.
With the obvious exception of Apple and maybe Samsung (which runs at least some portion of its IoT business on AWS), it’s easy to imagine a future where a large percentage of our consumer experiences are directly or indirectly served to us by Amazon, Google or Microsoft. If you’re not speaking directly to one of their devices, the “brains” of whatever you’re speaking to, or wearing or otherwise interacting with could very likely be running on one of their clouds.
Correction: Amazon announced AWS IoT in 2015, not 2016 as originally stated in this post.
Enterprise IT interviews and analysis: AI, cloud-native, startups, and more
4 
4 claps
4 
Written by
Founder/editor/writer of ARCHITECHT. Day job is at Pivotal. You might know me from Gigaom - way back in the day, now.
Once a site about next-gen enterprise IT and the people building it; now a place where Derrick Harris occasionally blogs about tech-related things.
Written by
Founder/editor/writer of ARCHITECHT. Day job is at Pivotal. You might know me from Gigaom - way back in the day, now.
Once a site about next-gen enterprise IT and the people building it; now a place where Derrick Harris occasionally blogs about tech-related things.
"
https://medium.com/@furrier/key-takeaways-from-the-wikibon-research-and-north-bridge-vc-2015-future-of-cloud-computing-survey-55896a9ff4c3?source=search_post---------126,"Sign in
There are currently no responses for this story.
Be the first to respond.
John Furrier
Jan 24, 2016·7 min read
This is a research note from Brian Gracely from Wikibon Research part of SiliconANGLE Media Inc. — home of @theCUBE, SiliconANGLE.com, and Wikibon.com
Wikibon.com partnered with North Bridge Ventures, as well as 30+ technology leaders, to survey nearly 1000 Cloud Computing practitioners about their current experiences and future expectations for Cloud Computing usage. The survey validated a number of on-going trends (e.g. SaaS is the dominant Cloud Computing consumption model) as well as providing insight into the shifts towards usage of emerging technologies such as SDN, PaaS and DBaaS. Insight into the “why” of Cloud Computing planning and spending were highlights of the survey, along with feedback about the top inhibitors that will prevent increased usage of Cloud Computing resources in the future.
The full 2015 Future of Cloud Computing survey presentation, video discussion and CrowdChat community discussion can be found here.
Private Cloud-only strategies are in decline, and Hybrid Cloud is evolving and growing. But IT organizations still aren’t sure what Hybrid Cloud means or how best to use it. Over the past 5 years, the focus of IT organizations has significantly shifted from Private Cloud strategies (-48.4%) to Hybrid Cloud (+19.2%) or Public Cloud (+43.3%) consumption models. This not only aligns to changing need for greater business agility, but also aligns to Wikibon’s contention that IT organizations need to hold their Private Cloud plans to a higher standard.
IT organizations are looking to evolve beyond basic IaaS services (SDN, PaaS, DBaaS). As Public Cloud providers such as AWS, Azure and Google continue to add new services, more companies are beginning to experiment with emerging Cloud technologies such as SDN (23%), PaaS (52%) and DBaaS (38%). This highlights that IT organizations are being pushed by development teams to keep up with the pace of innovation in the Public Clouds. It also illustrates that automated, API-driven, software-centric technologies are driving the agenda for future IT projects.
Figure 2: Emerging Cloud Technologies Showing Growth (Source: North Bridge Ventures — 2015 Future of Cloud Computing Survey)
Table 1: Emerging Technologies being Used or Evaluated over next 12–24 Months
IT organizations are getting more educated about the value of Cloud, but concerns about Security/Privacy are still high. While scalability is still the primary focus of cloud projects, the market has become better educated regarding the value of business agility vs. IT cost reductions. Survey respondents are also beginning to understand that Cloud Computing is less of an area of innovation, and more of a requirement for any new IT project. They are beginning to understand that there are no longer business projects that don’t have a significant technology component, and that they must be better prepared to respond within the timeframes needed to execute new business initiatives.
Figure 3: Cloud Computing Drivers, 2011–2015 (Source: North Bridge Ventures — 2015 Future of Cloud Computing)
But as 2015 comes to a close, Security (63%), Compliance/Regulatory (63%), Privacy (56%) and Performance (52%) are still top concerns and inhibitors to widespread Public Cloud usage. These are areas that are full of opportunities for companies to not only deliver innovative technology, but also educate both IT and business leaders about how to minimize these barrier to success.
Figure 4: Cloud Computing Inhibitors (Source: North Bridge Ventures — 2015 Future of Cloud Computing.
Over the past 4–5 years, Venture Capital firms are primarily investing in SaaS startups. VMware is very likely the last (new) major software vendor to primarily sell on-premises products. IT organizations will need to learn how to deal with SaaS, either directly or through platforms that integrate SaaS applications with existing applications. Unlike the dot-com period of the late 1990s and early 2000s, where startups would often raise $50M and spend the first $5M (CAPEX) on infrastructure, VCs are now mandating that infrastructure spending be directed to public cloud services and OPEX spending models. This not only lowers the barrier of entry for startups, but also it reduces VC risk and has a negative impact on traditional IT vendors that were often the recipients of that CAPEX spending.
Figure 5: Cloud and SaaS Financing (Source: North Bridge Ventures — 2015 Future of Cloud Computing)
In discussions with IT organizations, Wikibon has often found that SaaS spending is often controlled by groups outside of IT. While IT may get involved in some aspects of SaaS usage, such as Active Directory integration, it is an area that many IT organizations feel is a blindspot for them and a concern about data management and security.
Cloud Computing is being embedded in the majority of physical products and often becoming the differentiation of those products. More companies are beginning to offer Cloud Computing services for their customers. For many years, Cloud Computing was associated with digital products and websites. But that is evolving as computing functionality is being embedded in nearly every new physical product. This low-cost computing capability is beginning to unlock new, differentiated experiences for physical products. For example, automotive companies BMW (Apple), Mercedes (Pivotal Cloud Foundry), Tesla and others now have initiatives to enable new applications to deliver a better in-vehicle experience, as well as improvement on-going maintenance. This is an industry, that for decades, attempted to differentiate their products based on sleek external engineering or gas mileage, but are now focusing on a completely different area of uniqueness.
Figure 6: The Pervasiveness of Cloud (Source: North Bridge Ventures — 2015 Future of Cloud Computing)
Most customers are going directly to Cloud providers (IaaS, PaaS, SaaS). The buying supply-chain is being significantly disrupted. In the past, the buying process involved vendors, distributors, ISVs, system-integrators or VARs and in-house purchasing departments. The shift to on-demand public cloud services is radically changing not only the buying process, but impacting the companies that had previously been involved in the long supply-chain between vendor and customer.
Figure 7: The Evolution of Customer to Vendor Relationships (Source: North Bridge Ventures — 2015 Future of Cloud Computing)
These changes are having the following impacts on the buying process:
Vendor consolidation is happening, and customers are using fewer vendors. As we saw with major 2015 activities such as the Dell acquisition of EMC, Cloud Computing is having a significant impact on all aspects of the IT industry. Survey responses highlighted that customers are looking to work with less vendors than in the past, as they attempt to simplify overall operations and be more focused on creating business value rather than being an internal system integrator.
Figure 8: Customers seeking Fewer Vendors in the Cloud (Source: North Bridge Venture — 2015 Future of Cloud Computing)
Action Item: The move to Cloud Computing is no longer a technology discussion; it has become a core competency that every modern business will need to embrace and excel at in 2016 and beyond. IT organizations need to build stronger relationships with internal business groups to understand how technology is impacting their business, products and services. And IT organizations need to accelerate their skill-set evolution to be better prepared to manage Hybrid Cloud environments, emerging technologies and the expectations of Public Cloud “like” experiences for all new services.
Silicon Valley / Palo Alto entrepreneur; Founder CEO SiliconANGLE Media Inc. — home of @theCUBE, SiliconANGLE.com blog, Wikibon.com research
See all (2,466)
5 
5 claps
5 
Silicon Valley / Palo Alto entrepreneur; Founder CEO SiliconANGLE Media Inc. — home of @theCUBE, SiliconANGLE.com blog, Wikibon.com research
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/swlh/aws-ome-satellite-communication-via-cloud-computing-fcfc00536ba4?source=search_post---------127,"There are currently no responses for this story.
Be the first to respond.
Such a statement must’ve sounded like a sci-fi short story headline just a few years ago, but what follows below is very much real, thanks to modern cloud computing.
I’ve always been curious about how firms like ISRO, NASA and weather companies handle streaming…
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/digital-cloud-training/9-common-uses-of-cloud-computing-81739d997da?source=search_post---------128,"There are currently no responses for this story.
Be the first to respond.
There are many uses of cloud computing that can offer significant business advantages to companies. Cloud computing is a computing model and set of technologies that allows consumers to access cloud services using the Internet on a pay-as-you-go basis.
Despite being a relatively new paradigm that came into mass favour only within the last several years, cloud computing is becoming more and more popular by the day.
From government agencies and NGOs to non-profit organizations and small start-ups, the various uses of cloud computing have proven effective in providing an array of solutions to various problems.
That said, have you adopted cloud computing in your business? Well, if you haven’t, it’s possibly because you don’t understand its use cases and the benefits that you can achieve for your business.
Below are some common uses of cloud computing that should prompt you to consider how this technology can assist your business.
There are lots of options for how to store and access your data. There’s the hard drive on your laptop, the external hard drive you use for backing up and transferring data, network file shares, USB drives and more.
With so many storage options available, what makes cloud storage unique?
Well, the main reason why cloud storage is appealing is that files can be accessed and edited with ease. All you need is an internet connection and you can access your files from any device, anywhere.
There are several types of cloud storage available including block, file and object storage. These each fit different use cases from shared filesystems to block-based volumes and backup and archiving systems.
With cloud computing storage services such as Amazon S3, DropBox, or OneDrive, you will be provided with secure access and the scalability to increase or decrease storage based on your needs and budget. For this reason, this type of storage is not only secure but also extremely affordable.
Today, your business will be at a significant disadvantage if it doesn’t collect big data. This can be data about your customers, market trends, sales performance and more.
Companies of all sizes need big data for a myriad of reasons. Some collect it to discover new opportunities for business growth, while others do so to find solutions to complex problems.
Collecting and analyzing big data, however, doesn’t come easy. It demands the use of vast computing resources, which come with a hefty price tag.
If you were to purchase the resources needed for cloud computing, then you may have to slash budgets meant for other essential services such as marketing. This is something you don’t have to do if you adopt cloud computing.
The primary benefit of cloud computing is that it comes with a pay-as-you-go pricing strategy. This means that you will not have to pay for unused time — saving your business a significant amount of money. You only access resources and pay for them when you need them.
Without a doubt, cloud computing makes big data analytics simple, useful and inexpensive. For more information, Amazon Web Services (AWS) offer several analytics services for various use cases.
Today, we live in a world where cybercrime is the order of the day. No day will pass without cases of major data breaches, which at times become fatal for quite a number of businesses.
Traditional methods of data backup have proven effective in backing up data for a long time. Nonetheless, they are prone to viruses, and due to their portable nature, they can get lost and pose a threat to modern businesses.
Cloud-based backup and archiving is a solution to these challenges. It is easy to implement and provides maximum data security. With this approach, you can backup or archive your sensitive files to cloud-based storage systems. This provides the assurance that your data is still intact even if your live data becomes somehow compromised.
Some cloud computing services allow you to schedule backups to meet your unique needs. Additionally, you can encrypt your cloud backups and make it impossible for hackers and snoopers to access.
With cloud storage, you can get as much space as you require and store as much data as you need and will only pay for what you actually use.
Do you know the cost of not having a business continuity plan in place? Research has shown that over 75% of business that experience a disaster and do not have a disaster recovery strategy in place fail within 3 years of a disaster.
Traditionally, building a disaster recovery site and testing your business continuity plan has been an extremely expensive and time-consuming task.
But it doesn’t have to be that way anymore!
With cloud computing, you can build a disaster recovery solution in the cloud. In this model you create a replica of your production site and constantly replicate data and configuration settings.
In the event of a disaster at your production site, it is fast and relatively simple to launch your applications and data services in the cloud and get your business back up and running in no time.
An example of an automated and orchestrated disaster recovery solution is CloudEndure by AWS. Check this blog article to learn how you can automate your disaster recovery.
If you have developed an in-house application or software in the past, you can attest to the fact that the process is long-winded, expensive and pricey. It requires the installation and configuration of sophisticated hardware and software and the continuous training of the involved staff members.
This simply means that a simple project can take months to complete, and this could put you at a disadvantage in today’s super-competitive market.
Cloud computing providers offer many tools for continuous integration and continuous delivery that make development and testing faster, less complex and cheaper.
If you’re interested in learning all about the Developer Tools offered by AWS, check out the FREE AWS cheat sheets from Digital Cloud Training.
With these cloud development tools, you can gain an advantage by shortening your software delivery lifecycle time and ultimately gain an edge over your competition.
Running the physical servers and virtualization infrastructure to host your virtual machines requires significant investments in acquiring and managing IT infrastructure.
To save costs on this, businesses are turning to cloud computing, whose pay-per-use pricing scheme creates a perfect blend of quality and affordability. With Infrastructure as a Service (IaaS), you can say goodbye to your VMware licensing costs along with all of the hardware you run your virtual servers on.
This is because with IaaS you simply deploy virtual machines, also known as “instances” in the cloud, without having to manage any of the underlying hardware or hosting software.
But what if you don’t even want to manage the instances and their operating system software? This is where Platform as a Service (PaaS) comes in. When using the PaaS computing model you simply upload your code to the cloud service and everything is taken care of for you to launch and manage your application in the cloud.
An example of a PaaS service is AWS Elastic Beanstalk. This service allows you to simply upload code and Elastic Beanstalk then takes care of building out your Amazon EC2 instances, Auto Scaling groups, Elastic Load Balancers and even Amazon RDS databases.
Cloud computing allows people to access cloud-based communication tools such as calendars and emails. Also, messaging and calling apps such as WhatsApp and skype are all built on cloud infrastructure.
The messages and files you send and receive are stored in the cloud service and not just on your device. This makes it possible for you to access them from any device and any part of the globe through the internet.
Perhaps one of the most ignored applications of cloud computing is social networking. Platforms such as Facebook, Twitter and LinkedIn are examples of the Software as a Service (SaaS) cloud computing model.
Social media platforms are developed to help you find people you know — or connect with those you don’t know. They also give you many tools for sharing information and data such as tweets, photos, instant messages and posts.
Along with cloud storage, social networking is one of the most common use cases for consumer-driven usage of cloud services.
If you are using business management applications such as Enterprise Resource Planning (ERP) or Customer Relationship Management (CRM), you have already incorporated cloud computing into your management strategy.
Such enterprise-level applications are deployed using software as a Service (SaaS), which heavily relies on cloud computing models. They ensure convenient maintenance, security and management of your business’ essential resources. Additionally, they offer optimum efficiency to service providers and their consumers.
Cloud computing is doubtlessly a growing market and there are many advantageous uses of cloud computing services. New enterprises are rapidly transitioning to the cloud and it has become the ideal medium for testing and development, communication, storage and deployment of software.
With all these uses of cloud computing and the benefits outlined above, there is no reason why you shouldn’t incorporate cloud computing into your business today and gain a competitive advantage.
For further information on Cloud Computing, please also check out:
Get in touch with us here.
AWS Certification & Training
15 
15 claps
15 
At Digital Cloud Training, we’re passionate about helping students achieve their cloud career goals.
Written by
Founder of Digital Cloud Training, IT instructor and Cloud Solutions Architect with 20+ year of IT industry experience. Passionate about empowering his students
At Digital Cloud Training, we’re passionate about helping students achieve their cloud career goals.
"
https://medium.com/@neal-davis/how-cloud-computing-works-345d2226a790?source=search_post---------129,"Sign in
There are currently no responses for this story.
Be the first to respond.
Neal Davis
Mar 30, 2021·5 min read
Cloud Computing is something we’re all using every day, whether we realize it or not! The apps on your smartphone, the email you use, and the social media platforms we use are all powered by the cloud. So, how does it all work? Well, to understand how cloud computing works, there are several things you need to know about the whole concept of cloud computing. Read on!
You need to understand the meaning of cloud computing before you understand how it works. Basically, cloud computing is the delivery of computing solutions, including storage, servers, networking, analytics, intelligence, and software over the web to provide flexible resources, speedy innovation, and economies of scale. Most cloud computing services are based on a pay-as-you-go basis, which simply means that you only pay for the services you use. This helps businesses and individuals lower their operation costs, run IT infrastructure more efficiently, and scale their business needs.
The way cloud computing works is simple. Instead of owning data centers or IT infrastructure, organizations can rent access to everything — including storage and applications — from a cloud computing service provider.
To better understand how the cloud computing system works, it is prudent to divide it into parts: The front end and the back end. These two sections are connected to one another through a network, which is basically the internet. The front end is the side of the client or computer user, while the back end is the “cloud” part of the system.
The front end contains the client’s computer network and the application needed to access the cloud system. It’s worth noting that all cloud systems are not required to have a similar user interface. On the backend of the system, there are several servers, data storage systems, and computers that make up the cloud. A cloud computing system could include any computer program from video games to data processing. Generally, all cloud computing programs have their own dedicated servers.
Through cloud computing, businesses enjoy a wide range of benefits, including:
In return, cloud computing service providers benefit from significant economies of scale by offering the same services to thousands or even millions of clients. Cloud computing is simply a win-win!
From the explanation in the section outlined above, you now have a clear understanding of how cloud computing works and the benefits that come with it, right? You may now wonder how the technology that makes up cloud computing is managed?
Cloud computing technology is managed by a central server. The main purpose of this server is to manage traffic and client demands to make sure all applications and other services run smoothly. It uses a special software known as middleware to pursue a set of protocols. This permits networked computers to communicate with each other easily.
You cannot talk about cloud computing and fail to mention cloud storage. Cloud storage works by enabling users to easily access and download data from multiple devices, including smartphones, tablets, and laptops, via the internet. Through cloud storage, users can also edit files simultaneously with other users, and this makes cloud computing highly effective for a remote workforce.
Cloud computing prices vary depending on business needs. Individual users normally get cloud storage services for free, but you may need to pay some fee for additional storage. Price models include monthly and yearly rates depending on the service you have subscribed to.
Cloud computing is becoming a default option for small, medium, and large enterprises across the globe. Now that you have learned how this technology works, you can easily integrate it into your business and enjoy the amazing benefits that come with it. However, despite gaining popularity and enjoying mass adoption, cloud computing does come without its fair share of downsides. There are many risks associated with this technology, and cybercrime is the biggest of them all. Make sure you only subscribe to the most reputable cloud computing services. Also, use strong passwords for your applications, and utilize two-factor authentication. This way, you will make it much more difficult for hackers to access your systems and steal crucial information that could mean an end to your business.
For more info, contact our friendly Digital Cloud Training team here. We’ll support you in your cloud journey with the best hands-on AWS training.
Founder of Digital Cloud Training, IT instructor and Cloud Solutions Architect with 20+ year of IT industry experience. Passionate about empowering his students
6 
6 
6 
Founder of Digital Cloud Training, IT instructor and Cloud Solutions Architect with 20+ year of IT industry experience. Passionate about empowering his students
"
https://medium.com/@ilmizer/this-is-how-cudos-cloud-computing-technology-works-you-need-to-know-734488a4c8f9?source=search_post---------130,"Sign in
There are currently no responses for this story.
Be the first to respond.
Ilmizer
Dec 31, 2020·5 min read
Cloud Computing is a term that we have often heard, however, still less understand what it is the meaning, function, and how Cloud Computing works in daily life. In large companies and organizations, the importance of Cloud Computing is to make it more comfortable to associate from one computer to another without consuming a lot of money and energy.
Recently, the blockchain technology that underlies Bitcoin has grown in popularity. Blockchain is the latest financial technology that supports multiple Information Processing Units (IPU) on virtual financial transactions. Blockchain customers store their data on P2P networks, with the aim of effective utilization of computing resources. Two main algorithms: proof-of-work and proof-of-take, are used to ensure the security of blockchain transactions is truly maintained and decentralized.
In this article, I will review the blockchain application in cloud computing developed by CUDOS. Initially, I will briefly discuss the advantages and disadvantages of cloud computing and its integration with blockchain. Then the CUDOS method provides solutions to these problems while providing many conveniences in one platform.
If used properly as needed, working with data in the cloud can be of great benefit to any type of business. Some of the advantages that we can take advantage of are: 1)Cost-effective, 2)Unlimited storage, 3)Backup and Restore 4) Automatic Software Integration.
Although the use of the cloud provides a lot of convenience by being accessible anywhere and anytime, there are serious system problems. The SEC’s Office of Compliance Inspections and Examinations (OCIE) warned many companies about Technical issues, security, and prone to Attack in the cloud. We must recognize that new technology is prone to technical problems, even though it guarantees the best maintenance standards. In addition, we must always be connected to the internet, and we know that no internet is completely safe. This is very risky because it involves important personal data. So we really need a solution to solve this problem.
The computing solutions offered by Cudos are truly unique. To solve the misconfiguration of network-accessible storage systems, which lead to accidental data access, Cudos presents a completely decentralized platform. Users can truly feel freedom and security without any influence from third parties. In detail, users will experience decentralized services, including Governance and voting, staking, wallet to store CUDOS token, Multi-Chain, and Develop dApps. The two main compute solutions offered by Cudos are:
Why Do We Need Layer 2 Solution?
Layer-2 solutions suggest more scalable blockchain, sharding technology,and the transaction can be done via side chains. This makes the blockchain more efficient and fast. Cudos is the first oracle network using layer-2 solutions.
This video will give you a summary of what the Cudos project is primarily about.
Cudos provides many services in one platform. Apart from overcoming many problems in cloud computing today, you can also Empowering Smart Contracts, DeFi and the Internet. With guaranteed security and a truly decentralized system. In addition, there are some influential reasons that you should know why you should choose Cudos, that is:
1. Different from other similar projectsCudos combines the benefits of cloud computing and blockchain. It can decentralize data storage and can be used as part of a data logistics platform. This network allows users to virtually cooperate to analyze and maintain information securely. This means that all transaction data and information is stored in multiple locations in encrypted chunks and stored on multiple participating users’ computers. Thus forming a security layer for transactions and data storage.
2. Supported by a strong partner in validation: Blockventure Partner support is also great for strengthening investor and user trust. Currently, there are many well-known companies that have joined and supported Cudos, including: BLOCKVENTURE COALITION, OUTLIER VENTURES, etc. BlockVenture Coalition is officially a validator for the CUDOS Validator Network on a Decentralized Cloud Network. This cooperation is very beneficial for both of them in welcoming web 3.0
3. CUDOS token tradable on JanuaryCudos tokens will be listed in January 2021. Stay tuned for official announcements via official telegram channels and Medium. The CUDOS token has many uses that can be utilized to carry out all activities in the Cudos ecosystem. The three main functions of Cudos tokens are for gaming, staking, and charity. Check out the following infographic.
4. Competent developer and advisor teamEstablished since 2017, Cudos has made a lot of preparations and very significant developments, yes, three years is not a short time. With the support of a team who are professionals in their fields, and have a lot of experience in the field of computing and blockchain, CUDOS is here to bring effective solutions. Advisor consists of experienced people, such as Jorg Roskowetz (Director of Blockchain for AMD) and Chris Deering(Former President and CEO of Sony Entertainment Europe ‘father of the Playstation’)
There are many achievements that have been built well. Check out the full explanation in CUDOS Roadmap Recap.
The public sell will also be completed then just waiting for the listing of Cudos in January 2021. For those of you who are left behind the private sale, look forward to the official announcement from the Cudos team for listing on the official exchange. Ready for the surprise?
That’s my review of Cudos to solve problems in computing and improve Security, Interoperability, Scalability in blockchain networks. Hopefully this is useful, for more complete information please visit:
🌐 https://www.cudos.org/🐦https://twitter.com/CUDOS_🔖https://t.me/cudostelegram
telegram: https://t.me/ilmizerpublish0x: https://www.publish0x.com/@ilmizer
Crypto enthusiast — Copy Writer
See all (1,563)
85 
85 claps
85 
Crypto enthusiast — Copy Writer
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/cudos/4-trends-taking-cloud-computing-to-the-next-level-1d98ea21625b?source=search_post---------131,"There are currently no responses for this story.
Be the first to respond.
Here at CUDOS we’ve been working with cutting edge technologies in order to facilitate the evolution of several industries. A fusion of cloud and blockchain in conjunction with the latest hardware and software are all at the foundations of our project. We’re beginning to see more innovation with more devices being enabled to leverage infrastructure and crypto incentives, all with the aim of pushing for wider adoption.
We’re now coming towards the end of a tough year, and as we look to 2021 there is definitely a lot to be excited about when it comes to cloud computing and the emerging technologies that surround it. Here are 5 trends taking cloud computing to the next level.
Containerization is a form of operating system virtualization, through which applications are run in isolated user spaces called containers, all using the same shared operating system (OS). A container is essentially a fully packaged and portable computing environment. Containerization gained popularity with the opensource Docker engine and Kubernetes by allowing developers to build faster and more secure apps.
One of the main benefits of containerization is a container doesn’t require a full guest operating system or a hypervisor. This translates into a reduced overhead as well as faster boot times, smaller memory footprints and generally better performance.
Serverless computing is a method of providing backend services on an as-used basis that is growing in popularity at a very fast rate. A serverless provider allows developers to write and deploy code without having to worry about the underlying infrastructure. A business that is provided with backend services from a serverless vendor is charged based on their computational output and because of this, they do not have to worry about reserving and paying for a fixed amount of bandwidth or a specific number of servers, as the service is auto-scaling, which means it would automatically adjust to the users’ needs. Despite the name “serverless”, physical servers are still used but developers would no longer need to worry about them.
Machine learning is more than just a buzzword, it is in fact the fastest-growing sector in the serverless space. Machine learning is the application of artificial intelligence (AI) that provides systems with the ability to automatically learn and improve from experience without being explicitly programmed. Now before you start having sci-fi fantasies that may resemble classic films like The Terminator, Enter The Matrix and I-robot, let’s take a look at how machine learning is currently transforming our world. Machine learning companies are currently working to support research on climate change and healthcare. Some machine learning projects have even supported the battle against Covid-19.
Machine learning requires a lot of data and compute power to make the best and most accurate projections possible, cloud computing costs will become even more important to manage for businesses using machine learning. It seems cloud computing and machine learning are set to have a symbiotic relationship as we move towards a world driven by innovations supported by machine learning.
Proof of Stake (PoS) is a consensus mechanism used within cryptocurrency blockchain networks to achieve distributed consensus which is quickly growing in popularity. Proof of Stake was created to solve a few problems with the Proof of Work (PoW) model that was developed as a key mechanism within the Bitcoin network. Proof of Work requires all of the network participants (miners) to attempt to solve a complex sum, with the winner determined by the person who has the most compute power. Then the winner is then rewarded in that networks native cryptocurrency. Proof of Stake is not dependent on compute power of a miner, it instead requires miners to “Stake” their tokens on the network, the more you stake the more chance you have of randomly being selected as the winner.
Staking is a key part of our own project, many projects similar to our own have adopted staking as it tends to have positive impacts on the economics of a token. Many speculators seem to be under the belief that this leads to reducing the supply of tokens in circulation which would then have a positive impact on price.
To find out more about our staking mechanisms click on one of the links below and join our community!
Next Generation Cloud
12 
1
12 claps
12 
1
Written by
Next Generation Cloud
Next Generation Cloud
Written by
Next Generation Cloud
Next Generation Cloud
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/pcmag-access/what-is-cloud-computing-eeac522831fb?source=search_post---------132,"There are currently no responses for this story.
Be the first to respond.
The ‘cloud’ is a real buzzword, but what is it, how does it impact what you do, and is it anything really new?
By Eric Griffith
What is the cloud? Where is the cloud? Are we in the cloud right now? These are all questions you’ve probably heard or even asked yourself. The term “cloud computing” is everywhere.
"
https://medium.datadriveninvestor.com/fiserv-fisv-a-cloud-computing-company-that-makes-money-market-mad-house-190edd1e2f72?source=search_post---------133,"There are currently no responses for this story.
Be the first to respond.
Cloud-computing stocks are hot because of the remote work boom coronavirus created. Unfortunately, most cloud-computing companies such as Cloudflare (NYSE: NET) lose money.
Cloudflare; for example, reported a -$21.25 million quarterly operating income on 30 September 2020. In contrast, Fiserv Inc. (NASDAQ: FISV) reported a quarterly operating income of $542 million on the same day.
Fiserv (FISV); which provides financial-services Internet-of-Things (IOT) solutions makes money…
"
https://medium.com/coincentral-staff/what-is-cloud-computing-the-basics-of-digital-outsourcing-1bffe01475d9?source=search_post---------134,"There are currently no responses for this story.
Be the first to respond.
Cloud computing: the synergistic boardroom buzzword that you still pretend to know about.
Luckily, it’s a pretty simple idea, technically demanding, but simple none-the-less. On paper, cloud computing is just another way for humans to share resources and increase production.
When you use cloud computing you are essentially outsourcing a computer-related task the same way a company may choose to outsource a task like accounting, manufacturing, customer support, or human resources to name a few.
Cloud computing instead outsources tasks such as data storage, web server hosting, Bitcoin mining (warning), and software management among others.
In order to really understand the perks of cloud computing let’s paint a picture of two similar e-commerce businesses. Both businesses are selling a product and using a website as their primary sales portal. Both are also new businesses with a small customer base but can reasonably expect to increase traffic to their e-commerce store in the future.
The first business, let’s call it Tod’s Toys, is running their website on locally installed servers and hosts all their own data. Not to worry though, Tod’s Toys has an excellent CTO running the operation and has the current hardware/software stack purring along.
The second business, this one named Gupta’s Guitars, is a little more bespoke and decided to instead opt for hosting their website on a cloud server. Gupta’s Guitars also has a capable CTO monitoring the online store’s health.
In their beginning stages, Tod’s Toys and Gupta’s Guitars are seeing similar traffic rate to their stores. However, Tod’s Toys is noticing a higher operating cost coming from their web servers; they have more than they currently need. The toy store doesn’t mind though, as they expect traffic to increase into the server capacity they have.
Gupta’s Guitars, on the other hand, paid for their server use much more ad hoc. Their server access scales with traffic, so the guitar store hasn’t noticed any waste. In fact, while their traffic volume was low so was their cost for using the cloud servers. Naturally, they threw a guitar-fueled pizza party with their savings!
As predicted, both online stores begin to see a precipitous uptick in volume and sales. Gupta’s Guitars rejoices and probably throws another pizza party. Tod’s Toys, on the other hand, doesn’t have as long to celebrate.
The online toy store quickly pivots to scaling their server hardware as demand on their self-hosted platform outpaces their capacity. Potential customers are served 404 error messages instead of the spectacular toys that Tod’s offers. *Sad face*
You can see, cloud computing let Gupta’s Guitars outsource their server needs and as a result, focus on other aspects of their business.
A ridiculously oversimplified example but the key point is there.
Cloud computing for businesses, as in the above example, is typically referred to as enterprise cloud computing. This differs from other cloud computing services that may be more consumer-facing like Google Drive or MegaUpload (R.I.P.).
In either case, cloud computing is actually a stack of three generalized cloud provided services. At the base of the stack is the infrastructure cloud services also known as infrastructure as a service (IaaS). The middle layer is the developer’s layer known as platform as a service (PaaS). The top and the most visible layer is the software as a service (SaaS) layer also known as the application layer.
IaaS (infrastructure as a service) is the foundational layer made up of all the necessary hardware that makes the digital cloud tick. Despite the reference to watery vapor above us, cloud computing is made of some serious hardware, real, tangible, and often loud. IaaS is all of the physical hardware that stores and moves our zeros and ones.
Examples of IaaS providers: CloudSigma, Digital Ocean, Linode, Cisco Cloud Infrastructure Services, Microsoft Azure, Citrix Workspace Cloud
PaaS (platform as a service) is the next layer up, where the developers and programmers get involved. In this middle layer, IaaS providers lease chunks of cloud hardware to developers and programmers pre-installed with developer tools like Apache or MySQL. This middle layer is where IaaS providers and software developers overlap.
Examples of PaaS providers: Oracle Cloud, Salesforce Platform, Google Cloud Platform, Amazon Web Services
SaaS (software as a service) is the topmost and more familiar layer of the cloud stack. This is where applications and software are, and we see some familiar names like Spotify, Adobe Creative Cloud, Google Play Store, Storj, and Dropbox to name a few. The SaaS layer is essentially where cloud services become user-friendly for consumers and businesses alike.
Examples of SaaS providers: Slack, WordPress, Trello, Mailchimp, InVision, Zoom, Buffer, Contently, Netflix
Each layer of the cloud service stack enables the one before it. In short, you can think of the three layers like this: first, you need hardware. Second, you need a platform to build from. Third, you need applications so people can use the hardware.
While each use case will have much more granular pros and cons, the following are a few general benefits and drawbacks of cloud computing.
The next evolution to the cloud service stack should be one that can support a distributed infrastructure layer. By fragmenting smaller pieces of a sizable cloud infrastructure, we might be able to shift the centralization of hardware and alleviate that security vector.
If only there were a system of organization that could incentivize hardware providers to come together in a distributed method in order to provide cloud-like services to platform and software developers. If only.
Originally published at coincentral.com on November 23, 2018.
Follow us on twitter @realcoincentralFollow me on twitter @marshalletaylor
Curated editorials from the friendly decentralized…
10 
1
10 claps
10 
1
Curated editorials from the friendly decentralized CoinCentral journalists.
Written by
Canadian Creative and Professional Content Creator/Marketer | Armchair Economist | Travel Enthusiast
Curated editorials from the friendly decentralized CoinCentral journalists.
"
https://medium.com/swlh/3-reasons-you-should-consider-cloud-computing-31171ce6cfb5?source=search_post---------135,"There are currently no responses for this story.
Be the first to respond.
If you’re not yet familiar with cloud computing, it’s time to get up to speed. While network-based computing has been around since the 1960s, the term “cloud computing” didn’t take off until around 2006 when Google CEO Eric Schmidt coined the term at a tech conference. Since then, cloud computing has…
"
https://medium.com/@david-bl/cloud-computing-how-connected-is-your-business-3da68d852e76?source=search_post---------136,"Sign in
There are currently no responses for this story.
Be the first to respond.
David Bailey-Lauring
May 22, 2016·4 min read
Do you keep reading about “the cloud computing” but are not sure what this refers to?
The cloud, simply, refers to software and services that run on the Internet instead of your computer. Apple iCloud, Dropbox, Netflix, Flickr, Google Drive, Microsoft Office 365 — these are all cloud services. There are many advantages to using the cloud. You can access your files — videos, photos, documents, and other software that lives in the cloud on any device with an Internet connection. The cloud lets you enter bookkeeping data on your laptop at home and finish it on your smartphone while riding the train. You can then use it to amend on your PC at work.
This is because software applications including SugarCRM, Quickbooks and MailChimp (web based applications — web apps) to name a few are becoming inseparable from the cloud. All these services, automatically backup your data files to the Cloud.
There is a lot of personal data stored in the cloud that you did not create. Software-as-a-Service users (SaaS) store your customer records in the cloud, insurance companies put your claims there, and friends post photos of you on Social Media. A tremendous convenience when you want to access the information — but surely worrying when your personal data is being used by others you did not consent to?
Is it safe then? Only as much as you trust the company — and your own passwords. Most cloud companies have excellent security records. You can run from the cloud — but it is becoming increasingly difficult to hide. Your job might require you to log onto cloud-based software.
Instead of housing information on your hard drive or your phone’s memory, your Cloud files are stored in massive servers across the world.
But it is not just consumers who use the cloud. Businesses are increasingly ditching their internal servers and software in favour of cloud-based ones. Storing files and data in the Cloud has evolved into data migration and synchronization to ensure that data “speaks” to all software systems that access the cloud. For example, Zapier aids users so they can manage and ensure all data held in various web based applications are the same. Users enter customer data in bookkeeping or finance system like FreshBooks or QuickBooks, it is then checked for errors and mismatches in a central hub against data held in a CRM system like SugarCRM or Salesforce. Once users have checked the data, they can then proceed to sync ensuring that all records match across one, two, five or even ten different software systems.
If you are not already using a cloud computing software for your business, then there are certain key benefits that come with using a cloud solution that you should be aware of. These include:
Access to real-time data — This is one of the biggest advantages of using a cloud-based accounting system. Since users can access the system remotely, they can input data — such as expense reports and purchase orders — from wherever they are working that day, provided they have a decent internet connection. Meaning that the data you have access to is up to date, allowing for better real time decision making that is based on real data rather than hunches.
Remote access — Remote access is often the key selling point for any type of cloud software and for good reason. Being able to access your financial, contact, invoice data from any device, anywhere, at any time is hugely beneficial for business owners. Not only that, but it means that access to your systems can be expanded to your employees anywhere in the world, if needed. This can provide you with better visibility internationally.
No hardware costs — With cloud-based software, you do not require any in-house equipment or hardware. This is supplied by your cloud provider, meaning that there is no maintenance to worry about, no unexpected costs and you can benefit from the reliability you would expect from enterprise-grade cloud providers.
Always up to date — By using cloud computing, you can be confident that your software is updated. The cloud provider manages your software upgrades, and automatic updates delivered over the internet mean you will not have to worry about falling behind the competition.
Better security — Security is a major consideration, particularly when it comes to your company’s customer data. It may be surprising to many people to learn that cloud systems actually offer a greater level of security than many small and medium businesses could achieve.
Cloud computing is now an integral part of our lives. Now we must manage it effectively and securely.
David is the Managing Director of Blu Mint Digital; and writes about Digital Marketing and Entrepreneurship. This article he wrote was written for Cloutex.
Favour: Would you scroll down + tap the green heart to ‘Recommend’ this if you liked it?
Managing Partner @Blu_Mint | Content Writer | Feminist | Rockstar Daddy to 3 sons | Recovering chocoholic
5 
2
5 
5 
2
Managing Partner @Blu_Mint | Content Writer | Feminist | Rockstar Daddy to 3 sons | Recovering chocoholic
"
https://medium.com/hackernoon/10-reasons-to-give-cloud-computing-a-go-dc3184ec550e?source=search_post---------137,"There are currently no responses for this story.
Be the first to respond.
Cloud transition is not a buzzword anymore. With more than 52% of enterprise businesses having their digital transformation already completed, the cloud becomes the new standard.
The features like security, scalability, cost-efficiency, automated backups, and recovery form the basis of why the cloud is so great. Of course, there also are some myths and misconceptions, and we have already demystified some of these.
However, there are more subtle benefits that are often overlooked, yet can be a real game changer for your business. Thus said, today we list 10 reasons to give cloud computing a go.
The businesses can opt for one of the three main cloud computing types that fit them best. Want to have full control over the processes and ready to pay some extra? Choose the private cloud. Want to be able to scale the resources up and down quickly at low cost and do not expose any mission-critical systems? Public cloud is your choice. Want to get the best of two worlds? Try hybrid cloud and enjoy!
The businesses can dive as deep as they need to the cloud and choose the most appropriate cloud pyramid layer for them. Choose from Software as a Service, Platform as a Service or Infrastructure as a Service depending on what your business needs are.
Every cloud service provider (CSP), be it AWS or Azure, GCP or IBM Cloud provides a plethora of built-in features and tools that cover nearly any aspect of your business workflow. Should you need to develop some specific functionality, there are SDK and API gateways available to connect your third-party software to the cloud platform.
While some features are common to any provider, some are distinctive to one or another. Thus said, you can choose the tools and services that fit your specific business needs best. In addition, modern tools offer a chance to combine tools and features from multiple CSP’s using a multi-cloud strategy.
Cloud becomes the synonym of AI, actually. If you have the infrastructure, the computing resources and the tools to process your data flows and obtain valuable analytics results — why not do it? In fact, 54% of compiled in 2017 Fortune 500 list of companies use Big Data and AI to empower their business. We have told of 8 impressive business success stories based on Big Data and you can see for yourself that the impact is huge.
While this might seem an excessive expenditure item from the get-go, you will see the added value quite soon as your business scales up. Cloud platforms allow adding Big Data analytics and AI algorithms to your service package at any point, so the choice of when to do it is up to you.
When your staff works on a single cloud-based system and stores all the data in the cloud they can access all the data from anywhere, as all they need is an Internet uplink. In fact, up to 73% of business tasks involve collaboration over the Internet, which means they can be done with telecommuting.
In addition, up to 42% of office workers would agree to get up to 5% salary cutto be able to work from home, which means the business can save millions in wages in the long run. Thus said, going to the cloud can cut your salary expenses in addition to infrastructure spending, thus ensuring a healthier, stronger bottom line.
Long gone are the days when an important report could not be delivered and printed due to a server outage or a network issue. CSP’s take care of the infrastructure and ensure its uninterrupted availability, so your business workflow is streamlined. There are even more benefits of moving the infrastructure to the cloud, yet the safety of mind regarding the business continuity and permanent availability of all critical systems is invaluable for business.
Providing access to cloud-based software development platforms on a subscription basis can be much cheaper than purchasing multiple copies of proprietary IDE software. Keep in mind that cloud SaaS development platform providers handle the updates themselves, so your team always has access to the latest technology without having to handle the updates and compatibility issues.
Cloud solutions do not require time to build and polish the infrastructure, they are available once you issue the check. Speed is crucial in the rapidly evolving business landscape of the 21st century, and the solution that took 9 months for release might be already irrelevant. Add the infrastructure design, deployment, and management costs, and you will see why going to the cloud can be an essential part of success.
The question an entrepreneur has to ask quite often is — “what happens if this fails?” Compare the costs of buying the hardware to provision 128 servers needed to perform the task and the time/personnel needed to configure everything correctly — or rent them on an hourly basis. If the market expectations are unclear, is it worth the risk to plan 12 months ahead and invest in building the environment — or would it be better to use free (or almost free) cloud services using a flexible system of trial periods, discounts and spot instances or AWS reserved instances?
Every business should plan their expenses and profits in advance. Tracking the completion of these goals is essential to staying on the safe side. CSP’s provide informative logging tools and customer dashboards that help track the completion of business goals with ease. How much money is spent weekly? How much was spent over the last 6 months? Do we meet the self-sustainability threshold?
Using vast data visualization capabilities provided by cloud vendors the businesses can audit their operational performance and adjust their practices to reach the optimal spending and secure their bottom line.
As you can see, transition to the cloud can provide numerous benefits:
Keep in mind we took the obvious benefits out of the equation from the very beginning. Even receiving any three of the benefits of cloud computing can be a huge boost for your business, but being able to leverage all 10 makes cloud transition a no-brainer decision, doesn’t it?
Initially, I’ve posted this story on my company’s blog — https://itsvit.com/blog/10-reasons-give-cloud-computing-go/
#BlackLivesMatter
80 
1
how hackers start their afternoons. the real shit is on hackernoon.com. Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
80 claps
80 
1
Written by
DevOps & Big Data lover
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
DevOps & Big Data lover
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@david-bl/how-green-is-cloud-computing-1b50cfffc746?source=search_post---------138,"Sign in
There are currently no responses for this story.
Be the first to respond.
David Bailey-Lauring
Sep 2, 2016·5 min read
Cloud computing energy consumption has created a much publicised debate. On one hand where you have experts crediting cloud technology for saving up resources, there is a small section of experts who have criticised cloud for consuming high megawatts of power.
While the debate as to who is right & wrong has not really reached a consensus, I will try and state both the viewpoints and reach a conclusion.
Greenpeace has stated that cloud computer sites can consume up to 622.6 billion kilowatts per hour of power. The Cloud then is consuming huge power resources to satisfy its computing needs. Estimated Cloud consumption is 1–2% of the world’s electricity resources. According to a new research from Pike Research, the global market for green data centers will grow from $17.1 billion in 2012 to $45.4 billion by 2016.
John Engates, CTO of Rackspace has commented that cloud computing has increased power consumption by citing an example of product delivery. According to him, e-commerce that is also a part of computing technology has introduced individual delivery i.e. deliveries are now individually shipped to consumers rather than bulk deliveries to stores. The carbon footprint created by automobiles delivering these products offsets any savings that are made by using cloud based e-commerce web applications (BigCommerce, Magento, eBay etc).
Yet Cloud computing can have a positive effect on the environment. Carbon Disclosure Project’s report has stated that organizations that have adopted cloud-based software have reduced their energy consumption, carbon emissions and IT resources. By 2020, energy savings of $12.3 billion can be achieved in US alone (thanks to cloud computing).
The contribution of cloud to online economy has been incredible. Looking at cloud benefits in a broad perspective it is obvious to determine that because of cloud computing today the online economy has become so dominant. Online economy has made people save more resources. With more and more organizations going digital, huge quantity of resource consumption has been prevented.
We no longer have to drive our cars to obtain products when at a click we can order these online being delivered by our postal service. Due to the online economy, the rise of physical stores/offices has also decreased, reducing the demand on heating and lighting.
We do not have to travel nationally and globally to meet other when mobile and internet technologies have prevented physical travel, like Skype. Another example worth mentioning here are online electronic documents that have put a big stop to deforestation. As Cloutex is based in E-stonia there is a culture of doing everything online and this is something we strive to do in our office!
Lastly and more importantly we do not require large servers to be stored in every office. These are expensive and consume vast amounts of energy. Moving to the Cloud users access large data storage centres globally, that in reality consume large volumes of power lower when offset against if every office or household had one.
However it is important to assess the real impact of the server, and the energy efficiency of data centers and the type of equipment and devices that are used. It is also important to keep in mind that a data center has a more environmental impact than a system. For example if you are storing data on the public cloud but using a public cloud provider whose servers are not that efficient, are not well used and use electricity from higher-carbon-emitting sources, there could be scenarios where running your own servers is a greener option.Whether done in a private or public cloud configuration, as-a-service computing will be greener for (at least) the following five reasons.
1. Resource virtualization, enabling energy and resource efficiencies.
From a resource-efficiency perspective, less equipment is needed to run workloads, which proactively reduces data center space and the eventual e-waste footprint. From an energy-efficiency perspective, with less physical equipment plugged in, a data center will consume less electricity.
2. Automation software, maximizing consolidation and utilization to drive efficiencies.
The presence of virtualization alone does not maximize energy and resource efficiencies. To rapidly provision, move, and scale workloads, cloud-based infrastructure relies on automation software. By pushing the limits of traditional consolidation to automation, the less physical infrastructure is needed, that in turn maximizes the energy and resource efficiencies from servers.
3. Pay-per-use and self-service, encouraging more efficient behavior and life-cycle management.
The pay-as-you-go nature of cloud-based infrastructure encourages users to only consume what they need and nothing more. Combined with self-service, life-cycle management will improve, since users can consume infrastructure resources only when they need it — and “turn off” these resources with set expiration times.
4. Multitenancy, delivering efficiencies of scale to benefit many organizations or business units.
Multitenancy allows many different organizations (public cloud) or many different business units within the same organization (private cloud) to benefit from a common cloud-based infrastructure. Combined with automation, the ratio between peak and average loads becomes smaller, that in turn reduces the need for extra infrastructure. The result: massive efficiencies and economies of scale in energy use and infrastructure resources.
5. Synchronization of existing cloud based software applications
Synchronization provides business users to sync different types of data between various cloud based software. Again using automation this can be checked in one central hub and then synchronize to all web applications simultaneously, reducing the need to be online and the cost to do this.
Analyzing both sides of the argument, it is obvious that the innumerable benefits of cloud computing downplays its flip sides. Although energy consumption is high, overall, running business applications in the cloud is considered as more energy and carbon efficient than running it on-premise server rooms. Therefore the overall green benefits of cloud technology are way more appealing.
David is the Managing Director of Blu Mint Digital; and writes about Digital Marketing and Entrepreneurship. This article he wrote was written for Cloutex
Favour: Would you scroll down + tap the green heart to ‘Recommend’ this if you liked it?
Managing Partner @Blu_Mint | Content Writer | Feminist | Rockstar Daddy to 3 sons | Recovering chocoholic
6 
6 
6 
Managing Partner @Blu_Mint | Content Writer | Feminist | Rockstar Daddy to 3 sons | Recovering chocoholic
"
https://medium.com/hackernoon/demystified-6-myths-of-cloud-computing-f5d1731208d4?source=search_post---------139,"There are currently no responses for this story.
Be the first to respond.
Cloud computing has become the synonym of air — it is everywhere around us and every business needs to have free access to it to survive. Or is it so?
While the transition to the cloud is undoubtedly the right decision for nearly any company (and a step to be made rather sooner than later), this step should be a well-thought and evaluated one. A careful approach helps both avoid unpleasant mistakes of transition to the cloud and begin getting benefits from day one. Therefore, the decision like this should be made fully understanding the benefits of cloud computing, and not falling for any of the myths surrounding it. In this article, we demystify 6 most popular myths of cloud computing for your benefit!
Here are these myths, listed in no particular order:
Below we explain why all of these myths are not exactly what they seem to be.
While utilizing IaaS cloud solutions is definitely cheaper as compared to maintaining your own infrastructure, using cloud computing to the max is impossible without opting for quite a ton of paid SaaS solutions — or hiring a contractor to do this for you. Thus said, while cloud does cut upfront expenses, it imposes a bunch of other demanding expenditure items. The ROI and the worth of the dollars invested are definitely higher for the cloud, though. You simply pay only for the resources spent and don’t have to bother with redundancy, hardware upgrades and the equipment standing idle.
Building and maintaining your own solution is much more costly than using a ready SaaS from a trusted cloud solution provider. For example, collaborating on a Google doc or spreadsheet via a shared link is much better than using self-hosted alternatives or developing an own app. Thus said, there is yet to appear a cloud-based solution like Adobe Photoshop, Articulate Storyline or Adobe Captivate, Camtasia and a plethora of other standalone tools. A huge variety of software was, currently is and is going to be developed and released with no intention to connect it to the cloud. This does not make these tools bad — they are just meant to work this way.
The same goes for proprietary and legacy enterprise systems, many of which can never be moved to the cloud before being fully re-built from scratch. This does not mean these systems should be left untouched, either.
The best way of dealing with legacy software dragons is understanding the way they tick and building the cloud-native analogs; analyzing the existing data stores and moving them to the cloud in several stages while training the personnel to use the new system. This is done to finally have a fully-operational copy of the legacy systems and make a transition overnight, yet such approach is obviously time-consuming and expensive.
While Amazon AWS and Microsoft Azure are undoubtedly the market leaders amongst the cloud service providers, they are not the only solutions, nor are they perfect. This is why so many businesses opt for multi-cloud strategy, where cloud services from various providers can be combined with custom on-prem solutions to form the unique ecosystem meeting the requirements of your business best. Choosing the right management solution like Kubernetes helps accomplish such a project smoothly and reliably.
Even a private cloud might become a viable and feasible solution for some cases, as there are multiple types of cloud computing, ensuring businesses of all sizes can choose the type that fits them best.
While people worldwide were surfing naked photos of celebs stolen from their iCloud accounts back in 2014, the cloud service providers were investing hundreds of millions in tightening their security protocols. This practice definitely paid back tenfold, when CIA signed a $600 million contract with AWS, which was followed by the US Department of Defense moving their data to AWS in September of 2017.
Microsoft Azure is also hosting a variety of transnational enterprises like Adobe, GE Healthcare, and Honeywell and boasts serving 90% of Fortune’s Top 500 companies list. You’d bet giants like these pay close attention to their security — and cloud computing stands up to fulfill the promise.
Even after adopting hybrid cloud computing approach and building a sustainable multi-cloud infrastructure, many corporate businesses are still wary of trusting the cloud service providers with their mission-critical workloads and systems. They follow the paradigm of “keeping the essentials under own hood” adopted while early pilots and test use cases were underway.
However, a recent survey of more than 700 C-level executives by Oracle and MIT tech review showed that nearly a half of enterprises are already fully deployed to the cloud. 46% of the managers say their ability to affect and influence the processes in their organizations has improved significantly.
Previously posted this hint on my company’s blog.
#BlackLivesMatter
40 
how hackers start their afternoons. the real shit is on hackernoon.com. Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
40 claps
40 
Written by
DevOps & Big Data lover
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
DevOps & Big Data lover
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/technology-hits/effective-solutions-combining-iot-big-data-and-cloud-f4b83230790e?source=search_post---------140,"There are currently no responses for this story.
Be the first to respond.
IoT (Internet of Things), Big Data Analytics, and Cloud Computing are three distinct technology domains with overlapping use cases. Each technology has its own merits; however, the combination of three creates a synergy and the golden opportunity for businesses to reap the exponential benefits. This combination can create technological magic for…
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/codica/saas-vs-paas-vs-iaas-which-is-the-best-cloud-computing-model-for-your-startup-5dcb21e049e9?source=search_post---------141,"There are currently no responses for this story.
Be the first to respond.
Today more and more companies are moving their data to the cloud. This way, they cut corners on hardware and protect their sensitive information from internal data theft.
There are three main types of cloud computing which are software as a service (SaaS), platform as a service (PaaS), and infrastructure as a service (IaaS).
In this post, we will discuss the primary difference between these models. These insights will help you choose the right solution for your business.
Cloud computing means that on-demand computing services are delivered over the Internet on a pay-as-you-go basis. Simply put, this model allows storing and accessing data and apps on remote data centers. You no longer need to keep them on your hardware.
The global cloud computing market size is constantly growing. According to MarketsandMarkets, it may reach $623.3 billion by 2023.
The chart below by ZDNet shows forecasts for Saas, PaaS, IaaS revenue.
What creates the demand for cloud computing? Grand View Research defines the following reasons for its popularity:
There are three main cloud computing models: SaaS, PaaS, IaaS.
Let’s move to a detailed description of each delivery option.
SaaS (software-as-a-service) means ready software products that are delivered via the Internet on a subscription basis. If we compare SaaS vs PaaS vs IaaS, the first model is the simplest option to maintain.
Below you can see the growth of SaaS enterprise market size from 2009 to 2019 by Statista:
The most common examples are Google App Engine, Dropbox, JIRA, and others.
As an example of SaaS applications, here’s a screenshot of Jira — a project management web application:
Software-as-a-service is the most suitable option in the following cases:
We have discussed the key benefits of SaaS technology. Now let’s have a look at the downsides of this option:
As it has been mentioned, SaaS solutions offer businesses ready products. PaaS (platform-as-a-service) takes it a step further and provides clients with a cloud environment that allows developing custom apps. Thus, companies no longer have to invest in building and maintaining the infrastructure that their applications require.
What are the most famous examples of this cloud computing model? First off, we should mention Windows Azure, OpenShift, Heroku, and Google App Engine.
Take a look at extensive feature set offered by Heroku, a PaaS cloud platform:
Let’s discuss in what cases platform-as-a-service solutions can bring maximum value:
IaaS (infrastructure-as-a-service) is a self-service that gives users the opportunity to access and monitor hardware. It may include specialized processors, storage space, visualization services.
The examples are Google Compute Engine, DigitalOcean, Amazon Web Services (AWS), and Cisco Metacloud.
One of the first IaaS products that comes to our mind is Amazon Web Services.
IaaS seems the best possible option for:
All mentioned types of cloud computing define how the cloud is used in your company. More specifically, they show how you host, store, manage, and process data online.
Are SaaS vs PaaS vs IaaS very popular? Let’s see what specific figures will tell us:
The table below displays the key features of Saas vs PaaS vs IaaS.
This was an overview of SaaS vs PaaS vs IaaS cloud models. You have seen what business benefits they can bring. The choice of the right solution depends on the size and complexity of your business.
For more information about the above three types of cloud computing, check our full article: SaaS vs PaaS vs IaaS: Choosing the Best Cloud Computing Model.
If you have an idea for a great SaaS application, and need a reliable technical partner to help deliver it, let’s get in touch! Our team is highly experienced in SaaS development, we would love to share our expertise, and help you deliver a successful product.
www.codica.com
www.codica.com
www.codica.com
A Blog about technology, design, and products development…
102 
102 claps
102 
Written by
Software development consultancy. We are passionate about innovations and create great online marketplaces with Ruby on Rails, React, Vue and Angular.
A Blog about technology, design, and products development | Ruby on Rails, React and Vue.js
Written by
Software development consultancy. We are passionate about innovations and create great online marketplaces with Ruby on Rails, React, Vue and Angular.
A Blog about technology, design, and products development | Ruby on Rails, React and Vue.js
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.datadriveninvestor.com/saas-vs-iaas-vs-paas-cloud-computing-models-explained-with-its-benefits-276298aea66d?source=search_post---------142,"There are currently no responses for this story.
Be the first to respond.
Cloud Computing has become a hot topic for business and IT industries because of the advantages it provides to the digitally connected world to drive their business forward.
Cloud Computing has grown exponentially over the years and expected to enhance at an even faster pace. If you as an entrepreneur thinking to switch your business to the cloud, whether it is infrastructure deployment or application, make sure that you have got a clear idea about the differences and benefits of various models of cloud services.
www.datadriveninvestor.com
Cloud computing integrates mainly three models that are:
1. Software as a Service (SaaS)
2. Infrastructure as a Service (IaaS)
3. Platform as a Service (PaaS)
Each of these models delivers varied services and benefits. Thus, before picking any model for your business, ensure to understand the difference between SaaS, PaaS, and IaaS. Here below you can find the intro of each model and what services and benefits they offer.
1.Software as a Service (SaaS):
Software as a Service (SaaS) is a crucial element of the cloud-computing ecosystem. It helps the third party to rent or subscribe to a software application and execute it online, rather than purchasing, installing, and managing it. Today, most of the IT sector and businesses are adopting SaaS applications for fundamental business technologies such as email, CRM, HRM, Sales management, financial management billing, and collaboration. The leading SaaS providers incorporate Salesforce, Oracle, SAP, Intuit, and Microsoft.
Benefits of SaaS:
It reduces the time spent in installing and configuring of applications as well as issues that can be faced in the way of the software deployment.
It is cost-effective as you have to pay only for what you are using and not pay heavily on-used licensing.
The SaaS feature has flexible options as it is easy to change the usage plans according to the needs. It also allows subscribers from any location to access the software easily.
It is compatible with the new upgrades, as the subscribers have to simply log-on to already upgraded services.
2.Infrastructure as a Service (IaaS):
Infrastructure as a Service (IaaS) is an instant computing infrastructure, provisioned, and managed over the internet. Many organizations are investing in IaaS for accessing, monitoring, and managing remote datacenter infrastructures. Instead of purchasing the hardware outright, users can purchase IaaS as per the resource on-demand.
Benefits of IaaS:
It includes the ability to scale the resources up and down quickly as per the need of the customers.
It is affordable and cost-effective as you need to pay only for what you use.
It decreases the other expense and complexity of purchasing and handling your physical servers.
It also offers a consolidated disaster recovery infrastructure, by lowering down the costs and increasing manageability.
3. Platform as a Service (PaaS):
Platform as a Service (PaaS) is a cloud computing model in which the third-party provides users with hardware and software tools. Many small businesses rely on PaaS providers for development kits, database tools, and application management requirements to create and deploy applications. The PaaS providers also handle the infrastructure, which includes network, servers, operating systems, and storage.
Benefits of PaaS:
It assists the app makers to develop and deploy the apps in a simple and affordable way.
It allows the developers to create customized apps without any stress of maintaining the software.
It lowers the burden of developers by reducing the amount of coding.
It helps in streamlining the complete application management by reducing the unnecessary human configuration tasks.
Conclusion:
Every cloud models have various features and functionalities that fit for various industry needs. At GoodFirms, you can reach the cloud computing service providers that are evaluated and listed based on various qualitative and quantitative factors.
You can associate with the top cloud computing companies whether you are searching for cloud-based software to create inventive customized applications, looking for varied storage options or need to handle your entire infrastructure. Choose the best-suited model and migrate to the cloud for better prospects of the business.
empowerment through data, knowledge, and expertise.
69 
69 claps
69 
empowerment through data, knowledge, and expertise. subscribe to DDIntel at https://ddintel.datadriveninvestor.com
Written by
Research Analyst, Content writer at GoodFirms.co
empowerment through data, knowledge, and expertise. subscribe to DDIntel at https://ddintel.datadriveninvestor.com
"
https://medium.com/@urban-institute/bringing-cloud-computing-power-to-microsimulation-a48c1114934b?source=search_post---------143,"Sign in
There are currently no responses for this story.
Be the first to respond.
Data@Urban
Mar 26, 2019·6 min read
The Urban Institute maintains several microsimulation models to help researchers and decisionmakers better understand issues such as the Social Security system, tax policy, and health insurance policy. Microsimulation is a technique that takes a person-level (or other small unit-level) file as input to simulate behavior, such as a policy or government program, and generates output at the input unit-level and provides an overall summary of the results. Because of the level of detail in both the units and in the program applying the rules, microsimulation is a powerful tool that lets researchers explore a myriad of “what-if” scenarios.
The Urban-Brookings Tax Policy Center’s (TPC) microsimulation model uses an input file of representative tax filers and contains detailed code to simulate revenue and distribution estimates of the US federal tax system. It can simulate both current law and alternative policies. Users can modify the tax schedule to add tax brackets, modify tax rates, or change the way the child tax credit is phased out for married units. Running the model with these alternative parameters enables researchers to compare those results with those under current law. This capability allows for a detailed comparison between an alternative scenario and the current law baseline. Researchers can investigate who would be affected by such changes, what revenue effects would be, and so on.
Until recently, researchers have done much of this work manually. They translate alternative policy proposals into values and rules that are specified in an input file — called a parameter file — that defines the various policy options. With this parameter file prepared, the research team can then run the model and analyze the output. The model has been used for more than a decade, but over the past year, Urban’s Technology and Data Science team has joined forces with TPC researchers to move the model into the cloud, helping the model run faster, longer, and with more data.
But what if instead of just running a few alternative scenarios, we ran thousands? What if we didn’t look at just the set of options proposed but other similar options? Are there other scenarios that produce similar summary results but affect the population differently?
That is exactly what we’ve built a cloud-based architecture to do.
With this new capacity, we open up new opportunities to better inform researchers, policymakers, and the public. Instead of thinking about a single model result, we can now explore the output of thousands of different proposed policies. This means we can look at the space of results and discover which policy or types of policies produced a particular set of outcomes. We could eventually cull this field automatically for decisionmakers to produce a small set of the most impactful policies, based on their desired outcomes.
In one such example, researchers used the TPC microsimulation model to explore the workings of the Tax Cuts and Jobs Act (TCJA) with 9,216 model runs. Each model run specified a variation on some of the TCJA’s major provisions to help people understand the current law and the potential effect of alternative provisions and details. This ability allowed the research team to explore the model runs against each other and against the TCJA. They found key insights that would have been otherwise difficult to see. For example, they found that for low-income families with children, the child tax credit has the largest effect on their change in after-tax income. For this group, the other tax parameters we change have a dramatically smaller effect on income (although they do affect the total change in federal revenue). You can read more about this analysis in TPC’s feature, “The TJCA, What Might Have Been” and the corresponding data visualization feature.
With this understanding of the microsimulation model, let me now walk you through how the Tech and Data team built the cloud-based architecture.
Modeling in the cloud saves time and money
We began this journey confident that we could decrease run time of the model in a cost-effective manner. In its new state, the architecture reduces total run time by at least a factor of 65, and operation costs are under $0.02 per model run.
We can set up and run the model over 2,500 times in about four and a half hours. This time includes not just the model run itself but also the setup required to define the parameter file and the synthesis of those model run results — keeping results linked to their parameter options and generating estimates of revenues for each.
Without the ability to run in the cloud, we estimate it would have taken 260.4 hours — almost two weeks — to just set up and run the model 2,500 times. This time still does not account for organizing model output and constructing an analysis file, a task that is hard to estimate or imagine without bringing together many of the components that the cloud provides. Such long run times restrict the research team’s flexibility to respond to different policy proposals and the quick turnaround demands of a rapidly changing policy environment.
Cloud computing isn’t new, but creating a powerful new system was challenging
Using cloud computing to boost and supplement resources is not a new idea — it is, in fact, one of the main draws to running calculations on the cloud — and neither is the idea that we could discover something interesting or important from running a model several hundred or thousand times. High-performance computing has been used for years in government and across academia for physics, astronomy, and engineering. In many of these subject areas, data have always been quite large, whereas in social sciences, smaller data have historically been used, and personal computers and smaller servers have generally been well-suited for the task. Now that resources are more easily accessible in the cloud, we can replace personal computers with more powerful cloud-based compute resources.
As we thought about running a microsimulation model thousands of times, we kept several goals in mind:
· We wanted to make very few code changes to the actual model. The TPC team has a well-established model that is fully functional and produces reliable and accurate results. We want to keep the code easily readable and understandable to those who use it daily.
· We wanted the capacity to handle bigger datasets (scale vertically) and the capacity to handle large amounts of runs (scale horizontally).
· We wanted the process to be cost-effective.
· We wanted to allow for focus of expertise — letting the technical team focus on the technical aspects and letting the analyst team focus on analyzing the results.
Our challenge was to meet each of these objectives while working through an entirely new architecture. Each of the components in our new infrastructure — creating the parameter file that defines run time options, doing the model run, and creating the analysis dataset — was its own unique challenge requiring a technical solution. In a forthcoming post, I’ll walk you through how we resolved these challenges.
Moving forward
With these challenges behind us, we take the first step toward creating a full space of results to explore for optimal solutions. Instead of being reactive to the policy environment, we can proactively test policy levers, assess demographic and economic changes, and explore other options to find mechanisms that may reach similar results via a different path.
In the near future, we imagine using this microsimulation modeling at scale for other models, for alignments, and for creating other spaces to explore. Many models use an alignment process to fine-tune estimates and to ensure that they hit known targets — such as making sure total income in the model matches total income in the actual economy. In many cases, this is a manual process that takes time to set up and run. In the same way we were able to programmatically generate parameter files with small variations for the TPC microsimulation model, we could also begin to programmatically perform alignments and targets. We envision beginning to apply optimization and learning techniques to both explore and create result spaces.
-Jessica Kelly
Want to learn more? Sign-up for the Data@Urban newsletter.
Data@Urban is a place to explore the code, data, products, and processes that bring Urban Institute research to life.
8 
8 
8 
Data@Urban is a place to explore the code, data, products, and processes that bring Urban Institute research to life.
"
https://medium.com/@renatogroffe/cloud-computing-boas-pr%C3%A1ticas-em-aplica%C3%A7%C3%B5es-twelve-factor-app-1d94ebf36db5?source=search_post---------144,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
May 3, 2021·3 min read
Como podemos melhorar a implementação de aplicações na nuvem, tirando proveito de boas práticas e obtendo assim aplicações escaláveis, com alta disponibilidade, uma maior tolerância a falhas e se ainda valendo dos benefícios de automação no build/deployment oferecidos por DevOps?
E já que estamos falando em cloud applications, como tornar mais eficiente o gerenciamento de configurações, pacotes, ambientes e logs, obtendo assim uma maior produtividade a partir de serviços na nuvem?
A metodologia conhecida como The Twelve-Factor App (Doze Fatores) oferece uma série de recomendações úteis para estas questões e necessidades do dia a dia. No final de Março/2021 abordei com meu amigo Robson Rocha de Araújo em uma live no Canal .NET estes 12 fatores, destacando de que maneira soluções como Azure DevOps, GitHub Actions e o Microsoft Azure podem ser úteis na implementação de tais orientações.
A gravação está disponível no YouTube e pode ser assistida gratuitamente:
Ao longo desta apresentação frisamos aspectos como:
Os slides foram disponibilizados no SlideShare:
A seguir estão diversos posts que escrevi, cobrindo o uso de serviços do Azure, Azure DevOps, GitHub Actions, Docker e Kubernetes:
.NET + NuGet: automatizando a publicação de packages com GitHub Actions
Melhorando o gerenciamento de configurações com Azure App Configuration e Key Vault
MasterClass Azure DevOps: saiba como foi, participe gratuitamente e receba um certificado
MasterClass GitHub Actions: saiba como foi, participe gratuitamente e receba um certificado
Microsoft Azure: dicas, truques, conteúdos e eventos gratuitos | vol. 1
10 Serviços do Azure que você precisa conhecer na prática
Aprendendo Cloud Computing na faixa: artigos, vídeos, canais, comunidades…
Aprendendo DevOps na faixa: artigos, vídeos, canais, comunidades…
Docker — Guia de Referência Gratuito
Kubernetes — Guia de Referência Gratuito
GitHub Actions — Guia de Referência Gratuito
Azure DevOps — Guia de Referência Gratuito
The Twelve-Factor App
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
9 
9 
9 
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
"
https://faun.pub/4-cloud-computing-jobs-to-check-out-if-you-want-to-break-into-the-space-1963df24ef20?source=search_post---------145,"There are currently no responses for this story.
Be the first to respond.
Lately, we’ve been thinking about cloud computing jobs and titles we’ve been seeing in the space. One of the great things about talking with ParkMyCloud users is that we get to talk to a variety of different people. That’s right — even though we’re laser-focused on cloud cost optimization, it turns out that can matter to a lot of different people in an organization. (And no wonder, given the size of wasted spend — that hits people’s’ buttons).
You know the cloud computing market is growing. You know that means new employment opportunities, and new niches in which to make yourself valuable. So what cloud computing jobs should you check out?
Cloud Operations. Cloud operations engineers, managers, and similar are the people we speak with most often at ParkMyCloud, and they are typically the cloud infrastructure experts in the organization. This is a great opportunity for sysadmins looking to work in newer technology.
If you’re interested in cloud operations, definitely work on certifications from AWS, Azure, Google, or your cloud provider of choice. Attend meetups and subscribe to industry blogs — the cloud providers innovate at a rapid pace, and the better you keep up with their products and solutions, the more competitive you’ll be.
See also: DevOps, cloud infrastructure, cloud architecture, and IT Operations.
Customer Success, cloud support, or other customer-facing job at a managed service provider (MSP). As we recently discussed, there’s a growing market of small IT providers focusing on hybrid cloud in the managed services space. The opportunities at MSPs aren’t limited to customer success, of course — just in the past week we’ve talked to people with the following titles at MSPs: Cloud Analyst, Cloud Engineer, Cloud Champion/Cloud Optimization Engineer, CTO, and Engagement Architect.
Also consider: pre-sales engineering at one of the many software providers in the cloud space.
Site Reliability Engineer. This title, invented by Google, is used for operations specialists who focus on keeping the lights on and the sites running. Job descriptions in this discipline tend to focus on people and processes rather than around the specific infrastructure or tools.
Cloud Financial Analyst. See also: cloud cost analyst, cloud financial administrator, IT billing analyst, and similar. Cloud computing jobs aren’t just for technical people — there is a growing field that allows experts to adapt financial skills to this hot market. As mentioned above, since the cloud cost problem is only going to grow, IT organizations need professionals in financial roles focused on cloud. Certifications from cloud providers can be a great way to stand out.
As the cloud market continues to grow and change, there will be new cloud computing job opportunities — and it can be difficult to predict what’s coming next. Just a few years ago, it was rare to meet someone running an entire cloud enablement team, but that’s becoming the norm at larger, tech-forward organizations. We also see a trend of companies narrowing in “DevOps” roles to have professionals focused on “CloudOps” specifically — as well as variations such as DevFinOps. And although some people hear “automation” and worry that their jobs will disappear, there will always be a need for someone to keep the automation engines running and optimized. We’ll be here.
Originally published at www.parkmycloud.com on September 21, 2018
Follow us on Twitter 🐦 and Facebook 👥 and join our Facebook Group 💬.
To join our community Slack 🗣️ and read our weekly Faun topics 🗞️, click here⬇
The Must-Read Publication for Creative Developers & DevOps Enthusiasts
5 
5 claps
5 
The Must-Read Publication for Creative Developers & DevOps Enthusiasts. Medium’s largest DevOps publication.
Written by
CEO of ParkMyCloud
The Must-Read Publication for Creative Developers & DevOps Enthusiasts. Medium’s largest DevOps publication.
"
https://medium.com/cota-capital/beyond-the-hype-5g-and-cloud-computing-92e33f58df8?source=search_post---------146,"There are currently no responses for this story.
Be the first to respond.
I have three things to tell you:
Shall we?
5G is the latest “industry” standard for wireless communications, and it stands for 5th Generation. Let’s look at the history for a minute.If you do a rapid math, every 10 years there’s a generational advancement, where connection speeds improve by an order of magnitude (10x) or more, and technical improvements are introduced.
The first analog version, 1G, launched in the busiest city on Earth back then — Tokyo, Japan — in 1979, followed by the digital evolution, 2G, launched in Finland in 1991.In 2001 NTT-DoCoMo (in Japan, again) launched the first commercial 3G, which allowed at least 144 Kbit/s of connection speeds. Big deal back then.
To put things in context for you: the first iPhone was launched in 2007 (I regularly re-watch Steve Jobs doing the launch), but it was still only a 2G device, and didn’t even have GPS!
Although the first 4G was launched by Korea Telecom (now KT) in South Korea as early as 2006, we had to wait until 2009–2010 for large scale deployments in Sweden, Estonia, and then US and EU. Smartphone penetration was still low, compared to today’s 2020 (it’s hard to even remember these days). We now have billions of connected devices (mobile phones, IoT devices), and we need a faster and more reliable mobile network.
That’s where 5G comes into play, in three ways:
There are several interesting applications that will be made possible by 5G — but for that, I’ll point you to the Cota Access event already mentioned.
There are companies and founders everybody knows about, like Zuckerberg, Musk, Bezos. And then there are founders and companies that have been very successful, but whose product is “behind the scenes”, and you have no idea about them, even if you should. Sam Heidari is one of them, and I recently “sat down” (well, virtually) with him in this video podcast. But let me tell you about him first.
Sam Heidari joined Quantenna in 2009 as head of their R&D. Quantenna built a custom chipset to dramatically improve the performance of wireless devices (such as the router you use at home to connect to the internet).
He then became interim CEO in 2011; after several months, the board was so pleased by his work that they decided to keep him. At that point, the company was making roughly 1 million dollars in revenues. Fast forward to 2016, when Quantenna went public and revenues where in the ballpark of 120 million dollars. Sam and the company accomplished that in just 5 years.In 2019 Quantenna was acquired by ON Semiconductor for $1.07B.
I’ve never built a company worth almost a billion dollars. Sam did.
In the video podcast — you can watch it here — , Sam recounts his days at Quantenna, and we speak about technology, 5G and Edge Computing.
It’s a pity that we didn’t even have time to talk about his previous (successful) startups. Perhaps on another podcast down the line.
Finally, I wanted to tell you about this next event that Cota Capital is organizing. Given the current state of affairs in the world, it’s online only, and it’s free, and you can register here. The date is September 17th, 2020, at 10:00am PT (Pacific Time).
The event is called “Beyond the hype: 5G and the future of Cloud Computing”, and it includes four panelists:
Maryam Rofougaran — Co-CEO, COO and Founder of Movandi.comVeerbhan K. — Co-Founder & CEO of Quadric.ioSebastian Dreisch — GTM and BD at AWS WavelengthSanyogita Shamsunder, PhD — VP of Product Innovation at Verizon
I think we stroke a good balance between startups (two) and large companies (two). Please note that both Movandi and Quadric are companies in Cota Capital’s portfolio.
You can register here.
==
While the author of this publication is an Operating Partner with Cota Capital Management, LLC (“Cota Capital”), the views expressed are those of the author alone, and do not necessarily reflect the views of Cota Capital or any of its affiliates. Certain information presented herein has been provided by, or obtained from, third party sources. The author strives to be accurate, but neither the author nor Cota Capital guarantees the accuracy or completeness of any information.
You should not construe any of the information in this publication as investment advice. Cota Capital and the author are not acting as investment advisers or otherwise making any recommendation to invest in any security. Under no circumstances should this publication be construed as an offer soliciting the purchase or sale of any security or interest in any pooled investment vehicle managed by Cota Capiital. This publication is not directed to any investors or potential investors, and does not constitute an offer to sell — or a solicitation of an offer to buy — any securities, and may not be used or relied upon in evaluating the merits of any investment.
The publication may include forward-looking information or predictions about future events, such as technological trends. Such statements are not guarantees of future results and are subject to certain risks, uncertainties and assumptions that are difficult to predict. The information herein will become stale over time. Cota Capital and the author are not obligated to revise or update any statements herein for any reason or to notify you of any such change, revision or update.
On Venture Capital, tech, and human beings
8 

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
8 claps
8 
Written by
Tech, startups and investments. Global life. Italian heart.
A publication by Cota Capital, a multi-stage investment firm based in San Francisco, California
Written by
Tech, startups and investments. Global life. Italian heart.
A publication by Cota Capital, a multi-stage investment firm based in San Francisco, California
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://architecht.io/the-new-challenge-in-cloud-computing-is-standing-out-from-the-crowd-8790119a1ab3?source=search_post---------147,"This is a reprint (more or less) of Friday’s ARCHITECHT Daily newsletter. Sign up here to get it delivered to your inbox every morning.
If I have a major takeaway from the Amazon Web Services re:Invent conference this week (and Google Cloud’s parallel news stream), it’s that we’re witnessing the moment when cloud computing evolves into mainstream, table-stakes enterprise IT. You can see this not only in the sheer size and attendance of the the AWS event, or the Fortune rankings of the companies highlighted as customers, but also in the increasing homogeneity of the AWS, Microsoft Azure and Google Cloud platforms. As competition among those three pick up, there’s a pressure to fill perceived gaps that might make it more difficult to figure out who’s who.
Let’s start with the biggest news of the week, which was not an AWS product announcement at all. Rather, it was Google’s announcement that Diane Bryant — formerly head of Intel’s $17-billion-a-year Data Center Group — has joined the company as Google Cloud COO. (Although the coolest news of the week might have been Amazon’s description of how it built custom ASICs to handle certain tasks, while allowing the CPUs to focus their power on users’ workloads.)
When Google entered the cloud market in earnest, the conventional critique was that it has cutting-edge technology but no experience working with large enterprises. Now, with Bryant and former VMware CEO Diane Greene at the helm, you can’t really make that claim — at least at the leadership level. Google still has some very smart technologists and boundary-pushing technologies, but the company is pretty clearly trying to send a message that it must be taken seriously as an enterprise IT provider on par with AWS and even Microsoft.
If anything, Microsoft is probably the party responsible for pushing its peers in this direction as Azure grew into a viable option. AWS had to — and certainly did — react to this new threat to its market dominance, and Google had no real choice but to do the same if wanted to be taken seriously.
On the flip side, however, it was easy to see Google’s influence in the spate of announcements made by AWS this week. The Google-created Kubernetes technology is obviously responsible for pushing AWS to create its own Kubernetes service for container orchestration, and almost certainly inspired the “serverless” Fargate orchestration service — if only as a way to highlight how developer life could look beyond the complexity of Kubernetes. Microsoft is also a major player in the Kubernetes world.
And artificial intelligence. All the AI services AWS announced this week, from SageMaker (essentially a platform for building, training and optimizing AI models) to the various voice/text/video recognition services, owe their existence to Google. Not just because Google essentially kicked off the current deep-learning-based AI obsession back in 2012–2014, but also because Google Cloud offers its own versions of these things. So does Microsoft.
AWS’s talk about multi-region databases was reminiscent of Google’s Cloud Spanner story and Microsoft’s CosmosDB story. And everyone is now pushing hard around management of IoT devices, as well. AWS announced a deep-learning-powered, developer-focused camera on Wednesday. Google countered with its own on Thursday.
All of this competitive pressure is great for customers and probably will be great for cloud providers in the short run, but in the long run it seems like it will only really be great for AWS. Not only does homogeneity favor the incumbent, but AWS is also doing a really good job pushing the issue and distinguishing itself around serverless computing. That started with Lambda, but now has advanced to its Aurora database service and (I believe) some other places, well.
Competing on the merits of your respective products is a noble strategy, but it’s not always going to be enough. You can’t blanket the world in your cloud platform the same way you can blanket the world in Android phones to put the pressure on Apple. We’ll have to see how Google and Microsoft ultimately distinguish themselves from each other and from AWS in a meaningful and lasting way, while still making sure they’ve got all the must-have boxes checked.
techcrunch.com
aws.amazon.com
blog.mozilla.org
www.theregister.co.uk
architecht.io
www.nextplatform.com
www.technologyreview.com
www.cbinsights.com
www.recode.net
blog.blazingdb.com
thenewstack.io
cloudplatform.googleblog.com
www.datacenterknowledge.com
coreos.com
eng.uber.com
architecht.io
techcrunch.com
www.zdnet.com
svds.com
Enterprise IT interviews and analysis: AI, cloud-native, startups, and more
3 
3 claps
3 
Written by
Founder/editor/writer of ARCHITECHT. Day job is at Pivotal. You might know me from Gigaom - way back in the day, now.
Once a site about next-gen enterprise IT and the people building it; now a place where Derrick Harris occasionally blogs about tech-related things.
Written by
Founder/editor/writer of ARCHITECHT. Day job is at Pivotal. You might know me from Gigaom - way back in the day, now.
Once a site about next-gen enterprise IT and the people building it; now a place where Derrick Harris occasionally blogs about tech-related things.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@deepanshugahlaut/let-your-business-soar-high-with-cloud-computing-ad4ac9456613?source=search_post---------148,"Sign in
There are currently no responses for this story.
Be the first to respond.
Deepanshu Gahlaut
Jun 6, 2015·4 min read
Implementation of cloud computing is gaining impetus day by day and changing the all aspects of businesses, whether large or small. According to Gartner’s top 10 strategic technology trends,
Cloud computing will affect the business decisions through 2015 and in the next three years.
Cloud computing has emerged as the most reliable solution for protecting the data and files from theft, intrusion, virus and phishing attacks, disaster that might occur in the future, and infected data drives. Moreover, this technology renders the businesses free from the worries of accessibility, maintaining regular data backups, hardware maintenance, and daily software updates.
Simply put, Cloud computing is the use of internet to compute i.e. store, manage and process data and getting maximum benefits from the distantly placed specialized servers and related services. The term “Cloud” means a location on Internet where you put all your data files, applications etc.
Cloud computing services can be grouped into the following three categories: PaaS, SaaS and IaaS
The demand of Cloud computing is increasing day by day for both personal as well as business purposes. Following reasons are responsible for the tremendous increase in the demand of this technology:
2. Global Accessibility: Cloud computing enables its users to access the data and information from anywhere on the globe and at any point of time that increases collaboration with your team members. You can access your data anywhere — home, office or even on the move. Thus you no longer remain confined to your office or personal computer. You just need and Internet connectivity to access the data from the Cloud.
3. Cost Reduction: Cloud computing lets you save the CAPEX (Capital Expenditure) and OPEX (Operating Expenditure). With this technology, you save a lot of money as you do not have to spend on purchasing infrastructure hardware and software, and also on managing the team of IT experts. Due to the savings which cloud computing yields in the long run, more and more businesses are shifting towards this technology in order to strengthen their economy.
4. Technical Efficiency: Maintaining the data and servers on-site, is very tough as compared to maintaining the same on the cloud. Cloud computing eases the task of documents and hardware management because it simplifies the application set-ups, provision of virtualized infrastructure and resources. For example, in case of managed VPS, your service provider is solely responsible for server support, maintenance and upgrades.
5. Disaster Recovery: Your crucial data is not affected by the physical conditions of an organization, man made mistakes or natural disaster because the data servers are based in different geographical locations. Cloud computing eliminates chances of expensive service stoppage and let you resume your business operations as they were before the disaster.
Cloud computing has a lot of benefits to offer. Those mentioned above are only a few important ones. In order to enjoy the benefits of this technology, you actually need to adopt this technology to save money and to make your business more flexible.
I write on SEO, content marketing, accounting, latest technologies, and social media.
5 
3
5 claps
5 
3
I write on SEO, content marketing, accounting, latest technologies, and social media.
"
https://architecht.io/3-ish-things-to-read-on-open-source-and-cloud-computing-be4aa5557b28?source=search_post---------149,"This is a reprint (more or less) of Monday’s (abbreviated) ARCHITECHT Daily newsletter. Sign up here to get it delivered to your inbox every morning.
There are a few distinct, yet somehow related, items I want to highlight today. Here they are:
Taken together, I think these items should give us all pause to stop and think about open source business models in the era of the cloud. I read these and see a story of user choice (perhaps too much of it), technological innovation, strong communities and very difficult business considerations. The Facebook licensing stuff isn’t directly related, but it underscores the notion, sometimes overlooked, that open source software doesn’t exist in a world free of cold-hearted business concerns.
MongoDB is probably a fine technology and company, but it’s competing against companies like Oracle and, perhaps more accurately, AWS, Google and Microsoft, that absolutely dwarf it. MongoDB is a 10-year-old company with more than $100 million in annual revenue, and even still I have to imagine that every new every new product release is more or less an all-hands-on-deck situation. Major new changes to pricing for a core product might be an existential decision.
And, yet, the big cloud providers futz with data center architectures, produce highly secure boxes for moving terabytes of data, and switch to by-the-second billing seemingly without batting an eye. They also roll out a lot of new database services, and services in general, on a pretty steady pace. That’s not to say these things are easy or even that they’re always executed perfectly. But they must be done in order to keep up with the Joneses, so they get done.
So when a company like Timescale steps in with a new open source database and takes on companies like MongoDB, it’s a smart decision without, in my opinion, a whole lot of visible upside. It’s certainly easier to compete against MongoDB and its peers than it is to compete against Oracle but, realistically, you’re competing to become a small or maybe medium-sized fish in an enormous pond. Developers today have so many open source database options it’s nigh impossible to keep up. That’s not to mention the continued push by the Oracles, Microsofts and Amazons of the world. And MongoDB.
Still, a sufficiently revolutionary technology paired with a sufficiently revolutionary OSS business model could emerge as a giant. But getting heard above the noise, building a differentiated-enough product, building a self-sustaining community, and then figuring out how to monetize it are all really tough undertakings. This is compounded by the fact that there are other popular open source projects created and managed not by competing vendors, but by large technology companies with strategic interests not directly tied to revenue from that project.
It’s an issue that obviously goes well beyond databases. I firmly believe that open source is the future of enterprise software — especially infrastructure software — but there are days when it’s difficult to see how exactly that future shakes out without some serious reconsideration of what it really means to be a successful company.
blogs.nvidia.com
OK, the cloud thing isn’t anything we didn’t expect (every cloud will probably offer the latest Nvidia gear, as well as alternatives) but the fast new inferencing software deserves a second look. Mostly because while it might be blazing fast on high-end GPUs, the bigger concern for Nvidia is arguably that edge devices will require something much smaller than that.
www.theregister.co.uk
This really isn’t too big of a deal (it’s been done before), but it speaks to the point I made about Nvidia above. The battle for AI hardware dominance probably won’t be fought against traditional CPUs.
architecht.io
techcrunch.com
This is one of those capabilities I keep hearing about from various conferencing startups, probably because it’s a good use case for today’s AI models. If someone can actually nail this — relatively fast and accurately — I would consider it a very big deal.
spectrum.ieee.org
It’s fun to look at. But remember that there’s more to being a doctor than diagnosing stuff, and doctors don’t always have a nice image to go off of.
arxiv.org
This is not consciousness like we generally think about it in humans but, rather, a method for letting AI agents tap into some prior knowledge about a certain situation in order to inform its actions.
www.blog.google
You might have heard that identity and access management is a big deal right now. Google did, too ;-)
www.datacenterknowledge.com
There aren’t a lot of details here, other than than its tech moves networking “from server to edge, increasing data center efficiency.” My gut tells me this is more about IBM targeting edge workloads than it is about making IBM data centers more efficient.
architecht.io
www.theregister.co.uk
Standards are usually a good thing, especially in a connected world with so few of them. But you have to wonder how fog, or edge, networking will play out when certain companies aim to own the whole stack from device to data center.
www.datacenterknowledge.com
There’s lots of networking talk today, so we might as well throw in this, too. It’s actually a seemingly fair analysis on how and why Google rolled out direct connections to its data centers, and how its service stacks up against the more mature AWS version.
If you enjoy the newsletter, please help spread the word via Twitter, or however else you see fit.
If you’re interested in sponsoring the newsletter and/or the ARCHITECHT Show podcast, please drop me a line.
Use Feedly? Get ARCHITECHT via RSS here:
www.zdnet.com
I could be wrong, but this sounds similar in theory to the Data Hub that SAP announced yesterday. At any rate, data tooling, teams and platforms are still disparate, and there’s a concerted effort underway to unify them.
www.oath.com
This obviously would have been a bigger deal several years ago but … if you’re interested in kicking the tires on a new search/ML/ranking platform from Yahoo. It’s not entirely clear how this fits into the company’s existing big data environment (including its massive Hadoop deployment) but Vespa seems like its own thing and is designed for more-modern applications.
techcrunch.com
If you’re a software vendor dealing with lots of data and pretty much every announcement right now is not about machine learning, you’re doing it wrong.
architecht.io
www.bluedata.com
I’ve come across BlueData a few times over the years, but it seems like we should hear about them more. It could be just a lot of marketing, but a new release that hits all the buzzwords — TensforFlow, Docker, Spark, GPUs — seems like something that would pique some folks’ interest.
eagereyes.org
This is emblematic of the state of affairs around data education and appreciation generally, I think. Most people will learn some stats, but actually working with data in a meaningful way beyond that is still something reserved for select folks within select companies.
Enterprise IT interviews and analysis: AI, cloud-native, startups, and more
4 
4 claps
4 
Written by
Founder/editor/writer of ARCHITECHT. Day job is at Pivotal. You might know me from Gigaom - way back in the day, now.
Once a site about next-gen enterprise IT and the people building it; now a place where Derrick Harris occasionally blogs about tech-related things.
Written by
Founder/editor/writer of ARCHITECHT. Day job is at Pivotal. You might know me from Gigaom - way back in the day, now.
Once a site about next-gen enterprise IT and the people building it; now a place where Derrick Harris occasionally blogs about tech-related things.
"
https://medium.com/@jaychapel/future-trends-in-cloud-computing-all-point-to-optimization-6fc3bfbe6db?source=search_post---------150,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Oct 5, 2020·5 min read
Given our focus on public cloud cost control, we here at ParkMyCloud are always trying to understand more about the future trends in cloud computing, specifically the public cloud infrastructure (IaaS) and platform (PaaS) market. Now that public cloud has become ubiquitous, there’s a common theme. While new services and products continue to develop, more and more of them are focusing on not just creating capabilities that were previously lacking — they’re focused on optimizing what already exists.
Before we dive into optimization, let’s take a look at how the cloud market continues to grow in 2020 and beyond. Gartner estimates that $257.9B will be spent on public cloud services in 2020, up 6.3 from 2019 as outlined in the table below:
And according to IDC, almost half of IT spending is cloud-based, “reaching 60% of all IT infrastructure and 60–70% of all software, services and technology spending in 2020.” These projections come mid-2020, showing that even given the disruption this year, between Gartner and IDC, no one expects cloud adoption and spending to slow down any time soon. So what’s driving this growth and what are the future trends in cloud computing we should be on the lookout for in 2020 and beyond?
There is definitely a lot of hype around Blockchain, Quantum Computing, Machine Learning, and AI, as there should be. But at a more basic level, cloud computing is changing businesses in many ways. Whether it is the way they store their data, improvements to agility and go-to-market for faster release of new products and services, or how they develop and operate services remotely in today’s “locked-down world”, cloud computing is benefitting all businesses in every sector. Smart businesses are always looking for the most innovative ways to improve and accomplish their business objectives, i.e., make money.
When it comes to cloud technology, more and more businesses are realizing the benefits that cloud can provide them and are beginning to seek more cloud solutions to conduct their business activities. And obviously, Amazon, Microsoft, Google, Alibaba, IBM, Cisco, VMWare and Oracle plan to capture this spend by providing a dizzying array of IaaS, PaaS, and DaaS offerings to help enterprises build and run their services.
Cloud Automation Tools: as modern IT environments continue to become more diverse and distributed in the pursuit of key business goals, they also bear new challenges for the operation teams responsible for keeping everything running smoothly. The go-to strategy for taming the associated complexity can be summed up in one word — automation.
Automation tools, including some that incorporate AI, are on the rise in 2020. These new automation capabilities, along with comprehensive dashboards that provide a holistic view into multi-cloud operations, will become increasingly important for cloud and IT operations to support the lines of business regardless of where they place their workloads. These tools can help put the right workloads in the right place, manage costs, improve security and governance, and ensure application performance.
Desktop as a service (DaaS): DaaS is expected to have the most significant growth in 2020, increasing 95.4% to $1.2 billion. DaaS offers an inexpensive option for enterprises that are supporting the surge of remote workers due to the global pandemic and their need to securely access enterprise applications from multiple devices and locations.
Multi-Cloud and Hybrid Cloud: Once predicted as the future, the multi- and hybrid cloud world has arrived and will continue to grow. Most enterprises (93 percent) described their strategy as multi-cloud in 2020 according to a Flexera report (up 21% from 2018) and 87% have a hybrid cloud strategy. In addition, 71 percent of public cloud adopters are using 2+ unique cloud environments/platforms. These numbers will only go up in 2021. While this offers plenty of advantages to organizations looking to benefit from different cloud capabilities, using more than one CSP complicates governance, cost optimization, and cloud management further as native CSP tools are not multi-cloud. As cloud computing costs remain a primary concern, it’s crucial for organizations to stay ahead with insight into cloud usage trends to manage spend (and prevent waste) and optimize application performance.
It’s a complex problem, but we do see many organizations adopting a multi-cloud strategy with cost control and governance in mind, as it avoids vendor lock-in and allows flexibility for deploying workloads in the most cost-efficient manner (and at a high level, keeps the cloud providers competitive against each other to continually lower prices).
Growth of Managed Services: The global cloud managed services market is growing rapidly and is expected to reach $116B billion by 2025, growing from $62.4B in 2020 according to a study conducted by Markets and Markets. Enterprises are focusing on their primary business operations, which results in higher cloud managed services adoption. Business services, security services, network services, data center services, and mobility services are major categories in the cloud managed services market. Implementation of these services will help enterprises reduce IT and operations costs and will also enhance productivity of those enterprises.
Managed service providers — the good ones, anyway — are experts in their field and some of the most informed consumers of public cloud. By handing cloud operations off to an outside provider, companies are not only optimizing their own time and human resources — they’re also pushing MSPs to become efficient cloud managers so they can remain competitive and keep costs down for themselves and their customers.
While today, it sometimes seems like we’ve seen the main components of cloud operations and all that’s left to do is optimize them, history tells us that’s not the case. Cloud has been and will continue to be a disruptive force in enterprise IT for years to come as has the Global Pandemic of 2020, and future technology trends in cloud computing will continue to shape the way enterprises leverage public, private and hybrid cloud. Remember: AWS was founded in 2006, the cloud infrastructure revolution is still in early days, and there is plenty more XaaS to be built.
Originally published at www.parkmycloud.com on October 1, 2020.
CEO of ParkMyCloud
See all (317)
11 
11 claps
11 
CEO of ParkMyCloud
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@spencer.york/why-cloud-computing-is-ideal-for-small-businesses-695fe37dae5b?source=search_post---------151,"Sign in
There are currently no responses for this story.
Be the first to respond.
Spencer York
Jan 23, 2019·2 min read
For years, large corporations have taken advantage of cloud computing as a means to expand their businesses. However, up until recently, cloud computing was too expensive and not reliable enough for small business to adopt these same technological standards. As cloud computing becomes more affordable and more reliable, the number of small business migrating to this technology has continued to increase…
"
https://scientya.com/swiss-cloud-computing-provider-ncloud-swiss-ag-presents-cloud-innovation-in-crypto-valley-in-zug-a8fe9c7d23e3?source=search_post---------152,"There are currently no responses for this story.
Be the first to respond.
Zug is not just a beautiful Swiss city with a beautiful lake; Zug has become known worldwide as “Crypto Valley” — the Silicon Valley for cryptocurrencies. Leading technology experts, innovative companies as well as techies and nerds from numerous countries come to Zug to start new exciting projects, introduce new companies in this area, meet interesting people from all over the world or simply get a taste of the scene.
The Crypto Valley Association aims to build the world’s leading blockchain and cryptographic technology ecosystem. In this context, the event “First Tuesday Drinks & Networking together with n’cloud.swiss AG from Seengen and Crypto Explorers from the Silicon Valley” was held on May 1st.
During this event, Founder and Chairman André Matter and Chief Marketing Officer Pascal Dossenbach presented the company n’cloud.swiss AG. In addition to the innovative cloud platform www.ncloud.swiss, the launch of its own digital cryptocurrency named NCU Token was also presented to an international audience of investors, cryptospecialists and interested trade visitors.
For ambitious growth plans in up to 60 countries over the next 7 years, the company, which is still financed independently, is also open to investors and other forms of external financing opportunities. For this reason, the ICO project was launched. The NCU utility token also serves n’cloud.swiss customers as the digital currency for discounted purchase of n’cloud.swiss services and products, but may also be purchased or sold in the foreseeable future at so-called exchanges (stock exchange venues) which is extremely attractive for investors.
n’cloud.swiss AG emerged from an initiative of Netkom IT Services GmbH based in Seengen, Switzerland. Netkom has been successfully active in the IT market in Switzerland and abroad for 17 years, and is also active in various areas, for example as an IT partner for Swiss Golf House. n’cloud.swiss AG is the Swiss cloud response to Amazon AWS, Microsoft Azure, Google Cloud & Co. The innovative cloud platform offers all cloud models from the pure public cloud to the ON-PREM or hybrid cloud with numerous cloud services within one and the same software. The application possibilities are therefore practically unlimited, so that developers, software companies, SMEs, consulting companies up to large international corporations from all branches of industry or services can flexibly cover their cloud needs.
The digital world publication
200 
Subscribe to Scientya's newsletters and never miss out on exciting recent articles. Take a look.
200 claps
200 
Written by
Dynamic and data-driven marketing leader. Author, Mentor & Speaker. Top 50 Global Thought Leader. See www.yahyamohamedmao.com
Scientya is all about creating and sharing knowledge in the digital world. Switzerland’s fastest growing Medium publication  covers various topics including Blockchain, Artificial Intelligence, Cloud Computing, Health Care, Marketing and more.
Written by
Dynamic and data-driven marketing leader. Author, Mentor & Speaker. Top 50 Global Thought Leader. See www.yahyamohamedmao.com
Scientya is all about creating and sharing knowledge in the digital world. Switzerland’s fastest growing Medium publication  covers various topics including Blockchain, Artificial Intelligence, Cloud Computing, Health Care, Marketing and more.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@sidathasiri/cloud-computing-in-simple-3826f0d063c9?source=search_post---------153,"Sign in
There are currently no responses for this story.
Be the first to respond.
Sidath Asiri
Dec 22, 2017·5 min read
Simply “Cloud Computing” is delivering servers, storage, computing power, software, analytics and other such services over the internet.
Companies who offer these services are called “Cloud Providers”. Others can use these services and usually they are charged on the usage.
These are many reasons to use cloud computing these days and we will look into them later.
Cloud computing is similar to how you use electricity at home. There is a separate company which offers electricity and you are using their services. Based on how much you use, they charge you. This is the concept behind cloud computing as well.
Cloud computing is an information technology (IT) paradigm that enables ubiquitous access to shared pools of configurable system resources and higher-level services that can be rapidly provisioned with minimal management effort, often over the Internet. Cloud computing relies on sharing of resources to achieve coherence and economy of scale, similar to a utility — Wikipedia
Even you may not realize, you probably have used cloud services already. If you have used an online service to send an email, edit a document, watch a movie, listen radio, it’s most likely to cloud services power up them behind the scenes.
Cloud computing services can be categorized into 3 main categories.
IaaS — In this type of category infrastructure facilities like storage, computational power, networks, operating systems are provided. Service providers offer various packages and consumers have the freedom to chose the optimal one for the requirement.
PaaS — These services are mainly targeting the software developers to develop applications more easily. For example, they may not need to worry about setting up databases since these platforms support on persisting data and developers only need to configure application accordingly. Not only persistence data, PAAS comes with a bundle of much more services like Messaging, Authentication, Hosting, Analytics, Crash Reporting including everything need for a software developer. “Google Firebase” is a good example for this.
SaaS — In this category, some kind of software is provided as a service. Google doc is an example of this. Like a regular software, these also get updated as well. The only difference is we do not install them on the local machine. Instead, we can use them via a web browser.
We can deploy a cloud in 3 major ways as Public, Private or Hybrid.
Public Cloud — These are managed by third-party companies offering their storage, computational power etc for public use. Anyone can access them by creating an account and managing the services properly.
Private Cloud — These are maintained by companies for their private use. All the required infrastructure can be located in their data centers and they need to explicitly manage their resources.
Hybrid Cloud — Hybrid clouds make use of both public and private clouds by allowing applications and data to move between private and public clouds.
There are a relatively large amount of advantages of cloud computing is available and today most companies try to develop software utilizing as much as cloud services possible.
The main disadvantage of cloud computing is the security and privacy. When we use cloud services our data is at external parties and away from our control. They provide some pretty good security however we can not guarantee the security. If we have extremely important data, we can use private cloud computing with our own security protocols.
Cloud computing has come a long way over the last several years. With cloud computing and the technology behind it there are many potential opportunities and capabilities. There are thousands of possibilities beginning to form and there will incredible products based on clouds in future.
Thanks for reading and hope you enjoyed it
Cheers!
Associate Technical Lead at SyscoLABS
62 
1
62 
62 
1
Associate Technical Lead at SyscoLABS
"
https://medium.com/@Intersog/no-more-an-intellectual-property-destroyer-five-trends-in-open-source-cloud-computing-8887e5d23db9?source=search_post---------154,"Sign in
There are currently no responses for this story.
Be the first to respond.
Intersog
May 11, 2017·3 min read
Over the last few years, open source has grown into an intrinsic part of a developer’s journey. You can say that this phenomenon is being driven by the wisdom and power of the community. Furthermore, the ability to easily integrate a variety of open source solutions also adds to its popularity.
The community as a whole provides APIs to help developers integrate their tools into the system. Gone are the days when open source was labeled as an “intellectual property destroyer” as companies now have incorporated it into their plans to improve growth.
But with all the positive points that it has going for it, there are also some negatives that can have a significant impact on the future of this segment. Here are five open source cloud computing trends that you should be aware of.
Over the last couple of years, containers were not even close to becoming a mass-adoption solution. While they have been more or less perceived as a fad, this point of view seems to be changing.
As containers enable businesses to leverage their resources or highly portable assets to easily move into microservices, adoption levels are starting to rise. This, in turn, also enhances the stability and scalability of the applications at an affordable cost.
Open-source technologies only find acceptance when backed up by a solid foundation. For a while now, two foundations have continued to maintain their leadership:
Apache continues the FOSS tradition by maintaining an active community board and strict licensing practices. With over 180 projects (including CloudStack, Hadoop, and Maven), Apache will continue to be prominent in open source cloud computing.
The Linux Foundation has evolved to not only control Linux but also several other relevant undertakings like the following:
As architectures start to get more standardized, cloud infrastructure is also starting to look similar. This is inevitable as the underlying infrastructure is now less network topology and more service-based. As a result, businesses can focus more on the services that power it rather than the infrastructure itself.
Most businesses leverage open source technology to develop enterprise-class cloud-ready solutions. But with the source code out there for all eyes to see, open source projects are starting to lose the level of trust that they once enjoyed.
As some open source vulnerabilities attract headlines, more security companies will start to identify vulnerabilities in the code more often. Hopefully, this will lead to patches being produced rapidly to counter the negative perception.
At the moment, there aren’t any OpenStack-based public cloud service providers in North America that can challenge AWS. Furthermore, there is also a general sense of disillusionment surrounding the private cloud.
But outside the U.S., the situation is a little different as several companies are basing their development on an OpenStack platform:
While the U.S. public cloud market has more or less settled, the global market is still in a state of flux. OpenStack’s largest single region is China and the largest users are telecommunications companies.
The current trend around the world is for companies that were heavily into public clouds to start moving some parts of the workload in-house. This is done through an OpenStack private cloud because it’s significantly cost-effective.
Chicago-based provider of full-cycle custom software engineering and IT staffing solutions with own R&D Centers in North America and Europe.
6 
1
6 
6 
1
Chicago-based provider of full-cycle custom software engineering and IT staffing solutions with own R&D Centers in North America and Europe.
"
https://scientya.com/cloud-computing-and-its-role-as-the-digitization-tool-par-excellence-e95ac4418a17?source=search_post---------155,"There are currently no responses for this story.
Be the first to respond.
Over the past few years, cloud computing has being characterized by an explosive growth. According to Gartner, public cloud service revenues will grow from around $ 145 billion in 2019 to approximately $ 278 billion in 2021. Considered as the digitization tool par excellence, the cloud presents the possibility of improving one’s business at different levels. The advantages include cost optimization for operating and maintaining data centers, speed and agility, as well as flexibly capacity with no more uncertainty in determining the infrastructure capacity requirements.
Cloud computing is dominating today’s technology landscape. Most new technologies such as Internet of Things, Artificial Intelligence and Machine Learning are distinguished by extraordinary data processing skills and needs. Servers as a crucial part of cloud technology hold the data which these technologies need to access and to develop further skills. The cloud thus occupies the role of a truedigital epicenter defining the technology seismograph’s deflection. However, the cloud has always been accompanied by data privacy concerns. Customizable and real-time encryption by n’cloud.swiss as an example can separate security and network functions. As a result, an extra layer of data protection is added. On the other hand, private cloud solutions can enhance the security of company specific applications by implementing cloud technology within the company’s corporate datacenter. What is important to note is that there is no one-size-fits-all solution for cloud security. Hence, customization of the most effective solutions are the best possible solutions to meet specific needs.
Summa summarum, the cloud’s success is nowadays more and more influenced on the macro level for example with GDPR and CLOUD Act. Governments, regulators, policymakers and standards setting organizations have a big influence especially on standards for cyber security at the micro-level. For the cloud, this means more regulation but also more adoption opportunitiesthrough new customers who had data privacy concerns towards cloud based solutions. In other words, there is no end to the cloud’s explosive growth.
This article is originally my 300 word abstract submission for The LEGALTECH Book (Part 6: CyberSecurity) to have the chance to be an author published by Wiley.
The digital world publication
104 
Subscribe to Scientya's newsletters and never miss out on exciting recent articles. Take a look.
104 claps
104 
Written by
Dynamic and data-driven marketing leader. Author, Mentor & Speaker. Top 50 Global Thought Leader. See www.yahyamohamedmao.com
Scientya is all about creating and sharing knowledge in the digital world. Switzerland’s fastest growing Medium publication  covers various topics including Blockchain, Artificial Intelligence, Cloud Computing, Health Care, Marketing and more.
Written by
Dynamic and data-driven marketing leader. Author, Mentor & Speaker. Top 50 Global Thought Leader. See www.yahyamohamedmao.com
Scientya is all about creating and sharing knowledge in the digital world. Switzerland’s fastest growing Medium publication  covers various topics including Blockchain, Artificial Intelligence, Cloud Computing, Health Care, Marketing and more.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://architecht.io/cloudera-co-founder-on-the-future-of-big-data-ai-iot-and-cloud-computing-bb43a864571b?source=search_post---------156,"Cloudera has been the subject of so much hype and so much speculation during it’s nearly decade-long existence, you’d think the company is much larger and established than it actually is. Maybe that’s what happens when you jump on a technology like Hadoop early and become, really, the flag bearer for the big data movement.
This week’s episode of the ArchiTECHt Show podcast, however, is about the future, not the past. Cloudera co-founder and chief strategy officer Mike Olson discusses where the big data space is heading, driven by advances in fields such as artificial intelligence and machine learning, and by the inevitable migration of enterprise applications to the cloud. Olson covers a wide range of other topics, too, including the recent spate of cyberattacks against Hadoop clusters and his view on recent analyst reports citing slow growth in the Hadoop market (hint: he disagrees with that assessment).
Keep reading for highlights from the interview with Olson, and scroll to the bottom (or click here) for links to listen to the podcast pretty much everywhere else you might want to.
In the news section of the show, host Derrick Harris is joined by special guest Stacey Higginbotham—of StaceyOnIoT, Internet of Things Podcast and Gigaom fame—to talk about how the big Amazon S3 outage on Tuesday was a wakeup call for connected-device makers. Derrick and Stacey also discuss the creepy CloudPets connected teddy bear hack (which was facilitated by an insecure MongoDB deployment) and the state of AI for IoT, including how new optimized chips are set to catapult us into the future.
Here are the highlights from the interview with Olson, but anyone remotely interested in Hadoop, Apache Spark, big data or machine learning will want to listen to the whole thing.
“Think back to the early days, when you and I first started talking about big data. Big data was all the rage and everybody wanted some big data, and it was super, super exciting — but nobody really knew what to do with it once they got it. Over the years, that ecosystem has evolved and there’s a whole bunch of applications we can point to.
“ I feel like with deep learning in particular, even to some extent with machine learning, we’re at that early-phase hype right now. Everybody’s super excited about it, but the question is, pragmatically, ‘What can I do?’
“I’m absolutely confident that these techniques are going to make a huge difference to enterprises, and that early adopters are going to get unfair advantage in their markets because they know more than everybody else does. But there’s still more enthusiasm than practical knowledge available in field.”
architecht.io
“Diagnostic devices in medical settings … genome sequencers and analysis of the genome sequences that they produce … connected vehicles … Pretty much every factory floor is sensored up the wazoo these days, and you’re able to pay attention to capacity and likely failure on those lines. …
[T]he sensorization of everything is now getting going for real; if we talked about ‘big data’ before, we’re going to have to invent a new term, like ‘ginormous data,’ for what’s coming now.
…
“My view is that software and the problems that it solves co-evolve with one another. If we had been spewing data from sensors at this scale 15 years ago, then Kafka would have happened 15 years ago. … You could argue maybe we’re even a little bit behind the curve. That is … we’re seeing more changes to silicon infrastructure, generally, than I’ve seen in my career so far, in the coming 5 or 10 years.”
“If you think about it, back in the day, Microsoft dominated with the Windows platform and was the largest database vendor on Windows with SQL Server. But they also worked very hard to make sure that IBM and Oracle could run their products on Windows. And the reason they did it is that Microsoft, the company, still made money on the platform even if another vendor took the database revenue.
“If we can help enterprise customers run their workloads on Azure or on Amazon, the fact that we’ve got some higher-level product overlap doesn’t mean that that vendor isn’t a good partner for us, or that we’re not a good partner for them. We’re still storage, compute and a whole bunch of service consumption to the platform and driving revenue for everybody.”
architecht.io
“Broadly, hacks happen because people haven’t turned on the security framework that the platforms offer. It’s actually a fair debate to ask whether those things should be enabled by default or not. That is: Should the open source projects, as a matter of principle, require, or at least default to, [having] security turned on? I think those hacks have got the project management committees … in the various projects thinking about that.”
“We pay attention to what [Google and Facebook] are building and running because, basically, they are in the future sending back notes to the rest of us about what’s interesting and what works. I will say, as a 54-year-old database systems guy, that the pace of innovation across the board — on the device and in the data center — is absolutely breathtaking right now. And in my career, I’ve never seen it happen that we haven’t thought of ways to use all the available compute.
“That is to say, you’re going to see awesome stuff running on the phone [for example]. What’s it going to be? I’m not sure.”
architecht.io
Enterprise IT interviews and analysis: AI, cloud-native, startups, and more
5 
5 claps
5 
Written by
Founder/editor/writer of ARCHITECHT. Day job is at Pivotal. You might know me from Gigaom - way back in the day, now.
Once a site about next-gen enterprise IT and the people building it; now a place where Derrick Harris occasionally blogs about tech-related things.
Written by
Founder/editor/writer of ARCHITECHT. Day job is at Pivotal. You might know me from Gigaom - way back in the day, now.
Once a site about next-gen enterprise IT and the people building it; now a place where Derrick Harris occasionally blogs about tech-related things.
"
https://medium.com/@alibaba-cloud/free-trials-the-key-to-successful-cloud-computing-e7ec8000ddc5?source=search_post---------157,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Mar 30, 2018·7 min read
Finding the best cloud computing provider has never been easier, as almost all of them offer free trials and free starter credit. But don’t be tempted to rush in without some forethought.
The free trial has become one of the most popular ways for IT people to learn, research, test and evaluate. For some, it even provides enough base computing power to do real work for a certain period of time with no cost or financial risk at all.
But the ease with which cloud computing is available at low cost has its issues. It liberates and empowers users, and theoretically stimulates innovation, which most organizations want. But it brings with it proliferation, fragmentation, and some very strange expenses claims. Not to mention a frustrating challenge to the integrity and security of in-house systems and some rather worrying internal support tickets. (“Sorry? You want to allow an external service to talk to our LDAP server? Why, exactly?”)
The abundance of free tools and equipment at home also worsens the problem of Shadow IT and server creep — more people using their personal tools and techniques — and credit cards — to get things to happen at work, even to the degree of commissioning entire computer instances to get stuff done. After all, why put in a formal request for a new Customer Relationship Management system (and wait 6 months for it) if you can rent one today for $40 a month, set it up in 30 minutes and hide the cost among the stationery expenses?!
There is a vast range of suppliers for cloud computing services, but the resources from a global supplier make it worth considering over a local provider. They will have the geographic resources to cope with multi-country operations, of course. Those same resources will help them provide better security, reliability and continuity. It gives the organization flexibility in implementing its data hosting policies. But perhaps above all, the experiences that a cloud computing provider gains from being active in one territory means that it is well placed to launch new services and upgrades that have already been deployed and tested in others.
So, having decided to try out a cloud provider, what should you do first? In the case of Alibaba Cloud, for example, a free trial account is quick and easy to set up via the www.alibabacloud.com website. Once done, all new customers automatically receive USD $300 in free credits on their account.
The only prerequisite for receiving the free credit is that you need a verified and valid payment method in place before it gets applied. This can be a credit card, a PayPal account, or both.
The $300 credit is split across two areas: $50 is allocated to Elastic Compute Server (ECS) instances, Server Load Balancer and Elastic IPs. The other $250 is for database, data, media, network, and security services such as ApsaraDB, Object Storage Service, Table Store, Content Delivery Network, Data Transmission Service, E-MapReduce, Anti-DDoS Pro, Mobile Security, Web Application Firewall and ApsaraVideo Live.
All services are available on Subscription or Pay-As-You-Go.
The core product in any cloud trial, with any provider, is access to the base compute service. This will comprise a processor for an amount of time, with RAM, some storage and a pipeline to transfer data through. Vendors will offer a simple mix of these features at differing prices.
For example, as at the end of 2017, $50 would buy one processor with 1GB of RAM, 40GB of disk storage and 530GB of data for up to 60 days. Or perhaps two processors for half the time with a slightly reduced transfer capacity.
Still part of the core product will come with some extras. In the case of Alibaba Cloud, that includes basic Anti-DDoS protection, a must to help protect against cyber-attacks. You may also opt for a service called Server Load Balancer, which manages highs and lows in traffic by automatically distributing application requests across different availability zones. This service is provided for a specified time or volume of traffic and is ideal for maintaining high availability.
With any cloud service, it’s always important to remember that resources purchased with the free credit are not automatically stopped after the credit is expired. They will continue to incur charges to the account unless or until they are manually stopped and released.
Growing the skills, culture and competencies to manage a cloud computing operation within an organization is best performed step-by-step. Each step forward offers new learning, which will inform the next step. Do not bite off too much at one time — but use time to prepare.
As an IT manager, take time to use free trials effectively and sensibly. Some tips include:
· Know how to stop. Before commencing a trial, check the process for terminating an account or deleting the credit card payment method. If there’s a date on which the free trial converts to a paid subscription, put that date in your calendar.
· Work with finance, not at the CFO level but at the Accounts Payable and Procurement level. Have an honest and open meeting with the finance team to let them know what you’re trying to do and the financial options you are encountering. With a bunch of low-value high-volume transactions, credit cards and new expense payments about to hit, you’re going to appear like a flashing beacon on their dashboard, so get in there first as a friend — not later as a troublemaker.
· Embrace FinTech. Get prepaid credit cards complete with online dashboards which allow you to see itemized payments in one place.
· Treat each new trial account or trial server instance as a project in Trello, Asana or similar, which progresses through a limited number of phases to one quantified measurable outcome, with an owner and a countdown to trial expiry.
· Share Learnings. It is vital to find a time and place to compare outcomes, likes and dislikes with other trials or reviews. Even between teams that work in close proximity, there can be material difference in their experiences of performance, configuration, support, control and usability of the cloud services and their progress towards their trial goals.
· Someone on your team is going to need some financial skills in addition to technical ones. Build a shorthand language so that the most financially-oriented person on the team can talk about cloud usage briefly and simply in team meetings. For example, “10 instances, $750 per month spend, below budget, four renewals coming up”.
Ongoing spend, even on a free trial, should be monitored closely through the control panel. This will provide continuous feedback on the best way to balance resources with cost and provide the opportunity to tune the cloud service to the specific need and budget.
Don’t create servers that are more powerful than you need, otherwise you’ll burn through your free credit quickly. Keep track of the extras that you may spend on overruns for a free trial — unexpectedly large storage and big data transmission events can add unexpected costs, almost all of which can be avoided when cost monitoring and tuning become second nature.
Visit the cloud services console each morning. Tie the visits in with the team’s daily routine, ideally before the day kicks off. Your first port of call should be the spending summary dashboard. In the case of Alibaba Cloud, it’s under “Billing Management” and is updated daily, with itemized billing and historic trendlines.
Set threshold alerts so you get timely emails when monthly usage exceeds specified amounts. In the case of Alibaba Cloud the threshold can be as low as $1.
If your cloud provider offers the feature, use the “Refer A Friend” feature and check the dashboard regularly to see if any of your contacts have signed up. In the case of Alibaba Cloud, each referral earns you an additional $20 credit coupon.
Assign someone in your team with the task of downloading CSV usage records each week for each product (the server, load balancing, table store, content delivery, database and others) and spend a short time checking for anomalies. If usage is higher (or indeed lower) than expected, or vastly different from last week, find out why.
Assign a different person to approve payments from the person who fills a basket with services. It can be a hindrance at times, but it’s good practice. Most people are happy for the reduced burden of responsibility they carry, if the role of buyer and payer are split. Those team members will also realize that they are less likely to be blamed if anything goes wrong, as there is a much reduced likelihood that they could have been responsible.
Everyone in an organization wishing to progress with cloud computing should see it as a gentle flow and not a cataclysmic leap. There is no reason for any company not to engage with cloud computing immediately. With the barriers of cost, complexity, control and commitment fast melting, there are absolutely no barriers to taking the first step with cloud computing.
If a cloud project fails, take the opportunity to learn from the experience. Discuss and document the reasons why it failed, and use them as a stepping stone to consider trying again. Merely recognizing that you actually have the option to abandon a project is valuable. Doubly so when the IT cost of that project did not actually cost you or the company any money because it was all done with a free trial!
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
15 
15 
15 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://towardsdatascience.com/5-top-cloud-computing-platforms-with-certification-programs-956c48991738?source=search_post---------158,"Sign in
There are currently no responses for this story.
Be the first to respond.
Behic Guven
Jul 8, 2021·4 min read
Cloud computing has been an essential component of businesses’ digital transformation for many years now. And with the surge in the popularity of remote work in the past year, these services has risen even higher. These cloud computing services are very easy to use and helps us save time and…
"
https://medium.com/silicon-slopes/forbes-just-wrote-an-article-on-utahs-cloud-computing-prowess-626eba8f2ed?source=search_post---------159,"There are currently no responses for this story.
Be the first to respond.
This morning, Forbes tackled the rise of cloud computing and Utah tech: “How Utah’s ‘Silicon Slopes’ Became Cloud’s New Capital.” It’s a lengthy piece that mainly follows the exploits of Josh James (Domo), Aaron Skonnard (Pluralsight), and Ryan Smith (Qualtrics), and ties their experiences into a broader statement on the rise of Utah’s star. At one point, “Silicon Slopes is now for real” is literally said.
Anyway, read it. It’s interesting and also features a random cameo from Silicon Slopes’ own Clint Betts, who somehow weaseled his way into the header image. Good work, Clint. Let’s move on to some of the takeaways.
Then again, the topic at hand is how to attract more diversity to Utah — around Provo, Mormons make up 97% of the population — America’s eighth-whitest state and the third worst when it comes to gender pay disparity. “I’m sick of words and statements on this,” Smith says as the others nod. “Let’s settle on three or four things we agree on that we can actually do at our companies.”
Smith tells the others how Qualtrics ripped up its maternity-leave policy and started over with female employees in charge of it. Skonnard proposes that local tech companies release joint diversity reports. James wants to impose mandatory interviews for diversity hires in certain roles, in a startup version of the NFL’s Rooney Rule.
These are my favorite two paragraphs in the article and I also think every company should hang Smith’s words on a giant plaque over their entrance: “I’m sick of words and statements on this. Let’s settle on three or four things we agree on that we can actually do at our companies.”
Yes, too much talk has already happened, alongside minimal change. To see three of Utah’s entrepreneurial heavyweights taking the diversity problem to heart, brainstorm ideas, and start enacting change is a step in the right direction. Utah companies, join the fray.
At 2:15 a.m., the group releases their scribe, Clint Betts, and Skonnard departs for the hour drive north to Farmington, where Pluralsight is based. Smith and James linger to speak privately until after 6 o’clock. “I don’t think our own employees know how much we talk,” Skonnard says. “They’ll interview for jobs and think we won’t tell each other.”
Wow, okay. I used to think Clint was cool, then I read this and found out he’s a scribe basically held against his will to take notes for entrepreneurs. My image of him is forever altered, poor scribe-boy.
Collaboration helps. You won’t find turf wars or ego-driven battles among Utah cloud companies. In June, Domo sent a delegation to Health Catalyst for four hours, and that’s quite typical. Each niche within the same field is respected by the others. While James is rare in sitting on InsideSales’ board of directors, the whole group maintain endless text threads and even vacation together. “I’m a businessman, but we trust each other,” says Skonnard at Pluralsight. “There’s a shared vision for Utah that unifies us.”
Moral of the paragraph: keep working together, folks.
Like the companies themselves, the pitch often comes down to lower costs: a flat 5% state tax and a cost of living in Provo that’s 30% less than Seattle’s and 41% less than Boston’s — and half that of the Bay Area, despite the fact that salaries are only 27% smaller, on average.
Stats like this always pop up when discussing Utah, but the discrepancy between here and the Bay Area never ceases to amaze. Cost of living in Utah is 50% less, salaries 27% smaller. Seems like a no brainer, imho.
When Utah’s cloud companies eventually go public or get acquired, each founder says he’ll be cheering his peers on, even if that means breaking up the Utah guys’ collaboration. For better or worse, the adversity they faced is a thing of the past. Between Silicon Valley venture firms hunting in Utah and ascendant local shops like Sorenson Capital, which just launched its first venture capital fund, access to cash is much easier now, and talent will continue to trickle in, regardless of outcome. A recent barbecue for local tech workers in Lehi, halfway between Provo and Salt Lake City, drew 5,000 people. The challenge will be to retain Utah’s magic at scale.
Hey, a shoutout to the Silicon Slopes Summer Bash! Not only did it draw 5,000 people, it gave the world this gem:
Classic photo, just a local scribe taking a break for meat cutting duties. Is Clint secretly living in medieval times? Is this what Forbes is trying to tell us? Does he spend weekends dressed up in full-plate armor, riding a mighty stallion? Does he wake to a breakfast of meat and mead? WHAT IS EVEN HAPPENING HERE?
Empowering Utah's tech community to learn, connect, and…
10 
10 claps
10 
Written by
I once was a Bee Boy, now I’m a Slopes Man.
Empowering Utah's tech community to learn, connect, and serve.
Written by
I once was a Bee Boy, now I’m a Slopes Man.
Empowering Utah's tech community to learn, connect, and serve.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@neal-davis/the-best-programming-languages-for-cloud-computing-a016d55fc72?source=search_post---------161,"Sign in
There are currently no responses for this story.
Be the first to respond.
Neal Davis
Nov 11, 2020·6 min read
If you’re embarking on a career in cloud, you may be wondering what the best programming languages for cloud computing are. In this article, you’ll learn about some of the top programming languages that every cloud computing professional should know.
Cloud computing provides a wide range of computing services, including databases, storage, analysis, intelligence, software, networks and many more over the web for enhanced innovation, economies of scale and flexible resources. It comprises of various technologies which have a significant impact on the employability of developers.
The cloud can be exploited in a myriad of ways, including Infrastructure as a Service (IaaS), Platform as a Service (PaaS), Software as a Service (SaaS) and many more. This has been made possible through the use of programming languages.
Cloud computing has, without a doubt, created a new way in which technology resources are used. This has, in turn, come with several benefits, including enhanced security, increased speed, low cost of development and higher productivity.
If you’re working in the cloud or taking an AWS certification training path, you’ll need to choose the best programming languages for cloud computing to stand out. The good news is, these languages are readily available in the modern market. They all differ in community support, capabilities and structure.
To help you make an informed decision, below are some of the best programming languages for cloud computing:
Java is widely known as a general-purpose programming language. Today, it has positioned itself as one of the best programming languages for cloud computing and is used by millions of developers and executed in over 15 billion terminals across the globe. It is no wonder, therefore, that Java finds a spot at the helm of our list.
Java is highly versatile — a feature that makes it one of the few languages that can be used to create applications for websites, desktops, mobile devices and video games. The language is suitable for all programming tasks.
This programming language rides along with an array of benefits, including:
Thanks to its security features, robustness and ease of use, Java is one of the cloud computing programming languages that should be at the front of your list when you choose AWS certification and training.
If you want to realize serverless architecture, you can easily do it using a few programming languages, including Java. It features AOT (ahead-of-time) compilation of various frameworks, which allow you to efficiently address a big distributive size and a long cold start.
All major clouds including Amazon Web Services (AWS), Microsoft Azure and Google Cloud Platform (GCP) offer first-grade support for Java in their SDKs, something that enhances developers’ capacities and simplifies the development process.
PHP is a programming language that is easy to learn and manipulate, which has become increasingly popular in the world of cloud computing. Whether you want to automate websites or perform other functions, this is the ideal programming language for you.
This language has a powerful output buffer and seamlessly runs on windows and UNIX servers. It has an outstanding dynamism, which makes it a fantastic choice when developing applications with dynamic elements.
What makes PHP a darling for most developers in the cloud computing arena is that it can be used with a wide range of database management systems and runs smoothly in various operating systems. Being an object-oriented language, it can help you develop complex and large web applications.
PHP is reliable, safe, fast, and affordable. It’s therefore a cloud computing language you should consider using to fulfil unique development needs.
ASP.NET is one of the best programming languages owned by Microsoft. It is mostly used to develop web applications and websites with multiple functions. One of the reasons it has positioned itself as a fantastic cloud computing language is its ability to provide dynamic web pages and cutting-edge solutions that can be viewed across different browsers.
Beginners will find the ASP.NET framework easy to use. It comes with a host of benefits, including:
This list cannot be complete without mentioning python — a high-level language used by millions of developers across the globe. Despite being a highly respected programming language, it is surprisingly readable and can be used by novices and veterans alike.
Python combines various high-tech features such as speed, productivity, community and open source development, extensive support libraries, third-party modules and more to improve programming. Whether you want to create business applications, games, operating systems, computational and scientific applications, or graphic design and image processing applications, python has got you covered. Learning it increases your chances of landing lucrative gigs and joining the bandwagon of celebrated cloud computing experts.
Python is used extensively in the AWS Cloud and is natively supported by AWS Lambda. This is a great language to use for developing serverless applications on Amazon Web Services.
Golang might not be the programming languages you hear about every day, but it is undoubtedly one of the best in the cloud computing sphere. It is a simple and fast language commonly used for server development. Due to its highly advanced tools for concurrency and parallelism management and its robust framework, it is actively used to develop applications for IoT development, drones and robots.
As a beginner, you are probably looking for a programming language that will provide a perfect blend of functionality, ease of use and reliability. Ruby offers this and much more! It is one of the cloud computing programming languages that come with a wide range of benefits thanks to its vast ecosystem.
When you master Ruby, you will unlock thousands of opportunities in cloud computing since it has many resources for developing a myriad of applications. It has more than 60,000 frameworks and libraries to choose from; hence you can use it to develop any cloud software you can think of. In case you encounter a problem as a beginner, there is an active community of developers to help you out.
If you are looking for a speedy and scalable cloud programming language, you need to consider Node.js. This language is easy to manipulate and is highly effective in the development of end-to-end applications. It features a non-blocking, evented, asynchronous communication pattern that allows applications to handle a huge number of connections. Running on Google JS engine, this language is extremely fast, which makes it a favorite among many modern developers.
Cloud programming has become one of the coolest things in the technology-driven world we live in today. This has led to the development of new programming languages to complement traditional languages and offer fast, reliable, effective and cost-friendly design and execution of various applications.
Before you choose a programming language — especially as a beginner — make sure you perform thorough due diligence so that you select one that will meet your needs and your career goals. At the end of the day, the best programming languages for cloud computing are really just those that support you in reaching your full potential. So, choose wisely and best wishes for your cloud programming career.
Get in touch with us here.
Founder of Digital Cloud Training, IT instructor and Cloud Solutions Architect with 20+ year of IT industry experience. Passionate about empowering his students
5 
5 
5 
Founder of Digital Cloud Training, IT instructor and Cloud Solutions Architect with 20+ year of IT industry experience. Passionate about empowering his students
"
https://medium.com/cuelogic-technologies/saas-paas-iaas-decoding-the-3-cloud-computing-service-models-8f03b6c2be41?source=search_post---------162,"There are currently no responses for this story.
Be the first to respond.
Over the years, this technology paradigm has evolved through multiple phases. The earlier forms of computing that preceded modern cloud computing included grid, utility, and on-demand computing. The earliest forms of modern cloud computing that include Software (SaaS), Platform (PaaS) and Infrastructure (IaaS) emerged as a technological outcome that attended the dipping costs of computer and server hardware. Users could purchase individual servers to power their computing requirements.
The cloud paradigm emerged when software makers and hardware vendors combined multiple servers in a concerted bid to harness the immense computing power generated by a grid (or network) of connected servers. Concurrently, the evolution of digital connectivity technologies that underlie the World Wide Web in recent years formally brought about the modern concept of “cloud computing.” In recent times, the purveyors of technology have parlayed cloud-computing systems into multiple tiers of service, variously labeled as SaaS, PaaS, and IaaS.
(SaaS) is a software licensing and delivery model that has gained a significant presence in a wide range of modern corporate, business, scientific, and commercial applications.
SaaS technologies allow users to license proprietary software on a subscription basis — monthly or annual. As providers of an ‘on-demand’ service, SaaS service providers host the software on the cloud to which users connect through a browser and an Internet connection. As a hugely cost-effective alternative to on-premise software installations and packages, the SaaS model seamlessly delivers a variety of applications that pertain to enterprise resource planning programs, office, and communications software, payroll and accounting packages, human resources management, mobile applications, etc.
This computing paradigm remains vulnerable to unauthorized access and malevolent hacking expeditions in online domains. Digital miscreants have targeted businesses that operate on the cloud by blocking customer access to critical online systems. This poses real risks that can translate into an erosion of market value for SaaS service providers. In response, service providers must continuously invest in improving security and authentication processes on the cloud, thereby delivering incremental assurances to their clients and customers.
Additional challenges that figure in the development of SaaS-powered products and services include custom third-party payment integration, safe and well-defined database access compliant with GDPR norms, guaranteeing zero-downtime deployment,managing the subscription lifecycle, and building a fully customizable SaaS system.
PaaS is “a category of cloud services that provide a platform allowing customers to develop, run, and manage applications without the complexity of building and maintaining the infrastructure typically associated with developing and launching an app.”
PaaS providers host the hardware and software on their infrastructure, thereby releasing customers from any obligation to install in-house hardware and software to develop or run a new application.
PaaS technologies pose particular problems and risks for service providers. These challenges include balancing control, cost, and capacity of a PaaS-based service, providing full multi-tenancy support, designing role-based access controls, creating audit trails, and integrating third-party services into modern PaaS platforms. Additional challenges may emerge in the form of virtualization management, fine-tuning the PaaS compute architecture, designing inter-operability with other cloud services, and creating technically sound fault tolerance parameters.
IaaS operates by traditional cloud architecture. Per the IaaS cloud-computing paradigm, service providers host the infrastructure such as servers, storage units, networking hardware, virtualization or hypervisor layer, etc. This model negates the legacy business case for investing in on-premise data center infrastructure and equipment. Modern IaaS service providers also offer policy-driven services to clients and customers. These services include monitor service performance, detailed billing of customer services, log access, digital security, load balancing, backup, replication, and recovery, etc.
IaaS represents “the virtual delivery of computing resources in the form of hardware, networking, and storage services. It may also include the delivery of operating systems and virtualization technology to manage the resources. Rather than buying and installing the required resources in their own data center, companies rent these resources as needed,” according to popular definitions of IaaS.
A raft of business challenges have emerged to face service providers that offer IaaS services to clients and customers. These may include subscriber expectation management, defining support systems that handle different forms of payments from customers, accommodating the need for custom analytics that measures customer profitability and usage, managing the service value chain, and controlling business with multiple partners. Also, service operators must remain agile in terms of experimenting with product prices, product features, the configuration of service packages, and navigating the intricacies of customer licenses. They must also work to actively manage customer expectations and refine the concept of datacentre ‘in-the-sky.’
The business case for cloud computing technologies and frameworks will continue to burnish its relevance and utility years and decades into the future. The large and incrementally enormous volumes of data generated by modern scientific, commercial, and technological enterprises will require larger data centers powered by innovative technologies. These may form the central planks of local, regional, and national economies in the future. That said, the central role of the cloud may morph into differentiated expressions, marshaled by real-time processing technologies and a deeper engagement with refined versions of civilizational requirements.
Source: Cuelogic Tech Blog
Tech for leaders & developers
5 
Thanks to Kiarash Irandoust. 
5 claps
5 
Written by
Global organizations partner with us to leverage our engineering excellence and product thinking to build Cloud Native & Data-Driven applications.
Tech for leaders & developers
Written by
Global organizations partner with us to leverage our engineering excellence and product thinking to build Cloud Native & Data-Driven applications.
Tech for leaders & developers
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/cota-capital/5g-and-cloud-computing-a-panel-with-aws-verizon-movandi-quadric-b142cffe29a2?source=search_post---------163,"There are currently no responses for this story.
Be the first to respond.
On September 17th, 2020, Cota Capital held a new episode of Cota Access, a recurring webinar where we bring together founders, thought leaders, and leading executives to explore a variety of business and technology trends and topics that fundamentally impact large enterprises. This one was about 5G and Cloud Computing, and featured four great panelists, in addition to the best moderator ever (me!):
The focus was on technology, costs, and applications. We decided to not cover anything related to health concerns, as others already did.
A full recording of the event is available here for you to view, and it lasts for about one hour:
I want to highlight some of the most important points that we discussed, roughly following the same order in the video recording.
If you’re interested in the full conversation, I recommend you scroll back to the top of this post and watch the full video.
Finally, if you want to explore more about 5G and connectivity and hear from a great entrepreneur, check out my recent podcast interview with Sam Heidari, former CEO of Quantenna, which went public in 2016 with more than $120M in revenues (a 100x increase in just 6 years):
I hope you enjoyed this post, and please share your thoughts in the comments.
==
While the author of this publication is an Operating Partner with Cota Capital Management, LLC (“Cota Capital”), the views expressed are those of the author alone, and do not necessarily reflect the views of Cota Capital or any of its affiliates. Certain information presented herein has been provided by, or obtained from, third party sources. The author strives to be accurate, but neither the author nor Cota Capital guarantees the accuracy or completeness of any information.
You should not construe any of the information in this publication as investment advice. Cota Capital and the author are not acting as investment advisers or otherwise making any recommendation to invest in any security. Under no circumstances should this publication be construed as an offer soliciting the purchase or sale of any security or interest in any pooled investment vehicle managed by Cota Capiital. This publication is not directed to any investors or potential investors, and does not constitute an offer to sell — or a solicitation of an offer to buy — any securities, and may not be used or relied upon in evaluating the merits of any investment.
The publication may include forward-looking information or predictions about future events, such as technological trends. Such statements are not guarantees of future results and are subject to certain risks, uncertainties and assumptions that are difficult to predict. The information herein will become stale over time. Cota Capital and the author are not obligated to revise or update any statements herein for any reason or to notify you of any such change, revision or update.
On Venture Capital, tech, and human beings
15 
15 claps
15 
A publication by Cota Capital, a multi-stage investment firm based in San Francisco, California
Written by
Tech, startups and investments. Global life. Italian heart.
A publication by Cota Capital, a multi-stage investment firm based in San Francisco, California
"
https://medium.com/@dsilver829/udacitys-free-intro-to-cloud-computing-course-faf77d3d5c9e?source=search_post---------164,"Sign in
There are currently no responses for this story.
Be the first to respond.
David Silver
Jun 12, 2020·1 min read
Today Udacity launched a free, four-week Intro to Cloud Computing course that I have been working on for the last few months with Ami Malhoof. It’s a great course and if you are new to cloud computing, you should take it!
Ami had a really ambitious vision for this course, and I think it turned out really well. Over the course of five lessons, students:
That’s a lot to accomplish in just a few weeks! And it’s free!
Join us today :-)
I love self-driving cars and I work on them at Cruise! https://getcruise.com
8 
8 
8 
I love self-driving cars and I work on them at Cruise! https://getcruise.com
"
https://cloudrumblings.io/why-learning-to-code-alexa-skills-is-the-gateway-to-a-cloud-computing-job-c6bf795f5685?source=search_post---------165,"There are radical economic shifts underway. Society is moving from commodity-based capital toward intellectual capital. Not only will repetitive manufacturing jobs will be wiped-out — mundane white-collar work will also be eliminated.
The jobs of the future will be those which can’t be done by robots or replaced by artificial intelligence.
So where will these programming jobs exist? For those willing to invest in developing the intellectual capital necessary to compete in the new economy, all trends point to the public cloud.
In 2017, an estimated $122.5 billion will be invested in public cloud services. The cloud segment is growing fast — with spend expected to increase 30% annually to $203 billion by 2020.
continue reading …
cloud adoption and talent transformation | engage → educate…
7 
7 claps
7 
Written by
migrating talent to the cloud at acloud.guru
cloud adoption and talent transformation | engage → educate → evolve
Written by
migrating talent to the cloud at acloud.guru
cloud adoption and talent transformation | engage → educate → evolve
"
https://medium.com/@yahyaibnmohamed/cloud-computing-trifft-auf-%C3%BCber-100-jahre-erfahrung-und-tradition-bb18dcaeae3a?source=search_post---------166,"Sign in
There are currently no responses for this story.
Be the first to respond.
Yahya Mohamed Mao
Nov 2, 2018·4 min read
Die wertvollste Ressource der Welt ist nicht mehr Öl, sondern die Unmengen an Daten, die unser Alltag bestimmen. Unternehmen sind mehr denn je in der Pflicht, gut aufgestellt zu sein und im besten Fall die Richtung vorzugeben, um der Konkurrenz voraus zu sein. Ausser Frage steht dabei, dass die Schaffung unternehmensspezifischer Daten mit ausserordentlichem Zeitaufwand, Qualitätsarbeit und vor allem finanziellen Investitionen verbunden ist. Datenverlust gilt es daher so gut wie möglich zu vermeiden. Obwohl moderne Hardware und Software ziemlich zuverlässig sind, sind schwerwiegende Fehler nie hundertprozentig auszuschliessen. Sogenannte Backups bzw. Sicherungen der Daten sind eine Möglichkeit, Datenverlust zu vermeiden.
Das Landwirtschaftliche Bau- und Architekturbüro (LBA) zeichnet sich durch mehr als 100 Jahren Erfahrung aus. Als Tochterunternehmen des Schweizer Bauernverbandes im Jahre 1916 gegründet, steht das LBA für architektonische Interessenvertretung im ländlichen Raum. Dabei trifft nachhaltiges Bauen aus Tradition auf eine bemerkenswerte Innovationskraft. Heute ist das LBA an mehreren Standorten in der Schweiz vertreten, darunter Brugg, Bern, Zentralschweiz, Trimmis und Ostschweiz. Über die Jahre hat das Unternehmen beachtliche CAD-Datenmengen angehäuft, die es bisher in Servern an den verschiedenen Standorten gesichert hat.
“Wir sind bekanntlich ein Bau- und Architekturbüro. Bei unseren CAD-Datenmengen handelt es sich vor allem um Planungsdaten, die für uns von grosser Bedeutung sind. Unser Hauptaugenmerk lag darin, letztere zentral sichern zu können durch einen starken Partner und nicht wie bisher gehabt an verschiedenen Standorten”, betont Adrian Hitz, Geschäftsführer vom LBA.
Zur Sicherung seiner Daten hat sich das Unternehmen für die Lösung n’cloud Backup vom Schweizer Cloudanbieter n’cloud.swiss AG entschieden. n’cloud Backup macht verschlüsseltes Speichern der Daten im Schweizer Rechenzentrum und/oder lokal beim Kunden möglich. Damit zeichnet sich n’cloud Backup durch Flexibilität und Anpassungsfähigkeit aus. Der Kunde entscheidet ausserdem über Zeitabstände und Dauer der Aufbewahrung. Für das LBA hat das Team von n’cloud.swiss AG einen Server vor Ort installiert, in dem vor allem CAD-Daten on-prem bzw. lokal beim Kunden zur Verfügung gestellt werden. Sämtliche Daten werden zur Langzeitsicherung im n’cloud Datacenter gespeichert und täglich aktualisiert. Zusätzlich zum Backup wurden zahlreiche Post Office Protocol (POP) Konten auf n’cloud Hosted Exchange umgestellt. Während POP, was Funktionen betrifft, sehr eingeschränkt ist und lediglich dazu dient, E-Mails abzurufen, macht n’cloud Hosted Exchange Microsoft Outlook mobil. E-Mails, Termine und Adressen eines Benutzers werden zeitgleich und automatisch auf unterschiedlichen Geräten wie PC, Notebook, Tablet oder Smartphone synchronisiert. Darüber hinaus können Kalendereinträge unter Mitarbeitern geteilt werden, womit grössere Transparenz geschaffen wird und gleichzeitig die Kommunikation untereinander verbessert wird.
“Für ein traditionsreiches Unternehmen wie das Landwirtschaftliche Bau- und Architekturbüro mit n’cloud Backup die Sicherung seiner Daten zu ermöglichen ist was ganz Besonderes. Neben unserem langjährigen Kunden, dem Schweizer Bauernverband, nun auch dessen Tochterunternehmen als Kunden bei uns begrüssen zu dürfen freut uns alle im Unternehmen umso mehr”, so Ralph Mattli, Chief Digital Officer und Mitglied der Geschäftsleitung von n’cloud.swiss AG, der sich über die erfolgreiche Projektumsetzung vor Ort gefreut hat.
Über n’cloud.swiss AG
n’cloud.swiss AG ist ein international agierender und hoch qualifizierter Dienstleister in den Bereichen IT- und Cloudlösungen und zählt zu den führenden Anbietern in verschiedenen Märkten. Das inhabergeführte IT-Unternehmen wurde 2001 gegründet. Seither ist es vor allem im Cloud-Bereich eines der ganz wenigen Unternehmen weltweit, welches alle Cloud-Modelle und Services sowohl Public, On-PREM oder Hybrid als auch «managed», «semi-managed» oder «unmanaged» Support massgeschneidert anbieten kann. Überdies unterstützt das Unternehmen Partner mit seinem Innovation Center in der Implementierung von neuen IT- und Cloud-Technologien mit spannenden Projekten für zahlreiche Geschäftsfelder und Anwendungsfälle. Darunter zählen beispielsweise Projekte aus den Bereichen Blockchain, Machine Learning, Edge Computing, IA, Big Data etc. Sicherheit durch Qualität — Qualität durch Spezialisierung und Innovation. Für n’cloud.swiss AG sind das nicht nur Ziele, sondern gelebte Werte. Für einen wirksamen Schutz und ein Höchstmaß an Sicherheit und Innovation für unsere aktuellen und zukünftigen Kunden im In- und Ausland.
Dynamic and data-driven marketing leader. Author, Mentor & Speaker. Top 50 Global Thought Leader. See www.yahyamohamedmao.com
See all (793)
52 

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
52 claps
52 
Dynamic and data-driven marketing leader. Author, Mentor & Speaker. Top 50 Global Thought Leader. See www.yahyamohamedmao.com
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@rww/a-comprehensive-overview-of-iot-big-data-cloud-computing-f2f5708f6953?source=search_post---------167,"Sign in
There are currently no responses for this story.
Be the first to respond.
ReadWrite
May 3, 2021·8 min read
Today, IoT is the primary source of big data collection. The analysis and processing of this gathered data have given numerous modern analytic solutions. IoT is also the main reason for innovation in the modern world, with more robust information. It has given rise to new business opportunities.
Today, IoT technology has given a new meaning to the world “Smart.” By forming a relationship with other technologies like cloud computing and contributing to them in terms of information, the IoT technology helps new businesses and old ones in terms of growth.
Cloud computing advances processes and data analytics
For example, cloud computing has been advancing processes and data analytics in an economical and scalable manner. Such empowering capabilities are being provided to brands by the generation of Big Data through IoT.
Moreover, IoT is also responsible for generating analytics solutions that, without Cloud, were costly and complicated due to a complex infrastructure or architecture and storage and processing requirements.
Since the IoT was a data-driven technology, it contributed significantly to Big Data, influencing various velocity, accuracy, and reliability domains.
In a nutshell, the IoT is the future of businesses and lifestyles. With the integration of Big Data and Cloud, more innovative digital solutions that consist of better, more advanced analytics and data-oriented decision-making characteristics are becoming increasingly possible. Since IoT is a vast and continuously evolving field, it is getting difficult to understand the technology and its benefits.
So, in this article, we’re going to explore a comprehensive overview of What is IoT? We will explain the relationship holistically between IoT and Big Data and IoT and Cloud. We will also cover some of the advantages of using IoT integrated Big Data and Cloud computing.
IoT or the Internet of Things is generally a network that is interconnected with physical objects. The physical objects in the term “Internet of things” are defined by “things.” These things or physical objects are integrated with IoT sensors, software, and various other technologies, which enable them to exchange data quickly and efficiently by utilizing the Internet.
Additionally, interconnected systems such as digital machines, computing devices, physical objects operate automatically and exchange data quickly over the Internet without a human to human or human-to-machine interactions. This constant change of information also fosters machine learning and information collection.
For example, consider the Internet of things as a person with a sugar monitoring implant or a plan with a malfunction alerting system. In both cases, the system can inform the person or the driver of the potential damage before it occurs.
Every man-made or natural object can be assigned an IP address within the system to enable constant information exchange. This constant interchanging of information or data falls under the framework of IoT.
Moreover, the constant research, implementation, and up-gradation of IoT technology has led us to develop more advanced mechanisms through which data can be processed.
We have created an inexpensive sensor and excessive communication techniques through which billions of devices get connected and share information. Soon the automation will begin, and machines will join human users.
Speaking of the industrial implementation of IoT, several industries adapt to this new innovative technology to bring precision, accuracy, reliability, and efficiency to their processes. The term industry “Industry 4.0” literally refers to the implementation of IoT in industries that bring industrial revolution.
The following industries have adapted to IoT technology:
Global positioning systems (GPS) can be an example of IoT in the automobile industry. Also, there can be several technologies that, when integrated with automobiles, can help in avoiding accidents.
Manufacturing industries around the world are utilizing IoT to streamline and manage their manufacturing process and equipment maintenance.
Smart water pumps, irrigation systems, chemical levels, and pesticide volume monitoring systems can boost production and quality in the agriculture industry.
Integration or lifesaving technologies within the healthcare sector will help improve patients’ quality of life and ensure their health through constant monitoring.
Smart heating systems, automatic door locks, more advanced security features, and intelligent/smart lighting are some examples of the IoT being implemented in the real estate industry.
Moreover, IoT’s cloud connectivity feature enables engineers and developers to work on IoT systems and products, which enable quick and efficient engineering and development.
This process is further influenced by the network of information that IoT gathers. This ability of IoT takes it to the next level. Also, IoT’s disruptive role in giving rise to sustainable technology is worth mentioning.
IoT is constantly exchanging information, which means the technology is continually gathering valuable data and insights through interaction and reaching out to the world.
Also, the information collected by IoT is not limited to the devices owned by companies or organizations. This process includes every personal device, as well.
The relativity and interconnectivity of IoT, Big Data, and Cloud are used by thought-out business leaders/entrepreneurs globally.
These entrepreneurs understand the innovation and competitive edge IoT brings to the table, so benefitting from them has become necessary for businesses.
Moreover, the Internet has become a part of our lives. We want our devices to stay connected to the Internet to stay connected with each other.
This approach gives IoT a strategic advantage because internet connectivity is becoming increasingly important in the modern world. Today, nearly every personally or professionally bought device gets connected to the Internet instantly.
Moreover, digital transformation requires several devices to be connected to the Internet to share data vital for systematic and seamless communication that reduces gaps and improves productivity.
The data transferred is gathered by IoT for analysis through which patterns and inclinations are determined to improve the system’s performance.
Moreover, Cloud computing in IoT acts as a central hub for all IoT-enabled devices. Cloud connects all IoT devices and gathers data from them quickly. The technology stack involved in the process includes an infrastructure, servers, and storage where all the data is stored. Cloud computing also allows users or workers to access data whenever required.
Cisco estimated the IoT to generate 500 zettabytes of data yearly. This massive volume of data will increase in the upcoming years as more devices will connect to the Internet. To give an accurate number of the volume of data collected per year is extremely difficult to comprehend.
The type of devices connected to the Internet will include machines, smartphones, cameras, medical sensors, and smartwatches the list goes on. These devices will gather, analyze, share, and transmit data in real-time.
However, IoT’s primary purpose is to collect data, save, and process it. If the technology is deprived of data, its core functionality and capability will be compromised.
Moreover, according to research by IDC, there will be an estimated 41.6 billion devices connected to IoT that will generate approximately 79.4 zettabytes of data in 2025.
IoT’s data-driven approach is hugely beneficial for the industries, which is why several businesses are opting for implementing IoT in their organizations. However, IoT’s main advantage is its collection of data, precisely where Big Data comes into play. Big Data analytics plays an essential role in IoT since it analyzes the data generated by IoT-enabled devices, which is later used in making more informed decisions.
Another main advantage of Big data in IoT is that it gathers a large amount of information in real-time and saves it for accessing later on using different storage technologies.
Here are the four main steps that IoT Big Data follows to process and analyze the gathered data:
Because there are billions of devices connected to the Internet from which IoT is gathering and saving unprocessed data, super-fast analysis with large queries are required to gain insights from the data stored. Here the need for Big Data in IoT cannot be denied.
IoT and cloud computing are essential to one another because they work together to provide quality-oriented technical services. Both technologies may have different usage from each other. Still, they both are often considered as an integral part of each other and the components that lead to an overall better IoT service.
The relation between IoT and Cloud computing is simple; Cloud computing helps IoT store the data it has gathered. Moreover, cloud computing also gives users the advantage to access data wherever they are. Since the massive amount of data being collected by IoT requires quick saving — Cloud is the best and most efficient option.
Cloud computing is also highly cost-effective compared to other mediums of saving data, and It supports data saving, processing, analyzing, and monitoring in real-time.
The trending trio Big Data, IoT, and Cloud are considered to be an example of a perfect partnership. There are numerous advantages and benefits of utilizing these technologies to improve and reshape an organization’s operational efficiency. Here are some benefits outlined below:
Cloud-based solutions are the best way to meet the needs of Big Data hosting and analytics. You can conveniently expand sever capacity or hardware resources whenever necessary, and you can also expand your big data and data analytics.
Big data and cloud data can conveniently store vast amounts of data in a scalable manner. It also helps in processing and improving data analysis. Also, the lack of physical infrastructure needed to set-up big data, IoT, or Cloud leads to significant cost-effectiveness and no maintenance or support worries.
IoT, big data, and Cloud enable you to be on the edge of processes and access data quickly whenever you need it.
Since too many devices can put a strain on the Internet — these intelligent devices send data to servers for processing instead of sending them to central servers. Additionally, using IoT also gives you the advantage of improved security because data is continuously being synchronized. Any security breach gets detected immediately.
IoT, big data, and Cloud consist of built-in management tools that help you manage your resources, processing operations, and adding value to your business through sufficient storage.
IoT technology has been around us for quite a while now. Constant up-gradation, research, and bonding with new technologies have enabled it to become more improved. According to Gartner’s study, the IoT market is expected to be worth around $4 trillion by 2025.
Also, with the 5g technology becoming increasingly accessible around the world and new and more advanced smartphones and devices popping out each year — it is evident that IoT will continue to grow in the future.
IoT-enabled devices’ industrial implications have also taken a dramatic turn. We can expect unique innovations and possibilities of other usages of IoT to be discovered soon.
As of now, the medical, logistic, real estate, and several other industries are already leveraging the technology to improve efficiency, store important data, and boost growth by analyzing data.
A Comprehensive Overview of IoT, Big Data, Cloud Computing was originally published on ReadWrite by Abeer Raza.
The latest #news, analysis, and conversation on the #InternetOfThings
3 
3 
3 
The latest #news, analysis, and conversation on the #InternetOfThings
"
https://medium.com/nuadox/tony-flath-speaks-on-the-evolution-of-cloud-computing-af587c417fba?source=search_post---------168,"There are currently no responses for this story.
Be the first to respond.
Tony Flath is an Edmonton, Canada-based tech influencer who specializes in various fields including cloud computing, cybersecurity, IoT and artificial intelligence to name a few. I had the opportunity to sit down with him and discuss the evolution of cloud technology which is progressively utilizing advanced AI frameworks.
Tony Flath: Hi Phil, I’m doing great, thanks! I hope that you are doing well too. My professional experience has been an interesting one, from years of sales of complex solutions to 15 years of human resources management practitioner experience with a CHRP designation to 2016. Now working as a telecommunications and technology advisor with specialty in cloud and cybersecurity. As well over the last 5 years I’ve focused my social media efforts as “@TmanSpeaks” across social media where I am recognized as a technology influencer for cloud, cybersecurity, and artificial intelligence.
TF: Years back I worked with Ceridian Canada first as an HR product consultant and implemented complex HR systems with some pre-sales to account executives and solution consultants in the sales organization. I first learned on-premises and then was involved in the first HR SaaS before anyone even knew what cloud was. These years gave me great insight into devOps application development and cloud.
TF: Hell no! I certainly did see that shared infrastructure, platform, and application just made sense and devOps mentality meant you could build and innovate along the way. If anything this made me look at the world of work in this way and is certainly part of the reason I became a cloud disruptor.
TF: There are many clouds now, from public to private, to running on virtualized software “on-prem”. First there was the notion that hybrid cloud was required to manage cloud from one ‘single pane of glass’, one GUI to control all, this at first was a lot of hype without good control. Containers and Kubernetes have been showing great advancement for true multicloud control. Just look at IBM’s pending purchase of Red Hat and their release last year of the world’s first multicloud manager. Other public cloud providers like Amazon and Google have certainly showed more development and services for multicloud. What multicloud offers an organization is the ability to manage many workloads across many platforms with governance and devOps controls providing cost savings, increased controls, & application development enablement.
TF: IoT and edge computing are going to further cement multicloud with fog computing at the edge. IoT with rich Analytics and Big Data will continue to fuel machine learning and deep learning. Where I see huge opportunity is automation of code where artifical intelligence tools will know what data trends mean to customers and auto develop new code to better support application. There has already been evidence that AI neural networks are exceeding human capabilities. 5G will rock IoT with massive app development and at some point desktop and mobile will become one.
TF: AI is already bringing some game to cloud computing. First, let’s look at cloud cybersecurity. If you look at leading edge SIEM solutions like LogRhythm and Splunk, these solutions use “threat intelligence” obtained through machine learning and artificial intelligence to automate and use predictive analytics to detect possible threats as they happen based on patterns and traits known to contribute to cybercrime efforts. Microsoft’s recent announcement of Azure Sentinel, a complete cloud automated SIEM with cybersecurity professionals constantly monitoring, detecting and isolating in literally seconds. Another thing that AI can bring to cloud is better orchestration and control of a mass number of workloads running across clouds — a multicloud that not only enables better management of various cloud platforms but automates and controls those platforms for efficiencies and cost saving. A good example of this is Densify, that offers automated cloud and container workload management which results in significant savings.
TF: First and foremost, you need to have a good reason for why you are looking to cloud AI before just jumping into the “how am I going to do this?”… don’t just throw tech at it! Look at the economic outcomes from both cost savings and solution development enablement and then do your homework by looking at solutions and making sure to look at a proof of concept as many solutions are early days and you want to test and then scale deployment.
This interview was originally published on Nuadox.com on March 29, 2019 as “Tony Flath on the evolution from cloud to AI-enabled multicloud”. The text has been slightly shortened to improve clarity.
Innovation, science and tech news
4 

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
4 claps
4 
Written by
Founder of Nuadox | Tech & Innovation Commentator | Digital Strategist | MTL | More about me> psiarri.xyz
We are a media property producing, curating and aggregating insightful innovation, science and technology news. Based in Montreal, Canada; Nuadox supports collaboration and exchange.
Written by
Founder of Nuadox | Tech & Innovation Commentator | Digital Strategist | MTL | More about me> psiarri.xyz
We are a media property producing, curating and aggregating insightful innovation, science and technology news. Based in Montreal, Canada; Nuadox supports collaboration and exchange.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/dataseries/making-the-most-of-platform-as-a-service-cloud-computing-ce5701b47eba?source=search_post---------169,"There are currently no responses for this story.
Be the first to respond.
Many businesses have significant IT needs that change depending on what the current business focus is or what IT projects are in development. With cloud computing becoming such a major factor in business technology, it’s no wonder that there are cloud solutions available for your…
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@macromoltek/starting-up-with-cloud-computing-d301633bb0f1?source=search_post---------170,"Sign in
There are currently no responses for this story.
Be the first to respond.
Macromoltek, Inc.
Dec 11, 2018·4 min read
Cloud computing has long since left the “buzzword stage” of its life cycle, with most major companies integrating it into their technology and workflow to some degree. Cloud computing is often described through a number of analogies — from a “data lake” capable of storing all the data your firm has collected, to a cluster of virtual computers constantly popping in and out of existence to meet your needs.
Data centers — housing phenomenal numbers of servers — are the core of cloud computing. These servers are either made available to clients directly (known as Infrastructure-as-a-Service, or IaaS), or divided into virtual units pre-loaded with the necessary software for your application (known as Platform-as-a-Service, or PaaS). Many times these servers are specialized for certain tasks; some are capable of storing large volumes of data, others have high-performance processors for demanding workloads, and still others have specialized hardware, such as graphics cards for rendering and GPU computing.
So what is the advantage of using ‘cloud’ servers, and what makes them unique? The centralization of these servers in a single data center reduces the burden of infrastructure costs by taking advantage of economies of scale. These machines can then be rented out for only the time and capacity that the particular client requires. As a company using cloud services, this means that you are only paying for the storage and processing power you need at the moment, leaving you flexible to scale without wasting capital.
An excellent example of a company that leverages cloud resources to its advantage is Slack, the company responsible for the messaging application of the same name, which is used by workplaces across every industry. Soon after its launch, Slack experienced a dramatic surge in usage. Traditional scaling would require not only predicting future computing and data storage needs, but also enormous spending to build the infrastructure to meet those needs in-house. In order to scale rapidly, they instead turned to Amazon Web Services’s public cloud. By utilizing AWS’s compute instances, storage buckets, and relational databases, they had immediate access to a robust infrastructure capable of handling their growth.
The business world is embracing cloud computing, and the sciences are hopping on board. The availability of faster processing and massive storage volume is especially well-suited to scientists’ needs. In many fields — including bioinformatics — experiments previously only dreamed of are now possible. The PredictProtein suite, developed at the Rost Lab in 1992, was once a useful, if limited, sequence analysis and function prediction web service. The near-limitless storage and elastic processing of cloud computing, however, have allowed the service to expand its database and fluidly adapt to demand. Now, it can analyze a query sequence with exceptional speed, returning a wealth of information about the protein — including its secondary structure, solvent accessibility predictions, nuclear localization signals, disulfide bonds, unstructured loops, residue flexibility, interaction sites, family hits, homologous sequences, and more.
Here at Macromoltek, computing requirements vary dramatically at different stages in the project pipeline. At times, we might need a large number of concurrent processors and threads to handle parallel workflows. In other situations, we rely on fast, powerful processors to run our simulations. We take advantage of a variety of different hardware options, but establishing a traditional on-premise system to cover every possibility would be cost-prohibitive and extremely inefficient. Not only would there be the need for expensive infrastructure — and the technical expertise to maintain it — but those assets would also be seriously underutilized when they are not in focus. Mixing cloud services with traditional systems gives us the flexibility to rapidly scale up our capabilities to respond to new challenges.
In addition to our production pipeline, cloud resources are used extensively in Macromoltek’s development process. The plethora of hardware and performance options available in cloud instances lets us test our algorithms in a variety of virtual environments, so we can make informed purchasing decisions for on-premise systems. Furthermore, computationally demanding programs can be moved to the cloud to avoid interrupting the development workflow.
Using cloud resources has become a common strategy for growth-oriented firms and brings exciting new opportunities within reach. The possible future applications of cloud-centered scientific and enterprise computation are boundless. Cloud computing provides scaling flexibility and an enhanced capacity to take advantage of new technologies. This makes cloud computing an invaluable tool for any company and the standard for the future.
Links and Citations1. https://aws.amazon.com/solutions/case-studies/slack/2. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3533974/
Looking for more information about Macromoltek, Inc? Visit our website at www.Macromoltek.comInterested in molecular simulations, biological art, or learning more about molecules? Subscribe to our Twitter and Instagram!
Welcome to the Macromoltek blog! We're an Austin-based biotech firm focused on using computers to further the discovery and design of antibodies.
116 
116 
116 
Welcome to the Macromoltek blog! We're an Austin-based biotech firm focused on using computers to further the discovery and design of antibodies.
"
https://medium.com/@HOSTINGdotcom/calculating-cloud-computing-costs-capex-vs-opex-3345f6dc6f6f?source=search_post---------171,"Sign in
There are currently no responses for this story.
Be the first to respond.
HOSTING
Apr 15, 2015·2 min read
When a company considers moving to the cloud, one of the most discussed topics is how the investment will hit their balance sheet. While calculating cloud computing costs, the terms “CapEx” and “OpEx” are often raised, but both the definitions and the advantages of the two are often. . . shall we say, cloudy? Let’s clear up some confusion.
CapEx vs. OpEx — A Quick Overview
For those of us who shied away from accounting classes in college, here’s a quick overview. Cloud computing costs
CapEx (Capital Expenditure) Owning an asset is commonly considered a capital expenditure. It requires payment for the entire asset and the cost becomes an entry on the company’s balance sheet, depreciated over some period of time. Some examples of capital expenditures include buildings, computer equipment or a company car.OpEx (Operational Expenditures) These expenditures are associated with operating the business over a short period, typically a year. All payments during this year count against the income statement and do not directly affect the balance sheet. Some examples of operating expenditures include sales, utilities and rental car fees.With the explosion of data and mobile apps, companies are challenged to determine what resource requirements are necessary to successfully migrate and operate in the cloud — today and in the future. Many don’t want to pay a flat fee to build and maintain their own data centers since they may need to “spin up” or scale back their virtual machine (VM) capacity based on market demands (for example, during “Black Friday” for retailers or peak travel dates for airlines), short-term testing, or short-lived business initiatives. In those instances, committing a one-time capital expenditure and a multi-year depreciation schedule isn’t cost-effective or desirable.
Bottom-line: companies want to pay only for what they use — period. Cloud service providers allow organizations to adopt a “pay as they go” or OpEx model, which allows companies to engage the resources and expertise of cloud hosting experts, pay for what they use, and realize potential cost benefits throughout the year.
Rather than just selling hardware, speeds and feed, cloud solutions partners need to be adept at anticipating and addressing their customers’ changing resource requirements. AT HOSTING, our enterprise cloud solution architects possess a solid blend of business acumen and superior technical skills. Leveraging proprietary tool sets, they run tests in your environment prior to migration to fully understand your current and future resource requirements. After your cloud migration is complete, they continuously monitor utilization and adjust resource allocations as needed to minimize or even eliminate unnecessary capacity. So you can adjust your resources as demand fluctuates, knowing that you have the right resources at the right time without overspending.
And that looks good on any balance sheet.
We build and operate high performance clouds for business-critical applications. Parent to @Ntirety and @HostMySite.
10 
10 
10 
We build and operate high performance clouds for business-critical applications. Parent to @Ntirety and @HostMySite.
"
https://blog.getunid.io/daily-blockchain-industry-topic-nov-8th-5b8c4a49e74d?source=search_post---------172,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Reviewing and compile blockchain industry topic, updating market references and research environment.
Today, we are picking cloud computing and blockchain deployment in the future access.
cointelegraph.com
iExec has released to conduct Intel Software Guard Extensions（Intel SGX）SDK, first challenges to adapt this technology, privacy and securely based on blockchain cloud computing service.
Intel® SGX is most familiar with enclave solution, empowering cloud computing system without the fear of disclosing sensitive data to a third party. Security management is commonly discussed among the user, enterprises to pave robust infrastructure.
From the Garter report in 2017, Cloud Computing Market Projected To Reach $411B By 2020, and primary invested in the business process service. In accordance with dissemination of cloud computing database, more securely and private protection demands will be surpassing customer benefit in coming years, meanwhile blockchain will be activate on data management function. Plus, public service will be required strong and invulnerable infrastructure, will push upon the level for multiple adapters.
Nest is the big cloud computing player Oracle, announced to launch four new application.
techcrunch.com
Oracles is well-know cloud computing provider and supporting blockchain ecosystem as well. Monitoring supply-chain data management and collecting real time updates by detecting various ornament. Proof of provenance ensure like drug data, and intelligent temperature secure the process, product has been in specific environment while it delivers on the way.
Warranty tracking is ensuring interval dealt properly. These data combination enables to collective database and stronger network. Oracle database ad ERP system is advantage to distribute into the market and accounts on market status.
Lastly, the case of secret and secure message process patent by Cisco.
www.coindesk.com
Chatting is the room of people talking confidentially, and grouping specific participants sending message each other. Cisco patent is working on this behavior with blockchain technology.
This filing describes to develop technology in-and-out, comer to the space for chats. Securing group membership and encrypted context will be promising to all of our smooth delivery.
Thank you for reading daily Collabogate overview and please contact me if you want to be writer of research blogging.
Looking for Collaborator to start working with global network! Please register below the link for business collaboration together.
Make the world’s information trustworthy
59 
59 claps
59 
Written by
Co-founder Privacy by Design Lab
Our mission is to reimagine digital trust. We provide trust edge-to-cloud platform “UNiD” where any developers can easily build secure IoT service, simply.
Written by
Co-founder Privacy by Design Lab
Our mission is to reimagine digital trust. We provide trust edge-to-cloud platform “UNiD” where any developers can easily build secure IoT service, simply.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@richarddolan/advantages-of-cloud-computing-clearer-than-ever-3fa60a531abb?source=search_post---------173,"Sign in
There are currently no responses for this story.
Be the first to respond.
Richard Dolan
May 11, 2016·4 min read
It is somewhat fascinating to think about the fact that, only 10 years ago, cloud computing was not even close to being a priority for the average business or government agency in their IT provisioning strategies. Yet, in the time since, the vast majority of businesses have started to use the cloud for at least one process or function, while many more are beginning to leverage the technology for the entirety of their infrastructure, platform and software needs.
Even when the cloud was gaining traction around 2007 or so, the ways in which decision-makers approached the technology was far different than today. For example, adoption almost always revolved around a question of whether the firm would be leveraging a public cloud service or a private model, which then led to the rise of hybrid options that would divide up processes and functions between the two environments.
In the time since then, especially the past two years, the ideologies associated with the cloud have transformed immensely, and few companies are only looking at the technology through a narrow lens. Instead, multi-cloud and hybrid IT have risen to new levels of adoption and popularity in the public and private sectors, and ushered in a new era of computing power, spend efficiency, and service management customization that will likely persist for years to come.
Concerns regarding security are waning, while decision-makers across industries appear to have a much more comprehensive understanding of cloud technology today than at any other time before. Now, it has become clear that the public cloud is likely to be the fastest-growing segment in the near future, largely thanks to the progressive adjustments providers such as Amazon Web Services and Microsoft Azure have made to their product offerings regarding flexibility.
International Data Corporation forecasted spending on the public cloud to hit around $70 billion in 2015, driven by a wealth of innovation among vendors and developers that range in size and footprint. Of course, the big names are still going to be the largest market share holders for years to come, but smaller ones are contending and, at the very least, forcing their bigger counterparts to refine their own practices and offerings.
According to IDC, innovation and a wider range of options will really tell the story of the public cloud market for years to come, with the result being an expected three-fold increase in services through the end of the decade.
“The technological innovations and enabling capabilities unleashed by cloud have fostered new opportunities across the industries,” IDC Global Technology and Research Group Program Manager Eileen Smith affirmed. “As a result, it is necessary for both technology vendors and buyers to recognize the industry drivers and barriers of cloud deployment, to understand the business transformation brought by cloud, and to act upon the changes that will shape business and technology strategy in the coming years.”
The analysts identified the introduction of streamlined provisioning models as one of the more important and transformative changes that has taken place on an industry-wide basis, as this has effectively put more control into various departmental leaders’ hands in each organization. Remember that, not so long ago, companies were indeed trying to have their IT leaders, such as chief information officers, get more involved in these procedures.
This is not to say that CIOs are not involved, but rather that other types of managers and executives have a bit more authority and decisiveness when selecting various purchases in multi-cloud and hybrid situations.
“We have already seen such platforms and innovation communities in place in retail, financial services, media and other industries,” Smith added. “This will reshape not only how companies operate their IT but also how they compete in their own industry. Technology suppliers will continue to see significant demand for their industry-specific solutions. It is important for these suppliers to develop strong customer-partner relationships with cloud/platform players to seize new opportunities.”
Seeing as security was always the biggest setback for the cloud industry, it is important to understand that the dynamic has changed significantly across industries. InformationWeek has argued that the cloud will actually outperform in-house data centers in many situations when it comes to security, and this is a reasonable statement to make for several reasons.
For example, the source pointed out that cloud specialists, such as managed service providers, as well as the vendors themselves, have access to a wider range of intelligence than the average business would. Additionally, leveraging support services from providers that focus their efforts entirely on cloud maintenance, security and general solution management will tend to be a bit more strategically effective from a defense standpoint than trying to do too much with inexperienced staff members.
From security to spend management and beyond, multi-cloud, highly automated and managed hybrid IT is gaining steam.
Richard is the Senior Vice President of Marketing at Datapipe
See all (297)
3 
3 claps
3 
Richard is the Senior Vice President of Marketing at Datapipe
About
Write
Help
Legal
Get the Medium app
"
https://words.mayte.io/cloud-computing-services-game-changing-but-what-is-it-1b38135e52e4?source=search_post---------174,"We’ll get into cloud computing in a minute, lets back track a bit to understand why I’ve decided to write this post.
On one fine day in Melbourne, the team at Mayte sat down and went through their routine ideation workshop.
The process of bouncing around potential ideas for things to create, getting the creative juices flowing — a necessity when you’re out there putting jet packs on dinosaurs!
Among it’s competition, one idea rose from the crowd and caught the ‘ooohs and ahhs’ from the team. The idea was a hit. So much so that the office was soon filled with confetti and balloons, cake was shared and someone started a Mexican wave.
Unfortunately I wasn’t there but I don’t doubt that’s what happened.
Hi I’m Jonathan.
The idea that transpired a series of celebrations? That’s what I’m working on as my first project as Mayte’s newest member.
It is a project that aims to utilize the power of cloud computing services, a topic which I was previously unfamiliar with and I’m sure by now you’re itching to know what this illustrious idea is.
Well unfortunately that’s going to be the topic of a later post. Here at Mayte we don’t believe in instant gratification, we practice patience and grit!
Fine… I guess a little info wont hurt, the idea is a Smart Office. That’s all I’ll disclose for now!
What I’d like to bring to light is cloud computing services, it is a topic I once had absolutely zero knowledge of. I had heard about the Google Cloud Platform / Amazon Web Services but hadn’t yet looked into it.
I’ve now dabbled into these services and have gained a better understanding of the power of cloud computing services and how big of a role it will have in the ecosystem of future businesses.
I’m here to shed some light on the topic in hopes that you will walk away with an understanding of what it is.
Traditionally, people would run programs or applications on their own physical computer or server to carry out a set task. Cloud computing now offers an alternative.
You’re a local baker who sells cakes. You currently own ten ovens and therefore can only bake ten cakes at a time. Word gets around that you’re the best in town and you soon become bombarded with orders. You simply cannot meet the demand with ten ovens. Turns out there is a man in town who owns 100 ovens. Bakers can use his ovens for a small fee. You only pay for the ovens / time that you use.
You soon realise this is pretty efficient, you no longer have to worry about whether you have sufficient machinery or facilities to support your business. You can focus solely on what you do best, preparing the mix and decorating the cake.
Furthermore, the man offers additional services / benefits. His store comes with state of the art security to make sure your cakes aren’t stolen or messed with. He may also offer a delivery service to your clients.
This is my ‘in a nutshell’ take on what cloud computing services is and hope it’s an analogy that served you well.
Essentially you are outsourcing the facilities, services and the power you need, to a third party who specializes in just that.
Amazon Web Services who is currently the largest provider of cloud computing services, offers an array of products and services.
Compute
Storage
Database
+ much more!
Cloud computing services is a whole new world to me but it is something I definitely plan to grasp. In a future post, I’ll talk more about Mayte’s Smart Office idea as well as how cloud computing services is going to play a role in it!
Stay tuned!
Thoughts, learnings & opinions from the innovation team at…
54 
54 claps
54 
Written by
Developer and self improvement enthusiast.
Thoughts, learnings & opinions from the innovation team at Mayte
Written by
Developer and self improvement enthusiast.
Thoughts, learnings & opinions from the innovation team at Mayte
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@fedakv/how-cloud-computing-helps-save-your-time-and-money-it-outsourcing-company-it-svit-f9259fb8687a?source=search_post---------175,"Sign in
There are currently no responses for this story.
Be the first to respond.
Vladimir Fedak
Dec 7, 2019·9 min read
Cloud computing as a service is around 15 years old, and more than 50% of businesses globally run their IT operations in the cloud, according to the PWC report. However, there still are quite a lot of companies who are afraid of the hurdles of transition to the cloud and keep wasting their resources on outdated processes and technology, thus losing a competitive edge. If this is your case and you want to rectify the situation, we can explain how cloud computing helps you save time and money and how IT Svit can assist you with getting the most out of your cloud investments.
Cloud introduction was nothing short of a revolution. An idea of a common pool of resources combined through virtualization and available to anyone upon request helped reduce CAPEX, OPEX, time-to-market for new products and features, time to resolution for incidents and other risk factors. This allowed nimble startups to successfully compete with and overcome bulky enterprises that are still running outdated infrastructures and are working by outdated workflows. If gaining such a competitive advantage to help your business succeed is your aim — read on!
Everybody knows what a dedicated server is. Whether it stands in your server room or in a remote data center, it is a box of hardware with a disk drive, CPU, some volume of RAM, some Ethernet bandwidth and other chips and some fans for cooling. It has the following limitations:
You buy it yourself or rent the equipment from the data center. This server has to be manually installed and configured. It runs 24/7, even when your team is sleeping, on weekends or on holidays — you still pay for that server. It provides a specific amount of computing resources, and you pay for the package, regardless of how much resources you actually use. This server is located somewhere geographically and can be accessed only through one IP address, even if by several various routes. Most importantly, it can break at any time, so to have one server you should actually have another for backups — and the third one for load balancing — and as these three servers become outdated, you have to replace them. All of these limitations are quite obvious, costly and they hinder business growth a lot.
In March of 2006, a new approach was offered by Amazon, which officially launched its Amazon Web Services, the first cloud computing provider. The company allowed anyone to rent their existing server infrastructure, not for a flat fee, but on a Pay As You Go basis — so you end up paying only for the resources you actually consume, not for the whole package. Most importantly — all of these resources are combined into pools, so-called Availability Zones, which include all the hardware in all the AWS data centers in some geographical location, which are split not into pieces of hardware, but into virtual segments, so-called instances. These instances are flexible and can be configured to meet your unique needs.
You can rent any type of instance you need and the main benefit is that these are SCALABLE. If you suddenly need more resources due to a steep increase in workload — it can be easily done from your dashboard, and a new instance will be operational in minutes, not hours. Below we list 6 main benefits of the cloud, as compared to the dedicated server.
We listed the major limitations of a dedicated server hosting above. Let’s see how cloud computing solves these challenges:
By now you might want to remind us that free cheese can be found only in a mousetrap. Well, there is one condition: to use the cloud efficiently, you must know how to use the cloud efficiently. Otherwise, you might end up with giant bills, as your systems demanded too many resources — and the cloud provided them. To avoid this, you must know how to configure your cloud infrastructure and workflows right. In other words, you need DevOps expertise.
4 years after the AWS cloud release, software engineers came up with the idea of how to align the outdated software delivery culture with updated technology. DevOps is a practical implementation of Agile software development methodology, a culture of automating most of the routine infrastructure operations to streamline and simplify the software development and infrastructure management processes.
DevOps workflows are built on 4 key pillars:
IaC or Infrastructure as Code is an approach to system configuration, where all runtime environment parameters and settings are codified with simple descriptive language in textual files, which are called Terraform and Kubernetes manifests. These manifests are stored in your GitHub repository as any other code and can be adjusted and versioned as easily. Any developer can launch them and provision the required build, testing or staging environment without the need to wait while a system administrator configures it — or keep a test server farm running.
This solves a huge range of problems, from much shorter time-to-market for new products and features, to a significant reduction of bug numbers and ensuring continuity of environments, all the way from an IDE to production. This solves the main issue of software development — “works on my machine”, where the same code operates differently depending on the environment where it runs. With IaC, your code operates exactly the same everywhere, which means much more stable product operations.
CI or Continuous Integration is a DevOps best practice of doing software development in short batches. Instead of developing new features in separate GitHub branches and experiencing multiple merge conflicts while trying to integrate them in the main project, every developer writes the code in small chunks, runs automated unit and integrity tests using manifests to create the required testing environments, and pushes the tested code to the main trunk. This way, you have a fully-operational product that is always at its latest version and can be seamlessly updated. This is much better than waiting for annual releases, yes?
CD or Continuous Delivery is the practice of using certain DevOps tools to create automated workflows (so-called CI/CD pipelines), where multiple operations are executed in turn, using the output of the previous stage as the input of the next stage. For example, a developer can launch 1 script to pass the new batch of code through code commit, build, test and staging environment, where it can be tested by QA specialist and turned into a new in-app release or rolling update to be delivered to end-users without any downtime.
In the cloud infrastructure management field, CI/CD pipelines help automate the management of complex systems, as well as various routine operations like backups, replication, updates, scaling, data mining and processing, Big Data analytics, blockchain operations and much more.
The most important is the collaboration principle, however. All the company understands that the system administrators and Ops engineers are the most important people, as they run the software and underlying infrastructure, so they have a major say in all questions related to that. They are freed from unnecessary managerial control and micromanagement and tasked with a single goal — make sure there is no downtime and software is developed according to schedule.
To ensure such a result, these DevOps engineers must communicate and collaborate with Devs and QA engineers to discuss the best way to run your applications in production and develop new features. This allows them to plan the required system architecture, create the needed Terraform manifests for provisioning new product features and build CI/CD pipelines to automate this process. Then the Devs can use these pipelines to build the code quickly, while the DevOps engineers can concentrate on monitoring and running your production environments to ensure cost-efficiency and reliability of your IT operations.
DevOps culture helps startups and SMBs win the competition with large enterprises, as they can provide flexible services and adjust to the customer demands quickly while utilizing all the benefits of cloud computing. The key challenge here is the lack of skilled DevOps engineers on the market. Many companies try to obtain this expertise themselves, but the way of trial and error might be very costly. This is why many businesses are outsourcing their cloud infrastructure management and optimization to trustworthy DevOps service providers, who can ensure stability and cost-efficiency of operations so that your business can innovate, gain a competitive edge and succeed.
As a Managed Services Provider, IT Svit has ample experience with cloud infrastructure management and optimization, building CI?CD pipelines and providing end-to-end solutions for any business needs of our customers. Should you be interested to work with us — we would be glad to assist!
Originally published at https://itsvit.com on December 7, 2019.
DevOps & Big Data lover
2 
2 
2 
DevOps & Big Data lover
"
https://medium.com/@yahyaibnmohamed/the-cloud-computing-world-is-clearly-lead-by-amazon-aws-ca2779bca387?source=search_post---------176,"Sign in
There are currently no responses for this story.
Be the first to respond.
Yahya Mohamed Mao
May 24, 2018·1 min read
Thomas James
The cloud computing world is clearly lead by Amazon AWS. The three other big cloud providers are American as well. Serious competition outside the “American cloud giants league” comes from Alibabacloud in China. Even without the Cloud Act there are huge opportunities for n’cloud.swiss I would say. As you said Swiss quality, security and trust are clearly competitive advantages. Here’s a features demo https://www.youtube.com/watch?v=xoLm_jPeKfw
Dynamic and data-driven marketing leader. Author, Mentor & Speaker. Top 50 Global Thought Leader. See www.yahyamohamedmao.com
124 
124 
124 
Dynamic and data-driven marketing leader. Author, Mentor & Speaker. Top 50 Global Thought Leader. See www.yahyamohamedmao.com
"
https://medium.com/@drrimmer/the-fight-for-fair-use-in-australia-copyright-law-in-an-age-of-cloud-computing-736e271ff699?source=search_post---------177,"Sign in
There are currently no responses for this story.
Be the first to respond.
Matthew Rimmer
Feb 23, 2015·14 min read
Matthew Rimmer
Fair Use Week has celebrated the evolution and development of the defence of fair use under copyright law in the United States. As Krista Cox noted, ‘As a flexible doctrine, fair use can adapt to evolving technologies and new situations that may arise, and its long history demonstrates its importance in promoting access to information, future innovation, and creativity.’ While the defence of fair use has flourished in the United States, the adoption of the defence of fair use in other jurisdictions has often been stymied. Professor Peter Jaszi has reflected: ‘We can only wonder (with some bemusement) why some of our most important foreign competitors, like the European Union, haven’t figured out that fair use is, to a great extent, the “secret sauce” of U.S. cultural competitiveness.’ Jurisdictions such as Australia have been at a dismal disadvantage, because they lack the freedoms and flexibilities of the defence of fair use.
1. The Australian Law Reform Commission
There has been much angst that Australia has adopted features of United States copyright law — such as aspects of the Sonny Bono Copyright Term Extension Act 1998 (US) and the Digital Millennium Copyright Act 1998 (US) — without the countervailing benefits of a flexible defence of fair use. As Adam Turnerlamented, ‘Why did we gain the restrictions of US copyright law but not the rights?’
As it stands, Australian copyright law does not provide for a general defence of fair use. Instead, Australian copyright law has purpose-specific defences of fair dealing for criticism and review, research and study, reporting the news, use in judicial proceedings, and parody and satire.
In February 2014, the Australian Law Reform Commission led by Professor Jill McKeough released its groundbreaking report on Copyright and the Digital Economy. The two-year-long law reform project was an independent, fair-minded piece of research, showing wide community consultation and industrious research into the case law and the literature on the topic. The report recommended a number of simplifications and revisions to the Australian copyright regime, so that it would be better suited for an age of broadband and cloud computing.
The report recommended that ‘The Copyright Act 1968 (Cth) should provide an exception for fair use.’ The Commission emphasized:
Fair use also facilitates the public interest in accessing material, encouraging new productive uses, and stimulating competition and innovation. Fair use can be applied to a greater range of new technologies and uses than Australia’s existing exceptions. A technology-neutral open standard such as fair use has the agility to respond to future and unanticipated technologies and business and consumer practices. With fair use, businesses and consumers will develop an understanding of what sort of uses are fair and therefore permissible, and will not need to wait for the legislature to determine the appropriate scope of copyright exceptions.
The Commission suggested that the report would make Australia attractive to entrepreneurs, inventors, and start-up companies working in the field of information technology: ‘Of course, innovation depends on much more than copyright law, but fair use would make Australia a more attractive market for technology investment and innovation.’ In particular, a defence of fair use would be of benefit and assistance to search engines, social networks, cloud computing, and 3D printing.
The Commission stressed: ‘Fair use promotes what have been called ‘transformative’ uses — using copyright material for a different purpose than the use for which the material was created. This is a powerful and flexible feature of fair use’. The Commission noted that the defence ‘can allow the unlicensed use of copyright material for such purposes as criticism and review, parody and satire, reporting the news and quotation.’ The Commission recognised: ‘Many of these uses not only have public benefits, but they generally do not harm rights holders’ markets, and sometimes even enlarge them’. The Kookaburra case [PDF] has highlighted limitations of current Australia copyright law — where Men at Work’s quotation of a Girl Guides song was considered to be a copyright infringement.
Moreover, the Commission observed that the defence of fair use would also address a range of other copyright uses: ‘Fair use is also an appropriate tool to assess whether other transformative uses should be permitted without a licence, such as data mining and text mining, caching, indexing and other technical functions, access for people with disability, and a range of other innovative uses.’ The Commission stressed: ‘Copyright must leave ‘breathing room’ for new materials and productive uses that make use of other copyright material.’
In an age of Mickey Mouse copyright term extensions, the defence of fair use would be particularly helpful in dealing with the problem of orphan works — where the owner is lost or cannot be located. The Australian Law Reform Commission ‘considers reforms that would facilitate the use of orphan works to enable their beneficial uses to be captured in the digital economy, without creating harm to the copyright holder.’
The issue of disability discrimination has been a pernicious problem in Australian copyright law. Professor Ron McCallum from Sydney Law School has eloquently discussed the difficulties of access to cultural works for those with disabilities in Australia:
The Australian Law Reform Commission recommended ‘that access for people with disability should be an illustrative purpose listed in the fair use exception.’ Such a proposal is to be welcomed, particularly in light of the new World Intellectual Property Organization Marrakesh Treaty to Facilitate Access to Published Works for Persons who are Blind, Visually Impaired, or otherwise Print Disabled 2013.
The Australian Law Reform Commission also counselled against confusing or conflating fair use with copyright infringement: ‘Piracy will be no less criminal if fair use is enacted.’ The Commission commented: ‘If a person is prepared to infringe copyright laws by illegally sharing films with strangers over peer-to-peer networks, that person will presumably have little regard to laws that prohibit digital-to-digital copying of films for purely private use.’ The Commission concluded that their proposals would not undermine the rights of copyright owners.
2. The Coalition Government
The Attorney-General of the Coalition Government — George Brandis — has been hostile to the proposals of the Australian Law Reform Commission with respect to copyright exceptions. Tabling the report, the Attorney-General observed: ‘These recommendations will no doubt be controversial and the Government will give them very careful consideration.’ He stressed: ‘We are particularly concerned to ensure that no prejudice is caused to the interests of rights holders and creators, whether the proposed fair use exception offers genuine advantages over the existing fair dealing provisions and that any changes maintain and, where possible, increase incentives to Australia’s creative content producers.’ Brandis maintained that ‘those who create the great Australian films, the great Australian television dramas, the great Australian albums, depend upon robust intellectual property laws to protect their creative endeavours’. He feared: ‘Without strong, robust copyright laws, they are at risk of being cheated of the fair compensation for their creativity, which is their due’. Brandis took a copyright maximalist position in the debate over copyright law reform: ‘As I know from my many discussions with members of the industry, they are looking to the Government to ensure that their interests are protected, and this, the Government will do.’
In a speech at the National Library of Australia [PDF], the Attorney-General commented: ‘I remain to be persuaded that [the adoption of a fair use defence] is the best direction for Australian law, but nevertheless I will bring an open and inquiring mind to the debate.’ He signalled his dissatisfaction with the High Court of Australia decision in Roadshow v. iiNet. Brandis instead voiced his enthusiasm for stronger copyright enforcement measures, such as graduated response schemes and three-strikes policies for Internet users. Citing Baz Luhrmann’s film The Great Gatsby, Brandis commented:
The Great Gatsby, Australia’s most successful film at the local box office last year, is now centre stage after its haul of 13 AACTA Awards and an Oscar nomination. Unfortunately the success achieved by The Great Gatsby can lead to piracy of the film, placing the sustainability of our screen industry at risk.
One area for potential reform of this problem may be section 101 of the Copyright Act. This provision provides that an entity which authorises the infringement of copyright without the copyright owner’s permission is liable for that infringement.
He maintained: ‘The government will be considering possible mechanisms to provide a legal incentive for an internet service provider to co-operate with copyright owners in preventing infringement on their systems and networks.’ Brandis commented: ‘This may include looking carefully at the merits of a scheme whereby ISPs are required to issue graduated warnings to consumers who are using websites to facilitate piracy.’ Such an approach is controversial — given the dismal history of graduated response schemes in jurisdictions like New Zealand.
The Coalition Government has also supported the adoption of the Trans-Pacific Partnership. The intellectual property chapter of the Trans-Pacific Partnership would provide for longer and stronger protection of copyright in Australia — and also limit the range of copyright exceptions permissible for participating countries. The investment chapter of the Trans-Pacific Partnership would allow for copyright owners to challenge government reforms in respect of copyright law, IT pricing, and e-commerce.
The commentator, Stilgherrian, wondered whether the proposals of the Attorney-General were well-adapted to the digital age. He lamented: ‘Apart from some results from the so-called iiTrial (which he rejects) and the aforementioned ALRC report (which he rejects), Brandis’ speech bases most of its understanding of modern, digital copyright law on the words of Lord Thomas Macaulay and Charles Dickens — that is, from 1841 and 1842, respectively.’
The technology journalist Josh Taylor perceptively noted that Roadshow– the owners of The Great Gatsby — had made generous donations to both the Coalition Government and the opposition, the Australian Labor Party: ‘An analysis by ZDNet of the annual donor returns listed on the Australian Electoral Commission (AEC) of reported donations to the political parties shows that since 1998, Village Roadshow has donated close to AU$4 million in total to the Labor and Liberal parties both federally and in the state branches.’ This investigative piece of journalism raises concerns as to whether Roadshow is seeking to rewrite Australian copyright law.
Economist Peter Martin emphasized that simplicity and fairness will aid innovation. He pleaded with the Attorney-General to develop a flexible defence of fair use for Australia: ‘As a deregulationist [Brandis] knows that simple rules are often the best. It’s time for simple rules.’
Dr. Nicolas Suzor has noted that consumers should not be treated like pirates: ‘The most important thing fair use does is help distinguish “piracy” from what ordinary consumers and creators do all the time.’
Angela Daly has questioned the merits of a three-strikes system in Australian copyright law.
3. The Australian Labor Party
In response, Jason Clare MP, the Shadow Minister for Communicationsfor the Australian Labor Party, noted that the Australian Law Reform Commission had made a number of compelling arguments. He noted: ‘It’s the countries that best adapt to digital disruption that will be the most innovative, most productive, the wealthiest and the most successful.’ He observed that a trip to the Silicon Valley had ‘opened my eyes to the need to look at this very seriously and make sure out laws are up to date with the needs to create a vibrant digital economy.’
Ed Husic MP, the Member for Chifley, has been an advocate for copyright law reform, particularly in respect of consumer rights and IT pricing. He was critical of the proposals of the Attorney-General.
The recommendations are going to be facing an uphill battle. It looks like the shutters are pretty much being drawn down. Copyright is being used as a form of quasi-protectionism. The way that it is being applied is designed to maintain revenue as opposed to encouraging innovation. We’ve heard a lot about piracy today. Clearly pirates have had a very emotionally scarring experience on Coalition ministers — they dedicated a lot of time to it today.
Husic has championed the adoption of recommendations of the IT Pricing Inquiry — including the adoption of a defence of fair use.
Tim Watts MP, the member for Gellibrand in the House of Representatives, called upon his party, the Australian Labor Party, to be ‘the Promethean party — the bearers of the fires of political change.’ He lamented in his first speech that ‘Australian copyright law, in which all reproduction is prohibited — other than specific, narrow exceptions — is particularly problematic and is currently throwing sand in the gears of digital innovation in this country.’ Watts maintained: ‘In the absence of a broadbased fair-use exception, innovations like the Google search engine and the iPod were legally problematic under Australian law upon introduction — chilling incentives for digital innovation in this country’. He also highlighted that maker communities and 3D printing may be the subject of litigation by intellectual property trolls. In February 2014, Tim Watts MP commented on the Australian Law Reform Commission report on Copyright and the Digital Economy: ‘For copyright reform advocates such as myself, the report is a landmark moment in the journey towards a copyright law that will help, not hinder, Australia’s digital economy’. He supported the adoption of a defence of fair use in Australia:
Watts emphasized: ‘A copyright regime that permits innovation is required to attract the companies and communities that will make Australia a leader in the digital century ahead.’ He stressed: ‘Many online communities often transform other’s copyrighted work by adding new uses for data or by creating completely new artistic works through what US academic Lawrence Lessig calls “remix culture”.’ Watts was of the view that ‘Such an active relationship between content creators and their audiences should be celebrated, not punished, so long as these new uses are not unfair, considering a range of explicit considerations.’
It remains to be seen what the Australian Labor Party’s overall stance will be on copyright law, fair use, and the digital economy.
4. The Australian Greens
For the Australian Greens, Senator Scott Ludlam has been a staunch advocate of the introduction of a defence of fair use into the Australian Parliament. He was the sponsor of a bill on the subject — the Copyright Legislation Amendment (Fair Go for Fair Use) Bill 2013 (Cth). Ludlam argued that ‘Australian copyright law is out of date, inflexible, unnecessarily complex, imbalanced and virtually blind to digital communication technology such as smartphones used by three out of four Australian adults.’ He noted that 2006 reforms to the copyright act had been outdated: ‘While the law caught up with the video age eventually, advances in technology have served to make our laws nonsensical once again.’
Ludlam has advocated the adoption of a United States model of a defence of fair use:
A Fair Use reform would shift Australian law to the US model. Such a technically neutral doctrine would allow the law to respond to developments in technology, with the acceptability of new uses of content and technology determined when a dispute arises. In the Australian system, every new use or technology is forbidden until Parliament gets around to saying otherwise. Under the fair use model, decisions are not made on specific technology through legislation but on the nature and market effect of use of copyright works. A Fair Use doctrine allows people developing new technologies or those who are reproducing and transforming culture to make an assessment about whether their use is fair, and, if they are challenged, they have to defend their use or negotiate terms with the copyright holder. The alternative is a less flexible rule-based system where people with existing lobbying power may have an undue advantage in achieving new exceptions.
For the Australian Greens, Ludlam concluded that ‘Copyright reform is needed to remove discriminatory barriers that impact the visually impaired or force Australians to pay more for no good reason, to protect our learning and cultural institutions and provide fair rules, fair process and fair opportunities to defend use of copyrighted material.’ He noted: ‘Australian laws cannot continue to migrate assumptions about copyright from the printed or analogue age which is rapidly passing as we enter the digital age.’
Senator Ludlam has also been suspicious of the Trans-Pacific Partnership, with its proposals with respect to intellectual property and investment.
5. Civil Society Responses
The consumer rights’ group CHOICE Australia has promoted the adoption of a defence of fair use. ‘If you set out to design a law that consumers would inevitably and unknowingly break, in their millions, every day, the Australian Copyright Act would be what you would end up with’ said CHOICE CEO Alan Kirkland. ‘Despite being updated in 2006, our current copyright law fails to even address basic technologies like DVDs, let alone emerging areas such as cloud computing.’ CHOICE Australia has led a petition against the adoption of a three-strikes response to copyright infringement.
Electronic Frontiers Australia called for the prompt introduction of a defence of fair use under copyright law. Chair, Dr. Sean Rintel, commented: ‘EFA believes that the introduction of a broad fair use exception into Australian copyright law is a critical and long-overdue element in providing a strong, relevant and flexible copyright regime that will serve Australia well into the future. A broad fair use exception will enable greater innovation and creativity, will promote a higher degree of respect for copyright among Australian consumers and will remove a number of significant impediments to the development of a vibrant and competitive Australian cloud services industry.’
The Electronic Frontier Foundation observed: ‘Hopefully the government takes advantage of this critical moment to pass legislation that fosters innovation and creativity, instead of simply catering to legacy business interests.’
The Pirate Party Australia has become increasingly active in Australian politics. The Pirate Party supported the adoption of a defence of fair use: ‘The past twenty or so years we have adopted many of the negative aspects of the United States’ copyright system, but with few of the safeguards the American laws have’, commented Mozart Olbrycht-Palmer, Deputy Secretary of Pirate Party Australia. ‘While we increased our copyright term to life plus seventy years via the Australia-US Free Trade Agreement in the early 2000s, we did not import fair use as a flexible exception for using copyrighted material without a licence.’ The spokesman emphasized: ‘These reforms are long overdue and go a long way to ensuring that Australia has copyright laws that genuinely reflect the needs of our society.’
The technology sector and the libraries and cultural institutions sectorhave also supported the introduction of a defence of fair use into Australian copyright law.
Conclusion
The Australian Law Reform Commission report on Copyright and the Digital Economy makes an important contribution to scholarship on copyright law and the defence of fair use. The study will inform both domestic deliberations over copyright law reform, as well as larger battles over the Trans-Pacific Partnership. The report is a thoughtful, independent, rigorous, and perceptive piece of work, which will set agenda with respect to copyright law in Australia. The Australian Law Reform Commission’s work will be the catalyst for great academic, political, and community debate about the future shape and form of Australian copyright laws. The Attorney-General George Brandis has been dismissive of the proposals with respect to a defence of fair use under copyright law. He has instead shown a fondness for the highly controversial three-strikes proposal. A number of Australian Labor Party politicians — including Jason Clare, Ed Husic, and Tim Watts — have been sympathetic to the work of the Australian Law Reform Commission. Senator Scott Ludlam of the Australian Greens has campaigned upon the need for the adoption of the defence of fair use under copyright law. There has also been strong support from consumers, technology developers, and cultural institutions for the introduction of a defence of fair use in Australian copyright law. 2014 will see an almighty fight over the future of fair use in Australia.
Biography
Dr Matthew Rimmer is an Australian Research Council Future Fellow, working on Intellectual Property and Climate Change. He is an associate professor at the ANU College of Law, and an associate director of the Australian Centre for Intellectual Property in Agriculture (ACIPA). He holds a BA (Hons) and a University Medal in literature, and a LLB (Hons) from the Australian National University, and a PhD (Law) from the University of New South Wales. He is a member of the ANU Climate Change Institute. Dr Rimmer is the author of Digital Copyright and the Consumer Revolution: Hands off my iPod, Intellectual Property and Biotechnology: Biological Inventions, and Intellectual Property and Climate Change: Inventing Clean Technologies. He is an editor of Patent Law and Biological Inventions, Incentives for Global Public Health: Patent Law and Access to Essential Medicines, Intellectual Property and Emerging Technologies: The New Biology, and Indigenous Intellectual Property: A Handbook of Contemporary Research. Rimmer has published widely on copyright law and information technology, patent law and biotechnology, access to medicines, plain packaging of tobacco products, clean technologies, and traditional knowledge. His work is archived at SSRN Abstracts and Bepress Selected Works.
Matthew Rimmer, ‘The Fight for Fair Use in Australia: Copyright Law in an Age of Cloud Computing’, Fair Use Week, Harvard Library, 28 February 2014, http://blogs.law.harvard.edu/copyrightosc/2014/02/28/fair-use-week-day-five-with-guest-expert-dr-matthew-rimmer/
Professor of Intellectual Property and Innovation Law, QUT. #Copyright #Patent #Trademark #plainpacks #Access2meds #SDGs #Climate #IndigenousIP #trade #TPP
3 
3 
3 
Professor of Intellectual Property and Innovation Law, QUT. #Copyright #Patent #Trademark #plainpacks #Access2meds #SDGs #Climate #IndigenousIP #trade #TPP
"
https://medium.com/@neal-davis/entry-level-cloud-computing-jobs-roles-and-responsibilities-49cc6f047529?source=search_post---------178,"Sign in
There are currently no responses for this story.
Be the first to respond.
Neal Davis
6 days ago·13 min read
If you’re an individual interested in pursuing a career as a cloud computing professional — this article will inform you about the various entry level cloud computing jobs, as well as the scope of responsibilities.
Cloud computing is a hot topic these days. Many organizations are looking for motivated cloud enthusiasts to fill their vacancies — not only in the IT department but also in accounting, marketing, and more. Cloud computing has changed how we work, learn and play. Today’s businesses need professionals with the knowledge of what it takes to be successful in this new environment — enter cloud computing professionals!
If you want to work in the highly lucrative cloud computing sphere, follow these steps:
You can choose to build multi-platform expertise or specialize in the services of a specific cloud platform. For example, you can work on Amazon Web Services (AWS) or Microsoft Azure exclusively and make yourself a cloud computing expert in no time! Regardless of what you choose, following these steps will help you land your dream job in cloud computing.
If your dream is to work in cloud computing, there are various entry level cloud computing jobs that provide great scope for growth. Let’s analyze each one of them:
This is a hands-on role that involves managing cloud resources such as networks, storage, and virtual machines. These professionals are responsible for maintaining uptime of the systems they’re responsible for.
A cloud administrator must have excellent systems administration and network skills, as well as the ability to handle day-to-day issues that arise in a cloud computing environment. Additionally, knowledge of virtualization technologies such as hypervisors and VLANs is also required to carry out this role effectively.
The main role of a cloud administrator is to work with the IT department to deploy, configure, and monitor compute, storage and networking services running in the cloud. An ideal cloud administrator must have the ability to work with various operating systems because sometimes they are asked to manage devices that aren’t part of their own infrastructure or platform. This means working across many different platforms.
Certifications for entry-level cloud administrators should be kept up to date so you can prove your abilities when applying for job openings.
There are various certifications that aspiring cloud administrators should consider, including:
You can choose to do one certification, but the more you have the better.
This is a role that focuses on the development of cloud computing solutions. A cloud developer will work with other members in the IT department to design, create, and maintain applications that are used by end-users for specific purposes.
A good understanding of programming languages is required when you’re considering this career path. It’s also important to have a firm grasp of databases because most companies use their own database management systems within virtual machines. Cloud developers must be proficient at coding software programs/apps that can function well in multi-tenant environments where several clients share infrastructure resources simultaneously!
The main responsibilities of a cloud developer include:
Most companies look for certified cloud developers who can prove their ability in this area. Cloud developer certifications include:
A cloud Engineer’s primary responsibilities are to design, implement, and maintain cloud computing solutions for clients/organizations. They must have the ability to communicate with both technical and non-technical team members so they can understand business needs first hand.
At this level, you must be well versed in other areas like networking, storage systems, and virtualization. Cloud engineers also need to be able to take full advantage of different types of infrastructure services that are provided by companies in the cloud computing domain.
You must have knowledge and skills in using programmability interfaces that can be used to automate tasks, troubleshoot issues related to infrastructure resources, and enhance the overall performance of network devices. Additionally, you must be able to use programming languages such as Python, Ruby, and others to develop APIs that can easily integrate with cloud computing solutions.
Other main responsibilities of a cloud engineer include:
There are different sub-roles of cloud engineer, including:
Some of the certifications options in this role include:
This role mainly involves the management of cloud security. As a professional, you will need to know how to monitor, detect and respond in case of an attack. You must be able to manage your clients’ security requirements by understanding the industry best practices (with respect to cloud computing) along with their compliance needs.
Becoming a cloud security professional can be a very rewarding career choice.
There are several certification options you can choose from, including:
This is a specialized role that involves selling cloud computing services to clients. You will need to understand the business needs of your potential customers and offer them products/services which best suit their requirements in an efficient manner.
A Cloud Sales Executive must have solid communication skills so they can clearly explain how the capabilities offered by different cloud providers add value for businesses. This professional should also be able to negotiate with vendors on behalf of their clients, thus closing deals quickly and efficiently.
Skills required for this role include:
This role involves providing technical assistance to cloud computing users in case they run into any issues. You will need to know all about different types of clients, their requirements, and how the products/services offered by your organization can help them solve their problems.
You should also be able to communicate with various teams within an enterprise (other than just the IT department) without requiring too much guidance or instruction.
Technical skills required for this role include knowledge on:
Since this role involves interacting with people, having strong social and communication skills is a must!
This is basically a junior version of DevOps Engineer. The Systems Administrators are responsible for managing and maintaining cloud computing systems, including networks and operating systems used in such environments to provide support (24/365) during emergencies and ensure that all applications run smoothly.
This role is a great way to get started with your career in cloud computing. You will need to have solid troubleshooting skills, excellent communication, and problem-solving abilities since you’ll be working closely with clients/different teams within an enterprise on a daily basis.
Some of the certifications options include:
The roles of a TAM are similar to those of a Cloud Support Engineer, but they tend to involve more complex issues and require a greater level of expertise. For instance, your support tickets may include scenarios such as:
This role requires strong communication skills and the ability to work independently. A TAM should also have a good understanding of cloud computing platforms such as:
Strong technical knowledge of different technologies is expected, including but not limited to:
Some certification options for TAM include, but are not limited to:
This role is mainly focused on gathering information and analyzing data. It requires a strong understanding of information security and how different data is protected. To be a data analyst, you need to have skills in SQL and creating visualizations, using Business Intelligence (BI) tools, and using programming languages/statistical tools.
Other skills you need include:
Some of the certification options for this role include:
This role is focused on digital skills training — this includes topics in digital literacy, technology operations, and entrepreneurship. You will be required to create course content for employees who are looking to develop their existing knowledge or acquire new ones.
You should have good communication skills along with the ability to work independently as well as part of a team. Knowledge about cloud computing platforms and various technologies are also an advantage.
While there are no specific courses geared towards this role, below are some courses you should consider:
A cloud consultant offers guidance to companies looking to venture into the cloud. You will analyze a company’s technology infrastructure and provide suggestions on how to improve it. This may include migrating from an existing data center or building a new one, setting up private servers, and more.
In addition to strong communication skills, you should also have good technical knowledge of different technologies such as:
In most cases, you will need a bachelor’s degree in IT or computer science to take up this role. Since this position requires managerial skills, having an MBA is usually preferred.
When it comes to looking for entry level cloud computing jobs, there are many platforms that you can visit, including job boards, employers’ websites, and cloud computing communities and forums. Let’s take a closer look at each of these options.
Job boards are a great option for those looking for entry level cloud computing jobs. You can visit sites such as Indeed, Monster, and Glassdoor. Other great job boards include Dice, Hired, or AngelList, Cloudy Jobs, Indeed, LinkedIn, and LinkUp. You may also want to check out Stack Overflow and Women In Technology International (WITI).
If you are looking for entry level cloud computing jobs in specific regions, you can visit job boards that cater to your region such as CanadaJobs or USAJobs. You may also want to consider specialized sites like The Muse which primarily focus on cloud computing roles.
Employers’ websites are a great way to find jobs in cloud computing. Use Google to search for your desired job title to find a list of potential employers. Many companies have a career page with current job openings. Use a search engine like Google to look for companies you would like to work for and leverage their career pages to find what kind of professionals they are hiring.
You may want to consider joining online cloud computing communities or forums that cater specifically to those interested in the industry. Here are some great forums you should check out:
Cloud computing can be an exciting career choice for those who enjoy working with emerging technologies and solving complex problems. With the growing number of cloud applications, more opportunities are opening up in this area making it a great time to get into this field.
With all these options to choose from, you should be able to find the right entry level cloud computing jobs for you. Make sure to do your research and find out more about the cloud computing jobs you are interested in.
Here are the best ways get started today and gain the experience you need to land your first cloud job:
You may also be interested in the following articles (linked to category Cloud Career):
Founder of Digital Cloud Training, IT instructor and Cloud Solutions Architect with 20+ year of IT industry experience. Passionate about empowering his students
3 
3 
3 
Founder of Digital Cloud Training, IT instructor and Cloud Solutions Architect with 20+ year of IT industry experience. Passionate about empowering his students
"
https://medium.com/hackernoon/the-pakistani-it-market-a-market-made-for-cloud-computing-1a6b2fce2d0?source=search_post---------179,"There are currently no responses for this story.
Be the first to respond.
The Pakistani reluctance in investing in an own IT infrastructure is a meaningful sign for the cloud’s potential.
Cloud computing omnipresent in today’s Information Technology sector. Considered as the digitization tool par excellence, the cloud has attracted a lot of interest across the globe. As business development progresses, the infrastructure must be able to cope with new modern initiatives and steady growth. With cloud computing the possibility of having a modern and state-of-the-art IT infrastructure without the need for substantial capital investments and personnel increases becomes real. It is therefore not surprising that the cloud market revenue is expected to double in the next three years reaching up to 162 billion USD. However, surprising is the fact that more than 65% of the cloud market is occupied by only a few leading giant providers, mainly American such as Amazon AWS, Microsoft Azure and Google Cloud. The remaining 35% market share is in the hands of thousands of cloud providers scattered around the world.
The basics of what cloud computing is about
Salesforce CEO and Chairman Marc Benioff brings it straight to the point. Cloud computing is a better way to run your business according to him. The possibility of improving your business is something that is appealing to every decision maker across the globe. Who thought that the cloud would one day be considered as the digitization tool par excellence. The year 2000 will always be remembered as the millennium of the more and more outstanding pace of technological progress. New and old technologies are as close together as never before. The almost uncontrollable increase in human knowledge leads to endless incremental innovations. The hunt for the next big thing seems to be never ending.
Even though cloud computing still plays that role, one might think that it is being pushed aside by the likes of Artificial Intelligence (AI) and Blockchain. Interestingly, most new technologies are distinguished by a huge number of data and intelligence. In other words, “the many disparate servers which are part of cloud technology hold the data which an AI can access and use to make decisions and learn things like how to hold a conversation. But as the AI learns this, it can impart this new data back to the cloud, which can thus help other AIs learn as well” as noticed Gary Eastwood from the IDG Contributor Network. Same goes for blockchain and other data intensive technology. Cloud computing plays undoubtedly a key role in today’s technological progress.
The cloud, in other words, providing IT resources and on-demand applications over the Internet at usage-based prices
Whether you run apps that share photos with millions of mobile users or support critical business operations in your organization, the cloud is a technology providing quick access to flexible and cost-effective IT resources. When it comes to cloud computing, you do not have to invest in hardware in advance or spend a lot of time managing it. Instead, you can provide the exact type and size of computing resources you need to implement your latest breakthrough idea or operate your IT department. You can access as many resources as you need almost immediately by paying only for what you use. Cloud computing provides an easy way to access servers, storage, databases and a full range of application services over the Internet. Cloud providers such as Amazon AWS, Microsoft Azure, Google Cloud Platform or “Swiss made” n’cloud.swiss operate and manage the network-attached hardware needed for these application services, providing and using the resources you need through a web application.
Advantages of cloud computing, it’s not only about cost-efficiency
The cloud has become a technology that influences everyone’s daily life. The adoption of solutions and services in the cloud present a number of advantages and benefits, among others:
Why Pakistan?
Pakistan is a growing economy with many opportunities. In particular, with the regard to the Pakistani market, companies are known for their reluctance in investing huge capital to buy their own IT infrastructure independently although decision makers are aware of the importance of cutting-edge technologies businesses to compete within a competitive global market. Highly advanced technological and sophisticated environment without important cost is exactly what the cloud presents as one of its advantages. The likes of Amazon AWS and Microsoft Azure have already launched their cloud services in Pakistan.
“The cloud services market in Pakistan is on the rise. As a result, it presents a huge potential for cloud providers. Safety, reliability and outstanding quality without the need for heavy capital investments in IT infrastructure is indeed something decision makers in Pakistan look for. The reluctance in investing in an own IT infrastructure is a meaningful sign for the cloud’s potential”, explains Danish Ali Chaudhary, Business Development Manager at n’cloud.swiss AG.
From a more general point of view, Pakistan’s economy is the 25th largest in the world with a population of over 200 million. According to Pakistan Economist 36 percent of Pakistan’s overall economy is undocumented. When calculating per capita income these 36 percent are not taken into consideration. As one of the developing country, Pakistan is believed to have the potential of being one of the Next Eleven. Together with the BRICS, they are expected to become one of the world’s large economies in the 21st century.
#BlackLivesMatter
46 
how hackers start their afternoons. the real shit is on hackernoon.com. Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
46 claps
46 
Written by
Dynamic and data-driven marketing leader. Author, Mentor & Speaker. Top 50 Global Thought Leader. See www.yahyamohamedmao.com
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
Dynamic and data-driven marketing leader. Author, Mentor & Speaker. Top 50 Global Thought Leader. See www.yahyamohamedmao.com
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/cudos/%EF%B8%8F-cloud-computing-market-to-grow-5x-by-2030-b7da507fda02?source=search_post---------180,"There are currently no responses for this story.
Be the first to respond.
The cloud services market could grow 5 times by 2030. Technological developments and direct investments are fuelling this growth, while COVID-19 and the associated lockdowns and work from home policies also contribute. Ultimately, the market must be competitive, efficient and decentralised for all stakeholders to benefit.
According to a recent report, the global cloud services market could reach $1,620,597 million by the end of the decade. Based on a valuation of $325,689 million in 2019, this translates to compounded annual growth of nearly 16%.
The shift of enterprises toward cloud-based services and advancing technologies such as artificial intelligence (AI) and machine learning (MI) are the primary drivers behind this growth. On the funding side, significant investments by counties such as China, India, the U.S., and the U.K. in cloud-based projects could boost growth further, according to a recent study.
Another critical market expansion catalyst is COVID-19. First, consumers have been spending more time at home due to lockdowns. As a result, cloud-based services such as video-on-demand (VoD) and over-the-top (OTT) media have become more popular.
Second, many public and private sector organisations have implemented ‘work from home’ (WFH). Consequently, the need for software as a service (SaaS) based communication and collaboration tools has also increased. Jointly, these two factors have accelerated market growth.
Nonetheless, it is vital that the market is fair and competitive for consumers to benefit as well. Furthermore, this growth must happen in a sustainable way that honours the environment and utilises existing computing resources to minimise waste. Finally, a decentralised approach would allow individuals with spare computing power to earn by participating in the market while delivering a cheaper service for consumers.
The Cudos Network is a layer 1 blockchain and layer 2 computation and oracle network designed to ensure decentralised, permissionless access to high-performance computing at scale and enable scaling of computing resources to 100,000’s of nodes. Once bridged onto Ethereum, Algorand, Polkadot, and Cosmos, Cudos will enable scalable compute and Layer 2 Oracles on all of the bridged blockchains.
Next Generation Cloud
51 
51 claps
51 
Written by
Next Generation Cloud
Next Generation Cloud
Written by
Next Generation Cloud
Next Generation Cloud
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://architecht.io/hashicorp-ceo-doing-open-source-in-the-age-of-cloud-computing-and-kubernetes-dfad22fa2327?source=search_post---------181,"In this episode of the ARCHITECHT Show, HashiCorp CEO Dave McJannet discusses his company’s recent $40 million funding round, a result he credits to a maniacal focus on building specific products and making users happy. McJannet, who previously led marketing efforts at companies including SpringSource and Hortonworks, also shares his thoughts on evolving open source business models over the last decade, and on what it’s like to both compete with and capitalize on the growing excitement around Kubernetes.
Scroll to the bottom (or click here) for links to listen to the podcast pretty much everywhere else you might want to.
news.architecht.io
architecht.io
Enterprise IT interviews and analysis: AI, cloud-native, startups, and more
2 
2 claps
2 
Written by
Founder/editor/writer of ARCHITECHT. Day job is at Pivotal. You might know me from Gigaom - way back in the day, now.
Once a site about next-gen enterprise IT and the people building it; now a place where Derrick Harris occasionally blogs about tech-related things.
Written by
Founder/editor/writer of ARCHITECHT. Day job is at Pivotal. You might know me from Gigaom - way back in the day, now.
Once a site about next-gen enterprise IT and the people building it; now a place where Derrick Harris occasionally blogs about tech-related things.
"
https://medium.com/readwrite/introducing-the-technologies-set-to-redefine-cloud-computing-ffda1f5c4609?source=search_post---------182,"There are currently no responses for this story.
Be the first to respond.
For many years, enterprise cloud computing has been a careful balance — and sometimes an epic battle — between what is possible and what is practical on the ground. This dichotomy has led to a lot of confusion, which in turn can hold back development. Here is introducing the technologies set to redefine Cloud computing.
"
https://medium.com/@deshpandetanmay/carbon-footprint-calculator-for-green-cloud-computing-c536fb4f49d8?source=search_post---------183,"Sign in
There are currently no responses for this story.
Be the first to respond.
Tanmay Deshpande
Nov 17, 2021·4 min read
My previous article, “Climate Change Combat Guide For Software Engineers,” discussed how software developers can help reduce carbon emissions. COP26 Conference also highlighted the immediate need for action in these directions.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@dataart/%D1%81loud-security-fundamentals-infrastructure-hardening-and-cloud-computing-security-3be6b8fa08f?source=search_post---------184,"Sign in
There are currently no responses for this story.
Be the first to respond.
DataArt
Jun 29, 2020·1 min read
Learn cloud security fundamentals, explore popular benchmarks, learn how to protect different types of services, automate cloud compliance checks, and enhance operational security.
About the Webinar:
During the Сloud Security Fundamentals webinar, DataArt’s experts delved into the aspects of shared responsibility in cloud security and compliance check automation. They reviewed the popular benchmarks such as TrendMicro and CloudSploit, shared the approaches to identity and access management hardening, and much more.
In this cloud security tutorial, get insight on:
— Key differences in security requirements for on-prem and cloud security architecture
— Your duties keeping the cloud resources safe as a part of responsibility shared with the cloud provider
— Cloud infrastructure hardening methods and ultimate cloud security goals
— Popular security auditing benchmarks and tools, and how to adapt them to achieve your desired security level
— Installing software updates and vulnerability management after migrating to the cloud
We design, develop & support unique software solutions. We write about Finance, Travel, Media, Music, Entertainment, Healthcare, Retail, Telecom, Gaming & more
2 
2 
2 
We design, develop & support unique software solutions. We write about Finance, Travel, Media, Music, Entertainment, Healthcare, Retail, Telecom, Gaming & more
"
https://medium.com/pharos-production/10-top-guides-2019-devops-cloud-computing-aaa477d92550?source=search_post---------185,"There are currently no responses for this story.
Be the first to respond.
Give us a message if you’re interested in Blockchain and FinTech software development or just say Hi at Pharos Production Inc.
Or follow us on Youtube to know more about Software Architecture, Distributed Systems, Blockchain, High-load Systems, Microservices, and Enterprise Design Patterns.
Pharos Production Youtube channel
We have prepared a list of 10 awesome articles on DevOps and Cloud Computing topics. Here you go.
2. How to Backup and Restore Jenkins. Complete Guide.
3. How to Benchmark HTTP Latency With Wrk.
4. Erlang VM Tuning Guide for Elixir applications.
5. How to Synchronize Two Remote Git Repositories.
6. How to Deploy RabbitMQ to AWS With Ansible.
7. How to Deploy Quorum Private Blockchain to AWS With Ansible.
8. Installing Jenkins on Debian-like Linux. Complete Guide.
9. How to Install and Configure PostgreSQL 10 on Debian-like Linux.
10. Quorum Infrastructure. JPMorgan Constellation Upgrade Guide.
We hope we have helped you to grow. Whatever you do, do it well!
Pharos Production Team.
Software Development Company.
100 
100 claps
100 
Written by
CTO and Founder@ Pharos Production Inc. — Blockchain and Fintech Software Development Company.
Software Development Company. Fintech and Blockchain. Enterprise Solutions.
Written by
CTO and Founder@ Pharos Production Inc. — Blockchain and Fintech Software Development Company.
Software Development Company. Fintech and Blockchain. Enterprise Solutions.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/my-experience-at-the-alibaba-cloud-computing-conference-2018-c10ae0ea7c5?source=search_post---------186,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Nov 13, 2018·5 min read
By Sai Sarath Chandra, Alibaba Cloud MVP and Tech Share author
In this article, I want to share my experience at Alibaba Cloud Computing Conference 2018, which is also known as “Yunqi Conference” in China. There are many things that I could talk about but this will take more than a single blog post, so I will focus only on the major takeaways of this event. To give you a perspective, here are some quick facts about the Alibaba Cloud Computing Conference 2018:
All this started when I became an MVP and Tech Share author for Alibaba Cloud in late 2017. I was already working on some solutions using Alibaba Cloud in mid-2017 but the MVP was awarded to me on December. As an MVP, I was given a lot of opportunities to not only learn about Alibaba Cloud but also to share my knowledge with the developer community in India. I really enjoy meeting industry experts, connecting with the community, listening to their requirements, and discussing their problems while coming up with solutions.
As one of the first global Alibaba Cloud MVP, I had great support from the Alibaba Cloud team to make this initiative successful. With this opportunity, I have met some inspiring leaders from the community. I have also conducted and participated in various meetups across India. All in all, it is pretty amazing being an Alibaba Cloud MVP because it improves your subject matter expertise and also helps you find like-minded people.
As a Diamond-level MVP, I was awarded the opportunity to attend The Computing Conference 2018 as well as the MVP Summit 2018 in Hangzhou, China.
Now talking about the Computing Conference, trust me the conference is massive at least in my experience. There are so many power-packed and knowledgeable speeches from the leaders of Alibaba Group, various technology and business partners, as well as individual developers at the main forum. At first I was a little concern about the language barrier but there was live interpretation available for every speech, which was really convenient.
There are several sessions from different product leaders scheduled; some sessions were so popular that you needed to arrive early to get a seat. Apart from the sessions from the product leaders, there are also several sessions from the product developers where you can interact with them to understand about the product. Unfortunately, some of these sessions overlap and I could not attend all the sessions simultaneously.
I also enjoyed exploring the booths at the conference. There were a lot of interesting booths spanning across different technological areas within both hardware and software such as smart homes, drones, and Internet of Things (IoT). As an MVP, I had the opportunity to interview some of the product partners such as JetBrains, RedHat and SAP. I learned not only about their products but also on how partnering with Alibaba Cloud helped these companies.
Below are some of the most memorable events at the conference, as well as proposed launches that I am really excited about.
AliOS is something which I am not exposed to before, but it is now being applied to the automotive industry to change the way we drive. AliOS has been successfully launched in China and available across ten car brands. Some AliOS features that really amazed me are:
There are a multitude of product demos that were interesting, including:
There are a lot more that I could not cover in this blog, but be sure to stay tuned to Alibaba Cloud’s latest offering. There have been plans for new product launches such as Data Lake Analytics, IPv6 as a service, and much more.
That’s all from me for now, it was a great experience for me in Alibaba campus in Hangzhou, China. I look forward to The Computing Conference in 2019 and I hope to see you all there!
Further reading:
https://www.alibabacloud.com/blog/jack-ma-at-the-computing-conference-2018-%E2%80%93-exploring-the-role-of-data-in-new-manufacturing_594084
www.alibabacloud.com
Reference:https://www.alibabacloud.com/blog/my-experience-at-the-alibaba-cloud-computing-conference-2018_594145?spm=a2c41.12269797.0.0
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
51 
51 claps
51 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://architecht.io/enterprise-hybrid-call-it-what-you-will-but-its-all-the-rage-in-cloud-computing-70b772e54408?source=search_post---------187,"This is a reprint (more or less) of the ARCHITECHT newsletter from July 19, 2018. Sign up here to get new issues delivered to your inbox.
If you told me 10 years ago that Amazon Web Services, Microsoft and Google would be spending 2018 fighting for cloud computing dominance among large enterprises I probably wouldn’t have believed you. (Or maybe I would have. I don’t recall. Check the archive ;-)) At the very least, I probably wouldn’t have believed they’d be taking their fight inside the data center.
But here we are. After a week of speculation about whether AWS is building a network switch, AWS sticking EC2 instances inside local storage devices, and Google powering local Kubernetes applications (check out the this week’s earlier issues if you missed these stories), things just keep getting more interesting. Here’s what we learned today:
The day will come for artificial intelligence, serverless computing and other services that are only commonplace among a small segment of developers, but today the money seems to flow right back to data centers and VMware.
architecht.io
This is about Google’s new open source Cirq project, which aims to put quantum computing algorithms into the hands of more people (even if they don’t yet have access to quantum computers). If Cirq is the quantum equivalent of TensorFlow, that would be huge for Google, although a big difference is that deep learning was well established by the time TensorFlow was released.
technologyreview.com
IBM — and Watson Health, in particular — catch a lot of flak. But if this partnership works out, the more power to it.
zdnet.com
I really don’t know how suited these companies are to curing the types of diseases they’re tackling, but the idea of startups combining forces isn’t a bad idea. Especially in complex areas like medicine, where each can bring necessary knowledge to the table.
venturebeat.com
The company claims to do facial recognition that doesn’t sacrifice privacy. Although, one could argue the ability to use its tech on any camera or smartphone has investors more excited than the privacy angle.
techcrunch.com
It seems like a lot of companies have tried this, but they tended to say they were doing “machine learning.” $3 million isn’t a lot of money for this problem, but one advantage Lifebit might have is the ever-cheaper, easier access to computing power and AI systems.
techcrunch.com
architecht.io
As I’ve written on a few occasions now, if Apple is smart it will expand its search efforts to the point where it can actually compete with Google in AI. A working knowledge graph for Siri would be a great start.
venturebeat.com
Yup. This is a podcast interview with Andrew Feldman of Cerebras Systems, who has a long history building innovative hardware startups. For one, he was founder of Arm-based server startup SeaMicro, which was acquired by AMD.
oreilly.com
There are a handful of suggestions here, but I think most good AI advice boils down to the importance of automation versus insights. Taking some routine task and doing it better and faster will probably pay off faster than trying to use AI to actually learn something new.
hbr.org
Google researchers (including GAN godfather Ian Goodfellow) take a look at the motivations and possible techniques of attacks using adversarial examples. They also breakdown how these threats have been presented in research papers so far, and explain why those approaches don’t work so well in real-world threat scenarios.
arxiv.org
Basically, how does a bank or hospital, for example, access data from other sources without opening up a bunch of privacy risks.
arxiv.org
architecht.io
That this is still an issue is surprising. You’d assume Microsoft would be pretty good at forecasting demand and making sure it has the resources to handle it.
theregister.co.uk
Google explains the load balancer issue that briefly brought down Spotify and Snapchat earlier this week. It was a software bug as the company was updating the service with new features.
google.com
You probably aren’t running millions of servers that can perform trillions of configuration checks per day, but Facebook is. As technologies and problems from webscale companies continue to trickle down to mainstream enterprises, I wonder if there’s a point at which Facebook/Google scale destroys that funnel.
fb.com
Is this an isolated situation among companies providing cloud services in China? It’s not as if companies can open their own data centers in the mainland without playing by the government’s rules.
engadget.com
Slight marketing language aside, this is a decent overview of how we got where we are with containers. While I was at Mesosphere, I did this interview with Jason Hoffman (then at Ericsson), which covers container history much more colorfully.
mesosphere.com
I would argue that a lot of discussion overlooks the operations aspect of cloud-native computing, some of which is covered here. Whatever the benefits to developers, the folks in charge of managing infrastructure might win even bigger, as GitHub’s Jesse Newland explained on the podcast last year.
thenewstack.io
architecht.io
Reportedly, the algorithm-powered vetting system kills a lot of deals, which has some investors frustrated and trying to game the system. I think we’ve learned at some point over the past several years that gut instincts (and other subjective inputs) probably do still have a place in many decisions, despite what was preached during the rise of big data.
axios.com
Sounds like Python is the language of choice for many data scientists. I’d be curious to know how much this issue bubbles up to the business level, or if data scientists are largely left to their own devices when choosing tools, languages, etc.
zdnet.com
Enterprise IT interviews and analysis: AI, cloud-native, startups, and more
4 
4 claps
4 
Written by
Founder/editor/writer of ARCHITECHT. Day job is at Pivotal. You might know me from Gigaom - way back in the day, now.
Once a site about next-gen enterprise IT and the people building it; now a place where Derrick Harris occasionally blogs about tech-related things.
Written by
Founder/editor/writer of ARCHITECHT. Day job is at Pivotal. You might know me from Gigaom - way back in the day, now.
Once a site about next-gen enterprise IT and the people building it; now a place where Derrick Harris occasionally blogs about tech-related things.
"
https://medium.com/it-dead-inside/cloud-computing-stick-to-open-standards-82dadba800a0?source=search_post---------188,"There are currently no responses for this story.
Be the first to respond.
Let’s travel back to the late 80s. A rising software company known as Lotus released its answer to database technology, known as Lotus Notes.
"
https://medium.com/@alibaba-cloud/the-evolution-of-cloud-computing-ee19f58281d?source=search_post---------189,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
May 17, 2021·12 min read
By Jack Cai, Chief Architect of Elastic Compute, Alibaba Cloud Intelligence
Cloud computing is still in its infancy. While it is reshaping the business and technology world every day, it is also reshaping itself constantly to meet the needs of different customers. From deployment model to location coverage and operation model, cloud computing has never stopped evolving since its inception.
But no matter how far and wide the cloud extends itself, or how flexible and diversified its deployment and operation model will be, there is one principle that should be followed — CONSISTENCY. A consistent experience, backed by a consistent technology stack. This is exactly how Alibaba Cloud, as one of the most active innovators in the space, evolved its cloud offerings in the last ten years.
From a deployment model perspective, public cloud and private cloud used to be two separate worlds. Public cloud is typically owned and operated by a cloud provider, offering customers an “on-demand” IT service based on a shared infrastructure. Private cloud, on the other hand, is owned and often operated by the customer. Sometimes the operation of a private cloud is outsourced to the cloud provider or a third party. The table below provides a summary of the differences between the two.
Many Internet startups today typically are taking an “ALL-IN-CLOUD” approach, or more specifically, all in public cloud. In contrast, existing enterprises, especially large businesses, usually already have a dedicated on-premises infrastructure and need a more sophisticated approach to “cloudify” their IT.
Generally speaking, there are three common strategies that enterprises would adopt when developing their infrastructure:
Based on the above analysis, even though some enterprises may prefer to stick with private cloud for some time, in the long run, the reasons for adopting public cloud will become more and more compelling. This is especially true for businesses that are rapidly expanding and are looking for a more flexible deployment and operation option. Most enterprises will eventually employ a hybrid model to enjoy the advantages of both worlds.
The ideal way of a hybrid cloud is to have a consistent, unified and connected experience across the public and private cloud.
A consistent experience means the same development, deployment, security, scaling and operation interface, APIs and tools. In this scenario, users don’t have to learn different ways to use a public cloud and their own private cloud. Applications don’t have to adapt to different APIs. Operators don’t have to use different tools. All these bring higher efficiencies and save investment.
A unified experience means being able to manage resources and applications across public and private cloud in one interface. In short, one interface, multiple clouds. Users don’t have to log into different consoles to manage their cloud resources. This requires the public and private cloud to share some common capabilities, like identity management. In this regard, a private cloud effectively becomes a special “region” of the public cloud. This region is special because it’s only visible to and accessible by one particular customer, which comes to its “private” nature.
A connected experience means the public and private cloud shall be connected as necessary based on customer’s requirement. Connections can take place at different layers:
The best foundation to build a consistent, unified and connected hybrid cloud is one common technology implementation for both public and private cloud. But this is a challenging job. The OS to run a public cloud need to handle the management of hundreds of thousand servers. It is built for extreme scale and ultimate availability. As a result, the kernel itself may need hundreds of servers.
While the requirement of a private cloud differs in many areas, the most obvious difference being that its scale is much smaller, sometimes only tens of servers. So there is a strong requirement for a minimized cloud OS. This is exactly the same story as the Linux operation system, which supports both large servers that have hundreds of powerful CPU cores and small devices that have very limited computing resources such as a mobile phone.
Alibaba Cloud is the first major cloud vendor to take on this challenge. Back in 2014, Alibaba Cloud started to build a private cloud offering using the same Apsara Cloud OS that runs its public cloud, and delivered it to several early customers. On April 20th, 2016, Alibaba officially announced Apsara Stack Enterprise, which is a fully-fledged private cloud offering that includes most of our major public cloud services like ECS and OSS.
Then Microsoft announced Azure Stack in 2017. In 2018, AWS announced Outpost, taking a different approach to tackle the hybrid cloud market. Outpost is not a private cloud offering. Instead, it’s an extension of AWS’s public cloud, owned and run by AWS instead of the customer. While it does bring very consistent, unified and connected experience, it needs to be connected with a public cloud region’s management system in order to be fully functional, and is only able to tolerate a few hours network disconnection. In 2019, Google announced Anthos to manage applications in a hybrid environment. It is based on Google Kubernetes Engine which does bring consistent application management experience. However, it relies on existing virtualization infrastructure in place. In this regard, it’s not a self-contained private cloud stack.
Going forward, we will continue to see more consistencies across the public and private cloud, more unified management, and better connection. Hybrid will become more converged.
Hybrid cloud is not the only solution for cloud computing. From a location coverage perspective, it only covers major customer sites and public cloud regions. There are things in between and beyond them — the edges.
The key motivations for putting cloud on the edge are lower cost and lower latency. There are two driving scenarios: closer to the data and closer to the user. By moving computing power to the edge where it is closer to the source of the data, the time and cost for moving a potentially huge amount of data are decreased.
As the world become increasingly digitalized and as artificial intelligence become ubiquitous, the volume of the data that need to be analyzed is exploding. So it makes perfect sense to shift from a computation-centered world to a data-centered world. For instance, in autonomous vehicles, by moving the applications closer to their end “users” (which could be a device such as a vehicle), access latency can be dramatically reduced. This does not only improve user experience of those applications, but also critical in some scenarios such as autonomous driving.
The edge computing market is expected to have tremendous growth potential in the coming years. According to a report from Grand View Research Inc, the global edge computing market size will reach USD 43.4 billion by 2027, exhibiting a CAGR of 37.4% over the forecast period. Gartner also predicts that through 2025, the edge hardware infrastructure opportunity will grow to USD 17 billion.
There are multiple tiers of edges, as illustrated in the above picture. The edges that reside at the public cloud side, which I labeled “cloud edge”, include data centers local to certain cities or areas, operator data centers run by telecom and network operators, and base stations serving as network access points such as 5G cellular base stations. The edge at base stations are also referred to as “Multi-access Edge Computing”, or MEC in short. Cloud edges are typically owned and run by cloud providers. From left to right, these cloud edges offer descending latency when accessed by customer applications or end users. And from top to bottom, they have decreasing scale and provide less resource elasticity.
On the other side, the edges that reside at customers’ sites, which I labeled “device edge”, include private cloud, branch offices or factories, and fields where IoT devices are deployed. Device edges are typically owned and run by customers, but as innovations continue, they may be owned and run by cloud providers too, AWS’s Outpost being one example. Again, from left to right, these device edges offer descending latency when accessed by devices, and from top to bottom, smaller in scale and less elasticity.
All major cloud providers are aggressively investing in the edge cloud space. Alibaba Cloud addresses the cloud edge with the Edge Node Service (ENS) offering. With over 150 deployments covering 50+ cities, ENS was the first and by far the largest “edge cloud”.
For the device edge, in addition to the Apsara Stack, Alibaba Cloud also has the Link IoT Edge offering, which is purpose-built for IoT scenarios. Last month Alibaba Cloud announced CloudBox, which is similar to AWS Outpost and completes Alibaba’s edge portfolio. AWS uses Local Zone and Wavelength to cover the cloud edge, but today it only supports a limited set of locations. At the device edge, AWS has Outpost, Snowball Edge and IoT Greengrass for various scenarios. Azure is following quickly in the edge space too. In July 2020 Azure announced Edge Zone, which is still in preview right now.
The above table summarizes the various cloud deployments I have talked about so far. With the cloud now distributed farther and wider beyond the central region data centers, we are obviously witnessing the shaping of a new era — the distributed cloud era. Customers will have more options when choosing where to deploy their applications in order to save data moving cost, satisfy extreme latency requirement and improve end user experience. Yet again, a consistent experience will be vital across the distributed deployments of the cloud.
Distributed cloud is not the end of the cloud evolution story. While private cloud and device edge give full ownership and control to the customer, and public cloud and cloud edge take away the ownership and control but also the complexities in operations, there are other interesting combinations missing especially when locations are considered. AWS Outpost, for example, obviously implements one of those new combinations — it’s owned and operated by the cloud vendor but deployed in a customer’s data center.
Now let’s open for more possibilities. What if the customers want the ownership of the servers but not the operation and the data center? What if they want to reuse their existing server investment? What if a partner wants to take advantage of their existing data center infrastructure or their established customer base without building their own cloud?
Alibaba Cloud has always been a pioneer in the cloud innovation, not only technology-wise, but also business-wise. We heard our customers and partners asking for new possibilities. So at Alibaba’s Apsara Conference 2020, we announced two new offerings, namely Hosting Zone and Partner Zone.
Hosting Zone offers three key benefits. First, Hosting Zone allows customers to retain the ownership of the servers. Customers can even bring in their existing servers. Second, Alibaba Cloud is responsible for operating the Hosting Zone and makes a selected set of cloud services available based on customer’s requirement. Last but not least, Hosting Zone gives customers very flexible choices of data center locations. It could be in existing Alibaba public cloud data centers, or smaller local data centers close to the customer or their end users. Alibaba is even willing to work with customers to open new data centers to satisfy their needs.
Partner Zone, as its name implies, is for partners to collaborate with Alibaba Cloud in new ways. Partner Zone can be deployed in Partner’s data center, running on Partner’s servers, but operated by Alibaba Cloud. Partners can choose the cloud services that they want in their Partner Zone. They can build their own branding on top of Alibaba Cloud. With Partner Zone, partners are now empowered by one of the best cloud technology and operational support forces to play a whole new role in the cloud era.
With the addition of Hosting Zone and Partner Zone, the cloud has never been so flexible, as shown in the above table. Obviously Alibaba has again led the way to reinvent cloud. Despite of all the flexibilities in deployment and operation models, there is one principle that should not be sacrificed — consistencies.
Reusing the same Apsara OS to support all these different shapes of the cloud has always been Alibaba’s way to ensure consistencies. It’s never been an easy job though. Technology wise, this requires:
Business-wise, this requires:
Nevertheless, with a strong belief and commitment to our cloud strategy, we made all these possible for you.
Putting everything together, Alibaba Cloud’s elastic computing offering looks like the above diagram above. We believe that there are more possibilities to explore in the future, and we welcome you to continue exploring and innovating together with us.
The evolution of the cloud is never ending.
Jack Cai is the Chief Architect of ECS products at Alibaba Cloud Intelligence. His role covers PaaS, SaaS and IaaS. He was the lead architect of Huawei’s SaaS platform and IBM’s Bluemix PaaS application runtimes. Currently he is responsible for the architecture of Alibaba Cloud’s Elastic Compute portfolio, building a reliable, elastic, high-performing, inclusive and intelligent cloud computing infrastructure.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
2 
2 
2 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/@jaychapel/evolution-not-revolution-cost-optimization-still-tops-challenges-of-cloud-computing-d0eec7563f4?source=search_post---------190,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Jun 2, 2020·4 min read
Over the past five years, we’ve seen the challenges of cloud computing evolve — but ultimately, their core needs are the same as ever.
It’s interesting to experience and observe how these needs get translated into products, both our own and others. Depending on company growth and culture, Build ↔ Measure ↔ Learn cycles can continue to turn in a rapid fashion as ideas get adopted and refined over time. Or, they can become bogged down in supporting a large and often demanding installed base of customers.
In a few short years, tools built for optimizing public cloud have evolved into a number of sub-segments, each of which in turn has developed to meet customer needs. In part, this reflects a predictable maturation of enterprises using cloud computing as they have migrated from on prem to public or hybrid cloud, and adopted best practices to enhance performance and security while tackling overall growth in monthly spend.
Flicking through various analyst reports and “Cool Vendor” white papers, it’s fascinating to see how quickly cool becomes uncool as industry needs develop. Being a social scientist by training, longitudinal panel type surveys always grab my attention. RightScale/Flexera’s annual customer survey ticks a few of these boxes. No doubt the participants have changed, but it likely provides a valuable source of data on customer needs and challenges in cloud computing.
You do not need to go back to the first RightScale survey in 2013 to see some big changes. Even when comparing the 2016 to the 2020 survey in terms of company priorities in the cloud, it’s hard to believe that just a few years ago, the number one challenge regardless of maturity was related to a skills/resources gap. These priorities were followed by security and compliance issues, with the focus on cost optimization being a lower priority and then only for those at a more mature state. Roll forward to 2020 and cost management is now the number one cloud initiative in all but the most recent adopters of cloud where it sits at number two. Interestingly, security seems to have dropped off the top-five list. Governance has held on, although likely headed the same way. Conversely, cost optimization now sits atop all other initiatives.
What seems to be apparent when reading between the lines of such reports and when talking with customers is that unlike migration, security, and governance, there are still some large holes in companies practices when it comes to optimization and reducing cloud waste. Despite a plethora of tools on offer in 2020 that offer to bring visibility and cost management that the overall cloud waste number is actually still growing as infrastructure grows.
More money has been spent tackling security and governance issues — and these challenges in cloud computing need to be dealt with. But cost optimization can deliver ROI to free up budget to deal with these issues.
In the wake of COVID-19, finance teams across the world will now be sharpening their pencils and looking more aggressively at such measures. While cloud spending may rise, Gartner and IDC have both forecasted overall IT spending to drop 5–8%
As with security and governance, a mix of human behavioral and business process changes will be required, both of which can be supported by effective tooling, both native cloud provider and 3rd party ISV tools. Incentives to implement such changes are likely to be higher than in the past, albeit in a more cash-constrained world where low cost, ease of use, and most of all, quantifiable ROI will be prioritized. It has always appeared to me somewhat oxymoronic when I hear promises of reducing cloud waste through the use of expensive cloud management tools that charge based on a percent of your spend.
I foresee a wave of low cost, multi-cloud, and simple to use tools emerging. These tools will need to demonstrate a rapid ROI and be built to be used across engineering and operations (not just in the offices of the CIO/CTO/CFO) to ensure the self-service nature of cloud is not disrupted. A similar pattern will emerge as these tools become part of day-to-day cloud operations where cost optimization is part of the cloud culture. With this the need for specific cost optimization initiatives should be replaced by a new wave of needs, like application resource management.
Originally published at www.parkmycloud.com on May 27, 2020.
CEO of ParkMyCloud
See all (317)
9 
9 claps
9 
CEO of ParkMyCloud
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/tech-jargon/cloud-computing-3e86e47b47b6?source=search_post---------191,"There are currently no responses for this story.
Be the first to respond.
Suppose you want to start a new company. Your company would require a lot of computer hardware and software to setup the IT infrastructure and network for it. Cloud Computing is simply a way of taking all this IT infrastructure and operations and handing it over to someone else to build or manage, so that your internal team can focus on ways to build the business, rather than becoming experts at managing storage or protecting data. If the computing service can be provided to you over the Internet by another company, then exactly where the hardware and software is located and how it all works doesn’t matter to you at all. This is the underlying concept of Cloud Computing!
There are many example where we use cloud computing in our everyday lives without realizing it. When you search a term on Google, your computer is not trying to find the answers to your search. The words you type are sent over the internet to Google’s datacenters where they dig out the results and send back the answer to you. These datacenters maybe local or can be sitting anywhere in the world like Tokyo, or Beijing or anywhere else.
“You don’t generate your own electricity. Why generate your own computing?” — Jeff Bezos, CEO, Amazon
Think of cloud computing as renting a fully serviced flat instead of buying a home of your own. Clearly, the advantages and the convenience have helped Cloud Computing grow in the world of technology.
This post was also published in the “Tech Jargon of the Week” column in The Campbell Express and on my blog.
Have you been in a conversation where everyone is speaking…
2 
2 claps
2 
Written by
Researcher & Innovative Engineer
Have you been in a conversation where everyone is speaking around this “techie” word unknown to you? Do you find it difficult to keep up with all the unfamiliar jargon and acronyms? No need to sweat, as this publication will teach you the meaning of some commonly used tech words.
Written by
Researcher & Innovative Engineer
Have you been in a conversation where everyone is speaking around this “techie” word unknown to you? Do you find it difficult to keep up with all the unfamiliar jargon and acronyms? No need to sweat, as this publication will teach you the meaning of some commonly used tech words.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@inmediatesg/digitalizing-the-insurance-industry-thru-cloud-computing-7fa8015790df?source=search_post---------192,"Sign in
There are currently no responses for this story.
Be the first to respond.
Inmediate.io
Dec 2, 2019·2 min read
With digitalization rapidly transforming the business landscape, it is well established that the first step for any organization going digital is to embrace the cloud.
For insurance companies, cloud technology is a godsend.
Insurance companies are often seen as businesses struggling to maintain and manage legacy systems while also attempting to deliver on customer expectations.
Moving to the cloud provides enormous opportunities to replace traditional, ineffective systems used by insurers to reduce costs, build a customer-centric enterprise, and increase policyholder confidence.
Insurers have always been under pressure to cut costs, and cloud computing can relieve this pressure.
Being a technology that requires revenue expenditure instead of capital expenditure, it can significantly decrease initial costs. With the scalable and more efficient IT services that cloud computing offers, maintenance costs are also reduced.
The cloud also makes the production of documents, archiving of data, and other processes simpler. Insurers can scale cloud-based IT services according to their needs, reducing human errors and vastly lowering processing time.
To stay relevant in the insurance industry, keeping up with consumer behaviors and trends must be an iterative process and requires more than just data.
The avalanche of customer information that insurance companies possess would be useless without analytics that can derive value from it. Cloud computing can help here. Not only can cloud-based solutions collect and analyze data, but they also help extract the most relevant, useful insights.
With the understanding gained from data, insurers can develop relevant products and optimize claims and policy servicing capabilities.
Insurers can now come up with personalized products for clients, instead of force-fitting them into a one-size-fits-all solution. Cloud computing helps insurers align service levels, products, and customer interactions.
When insurance companies, through a variety of initiatives, prove to customers that they have their best interest at heart, they’re able to earn the trust and loyalty of customers and turn them into advocates of their business.
Although there’s no denying the fact that cloud technology provides many benefits to insurers, given the kind of data handled in the industry, companies need to be mindful of data privacy requirements when engaging with cloud service providers.
Unfortunately, neglecting to do so could hamper policyholder confidence and negate the benefits of moving to the cloud.
Introducing Inmediate: a platform on which customers, distributors and insurers using smart contracts connect. https://inmediate.io
2 
2 
2 
Introducing Inmediate: a platform on which customers, distributors and insurers using smart contracts connect. https://inmediate.io
"
https://medium.com/@alibaba-cloud/10-years-of-cloud-intelligence-how-will-alibaba-change-the-future-of-cloud-computing-aae9862f2f7a?source=search_post---------193,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Oct 8, 2019·10 min read
Why Alibaba Cloud Intelligence? In this article, we look at Alibaba Cloud’s President Zhang Jianfeng’s discussion in a speech given at a recent Beijing summit. In it, Zhang discusses the reasons for the push behind Alibaba Cloud’s rebranding as Alibaba Cloud Intelligence throughout its 10-year journey. He also described the journey that took Alibaba Group and Alibaba Cloud, being at the core of the Group’s IT infrastructure and internet technologies, to its current place in the market. Next, we look at where Zhang sees the brand standing in the cloud computing space. And last, with this rebranding marking the tenth anniversary of Alibaba Cloud, this article outlines what Zhang sees as the future goals for the brand over the next ten years, specifically pushing forward the concepts of intelligence and an integrated cloud ecosystem.
To understand Alibaba Cloud and the rebranding to Alibaba Cloud Intelligence, you need to know a bit more about the journey that Alibaba Cloud took over the past ten years to get to its current place in the market.
Ten years ago, Alibaba Cloud was pretty small, and e-commerce wasn’t that big of a thing either. At the time, Alibaba had no idea what sort of things would be in store for the e-commerce industry. Zhang said that he especially felt that way in 2005, during his second year on the Taobao project. But he also distinctly remembered Jack Ma, showing the foresight that lead Alibaba to where the brand is today, saying that they ought to make the payment system that would become Alipay — a technology that now makes carrying cash in China almost a thing of the past. Zhang, in his speech, also recalled Jack reiterating that the future is cloud. Bit by bit, Alibaba Cloud got to where they are today, and now being in the tenth year of Alibaba Cloud — being one of the largest cloud service providers in the world.
In the ten years of its existence, Alibaba Group has become the first cloud provider in China, officially opening for business in 2009. Before this time, no other company in China had ever considered making cloud services as their main form of doing business and as a technological future. Before 2009, Alibaba Group was focused on the e-commerce platform of Taobao, but Alibaba was in the red for the first six years working on this project. Everyone at Alibaba didn’t really know what the exact business model was. But, despite all of this, the penetration rate of e-commerce in the market was increasing year on year, with Alibaba’s market penetration slowly reaching 10%, before reaching 40% after 10 years of hard work.
Alibaba has also developed China’s first cloud operating system as well as the City Brain platform. Alibaba Cloud is different to other cloud providers because, as part of the larger Alibaba community, Alibaba Cloud has developed several of its own in-house technologies and proprietary solutions, which have powered Alibaba’s e-commerce and mobile payment platforms. It is this practical experience and innovation that distinguishes Alibaba Cloud from its competitors. On top of that, consider this. In 2017, Alibaba Cloud’s main operating system, developed in house, Apsara Distributed Operating System, won the China Institute of Electronics (CIE)’s Technology Progress Award (Special Class), which was the first time in fifteen years for CIE to award a special class award.
Last, Alibaba Cloud has quickly come to have millions of customers today. Several industries have found new ways to innovate with the rise of cloud, many of whom are ecosystem partners of Alibaba Cloud. Many more industries and services, typically not connected to the web and cloud, have grown closer ties with the services and solutions in the cloud. And amid of all of this, there’s been all sorts of ripple effects in the infrastructure of everything, including us, with there now being the projects Industry Brain and Agriculture Brain, to name just two. All of these things have been grown through close ties between several traditional industries and the data intelligence capabilities of Alibaba Cloud.
Alibaba Cloud understands how cloud, data, and intelligence at important things that will fundamentally change everything that we do, and will also transform how Alibaba does cloud. In fact, cloud and AI technologies can be the impetus to set off another industrial revolution, just as electricity was and did in the past. Today electricity is infrastructure: companies don’t make their own power plants; they buy electricity from the grid. With an increasing number of industries connected to the cloud, there are several new opportunities for innovation and change in the space.
So, with Alibaba Cloud’s rebranding to Alibaba Cloud Intelligence, you may ask, what is all entailed by the transition? And what is different about Alibaba’s version of cloud compared with our competitors? Does Alibaba Cloud have any unique advantages?
The first answer to this question is that Alibaba’s bringing the whole of its IT infrastructure to the cloud. Alibaba as of now is making the transition to move all in on its IT infrastructure, deciding to move off of the infrastructure provided by IBM, Oracle, and EMC. Behind this move is the hope to expand the overall capacity and scope of the services offered at Alibaba in a cost- and time-efficient manner. Next is the idea of creating a new IT system that utilizes cost-efficient hardware along with a large-scale distributed system of scheduling, which are both more optimized than traditional systems.
Alibaba hopes that, through migrating all of its IT infrastructure to the cloud and allowing it to be transformed by Internet-related technologies, Alibaba can use the core technologies of the Internet to construct a complete IT system. Also, with this change, all of Alibaba’s technology will become one system shared with Alibaba Cloud. That is, the technology behind Alibaba will be the same as the products and services you can find on Alibaba Cloud, and all of Alibaba Cloud’s customers will for the first time be using the same underlying infrastructure that Alibaba uses internally.
The second answer, and the more important of the two, is that Alibaba Cloud is built on a cloud infrastructure that ultimately centers on data and intelligence, allowing Alibaba Cloud to firmly enter the machine learning, AI, and data intelligence space, going well beyond the Infrastructure-as-a-Service layer of cloud. Involved in this is a transition from IT infrastructure to middleware and a constant movement towards innovating with data and intelligence applications.
Currently, all of Alibaba’s retail platforms and electronic payment systems use internet-based technology to function, built on a system with Internet-centric and on-cloud technologies. This will only increase in the future. Through having big data along with machine learning, AI, and other intelligent technologies being at their core, Alibaba Cloud can retrieve data and process data in a more natural way, having designs that are both future proof and advanced. For Alibaba, “data” is not merely a raw resource, but is more like the lifeblood of the company. Fully realizing the capacity of “data,” changing a company into an intelligent one.
When it comes to intelligent companies, just consider Alibaba for a second. There’s are two billion products on Alibaba’s e-commerce platforms, and 11 million sellers, and there’s roughly three-hundred million visitors every day. All of this adds up to a lot of data.
So what is Alibaba able to do with all of this data? Well, in today’s Taobao Mobile App, all users using this app see pages that are specifically designed to their personal tastes and shopping experience, with every click leading to pages that are generated in real time. Involved in all of this is the fact that Alibaba processes massive amounts of data and does all of this in real time — being two crucial capabilities behind building a better user experience on the App. Doing all of this in real time is extremely difficult because real time doesn’t mean in minutes; it means in seconds and milliseconds. With these kinds of speeds, the user is unware of what is going on, and pages load fluidly, being at a speed that doesn’t affect the user’s overall browsing experience. At the same time, the data of each user is processed separately, which adds up to an incredibly massive the sum of data being pushed through the system continuously.
For all of these corresponding systems to work, Alibaba needs to be able to create an infrastructure that can wholly support big data and the corresponding algorithms, having the technological prowess to go through the entire process of collecting, processing, and storing data quickly.
Now let’s talk about intelligence. Today, the vast majority of the time, data processing is not purely dependent on computing power, but rather also it involves intelligent algorithms. In many ways, computing power is just the infrastructure layer. But, algorithms, on the other hand, are something beyond that. In fact, now algorithms are closely tied with the business operations of several different industries across the globe. Alibaba, given its unique position in China, has been able to create a complete intelligence platform through creating algorithms in the process of collaborating with our ecosystem partners across various industries. At Alibaba Cloud, we think that, through bringing the IT infrastructure to the cloud and core technologies online, as well as through engineering our platforms and capabilities in big data and intelligence, we are able to create a framework that is worthy of being called Alibaba Cloud Intelligence. It’s not just infrastructure — it’s the data and intelligent algorithms that exist on top of this infrastructure as well. This is what is at our core at Alibaba Group.
Today Alibaba Cloud is investing through increased research into new-generation technologies and further partnerships in an increasing number of industries to enrich and empower these industries. Both of these moves reflect Alibaba Cloud’s goals for the future. Alibaba Cloud has been working on its core capabilities, hoping to increasingly to bring its underlying infrastructure to the cloud, reinforce and strengthen its internet technologies, and create a data- and intelligence-focused platform. As a part of these goals is increased involvement in several industries, as already seen in the past several years. The applications for the data intelligence are endless, and Alibaba Cloud hope both now and in the future to create firm and deep relationships with ecosystem partners across various industries.
Three years ago, Alibaba established the Alibaba DAMO Academy, dedicated to promoting industry-leading computer science and related research for practical industry scenarios. Moreover, Alibaba Cloud have continually increased investment into the program, with widespread investments in the fields of quantum computing, machine intelligence, embedded chips, and database systems. On the chip end, the development of IoT end embedded chips has been moving along steadily, with two-hundred million chips sold last year, and the first Alibaba networking processing unit (NPU) model, which was developed internally, to be officially released in the second half of this year. The capabilities of this chip is much more advanced than equivalent chips, being a difference of not one or two times better, but rather ten times better.
Next, Alibaba offers several products that can enrich and empower several strategic industries. One of these is retail. In this industry, Alibaba has continually invested and explored the concept of new retail, offering several products, which can be used by a variety of our clients. Another industry is finance. Alibaba’s Ant Financial Services integrates financial services with advanced internet technology and data intelligence capabilities to provide innovative internet and mobile financial services, with services such as Yu’ebao, an investment management service, and Zhima Credit, a private credit scoring and loyalty program, all of which have gained great popularity in China already. And a third example is digital government. Starting in Zhejiang, and then to several other places in China, government entities have started to use internet technologies and data intelligence, changing the underlying system from one based on departments to one based on users and data.
Alibaba Cloud hopes to create an integrated cloud, one build with its many ecosystem partners and companies across several industries. The hope is that Alibaba Cloud’s services and technological solutions can become “integrated” in the solutions adopted by our ecosystem partners.
That is, Alibaba Cloud is interested in creating Platform-as-a-Service (PaaS) and Infrastructure-as-a-Service (IaaS) solutions — which can serve as the layers for the core technologies and internet capabilities on the cloud — with ecosystem partners and companies further developing Software-as-a-Service (SaaS) solutions on top of these underlying technological and intelligence solutions. By collaboration, Alibaba Cloud hopes that ecosystem partners can make data and intelligence part of their service solutions, with their core products and solutions using Alibaba’s core capabilities.
Alibaba Cloud hopes that, in the next ten years, ecosystem partners across several industries will upgrade and transform their existing systems through migration to the cloud and the development data intelligence capabilities and internet technologies. Cloud and data intelligence are the future, and Alibaba Cloud hopes to expand its involvement with more industries, bringing the power of cloud to more spaces of commerce and industry.
To summarize, Alibaba Cloud considers the following four aspects to be what makes Alibaba Cloud “Intelligent” both now and in the future:
Learn more about Alibaba Cloud’s journey in the past 10 years by visiting https://www.alibabacloud.com/campaign/10-year-anniversary
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
2 
2 
2 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/fwd50/cloud-computing-deep-dives-at-fwd50-2019-7d39b1f9405f?source=search_post---------194,"There are currently no responses for this story.
Be the first to respond.
Fifteen years ago, the term “cloud computing” was on the front page of every tech magazine. For that matter, fifteen years ago, we had tech magazines. Today, the term has almost lost its meaning in many IT conversations. We don’t say, “cloud storage”; we say “storage,” and assume on-demand, third-party-operated computing systems are part of that strategy.
And this is a big year for Federal cloud infrastructure. More than a dozen government departments are embracing a cloud-first strategy.
To understand the magnitude of this shift, we first need to nail down terminology — which, even a decade after the first real cloud adoption, can still cause confusion.
Clouds come in three distinct types:
There are myriad variations on this simple category. Some platforms, like Airtable or Google Sheets, have powerful programming languages built into them that blur the line between SaaS and PaaS. And many on-demand functions, from image processing to search lookups to content delivery networks, are simply services.
Behind this all is one fundamental idea: Own the base, rent the spike. Computer workloads are bursty, particularly those that are seasonal (like tax filings) or deal with data (like building a machine learning model from unlabelled data sources.) Making compute resources shareable makes them more cost-effective; making them easy to use reduces the time to create a new app or run a prototype.
But look to these strengths, and you’ll find some of the issues that still plague cloud adoption:
Drawing the right line between what you control, and what you use in the cloud, is critical if you’re to retain data sovereignty, effective cost management, and the ability to move workloads around. This is complicated stuff you need to get right.
So we’ve packed this year’s event with lots of expert content on cloud migration and adoption, including:
The move to cloud is one of the most significant shifts in government IT adoption since we moved off mainframes and onto distributed clients. It’s important to get it right — and FWD50 has experts from around the world teaching can’t miss content. Grab your ticket here to chart your best move to the cloud.
How do we use technology to make society better for all?
1 
1 clap
1 
Written by
Writer, speaker, accelerant. Intersection of tech & society. Strata, Startupfest, Bitnorth, FWD50. Lean Analytics, Tilt the Windmill, HBS, Just Evil Enough.
How do we use technology to make society better for all? Each year at FWD50, the world's digital governments convene to chart the course of policy, platforms, and innovation.
Written by
Writer, speaker, accelerant. Intersection of tech & society. Strata, Startupfest, Bitnorth, FWD50. Lean Analytics, Tilt the Windmill, HBS, Just Evil Enough.
How do we use technology to make society better for all? Each year at FWD50, the world's digital governments convene to chart the course of policy, platforms, and innovation.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/readwrite/cloud-computing-for-businesses-will-covid-19-surge-cloud-adoption-b882ba1060ab?source=search_post---------195,"There are currently no responses for this story.
Be the first to respond.
Globally, the education system became online, doctors examined their patients over the Internet, and the corporate professionals worked from their home delivering their productivity; the world, in a way, started connecting virtually more than ever before.
But this global crisis came out to be the cloud having a silver lining for the cloud computing market. The cloud has not only been a boon for the businesses but also for the consumers to cope with the struggles of effective work management, entertainment, collaborations, and much more.
This article will throw a light on the cloud computing adoption by businesses and its future scope in the Covid-19 era.
Cloud computing offers huge benefits to the businesses for expanding their business capabilities such as:
These factors help the businesses build the resiliency to navigate during the crises.
In the phase of unprecedented disruption, as consumers, we all witnessed how we gave a tough fight against social distancing practices with the accelerated use of Netflix, Zoom, Twitter, and Slack — the Amazon Web services (AWS) public cloud-hosted services.
From the market perspective, many businesses are actively adopting cloud computing services to seamlessly manage their operations remotely by creating virtual workspace, managing workforce, storing crucial healthcare data, handling accounting and taxation, and supporting digitizing the manual processes.
A Flexera study indicated the increased use of cloud computing resources during pandemic among enterprises and small and medium-sized businesses. Out of the 57% of the respondents, 31% shows a slightly higher usage than they planned, and 26% shows a significantly higher usage than planned.
Image source: Flexera
Microsoft CEO, Satya Nadela, recently said that the impact of Covid-19 was such that just in two months of its 3rd quarter, Microsoft has seen the two years’ worth of digital transformation. With a “big bang approach”, he referred to the quick adoption by deploying the cloud services fast.
According to a study, The expected cloud market size in 2019 is to rise from USD 233 billion to USD 295 billion by 2021, at a Compound Annual Growth Rate (CAGR) of 12.5% during the predicted time frame.”
Taking into consideration the eCommerce space, with the rising number of Covid-19 cases after unlocking in many countries, merchants have observed a huge shift in consumer habits. Shopping from brick-and-mortar stores while maintaining social distancing is causing unprecedented levels of disruption for buyers which has encouraged them to shift their gears to shop online.
As of May 31, 27 percent of the US respondents said that they purchased hygiene products online instead of offline.
The increase in the number of online shoppers during the Covid-19 times has caused a ton of light bulb moments for the retailers globally to migrate their businesses to online platforms. Besides, the existing eCommerce store owners have been nudged to escalate their eCommerce game by adopting advanced technologies to increase their sales in 2020, for better customer satisfaction and timely product deliveries.
Adopting cloud computing helps merchants easily upscale or downscale their business. It not only helps merchants to reduce the costs of goods and services but also enables new companies to grow. At the consumer end, the cloud provides ease of shopping experience, anytime and anywhere delivery, cashless digital payments, and much more.
The pace with which eCommerce sales is booming during the Covid-19 times, it is sure that they will outperform post-Covid and, thus, lead to an increase in cloud market growth.
Many enterprises (organizations with a number of employees more than 1000) are choosing the lane of cloud computing technology due to its multiple advantages.
In a survey of 750 respondents who were asked to share their insights into what drove them to move their organization to the cloud-based platforms, 73% of the respondents said cost-effectiveness is the main reason for their adoption. And, 61% of the respondents said migrating more workloads to the cloud was the main reason for their cloud adoption.
Public cloud is the most popular cloud option used by enterprises with a usage share greater than 61%. Talking about the public cloud providers for the enterprises, the order of dominance is Amazon, Microsoft, and Google over the rivals Alibaba Cloud, IBM Cloud, and Oracle Cloud. Amazon Web Services hold the first place with an adoption rate of 76% followed by Microsoft Azure with an adoption rate of 69%, and Google Cloud is the third place holder with 34%.
The statistics below depicts the order of public cloud service providers adopted by enterprises till now in 2020.
Image source: Flexera
The cloud adoption rate by enterprises in 2020 is expected to grow more as compared to the last year. Based on the existing trends, 59 percent of enterprises expect cloud technology usage to exceed prior plans due to Covid-19.
An estimate shows that this year in the U.S., the average time consumed with subscription OTT video content will exceed 62 minutes per day which is 23.0% up from 2019. With Netflix gaining the first position, the average time spent will exceed 30 minutes per day in the US in 2020, which is up more than 16% from 2019.
With most of the people staying at home, the exponential escalation in the video-streaming services has resulted in expanding the cloud market.
Many IT and ITES companies are planning to opt the long-term work from home options. For instance, Twitter has announced a permanent work from home opportunity for its employees.
Based on the current scenario, the need for seamless collaborations over the Internet calls for strong cloud infrastructure has risen. The increase in the usage of SaaS solutions to support the remote workforce contributes to market growth as well.
Not only the IT companies, but the finance industry has also accelerated taking the route of SaaS-based cloud accounting solutions.
Many small businesses have shown interest in cloud adoption. Their major workload runs in public clouds (43%) as compared to the private cloud (35%) with AWS as the preferred public cloud.
In Covid-19 times, the cost-effectiveness and the data security that cloud provides without hiring the trained staff, it is becoming the need of the hour for many small business entrepreneurs.
According to Gartner, 50% of US organizations are actively using the cloud. Both the private and the public cloud are being adopted by the government bodies with the public cloud services growing in double-digits.
And, through 2021, the spending forecast will grow on an average of 17.1%. The government bodies find effective cost savings and delivery services are the key drivers of cloud adoption in government.
Like many businesses experiencing economic slowdown, the fintech industry has experienced the fiscal shocks and stock market slumps during the global crises.
Where everything came to a halt for some time, the finance industry changed the game at global level. They accelerated cloud adoption by providing the customers with branch-less banking, increased digital payment apps, and contactless payments via mobile phones to reduce social contact.
Bookkeeping has evolved with the changing technology trends. The traditional software is being taken over by cloud technology. With the pandemic, where work from home is the new fashion of working, many bookkeepers are now using cloud-based accounting software.
While many businesses were upended, shifting to cloud-based software has been quite a relief for the bookkeepers to save their employment and their employer’s business. Many bookkeeping apps are available which helps businesses eave an impact on their clients. The need to continue business operations has pushed entrepreneurs to opt for cloud technology.
Regardless of which time zone they are in, the cloud culture has enabled the accountants, managers, and bookkeepers to monitor their business’s financial health, providing ease of work collaboration and anytime or anywhere data access with enhanced security.
Many companies are opting for cloud-based tax compliance solutions amidst Covid-19 situations. Both small businesses and big enterprises are considering choosing the digital approach for they provide the benefits of risk mitigation and flexibility.
Since, the Covid-19 occurred in the season of tax filing and returns in many countries, the tax professionals around the globe are pushed to pick cloud-based solutions for providing effective tax solutions to their clients. This approach not only enables the tax filing process faster but also allows the tax professionals to meet client’s needs in a cost-effective and the most reliable way.
The professionals are getting inclined to making a switch towards cloud-based tax software that post Covid-19 this trend will keep growing.
As per a study, it is expected that the growth in cloud computing in the education market will be observed from USD 8.13 billion in 2016 to USD 25.36 billion in 2021. And, the compound annual growth rate expected is 25.6%.
But with the pandemic, the speed of adoption seems to increase from the expected rate. With unexpected lockdowns, the educational organizations experienced a complete shutdown, including schools, colleges, and universities, in many world sections.
The education however didn’t come to a halt, kudos to the man-made cloud technology. Though major universities are already running on cloud models for offering innovative eLearning experience, the pandemic forced many school-going students to go for their academics via online platforms which led to the expansion of the cloud market. Many higher education providing organizations are investing in cloud computing services to fulfill their need of a centralized system for academic process management.
Cloud computing has proved to be a boon for the educational institutions to bring everyone onto a virtually connected campus, transforming the learning process. Supporting the concept of ‘learning beyond classrooms,’ cloud computing vendors are sure to continue growing post-Covid-19.
The Healthcare industry has faced maximum challenges in 2020. With the number of Coronavirus patients increasing day by day, the generated data has increased more than before. Likewise, gathering the patient data, managing it, and storing it securely comes up as a big challenge for healthcare professionals. But, cloud computing has proved as the savior for the healthcare industry.
Cloud computing has provided immense benefits that have helped health care professionals continue providing their services remotely in a safe manner, even in pandemic times. The seamless collaboration of the cloud helps health professionals easily share the patient data with other healthcare providers, enabling them to provide them better services.
Besides, the adoption of technology by the healthcare industry, the increase of digitization, and the rising GDP helps in the modernization of healthcare services have added to the cause of the market growth.
Currently, North America tops the list of healthcare cloud market due to the presence of a huge patient population and major medical and technical companies. With more organizations opting for cloud culture globally during the Covid-19 crises, the healthcare cloud market is sure to boom at a great pace. And, Cloud computing, along with AI and machine learning is bringing the smart hospitals into existence.
In a nutshell, Covid-19 has been a catalyst to increase the space of cloud adoption opportunities in each industry bringing in the Digital Transformation by storm.
Like any technology adoption, the cloud computing implementation comes up with a prominent set of risks and challenges which include:
It is quintessential to have a trusted cloud hosting provider who can help you provide quick, reliable, and cost-effective solutions for successful cloud operations.
Now, if you ask, should you go for a cloud computing approach? The answer is ‘Yes’ as it has been successful in providing benefits of flexibility (37%), Disaster recovery (38%), and relieving IT staff’s job (36%).
With a huge number of businesses, including small businesses, enterprises, fintech industry, entertainment industry, education industry, etc. transitioning to the cloud computing models to provide enhanced and unified user experience, the global adoption rate is undoubtedly increasing compared to previous years.
In 2019, the US gained the top position by spending $124.6 billion preceded by China investing $10.5 billion, the UK spending $10 billion, Germany spending $9.5 billion and Japan spending $7.4 billion.
In 2020, the global crisis has further added fuel to the cloud adoption fire, making the investors spend more in the market. For example, China-based Alibaba Cloud, Hangzhou, which is the top cloud provider in Asia, has declared $28 billion of investment in its cloud infrastructure over the next 3 years.
Converting the crisis into opportunities, adopting cloud computing strategies to cope up with the challenges, this Covid-19 has definitely created a surge in cloud adoption globally.
Cloud Computing for Businesses: Will Covid-19 Surge Cloud Adoption? was originally published on ReadWrite on October 13, 2020 by Vishwa Deepak.
ReadWrite is the leading media platform dedicated to IoT…
2 
1
2 claps
2 
1
Written by
The latest #news, analysis, and conversation on the #InternetOfThings
ReadWrite is the leading media platform dedicated to IoT and the Connected World. We work with the industry's top technologies, thinkers, and companies to tell the stories that drive this world forward.
Written by
The latest #news, analysis, and conversation on the #InternetOfThings
ReadWrite is the leading media platform dedicated to IoT and the Connected World. We work with the industry's top technologies, thinkers, and companies to tell the stories that drive this world forward.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@Apiumhub/cloud-computing-a-growing-trend-in-2017-a91ec066c43d?source=search_post---------196,"Sign in
There are currently no responses for this story.
Be the first to respond.
Apiumhub
May 25, 2017·8 min read
In the past few years, cloud computing has become a common word. In fact, According to the Worldwide Semiannual Public Cloud Services Spending Guide, worldwide spending on public cloud services will grow at a 19.4% CAGR, from almost $70 billion in 2015 to more than $141 billion in 2019 with companies investing in cloud services for new competitive advantages. In the following overview about cloud computing you will get more information about what is cloud computing, the different types of service models, the benefits of cloud computing and most importantly, the cloud computing trends in 2017.
To summarise it, it’s more or less when you store data and programs on a cloud instead of storing it on the hard drive of a computer. What do you need to access the data? An internet connection. So yes, it’s about sharing, storing, processing and managing resources delivered over a network of remote servers hosted on the Internet.
Here, we talk about offering services that are related to hardware. It’s basically using a third party provider for hosting a hardware. It can be said that you are paying to access hardwares over the net, this can mean server or storage services. An example would be if you have a subscription with a hosting company to store files on their servers with a pay per use model.
Leading vendors:
Here, you are getting a development platform on the cloud. What does that mean? You get access to a platform that allows you to develop and build applications that run on systems software or hardware that belong to other companies. For example, developing an ecommerce web that is running on a vendor’s server.
Leading players:
Ss its name says it, here, you get a complete software on the cloud. So you are running a complete application on the system of a third party, hosted on the cloud. A very common example would be a CRM system, here at Apiumhub we use ZOHO that enables us to get a sort of overview of our sales cycles.
Leading players:
A private cloud is when an infrastructure is either hosted on-site on a company’s intranet, or hosted in the data center of a service provider and the resources are not shared with other organizations. We use it to increase storage capacity and the power of the processor. It is often used by big companies with big amounts of data or that have strict regulations about their data and certain types of regulations. The main advantage of going for a private cloud is that it offers a higher level of control and security. Also, it’s more customisable and therefore adapter to the specific IT requirements of companies. In general, the disadvantage would be that the maintenance and management is part of the responsibility of the company.
On the public cloud, the data of a company is stored in the data center of a third-party provider, on a shared hardware, and the storage and processor capacity are not owned by that company. Usually, small to medium sized companies use public cloud computing due to various advantages it provides them; as it’s not their hardware, it implies that there are no maintenance costs to the client and that they are not responsible for the management. Another advantage is that the time it takes for testing & deploying is decreased. Basically, it delivers agility, scalability and efficiency. Although security breaches are kind of rare, some businesses get scared off by that when it comes to public cloud.
The name is quite clear, hybrid cloud is a mix of private & public cloud services, it’s almost as if you get the best of both! What is does is that is allows to move between both clouds and enables you to leverage the beast of what each one offers. The main advantage is that you get huge flexibility and much more options, you can for example put the most critical operations on the private cloud and the rest that on the public, increasing your agility.
Now that we all kind of got what is cloud computing and the different types we’ve got, here’s the part that keeps on evolving and that is really great to know about; cloud computing trends in 2017!
The development of native apps keeps on increasing and cloud providers are more and more focusing on how to provide services for those applications that are more complex and that need things like time-based analytics, omni-channel support, and microservice support.
You probably all know what IoT is and how much it grew in the past year. Well, it’s still the beginning. In 2017, we might reach millions of sensors and other devices coming online! All of is mostly focused around smart cities, Connected Buildings, Predictive Maintenance and autonomous traffic
Don’t be shocked to see more and more image recognition and voice interfaces. Between the release of TensorFlow by google and the three new machine learning services announced by amazon, you can be sure that in 2017 it will continue on growing, specially because it is becoming easier for developers to use and integrate into applications.
Since 2015, the use of hybrid cloud has continued to grow and is expected to grow even more in 2017. Making hybrid cloud work implies having an audit function to verify that the service still fits for purpose. This is getting us to a new position “a cloud service broker” that is responsible for defining services and choose the best way to manage and secure them.
When talking about next-generation cloud applications, containers are very important. In the last two years, many companies have started using container technologies like Docker, to help them standardize the way they package and deploy code. It enables developers to really manage code.
This technology is crucial to agile development and to microservice architectures. As we will get more applications directed towards microservices, container platforms that run microservices will be more on demand. Other than being one of the most exiting cloud computing trends for developers (at least it’s the case here at Apiumhub), this obviously implies that there will be new challenges, for example companies will have to be more careful with security, monitoring, storage and networking issues.
As the demand for the cloud services is that high and will continue on growing, scalability and maintenance costs are therefore more and more important. Hyperconverged infrastructure solutions is great because it offers to help by proposing pre-integrated compute and storage resources that will help companies when it comes to getting their cloud implementations running faster. Hyper converged platforms will be more used for cloud architecture in 2017 and will become the default infrastructure platform when it come to building the private part of a hybrid cloud.
Cloud is not seen as if it’s only for small companies anymore. Huge players have started to notice the advantages and facility of it, the fact that it cuts costs and risks. So yes, we will continue on seeing the transitions of infrastructures into the cloud.
But more specifically, the public cloud. In fact, C levels are getting more and more comfortable when it comes to hosting software in the public cloud and we expect this to be one of the cloud computing trends that will grow the most.
Last year we saw many self service solutions and data has become more and more easy to push on the cloud without having a technical background. Hopefully we will soon say goodbye to the complexity of data integration & transformation, it might be more of a copy/paste action.
With all those growing cloud computing trends, there are much more companies using the cloud for their business, there will be more security standards that will try to get better migration to the cloud and therefore encouraging more businesses to integrate and adopt the cloud. This increases the demand for cloud expertise. Cloud-focused training are more frequent and training programs are focusing on cloud security, hosted databases, and infrastructure as a service.
Enough with cloud computing trends! By now you must have understood that with cloud computing you get many benefits. In a way you are enabling access to data from almost anywhere and with the growth of digital devices around us, we are just making it all more efficient and available. Here are the five main benefits of cloud computing:
With a purchased software you usually get yearly releases but when you’re using cloud computing services, it’s easy, you can get an upgraded system immediately. So you get the latest versions when they are released, including new features and functionalities and that on a regular base and with the latest technology. This can mean that you get up-to-date versions of software and upgrades to servers and processing power.
Yes, you reduce the costs. Different costs. First of all, as companies have smaller (or none) data centers by using cloud computing, it implies that you reduce costs because you don’t need to buy equipment, hardware, facilities, utilities, etc. you reduce the number of servers and software costs. Other than that, you are also lowering the staff costs and system maintenance costs.
Granting access to your employees means that you are boosting their flexibility. In fact, you can access, edit and share data from anywhere you are, because you only need a device and an internet connection (some apps even work offline). Most of employees are quite happy when they know that they can take their work anywhere. Happy employees means higher productivity.
In fact, it’s much more flexible. You can play with your capacity, scale up or down your storage following your specific needs, may they be changing or not. If your needs increase or decrease, you really don’t need to worry about it.
Obviously all businesses want to protect their data. There are many situations, which might rarely happen, but when they do, represent a tremendous problem as for example, natural disasters or power failures. Well with the cloud, everything is backed up in a secure place. This means that you can always access your data unless your device is broken or that you have no connection. But that’s not as big of a problem than losing the whole data, you just need to get another device. Your data is in the cloud, you can access it no matter what happens to your machine.
Don’t forget to subscribe to our monthly newsletter to receive latest news in the software world!
Originally is published on https://apiumhub.com/tech-blog-barcelona/.
Software architecture, web & mobile app development www.apiumhub.com
2 
2 
2 
Software architecture, web & mobile app development www.apiumhub.com
"
https://medium.com/@TechJobs_NYC/daily-report-cloud-computing-is-proving-highly-profitable-4d50d5b08ec4?source=search_post---------197,"Sign in
There are currently no responses for this story.
Be the first to respond.
Technology Jobs NYC
Jan 29, 2016·1 min read
Daily Report: Cloud Computing Is Proving Highly Profitable By JOSEPH PLAMBECK The earnings reports released by Amazon and Microsoft reinforced that cloud computing is a booming, multibillion-dollar business. Published: January 28, 2016 at 07:00PM via NYT Technology http://bits.blogs.nytimes.com/2016/01/29/daily-report-cloud-computing-is-proving-highly-profitable/?partner=IFTTT
The official account for https://www.technologyjobs.nyc! Follow for exclusive job opportunities, tips and tricks to help you get your next #technology #job
2 
2 
2 
The official account for https://www.technologyjobs.nyc! Follow for exclusive job opportunities, tips and tricks to help you get your next #technology #job
"
https://medium.com/@TechJobs_NYC/microsoft-profit-and-revenue-fall-but-cloud-computing-grows-18ca1e8b754c?source=search_post---------198,"Sign in
There are currently no responses for this story.
Be the first to respond.
Technology Jobs NYC
Jan 29, 2016·1 min read
Microsoft Profit and Revenue Fall, but Cloud Computing Grows By NICK WINGFIELD Quarterly results underscored Microsoft’s good and bad trends as software floundered while the cloud business continued to grow. Published: January 28, 2016 at 07:00PM via NYT Technology http://www.nytimes.com/2016/01/29/technology/microsoft-earnings.html?partner=IFTTT
The official account for https://www.technologyjobs.nyc! Follow for exclusive job opportunities, tips and tricks to help you get your next #technology #job
2 
2 
2 
The official account for https://www.technologyjobs.nyc! Follow for exclusive job opportunities, tips and tricks to help you get your next #technology #job
"
https://medium.com/@kyleake/principles-of-cloud-computing-on-premise-vs-cloud-based-servers-39a875d71075?source=search_post---------199,"Sign in
There are currently no responses for this story.
Be the first to respond.
Korkrid Akepanidtaworn (Kyle)
May 1, 2019·6 min read
Historically speaking, software is purchased and installed on computers in a company, also known as an “on-premise” system. Often, server administrators are responsible for managing all servers, software upgrades, and workstation upgrades, backup, etc. The list can go on and on. This model used to work well...
"
https://medium.com/lambda-architecture/google-cloud-computing-ab42dff7a0e8?source=search_post---------200,"There are currently no responses for this story.
Be the first to respond.
For Google’s cloud computing offering, the Google Cloud Platform, 2013 was an exciting year. Over at their blog they currently run a review and one of the posts is around one of the key events: Google Compute Engine (GCE) becoming GA. I personally find this very exciting and want to contribute with some further material around GCE, partly stuff I stumbled upon, partly stuff produced by yours truly.
Some heads-up before we start: this is gonna be a mixed bag, some of the stories are rather low-level and tekkie while others are more general-purpose/business oriented. And off we go!
http://www.wired.com/wiredenterprise/2013/04/google-compute-open/
According to recent tests by a company called Scalr — an outfit that helps businesses manage their use of Amazon EC2 and other cloud services — virtual servers boot 4 to 10 times faster on Google Compute Engine than on EC2, and in some cases, the company says, you can move data onto Google as much as 20 times faster.
http://www.networkworld.com/news/2013/120313-google-compute-engine-amazon-web-services-276537.html
Google will likely be in this fight for the long-haul and it may push AWS to innovate faster technically. Google’s cloud executive hinted to the New York Times that the company hopes to offer a more reliable and higher-performing cloud compared to that of AWS. RightScale, which is a company that allows customers to access public clouds from multiple providers, notes in a blog post that it has been impressed with Google cloud’s performance and that one of its customers is using GCE over AWS because of the reliability and scalability.
http://www.infoq.com/news/2013/12/qubole-on-gce
Qubole, a managed Hadoop-as-a-Service offering is now available on Google Compute Engine (GCE). Qubole was so far only available on Amazon’s AWS and this announcement follows only a few days after Google releasing GCE into general availability.
http://datadventures.markbox.io/2013/12/29/storm-on-gce
In this post I describe how to set up and use Storm on GCE. You can find the underlying code over on GitHub.
Hope you found the one or other resource here useful and I’m very much looking forward to the developments in GCE and Google Cloud in general, in 2014!
Stories around applying the Lambda Architecture for…
2 
2 claps
2 
Written by
Solution Engineering Lead in the AWS open source observability service team
Stories around applying the Lambda Architecture for building large-scale data processing systems.
Written by
Solution Engineering Lead in the AWS open source observability service team
Stories around applying the Lambda Architecture for building large-scale data processing systems.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/hackernoon/cloud-computing-security-issues-and-how-to-meet-them-head-on-5a37a45f4759?source=search_post---------201,"There are currently no responses for this story.
Be the first to respond.
As McKinsey stated, the changes in how enterprises use technology have made corporate environments harder to protect while increasing the importance of their protection at the same time. When digital data becomes more extensive, businesses are expected to become more ‘open’ and connected, even though the cybercrime landscape evolves year after year.
Cost-saving, flexibility, mobility and security, to some extent, are forcing rapid cloud adoption. A focus on Platform as a Service (PaaS) and Infrastructure as a Service (IaaS) allows organisations to adopt smooth digital transformation and shorten the time to market. Supply chains are also becoming more sophisticated and interconnected. That means responsibility is shared with a vendor, but accountability is still on the business itself.
Deploying cloud technology inevitably implies a loss of control in some respects. When it becomes harder to know exactly what data you even own, asset management, vulnerability and incident management become more challenging.
As always, awareness of the challenges involved is key to reducing cloud computing security issues and compliance exposure:
Vendor risk. Before cloud adoption enterprises enjoyed expansive control over on-premise IT equipment, vendor risk was limited to firmware and software updates. Cloud implementations imply far broader vendor risk. Cloud vendors are responsible for everything from network security to regulatory compliance, and it is challenging for clients to verify the assurances put in place by vendors.
Data regulations. Enterprises that handle personal and financial data face stiff penalties for violating data protection requirements. Regulations such as GDPR mandate that data is controlled and protected to a high degree. It is easier to comply with data regulations when data is stored on-site or on equipment controlled by an enterprise. Data in the cloud is a different matter altogether.
Credentials and data leakage. Utilising the cloud implies that a wider range of parties will have access to enterprise data, from a wider range of locations. Controlling access rights become more difficult once physical barriers are removed, but handing data to an independent third party automatically implies a loss of control.
While it’s pretty easy to scale to the cloud from a technological perspective, proper governance needs to become a top priority, including compliance, risk management, vendor management, proper data classification, access control and change management.
It is impossible to extinguish all technology security exposures from a closed, on-premise computing platform and it is even more challenging to do so in a cloud environment. However, risk mitigation can be effective in reducing the opportunities for loss, harm or noncompliance to minimal levels.
Once your enterprise is aware of the unique cloud computing security issues and compliance risks that enterprise cloud computing poses, it can take mitigating actions:
Measure your risk exposure. Every enterprise adopts cloud computing in a different shape and form. Public clouds can be more cost-effective than a private or hybrid cloud, but they involve relinquishing more control over security and compliance aspects. Similarly, opting for Software as a Service (SaaS), instead of IaaS, combined with your own software, implies less direct control over the software environment. Choose the solution that matches your and your client’s risk tolerance.
Risk-profile your vendors. When using cloud computing, enterprises should remain vigilant against vendor risk. Consider questions around ownership, vendor sustainability and security practices. However, these questions should also be asked of the vendor’s partners as cloud risk management also implies managing risks at the weakest link.
Rapidly learn from failure. Cloud security breaches are constantly in the news, and in many cases, the attackers found brand-new exploits. Wise enterprises will rapidly learn from the mistakes of others and ensure cloud computing practices are quickly adapted to guard against rapid changes in the security environment.
Tightly manage user behaviour and credentials. The cloud is easy to use, accessible and open. Users should be educated in good security practices, while enterprise IT management should insist on inconvenient but effective practices such as two-factor authentication. Also, user credentials should be managed with extreme care: cloud credentials are effectively the keys to the premises.
Get to grips with data compliance. In taking advantage of the cloud, enterprises should strongly focus on the detailed terms of service and ensure that public clouds, hybrid clouds and SaaS/PaaS meet local and international regulatory standards. Understand where your data is stored and ensure that you only work with cloud vendors that practice the required compliance regimes.
From the compliance perspective, you will rely on your vendor’s capabilities to provide data security, resources and workloads. Make sure you’ve covered the essential aspects from your side as well:
“Remember, you’ll be limited when conducting cloud audit, and you’ll need to rely on the 3rd party opinion to verify the cloud provider claims. So, focus on really important audit types like SOC2 Type 2, ISO 27001, CSA STAR, FedRAMP, etc., and check the scope whether those cover all the components you need”, says Iurii Garasym, the Director of Corporate Security at ELEKS. “Also, run penetration testing, monitor the provider yourself where possible, request and check recent OWASP Top10 pen-test reports against their API and portal.”
Whether cloud computing security issues and compliance aspects should be managed using internal capabilities or indeed by a public cloud provider or SaaS operator depends on the level of internal expertise your enterprise enjoys, the type of data that is handled and the client environment you operate in. Regardless, deploying an expert in cloud security will be highly beneficial.
Cloud risks are unique and require evaluation by a partner that intimately knows how the cloud works, whether it involves developing cloud apps from scratch or migrating existing workflows to the cloud.
Contact us for an end-to-end review of your enterprise cloud security objectives; we can guide you towards maximum cloud utility with minimum risk.
Originally published at eleks.com on October 19, 2018.
#BlackLivesMatter
1 
1 clap
1 
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
Your technology partner for software innovation and market-leading solutions: https://eleks.com/
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
"
https://medium.com/@efipm/5-ratings-and-3-use-cases-in-the-complex-world-of-cloud-cloud-computing-in-banking-5559c282e2e3?source=search_post---------202,"Sign in
There are currently no responses for this story.
Be the first to respond.
Efi Pylarinou
Oct 30, 2020·5 min read
The complexity of migrating and adapting to the Cloud in Banking is a fact. What is worrisome is that the Cloud in Banking remains misunderstood, despite the fact that roughly two thirds of C-suite decision makers are bullish on the Cloud. In reality, a very small percentage of Banks has actually transformed their business by embracing the Cloud…
About
Write
Help
Legal
Get the Medium app
"
https://pieceofcakelabs.com/best-resources-to-learn-about-cloud-computing-d89a3b9ff072?source=search_post---------203,NA
https://medium.com/@alibaba-cloud/cloud-computing-and-cyber-security-paves-the-path-to-fourth-industrial-revolution-77fa15d4a43a?source=search_post---------204,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Jun 4, 2018·4 min read
By Kenny Tan, Malaysia & Thailand Country Manager at Alibaba Cloud
In my humble opinion, Cloud Computing is at the heart of the Fourth Industrial Revolution (4IR) in which technology emerges as the key enabler within societies. At the same time, Cloud Computing is also the driving force behind more productive and efficient business models.
What’s more, Cloud Computing drives down capital expenditure (CAPEX) on traditional infrastructure and empowers companies to concentrate on what truly matters — their business — safe in the knowledge that scalability is a thing of the past.
Traditional IT infrastructures was mainly about CAPEX whereby companies and organizations need to buy the necessary hardware to create their respective infrastructures. With the Cloud, it has evolved to operating expenses (OPEX), whereby it’s similar to the utilities model: Companies just need to pay for what they use; thereby enabling them to scale as and when the need arises.
Long and short of it all, Cloud Computing is accelerating time to value, driving higher adoption of new technologies and helping businesses innovate faster in an increasingly competitive world.
That said, Cloud Computing also comes with its challenges such as cyber security and in keeping users’ data safe and secure.
With corporate spying and digital theft are on the rise, leading Cloud providers such as Alibaba Cloud are constantly on the lookout and vigilant of potential cyber threats.
Cybersecurity is a top priority for Alibaba Cloud. We are dedicate to make our data centers and infrastructure more secure so as to constantly protect our customers‘ data and business models.
What’s more, there’re also other responsibilities that we have to look into: Such as international compliance and regulation, data governance and sovereignty — and the need to always respect the law of countries which we operate in.
At Alibaba Cloud, customers are our top priority and at the core of everything that we do. That is why we have grown so strongly in recent years as more and more customers transition to the Cloud with Alibaba Cloud, seeking collaboration to make them more productive while driving down IT expenditure and realizing greater return-on-investment (ROI).
Our growth speaks for itself: Today, we are the №1 Cloud provider in China, and among the world’s top 3 IaaS providers according to Gartner, making us a truly global Cloud provider.
As mentioned earlier, trust is the ultimate currency in business, and we also facilitate greater trust with customers through our world renowned services and consulting services to large enterprises. Our clientele includes enterprises involved in banking and financial services, and those with constantly high daily turnover.
AirAsia is one such company which relies on Alibaba Cloud to protect itself from Bots attacks, allowing for better customer experience and safer skies.
Ranked as one of the world’s best low-cost airline, AirAsia operates scheduled domestic and international flights to more than 165 destinations spanning 25 countries. Needless to say, it is heavily reliant on its online ticketing platform. Because of this, their ticketing platforms could be exploited by crawlers and botnet attacks, affecting both website and app performance but also potentially exposing customer data.
This was a (previous) business and operational challenge for AirAsia as it was difficult to distinguish legitimate customers from crawlers and bots which directly compromised airline bookings and revenue. Also, the AirAsia’s website and app experienced massive traffic flows as a result of bots which also led to additional operational costs.
To cut a long story short, AirAsia and Alibaba Cloud collaborated successfully to address this challenge. Alibaba Cloud’s CDN network in China enabled improvements in the latency and Round Trip Time (RTT) of the AirAsia’s website. Moving to Alibaba Cloud’s Content Delivery Network (CDN) also ensures traffic optimization for access based on user profiles.
As a result, AirAsia’s and Alibaba Cloud’s security teams worked together to block illicit traffic during the initial days, thanks to the extensive database of known hackers and their patterns. After thwarting high-level hackers and uncovering complicated hacking patterns, AirAsia and Alibaba Cloud security teams customized rules and applied them to Alibaba Cloud’s Web Application Firewall (WAF). The security teams also worked together to implement stricter security checks including custom built captcha solutions.
By working with Alibaba Cloud’s security professionals, AirAsia successfully identified that 90 percent of the traffic were botnets. Alibaba Cloud also provides weekly security reports and regular updates to AirAsia.
Combining the experience of Alibaba Security Team, the product capabilities and our world-class Managed Services; Alibaba Cloud successfully addressed AirAsia’s challenge and was the perfect answer to the market’s (previous) inability to adequately protect customer resources from botnet attacks.
As stated by AirAsia Group CIO Hogan Declan:
“The Alibaba Cloud team came not just as a infrastructure provider, but a true partner who is willing to understand and learn about our business. With the proactive and professional services provided by the team, they have made a real impact on our business especially on cloud security mitigation and proved themselves and their technologies. With the quick response and dedication of the Enterprise Support, we are comfortable to move more core application and innovation onto Alibaba Cloud.”
Similar to AirAsia’s tagline — “Now everybody can fly” — I would like to say this: “With Alibaba Cloud, enterprises can fly even higher and fulfill their innermost capabilities and deepest potential… the sky’s the limit!”
In closing, I would like to share my innermost thought: “It is helping customers Win that makes my job in Alibaba Cloud so fulfilling, and which gets me so excited to go to work every morning.”
To learn more about Alibaba Cloud’s services in Malaysia, visit https://www.alibabacloud.com/campaign/malaysia
Reference:
https://community.alibabacloud.com/blog/cloud-computing-and-cyber-security-paves-the-path-to-fourth-industrial-revolution_593713?spm=a2c41.11623113.0.0
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
1 
1 
1 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://blog.sabio.la/most-in-demand-tech-skill-of-2019-cloud-computing-e9eff032906a?source=search_post---------205,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Sabio was created to ensure that the greater Southern California region had the software engineering talent it needed to thrive and grow back in 2013. Over the past five years we have trained hundreds of full stack web developers and they are now working at amazing companies like MedMen, Microsoft and Amazon.
Our real mission; however, was to ensure that the tech skill-gap was eliminated, that thriving and growing companies throughout Southern California had dynamic employees needed to build amazing software that would be used by a global audience.
A new technical skill gap is upon us in 2019, as detailed by a LinkedIn 2019 Skill Gap report:
“The most in-demand hard skill of 2019 is Cloud Computing.”
That is why in late 2018 we secured approval from Amazon Web Services, the leading Cloud provider, to run their signature AWS Educate Courses at our various Southern California campuses.
To help people learn how to become awesome cloud professionals, we are hosting a fun and engaging Cloud Meetup on Jan 24th in Culver City, and we hope you can join us, RSVP here.
Sabio will always be about closing the hard-skill gap. Developing exceptional technical professionals ensures communities are strong and that California continues to be the leader in tech innovation.
Learn more about Sabio at any of our Info Sessions.
Hear directly from our fellows via our Course Report reviews.
Sabio's success stories and news
50 
50 claps
50 
Written by
Lead by the most senior coding bootcamp staff in the industry, we are the premier Software Engineering program in Southern California. #CodingBootcamp
Sabio's success stories and news
Written by
Lead by the most senior coding bootcamp staff in the industry, we are the premier Software Engineering program in Southern California. #CodingBootcamp
Sabio's success stories and news
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/heterogeneous-computing-for-ai-and-big-data-alibaba-cloud-computing-conference-3da843e5a75d?source=search_post---------206,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
May 16, 2018·3 min read
The heterogeneous computing technology is evolving rapidly in the field of big data and AI in recent years. To cope with this technology revolution, Alibaba Cloud Heterogeneous Computing has made great achievements in both product type and application.
Pan Yue, a senior product expert of Alibaba Cloud, made an in-depth presentation on Alibaba Cloud Heterogeneous Computing in the era of big data and AI at the Computing Conference Shenzhen summit on the morning of March 29, 2018.
Pan Yue introduced that Alibaba Cloud provides a wide range of “heterogeneous acceleration platforms for multiple scenarios”, including GA1 instance (AMD S7150) for graphic and image rendering, GN5 (Tesla P100) instance for AI training and reasoning, GN5i (Tesla P4) instance for AI reasoning and video transcoding applications, and GN6 (Tesla V100) instance for advanced computational capabilities in the fields of AI and high-performance computing, and multiple FPGA instances for image transcoding, genetic computing, and database acceleration.
GN6 instance is specifically designed for deep learning training and high-performance computing. With Tesla V100 in the latest NVIDIA Volta architecture, GN6 instance delivers 12X higher computing performance than Tesla P100 (the previous generation), solving the outstanding problems for engineers and experts. GN6 (Tesla V100) is now available for public beta and will be coming soon.
Alibaba Cloud has been and will be always professional and precise towards issues related to heterogeneous products. Most people may choose Pascal-Based Tesla P40 as the GPU because of its higher single-precision floating-point capability, which is 12 teraFLOPS (TFLOPS) while only 10.6 TFLOPS for Tesla P100, according to the product manuals. However, Alibaba Cloud performed tests in different scenarios and methods and found that the performance of Tesla P100 is 20% higher in AI application. Here are the test results:
In the main forum at the Computing Conference Shenzhen, Alibaba Cloud and NVIDIA reached an agreement to enhance their cooperation in the GPU ecosystem, including the setup and promotion of NVIDIA Deep Learning Institute (DLI) at Alibaba Cloud University and NGC container images, a new feature released for AI.
NGC is a deep learning ecosystem developed by NVIDIA, which allows developers to access the deep learning software stack for free and to establish a development environment suitable for deep learning. The developers can get various NGC software from the Alibaba Cloud marketplace, including Caffe, Caffe2, CNTK, MXNet, TensorFlow, Theano, and Torch.
FPGA is getting the recognition of the public and applied in the general computing field from the special computing field. Alibaba Cloud FPGA as a Service is an FPGA heterogeneous computing service that supports the mainstream FPGA platforms. It is designed as a systematic platform with uniform interfaces for computing capability output under the premise of ensuring cloud security.
It provides an integrated cloud development environment for more FPGA engineers to develop products on the cloud. Alibaba Cloud is still making use of the strengths of Alibaba and her partners to establish and improve an FPGA ecosystem with a view to serve more customers with better FPGA computing capabilities.
Currently, Alibaba Cloud FPGA as a Service can be found in image transcoding and compression, database acceleration, genetic computing, and other fields.
Alibaba Cloud heterogeneous platform for elastic computing aims to provide high-quality products and services, help organizations to realize scientific and technological innovation, and make computing and AI a true inclusive technology in a secure and stable environment.
Reference:
https://www.alibabacloud.com/blog/heterogeneous-computing-for-ai-and-big-data--alibaba-cloud-computing-conference_591173?spm=a2c41.11534526.0.0
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
1 
1 clap
1 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/swlh/applying-security-to-your-cloud-computing-c118e66577d1?source=search_post---------207,"There are currently no responses for this story.
Be the first to respond.
When talking about the benefits of cloud computing, the questions I hear most are those concerning cloud security. Is the cloud a safe place to store and interact with sensitive data? Should I be worried about hackers?
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@onlyincrypto/most-promising-cloud-computing-and-blockchain-storage-projects-12ff1d929f9b?source=search_post---------208,"Sign in
There are currently no responses for this story.
Be the first to respond.
Tony Simonovsky
Jul 19, 2018·5 min read
Blockchain and data decentralization have been penetrating all walks of life for quite a while, so it was only a matter of time before we were introduced to the first decentralized cloud platforms. There are many projects out there aiming to revolutionize the field. In this article, we come up with the most promising solutions, based on their execution, project positioning, community support, and media coverage.
Before we get down to the list, here are a few general patterns that describe the segment:
What each of these platforms does is, basically, rent out unused hard drive space. This somewhat resembles the Uber model where the stakeholders — drivers and passengers — make the utmost use of actual cars, utilizing their full potential. The same applies to hard drive space.
Once you are inside this ecosystem, you could also submit extra space for rental on the blockchain and gain some cash. A slew of blockchain platforms help you share unused storage, CPU resources or bandwidth with others — for a reasonable fee.
Storing corporate data in the cloud raises a bunch of security issues that cloud service providers have been unable to address in their entirety. It’s a question of trust and QoS declared by responsible third parties, and sometimes you just can’t put that much sensitive data at risk. A distributed approach helps mitigate those risks through the use of transparent blockchain technology.
Without further ado, let’s see what the market has in store for us.
Storj
Storj is the first solution that comes to mind when someone mentions decentralized cloud storage. It builds on Ethereum blockchain and offers a safe way to store data without using server farms. Storj employs spare space on users’ HDDs to distribute data globally. As expected, there is required encryption in place. Naturally, no single user can read or write other users’ data. The data is protected with personal keys and safe from prying eyes.
Storj also includes pro features just like Amazon S3, so enterprises may benefit from using the service at a lower cost compared to the traditional providers. Those who opt for Storj use smart monthly contracts to purchase or rent storage and arrange specific terms. Somewhat surprisingly, Storj partners with the big guys like Microsoft (namely, the Azure technology) to create decentralized apps leveraging its network. Smart move since this leads to even greater demand for Storj space.
Filecoin
Filecoin utilizes the same model as Storj with clear benefits to all potential stakeholders.
Take miners: they may have tons of unused storage that could be put to work. Filecoin offers this audience special ‘mining software’ to get remunerated for processing and fulfilling storage requests. Clients, on the other hand, have an opportunity to tweak their storage scenarios and achieve an optimal price-to-performance ratio. This way, the global Filecoin storage and retrieval markets complement each other.
Filecoin goes the extra mile to educate people about Distributed Storage Networks (DSN) and the algorithms like Proof-of-Replication (PoR) and Proof-of-Spacetime (PoSt). From the tech and business perspective, Filecoin has a lot in common with IPFS — they share ‘adjacent’ technologies and even some core developers. IPFS, in its turn, aims to replace HTTP and construct a better web space by transferring colossal data volumes with higher performance and efficiency.
MaidSafe
MaidSafe is a startup blockchain storage provider similar to IPFS. Like many projects of this flock, MaidSafe sees its goal in implementation of SAFE (Secure Access For Everyone) — a secure, decentralized, distributed next-gen network. SAFE is yet another name for the same concept of employing redundant hard drive space, power and connectivity offered by users worldwide.
The idea behind MaidSafe is to offer users proprietary tokens to unlock the platform’s services. For instance, users may back up their data or host sites building on the MaidSafe Network.
The venture is now at an early stage, yet it looks promising. The latest release encapsulates more secure network access bundled with the SAFE Browser.
Golem
Golem is one more IPFS sibling. This ‘creature’ is an open sourced, decentralized, universally accessible HPC entity. As with competing solutions, Golem accumulates the combined power of user computers — from laptops to huge datacenters. Anyone who signs up will be able to compute pretty much anything from simple web code up to rocket science research in a decentralized manner, at a reasonable cost. Golem is now in beta, it works well and offers a decent UI. Worth checking out.
GridCoin
An odd fish in the decentralized pond, GridCoin is an open-source blockchain computing project based on the BOINC platform. BOINC is a grid computing system engineered by Berkeley. The system did a lot of things, such as decoding signals gathered with radio telescopes. Due to its legacy, GridCoin centers around scientific research aided by volunteers from the entire world. As such, the project professes philanthropic rather than commercial values.
Users contributing to the accumulated computer power in the network earn GRC tokens. GridCoin builds on a consensus algorithm called “Proof-of-Research” which remunerates the contributors according to their tangible scholarly achievements and valuable research conducted.
GridCoin users have a chance to contribute computing resources to a plethora of subjects: astronomy, biology, math, physics, and whatnot. It empowers discovery of new medicine, study of space signals, resolving protein structures… Ain’t it a great application of electricity compared to, say, mining?
Now, back to blockchain storage providers, let’s consider the commercial aspect. All in all, decentralized storage makes a great bang for the buck. At a certain point in 2017–early 2018, you could spot distributed storage offerings 10x cheaper than traditional cloud storage. It’s due to this trend among others that Google and Amazon had to review their pricing models to become more competitive. As of early Q3 2018, blockchain storage still has a price edge over the competition, although it’s nowhere close to the sticker-shock of the recent past. Compare Amazon S3 Standard at $0.023/GB/month with Storj at $0.015/GB/month.
Regardless of today’s rates, we need to admit that decentralized storage is gaining a strong foothold and it’s here to stay. Investors should really keep an eye on the rising stars in the field, and the industry juggernauts should probably limit their appetites.
Digital nomad, entrepreneur, blockchain enthusiast. I help companies raise money through token sales.
1 
1
1 
1 
1
Digital nomad, entrepreneur, blockchain enthusiast. I help companies raise money through token sales.
"
https://medium.com/cloud-academy-inc/9-great-job-sites-to-kick-start-your-career-in-cloud-computing-b8a1534dda9e?source=search_post---------209,"There are currently no responses for this story.
Be the first to respond.
It’s a great time to be in the cloud computing industry. As established companies move toward cloud technologies, companies expand IT departments, and new companies form, IT professionals with cloud computing skills are in high demand. Computerworld’s 2017 salary survey showed that, compared to all IT professionals, those working in cloud computing were commanding the highest compensation, at an average of $129,743.
In this post, we’ll look at some of the most common cloud positions that companies are hiring for and where to look for your next cloud computing job.
If you’re looking to start a career in cloud computing, these are some of the most common roles that require cloud skills.
Engineer is your general purpose engineer for building features and fixing bugs in any type of application. There are entry, mid, and senior level cloud engineer positions open across the industry.
Data engineer is the gateway to business insight. Data engineers build the data infrastructure to answer business questions. There should be entry, mid, and senior level opportunities in growing teams.
The system administrator maintains various IT systems. This may cover system provisioning, configuration, maintenance, and decommissioning. Also, this position may be undergoing the most rapid change of the bunch.
The DevOps Engineer and Site Reliability Engineer (SRE) are two sides of the same coin as they work on improving the quality in the final product. Both touch all phases of the software development process and act as force multipliers for the rest of the team. These are in-demand positions, and the requirements are changing fast.
The architect is your IT city planner. They make the big decisions on what goes where, how to prioritize each area, and what the shape of the final solution looks like. It requires years of knowledge to answer these kinds of questions and bridge the gap between business and tech.
These are just some of the general types of positions that you will find. However, as you’ll likely discover in your job search, the actual positions are not always so neatly defined. It’s likely that there will be a lot of overlap between these positions in many companies. This is especially true the smaller a team is. It’s simply not possible to specialize in a team of three. However, a team of 300 is a completely different story, so be prepared to change hats and move between different technical domains in your daily work.
If you’re already working in one of these roles, the odds are that your existing job is not going away if your company is moving to the cloud. It’s more likely that your responsibilities will change to match cloud architecture. Commonly, this means a move toward more programming, automation, and abstraction. This means that you will want to be prepared to gain some new skills and lose some old ones.
Learn more about these positions and the major cloud computing platforms in our learning path, Considering a Career Cloud Computing?
Now that you know what types of roles are out there, it’s time to start looking for the job that matches your experience and goals. Here are some of the sites to consult in your job search.
The Institute of Electrical and Electronics Engineers (IEEE) JobSite offers featured positions and a searchable database for jobs and internships in engineering and technology, as well as links to upcoming career fairs and webinars.
ComputerWork, now part of JobServe Limited, has been a leading IT employment website since 1995. Refine your search with salary or rate criteria or search by skill and role.
In addition to helping startups raise money and apply for funding, AngelList also a great resource for finding jobs in the startup world. AngelList matches candidates with employers. You’ll need to create a profile get started, and you can privately send your profile to the companies you’re interested in. If they’re also interested, you’ll be able to continue the conversation via email or through AngelList. It’s 100% free.
Glassdoor.com is a great place to search for jobs and see what current and former employees have to say about their employers. You’ll also have access to company-specific information such as salary reports, interview questions, and more.
Women in Technology International (WITI) is a leading association for helping women advance in all sectors of technology. Search all technology jobs, get a free resume review, and connect with the association through networking events.
LinkUp posts jobs exclusively from company websites and offers up-to-date, high-quality listings. Search their database, automate with alerts, and save listings to apply later.
StackOverflow Jobs is part of the StackOverflow online programmer community that lists high-quality developer jobs. Browse by technology, salary, location, company, and more.
Dice.com offers a searchable database of tech jobs, and includes information on salaries, trending skills, and career resources.
The Muse is a career resource that offers not only a searchable jobs database, but career support. Browse jobs by companies, trending, job title, and location. With the Muse, you get a closer look inside the companies themselves so that you can see what it’s like to work there. Check out their resources for making a career change, networking, creating your resume, and more.
Now that you know where to look for your next job in cloud computing, it’s time to start brushing up on your skills! Whether you’re starting from scratch in IT, want to bring your existing skills up to date, or would like to add experience with some of the newest cloud services to your resume, Cloud Academy is your source for the most up-to-date cloud training available.
Explore our Content Library of courses, hands-on labs, quizzes and learning paths for AWS, Microsoft Azure, Google Cloud Platform, DevOps, containers, and more.
Originally published at cloudacademy.com on June 19, 2017.
Cloud Training Has Never Been Easier.
3 
3 claps
3 
Cloud Training Has Never Been Easier. Continuous self-paced training for all cloud technologies.
Written by
The proven platform to assess, develop, and validate hands-on technical skills.
Cloud Training Has Never Been Easier. Continuous self-paced training for all cloud technologies.
"
https://medium.com/@jovansh/six-major-advantages-of-cloud-computing-ff8301a86f1?source=search_post---------210,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jovan S Hernandez
May 4, 2020·5 min read
The year is 2020 and the State of the Cloud is strong. Some might say not only is the state of the cloud strong, but it’s also more present and critical than ever before.
Thanks to the Coronavirus, the world has seen a massive shift to adopting SaaS and Cloud services overnight. Many companies who took the initial steps…
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@RohitAkiwatkar/cloud-computing-lets-keep-it-simple-5402c246be66?source=search_post---------211,"Sign in
There are currently no responses for this story.
Be the first to respond.
Rohit Akiwatkar
May 11, 2016·3 min read
Sometimes a buzz-word becomes so popular that it’s fundamental meaning becomes vague. Such is the case with ‘Cloud computing’. Cloud computing is the general term for anything that involves delivering hosted services over the Internet. But many people are confused by the concept of cloud computing.
This post summarizes the concept of cloud for everyone to understand. As stated by Google — Cloud computing is the practice of using a network of remote servers hosted on the Internet to store, manage, and process data, rather than a local server or a personal computer. In the simplest form cloud computing can be explained by Google account (Google drive). Everyone uses Gmail to write e-mails, G-docs to save drafts, G-sheets to calculate. These are the examples of Software as a Service (SaaS) model of cloud computing. Thus using Google docs (hosted remote Google server) instead of MS-Word (personal computer) for writing the blogs is one of the examples of cloud services. So practically a 13-year-old child is capable of using the cloud services.
For an organization, ‘moving to the cloud’ refers to changing traditional model of dedicated hardware to shared cloud infrastructure and pay as you go. As explained in the blog ‘5 Reasons Why Cloud can Transform Your Business’, adopting cloud for businesses can result in cost efficiency, increased storage space, scalability, and lean management. It also summarizes the role of cloud consultants.
There are three service models offered by cloud-computing providers. Through a comprehensive feasibility, analysis businesses can choose which model fits their needs.
Cloud computing in the form of IaaS provides virtual machines which involve storage spaces, firewalls, load balancers, IP addresses, virtual local area networks etc. Prominent players in this segment are Amazon Web Services (AWS), Google Compute Engine, Rackspace and Microsoft Windows Azure.
Platform as a service (PaaS) model is simply a layer over IaaS. Cloud providers deliver a computing platform, typically including the operating system, programming language execution environment, database, and web server. Salesforce, Microsoft Azure, and Google App are widely popular examples of PaaS.
In SaaS, users are provided access to application software and databases already on the cloud. The ‘on-demand software’ such as email (Gmail, Yahoo) and social networking sites (Facebook, LinkedIn) provide users the access to application software and databases.
In a nutshell, the difference between the three comes down to how much of the cloud service the provider manages vs. how much the consumer manages.
The cloud has evolved from a technology initially adopted for efficiency and cost savings, to an innovation powerhouse. It’s safe to say that cloud technology will continue to drive transformation, as well as healthy competition. Thus cloud is not just for technical people anymore and it’s is crucial for businesses in all the sectors.
Rohit Akiwatkar is a part of Maruti Techlabs team. To read more about Rohit please check his LinkedIn profile. To learn more about Maruti Techlabs please visit www.marutitech.com or check its LinkedIn page
Tech consultant Simform. Sharing my views on emerging technologies Cloud, DevOps, Serverless. Also a fitness enthusiast and avid reader.
See all (61)
2 
2 claps
2 
Tech consultant Simform. Sharing my views on emerging technologies Cloud, DevOps, Serverless. Also a fitness enthusiast and avid reader.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@rww/how-to-simplify-complex-business-processes-with-cloud-computing-tools-6c72f8f3bb0a?source=search_post---------212,"Sign in
There are currently no responses for this story.
Be the first to respond.
ReadWrite
Mar 12, 2021·7 min read
Most businesses turn to the cloud to save money and improve efficiency. Cloud migrations offer businesses the opportunity to simplify functional business processes and their IT architecture.
Rather than spending money on initial software development and maintenance, companies turn to cloud-based infrastructure solutions to outsource the responsibility for upgrades and management.
Companies that utilize the cloud can expedite internal processes and make customer data points more accessible from anywhere in the world.
Organizations using legacy on-premises applications must take steps to modernize their IT and implement cloud computing tools and systems.
The market is changing dynamically. Companies are moving to the cloud to remain competitive, scale operations, and accelerate growth.
Cloud computing tools benefit the organizations that use them in quantitative ways to provide insights on how to improve their businesses.
Some of the processes that are simplified include:
Data integration involves combining data from various sources for analysis, reporting, and business intelligence.
Cloud data integration tools can be accessed through a web browser and are hosted on a third-party provider’s server.
Below are some popular cloud-based data integration tools that enable businesses to integrate data from multiple sources quickly:
Incomplete or inaccurate data has a significant negative effect on your company’s bottom line. Integration tools help companies maintain quality data while enabling them to get the most out of growing data volumes.
Cloud monitoring tools track safety, performance, and availability of critical cloud services and apps. These tools monitor, manage and evaluate infrastructure, services, and computing architecture.
Below are several of the more popular cloud monitoring tools:
Companies use cloud monitoring tools to identify emerging defects to prevent minor issues from growing into more pervasive problems.
Anyone who uses the Internet knows how difficult it is to remember passwords for every site they use. Many people try to use the same password or variations of it because it is easier to remember.
However, this practice often results in weak passwords and poses a security threat. How can you use a unique and strong password on all the websites, systems, and applications you use?
Unless you have a photographic memory, it is impossible to remember the vast number of passwords needed to access almost every site online.
For businesses, using a password manager is a critical security feature. It will:
Rather than having to remember way too many logins for business processes such as cloud storage, Google Drive, and collaboration sites, a password manager will remember your passwords for you.
When using a password manager, users only need to remember one password to the manager itself. Although modern browsers have built-in password managers, they are not secure.
For example, Chrome stores master passwords in an unencrypted form. Hackers can gain access to your master password and then get into all your password-protected sites.
When choosing which password manager to use, avoid the services that allow you to retrieve your master password. Even though it is a pain to reset your master password, the consequences will be much more severe if it is compromised.
In addition to not having to remember tons of passwords, using a password manager also protects from some types of phishing attempts.
While you might not recognize a clone site in an email, your password manager will by not auto-filling your login information.
Skimping on security tools is not a wise option, so many businesses today find that they are spending a lot of money on security.
You may be tempted to skip this step, but recent hacks reported in the news should persuade you against it.
The statistics above are scary when you think about the havoc a cyberattack can have on your business. This is driving companies to increase their spending on security tools.
However, many small businesses cannot afford to pay for expensive security tools, nor do they have the time and resources to understand how to implement them.
There is a new tool called DevSecOps that combines existing cloud computing tools and integrates security to simplify the process.
DevSecOps combines two methodologies into a single unified framework. It ensures faster coding by focusing on automating code production and streamlining efficiency. SecOps deals more with security issues.
By utilizing both frameworks, DevSecOps enables teams to streamline development processes and security together.
Instead of implementing security processes after code has been written, DevSecOps increases coding and production efficiency and streamlines the process by focusing on security from the beginning.
If security issues occur early in the development process, they can be caught and attended to before software is released to customers.
The benefits to security teams and developers of adopting DevSecOps far outweigh its complexity and the time and resources necessary to implement it.
Cloud optimization is the process of selecting and assigning the best resources to an application or workload. Achieve better efficiency by balancing the best-fit infrastructure against compliance, cost, and workload performance.
Below are several cloud computing resource optimization tools to minimize performance risks and maximize savings:
There are considerable differences in each of the tools’ capabilities. Some only report on underutilized instances, whereas others can automatically execute precise rightsizing using AI-based pattern recognition.
The ability to scale your business can be inhibited by legacy business processes that were appropriate when implemented based on the business climate and technologies available.
Adopting cloud computing enables companies to accelerate the deployment of new capabilities to maintain a competitive advantage.
By untangling legacy business systems, moving to the cloud also helps businesses improve their functional processes.
Cloud computing tools and systems enable businesses to replace inefficient processes and respond to market conditions faster.
How to Simplify Complex Business Processes with Cloud Computing Tools was originally published on ReadWrite by Julie Weishaar.
The latest #news, analysis, and conversation on the #InternetOfThings
See all (1,540)
3 
1
3 claps
3 
1
The latest #news, analysis, and conversation on the #InternetOfThings
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/cointiger/cointiger-launches-ipfs-filecoin-cloud-computing-sale-phase-1-together-with-dasheng-venture-capital-cd807805dd84?source=search_post---------213,"There are currently no responses for this story.
Be the first to respond.
Fellow CoinTiger users,
We are excited to announce that Dasheng Venture Capital IPFS·Filecoin Cloud Computing Sale Phase 1 will be launched at 2020/07/08 18:00 (UTC+8)（Users who purchase computing power in this campaign will be eligible to participate in and share the IPFS official 4 million FIL testing reward program activities on July 20th）, Details as follow:
Campaign Duration: 8 July 18:00–13 July 18:00, 2020 (UTC+8)
Campaign Requirements:
Each person is limited to 40 shares, and 1TB computing power contains 1000 shares
Each 1TB computing power = 300 USDT + 200 MKC giveaway
Buyers should scan MKC wallet’s QR code before the sale, and buyers are required to register MKC wallet with your mobile number that is used to register the CoinTiger account.
MKC wallet link (Please open it on the phone) : https://1.mkc.hunanmaster.com/mobile/Login/login
Participation Steps:
1）Fill up the form.
2）Prepare adequate USDT in your account.
3）Deduction will be made in number order when the campaign ends at 2020/07/13 18:00 (UTC+8)
4) Buyers should scan MKC wallet’s QR code before the sale, and buyers are required to register MKC wallet with your mobile number that is used to register the CoinTiger account.
Product Brief:
Product name: IPFS·Filecoin Cloud Computing Sale Phase 1
Proof of equity: Using equity for each computing power is permanent, and they belong to Dasheng Venture Capital mining pool. Settlement can be applied when cloud computing power reaches 192T in according to national standard quality.
Technical service fee: 20% (deduction will be made on the day if we take Filecoin or POC as a unit)
Distribution: Daily delivery (expected to start on the 8th day after the mainnet launch)
Yield expectation: It is expected that the daily output of a single T will be 0.2FIL-0.5FIL / day / TB, and the annual output of a single T will be 73FIL-183FIL. MKC cloud computing power and comprehensive profits for other high-qualified POC currency will be sent for free as the attachment. (The impact of fluctuations and mining difficulty might be affected by actual conditions)
MKC Introduction:
MKC is a token of cloud computing power based on ERC20 smart contract development and relying on Dasheng Venture Capital’s IPFS·POC mining pool computing power as the core. MKC has the following effects:
1.Each TB cloud computing power corresponds to each TB real enterprise hard disk storage space
2.After MKC is launched on Filecoin’s mainnet, MKC can be used to exchange for the IPFS cloud computing power of Dasheng Venture Capital
3.After MKC is launched on Filecoin’s mainnet, MKC can be used to lease for the IPFS cloud computing power of Dasheng Venture Capital
4.Dasheng Venture Capital’s POC mining pool will distribute MKC dividends according to MKC holding and mining amount
Risk warning:
1. The output based on IPFS cloud computing power will be affected by the fluctuation of Filecoin price and mining difficulty. The IPFS cloud computing power leasing service cannot make capital or currency-based capital preservation and refund commitments to users. Please carefully assess your risk tolerance and participate within acceptable limits. Dasheng Venture Capital IPFS Cloud Computing Rental Service reserves all rights to interpret this product and agreement.
2. The user understands and accepts that if changes in objective conditions such as the formulation or modification of relevant national laws, regulations, and regulatory documents lead to the suspension or prohibition of Filecoin mining, this contract is automatically terminated and the two parties shall not pursue each other. Liability, the resulting losses must be borne by yourself, and the fees paid by the user are not refundable.
3. This product does not involve digital asset transactions. If users participate in digital asset transactions on their own, they should bear their own responsibilities and risks.
CoinTiger team
2020/7/8
About CoinTiger
CoinTiger platform has an ecosystem that includes web, iOS, Android and exchange service system (e.g Cryptocurrency, FIAT, Future, and Leveraged Tokens Trading, Ticker Capital, Lab, Security Alliance, Tiger Knights, IEO, Tiger Forum), and is now one of the most popular exchanges in the world, which formed sub-branches in countries such as Korea, Japan, Russia, Italy and more. CoinTiger is founded in November 2017 and has exceeded 3.5 million users today. CoinTiger also provides trading and investing services for 158 countries with over 100 quality assets. CoinTiger is also the first crypto exchange to implement a 100 % equity mechanism for its native TigerCash (TCH) coin. The first thing CoinTiger has done is to serve its users well and deliver secure, efficient, and reliable services with the highest level of global ecosystems, banking security, efficient and best possible product experience, easy and fast transaction services, and a 7X24 customer support team.
Social Media Handles
WebsiteFacebookTwitterMedium BlogReddit
Telegram Channels
English中文群Tiếng ViệtРоссиеюIndonesianالعربية日本語Türkçe한국어NigeriaहिंदीবাংলাBrasilคนไทยPhilipinesFrenchItalianAnnouncement
Kakaotalk
http://open.kakao.com/o/gwCm6SVb (Korean)
CoinTiger APP Links
Official Link: https://www.cointiger.com/en-us/#/app_download
Android: https://play.google.com/store/apps/details?id=com.cointiger.exchange
CoinTiger is a global and innovative crypto asset exchange…
10 
10 claps
10 
CoinTiger is a global and innovative crypto asset exchange that provides multi-crypto-currency trading services in multi-language to blockchain enthusiasts.
Written by
CoinTiger is a global and innovative crypto asset exchange that provides multi-crypto-currency trading services in multi-language to blockchain enthusiasts.
CoinTiger is a global and innovative crypto asset exchange that provides multi-crypto-currency trading services in multi-language to blockchain enthusiasts.
"
https://medium.com/@alibaba-cloud/interview-with-powermew-how-cloud-computing-big-data-and-vr-is-transforming-the-real-estate-8d1796bda50c?source=search_post---------214,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Jan 16, 2018·14 min read
“Heroes create the trend, and the trend transforms others into heroes too,” remarked Eddie Chan, CTO & Co-founder of PowerMew.
In comparison with other companies in the real estate sector, PowerMew differentiates itself from the competition by utilizing the latest technologies, including cloud computing, big data and VR, to transform the experience of property transaction for buyers, tenants and real estate agents.
How is PowerMew leveraging new technologies to boost their business? And as a latecomer in a mature market, what competitive advantages have they gained from frontier technologies? These are the questions we posed during our interview with Eddie Chan at the 2016 Alibaba Cloud Computing Conference. In this interview we take a deep dive into the inner workings of PowerMew and find out from Eddie how their cloud architecture is transforming the industry.
Transcript of the interview is as follows:
Q: Eddie, to start our interview do you mind please sharing with us PowerMew’s elevator pitch?
A: PowerMew Technology Limited provides a revolutionary virtual reality experience for exploring apartments. With its social networking platform called MewMe (http://www.mewme.com), we believe that it will become the next “LinkedIn” in the real estate industry to connect and communicate between proprietors, buyers, tenants and estate agents. MewMe refers to the combination of “a cat purring”, “mobile estate web” and a “private space” for users. Applying the latest 360-degree camera technology, every corner of the flat can be reached and displayed to users in 4K resolution. MewMe provides virtual tours of different apartments so that the user can visit from anywhere in the world.
Q: What inspired you to start PowerMew?
A: Real estate is one of the most important pillars of Hong Kong’s economy. According to statistics made available from the government, there are over 500,000 active landlords and tenants in Hong Kong. Specifically, in 2015, there were 4,009 real estate agency companies (with EA-License), among which around 99% (around 3,900 real estate agency companies) were small or middle-size firms. The majority of these companies are small firms and individuals. In terms of market share, the four biggest real estate agency firms dominate the property market with 70% of the total market share. This makes the real estate market in Hong Kong unhealthy.
Distribution of Real Estate Agents in Hong Kong
In Hong Kong, the real estate agency business is still operating in a traditional model where potential buyers approach the well-known agency companies. In 2015 there were 40,000 real estate agents. Around 17,000 agents, or 43% of the total number of real estate agents, belonged to the four biggest real estate agency firms, including Centaline Property, Midland Realty, Ricacorp Properties and Hong Kong Properties. The rest either are either small firms or operating as individuals.
A growing monopoly exists in the Hong Kong real estate business. Small firms and individuals find it difficult to survive, which may finally lead to a complete monopoly. Every large real estate agency firm has their own property database. Although the database is comprehensive, they are not open. Therefore, due to a lack of information and resources, small firms and individual find it difficult to survive. Small firms find it difficult to negotiate deals with more than three district areas. Because information is possessed only by the big firms, other players in the real estate business, including landlords, tenants, sellers or buyers, cannot make wise decisions based on accessible and comprehensive information.
Higher stamp duties to cool Hong Kong’s property market have affected the real estate agents’ livelihoods also. Sales of both first-hand and second-hand homes have dropped significantly. In July 2013, Hong Kong real estate agents organized a protest against measures designed to cool the property market. Major domestic agencies and international property consultants are also diversifying their businesses. However, many small firms and individual agents (around one-third) were forced out of the market. Many agents will be coming out from the large firms and shifting to other businesses as well. A small portion of agents may join small agency firms or turn themselves into individual agents to earn higher commissions and gain more time to handle their self-employed business. It is a golden opportunity for the MewMe social networking platform to react to these changes. Real estate agents will find our mobile platform very useful to attract customers.
Total Outbound Investment from China from the Year 2009 to May 2016
PowerMew itself has expanded business to Greater China. We believe there is a pressing need for Chinese people to buy overseas properties. In 2015, Chinese investors spent over USD $27 billion to buy U.S-based properties. The figure keeps on rising each year and has grown at the rate of 600% between 2010 and 2015. Our office is located in Hong Kong, a highly internationalized city close to the Mainland, and has great advantages in trading, logistics and financial services which play a key role for connecting potential Chinese investors to the global real estate market. With our MewMe platform, overseas developers and agencies can utilize VR technology to provide highly transparent information and attract potential Chinese investors to explore the property investment market in foreign countries.
Q: What sets PowerMew apart from your competitors? And what unique problem do you solve?
A: We have carried out substantial research on our competitors and based on the combined strengths of our team, we deliver superior technologies.
First, there is no social networking platform (technology) for the real estate market. The existing products are not transparent and clients have only limited communications. In contrast, we offer a social networking platform, which connects all stakeholders inside the real estate industry.
Second, their search engines apply only traditional queries in relational database systems and do not provide any further analysis, whereas our system applies Big Data technology to implement a powerful and fast search engine for both property analysis and property information.
Third, there is no VR technology available in the Hong Kong property industry for property inspection, which means buyers have to spend a lot of time and effort on visiting properties they are interested in and filtering out properties that are not suitable. Fortunately the filtering time and process can be greatly reduced with the VR technology provided on our platform. Buyers can inspect and get a feel of the property instantly via our tools and services.
Finally, not all existing products provide relevant information about overseas properties, including laws and tax regulations. Our platform fills in the blanks and provides information to investors to inform their decision-making.
Chinese investors tend to like investing in stocks, precious metals and properties. According to a research completed in 2015 by the CBRE Asia Pacific Research Team, there has been a surge in the number of Chinese nationals investing in properties in such places as the U.S, Japan, Taiwan and Russia. With continued globalization, the economic growth of a local real estate market is no longer restricted to its local market by limited information. Economies are now open to foreign investors.
Chinese investors though face the following issues when trying to invest in overseas property markets:
1. Poor understanding about the real estate market conditions. Deals may be mispriced. The price and condition may be inaccurate, and even worse, the property might not even exist!
2. Time-consuming and unsafe site visiting experience. It takes a lot of time to travel to foreign countries to view the properties, and it could all be in vein if the actual conditions of the properties are not suitable for investment. Visiting properties or letting strangers guide you through a foreign city may also jeopardize your personal safety.
3. Limited resources on overseas properties. There are limited channels for Chinese investors to collect information on overseas properties. The personnel who are providing the information may not even be qualified. The Chinese investors are also unable to verify the correctness of the information they have been provided.
On account of Hong Kong’s status as a large international hub with a strong international network, we could become the intermediary between China and other countries. These common issues are due to a lack of transparent information in the locations and regulations of different countries. To counter this, we extend our social networking platform from a local scope to the global scale, which will allow property agents and potential investors in different countries to join our platform. We also provide a VR experience for potential buyers and tenants to gain a more realistic feel for the properties. The objectives are:
(a) To enable users to virtually immerse themselves into the real estate properties without travelling aboard
(b) To establish a social network and ontology for connecting all stakeholders around the world in the real estate business
© To enable better price estimation and quality assurance for a massive database using big data technology
(b) To allow the users to access real estate information and co-related services around the globe seamlessly
Q:As an intelligent social platform in the estate industry, what is your definition of ‘intelligent’? How do you implement ‘intelligence’ into your business solution?
A: We define “intelligent” in terms of ACE, “A stands for analytical suggestions”, “C stands for convenience” and “E stands for efficiency.”
Let’s talk about “Analytical suggestion”:
We have just launched our application “MewMe” this year (May 2016), but we have collected a lot of big data that consists information on 2.8 million Hong Kong properties, 400,000 transaction records and 2,000 social media contacts. A lot of useful information can be extracted from the big data we have collected. First, we can have a better understanding of the market price and trends. Big data analytics can substantially improve decision-making and minimize risks when buying/renting properties. Big Data technology allows ever narrower segmentation analysis of customers and therefore much more precisely tailored agency services.
“Convenience”:
Buying or renting a property has never been easier, or more fun. With the help of MewMe’s VR solution, investors/potential buyers/tenants can simply put on the VR goggles with a smartphone, and then walk around the virtual space of any property available in the world instantly.
“Efficiency”:
90% of time can be saved in travelling if the user can view the property via VR goggles in advance. Real estate agencies could spend less time and money to collect the property and clients’ information or to advertise their properties. Landlords could post their properties for sale and rent instantly. All users could share text, pictures and property information via MewMe’s messenger app. MewMe is more than a real estate database, it is an intelligent social platform which can help all users to find the place they want.
Q:You have mentioned that you are using cloud computing and big data technologies, can you describe how you leverage them to power your business? What competitive advantages do you gain from these new technologies? Do they alter your business model?
A: We use a traditional Chinese philosophy to answer all these questions, “Heroes create the trend, and the trend transforms others into heroes too.” Cloud computing and big data technologies are widely adopted in many different business domains. It is a MUST that we adopt these two technologies to our business.
Let’s talk about how the cloud computing technology affects our business. As a startup company, money is a critical issue for success. If we do not have any cloud provider solutions, we have to spend enormous amounts of money to buy expensive hardware servers, which somehow, for a startup, it is difficult to evaluate how powerful our server configuration are going to be. With the support of cloud computing services, we can cut down the cost of hardware overheads and we can focus on the real development of our platform, rather than spending endless time on connecting wires, installing server software and worrying about data security.
We also need entry to the China market. For us Alibaba Cloud is the №1 choice. We know Alibaba Cloud has an excellent track record with regards to reputation, performance and innovation.
Since launching MewMe in Google Play and the App Store in May 2016, the number of users has raised steadily. Given the growing number of MewMe users, we have been using Alibaba Cloud’s Server Load Balancer (SLB) to cope with user traffic efficiently. The public facing SLB automatically distributes incoming visitor HTTP requests to multiple Elastic Compute Service (ECS) web servers. SLB also minimizes response time and improves the user experience. The web and application servers are deployed on ECS instances, which are excellent for a startup because ECS is based on the web application’s real-time demands. In addition, Alibaba Cloud ApsaraDB for RDS takes automatic backups of data to provide data redundancy and partition data to achieve scalability, which saves the customer time and optimizes resource use. ApsaraDB for RDS is integral in underwriting data security, as well as the high level of built-in security in other Alibaba Cloud services, including identity and access management at resource and network level. The MewMe platform stores and backups static resources and content (e.g. images) on Alibaba Cloud Object Storage Service (OSS). OSS also enables us to archive large amounts of data on the cloud.
Alibaba Cloud Content Delivery Network (CDN) delivers static content with a global network of edge locations so that visitors from anywhere can access the MewMe platform with quick load time. It also helps us to effectively shorten website response times to milliseconds, ensure smooth image browsing and handle large volumes of traffic. Moreover, MewMe utilizes Alibaba Cloud Security Service and Alibaba Cloud Monitor to monitor the health of MewMe deployment and get instant updates and alerts. The cloud security features employed by Alibaba Cloud minimize the threat to sensitive information and make MewMe a more reliable platform.
Now let’s talk about ‘big data technology’:
Basically, we have developed a real estate search engine that allows one to search for all required property information to complete a property deal. However, a successful real estate agent should also know the property trends based on past real estate transactions. Every transaction record will generate a tremendous amount of data with time and location information, which we refer to as spatio-temporal data. As the size of the data is continuously growing, it will outgrow the capabilities of any serial processing techniques and it is therefore necessary to perform the data analytics in parallel. More specifically: 1) We implement a Hadoop-based storage system for big spatio-temporal property data analytics. The Hadoop-based storage system speeds up search time by avoiding the need to scan the whole dataset when a spatio-temporal range is given.
2) We implement a novel data model, which has special treatments on three core attributes including a property ID, a location and a transaction time. Based on this data model, it can hierarchically partition data using all core attributes, which will enable efficient parallel processing of spatio-temporal range scans.
Unfortunately in Hong Kong, for the big data model, we do not have any solution available from Alibaba Cloud. So we would love to hear and transit our system to Alibaba Cloud once the big data service is available.
Summary of technologies PowerMe is utilizing
Q: What challenges have you encountered using cloud computing? And how have you dealt with them?
A: We have encountered technical challenges with regards to changing different types of frameworks and database systems. For a startup company, we always have a “trial-and-error” process for adopting new technologies. It’s always a painful and time-wasting experience when we need to reformat or reinstall configurations on our own servers. But now, owing to the flexibility of cloud technology, we are able to setup any OS server, adopt different frameworks and install any non-SQL database system with a few clicks. It is a piece-of-cake now to setup our instant messaging system for real time communication among our users. Text and multimedia messages can be sent and received in the instant messaging system, which allows dynamic interactions between agents and their clients, regardless of their physical locations.
Thanks to Alibaba Cloud’s ECS, MewMe provides a personal profile system in which professional users can post updates on currently for-sale/for-lease properties. Any user can easily follow multiple personal profiles to be notified of the latest information posted. And professional users can conveniently share relevant information to a large audience with a few simple clicks. We believe this can provide a new instant channel of communication for agents and their clients, and it can enhance the efficiency of communication.
With OSS and CDN technologies, MewMe can provides seamless and flawless 360-degree photo streaming services to our customers. Customers can now look at a lot of properties in one place without travelling, in other words, it helps to increase productivity and reduce the time to close a deal.
Q:In your opinion, what is the biggest deciding factor when it comes to choosing a cloud provider for a SaaS company?
A: Two points: Performance and Extendibility
Performance is always our №1 priority to choose a cloud provider. It really is life-and-death decision for your business. Loading content instantly is that moment of truth when you can fall in love with your application.
Extendibility is about how the cloud provider can provide their coverage of the services and help our business to expand quickly. We realize that Alibaba is the world’s largest retailer and Alibaba’s annual Global Shopping Festival on China’s Singles Day is a perfect example, showing that Alibaba Cloud has the ability to handle IT problems, with up to 5-hundred million users! So we now grasp their technology and have expand our business to Greater China.
Q: What is your roadmap for development over the next 1–3 years?
A: We are developing a VR social network for real estate businesses to enhance the transparency of property information and enable users to view properties (including Hong Kong overseas properties) instantly, which in turn helps to save the time for travelling and seeing the actual properties.
There are still no such VR social networks for the real estate business in the world. We have already developed an immersive virtual reality system, which enables motion and gesture interactions. We will build a function for live VR video streaming and make use of it as a new way to share property information.
We plan to set up a database to store, organize and manage the virtual reality data of the properties around the world. Meanwhile, we apply the graph theory and implement an algorithm to calculate the shortest distance to match interested clients with properties and establish an extendable social network for the property industry, which connects to other co-related services or social networks. For example, agents could interact with their clients via multimedia instant messaging and their personal profile pages on the MewMe platform.
Initially, MewMe covers Hong Kong and it is expected that in 2020 MewMe will cover Mainland China, Macau, Taiwan, UK, America and Europe.
Conclusion
As a new player in real estate market, PowerMew is gaining giant competitive advantages from frontier technologies. With Cloud Computing, PowerMew is able to provide services in the global market without borders, and reduce IT costs at the same time. Due to big data, PowerMew can deliver intelligent suggestions to property buyers. In addition, by integrating VR solutions with property transaction platform, PowerMew makes it possible for users to view properties instantly without travelling abroad.
Reference:
https://www.alibabacloud.com/blog/Interview-with-PowerMew---How-Cloud-Computing-Big-Data-and-VR-is-Transforming-the-Real-Estate-Industry_p65222?spm=a2c41.11149733.0.0
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
1 
1 clap
1 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/serverless-the-next-decade-of-cloud-computing-614690769d0e?source=search_post---------215,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Oct 21, 2020·9 min read
Catch the replay of the Apsara Conference 2020 at this link!
By Alibaba Cloud Serverless
Moore’s law summarizes the experience that the performance of processors in the integrated circuit field doubles every two years. The experience seems to apply to the technological evolution in the cloud computing field.
Ten years ago, UC Berkeley predicted that cloud computing would flourish. Customers pay for cloud resources on-demand without building their own data centers, just like in leasing mode. This reduces the investment of enterprises in IT. With rich product support on the cloud, cloud computing accelerates the launch and iteration speed of the business and promotes the core competitiveness of enterprises. Cloud computing ensures the stability and security of cloud-based businesses.
The benefits of cloud computing include high economy, efficiency, and openness. Various market analysis reports indicate that the cloud is gradually replacing the traditional IDC market.
We are always curious about the future and excited about the transition from old to new.
In 2015, AWS launched Lambda, and the word “serverless” first appeared in public. Subsequently, Alibaba Cloud launched Function Compute in 2017, and the serverless application engine SAE and serverless container service ASK in 2019. UC Berkeley again predicted that serverless would replace serverful computing. As a result, serverless has attracted extensive attention in the industry.
Serverless brings the benefits of cloud computing to extremes: the highest economy, efficiency, and openness. It reduces idle resources to zero through an event-triggered mechanism. Serverless operates just like a signal light, which consumes power only when people approach and automatically go off when people leave. This realizes real elasticity. At the same time, serverless eliminates the need to care about the O&M of the IaaS layer (servers).
Some may ask whether serverless has application restrictions because everything has two sides. Zhao Peng, a Senior Technical Expert at Ant Group, asked this question to Charlie, the Chief Analyst of Forrester, and Zhang Xin, the Founder of ZStack.
Charlie: “Forrester has a long and in-depth study in serverless. In summary, serverless is applied to three types of scenarios. First, serverless provides hyper-large-scale support. When enterprises in the finance, education, medical care, and manufacturing industries use cloud platforms to build businesses, serverless technology can be used to support services with high workloads. Second, serverless is super-dynamic. Consumers interact with services provided by enterprises through digital or physical touchpoints. The mode and frequency of interaction will test the dynamic support capability of a platform. This makes serverless a good choice. Third, serverless provides ultra-high-efficiency support. Developers can implement hyper-large-scale and super-dynamic business needs in a more efficient way.”
Zhang Xin: “We must have a consensus on the application scenarios of serverless: Serverless is not a complete technology stack, so it is not a silver bullet that can solve all problems. Serverless is a supplement to cloud-native technology and can serve as glue. Serverless is an event-driven architecture, which triggers resources by events in the form of code snippets to decouple business in the cloud-native system and reduce cloud costs.”
In addition to the restrictions in application scenarios, serverless is still in the early stage of technology exploration because it is different from container technologies that have unified de facto standards and a rich ecosystem. Does serverless cause being locked to a specific vendor and which container techniques it is associated with?
Henry (Founder of the Harbor Project): “Serverless is like a very simple version of PaaS. Its business implementation only requires writing a small amount of code or the core business logic. All the rest are processed by the platform. Code running and service calls are performed by the platform. This means that code written for a serverless platform cannot run on another platform. Of course, open-source projects, such as CloudEvents, that create unified event formats can improve the interoperability between services on different platforms and promote the standardization of serverless platforms.
Containers and Kubernetes have become the mainstream techniques and operating platforms for cloud-native applications. Running serverless loads on Kubernetes implements rapid startup and auto scaling. The two techniques will complement each other and achieve joint development. In summary, serverless technology is playing a more important role in the development of container technology. In addition, the requirements of serverless technology will be increasingly considered in the development of container technology.”
Alibaba Cloud Serverless Expert Yang Haoran answered the question of being locked to serverless technology vendors. “We believe that serverless will be vendor-free. Alibaba Cloud Function Compute deeply integrates the open-source container ecosystem and gives full play to their respective advantages. Alibaba Cloud EventBridge supports CloudEvents to unify the cloud event standards. We will also launch a more open developer tool chain to help users employ function services on various platforms at lower costs. We want to promote the serverless implementation in China in the form of a community.”
Working with open-source communities and manufacturers to promote the development of the serverless ecosystem will make serverless technology more economical, efficient, and open-ended.
To ensure the maturity and stability of commercial techniques, vendors must be at the forefront of practicing their techniques and actively sharing their practical gains.
Ten years ago, Alibaba Group applied microservice technology to e-commerce scenarios and continuously optimized it. Then, Alibaba Group pushed the open-source release of microservice frameworks and components, such as Apache Dubbo, Apache RocketMQ, Nacos, Sentinel, Seata, Spring Cloud Alibaba, and ChaosBlade. They greatly enriched the Java ecosystem and accelerated the implementation of microservices. Today, the entire Alibaba economy is practicing serverless technology, including Taobao, Tmall, Alipay, DingTalk, Fliggy, Xianyu, and Yuque, to extend the serverless application scenarios to fields, such as frontend full-stack, mini programs, microservices, new retailing, and gaming entertainment.
Xianyu is one of the active explorers of serverless.
Wang Shubin (Xianyu Architecture Expert): “Xianyu is a bit like Taobao and also has e-commerce properties in terms of business characteristics. On the other hand, Xianyu is an independent app that has its own closed traffic loop and user characteristics. Therefore, in terms of technical architecture, most of Xianyu’s basic capabilities (in the lower part of the preceding figure) depend on the e-commerce business systems of Alibaba Cloud and Taobao due to the similar attributes. These basic capabilities are the cornerstone to support the rapid iteration and development of Xianyu. Xianyu needs to have innovative and fast R&D capabilities to meet its business needs due to its unique features. These basic capabilities are the key to the rapid development of the architecture.”
The Flutter and HTML5 cross-platform frameworks release productivity. In many scenarios, terminal personnel can develop the BFF layer directly on the server based on serverless technology to form a cloud-server integrated R&D model. This significantly reduces server-cloud collaboration because collaboration is one of the key factors that deteriorate R&D efficiency.
What scenario is appropriate? The first scenario is the interaction-intensive pages. For example, on the orders page, prices need to be recalculated when users change the order address, transaction method, or discount method. This produces high cloud-server collaboration costs. The c integrated model can be used to make interaction and data logics unified. The second scenario is the activities and programming pages. Such pages can be launched quickly and have lightweight logics on the server. These pages are mainly used for data assembly and suitable for integrated programming. These scenarios are the most applicable. There will be more lightweight scenarios as server developers improve the function computing capabilities.
In addition to integrated services, more glue services are provided by server developers. These services are developed on traditional applications, which encounter large, conflict, and coupling issues. We can use the serverless architecture to split traditional applications, so developers can focus on their business, achieve fast delivery, and significantly improve R&D efficiency.
In addition to Wang Shubin of Xianyu, Shimo Docs Serverless Expert Wan Ming, Timing Microservice Architecture Expert Zheng Chao, and Director of the Lianhua Huashang Information Center Wang Jianzhong are also exploring serverless technology.
Wan Ming: “We use serverless to change and merge tables and to compute table rendering results. Serverless improves development efficiency and reduces computing costs in CPU- and memory-intensive scenarios.”
Zheng Chao: “We do not have an O&M team. With the automatic elasticity of SAE, within seconds, we can easily and dynamically scale services to handle high and low traffic. This not only guarantees the SLA of the system, but it also saves about 35% of the hardware costs compared with the previous ECS retention method.”
Wang Jianzhong: “We transformed the core online business into a serverless mid-end model, which uses “Function Compute, API Gateway, and Tablestore” as the computing, networking, and storage cores to elastically support resource needs during daily hours and major promotions. The project cycle is reduced from six months to three months, and the resource upgrade cost was reduced by about 100%.”
Enterprise customers, such as Shimo Docs, Timing, and Lianhua Huashang, can quickly use serverless technology in business applications thanks to the rich serverless products and solid underlying techniques of Alibaba Cloud.
Ding Yu (Alibaba Cloud Intelligence Researcher): “Alibaba Cloud provides a wide range of serverless products, including Function Compute, application-oriented SAE, Serverless Kubernetes for container orchestration, and ECI for container instances. They constitute the most complete serverless product matrix among all current cloud vendors. Behind these serverless products are the four core techniques of Alibaba Cloud infrastructure: the X-Dragon architecture, sandbox container, Apsara Distributed File System, and Apsara Cloud Network System. They provide a solid cornerstone and strong core competitiveness for serverless technology.
Perfect serverless products are accompanied by complete backend cloud services and rich developer tools. For example, the newly released Event Bridge can easily connect cloud services, cloud applications, and SaaS applications, to further accelerate the end-to-end integration of Alibaba Cloud serverless products. Serverless Workflow uses a visual function orchestration model to easily construct complex tasks, such as media processing, machine learning pipelines, and automated O&M processes.
In addition, Serverless-tools and Serverless Application Center were launched to build a more open and standard serverless community with no vendor binding. Container images and performance instances are a combination of the container ecosystem. It overcomes the minimum operating unit of functions and makes it easier for developers to use. One-step cloud migration and one-click serverless transformation may come true.”
Looking at the figure at the beginning of this article, software technology has gone through the phases of physical machines, virtual machines, cloud computing, and container technology. The truly hands-free serverless is coming soon. For the question of what the trend of serverless will be in the next decade, Alibaba Cloud Senior Technical Expert Situ Fang gave this answer:
Situ Fang (Alibaba Cloud Senior Technical Expert): “Over the next decade, serverless will be open, standard, and free of vendor binding. Serverless will be closely integrated with the cloud-native ecosystem. Serverless will be able to be integrated with server application in a simple manner and support complex business at low costs.”
Catch the replay of the Apsara Conference 2020 at this link!
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
1 
1 clap
1 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/what-is-serverless-cloud-computing-anyway-ffe4cfe7517b?source=search_post---------216,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Feb 26, 2020·3 min read
Exploring some of the ideal use cases, costs and infrastructure behind serverless cloud computing
The conventional view of cloud computing is that of taking existing servers out of your own data center and hosting them elsewhere. In that scenario, the basic building block is the cloud server, which might run Windows or Linux and host a website or two, email, a database engine, and so on.
You’ve probably also heard mention of serverless cloud computing. But how can that be? Where do you host your website, database and so on if you haven’t first created one or more virtual servers?
Serverless cloud computing is provided by Alibaba Cloud within a product set known as Function Compute. This actually provides a useful clue as to how the concept actually works. The basic building block is no longer a server but a block of code, which is known as a function. The function is executed by an event known as a trigger. This can be an API call, a call to a particular URL, a timer reaching a predetermined value or the appearance of a new file in a directory, for example.
Functions still run on cloud servers, but those servers are part of Alibaba Cloud’s fully-managed core infrastructure. You don’t need to build, patch, secure or pay for your own servers. Instead, you simply create and upload functions, and then create triggers which cause them to run.
Function Compute is massively scalable, without the need for you to implement any load balancing or contingency planning. Whether a function gets called once a month, or a million times a minute, the underlying infrastructure will take care of it.
Billing is straightforward and easy to budget for. The cost of calling a function is made up of three components: request, duration, and an optional Internet traffic fee.
Request fees are USD $0.20 per million. The duration component is measured in gigabyte seconds, and is calculated by multiplying the time your function takes to run by the amount of RAM you allocated to it. For example, a function which runs for 0.1 seconds and is allocated 0.5GB of RAM will incur a cost of 0.05 gigabyte seconds. Gigabyte seconds cost $0.00001668, which is around 6 cents per gigabyte hour.
As an Alibaba Cloud account holder, your first 1 million Function Compute requests and 400,000 gigabyte seconds each month are completely free. Therefore, you could trigger around 32,200 function calls per day, each allocated 0.4GB of RAM and running for 100 milliseconds, and pay absolutely nothing. This compares very favorably with conventional cloud servers, especially as there’s no need to expend additional time and effort securing, patching and updating your own VMs.
If your function generates traffic on the public Internet, this is charged at between $0.07 and $0.13 depending on the region.
Function Compute currently supports python, node.js, Java, C# and PHP. And while it doesn’t necessarily replace all aspects of your cloud computing requirements, there are clearly some situations which can transfer very well. For example, when a user uploads an image to your website, this could trigger a function which compresses the file in order to save on storage costs. Other ideal uses for Function Compute include web crawlers, data and log analysis, automated backups, and implementing APIs. Coupled with OSS storage, Function Compute also makes a great back end for IoT devices and mobile apps, especially as it’s so scalable and cost effective.
You can even implement an entire serverless website. Create your content (static HTML files or database-driven) and upload the files to an Alibaba Cloud Object Storage Service (OSS) bucket. Then create triggers which retrieve and display the relevant content in response to a particular URL being accessed. You can find a complete step-by-step tutorial on how to do this at https://www.alibabacloud.com/blog/create-a-serverless-website-with-alibaba-cloud-function-compute_594594 .
To learn more about Function Compute, see https://www.alibabacloud.com/products/function-compute .
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
3 
3 claps
3 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/cloud-simplified/what-are-the-security-risks-of-multi-cloud-computing-19b73716ea6b?source=search_post---------217,"There are currently no responses for this story.
Be the first to respond.
by Harold Bell
As if there weren’t enough security risks in the realm of traditional IT, the cloud presents some unique vulnerabilities. “The cloud,” although a deceptively singular phrase, is many things and comes in many forms. From infrastructure (IaaS) and platforms (PaaS), to software-as-a-service (SaaS) offerings. Each new service introduces equally new challenges in terms of both integration and security. And keep in mind, that’s just the cloud landscape in a singular sense. What about organizations with many public, private, and/or hybrid clouds? The emergence of Amazon Web Services (AWS), Microsoft Azure, and Google Cloud Platform (GCP) has given choice to this space. As a result, multi-cloud architectures are introducing new challenges to already complex security models. Leaving cloud operators and admins alike scratching their heads in terms of developing the right security posture. So, without belaboring any further, this blog seeks to answer — what are the security risks of multi-cloud computing?
Before jumping into multi-cloud security, let’s consider what security is and why it’s important. One of the most simplistic ways to describe security is that it’s basically insurance. Which is true to a point. Insurance is one of those services that you buy, hoping that you never have to use it. Likewise, rarely do organizations implement security programs because they want to, but rather in anticipation of a breach. The key point here is that a proper security program is proactive. It actively helps organizations avoid threats to their data, employees, and reputation. An organization that takes security seriously can turn it into an asset that enables the business. This means viewing it as more than just a part of a risk management portfolio.
IT security professionals only have so much influence in the data integrity discussion. With that in mind, organizations of all sizes need to implement robust data governance models and policies. Data governance defines exactly what data an organization has, how it’s used, how it’s managed. These organization-wide frameworks also outline in which systems and locations authoritative data is housed. As well as identify who has access to it and other meta properties.
Different divisions using data differently can wreak havoc on their organizations by making decisions on the basis of different assumptions. The security policies and frameworks you create for your single location or multi-cloud-centric company can’t be developed in a vacuum. You have to engage a broad group of stakeholders. Your security policies will intersect these other efforts and will both inform them and be informed by them.
Access management is one of the most critical areas of cloud security and organizations need to plan very carefully to ensure a sound security posture. A Dow Jones Customer Intelligence Study found that “68% of executives whose companies experienced significant breaches in hindsight believe that the breach could have been prevented by implementing more mature identity and access management strategies.” Unfortunately for you, even with an effective process in place, privileged credentials alone are often not the solution. A large number of breaches involve privileged credentials. Which indicates that once a boundary is breached, via an employee with a lower level of access potentially, the credentials of someone with administrative access is eventually obtained. One defense against this is the use of multi-factor authentication. You will see a greater infusion of authentication apps for mobile devices in the coming year.
To ensure the security of your infrastructure and data, it is important that you design a strong RBAC (role based access control) strategy along with a directory service to manage centralized access. Build policies to ensure that your staff has the least privileged access for what they need to access. Enact need-based access policies so employees get access to specific resources only for a limited time and access expires after a certain duration. Perform regular audits on a quarterly or yearly basis, according to your business requirements, to ensure that only valid users exist in the system.
Remember, IT no longer has full control over the provisioning, de-provisioning and operations of the cloud infrastructure. This decentralized ownership has increased the complexity for IT teams to provide the compliance and risk management policies required to protect their businesses. IT needs to find new ways to exert soft controls to protect the business, while not inhibiting the agility their stakeholders expect now from the cloud.
If you work with data in any capacity, it’s likely you’ve heard someone say, “Security is everyone’s responsibility.” And to be honest, that’s a true statement. We’ve obviously just established how vital an individual’s impact can be when it comes to cloud security. That is exponentially amplified in a group setting. In order for a business of any size to stay protected against the growing threat landscape, everyone has to maintain some level of awareness and do their part in fending off bad actors. As with all things IT though, there is a bit more nuance. Different people sometimes have different ideas about where to draw the line on getting a job done and making sure that security is front and center. Too many people believe that moving to the cloud releases them from certain obligations such as backups and security. Their rationale is that the cloud provider handles these functions.
This is dangerous thinking that can leave you exposed. In the world of cloud, shared responsibility means that your organization retains security responsibility for certain aspects of the environ-ment while the cloud provider handles other aspects. The provider ensures that its service is secure. The provider makes sure that the services that you’re consuming are protected, including the hardware and the software. You, however, retain significant responsibility for the applica-tions and data that you’re operating in the cloud, including secur-ing the operating systems, networks, and firewalls that support your applications. Can you imagine the chaos that would ensure if large public cloud providers were forced to control everything clients did just to make sure that it remains secure? The cloud provider secures what it provides, but you’re responsible for the rest.
If you’re not confident about your security posture in any of these areas, don’t fret. Nutanix is here to help. We’ve recently partnered with Wiley Publications to produce the Multi-Cloud Security for Dummies Ebook. We offer knowledge and stories to aid in your quest to provide your organization with the best possible security posture. The ebook will help you secure your multi-cloud environment while enabling the activities that make your business successful.
© 2019 Nutanix, Inc. All rights reserved. Nutanix, the Nutanix logo and the other Nutanix products and features mentioned herein are registered trademarks or trademarks of Nutanix, Inc. in the United States and other countries. All other brand names mentioned herein are for identification purposes only and may be the trademarks of their respective holder(s).
Originally published at https://www.nutanix.com.
An inclusive approach to cloud management.
1 
1 clap
1 
Written by
We make infrastructure invisible, elevating IT to focus on the applications and services that power their business.
An inclusive approach to cloud management.
Written by
We make infrastructure invisible, elevating IT to focus on the applications and services that power their business.
An inclusive approach to cloud management.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@nutanix/cloud-computing-in-2017-an-op-ed-from-the-cloud-geeks-b3b1ee63a2cc?source=search_post---------218,"Sign in
There are currently no responses for this story.
Be the first to respond.
Nutanix
Jan 10, 2017·6 min read
Digital transformation has changed the way organizations work, and so has the cloud. Following along the lines of VMWare’s Ex CEO Paul Martiz, cloud geeks across the globe have been saying it loud now ‘Cloud Computing in 2017 is about how you do computing, not where you do computing.’
Forrester, in one of its recent report, says, “Cloud computing will continue to disrupt traditional computing models at least through 2020. Starting in 2017, large enterprises will move to cloud in a big way, and that will supercharge the market. We predict that the influx of enterprise dollars will push the global public cloud market to $236 billion in 2020, up from $146 billion in 2017.”
While these numbers are enough to validate that ‘Cloud is the New Black,’ it also sends out clear signals that it is imperative to take the right measure and shove in the right strokes to get the most from your cloud computing in 2017.
In 2016, we saw that many enterprises failed to achieve success with cloud computing, especially public cloud. For the reason that they failed to develop a cloud strategy rooted in the definition and delivery of IT services linked to business outcomes. More so, they missed out leveraging the real benefits of elasticity feature that a cloud offers. They purchased instances in bulk to handle peak demands, like how they did with on-premise IT infra, and then turned a blind eye towards idle resources that could be optimized easily.
They also overlooked the fact that ‘anything and everything’ on the cloud can be codified and APIs can be made use of to automate the tasks on the cloud completely. Essentially, to go the NoOps way while on the cloud.
So, 2017 is the year where you put these in perspective and introspect how you can align these in your cloud strategy so that IT are seamlessly linked to business outcomes. Here’re few tips from our tech geeks on what to focus in the cloud for 2017:
Every business has its own ideas on how best to determine cloud ROI. However, they will have to think beyond Capex and Opex to get the cloud economics right. Our cloud geeks say that to get maximum ROI of your cloud, the first step is to establish the right policies, and closely monitor as well as regulate the resource usage every day. By bringing in a discipline with the right policies and budgeting, you can easily govern the costs. Plus, automating the tasks to monitor and streamline the cloud spend continuously will definitely help bring down the TCO.
There’s a myth that has remained with many companies even today — public or hybrid cloud present compliance challenges, unlike private clouds where control and customization are much easier. With the increase in the adoption of cloud, things have changed. Increasingly cloud service providers are open to dialogues when it comes to SLA, and also provide services that comply with PCI DSS, HIPAA, and other regulatory requirements. Another block that many of our customers are concerned is noncompliance due data’s location. It is simple. Locate your data, and during an audit, justify its location along with the measures that are in place to protect it.
While compliance and assuaging DDoS attacks plays a major role in cloud security, an API-driven strategy that puts all things in the right place at the right time also plays an equally important role. Automation is the future. And to get there, APIs are the keys to unlock the door. By automating security, it helps you code in the practices that make your data comply with your company’s security policy, identify vulnerabilities on running instances, and further fix those vulnerabilities in split seconds.
In this age of IoT, many are skeptical about the security of these connected devices talking on the cloud. At the end of the line how much ever a technology advances to help protect our networks and devices, security is ultimately a shared responsibility on the cloud. To know more, read the Botmetric blog on ‘Bridging the Cloud Security Gaps: With Freedom Comes Greater Responsibility.’
Serverless architectures have already taken the cloud computing by the storm. With the amount of interest leading cloud services providers are showing, especially AWS, Azure, Google, and IBM, it will be the theme of 2017. So, DevOps teams need to be hands-on in choosing the right services and be nimble. One observation we made during 2016 was that there is a common misconception in the DevOps community that going serverless is NoOps. It is not! DevOps team still need test, deploy, log and monitor the code.
Machine learning, and its subset Deep Learning, is now no more restricted to just the Big Data applications. It is slowly seeping into the walls of DevOps sitting on the cloud to help them improvise IT operations. Especially to minimize human intervention. More so, applying machine intelligence to problem-solving will be a norm very soon. For instance, it will come in handy to fix the alerts flood & monitoring fatigue caused by a company’s operational management systems in this 24*7 uptime world. Read this Botmetric blog Assuage Alert Fatigue Mess with DevOps Intelligence to know how machine intelligence can help solve this issue.
Even though this technology is in a quiet nascent stage now, it has made its footprints felt in the cloud computing. From our observation, 2017 will be the year, it will be used voraciously by many cloud management and SaaS products, where in they will offer data-driven ‘Bots’ that can automatically capture and interpret existing data, manipulate it, and trigger responses and do more intelligently and smartly. In short: Gear-up yourself to embrace this gen-next of deep learning.
While RPA, machine learning, automating security, etc. are making strides towards efficient cloud computing, NoOps will be the next wave of efficient cloud computing in 2017. Soon automating all the operations will be the norm. Building cloud as a code, with just the Dev team and investing Ops team into development efforts & innovation, will be the way forward. Are you game?
In 2006, AWS created the first wave of cloud computing. A decade later, AWS has again created the second wave. Apart from the fact that it is currently operating at an $11 Billion USD, it has also announced 50+ services. This is helping change the landscape of enterprise cloud computing. As an AWS Technical partner, here’re few tips from team Botmetric that you can take into account in 2017:
1. Use Lambda@Edge to deliver a low latency UX for customized web applications, and more so, run code at CloudFront edge locations without provisioning or managing servers.
2. Integrate Blox, the new open source scheduler for Amazon EC2 container service.
3. Leverage AWS CodeBuild and AWS Elastic Beanstalk without fail.
4. Buy Convertible RIs and leverage Regional Benefit to get the most out of EC2s.
5. Use new features available in S3 Storage. For instance: Object Tagging, S3 Analytics, Storage Class Analysis, S3 Inventory, and S3 CloudWatch Metrics.
According to a leading survey, Microsoft Azure accounts to 28.4% of the global IaaS ecosystem and is quickly catching up with AWS in the race to tap this growing market. With its growing portfolio of services (with support for machine learning, DevTest Labs, Active Directory, Log Analytics, BotService, etc.) and continued patronage among Microsoft aficionados, the Azure PaaS is also gaining ground, especially among enterprises.
With Google accelerating its move to cloud data warehousing and machine learning, it is also up for the race with AWS and Azure stealing 16.5% share of them from the IaaS market.
At Botmetric, we talk about cloud computing and DevOps everyday. Not just with a bunch of clients, but with other cloud geeks in the ecosystem too, closely observing the industry trends and practical challenges cloud engineers face every day. To this end, we have developed a close-knit collaboration with other partners and seek to make cloud management a breeze for all using automation. Hence this write-up, and we hope that we have thrown some light on the trends that will reign the cloud computing in 2017. Let us know if we have missed on something here. Plus, do share your view on the state of cloud computing in 2017 on Twitter, Facebook, & LinkedIn.
We make infrastructure invisible, elevating IT to focus on the applications and services that power their business.
1 
1 
1 
We make infrastructure invisible, elevating IT to focus on the applications and services that power their business.
"
https://rominirani.com/what-is-the-definition-of-cloud-computing-1b226105a599?source=search_post---------219,"This blog post was original published at the following URL : http://www.xoriant.com/blog/cloud-computing-for-isvs/cloud-computing-the-definition.html
Cloud Computing now finds a way through most technical discussions. Irrespective of the medium (Web Search, Twitter, Online Journals), you will find Cloud Computing being discussed. But ask anyone the definition of Cloud Computing and you will be hard pressed to get two exact definitions from different people.
Some define it as servers available for rent to storage or applications that we access from the browser, etc. All of them are right in ways. But is there a definition that describes the essence of Cloud Computing. While there might be various definitions of that, we shall look at one of the definitions of Cloud Computing in this blog post. It is known as the 5–3–4 Formula.
The 5–3–4 Formula is further broken down into the following:
5 key characteristics
The key characteristics are:
3 delivery models
Cloud Computing is typically delivered in 3 models and each one builds on the other
4 deployment models
The 4 deployment models available are given below:
Typically, the public cloud is what is best known to most of us. While classification does exist for other types like private, hybrid (mix of public/private) and community — they are not that prevalent and no clear classification exists. So for all practical purposes, when we refer to the cloud, it is public and with appropriate authentication and access control mechanisms built in.
So the next time, someone asks you to define “Cloud Computing”, you can simply say “5–3–4”.
Technical Tutorials, APIs, Cloud, Books and more.
30 
30 claps
30 
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
Technical Tutorials, APIs, Cloud, Books and more.
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
Technical Tutorials, APIs, Cloud, Books and more.
"
https://medium.com/@fedakv/how-to-choose-a-cloud-computing-model-right-fc04031df739?source=search_post---------220,"Sign in
There are currently no responses for this story.
Be the first to respond.
Vladimir Fedak
Jan 23, 2020·8 min read
At some point in the business lifecycle, a decision about cloud migration must be made. Many startups are cloud-based from the start, while many SMBs and global enterprises are still reluctant to move to the cloud, as they have invested heavily in building an on-prem or dedicated server infrastructure. Besides, many business stakeholders are still not sure what the cloud is and how to use it right. This article explains this, lists the cloud approaches available and helps choose the right cloud computing model for your business.
Traditionally, the businesses stored their data and ran their workflows on dedicated servers, which were either self-hosted on-prem or rented in dedicated data centers. While this approach seemed to be the most secure, it ultimately posed multiple challenges and limitations both to software development and IT operations in general. The data could be quite easily stolen, corrupted or simply lost in a fire. This is why when Amazon, Google and Microsoft started to lease their computing resources based on subscription, many businesses hopped on the opportunity to use this “cloud” hosting.
What is the cloud, first of all? Cloud is the umbrella term, covering the services of data storage, service delivery, application development and deployment to production using the cloud service platforms like AWS, Google Cloud, MS Azure, DigitalOcean and others.
In terms of hardware, cloud data centers contain the same dedicated server racks, that are combined into clusters. The key difference lies in the software field, as all the cloud resources are united in a single pool through the technology called virtualization. However, it is different from traditional virtualization technologies, where every physical server hosts a bunch of virtual machines.
In the cloud, all the hardware resources available are combined and split into virtual machines (so-called instances) using Docker containers, Kubernetes and Terraform, as well as a variety of other tools. This helps save a ton of resources, as there is no need to run a separate master OS on each bare metal server — all the servers of the cluster are governed at once. Every cloud platform provides these services under different titles — AWS Elastic Compute 2 for running instances, AWS S3 for data storage, Google Kubernetes Engine for running infrastructure with Kubernetes, etc. — but all of these services are enabled using the tools listed below.
Terraform — an open-source infrastructure orchestration tool from HashiCorp. It creates an inventory of all the resources available and allocates the resources required for the tasks at hand. Think of it as of your PC disk drive file system, which manages the disc space and enables all the operations on this computer. The key part here is that not only all servers in a rack are combined, but all the racks in a data center are also combined, as well as all the data centers in some local geographical vicinity.
Kubernetes — an open-source configuration management tool originally developed by Google Cloud Platform. It is now freely available on most cloud platforms (provided you have the expertise required to configure and run it correctly) and is also provided as a paid service for container management. Kubernetes ensures the efficiency and transparency of data and application management in production, as it enables automated container creation and deletion on request, security, monitoring, Continuous Delivery pipelines and much, much more.
Docker — an open-source tool for creating and managing containers — lightweight code packages containing all the required runtime to launch and operate your application. Due to this approach, a single hardware server can host dozens of containers, depending on its capacity, and waste no resources on running multiple virtual machines for them. This provided 300% virtual infrastructure performance increase and revolutionized the IT industry.
There are many more tools used in the cloud workflows:
All of these tools integrate with Kubernetes and the list is far from full. The point is, cloud computing providers like AWS, Google Cloud or MS Azure provide the full stack of technology required to design and run end-to-end software ecosystems that help any business reach their objectives.
What benefits does cloud computing provide, as compared to traditional dedicated servers?
Thus said, cloud computing is literally the best kind of investment, ensuring security, scalability, cost-efficiency, and high-availability of your IT operations.
Thus said, there are several cloud computing models that fit various business needs:
Thus said, every business can benefit from using cloud hosting and choose the right cloud computing model for their project. The point here is to have sufficient expertise to configure the systems and workflows correctly. We would be glad to answer any questions on the topic and provide assistance with cloud computing configuration. If you need such services — let IT Svit know, we are always ready to help!
Originally published at https://itsvit.com on January 23, 2020.
DevOps & Big Data lover
1 
1 
1 
DevOps & Big Data lover
"
https://medium.com/@Technews/alibaba-all-set-to-push-in-the-cloud-computing-domain-695bf44b46fd?source=search_post---------221,"Sign in
There are currently no responses for this story.
Be the first to respond.
Tech News
Jan 28, 2016·3 min read
Alibaba Group Holding operates several online market places where it offers Business to Business (B2B) and Business to Consumer (B2C) services. The people of China buy and sell their desired product on these platforms. From being an e-commerce behemoth, it also started to develop its business in the cloud computing sector. The company wanted to be known for something else than its core retail business.
According to sources, it is believed that the Alibaba Group is all set to take a major step forward in the cloud computing industry. The firm will be starting a big data cloud platform in the coming times through which it plans to do its major business. The newly developed cloud business will develop data asset technology available all across the boundaries of China.
The cloud computing division of Alibaba, AliCloud, stated “the cloud ‘Big Data Platform’ would offer an initial 20 products or solutions and services, which would cover all aspects of the so-called data development chain. This includes data services and visualization products that assist with data processing and analysis, but also provide a compute engine with capabilities for machine learning.”Alibaba is working hard to improve its cloud computing segment. A couple of days ago, the company joined hands with Nvidia Corporation in order to work together for the first ever GPU based high performance cloud computing platform in the country. If sources are to be believed then Alibaba’s recent push in the market will see a massive jump in revenue. Soon, the cloud computing services will worth over $1 billion of the company’s revenue by the end of 2018.
AliCloud will invest nearly $1 billion on various Big Data cloud technologies for companies. The president of the subsidiary, Simon Hu, stated at the launch in Shanghai, “The Big Data Platform fulfills our vision of sharing our vast data troves that will create immense value to our users. What we want to do at Alibaba is turn the data-processing capacity and data-security capabilities that we’ve accumulated over the last decade into a product, so that data becomes a resource and a service that we can provide our clients.”
Mr. Simon Hu further added that its company will follow a vigorous expansion plan in the Asian region alongside maintaining more or less the same pace for the growth in Europe and Middle East areas as well. For that matter, Alibaba is opening its second data center in Silicon Valley to not only meet its rising demand in cloud industry but compete against the likes of top name cloud businesses as well.
The public cloud sector offers a market opportunity worth $120 billion across the globe, so now the Chinese e-commerce giant is all geared up to compete with Amazon- the uncrowned king of the fraternity whose revenues escalated by $2.1 billion during the third trimester of the fiscal year of 2015.
Tech News is like a comprised platform for users to find all technology news on one blog.
2 
2 
2 
Tech News is like a comprised platform for users to find all technology news on one blog.
"
https://medium.com/@Apiumhub/cloud-computing-trends-to-watch-in-2017-6e0d8696256b?source=search_post---------222,"Sign in
There are currently no responses for this story.
Be the first to respond.
Apiumhub
Jul 6, 2017·3 min read
The next generation digital businesses might be almost all of cloud platforms due to immense benefits and popularity of could computing technologies. Let’s dive into some of the cloud computing trends in 2017 and an infographic illustrating statistics that indicates the growth and popularity of cloud computing in 2017 and beyond.
Amazon and Microsoft, like cloud providing companies are experiencing exponential growth thanks to cloud adoptions among the masses. Enterprises and leading organizations are increasing their spending on cloud and adopting various services gradually.
Public cloud IaaS services are projected to share billions of revenues from software and hardware needs in coming decades.
The cloud presence is becoming a norm in almost 70% enterprises across the globe, and they have at least one application running on the cloud.
Among the various services covered by cloud computing platforms, data analytics, and data management are seemingly leading in cloud adaptation trends in 2017 and beyond.
Different models of clouds are exhibiting different rates of adoption like a private cloud is exhibiting 77% growth while hybrid 71% and enterprise cloud 31% proportional increase. Therefore, predictions are that total IT budget by the organization for cloud would be 28%.
Almost 46% organizations are integrating cloud APIs for databases, messenger systems, and storage systems and trends are increasing rapidly for other integrations too.
Cloud migration in private cloud is highest against public and hybrid clouds, and it may reach from 45% to 60% in next 18 months.
To conclude, if you think cloud adoption is in favor of your organization and wish to follow trends in the market, you have to look for an eminent cloud service consultant company for technical and all sorts of help. It may prove your cloud migration efforts successful and cost efficient in all respects.
Marc Olson is an experienced content writer, working at Cloud247, a leading Microsoft Cloud Service Provider in Canada. He has been in the industry for the past 10+ Years.
Tech trends in 2017 for software developers & architects
Top software development blogs in 2017
Cloud computing, a growing trend in 2017
Don’t forget to subscribe to our monthly newsletter to receive latest news in the software world!
Originally is published on https://apiumhub.com/tech-blog-barcelona/.
Software architecture, web & mobile app development www.apiumhub.com
1 
1 
1 
Software architecture, web & mobile app development www.apiumhub.com
"
https://medium.com/@golfapipol/codemania-100-part-4-%E0%B8%84%E0%B8%B4%E0%B8%94%E0%B9%84%E0%B8%A7-cloud-computing-platforms-78c6c5dd3eef?source=search_post---------223,"Sign in
There are currently no responses for this story.
Be the first to respond.
Apipol Sukgler
May 15, 2019·1 min read
ปัญหาของ Data Mining คือข้อมูลมีจำนวนมหาศาล จำเป็นต้องเข้าใจอัลกอริทึม รวมทั้งต้องมี HPC (Hight Performance Computing) ใช้พลังของเซิฟเวอร์เยอะ ซึ่งการใช้งาน Cloud ก็ตอบโจทย์
มาทำการเข้าใจ Big Data Life cycle
ซึ่งการหา The right algo มันใช้เวลาเนอะ ทำยังไงให้การปรับปรุงเร็วที่สุด
โดยวิทยากรอธิบายวิธีการเขียน MapReduce ใน Spark มีตัวอย่างเป็น เอา Access log ของ apache มาหา IP Address ที่เข้าเว็บเรา
ทีนี่เราต้องการทำ Analysis ข้อมูล โดยให้เป็นแบบ Near-real time SLA ต่ำกว่า 1 Minute ต้องทำยังไง
เพิ่มเครื่อง! (เอ่อ คงไม่ใช่ทางเลือก)
มีวิธีอีกมะ
Cloud ไง แต่เครื่องมันก็ใช้เวลาในการ setup
ในคิดไวใช้เรื่องของ Machine Learning เข้ามาทำเรื่อง Auto scale ของ cloud ทำให้ได้เป็น Cloud Platform Service เพื่อใช้แก้ปัญหา Data Mining ถ้าใครสนใจก็สามารถมาลองใช้กันได้ครับ
Full-Stacked Developer. Let’s share to the world :)
1 
1 
1 
Full-Stacked Developer. Let’s share to the world :)
"
https://medium.com/eleks-labs/the-emerging-trend-of-distributed-cloud-computing-e2d476be1d8?source=search_post---------224,"There are currently no responses for this story.
Be the first to respond.
2020 has accelerated the new wave of digital innovation where businesses shift their on-premises workloads to the cloud. Distributed cloud computing is set to revolutionise the current cloud computing model and impact business infrastructure. This article explains the notion of the distributed cloud and how it can power traditional enterprises’ operations.
In our digital age, cloud computing — which provides the possibility to deliver services via the Internet using a remote server network — is a necessity. According to Research and Markets, the worldwide cloud computing market is predicted to reach from $371.4 billion in 2020 to $832.1 billion by 2025. And as predicted by Gartner, by 2024 the majority of cloud service platforms will offer at least some distributed cloud services.
The distributed cloud has been around for several years now. It refers to the distribution of public cloud services to varied geographical locations, while the operation and governance of services remain the cloud provider’s responsibility.
The concept of the distributed cloud is the first to include a geographical layer in its definition. Having cloud computing services in close proximity to business operations can ensure latency reduction and better bandwidth. Since data is processed among several locations positioned near customers, it doesn’t need to be transferred over large distances. Thus, the distributed cloud can increase overall performance and security.
The geographically dispersed services provided by the distributed cloud enable businesses to reap many benefits. The distributed cloud offers availability, and the user can access data stored in the cloud from anywhere and at any time. Moreover, disaster recovery in distributed cloud computing involves the transfer of critical information to secondary storage once the system detects a failure. In the distributed cloud, backup and disaster recovery can be automated, and unlike traditional disaster recovery, it requires no maintenance at the user’s end.
Compliance with regulations. Transferring protected information is becoming increasingly complicated as countries introduce data protection laws and regulations. Data sovereignty laws require the keeping of certain digital information within the borders of the country in which it was collected or processed.
A distributed cloud allows your data to stay in-country while migrating it or application on a public cloud. In other words, if the specific country has a data centre or third-party cloud with available capacity, the distributed cloud provides the possibility to operate there.
Data security. Cloud security is not necessarily more secure than on-premises capacities. 49% of businesses fail to encrypt datasets, leaving information vulnerable. Moreover, in a centralised data processing approach, the data is stored in one location. Therefore, in case cyber attackers successfully pass through your security system, the entire dataset can be shut down until the issue is resolved.
However, the lack of security is down to user error and not encrypting; thus, the cloud is more secure if the data is properly encrypted. In addition, the distributed cloud allows the housing of sensitive information across several smaller clouds. It makes a business less vulnerable if a cyberattack takes place. Since data is distributed among several locations, getting to one cloud discloses only a small percentage of it. And if one cloud nodule has to be closed down, other clouds remain unaffected.
More flexibility. The distributed cloud enables businesses to leverage different cloud providers’ capabilities, solutions and infrastructures to meet their needs. Moreover, the distributed cloud provides one centralised interface, which, among other things, allows streamlined security monitoring and data access. The distributed cloud offers a single control pane to configure your public cloud and seamlessly execute services, workloads and applications.
Despite the advantages of cloud computing implementation, some concerns remain. For instance, distributed computing systems can be harder to maintain and deploy, and their deployment costs are higher than a single system. However, in the long run, cloud solutions offer business greater flexibility and enhanced performance, helping them thrive in today’s dynamic business world.
Covid-19 has altered the ways we all work and driven businesses to embrace cloud computing. Now, it’s time to embrace cloud technology or be left behind. Contact us today to learn how we can help you leverage cloud computing potential for your business.
Originally published at https://eleks.com on January 5, 2021.
IT Industry best practices, business insights and trends how…
1 
1 clap
1 
We’ve been helping industries including logistics, retail, agriculture, healthcare and government, transform their operations through innovative technologies — for almost 30 years.
Written by
Your technology partner for software innovation and market-leading solutions: https://eleks.com/
We’ve been helping industries including logistics, retail, agriculture, healthcare and government, transform their operations through innovative technologies — for almost 30 years.
"
https://medium.com/what-it-takes/dropbox-s-multi-channel-strategy-5f1a051788bd?source=search_post---------225,"There are currently no responses for this story.
Be the first to respond.
Capitalizing market shares in cloud computing on a global scale is what Thomas Hansen, Global VP Sales & Channels at Dropbox, has been over performing in, over the last years. Before making the move to Dropbox, Thomas was 14 years at Microsoft and in his last role, responsible for the SMB worldwide. “In order to reach the existing 100 Million small businesses around the globe with SaaS, it is key to roll out a multi-channel strategy”. But what does it mean and how does it work?
Off the record: If you like the no-BS style — you are reading/watching the right thing by the right author, therefore, make the time, unlock the 10 minutes by tweeting this post. You are the man / woman!
Software-as-a-Service is becoming utility to any company. But, still many cloud startups are having challenges to grow top-line, because of a weak go-to-market strategy. In fact, it’s never about a bad product when startups pull the plug, but due to poor traction. In the interview, Thomas shares his lessons learned in how to roll out a global multi-channel strategy for a low-touch cloud product and its unit economics. A strategy, that slingshots Microsoft and Dropbox cloud portfolio, into millions of businesses worldwide, by leveraging the power of local channel partners.
In a couple minutes of your time watching the video, you can expect to hear / learn about:
(!!) And, the most genuine way of showing virtual appreciation for a writer is to share & tweet. In case you haven’t done it yet … I’m looking forward to it! Thank you for reading/watching this and stay tuned for next week’s post.
Originally published at whatittak.es.
Startup Lessons Learned.
1 
1 clap
1 
Written by
Serial entrepreneur w/ 2 exits, author, faculty, investor, philanthropist.
Startup Lessons Learned. From Entrepreneur To Entrepreneur; By Tommaso Di Bartolo, a Silicon Valley serial entrepreneur w/ 2 exits, author, advisor & angel investor. Snapchat: todiba
Written by
Serial entrepreneur w/ 2 exits, author, faculty, investor, philanthropist.
Startup Lessons Learned. From Entrepreneur To Entrepreneur; By Tommaso Di Bartolo, a Silicon Valley serial entrepreneur w/ 2 exits, author, advisor & angel investor. Snapchat: todiba
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@exastax/the-future-and-benefits-of-cloud-computing-9ed0e9967ec8?source=search_post---------226,"Sign in
There are currently no responses for this story.
Be the first to respond.
Exastax
Sep 8, 2017·1 min read
Each year organizations are embracing Cloud Computing more. Since the cloud adoption increases, companies are improving their services by utilizing Cloud in so many level such as achieving a greater scalability, higher performance, and faster time to market to name a few.
If you are wondering what all the fuss is about, this infographic will help you to have a better understanding of why organizations are moving to The Cloud by explaining top 10 benefits of Cloud Computing, the present and the future of the Cloud with some significant numbers. Enjoy!
This post was originally posted at www.exastax.com/blog
Transforming how you work with data
See all (17)
1 
1 clap
1 
Transforming how you work with data
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/checking-up-on-costs-saving-more-with-cloud-computing-23fbfb15bf?source=search_post---------227,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Aug 12, 2019·3 min read
Smart habits to keep an eye on your cloud expenditure
Moving some or all of your IT infrastructure to cloud computing makes good financial sense. No longer does a new server or a terabyte of storage require thousands of dollars’ worth of up-front expenditure and a lengthy planning process. IT staff can create what they need in just a couple of minutes, and pay by the month or even by the hour.
It’s important to keep a watchful eye on your cloud computing expenditure. Alibaba Cloud makes it easy to do this, straight from the management console. Just click on Billing Management to see your current monthly charges, and to browse previous months’ invoices for comparison. Under Usage Records you can even download detailed information about any specific Alibaba Cloud service (Elastic Compute Service, Object Storage Service, and so on) between any two arbitrary dates, to see precisely what you’re spending money on.
Why not set aside half an hour every week to do this? Remember to check all of your Alibaba Cloud accounts, if you have more than one.
Release any pay-as-you-go server instances which are no longer being used, such as those that were set up for one-off evaluation or testing purposes. If you use the Object Storage Service (OSS), look for storage buckets that are only accessed very occasionally and consider moving them from standard storage to Infrequent Access or Archive mode. This will save you around 50 percent and 75 percent respectively on monthly storage costs.
Use CloudMonitor to keep an eye on server performance. The dashboard makes it easy for users to visually monitor information, so they can view historical data on key metrics such as free RAM, CPU load, disk space and so on. You can save money by removing or reducing resources that are not being used. For example, if a server never exceeds 50 percent CPU usage then reduce the specification of the server.
To avoid unexpected downtime, configure CloudMonitor alarms to automatically alert you by text message or email if specific thresholds are breached. This will give you advance warning of impending problems such as low disk space or poor connectivity.
The basic CloudMonitor service is free to use.
In addition to the weekly technical-level check, do a quarterly management-level audit too. Create, and keep updated, a service catalog. Use a database, or even just a spreadsheet, to record every cloud computing resource that you use. OSS storage buckets, ECS server instances, ApsaraDB RDS databases and so on. Against each resource, note which service or facility it helps provide, who the key users are, who created the resource, an emergency contact number for the owner, when its continued existence is next due to be reviewed, its typical monthly cost, and any current observations such as disk space getting low or CPU usage running high.
As part of the same audit process, spend a little time checking out the Alibaba Cloud website for details of new products and services that have launched in the past quarter and might be of use. For example, this could include new billing methods, more powerful server instances, new hosting regions and so on.
With cloud computing, no longer can you just poke your head round the door of the data center and quickly check that everything looks the same as it did last week. But all the tools that you need to keep on top of your Alibaba Cloud infrastructure are easily at hand, and free to use.
To learn more about how to run a cost-effective and successful business on the cloud, download our ebook A Cloud for Business: Running a Cost-effective Company on the Cloud.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
1 
1 clap
1 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@TheDigitalTP/is-cloud-computing-useless-without-a-cloud-exit-strategy-ec8a25e8fdaf?source=search_post---------228,"Sign in
There are currently no responses for this story.
Be the first to respond.
The Digital Transformation People
Oct 8, 2018·4 min read
Today’s IT of a company has the role of a real service department. It also covers issues such as cost-efficient IT operations, data security, failure security, BYOD (Bring Your Own Device), mobility and much more. Consequently and logically, of course, ageing IT infrastructures turn out to be a thorn in the side of every decision-maker with a future-oriented thinking.
The more modern IT infrastructures are, the more they count as a competitive advantage. The idea is to apply a strategic and innovative approach to technology. The solutions must, therefore, be perfectly tailored to business requirements.
However, as business development progresses, the infrastructure must be able to cope with new modern initiatives and steady growth. Cloud computing with its role as the digitization par excellence is playing an increasingly important role in the IT world. As the “next big thing”, the cloud stands for a changing world. Almost like victorious gladiators from ancient Rome, cloud-based software solutions expel long-held beliefs and products from apparently impenetrable IT landscapes. Decision-makers from companies, whether small, medium-sized or large, are more than ever interested in equipping their companies with cloud-based technologies.
Surprising is the fact that only a few years ago the majority of companies were still very closed towards cloud computing. In the 2016 report “The Business-Technology Performance Connection for SMBs”, the SMB Group has shown the shift towards cloud computing. This development is explained by the fact that more and more companies see the cloud as the most cost-efficient, flexible and best solution for providing a future-oriented IT infrastructure.
In fact, today’s decision-makers deal with issues such as digitization, Big Data, Internet of Things (IoT) and, in general, the need for innovation to remain competitive. At the same time, however, costs must be kept as low as possible. The opportunity of having a modern IT infrastructure without the need for substantial capital investments and personnel increases, which also meets the high standards of services and products is a chance every decision maker wants to take.
Due to the rise of the cloud computing industry and many competing companies shifting to the cloud, there is definitely a high risk of making quick shot decisions. Gartner, Inc. predicts that over the next five years more than one trillion dollars of IT spending will be directly or indirectly affected by the shift to the cloud. Forbes goes even further by making clear that the risk of remaining closed towards the cloud is more harmful for one’s business than moving towards it.
However, adopting a new technology is one thing, the question or risk of making an irreversible investment is another, especially in the case of cloud computing. Companies seem to be over-excited towards cloud computing to think about a cloud exit strategy. Without a clear cloud exit strategy, one may end up locked into a single cloud provider. The latter may have the advantage of making the price at will and the customer might have no opportunity to leave anymore since time, effort and money to exit the service are considered too high.
Unsurprisingly, the majority of companies regardless their size, have a preference for well-known cloud providers, because established ones like Amazon AWS, Microsoft Azure, Google Cloud Platform, IBM Softlayer and Alibabacloud are believed to respect their promises and to stay in business. Today Amazon, Microsoft and Google lead in Gartner’s Magic Quadrant. Together they share a market with a value of billions of dollars. However, is choosing a big name the right strategy?
This is indeed a question that allows new entrants to this promising market such as n’cloud.swiss to convince customers by pointing out several advantages. The cloud platform which is paving its way with an aggressive expansion strategy as the “Swiss made” alternative to the major cloud providers, scores with a number of advantages for customers that range from agility, flexibility and adaptability to a maximum amount of freedom to build their cloud according to their needs.
The Cloud Act seems thereby to play in their cards given the fact that the cry for a non American cloud provider and a serious alternative to Amazon AWS, Microsoft Azure & Co. is louder than ever. Global IT service provider Luxoft has recognized the potential of a Swiss alternative and is planning according to Business Insider to create the first customizable blockchain-based e-voting system in Switzerland by deploying the platform with the help of not only Amazon AWS but with the help of n’cloud.swiss as well.
All things considered, the entry into the world of cloud computing seems to be easy — however, the exit can turn out to be financially disastrous. American giant Apple is known for making it hard for customers to turn their back to Apple’s World. Dan Tynan once summarized Apple’s strategy as follows:
“Once you enter the Big Tent of Apple, it’s exceedingly hard to find the exit.”
The question now is: How should cloud providers and their customers deal with the issue surrounding the importance of having a cloud exit strategy?
Originally published at www.thedigitaltransformationpeople.com on October 8, 2018.
Helping you discover the what, how and who of digital transformation. We’ve built a network of incredible people who can help you make a success of yours.
See all (1,331)
1 
1 clap
1 
Helping you discover the what, how and who of digital transformation. We’ve built a network of incredible people who can help you make a success of yours.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/5-ways-cloud-computing-is-empowering-global-start-ups-to-achieve-their-goals-a45eb474e4ce?source=search_post---------229,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
May 12, 2021·5 min read
The term Cloud computing was coined in late 1996 but it was not before 2006 when it started gaining popularity. Fast forward to 2021, Cloud computing has revolutionized not only the digital world but has also transformed the destiny of start-ups and small businesses. In this article, we will discuss the major benefits of cloud for Start-ups and small businesses but before moving ahead let us first understand what the cloud means
Cloud computing entails a wide range of virtual resources like software, databases, storage, servers, intelligence, and analytics — delivered through internet. It effectively promotes virtualization and significantly reduces the requirement of hardware resources. With cloud computing, start-ups can minimize the costs and management hassles while enjoying reliable performance and security. Moreover, the virtual ecosystem allows cloud computing high-end scalability. The start-ups can work with most sophisticated cloud applications at fractions of the cost- charged as per usage.
The very architecture of cloud computing is fully attuned to the major needs of start-ups. It makes cloud computing a natural choice for the start-ups that want to use technology as a potent tool for transcending resource limitations. Here are a few ways in which cloud computing benefits start-ups:
Maintaining cost efficiency is a must for start-ups, especially the bootstrapped start-ups. In the conventional IT ecosystem, it requires constant monitoring, manual intervention, and installation/removal of complex hardware and software products.
Due to virtualization, the cloud assures an instant, smarter way to scale up/down the resources depending upon the changing needs. Instead of spending a fixed cost (one-time or periodical basis) on your hardware/software assets and figuring out the maximum utilization possibilities, it is much easier to invest in cloud computing that automatically adjusts the resources as per the user needs and thus promotes low cost computing.
One of the major concerns of start-ups is to increase productivity without inflating the costs. Talking in terms of IT & communication ecosystem, a major percentage of cost is spent on buying and managing hardware components like CPU, etc. — not to mention the associated running costs, service, and maintenance. Cloud offers strong virtual substitutes with the same performance caliber that eliminates the need for hardware.
The cloud facilitates virtualization and enables start-ups to save huge infrastructure costs, use much less power and reduce the overall IT expenses without compromising on productivity. It also eliminates the requirements of vast spaces. Along with these immediate cost savings the cloud also assists in reducing the running costs by taking off a chunk of heavy continuity costs spent on upgrades, maintenance, support, installations, troubleshooting, and repairing. It translates to a significant saving that can be utilized towards achieving core competencies and meeting business development goals.
Collaboration is vitally important for the success of start-ups. What they need is a strong, affordable medium to access, share and work simultaneously on different documents, files, and applications- any time, anywhere, and through any device. It helps in accelerating their productivity by harnessing different capabilities and accumulating efforts of skilled manpower.
The cloud acts as an omnipresent digital vault for such start-ups allowing them to create, access, and store numerous files and work simultaneously with their globally distributed professionals on the same project with real-time efficiency. It promotes and strengthens collaborative work culture transcending the time and geographical limitations.
To gain the best benefits out of digitization, you need a high level of scalability and mobility. Businesses are not limited to local transactions anymore but they have embraced a global approach which means you should have the ready capabilities to serve global businesses and clients in real-time- no matter whether you are at the workplace, in your home, or even traveling. Cloud computing offers 24-hour accessibility to the task materials and tools to every member of your team empowering them to cater instantly to the needs of potential clients even if they are not at the workplace. It helps you in three ways- tapping global opportunities more efficiently, maxing out the potential of your team members, and better remote team management.
There are other perks of cloud computing as well. It streamlines remote business management allowing you to stay updated and enjoy complete control over your business transactions even while working in a remote environment. The virtual tools in the cloud convert each of your staff members’ net-enabled devices into a scalable work device which saves you from buying and maintaining numerous workplace devices. Cloud computing also plays a vital role in effective remote management through various connected devices through cloud-based IoT platforms.
When you deal with massive data with in-house capabilities, you also need to ensure the security of that data which involves building an array of physical and digital boundaries (firewalls, antimalware, etc.) along with developing and enforcing specific security guidelines. It requires a significant investment while also burdening the start-ups with the security management hassles that often interrupt the agile and open-door startup culture. By investing in the cloud the start-ups can delegate the security management to expert security professionals of Cloud Company at reasonable monthly expenses. Along with saving costs and management hassles it also helps start-ups to enjoy resilient security against various cyberattacks and threat actors.
Cloud computing has gained very high popularity in recent years due to its cost-efficient model that allows clients to extract the best output out of their IT dollars. It has empowered start-ups to scale up their productivity and compete confidently against bigger players. By wisely incorporating cloud computing in their day-to-day activities the new businesses can enjoy various competitive benefits in the market.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
4 
4 claps
4 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@TheDigitalTP/cloud-computings-impact-on-healthcare-277af584d791?source=search_post---------230,"Sign in
There are currently no responses for this story.
Be the first to respond.
The Digital Transformation People
Oct 26, 2018·4 min read
If you’ve been visiting the doctor regularly, good for you! Some parts of the digital-first healthcare experience will already be familiar to you. But for those of us who might’ve let a few years go by without a routine checkup, some of technology’s impact on health care might take you by surprise.
Cloud technology has been especially consequential and helpful when it comes to providing health services and observing the related administrative requirements. Let’s take a quick look at why and how.
Cloud Computing Facilitates Faster Check-Ins and a Better Customer Experience
When you walk into a doctor’s office today, it’s quite likely you’ve already received a text notification confirming your appointment, you’ve electronically submitted some of the paperwork and patient history forms and you probably already know which health professional you’re there to see. You may even have had a video chat with them before making them your primary care physician or coming to them as a referral for a specific issue.
Cloud technology removes a lot of the tedium of patient paperwork — including checking in once they’re at the office — making everything pretty much a breeze. And it ensures the changes patients make to things like billing details using their insurance company’s web app are up to date next time they visit the office.
Electronic Health Records
EHRs — electronic health records — are now mandatory, according to federal law. Making this change was a sterling example of government and private industry working for a convenient, but also a customer-centric solution to a longstanding problem.
With EHRs, health care providers can use the cloud to call upon the complete available histories, including surgeries and medications, for each patient they treat. It means no faxing and no concerns over doctors grabbing the wrong file by mistake.
Naturally, with HIPAA and other regulations that touch the healthcare industry, security and privacy are top concerns. Health care providers have had to make infrastructure and hiring investments when it comes to bandwidth, uptime, network security, and IT personnel. Choosing to outsource IT staff is now both a boon to a health system’s security and a cost-cutting measure, for example.
The work has been worth it for the industry as a whole, however. Among the benefits already listed, this web of data infrastructure technologies is creating a way to accumulate and study the long, rich medical histories of millions of patients. That brings us to our second point.
Wearables and the Internet of Things
The consumer-level availability of portable and wearable technologies has forever changed how health professionals provide care.
Wearables such as step counters and smartwatches, along with other Internet-connected devices like smart scales and wireless blood pressure monitors, together represent a vast cloud network intended to gather as much data, as quickly as possible, about as many people as possible.
According to Fast Company, Apple Watch Series 4 didn’t secure final FDA approval for its new electrocardiogram monitor until the very last second. That’s how fast things are progressing when it comes to the ways we can measure and transmit health-related data. But now that it’s here, it represents an essential piece of a newly decentralized health care system. Many similar products from other companies are likely to follow.
What does this mean for doctors and patients? For doctors, the availability of this data is making it easier to render accurate diagnoses and spot anomalies in patient histories. To be sure, some of this data is white noise — but a lot of it has already been instructive even in smaller practices. Third parties already have tools available, based on health APIs from companies like Apple and Google, that let doctors and patients investigate historical data from wearables in a useful way.
It’s not just a faster and more secure way to exchange records. With a few more years of hardware advancements, it’ll likely lead to new ways to make advance diagnoses for diseases and forecast the likelihood of injuries.
Networked Computers Provide Modeling and Analytical Tools
Computing power is a vital commodity. It’s why there are multiple ways to donate some of your computer’s unused processing power to the cause of modelling diseases and iterating new attempts at a cure. It sounds far-fetched, but it’s not: Scientists have long relied on computer models to understand how drugs react with bacteria, for example, or even how specific cells undergo evolution over a rapid series of new generations.
Networking and cloud technology, not to mention high-speed processors, now make it possible for researchers to leverage an ever-larger pool of computing power for drug and disease treatment research. If you want a painless way to “give back” to society, research how supercomputers have become instrumental in fields like genomics, drug design, chemotherapy, immunotherapy and many other fields related to human health — and then consider firing up your old laptop and donating some processing power to Folding@Home, BOINC or a similar initiative.
Together, each of these technologies represents an industry in a state of transition. Changing something as personal as the doctor-patient relationship might sound scary, but incorporating new technologies will create many new opportunities for everybody involved to enjoy a safer, more accurate and less wasteful healthcare experience.
Originally published at www.thedigitaltransformationpeople.com on October 26, 2018.
Helping you discover the what, how and who of digital transformation. We’ve built a network of incredible people who can help you make a success of yours.
See all (1,331)
1 
1 clap
1 
Helping you discover the what, how and who of digital transformation. We’ve built a network of incredible people who can help you make a success of yours.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@TechJobs_NYC/tech-companies-new-and-old-clamor-to-entice-cloud-computing-experts-a7edffb19acb?source=search_post---------231,"Sign in
There are currently no responses for this story.
Be the first to respond.
Technology Jobs NYC
Mar 7, 2016·1 min read
Tech Companies, New and Old, Clamor to Entice Cloud Computing Experts By QUENTIN HARDY As some tech sectors show signs of slowing, cloud services have created remarkable demand for highly educated engineers and mathematicians. And they are being compensated very well. Published: March 6, 2016 at 07:00PM via NYT Technology http://www.nytimes.com/2016/03/07/technology/tech-companies-new-and-old-clamor-to-entice-cloud-computing-experts.html?partner=IFTTT
The official account for https://www.technologyjobs.nyc! Follow for exclusive job opportunities, tips and tricks to help you get your next #technology #job
1 
1 
1 
The official account for https://www.technologyjobs.nyc! Follow for exclusive job opportunities, tips and tricks to help you get your next #technology #job
"
https://medium.datadriveninvestor.com/importance-of-containers-and-virtualization-in-cloud-computing-b0e312b78a77?source=search_post---------233,"There are currently no responses for this story.
Be the first to respond.
We will now talk about virtualization and containers, which have become a very important topic in the software engineering field, primarily because there’s a lot of focus on it when it comes to cloud computing.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@RedPixie/10-ways-cloud-computing-can-help-your-business-d6ba34b94cf3?source=search_post---------235,"Sign in
There are currently no responses for this story.
Be the first to respond.
RedPixie
Mar 29, 2016·2 min read
A lot has been said about the impact and potential of cloud computing. This enterprise platform has come a long way in just a few years, seeing widespread adoption by organisations appreciative of the scope that it provides to boost corporate efficiency, productivity and revenue — and 2016 will see a true acceleration of the development of thenext-generation cloud.
But what benefits could cloud migration bring to your firm right now? How could it make your business’s life — including that of both rank-and-file team members and higher-ups — easier over the coming 12 months?
Covering the categories of flexibility, the reduction of spending on technology infrastructure, disaster recovery, automatic software updates, capital expenditure reduction, increased collaboration & communication, working from anywhere, document control, security and environmental friendliness, we consider the 10 big advantages of cloud computing in 2016.
You might be interested to know, for example, of the ease with which a cloud computing plan can be adapted to meet your organisation’s specific requirements at any one time, or the enhanced ability that it gives you to budget in accordance with those needs, for the short, medium and long term.
Created by Jake Chody | Marketing Manager, RedPixie | Originally posted on https://redpixie.com/blog/
RedPixie go beyond technology. Building and managing Azure hybrid cloud solutions for the financial sector.
See all (1,037)
2 
1
2 claps
2 
1
RedPixie go beyond technology. Building and managing Azure hybrid cloud solutions for the financial sector.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@HOSTINGdotcom/top-cloud-computing-stories-of-2015-1698ea897c01?source=search_post---------236,"Sign in
There are currently no responses for this story.
Be the first to respond.
HOSTING
Jan 7, 2016·4 min read
Cloud adoption continued to grow at an accelerated pace for most of 2015. However, this growth was tempered by concerns about cloud security, especially in the healthcare sector which experienced massive security breaches. While Forrester predicts that the global cloud computing market will grow 22% annually to $241 billion in 2020, many cloud providers are feeling competitive pressures. Join us as we take a look at the top five cloud computing stories of 2015.
Hewlett-Packard Enterprise exits the public cloud market
A pre-split Hewlett-Packard Enterprise (HPE) devoted much of 2015 to fending off rumors that it was going to walk away from the public cloud market. An April, 2015, article in the New York Times published quotes from Bill Hilf, HPE’s senior vice president of cloud product management, that implied the hardware giant was ceding the public cloud business to heavy-hitters such as Amazon Web Services (AWS) and Microsoft Azure.
“We thought people would rent or buy computing from us,” said Bill Hilf, the head of HP’s cloud business. “It turns out that it makes no sense for us to go head-to-head.”
A week following the NYT article, Hilf posted a blog that emphasized HPE’s commitment to the public cloud, stating that the company’s “portfolio strategy to deliver on the vision of Hybrid IT continues strong.”
With Microsoft Azure, AWS, Google and IBM estimated to control more than half the worldwide cloud infrastructure service market, it came as no surprise when HPE released a statement six months later, confirming its closure of its public cloud service by January 31, 2016.
Financial Conduct Authority declares cloud “acceptable” for financial services firms to use
The financial services industry has been one of the last verticals to embrace the cloud. That may change in 2016, based on an advisory note issued by the Financial Conduct Authority (FCA) in November, 2015, stating that used correctly, cloud technologies are “acceptable” for use by financial services firms.
“We see no fundamental reason why cloud services (Including public cloud services) cannot be implemented with appropriate consideration in manner that complies with our rules.” — Proposed guidance for firms outsourcing to the ‘cloud’ and other third-party IT services, Financial Conduct Authority
While the FCA’s guidance may serve as a positive first step, many financial services firms remain wary of the cloud, citing security concerns.
Healthcare organizations became the target of choice for cyber criminals
In February, 2015, Anthem, the second largest health insurer, announced that the personal information of approximately 80 million customers and employees, including its chief executive, as the subject of a “very sophisticated external cyberattack.” The information accesses including names, Social Security numbers, birthdays, physical and email addresses and employment information, including income data. In other words, a goldmine of information for hackers to leverage.
While the Anthem cyberattack is the largest breach of a healthcare company to date, many more followed. Healthcare data has become the “It Girl” for cyber criminals. Notoriously slow for upgrading IT systems and adopting new technologies, the healthcare industry is also ill-equipped to handle the “data tsunami” caused by an influx of new patients to the healthcare system. Cybercriminals have been able to mine these data archives, leveraging protected health information (PCI) to file fraudulent medical claims, purchase illegal prescriptions and even take out loans.
According to the Ponemon Institute, data breaches cost the healthcare industry $6 billion annually. It estimates that the impact of a data breach per organization is $2,134,800. While healthcare organizations are scrambling to put new security measures in place, the Office of Civil Rights (OCR) has already announced its next wave of audits — promising steeper fines for organizations with security lapses.
Hybrid cloud is two to five years away from mainstream adoption, Gartner predicts
Hybrid cloud computing is still in its early stages, with global research and advisory firm Gartner Inc. predicting that it is still “two to five years” away from mainstream adoption. While analysts and suppliers continue to talk about hybrid clouds, Gartner’s Hype Cycle provided a reality check — declaring that just 10–15% of enterprises are currently using them.
Third-party managed cloud services are rapidly gaining acceptance
In mid-2015, Structure Research issued a Hot Topic Report, Managing Third Party Clouds, which stated, “One of the most recent developments in the hosting and cloud infrastructure space is the emergence of managed services offerings for massive-scale cloud infrastructure.” AWS and Azure offer an abundance of cheap, raw infrastructure and a plethora of cloud development tools — perfect for companies that want to take a DIY approach to building and managing their environments. However, those that expect a high level of service are turning to managed cloud service providers to supply the essential security, compliance and support resources they need to effectively run their cloud environments.
We build and operate high performance clouds for business-critical applications. Parent to @Ntirety and @HostMySite.
1 
1 
1 
We build and operate high performance clouds for business-critical applications. Parent to @Ntirety and @HostMySite.
"
https://itnext.io/decentralizing-cloud-computing-through-edge-d32890614e4b?source=search_post---------237,NA
https://medium.com/@solyarisoftware/of-course-in-2017-we-are-all-aware-of-the-fundamental-importance-of-cloud-computing-7b61e113e9bc?source=search_post---------238,"Sign in
There are currently no responses for this story.
Be the first to respond.
Giorgio Robino
Mar 16, 2017·1 min read
Alex Casalboni
Of course, in 2017 we are all aware of the fundamental importance of cloud computing and “scalability” issues. Nevertheless this is a momentary trend of these recent years. In my view will go soon toward the era of peer to peer distributed computing.
All in all, Alex, I can just add my modest suggestion to our self, computer scientists or (software) engineers:
please never mix a technological pattern with a business strategy🤑, or do it, aware of this “bastard” deliberate way🤦
So, back to naming, you can call serverless FaaS or whatever you want, but for me is just a commercial strategy of big players of cloud computing that have to motivate huge investments already done. So please call serverless with the right name:
Serverless paradigm is just returning to Mainframe!🙄
Nothing against seventies Big Blue monopoly; I envy a bit the good old days where brilliant minds created that OS, but nowadays we have to face new challenges of billions of interconnected autonomous conversing actors (men and machines) 🤔 🤔 🤔 🤔 🤔
Conversational AI technical lead at @almawave. Working now on healthcare multimodal assistants. In past, as researcher at ITD-CNR, I built CPIAbot.
1 
1
1 
1 
1
Conversational AI technical lead at @almawave. Working now on healthcare multimodal assistants. In past, as researcher at ITD-CNR, I built CPIAbot.
"
https://medium.com/@edgecast/how-cdns-can-adapt-to-the-cloud-computing-era-cd4c41513fde?source=search_post---------239,"Sign in
There are currently no responses for this story.
Be the first to respond.
Edgecast
Dec 12, 2017·6 min read
“Build like the cloud; deliver like the cloud” is an internal mantra we have been using in our Development and Product teams at Verizon Digital Media Services for the last two years. Cloud computing, in which remote servers, virtualization, Infrastructure as a Service (IaaS), and orchestration tools make the deployment and scaling of applications easier, is quickly becoming the standard way that software is built, deployed and delivered.
This puts content delivery networks (CDNs) in something of a predicament. While customers crave the speed and ease of the cloud, they’re unwilling to compromise on the performance, reliability, security and scale advantages that purpose-built CDNs still provide. That means CDNs have to continue to meet and exceed customer expectations, while at the same time finding new ways to integrate with cloud computing platforms and tools.
Here are some ways in which we are adapting to keep pace with the cloud paradigm.
Build with cloud tools in mind.Customers have always used application program interfaces (APIs) to easily integrate their applications with third-party services. Our Edgecast Content Delivery Network differentiated itself early on with a foundation of enabling customers and partners to easily self-service their CDN accounts and configurations via portal-based tools and APIs. Increasingly though, customers are adopting cloud APIs and DevOps toolsets to integrate applications and other workloads into the cloud. As the application development life cycle progresses from development to testing to staging to production deployments, popular configuration management and orchestration tools such as Puppet, Chef, Salt, Terraform and others are used to automate and regulate these activities and stages.
One of the ways we are adapting the CDN to support these tools is by using them ourselves. While CDNs have always (and likely will always) push the envelope of performance by optimizing using bare metal servers and full-stack tuning, we have been using cloud environments for development, testing, staging, prototyping, data/analytics and other uses. This has made our own processes more agile and robust, while letting us learn the art and science of DevOps centric, testable-by-design, CI/CD-friendly development practices. In other words, it has taught us to build the way our customers build and to adapt our own configuration tools and interfaces to what our customers expect.
As we have adopted these practices, we have started building a set of interfaces and tools we are calling EdgeControlspecifically to provide this functionality to the Edgecast CDN. It was as natural as looking at what tools are popular and useful in open source and DevOps communities, looking at how they’re leveraged against compute, storage and network infrastructure, and seeing how we could fit our CDN into those tools and workflows more organically.
The cloud works in real time. CDNs should, too. It’s easy to see the cloud’s appeal to customers; it’s a faster way to deploy software than ever before. According to a Vanson Bourne report on the business impact of the cloud, there’s more than a 20 percent speed improvement in a software’s time to market when it’s hosted in the cloud. When an application is deployed into the cloud, it’s dynamic and quick, as well as adaptable; a developer can just as easily spin up one instance or multiple at the same time, often using automated orchestration and deployment tools.
A CDN needs to work just as quickly and efficiently. If a customer is launching an application in the cloud, it’s the CDN’s job to ensure it is incorporated into that custom automation and app deployment process at the same time. Testing the application and then testing the CDN shouldn’t be a two-step process for customers to consciously think about, but rather two things that can happen in tandem. The CDN can play an appropriate role in the same testing process, and when it happens concurrently, developers can get data back out of the CDN during testing for enhanced metrics and analytics monitoring. And having a responsive CDN during the initial testing step ensures that the testing environment replicates the eventual production environment as closely as possible: from how the software performs in different regions, to how it mitigates potential security issues, to how it handles load or offloads certain business logic to edge servers.
At Verizon, our increasing internal use of cloud infrastructure, tools and automation has shown us that testing the CDN as an afterthought can not only be tedious, but less efficient. EdgeControl was born out of discussing needs with our customers and partners, building to complement our own internal processes, and monitoring our competitors. It’s not enough to provide API hooks, configuration and propagation; these things also need to happen in real-time, so we’re working on making more of our components real-time as well, including configuration APIs, ingest and propagation and feedback.
Automate, automate, automate. Finally, it’s vital for a cloud-integrated, real-time, responsive CDN to have extensive automation capabilities. The more that the CDN is a secondary, manual step in the application deployment process, the less it is likely to be updated and in sync with application changes, the fewer automated-testing processes are likely to incorporate the CDN component (including any edge-deployed application logic), and the less reliable and predictable the end-to-end deployment process becomes.
From code testing to software deployment, the end goal should be to have customers do as little manually as possible. Instead, they ought to be able to make long-term configurations in the CDN to reflect a new content profile, a new application or API, or even just changes in the application environment, such as new regions being deployed, elasticity changes in deployed instances, or other characteristics of the origin/cloud environment that the CDN could or should be responsive to. Even better would be if that configuration could be tied to the customer’s existing software configuration tool so the same automation processes and testing tools work together. The ability to automate in advance doesn’t just make software deployment easier, but also more reliable, since it can be configured well before anything goes live on the CDN. This has the added benefit of fitting into the more dynamic, continuous integration deployment model that is becoming more common.
For an application that lives in the cloud, the CDN is the infrastructure that sits in front of it. That means when a customer updates the application’s code, the CDN should automatically be aware of the change, and trigger an update in its configuration if need be. Our EdgeControl toolset is being developed with an eye toward how that process can be scripted in advance through an automated deployment process to avoid having our customers do a secondary configuration. The results will be a quicker, more efficient and more reliable deployment process.
Many of these improvements are as natural as asking ourselves, “How can we build this to be more like the cloud?” The answer will be emerging in new capabilities and tools we are rolling out now and in coming months which include, improving our configuration propagation times from ~40–60 minutes to less than 5 — a more than 90 percent improvement); exposing more native configuration syntax to allow for the full power of our edge performance engine to be exposed to developers; expanding our APIs and command-line (CLI) tools to increase scripting and automation integration opportunities with the most common DevOps toolsets and frameworks; and taking the covers off of extensive metrics, analytics and other data that can provide real-time feedback on performance and utilization, etc. We’re excited about this coming transformation and the promise of enabling cloud applications and developers to interact with our platform more efficiently and natively.
To learn more about how our CDN acts like and integrates with the cloud get in touch with us today.
Formerly Verizon Media Platform, Edgecast enables companies to deliver high performance, secure digital experiences at scale worldwide. https://edgecast.com/
2 
2 
2 
Formerly Verizon Media Platform, Edgecast enables companies to deliver high performance, secure digital experiences at scale worldwide. https://edgecast.com/
"
https://medium.com/@jaychapel/cloud-computing-green-initiatives-on-the-rise-26765bc9cc5?source=search_post---------240,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Apr 24, 2018·3 min read
Over the past couple of months, we have seen a lot of articles about the Big Three cloud providers and their efforts to be environmentally friendly and make cloud computing green. What are Amazon Web Services (AWS), Microsoft Azure, and Google Cloud Platform (GCP) doing to make their IaaS services as green as possible? Does moving to the cloud help enterprises with their green initiatives and use of renewable energy?
It seems the cloud providers are focused on using renewable energy like solar and wind to power their massive data centers and are very actively touting that fact.
For example, Microsoft recently announced a new renewable energy initiative, the Sunseap project. This project, Microsoft’s first Asian clean energy deal, will install solar panels on hundreds of rooftops in Singapore, which they claim will generate 60MW to power Microsoft’s Singapore datacenter — making Microsoft Azure, Office 365 and numerous other cloud services. This deal is the third international clean energy announcement, following two wind deals announced in Ireland and The Netherlands in 2017. That’s pretty cool in my book, so kudos to them.
Google made a similar announcement recently, albeit a little more general, where they tout that Google is now buying enough renewable energy to match the power used in its data centers and offices. Google said that last year its total purchase of energy from sources including wind and solar exceeded the amount of electricity used by its operations around the world. According to a recent blog written by Google, they are the first public cloud, and company of their size, to have achieved that feat, so says Urs Hölzle, Google’s senior vice president of technical infrastructure. Now we can’t verify this but let’s take them at face value given the data in the chart below:
One observation we have in looking at this chart — where are IBM and Oracle? Once again, the Big Three always seem to be several steps ahead.
Speaking of, we’ve looked at Microsoft and Google, what about AWS? According to AWS’s self-reports, it seems that they are behind both Google and Microsoft in terms of relying 100% on renewable energy. AWS states a long-term commitment to achieve 100% renewable energy usage for their global infrastructure footprint, and had set a goal to be powered by 50% renewable energy by the end of 2017 (we could not find a recent 2018 update).
Moving to the cloud has many benefits — time to market, agility, innovation, lower upfront cost, and the commitment to renewable energy.! There’s one other way for cloud computing to be more sustainable — and that’s by all of us using fewer resources. In our small little way, ParkMyCloud helps — we help you turn cloud stuff off when its not being used, kind of like following your kids around the house and shutting off the lights, your at-home green initiative — you know you can automate that using Nest, right? Saving money in the process? That’s a win-win.
Originally published at www.parkmycloud.com on April 24, 2018.
CEO of ParkMyCloud
1 
1 
1 
CEO of ParkMyCloud
"
https://medium.com/knoldus/experiences-at-cloud-computing-conference-pune-2011-3cbfcac1149f?source=search_post---------245,"There are currently no responses for this story.
Be the first to respond.
I was one of the speaker of the second IndicThreads conference held at Pune on 3–4th June 2011.
Sessions at the conference dealt with key topics like Cloud Security, Amazon Elastic Beanstalk, Legal Issues in Cloud Computing, OpenStack, Xen Cloud Platform, Rails and CouchDB on the cloud, CloudFoundry, Gigapaces PAAS, Monitoring Cloud Applications, ORM with Objectify-Appengine, Scalable Architecture on Amazon AWS Cloud, Cloud Lock-in, Cloud Interoperability, Apache Hadoop, Map Reduce and Predictive Analysis.
My talk focussed on managing persistence on GAE. It dealt with choices available to a developer and then focussed on doing it with Objectify-Appengine.
[slideshare id=8212930&doc=easy-ormnesswithobjectify-110605095511-phpapp01]
Demo application is present here for download. For the instructions on how to run it please read the wiki.
Knols, Insights and Opinions from the curious minds at…
1 
1 clap
1 
Knols, Insights and Opinions from the curious minds at Knoldus Inc.
Written by
Group of smart Engineers with a Product mindset who partner with your business to drive competitive advantage | www.knoldus.com
Knols, Insights and Opinions from the curious minds at Knoldus Inc.
"
https://medium.com/@alibaba-cloud/free-courses-on-cloud-computing-7e3c61a4375a?source=search_post---------247,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Feb 22, 2019·3 min read
As a global cloud provider and NO.1 public cloud in Asia, Alibaba Cloud will provide you with the right tools and resources to build your future career.
You can get Alibaba Cloud Elastic Compute Service (ECS) for only $0.99 for an entire year. That’s up to $687.93 in savings. In addition, you can claim 10 training courses and receive 50GB/month data transfer voucher for 12 months.
You can click here to get more details. Here are some introductions below.
Why is this event worth attending?
ü Get a head start on your career in the cloud. With access to our select Elastic Compute Service (ECS) for only $0.99 and ten free training courses, Alibaba Cloud is here to make your journey to cloud computing more accessible.
ü Upon successful verification, college educators will receive access to discounted Alibaba Cloud Elastic Compute Service (ECS) along with 15 Apsara Clouder training courses. More coming soon.
ü The Alibaba Cloud University Cooperation Program (AUCP) is a global education program aiming to support schools and students around the world to gain access to Alibaba Cloud resources and nurture future talents in the cloud computing, cloud security, and big data industries Learn more.
What are the rules?
1. For now, this offer is exclusive to university students from selected countries & regions after successful verification of enrollment. Supported countries and regions are USA, Canada, Mexico, Brazil, Germany, Poland, France, Australia, New Zealand, Hong Kong, Taiwan, Japan, Philippines, UK, Turkey, and Thailand. We are working on extending the program to more countries and regions in the coming months.
2. Identity verification is required before verifying one’s education educational status if 1) The names on the credit card associated with your account do not match your enrollment record or 2) PayPal is your preferred method of payment.
3. Student Verification Tips:
• Each account can submit the verification request twice.
• Please use the same name in identity verification (if you are asked to do so as above-mentioned) and student verification.
• If you are not able to request verification, you may not be eligible for this program.
• If you already uploaded supporting documents and are not approved, you can also send an email for details of the disapproval or request for further review.
• If you have an enterprise account, you are not eligible to request for student verification.
4. The offer is limited to 1 server per student. The offer includes:
• $0.99 for select prepaid ECS servers on a yearly subscription. The supported instance types might be changed time to time.
• 10 free clouder certification courses for students and 15 free clouder certification courses for educators. Regular, non-discounted prices apply for subsequent course.
• 50GB data package for eligible students for 12 consecutive months from the date of activation
How to join the education program?
Step 1: Create an account
Register an Alibaba Cloud account and add a method of payment. It is recommended to add your credit card for faster processing time. No upfront cost required.
Step 2: Verify you’re a student
Submit your application to verify your student enrollment status. You may have to take an extra step to verify your identity if your credit card information does not match your enrollment record.
Step 3: Get student benefits
Purchase ECS cloud server for $0.99 and enjoy the free training course. You will also receive 50GB data transfer per month for 12 months after purchasing your ECS instance.
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://lazaroibanez.com/six-advantages-of-cloud-computing-e5781003a727?source=search_post---------248,"Cloud computing seems to be one one of the buzzwords of the decade and more businesses are making the switch every day. It allows you to set up what is essentially a virtual office to give you the flexibility of connecting to your business anywhere, any time. With the growing number of web-enabled devices used in today’s environment access to your data is even easier.
"
https://chatbotnewsdaily.com/4-benefits-of-using-ai-in-cloud-computing-28317d33ac24?source=search_post---------249,"Businesses are constantly on the lookout for new solutions and improvements to make sure they can get even more productive and grow their presence. From enabling their employees to streamline their work, automating a wide array of processes, all the way to establishing greater security, there are so many different ways for businesses to use technology and its many innovations.
However, some combinations are more powerful than others, while there are certain pairs that bring almost immeasurable potential for growth and improvement to companies everywhere. One of those winning combinations is that of using artificial intelligence and cloud computing.
Both have been around for a while now, and companies have already been using them long enough to understand their many perks and ways to implement them. Others, however, are still stuck using some of the more traditional, less productive solutions, and have yet to see the advantages of this duo.
In case you’re about to change your internal organization for the better and implement both, here are a few advantages you can expect to experience within your organization fairly quickly.
Cloud platforms that utilize AI are not just self-managed and almost entirely automated, but they also provide unprecedented levels of security across the board.
The data you store on the cloud will be protected by several layers of security measures integrated within the cloud, and when you add AI into the mix, you’ll practically create an impenetrable wall of safety for your data, employee files, and all other information stored on the cloud.
AI working closely with your cloud will actually be a powerful ally to detect early signs of security threats or weakened measures that you can fix and prevent data breaches. Constant data analysis and implementing various filters can help your AI work in favor of greater cloud security so that collaboration as well as stored data will be safe and sound.
Based on ongoing monitoring and analysis, your AI will quickly come up with new, advanced security protocols to preserve your business integrity.
Sifting through endless piles of constantly multiplying data makes it impossible for your employees to come up with relevant solutions for your business.
AI and the cloud make such human-heavy work obsolete in the best possible way, making room for integrated data analysis without any chance of human error in the process.
Modern-day companies use enterprise cloud computing ecosystems to store mounds of data as well as software tools, including solutions that focus on data analytics and reporting. That means that data analytics based on advanced AI solutions on the cloud can bring numerous data-based reports to help any business improve its position in the industry.
So, in addition to storage, AI paired with the cloud significantly helps with data analytics, reporting, and finding innovative, industry-specific solutions.
Cloud technology has become a norm for every business that wants to offer seamless customer experience. From cloud-based CRM software, to AI-powered chatbots, the benefits of using AI in cloud computing are numerous.
Using AI-powered chatbots will allow you to offer 24/7 support to your customers, which is something that customers have grown to expect in today’s digital world where pretty much everything is available within one click.
Moreover, AI has become so sophisticated that it can gain a comprehensive understanding of problems and offer fast and effective solutions while providing communication experience that much resembles human interactions.
As mentioned before, AI and machine learning play a big role when it comes to collecting and analysing data stored in the cloud, so using this technology to make sense of data collected from customers’ interactions with support chatbots can help you provide even more effective support and make smarter decisions further down the line.
Working with a handful of employees means that you can easily communicate face-to-face at the office, at a meeting, or even via email without cluttering your inbox.
However, when you intend to grow your business, you know you’ll work with many more people on a daily basis, not to mention remote collaborations and freelancers that might jump in on occasion to help you on a project basis.
You can move all of your projects to the cloud, paired with all of the tools that you use, to allow your teams to work together seamlessly. They can have different access levels depending on the security protocols of your business, but they can definitely use the cloud to store projects and collaborate delay-free on each task.
Then again, utilizing AI can help your teams arrange virtual meetings, set up presentations, and implement chatbot technology to keep their workload under control.
Finally, employers can use various AI-based solutions to make employee evaluations more seamless and unbiased, based strictly on their performance and collected data.
There are all kinds of solutions and systems that bring benefits to any business, be it a chat platform that keeps your employees engaged, or a payroll system that automates the process while keeping it safe.
However, few solutions have that almost infinite capacity to bring on more applications and integrations that will allow your brand to evolve, which is precisely what cloud computing and AI, especially combined, do.
By nature, AI is not stagnant, and when implemented correctly, it can improve your entire existing infrastructure and enable you to introduce major system-wide changes to your organization without wreaking havoc.
Then again, the cloud can do the same, allowing you to integrate all of your software solutions, apps, and other internal tools into the cloud, to make them available for all team members and keep your work processes as seamless as possible.
Running a business is a difficult task and a constant challenge, so keeping your mind and your eyes open for novel solutions is the only way to keep up with the constantly changing business climate and to find ways to move forward.
Thankfully, advanced AI solutions paired with cloud computing provide promising options for businesses of all scopes and sizes, making them must-haves for any organization aiming to grow.
Author bio: Elaine Bennett is a digital marketing specialist focused on helping startups and small businesses grow. Besides that, she’s a regular contributor for Bizzmark Blog and writes hands-on articles about business and marketing, as it allows her to reach even more entrepreneurs and help them on their business journey.
Your #1 Source about Chatbots in Europe
Written by
Your #1 Source about Chatbots in Europe
Your #1 Source about Chatbots in Europe
Written by
Your #1 Source about Chatbots in Europe
Your #1 Source about Chatbots in Europe
"
https://movebla.com/cloud-computing-a-moda-agora-%C3%A9-fog-computing-16384178e273?source=search_post---------251,"Aplicativos na cloud (nuvem). Não tem palavra mais usada nos últimos anos que essa para definir a tecnologia que usamos no cotidiano. Serviços, tecnologias, aplicativos. Tudo foi pra nuvem. O que está gerando nuvens bem gordinhas. Segundo levantamento da Cisco, serviços na nuvem geram aproximadamente 2.5 exabytes de dados por dia!
Quando uma empresa coloca seus dados na nuvem, ela procura economizar com o custo e o trabalho de armazenar dados por conta própria. E o resto ela resolve com uma conexão de alta velocidade. Porém, nesse mundo de conectividade de massa em que estamos, as limitações das redes sem fio ainda são muitas. Imagine por exemplo navegar num app na nuvem com uma conexão EDGE. Impraticável. Por isso mesmo aplicativos são o que são em dispositivos móveis: parte das informações e do poder de processamento fica a cargo do dispositivo.
Isso uma hora vai ser caótico. Se cada vez mais temos dispositivos inteligentes e conectados, como dar conta de que tudo na nuvem rode em uma velocidade satisfatória? Empresas como a Cisco já estão trabalhando nisso e surgiu um novo termo: a computação em névoa (ou fog computing).
Um aplicativo na “fog” funcionaria basicamente como na cloud: dados, processamento, serviços, tudo lá. O que muda é a distribuição dessas nuvens. Imagine uma ponte entre as nuvens, que facilitaria o caminho de dados entre elas. Este é o papel das névoas (fog). É como se a nuvem estivesse mais perto da gente para que o acesso a seus dados seja mais rápido e de performance satisfatória. Não estamos falando de servidores melhores, mas sim máquinas menos poderosas e dispersas, criando o caminho entre a nuvem e o dispositivo a receber a carga de dados.
A princípio, a Cisco trata a névoa como uma nova arquitetura. Cada roteador ou equipamento de rede da Cisco vai ganhar poder computacional (!) para dividir com a nuvem o poder de processamento de um serviço. E essa nova arquitetura (denominada IOx) vai estar nos equipamentos da Cisco antes do começo do segundo semestre. É como se cada roteador pudesse rodar um Linux próprio.
A ideia da Cisco é que a arquitetura seja um padrão de conectividade que desenvolvedores possam usar em seus serviços. Assim, aproveitariam o poder a mais dessas “névoas” para acelerar tarefas simples que precisam ser acessadas com rapidez. Por exemplo, imagine um carro conectado. Que precisaria reportar continuamente que está em boas condições. Um roteador “névoa” pode coletar esses dados e processá-los por conta própria. E depois reportar esse processamento para a nuvem por uma conexão de internet. Melhor do que mandar milhares de mensagens “está tudo ok” por uma rede que pode não ser tão veloz (3G em áreas urbanas, por exemplo).
Se isso vai pegar, depende muito do alcance da Cisco, uma das mais fortes fornecedoras de equipamentos de rede do mundo. Mas é fato de que se queremos um mundo 100% na nuvem, vamos precisar de uma boa ajuda de processamento local.
O mundo do trabalho mudou. Vamos falar sobre isso?
Written by
Estudando colaboração e cultura pop. Cosplay de mim mesmo na falta de verba.
O mundo do trabalho mudou. Vamos falar sobre isso?
Written by
Estudando colaboração e cultura pop. Cosplay de mim mesmo na falta de verba.
O mundo do trabalho mudou. Vamos falar sobre isso?
"
https://medium.com/techloy/kenya-is-hosting-the-east-africa-big-data-analytics-and-cloud-computing-summit-next-month-340e283781e1?source=search_post---------253,"Sign in
There are currently no responses for this story.
Be the first to respond.
Techloy
Apr 16, 2018·1 min read
Is East Africa ready to unlock the big data value?
The East Africa Big Data Analytics and Cloud Computing Summit was born out of a much-needed opportunity to unite the data and analytics players and potential end-users of their expertise.
The event, scheduled for May 2nd and 3rd, 2018 at the Radisson Blu, Nairobi, Kenya, will feature 10+ industry expert keynote presentations, 12 panel discussions and 10 breakout sessions/workshops.
Topics that would be covered at the Summit include Big data analytics, Machine learning techniques, Predictive modeling and analytics, Data security, Data mining, Cloud computing and Cyber security.
Register here.
Covering news and insights into technology and business in Africa. Get in touch: techloy.com@gmail.com | +2347030851462
Techloy is a digital publication covering news and data about business, finance and technology, using infographics and charts to help people and brands make better decisions.
"
https://beehivestartups.com/six-utah-companies-recognized-for-cloud-computing-prowess-747df47007db?source=search_post---------254,"The requested URL was not found on this server.
Additionally, a 404 Not Found
error was encountered while trying to use an ErrorDocument to handle the request.
"
https://medium.com/@GregHemmings/thoughts-on-public-enemy-throwing-away-cds-and-the-carbon-costs-of-cloud-computing-e10df7094cf6?source=search_post---------255,"Sign in
There are currently no responses for this story.
Be the first to respond.
Greg Hemmings
Dec 30, 2015·3 min read
1989…a number…another summer… get down! To the sounds of the funky drummer…
This is my blog of adventures and insights as a positive social impact filmpreneur.
2 
2 claps
2 
This is my blog of adventures and insights as a positive social impact filmpreneur.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@dalmaer/the-true-promise-of-modern-cloud-computing-empowering-imagination-and-creation-7ab1593bc8d9?source=search_post---------256,"Sign in
There are currently no responses for this story.
Be the first to respond.
Dion Almaer
Aug 14, 2013·3 min read
I was at the lovely Portland airport with Anthony Marcar who introduced me to Antoni Batchelli of Pallet fame.
For those who don’t know, Pallet is a fascinating take on programmatic automation of infrastructure, (written (in Clojure)). We like it a lot and it powers some of our eReceipt infrastructure. It flips the model of Chef and Puppet to some extent, so your programs become really aware of their environment and can scale themselves.
One comment from Antoni really hit home for me that day:
Creating instances is so inexpensive now that you can start to think of them in the same vein as we used to think of ‘creating a tmp variable’.
If you mull on that for a minute you start to re-think how you architect things in fundamental ways. E.g. Instead of trying to fix nodes, just nuke them and start a new instance!
When the price of something becomes so cheap that you can afford to throw it away (or recycle it) rather than fix it, the game often changes. This can sometimes be bad, when all of the costs are not accounted for (e.g. it doesn’t seem right to ship water in plastic bottles from Fiji… you know, for the environment). When the flywheel is positive though, innovation can happen.
When I think about what makes humans interesting and different, a lot of it comes down to the fact that we can imagine and create. What does that mean?
You can make something appear in your head. If you read the word “Elephant” it has probably just appeared for you. This is pretty awesome, being able to place different realities into your mind
You can go even further than that, and create an image in your head of something new… a creation! It is pretty amazing that we can come up with new creations isn’t it?
I was looking at Joyent’s Manta service, which combines the worlds of compute with object storage. This unification is very interesting, and it made me think of creation in the mind.
What will be created in a world where it is cheap to scale computation tied to the data that you need?
With Manta I can create these worlds that contain the data that I need, with the environment surrounding the data. E.g.
I have this archive which contains the data that I care about as well as some node.js programs. Start up a few thousand instances of that world and Map Reduce (“OH: split and concat in Unix terminology ;)”) across them.
And just like that, you conjured a new universe, output a new creation, and the party moves on. As the technology matures the conjuring infrastructure is going drastically down, which means the limits are more on your imagination.
Fortunately time has told us that the human imagination is somewhat boundless.
Developers @ Shopify; Karaoke Ready
Developers @ Shopify; Karaoke Ready
"
https://pieceofcakelabs.com/free-cloud-resources-to-get-hands-on-cloud-computing-310fe920553c?source=search_post---------257,NA
https://medium.com/@alibaba-cloud/the-power-of-cloud-computing-in-ai-technology-285839e91a2?source=search_post---------258,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Jul 27, 2020·13 min read
At this Apsara Conference, Alibaba revealed the numbers behind its AI prowess along with a host of new products and solutions in AI.
During this year’s Apsara Conference, Alibaba revealed the numbers for the adoption and processing capabilities of Alibaba’s AI Platform for the first time. From these figures, you can catch a glimpse of the true power of Alibaba’s AI Platform:
Also at the conference, Alibaba, and more specifically Alibaba Cloud, showed how they were leaders in everything AI related-from straight down to the AI chips to a whole host of applications and from AI-empowered cloud services and platforms to algorithms and other industrial solutions.
An avid fan and contributor to Alibaba Cloud created the below illustration of Alibaba Cloud’s AI architecture. The illustration makes for a good summary of the AI offerings at Alibaba Cloud and where they appear in Alibaba Cloud’s overall AI architecture:
To get to where Alibaba is now, Alibaba Cloud had to make great strides in chip design, machine learning algorithms, and along with its service and platform offerings powered by AI.
Alibaba Cloud’s semiconductor subsidiary Pingtouge launched the AI inference chip Hanguang 800 back in September of this year. As releaved in the conference, the chip is so powerful that it has the equivalent computing capabilities of 10 GPUs, ranking first among chips in terms of performance and energy efficiency in the world.
Alibaba Cloud ranks as one of the top cloud service providers in the world and the number one provider in the Asia Pacific region. Alibaba Cloud offers the most diverse and largest-scale AI cluster offering of any cloud service provider in Asia, as a stern believe and prominent leader in the public cloud market with a vast selection when it comes to processing platforms, of which there are CPUs, GPUs, FPGAs, NPUs, super computing clusters, and, of course, the new third generation of X-Dragon Architecture. Naturally, the suite of services offered by Alibaba Cloud are well integrated with each other, working together to provide unparalleled support to industrial AI applications.
In particular when it comes to AI, Alibaba Cloud’s platforms are the Apsara AI Platform, Apsara Big Data Platform, and AIoT Platform. These platforms are helpful to developers as they can significantly lower the threshold for developers to work in the new and exciting field of AI. Adding to this, the Apsara AI Platform is the first commercial, cloud-based machine-learning platform in China to offers support for large-scale algorithms. This platform is able to process tens of billions of features and around 100 billion training samples.
When it comes to the research about AI algorithms, Alibaba Cloud has won over 40 firsts for papers on a variety of topics, including AI algorithms for natural language processing, speech recognition, and computer vision.
Alibaba Cloud’s AI offerings provide a host of services inside and outside of Alibaba Group.
Within Alibaba Group, the Ali Xiaomi (literally Ali honeybee) Chatbot serves more than 5 million users each day. And, when it comes to AI-power automation applications, Alibaba Cloud has invented heavily into smart speakers and autonomous driving. Alibaba’s AliGenie is currently the largest Chinese-language intelligent voice assistant in the world. And Alibaba has taken the plunge to make autonomous driving applications continue to evolve from individual AI-powered vehicles to vehicle-road coordination.
Alibaba Cloud is a front-runner in applying AI in several different industrial scenarios. For example, in as early as 2015, Alibaba Cloud worked together with ecosystem partners to actively promote the application of Internet technologies in traditional industries. Similarly, Alibaba Cloud’s AI technologies and applications, including those used in the solution ET City Brain, which have already enjoy massive application in a variety of industries, including transportation, public safety, and even medical services.
(https://www.alibabacloud.com/blog/investing-in-ai-the-smart-way-lessons-from-7-tech-giants_594063)
In this blog, we will discuss the “deep value” aspect of artificial intelligence, which every AI investor and enterprise should consider.The advent of artificial intelligence (AI) triggered a research and development (R&D) boom in various industries around the world. Due to powerful computing capability and excellent smart system functionality, AI has become more critical to people’s everyday lives. In turn, this has led tech giants and venture capitalists to increase investment in artificial intelligence companies.
As AI becomes more popular around the world, CCID expects that the global AI industry market is likely to approach USD $39 billion in 2018 and USD $58 billion by 2020. In the eyes of many investors, artificial intelligence technology can make more informed decisions, allowing entrepreneurs and innovators to create products of even higher value for their customers.
However, judging by the history of the two waves of the artificial intelligence technology, as well as the decisions of global tech companies such as Alibaba and Apple, focusing on AI technology investment alone is not a wise decision. But why? Because the deeper value behind the technology is even more critical.
Data from CB Insights shows that the total amount of financing for artificial intelligence startups worldwide reached a record 15.2 billion USD in 2017. Internet companies have become more prominent in artificial intelligence investment. According to PitchBook data, the artificial intelligence and machine learning field received a total of more than 10.8 billion USD in venture capital in 2017. These figures indicate that the market is optimistic about the future of artificial intelligence. AI enterprises hope to empower AI technology, inevitably transforming artificial intelligence into a representative trend of our generation.
However, as far as the currently popular AI goes, we are already in its third wave of popularity. In March 2016, AlphaGo’s game of Go against Lee Sedol brought this technology to the forefront of the public consciousness, putting the words “artificial intelligence” on everybody’s lips.
As for the first two waves of popularity, the first happened in the 1950s, when Alan Turing proposed the symbolic Turing test. Milestone technologies and applications such as mathematical proof systems, knowledge reasoning systems, and expert systems had suddenly set off the first artificial intelligence craze among researchers.
The second wave came about in the 1980s, when technology based on statistical models quietly emerged. Not only did it see the progression of speech recognition and machine translation technologies, but artificial neural networks also found application in areas such as pattern recognition. More importantly, in 1997, the Deep Blue computer system defeated the human chess master Gary Kasparov, which brought public enthusiasm to a peak.
It was during this period that AI first received significant hype and rapid increases in investment. Due to the situation at that time, investors did not adequately consider the value behind these entrepreneurial ideas but merely raised funds for what they believed to be an exciting technology. This led to the failure of most of the first generation AI startups, and they eventually faded away. For example, artificial intelligence companies founded in the 1980s such as Symbolics, IntelliCorp, and Gensym have either wholly transformed or no longer exist.
Now, 40 years later, we are facing a very similar problem. Even though the technology today has become more complicated, there is still the undeniable fact that AI has yet not created sufficient value for consumers. This is why Xiao Zhijun believes that investing in AI or “deep technology” is not a wise decision. To the contrary, it is the outlook of Xiao Zhijun that the deeper value behind AI, rather than the technology itself, should be the target of investment.
Neural synapses in mammal brains are also capable of performing deep learning. New advances in AI and neuroscience aim to emulate, or even surpass, th.Neural networks are inspired by the biological neurological network system. But do our brains learn the same way as a computer does deep learning? The answer to this question can possibly bring us to a more powerful deep learning model, and, on the other hand, also help us to better understand human intelligence.
Do Biological Systems Use Deep Learning?On December 5th, Blake A. Richards — a CIFAR researcher from the University of Toronto — and his colleagues published an article on eLife titled Towards deep learning with segregated dendrites. They described an algorithm with which to model deep learning in human brains. The network they built indicates that the neural synapses of certain mammals have the correct shapes and electric characteristics to make them suitable for deep learning.
Not only that, their experiments showed how the brain works with deep learning in a method that’s surprisingly close to the natural biological model. This is hopeful in helping us further understand how we evolved our learning ability. If the connection between neurons and deep learning is confirmed, we can develop better brain/computer interfaces and we will likely gain a number of new abilities ranging from treating various kinds of disease to augmented intelligence. The possibilities seem endless.
Evidence of Deep Learning on Neurons in a Mammalian BrainThis research is done by Richards and his graduate student Jordan Guerguiev, along with DeepMind’s Timothy Lillicrap. The neurons used in the experiment are the cortex dendritic cells of a mouse brain. The cortex is responsible for some high order functions such as sense, movement, spatial logic, consciousness and human languages. The dendritic cells are the bumps derived from the neurons. Under the microscope, they look somewhat like a tree bark. Dendritic cells are the input channel of neurons and they deliver electrical signals from other neurons to the main body of a neuron cell.
Using the knowledge of this neuronal structure, Richards and Guerguiev built a model called the “multi-compartment neural network model.” In this network, neurons receive signals in separate “cubicles”. Due to its piecemeal nature, different layers of simulated neurons can cooperate to achieve deep learning.The results indicate that when recognizing hand-written numbers, a multi-layer network is significantly better than a single-layer network.
Algorithms that use multi-layer network structures to identify higher order representations are at the core of deep learning. This suggests that mouse brain neurons are able to do deep learning just like artificial neurons. “It’s just a set of simulations, so it does not accurately reflect what the brain is doing, but if the brain can use the algorithms which AI are using, that’s enough to prove that we can conduct further experiments,” said Richards.
In the early 2000s, Richards and Lillicrap took Hinton’s classes at the University of Toronto. They were convinced that the deep learning model to some extent accurately reflects the mechanics of the human brain. However, there were several challenges to validating this idea at the time. First of all, it is not yet certain whether deep learning can reach the level of complexity in a human brain. Second, deep learning algorithms typically violate biological facts, which have already been demonstrated by neuroscientists.
The patterns of activity that occur in deep learning by computer networks are similar to the patterns seen in the human brain. However, some of the features of deep learning seem to be incompatible with the way the human brain works. Moreover, neurons in artificial networks are much simpler than biological neurons.
Now, Richards and some researchers are actively seeking ways to bridge the gap between neuroscience and artificial intelligence. This article builds upon Yoshua Bengio team’s study on how to train neural networks in a more biologically viable way.
Alibaba Cloud heterogeneous platform for elastic computing aims to provide high-quality services for organizations to realize scientific and technological innovations.The heterogeneous computing technology is evolving rapidly in the field of big data and AI in recent years. To cope with this technology revolution, Alibaba Cloud Heterogeneous Computing has made great achievements in both product type and application.
Pan Yue, a senior product expert of Alibaba Cloud, made an in-depth presentation on Alibaba Cloud Heterogeneous Computing in the era of big data and AI at the Computing Conference Shenzhen summit on the morning of March 29, 2018.
Pan Yue introduced that Alibaba Cloud provides a wide range of “heterogeneous acceleration platforms for multiple scenarios”, including GA1 instance (AMD S7150) for graphic and image rendering, GN5 (Tesla P100) instance for AI training and reasoning, GN5i (Tesla P4) instance for AI reasoning and video transcoding applications, and GN6 (Tesla V100) instance for advanced computational capabilities in the fields of AI and high-performance computing, and multiple FPGA instances for image transcoding, genetic computing, and database acceleration.
GN6 instance is specifically designed for deep learning training and high-performance computing. With Tesla V100 in the latest NVIDIA Volta architecture, GN6 instance delivers 12X higher computing performance than Tesla P100 (the previous generation), solving the outstanding problems for engineers and experts. GN6 (Tesla V100) is now available for public beta and will be coming soon.
Alibaba Cloud has been and will be always professional and precise towards issues related to heterogeneous products. Most people may choose Pascal-Based Tesla P40 as the GPU because of its higher single-precision floating-point capability, which is 12 teraFLOPS (TFLOPS) while only 10.6 TFLOPS for Tesla P100, according to the product manuals. However, Alibaba Cloud performed tests in different scenarios and methods and found that the performance of Tesla P100 is 20% higher in AI application. Here are the test results:
This article describes how to use the Online Predictive Deployment feature of Alibaba Cloud’s Machine Learning Platform for AI (PAI) to monitor user health in real time.
Models generated on the Alibaba Cloud Machine Learning Platform for AI (PAI) can be deployed online to generate APIs that can be invoked by other services. This document is based on the Heart Disease Prediction Case, and describes how to use the Online Predictive Deployment feature of the machine learning platform to monitor user health in real time.
Click Deploy in the lower section of the current experiment interface and select Online Predictive Deployment. Select the logistic regression model generated in the heart disease prediction case, as shown in the following screenshot.
Step 2: Model Deployment Information ConfigurationGo to the model configuration page, as shown in the following screenshot.
Select a corresponding project. If you are using this for the first time, you need to enable the online prediction permission, which will be enabled in real time upon request. Set the number of instances occupied by the current model. Instances are described as follows.
IoT, Big Data and AI are three of the most popular terms of recent times. In this course, we will not only introduce how these technologies are linked in Alibaba Cloud, but also how they have paved the way for the technological progress we have come to expect to help us to win the second half of the internet era.
In this course, you will have an overview of Big Data and AI Productes to empower your business in an easy way.
This course will introduce Alibaba Cloud’s ultra-intelligent AI Platform for solving complex business and social problems. Powered by new advanced technologies, Alibaba AI technology is powering global breakthroughs in artificial intelligence and machine learning.
This session introduces how to use Alibaba Cloud Machine Learning Platform For AI to create a heart disease prediction model based on the data collected from heart disease patients.
How to use Alibaba Cloud advanced machine learning platform for AI (PAI) to quickly apply the linear regression model in machine learning to properly solve business-related prediction problems.
To upload data on the Machine Learning Platform for AI web interface, make sure that the data is less than 20 MB. To upload data that is greater than 20 MB, you must download the MaxCompute client and then use the tunnel command.
To set the algorithm parameters, drag an algorithm component to the canvas and click the component. The corresponding parameters are displayed in the right-side pane.
If Machine Learning Platform for AI has successfully run a component, it marks the component with a green check. You can right-click a component with a green check to view data or evaluation results.
To generate a model, you must first select Setting > General > Auto-generate PMML from the left-side navigation pane. After successfully running an experiment, you can select Model from the left-side navigation pane to check the corresponding model. To view the model parameters, right-click the model. To download a model, right-click the model and select Download PMML.
PMML is a standard model description file. A PMML file downloaded from Machine Learning Platform for AI can be applied to open-source engines, such as Spark.
The following figure shows the architecture of the Machine Learning Platform for AI.
The bottom layer is the infrastructure layer that consists of CPU and GPU clusters.
The second layer from the bottom is the Alibaba computing framework, which includes MapReduce, SQL, MPI, and other computing methods.
The layer in the middle is the model algorithm layer, which includes the data preprocessing, feature engineering, and machine learning algorithm and other basic components to help users complete certain fundamental jobs.
The top layer is the application layer. The data mining of Alibaba internal search, recommendation, Ant Financial and other projects depends on the Machine Learning Platform for AI.
Intelligent Speech Interaction is developed based on state-of-the-art technologies such as speech recognition, speech synthesis, and natural language understanding. Enterprises can integrate Intelligent Speech Interaction into their products to enable them to listen, understand, and converse with users, providing users with an immersive human-computer interaction experience.
Relying on Alibaba’s leading natural language processing and deep learning technology, based on massive e-commerce data, we provide customized high-quality machine translation services for Alibaba Cloud users.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/cloud-computing-made-really-easy-with-sas-from-alibaba-cloud-4351c530f727?source=search_post---------259,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Sep 10, 2020·7 min read
If you’re daunted by the prospect of setting up a blog or e-commerce site in the cloud, Simple Application Server from Alibaba Cloud is for you. The number of available configuration options has been pared to a minimum so you can be up and running in just a couple of minutes, even if you’re not an IT expert and have never created a cloud server before.
Cloud computing has revolutionized modern IT. Companies are moving existing data centers to the cloud in order to save money and to become more agile. Organizations that previously constructed brand new data centers are now choosing the cloud instead, to avoid the large costs traditionally associated with planning, building, commissioning and managing a new installation.
Cloud providers such as Alibaba Cloud offer a wide range of features that allow even the largest company to replicate their complex data center off-site. Using our flagship product, Elastic Compute Service (ECS), customers can choose from a range of geographical regions and availability zones, and can also replicate key servers across zones/regions in order to provide resilience, fault tolerance and guaranteed availability. They can pay for servers by the hour or month, or use more complex billing arrangements such as buying preemptible instances, where you bid the maximum amount you’re willing to pay and the server will automatically start running when the market price reaches the bid amount.
Alibaba Cloud ECS offers hundreds of combinations of server architecture, both as virtual machines and Bare Metal Instances, or even super-computing clusters. You can choose whether your server is optimized for raw compute speed, or memory, or Big Data. You have a choice of operating system, and dozens of preconfigured images.
If you know that your bandwidth requirements, i.e the outbound internet traffic from your server, will be relatively constant then you can opt to pay for this traffic by bandwidth. If traffic levels are less predictable you can choose a more flexible option, where you pay a small fee for each gigabyte used.
You can create one or more Virtual Private Clouds and virtual switches to contain your ECS cloud servers, and assign IP address ranges to each. These, along with firewall rules and security group settings, ensure that any servers which need to communicate with each other can do so.
And, perhaps even more importantly, vice versa. For added security, Resource Access Management is a fully featured identity and access control system that allows you to grant access to your cloud datacenter resources to individual employees or contractors on a highly granular level.
When high performance and 24/7 availability are crucial, you can create multiple servers and assign them to a load balancer which will automatically direct incoming traffic to the least busy node. If demand is going to be irregular, auto-scaling will automatically spin up additional servers when needed, and release them when demand drops.
As an alternative to cloud servers, you can run containers using industry standard technologies such as Container Service for Kubernetes. Or you can go serverless with Alibaba Cloud Function Compute.
And of course, no cloud-based data center is complete without databases. You can install your own RDBMS on one or more cloud servers, or you can use Alibaba Cloud ApsaraDB RDS. For massive, unlimited storage, you might prefer Object Storage Service
All of these features, and more, make Alibaba Cloud the ideal choice for creating cloud-hosted datacenters. But what if you’re not an IT expert? What if you are a small startup without dedicated IT staff, and you don’t have the in-house skills to understand and configure everything we’ve talked about so far? Perhaps you simply want to create a small WordPress or e-commerce site and you don’t want to pay a consultant to help you set it up?
The good news is that Alibaba Cloud has exactly what you need. The product is called SAS, or Simple Application Server. Unlike ECS with its multitude of options, SAS is incredibly quick and simple to set up, and you don’t need to be an IT whizz. You can choose from a small set of carefully-curated server images that are already installed with all of the software you need, ready to run. These include WordPress and OpenCart. So once your SAS server is created, you’re ready to go. In the case of WordPress, this means you can log straight into your new blog and start creating content.
As an example, let’s run through the steps involved to create an Simple Application Server cloud server running WordPress from scratch. You start by logging into your Alibaba Cloud account and choosing SAS from the console screen, and then clicking the button to create a new instance.
The new instance screen is very easy to understand and there are only four things that you need to choose before your server is ready to launch. First, the region in which your server will be hosted. Currently supported options are Silicon Valley, Sydney, Frankfurt, Hong Kong and Singapore. If your business is based in China, choose Hong Kong. Otherwise choose the location that is closest to the majority of your users.
Next, you choose your server image. Pre-configured instances include WordPress, Joomla, Drupal, OpenCart and Plesk. If you’d prefer a bare operating system with no other applications, ready for you to configure as you wish, you can choose from various versions of Linux and Windows Server. For this example we’d choose a WordPress image running on Linux.
OK, two steps down and two to go. In step three you choose your pricing plan. There are currently six available, and they all include everything you need for your server: CPU cores, memory, a solid state SSD disk, and data transfer. You can add an extra data disk if you want one, up to 16 TB, but for a simple site such as a blog this probably won’t be necessary.
For Linux-based servers, even those with an application such as WordPress already installed, the price starts from just US $3.40 a month. This is perfectly sufficient for a site that gets a few thousand hits per day. Windows-based server pricing is higher but it includes the Windows Server licence.
The fourth and final step is to choose your subscription option. How many months do you want the server to run for? And once that period is up, do you want the service to renew automatically? By default, the subscription won’t auto-renew, thus helping you to avoid any unnecessary expenses.
When you’re ready, you click on the Buy Now button and after a minute or so your server will be up and running, and in the case of this example your WordPress site is now ready to use. The server will show up in the list of SAS instances on your management console.
At the bottom right hand corner of the server’s information panel you can see its public IP address. To view the WordPress site you simply enter this address into your browser. To be able to access it by name, you’ll need to log into the site on which you registered your domain name and “point” the name at this address.
To explore your new server from the command line you can log in from your browser. Just click the icon above the IP address, at the top of the information panel. No password is needed because you are already logged into your Alibaba Cloud account.
While you’re logged in, you can look up the admin password for managing the WordPress site. At the command prompt, type:
Make a note of the password then log out by typing exit. To reach the admin area of your new WordPress site, add /wp-login.php to the end of the IP address when typing it in a browser. Log in with a username of admin, and the password you just retrieved from the server. You can now start creating blog articles, applying a theme, installing add-ons, and so on.
Alibaba Cloud’s ECS service allows companies and IT experts to plan and create complex cloud-based networks and data centers. If you’re not an IT expert and you want to create a cloud server, you will find it much less daunting to use the SAS service instead.
SAS lets you create a fully functional cloud server, complete with WordPress or other market-leading application already installed, in just a couple of minutes. It’s ideal for small companies and startups which don’t have complex needs, or for anyone who wants to experiment with cloud servers that don’t need to communicate with each other. And with prices from less than $4 per month it’s an incredibly cost-effective way to introduce yourself to the power of cloud computing.
If you don’t already have an Alibaba Cloud account, you can create one for free at https://www.alibabacloud.com/
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/bits-n-pieces/what-is-cloud-computing-all-about-a2c271dd857f?source=search_post---------260,"Sign in
There are currently no responses for this story.
Be the first to respond.
Sanchit Gera
Jan 6, 2021·4 min read
Cloud computing is a roughly 400 billion dollar industry that has completely transformed the way software gets built today. But cloud computing as we currently use it is a fairly recent idea.To fully understand what it is and why it is all the rage, you first need to understand what came before it.
Let’s say it’s 1995 and you’re a young founder of hot internet startup. You’ve a great million dollar…
"
https://medium.com/@beth-kindig/how-to-pick-long-term-stock-winners-in-cloud-computing-beth-technology-44a827ee2be4?source=search_post---------261,"Sign in
There are currently no responses for this story.
Be the first to respond.
Beth Kindig
Sep 13, 2019·7 min read
Cloud software stocks suffered a reversal that has produced losses of close to 50% from record highs.
The story for those stocks hasn’t changed, but the valuations have, and that could be a good thing for investors who know what they own.
The biggest risk for investors in cloud stocks isn’t the losses that have pummeled prices over the past two weeks, but rather the big reversal that may scare them away from the sector. It’s painful to watch large declines in stocks, yet nobody wants to miss out on a potential 10-bagger either. When the market rewards, and penalizes, all cloud software stocks equally, with little differentiation, it’s prudent to choose a select group that has compelling stories for a buy-and-hold strategy.
An investing adage is to buy when others are fearful. I would say to buy when others can’t differentiate among companies. Clearly, from what we saw over the past two weeks, a broad range of companies are being lumped together, with little recognition as to which are the winners.
To put it simply, this is a great time to know what you own as the story for these stocks is much deeper than the simple descriptor of “cloud software.”
In the graphic above, the orange line represents the First Trust Cloud Computing ETF SKYY, +0.12% which holds about 60 positions, all of which dominate the cloud space. The light-blue line is the Consumer Staples Select Sector ETF XLP, +0.41%, which tracks the consumer staples sector, a group that’s thought to be impervious to recessions. Those have historically been the laggards in this long, growth-driven bull market. The dark purple line is the benchmark S&P 500 Index SPX, +0.29%.
Before the stock market correction in May, cloud stocks were the leader in the market, while the staples were trailing. However, since that first correction, and up till today, there’s been a reversal. Over the past week high-growth leaders in this bull market, mostly cloud, have taken big hits.
For example:
Workday WDAY, -1.26% is down 25% from its high. Twilio TWLO, +3.08%, down 26%. Okta OKTA, -0.20%, down 22%. Zoom ZM, +1.02%, down 27%. MongoDB MDB, -1.57%, down 29%. PagerDuty PD, -0.38%, down 47%. (All prices are current as of 2 p.m. Eastern time Sept. 11.)
To start, cloud software needs to be broken up into categories to look more closely at the markets they serve. Here are some examples:
• Twilio is at the intersection of communications and mobile.
• CrowdStrike CRWD, +0.80% and Okta are security companies.
• PagerDuty simplifies operations, and Workday simplifies human resources and the finance department.
• Alteryx AYX, -0.74% and Splunk SPLK, +0.02% are big data analytics.
• Salesforce CRM, +0.43% is customer relationship management (CRM), but the market it serves is the sales and marketing industry.
• Zoom simplifies communications for enterprises and business-to-business (B2B), as does Slack WORK, +1.47%.
• Veeva Systems VEEV, -0.08% serves the life sciences and pharmaceutical industry.
Twilio has more in common with Skype, and even Verizon VZ, +0.50% and AT&T T, -0.93%, than it does with Okta. Workday has more in common with SAP and Oracle ORCL, -4.26% than it does with Alteryx. Yet, cloud stocks experienced a categorical black swan as if they all serve the same markets.
What this means is that some investors don’t understand these companies. The viability of a company’s product and how it fits the market is not factored into the investments, and this is creating a window of opportunity for investors who take the time to study individual stocks.
The more logical explanation is that there was a clearance sale for overpriced stocks, and the common denominator in the sell-off was high valuations. However, the issue with this theory is that some of the companies will go on to be big winners, and higher price-to-sales (P/S) or enterprise-value-to-sales (EV/S) ratios are warranted because of the enormous markets they serve in contrast with their small size.
For instance, Zoom’s P/S and EV/S are gut-wrenching (no argument there), but the company’s revenue growth is unusual. You’d be hard-pressed to find triple-digit revenue growth for eight straight quarters in the stock market. The prospects of disrupting Cisco and other enterprise telecommunications at a $22 billion market cap is worth more than other companies that are serving smaller, more saturated markets. Zoom’s rapid growth, which is unprecedented, makes it hard to pinpoint a fair valuation.
Workday is another example of a company that carries high P/S and EV/S ratios, in this case twice as high as its large competitors, Oracle and SAP. In this scenario, you get a pure-play option that is moving quickly on machine learning to reduce the overhead required in human resources and finance departments. What Workday’s software aims to do, which is to reduce the number of employees needed in those departments, delivers a value that is worth many times over the cost of the software. If Workday is successful, the company will be worth much more than two times its current market cap, whereas, Oracle and SAP are essentially defending territory.
See: Beth Kindig runs a forum on tech stocks where she answers readers’ questions.
Splunk has P/S and EV/S ratios of eight compared with Alteryx’s 24 and 22, respectively, yet both were affected by the sell-off despite being in similar markets. Splunk should have held steady if this was a clearance sale on high valuations.
The evidence doesn’t point to a rational reason for the sell-offs. Some stocks are priced high, but knowing which ones deserve to be is going to be more important than ever.
• The larger the market, the safer the investment. Can every enterprise employee in the world use the product for maximum scale? Does the product solve a pain and reduce overhead for businesses? These will outlast the more niche markets and products that are considered a convenience. To illustrate, if you are providing software for office communications that replaces office telecom equipment, not only is your product a necessity but it will be the solution to high telecom bills during a time when costs are being cut. There are numerous examples of fulfilling a (non-negotiable) necessity while reducing costs in the cloud software category.
• Ignore earnings estimates. Many estimates were lowered this past year, and when companies “beat” earnings estimates, they were actually declining year-over-year, sometimes substantially. Apple AAPL, -0.23% is a prime example of this. The stock continues to reach all-time highs and “beat earnings” despite two straight quarters of negative growth and mere 1% growth in the most recent quarter. That may work for Apple, but smaller-cap companies that are declining won’t last long, especially in a value rotation. Another example is Okta, which I believe had weakening fundamentals yet beat earnings estimates. Okta is now one of the hardest-hit cloud software stocks over the past two weeks, with a 22% drop since the end of August.
• We hear a lot about competitive moats, yet high switching costs is a protective buffer that serves two purposes: It locks in subscription revenue and staves off competitors. Often, switching from a cloud software provider will cost a customer time and resources. Look for companies that have high switching costs.
My prediction is this may be one of the last cycles when tech is considered less safe than value stocks. As the market will find out (the hard way), cloud software is actually very safe. It is insulated from trade wars and overseas manufacturing issues. It reduces costs for enterprises, which is ideal for a recession. Lastly, cloud software is at the beginning of a rapid growth cycle compared to its counterparts in tech — such as mobile, e-commerce and advertising — which are reaching saturation, are finding themselves in the cross hairs of anti-trust and are susceptible to consumer spending changes.
The best companies in the category of “cloud software” will continue to post rapid growth regardless of economic conditions, and the investors who run from this sector will suffer bigger losses from missed opportunities than investors who know their winners.
This article appeared on MarketWatch September 12th, 2019.
I’m an industry insider who writes free in-depth analysis on public tech companies. In the last 12 months, I predicted Facebook’s Q2 crash, Roku’s meteoric rise, Uber’s IPO flop, Zoom’s IPO success, Google’s revenue miss and more. Be industry-specific. Know more than the broader markets. Sign up now. I look forward to staying connected.
If you are a more serious investor, we have a premium service that offers institutional-level research and entry/exit options. This membership offers a competitive edge in identifying growth opportunities and reducing risk in the tech sector. Learn more here . Join 3,003 other tech investors who receive weekly stock tips:
Originally published at beth.technology on September 13, 2019.
Senior Product Evangelist in data and security. All things #startups #mobile, #data #security and #IoT. Snowboarder, book worm.
See all (1,477)
Senior Product Evangelist in data and security. All things #startups #mobile, #data #security and #IoT. Snowboarder, book worm.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/peloton-engineering/the-future-of-cloud-computing-and-databases-nosql-meetup-66b259246d3a?source=search_post---------262,"There are currently no responses for this story.
Be the first to respond.
On Monday, June 8, 2015, we will be hosting A Database Month event with a talk from Zigmars Rasscevskis, Chief Executive Officer, Clusterpoint. There will be pizza, refreshments, t-shirts & swag, and a chance to win an Oculus Dev Kit.
You can read all about the event in detail on Meetup.com.
The official Blog for Engineering @ Peloton
The official Blog for Engineering @ Peloton
Written by

The official Blog for Engineering @ Peloton
"
https://medium.com/@SchneiderElectric/where-is-the-edge-in-cloud-computing-f50f55d657bd?source=search_post---------263,"Sign in
There are currently no responses for this story.
Be the first to respond.
Schneider Electric
Aug 25, 2016·3 min read
The headline above defines the question the cloud computing industry is in the process of working out right now, and the answer is not what you might think.
When cloud computing and the Internet were initially developed, the primary applications were email and the very beginnings of social media — remember MySpace? Technologists dreamed of a day when voice could be transformed into digital packets and moved over the Internet. This technology — voice over IP (VoIP) — took decades to mature to the point where it became viable but VoIP is now a preferred technology to analog phone lines. The main inhibiting factor was latency, which caused packet loss and loss of voice quality.
Today we are attempting to make another huge technological leap — moving the business applications we all use every day off of our PCs and servers and into the cloud. It’s not a monumental technical challenge to take IT applications and put them in a large, remote cloud data center and call them “services.” But it is a huge challenge to duplicate the speed at which these services run on local servers, considering that centralized cloud data centers can be hundreds or even thousands of miles away from the users.
Super high bandwidth is built into long haul networks enabling them to transmit data at high speeds. But the main problem is network congestion. The same data that you need to make a business or even life or death decision is traveling on the exact same network as the video of little Susie riding her bike for the first time without training wheels. If the video of Susie and others gets to the network transfer point or “hop” before your medical record does, yours has to wait. And it’s only going to get worse — Cisco estimates global IP traffic increased more than fivefold from 2010 to 2014 and will increase nearly threefold by 2019.
Customers increasingly expect good, reliable performance from their applications and cloud services. While they may understand that events such as severe weather can cause network downtime, if a new Game of Thrones episode premieres and causes massive network delays well into the next day — that’s unacceptable.
Avoiding such congestion is forcing service providers to move data closer to the user. I am calling this a move to the edge and it’s starting with regional data centers. Regional data centers are not the physical end of the network, but it’s a move closer to end users and as such will reduce latency and transmission costs, while increasing security and in many cases help companies comply with data sovereignty regulations. That’s why the strategy of Microsoft, Google and other service providers is to cache heavily used subsets of data in regional data centers to serve major markets or urban areas.
So these providers are building regional data centers right? Nope, the time it takes for permitting and the number of data center design and build experts are prohibitive. Not to mention that they need the data centers now. The logical answer is to house their cloud data centers in regional data centers owned by colocation companies. These are the companies building medium-sized data centers in strategically located urban areas. This enables the Internet giants to address issues around service levels, transmission costs, security and data sovereignty regulations.
Clearly this is a significant opportunity for colocation providers. It’s just one of several outlined in our new, free report, “Opportunities and Threats to Colocation Providers from Around the Globe.” Click here to download your copy now.
This blog first appeared on the Schneider Electric blog site.
Steven Carlini is the Sr Director, Data Center Global Solutions for Schneider Electric. Steven is responsible developing integrated solutions and communicating the value proposition for Schneider Electric’s data center segment including enterprise and cloud data centers. A frequent speaker at industry conferences and forums, Steven is an expert on the foundation layer of data centers which include Power & Power Distribution, Cooling & Technical Cooling, Rack systems, Physical Security, DCIM Management solutions that improve availability and maximize performance. Steven has been responsible for guiding the direction of many industry changing products and solutions that solve real customer problems or give businesses competitive advantages. Steven holds a BS in Electrical Engineering from the University of Oklahoma, and an MBA in International Business from the CT Bauer School at the University of Houston.
Our connected technologies reshape industries, transform cities and enrich lives. At Schneider Electric, we call this Life Is On. www.Schneider-Electric.com
1 
1 
1 
Our connected technologies reshape industries, transform cities and enrich lives. At Schneider Electric, we call this Life Is On. www.Schneider-Electric.com
"
https://medium.com/@alibabatech/rise-of-new-cloud-computing-interface-underpins-olympics-on-the-cloud-a151f447bb7c?source=search_post---------264,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Tech
Sep 2, 2021·10 min read
Introduction: Alibaba Cloud supported the global live broadcast of the Olympic Games this year for the first time, marking an important step in the Olympics going digital. Technology was of paramount importance for this year’s Olympics, which coincided with the global COVID-19. We believe that this historic event will open an era in which more sports enthusiasts choose to “watch games on the cloud” as the primary way of enjoying international sports events.
Author: Zhimin and Jiaxu
This year’s Olympics recorded several firsts, including the first to postpone its schedule and the first to limit the number of spectators, and is destined to leave a special mark in Olympic history. However, in addition to the many firsts, China’s technological power has also made a historic breakthrough on a key track of this global sports event.
Alibaba Cloud supported the global live broadcast of the Olympic Games this year for the first time, marking an important step in the Olympics going digital. Technology was of paramount importance for this year’s Olympics, which coincided with the global COVID-19. We believe that this historic event will open an era in which more sports enthusiasts choose to “watch games on the cloud” as the primary way of enjoying international sports events.
This was a true “Sports Event on the Cloud”. Alibaba Cloud not only provided a wealth of cloud computing resources including storage, computing, and networking to support core events in the games, but its container service also played a critical part, embodying a key trend in which the container is becoming a new interface for accessing resources on the cloud and the preferred method for global application deliveries. For example, the Alibaba Cloud Container Service for Kubernetes (ACK), the best container execution environment on Alibaba Cloud, and the Alibaba Cloud Container Registry (ACR), the best containerized application distribution infrastructure, were both driving international sports to evolve digitally by outputting cloud-native capabilities that feature efficiency and stability, extreme elasticity, security and intelligence.
Echoing the progress and transcendence embodied in the Olympic motto of “Faster, Higher, Stronger — Together”, ACK strives to deliver the very best capabilities. During this year’s widely viewed Olympics, Alibaba Cloud’s enhanced container service ACK Pro and container image service for enterprises ACR EE delivered impressive performance, while ensuring a robust base for creating and running upper-level applications, and demonstrating to the world China’s “cloud-native power”.
Each and every move of this Olympic Games fell under the spotlight due to the particularity of the timing and the huge challenges faced by the organizers. The official website of the games is the most authoritative platform for publishing real-time event information. Thanks to the high-availability, dual-active architecture of ACK Pro in Frankfurt, Hong Kong and other regions, the website delivered high performance and provided stable, reliable, and safe access from across the world throughout the period. Alibaba Cloud’s container technology has played a key part in guaranteeing timely updates schedules, events, athletes, and Olympic stories to the world.
Given the huge scale of the Olympic Games, it is no exaggeration to describe its data needs as “massive”. A huge database is the only solution to process such information efficiently. The database received information from event result reporting applications, collected information such as the event start time and the athlete’s performance for centralized processing, and then sent data to other applications.
The event database used ACK Pro to create a high-availability architecture supporting remote disaster recovery in multiple regions including Tokyo and Frankfurt, helping to ensure data security, business continuity, and comprehensive data protection for applications. In addition, the system has stringent real-time requirements to ensure real-time data collection, processing, and output. The excellent performance of ACK Pro and ACR EE fully met these real-timeliness requirements. Alibaba Cloud allowed nodes to expand rapidly and pods to respond to traffic bursts, thanks to ACR EE’s large-scale distribution of container images and ACK Pro’s extreme elasticity.
In addition, the rapid DevOps deployment capabilities of the container technology was leveraged in automatic media labeling to integrate data from different sources, such as athletes’ entry time and scoring time. This allowed databases to be established and the metadata related to OBS video images to be enhanced via AI. This project benefitted from ACK Pro during deployment and establishment to improve the automation of media labeling.
Technology has enhanced the public interaction with the games via a diversity of novel and interesting online approaches, despite the strict limitation imposed on watching the games on site. For example, an Olympic-themed adventure mobile game launched by PinQuest allowed users to embark on their own “Olympic Village Adventure” on mobile phones. The game is supported by Alibaba Cloud Serverless Kubernetes (ASK), which brings extreme elasticity into key modules. It was initiated and quickly released more than 10 days before the event, fully demonstrating the rapid deployment and extreme elasticity of containers.
Rome was not built in a day. What enabled the extensive application and satisfying performance of container services at this year’s Olympics Games was Alibaba’s core technical capabilities, accumulated and honed during the cloud-native transformation for more than 10 years.
ACK provides the most competitive container service in the industry and has been China’s largest player in terms of market share for many years. In addition to large-scale sports events such as the Olympics, ACK has also become the backbone of large-scale business or entertainment events such as the Double 11 shopping carnival, the 618 shopping carnival, and the Spring Festival Gala. It supports the group’s core e-commerce businesses, retail cloud Jushita, logistics cloud Cainiao CPaaS (Communications Platform as a Service), Middleware MSE, edge clouds CDN and ENS, as well as allowing AI, databases and DingTalk audio and video modules to go cloud-native. These practices have helped ACK accumulate core technical competitiveness in diverse environments.
Alibaba Cloud’s container services are now available in 24 regions around the world, covering China, Asia Pacific, North America, and Europe and enabling global deployment, built-in optimal and high-availability practices, and disaster recovery and backup solutions. This makes them highly suitable for global business scenarios seeking to improve system availability and stability. For the Olympic Games with its very stringent data reliability and SLA demands, the customer deployed multiple trans-continental container clusters based on ACK Pro and ACR EE covering the Frankfurt, Hong Kong, and Tokyo regions, and recorded zero failures and satisfactory stability throughout the process.
ACK is one of the world’s first service platforms to pass Kubernetes conformance certification. It provides high-performance containerized application management services and supports lifecycle management of enterprise-level Kubernetes containerized applications. As a leading domestic cloud computing container platform, ACK has been growing together with its customers in all kinds of industries since its debut in 2015.
ACK has upgraded its technical capabilities over the past year, including a 30% increase in its high-performance cloud-native container network Terway over the community network, high-performance storage CSI’s support of efficient volume management of large-scale X-Dragon hosts in the database, and ASK’s extreme elasticity upgrade. In terms of large-scale scheduling, ACK has enabled efficient and stable management over tens of thousands of container clusters, the largest container cluster group in China, being the first domestic manufacturer to pass the large-scale certification (10,000 nodes, 1 million pods) by the China Academy of Information and Communications Technology (CAICT).
A professional managed cluster (ACK Pro) is a cluster type evolved from a standard managed cluster (ACK) and inherits all the advantages of the original managed cluster, such as master node management and high availability of master node. Compared with the original managed cluster, ACK Pro enhances the reliability, security, and scheduling features of the cluster and supports an SLA that contains a compensation standard, making it suitable for enterprise customers who run large-scale business in the production environment and have high requirements for stability and security.
· More reliable management of the master node: stable support to the management of large-scale clusters; etcd disaster recovery and backup, and hot and cold data backup mechanism to maximize the availability of the cluster database; and observability of key indicators of the managed components to help you better predict risks.
· A more secure container cluster: encrypted disk used by default for storage on the management plane etcd; kms-plugin component installed on the data plane to encrypt and store secret data on disks; open-type security management and an advanced version of security management with stronger detection and automatic repair capabilities for running containers.
· Smarter container scheduling: ACK integrates the kube-scheduler with enhanced scheduling performance and supports multiple intelligent scheduling algorithms to support NPU scheduling and optimize container scheduling capabilities in business scenarios such as massive data computing and high-performance data processing.
· SLA assurance: ACK supports SLA with a compensation term, with the availability of its cluster’s API server reaching 99.95%.
Alibaba Cloud Container Registry (ACR) is a securely managed and efficient distribution platform for OCI-compliant cloud-native products such as container images and Helm charts. ACR EE supports acceleration throughout the chain such as global synchronization acceleration, large-scale and large-image distribution acceleration, and multi-code-source construction acceleration, and is seamlessly integrated with ACK to help enterprises cut delivery complexity and create an all-in-one solution for cloud-native applications.
ACK is the largest container service in China in terms of user size, supporting tens of thousands of Kubernetes clusters, so efficient and stable management of the massive clusters is essential. ACK uses the following methods to build a stability assurance system.
· Integrated operation and maintenance
The ACK uniform operation and maintenance platform integrates cluster monitoring, alarming, logging, inspection, metadata management, asset management and other functions across the network and supports real-time observation and management of all clusters in the 24 regions connected to the network. For example, if an exception occurs in a master component or a system component of a user’s Kubernetes cluster or the cluster suffers an abnormal event, the exceptions or abnormalities can be observed on the operation and maintenance platform and an alarm will be automatically triggered. The efficient management platform enables ACK to manage tens of thousands of clusters in the network with higher stability.
· All-scenario diagnosis
ACK provides a Center for Internet Security (CIS) reinforcement service to allow users to perform in-depth inspection and diagnosis on key elements of cluster operations including the network, nodes, components, and services in coverage. It ensures professional inspection and diagnosis and a user-friendly experience to enhance users’ cluster management capabilities. Users can inspect running clusters and services and generate inspection reports. With ACK, users are not just deploying and using Kubernetes strengths, but more importantly, they become empowered by Kubernetes professional capabilities and benefit from its deep-embedded capabilities.
· Well-structured support pre-plan system
ACK developed a full-process support solution targeting the Olympic Games based on its existing support solutions, including pre-plans, contingency plans, fault drills, and duty scheduling. ACK has rich experience in providing support and its capabilities are constantly honed during annual events like the Double 11 shopping carnival, the 618 shopping carnival, and the Spring Festival Gala. These large-scale events entail complex and comprehensive tasks and ACK has recorded nearly zero failures supporting them.
Apart from the above major events, ACK also organizes regular chaos-based fault drills and surprise attacks internally. In such drills, faults are injected to the chaotic system randomly and the ACK team members on duty receive an alarm and handle the issue immediately according to plans in the system. Such regular training has tempered the team’s emergency responses and allowed them to meet the 1–5–10 objective (namely, issue the alarm within 1 minute, locate the fault within 5 minutes, and resolve the fault within 10 minutes). These support systems, which have been repeatedly honed through real-life combat, were applied to the Olympics support program thereby guaranteeing stable and smooth progress of the games.
ACK was deeply engaged in the support to this year’s Olympic Games, and delivered steady performance of core tasks associated with the official website and event data processing with its industry-leading cloud-native technology, products, and services. In collaboration with Alibaba Cloud’s other services, ACK contributed to the success of the “Olympics on the Cloud”.
ACK will also play a supporting role in the upcoming Paralympic Games and Winter Olympics. Alibaba Cloud has been building efficient, secure, intelligent, and boundless container technical capabilities and rock-solid service quality, allowing science and technology and the Olympic rings to enhance each other, and to help more sectors and enterprises around the world speed up their digital transformation.
Alibaba Tech
First hand and in-depth information about Alibaba’s latest technology → Facebook: “Alibaba Tech”. Twitter: “AlibabaTech”.
First-hand & in-depth information about Alibaba's tech innovation in Artificial Intelligence, Big Data & Computer Engineering. Follow us on Facebook!
First-hand & in-depth information about Alibaba's tech innovation in Artificial Intelligence, Big Data & Computer Engineering. Follow us on Facebook!
"
https://medium.com/peloton-engineering/event-recap-the-future-of-cloud-computing-and-databases-6e7f57bd3753?source=search_post---------265,"There are currently no responses for this story.
Be the first to respond.
This past Monday we hosted A Database Month event with a talk from Zigmars Rasscevskis, Chief Executive Officer, Clusterpoint.
The talk touched upon a lot of things, such as: learning how distributed databases can serve as a solid foundation for massively-parallel and instantly-scaleable distributed computing, leveraging NoSQL technologies, and its community, as it contributes to the architecture of a ‘future computer,’ and practical matters of large-scale distributed system design.
There were nearly 250 people in attendance, and we were happy to make some new friends! Check out more photos from the event here. We will update this post once the live-stream recording is available.
The official Blog for Engineering @ Peloton
Written by

The official Blog for Engineering @ Peloton
Written by

The official Blog for Engineering @ Peloton
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/verizon-ventures/the-next-generation-of-5g-iot-infrastructure-cloud-computing-bf8e43a34345?source=search_post---------266,"There are currently no responses for this story.
Be the first to respond.
By Vijay Doradla
In a recent video interview, Verizon Ventures’ Vijay Doradla sat down with The Fabric’s CEO, Rajan Raghavan to discuss Verizon Ventures’ recent investment in The Fabric as well as emerging technologies in the enterprise including 5G, IoT infrastructure, cloud computing and more.
The Fabric sits at the center of innovation by co-creating companies focused on cloud and IoT infrastructure and accelerating them in this industry. To date, The Fabric has successfully built and launched companies such as VeloCloud (acquired by VMWare) and Perspica (acquired by Cisco) among many others.
As new technologies move into the enterprise, Verizon Ventures strives to have a strong pulse on the market and partnerships that can disrupt these markets. For that reason, Verizon Ventures saw The Fabric as a natural partner. Our partnership expands beyond building relationships with The Fabric’s entrepreneurs and towards the ability to co-create ideas together and help solve pain points present in the industry today.
Read more from Verizon Ventures:
CEO Center Stage: Rajan Raghavan, CEO and Co-Founder of The Fabric
Insights on IoT: How The Internet of Things is Impacting the Job Market
2018 Predictions from our Portfolio
Smart funding for driven entrepreneurs
Written by
We seed the future of technology by backing big ideas, with funding, strategic connections, and expertise.
Smart funding for driven entrepreneurs
Written by
We seed the future of technology by backing big ideas, with funding, strategic connections, and expertise.
Smart funding for driven entrepreneurs
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://blog.workfusion.com/is-crowd-computing-secure-folks-asked-the-same-thing-about-cloud-computing-too-8ed56f1175cd?source=search_post---------267,"New Course: Get to Know Intelligent Automation Cloud
Learn more about the new free course in WorkFusion’s Automation Academy, “Get to Know Intelligent Automation Cloud”
January 5, 2022                                             1 min read                                    
Benefits of Intelligent Automation: IA vs RPA
Everest Group research explains: For long-term success, companies need Intelligent Automation solutions with integrated Artificial Intelligence.
October 21, 2021                                             5 min read                                    
AML Experts Open Up About the “Pandora Papers”
How Pandora Papers report expands on previous disclosures of global offshoring and wealth shelters, and what it portends for fighting financial crime.
October 18, 2021                                             3 min read                                    
How to Help Your Employees Embrace Automation
When employees know what to expect from changes, they are more confident in their roles. Here, we share the best practices for better staff retention.
January 12, 2022                                             2.5 min read                                    
Ho-Ho-How Santa Has Optimized Operations at the North Pole
Enhanced experiences and more efficient decisions through Intelligent Automation.
December 15, 2021                                             2.5 min read                                    
The Role of Automation in a Seamless Digital Employee Experience
 Learn how automation improves the digital experience and leads to increased satisfaction for employees.
September 30, 2021                                             1.5 min read                                    
5 Ways to Transform Your Bank’s Document Processing
We discuss top IDP use cases in banking and finance.
November 18, 2021                                             2 min read                                    
Top Use Cases for Automation in the Insurance Industry
Learn what use cases have the highest automation potential in such areas of insurance as underwriting, claims processing and more.
October 28, 2021                                             11 min read                                    
Leap Past KYC Challenges to a pKYC Solution
Banks implementing this technology will stay ahead of both competitors and bad actors. Read to know what this technology is.
August 16, 2021                                             4 min read                                    
Q&A: WorkFusion CTO Answers Your Questions about Intelligent Document Processing
WorkFusion CTO Peter Cousins answers the audience's questions about Intelligent Document Processing (IDP) from our recent webinar with Everest Group.
November 24, 2021                                             1.5 min read                                    
5 Takeaways from Our Intelligent Document Processing Webinar with Everest Group
WorkFusion and Everest Group experts share insights and advice about Intelligent Automation and IDP.
November 11, 2021                                             2.5 min read                                    
5 Lessons from Our Insurance Automation Webinar with Mindtree and ISG
Insurance automation experts from WorkFusion, Mindtree, and ISG share their wisdom in the “Aim for Success with Intelligent Automation in Insurance” webinar. 
October 14, 2021                                             2.5 min read                                    
Critical Capabilities of Our Intelligent Automation Platform, Part 2
Learn about the critical capabilities of WorkFusion's Intelligent Automation platform, which play essential roles in scaling your automation program.
July 17, 2020                                             4 min read                                    
Critical Capabilities of Our Intelligent Automation Platform, Part 1
Read about the critical capabilities of the Intelligent Automation Cloud, that ensure success during automation adoption, expansion, and scaling.
July 1, 2020                                             4 min read                                    
Everyday AI Explained, Part 2: Three Core Capabilities
Beyond telling you about what Everyday AI is, we’re going to show you exactly what it’s capable of doing, and what benefits it can bring to your business.
August 10, 2018                                             3 min read                                    
How to Help Your Employees Embrace Automation
When employees know what to expect from changes, they are more confident in their roles. Here, we share the best practices for better staff retention.
January 12, 2022                                             2.5 min read                                    
New Course: Get to Know Intelligent Automation Cloud
Learn more about the new free course in WorkFusion’s Automation Academy, “Get to Know Intelligent Automation Cloud”
January 5, 2022                                             1 min read                                    
Ho-Ho-How Santa Has Optimized Operations at the North Pole
Enhanced experiences and more efficient decisions through Intelligent Automation.
December 15, 2021                                             2.5 min read                                    

"
https://medium.com/@alibabatech/building-a-virtuous-circle-for-cloud-computing-in-singapore-97c066aa05c8?source=search_post---------268,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Tech
Mar 26, 2021·1 min read
By Derek Wang, General Manager of Alibaba Cloud Singapore
Singapore businesses already lead Southeast Asia in Cloud adoption. According to a recent survey by Alibaba Cloud “Role of Cloud in Asia and Confidence in Asian Innovation Survey”, nine in 10 IT decision-makers here report that their companies are using at least one Cloud-based technology. However, with broad adoption comes maturity. Our survey also found that Singaporean companies are the region’s least satisfied with how Cloud-based tools or digitalisation efforts have helped them cope with business and operational needs during the pandemic.
The maturity of IT and Cloud adoption, and attitudes in Singapore have grown over the years, but how can businesses derive more value from the mature tech infrastructure and reap more rewards for cloud?
Read more:
www.frontier-enterprise.com
Alibaba Tech
First hand and in-depth information about Alibaba’s latest technology → Facebook: “Alibaba Tech”. Twitter: “AlibabaTech”.
First-hand & in-depth information about Alibaba's tech innovation in Artificial Intelligence, Big Data & Computer Engineering. Follow us on Facebook!
First-hand & in-depth information about Alibaba's tech innovation in Artificial Intelligence, Big Data & Computer Engineering. Follow us on Facebook!
About
Write
Help
Legal
Get the Medium app
"
https://yanpritzker.com/what-cloud-computing-is-not-f6c33f6c0814?source=search_post---------269,"The Wall Street Journal just published an article trying to define cloud computing. Although hardly anyone can be faulted for not understanding the cloud (ahem) of jargon spewing from every marketing department of every IT department scrambling to be relevant without actually innovating, I wanted to help by offering a sort of proof by contradiction.
By examining the types of companies and technologies claimed to be “cloud computing” (by themselves, or others), and then showing that what they are providing has been around for many years and already has its own label, we can hopefully narrow down what exactly is the important bit about cloud computing.
So…what cloud computing is NOT…
See more: Storing your stuff online is not cloud computing, What is Cloud Computing?
Shameless plug: Elastic Server will have you running in the cloud in minutes.
co-founder swanbitcoin.com. Former CTO -  Reverb.com
Written by
co-founder swanbitcoin.com. Former CTO, Reverb.com.
co-founder swanbitcoin.com. Former CTO -  Reverb.com
Written by
co-founder swanbitcoin.com. Former CTO, Reverb.com.
co-founder swanbitcoin.com. Former CTO -  Reverb.com
"
https://medium.com/cointiger/cointiger-launches-ipfs-filecoin-cloud-computing-sale-phase-2-together-with-dasheng-venture-capital-15c36de9a476?source=search_post---------270,"There are currently no responses for this story.
Be the first to respond.
Fellow CoinTiger users,
CoinTiger and Dasheng Venture Capital IPFS project team have joined forces to launch Dasheng Venture Capital IPFS·Filecoin Cloud Computing Sale Phase 2 subscription campaign. At the same time, users who successfully subscribed for phases 1 and 2 will be able to subscribe to half-price MKC. Details as follow:
Campaign 1: Dasheng Venture Capital’s IPFS·Filecoin Cloud Computing Sale Phase 2
Campaign Duration:
29 July 18:00–31 July 18:00, 2020 (UTC+8)
Campaign Rule 1: Cloud Computing Sale Phase 2 can be rent
1.Cloud Computing Sale Phase 2 asset cycle is 365 days (from Main Net listing)
2.40 Shares per person. 1TB= 1 Share, 198 USDT per share
3.Subscribe 1 share, get 50 MKC for free
4.Use CoinTiger contact number to register MKC wallet
5.MKC wallet link (open on app）: https://1.mkc.hunanmaster.com/mobile/Login/login
Campaign Rule 2: Hold FIL to Subscribe Cloud Computing Sale with 10% Discount
1.We will take a snapshot at 18:00 on July 28, 2020 (UTC+8) for users who hold 4 FIL or more
2.Users who meet above requirement can subscribe cloud computing sale phase 2 with a 10% discount(Original price is 198 USDT, 10% discount is 178 USDT)
3.The full amount (198 USDT) will be deducted when the subscription is placed, and the discount part (20 USDT) will be returned to users’ account within 5 business days when the campaign ends
Subscription:
1. Visit Cloud Computing Sale Phase 2 subscription page on CoinTiger
2. Trading account has an adequate asset
3. Follow the steps to complete the subscription process
4.Confirm in MKC wallet (Data will be synchronized when cloud computing sale page is online )
Product Brief:
Product name: IPFS·Filecoin Cloud Computing Sale Phase 2
Proof of equity: Using equity for each computing sale is 365 days, and they belong to Dasheng Venture Capital mining pool. Settlement can be applied when cloud computing sale reaches 192T according to national standard quality.
Technical service fee: 20% (deduction will be made on the day if we take Filecoin or POC as a unit)
Distribution: Daily delivery (expected to start on the 8th day after the main net launch)
Yield expectation: It is expected that the daily output of a single T will be 0.2FIL-0.5FIL / day / TB, and the annual output of a single T will be 73FIL-183FIL. MKC cloud computing sale and comprehensive profits for other high-qualified POC currency will be sent for free as the attachment. (The impact of fluctuations and mining difficulty might be affected by actual conditions)
Risk warning:
1. The output based on IPFS cloud computing sale will be affected by the fluctuation of Filecoin price and mining difficulty. The IPFS cloud computing sale leasing service cannot make capital or currency-based capital preservation and refund commitments to users. Please carefully assess your risk tolerance and participate within acceptable limits. Dasheng Venture Capital IPFS Cloud Computing Rental Service reserves all rights to interpret this product and agreement.
2. The user understands and accepts that if changes in objective conditions such as the formulation or modification of relevant national laws, regulations, and regulatory documents lead to the suspension or prohibition of Filecoin mining, this contract is automatically terminated and the two parties shall not pursue each other. Liability, the resulting losses must be borne by yourself, and the fees paid by the user are not refundable.
3. This product does not involve digital asset transactions. If users participate in digital asset transactions on their own, they should bear their own responsibilities and risks.
Campaign 2: Limited-Time Subscribe Half Price MKC
Campaign Duration:
31 July 18:00–1 August 18:00, 2020 (UTC+8)
Campaign Requirements:
1.Users who have subscribed for cloud computing sale phase 1 or 2 will be able to access half price MKC.
2.Users subscribe by share, 100 MKC per share, each user is limited to 3 shares, a total of 300 MKC.
3.Total share is 90,000 MKC. First come first served.
4.Half price MKC will be distributed to users accounts in 2 hours when the campaign ends.
5.Deduction will be done for users who do not meet the requirements of subscription, but funds will be returned to users’ accounts when the campaign ends.
MKC Introduction:
MKC is a token of cloud computing sale based on ERC20 smart contract development and relying on Dasheng Venture Capital’s IPFS·POC mining pool computing sale as the core. MKC has the following effects:
1.Each TB cloud computing sale corresponds to each TB real enterprise hard disk storage space
2.After MKC is launched on Filecoin’s main net, MKC can be used to exchange for the IPFS cloud computing sale of Dasheng Venture Capital
3.After MKC is launched on Filecoin’s main net, MKC can be used to lease for the IPFS cloud computing sale of Dasheng Venture Capital
4.Dasheng Venture Capital’s POC mining pool will distribute MKC dividends according to MKC holding and mining amount
CoinTiger team
2020/7/24
About CoinTiger
CoinTiger platform has an ecosystem that includes web, iOS, Android and exchange service system (e.g Cryptocurrency, FIAT, Future, and Leveraged Tokens Trading, Ticker Capital, Lab, Security Alliance, Tiger Knights, IEO, Tiger Forum), and is now one of the most popular exchanges in the world, which formed sub-branches in countries such as Korea, Japan, Russia, Italy and more. CoinTiger is founded in November 2017 and has exceeded 3.5 million users today. CoinTiger also provides trading and investing services for 158 countries with over 100 quality assets. CoinTiger is also the first crypto exchange to implement a 100 % equity mechanism for its native TigerCash (TCH) coin. The first thing CoinTiger has done is to serve its users well and deliver secure, efficient, and reliable services with the highest level of global ecosystems, banking security, efficient and best possible product experience, easy and fast transaction services, and a 7X24 customer support team.
Social Media Handles
WebsiteFacebookTwitterMedium BlogReddit
Telegram Channels
English中文群Tiếng ViệtРоссиеюIndonesianالعربية日本語Türkçe한국어NigeriaहिंदीবাংলাBrasilคนไทยPhilipinesFrenchItalianAnnouncement
Kakaotalk
http://open.kakao.com/o/gwCm6SVb (Korean)
CoinTiger APP Links
Official Link: https://www.cointiger.com/en-us/#/app_download
Android: https://play.google.com/store/apps/details?id=com.cointiger.exchange
CoinTiger is a global and innovative crypto asset exchange…
CoinTiger is a global and innovative crypto asset exchange that provides multi-crypto-currency trading services in multi-language to blockchain enthusiasts.
Written by
CoinTiger is a global and innovative crypto asset exchange that provides multi-crypto-currency trading services in multi-language to blockchain enthusiasts.
CoinTiger is a global and innovative crypto asset exchange that provides multi-crypto-currency trading services in multi-language to blockchain enthusiasts.
"
https://medium.com/@alibaba-cloud/the-attraction-of-cloud-native-to-the-cloud-computing-ecosystem-2a13459a5a70?source=search_post---------271,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Feb 25, 2021·9 min read
On February 2, 2021, the Cloud Native Computing Foundation (CNCF), the world’s top open-source community, officially announced its new term of Technical Oversight Committee (TOC). Zhang Lei, a Senior Technical Expert of Alibaba Cloud, was elected as the only representative from a Chinese enterprise among the current nine TOC members.
CNCF explained in its official announcement that Zhang Lei was elected because of his outstanding contributions to Kubernetes. “Zhang Lei is a Co-Maintainer of the Kubernetes community and a Co-Chair of CNCF App Delivery SIG. He is in charge of the Kubernetes and large cluster management systems at Alibaba.”
Founded in July 2015, CNCF is an affiliate of the Linux Foundation. CNCF devotes itself to maintaining and integrating open-source technologies while focusing on cloud-native to serve cloud computing. It also supports application orchestration based on the containerized microservice architecture. Currently, CNCF has more than 300 member companies, including AWS, Azure, Google, Alibaba Cloud, and other mainstream global cloud computing vendors. CNCF TOC is composed of nine representatives with rich technical knowledge and industry background. Those representatives provide technical support and guidance for the cloud-native community.
Since 2017, Alibaba has invested a lot in cloud-native technologies and participated in the development and maintenance of many top-level open-source projects, such as ETCD, Kubernetes, and ContainerD. It has also completed the self-upgrade of the overall infrastructure through the cloud-native technology stack. Some star projects, such as the exclusive open-source Dubbo and RocketMQ, are donated to the Apache Foundation and graduated as top projects. Since the release of open-source Spring Cloud Alibaba, it has become the most active Spring Cloud implementation of Spring Cloud with the best development experience. Dragonfly has become a CNCF incubation project, and OpenYurt and OpenKruise have entered the CNCF Sandbox. What’s more, Alibaba has released the world’s first Open Application Model (OAM) with Microsoft Azure to lead the cloud-native standard application delivery ecosystem. After the release of OAM, Alibaba launched KubeVela, a core engine of the cloud-native platform based on OAM. KubeVela is the implementation of the Kubernetes ecosystem in constructing standard application delivery systems. Together with Nanjing University, Alibaba has opened Fluid, a cloud-native infrastructure project. Fluid is an important component for big data and AI to embrace the cloud-native. In addition, Alibaba Serverless Devs has become the first Serverless Tool from China among CNCF Landscape Tools.
By the end of 2020, over ten Alibaba projects entered the CNCF Landscape, which ranks among the top ten in the world in terms of the number of projects in the Kubernetes community.
Currently, Zhang Lei is the only member of CNCF TOC from a Chinese enterprise. Born in 1989, he is one of the youngest early maintainers of the Kubernetes community. He initiated and participated in the design of multiple Kubernetes features, such as Container Runtime Interface (CRI), equivalent class scheduling, and topology resource management. With his continuous influence in the Kubernetes community, Zhang Lei was elected as the CNCF official ambassador in 2016 and served as the KubeCon reviewer and Keynote Speaker for several consecutive years. In 2019, Zhang Lei was elected as the Co-Chair of the CNCF Application Delivery Team. So far, he is the only Chinese Co-Chair among the seven CNCF teams.
At Alibaba, Zhang Lei has participated in the design of the cloud-native application infrastructure of Alibaba Cloud and the maintenance of the largest public cloud container cluster in China. His “application-centered” standard application delivery system gives birth to a series of prospective and leading open-source technologies for cloud-native application management.
Zhang Lei and his team jointly developed OAM with CTO Office of Microsoft Azure. OAM is the first standard model and framework for cloud-native application delivery and management in the industry. It has quickly become the model for several enterprises worldwide to build their cloud-native application platforms, such as MasterCard and 4Paradigm. It has also become a standard project of the China Academy of Information and Communications Technology (CAICT) and the Ministry of Industry and Information Technology (MIIT) called “General Requirements and Reference Framework for Cloud Computing Open Architecture .” OAM has been listed as one of the “Top Cloud-Native Technology Trends of 2020” by TheNewStack and “Top 10 New Open-Source Projects in 2020” by InfoQ.
Today, no one will question the rationality of a platform team’s adoption of Kubernetes as its infrastructure. In 2020, Kubernetes projects have almost achieved the final goal. They aim to bring platform-layer abstractions to the cloud computing infrastructure, which allows the platform team to construct “everything” based on these abstractions.
However, what is “cloud-native” exactly? Why is it so attractive to the cloud computing ecosystem? What is the future of cloud-native in China? Let’s learn about the opinions of Zhang Lei through a Q&A session:
Q: Congratulations! You are now one of the nine CNCF TOC members! Let’s have a brief self-introduction first, shall we?
Zhang Lei: Currently, I am responsible for technical work related to the infrastructure of the cloud-native application platform at Alibaba Cloud. I am also involved in promoting the construction of Alibaba’s core open-source projects, such as OAM/KubeVela, OpenKruise, and OpenYurt. Before working for Alibaba Cloud, I mainly worked for the upstream part of the Kubernetes community. I was one of the early initiators and maintainers of multiple core features, such as CRI and scheduler. I was also a member of the KataContainers project team. Recently, my team and I have worked with CNCF TOC and over 40 companies to promote the establishment of a vendor-neutral working group for GitOps application delivery.
Q: What is your opinion about the development and evolution of cloud-native in recent years?
Zhang Lei: Cloud-native technologies are becoming more popular. We can see that this new way of application delivery is combining with key technologies, such as the standard application model and Mesh-based progressive release. The combination has become the mainstream method of building application platforms in the industry.
The well-known cloud-native is a set of best practices and methodologies for using cloud computing technologies to reduce costs and improve user efficiency. Therefore, the cloud-native has innovated and evolved since its birth.
Whether it is the great success of container technologies represented by Docker in 2014, or the rapid rise of container orchestration technologies represented by Kubernetes in 2019, or the almost “all-encompassing” ubiquity of cloud native today, the concept of cloud native is continually evolving from theory to practice, and has successfully helped us develop new ideas and architectures. The continuous evolution of cloud-native is based on its core idea, which gradually affects all aspects of the cloud computing field. This is a main development trend of the cloud-native ecosystem in recent years.
Q: What do you think were the major changes in the cloud-native field last year? What influences will they bring?
Zhang Lei: In 2020, we can see that the rapid popularization of cloud-native is bringing changes based on the cloud to more fields. These fields can be integrated into the capability pool of cloud computing through the cloud-native system, which reduces costs and improves efficiency for end users. Let’s use the CNCF open-source community as an example. In 2020, OpenYurt and OpenKruise from Alibaba entered the CNCF Sandbox. Virtual Cluster from Alibaba became an official sub-project of Kubernetes, and many core projects, such as OAM/KubeVela were incubated. The emergence and popularization of these open-source technologies promote the continuous development and evolution of the cloud-native ecosystem. They are also making it a reality to provide the benefits of cloud computing.
In 2020, cloud-native remained the fastest-growing element in the entire cloud computing ecosystem. With this growth momentum, it is already time to think about the future developments of cloud-native. Many cloud vendors and teams are actively investing in and exploring the future cloud-native technologies in different fields.
Q: What do you think cloud-native will become in the future?
Zhang Lei: Today, cloud-native is getting closer to the idea that “software is naturally born and grows on the cloud.” However, the development of cloud-native has also revealed many problems. For example, the original cloud-native technology focused too much on infrastructure abstraction and management while neglecting the end-user experience. Fortunately, the changes in the cloud-native field in 2020 have shown that the cloud-native community is gradually moving closer to end users by submerging capabilities and producing value. This explains the influence brought by many cloud-native technologies after Kubernetes. For example, Service Mesh is rapidly changing middleware and microservice governance technologies. GitOps is having a critical influence on the continuous delivery field. OAM and Dapr are solving the problems of application abstraction models and service access models.
Over the next few years, we expect the inherent agility and user stickiness of cloud-native technologies will be applied further. Cloud-native will witness further application in more vertical fields, such as database, AI, and edge, together with the huge capability pool of cloud computing. Thus, cloud-native will widely change the underlying infrastructure of cloud computing and the deployment and distribution of cloud applications. It may directly reflect the “ubiquity of cloud computing” in the future.
Q: What areas will you focus on after being a TOC member?
Zhang Lei: In the future, I will work with CNCF TOC to continuously focus on upper-layer technology fields that can bring direct value to end users, such as application management and delivery, cloud-native programming models, and cloud developer experience. Together with the community, I will help better incubate and introduce potential open-source projects in these fields to CNCF. At the same time, TOC will make long-term plans, especially on key underlying technologies, such as WebAssembly and eBPF, which have recently risen rapidly. Don’t be surprised if Kubernetes is no longer the best-known project in CNCF in the near future.
Q: Do you have any ideas or suggestions on how to promote the development of the cloud-native ecosystem in China?
Zhang Lei: Many people think that the best and most robust cloud-native community in the world today is in China since Google and AWS do not use Kubernetes. Today, the cloud-native community in China has gathered many professionals, and the cloud-native scenarios and environments are growing rapidly. These are what cloud-native developers around the world dream about. They facilitate the rapid development of the cloud-native ecosystem in China and help release the benefits of cloud-native.
Therefore, as a member of the cloud-native ecosystem from China, we should not follow and be bolder in innovation. At the same time, we should be more confident to introduce our technologies to the world and actively work with some neutral organizations to speak our minds. These organizations, such as CNCF, must have global influence and consider our discourse power. We should actively incorporate users, participants, and contributors from North America and Europe into our community. The full collaboration of various Chinese cloud-native vendors, community members, and open-source project maintainers is required. I think that the nearly-fully-localized KubeCon summit and the cross-company and cross-region Cloud-Native Programming Hackathon will soon bear fruit in the Chinese cloud-native community.
What exactly is cloud-native? Since its emergence, many engineers, teams, and enterprises that have encountered cloud-native have asked this question.
Cloud-native is a set of best practices and methodologies that “uses cloud computing technologies to reduce costs and improve efficiency for users.” Cloud-native is in continuous self-evolution and innovation. As Zhang Lei said in an article, “This continuous vitality with “no exact definition” is the source of the attraction of “cloud-native” to the cloud computing ecosystem.”
Zhang Lei said, “Cloud-native requires continuous thinking, accumulation, and innovation from the entire cloud-native community to implement and update cloud-native technologies. By doing so, cloud-native can produce more value and a better experience for end users with its technologies. It realizes the goal of building a simple and easy-to-use cloud-native platform. This is what I have always invested in.”
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/alibaba-unveils-ai-chip-to-enhance-cloud-computing-power-872bd04a80c6?source=search_post---------272,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Oct 15, 2019·3 min read
Hangzhou, China, September 25, 2019 — Alibaba Group (NYSE: BABA) today unveiled its first AI inference chip developed by T-Head under the Alibaba DAMO Academy, an initiative to lead technology development and scientific research.
The high-performance AI inference chip, a neural processing unit (NPU) named Hanguang 800, that specializes in the acceleration of machine learning tasks, was announced at Alibaba Cloud’s annual flagship Apsara Computing Conference. It is currently being used internally within Alibaba’s business operations, especially in product search and automatic translation on e-commerce sites, personalized recommendations, advertising, and intelligent customer services. These areas require extensive computing power for the AI tasks to optimize the shopping experience.
“The launch of Hanguang 800 is an important step in our pursuit of next-generation technologies, boosting computing capabilities that will drive both our current and emerging businesses while improving energy-efficiency, “ said Jeff Zhang, Alibaba Group CTO and President of Alibaba Cloud Intelligence. “In the near future, we plan to empower our clients by providing access through our cloud business to the advanced computing that is made possible by the chip, anytime and anywhere.”
A key goal for Alibaba Cloud is to offer a leading technology infrastructure that benefits companies of all sizes and narrows existing gaps in the access to technology, ultimately making the world more inclusive.
Propelled by a self-developed hardware framework, as well as highly-optimized algorithm designs that are tailored for business applications such as retail and logistics in the Alibaba ecosystem, Hanguang 800 has recorded remarkable performance in tests. The single-chip computing performance reached 78,563 IPS at peak moment, while the computation efficiency was 500 IPS/W during the Resnet-50 Inference test. Both performance scores largely outpace the industry average, showcasing advantages underscored by a remarkable balance between powerful computing capabilities and the highest level of computational efficiency.
For example, around one billion product images are uploaded to Taobao, Alibaba’s e-commerce site, every day by merchants. It used to take the machine one hour to categorize such a large volume of images, and then tailor search and personalized recommendations to be provided to hundreds of millions of consumers. But with Hanguang 800, it now only takes the machine 5 minutes to complete the same task.
Alibaba’s research unit, T-Head — whose Chinese name is “Pintouge,” meaning “honey badger” — leads the innovation around chip design for both cloud and edge computing. They are also responsible for nurturing an inclusive edge-to-cloud computing ecosystem by collaborating with global partners in the chip industry. Earlier this year, T-Head debuted XuanTie 910, a high-performance IoT processor based on RISC-V, the open source instruction set architecture (ISA). XuanTie 910 was designed to serve the heavy-duty IoT applications which require high-performance computing, such as AI, networking, gateway, self-driving automobiles and edge servers. Global developers have been able to successfully access certain code within the high-performance processor and leverage this technology to develop prototypes for their own chips.
Our mission is to make it easy to do business anywhere. We aim to build the infrastructure of commerce. We envision that our customers will meet, work and live at Alibaba, and that we will be a company that lasts at least 102 years.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/vaidikkapoor/what-is-cloud-computing-53c1f514734?source=search_post---------273,"There are currently no responses for this story.
Be the first to respond.
Dated: March 20, 2013
I wrote this article while I was doing my undergrad for students who I was guiding while mentoring in Sytems and Network Programming lab course work. Every lab course would have a semester end assignment (or what we would call semester projects). Cloud Computing was one of the areas in which students could do their projects. But without any understanding of the area, students would decide to make their projects under Cloud Computing and end up doing things that were not even closely related to the area. Unfortunately, there was not much help from the faculty as well. This article was an attempt to explain the topic/area to the students of the lab that I was assisting.
Many students have discussed their project ideas for their Systems & Network Programming course with me. One thing that I have noticed a lot is that a lot of students have been trying to do something in Cloud Computing. However, I get this feeling that they have just proposed this without completely understanding what Cloud Computing really is. This post is an attempt to explain Cloud Computing to students, what they can do with it or more specifically what they can do for their Systems & Network Programming course.
Before further continuing this discussion, I’d request anyone who is doing a project related to Cloud Computing (or anyone who is just interested as well) to go through the following links first to get an idea about what it is:
Did you go through the above links? If not, please go through them and then continue.
What seems to me as most of the students understand by Cloud Computing is something really fancy. What I generally hear is that “we will do this (something) on the cloud”. And when I ask “how do you do it” — there is no answer. The truth is that you know the answer. If you have gone through the links I have shared with you above, I think it is safe to assume that you understand that Cloud Computing is nothing but:
Since you are just working on virtual machines that you don’t physically have, it just means that anything that you can run on your laptop, you can run it on that virtual machine provisioned to you by a Cloud service provider (like Amazon Web Services or Rackspace). So when you request a new virtual machine, then you ask for a virtual machine (VM) with a particular configuration decided by you and what Operating System you want installed on it: Linux or Windows. And now that you have your VM, all you have to do is move your files from your laptop to your new VM, install the required software/libraries/whatever, and you are good to go.
But this is not Cloud Computing? This is just using the Cloud because it is not feasible for you to have resource of your own at this point in time. At the end of the day, they are just another set of (virtual) machines that you are using. That’s it!
So, don’t think Cloud Computing is rocket science or I should say using Cloud services is not rocket science. You just need to spend a day or two and you will be good to go.
What is rocket science is, though, how do these Cloud services come into existence? Who makes these services and how to they technically achieve this? Give a thought to this and read more about it. I’d suggest reading this research paper. This does not explain the challenges clearly but you might get an idea about the complexities of building a cloud infrastructure to provide external or internal cloud services.
Now coming back to your projects, what most of the students seem to understand of Cloud Computing is using the Cloud. There are some who don’t understand the term fully at all. That’s alright. Please read more if it interests you and if you want to use the term safely. As I requested, please go through the links I shared in the beginning. Just running some program/software that you have written on a cloud VM instead of your own machine does not make your project any special WRT Cloud Computing. So your project in that case does not make much use of Cloud Computing. Probably, there needs to be more to your project.
Do you remember that I mentioned something about scaling earlier? Well, there are two types of scaling — vertical and horizontal. Vertical scaling means adding more resources like RAM, CPU, etc. to a given machine or VM — I gave an example of this earlier. Horizontal scaling means instead of adding resources to a machine or VM, you add more machines and/or VMs and make a cluster out of it and then distribute work in that cluster. If you want to do something serious in Cloud Computing, then I suggest that you work on project that goes on these lines. What can you do on these lines? For example: you have been writing a lot of programs based on theclient-server model for your lab assignments. Lets say you want to build a service to provide live cricket scores. How will you do this? One way to achieve this is write a server that somehow gets live scores of matches and write clients that request the server for the scores. The clients can be anywhere in the network. Lets say its India vs Australia today and everyone is excited to know scores and everyone has this client. If everyone (lets say 100) people try to get scores at the same time, how will you server take care of this? Its easy and you have done this. For every new connection, create a new thread. But, every new thread and every new socket comes with a cost. The OS under a given hardware capacity cannot handle more than a fixed number of threads depending on what you are doing in those threads. So how will your server serve more than 100 people at the same time? You can do one thing. You can perhaps add more hardware resources and your server will be able to handle more incoming requests. But how much can you add? There is obviously a limit to that. In that case, you start scaling horizontally. How does that work? A general idea is that you can have multiple computers form a cluster in such a way that one machine receives all the request and does something so that those requests are performed on some other machine(s). This way the front-facing machine will be majorly responsible for handling connections and since all the work is done by other machines, the resources you save up on will help you handle more connections.
To read about more scalability and vertical and horizontal scaling, check out these links:
In real world, this is not the only thing that people do make scalable software. There is a lot of other things that need to be taken care of. But its a step-by-step process. The above example was to give you a general idea of how to proceed. On these lines, you can work in any direction and learn how to develop scalable systems. An amazing use-case to make use of Cloud Computing could be to support auto-scaling. For example if we consider the cluster in the above example, if it can handle X number of connections at the same time and the bottle neck is not handling the number of connections but the number of machines that can do the required work at the same time. Now, most of the cloud service providers give you a way to write simple scripts to automatically create new VMs. So, you may do something like:
Again, the above mentioned way is just to show you a really really simple picture. Things get more complex as you build more complex systems.
Read this question on Quora. This explains auto-scaling with an example.
Another aspect of doing a project related to Cloud Computing is doing something for building a cloud infrastructure. Big or small is not the question right now. For example, the paper I shared above was an academic project. The project is called Eucalyptus and is open-source. That means that you can get that project’s code and change it and/or contribute to that project. If you have an idea for feature addition or improvement in the current implementation of Eucalpytus, then you can try to do that as well. These types of projects are more central to Cloud Computing. This will also involve a lot of low-level sytem programming. Similarly, there are other projects called OpenStack and Apache CloudStack.
I hope I have been able to explain Cloud Computing clearly. If you still have doubts, please get in touch with me and I will try my best to help.
If you are really interested in knowing more about Cloud Computing, here is a free resource by Rackspace Hosting to start with:
Originally published at vaidik.in.
Thoughts on software engineering and technology
Thoughts on software engineering and technology
Written by
Software Engineer, Building Tech at Grofers
Thoughts on software engineering and technology
"
https://medium.com/@alibaba-cloud/alibaba-cloud-helps-us-picture-the-cloud-computing-future-over-the-next-decade-f5dcc08906ce?source=search_post---------274,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Sep 23, 2020·20 min read
By Ningchuan, Cloud Technology Era
Undoubtedly, the COVID-19 pandemic in 2020 has redefined the cloud computing industry. With the explosive growth of online business due to the pandemic, many enterprises and institutions have migrated their offline business online. Even the most traditional industries, such as education, agriculture, real estate, and construction, have begun to migrate to the cloud on a large scale. Public cloud data centers in Europe have seen an explosion in demands, which was previously unimaginable.
In the Chinese market, the cloud computing industry experienced a period of expansion that spans from 2010 to 2020. Alibaba Cloud put down its first line of code in 2009 and launched the first generation of Elastic Compute Service (ECS) in 2010. As one of the premier enterprises in China’s public cloud industry, Alibaba Cloud has connected the Chinese market to the public cloud. The market was initially suspicious since people had a poor understanding of the technology. People gradually embraced cloud computing as it grew, and as they started understanding the technology and market structure. Eventually, many vendors adopted cloud computing, and China’s cloud computing industry went through its initial growth stage from 2010 to 2020.
Now, we want to know how the cloud computing industry will develop over the decade following the COVID-19 pandemic. On July 15, 2020, Alibaba Cloud held its annual elastic computing launch event and announced the release of its proprietary third-generation X-Dragon cloud servers and new-generation ECS product family. When talking about the next decade of cloud computing, Zhang Xiantao, the Head of Elastic Computing at Alibaba Cloud, says the cloud computing model will reversely reconstruct basic hardware and software. The purpose of Alibaba Cloud’s proprietary integrated hardware and software architecture and comprehensive implementation of cloud-native is to reconstruct the cloud computing infrastructure. This is the idea for the future.
Do Alibaba’s proprietary X-Dragon servers allow enterprises to recreate their own data centers? During an interview at the annual 2020 Alibaba Cloud launch event, Zhang Xiantao laughed and said that many enterprise customers asked if they could purchase these servers for their own data centers after learning about the exceptional performance. This marks the shift from the first to the second decade of cloud computing.
First, let’s look at the performance of X-Dragon. The third-generation X-Dragon cloud server product family provides up to 208 CPU cores and 6 TB memory. Their cloud disk input/output operations per second (IOPS) can reach 1 million, with a network forwarding traffic of up to 24 million and network bandwidth of up to 100 GB. These indicators are all the best in the industry. X-Dragon supports multiple computing entities, including CPUs, GPUs, NPUs, and FPGA. For rapid scaling, 500,000 vCPU cores can be ready in 3 minutes. X-Dragon is also the optimal vehicle for cloud-native.
From the launch of the first-generation X-Dragon servers in 2017 to the third generation in 2020, Alibaba Cloud has launched three generations of proprietary X-Dragon servers in just three years. Compared with the previous generation, the third-generation X-Dragon cloud servers improve overall performance by 160%, working at least 30% faster than any competing cloud servers. In short, they provide the best overall computing power among cloud servers in the world. In addition to many performance enhancements, Alibaba Cloud improved the stability of individual ECS instances from 99.95% to 99.975% and the stability of cross-zone multi-instance layouts from 99.99% to 99.995%. These stability indicators are the highest in the world.
Can an enterprise achieve the same performance and stability by buying an X-Dragon server and installing it in its own data center? The answer is no. This is because X-Dragon servers are not standalone servers that can be installed in an enterprise data center. Instead, they function only when being integrated with Alibaba Cloud’s infrastructure. X-Dragon servers require the collaboration of tens or hundreds of thousands of integrated hardware-software devices to achieve their full potential.
The functionality of X-Dragon servers can be seen from shipping containers that reformed the world over the last century. In the 1950s, the invention of shipping containers shortened ocean shipping times by 85%, reducing shipping costs 30-fold. The significance of this innovation to human productivity cannot be overstated. However, product manufacturers cannot just buy a container and take it to their warehouse to enjoy enhanced logistical efficiency and productivity. Containers cannot work independently of the global infrastructure of cargo ships, docks, loading and unloading systems, railways, trucks, and other facilities. Containers completely revolutionized bulk cargo transportation because companies no longer had to buy their own ships. This led to a completely new way of thinking. In turn, the modern global system of logistics based on containers has changed the physical forms of containers and container facilities throughout the industrial chain. From the earliest container in 1830 to the first modern container in 1956, containers have changed dramatically.
Similarly, the public cloud model has to go through the same process. Ten years ago, the original public cloud system was built around independent servers, storage devices, networks, and other equipment and software. Today, the global public cloud system will lead to a redesign of servers, storage devices, networks, and other equipment and software. The biggest change lies in the transformation from standalone devices to interconnected standardized facilities. In an interconnected standardized system, you cannot extract an individual component and expect it to provide the capabilities of the entire system.
For Alibaba Cloud ECS instances, the X-Dragon computing platform, Apsara Distributed File System storage platform, and Apsara Cloud Network System network platform, together with the Alibaba Cloud Linux 2.0 operating system and cloud-native software system allow for better capabilities between cloud platforms to enable higher performance.
Over the past decade, Alibaba Cloud started from scratch and gradually improved its products and services by serving major customers. We launched ECS 1.0 for webmasters of small- and mid-sized enterprises in 2010. In 2015, we released ECS 2.0 for some challenging scenarios, such as 12306.cn. Then, in 2017 and 2018, our X-Dragon architecture provided support for the Double 11 Global Shopping Festival. “You could say that over 10 years of development, we have reshaped the entire computing service model and developed the capabilities to serve all business scenarios”, said Zhang Xiantao.
(Alibaba Cloud’s evolution over a decade)
What will cloud computing look like over the next decade? Over the past 2–3 years, Alibaba Cloud has been asking this question and exploring the answers. In September 2019, Alibaba Cloud released its third-generation X-Dragon architecture, which comprehensively upgraded the service capabilities of X-Dragon servers. This proprietary integrated hardware-software architecture greatly improves computing efficiency and network and storage service capabilities. Based on third-generation X-Dragon cloud servers, Alibaba Cloud created its next-generation ECS product family, including 6th-generation enhanced instances, 7th-generation high clock-speed instances, memory-enhanced hardware instances, new-generation GPU instances, NPU instances, supercomputing cluster instances, and the world’s first persistent memory instances.
The value of third-generation X-Dragon cloud servers is passed to users in the form of ECS instances. The 6th-generation enhanced ECS instances are composed of computing, storage, and network components. For computing, ECS instances use Alibaba Cloud’s proprietary Dragonfly Hypervisor, instead of KVM, XEN, or another virtualization architecture. The lightweight Dragonfly Hypervisor consumes fewer resources and significantly increases virtualization efficiency while reducing computing jitters to several millionths. In terms of storage, 6th-generation enhanced instances come standard with enhanced SSD (ESSD) cloud disks. The latest generation of ESSDs can reach 1 million IOPS per disk, with a single-channel latency as low as 100 microseconds. The outstanding performance of ESSDs greatly improves the storage capabilities of the 6th-generation enhanced instances. For network capabilities, Alibaba Cloud’s proprietary X-Dragon architecture provides excellent network I/O capabilities, with up to 24 million forwarding PPS per disk and a packet latency as low as 21 microseconds. These highlights ensure industry-leading network capabilities.
The 6th-generation enhanced instances combine the advantages of third-generation ECS Bare Metal (EBM) instances, Alibaba Cloud proprietary software and hardware, and other technologies. For example, a single ESSD provides 1 million IOPS, 4 Gbit/s throughput, 100-ms latency, and 5s-or-less snapshot backup intervals. By integrating these capabilities with third-generation X-Dragon cloud servers, the result is exceptionally powerful. In terms of networking, the 6th-generation enhanced instances provide forwarding capabilities of 24 million PPS, while the top performers in the industry are 10 million PPS. In addition, the 6th-generation enhanced instances feature a packet latency of 21 microseconds, compared to 30 microseconds from its nearest competitor. The 6th-generation enhanced instances support over 100 ENIs, while other instances in the industry only support up to 20.
In addition to their superiority in single-point applications, the 6th-generation instances also provide unparalleled end-to-end performance. In MySQL scenarios, these instances perform 190% better than any competitor. In Nginx scenarios, they outperform other competitors by up to 86%. In Redis scenarios, they outperform other competitors by up to 103%. Zhang Xiantao emphasized that, whether in terms of computing, networking, storage, or end-to-end performance, Alibaba Cloud 6th-generation enhanced ECS instances are superior to any competing products.
From the unparalleled performance of Alibaba Cloud 6th-generation enhanced ECS instances, we can better understand Alibaba Cloud’s views on the evolution of cloud computing technology and especially public cloud technology over the next decade. The large-scale operations and overall coordination of public clouds will drive a reconstruction in the underlying software, hardware, and applications to support larger-scale public clouds. Simply put, a chemical reaction between software and hardware in public cloud systems will create new species of products and services.
(Alibaba Cloud’s experience over the past decade)
In terms of software, Alibaba Cloud has comprehensively implemented cloud-native. Without a doubt, cloud-native is the cloud computing standardization technology of the present and future. This has been especially true since 2019 when Kubernetes and container technology laid a foundation for cloud-native technology and attained absolute dominance in the market. Kubernetes and containers are the equivalents of a set of standards for shipping containers, cargo ships, and docks in the maritime logistics industry. They compose the system of technical standards that will define cloud computing for the next decade and beyond.
Alibaba Cloud is now going all-in on cloud-native. Since Alibaba’s movement towards containers in 2011, Alibaba has been on a journey in the direction of cloud-native for 10 years. Over this decade, Alibaba Cloud has drawn on the strength of the richest selection of container products and services in the industry to grow in excess of 400% for several years in a row. Alibaba Cloud offers a rich portfolio of products and services in the container, service mesh, and serverless sectors. We have China’s largest family of cloud-native products, have made the most comprehensive contributions to the open-source cloud-native community, have experience with the largest cloud-native applications, and possess the largest cloud-native customer base. We provide over 20 products and services in 8 categories, including underlying infrastructure, data intelligence, and distributed applications. Our products and services can meet the needs of a wide range of industry scenarios.
In Gartner’s 2020 report on container services, Alibaba Cloud was the only Chinese vendor ranked. In March 2020, Gartner published its second Competitive Landscape: Public Cloud Container Services report. This report compares 10 features, including serverless Kubernetes, service mesh, and container images. Providing 9 of the 10 features, Alibaba Cloud and AWS offer a more comprehensive product portfolio than all of the other vendors.
The cloud-native software capabilities of Alibaba Cloud combine with our X-Dragon servers and other proprietary hardware to produce a chemical reaction. For example, the 6th-generation enhanced instances provide more than 100 ENIs, and the increase in the number of network interface controllers (NICs) per instance significantly improves container performance. However, the combination of EBM instances and container technology can provide higher performance than physical machines. Specifically, the difference in performance between EBM instances that run containers and X-Dragon and physical servers of the same specifications is 20% to 30%. Moreover, EBM instances also support Alibaba Cloud security containers, improving end-to-end security and isolation capabilities while increasing performance by 30% compared with open-source solutions. In addition, EBM instances support Alibaba Cloud’s new confidential computing containers to effectively ensure data security through the integration of hardware and software.
Alibaba ran the 2019 Double 11 Global Shopping Festival on X-Dragon EMB instances, increasing its performance by 20% to 30% and reducing total costs by more than 50%. During the COVID-19 pandemic, the container + EBM solution has provided exceptional performance in helping DingTalk deal with the heaviest traffic it has ever experienced. Previously, DingTalk was completely deployed on common physical machines. However, it saw a sharp increase in use because government institutions, enterprises, and schools all needed to turn to online collaboration during the pandemic. In response, DingTalk adopted a cloud-based elastic deployment solution based on EBM instances and containers. This allowed DingTalk to quickly scale out its computing capabilities by 100,000 CPU cores, providing the necessary support for its business applications.
Zhang Jianfeng, the President of Alibaba Cloud Intelligence, recently said that the cloud is a new computing architecture that represents a comprehensive upgrade from the large and compact servers of the PC era. The cloud is characterized by distribution. It places a greater emphasis on elasticity based on Internet distribution as well as more powerful and extensive scheduling, reuse, and security. This is a very important feature of the cloud. The development of cloud computing requires a complete cloud system. Over the past decade, Alibaba Cloud has refined this kind of system. Over the next ten years, Alibaba Cloud will use this cloud system to reconstruct its underlying hardware and software to support a larger-scale public cloud.
By Ningchuan, Cloud Technology Era
Undoubtedly, the COVID-19 pandemic in 2020 has redefined the cloud computing industry. With the explosive growth of online business due to the pandemic, many enterprises and institutions have migrated their offline business online. Even the most traditional industries, such as education, agriculture, real estate, and construction, have begun to migrate to the cloud on a large scale. Public cloud data centers in Europe have seen an explosion in demands, which was previously unimaginable.
In the Chinese market, the cloud computing industry experienced a period of expansion that spans from 2010 to 2020. Alibaba Cloud put down its first line of code in 2009 and launched the first generation of Elastic Compute Service (ECS) in 2010. As one of the premier enterprises in China’s public cloud industry, Alibaba Cloud has connected the Chinese market to the public cloud. The market was initially suspicious since people had a poor understanding of the technology. People gradually embraced cloud computing as it grew, and as they started understanding the technology and market structure. Eventually, many vendors adopted cloud computing, and China’s cloud computing industry went through its initial growth stage from 2010 to 2020.
(Zhang Xiantao, the Head of Elastic Computing at Alibaba Cloud)
Now, we want to know how the cloud computing industry will develop over the decade following the COVID-19 pandemic. On July 15, 2020, Alibaba Cloud held its annual elastic computing launch event and announced the release of its proprietary third-generation X-Dragon cloud servers and new-generation ECS product family. When talking about the next decade of cloud computing, Zhang Xiantao, the Head of Elastic Computing at Alibaba Cloud, says the cloud computing model will reversely reconstruct basic hardware and software. The purpose of Alibaba Cloud’s proprietary integrated hardware and software architecture and comprehensive implementation of cloud-native is to reconstruct the cloud computing infrastructure. This is the idea for the future.
Do Alibaba’s proprietary X-Dragon servers allow enterprises to recreate their own data centers? During an interview at the annual 2020 Alibaba Cloud launch event, Zhang Xiantao laughed and said that many enterprise customers asked if they could purchase these servers for their own data centers after learning about the exceptional performance. This marks the shift from the first to the second decade of cloud computing.
(Third-generation X-Dragon cloud servers inaugurate the era of super performance)
First, let’s look at the performance of X-Dragon. The third-generation X-Dragon cloud server product family provides up to 208 CPU cores and 6 TB memory. Their cloud disk input/output operations per second (IOPS) can reach 1 million, with a network forwarding traffic of up to 24 million and network bandwidth of up to 100 GB. These indicators are all the best in the industry. X-Dragon supports multiple computing entities, including CPUs, GPUs, NPUs, and FPGA. For rapid scaling, 500,000 vCPU cores can be ready in 3 minutes. X-Dragon is also the optimal vehicle for cloud-native.
From the launch of the first-generation X-Dragon servers in 2017 to the third generation in 2020, Alibaba Cloud has launched three generations of proprietary X-Dragon servers in just three years. Compared with the previous generation, the third-generation X-Dragon cloud servers improve overall performance by 160%, working at least 30% faster than any competing cloud servers. In short, they provide the best overall computing power among cloud servers in the world. In addition to many performance enhancements, Alibaba Cloud improved the stability of individual ECS instances from 99.95% to 99.975% and the stability of cross-zone multi-instance layouts from 99.99% to 99.995%. These stability indicators are the highest in the world.
Can an enterprise achieve the same performance and stability by buying an X-Dragon server and installing it in its own data center? The answer is no. This is because X-Dragon servers are not standalone servers that can be installed in an enterprise data center. Instead, they function only when being integrated with Alibaba Cloud’s infrastructure. X-Dragon servers require the collaboration of tens or hundreds of thousands of integrated hardware-software devices to achieve their full potential.
The functionality of X-Dragon servers can be seen from shipping containers that reformed the world over the last century. In the 1950s, the invention of shipping containers shortened ocean shipping times by 85%, reducing shipping costs 30-fold. The significance of this innovation to human productivity cannot be overstated. However, product manufacturers cannot just buy a container and take it to their warehouse to enjoy enhanced logistical efficiency and productivity. Containers cannot work independently of the global infrastructure of cargo ships, docks, loading and unloading systems, railways, trucks, and other facilities. Containers completely revolutionized bulk cargo transportation because companies no longer had to buy their own ships. This led to a completely new way of thinking. In turn, the modern global system of logistics based on containers has changed the physical forms of containers and container facilities throughout the industrial chain. From the earliest container in 1830 to the first modern container in 1956, containers have changed dramatically.
Similarly, the public cloud model has to go through the same process. Ten years ago, the original public cloud system was built around independent servers, storage devices, networks, and other equipment and software. Today, the global public cloud system will lead to a redesign of servers, storage devices, networks, and other equipment and software. The biggest change lies in the transformation from standalone devices to interconnected standardized facilities. In an interconnected standardized system, you cannot extract an individual component and expect it to provide the capabilities of the entire system.
For Alibaba Cloud ECS instances, the X-Dragon computing platform, Apsara Distributed File System storage platform, and Apsara Cloud Network System network platform, together with the Alibaba Cloud Linux 2.0 operating system and cloud-native software system allow for better capabilities between cloud platforms to enable higher performance.
Over the past decade, Alibaba Cloud started from scratch and gradually improved its products and services by serving major customers. We launched ECS 1.0 for webmasters of small- and mid-sized enterprises in 2010. In 2015, we released ECS 2.0 for some challenging scenarios, such as 12306.cn. Then, in 2017 and 2018, our X-Dragon architecture provided support for the Double 11 Global Shopping Festival. “You could say that over 10 years of development, we have reshaped the entire computing service model and developed the capabilities to serve all business scenarios”, said Zhang Xiantao.
What will cloud computing look like over the next decade? Over the past 2–3 years, Alibaba Cloud has been asking this question and exploring the answers. In September 2019, Alibaba Cloud released its third-generation X-Dragon architecture, which comprehensively upgraded the service capabilities of X-Dragon servers. This proprietary integrated hardware-software architecture greatly improves computing efficiency and network and storage service capabilities. Based on third-generation X-Dragon cloud servers, Alibaba Cloud created its next-generation ECS product family, including 6th-generation enhanced instances, 7th-generation high clock-speed instances, memory-enhanced hardware instances, new-generation GPU instances, NPU instances, supercomputing cluster instances, and the world’s first persistent memory instances.
The value of third-generation X-Dragon cloud servers is passed to users in the form of ECS instances. The 6th-generation enhanced ECS instances are composed of computing, storage, and network components. For computing, ECS instances use Alibaba Cloud’s proprietary Dragonfly Hypervisor, instead of KVM, XEN, or another virtualization architecture. The lightweight Dragonfly Hypervisor consumes fewer resources and significantly increases virtualization efficiency while reducing computing jitters to several millionths. In terms of storage, 6th-generation enhanced instances come standard with enhanced SSD (ESSD) cloud disks. The latest generation of ESSDs can reach 1 million IOPS per disk, with a single-channel latency as low as 100 microseconds. The outstanding performance of ESSDs greatly improves the storage capabilities of the 6th-generation enhanced instances. For network capabilities, Alibaba Cloud’s proprietary X-Dragon architecture provides excellent network I/O capabilities, with up to 24 million forwarding PPS per disk and a packet latency as low as 21 microseconds. These highlights ensure industry-leading network capabilities.
The 6th-generation enhanced instances combine the advantages of third-generation ECS Bare Metal (EBM) instances, Alibaba Cloud proprietary software and hardware, and other technologies. For example, a single ESSD provides 1 million IOPS, 4 Gbit/s throughput, 100-ms latency, and 5s-or-less snapshot backup intervals. By integrating these capabilities with third-generation X-Dragon cloud servers, the result is exceptionally powerful. In terms of networking, the 6th-generation enhanced instances provide forwarding capabilities of 24 million PPS, while the top performers in the industry are 10 million PPS. In addition, the 6th-generation enhanced instances feature a packet latency of 21 microseconds, compared to 30 microseconds from its nearest competitor. The 6th-generation enhanced instances support over 100 ENIs, while other instances in the industry only support up to 20.
In addition to their superiority in single-point applications, the 6th-generation instances also provide unparalleled end-to-end performance. In MySQL scenarios, these instances perform 190% better than any competitor. In Nginx scenarios, they outperform other competitors by up to 86%. In Redis scenarios, they outperform other competitors by up to 103%. Zhang Xiantao emphasized that, whether in terms of computing, networking, storage, or end-to-end performance, Alibaba Cloud 6th-generation enhanced ECS instances are superior to any competing products.
From the unparalleled performance of Alibaba Cloud 6th-generation enhanced ECS instances, we can better understand Alibaba Cloud’s views on the evolution of cloud computing technology and especially public cloud technology over the next decade. The large-scale operations and overall coordination of public clouds will drive a reconstruction in the underlying software, hardware, and applications to support larger-scale public clouds. Simply put, a chemical reaction between software and hardware in public cloud systems will create new species of products and services.
In terms of software, Alibaba Cloud has comprehensively implemented cloud-native. Without a doubt, cloud-native is the cloud computing standardization technology of the present and future. This has been especially true since 2019 when Kubernetes and container technology laid a foundation for cloud-native technology and attained absolute dominance in the market. Kubernetes and containers are the equivalents of a set of standards for shipping containers, cargo ships, and docks in the maritime logistics industry. They compose the system of technical standards that will define cloud computing for the next decade and beyond.
Alibaba Cloud is now going all-in on cloud-native. Since Alibaba’s movement towards containers in 2011, Alibaba has been on a journey in the direction of cloud-native for 10 years. Over this decade, Alibaba Cloud has drawn on the strength of the richest selection of container products and services in the industry to grow in excess of 400% for several years in a row. Alibaba Cloud offers a rich portfolio of products and services in the container, service mesh, and serverless sectors. We have China’s largest family of cloud-native products, have made the most comprehensive contributions to the open-source cloud-native community, have experience with the largest cloud-native applications, and possess the largest cloud-native customer base. We provide over 20 products and services in 8 categories, including underlying infrastructure, data intelligence, and distributed applications. Our products and services can meet the needs of a wide range of industry scenarios.
In Gartner’s 2020 report on container services, Alibaba Cloud was the only Chinese vendor ranked. In March 2020, Gartner published its second Competitive Landscape: Public Cloud Container Services report. This report compares 10 features, including serverless Kubernetes, service mesh, and container images. Providing 9 of the 10 features, Alibaba Cloud and AWS offer a more comprehensive product portfolio than all of the other vendors.
The cloud-native software capabilities of Alibaba Cloud combine with our X-Dragon servers and other proprietary hardware to produce a chemical reaction. For example, the 6th-generation enhanced instances provide more than 100 ENIs, and the increase in the number of network interface controllers (NICs) per instance significantly improves container performance. However, the combination of EBM instances and container technology can provide higher performance than physical machines. Specifically, the difference in performance between EBM instances that run containers and X-Dragon and physical servers of the same specifications is 20% to 30%. Moreover, EBM instances also support Alibaba Cloud security containers, improving end-to-end security and isolation capabilities while increasing performance by 30% compared with open-source solutions. In addition, EBM instances support Alibaba Cloud’s new confidential computing containers to effectively ensure data security through the integration of hardware and software.
Alibaba ran the 2019 Double 11 Global Shopping Festival on X-Dragon EMB instances, increasing its performance by 20% to 30% and reducing total costs by more than 50%. During the COVID-19 pandemic, the container + EBM solution has provided exceptional performance in helping DingTalk deal with the heaviest traffic it has ever experienced. Previously, DingTalk was completely deployed on common physical machines. However, it saw a sharp increase in use because government institutions, enterprises, and schools all needed to turn to online collaboration during the pandemic. In response, DingTalk adopted a cloud-based elastic deployment solution based on EBM instances and containers. This allowed DingTalk to quickly scale out its computing capabilities by 100,000 CPU cores, providing the necessary support for its business applications.
Zhang Jianfeng, the President of Alibaba Cloud Intelligence, recently said that the cloud is a new computing architecture that represents a comprehensive upgrade from the large and compact servers of the PC era. The cloud is characterized by distribution. It places a greater emphasis on elasticity based on Internet distribution as well as more powerful and extensive scheduling, reuse, and security. This is a very important feature of the cloud. The development of cloud computing requires a complete cloud system. Over the past decade, Alibaba Cloud has refined this kind of system. Over the next ten years, Alibaba Cloud will use this cloud system to reconstruct its underlying hardware and software to support a larger-scale public cloud.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/@auth0/4-ways-cloud-computing-can-improve-healthcare-cybersecurity-ba53695c2440?source=search_post---------275,"Sign in
There are currently no responses for this story.
Be the first to respond.
Auth0
Nov 9, 2018·6 min read
The healthcare industry is the second largest in the United States, but average spend on cybersecurity is just half as much as other industries, according to a recent TechCrunch report. This is surprising (and unfortunate), given how valuable patient records can be to cybercriminals. If a thief obtains medical records, he or she can do a host of things, including the following:
Hospitals, private practices, and third parties that store and process patient data are ripe for hackers.
In 2017 there were more than twice the amount of attacks on healthcare organizations vs. other industries — and is just the start of what we’ve seen so far this year. According to a recent Protenus report, between April and June of 2018, more than 3 million patient records were exposed. Threats are multiplying, and healthcare cybersecurity across teams is not keeping pace.
Last year, we highlighted how many healthcare organizations were facing challenges in modernizing their legacy systems. Since then, the issue of security has become more acute in the industry. Below, we detail four key reasons why cloud solutions are critical for healthcare organizations, and we offer suggestions on how to make the transition.
When you move data to the cloud, it gives you the chance to aggregate disparate streams of information in a single place. Even if the majority of your patient data is protected, when you have information in many different sources, such as spreadsheets, hard drives, and even hard copies, it can be easy for something like a prescription or a note from an operation to get lost in the flood.
Moving data to the cloud opens new opportunities for data management and integration (DMI) practices. While DMI is a complex and evolving space, in a nutshell, it is a set of policies and procedures that an organization puts in motion to be sure the right people have timely access to accurate data.
For hospitals, it’s essential that records are clean and up-to-date before an IT professional moves them or merges them with other information. If errors occur in patient names, medical histories, financial information, or prescription information, for example, it can end in a costly lawsuit and a stained reputation — not to mention physical harm if patients are not treated correctly.
For many organizations, downtime means lost business opportunities. Teams can’t respond to customer requests, follow up on leads, and deliver products and services if computer systems are suspended.
(Some companies are so concerned about this, given the enormous uptick in data breaches in 2018, that they’ve begun to take out cybersecurity insurance policies that can help recoup losses.)
In healthcare, more than money is on the line. If systems are down, staff aren’t able to book emergency appointments, and doctors and nurses aren’t able to confirm medications or timing for procedures. Lives are at risk.
In 2018, at New York’s Jones Memorial Hospital, computer systems were down for a week after a cyberattack. Jones Memorial is a small acute-care facility in a rural area. During the time it took to detect and clean up the issue, the hospital had to revert to manually entering patient data into medical charts and recreating prescription lists. Although the hospital had prepared for these downtime procedures, it still left more room for error and wasted time that staff could have spent delivering care.
Cloud vendors like Microsoft and IBM offer additional precautions, like up-to-date software, that can help mitigate the chances of a breach due to holes. They can also help solve common problems such as load balancing, which often leads to outages. If activity is too much for a single server to handle, these larger providers are able to spread demand over multiple servers, ensuring that you don’t short-circuit in times of high activity.
Cloud computing is far more than a repository for your information. It opens up a world of new opportunities for IT admins, helping them better understand how employees and third parties use their systems — and who is accessing what digital files at any time.
When you’re based in the cloud, you can begin to tack on additional features, such as Auth0’s user-management software, which quickly pulls details on existing users at any time.
This level of visibility makes it easier for admins to monitor for unknown visitors or strange behaviors. If, for example, they see “John” trying to gain access to a database he isn’t permitted to view, an IT executive can pull his profile, understand why he might be trying to get in, and either grant or block access. Admins even have the ability to log in as any user and attempt to troubleshoot from his or her vantage point.
Additional cloud solutions, such as identity management, can also help protect you from your own employees. Nearly one-third of the attacks on patient records in 2018 came from within the organizations themselves. You want to be sure you don’t have medical data still floating in file cabinets or on unprotected servers.
Providers like Amazon Web Services, Google Cloud, and Microsoft Azure come with on-demand teams and resources that can help customize cloud plans as organizations develop over time. Even if hospitals aren’t necessarily in hypergrowth mode like many startups, they still have shifting needs. These could be for increased or reduced load capacity, depending on staffing and patient demand; more private cloud requirements, given a larger volume of high-touch data; or a desire to move more to the public cloud, due to cost restraints.
Many healthcare organizations opt for a combination of public and private cloud deployment. Both offer flexibility for expansion and require less maintenance than in-house storage. The public cloud is less expensive; however, teams often go for at least a portion of data storage in the private cloud, given the following:
Due to the confidential nature of their work, government agencies and financial institutions also often opt for private cloud use. This gives an even greater sense of its capacity.
Security for healthcare teams is only getting more complicated. Medical devices, for example, are a growing concern. In a worst-case scenario, a cyberattack could tamper with pacemakers or other life-supporting devices. To this end, the U.S. Food and Drug Association recently released a new cybersecurity playbook, which, in part, sets new guidelines for creating medical devices with software that is adaptable to new threats.
The interconnected nature of delivering care reveals how an attack on one portion of the organization has the potential to affect all others. While cloud computing won’t guarantee protection against cybercrime, it will keep a team’s data organized and as secure as possible.
Finally, as healthcare teams work to comply with HIPAA, cloud computing helps providers and third parties stay prepared in the event of an audit with more visibility into day-to-day operations and tighter user permissions.
Auth0, a global leader in Identity-as-a-Service (IDaaS), provides thousands of enterprise customers with a Universal Identity Platform for their web, mobile, IoT, and internal applications. Its extensible platform seamlessly authenticates and secures more than 1.5B logins per month, making it loved by developers and trusted by global enterprises. The company’s U.S. headquarters in Bellevue, WA, and additional offices in Buenos Aires, London, Tokyo, and Sydney, support its customers that are located in 70+ countries.
For more information, visit https://auth0.com or follow @auth0 on Twitter.
Originally published at auth0.com.
Identity Is Complex, Deal With It. Auth0 is The Identity Platform for Application Builders.
Identity Is Complex, Deal With It. Auth0 is The Identity Platform for Application Builders.
"
https://medium.com/@alibaba-cloud/breakthrough-in-alibaba-cloud-computing-capabilities-bigbench-reaches-100-tb-world-record-5bd119b145ce?source=search_post---------276,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Dec 5, 2017·4 min read
In the first day of the 2017 Hangzhou Computing Conference on Oct. 11, Alibaba Cloud President Hu Xiaoming introduced a next-generation computing platform MaxCompute + PAI.
In the main forum on the 12th, Zhou Jingren, Alibaba Group Vice President and director from the Search Division and Computing Platform Division, said that data lays the foundation for artificial intelligence innovation, and possessing plenty of computing capabilities to help fully release the value of the data. Later, Zhou Jingren released BigBench On MaxCompute[1] 2.0 + PAI with Rob Hays, Vice President of Intel’s Data Center Division. The release broke the best records set by TPCx-BB[2] and reflected the extremely robust data processing capabilities of MaxCompute and the absolute strength of public cloud compared to the traditional model.
At present, the maximum capacity publicized by TPC is 10 TB, the best performance is 1491.23 BBQpm, and the best price/performance ratio is 589 Price/BBQpm. Alibaba Cloud’s BigBench on MaxCompute 2.0+PAI expands that capacity to 100 TB for the first time in the world, which is also the first benchmark to be based on public cloud services. Engines running on this platform achieve 7000 points.
It was reported that MaxCompute test environment would be open for one month on public cloud after the conference and that the BigBench On MaxCompute+PAI SDK (inherited from TPCx-BigBench and enabling it to run on the big data environment of Alibaba Cloud) would be open-source for developers to use.
The great capacity breakthrough of BigBench on MaxCompute owes to MaxCompute’s mass data processing capabilities and machine learning algorithm efficiency. MaxCompute, based on the Apsara distributed OS developed by Alibaba Cloud, can connect more than 10,000 servers in a single cluster and process Exabytes of data.
MaxCompute next-generation engines get continuous and in-depth performance optimization in the Compiler, Optimizer, and runtime. In addition to high-performance computing, Alibaba Cloud PAI provides users with a robust algorithm experiment platform which includes traditional machine learning as well as the latest in deep learning and enhanced learning. PAI provides a great number of algorithms and tools to meet algorithm requirements in different business scenarios. The platform is also optimized for performance and data capacity.
Furthermore, MaxCompute and Intel processor integration and in-depth optimization enable full use of Intel Xeon® Scalable Processor’s structural strengths. Rob Hays, Vice President of Intel’s data center division, said “We are delighted to be working with Alibaba Cloud to optimize MaxCompute on the latest Intel® Xeon® Scalable processor platform and to witness the excellent performance of MaxCompute in the BigBench test.”
Well, What computing bonuses does BigBench on MaxCompute2.0+PAI bring for developers to help them seize more market opportunities?
BigBench on MaxCompute is also an industrial benchmark, which demonstrates the software stack integrity of MaxCompute in big data processing and the superior performance in capacity, cost, and scalability.
BigBench on MaxCompute is very easy to access. Enterprise customers can connect to the platform provided they have prepared:
[1] BigBench on MaxCompute is derived from TPCx-BB, so it is compatible with all TPCx-BB semantics. [2] TPCx-BB (BigBench) was released by Transaction Processing Performance Council (TPC) in Feb. 2016. First E2E big data analysis app-level benchmark.
Source article: https://cloudfocus.alibabacloud.com/Breakthrough-in-Alibaba-Cloud-Computing-Capabilities---BigBench-Reaches-100-TB-World-Record-225310
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/cloud-simplified/devops-and-cloud-computing-beyond-the-hype-db49877047c?source=search_post---------277,"There are currently no responses for this story.
Be the first to respond.
by Harold Bell
Cloud and DevOps have played a big role in helping IT address some of the biggest transformative shifts of our times. The rise of the service economy, unprecedented pace of disruption, and infusion of digital into everyday life are just some examples. These are the shifts that are driving business in the 21st century.
The market is dynamic and organizations have legacy applications that coexist with microservices and on-premises cloud infrastructure. Essentially we live in a hybrid world — which includes infrastructure, tools, processes, and applications. DevOps, in that respect, can support multiple aspects of an organization’s hybrid existence. DevOps supply a wide variety of tools, from biometric authentication tools to code development tools. The astounding rise of containers, microservice architectures, and the proliferation of machine intelligence is helping organizations solve their everyday problems.
DevOps establishes an environment where developing, testing and releasing software can happen frequently, rapidly and more reliably. As mentioned, it emphasizes collaboration between software developers and other IT professionals. So far, it has proven to be an amazing method with the potential to push technology even further.
An essential element of DevOps is that development and operations are bound together, which means that configuration of the infrastructure is part of the code itself. This basically means that unlike the traditional development process, the machine becomes part of the application. This is almost impossible without cloud, because in order to get better reliability and performance, the infrastructure needs scale up and down as needed.
DevOps has gained its spotlight in the software development field and is growing. The practice has seen a tremendous increase in adoption in recent years, becoming an essential component of software-centric organizations. But when DevOps and cloud come together is when the real magic is created. Some of the most promising innovations and developments in the IT space are taking place around DevOps and hybrid cloud. Enterprises are moving beyond agile planning. They are migrating to cloud. Cloud offers automated provisioning and scaling to accommodate applications. The elastic nature of cloud enables better scalability. DevOps streamlines and accelerates application releases because the development is fast and reusable in nature.
DevOps is dictating a new approach to cloud development. With the increased adoption of cloud, many software companies are experiencing a transition from being product-oriented to service-oriented. Previously, companies develop products and hand it over to customers. Now, they also take care of the operations after the product has been delivered. To this end, the cloud has come out as a clear winner, as it makes service delivery along with product delivery a breeze.
Read the rest of the blog here: https://www.nutanix.com
© 2019 Nutanix, Inc. All rights reserved. Nutanix, the Nutanix logo and the other Nutanix products and features mentioned herein are registered trademarks or trademarks of Nutanix, Inc. in the United States and other countries. All other brand names mentioned herein are for identification purposes only and may be the trademarks of their respective holder(s).
An inclusive approach to cloud management.
Written by
We make infrastructure invisible, elevating IT to focus on the applications and services that power their business.
An inclusive approach to cloud management.
Written by
We make infrastructure invisible, elevating IT to focus on the applications and services that power their business.
An inclusive approach to cloud management.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/bitcherryglobal/decentralized-cloud-computing-platform-bitdns-officially-join-bitcherry-all-spark-plan-3a2d8fca5175?source=search_post---------278,"There are currently no responses for this story.
Be the first to respond.
April 14, 2021, decentralized cloud computing platform-BitDNS became the first to join BitCherry distributed business ecosystem partners.BitDNS will provide BitCherry Chain applications with distributed domain name resolution services and a key to the center to help businesses and individual users more conveniently, secure, and trade digital assets and manage data information blockchain. In the future, BitCherry and BitDNS will build a decentralized network ecosystem to establish a secure and convenient distributed business system on the chain.
🚀 About All Spark Plan 🚀
✅ Apply All Spark Plan: http://bit.ly/AllSparkPlanForm
✅ Build Blockchain On BitCherry: https://developers.bitcherry.io/#/apiReferenceManual
Official Website: http://bit.ly/BitCherryOfficialWebsiteTwitter: http://bit.ly/BitCherryTwitterTelegram: http://bit.ly/BitCherryTelegramDiscord: http://bit.ly/BitCherryDiscord
The world’s first commercial scalable blockchain…
Written by
https://www.bitcherry.io/
The world’s first commercial scalable blockchain infrastructure based on IPv8 technology
Written by
https://www.bitcherry.io/
The world’s first commercial scalable blockchain infrastructure based on IPv8 technology
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@TheDigitalTP/the-future-of-the-cloud-cloud-computing-in-2019-60b7508b460c?source=search_post---------279,"Sign in
There are currently no responses for this story.
Be the first to respond.
The Digital Transformation People
Jan 17, 2019·5 min read
We may have reached a tipping point where artificial “intelligence” and “big data” snap up most of the tech headlines, but make no mistake: Cloud computing is as vital as ever.
And no wonder: It’s the computing paradigm that’s helped to shape nearly all the others in recent years. But where does “the” cloud seem to be headed in 2019 and beyond — and what does that trajectory mean for business? Let’s take a look.
Cloud software providers will continue to compete for new verticals
“Enterprise,” broadly, has been a significant beneficiary of cloud computing’s many benefits. But right now, there are three big names more or less monopolizing a lot of the opportunity in the platform as a service (“PaaS”) and infrastructure as a service (“IaaS”) industries: Google Cloud, Amazon Web Services and Microsoft Azure. There are many other household names in this space too, such as Salesforce and Oracle, but every last one of them will need to redouble their efforts to differentiate their services, applications and customer relationship management to suit a wider variety of industries and verticals.
Specifically, expect to see offerings from these companies branch further into controllers for robotics and automation, AI-powered enterprise management systems and more. Using cloud-based enterprise planning and facility management means using time and other resources more effectively and running a leaner operation overall.
The capacity of the world’s data centers will expand almost exponentially (because they have to)
Human beings produce an astonishing amount of data in an average calendar year: as much as 2.5 quintillion bytes of data per day, according to IBM.
What this means is that worldwide demand for cloud storage capacity will continue to grow at an astonishing rate — including, quite likely, at your own business, if you’re an organizational leader. Whether or not your company’s service offerings are specifically centered on providing telecommunications or connectivity services, you still probably rely on a mixture of any number of the following cloud-centric tasks:
As the potential applications for cloud computing have expanded, the cost has inevitably fallen, giving rise to even further growth and even faster adoption. We can likely expect 2019 to even further improve on the growth the cloud saw between 2017 and 2018 when our collective storage capacity rose from 600 exabytes to 1.1 zettabytes. In case these are numbers you don’t use a lot at home, that’s double the capacity in one year.
A lot of those far-future-sounding technological breakthroughs for business, such as AI in systems management and advanced analytics, depend in part on how well company leaders make the case to “buy in” and reap the rewards. Increasingly, as costs for cloud infrastructure — and infrastructure services — fall further in 2019 and beyond, that becomes an easier case to make.
The Internet of Things will become even more accessible
We’ve seen an explosion of industrial potential from the IoT in recent years. Consider it a welcome side-effect of cloud infrastructure becoming generally more attractive as an investment with a positive ROI.
Even the smallest machine shops can retrofit their CNC machines with sensors and intranet connectivity for a greater degree of control, far higher tolerances in some cases and more proactive maintenance efforts. The result — thanks to the proliferation of connectivity technologies, especially the cloud — will be a continued convergence of the physical (equipment and tools) with the digital (wireless connectivity and “awareness”), throughout 2019 and beyond.
Of course, you can have all of the sensor-laden fabrication, assembly or product handling equipment in the world, but without a robust company network to run it, and more than likely a software provider who knows the IoT inside and out, it wouldn’t mean much. Many of the names you’ve come to recognize in cloud computing, web hosting, business and enterprise management applications and more are making significant moves in offering IoT-focused services and infrastructure: AWS IoT (Amazon), ARM, AT&T, Cisco, Dell, Ayla, Bosch and Fujitsu, among others.
New business models will make business and personal clouds even more configurable
The concept of serverless computing isn’t brand new, but we can expect it to reach new heights of popularity in 2019. Put most simply, serverless computing replaces client-side cloud infrastructure — servers — with collections of APIs and dashboards, provided by IaaS companies, which they can tailor to their business needs and then have the IaaS company seamlessly assemble the required infrastructure on their end as needed.
Companies engaging in this kind of business model going forward will enjoy a number of benefits. Most notably, they’ll be offloading configuration duties to a cloud service provider, and each company won’t need to build or rent its own server facilities anymore and risk needing another update in the future when business needs change or growth happens.
Cloud computing, 5G’s murky future and more
Expect to see even further changes in 2019 that will likely directly impact the way cloud-based services and infrastructure are built, rented and managed. For instance, there’s a full-blown “tech war” going on between the U.S. and China for dominance in the still-nascent 5G industry.
The idea of national and even global 5G networks promises an even faster and more convenient way for enterprises to connect their equipment, workstations and remote workers. But corporations aren’t quite honest about how fast it’s supposed to be, national powers are arresting each others’ representatives to draw political lines in the sand, and the Wi-Fi Alliance is promising we’ll always need Wi-Fi no matter how 5G ends up working out.
The point is, there’s a lot going on — but there’s always going to be a cloud. As we’ve seen here today, the fact that fewer companies will have to build their own cloud infrastructures in the future means there are that many more small businesses who can insulate themselves against changing software and hardware standards, leaving it up to the vendors to compete for dominance instead.
Originally published at www.thedigitaltransformationpeople.com on January 17, 2019.
Helping you discover the what, how and who of digital transformation. We’ve built a network of incredible people who can help you make a success of yours.
See all (1,331)
Helping you discover the what, how and who of digital transformation. We’ve built a network of incredible people who can help you make a success of yours.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@knoldus/know-about-cloud-computing-architecture-75d2e8d89507?source=search_post---------280,"Sign in
There are currently no responses for this story.
Be the first to respond.
Knoldus Inc.
Jul 5, 2021·3 min read
What is the cloud: It is the delivery of user on-demand resources like servers, storage, and software over the internet. In simple words pay and use a service provider. It gives the capacity to assemble, plan, and oversee applications on the cloud stage. The best example is AWS(Amazon Web Services), Google e.t.c.
In cloud, the service provider is the vendors who provide services to manage applications globally through the internet. Example:- Microsoft Azure
Front-end used by client or service consumer. In front-end framework contains all the customer side interface equipment parts and programming segments like information stockpiling, worker, virtualization programming etc. It provides a Graphical User Interface to end-users.
The back end deals with every one of the projects that run the application toward the front. It has an enormous number of information stockpiling frameworks and administrations.
Application:- Based on the user need the application gives yield to the end-user(with the asset) in the back end.
Service:- Service is an important component of architecture. Its task is to provide utility in the architecture. Few services that are widely used among the end users are storage application development environments and web services.
Storage:- It keeps up and deals with any measure of information over the web. A portion of the instances of capacity administrations is amazon S3, Oracle Cloud-stockpiling. The capacity limit shifts relying on the help gives accessible on the lookout.
Management:- It manages resources for a specific task and the components of the cloud environment. It oversees parts like application, task, administration, security, and cloud framework.
Security:- Security is a fundamental piece in architecture. It ensures assets, documents. It moves to set aside data of both cloud customers and providers.
1. Hypervisor:- It is a virtual working stage for each client. It is utilized to virtualize the actual machine. It runs a different virtual machine toward the back which comprises programming and equipment.
2. Management Software:- Its obligation is to oversee and screen cloud activities. All the operational assignment is finished by the board programming. It improves the presentation of the cloud. Like High security, Flexibility, full-time access.
3. Deployment Software: — The Deployment software contains all the mandatory installations and designs needed to run a cloud administration. Each arrangement of cloud administration is performed utilizing organization programming.
The three different deployed models are :-
(i). Saas:- Saas stands for Software as a service. It hosts and manages the applications of the end-user. The best example is Gmail.
(ii). Pass:- Pass stands for Platform as a service. It helps developers to build, create and manage applications. Example:- Microsoft Azure
(iii). Iass:- Iass stands for Infrastructure as a service. It provides service on a pay-as-you-go pricing model. AWS is example of Infrastructure as a service.
4. Network:- It helps to connect back-end and front-end, permits each client to get to cloud assets. It assists clients with associating and modify the course and practice. It is exceptionally adaptable, secure, and practical.
5. Runtime Cloud :- It provides execution and runtime environment to the virtual machine.
6. Cloud Storage:- Distributed storage is the place where each information is put away and gotten to by a client from anyplace on the planet over the web. Information can be adjusted and recovered from distributed storage over the web.
Group of smart Engineers with a Product mindset who partner with your business to drive competitive advantage | www.knoldus.com
Group of smart Engineers with a Product mindset who partner with your business to drive competitive advantage | www.knoldus.com
"
https://medium.com/@alibaba-cloud/alibaba-cloud-empowers-chinas-open-source-cloud-computing-communities-b7e6507fe690?source=search_post---------281,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Oct 17, 2018·6 min read
Since last year, the LinuxCon + ContainerCon + CloudOpen China (LC3) conference had entered China at the time when open source cloud computing was booming in the technical circle in China, attracting more attention from international counterparts.
Besides classic open source topics, the LC3 conference focuses on current hot technical areas such as AI, deep learning, blockchain, container, serverless, microservices, and automatic orchestration. The LC3 conference is hosted by the Linux Foundation, which develops from the Linux project at the beginning to many excellent projects today such as Kubernetes, CNI, Containerd, DPDK, and Xen and several sub-foundations such as CNCF, CF, Hyperledger, and ONAP.
Alibaba Cloud has always kept compatible with the community’s open source standards. For example, Alibaba Cloud is one of the first platforms that passed Kubernetes consistency authentication, supported Containerd 1.1, provided the high-performance network Terway plug-in, and supported open source tools such as CNI, Helm, and Istio. The Alibaba Cloud platform also supports the use of deep learning frameworks such as Trensorflow and Caffe and blockchain frameworks such as Hyperledger. Meanwhile, Alibaba Cloud has proactively joined many renowned international open source organizations, including the Linux Foundation, CNCF, CF, Free Software Foundation, and Apache Software Foundation.
Open source is an inevitable trend in the primary technology field. It is evident that a basic technology without getting open sourced does not have vitality as its broad use is difficult. The development history of open source software brings excellent reference value to people inside and outside the software industry.
Upgradation and sophistication of the entire technology will occur after the extensive use of an open source project in business. Earlier, this process may have taken three, five, or even ten years. However, with open source, it is possible within months.
The most well-known open source software is the Android system. Statistics show that 81.7% of mobile users are using the Android system currently, which is undoubtedly one of the most influential software. Without this open source functionality of Android, there may not be so many mobile phones and related ISVs. It is unpredictable when a similar mobile Internet ecosystem will come.
Linux is indeed one of the most successful open source software in both its current ecosystem building and technical evaluation. Linux has also contributed and laid the foundation for today’s cloud computing. Hadoop, Spark, and OpenStack are evidently successful. These open source projects use underlying technologies and support development of big data and cloud computing.
Another example is the ApsaraCache project of Alibaba Cloud ApsaraDB for Redis. Before making it open source officially, this project has been technically improved for four years and tested in more than 10,000 instances in the production environment. As said by the Redis founder Salvatore, it is marvelous to open the source code of ApsaraCache as it can attract more Redis core specialists worldwide to participate in the project and further improve stability and availability of the product. He also said that Alibaba Cloud is capable of developing Redis products. He hoped to work with Alibaba Cloud together to further improve the product features.
The open source technologies have penetrated into various fields; including operating systems, databases, compiling toolchains, web servers, today’s distributed system management projects, deep learning frameworks and tools, and different blockchain technologies. They also reflect the trends and dynamics of the technical circle, increasing its vitality. From another perspective, the growth of open source supports the business development because many customers hope to gain business support and values.
The entry of LC3 into China indicates that unlike the past, Chinese enterprises now become increasingly important in the open source field. Almost all representative enterprises in China’s IT industry are playing an essential role in the open source field. They have now become one of the leaders in the open source world instead of requesters in the past. Vendors outside China are more interested in the Chinese open source environment with latecomer advantages compared with European and American enterprises with decades of software development history.
Chinese software companies can utilize reliable technologies that have been verified by their counterparts. With this advantage of open source, these companies can use new technologies faster than their Western counterparts. Instead of thinking about how to cultivate the open source field, Chinese enterprises need to consider how to excel in this field.
Chinese enterprises have already worked on open source to adapt to the era of enterprise-level market bursts. Some enterprises have already run far ahead of their competitors. Alibaba Cloud, with the open and win-win genes, has instinctively chosen the way of open source. Alibaba Cloud is always proactively cooperating with international open source organizations and is an active developer of Linux. It has submitted more than 290 patches for the Linux kernel, making the most significant contribution among all Internet companies in China. In the latest organizations ranking released by GitHub, Alibaba has risen to the sixth place and has more than 150 open source projects.
The Computing Conference is a perfect window to observe the movements of the Alibaba family. This conference shows Alibaba’s determination in the open source road. In the Computing Conference 2017 in Hangzhou, Jeff Zhang, CTO of Alibaba Group, publicly demonstrated Alibaba’s open source omnidirectional map from 110 to 150 open source projects, Star 170000+, and from device to cloud, IoT, and intelligent full-stack technology feedback.
Chinese enterprises have gradually become trendsetters in the open source world, especially in the enterprise-level market. With the development of emerging technologies such as cloud computing and IoT, open source faces both challenges and unprecedented opportunities. Chinese enterprises are expected to catch up with competitors in the new wave of technology, and Alibaba Cloud will keep moving forward in the wave of open source.
Alibaba Cloud does not aim to show its strength in the open source field. Instead, it wants to invite more and more top engineers through open source to make progress together in the multi-scenario application. Early in 2012, Alibaba Cloud’s open source technology has bloomed on the cloud. The following are some classic examples.
In 2012, the third-generation distributed message-oriented middleware RocketMQ announced its open source. Behind each transaction of millions of users, RocketMQ undertakes the message flow of the Alibaba Cloud production system. In the 11/11 shopping spree in 2016, RocketMQ successfully undertook a record transaction peak (175,000 transactions/second) and guaranteed that the message delay time of 99.996% was within ten milliseconds.
Also, in the Computing Conference 2016, many people witnessed that the status of the AliSQL code library on GitHub changed from Private to Public. The handshake between Michael Wideneus (father of MySQL) and Chu Ba (father of AliSQL) indicated the continuous advancement of the database technology in the cloud development process.
Last year, Alibaba launched the OpenMessaging project and developed the distributed message-oriented middleware and streaming application development standard together with Yahoo, DiDi, and Streamlio. Currently, this standard has officially entered the Linux Foundation. It is the first international standard in the distributed message field that is launched worldwide from China. This standard provides an open API that can interconnect with other measures without the limitation of programming languages. It meets the enterprises’ requirements for scalability, isolation, and security. Moreover, it provides large-scale industrial support and assists adding of standard reference points and standardized tests.
In the LC3 conference in 2018, Alibaba Cloud officially announced the open source of the MongoShake data replication platform, which can replicate data across data centers with a maximum processing capability of up to 500,000 QPS. This platform enables efficient disaster recovery and multi-active business across data centers. It solves the problems of inflexible deployment and disaster recovery of multiple data centers and single-point writing, brings convenience to the company, and reduces the O&M pressure. Compared with the existing solutions in the industry, it provides better features and improved performance.
Year by year, Alibaba Cloud keeps moving forward in the open source field. As mentioned before, Alibaba Cloud does not aim to show its strength in the open source field. Instead, it wants to invite more and more top engineers to make progress together in the multi-scenario application by starting from open source technologies.
Read similar blogs and learn more about Alibaba Cloud’s products and solutions at www.alibabacloud.com/blog.
Reference:https://www.alibabacloud.com/blog/alibaba-cloud-empowers-china%27s-open-source-cloud-computing-communities_594061?spm=a2c41.12125413.0.0
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/@3commastutorials/blockchain-the-future-of-cloud-computing-881679112c64?source=search_post---------282,"Sign in
There are currently no responses for this story.
Be the first to respond.
3Сommas Blog
Jul 21, 2020·7 min read
The term “Blockchain” has become synonymous with “cryptocurrency” for many of us. However, it is in fact, the latest and greatest financial technology that completely transforms business transactions, supporting and using various cryptography models. Although the first boom in blockchain popularity did come from cryptocurrencies, the ongoing expansion of this technology into other areas is now in full swing.
One of the most explosive innovations developing rapidly is the use of blockchain technology in cloud computing. Many organizations use cloud storage and benefit from cloud computing technology. If blockchain gets incorporated into the mix, the chances of witnessing a real revolution in shaping entire industries rise dramatically. In this article, we will take a closer look at the application of blockchain in cloud computing systems.
What is cloud computing?
Cloud operators provide computing services such as storage, databases, networks, and data processing over the Internet rather than on local servers or personal computers. “The cloud” refers to a platform for hosting computing and storage resources. This is a so-called virtualized data center that is available for rent in exchange for a set fee.
Cloud services are more cost-effective than traditional data centers. Many large organizations worldwide are using cloud computing systems to solve data management challenges, reduce infrastructure, and maintenance costs.
Cloud computing working principles
Cloud computing services provide browser-based dashboards that enable IT professionals and developers to organize resources and effectively manage accounts.
Cloud services are also designed to work with REST APIs and Command Line Interfaces (CLI). Cloud computing uses a traditional distributed database architecture where data is stored on the computers of all participants.
Cloud computing history
Cloud computing can be traced back to the 1960s when John McCarthy and Douglas Parhill explored the idea of ​​providing computing as a public utility.
IBM introduced the practical application of resource sharing in the 1970s with the concept of “time-sharing.” At a time when users were constrained by booking times and sequential computing or “batch processing,” IBM introduced RUSH (Remote Shared Hardware Users). This allowed multiple users to simultaneously use one computer’s computing resources through so-called virtual terminals, also known as “thin clients”.
These events are considered to be the birth of virtualization in computing, and in the 1990s, they formed the stepping stone to cloud computing. As the computing power of machines and the bandwidth of the Internet increased gradually, companies began to use these resources on-demand and in a dynamically scalable manner. Salesforce introduced in 1999 can be called the first successful cloud computing implementation to host its CRM system.
Cloud computing today
Cloud computing embodies the foundations of digital business. This industry has grown at an extraordinary pace over the past ten years. The cloud computing industry is expected to surpass $210 billion by 2022. We are seeing more and more ways to connect and collaborate, moving to the cloud.
This accelerated growth is due to the increase in the volume of work and the number of games on the Internet, increased consumer demand for bandwidth and content, the global growth in the number of smartphones, the rapid development of web and mobile applications, and high costs of managing developers on a local server.
Leading Centralized Cloud Service Providers (CSP) — Amazon Web Services (AWS), Google Cloud, Microsoft Azure, and Alibaba Cloud — dominate with a 71% market share. AWS alone is three times the size of Microsoft Azure, the second-largest player in the CSP market. Cloud computing is a concentrated and oligopolized market with ineffective pricing and anti-competitive practices such as split mining, where a large CSP copies the products of a small cloud company (already offered on its platform) and sells products that often include developments from unrivaled competitors. With market oligopolization, innovation, choice, and flexibility are stifled, and ongoing services are costly. In this market context, there are 8.4 million data centers, and experts estimate that 85% of server capacity is underutilized.
Blockchain and cloud computing
Companies around the world have realized that meeting the demands of digital business infrastructure takes a more decentralized approach. For greater access, flexibility, efficiency, and security, the cloud must evolve into a decentralized infrastructure. Innovations such as serverless technology are causing the computing paradigm shift, freeing developers from configuring and managing the server, and allowing them to manage the resources required to run applications.
Interest in blockchain technology is growing throughout the world and in every industry. Numerous leaks in the cloud computing sector have pushed many startups to consider using blockchain technology for financial transactions and storing confidential information. This shows that the centralized cloud computing approach is starting to lose its grip. This situation brings us back to a hybrid cloud approach, where companies incorporate blockchain in cloud computing and store data in both public and private clouds.
Using blockchain technology in cloud computing is associated with a number of benefits:
Blockchain-based cloud computing projects
Golem is a decentralized marketplace where you can rent or lease your computing power in exchange of Golem tokens. It’s a kind of a supercomputer utilizing the power of on-chain computing. Their solution is designed for a small, addressable market, requires users to learn a new language, and has no cost advantage since there is no fixed pricing structure.
Dfinity is committed to exploring the territories of the blockchain interconnected with the Ethereum project’s original goals. Dfinity’s vision is built around the idea that the new Internet infrastructure should support a wide range of applications for end-users and enterprises. Social networking, messaging, search, storage, and peer-to-peer interaction are all examples of functionality that they plan to host on their public Web 3.0 cloud. It is worth saying that Dfinity is focused on smart contracts that have many restrictions.
Blockstack aims to provide a new type of decentralized Internet that gives users more control over their own data and makes it harder to censor content. Blockstack makes building decentralized apps as easy as building cloud apps. Developers use common libraries that simplify the process of creating solutions on the blockchain and do not require learning a new programming language. Users receive a universal login that does not require passwords and an encrypted drive that allows them to transfer their data to any application they want. Note that the project is based on the Bitcoin protocol and is not fully suitable for computing.
Ankr Network provides a cloud computing resource management platform, building an ecosystem of resource users, providers, application developers, consumers, and more. Ankr is building a complete cloud infrastructure and marketplace for containerized cloud services. Datacenter owners can monetize spare computing resources with the Ankr cloud platform. Node operators can use unoccupied cloud resources from data centers to launch blockchain nodes at a competitive price. The ANKR token is used as an asset for incentives, payments, and placement on the Ankr network.
Akash Network is the icing on the cake of our review. As a decentralized alternative to the cloud oligopoly, Akash Network is developing Supercloud. This decentralized cloud computing marketplace allows any data center and anyone with a computer to become a cloud service provider by securely offering their unused computing cycles. The Supercloud platform efficiently utilizes Tendermint and Cosmos and achieves impressive results by integrating cutting-edge containerization technologies with a unique bidding model to accelerate adoption. AKT is a utility token to provide economic security, stimulate early adoption, and stabilize exchange rates. Akash is also developing Supermini, a powerful portable supercomputer that allows users to become Akash cloud providers from the convenience of home or office. Akash technology is designed to meet general computing needs and broader market use. The platform is designed to serve the mainstream market and meet its growing needs. If the app runs on existing cloud providers such as AWS, Google Cloud, or Azure, it will be able to run on Akash.
The future is near
Blockchain is definitely the next step in ensuring information security. Data is stored on numerous nodes around the world and does not have a single point of vulnerability. This solves the problem of data protection in many possible unexpected scenarios. Files uploaded to the blockchain are controlled and accessible only to those parties who have a corresponding key to access encrypted files. Existing blockchain projects have certain drawbacks, but solutions such as Akash Network allow secure access to cloud computing at a tenfold cost reduction compared to the current vendors on the market.
Trading blog. Tools, bots, strategies.
Trading blog. Tools, bots, strategies.
About
Write
Help
Legal
Get the Medium app
"
https://irelandstechnologyblog.com/editorial-the-benefits-of-cloud-computing-5b7fab82cb0a?source=search_post---------283,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
If you’re not aware of the impact that cloud computing systems have had on the world in recent years, then we’d struggle to imagine where you’ve been all this time.
Whether we’re talking about business environments, educational organisations and universities, or even the family home, the cloud has become central to our technology needs. Take a look at these benefits of using a cloud system and get yourself up to speed with what all of the fuss is about.
In the world of business, cloud computing has a wealth of benefits; the most prominent being flexibility. Having data and programmes based online, as opposed to on a server in an office, means that employees can work from anywhere on their own computers. Long gone are the days when situations like bad weather and delayed flights could impact the day-to-day running of a business.
There’s also less risk of losing important data when it’s stored on a cloud system. We’re all familiar with news reports of politicians and government officials leaving laptops containing vital information on trains and in airports. In the latest of these incidents, IT Pro reported that the Information Commissioners Office (ICO) will be fining East Lincolnshire Council in the UK £80,000 for losing sensitive data. The loss occurred when a memory pen went missing from a laptop — a risk which is eliminated by the use of data clouds.
Thought the benefits are clear, it’s important that a business chooses the right cloud solution. Companies like TSG offer a range of cloud computing, although they highlight that the nature and scale of a business will determine which service is most appropriate and effective.
Having a document in one place, which can be accessed by multiple users, is also an effective tool for increasing efficiency in the workplace. Rather than having multiple editions of a document flying round via email, all with different names and formats, one document in one place is the most efficient business solution.
The modern business environment relies on a safe and secure place to store important data. As business picks up, more data is produced and naturally, the demand for more space becomes apparent.
As well as reducing the risk of lost data, cloud computing solutions are an ideal option when extra data space is required, eliminating the need for a business or organisation to expand their data centre. In this situation, cloud computing can be a much more cost effective means to improvement.
Ireland’s Technology Blog is an Irish technology blog…
Written by
Ireland's Technology Blog - EVERYTHING TECHNOLOGY, MOBILE, TV, INTERNET AND MORE - Technology consultants for Tipp Fm Radio
Ireland’s Technology Blog is an Irish technology blog covering all aspects of technology no matter how small or large. We feature everything from home technology, personal technology, gaming, smartphones and all the latest breaking news from around the world.
Written by
Ireland's Technology Blog - EVERYTHING TECHNOLOGY, MOBILE, TV, INTERNET AND MORE - Technology consultants for Tipp Fm Radio
Ireland’s Technology Blog is an Irish technology blog covering all aspects of technology no matter how small or large. We feature everything from home technology, personal technology, gaming, smartphones and all the latest breaking news from around the world.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/using-alibaba-expressconnect-for-hybrid-cloud-computing-859a2f532972?source=search_post---------284,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Jan 22, 2018·5 min read
If you run workloads on the cloud, but need the privacy and security features of a VPN-protected network, Alibaba Cloud offers an interesting solution, called ExpressConnect. ExpressConnect allows you to set up a secure network framework.
ExpressConnect can be particularly useful in situations where you need to maintain a hybrid cloud, with some of your business-critical workloads hosted in the cloud, and others run locally. Such an architecture may require you to move sensitive data between the cloud and on-premises infrastructure. ExpressConnect provides a way to transfer such data securely.
In this article, I walk through the steps of using ExpressConnect to build a secure hybrid cloud connection framework using Alibaba Cloud.
To create an ExpressConnect, you first need to set up a Virtual Private Cloud (VPC). To do this, on the main dashboard, select VPC (Virtual Private Cloud):
Then select the zone where you want to create the VPC. In my case, I chose China East 1 (Hangzhou).
On the next screen, we will add the name of our VPC.
The next screen covers the configuration of the named switch. The CIDR is predefined. The Zone within the region that we selected is also finalized here, and the range of IP that this switch will have.
Here, our VPC is created.
Now, we can go on to create our ExpressConnect instance using the button on the left-hand side of the VPC configuration menu.
Select the above option. and you will drop into the ExpressConnect configuration page.
Let’s create a Route Interface by clicking the Create Route Interface button on the right side at the top of the screen, so that a route is created between one environment and another. It is necessary to create a VPC in each region that you want to interconnect — Otherwise, you will not advance to the next screen. The amount you will pay monthly or per use is directly related to the locations you wish to link to.
After selecting the locations and the interfaces that they will connect to, click Buy Now. The screens that follow are payment screens (so you do not need the print of them in this article). After confirming the payment, the route created in the Express Computing dashboard will appear as shown below.
Notice that on the right side, we have the option of Initiating, because when creating the connection, it does not start automatically. When we click on Initiate, the connection between the parties is established.
After configuration and execution, the route settings can be changed, even with other routes from the same account or with routes configured in another account, if required. This is important in situations when you have partner companies, when one company buys the other, or there is some kind of federation between companies.
Click Vrouter on the name of your route to view the interconnections.
All network interconnections are made through this screen. Those comfortable with network settings will find it easy to manipulate the info here. We can create new switches and new routers through this interface.
In the VPC overview, we can find the active configuration information.
In this VPC, I have an instance running with CentOS.
ExpressConnect reminds me of one of my first experiences with Microsoft training, back when Active Directory replication started. At the time, if we had domain controllers that were distant from each other, we could have serious problems with replication. A VPN solution like ExpressConnect eliminates these problems, thanks to the synchronization between domains and the information that it provides.
As I have shown in this article, ExpressConnect is straightforward to set up, providing an easy solution for building the secure connection framework required for a hybrid cloud architecture.
Brena Monteiro is a Fixate IO Contributor and a software engineer with experience in the analysis and development of systems. She is a free software enthusiast and an apprentice of new technologies.
Reference:
https://www.alibabacloud.com/blog/Using-Alibaba-ExpressConnect-for-Hybrid-Cloud-Computing_p259158?spm=a2c41.11172417.0.0
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@developernationworld/a-shift-from-running-code-on-laptop-desktop-computers-to-cloud-computing-solutions-edb39e5cd5a0?source=search_post---------285,"Sign in
There are currently no responses for this story.
Be the first to respond.
Developer Nation
Jun 10, 2020·5 min read
Author: Michael Carraz
In this article we’ll explore where ML developers run their app or project’s code, and how it differs based on how they are involved in machine learning/AI, what they’re using it for, as well as which algorithms and frameworks they’re using.
If you’re new to Data Science and Machine Learning you might also want to learn more about the best programming language for ML
The share of ML developers who write their app or project’s code locally on laptop or desktop computers, has dropped from 61% to 56% between the mid and end of 2019. Although the five percentage points drop is significant, the majority of developers continue to run their code locally. Unsurprisingly, amateurs are more likely to do so than professional ML developers (65% vs 51%).
By contrast, in the same period, we observe a slight increase in the share of developers who deploy their code on public clouds or mainframe computers. In this survey wave, we introduced multi cloud as a new possible answer to the question: “ Where does your app/project’s code run? “ in order to identify developers who are using multiple public clouds for a single project.
As it turns out, 19% of ML developers use multi cloud solutions (see this multi-cloud cheat sheet here) to deploy their code. It is likely that, by introducing this new option, we underestimate the real increase in public cloud usage for running code; some respondents may have selected multi cloud in place of public cloud. That said, it has become increasingly easy and inexpensive to spin up a number of instances and run ML models on rented cloud infrastructures. In fact, most of the leading cloud hosting solutions provide free Jupyter notebook environments that require no setup and run entirely in the cloud. Google Colab, for example, comes reinstalled with most of the machine learning libraries and acts as a perfect place where you can plug and play to build machine learning solutions where dependency and compute is not an issue.
While amateurs are less likely to leverage cloud computing infrastructures than professional developers, they are as likely as professionals to run their code on hardware other than CPU. As we’ll see in more depth later, over a third of machine learning enthusiasts who train deep learning models on large datasets use hardware architectures such as GPU and TPU to run their resource intensive code.
Developers who do ML/AI research are more likely to run code locally on their computers (60%) than other ML developers (54%); mostly because they tend to work with smaller datasets. On the other hand, developers in charge of deploying models built by members of their team or developers who build machine learning frameworks are more likely to run code on cloud hosting solutions.
Teachers of ML/AI or data science topics are also more likely than average to use cloud solutions, more specifically hybrid or multi clouds. It should be noted that a high share of developers teaching ML/AI are also involved in a different way in data science and ML/AI. For example, 41% consume 3rd party APIs and 37% train & deploy ML algorithms in their apps or projects. They are not necessarily using hybrid and multi cloud architectures as part of their teaching activity.
The type of ML frameworks or libraries which ML developers use is another indicator of running code on cloud computing architectures. Developers who are currently using big data frameworks such as Hadoop, and particularly Apache Spark, are more likely to use public and hybrid clouds. Spark developers also make heavier use of private clouds to deploy their code (40% vs 31% of other ML developers) and on-premise servers (36% vs 30%).
Deep learning developers are more likely to run their code on cloud instances or on-premise servers than developers using other machine learning frameworks/libraries such as the popular Scikit-learn python library.
There is, however, a clear distinction between developers using Keras and TensorFlow — the popular and most accessible deep learning libraries for python — compared to those using Torch, DeepLearning4j or Caffe. The former are less likely to run their code on anything other than their laptop or desktop computers, while the latter are significantly more likely to make use of hybrid and multi clouds, on-premise servers and mainframes. These differences stem mostly from developers’ experience in machine learning development; for example, only 19% of TensorFlow users have over 3 years of experience as compared to 25% and 35% of Torch and DeepLearning4j developers respectively. Torch is definitely best suited to ML developers who care about efficiency, thanks to an easy and fast scripting language, LuaJIT, and an underlying C/CUDA implementation.
Hardware architectures are used more heavily by ML developers working with speech recognition, network security, robot locomotion and bioengineering. Those developers are also more likely to use advanced algorithms such as Generative Adversarial Networks and work on large datasets, hence the need for additional computer power. Similarly, developers who are currently using C++ machine learning libraries make heavier use of hardware architectures other than CPU (38% vs 31% of other developers) and mainframes, presumably because they too care about performance.
Finally, there is a clear correlation between where ML developers’ code runs and which stage(s) of the machine learning/data science workflow they are involved in. ML developers involved in data ingestion are more likely to run their code on private clouds and on-premise servers, while those involved in model deployment make heavier use of public clouds to deploy their machine learning solutions. 31% of developers involved across all stages of the machine learning workflow — end to end — run code on self hosted solutions, as compared to 26% of developers who are not. They are also more likely to run their code on public and hybrid clouds.
By contrast, developers involved in data visualisation or data exploration tend to run their code in local environments (62% and 60% respectively), even more so than ML developers involved in other stages of the data science workflow (54%).
Developer Economics 18th edition reached 17,000+ respondents from 159 countries around the world. As such, the Developer Economics series continues to be the most global independent research on mobile, desktop, industrial IoT, consumer electronics, 3rd party ecosystems, cloud, web, game, AR/VR and machine learning developers and data scientists combined ever conducted. You can read the full free report here.
If you are a Machine Learning programmer or Data Scientist, voice your opinion in our current survey to shape the next State of the Developer nation report.
A global community of software creators who want to influence how software is built.
See all (429)

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
A global community of software creators who want to influence how software is built.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/due/6-ways-cloud-computing-technology-is-changing-cccdd1d25a9e?source=search_post---------286,"There are currently no responses for this story.
Be the first to respond.
And, it’s not just everyday technology users that have enjoyed the shift. The cloud has had a massive impact on the business world. It’s changed the way we share information in every possible sense. For example, in 2012 the New York Times declared that year the boom of MMOCs: massive open online courses. For the first time, organized education began to truly scale. It was all because of the cloud.
That shift happened a mere six years ago. Cloud computing has already become in everyday society. Yet, there is still so much more room to grow. The industry is changing faster than ever. Here’s how:
Even though the demand for education platforms online in 2012 was increasing exponentially, tech companies were still fairly new. According to The Next Web, “Internet users have grown by 82%, or almost 1.7 billion people, since January 2012. That translates to almost 1 million new users each day, or more than 10 new users every second.”
Just from a volume perspective, the more people using the Internet and the more companies that establish themselves as cloud-based will only lead to more demand for cloud computing. Imagine having to upload and download documents back and forth and then being introduced to Google Documents.
As a user, once you’ve experienced something that seamless you’re going to want and expect that same type of experience everywhere you go.
The beauty of cloud-based platforms is their short learning curve. Plus, they have a low barrier to entry in terms of cost and seamless integration into organizations.
According to Business Insider, Toyota’s top technology executive in North America, Zack Hicks, has stated Toyota’s move to all sorts of cloud technologies — and ditching other expensive enterprise tools. The article explains, “Hicks freed up more of his team from desktop-support drudgery when Toyota signed a massive deal with Microsoft Office 365, its cloud alternative to Exchange email and Microsoft Office desktop software. The deal took more than two years, but in the end, Toyota is ditching IBM’s Lotus Notes and putting its entire worldwide workforce of 200,000 employees on Office 365.”
And, more companies will follow suit in their own ways.
There is a new technology in town. If you haven’t heard, blockchain is very quickly disrupting every possible industry. This includes everything from finance to big pharma to the cloud.
Right now, the cloud industry is dominated by “the big three”: AWS, Microsoft Azure, and Google Cloud.Because these three massive companies own so much of the space, these centralized companies are the ones setting the prices.
What blockchain companies like Akash are doing is leveraging the blockchain to turn the cloud industry into an open marketplace. Price is determined by supply and demand for available computing power.
For example, on the Akash network, users can “rent” computing power from idle data centers. They only use the power they need for the amount of time they need it. In turn, data centers and large-scale companies with hardware they aren’t currently using can turn these assets into new revenue streams.
By creating decentralized ecosystems, blockchain technology will have a massive impact on the way the business world uses the cloud.
A 2016 survey by RightScale found that companies adopting cloud services do not worry about security nearly as much as they would if they were storing data on native servers. The demand for cloud services is only increasing. Providers are spending more resources to ensure data safety.
The security role blockchain technology will play is interesting. The RightScale survey states “26 percent of respondents identify cloud cost management as a significant challenge in 2016, a steady increase each year from 18 percent in 2013.”
One of the reasons costs are such an issue in the cloud industry is because companies struggle to optimize for unused workloads. As a result, many companies sit on unused resources. This is where blockchain companies can help.
While it’s certainly safer to store data in the cloud, there is room for improvement. This includes optimizing for the costs associated with that safety.
One of the biggest impacts cloud computing has had on the business world has been improving how teams work. This includes whether they’re sitting in the same office or on different sides of the planet.
When data is accessible 24 hours per day, 7 days per week and changes can be made live, suddenly workflows become seamless. It’s safe to say this trend is only going to continue. Imagine what the world is going to look like when the ease of Google Docs is combined with artificial intelligence and machine learning. For example, if you’re constantly making the same kinds of changes to documents in the cloud, the cloud may soon be able to learn from you and suggest those changes to other team members autonomously.
For a long time, companies have felt inclined to keep everything in-house. Many companies felt they should invest hundreds of thousands of dollars in building their own servers, CRM tools, databases and file-storing programs. Now, so many of those services are being offered on the cloud. And, they are usually free or very cheap to start. Therefore, it’s hard to justify such a substantial cost just to keep everything in one room.
In addition, cloud companies see the massive potential for enterprise services. They can replace many of these unnecessary in-house tools and platforms. As demand here increases so will their efforts to ensure cloud computing technology and are vastly superior to the ones built internally.
6 Ways Cloud Computing Technology Is Changing was originally published on Due on April 30, 2018 by Peter Daisyme.
We teach you everything you need to know to get paid…
We teach you everything you need to know to get paid quicker, close more deals and get the best deals on payments alive!
Written by
We make it easy to create professional looking invoices, capture expenses and effortlessly track your time with https://due.com
We teach you everything you need to know to get paid quicker, close more deals and get the best deals on payments alive!
"
https://medium.com/@fedakv/types-of-cloud-computing-all-you-need-to-know-it-outsourcing-company-it-svit-bdd297d2ffd3?source=search_post---------287,"Sign in
There are currently no responses for this story.
Be the first to respond.
Vladimir Fedak
Mar 30, 2020·10 min read
20 years ago we walked around with floppy disks. 15 years ago USB drives were thought the best way to transfer data between computers fast — but then Amazon launched AWS, its public cloud computing platform and the world has changed forever. Nowadays, everyone from freelancers to startups and global enterprises uses various cloud computing types to obtain, process and store all kinds of data and run their applications to generate revenues.
A forecast from IDC (International Data Corporation) promises worldwide spending on cloud services to reach $280 billion in 2021 and continue to grow at CAGR of 22%. However, while many companies have already adopted the cloud-first approach to their operations, many businesses are still unsure how to select the most suitable amidst different cloud computing types. This article will explore all you need to know about cloud computing types at a glance and will list business benefits to obtain from moving your IT operations to the cloud.
What is cloud computing, in simple words? It is an approach to hardware utilization when all the computing resources owned by any cloud service provider are virtualized, pooled together and made available to users on demand. Every customer can scale their cloud instances up and down with ease to meet their workload challenges while paying only for the resources consumed (PAYG billing model), thus making the cloud much more cost-efficient, as compared to renting dedicated servers. Multiple cloud computing benefits like on-demand availability, simple scalability, high security and cost-efficiency of all IT operations are the factors that contribute to the growing adoption of the cloud by businesses around the globe.
Let’s just outline the infrastructure of a large corporation. Let’s say they have a data storage server that has to store all their financial records. If it was a single server, the enterprise risks losing all the data when its hard disk goes out of order. Therefore, at least a single replica is needed. But in order to ensure all the data is safely copied to both servers, you need a load balancer — a virtual machine on a third server, or just a third server.
Thus said, you need to maintain 3 servers and replace their faulty components st some time. You also have to pay for humidity control, cooling and fire control equipment, as well as have a secondary power supply on standby. Besides, the servers are hardly ever working at 100% of their capacity, so you always overpay for the resources you actually never used. Therefore, by combining CAPEX for purchasing hardware, software and supplementary systems with OPEX of paying for the maintenance and paying wages to IT specialists, you can see why the IT department is the cost center of expenses or any enterprise that wants to run its operations on-prem.
What is even worse, all these servers must be configured manually and separately. This always leads to a mess that should just be left alone, as restructuring it is literally impossible.
Enterprises were trying to solve this challenge in several ways to meet their demands by sharing their physical computing resources.
Nowadays, large corporations and small startups alike use cloud computing to support their daily IT operations. It is useful for every industry — from small fintech startups to global manufacturing and retail giants, insurance and banking, real estate and telecom, software development and IT services.
As we briefly mentioned before, the main principle and killing feature of cloud computing is the fact that the resources provided to customers are elastic and can be scaled up and down independently in no time. With conventional dedicated servers, you have to buy or rent them in a remote data center in a pre-configured configuration.
First of all, if you don’t use all the server resources — you still pay in full. Secondly, if you ever need to increase the resources available, you need to buy another server — and double your expenses as hardware has quite limited horizontal scalability. Thirdly, but quite importantly, keeping an eye on your system parts and replacing the faulty components is your responsibility entirely, not to mention each adjustment is done manually and takes some time
Everything is different with the cloud. Your apps and data are still run on the same Linux (or Windows) server, but the way they are organized is totally changed. Computing resources of all the servers in all the racks in all the data centers are united into a common pool through virtualization. Every user pays for his/her instance — some quantity of computing resources assigned to them.
If the project requirements change and you need more resources — these can be added in a single click in your cloud computing dashboard — both vertically (adding more preconfigured instances) and horizontally (adding more RAM or CPU power or disk storage space independently). Shutting down these resources also takes a single click only.
This process can be repeated as many times as you need with literally no effort wasted. Most importantly, new instances are provisioned automatically — and you never overpay for idle resources, not to mention there is no need to pay for additional equipment or server hardware maintenance. Cloud data centers purchase server hardware in droves, so they simply rep
You can name all kinds of cloud computing benefits, based on your business niche and modus operandi. Here is our top 5 pick:
There are three main types of cloud computing: providing Infrastructure-as-a-Service (IaaS), Platform-as-a-Service (PaaS) or Software-as-a-Service (SaaS). Let’s take a closer look at these different cloud computing types and explain what is the best use for each of them.
Your system engineers know better how to build a cloud environment to meet your project requirements. All they need for this are building blocks — and this is exactly what IaaS is all about. Cloud service providers like AWS, Google Cloud, Microsoft Azure, IBM Cloud, Rackspace and others give DevOps specialists and system engineers access to the core of their platforms, so they can configure storage, networking, servers, security, deploy Terraform and Kubernetes and configure everything the way they need it, within the limits of their cloud account. But tread with caution, as errors can be quite costly with IaaS.
Back in the days, when AWS was a novelty that was actively growing and developing new features, one enterprise moved there from legacy infrastructure and enjoyed the scalability of the cloud. However, there was a minor bug in their operations — a data processing job was queued for execution, but never reporting successful completion. They thought it was caused by concurrency settings, so they just removed the limit on the number of jobs allowed to run at the same time.
In fact, the job was running, but failing due to incorrect process configuration. Once the limit on the concurrent jobs was removed, the system started spanning EMR clusters to perform the job and did it till it exhausted all the resources of us-east-1 Availability Zone. This resulted in an invoice for several million dollars awaiting the company in the morning. This case served as a basis for developing an internal AWS cloud monitoring solution, which later became AWS CloudWatch.
As you can see, IaaS is the most powerful — and the most error-prone of the cloud computing types.
PaaS is the next layer of the cloud computing pyramid. It means the cloud vendor provides various platforms as a service, which is very useful for developers. Due to PaaS, developers can use AWS CodeDeploy, CodePipeline and other tools, as well as Google App Engine or IBM Foundry to quickly develop their apps without ever having to configure the server.
Other PaaS tools include various cloud software, operating systems, middleware and all the environments required to support the software delivery lifecycle. These PaaS offers are accessed via the cloud vendor’s dashboard, are often activated and deactivated in a single click and allow hosting the required databases and data sets, BI tools and development solutions required on various stages of software development.
SaaS is the uppermost level of the cloud pyramid. It includes IaaS and PaaS under the hood, but the customers cannot change anything apart from the settings of the software they consume. This is the level of applications like Office 360, Adobe products, Google G Suite, Salesforce and other cloud-native applications for businesses and individuals. You simply pay for receiving the functionality you need, but have no control over its installation, configuration or updates — all of this is done by the service provider.
There are 3 different cloud computing services types, distinguished by the level of customer control over the cloud resources he uses. These types are public, private and hybrid cloud, as well as multi-cloud strategy — and all of them have their benefits and shortcomings.
Public cloud means you share your resources with all other customers in that Availability Zone, just like using a public restroom. It is a cheap, efficient and reliable way to host your customer-facing applications. You get the zero CAPEX, on-demand scalability, high-availability and security of the cloud — but you do not have any SLA coverage. In addition, you have no control over where your sensitive data is processed — and there are stringent regulations that limit the businesses to storing the PII of the US citizens on US soil, for example. Thus said, private cloud cand be the solution in this case.
The private cloud is a sector of public cloud dedicated to serving a single customer. This way you can ensure your sensitive data physically resides within the US boundaries if your regulatory legislation demands it. Thus said, you retain full control over the configuration of software and networking resources, while the provider is still responsible for hardware maintenance and utility bills. Alternatively, private cloud solutions can be built in on-prem data centers, which is the preferred approach of global banking and financial institutions, as well as scientific and governmental institutions. In that case, you need to use tools like OpenShift and OpenStack to build and manage your private cloud system.
Hybrid cloud is a combination of public and private cloud, where an organization uses the unlimited resources of public cloud to process intense workloads while keeping all mission-critical data and systems running in the security of their private cloud. This is quite a popular approach for global corporations, who ensure both cost-efficiency and security of their IT operations with hybrid clouds.
Nobody said you cannot use Amazon web services in tandem with Google Cloud platform — and so many companies do it. They build complex modular systems that include components from multiple cloud vendors and can sometimes even replace such modules with relative ease. This is a popular approach for global corporations, who have to run their workloads in regions not covered by any single cloud platform, so they have to combine services from several vendors.
Before selecting a cloud platform to deploy to, it is prudent to assess the range of its services, the data center locations, the pricing model, the customer support plans and data recovery scenarios available — in other words, it is good to have an in-depth knowledge of different cloud computing services and products to select the best fit for your project requirements.
IT Svit has such expertise due to 5+ years of providing managed IT services for various cloud computing types and projects of varying scale. We are known as the leader of the IT outsourcing market in Ukraine and the leading Managed DevOps Services Provider worldwide. We would be glad to provide an end-to-end solution for your projects and help you reach your business objectives. Order your free consultation today!
Originally published at https://itsvit.com on March 29, 2020.
DevOps & Big Data lover
DevOps & Big Data lover
"
https://medium.com/@RedPixie/advantages-of-cloud-computing-the-ultimate-guide-359234bd73af?source=search_post---------288,"Sign in
There are currently no responses for this story.
Be the first to respond.
RedPixie
May 30, 2017·11 min read
While there is a great rhetoric surrounding the cloud, we feel that there needs to be a more conclusive discussion around the advantages of cloud computing.
This should enable both technology and business outcomes to come together and create beneficial change.
As such, this article will bring collate the various cloud advantages, with supporting evidence and examples that you can apply to your business.
As can been seen from the various press around big data, data science and machine learning — the cloud is becoming a focal point for digital transformation.
It provides a necessary platform to which organisations can sprint into value-lead projects.
If you stand back, and look at some of the key trends enterprises have been prioritising in recent years, cloud is very much leading the way.
This does not infer that you need to go full-out cloud — hybrid solutions are becoming the preferred route to various business, whereby they can put ‘quick-win’ workloads into the cloud.
So, with this dynamic change, enterprise organisations have come a long way in just a few years. We are seeing widespread adoption by companies who are appreciative of the scope that the cloud provides in boosting corporate efficiency, productivity and revenue.
2017 will prove to be the year of a true acceleration of the development of the next-generation cloud.
While this article goes into great depth around the general and industry specific advantages of cloud computing, you may just be looking for a high-level overview.
We have got you covered.
See below a conclusive infographic that evaluates the ease with which a cloud computing plan can be adapted to meet your organisation’s specific requirements at any one time, or the enhanced ability that it gives you to budget in accordance with those needs, for the short, medium and long term.
If you feel that this would be valuable to your community, hit one of the share buttons on the left ↖️
Having demonstrated a birds-eye-view perspective on the subject above, here are the 10 key advantages explained in detail.
They each cover:
However, if there is anything else you think would be helpful, we’d love for you to comment below.
Firstly, we start with the matter of flexibility and agility. The means in which a company can expand instantly through cloud-based services.
In addition to expanding operations and reach, organisations are equally empowered to take existing servers, such as the use of excel as a business-critical application, and expand its capabilities to no end.
This form of utility is especially useful to businesses that have established key revenue opportunities, but lack the infrastructure to exploit them.
You may frequently hear people tout this as one of the most practical advantages of cloud computing — it is easy to understand why.
Questions to consider:
Supporting evidence:
Flexibility is so crucial that 65% of respondents to an InformationWeek survey said “the ability to quickly meet business demands” was an important reason to move to cloud computing.
Further reading:
Well… maybe not reduce spending. But better prioritising spending.
What does that mean?
It means that organisations can ensure that spending can actually be quantified from a financial risk perspective. This can be better understand than previously with a comprehensive TCO — see the great article below.
With this mentality you can reduce the waves of IT teams that are spending 70% of their time of fixing issues around updating infrastructure. By doing so, you afford them the freedom to explore matters like self-service IT and other avenues where they have not yet had the resources to explore.
Questions to consider:
Supporting evidence:
Here is a really great chart that demonstrates the different outlooks on IT spending over the next few years.
Further reading:
Disaster recovery seems to be one of the most overlooked advantages of cloud computing.
Why?
Maybe because these events happen less frequently than the very popular phising attempts. Nevertheless, when you elect to trust in the cloud, your approach to considering all elements of your disaster recovery plan change.
Cloud computing providers take care of most issues, and they do it faster.
Make no mistake, there will always be human related incidents, such as the AWS coding error, but global backups mitigate against the risk of cloud computing.
Questions to consider:
Supporting evidence:
75% of all downtime is reported to be due to a power outage. Hardware and human errors round up the top three. Source
Further reading:
With the free and simple distribution of updates, the approach to IT solutions [including enterprise] is very much grounded with an iterative approach.
Hence the current ‘evergreen’ Windows 10.
This means that your everyday SaaS solution to business critical applications and more are continually eager to apply fixes, features and general improvements.
However, in an age where server maintenance was a significant portion of IT’s time, this revelation places the responsibility on the cloud computing suppliers.
As such, time is freed up to focus on innovation and enablement. Which in-turn means internal teams can rapidly deploy their work company-wide, and beyond the office.
Questions to consider:
Supporting evidence:
In 2010, UK companies spent 18 working days per month managing on-site security alone.
Further reading:
Whether the argument is AWS vs Azure — all cloud providers are very much focused on providing a pay as you go service.
More than just a monthly overview, they are providing contextual minute-by-minute pricing structures.
This means that there’s no need for capital expenditure at all.
This also has the benefit whereby business have minimal project start-up costs and predictable ongoing operating expenses. We have seen this factor very beneficial in organisations where those in modelling and data-lead teams are able to quickly spin off a server to see what their work would look like in scale.
The matter of CapEx vs OpEx is frequently referred to, however, this is the type of practical, business advantage to cloud computing.
Questions to consider:
Supporting evidence:
We often find enterprises paying in the range of 40 percent for over-capacity. Source
Further reading:
The age of productivity is among us, and core to that belief is the ability to effectively collaborate and communicate with staff.
“Yes, I know this is important”
But the phenomenon is far beyond people working in the same office, this is about satisfying a possibly global workforce to create dynamic teams. Teams that are informed and avoiding unnecessary duplication.
The cloud helps to dissolve a great deal of these issues.
What does that mean?
From practical improvements, such as the ability to sync up and work on documents and shared apps simultaneously, to following colleagues and records to receive critical updates in real time.
This effortless, controlled access is made ever possible for IT to enable with the cloud.
Questions to consider:
Supporting evidence:
A survey by Frost & Sullivan found that companies which invested in collaboration technology had a 400% return on investment.
Further reading:
As long as employees have internet access, they can work from anywhere.
While this rhetoric has been spoken for a while, there has been a recent change in dynamic that actually makes the possible for employees of all businesses.
Security.
The recent developments in user specific rights management has meant that the often vulnerable personal Dropbox and personal cloud providers can be controlled through specific policies of IT departments.
Such developments have made the ever-knowing business case of a distributed workforce possible. It no longer creates vulnerability, and as such, the most confidential of data can be accessed globally, and from many devices while maintaining its integrity.
In addition, this flexibility positively affects knowledge workers’ work-life balance and productivity.
Questions to consider:
Supporting evidence:
One study found that 42% of working adults would give up some of their salary if they could telecommute, and on average they would take a 6% pay-cut.
Further reading:
In conjunction with the point 6 & 7, document control is a cloud computing advantage that impacts the the daily interaction of users.
This benefit specifically takes away the need to continually send iterations back and forth, increasing the possibility of overlapping.
It equally avoids the instability that would otherwise slow down scaling organisations. Specifically when their is a reliance on accessing the right copy of sensitive information.
Questions to consider:
Supporting evidence:
73% of knowledge workers collaborate with people in different time zones and regions at least monthly. Source
Further reading:
‘Security is a process. Not a product.’ — Bruce Schneier
The matter of data security is a difficult subject because there tends to be a see-saw approach whereby you are either secure or you aren’t which in reality is not the case.
As Bruce Schneier notes, it is a process. One that has to be continually reviewed and never taken from granted. The way in which the cloud computing market has had to adapt has meant that cloud providers are ever under the pressure to provide far great security solutions that those on private servers, in order to break down the mental barrier.
And — it has.
Cloud providers have made sure that products like Microsoft Azure have really become [almost] impenetrable business tools. In addition, the level of capital investment that goes into their process, means that no private solution is as compelling from this perspective.
However, there is no need to reconsider everything, one of the most compelling advantages of cloud computing, specifically larger organisations, is that a hybrid approach can enable them to move select data, and once comfortable, build a roadmap.
Questions to consider:
Supporting evidence:
Cybercrime damage costs to hit $6 trillion annually by 2021. Source
Further reading:
10. Saving the environment with 30% less consumption
Lastly, you have the environmental impact of the cloud.
While this may not be the most economically driven cloud computing advantage, it is definitely a beneficial one for organisations that are looking to manage their public presence.
Through using a cloud provider, the are incredibly conscious of their impact, and give you the frame of mind to continue.
In addition, businesses using cloud computing only use the server space they need, which decreases their carbon footprint.
Questions to consider:
Supporting evidence:
Using the cloud results in at least 30% less energy consumption and carbon emissions than using on-site servers.
Further reading:
So there you go, here are 10, very compelling cloud computing advantages.
So what?
Good question. The importance of these 10 factors is not to use every single one of them and put all of your data in the cloud, quite the contrary.
The methodology is that you can use the list to consider various elements of your strategy and understand if selective use of a hybrid model is beneficial to your organisation.
While it would be easy to see that ‘all in cloud’ is the way to go, in reality, the means of the business, its industry, its customers, its regulations are very important to consider.
Nevertheless, we hope that in discussing these with your colleagues, and possibly different teams, you can collectively create a more suitable structure.
So in one handy list…
Let us know which of these advantages of cloud computing was the most interesting below. Should you wish to learn more, download this hybrid cloud jumpstart guide ⇓
Originally published at redpixie.com on May 30, 2017.
RedPixie go beyond technology. Building and managing Azure hybrid cloud solutions for the financial sector.
RedPixie go beyond technology. Building and managing Azure hybrid cloud solutions for the financial sector.
"
https://medium.com/@alibaba-cloud/how-to-connect-the-consumer-experience-with-cloud-computing-81d5b5345e81?source=search_post---------290,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Feb 4, 2020·3 min read
Remember when a customer would just walk into a store and buy something? Well, times have changed. Today’s consumers shop across multiple devices and multiple channels.
As a result, retailers need to provide an omnichannel shopping experience where consumers can research, buy and interact with goods across the boundaries of the physical and online worlds. It’s a vital service in today’s highly competitive world of commerce with those businesses that adopt omnichannel strategies achieving 91 percent greater customer retention rates year-on-year compared to those that don’t.
But connecting your consumer experience is a complex undertaking as customers can switch touchpoints multiple times and for single transactions. For example, a consumer may see a product on a social media channel and then go to your physical store to make the purchase. Alternatively, they may see a product in store, but it isn’t available in their size or style. So, they go online to make the purchase. A consistent experience offering relevant content needs to be readily available for every customer.
Our Content Management System (CMS) enables these cross-channel experiences, empowering retailers to manage content across different channels. Depending on the number of users and amount of content, our CMS backend system can be deployed on a single or multiple ECS instance and our ApsaraDB for POLARDB is used as the default backend database.
Thanks to both these technologies, a superior user experience is provided. Not only does CMS provide multi-channel bandwidth, but ApsaraDB for POLARDB can also auto-scale to up to millions of queries per second and is six times fasters than a standard MySQL database.
What’s more, if a retailer needs to distribute its content around the world, our Content Delivery Network (CDN) supports a millisecond response time and 120Tbps of bandwidth to reduce latency across your network, no matter where your customers are. Coupled with our China Gateway solution, there’s no corner of the globe you can’t reach potential buyers by providing an omnichannel shopping experience with Alibaba Cloud.
Retailers need to empower consumers to not only make omnichannel purchases but also enable cross-channel interactions with their brand.
For example, customers are increasingly creating content for retailers, commonly in the form of online reviews and demos. Such user-generated content helps retailers deliver a positive online presence and build trust in their brand. Using our Object Storage Service (OSS), these media-rich files can be stored on OSS as this content is edited, helping retailers reduce storage costs.
The digital domain can also be brought into physical stores where, for example, assistants armed with tablets can help customers locate products in their desired size or style, arrange a home delivery or just make a purchase without queuing up at the tills.
Smart stores are also increasing in popularity, where consumers interact with the store via their smartphones or in-store electronic screens to research goods, make purchases and access exclusive promotions and information.
Creating an omnichannel shopping experience can seem like a daunting prospect for traditional retailers because their business models are not well adapted to this way of working. This is where Alibaba Cloud can help.
If you would like to find out more about connecting your consumer experience, you can talk to our experts for a custom consultation.
To explore how digital transformation is affecting the retail industry, download the whitepaper Smart Shopping: Digital Transformation Strategies for Retail
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/@bpiatt/cloud-computing-forces-it-evolve-or-perish-2010-20933b9a002f?source=search_post---------291,"Sign in
There are currently no responses for this story.
Be the first to respond.
Bret Piatt
Feb 5, 2016·3 min read
[The full original text of my post from 2010 is below the “. . .” separator.]
While reading this today we’re well on the way to seeing it happen. A discussion started by a friend on Facebook the other day where he discussed how his hardware level skills are getting rusty because he’s now thinking at the AWS API tier and not the BIOS or CMOS layer of the IT stack anymore. His perspective was if you get excited about the hardware stuff these days then your opportunity for a career only exists at a handful of companies so you better be excellent and if not time to refresh your skills to the broader area of market demand. It’ll be interesting to check back in here in 2020 to see just how far along the evolution we’ve gone.
[Original post from 2010]
When I started this blog I thought I’d be talking about technology on a regular basis and so far I haven’t. This is still somewhat business related but it is also very tech heavy. The tech focused pieces I intend to explain at a level that an average “nerd” gets but the average adult can read.
Earlier today I spent an hour watching one of the Rackspace founders deliver a training video intended for new hires in 1999. In the video they go through the complexity of ensuring hardware works properly together, that the OS is installed properly, and that DNS is configured properly. Now just 10 years later much of this is significantly simplified. When is the last time you spent time dealing with an “IRQ conflict” or “checking jumper settings” (hardware related troubleshooting that is auto-magic today)?
Now as we move to cloud computing with pre-defined virtual machine images the “OS is installed properly” piece is going away. Projects like TurnKey Linux will lead to one-click application stacks on top of an OS. For much of the IT community their career has been performing these tasks. Now instead of an application developer needing a system administrator to “build the server” they go to a web based control panel, pick the system type they want and click “create” and the server is spawned.
It isn’t that the system administrator career is being completely eliminated; rather instead of every company needing their own system administrators in the future the computing providers will need them and general business will only need to have an IT staff that works on their specific business applications. Business won’t need to have many other “building block” level IT roles either: networking, desktop support, and storage/backup administrators.
Many in the IT industry think I’m taking things a bit far when we have this discussion. I don’t believe it’ll happen over-night but during the next 10-20 years it will. Looking back in the past nobody has a “typing pool” to type up hand written notes, a “courier” to deliver a message across town in a hurry, or a “research” department to go look up basic information we all have access to now through a search engine in a matter of seconds.
This is where the “evolve or perish” comes in. If you’re within 10 years of retirement and focused on the building blocks you may want to consider a job at an infrastructure company or risk the business you work for now eliminating your position in a transition to cloud computing. If you’re at the start of your career and focused on those building blocks you need to be the best and brightest in your field so you can obtain one of the service provider jobs in a much smaller market going forward. Your other option is to evolve and move further up the application stack. This could mean learning how to properly architect an application to make the most cost effective use of the utility priced OS clouds or it could mean going all the way up the stack to interface design.
This isn’t all doom and gloom. Evolution and automation like this increase productivity allowing us to focus on moving forward more rapidly. If you enjoy your IT industry job start asking your employer what you can learn above and beyond the building blocks to help out. While you may not need to today it is much better to be ahead of the game rather than waiting around for a layoff to start learning in panic mode.
>
Work: CEO of Jungle Disk / Interests: Cloud Computing, Computer Security / Hobbies: Golf, Food, Wine, Finance, Economics, Politics
Work: CEO of Jungle Disk / Interests: Cloud Computing, Computer Security / Hobbies: Golf, Food, Wine, Finance, Economics, Politics
"
https://medium.com/@expertincrm/what-is-aws-cloud-computing-101-rolustech-76d873faf408?source=search_post---------292,"Sign in
There are currently no responses for this story.
Be the first to respond.
Rolustech
Jan 22, 2021·4 min read
Now you most likely have heard the term cloud used to refer to anything stored on the internet. Like your pictures and other important documents of all formats. That is not entirely untrue, but this is just a small part of what the cloud can do. This is cloud storage, a small subset of cloud computing, and with it, there is much more made possible.
Cloud computing refers to demanding compute resources across the internet, this means no ownership of those resources and it will follow a pay-as-you-go model of service. You will not be maintaining servers and data centers, that becomes the cloud provider’s concern and not yours. There are three main types of cloud computing currently available, they are Infrastructure as a Service, Platform as a Service, and Software as a Service. AWS possesses all three of these services for businesses.
You could say Infrastructure as a Service houses the fundamental parts of cloud computing. With it, you can get access to virtual computers, storage space, and various networking features. As for your IT resources, you will obtain adaptive control and management. Out of the three, it is the most familiar to developers in terms of resource types.
Platform as a service takes away the need to manage hardware resources and operating systems. Your focus can now purely be on the deployment and management of applications. Other things that Amazon will do for you are procuring resources, planning capacity, regularly maintain software, and applying patches. Basically, a lot of the important steps in running your applications will be automated by the service.
When people refer to Software as a Service, they are generally referring to end-user applications like Gmail. The service provider, AWS, will provide you with the complete product as well as manage it. With this offering, you won’t be worrying about software maintenance or any of the backend processes. Your primary concern will be mastering the usage of the software in question, nothing more and nothing less.
As a cloud computing platform, AWS is the largest and most adopted in the market. Data centers and resources are located all over the world. AWS uses machine learning and artificial intelligence to make use of its cloud infrastructure and provide the best possible services.
The function of AWS is made possible with the wide range of tools that they have developed. They are many in number and cover functions like analytics, cloud application development, computation, management, databases, and much more.
AWS provides various options for storage on their cloud platform, the most used being AWS S3. This is a scalable storage option and is suited for every type of business, big or small. Store for all of your applications securely in S3 with no worries when retrieving.
AWS S3 Glacier is a low-cost version of standard S3, specifically for data that has long-term usage. Data that is used the least often is ideally stored here, so to access it you just have to inform AWS so they can retrieve it in advance.
DevOps is a type of management philosophy that is for increasing collaboration between IT operations and software development. By achieving this the goal is to increase efficiency in the development and deployment of all applications in-house. AWS provides just the tools to leverage this new practice like AWS CloudFormation, AWS OpsWorks, AWS CodePipeline, and AWS CodeBuild.
AWS makes sure that the data you have stored on their servers are monitored 24/7 to prevent breaches. This practice makes sure that they meet even the most stringent security requirements. This is why a number of banks are also present on AWS.
AWS Identity and Access Management (IAM) is used to set access permissions to users of your organization. You can use the AWS Management Console to see the status and number of permissions that you have currently set.
The benefits of Cloud Computing with AWS are numerous. Work is efficient and secure when you stick with the largest cloud platform there is. AWS provides a large toolset to tackle any type of application you wish. Not having to prepare your own infrastructure is a huge benefit because of the costs and time involved. Leaving it to AWS will allow you to focus on what matters, the usage of the software to enhance your business process.
Rolustech is an AWS certified firm and has completed several projects in AWS DevOps, AWS Cloud Application Development, Machine Learning & Artificial Intelligence, and more. Contact us now for a FREE Business Analysis. We will be glad to assist you!
Originally published at https://www.rolustech.com on January 22, 2021.
A SugarCRM, Salesforce, and Magento Certified Developer & Partner Firm. Tweets on #CRM #Sales #Marketing #Social + many more.
2 
2 
2 
A SugarCRM, Salesforce, and Magento Certified Developer & Partner Firm. Tweets on #CRM #Sales #Marketing #Social + many more.
"
https://medium.com/@aremorch/how-cloud-computing-will-revolutionize-modern-hotel-marketing-and-tap-into-new-uncontested-markets-b2b9db7d944b?source=search_post---------293,"Sign in
There are currently no responses for this story.
Be the first to respond.
Are Morch
Aug 5, 2018·6 min read
Cloud computing has had a massive impact on the business world. It’s changed the way we share information in every possible sense. Effective Social Media Marketing frameworks, new technology, and cloud computing will revolutionize modern Hotel marketing and tap into uncontested markets.
Today cloud computing services offer multidimensional values, including agility, scalability and cost benefits.
Cloud computing services are broken into three main services;
IaaS allows Hotels to rent the infrastructure itself; servers, data center space, and software. The biggest advantage of renting, as opposed to owning, infrastructure is that Hotels can scale up the amount of space needed at any time. During peak seasons, Hotel may require more server space to handle the heavy traffic than in the off-season. Using IaaS allows the Hotel to save money by only paying for what they with use within a certain time frame.
PaaS allows Hotels to create applications, collaborate on projects, and test application functionality without having to purchase or maintain infrastructure. Development platforms can be accessed as long as there is an internet connection, allowing Hotel team members to stay connected and keep working.
With SaaS, the new and existing software can be quickly deployed, since it is already installed on the cloud server. As with PaaS Hotels only need to access a computer with Internet access to use the software, and they never have to worry about upgrading or patching the software. SaaS can reduce costs since Hotels only pay for exactly what is needed and do not have to maintain the software.
Source: Rackspace
Today there are several great providers that are dedicated to providing Hotels with an optimal Cloud Computing Experience Package.
Writing this article became a dual research purpose for me. I am in the process of moving my service over to a cloud platform myself. So, my research became valuable to my business also.
An important part of my goals and mission is to strive for delivering the ultimate experience for my audience and clients. And it is important then to be in a position with tools and services that allows us to deliver above and beyond.
The selection of the most appropriate cloud provider can be quite challenging. Cloud providers’ offerings differ in many aspects and cloud adopters need to evaluate several factors, i.e., from compliance with regulatory requirements to certifications, from security options to customer performance support.
Before you can effectively select a suitable provider, you need to understand your Hotels specific business needs. This sounds pretty obvious, but clarifying your specific requirements and minimum expectations, in advance of assessing providers ensures you are comparing them all against your checklist, instead of comparing one against the other. This is the quickest way to move from a long list to shortlist.
The Ultimate Cloud Computing Experience Package should provide:
Hotels today will need easy access to data and reports that allow them to make an informed decision, and the management team can control the processes from anywhere.
Implementing a Cloud Computing Experience Package also comes with an important benefit that supports happiness and productive of your Hotel Team. Dedicated automation will free up some time-consuming tasks.
In my research, I looked for Cloud solutions dedicated towards the Hospitality Industry. And it was also important they meet the requirements for The Ultimate Cloud Computing Experience Package. In addition, that they were compliant with regulations and up to date on certification requirements.
My friends at mycloud Hospitality meets all of these requirements.
mycloud Hospitality will work closely with your Hotel and management team and switch the old system to mycloud in two phases to ensure that Hotel and management team don’t feel any pressure.
It starts with taking the Property Management System live first while the mycloud Hospitality team and Hotel management configured the Point of Sales, Social Media Listening, Social Media marketing and e-distribution aspects before setting the complete system live.
Hotels that have gone through this process changing from old obsolete system to a new evolving mycloud system experience a whole new shift.
The mycloud Hospitality pride themselves in being there for the Hotel during the whole transaction process. Any query or challenge your Hotel runs into will get a proper attention.
The mycloud system allows them to attract and improve revenues because of complete automation and integrated e-distributions. And the system is able to create accurate forecasts which result in the Hotel being able to manage guest expectations and deliver new unique guest experiences.
If you are unsure if Cloud Computing is the right solution for your Hotel, then give it a spin for Free.
Feel free to let my friends at mycloud Hospitality know I sent your Hotel there.
Cloud Computing is the future for Hotels. I highly recommend give it a try.
Enjoy!
When your Hotel serves your community they will reward you back with serving you.
From a Hotel and Social Media Consultant’s standpoint, the Social Media Community can offer Hotels a one-stop-shop for managing their Social Media marketing such as the community infrastructure, database, listening functionality, profile management, collaboration, content marketing and management, and of course, the analytics.
A Social Media Community will help your Hotel to grow smarter and faster when you focus on the value proposition and collaborate in effective ways.
If your Hotel need help to put it all together then make sure to the let us know.
Today’s Social Media Management tools provide advanced options that will help Hotels with these tasks. If you need a quality team the help your Hotel pulls this off, then get in touch with my partners at Founders Media.
With the right team behind your Social Media efforts, it can become cost-effective and generate a new revenue stream for your Hotel.
Social Media Marketing Mastery for Hotels is NOW Open! Click Here if you are ready to take your Hotel to a new level!
To get your Hotel started here are a few FREE gifts and resources from me and my partners Founders Media.
1. FREE access to my Hospitality Gone Social Vault (Click Here to Unlock)2. FREE ebook from Founders Media (Click Here to Unlock)3. FREE 21 Days To Social Media Mastery for Hotels email course (Click Here to Unlock)4. FREE access to #HotelPodcast — A Podcast about Social Media Mastery for Hotel (Click Here to Unlock)5. FREE access to our Facebook Group — Hospitality Gone Social (Click Here to Unlock)
Are Morch is the founder and owner of Are Morch — Hotel Blogger and Social Media Consultant. Get more from Are on Facebook | Twitter | Google Plus | LinkedIn | Pinterest | Instagram
Originally published at aremorch.com on August 5, 2018.
Digital Marketing Coach | ❤️All things Hotels 🏩 and Digital Marketing | Digital Marketing for Hotels http://bit.ly/HotelCoach
See all (1,391)
Digital Marketing Coach | ❤️All things Hotels 🏩 and Digital Marketing | Digital Marketing for Hotels http://bit.ly/HotelCoach
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@techgenyz/6-major-pros-and-cons-of-cloud-computing-you-should-know-b5ad092adf78?source=search_post---------294,"Sign in
There are currently no responses for this story.
Be the first to respond.
TechGenyz
Feb 24, 2018·1 min read
Cloud computing is a software which offers you to save your enormous data online and use it at any time from anywhere, any device. It has been used by many organizations to manage their huge data.
It is very easy to access and gives you an option to share your data with others. Some companies hire Website Development Companies to manage their official data but, the majority of companies use cloud computing to do that. Let’s explore the pros and cons of cloud computing.
TechGenyz is a leading source of latest technology news, updates on future tech stories, news on Virtual Reality, Augmented Reality, gaming, apps and more.
TechGenyz is a leading source of latest technology news, updates on future tech stories, news on Virtual Reality, Augmented Reality, gaming, apps and more.
"
https://medium.com/@alibaba-cloud/how-cloud-computing-has-revolutionized-o-m-ae3cdd155262?source=search_post---------295,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Jul 28, 2021·13 min read
Now is the best time to develop operation and maintenance (O&M) more professionally.
Lu Tang, who is in charge of elastic computing stability at Alibaba Cloud, mentioned that, “Ops was created only with experience without officially being considered as a specific field. Now, based on the previous experience, it provides standardized services to the outside world.”
He believes that this era helps make O&M truly professional. Accordingly, O&M personnel can reduce costs and improve efficiency for enterprises with their skills and experience. However, this can be a challenging time for many O&M professionals. Today, they need to know not only the business architecture and code but also the kernel and hardware, together with various O&M tools.
With the development of cloud computing, DevOps, and other technology trends, O&M personnel are facing more and more challenges. The problems they faced and the ways they worked before are also being redefined.
This article discusses the concept of O&M in the following three aspects:
Apart from a small number of IT O&M personnel in early large enterprises, China’s O&M industry has emerged in-line with the development of the internet industry in the 1990s. Therefore, the O&M of the internet industry takes the lead and guides the trends and directions in the O&M field.
After 20 years, the internet era has now entered a mature stage. Traditional enterprises are prioritizing digitization, and the challenges and environments that O&M personnel need to deal with have transformed a lot.
1.1) The increasing complexity of enterprise IT systems and greater O&M challenges require a higher degree of automation.
Enterprise IT systems are becoming more and more complex due to increasing digitalization and business growth. A large number of network devices, servers, middleware, and microservices of business systems make the job of IT O&M personnel challenging. Even if working overtime for maintenance, deployment, and management, the business will often face interruptions due to various failures, seriously impacting operations.
Meanwhile, as the market becomes more competitive, businesses should iterate faster to seize market opportunities, especially in the internet industry. The speed of product marketization or iterations has become integral to success. Supporting the rapid business iterations has become another challenge for O&M personnel. Obviously, manual O&M is barely sustainable, so the O&M industry in China turned to automation.
As the saying goes, a handy tool makes a handyman. With technological developments and O&M tools for automation — including event monitoring and early-warning, automated deployment, automated orchestration, and self-diagnosis — enterprises can improve the O&M efficiency.
1.2) After cloud computing evolved and leveraged extensively, many changes occurred in O&M objects, O&M tools, and even skills. Consequently, the rise of DevOps has attracted attention.
Generally, many enterprises divide O&M into two levels:
Infrastructure O&M: It mainly focuses on IT infrastructure management, including monitoring, notifications and alarms, maintenance, and deprovisioning of physical resources such as servers, switches, and networks.
Application O&M: It mainly focuses on business O&M, including the release and removal of some business applications, release deployment, scaling, and other functions.
For a business, efficiency improvements in application O&M can more directly accelerate the efficiency and growth of business iterations. While the infrastructure O&M serves as the foundation, enterprises building their own data centers focus their O&M primarily on infrastructure.
Cloud computing is characterized by “software or services define everything.” Cloud vendors undertake the maintenance and virtualization of the underlying infrastructure. After migrating to the cloud, the main objective of enterprise O&M has shifted from hardware — such as servers — to service API-oriented O&M, including host O&M and application O&M. Accordingly, automated deployment pipelines and continuous delivery DevOps are gaining more and more attention.
Technological development continuously tries to shield the underlying infrastructure and drive the developers’ attention away from the underlying resources, as is the case in serverless, function computing, and other trending topics.
In the earlier stages, there might be several O&M personnel in an enterprise jointly responsible for the maintenance of specific applications from “the bottom to the top.” However, even with more O&M personnel recruited due to business expansion, such a strategy of personnel working together for O&M is unsustainable. In fact, in many large enterprises, much O&M work has begun the preliminary “platformization,” namely to centralize the management of the underlying resources to reduce management costs. Such “platformization” has also promoted services and standards for public components within some enterprises. However, this method is not as efficient as the scale effect of cloud vendors.
The external form of platformization is cloudification. Observing from the inside of enterprises, cloudification has become an irreversible trend. To quote from an article, one of the important features of cloud computing is that it is “out-of-the-box,” with the cloud providers offering centralized O&M management and delivering to end-users as a service. This frees cloud users from much of the tedious daily O&M work and allows them to pay attention to their business development, thereby improving the entire industry’s operational efficiency.
1.3) The Rise of artificial intelligence and big data
In recent years, common O&M concepts not only included DevOps but also DataOps and AIOps, reflecting the need for intelligent and data-driven O&M.
Intelligence is the higher pursuit of automation to save time for the O&M personnel further. Artificial intelligence (AI) is now widespread in all fields that can be automated, and the O&M field is no exception, which must be one of the essential directions for development. However, intelligent O&M is a bit far away as most enterprises have not yet fully automated or even initially codified at scale.
Changes to the environment have brought about three trends of automation and standardization, development and operations (DevOps), and artificial intelligence for IT operations (AIOps). Relevant concepts, even a complete transformation, are needed for the enterprise O&M system. Alibaba Cloud ECS team believes that to build a future-oriented O&M system, besides focusing on the above new trends, it is necessary to pay attention to the changes in the working boundary and implementation paths of enterprise O&M in the cloud era.
Among many trends in this era, the large-scale popularization of cloud computing undoubtedly had the most significant impact on O&M. The migration of business applications to the cloud led to fewer underlying O&M tasks and a large number of discussions about the O&M personnel crisis.
The ultimate goal of O&M personnel is to help the business realize business value by efficiently coordinating IT resources.
The four aspects most concerned by O&M are efficiency improvement, stability, security, and cost optimization.
Nowadays, these four aspects are still what the O&M personnel are pursuing. However, in the cloud computing era, the working boundaries, implementation methods, and paths have undergone tremendous changes.
2.1) Continuous efficiency improvement: Single-point automation to standardization
Originally, it was common to improve efficiency by writing shell scripts or with the help of open source tools. However, this approach is often single point, separated, and non-standardized. Sometimes, two engineers use different scripts and tools. Additionally, due to different O&M organizational structures and divisions of work in an enterprise, redundant capacity construction or an information isolated island may occur, causing lower O&M efficiency.
Therefore, we can say that O&M in the past was created based on experience and was not systematic enough. But experience often relies on personal accumulation.
DevOps, GitOps, and Infrastructure as Code (IaC) programmable infrastructures are emerging today to change this single-point, non-systematic “automation.” Cloud computing provides various out-of-the-box tools that drive DevOps forward, on top of shielding the underlying hardware. This changes the keyword for O&M efficiency improvement into coding and standardization. In combination with the characteristics of their own enterprises, O&M personnel provide the platform-based experience after abstraction and productization to R&D engineers.
2.2) Stability and reliability with less attention to the underlying layer and more attention to applications and services
Stability was the cornerstone for O&M. Traditional O&M deals with physical machines and network devices. In addition, enterprises must build a disaster recovery, monitoring, and alerting system to ensure stable service operations.
Today, cloud computing platform-based technologies such as large-scale geo-disaster recovery and hot migration have achieved relatively high service levels. The O&M personnel of the enterprises may only need to use a few simple APIs or clicks based on the suggestions of cloud vendors to avoid the impact of infrastructure on business. Now, they can do what they need to do simply with clicks.
However, since service stability equals infrastructure stability plus code stability, the O&M teams spend more time focusing on the stability of applications and services. At last year’s Global O&M Conference, the concepts “technical operations” and “BizOps” began to emerge, both of which are new value directions for O&M.
The era of O&M focusing on machines is over. Technical operations require that O&M personnel participate more in business and improve the user experience. For example, various issues should be considered, such as whether the cluster should be scaled out during the promotions, whether the bandwidth is sufficient, and examine the stress testing data. BizOps advocates the feedback and interaction between the application O&M engineers — who know the system health best — and the demand side business personnel. The idea is that “O&M makes good systems.”
2.3) Safety: Self-responsibility to shared responsibility
There are many security dimensions from vulnerability prevention and network attack prevention to code checking, permission management, and log auditing commonly used by enterprises — and all the way to trusted computing of higher levels, full-link encryption, and so on.
In large enterprises, specially set up security teams may have to perform these. For example, for log auditing, a security team needs to collect each log record, analyze and match it one by one, and iterate it with the iteration of the business code. This high complexity also makes many small businesses give up or use expensive third-party solutions.
Cloud services directly provide security with multi-level and comprehensive processes and support fine-grained permission management. For example, all operations on the cloud are recorded and can be audited and traced afterward, which undoubtedly costs enterprises significant. Alibaba Cloud’s VPC network provides convenient network isolation and traffic control for enterprises. The latest cloud servers of Alibaba Cloud Elastic Compute Service (ECS) are all equipped with security chips to enable the trusted startup of servers without tampering. On this basis, the encrypted computing isolation ring enclave is used to ensure that data is available but invisible. This also meets the financial-level security requirements.
When the internet data center (IDC) was popular, enterprises were solely responsible for IT security. In recent years, the shared cloud security model has been accepted universally in the industry. Cloud vendors are accountable for the security of the cloud infrastructure, while users are responsible for the security of business applications above the virtualized layer. Users can select appropriate products in the cloud security market to secure their content, platform, application, system, and network. At the same time, users must implement permission control well to avoid problems such as deleting the libraries.
2.4) Cost optimization: Fixed cost to FinOps
Technically, the “software defines everything” feature of cloud computing has changed the working methods of O&M and developers. Its “elastic” feature also provides a “cost optimization method” for enterprises to minimize idle resources.
In terms of business model, the “leasing” model of cloud computing is different from the traditional IT hardware procurement. The financial needs of enterprises should transform from capital expenditure (CAPEX) to operational expenditure (OPEX). Cloud computing has a wide range of cost calculation modes, further helping enterprises achieve the best balance between IT flexibility and low cost. Therefore, for O&M personnel, operations on the cloud mean a shift in thinking about cost optimization.
As enterprises migrate more of their core business applications from data centers to the cloud, there is an increasing need for budgeting, cost accounting, and optimization in cloud-based environments. It is a significant conceptual and technological change from a fixed cost model to a variable and pay-as-you-go model of cloud billing. However, most enterprises do not yet have a clear understanding of cloud financial management and technical means. The State of FinOps Report 2020 indicated that nearly half of the respondents (49%) have little or no automated methods to manage cloud spending.
FinOps is introduced to help enterprises better understand cloud costs and IT benefits. It is a way of cloud financial management and a change in the enterprise IT operating model. Its goal is to improve the organization’s understanding of cloud costs to enable better decision-making. In August 2020, the Linux Foundation announced the establishment of the FinOps Foundation to advance the cloud financial management discipline through best practices, education, and standards.
A practitioner in the FinOps community shared a practice case from the banking industry. The story is that the monthly cost was reduced by 60% compared with the local deployment cost by transforming an application’s architecture to serverless. He pointed out that the results in reducing the cloud costs have been mixed, which is affected by the level of cloud cost optimization in enterprises. He divided cloud technologies into three stages of crawling, walking, and running. After using cloud-based cost optimization methods, enterprises can achieve significant cost optimization results.
Cloud vendors are currently improving their support for FinOps, helping enterprise financial processes better adapt to the variability and dynamic nature of cloud resources. For example, AWS cost explorer and Alibaba Cloud cost centers can help enterprises better analyze and allocate costs. Moreover, enterprises also need to reduce costs using agile auto-scaling, service selection, infrastructure as a service (IaaS), and flexible cost calculation modes to get the most out of the cloud.
To recap, cloud vendors complete the monitoring and scheduling of hardware devices and hardware in the cloud. The focus of enterprise O&M has changed to the design and construction of the internal O&M system. In other words, it is necessary to abstract and productize the experience in deep combination with the enterprise characteristics to form a set of O&M systems that fit the enterprise itself.
Based on the new trends in O&M automation, DevOps, AIOps, and DataOps, as well as the changes in O&M boundary in the cloud era, a good O&M system should have the following four characteristics.
3.1) Automation and standardization: Using concepts such as DevOps and Infrastructure as Code (IaC)
The foundation of DevOps is not just IaC, but everything as code. Only with the code standardization can be achieved, and O&M platforms and developers can communicate smoothly through standard APIs. Coding is also the basis for the ultimate goal of “AIOps” or “NoOps.”
The ECS automated O&M kit released by Alibaba Cloud ECS embodies the concept and design of IaC. Resource Orchestration Service (ROS) and Operation Orchestration Service (OOS) allow users to implement automated deployment and batch O&M operations through templates, together with drag-and-drop operations for convenience. Gartner, a research organization, recognized “automated cloud orchestration and optimization” in 2021 as top 10 cloud computing trends. Alibaba Cloud’s ROS and OOS, AWS’s CloudFormation, and Terraform are similar automated orchestration tools.
The Alibaba Cloud ECS automated O&M kit provides complete and comprehensive monitoring of underlying resources and is available to users in the form of events. Users can subscribe to cloud resources through OpenAPI or cloud monitor to facilitate the building of an event-driven automated O&M system. This is also a foundation to construct an automated O&M system.
3.2) Specific permission management and rapid-integrated security capabilities
With operation tracing and auditing, permission management can effectively control enterprise security risks, prevent database deletion and other events, and enable investigation and review afterward.
The cloud assistant of the Alibaba Cloud ECS automated O&M kit records all the operations in ECS. Orchestration tools such as ROS and OOS also support permission management. As mentioned above, Alibaba Cloud has comprehensive security capabilities. In fact, enterprises build their O&M systems in Alibaba Cloud with automated tools and leverage the underlying intelligent O&M capabilities of Alibaba Cloud. Those are the complete O&M systems that enterprises enjoy on Alibaba Cloud.
3.3) Comprehensive coverage, including automated performance management and cloud finance management tools, which can assist cloud cost optimization.
In the early days, O&M usually focused on single-point automation. However, the O&M system should focus on automating the overall process, covering the entire lifecycle of resources and business applications.
The Alibaba Cloud ECS automated O&M kit provides the entire lifecycle management for cloud servers, including cloud migration, deployment, routine O&M, and elastic scaling. Elastic Scaling Service (ESS) and Auto Provisioning Group (APG) perform resource scaling based on different scenarios. Resource optimization advisors can identify resources with low usage, and users can adjust these resources to improve resource utilization and reduce costs.
3.4) Intelligence and digitalization
Full implementation of AIOps is still ideal for most enterprises. Still, an O&M system at least has the basis for upgrading to intelligence, that is, coding standardization or with part of intelligent functions. In the Alibaba Cloud ECS automated O&M kit, intelligence is mainly used in the cloud keeper. Cloud keeper refers to a series of intelligent functions of Alibaba Cloud ECS that users can hardly perceive, covering automatic fault diagnosis and repair, automatic monitoring, and analysis and optimization of resources. In addition, intelligent O&M capabilities such as hot migration at the underlying layer of Alibaba Cloud ECS are also included.
DevOps and cloudification are two main trends from IDC-host to the cloud-host era and the build-on cloud era. Neither personnel of O&M and R&D, nor enterprises and cloud vendors, can reverse it.
It is better to embrace new technological trends and make the trends into technological dividends and competitiveness instead of lamenting the rapid pace of the times. Practitioners are actively gaining relevant knowledge, while Alibaba Cloud, as a cloud vendor, also hopes to implement DevOps in China and help Chinese enterprises improve their digitization and automation capabilities.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/@coralineltd/%E0%B8%84%E0%B8%B3%E0%B8%96%E0%B8%B2%E0%B8%A1%E0%B8%97%E0%B8%B5%E0%B9%88%E0%B8%9E%E0%B8%9A%E0%B8%9A%E0%B9%88%E0%B8%AD%E0%B8%A2-%E0%B9%80%E0%B8%81%E0%B8%B5%E0%B9%88%E0%B8%A2%E0%B8%A7%E0%B8%81%E0%B8%B1%E0%B8%9A-cloud-computing-12aafd39d260?source=search_post---------296,"Sign in
There are currently no responses for this story.
Be the first to respond.
CORALINE CO. LTD
May 18, 2021·1 min read
จากประสบการณ์การพัฒนาโครงการทั้งระดับเล็กและใหญ่ที่มีความเกี่ยวข้องกับ Data วันนี้ Coraline ได้รวบรวมคำถามที่เกี่ยวข้องกับ Cloud Computing ไว้ 4 คำถาม ดังนี้
1. Cloud ปลอดภัยหรือไม่ ข้อมูลจะรั่วหรือไม่
ตอบ: ระบบความปลอดภัยของ Cloud ไม่ได้แตกต่างกับระบบความปลอดภัยแบบ On-premise เราสามารถ Setup ระบบทั้งหมดได้ตามที่เราต้องการ ยิ่งไปกว่านั้นเนื่องจากระบบ Cloud จะได้รับการดูแลโดยผู้เชี่ยวชาญของบริษัทเจ้าของ Cloud อย่างต่อเนื่องทำให้บริษัท Cloud มักจะ Update ระบบความปลอดภัยอยู่ตลอดเทียบกับระบบแบบ On-premise ที่เราต้องดูแลเองและอาจจะไม่สามารถจัดซื้อจัดจ้างระบบใหม่ๆได้รวดเร็วนัก
สำหรับเรื่องข้อมูลรั่วต้องบอกว่าโอกาสเกิดก็ยังมีอยู่บ้างแต่ส่วนใหญ่รั่วจากฝีมือของพนักงานที่เข้าไปใช้ Cloud มากกว่าการที่ระบบรั่วเอง ทั้งนี้ในกรณีที่มีข้อมูลรั่วจากระบบ Cloud ทางบริษัทเจ้าของ Cloud เขาก็จะต้องชดใช้ค่าเสียหายให้ ในทางกลับกันในกรณี On-premise การที่ข้อมูลรั่วหรือถูก Hack ระบบ ทางบริษัทจะไม่สามารถเรียกร้องค่าเสียหายใดๆได้
2. ใช้ Cloud ยี่ห้ออะไร
ตอบ: โจทย์แต่ละโจทย์จะเหมาะกับ Cloud แต่ละยี่ห้อไม่เหมือนกันเช่นการใช้ IoT อาจจะเลือกใช้ Google Cloud เพราะสามารถใช้ BigQuery ที่ทรงพลังได้ การใช้ Cloud เริ่มต้นสำหรับ Data Lake อาจจะเลือกเป็น AWS ได้เพื่อความยืดหยุ่นในการใช้งานและการนำข้อมูลเข้า S3 มีค่าใช้จ่ายน้อยเป็นต้น
การคิดราคาของ Cloud แต่ละยี่ห้อจะไม่เหมือนกัน เช่น Azure กับ Oracle จะประหยัดกว่าถ้าซื้อเป็นรายปีเป็นต้น
นอกจากนี้ Architecture และเครื่องมือที่ใช้ใน Cloud แต่ละยี่ห้อก็ไม่เหมือนกันทำให้ผู้เชี่ยวชาญด้าน Cloud แต่ละยี่ห้อจะต้องผ่านการอบรมและผ่านการทดสอบก่อนที่จะใช้งาน Cloud
3. Cloud ไทย กับ Cloud ต่างชาติ ต่างกันหรือไม่
ตอบ: ต่างกันในหลายประเด็นโดยมีประเด็นหลัก 2 เรื่อง
- เรื่องความเร็วในการเข้าถึง Cloud ของไทยจะเร็วกว่าเนื่องจากมี Server อยู่ในประเทศไทย
- เรื่องเครื่องมือต่างๆ ของไทยเท่าที่มีในตลาดจะขายเฉพาะในส่วนของ Storage เท่านั้น ทำให้ในการใช้งานจำเป็นต้องมีการลงเครื่องมือต่างๆแยกส่วนกัน ในขณะที่ Cloud ต่างประเทศจะมีเครื่องมือต่างๆที่พร้อมและทันสมัยกว่า
4. ต้องใช้ Cloud ยี่ห้อเดียวหรือไม่
ตอบ: ไม่จำเป็น สามารถใช้ Cloud ร่วมกับระบบ On-premise ก็ได้หรือจะใช้ Cloud มากกว่า 1 ยี่ห้อก็ได้ ขึ้นอยู่กับวัตถุประสงค์ในการใช้งานและงบประมาณในการลงทุน
ท่านใดกำลังสนใจพัฒนาโครงการบน Cloud หรือทำโครงการ Data Migration สามารถติดต่อทีมงาน Coraline ได้ทุกช่องทาง
Tel: 099–425–5398
Email: inquiry@coraline.co.th
FB Page: Data Driven Business by Coraline
#BigData #DataScience #Optimization #ProductivityImprovement #DigitalTransformation #MachineLearning #ArtificialIntelligence #DataManangement #DataGovernance
#Coraline ให้คำปรึกษาการทำ Big Data, Data Model, Artificial Intelligence และ Digital Transformation เพื่อเพิ่มศักยภาพของธุรกิจ
We seek to be the acknowledged leader in Data Science & Operations Research in searching for new solutions and bringing customer’s big data into real action.
See all (11)
We seek to be the acknowledged leader in Data Science & Operations Research in searching for new solutions and bringing customer’s big data into real action.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/365datascience/why-cloud-computing-is-critical-for-a-data-scientist-16f3d1de6b9d?source=search_post---------297,"There are currently no responses for this story.
Be the first to respond.
Cloud Computing and Data Science
Did you know that US retail giant Walmart generates 2.5 petabytes of data from approximately 1 million customers every hour?
And in case you’re wondering how much a petabyte is, as I did when I first read this, it is equal to 1 million gigabytes. The equivalent of 13.3 years of HD video.
Considering that Walmart locations are open for business for more than 10 hours a day, we get a staggering 130 years of HD video and 25 petabytes of data collected on a daily basis!
Yes, there aren’t many companies like Walmart.
But even smaller enterprises nowadays generate huge amounts of data, so, it becomes increasingly more challenging to take advantage of such information abundance.
And yes, data science is at the heart of all that. But before we can apply data science, we must do justice to another crucial player — the cloud and cloud computing in general. That’s exactly what we will focus on in this article.
To understand the advantages cloud computing provides when it comes to data science, let’s imagine a world with as much data as we have today, but without servers.
In such an unfortunate scenario, firms would need databases that run locally, right?
This unfortunate world would have several main drawbacks:
Doesn’t sound like the perfect scenario, does it?
And then these servers had drawbacks of their own.
You see my point, right?
They overshadow local servers in almost every conceivable aspect. And, in fact, data scientists should be focused on developing great algorithms, testing hypothesis, taking advantage of all available data without having to wait hours to see the results of the tests they are performing and certainly without having to worry how much memory space they have left on their computer.
And yes, sometimes data scientists do end up waiting for long hours for an algorithm to train, but with a cloud, they have the option to pay more and get the job done faster.
That being said, the biggest winners are smaller entities, as they get cheap access to the same tools as enormous corporations. And this is why cloud technologies are a huge enabler. They create a level playing field and allow small players to compete with much bigger ones.
Remember when, all of a sudden, people around the world were able to open e-commerce stores and compete on a global scale with the established firms?
The fact that data scientists and data analysts can rely on data stored on the cloud truly makes their life so much easier!
In addition, most cloud providers allow data scientists to access readily installed open-source frameworks right away. This is not only super convenient but can also be a huge time saver.
Alternatively, if you wanted to use Apache Spark in the conventional way you would have to:
That’s the setup you need to go through if you are working on your own pc. However, if you are using a cloud service, you’ll be able to start working with the Apache Spark framework right away! Yep, it’s been already installed for you. The same is valid for many different open-source frameworks.
Over the last few years, Amazon Web Services, Microsoft Azure, and Google Cloud have tried to boost their cloud services in terms of capability to run machine learning algorithms. The Big 3 of cloud services focused on this area extensively, as they realized it could be an important source of competitive advantage in the long run. And, in case you’re wondering:
For example, thanks to cloud-based machine learning, a small e-commerce retailer could run a real-time recommender system algorithm to improve the product offering shown to customers based on the products they have already added to their cart. In this type of business, every website click can be interpreted as a particular type of intention and signal, and hence the real-time updated algorithm operating in the cloud will be able to make a suggestion that improves the chances of making a conversion and maximizing revenues.
It is still unclear who will win the cloud war between giants like AWS, Microsoft Azure, and Google Cloud. But one thing is certain.
This is a service that benefits greatly small and medium-sized businesses, enabling them to level the playing field when competing against large multinationals with superior IT infrastructure.
Check out the complete Data Science Program today. Start with the fundamentals with our Statistics, Maths, and Excel courses. Build up a step-by-step experience with SQL, Python, R, and Tableau. And upgrade your skillset with Machine Learning, Deep Learning, Credit Risk Modeling, Time Series Analysis, and Customer Analytics in Python. Still not sure you want to turn your interest in data science into a career? We also offer a free preview version of the Data Science Program. You’ll receive 12 hours of beginner to advanced content for free. It’s a great way to see if the program is right for you.
Originally published at https://365datascience.com on March 12, 2020.
365 Data Science is an educational platform
365 Data Science is an educational platform
Written by
Become an in-demand #DataScience professional TODAY with Unlimited access to the 365 Data Science Program -> https://365datascience.com
365 Data Science is an educational platform
"
https://medium.com/@alibaba-cloud/four-measures-to-achieve-good-governance-in-cloud-computing-78a56b602e5a?source=search_post---------298,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Jan 5, 2018·3 min read
The use of cloud computing by organizations is on the rise. According to 451 Research, organizations will spend over a third of their IT budgets on cloud hosting and services in 2017, up from 28% in 2016. The increasing demand is driven by the many benefits cloud computing can provide to businesses, such as eliminating the costs of maintaining expensive IT infrastructure on-site.
For enterprises that are seeking to shift from internal to cloud-based IT services, good governance is critical when they make the move. Good governance simply means organizations following a set of measures or policies to ensure they make the most of cloud computing and minimize the risks. Here’s a list of four measures businesses should take to achieve good governance when they decide to embrace cloud technologies.
When businesses transfer their data to the cloud, it becomes susceptible to being breached by hackers and cybercriminals. To lower the risk of unauthorized actors accessing this data, various security procedures should be taken when the data is moved to the cloud. These include encrypting the data, or placing particularly sensitive data in a private cloud rather than a public one. A private cloud segregates the sensitive data of an organization from all other data, making it harder to reach.
Business leaders need to have plans in place in the event that their cloud service fails and goes offline, which could have catastrophic consequences on the operation of a business. Contingency steps are required to minimize the impact of such a scenario. One step is to keep a copy of critical data and services on local IT infrastructure, so that core aspects of the business can continue to function if cloud services are not available.
Cloud data centers which store data could be based anywhere in the world. But certain types of data are not allowed to be transferred to certain countries, due to data privacy and sovereignty laws. This is especially true for data related to strongly regulated sectors such as finance or health. Businesses are advised to seek legal advice before entering a cloud arrangement, to ensure their data is not transferred to a data center in a country where that data may be unlawful.
The most important measure of all is choosing a reliable cloud provider. A reliable provider will help a business achieve good governance. This is because a good provider will offer data encryption or private cloud functionality to keep data secure. A good provider will further have its own contingency plans in place for moments of failure, such as disaster recovery mechanisms that get services and data back online straightaway. A good cloud provider will also offer geographic flexibility, meaning that they will allow enterprises to transfer their data to jurisdictions where that data does not contravene any laws.
To find a good provider, businesses should consider factors such as client testimonials about the provider’s performance, how long the provider has been operating as well as the company’s financial stability. Certification should also be taken into consideration. Most international cloud companies are certified and audited for reliability. However, this is not always the case with Chinese providers. Therefore if a business is seeking a Chinese cloud company, they should check to see whether that company has been awarded any globally-recognized certifications concerning their reliability.
The allure of cloud computing is great, and businesses may be tempted to jump in immediately without accounting for the risks. Such recklessness is asking for trouble, since a hasty and uninformed utilization of cloud computing could affect the very survival of a business. It often takes a long time (even years) to transfer data to a cloud computing service. If the service subsequently develops issues which have not been accounted for and the data becomes impaired, years of investment are potentially wasted. This is why good governance, achieved through the measures described above, is vital to ensure that cloud computing can be harnessed to help a business grow rather than put it in danger.
Reference:
https://www.alibabacloud.com/blog/Four-measures-to-achieve-good-governance-in-cloud-computing_p68907?spm=a2c41.11113857.0.0
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/the-holy-trinity-of-cloud-computing-f84578dd7fa1?source=search_post---------299,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Jan 5, 2018·4 min read
To the casual observer, it may seem that cloud computing has finally gone mainstream. And to a certain extent it has. From consumers saving their photos to their online storage account to businesses using powerful cloud-based algorithms to unlock the secrets of their data, the cloud is everywhere. But ubiquity does not necessarily herald progress, or power; today’s cloud industry is one framed by a lack of understanding about how to deliver truly useful on-demand computing.
The problem lies mainly in the assumption that scale alone is the solution — and it is not. Size is all too often being pursued at expense of power and functionality, two vital ingredients for any cloud platform of note. Data is being stored, but can it also be quickly and efficiently processed, and then mined for its hidden layers of value? This is the modern paradox of distributed computing, but one that is easily solved by the holy trinity of cloud computing: the rare yet extraordinarily powerful combination of big data, computer processing, and rich algorithms.
Data, data, everywhere
The facts are plain: that we’re producing more data than ever — as much in the last two years as in the rest of the entirety of human civilization in fact; that a trillion photos were taken last year (80% with smartphones), with billions shared and stored online; and that within three years, a third of all this data will pass through the cloud. As more and more of the world comes online, so the amount of data that users generate continues to increase. But how to make sense of this vast ocean of information? Simple storage alone is not enough.
Although businesses are now sitting on vast amounts of information, not all data is created equal. Low-quality data is an ongoing and growing problem, as businesses struggle to organize and collate the data they are generating. This is also exacerbated by a general lack of structure, stemming from a wide mix of data sources such as social, retail, transactional, machine-to-machine etc. When real-time data is added into the mix, organizations are often left with a cluttered, multi-scenario environment where the entire ecosystem lacks a functioning backbone.
Power in the machine
To be more than just information, data needs to be processed, a massive computational undertaking. Tools such as Hadoop have made the processing of large-scale data in cloud environments many multiple of times easier. Open source software such as this have rapidly reshaped the distributed computing industry, and helped begin to make sense of the large data sets emerging from SMEs right up to multinational organizations. New generations of similar tools — such as NoSQL, for example — offer even more features, and can process and store any data, even unstructured. While they lack the scalability of Hadoop, this new generation of database is perfect for the device-heavy (and very imminent) Internet of Things.
When combined with tools such as offline and real-time computing platforms, suddenly a cloud environment becomes an incredibly powerful and flexible distributed machine. This then opens the door for external parties to take advantage of the capabilities (and cost efficiencies) associated with the cloud, leading to the rise of Software as a Service (SaaS), Platform as a Service (PaaS), and Infrastructure as a Service (IaaS). Of course, some organizations choose to run some or all of their operations on an on-premise or private cloud, but these are limited by the resources of that organization. Thus we see the increasing popularity of hybrid cloud environments, with businesses creating a bespoke computing environment that draws on both the public and private cloud.
Mathematical magic
To organize and process your data is one thing, but to truly understand it is another. Some call this the holy grail of data — the ability to derive meaningful insights from large, seemingly benign data sets. Many cloud scientists live by the mantra that data without insight is largely useless, and that value will never be derived from managing data alone. Part of realizing this goal is to move the data away from the traditional minority of data scientists, and open it to developers and analysts who can bring the power of the ecosystem to the table.
Supporting this evolution are increasingly powerful algorithms that feed off the powerful computing platforms and mine big data sets for insights and value. Traditionally developed in laboratories, the better cloud providers are starting to integrate these tools across the platform as they seek to maximize their efficiency. The result is an accelerated integration of data across distributed computing platforms, and a new generation of insights from previously unrelated data. For example, meteorological data taken on its own has limited value, but when combined with agricultural or retail data it can suddenly unlock significant commercial value.
Better together
Cloud computing has evolved into one of the most exciting and important trends in business and IT today, a far cry from the early (largely unsuccessful) days of data warehousing. From its ability to store vast amounts of information to the innovative ways it processes it and then analyses it to derive value, the holy trinity of cloud computing is delivering insights to businesses around the world every second of every day. But each of these three core elements on their own are just facets of a bigger picture. To deliver one without the others is to fly in the face of efficiency and relegate true value to nothing more than an afterthought.
Reference:
https://www.alibabacloud.com/blog/The-Holy-Trinity-of-Cloud-Computing_p69216?spm=a2c41.11113793.0.0
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://yanpritzker.com/what-is-cloud-computing-13bef5544859?source=search_post---------300,"The term Cloud Computing is on the rise according to google trends. But what does it all mean? Just like “Web 2.0” it’s an overloaded phrase with tons of connotations and many attempts at definition. While perusing a cloud blog by James Urquhart, I was inspired to try to define the term myself. So here’s what cloud computing means to me.
[shameless plug] check out Elastic Server On-Demand.
co-founder swanbitcoin.com. Former CTO -  Reverb.com
Written by
co-founder swanbitcoin.com. Former CTO, Reverb.com.
co-founder swanbitcoin.com. Former CTO -  Reverb.com
Written by
co-founder swanbitcoin.com. Former CTO, Reverb.com.
co-founder swanbitcoin.com. Former CTO -  Reverb.com
"
https://iltb.net/threats-posed-to-privacy-by-cloud-computing-c4f1cf36da4?source=search_post---------301,
https://medium.com/@reichental/cloud-computing-unlocks-innovation-opportunities-for-city-cios-cc6655e6876?source=search_post---------302,"Sign in
There are currently no responses for this story.
Be the first to respond.
Dr. Jonathan Reichental
Sep 13, 2020·5 min read
The adoption of cloud computing and XaaS by city CIOs presents enormous opportunities for better community experiences, lower costs and a long overdue platform to unleash the innovation that cities so desperately need and desire.
Local governments are generally slower in embracing emerging technologies than other sectors of the economy. That’s no big revelation to anyone. This hesitancy is often for good reason. Many city systems are essential services and they don’t invite experimentation and risk-taking. The private sector by definition must push innovation and take higher risks because their competitiveness in the marketplace often depends on it. Governments don’t have the same motivations. That doesn’t let them off the hook though. When cities are ready to take the plunge, emerging technologies coupled with innovative processes can lower the cost of operations, deliver better community experiences, and automate and accelerate many services.
Cloud computing, at first slow to be used in the public sector, is now picking up the pace of adoption in cities. It’s used in some form by over 50% of all governments and spending on cloud computing is increasing by around 17% per year. It’s creating all manner of government efficiencies. The city of Honolulu in Hawaii made a commitment to cloud services that has enabled new services and brought down technology costs. But what about innovation? Government use of cloud computing is finally beginning to deliver that promise too. However, realizing new value can be elusive and often secondary to other, more pressing, priorities. How might city CIOs leverage cloud computing as a platform for innovation and positive change in their communities?
City agencies have traditionally built and managed their own on-premises datacenters. This is not unlike the history of technology in the private sector, although cities have clung onto their preference for owning and maintaining their technology infrastructure far longer. Now, cloud computing and its derivative: everything-as-a-service (XaaS), is in full adoption mode.
Cloud computing is a style of computing that provides technology services over the Internet. Today, those services are deep and wide. Almost everything can be provided this way. That’s where the term Everything-as-a-Service or XaaS originates. These computing services span from software applications to storage and identity management, from development platforms and communication suites to artificial intelligence-as-a-service and much more. No longer burdened with capital costs and long-term contracts, XaaS is providing City CIOs with service flexibility, little-to-no maintenance, peace of mind with built-in disaster recovery, and better financial terms.
Mayors, city managers, community members and other stakeholders are demanding more innovation at city hall and across their city. Armed with an array of on-demand XaaS capabilities, reduced capital expenditures, and freed up staff who would otherwise be deep in old-school datacenter management, CIOs can finally unlock innovation opportunities.
The ability to move quickly on needs and subscribe to state-of-the-art solutions, often by simply entering a corporate credit card, is making XaaS a core driver of digital transformation and is supporting smart city initiatives. It’s reducing the need for cities to build solutions since so many government-focused software-as-a-service (SaaS) applications are now available. It’s reducing project implementation timelines because the needs for hardware acquisition and deployment, software installation, integration and complex configuration, are either minor or eliminated altogether.
With the complexity and maintenance of technology infrastructure diminishing through cloud computing, the city CIO and team can focus on higher-value work. This means that over time, the focus can be on understanding city needs and strategy, deeper business analysis, and even experimentation. The CIO becomes a peer of other departments rather than being relegated to a back-office service provider.
Cities across the world are finding that XaaS is enabling all manner of rapid and innovative solutions. Los Angeles used cloud services to develop a system that sends an earthquake alert to mobile phones one-minute before it hits. The non-stop city of Tel Aviv, Israel has become a leading smart city through its use of digital services and XaaS. The CIO, Liora Shechter, championed a system called DigiTel that provides a one-stop platform for citizens to communicate with government and receive personalized services.
Widely adopting XaaS is not without challenges and risks. Many city technology staff continue to be conservative on making rapid moves away from on-premises solutions. The detachment from traditional hands-on oversight that includes swapping out failing hardware to configuring cybersecurity systems, doesn’t sit well with many. Some will need to be pushed into this new era, rather than gleefully skipping into this digital transformation.
Cloud computing and XaaS also means losing some level of control. This includes narrower options for configuration. With multitenancy (the vendors infrastructure is designed to deliver consistent experiences that scale to all users) all subscribers get the same services. CIOs must also be comfortable with having less of a role in security and with a higher risk to privacy. With XaaS, that’s mostly the vendors job. Subscribing to cloud services can also expose organizations to a form of lock-in. This means that discontinuing services from one vendor in favor of another can present real difficulties in moving data smoothly to the new platform. Finally, cities are required to ensure compliance with regulatory requirements. That might mean that data needs to be handled a certain way including being stored in the same country as the city.
Despite, these real risks and challenges, the adoption of cloud computing and XaaS by city CIOs presents enormous opportunities for better community experiences, lower costs and a long overdue platform to unleash the innovation that cities so desperately need and desire.
=========================
This article first appeared in Industry Tech Insights.
You can learn more about XaaS in my LinkedIn Learning Online Video Course.
I discuss the role cloud computing and other technologies in local government in my bestselling book, Smart Cities for Dummies.
Multiple award-winning technology and business leader. Best-selling author. Professor. Idea machine.
Multiple award-winning technology and business leader. Best-selling author. Professor. Idea machine.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@Fonality/out-of-the-blue-cloud-computing-and-emergency-preparedness-28225ade845?source=search_post---------303,"Sign in
There are currently no responses for this story.
Be the first to respond.
Fonality
Mar 23, 2017·2 min read
Don’t panic, the world’s not about to end. That being said, operational resilience is a key factor for any business looking to prepare for the unexpected.
The ability to adapt and react to change is an important, but sometimes overlooked, part of business. Continuity of operation is key. So when lightning strikes, getting back on your feet as quickly as possible is critical.
Thanks to cloud computing technology, unified communication platforms like Fonality’s Heads Up Display (HUD) afford companies both large and small a level of preparedness that allows them to remain active and productive during times of emergency, reducing the impact disasters have on overall operation.
Businesses need to be resilient
Communication channels are the backbone of many business, yet we often overlook how fragile they can be during an emergency, especially when it comes to storage and data accessibility. For example, the loss of a server for any length of time can spell disaster for things like email, video conferencing and VoIP.
According to the Institute for Business and Home Safety, 25 percent of businesses don’t reopen after a natural disaster. While many companies have a disaster recovery plan, rather than focusing on preparing for specific threats, the ability to cope with any change is paramount for long-term stability. By hosting data in the cloud, your information isn’t subject to the same factors that can put other businesses at risk. Once things return to normal, you have the assurance that you can pick up where you left off.
Emergencies, productivity and the safety of employees
Employee safety is of the utmost importance during any emergency. Often it’s safer for employees to remain at home in such instances, but depending on your business, doing so might come at the cost of productivity.
By using hosted PBX, employees can stay in touch and work remotely for extended periods of time. Even if your base of operation is inaccessible, your data isn’t. With unified communications hosted online, your employees have all the tools they need to do their job wherever there is internet access. Suddenly, your business has the flexibility to adapt where and when your employees work, should something strike out of the blue.
At Fonality, we aim to provide businesses big and small the ability to adapt to change (both the expected and unexpected kind). For a complete list of action items that will prepare you for anything, download our Disaster Preparedness Checklist.
Fonality’s reliable VoIP service and business phone systems serve more than 30,000 businesses and help streamline business communications.
See all (888)
1
1
Fonality’s reliable VoIP service and business phone systems serve more than 30,000 businesses and help streamline business communications.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@nowsourcing/everything-you-know-about-cloud-computing-is-wrong-24e497b26d8c?source=search_post---------304,"Sign in
There are currently no responses for this story.
Be the first to respond.
Brian Wallace
Dec 8, 2014·2 min read
What does cloud computing have to do with popular Chinese-pop songs about chickens? Not much — it’s an analogy. People throw around buzzwords in the cloud computing world with abandon, yet many do not know what they are actually talking about. Much in the same way that over the top music videos are getting people’s attention.
So if you’re in the know about the private cloud, you’re probably aware that OpenStack is changing the way businesses connect to the cloud. Whether it’s through data storage, dashboard features, or just plain organization, OpenStack is the IaaS (infrastructure as a service) for everyone. It’s one of the fastest growing cloud technologies with one of the simplest goal in mind: providing a reliable connection to the cloud. But what exactly is OpenStack, and how can we make sense of it?
Developed in 2010 from a joint effort between NASA and Rackspace Hosting, OpenStack is a free and open-source cloud based computing software system. OpenStack is intended to bring businesses into the cloud-age and help them upgrade their computing while still running on their own standard hardware. The OpenStack IaaS is currently managed by OpenStack Foundation, a non-profit corporation that helps to develop, grow, and promote OpenStack. Over 200 well kknown and internationally renowned companies are backing this project with their support, including AT&T, Cisco, Intel, IBM, and Yahoo!.
Check out this infographic for more about the movers and shakers of the OpenStack movement to see if it’s right for you and your business. Have any thoughts or comments about it? Join in on the conversation by dropping us a comment and if you like what you see, don’t forget to share this infographic with your friends!
Founder of NowSourcing. Contributor to Hackernoon, Google Small Business Advisor, Podcaster, infographics expert.
Founder of NowSourcing. Contributor to Hackernoon, Google Small Business Advisor, Podcaster, infographics expert.
"
https://medium.com/@alibaba-cloud/catch-the-finals-of-the-2018-global-blockchain-competition-at-alibaba-cloud-computing-conference-337ca7f17173?source=search_post---------305,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Sep 5, 2018·3 min read
On June 25, Alibaba Cloud announced the 2018 Global Blockchain Competition for blockchain developers in the world, covering a range of propositions in multiple fields such as data asset, supply chain finance, and tracing technology. Contestants can choose propositions and submit their solutions. Alibaba Cloud expresses the hope to collect blockchain solutions from a wide audience and introduce industry customers’ verification solutions and results to apply the blockchain technology to the real economy and boost the popularity of this technology.
“Blockchain is a core technology for driving the next-generation Internet transformation. We hope the blockchain competition can motivate developers in the world to collaborate and innovate,” said Yi Li, technical director of Alibaba Cloud blockchain products. “The application ecosystem and openness are the important factors determining the long-term development of blockchain technology. Alibaba Cloud will support the open-source and enterprise-level blockchain technology Hyperledger Fabric and Ant Blockchain to provide customers with more open choices.”
It is reported that the competition’s judging panel consists of dozens of experts from the sectors focusing on the blockchain technology and the most authoritative representatives of this technology, including Hyperledger Foundation, Deutsche Telekom, Ziggurat Internet Technologies, MYBANK, and Alibaba. They will review the contestants’ works from their professional point of view. Long Wenxuan, director of Hyperledger Foundation in China, made a speech on site to express Hyperledger’s support for the competition and wish a successful completion of the competition.
Blockchain is an innovative distributed ledger technology featuring decentralization, openness and transparency, and anti-tampering. The competition encourages contestants to give full play to their talents to contribute to the rapid and healthy development of blockchain technology by building innovative applications adapted to service scenarios on Alibaba Cloud’s Hyperledger Fabric-based blockchain technology platform.
The world is at the starting point of a new round of Internet technology transformation, and China is becoming a center of new technologies represented by blockchain. In the 2017 Global Blockchain Enterprise Patent Ranklist (Top 100), Chi nese enterprises account for 49%, whereas U.S. enterprises account for 33%. Alibaba Group ranks №1 with 49 global patents, followed by Bank of America.
Currently, Alibaba’s blockchain technology experiences deep application in the financial sector. On June 25, AlipayHK launched the world’s first e-wallet that uses blockchain for cross-border remittance. A remittance was completed by Grace, a Filipino who works in Hong Kong, in only 3 seconds. Several months ago, Alibaba Cloud launched a series of blockchain solutions, allowing developers to configure and deploy a blockchain application in a matter of minutes. The solutions have been applied to authentic product tracing of luxury goods sold on Tmall, with remarkable success.
Catch the Finals at Hangzhou in September!
The Alibaba Cloud has divided the 2018 Global Blockchain Competition into preliminary, semi-final, and final rounds. The final round will be held at the 2018 Alibaba Cloud Computing Conference in Hangzhou. Industry leaders and major customers will gather together to explore the rapid development of blockchain technology.
We look forward to seeing you at the 2018 Computing Conference!
Reference:
https://www.alibabacloud.com/blog/catch--the--finals--of--the--2018--global--blockchain--competition--at--alibaba--cloud--computing--conference_593940?spm=a2c4.11966377.0.0
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@TebbaVonMathenstien/cloud-computing-was-absolutely-conceived-of-independently-and-aws-wasnt-even-the-first-player-in-895729874ccd?source=search_post---------306,"Sign in
There are currently no responses for this story.
Be the first to respond.
Tyler Elliot Bettilyon
Sep 5, 2019·1 min read
Evan Kozliner
Cloud computing was absolutely conceived of independently, and AWS wasn’t even the first player in that market. Rackspace launched in 1998, 4 years ahead of AWS’ 2002 informal launch, and 8 years ahead of the 2006 official launch: https://en.wikipedia.org/wiki/Rackspace
The AWS styled multi-tenant model was an innovation, to be sure. And in general, I am happy to concede that AWS has done significant and pioneering work: The state of the art in cloud computing would be significantly diminished in a world where AWS never got off the ground. But it would certainly exist.
Even if the cloud computing marketplace didn’t exist, it’s not clear to me that Amazon should get significant credit for the innovative products that are now hosted on AWS: Time-sharing on super computers was a thing way before AWS. Data centers were as well.
AWS — like other Amazon services — makes stuff easier. That ease of use clearly does help to empower innovators, but AWS doesn’t get to take full credit for innovative software products just because the creators use AWS.
A curious human on a quest to watch the world learn. I teach computer programming and write about software’s overlap with society and politics. www.tebs-lab.com
See all (349)
1
1
A curious human on a quest to watch the world learn. I teach computer programming and write about software’s overlap with society and politics. www.tebs-lab.com
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@HOSTINGdotcom/cloud-computing-success-starts-with-the-data-center-b658d491d49d?source=search_post---------307,"Sign in
There are currently no responses for this story.
Be the first to respond.
HOSTING
Apr 4, 2015·2 min read
With the increased hype around cloud computing, companies don’t always know how they should evaluate potential cloud service providers (CSPs). Since there are more than 3,000 CSPs to choose from, establishing selection criteria can be daunting. It doesn’t have to be complicated. Remember, a successful cloud computing experience begins with a secure, energy-efficient, strategically located center. Below are three areas in which to measure a CSPs’ data centers.
The golden rule for real estate also applies to data centers. Location, location, location is a critical factor to consider during your evaluation. Many CSPs boast of their number of data centers. However, if having your data assets in close proximity to your company’s physical location or in-house IT team is a priority, some CSPs may fall off your list.
Location also matters in regards to energy efficiency and availability. Carefully consider the data center’s location, network architecture and power grids. Many issues, such as power or network connectivity, are related to larger concerns such as where a data center is built. With self-sustaining, energy-efficient data centers that take advantage of local and renewable power sources, cloud providers can reduce their notoriously large carbon footprint and the risk of outages for end users.
Arbor Networks’ latest Worldwide Infrastructure Security Report found that data centers have become “magnets” for DDoS (Distributed Denial of Service) attacks. Customers evaluating CSPs need to understand the specific security measures CSPs have in place to protect their data assets.
Power consumption from data centers across the U.S. continues to grow exponentially. A one-year investigation led by the New York Times revealed that many data centers are designed to use an enormous amount of electricity. Reports also show that data centers across the globe consume thirty billion watts of power, which is comparable to that outputted from thirty nuclear plants. US data centers account for 25–34% of this load.
When evaluating data centers, ask about the CSP’s “green standards.” Building self-sustaining, highly energy-efficient data centers may not be an option for every CSP. However, CSPs should consider using local resources and adopting a micro-grid strategy to powering their data centers.
We build and operate high performance clouds for business-critical applications. Parent to @Ntirety and @HostMySite.
We build and operate high performance clouds for business-critical applications. Parent to @Ntirety and @HostMySite.
"
https://medium.com/@alibaba-cloud/how-does-cloud-computing-help-analyze-critical-covid-19-cases-80-faster-dfd080876fa9?source=search_post---------308,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Jul 27, 2020·5 min read
Bolster the growth and digital transformation of your business amid the outbreak through the Anti COVID-19 SME Enablement Program. Get a $300 coupon package for all new SME customers or a $500 coupon for paying customers.
By Alibaba Cloud ECS
Although the impact of the coronavirus pandemic in Mainland China has been gradually easing, but the war of public scientific research institutions against the virus continues. Researchers and health workers are still studying the virus day and night, tracing where it came from, how it infects people, and how we can eliminate it. Science and technology have long provided important support for scientific researchers, and enormous computing power is involved in virus analysis.
Sun Yat-sen University School of Medicine is one of the scientific research institutions waging a continuous war against the virus. It was also one of the institutions that Alibaba Cloud provided free high-performance cloud computing support during the pandemic.
During the pandemic, the School of Medicine cooperated with many hospitals and centers for disease control and prevention. They plan to acquire over one thousand nucleic acid samples of COVID-19 patients in Wuhan and the whole-course data of some patients. Researchers hope that the origin, evolution, and pathogenesis of the COVID-19 virus can be better understood through genome sequencing and sequencing data analysis.
Shi Mang, a professor at the School of Medicine, explained that researchers look for factors correlated with critical COVID-19 cases in three areas: the pathogen, the microenvironment (bacteria or microorganisms co-infected with the pathogen), and the human immune system.
The School of Medicine uses macro transcriptomics in an attempt to extract all the key genetic information from the original sample. Then, scientists analyze the genetic information to reproduce the dynamic interactions of the pathogen infection, the host immune system, and the internal environment of the body to find the key factors that lead to critical cases.
Only with a comprehensive understanding of the factors that are correlated with critical cases can we predict the patients most at risk so that doctors intervene early and administer the proper treatment. This will guide the development of drugs to mitigate and prevent serious cases.
Scientists will summarize the analyzed clinical information, virus information, genome information, and test result data to construct big data on COVID-19 infections. This data will facilitate investigations into the origin of the virus and subsequent research.
In the research process, scientists first need to sequence the original samples through a high-throughput gene sequencing platform to obtain complete genetic information of the virus, bacteria, and hosts. However, this is fragmented information. Further bioinformatics analysis, including sequence splicing and annotation, is needed to interpret the key organic processes occurring in the patient’s body at the time the sample was taken.
The splicing and comparison of genetic information require high-performance computing and the massive data volumes require enormous computing power.
Over 1,000 cases needed to be processed in this experiment, at least two sets of data were to be collected for each case, each about 2 GB to 3 GB in size. Some cases had dozens of samples in order to collect the whole-course data. In addition, control group information was needed. For comparison purposes, Sun Yat-sen University scientists also needed to download about 8,000 samples from the U.S. National Center for Biotechnology Information (NCBI), which would produce a huge amount of data.
In the race to beat the pandemic, the time advantages of high-performance computing on the cloud are of critical importance because it can accelerate the research process.
Alibaba Cloud helped Sun Yat-sen University construct a complete computing process from raw data to the final analysis results. The elasticity of cloud services makes it possible to deliver a wealth of computing resources in a short time. As a result, researchers can use Elastic High Performance Compute (EHPC) to deploy a super computing cluster environment with one-click and then dynamically add or remove cluster nodes as needed, eliminating the need to manage and maintain servers. They do not need to spend a lot of energy on maintenance and can focus on research instead.
The Alibaba Cloud EHPC team also assisted Sun Yat-sen University in implementing bioinformatics and scientific computing using Alibaba Cloud services, which provided 104 core processors. Compared with offline servers, the data assembly and comparison performance improved by over 25%.
With the high performance and abundant resources of Alibaba Cloud, the time required to analyze each sequencing result was reduced from 12 hours to 2 hours. At this rate, a computing project that previously had taken an entire year can now be finished in about two months. This increase in speed significantly increased productivity, accelerated the research process, and won time in the fight against the epidemic.
To learn more about Alibaba Cloud’s high-performance computing capabilities, visit the EHPC product page
While continuing to wage war against the worldwide outbreak, Alibaba Cloud will play its part and will do all it can to help others in their battles with the coronavirus. Learn how we can support your business continuity at https://www.alibabacloud.com/campaign/fight-coronavirus-covid-19
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@joseantonio11/bigdata-22e-big-data-technologies-cloud-computing-fe04b2830fc9?source=search_post---------309,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jose Antonio Ribeiro Neto (Zezinho)
Dec 2, 2018·2 min read
The emergence of Big Data was made possible by the convergence of various technologies.
Among them we have:
Let’s know a little more about Cloud Computing.
CLOUD COMPUTING
Cloud Computing has quickly changed personal and professional computing, enabling individuals and small businesses to have the same technologies (formerly restricted to large companies) available to compete as equals in the digital world.
Its application extends to many fields of technology, from the simplest, such as file storage, to the most sophisticated such as server virtualization and enterprise computing services.
The most common services offered by Cloud Computing are:
1- IaaS (Infrastructure as a Service) — Get only Hardware (Virtual Machines, Servers, Storage). 2 — PaaS — (Platform as a Service) — Obtain the Computational Environment (Database, Web Servers, Development Tools). 3 — SaaS — (Software as a Service) — Obtain Software on Demand (CRM, email, virtual desktops, games, administrative systems). 4 — XaaS — (Anything as a Service) — Get everything you need in Information Technology to make the business work.
CURIOSITIES ABOUT CLOUD COMPUTING
Article selected from the eBook “Big Data for Executives and Market Professionals.”eBook in English: Amazon or Apple StoreeBook in Portuguese: Amazon or Apple StoreeBook Web Sites: Portuguese | English
Author. Big Data Researcher. USA WebCT IT Executive. Director of Education and Technology. Portuguese Brazilian citizen. Peace for everyone. bit.ly/2WDtZUA
See all (36,838)

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
Author. Big Data Researcher. USA WebCT IT Executive. Director of Education and Technology. Portuguese Brazilian citizen. Peace for everyone. bit.ly/2WDtZUA
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@HOSTINGdotcom/the-top-four-impacts-of-cloud-computing-on-e-commerce-applications-f5923ed10e66?source=search_post---------310,"Sign in
There are currently no responses for this story.
Be the first to respond.
HOSTING
Mar 9, 2015·3 min read
The Top Four Impacts of Cloud Computing on E-commerce Applications
The ability to lower costs, accelerate deployments and respond quickly to market opportunities and challenges are just a few reasons why so many IT leaders are leveraging cloud-based e-commerce applications. Given the variety of solutions, IT leaders must research their options carefully in order to select the one that best meets their needs. Following are the top four impacts of cloud computing on e-commerce applications and steps IT leaders should take during their evaluation process.
Cloud-based e-commerce applications allow companies to respond quickly to market opportunities and challenges — as long as they engage IT
It’s easy for business leaders to focus on the benefits of cloud computing without considering the time and effort involved in implementing a viable solution. However, whatever cloud computing solution they select, the application will need access to customer data, product data, fulfillment systems and other operational systems in order to support e-commerce. Cue the IT team.
As we covered in our recent blog post, Cloud Computing Investments and the “New IT,” today’s IT staff is being tasked to serve as trusted business advisors to their organization’s customers, partners and executives. Prior to an organization investing in a cloud solution, the IT team must clearly outline the business goals, objectives, costs and benefits. They should also review what systems and data must be integrated (i.e., fulfillment systems, customer service systems and customer and product data sources) in order to achieve the goals. Finally, IT should explain how failing to integrate other systems will impact overall business results.
Cloud-based e-commerce applications enable IT and business leaders to evaluate new opportunities without large upfront investments
With smaller capital expenditures (CapEx) needed to launch a site, combined with operational expenditures (OpEx) that are typically billed on a “pay as you go” basis, organizations can shift their business online with minimal investment risk. A faster time to deployment allows them to streamline time-to-market for their products and services.
For companies looking to expand into new geographical regions or test an e-commerce business model with a new brand or product line, or as a proof of concept (POC), cloud-based e-commerce applications provide them with flexibility and scalability at a reasonable investment. However, it’s important for IT to evaluate the total costs involved in implementing a cloud solution including: integration, customization requirements, migration costs, e-commerce seasonality and peak loads and scalability.
Consumerization of the online customer experience requires closer scrutiny of solution offerings
While many B2C companies use e-commerce platforms for direct sales, B2B organizations are also leveraging them to add transactional capabilities to their informational sites. In addition, the online experience is becoming more “consumerized,” meaning that B2B buyers expect a retail-like customer experience — even when visiting non-retail sites. Cloud solution providers (CSPs) that focus solely on creating retail models are often not well-versed in B2B requirements which can be more complex. As a result, their offerings don’t include B2B functions, such as easy entry of large orders and repeat orders, segmented product catalogues that are based on a client hierarchy and buying privileges, configure price quote capabilities and extended payment terms. IT leaders have an unprecedented number of CSPs from which to choose. However, they need to carefully evaluate ones that have experience meeting their industry-specific needs, whether it’s B2B, B2C, or a combination of both.
IT leaders must understand the pros and cons of cloud-based ownership models in order to select the right solution for their needs
In addition to selecting a cloud-based e-commerce solution, IT leaders are tasked with choosing from a variety of ownership options. For example, an organization can use an Infrastructure-as-a-Service (IaaS) model to run its application package or custom e-commerce software. It can use a managed service provider to manage its applications or create custom e-commerce software on IaaS. Or the organization can subscribe to a Software-as-a-Service (SaaS) e-commerce application. Each of these options must be mapped back to the organization’s financial resources and technology requirements. IT leaders also need to determine the features and functions they require for their organization’s online selling initiatives. They should collaborate with an experienced CSP to develop a solution that supports their organization’s long-term online sales objectives, and outlines current and future technology needs. Finally, IT needs to evaluate the total cost of ownership (TCO) of their cloud model versus the traditional licensed approach for a pre-determined period of time to evaluate its ROI.
We build and operate high performance clouds for business-critical applications. Parent to @Ntirety and @HostMySite.
We build and operate high performance clouds for business-critical applications. Parent to @Ntirety and @HostMySite.
"
https://medium.com/@alibaba-cloud/what-is-cloud-computing-4ddf4ebd995e?source=search_post---------311,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Dec 4, 2017·1 min read
Cloud computing has received much attention since its birth as an innovative application model in the field of information technology. Thanks to features such as low costs, elasticity, ease of use, high reliability, and service on demand, cloud computing has been regarded as the core of next-generation information technological reform. Cloud computing has been actively embraced by many businesses, and is transforming industries such as the Internet, gaming, Internet of Things, and other emerging industries. Most enterprise users, however, are often restricted by traditional IT technical architecture and lack the motivation as well as the technical expertise for migration to the cloud.
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/@jaychapel/how-to-get-the-cheapest-cloud-computing-7561e6fa9848?source=search_post---------312,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Oct 10, 2017·3 min read
Are you looking for the cheapest cloud computing available? Depending on your current situation, there are a few ways you might find the least expensive cloud offering that fits your needs.
If you don’t currently use the public cloud, or if you’re willing to have infrastructure in multiple clouds, you’re probably looking for the cheapest cloud provider. If you have existing infrastructure, there are a few approaches you can take to minimize costs and ensure they don’t spiral out of control.
There are a variety of small cloud providers that attempt to compete by dropping their prices. If you work for a small business and prefer a no-frills experience, perhaps one of these is right for you.
However, there’s a reason that the “big three” cloud providers — Amazon Web Services (AWS), Microsoft Azure, and Google Cloud — dominate the market. They offer a wide range of product lines, and are continually innovating. They have a low frequency of outages, and their scale requires a straightforward onboarding process and plenty of documentation.
Whatever provider you decide on, ensure that you’ll have access to all the services you need — is there a computing product, storage, databases? How good is the customer support?
For more information about the three major providers’ pricing, please see this whitepaper on AWS vs. Google Cloud Pricing and this article comparing AWS vs. Azure pricing.
Of course, if your organization is already locked into a cloud computing provider, comparing providers won’t do you much good. Here’s a short checklist of things you should do to ensure you’re getting the cheapest cloud computing possible from your current provider:
While finding the cheapest cloud computing is, of course, beneficial to your organization’s common good, there’s no need to let your work in spending reduction go unnoticed. Make sure that you track your organization’s spending and show your team where you are reducing spend.
We’ve recently made this task easier than ever for ParkMyCloud users. Now, you can not only create and customize reports of your cloud spending and savings, but you can also schedule these reports to be emailed out. Users are already putting this to work by having savings reports automatically emailed to their bosses and department heads, to ensure that leadership is aware of the cost savings gained… and so users can get credit for their efforts.
Originally published at www.parkmycloud.com on October 10, 2017.
CEO of ParkMyCloud
CEO of ParkMyCloud
"
https://medium.com/@Fonality/cloud-computing-can-help-your-business-reach-the-next-level-5c8032aa4469?source=search_post---------313,"Sign in
There are currently no responses for this story.
Be the first to respond.
Fonality
Jan 27, 2017·2 min read
The typical business today is not only fast and efficient in everything it does — it’s also highly collaborative. Modern employees can share information quickly and seamlessly, using it to complete projects and get everyone involved. This is great because it allows everyone to play to their strengths and contribute whatever they do best.
Nowadays, the best way to share information is to rely on cloud services. Why bother with outdated hardware or confusing, bulky emails full of attachments when you can simply use the cloud to empower employees who want to communicate and work together?
The benefits of cloud computing
Of course, one element of your company’s communication infrastructure is its phone systems — making calls in and out regularly is a fundamental part of doing business. But work in the 21st century largely revolves around data, so it’s important to have cloud services in place as well.
According to the Harvard Business Review, Benefits of the cloud include increased collaboration and greater data insight. In addition, employees have complete control over how, when and where they access their data, and they don’t have to worry about data centre providers or IT supervisors getting in their way.
From a managerial standpoint, the cloud makes it easy for departments to pool information and collaborate on projects. The technology’s rapid elasticity makes it easy for them to scale their IT architectures up or down as their business needs change.
Choosing the right provider for you
When it comes time to choose a cloud provider, it’s important first to consider the scope of your company’s needs. Are you looking for a large-scale cloud platform or a small one? Do you need complex data analysis capabilities, or just simple storage? Are your demands static, or changing over time as your business evolves?
Entrepreneur Magazine recommends carefully examining the range of providers out there and scrutinising the subtle differences among them. It’s possible that one is a better fit than the rest for the specific applications you have in mind.
Make deployment simple and easy
The cloud offers your business the capability to take all communications and data-driven projects and bring them together. Your customer database, your internal workflows, your VoIP phone system — why not bring it all together under one umbrella?
Fonality makes that possible. Moreover, we seek to simplify the process for you, making it so you can easily deploy cloud technologies and pay for them in whatever manner is best for you. The long-term goal is to make managing IT easier for your business, not harder.
For help getting started, check out our FREE eBook: Taking Business Communications to the Cloud.
Fonality’s reliable VoIP service and business phone systems serve more than 30,000 businesses and help streamline business communications.
Fonality’s reliable VoIP service and business phone systems serve more than 30,000 businesses and help streamline business communications.
"
https://medium.com/@alibaba-cloud/why-cloud-computing-can-help-smes-thrive-6ccd9b3171f5?source=search_post---------314,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Aug 11, 2020·3 min read
To learn more about how cloud computing can help SMEs thrive in an online world, download the Successful eCommerce Promotions — an Alibaba Cloud Guide for SMEs whitepaper today.
SMEs can struggle to get noticed in the crowded e-commerce space where multinationals often lead the way, using their seemingly limitless resources to grab the lion’s share of opportunities. This is where Alibaba Cloud can help businesses of any size stand out, scale up, and succeed in the world of online shopping, using the elastic and cost-effective nature of the cloud.
For starters, we have plenty of experience in the world of e-commerce. Our cloud-based solutions power the Double 11 Global Shopping Festival where, in 2019, the event recorded RMB 268.4 billion gross merchandise volume (GMV), setting yet another Double 11 record.
For SMEs, large-scale events like Double 11 represent a tremendous opportunity to reach new global audiences — but only if you have the right cloud-based solutions in place.
Cloud-based solutions provide SMEs with a highly scalable architecture, where a range of cost-effective products and services protect your business at every point in the online shopping lifecycle.
Compared to traditional infrastructures, the highly scalable and automated nature of a cloud-based environment unlocks a range of benefits to SMEs:
If you’d like to find out how to capitalize on events such as the Double 11 Global Shopping Festival, and ready your e-commerce site to survive and thrive in today’s complex online market, download the Successful eCommerce Promotions — an Alibaba Cloud Guide for SMEs whitepaper today.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/@TechJobs_NYC/what-microsofts-xamarin-purchase-says-about-the-cloud-computing-fight-c3f09a597383?source=search_post---------315,"Sign in
There are currently no responses for this story.
Be the first to respond.
Technology Jobs NYC
Feb 25, 2016·1 min read
What Microsoft’s Xamarin Purchase Says About the Cloud Computing Fight By QUENTIN HARDY As it competes with Amazon and Google, the Windows maker is trying to make it easy for customers to migrate their technology to mobile and the cloud. Published: February 24, 2016 at 07:00PM via NYT Technology http://www.nytimes.com/2016/02/25/technology/what-microsofts-xamarin-purchase-says-about-the-cloud-computing-fight.html?partner=IFTTT
The official account for https://www.technologyjobs.nyc! Follow for exclusive job opportunities, tips and tricks to help you get your next #technology #job
The official account for https://www.technologyjobs.nyc! Follow for exclusive job opportunities, tips and tricks to help you get your next #technology #job
"
https://medium.com/@jaychapel/3-things-companies-using-cloud-computing-should-make-sure-their-employees-do-6f9753d7087b?source=search_post---------316,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Oct 24, 2017·2 min read
These days, there’s a huge range of companies using cloud computing, especially public cloud. While your infrastructure size and range of services used may vary, there are a few things every organization should keep in mind. Here are the top 3 we recommend for anyone in your organization who touches your cloud infrastructure.
OK, so this one is obvious, but it bears repeating every time. Keep your cloud access secure.
For one, make sure your cloud provider keys don’t end up on GitHub… it’s happened too many times.
(there are a few open source tools out there that can help search your GitHub for this very problem, check out AWSLabs’s git-secrets).
Organizations should also enforce user governance and use Role-Based Access Control (RBAC) to ensure that only the people who need access to specific resources can access them.
There’s an inherent problem created when you make computing a pay-as-you-go utility, as public cloud has done: it’s easy to waste money.
First of all, the default for computing resources is that they’re “always on” unless you specifically turn them off. That means you’re always paying for it.
Additionally, over-provisioning is prevalent — 55% of all public cloud resources are not correctly sized for their resources. The last is perhaps the most brutal: 15% of spend is on resources which are no longer used. It’s like discovering that you’re still paying for that gym membership you signed up for last year, despite the fact that you haven’t set foot inside. Completely wasted money.
In order to keep costs in check, companies using cloud computing need to ensure they have cost controls in place to eliminate and prevent cloud waste — which, by the way, is the problem we set out to solve when we created ParkMyCloud.
Third, companies should ensure that their IT and development teams continue their professional development on cloud computing topics, whether by taking training courses or attending local Meetup groups to network with and learn from peers. We have a soft spot in our hearts for our local AWS DC Meetup, which we help organize, but there are great meetups in cities across the world on AWS, Azure, Google Cloud, and more.
Best yet, go to the source itself. Microsoft Azure has a huge events calendar, though AWS re:Invent is probably the biggest. It’s an enormous gathering for learning, training, and announcements of new products and services (and it’s pretty fun, too).
Originally published at www.parkmycloud.com on October 24, 2017.
CEO of ParkMyCloud
CEO of ParkMyCloud
"
https://medium.com/@lightspeedvp/top-5-trends-for-enterprise-cloud-computing-in-2010-cc6db6617574?source=search_post---------317,"Sign in
There are currently no responses for this story.
Be the first to respond.
Lightspeed
Jan 6, 2010·2 min read
by Ravi Mhatre, Lightspeed Venuter Partners
Lightspeed has invested across multiple enterprise infrastructure areas including database virtualization (Delphix), datacenter and cloud infrastructure (AppDynamics, Mulesoft, Optier) and storage virtualization (Fusion I/O, Pliant, Nimble).
This year we wanted to profile several important trends that we see emerging for Cloud Computing in 2010: 1. Enterprises move beyond experimentation with the cloud. Enterprises will start to deploy production cloud stacks with thousands of simultaneous VMs. They will increasingly be used as a resource for both pre-production and production workloads. CIOs and IT managers will test the benefits of creating and managing internal, elastic virtual datacenters — self-service, automated infrastructure with integrated and variable chargeback and billing capabilities, all built on commodity hardware.
2. Management software to deal with scaled cloud environments moves to the forefront. As infrastructure environments become increasingly dynamic and virtualized, the “virtual datacenter” or VDC will emerge as the new enterprise compute platform. New management platforms must be developed to apply policy and automation across thousands of transient servers, fluid underlying storage and networking resource pools, and variable workloads which often need to be dynamically migrated from one part of the VDC to another. Without new management tools, enterprises will fall short in their ability to achieve true “cloud economics” in their cloud environments.
3. Enterprise policy for dealing with public clouds starts to emerge. To counter the security and financial concerns around internal developers using public cloud providers such as Amazon on an ad hoc basis, CIOs and CFOs will start to craft their enterprises’ public cloud policies and centralize purchasing and procurement. Larger enterprises, due to security or compliance restrictions may initially prioritize internal private cloud development to recognize the benefits of cloud computing without compromising their data.
4. Public Clouds; “Its not just about Amazon”. Other mid and large-sized vendors (i.e. Microsoft, IBM, Rackspace, AT&T, Verizon, and others) continue to gain share in this rapidly growing market and 3'd party software matures which enables tier-2 and tier-3 service providers to get into the game of providing cloud services as a complement to traditional web and server hosting. EC2 becomes the commodity service offering as higher-end providers seek to differentiate their cloud offerings with SLA-based premium services and better management capabilities.
5. VMware has to rethink its business model. As Hyper-V,Xen and KVM continue to commoditize the hypervisor and gain enterprise market share, cloud computing starts to encroach on traditional ESX/vSphere use cases for application and server consolidation. Value continues to move up the stack into integrated management features and scale-out application support. To counter enterprise adoption of other hypervisor and cloud over-lay platforms, VMware will be forced to adjust pricing and licensing models to account for scale-out cloud deployments on top of hundreds or thousands of commodity servers.
Serving multi-stage companies on a mission to build the future. Tomorrow, Built Today. http://medium.com/lightspeed-venture-partners
Serving multi-stage companies on a mission to build the future. Tomorrow, Built Today. http://medium.com/lightspeed-venture-partners
"
https://medium.com/processmaker/processmaker-wins-the-2018-stratus-award-for-cloud-computing-as-top-saas-company-773a7650f9c3?source=search_post---------318,"There are currently no responses for this story.
Be the first to respond.
August 31st, 2018
We are very pleased to share that ProcessMaker received the 2018 Stratus Award for Cloud Computing in the “Software as a Service” category.
The Stratus Award from Business Intelligence Group recognizes companies and individuals that are leading innovators in the Cloud. The group looks for solutions that provide a true differentiation in the market.
ProcessMaker was chosen for our unique offering in the Workflow and Business Process Management market. ProcessMaker is the industry’s easiest to use enterprise-grade, SaaS-managed workflow automation and BPM platform. Hundreds of commercial customers running tens of millions of workflow cases per year rely on ProcessMaker to automate their most critical workflow processes.
Workflow automation takes many shapes and formats. There are numerous systems which allow small companies and teams to create very simple forms and workflows. However, today’s enterprises are very complex and have unique needs. These enterprises need an enterprise grade workflow system that can do a number of things:
Workflow automation systems that generally are able to comply with these requirements are just too complicated and clunky to be loved by users. That is where ProcessMaker comes in — robust enough for the enterprise but beautiful enough to be loved by the users.
It’s an honor to have been chosen for the Stratus Award, and we see it as yet another validation from the market regarding our approach. ProcessMaker proves that there IS, in fact, a better way to do workflow and business process transformation for the enterprise.
We are thrilled to receive this award and are excited to share great things that will be announced by ProcessMaker in the months to come. I invite you to sign up for a free trial account or contact us for a free online process modeling building session with one of our experts where we will show you what makes ProcessMaker so special.
Enterprise Workflow Automation Software.
Enterprise Workflow Automation Software. Programmers code less, business analysts do more, and organizations run better. Open source and available on the cloud.
Written by
Former entrepreneur. Director of Product Marketing @processmaker. @AWS Solutions Architect. Builder. Writer. Programmer.
Enterprise Workflow Automation Software. Programmers code less, business analysts do more, and organizations run better. Open source and available on the cloud.
"
https://medium.com/@ashishsharma31/6-options-to-consider-in-enterprise-cloud-computing-43b6b9725793?source=search_post---------319,"Sign in
There are currently no responses for this story.
Be the first to respond.
Aashish Sharma
Dec 14, 2016·3 min read
Enterprise cloud computing options are complex. Each company strives to offer adaptable services to best fit the needs of as many businesses as possible. When you’re a small business or an entrepreneur trying to figure out which cloud solution works for you, you’ll be confronted with lots of options. Determine your most important cloud needs before settling on a provider.
Operating in Linux and Windows, Microsoft Azure is a popular choice for developers who need object storage — that is, for those who need to develop apps in the cloud. Microsoft Azure also offers hybrid cloud services for enterprises that already have servers on premises. If your company already uses Microsoft for just about everything, then integrating with Azure is an easy option.
Google Cloud Storage offers scalable cloud services that integrate with other Google clouds. Some businesses will like that Google offers multiple cloud services depending on what kind of cloud computing an enterprise needs. Others might not enjoy needing to book several Google clouds to do everything they need to do, from storage to app development and more. Google Cloud Storage is a good service for businesses that rely heavily on project management solutions.
As one of the fastest-growing cloud providers, Oracle is always looking to innovate and expand. Oracle CEO Mark Hurd recently highlighted the company’s goals of creating data centers and providing cloud services in India, one of the hottest IT markets on the globe today and one of the most popular outsourcing locations for data entry. Oracle focuses on offering one cloud with all the services you need rather than separate clouds for different applications.
Start with two free gigabytes of space when you sign up for Dropbox. That won’t be enough for most small businesses and isn’t enough even for many individuals, but it is enough for you to decide whether you like the service. Dropbox is a popular cloud service provider for individuals and small businesses because of how easy it is to use. Dropbox provides enterprise solutions, too, but it’s not as popular among big businesses as other cloud providers on this list are.
Amazon S3 stands for Amazon Simple Storage Service and is one of Amazon’s scalable cloud services. With Amazon S3, you pay for the storage you use and scale it up or down depending on what your business needs. Amazon has many cloud platforms, and though they integrate seamlessly, running multiple clouds might be overwhelming for smaller businesses.
Mozy by Dell is the kind of cloud service that businesses go for when they need a little of everything. It offers the enterprise solutions that growing businesses and organizations with lots of telecommuting employees crave, such as user access, administrators, and large amounts of storage.
Many of these platforms might seem to offer similar services at first glance. Researching them further, however, shows that their advantages and disadvantages are distinct. Once you identify the needs of your small business or startup, you’ll be able to pinpoint the cloud computing option that works best for you.
Originally published at Mab Tech Blog.
Aashish Sharma is a Founder and Blogger at https//www.entrepreneuryork.com, specializing in Social Media and Digital Marketing.
Aashish Sharma is a Founder and Blogger at https//www.entrepreneuryork.com, specializing in Social Media and Digital Marketing.
"
https://medium.com/@loginradius/identity-management-in-cloud-computing-bbd048ccb462?source=search_post---------320,"Sign in
There are currently no responses for this story.
Be the first to respond.
LoginRadius
Feb 10, 2021·5 min read
Innovations in the user identity management space have been a trend in the past couple of years. Most of these developments across business and technology fronts have been around identity management in cloud computing, enabling the authentication and authorization processes right in the cloud.
The primary goal of identity management in cloud computing is dealing with personal identity information so that a user’s access to data, computer resources, applications, and services is controlled accurately.
Identity management in cloud computing is the subsequent step of identity and access management (IAM) solutions. However, it is a lot more than merely a straightforward web app single sign-on (SSO) solution. This next generation of IAM solution is a holistic move of the identity provider right to the cloud.
Known as Directory-as-a-Service (DaaS), this particular service is the advanced version of the conventional and on-premises solutions, including Lightweight Directory Access Protocol (LDAP) as well as Microsoft Active Directory (AD).
The following are a few advantages of identity management in cloud computing:
Identity management in cloud computing incorporates all categories of user-base who can operate in diverse scenarios and with specific devices.
A modern cloud Identity and Access Management (IAM) solution helps to:
These abilities help build a platform that connects users to virtually all IT resources through any provider, protocol, platform, or location.
IT admins know that legacy identity management systems (in most cases) struggle when they are matched to cloud services and the likes of AWS.
So, the newest approach to identity management in cloud computing extends your current directory to the cloud with a commanding, easy-to-use SaaS-based solution.
LoginRadius enables businesses to provide an enhanced consumer experience and protects the managed identities. Utilizing the CIAM platform, organizations can offer a streamlined login process, registration, password setup, along with safeguarding consumer accounts and complying with precise data privacy regulations.
LoginRadius enables this by providing open-source SDKs, integrations with more than 150 third-party applications, pre-designed and personalized login interfaces, and superior security products such as RBA, MFA, and Advanced Password Policies. More than 3,000 businesses appreciate the platform with monthly reachability of 1.17 billion users globally.
Compared to conventional deployments with on-premise servers, LoginRadius facilitates everything for its customers, including upgrades, maintenance, data and infrastructure management, security, compliance, and complete privacy with 24/7 technical support.
The LoginRadius identity platform increases the value of businesses by integrating with hundreds of third-party tools. The cloud directory offers everything an engineering team requires to manage consumer data. It enables you to tailor the abilities as needed. However, the platform is API driven, meaning it is easily accessible by developers.
Further, when consumer data is completely locked away across silos, businesses will face multiple challenges. LoginRadius offers integrations to take apart data silos and the challenges that come with them.
The cloud identity platform completely complies with precise privacy regulations and makes things simpler by giving consumer control when the data is entirely centralized.
Identity management in cloud computing is highly critical to your organization. It can persuade the productivity of your employees and the security of your organization. It can also have immense control over what technology solutions you select.
However, IAM solutions have to be supple across identity management and access control in cloud computing to match the current complexities of the computing environment.
If you are locked into some conventional platforms or service providers because of your active directory ad service, explore a vendor-neutral cloud identity management solution.
Originally published at https://www.loginradius.com.
LoginRadius customer Identity management platform serves over 3,000 businesses with a monthly reach of over 1.2 billion users worldwide. https://loginradius.com
See all (387)
LoginRadius customer Identity management platform serves over 3,000 businesses with a monthly reach of over 1.2 billion users worldwide. https://loginradius.com
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/cloud-computing-gets-foggy-5fa0376a9474?source=search_post---------321,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Dec 27, 2017·4 min read
The Internet of Things (IoT) is usually characterized as billions of smart devices, appliances or sensors which are connected to the cloud. The cloud processes and analyzes the data received from these sources to generate valuable insights. For example, the cloud may process data received from your smart car on the kind of music you listen to while driving, which will enable it to suggest a personalized playlist that matches your preferences.
However, for an increasing number of IoT devices, the cloud is not the most effective platform to process the data from these devices. Instead, it is better for the data to be processed much closer to where the devices are physically situated, rather than being transferred to a cloud data center. This method is known as fog computing.
There are certain industries which require the data from their IoT devices to be processed and analyzed instantaneously and cannot afford the delay caused by sending the data to the cloud for analysis and waiting for the insights to come back. For example, in the oil industry, smart sensors on oil pipelines send readings that need to be promptly analyzed since the readings may suggest that oil pressure has unexpectedly increased, which requires the pumps to slow down immediately to avert a catastrophe. In smart vehicle to vehicle communications, the prevention of accidents and collisions would be jeopardized by the latency caused by sending data to the cloud. And of course, in healthcare and medicine, any lag in processing data from smart medical apparatuses could prove fatal.
Fog computing eliminates the latency caused by IoT data making a roundtrip to a cloud data center by processing IoT data locally. The data is analyzed locally by what are known as ‘fog nodes’. Any system with sufficient computing and networking capability can be a fog node. This includes routers, gateways, laptops, or even the smart device itself.
The physical architecture of fog computing is thus similar to the traditional cloud architecture. Both architectures involve IoT devices in a particular location sending data for analysis to either fog nodes or cloud data centers. The difference between the two is that fog nodes are situated in the vicinity of the devices, whereas cloud data centers are located remotely. The architecture of cloud computing is therefore spread out geographically (i.e. data centers and IoT devices are in different locations), whereas the architecture of fog computing is geographically concentrated (i.e. fog nodes and IoT devices are in close proximity).
In addition to eliminating latency for IoT devices that produce time-critical data, fog computing offers other advantages as well. For a start, it reduces the bandwidth burden on cloud data centers. According to one estimate, there will be over 50 billion IoT devices by 2020. Traffic from all of these devices to cloud data centers will push the bandwidth limits of data centers to breaking point. By having some of the data IoT devices generate be processed by fog nodes, cloud data centers are less likely to undergo strain due to excessive traffic.
A further advantage of fog computing is that it enhances the security of any sensitive or confidential data produced by IoT devices, such as medical patient data, by not sending it over the Internet where it may be susceptible to leaks or breaches. Fog computing also makes it harder to for hackers to hijack IoT devices to perform DDoS attacks (such as the Mirai botneck attack in October 2016), because the fog nodes add an additional security layer which hackers have to penetrate before they can take over IoT devices.
Last but not least, fog computing helps with compliance to data regulation laws in certain jurisdictions which prohibit sensitive data being transferred to cloud data centers in other countries.
The term fog computing is an appropriate one, because the word fog is derived from the meteorological concept of a cloud being close to the ground, and in a sense fog computing brings the cloud computing capabilities of data analysis closer to the IoT devices which produce the data. The term fog computing was originally coined by Cisco in 2014.
Fog computing is also sometimes referred to as edge computing, because it involves processing IoT data on the edge of a network, which is located in between an IoT device and the cloud. Companies such as IBM and Akamai use the term edge computing.
While the notion of fog computing was introduced by Cisco in 2014, since then the idea has been embraced by many high profile technology firms. In fact, Intel, Microsoft, Dell and ARM teamed with Cisco and Princeton University to create the OpenFog Consortium in 2015, which has the aim of promoting the architecture of fog computing. Many of these companies have also started to manufacture gateways and routers which can act as fog nodes for IoT devices.
Given this enthusiastic embrace of fog computing, it comes as no surprise that the IDC predicts that by 2019, 45% of IoT-created data will be processed locally.
In conclusion, it’s worth highlighting that fog computing does not replace the cloud for IoT devices, but rather complements the cloud and even makes the cloud more effective. Fog nodes are used for evaluating time-critical or sensitive IoT data, which frees up cloud resources to more effectively perform other tasks, such as analyzing large and complex datasets from IoT sources when time is not of the essence.
Reference:
https://www.alibabacloud.com/blog/Cloud-computing-gets-foggy_p73364
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://yanpritzker.com/storing-your-stuff-online-is-not-cloud-computing-b3e4ea2472a4?source=search_post---------322,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
I’ve noticed people have been saying things like “I am cloud computing because my mail is now on gmail ZOMG”. Storing your mails on the internets is not cloud computing, it’s just online storage. Uploading pics from your phone directly to the web is not cloud computing. Google docs is not cloud computing. Just storing something on the Intarweb does not mean you’re “using cloud computing”. So stop abusing my favorite buzzword :-)
Cloud computing is a far more interesting and far reaching shift than the ability to store your stuff ‘out there’. I think the fundamental principle that defines cloud computing is on-demand resource provisioning. Whether it’s storage or computing power, it means that startups no longer have to spend money up front on data centers. It means that enterprises can save tons of money by not having servers out there idling and burning cash.
And even though there are detractors who will say “cloud computing is grid technology rebranded with a new buzzword”, they are just like the people who said “AJAX is DHTML and we had it in the 90s”. These people are missing the point. Having terminology to describe a phenomenon is a Good Thing. It enables us to easily refer to it and build on top of it. But let’s make sure we understand what’s happening before we apply this new buzzword to every online service under the sun, because then we’re limiting its usefulness.
co-founder swanbitcoin.com. Former CTO -  Reverb.com
Written by
co-founder swanbitcoin.com. Former CTO, Reverb.com.
co-founder swanbitcoin.com. Former CTO -  Reverb.com
Written by
co-founder swanbitcoin.com. Former CTO, Reverb.com.
co-founder swanbitcoin.com. Former CTO -  Reverb.com
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@djabatt/google-also-totally-messed-and-missed-blogging-social-photo-hosting-and-cloud-computing-cf9c9e949bf9?source=search_post---------324,"Sign in
There are currently no responses for this story.
Be the first to respond.
Anthony Batt
Jul 12, 2017·1 min read
Daniel Colin James
Google also totally messed and missed blogging, social, photo hosting and cloud computing. They’re were smart to have purchased youtube but larger failed at capturing the total enterprise value of the content produced on the platform. Amazon has squarely beat them at most innovations. AWS is a huge win with the dev and enterprise community. Additionally Amazon's content is also surprisingly good. Their Amazon studios is making a great content and their streaming music service is solid. I worry about Google as a consumer product in long term. I am not sure driverless cars is the answer. That said they have super smart people working their and I bet they will be making big changes to keep their positions strong.
I’m a curious creative product designer & entrepreneur. Today cofounder of Wevr. In the past President of Katalyst, cofounder of BUZZmedia, Thrash Lab & Metapa.
I’m a curious creative product designer & entrepreneur. Today cofounder of Wevr. In the past President of Katalyst, cofounder of BUZZmedia, Thrash Lab & Metapa.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@TechJobs_NYC/cloud-computing-brings-sprawling-centers-but-few-jobs-to-small-towns-2b7d2e77d82b?source=search_post---------325,"Sign in
There are currently no responses for this story.
Be the first to respond.
Technology Jobs NYC
Aug 26, 2016·1 min read
Cloud Computing Brings Sprawling Centers, but Few Jobs, to Small Towns By QUENTIN HARDY As big companies race to build oversize data networks, communities are finding that the giant buildings can operate with only about 30 people. Published: August 26, 2016 at 06:00PM via NYT Technology http://www.nytimes.com/2016/08/27/technology/cloud-computing-brings-sprawling-centers-but-few-jobs-to-small-towns.html?partner=IFTTT
The official account for https://www.technologyjobs.nyc! Follow for exclusive job opportunities, tips and tricks to help you get your next #technology #job
1 
1 
1 
The official account for https://www.technologyjobs.nyc! Follow for exclusive job opportunities, tips and tricks to help you get your next #technology #job
"
https://medium.com/@jaychapel/cloud-nine-ten-or-eleven-what-all-those-cloud-computing-growth-statistics-really-mean-af65405c0bfb?source=search_post---------326,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Jul 25, 2017·3 min read
Growth in the various cloud platforms has become a dinner party conversation staple of those in the tech industry, in much the same way that house price appreciation was in the mid-2000’s. It’s interesting, everyone has an opinion about cloud computing growth statistics and it’s not entirely clear how it ends.
So let’s start with some industry projections. According to Gartner, the global infrastructure as a service (IaaS) market will grow by 39% in 2017 to reach $35 billion by the end of the year. IaaS growth shows no sign of slowing down and is expected to reach $72 billion by 2021, a CAGR of 30%. This revenue is principally split by the big-four players in public cloud: Amazon Web Services (AWS), Microsoft Azure (Azure), Google Compute Platform (GC) and IBM.
The approximate market share of these four public cloud platforms at the end of the first quarter of 2017 can be seen in the Canalys chart below. The reasons these numbers are only approximate is that each of these vendors include (or exclude) different facets of their cloud business and each seek to ensure their growth remains opaque to the investor community.
However, Amazon reported their earning in April 2017 and showed revenue growing 43 percent in the quarter to $3.66 billion, an annualized run rate of some $14.6 billion. Meanwhile Microsoft reported their cloud earnings in July 2017 and that its annualized revenue run rate was just under $19 billion. However, this includes a lot more than just IaaS and once non-IaaS is removed, analysts suggest that revenue is likely at the $6 billion run rate. Google cloud business is even harder to separate but its cloud revenue was estimated to be some $1 billion at the end of 2015, and although they seem to have hit their stride in the last year or so they clearly have a lot of ground to make. Current estimates are for approximately $2.5 billion in 2017. Lastly, IBM are estimated to be of a similar size to Google but appear to have a lot less momentum than the others, and certainly based on the requests we hear from our customer base IBM is not very often, if ever, referenced.
OK so other than guessing on the winners and losers why does this matter? In our humble opinion, it matters because this scenario creates increased competition and competition is good for consumers. It’s also relevant as companies have a choice, and many are looking at more than one cloud platform, even if they have not yet done anything about it. But what is really interesting, and what keeps us awake at night, is how much of this consumption is being wasted. We think and talk about this waste in terms of three buckets:
1) Always on means always paying — 44% of workloads are classified as non-production (i.e., test, development etc.) and don’t need to run 24×7
2) Over provisioning — 55% of all public cloud resources are not correctly sized for their workloads
3) Inventory Waste — 15% of spend on paying resources which are no longer used.
Combine these three buckets and by our reckoning you are looking at some estimated $6 billion in wasted cloud spend cloud in 2016, growing to $20 billion by 2020. Now that is something to really care about.
Few tools exist to actively monitor and manage this waste, and today there is not a cloud waste management industry per se, and currently tech analysts tend to lump everything under ‘cloud management’. We think that will change in the near future as cloud cost control becomes top-of-mind and the industry is able to leverage cloud computing growth statistics to calculate the scale of this industry problem.If you are in the cloud this is definitely something you should know about. Maybe you should consider optimizing your cloud spend now (before your CTO, CIO or CFO ask you to do so).
Immediate Access No Agents No Downloads Fully Functional
Immediate Access No Obligation Confidentiality Guaranteed No Credit Card Required
Originally published at www.parkmycloud.com on July 25, 2017.
CEO of ParkMyCloud
CEO of ParkMyCloud
"
https://medium.com/pragmatic-programmers/fugue-computing-is-cloud-computing-1b1e3731143e?source=search_post---------327,"Sign in
There are currently no responses for this story.
Be the first to respond.
The Pragmatic Programmers
Jan 28, 2021·3 min read
👈 Chapter 1 Simple Themes and First Steps | TOC | Creating and Deploying Your First Fugue Composition 👉
We just said that you’ll program infrastructure in this chapter. That is, you’ll code directly against the cloud. How? You don’t configure a pile of complex infrastructure. Instead, you declare how the components of your application deploy, scale, and interact using a domain-specific language called Ludwig.
"
https://webeconoscenza.gigicogo.it/cloud-computing-6d521ee3bf10?source=search_post---------328,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Ieri ho passato una bellissima serata con i manager di Google Italia della divisione Enterprise.
Il tema del cloud computing e della Infrastructure as a Service ha scaldato il clima presso il ristorante Le Calandre dove diversi CIO hanno dibattuto, in modalità del tutto informale, sul futuro dell’IT e sulle tendenze in atto.
E’ chiaro che, come ho già elucubrato in questo articolo su eGov, la tendenza a spostare applicazioni e servizi in modalità hosted ha i suoi pro e i suoi contro ma è indubbio che i maggiori player industriali tendono ormai a offrire (se pur in maniera differenziata) soluzioni che, nel breve, potrebbero rappresentare un importante ridimensionamento delle infrastrutture in house.
Certo, il patrimonio dati e il knowledge delle aziende vanno tutelati ma è indubbio che la contrattualistica e gli SLA proposti da aziende come Google, Microsoft (e non solo) hanno il vantaggio diretto e visibile di ridurre i costi di esercizio e di manutenzione. Ma anche la scalabilità è garantita e questo è un fattore trainante verso il concetto EaaS.
Non abbiamo dibattuto esclusivamente di temi così noiosi :-) Sono emersi, infatti, nuovi scenari sulle Google Apps che ancora devono essere divulgati in Italia e su quali tornerò dopo un adeguata sperimentazione.
Alla fine, soddisfatti, e sicuri di reincontrarci, ci siamo portati a casa il libro di Nicholas g Carr sul Big Switch
con la consapevolezza che il futuro dell’IT passa ANCHE per la User Experience.
Blog personale di Gianluigi Cogo
Written by
https://www.gigicogo.it
Blog personale di Gianluigi Cogo
Written by
https://www.gigicogo.it
Blog personale di Gianluigi Cogo
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/emerging-technologies-india/cloud-computing-whats-new-cloud-service-brokerage-deca87141da3?source=search_post---------329,"There are currently no responses for this story.
Be the first to respond.
As a part of infrastructure practice team we have been watching what’s happening in the world of cloud computing. A lot many people were implicated with the support and integration services offered by various cloud vendors, and by looking at cloud vendor’s vision around support, security and integration it always appears that they are not going to fill all the missing pieces of the picture that cloud consumers want to see in a service provider. Cloud of doubt no more lingers on if you notice that National Institute for Standards and Technology (NIST) has now added cloud brokerage to its definitions for cloud computing
If you are looking for some standards here you might need to wait for a long long time, but if you are ready to experiment that you will use one cloud offering as a backup of other, or rapid deploy some of your workload from one cloud to another, then cloud brokerage looks a good approach for the future. You could also call it an intermediary service provider who involves customizing services from multiple vendors.
Image Source: nist.gov
Cloud Broker: An entity that manages the use, performance and delivery of cloud services, and negotiates relationships between Cloud Providers and Cloud Consumers.
As cloud computing evolves, the integration of cloud services can be too complex for cloud consumers to manage.
A cloud consumer may request cloud services from a cloud broker, instead of contacting a cloud provider directly.
A cloud broker is an entity that manages the use, performance and delivery of cloud services and negotiates relationships between cloud providers and cloud consumers.
In general, a cloud broker can provide services in three categories :
Service Inter-mediation: A cloud broker enhances a given service by improving some specific capability and providing value-added services to cloud consumers.
The improvement can be managing access to cloud services, identity management, performance reporting, enhanced security, etc.
Service Aggregation: A cloud broker combines and integrates multiple services into one or more new services.
The broker provides data integration and ensures the secure data movement between the cloud consumer and multiple cloud providers.
Service Arbitrage: Service arbitrage is similar to service aggregation except that the services being aggregated are not fixed.
Service arbitrage means a broker has the flexibility to choose services from multiple agencies.
The cloud broker, for example, can use a credit-scoring service to measure and select an agency with the best score
Source:nist.gov
With cloud broker, you can deliver all your services and you can make sure that your corporate security policies will be enforced, regulatory compliance will be met, auditing will be done on all cloud vendors. You can also include standard services like reporting, performance and utilization trends etc. In the world that is hungry for connected services and machine-to-machine communications, cloud services broker can meet multiple needs, Cloud Service Brokerage will definitely help in scenarios that involve multiple cloud service providers, additionally it can really proliferate the fear of complex inter-dependencies among various SLAs.
We will see Gartner and Forrester talking a lot about Cloud Service Brokerage as a new business model paradigm -as cloud adaption evolve during the next five years,and i might not be exaggerating when i say that in coming years all traditional IT service providers will be competing for top position in Cloud Service Brokerage.
Let us know what are your views around Cloud Brokerage Service.
Emerging Technologies India
Written by
Exponential Thinker, Lifelong Learner #Digital #Philosophy #Future #ArtificialIntelligence https://FutureMonger.com/
Emerging Technologies India
Written by
Exponential Thinker, Lifelong Learner #Digital #Philosophy #Future #ArtificialIntelligence https://FutureMonger.com/
Emerging Technologies India
"
https://medium.com/the-technews/microsoft-to-contribute-1-billion-of-cloud-computing-services-to-charity-8fd42ee82715?source=search_post---------330,"There are currently no responses for this story.
Be the first to respond.
Microsoft reported today another arrangement to give $1 billion worth of cloud computing services to not-for-profits, foundations, and colleges throughout the following three years, close by another arrangement to convey broadband to underserved regions.
The thought, clarifies Microsoft CEO Satya Nadella in a blog entry, is to offer access to the power managed by public cloud stages like Microsoft Azure even to those not-for-profits and foundations who generally won’t not have the capacity to bear the cost of it.
Nadella says:
“If cloud computing is one of the most important transformations of our time, how do we ensure that its benefits are universally accessible? What if only wealthy societies have access to the data, intelligence, analytics and insights that come from the power of mobile and cloud computing.”
Those cloud administrations incorporate the Microsoft Azure processing cloud and the Office 365 cloud efficiency suite, both major Microsoft items and the center of a great part of the organization’s system. This activity will be managed by the organization’s recently shaped Microsoft Philanthropies branch.
In another blog entry, Microsoft President and Chief Legal Officer Brad Smith sets out the objective of serving 70,000 non-legislative associations (NGOs) in the first year of this system, giving administration’s “with an honest estimation of near $350 million.”
“Taken together we believe these steps will ensure that nonprofit organizations and university researchers around the world obtain the access they need to pursue cutting-edge solutions to the world’s most pressing problems,” writes Smith.
A technology media that aims at latest technews, tech…
A technology media that aims at latest technews, tech events, gadgets, smartphones, tools, innovations, startups and many more…
Written by

A technology media that aims at latest technews, tech events, gadgets, smartphones, tools, innovations, startups and many more…
"
https://medium.com/ipg-media-lab/cloud-computing-takes-us-by-storm-d39ff244340d?source=search_post---------331,"There are currently no responses for this story.
Be the first to respond.
April 1, 2009–There’s been a great deal of discussion lately about cloud computing. However, many people don’t understand what cloud computing really means, what it can do, and why it is so important. To address these issues, the lab has put together a handy new guide. First though, one needs to understand the ideas behind cloud computing.
About ten years ago, a number of computer engineers realized that they would soon fill the tubes connecting the Internets with things like email spam, viruses, and bittorrents. Our tubes were going to become sewers. Something had to be done.
To really understand the brilliance of cloud computing, one needs to take a moment to look up at the sky. It’s big, it’s blue, and generally speaking it’s empty, except for those big puffy cloud things. And there’s the opportunity. Rather than running data through the tubes in the ground, we can run it through the clouds. The best type of cloud computing is Nimbus. There’s tons of storage, and allows widespread access.
Cumulus is another highly desired cloud computing solution, as it can be densely packed with processing power. Thin and wispy clouds are less than ideal, and can prove problematic if relied on as primary infrastructure.
However, there are certain obstacles cloud computing needs to rise above. High winds can prove devastating to cloud arrays. Also, if clouds become too saturated, data leakage is very likely. The trick to effective cloud computing is linking in via very high towers, or on mountains. Also, maintenance of these rigs can be difficult, so finding the right talent is key. For cloud computing, it’s best to hire Care Bear sys-admins, as their overall performance is unmatched.
It’s time to put the umbrella away — cloud computing is on its way, and here to stay.
…Happy April Fools! Affectionately, the Lab.
The media futures agency of IPG Mediabrands
The media futures agency of IPG Mediabrands
Written by
Keeping brands ahead of the digital curve. An @IPGMediabrands company.
The media futures agency of IPG Mediabrands
"
https://sahilparikh.com/gmails-fiasco-it-is-not-cloud-computing-to-blame-b9237d5cfdd1?source=search_post---------332,NA
https://medium.com/@toddbnielsen/cloud-computing-the-fantasy-the-facts-and-truth-the-marketing-ignores-94bd71134c7a?source=search_post---------333,"Sign in
There are currently no responses for this story.
Be the first to respond.
Todd Nielsen
Jun 11, 2020·12 min read
Todd Nielsen: Okay. Welcome everyone to the Business Innovation Technology Experience. This is Todd Neilsen with JMARK Business Solutions and we have Kristina Coons, Thomas Douglas and Dax Bambrough here today. And for this episode we’re going to talk about, another great topic, the cloud computing hype that exists in the world. A few years ago there was data that cloud computing was going to essentially take over everything and we’ve come to learn that some of that hype isn’t quite accurate. So how would we describe what is cloud computing? How would we describe exactly what that is? Because it’s kind of a nefarious, complicated topic, muddy.
Thomas Douglas: Yeah, absolutely. Well so first, let’s understand that cloud computing has really been retitled in the last few years. Cloud computing, the concepts about it, have been around for well over 10 years, maybe 15 or 20 years. If you go all the way back to the green screens of banks when you used to walk in, that effectively was cloud computing. It’s where you take a single set of resources and you put them in a data center and then you deliver a component of technology out to consumers of that technology. So the idea of cloud computing has been around for a really long time and the banks were really the first ones to embrace it fully because it costs too much money to put those resources in every single bank.
Thomas Douglas: That evolved into insurance and that evolved into many other industries. So the basic tenant of cloud computing is when you have a set of resources that can be utilized to deliver a set of services to multiple entities and that you can use more or less of those resources at any given time based on the needs of the application or the environment or the business or whatever it is. So those shared resources create the ability to get the most out of the technology dime spent rather than having to invest everything inside your organization to facilitate that same capability.
Todd Nielsen: So why do we have so many people that think that they’ve got to move to the cloud? I mean, hat’s the attraction and the pitfalls, so to speak? I mean, it’s quite surprising actually.
Todd Nielsen: Yeah. Well it’s like anything else. It all starts with marketing. When you’ve got good marketers and you’ve got a great set of individuals and companies that are going, “You’ve got to have cloud, you’ve got to have cloud.” Everybody thinks that they’ve got to have clouds. It’s like when the new Air Jordan comes out, it’s like, ah. I guess I’m getting old if I’m referencing Air Jordan’s, huh? There’s new ones now. But when it comes to the next phase of that, beyond the marketing piece of it, is kind of the fear. People think that because of the security challenges and issues of today’s world, that if they take their environment, their data, and they move it to a cloud environment, that that makes it impervious to security breaches or threats or whatever it may be, which is also false. One of my mentors said that that unlike actual clouds, the decision to move to a cloud is not weightless and I believe that that’s absolutely true, that it’s a huge decision. It’s quite complicated. It requires tons of consideration and due diligence to ensure that it’s done right.
Kristina Coons: Can you talk about a reason why a company would not want to adopt cloud computing?
Thomas Douglas: Yeah. So the number one reason that we take people out of the cloud who have tried it is because the lack of integrations in the applications or the compatibility itself. So if you think about the average small business that utilizes anywhere from say three to 10 core applications that help the business to operate, it’s necessary sometimes for many of those applications to communicate with one another. And if you have one application that’s ready to go to the cloud and another application that’s not because it requires a certain database or it requires a certain design or a certain amount of bandwidth inside the business, then it becomes very challenging to make application A talk to application B and therefore everything begins to break down. So that’s probably the number one reason. But the number two reason is the experience for the end user, that they haven’t done enough due diligence to make sure that if it’s going to be moved into that environment that that they’ve tested it and they verified that it is okay to be delivered through a cloud environment.
Thomas Douglas: And in particular, a specific style cloud environment. People years ago, when we were dealing with traditional computing, we thought it was complicated, but when you break it down it was actually kind of simple. You had a server, you had a switch and you had a workstation. It was really easy to make those three things communicate. In today’s world, you’ve got the workstation that has to go through the switch that has to go through the server that then communicates to the firewall that then has to go out on the internet that then has to make it to the security protocol to make it into the cloud environment and then some random data center that we don’t have any control over and then we have to go all the way back to make the experience really positive. So there’s a great deal of testing involved in order to make sure that the experience for the end user is done well.
Thomas Douglas: Not to say the cloud shouldn’t be used. There’s environments where it’s awesome but it’s not the end all be all. What we have shared with our clients is almost every business that is going to operate for the next 10 years is going to operate in a hybrid environment. There’s going to be some applications that were born in the cloud, that are delivered from the cloud and everything’s awesome. There’s some applications that need to live on prem and there’s some applications that need to move information back and forth and utilize both. And the important part is that you have a CTO, a strategist, a technologist who can come in and help you to identify what the right platforms are for the right applications to create the experience that you want for your business.
Todd Nielsen: Yeah, I think it’s important to understand a little bit behind the complexity of cloud computing because in the early days, a lot of technology professionals really took cloud computing and went, “Oh, that’s just a server and a data center. I can do that.” And we went through this round and round where we had distributors that would start a cloud program and they thought that would be the thing to do. And most of the distributor’s cloud are now gone. And we had manufacturers, Dell, HP, IBM, that were throwing out cloud platforms and their platform has completely evolved because they were trying to make an end all for everybody. And then you have the MSPs and the IT companies and entrepreneurs who thought I’m just going to throw some servers in a data center and call it cloud.
Todd Nielsen: I know of at least, I don’t know, a dozen experiences where IT companies sent out a message to their clients saying, “We’re closing in 30 days. Get your data off.” Because it’s really complex. It’s not just throwing a data in a data center or throwing a server in a data center. There’s just so much that has to interact. You have potentially printing issues up the ying-yang when stuff is off site at a different place. There’s applications that have to talk to each other and it’s like you said, we’re not putting down cloud computing. Cloud computing absolutely is wonderful in the right circumstance, but for the end all, let’s move all of our data to the cloud, all of our applications, have nothing on site, have dumb terminals sitting in our office, that idea rarely works.
Thomas Douglas: You’re exactly right it. It can be done and for businesses that start from the very beginning with that decision made, so they’re only going to consider applications that run from a cloud environment that are delivered through that environment that are compatible with other applications. It can be a functional environment, but what we find is almost always there comes a day where you need to have local computing for certain things. CAD is a good example of that where you can do some things in a cloud environment in terms of storing your data and synchronizing your data back and forth. You need a high powered workstation to be able to do CAD and heavy building design and some of those kinds of things. And so there are fundamental environments that they work well and there are fundamental environments that do not, and it’s up to a technologist to work their way through that environment, an expert who understands those dynamics and design an intentional environment to create those experiences. And it’s certainly not easy, but it can be done and it shouldn’t just be looked at as the end all be all like you described.
Todd Nielsen: Yeah. And going back to the definition, as we’re talking about this, I mean just on the marketing team at JMARK, I mean we have dozens of applications that are cloud based. There is various cloud applications used throughout the company. Back five years ago, all of these cloud applications were called SAS, software as a service. So it’s just a transformation and I think, to be honest, that is the place where it makes the most sense for companies to look for. There are times when it’s better to have an application. Look at the growth of Salesforce. Look at the growth of some of these big companies that offering CRMs online and that’s what I think where the real key is. But taking your applications that are just your Windows-based application, server-based applications and trying to operate them in a cloud environment can spell a lot of heartache.
Thomas Douglas: Yeah, there are new technologies emerging. Microsoft launched their Virtual Desktop environment this past year and it may have the potential to really help what it means to be able to do what you just described when you take a Windows-based cloud server application and move it into a hosted environment of some kind. And I expect that we’ll see more of that. But as it stands today, it’s the hybrid that is the magic sauce. It’s where you can use the right technology for the right purpose. No different than you would use a car for the right purpose. You don’t drive a school bus to work every day because you don’t need a school bus to drive it to work. You get in a sedan or a truck or whatever it is.
Thomas Douglas: And so it’s about using the right technology to solve the right problem and a hybrid environment facilitates that. When you need to move something up so that you can crank on it, utilize artificial intelligence from IBM or from Google or utilize a particular service from Microsoft, you have the capability of doing that. But there are certain things that need to come down and become local and be done right in front of you or in a local server environment. So again, it’s really about being intentional in the design so that everybody can be successful.
Todd Nielsen: So let’s talk about the idea of being ready to move to the cloud. So how do you know if you are actually ready? If your organization is actually mature enough to move to the cloud?
Thomas Douglas: It’s a great question. I believe that it starts with a cloud readiness assessment. So you have to understand the technology and the dynamics of what’s in the environment and that includes the bandwidth and the redundancy of the bandwidth. So if you’re going to utilize a cloud application, in many organizations still there’s only one path to the internet and that has to immediately change so that you have a fallback so that you can get to the internet in multiple paths and in the application compatibility and then, do you have the technology at the desktop or the user level that interfaces with the cloud environment properly?
Thomas Douglas: The workstation oftentimes is the window to the cloud, as funny as that sounds, but that’s the truth. And so is your network designed intentionally to use both the local resources and the cloud environments and if not, why? A perfect example of that is if you’re running a Windows 10 Home Edition on your workstation, you are not cloud ready. You need to have Windows 10 Professional Edition, you need to have a domain, you need to be federated with Active Directory and Office 365 or G Suite. And if you’re not doing those things, then you’re going to have major compatibility issues. So a cloud readiness assessment is step one to determine if a business can utilize cloud technologies and if the answer to that is yes, then it becomes how?
Todd Nielsen: Yeah. And we use this idea of maturity in our cloud readiness assessment, measuring operational maturity, the backup and restoration, the disaster recovery network infrastructure, software, compliance and security and all of these things, they aren’t necessarily a negative to an organization. It’s not about necessarily even being mature enough to move to the cloud. It often depends on just the dynamics of the business and what types of applications are used. And sometimes it’s even the geography of where you are and how big your pipe is and what it can hold. So I agree that understanding cloud readiness and not just taking the leap. I remember working with you on a supermarket, I think it was a couple of years ago, we did a cloud readiness assessment for, and ultimately through that they realized that it was totally the wrong move.
Thomas Douglas: Yeah, absolutely. When you start to dive into it, one of the first things that can kill cloud is the economics. While there are some things that are less expensive in a cloud environment, there are others that are much more expensive. And until you know your environment and how much it’s going to cost to run your environment in a cloud environment, the assessment may uncover the fact that that by moving, you’re actually going to double the cost of what it means to run it for your organization. And that’s another reason that we’ve pulled some organizations out is because it’s utilitarian. In other words, you build by how many cycles, how much compute you’re using, how much storage you’re using and if you don’t have a good appreciation for that before you get there, it’s like the electric meter just spinning out of control and your bill climbing and climbing. You’ve seen a lot of businesses who’ve got into an environment where their costs of cloud just spiraled literally out of control and they had to pull it back and put some governors in place to make sure that that wouldn’t happen anymore.
Todd Nielsen: The last thing that I think we should talk about, because I think we will have many, many episodes on cloud computing, on pros and cons and different things we can talk about, but there is a dirty little trick happening in the MSP industry on cloud computing and that is that there are some IT companies out there that have figured out a decent to mediocre way to do cloud and then they will move their clients up there and they will essentially never be able to get out. It’s kind of a sticking point. Can you talk a little bit more about that? You’ve talked to a lot more of these organizations than I have.
Thomas Douglas: Yeah, it’s a really dangerous place to be. And one of the first pieces of advice that I would give any business that’s considering moving to a cloud environment is move to one that is a universal cloud platform, one of the big boys, so Microsoft, Google, Amazon, one of those. Because when you get into a proprietary environment and you need to move, then it becomes extremely painful to get out of it. We’ve actually moved a few organizations in that and it took us six months to settle the network down, if you will. So we were able to move all of the data and to recover everything within a matter of weeks that was carefully planned and executed really well. But the network itself was designed to run in that environment and then that provider provided a horrible client experience.
Thomas Douglas: They got mad at him and so it was this hostile environment for the customer to be in. It was just a horrible situation. And so getting trapped, if you will, into that proprietary data center style environment can be the demise of a business. It could literally run an organization out of business if they’re not careful. So when considering the partner, when considering the platform and the environments, the agreements to be involved with, make sure that it’s one that’s portable from one managed service provider, one technologist to another, so that you’re not stuck in that situation.
Todd Nielsen: Good advice. And as always, we’ll post information on cloud computing and links to other resources when we post this episode. Until then, take care.
Thomas Douglas: All right, thank you everybody.
Kristina Coons: Bye.
Originally published at https://www.jmark.com on June 11, 2020.
I create miracles of profitability & success by forming cultures that get things done! I love leadership, building success ...always seeking greater wisdom.
See all (5,525)
I create miracles of profitability & success by forming cultures that get things done! I love leadership, building success ...always seeking greater wisdom.
About
Write
Help
Legal
Get the Medium app
"
https://yanpritzker.com/a-good-week-for-cloud-computing-aa06817737c6?source=search_post---------334,"Amazon offers 99.95% SLA
Amazon announced today that it was exiting beta and offering a 99.95% SLA within a region. Hopefully this is going to put some cloud naysayers to rest, at least on the reliability front. Amazon is offering accountability in the form of service credits if it violates the SLA. Now this may not be enough for some of you with mission critical applications, but my guess is that most people out there are going to be just fine with it, considering the cost savings that on demand cloud infrastructure provides. Oh yeah, they’re going to be offering Windows servers too. This should make some dot-net-heads pretty happy.
There are already hundreds (thousands?) of companies taking advantage of Amazon EC2 computing resources, and those that aren’t are going to catch up real quick, especially as they realize how much money they are wasting on static server resources that are mostly sitting around idling. You just can’t afford that, not in this economy. Companies are going to wise up and start cutting costs on non critical infrastructure and pushing it into the cloud. And as they gain trust for the cloud, pieces of critical infrastructure are going to follow.
Rackspace building weapons of mass destruction
Rackspace just acquired Slicehost and JungleDisk in what appears to be an effort to shore up its arms race against Amazon. They are still pretty far behind true cloud infrastructure (by this I mean on-demand api-driven resource allocation) but maybe Slicehost can make this happen for them. I’ve been a loyal Slicehost customer for close to two years now, and they’ve declined to accept uploaded virtual images thus far, but maybe that will change. See below for why Slicehost isn’t really a cloud, yet.
Your VPS ain’t a Cloud
Many ‘cloud’ vendors are still just rebranded VPSs. We’ve had virtualized infrastructure in hosting companies for years. What makes a true cloud like Amazon EC2 is that it only takes a credit card and a minute to get computing resources. The other key is that manual tweaking and hand provisioning are going the way of the dinosaur. You need to be able to get a new server up and running with your latest environment and software in minutes, not hours, days, or weeks.
Hosting solutions that require you to first acquire resources by booting up an image and then installing your software are going to be left in the dust. Amazon lets you upload a virtual image you create, which means you can mange your own image catalog, and if you’re using something like Elastic Server then you can dynamically provision your servers from recipe templates that ensure the quick reproducibility of your stack to any virtual format, whether it’s in your datacenter, or up in the cloud.
In 2009 we’re going to start to see companies moving to virtual and cloud infrastructure and dynamic provisioning to cut costs and gain agility. It’s going to be an interesting year.
co-founder swanbitcoin.com. Former CTO -  Reverb.com
Written by
co-founder swanbitcoin.com. Former CTO, Reverb.com.
co-founder swanbitcoin.com. Former CTO -  Reverb.com
Written by
co-founder swanbitcoin.com. Former CTO, Reverb.com.
co-founder swanbitcoin.com. Former CTO -  Reverb.com
"
https://medium.com/@jaychapel/interview-qcentive-saves-significant-amounts-on-aws-while-enabling-cloud-computing-in-healthcare-ef441ca010a?source=search_post---------335,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Mar 8, 2018·6 min read
We talked with Bill Gullicksen, Director of IT at Qcentive, about how the company is using ParkMyCloud to save money on their AWS costs while enabling cloud computing in healthcare.
We are a 2-year-old healthcare startup founded through the venture capital arm of Blue Cross Blue Shield of Massachusetts (BCBSMA). We build systems for the healthcare industry to help reduce costs in healthcare and provide efficiencies. Our cloud-based payment platform will facilitate the development and management of value-based contracts between healthcare companies. We are excited to be one of the earliest vendors authorized to take healthcare information and move it securely to the cloud.
Healthcare has historically been cloud-averse due to issues like privacy and security concerns. In order to prove the use case for cloud computing in healthcare, we needed to build out a prototype and go through many months of meetings–producing artifacts to prove that we could move data to the cloud in a HIPAA/HITECH-compliant and secure manner.
We’ve recently released our first prototype module of the application by taking years of patient and healthcare contract information, loading it all into AWS, and then putting our application on top of it. Our application allows our health plan customers and their value-based contracting provider partners to analyze healthcare claim records, emergency room visits, etc. and to quickly calculate how to potentially realize savings in those areas.
We had a few architects just going to town on AWS during the first year we were in business. They were building away, and all of a sudden our monthly AWS costs began to ramp up. We were spending a lot of money on Amazon and we didn’t even have a working application yet!
Last summer I was put in charge of our AWS operations and was asked to address our AWS costs. I asked, “what can we do to get some of these costs under control?” We started out with some rightsizing exercises and scaled some stuff back and that got us some savings. We found areas where we have had some stability and used Reserved Instances there, allowing us to get a 30–40% discount, but we didn’t want to do long-term commitments so we only did those for a year.
For the remaining instances, I realized we pay by the minute and we really don’t need to be running instances 24/7. That’s that’s when we started thinking about how to schedule instances to shut down. I could do that and turn them off with AWS tools, but then telling an instance to turn itself back on at 6 in the morning–I didn’t have a way to do that. And that’s when I found out about ParkMyCloud and said this looks perfect — we can schedule instances to get them running 12 hours a day, 5 days a week instead of 24/7 and we’ll probably cut our costs in half.
ParkMyCloud was the perfect tool for what we needed at the time and it also gave us a side benefit where we could give developers, QA people, and even data analysts and business folks the ability to turn an instance off when they’re done, or turn it on without having to write a bunch of complex policies within AWS.
Before, if we only wanted certain people to be able to manipulate a handful of instances, I had to put those instance IDs in the policies. Instance IDs frequently change, so running custom policies was taking a lot of overhead and we got the benefit from ParkMyCloud of just assigning them teams. Now, whether the instance IDs change or not, there’s no extra work for our IT team.
That’s why we chose ParkMyCloud and why we’ve been using it for 6–7 months now. For me it was great, very simple to set up, simple to use, easy for non-technical users and with very little effort from me and my technical staff, so it’s been perfect.
Those are the bulk of it. We have other cloud-tracking subscriptions that we use sometimes. They are very simple but I just use it for looking at the daily spend, seeing if there’s any unexpected spikes, things like that. I can use it for finding resources that are no longer being used. It’s nice to have for identifying orphaned volumes and gives me a simple, easy way to clean some of that up, but we get our biggest use out of ParkMyCloud.
We’ve taken some schedules off just to keep some systems up for a while, but our rule of thumb has been to put a schedule and a team on everything. Even if a schedule is running 24/7/365, we want to at least have a schedule on it and know that it’s a conscious business decision we made to keep that up versus “it just slipped through the cracks and we never looked at it.”
Somewhere around 15 or so users.
I’m Director of IT and we’ve got a Director of DevOps and a DevOps engineer–we are the three technical resources around infrastructure. Then we’ve got around 10 or so software developers that all have access so they can spin up their dev environments and spin them down when they’re not working.
We have a flexible schedule. Some of our software developers do their best coding at 3 in the morning. If they get up with an idea and they want to code, they need the ability to start up instances, do what they need to do, and then turn them off when they’re done. So they’re all in there, our QA department and some business analysts that do a lot of data analysis and database querying are also using ParkMyCloud.
Our initial savings with ParkMyCloud were significant and the product paid for itself quickly. Based on business needs, our costs can escalate rapidly so we estimate we’re saving up to 20% on our costs on a monthly basis.
We’ve got a lot of instances that we keep normally parked now and we only turn them on when there’s a workload to run. And then we’ve got probably another 40 or 50% of our instances that only run Monday through Friday, from 7:00 AM to 7:00 PM, so we’re getting that savings there which to me is bigger savings than dealing with Reserved Instances.
Things like Reserved Instances look great the day you buy them, but then the first time you have to change the size on something, all of the sudden you’ve got Reserved Instances that you’re not using anymore. With ParkMyCloud that never happens, it’s all savings.
We were interviewing an external technology company, G2 Technologies in Boston last summer that was being brought in to augment our CI/CD process. While they were in we asked, “hey, do you know any good methods for doing scheduling?” — and they said take a look at ParkMyCloud.
I was surprised how simple ParkMyCloud was to get up and running. It was a couple of hours from signing up for the trial to having most of the work done and realizing savings, which was great. The release of your mobile app has been fantastic because it’s nice if we need to turn something on for somebody that doesn’t have access on a Saturday when I’m 30 miles away from my computer. I can do it anywhere with the mobile app.
You’re welcome!
Originally published at www.parkmycloud.com on March 8, 2018.
CEO of ParkMyCloud
CEO of ParkMyCloud
"
https://medium.com/@TechJobs_NYC/google-races-to-catch-up-in-cloud-computing-d78bb753cd1c?source=search_post---------336,"Sign in
There are currently no responses for this story.
Be the first to respond.
Technology Jobs NYC
Jul 25, 2016·1 min read
Google Races to Catch Up in Cloud Computing By QUENTIN HARDY Analysts say the global cloud-computing business will be worth $67 billion by 2020. Amazon and Microsoft are the current leaders in the industry. Published: July 24, 2016 at 06:00PM via NYT Technology http://www.nytimes.com/2016/07/25/technology/google-races-to-catch-up-in-cloud-computing.html?partner=IFTTT
The official account for https://www.technologyjobs.nyc! Follow for exclusive job opportunities, tips and tricks to help you get your next #technology #job
The official account for https://www.technologyjobs.nyc! Follow for exclusive job opportunities, tips and tricks to help you get your next #technology #job
"
https://medium.com/@lightspeedvp/top-5-trends-for-enterprise-cloud-computing-in-2010-c1fa009fdcb8?source=search_post---------337,"Sign in
There are currently no responses for this story.
Be the first to respond.
Lightspeed
Jan 6, 2010·2 min read
by Ravi Mhatre, Lightspeed Venuter Partners
Lightspeed has invested across multiple enterprise infrastructure areas including database virtualization (Delphix), datacenter and cloud infrastructure (AppDynamics, Mulesoft, Optier) and storage virtualization (Fusion I/O, Pliant, Nimble).
This year we wanted to profile several important trends that we see emerging for Cloud Computing in 2010: 1. Enterprises move beyond experimentation with the cloud. Enterprises will start to deploy production cloud stacks with thousands of simultaneous VMs. They will increasingly be used as a resource for both pre-production and production workloads. CIOs and IT managers will test the benefits of creating and managing internal, elastic virtual datacenters — self-service, automated infrastructure with integrated and variable chargeback and billing capabilities, all built on commodity hardware.
2. Management software to deal with scaled cloud environments moves to the forefront. As infrastructure environments become increasingly dynamic and virtualized, the “virtual datacenter” or VDC will emerge as the new enterprise compute platform. New management platforms must be developed to apply policy and automation across thousands of transient servers, fluid underlying storage and networking resource pools, and variable workloads which often need to be dynamically migrated from one part of the VDC to another. Without new management tools, enterprises will fall short in their ability to achieve true “cloud economics” in their cloud environments.
3. Enterprise policy for dealing with public clouds starts to emerge. To counter the security and financial concerns around internal developers using public cloud providers such as Amazon on an ad hoc basis, CIOs and CFOs will start to craft their enterprises’ public cloud policies and centralize purchasing and procurement. Larger enterprises, due to security or compliance restrictions may initially prioritize internal private cloud development to recognize the benefits of cloud computing without compromising their data.
4. Public Clouds; “Its not just about Amazon”. Other mid and large-sized vendors (i.e. Microsoft, IBM, Rackspace, AT&T, Verizon, and others) continue to gain share in this rapidly growing market and 3'd party software matures which enables tier-2 and tier-3 service providers to get into the game of providing cloud services as a complement to traditional web and server hosting. EC2 becomes the commodity service offering as higher-end providers seek to differentiate their cloud offerings with SLA-based premium services and better management capabilities.
5. VMware has to rethink its business model. As Hyper-V,Xen and KVM continue to commoditize the hypervisor and gain enterprise market share, cloud computing starts to encroach on traditional ESX/vSphere use cases for application and server consolidation. Value continues to move up the stack into integrated management features and scale-out application support. To counter enterprise adoption of other hypervisor and cloud over-lay platforms, VMware will be forced to adjust pricing and licensing models to account for scale-out cloud deployments on top of hundreds or thousands of commodity servers.
Serving multi-stage companies on a mission to build the future. Tomorrow, Built Today. http://medium.com/lightspeed-venture-partners
Serving multi-stage companies on a mission to build the future. Tomorrow, Built Today. http://medium.com/lightspeed-venture-partners
"
https://medium.com/@jaychapel/the-cost-of-cloud-computing-is-in-fact-dropping-dramatically-85a09280cc92?source=search_post---------338,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Jan 23, 2018·5 min read
You might read the headline statement that the cost of cloud computing is dropping and say “Well, duh!”. Or maybe you’re on the other side of the fence. A coworker recently referred me to a very interesting blog on the Kapwing site that states Cloud costs aren’t actually dropping dramatically. The author defines“dramatically” based on the targets set by Moore’s Law or the more recently proposed Bezos’ Law, which states that “a unit of [cloud] computing power price is reduced by 50 percent approximately every three years.” The blog focused on the cost of the Google Cloud Platform (GCP) n1-standard-8 machine type, and illustrated historical data for the Iowa region:
Date N1-standard-8 Cost per Hour January 2016 $0.40 January 2017 $0.40 January 2018 $0.38
The Kapwing blog also illustrates that the GCP storage and network egress costs have not changed at all in three years. These figures certainly add up to a conclusion that Bezos’ Law is not working…at least not for GCP.
If we turn this around and try to apply Bezos’ Law to, well, Bezos’ Cloud we see a somewhat different story.
The approach to measuring AWS pricing changes needs to be a bit more systematic than for GCP, as the AWS instance types have been evolving quite a bit over their history. This evolution is shown by the digit that follows the first character in the instance type, indicating the version or generation number of the given instance type . For example, m1.large vs. m5.large. These are similar virtual machines in terms of specifications, with 2 vCPUs and about 8GB RAM, but the m1.large was released in October 2007, and the m5.large in November 2017. While the “1” in the GCP n1-standard-8 could also be a version number, it is still the only version I can see back to at least 2013. For AWS, changes in these generation numbers happen more frequently and likely reflect the new generations of underlying hardware on which the instance can be run.
In any event, when we make use of the Internet Archive to look at pricing changes of the specific instance type as well as the instance type “family” as it evolves, we see the following (all prices are USD cost per hour for Linux on-demand from the us-east-1 region in the earliest available archived month of data for the quoted year):
*Latest Internet Archive data from Dec 2017 but confirmed to match current Jan 2018 AWS pricing.
FWIW: The second generation m2.large instance type was skipped, though in October 2012 AWS released the “Second Generation Standard” instances for Extra Large and Double Extra Large — along with about an 18% price reduction for the first generation.
To confirm that we can safely compare these prices, we need to look at how the mX.large family has evolved over the years:
Instance type Specifications m1.large (originally defined as the “Standard Large” type) 2vCPU w/ECU of 4, 7.5GB RAM m3.large 2vCPU w/ECU of 6.5, 7.5GB RAM m4.large 2vCPU w/ECU of 6.5, 8GB RAM m5.large 2vCPU w/ECU of 10, 8GB RAM
A couple of notes on this:
The net average of the 3-year reduction figures is -58% per year, so Bezos’ Law is looking pretty good. (And there is probably an interesting grad-student dissertation somewhere about how serverless technologies fit into Bezos’ Law…) When you factor the m1.large ECU of 4 versus the m5.large ECU of 10 into the picture, more than doubling the net computing power, one could easily argue that Bezos’ Law significantly understates the situation. Overall, there is a trend here of not just a significantly declining prices, but also greatly increased capability (higher ECU and more RAM), and certainly reflecting an increased value to the customer.
So, why has the pricing of the older m1 and m3 generations gone flat but is still so much more expensive? On the one hand, one could imagine that the older generations of underlying hardware consume more rack space and power, and thus cost Amazon more to operate. On the other hand, they have LONG since amortized this hardware cost, so maybe they could drop the prices. The reality is probably somewhere in between, where they are trying to motivate customers to migrate to newer hardware, allowing them to eventually retire the old hardware and reuse the rack space.
There is definite motivation here to do a lateral inter-generation “rightsizing” move. We most commonly think of rightsizing as moving an over-powered/under-utilized virtual machine from one instance size to another, like m5.large to m5.medium, but intergenerational rightsizing can add up to some serious savings very quickly. For example, an older m3.large instance could be moved to an m5.large instance in about 1 minute or less (I just did it in 55 seconds: Stop instance, Change Instance Type, Start Instance), immediately saving 39%. This can frequently be done without any impact to the underlying OS. I essentially just pulled out my old CPU and RAM chips and dropped in new ones. Note that it is not necessarily this easy for all instance types — some older AMI’s can break the transition to a newer instance type because of network or other drivers, but it is worth a shot, and the AWS Console should let you know if the transition is not supported (of course: as always make a snapshot first!)
For the full view of cloud compute cost trends, we need to look at both the cost of specific instance types, and the continually evolving generations of that instance type. When we do this, we can see that the cost of cloud computing is, in fact, dropping dramatically…at least on AWS.
Originally published at www.parkmycloud.com on January 23, 2018.
CEO of ParkMyCloud
CEO of ParkMyCloud
"
https://medium.com/@TechJobs_NYC/gathering-cloud-computing-28040fa51608?source=search_post---------339,"Sign in
There are currently no responses for this story.
Be the first to respond.
Technology Jobs NYC
Jul 29, 2016·1 min read
Gathering Cloud-Computing By PUI-WING TAM Oracle’s announced purchase of NetSuite is aimed at getting deeper into the game, while Amazon drew more than half its operating earnings from the segment. Published: July 29, 2016 at 06:00PM via NYT Technology http://www.nytimes.com/2016/07/30/technology/gathering-cloud-computing.html?partner=IFTTT
The official account for https://www.technologyjobs.nyc! Follow for exclusive job opportunities, tips and tricks to help you get your next #technology #job
The official account for https://www.technologyjobs.nyc! Follow for exclusive job opportunities, tips and tricks to help you get your next #technology #job
"
https://medium.com/@expertincrm/your-guide-to-salesforce-cloud-computing-rolustech-e485ccc5bad1?source=search_post---------340,"Sign in
There are currently no responses for this story.
Be the first to respond.
Rolustech
May 22, 2020·4 min read
Salesforce is one of the most prominent Cloud Computing applications out there. The cloud computing features heavily in all of the services Salesforce provides, ranging from CRM to ERP, Business Analytics, and even Mobile Application Development.
Within its CRM, Salesforce divides its services into four major clouds, namely, Services, and. Another application is the Salesforce Community Cloud where clients, vendors, and partners all come together for a shared experience.
This heavy dependency on cloud services begs the question: what are the benefits of cloud computing? And what does its future look like? Let’s find out!
Cloud computing involves the ‘outsourcing’ of computing services, such as software, data storage, servers, and much more. The outsourcing is done over the internet and users can access the data or services from anywhere in the world and through any device.
The biggest difference that the use of cloud services made to the way computing works, is that instead of users being tied to any particular device, information is kept on a globally accessible network that is always within reach. This makes the information much more accessible and frees up the processing power of devices, thus making room for a more efficient setup.
Based on what service is being provided, there are three main models of cloud computing. These services are delivered to end-users from remote hubs.
SaaS is one of the most popular types of cloud computing, which allows users to deploy ready to use applications over the internet. The applications are not needed to be downloaded and any issues with the maintenance are managed by the service providers. These programs are easy to use and provide basic functionalities to end-users. SaaS also has the additional benefit of having the best customer service availability. The Salesforce platform’s CRM tools are an example of SaaS.
IaaS is a more open-ended service that enables its users to allow for tailored software creation by customizing the software according to their own needs. This additional room allows users to address their short term or long term needs with changes in their platform, which can be expanded over time as business operations do. IT industries that are already well versed in deploying software tools for their use can benefit from IaaS by getting a cheaper cloud solution and building upon it themselves.
This branch of cloud computing allows its users to customize, develop, and run applications without having to worry about the backend coding or infrastructure issues such as that of storage and memory. These backend services are provided by the original vendor, while the user enjoys the ability to use convenient features such as drag-and-drop to customize their platform.
PaaS can be further classified into public, private, or hybrid clouds, depending on where the cloud is hosted and which party manages the infrastructure. PaaS could also be open source or closed source and further, can be classified as mobile compatible or otherwise.
Cloud computing is the latest in the list of innovations that have revolutionized the way nearly every business and industry operates. The benefits of adopting this technology have far-reaching implications, that eventually lead to a more efficient way in which users run their business.
The advent of cloud computing has led to a new surge of innovation when it comes to streamlining business operations. Salesforce leads the crowd with its specialized cloud services each tailored according to the specific needs of clients when handling business operations. The Sales, Services, Marketing, and Commerce clouds each come with their own specifications and can also be combined to get the best out of their constituents. Rolustech can lend its Salesforce Consultation services to help you choose which Salesforce Cloud Computing solution is the best for you.
Rolustech is a Certified Salesforce Partner and Consultant. We have helped numerous firms with Salesforce Integrations and Customizations. You can Contact Us to learn more about the Salesforce Platform and how it can help your business.
Originally published at https://www.rolustech.com on May 22, 2020.
A SugarCRM, Salesforce, and Magento Certified Developer & Partner Firm. Tweets on #CRM #Sales #Marketing #Social + many more.
A SugarCRM, Salesforce, and Magento Certified Developer & Partner Firm. Tweets on #CRM #Sales #Marketing #Social + many more.
"
https://medium.com/@jaychapel/how-upgrade-cycles-impact-the-cost-per-instance-in-cloud-computing-e782db4d89e0?source=search_post---------341,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Apr 17, 2018·3 min read
I have recently spent an increasing amount of time discussing (arguing) about whether the cost per instance in cloud computing is going up or down. The reason for this is that while objective analysis by reputable third parties shows that computing costs are reducing, what we observe from our own standpoint is that the average cost per instance that customers are managing in the ParkMyCloud platform is actually increasing. Following on from a recent blog by our CTO (The Cost of Cloud Computing Is, in Fact, Dropping Dramatically) we decided to undertake some more detailed analysis to look at this phenomenon.
We identified a cohort of our customers who had been with ParkMyCloud for at least one full year and looked at what happened to their average cost per instance over a one-year time period. What we discovered was that the average cost per instance, as charged by the cloud provider, had indeed increased from $214 to $329 per instance per month for our customers using Amazon, Microsoft and Google clouds — a 65% increase. Set against the backdrop of the reported falling costs of cloud computing, this clearly seems to be an anomaly. Or is it?
Digging a little deeper, we discovered that two-thirds of our customers were spending an increased amount per instance per month over the last 12 months and only one third were paying the same amount or less than before. Interestingly, of those who saw a price increase, one third saw their average cost per instance increase by more than 25%.
So what do we think is happening? One possible explanation is something we will refer to as The Apple Upgrade Syndrome. Each time there is an iPhone upgrade cycle, Apple’s product marketing gurus carefully price the new products — and they also adjust the pricing on their older products. When we walk into the Apple Store to peruse the new offerings, we have a clear choice of either purchasing the previous flagship model at a discounted price, or the new, sexy upgraded model at a price premium. A rational actor should buy the discounted model, which just the day before was hundreds of dollars more. But that’s not what most of us do. What we want is the new model with the additional bells and whistles (e.g. face tracking technology and studio lighting settings for the camera) and are willing to pay the extra. As a result, despite the overall the cost of mobile computing falling, your monthly phone bill keeps increasing.
We believe that the same phenomenon is at work in cloud computing when the new generations of instances are released, and the cloud computing buyers decide to trade-up to these new more powerful instances (e.g. more cores, more memory, etc.), despite the fact that previous generations of instances might actually have their prices reduced. So while Amazon, Microsoft or Google might pronounce a “25 percent improvement in price-performance” for a new generation of instances, the reality is that new instances cost more and therefore drive up the monthly spend.
Next, we’ll share a more in-depth analysis that will review the instance types driving these increases. At the end of the day, we are all likely correct. The cost of cloud computing is indeed going down, but the average cost per instance is actually going up.
Originally published at www.parkmycloud.com on April 17, 2018.
CEO of ParkMyCloud
See all (317)
CEO of ParkMyCloud
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@Eastwick/eastwick-unveils-noob-demystifies-cloud-computing-6ba29a0dcee8?source=search_post---------342,"Sign in
There are currently no responses for this story.
Be the first to respond.
Eastwick
Jan 21, 2015·3 min read
Revolutionary Neo-Organizational Optimization Brainstorm™ technique exposes what really happens in the cloud
Eastwick Un-Demystifies the Cloud infographic
Sharing cutting-edge information with our clients and community is a hallmark of Eastwick’s commitment to success in Silicon Valley. Today, we’re making you among the first to glimpse at the results of our latest research project: the revolutionary patent pending NOOB (Neo-Organizational Optimization Brainstorm) technique. Seven years in the making, NOOB aggregates social collaboration platforms, neural networks, Big Data analytics, sentiment tools, and virtual payment systems to finally reveal the enigmatic logistics and inner workings of the cloud.
According to a recent study, one in five Americans (22 percent) admit that they’ve pretended to know what the cloud is and how it works. Using the revolutionary NOOB approach, Eastwick has peeled back the fluffy layers of cloud computing and uncovered what clouds REALLY do to bring the Internet to today’s desktop, optimizing productivity and elevating the Cumulous Quotient™ that powers innovation and business valuation and other stuff that keeps our global economy strong.
What really happens in the cloud
Neo-Organizational Optimization Brainstorm™ has revealed:
A sunny outlook for cloud computing
The most important takeaway from the NOOB approach findings is that the cloud is viewed favorably by the majority of Americans, and when people learn more about the cloud they understand it can vastly improve the balance between their work and personal lives. In addition to the fact that 68% of Americans see cloud computing as the future and the key to saving the economy, some reported benefits of cloud computing include:
The future of NOOB
After years of research and development, Eastwick’s introduction of the Neo-Organizational Optimization Brainstorm™ technique is accelerating competitive advantage and capitalizing on market innovation. Eastwick’s top data scientist noted, “we’ve finally cracked the code — this is an extraordinary breakthrough that will have a lasting impact on the industry, greater technology, and the world.” In today’s increasingly glo-mo-so market arenas, enabling B2B/C/P collaboration in BYOD workspaces requires constant MVP innovation and market fragmentalizationing to ensure bilateral relevance participation and universal scalability — in the cloud.
Originally published at www.eastwick.com on April 1, 2013.
We help clients shape markets, outshine competitors, and connect with the audiences who fuel business growth.
We help clients shape markets, outshine competitors, and connect with the audiences who fuel business growth.
"
https://medium.com/@techgenyz/reasons-big-firms-are-switching-to-cloud-computing-fd6c6d13a768?source=search_post---------343,"Sign in
There are currently no responses for this story.
Be the first to respond.
TechGenyz
Jun 14, 2017·1 min read
Ever since cloud computing came into existence, businesses all over the World have adopted the technology at an unstoppable rate. They see it as a perfect way to carry all their operational activities in a flexible and cost effective manner. However, there is a common notion that cloud computing only gives you massive storage options and an easy medium to share files. But this is not the case. There’s a lot more than this that cloud computing can offer a business firm.
In this post, I am going to shortlist few of the reasons as to why big firms are switching to cloud computer.
TechGenyz is a leading source of latest technology news, updates on future tech stories, news on Virtual Reality, Augmented Reality, gaming, apps and more.
TechGenyz is a leading source of latest technology news, updates on future tech stories, news on Virtual Reality, Augmented Reality, gaming, apps and more.
"
https://medium.com/foundations/ucisa-cloud-computing-event-f122b447686a?source=search_post---------344,"There are currently no responses for this story.
Be the first to respond.
I attended UCISA’s Cloud Computing Seminar last week, a pretty good event overall though, like many ‘cloud’ events, there was quite a mix of IaaS (e.g. Amazon Web Services (AWS)) and SaaS (e.g. Google Apps) presentations so it sometimes felt like the programme was jumping around a bit. There is no doubt that the ‘cloud’ is generating a lot of interest at the moment, which is gratifying since it is also the topic of our annual Eduserv Symposium this year (May 12th in London).
Phil Richards, of Loughborough, talked about their partnership with Logicalis, building what he called a ‘hybrid cloud’ comprising both an on-campus virtualisation infrastructure and some in-the-cloud burst capacity (based on Logicalis’ Cooperative Cloud Service). He seemed to make the point that research and teaching, being two of the key differentiators between universities, are inappropriate for outsourcing to the cloud. Well, yes, I tend to agree — but that doesn’t mean that the compute infrastructure on which those things are built can’t be outsourced? With the exception of some very specialised cases, I doubt that many people choose their place of research or learning based on the size of its data centre?
And, as David Wallom of OeRC pointed out in his talk about the FleSSR project, outsourcing to cloud infrastructure is already happening, albeit in a rather ad hoc and bottom-up way. He suggested that most institutions (certainly research intensive institutions) will probably have around 200–300 researchers that are already using AWS (or equivalent) for some aspects of their research. So, for at least some researchers, the decision to use cloud infrastructure has already been taken, often on the back of a personal credit card! The problem for universities is that it is happening in an unmanaged (and in some senses unmanagable) way.
Given the announcement of UMF funding a couple of weeks ago, which includes a pilot “virtual server infrastructure (a ‘cloud’)” hosted by us, and given our involvement in the FleSSR project, we now fall rather squarely into the camp of those people thinking about building shared ‘cloud’ infrastructure services for the education sector. Understanding both the needs of those individual researchers who are currently choosing to go to Amazon and those of university IT Services who likely have more strategic ‘virtualisation’ issues in mind brings, I suspect, some interesting tensions, not least around business models (which was the topic of Matt Johnson’s talk), pricing models and, ultimately, sustainability.
Interestingly, JANET’s new role as a broker for the “procurement of shared virtual servers and data centre capacity” (as part of the UMF funding announcement) got positive support on a couple of occasions during the day, with the speakers from UCD saying that they’d like to see a similar service being set up in Ireland.
So-called ‘cloud bursting’ was also referred to several times during the day as being an attractive option. This approach, like that adopted by Loughborough, retains virtualisation/compute and storage capacity in-house but uses the cloud to meet demand when it exceeds local capacity (‘bursting’). This is also the architectural approach being investigated by the FleSSR project. What is not clear to me, when we view the UK HE community as a whole, is the extent to which this kind of approach is able to achieve such significant overall cost savings when compared to a more whole-hearted ‘push everything out to a shared cloud provider’ model, nor the ease with which such cloud-bursting services can become sustainable.
From our perspective, the issues around business models, costing and sustainability are taxing our minds at least as much as the nuts and bolts of building the infrastructure as we consider the future of both FleSSR and our UMF pilot. More anon…
Originally published at efoundations.typepad.com on February 22, 2011.
A blog by andypowe11
A blog by andypowe11
Written by
Cloud CTO, Jisc
A blog by andypowe11
"
https://medium.com/stealthmode-blog/the-evils-of-cloud-computing-766dde3e7677?source=search_post---------345,"There are currently no responses for this story.
Be the first to respond.
Like everybody else, I’ve been migrating my information to the Cloud. I started with GoogleDocs, but now it’s almost ALL all there: my social graph, my online banking, my online tax filing, Facebook, Twitter, even my business bookkeeping and some of my medical data. My hard drive is pristine, except for my music and my iPhoto library. For me, it seems like such a convenience to sync everything to MobileMe that it’s been a no-brainer of a switch. Until I heard Brad Templeton, President of the Electronic Frontier Foundation, speak at BIL.
Templeton reminded me that as I had suspected, “cloud computing” began with the mainframe, which made timesharing necessary and popular, because computing time and cycles were scarce and expensive.
Timesharing gave way to the disruptive force of personal computing, and then of client/server computing. Ironically, cloud computing has brought us back to timesharing, albeit in a different format..
Facebook, Templeton says, is the best example of timesharing and its potential dangers. When you use Facebook, your information is not on your hard drive, but on Facebook’s own site. Then Facebook has these other “apps.” They sound so cool and innocent. But every time you use an app, you are granting it permission to take your information and perhaps put it in the wild. As Templeton said, “data exported is data lost.”
Templeton takes this further, believing that data in the hands of third parties does not have the protection of the 4th amendment (the right to privacy). With the exception of certain types of data, such as email and medical information, which have statutory protection, the data we transfer to the cloud stands unprotected.
Using a little hyperbole,Templeton refers to this as “Erasing the 4th Amendment,” and counsels us to at least think about what we’re doing before we rush headlong into what he visualizes as a potential mushroom cloud.
“Should we erase the Bill of Rights so casually,” Templeton asks, “when if the same data remained on your own hard drive, it WOULD be protected.” What’s the rush to get it into the cloud?
Simple. It’s too tempting to migrate and take the chance, , because you miss out on all the coolness if you don’t contribute your data. You don’t get to participate. Facebook reverses the signup dynamic. You can sign up easily, without much barrier to entry, and without entering much information. But over time, more and more information is added, until Facebook knows everything about you! And then there are those Facebook apps — effectively another company’s web site inserted into a Facebook page, asking to get your data so it can recommend a movie for you, or find you a friend. What have we done?
We are enjoined to consider the balance between privacy and openness, and at least think when we change it. We are really shifting the balance between the public and the private as we speak.
Why does privacy matter? Because in shy people, behavior is different when you think you are being watched (Heisenberg Uncertainty Principle) There are two choices: Let the privacy people have their way, or OR, if you want privacy, you can’t use our cool new app. We need something in between.
A long web form is an impediment, but what you make easy to do you make easy to ask for. Thus, ease of use and data portability are really bugs. They make it too easy to ask for information. End-user control inhibits negotiation — you can opt out, but only out of the whole thing. You can’t negotiate which parts of the agreement you want to participate in.
So ironically, cloud computing inhibits the power of the user, even if it makes things more convenient. In the cloud, I can’t make Facebook faster by buying a faster computer,nor canI switch away without a huge re-export of my data (data portability). Templeton refers to this as BEPSI, the Bulk Export of Your Personal and Sensitive Information. And once data is exported, it is lost.
How all this data in the cloud could be used also frightens Templeton, who says the history of humanity is a long chorus of police states punctuated by a few staccato notes of freedom. We must take care not to build the infrastructure of a police state.
Cloud computing is actually changing the world. Once we put all our data thru somebody else’s pipes, they can throw the lever toward a police state. We probably don’t want to be the inadvertent cause of that eventuality.
Francine Hardaway’s Blog About Entrepreneurship…
Francine Hardaway’s Blog About Entrepreneurship, Technology, and Life
Written by
Co-founder, Stealthmode Partners, helping entrepreneurs succeed
Francine Hardaway’s Blog About Entrepreneurship, Technology, and Life
"
https://medium.com/@knoldus/all-you-need-to-know-about-cloud-computing-f0e6aceddc74?source=search_post---------346,"Sign in
There are currently no responses for this story.
Be the first to respond.
Knoldus Inc.
Dec 9, 2021·7 min read
Cloud computing is a term for characterising the facilitating administration over the internet. It
implies securing and getting to your data, records and tasks over the web instead of your PC hard drive.
Cloud computerises your data at whatever time, any place utilising any service over the internet. It gives
us opportunity of doing things with more abilities power & straight forwardness.
We use cloud services in our daily basis.
Cloud-based software offers companies from all sectors a number of benefits, including the ability to use
software from any device either via a native app or a browser. As a result, users can carry their files and settings
over to other devices in a completely seamless manner.
Cloud computing is far more than just accessing files on multiple devices. Thanks to cloud computing services,
users can check their email on any computer and even store files using services such as Dropbox and Google Drive.
Cloud computing services also make it possible for users to back up their music, files, and photos, ensuring those
files are immediately available in the event of a hard drive crash.
The cloud structure allows individuals to save storage space on their desktops or
laptops. It also lets users upgrade software more quickly because software companies can offer their products
via the web rather than through more traditional, tangible methods involving discs or flash drives.
This allows users to download new versions and fixes to their programs easily.
With all of the speed, efficiencies, and innovations that come with cloud computing, there are naturally, risks.
Security has always been a big concern with the cloud especially when it comes to sensitive medical records
and financial information. While regulations force cloud computing services to shore up their security and
compliance measures, it remains an ongoing issue.
Encryption protects vital information, but if that encryption key is lost, the data disappears.
Servers maintained by cloud computing companies may fall victim to natural disasters, internal bugs, and power outages, too.
The geographical reach of cloud computing cuts both ways: A blackout in California could paralyze users in New York, and a firm in Texas could lose its data if something causes its Maine-based provider to crash.
As with any technology, there is a learning curve for both employees and managers.
But with many individuals accessing and manipulating information through a single portal, inadvertent mistakes can transfer across an entire system.
Although, there might be less disadvantages and have more features to use.
Their are three main categories of Cloud Service model:
Software as a Service (SaaS)
Platform as a Service (PaaS)
Infrastructure as a Service (IaaS)
SaaS is stand for software as a service it is a model for allocation of software where a user can access data over the
internet. In this model, the software provider host and retain the servers, databases and code that comprise
an application.
It is not important that the user should have the knowledge of the back-end infrastructure.Scalable services or resources over the web and well defined APIs.
The client can also access services without installing the software on their device.SaaS provider look after identifying and fixing the bug.With the help of centralized cloud, software updates are made simpler.The automation of technology updates.
Malware attacks can affect the use of SaaS service in web browser.
Third-Party involvement is required for Proper security mechanism.
PaaS is stand for platform as a service is a type of cloud computing service delivery model that provides platform and a solution stack as a service which include resources such as, operating system, programming language, compiler, run-time environment, servers like database server, web server, etc.
It provides built-in scalability, web service interface and security to connect other applications outside the PaaS.
PaaS provides many things like common packages/services for network, payment integration, and databases.
It provides facilitates to work with multiple platforms.
The application can be up grade to authorise users only.
It handles all the applications dynamic resource requirement.
Migration from one PaaS vendors’ application to another PaaS vendor will create some problem
Some of the applications developed may be local while others are from the cloud; which may increase the complexity.
Figure 2: Cloud Service Delivery Models
IaaS is stand for infrastructure as a service. Cloud infrastructure model, is a self-service model for accessing, monitoring
and managing remote data center infrastructure, such as, compute storage (virtualized or bare metal), networking
and networking services (e.g. firewall).
Infrastructure as a service (IaaS) comes in all different shapes, sizes and colors.
There are 8 types of IaaS providers
The Developers utilize the IaaS as the testing of their applications on multiple environments.No need to purchase suitable dedicated servers.
The consumer can maintain and upgrade the tools and database system installed on IaaS.
There are various types of clouds, each of which is different from the other.
Private clouds are reserved for specific clientele, usually one business or organisation. The firm’s data service centre may host the cloud computing service. Many private cloud computing services are provided on a private network.
Public clouds provide their services on servers and storage on the Internet. These are operated by third-party companies, who handle and control all the hardware, software, and the general infrastructure. Clients access services through accounts that can be accessed by just about anyone.
Hybrid clouds are, as the name implies, a combination of both public and private services. This type of model allows the user more flexibility and helps optimize the user’s infrastructure and security.
Services can be both public and private — public services are provided online for a fee while private services are hosted on a network to specific clients.
Businesses can employ cloud computing in different ways.
Some users maintain all apps and data on the cloud, while others use a hybrid model, keeping certain apps and data on private servers and others on the cloud.
When it comes to providing services, the big players in the corporate computing sphere include:
Google Docs, Microsoft Office 365: Users can access Google Docs and Microsoft Office 365 through the internet. Users can be more productive because they can access work presentations and spreadsheets stored in the cloud at any time from anywhere on any device.
Email, Calendar, Skype, WhatsApp: Emails, calendars, Skype and WhatsApp take advantage of the cloud’s ability to provide users with access to data remotely so they can access their personal data on any device, whenever and wherever they want.
Zoom: Zoom is a cloud-based software platform for video and audio conferencing that records meetings and saves them to the cloud, enabling users to access them anywhere and at any time.
Amazon Web Services is 100% public and includes a pay-as-you-go, outsourced model.
Once you’re on the platform you can sign up for apps and additional services. Microsoft Azure allows clients to keep some data at their own sites. Meanwhile, Alibaba Cloud is a subsidiary of the Alibaba Group.
Due to competition, providers must constantly expand their services to differentiate their services. This has
led public IaaS providers to offer far more than common compute and storage instances.
For example, serverless, or event-driven computing is a cloud service that executes specific functions, such as
image processing and database updates. Traditional cloud deployments require users to establish a compute instance and load code into that instance. Then, the user decides how long to run — and pay for
that instance.
With server-less computing, developers simply create code, and the cloud provider loads and executes that code in response to real-world events, so users don’t have to worry about the server or instance aspect of the cloud deployment.
Users only pay for the number of transactions that the function executes. AWS Lambda, Google Cloud Functions and Azure Functions are examples of serverless computing services.
Hope, We have got to know about the Cloud Computing
For more blogs please visit blogs.
Group of smart Engineers with a Product mindset who partner with your business to drive competitive advantage | www.knoldus.com
1 
1 
1 
Group of smart Engineers with a Product mindset who partner with your business to drive competitive advantage | www.knoldus.com
"
https://medium.com/@logz05/cloud-computing-for-market-research-ebf3f3f45fb?source=search_post---------347,"Sign in
There are currently no responses for this story.
Be the first to respond.
Nick Loggie
May 6, 2020·3 min read
In this internet age, everything is migrating to the cloud. From data storage to web applications, a lot of things are being hosted on the cloud, and it has proven to be one of the most powerful technologies of our decade.
Empowering companies by letting them create applications and products that can be accessed from anywhere in the world has greatly impacted the tech sphere. Cloud computing has drastically reduced the costs of operating and establishing a business or launching a product. With many of the world’s largest companies like Amazon, Facebook, Instagram, etc. moving on to cloud computing, it is high time other businesses do so too.
When cloud engineering is rightly leveraged, cloud technology in the business sector will help in building many powerful and scalable businesses.
Market Research is one of the most fundamental necessities of any business, product, service or a startup. Before you start any venture, you tend to do thorough market research to understand the current scenario.
Market Research often uncovers many interesting facts and helps in taking the right decisions. To understand how cloud technology will aid market research, we need to understand what market research means.
In general, market research comprises the collection and analysis of two kinds of data. The first is known as the primary information, and the other one is known as secondary information. While primary information is something that needs to be compiled by yourself, secondary information is the one that is already existing.
If you wanted to launch a financial tech product which helps in calculating a credit score, knowing the number of people who place much emphasis on their credit score will be the primary data. This can be done through surveys, person to person interviews, online forms, etc. On the other hand, if you directly search for studies and research which discuss digital literacy or the number of people using mobiles in a given country, it is secondary research.
While curating secondary information doesn’t often demand a lot of efforts, gathering first-hand information, or the primary information is often painstaking. One can make the most out of cloud technology in market research when it comes to curating primary information.
Cloud technology has enabled three forms of services. Software as a Service (SaaS), Infrastructure as a Service (IaaS) and Platform as a Service (PaaS). All these services can help businesses in market research.
Consider an example. Let’s say you want to start a website which sells used books from people. For your initial market research, you’d have to gather information from a particular set of people in a demographic you have chosen. After doing so, you need to question them, record their answer and analyze it further to derive results which help you in your market research.
In this scenario, let’s say you use a SaaS model product for your market research. You create a well-designed form with great questions and push them to the demographic you are targeting. Now, all their responses and their data which they disclose to you are stored on a cloud server. Again, you can apply another SaaS product which analyses this data in the cloud to deliver the results of your market research.
On the other hand, if you are to perform a heavy marketing research campaign wherein you need a lot of processing power and storage to host the data, you can leverage the IaaS model of cloud technology. By using better infrastructure through the cloud and cutting down on the extra costs you will incur to buy new infrastructure, you can effectively perform market research.
Cloud technology enabled technological developments such as SaaS, IaaS, and PaaS. All these three technologies have given rise to many tools and websites that will be of utmost use to any business looking towards performing market research.
By unleashing the power of cloud computing, you can reduce the operational costs and achieve greater results in considerably less time.
Originally published at Adlibweb.
Digital Marketing Consultancy specializing in International B2B #contentmarketing #digitalmarketing #InfluencerMarketing #socialmediamarketing #globalmarketing
Some rights reserved

Digital Marketing Consultancy specializing in International B2B #contentmarketing #digitalmarketing #InfluencerMarketing #socialmediamarketing #globalmarketing
"
https://medium.com/bittrexglobal/the-future-of-decentralized-cloud-computing-featuring-greg-osuri-of-akash-network-d8cfcf9198e0?source=search_post---------348,"Sign in
There are currently no responses for this story.
Be the first to respond.
Bittrex Global Team
Nov 23, 2021·28 min read
Listen on:SpotifyApple PodcastsStitcher
This week on The Bit, we’re joined by Greg Osuri, CEO of Akash Network, to speak about a world where cloud computing is permissionless, sovereign, and open, where and where builders of the internet have greater freedom to expand the human experience.
Here are the show notes:
[5:35] Akash Network Vision[16:41] The Competition[27:50] California and Blockchain[33:29] Widespread Adoption[40:49] The Future of Akash
Chris Sinkey: Hi, welcome to The Bit, the Bittrex Global podcast, where we give you the inside scoop on all things crypto. My name is Chris Sinkey, and I’m the Chief Business Officer at Bittrex Global. Even though the idea behind cryptocurrency and blockchain systems emerged a little over a decade ago, the ecosystem has grown so exponentially and so rapidly that sometimes it’s hard to keep track of all the innovation going on around the space. Akash Network is a perfect example of this, with a mission to transition cloud computing from a centralized model to a decentralized system.
Akash Network is showing us the benefits of what a decentralized cloud computing world can look like, faster, more flexible, and lower cost. So to help us get a better grip on Akash Network and its mission to bridge the gap and link cloud computing to the blockchain, I’m speaking today with the co-founder and CEO of Akash Network, Greg Osuri, who brings a wealth of experience in open source development and cloud computing. So Greg, welcome to The Bit; we’re excited to be chatting with you today. We’d love to learn all about you and your background.
Greg Osuri: Thanks so much, Chris, for that kind introduction and for having me. To simply put it, I build things for people that build things. I’ve been a programmer for a little over 25 years, in which the last 13 years, I was primarily building and shipping developer tooling. I founded a company called Angel Hack to take hackathons to masses in 2013.
I helped launch a few companies, the most notable one being Firebase, which eventually got acquired by Google. My passion for helping developers succeed led me to founding a company, Overclock Labs. The goal was to solve a very critical problem — make your applications available to users in a frictionless and scalable way.
The overwhelming problem back then, even now, happened to be that you could have a scalable infrastructure, but it was not easy to use, or it was easy to use, but it wasn’t scalable. So that was a mission for Overclock Labs. As we were deploying the solution to various customers in their data centers, we discovered another big problem: underutilization. So, it turns out; most compute around 85% of compute that sits in data centers remains unused.
That’s when the idea of unlocking this unused compute and making it available to users in the same frictionless way, came about and that’s the genesis for Akash Network. Five years later, here we are, with hyper-growth and a highly usable product.
Chris Sinkey: I love it. Thanks for that background. Very interesting, and I think we’re going to get into a great discussion today about the trends shaping the broader crypto space. Also, the future of cloud computing, and a particular interest, of course, is where Akash Network fits into that equation. But right off the bat, I got to do a plug for Bittrex Global and let everybody know that AKT, the ticker name of Akash Network’s token, is listed on Bittrex Global and Bittrex Global did integrate the mainnet of Akash Network.
So for any of those that are interested in trading it, Bittrex Global is one more place where you can do that. Before we dig into the broader trends and the next question on the list, we like to ask all of our guests this question: how did you get into crypto in the first place? How did you hear about it? When did you buy your first token? I know you talked about it a little bit already, but the first entry into Crypto is always a fun story.
Greg Osuri: My first entry to crypto was around 2012. Around that timeframe, I bought Bitcoin as a utility to pay for things, and those things ended up being the most expensive things I ever bought. It was a hell of a party. But more seriously, I think, in 2016, we took the red pill. Before that, we experimented; as I mentioned, I founded Overclock Labs to make scalable infrastructure easy to use.
The applications we were designing were an extremely fast infrastructure by going to the edge, so we call it edge computing. One of the technical challenges for edge computing was to ensure the configuration data was consistent across all the edge data centers. We had to choose a data structure that was very similar to a Merkle tree, and it’s synced using a BitTorrent style replication mechanism. So when you put a BitTorrent and Merkle tree together, you end up with a blockchain.
So that’s really how we started getting into the early days of blockchain in 2016. And we formalized the blockchain as a core data structure in 2017. So to answer your question, we got into blockchain out of necessity to solve an edge computing data consistency problem. The token on top of the blockchain came because of our foray into proof-of-stake.
And my background as an economist made me realize that we can conduct experiments in a micro-climate. We can use the monetary sovereignty benefits that crypto provides to bootstrap networks like Akash to form the foundation.
[5:35] Akash Network Vision
Chris Sinkey: And so, for some of our listeners that might not be that familiar with Akash, and you’ve talked a little bit already about what Akash is, can you succinctly describe the vision for Akash and what you’re trying to do?
Greg Osuri: To simply put, a little history lesson, right? So if you look back at why the internet was created, the internet was created as a solution to the centralization problem, which was threatened by the Cold War, right. So the idea of having built a communications network that does not have a center was a novel idea in the 60s, and that gave birth to the internet. But somewhere down the line, the internet became extremely centralized, protected by gatekeepers. And today, if you look at the landscape, over 80% of the cloud infrastructure, which is the backbone for the internet, is controlled by four companies, and Amazon is the biggest one.
And the outcome is behavior by these companies that may or may not be aligned with the general public. More importantly, when you have so much power concentrated into a single entity, like Amazon, I think it poses a major threat to civil liberties. So, the outcome and we see what happens when you have so much concentration of power, one of the outcomes is censorship, right. Whatever side of the political spectrum you fall under, the fact that there is a single company that has so much power over civil liberties is concerning. So, on top of that, when you have few companies operating what could be considered as one of the most critical layers of our society, which is a cloud, you end up with inefficiencies. Oligopolies tend to give inefficiencies, and they also tend to create competitive modes for newcomers that prevent newcomers from participating in what is supposed to be a competitive economy.
The mission for Akash is to decentralize, break this oligopoly using a novel mechanism wherein people, the community of users, are in charge or in control. So Akash, by definition, is a decentralized cloud that is censorship-resistant, self-sovereign, and permissionless. A core of this decentralized cloud is a cluster of computers spread across Cloud grid computers that offer cloud-like, if not better features, that the current cloud operators provide at a significantly lower cost, which is driven by the market. Also, the experience is so much more centered around the user, which is the developer. So, Akash is a modern, open, developer-first, cost-optimized cloud.
The best analogy I can give: Akash is like Airbnb of computing, wherein Amazon is like Hilton. I’m saying that because the computing behind the Akash Network does not come from a single provider or a single company but rather comes from a globally distributed network of cloud-grade data centers. At the same time, you look at something like Amazon, where they own all their data centers and price accordingly. With Amazon, you get this consistency, like, just like how you will get with Hilton. Whereas with Akash, you get this variety, just like how you would get with Airbnb.
Chris Sinkey: Very interesting. And Akash Network recently celebrated a very important milestone, is that right? The launch of MAINNET 2 back in March?
Greg Osuri: Correct. That was seven months ago.
Chris Sinkey: And then what’s the growth and demand for Akash Network look like since then?
Greg Osuri: So since we launched seven months ago, we have 30,000 deployments, application deployments, and over 450 active applications currently running on Akash. We registered a triple-digit growth month over a month till about last month.
We were actually doubling in terms of live applications coming online on the Akash Network. Akash Network is generating revenue; we hit, I believe, $10,000 in monthly recurring revenue for a six-month project. I believe Zack yesterday from Yahoo Finance called Akash as an Amazon competitor that’s growing faster than Amazon. So that’s where we are.
Chris Sinkey: Oh, wow, that well, that sounds pretty impressive, to say the least. I have to watch that interview.
Greg Osuri: Thank you.
Chris Sinkey: And you’ve got something called the Akashlytics Dashboard; is that right?
Greg Osuri: Correct.
Chris Sinkey: Is that publicly facing?
Greg Osuri: That is publicly facing. I believe they’re making some modifications, and I believe the data may not be accurate. Still, we will be launching our own dashboard on the 28th of October, which is coming up, so we should have an official Akash Network dashboard. We have a few other community-managed dashboards — Akashlytics being one.
Chris Sinkey: Got it. So it’s kind of a community-managed block explorer for Akash Network, more or less analytics overlay?
Greg Osuri: Yeah, deployment analytics dashboard, I would say. We have a few other block explorers that don’t track deployments; there is MintScan, there’s Aneka and a few others.
Chris Sinkey: Are there any other applications that are sort of community-built and managed, or any that the Akash team has built and continues to manage that our users might be interested in interacting with?
Greg Osuri: Several actually. Akash takes a community-first approach. We have community build applications, more applications than then Akash team. The core team is responsible for building and maintaining, along with other participants across different companies, that the core technology, but most of the applications come from the community. If you want to understand the full extent of the applications Akash Network hosts, I recommend checking out something called Awesome Akash — it is on our GitHub.
You will see hundreds of different applications that are built on Akash, and the most notable one running on Akash is Osmosis. Osmosis is a decentralized exchange for the Cosmos ecosystem. It is the premium decentralized Exchange, which now hosts Terra and I believe has about half a billion dollars TVL. Itt’s running on Akash. Another very interesting application that we have seen a lot of momentum in is NFT’s and games. So there is a new game called Strange Clan that was launched a few weeks ago.
This is running exclusively on Akash, and those are some of the fun things that you can do right now on Akash. Of course, there are many other applications that people are running from mining or whatnot, but I would say Osmosis and Strange Clan are my two favorite things.
Chris Sinkey: Okay, I’ll have to check those out. And when it comes to the token, AKT, what’s the technology that is based on? Or did you guys open source it from scratch? Is it proof-of-stake? A fork?
Greg Osuri: Akasha is a sovereign blockchain. It is an independent blockchain that’s secured by its own set of validators in a proof of stake scheme, where the AKT token provides the security for the blockchain as a primary purpose. The AKT token is also the token that people use to pay for hosting on the AKASH network, and it is a token that providers use to receive payment.
As a staking token, Akash, AKT has incentives that are very attractive for stakers to secure the blockchain, and the incentive is staking rewards, which is north of 45% to 50% APR. When you compound that, I believe it’s somewhere around 70 to 80%. Akash token is also a highly liquid token, especially on decentralized exchanges like Osmosis, wherein you can provide liquidity for further incentives. I believe Osmosis is offering around 130% to 150% APR. Compound that again is like 200%+ APY. The other blockchains and other decentralized exchanges like Sifchain offers north of 200% APR.
The primary purpose of AKT token is to secure the blockchain and be used as a medium of exchange in the Akash Network, which is doing very healthy. There are also secondary income generating or passive income-generating opportunities by application such as Osmosis, or Sifchain.
Chris Sinkey: That’s great. I was just going to ask you what the utility use cases were for AKT, and you went right into it and just not knocked that out. So that’s really great to hear that it not only secures the network, but you’ve got active use cases for AKT right now.
The APR sounds amazing. Do you worry a little bit about supply overhang and downside pressure on the price with an APR that high? Where do you think the APR will go over the next year or two.
Greg Osuri: So the APR is on a decay curve. So it comes down every block period every six seconds. Akash is a fixed supply token — there are 389 million tokens ever in existence that will take another 80 years by the time you see full supply being reached, and the APR is essentially a way to unlock that supply.
The way Akash holders look at the APR is really an opportunity to acquire and hold on to AKT tokens because as the APR gets less meaningful, you may not have the same opportunity that you have right now. So the idea behind the staking rewards is to create incentives for folks to stake the network, which in turn security network. So there’s a proposed idea where a percentage of the hosting fees paid to the network goes back to the stakers.
The economy is designed to give ample amount of time or sufficient time for Akash adoption to catch up, which is an amazing growth curve, as you can see, and hopefully come to an equilibrium with the staking rewards.
[16:41] The Competition
Chris Sinkey: That’s great. Let’s transition to talking about competition. Other storage-based projects in the space or super computer-based projects in the space like SONM, Golem, and Siacoin — what do you think sets Akash apart from that group? How would you frame the differentiation?
Greg Osuri: Sure. Let’s break down the space a little bit. Akash is not a storage platform; it is a distributed computation platform. What that means is Akash can provide storage that’s ephemeral in nature, meaning the storage goes away once your lease with a provider ends. So, you’re responsible for ensuring that you have backups, and where do you back this up? On another decentralized storage network like Sia or FileCoin. Each of them offers different value propositions.
Akash works in complement with the storage networks. Similarly, with other layers in the decentralized, or DWS space, the decentralized web services, where you have Handshake as your primary DNS resolver or even ENS domains, then you have compute layer, which is Akash, and you have other specialized compute providers like Livepeer. Even iExec is very different. iExec only does background jobs, whereas Akash can do full applications. Akash can host, for example, a database or s WordPress website, whereas iExec cannot.
The iExec model is similar to Ethereum, where the user pays the gas fees, whereas the Akash model is similar to AWS, where the developer pays for hosting. Completely different things. But iExec is a specialized compute provider.
Then you have something like storage. Like I said, with FileCoin and Sia, you have potentially key management services. A lot of these services that you’re used to in the Web 2.0 world provided by big cloud are getting decentralized, in this new category called DWS. Akash is an important player because with Akash, users can compose different decentralized cloud networks and leverage using computational composability.
So without competition, you cannot use Handshake directly with the FileCoin today. In order to use it, you need Akash. Akash is this glue that brings all these decentralized networks together.
Chris Sinkey: Well, you answered my next question already, but I’ll ask it again, just to make sure that we covered it all, which is: How exactly does Akash link the cloud to the blockchain? And how does this fit into today’s old-world cloud ecosystem?
Greg Osuri: Yeah, and I actually get into more examples. We see quite a bit of Sia, Skynet used with Akash, and I really like Skynet. Akash doesn’t do long-term storage — any storage you do on Akash is short-lived. So, say you want to do a decentralized WordPress.
What you normally do is host the WordPress application and the MySQL server on Akash, back up MySQL database onto Skynet, connect the WordPress application, the Apache webserver to Handshake,, and maybe even use DVP or Sentinel to automatically resolve handshake. So you can use dVPN, which is a decentralized VPN, to automatically resolve your domain name running on Handshake, which connects to Akash, wherein the Akash data that you’ve stored is backed up to Skynet.
That’s just one example of how people use Akash and other protocols. Another great example I saw the other day, I’m not sure if you know about this protocol called Radicle, but Radicle is trying to decentralize GitHub. GitHub is owned by Microsoft and heavily centralized, and they are known for their censorship.
So GitHub Pages, for example, is used by a lot of DeFi projects to host their front ends. That is a big threat to our existence as a decentralized Web 3.0 future. So Radicle is trying to decentralize GitHub, but does not have GitHub pages. So, if someone built an application where you can push your code to Radicle, and your code, if you have a website, will automatically get deployed onto Akash, and now you have a full-on decentralized version of GitHub Pages.
Those are some of the examples the community has come up with and are using, and I’m a big fan of this new application — I’m going to host the source code now on Radicle and use this amazing mechanism to keep updating my website. What we’re noticing here is a birth of a new class of web 3.0 services called DWS that’s actually gaining adoption, and making money, as it’s gaining adoption. So I’m really excited to partake in this phenomenal revolution.
Chris Sinkey: Can you give us a rundown of where the decentralized cloud computing space currently stands and what might be in store for the future down the road?
Greg Osuri: The industry is expected to be worth $800 billion by 2028. I think that’s the estimated number right now. It’s about $380 billion — t’s humongous. It’s a trillion-dollar market, and it’s controlled by four companies — Amazon, Google, Microsoft, and Alibaba. Amazon has about 300 services that people use. In order for the DWS to be competitive with Amazon, DWS needs to offer these services, and we don’t have those services.
Chris Sinkey: And DWS, you mean decentralized web services, right?
Greg Osuri: Decentralized web services — that’s a category.
Chris Sinkey: I just wanted to clarify that for the listeners just to make sure that they knew.
Greg Osuri: I’m not too sure who coined the term, but it’s a new term that’s been coined, which is a replacement for decentralized replacement for Amazon Web Services that Akash is a part of. So, Messari wrote a report recently called The Dawn of Web3 Network Revenue. There’s a website called web3index.org — I highly recommend you check it out. It tracks revenue and real progress for a lot of these networks.
Not the staking rewards or none of the extra stuff these networks offer, but the actual usage revenue. So you’ll see that there are about six services that are actually making money, offering real valuable services to users that could be on Amazon but chose not to and are instead using the DWS services. So that’s just an indication that we have reached product-market fit to a certain degree, and we are currently in the growth stage. So, the product-market fit stage is one of the biggest hurdles that DWS had, and now we kind of crossed that.
The big challenge is how do we grow? How do we grow to the $800 billion industry by 2020? By focusing on important things. So a big missing piece right now is developer experience — interoperability. So I think, without investments in IBC or inter-blockchain communication, we can solve a lot of the inefficiencies in the next year or so. So, the big challenge right now is just getting all these protocols working in a cohesive way, where the user can pay using a single token. So, big emphasis on user experience.
Chris Sinkey: We can’t have a crypto-themed podcast in 2021 without bringing up DeFi to shift the subject a little bit. So what is Akash Network’s position on DeFi? Are there any plans to integrate it, specifically DeFi applications onto the platform?
Greg Osuri: Well, Osmosis, which is one of the biggest, fastest-growing decentralized exchanges, is running on Akash. Sifchain announced plans to move to Akash. We have seen quite a bit of different applications moving to Akash, and I believe Polygon announced plans, and there’s going to be some exciting stuff to move Akash.
So, we’ve seen quite a bit of DeFi adoption, and I think the trigger for that is heavy regulation. So the idea is if you can remove the corporation out of the way, and when you have a DAO-controlled website, that’s a lot more decentralized than what we’re seeing these days. When you have DAOs, control websites, Amazon is not really an option. Because you have to elect somebody in the DAO that has to pay using their credit card.
The best possible solution for a DAO to host data service is Akash, wherein you can control using a key, ideally multisig. So Akash is providing decentralized data services for DAOs, and a lot of the DeFi is moving to DAOs, and that’s why they’re choosing Akash Network as their hosting platform.
So I think this new evolution of DeFi 2.0, which is driven by DAO and, a lot of the DeFi 2.0 projects actually have DAO controlled treasuries, and now they will have DAO controlled data services and Akash. So we were working with a few amazing platforms, and you’re going to see a lot more announcements in the next few weeks about our progress here.
Chris Sinkey: You mentioned Cosmos a couple of times, and you just mentioned Polygon — what’s the connection between the two? What makes Akash Network attractive to those platforms?
Greg Osuri: I don’t know if you know that Polygon is a fork of Cosmos hub.
Chris Sinkey: I did not know that that’s interesting.
Greg Osuri: It is a fork of Cosmos SDK. They’ve changed quite a bit now, but they began as a fork. They did a lot more work on top of the fork, and I don’t think it’s any longer a Cosmos SDK base chain, but the operational schematics are very similar to the Cosmos SDK chain. So, we are working with Polygon to offer the ecosystem a decentralized cloud hosting option, which is critical now.
We’re potentially also announcing a few other plans that you’ll soon find out about in a week or so. There’s a lot of excitement that’s happening — I can’t really talk about it right now. I want to keep the surprise fresh.
[27:50] California and Blockchain
Chris Sinkey: Yeah, keep the surprise fresh. Keep us on our toes, and we’ll look forward to those announcements. Thanks for answering that question. Segwaying into the next subject, it’s definitely slightly off-topic, but I don’t think we could let you leave the podcast without having you share your role in getting California’s first blockchain law passed. Can you give us a behind-the-scenes look at how everything played out there?
Greg Osuri: Yes. This was in 2018. Crypto was crazy. We were just trying to get the attention of the regulators. Right now, we have too much attention, which is, you know, whatever. But back then, regulators or lawmakers really didn’t want to touch anything crypto. I felt it was very important to get some of the core definitions into California law, considering California is home to the most innovative companies.
So me along with an early supporter of Akash, we helped propose a few amendments to the Privacy and Consumer Protection Act of 2018 to include the definition of blockchain, the definition of smart contract into the law so that we can have the foundation we can build upon for further legislation for crypto.
It was modeled after the Wyoming Bill, and Wyoming is one of the first states to actually make quite a bit of progress. Now that we have the foundation for the California blockchain legislation, we can build amazing legislation that uses the foundation. So it was at a time, and when nobody wanted to talk about crypto in the legislative sphere, we took the first step, and hopefully, that helped quite a bit further along to make crypto legitimate.
Chris Sinkey: I applaud your work in that space. I think we all do at Bittrex Global. As somebody working for a centralized exchange for the past four years, I can tell you I’ve witnessed firsthand the difficulties in working with regulators to help them understand what we’re doing, why we’re doing it, and why it’s important for the world.
We’ve been targets. The Fiat on-ramps are a major source of regulatory attention. Every user on Bittrex Global has to go through KYC, and we comply with AML regulations around the world. We actually segregated our userbase in 2019. There’s Bittrex US, and then there’s a Bittrex Global, and that’s the company that I work for now. And that company’s headquartered out of Liechtenstein and only accepts non-US customers.
Part of that is the security laws and the differences between them. Unfortunately, there’s so much discrepancy between whole regions of the world and how they treat cryptocurrency. The one thing I hope the US gets right is a safe harbor for many of these projects that are going from zero to utility token status.
They’ve got that ramp in Europe, but they don’t in the US yet. So anyway, that’s my regulation plug. Sorry to take it off a tangent, but something that I think would be really important for the industry and US adoption.
Greg Osuri:
You brought up a very important point called Safe Harbor Commissioner Hester Peirce actually proposed an amazing piece of the framework that’s actually blogged about on our website in 2019. Commissioner Peirce really proposed a comprehensive framework for new tokens to graduate from what they considered securities at the beginning to more commodities, including various disclosure requirements, including various progress support, and some technical requirements, like providing a block explorer, and whatnot.
Those are some of the things that we are doing anyway. Still, it would be great for SEC to adopt that as a more formal framework and give some regulatory clarity, especially to young companies and networks like Akash, as to where we stand. Still, unfortunately, the regulatory situation is that they’re taking enforcement instead of clarity.
Enforcement first approach is very hostile to our industry, but our industry is known for mobilizing. I think in the end, it’s going to be great, we’re all going to make it, but in the short-term, the mid-term, I don’t think it’s going to be very pleasing. Short-term pain for long-term gain.
[33:29] Widespread Adoption
Chris Sinkey: Yeah, and in my mind, Ethereum was really the poster child for that Safe Harbor concept. That graduation from security to a utility token, with Ethereums initial coin offering clearly a security in the eyes of US regulators, but now very decentralized and distributed around the world. It’s absolutely a utility token — it’s a commodity. So, let’s segue into the next question, which is around adoption.
As the space as cryptocurrency continues to grow and mature, we’re seeing so many interesting projects come onto the scene that they’re trying to tackle and solve different problems and improve efficiencies in every corner of our economy. But a lot of these projects can be very difficult for the average person to wrap their head around and use. So, from your perspective, what do you think needs to be done to accelerate mainstream adoption of cryptocurrency in the blockchain?
Greg Osuri: Good question.
Chris Sinkey: And the second piece of that is, what does Akash Network do to target and educate new users and get new users into the network and drive adoption?
Greg Osuri: Yeah, it’s a loaded question — lots of things are happening.
Chris Sinkey: It is. Our community likes to ask loaded questions, what can I say. But it’s an interesting one for me too — I’m very curious about your response to this?
Greg Osuri: If you look at the work so far that’s being done, it is mostly from an infrastructure standpoint. So building the infrastructure, the Layer-1s, the Layer-2s, and the sovereign chains to support an adoption that can expand. So, we are slowly graduating from the infrastructure focus to application focus with DeFi and NFT’s.
NFTs really opened up quite a different perspective that I realized very recently, especially talking to more traditional Web2 founders and Web2 participants — they’re all doing NFT’s now. When people talk about NFTs, they don’t talk about blockchain. So I think we are starting to see the shift of mainstream adoption, especially with NFTs, and it’s not anymore “if” we’re going to have mainstream; it’s about “when” we go mainstream, and it’s starting to happen in ways that we never anticipated before.
So, of course, we built infrastructure first, and now we are building the applications. If you look at Axie Infinity, I think they did billions of dollars in quarterly revenue. If you look at OpenSea, it did $10 billion in GMV. This quarter, that’s a clear indication that there’s quite a bit of mainstream demand for these applications.
Chris Sinkey: Opensea did $10 billion in quarterly GMV?
Greg Osuri: I believe so. It could be monthly. Don’t quote me on the actual timeframe; it’s for sure less than a quarter. I can sit here and predict what needs to be done, but ultimately it’ll be something like NFTs — the whole, make it mainstream. I never saw the coming of NFTs like what they have right now. But I think fundamentally, it’s infrastructure. It’s really hard for someone to self custody their tokens.
Custody is only as good as a person holding it. So, if I talk to someone like my dad, he would hate holding his tokens, because he’s afraid that he might lose them. So, from an on-ramp and off-ramp standpoint, I think there’s a lot of infrastructure that needs to be solved.
From a custody standpoint, that needs to be solved. From a wallet infrastructure standpoint, the wallets not using ledgers is not the best experience, so that needs to be solved. There are many problems that need to be solved for us to gain mainstream adoption. Even with these restrictions and friction points, we are seeing NFTs explode — it’s a two-pronged approach.
One is verticalization, where you have NFTs and DeFi attracting a lot more mainstream users — DeFi attracting traditional finance users and NFT’s attracting traditional collector users to improve the core infrastructure to support these verticals. So there’s a lot of work both horizontally and vertically that needs to happen.
With Akash and what we’re doing, we are making about $10,000 in monthly recurring revenue. That was $0 6 months ago. That revenue is coming from developers hosting websites on Akash instead of hosting them on Amazon.
Why? Because Akash is a functioning and usable product. You don’t have the gas fee limitations two-pronged in Ethereum because Akash is a sovereign chain. If I build something on Ethereum, you’ll end up paying, let’s say, $70 every time you want to update your blog. So that’s not feasible.
With Akash, it’s less a 1/4 a cent for the gas fee. The reason for that is that whatever happens on Akash Network is only related to Akash Network functions and transactions and nothing else. We’re not a Layer-1. So the sovereign blockchains are an amazing way to scale. Similar economics are on an Osmosis.
Osmosis is a decentralized exchange for mostly Cosmos ecosystem tokens now, and the gas fees on that are free. It’s next to nothing. My parents actually use Osmosis. My sister uses Osmosis, and they all love it compared to using something like Uniswap.
So a lot of these problems are getting solved, slowly but surely. Something like Osmosis that was launched three months ago, has about half a billion dollars in TVL without an Ethereum bridge. That’s something to something to say for. Once these bridges come, I think their TVL is just going to explode. But, without a bridge, coming just from the Cosmos ecosystem, half a billion dollars in TVL is very usable.
It’s actually very pleasant if you use Osmosis. One of the beautiful things about crypto, in general, is the unified identity — I think that’s underrated, right? So I use something called Keplr as my browser base wallet for Akash, and I can use the same wallet to login to Osmosis. There’s not a single username, password, or email address. I get a form or a CAPTCHA I need to solve. If crypto solves the CAPTCHA problem, that alone is a good reason for mainstream adoption.
[40:49] The Future of Akash
Chris Sinkey: Yeah, I agree. For those listening on the phone, a couple of definitions. TVL stands for Total Value Locked in an ecosystem. And GMV stands for Gross Merchandising Value. I just wanted to make sure we weren’t losing people with our acronyms. So you talked about it a little bit earlier, but just to kind of wrap things up based on what you can share, what are the short and long-term visions that you have for Akash Network? Let’s say it’s six months, one year, five years?
Greg Osuri: Yeah, we published a roadmap recently, if you go to akash.network/roadmap, you’ll see a very detailed, thorough roadmap as to what we’re going to do in the short term in 2022. Right now, Akash is an amazingly usable platform, but it’s bare-bones. It can run these things called containers, where your application is completely packaged into this small, shippable unit called containers, and you can ship that to Akash, and Akash will run for you. But it’s bare bones.
A big challenge for Akash right now is you got to manage your infrastructure yourself. So the long-term vision by the end of 2022, we’re going to have a services marketplace where you do not need to manage a lot of infrastructure on Akash, but rather these infrastructures are managed by someone else in a decentralized way. As I said, Amazon has about 300 services. Akash will have a similar offering but with a lot more services because it’s decentralized and permissionless.
Once we can manage services, I think we’ll capture a significantly higher market share than Amazon is currently thriving on. Before that, we’re going to have GPUs. I’m really excited about GPUs because GPUs are the foundation for machine learning. Machine learning is extremely expensive on the cloud today. Almost on an annual basis, reports talk about how expensive it is to run machine learning or data-intensive workloads.
Look at public companies. The paperwork they file before going public, you’ll notice something like Snowflake, which spends about 50% of its margins on the cloud today, especially machine learning. So any optimization, even 10% to 20% reduction in that cloud bill, is a big deal for a lot of these companies.
The majority of the cloud bill is for machine learning. So we’re going to enable GPUs, and we’re going to offer them significantly lower. Why? Because there’s a ubiquitous amount of GPUs all around the world. Ethereum, moving from proof-of-work to proof-of-stake, will free up enormous amounts of GPUs that have nowhere to go.
Apple announced its M1 plans. The new laptops, don’t quote me on this, but these are 24 core GPUs on them that are not being used most of the time. You’re going to have ubiquitous amounts of GPUs that are all over the spread across different computers all around the world.
And more interestingly, we’ve been having early conversations with autonomous vehicle manufacturers, and some of these AVs have six to seven GPUs on the car. These are T20 chips — they are very powerful GPUs that are not used when they’re being charged in the night. So those GPUs can go on Akash Network too.
So there are a lot of GPUs that can go on Akash Network. That’ll unlock machine learning and data-intensive workloads. Someone like Snowflake will be more than happy to save 10 to 20% on the 50% margin that they’re paying to Amazon. So I think that’s going to put Akash on a different plane altogether. I’m very excited about a lot of these technologies that you’ll soon see on Akash.
Chris Sinkey: When do you think Ethereum is going to actually move to proof-of-stake and free up those GPUs?
Greg Osuri: I think another nine months to a year’s time. It’s slowly happening, but it’s a gigantic decentralized network. I think at the end of the year, they will move away from proof-of-work.
Chris Sinkey: I think that’s a good segue into my last question for you, which is definitely more general and sort of away from Akash. But it’s something we’d like to ask everybody that comes on, which is, what are you excited about in the space regarding other projects or other cryptocurrencies that you’re watching that you think the audience should be paying attention to as well?
Greg Osuri: I want to say something, but I don’t want to get in trouble.
Chris Sinkey: No, you’re not going to get in trouble.
Greg Osuri: I’m very excited about Cosmos’ ecosystem. I think Cosmos’ ecosystem is probably the best-built ecosystem right after Ethereum that is not getting the attention it deserves. With some amazing projects like dVPN, Persistence, Terra, and Secret Network, I’m excited because Cosmos’ ecosystem enabled IBC, or inter blockchain communication. It’s the only ecosystem where you have multichain that’s fully functioning, secure, and composable, so I’m very excited.
There’s a website called map of zones — I highly recommend it. Check it out, and you can visualize all these different blockchains communicating with each other in an incredible way. So I’m very bullish on Cosmos’ ecosystem, and things that I’m super bullish that are not yet live are something like Strange Clan or NFTs; NFT gaming in particular. I think NFTs as collectibles are great. But, when you can use those collectibles in a productive way, I’m super bullish on that.
Another area I’m really excited about is cross-chain composability. Agoric is doing some amazing things. Mark Miller and Dean Tribble founded Agoric. Some history — they wrote these papers in the 1980s. Nick Szabo, the creator of smart contracts, used to work for these guys, Dean Tribble. So you certainly want to watch out for this team called Agoric.
They’re doing cross-chain composability, and these guys really know smart contracts in and out. So in another network called Juno Network, which is on Akash recently launched, sorry not on Akash, on Cosmos, I believe they’re using Akash as well. I’m really excited about that. So the areas I’m really excited about are cross-chain composability, DWS, and Cosmos’ ecosystem.
Chris Sinkey: Well, I got some studying to do; I want to go dig in and learn. In my day job here at Bittrex Global, I don’t get a lot of time to dig into the ecosystem, so we’re heads down trying to run a company. It’s conversations like these that get me really excited about the future. I think, for everybody listening, you just got a masterclass in the future. So I hope you paid attention.
Greg Osuri: I picked Solana in 2018. I like to pick projects that are not launched but show great progress. Actually, if you go to Solana on YouTube, you’ll see one of the first and second videos. It’s a video of me interviewing Anatoly in 2019. You can’t see our faces because it’s a really dark and terrible video, but we can certainly hear voices. I told people why I’m really excited about Solana ad not I wish people took me seriously back then. But nevertheless…
Chris Sinkey: I think they’re taking you seriously now.
Greg Osuri: Nevertheless, I’m really excited about pre-launch tokens, Agoric and Strange Clan, to name a few.
Chris Sinkey: Thank you so much Greg for coming on the podcast and talking with us about Akash Network and all things crypto, and blockchain. I know I learned quite a bit, and I hope our audience did as well about everything that we discussed here today.
Chris Sinkey: We’re definitely looking forward to hearing the news that’s up and coming about Akash in the coming weeks, so we’ll stay tuned for that, and thank you so much again for choosing to list Akash on Bittrex Global.
Greg Osuri: Thank you so much, Chris; I had a great time as well.
Chris Sinkey: Thanks for listening to The Bit. The Bittrex Global podcast. Our guest today was Greg Osuri, Co-Founder and CEO of Akash Network. To learn more about Akash Network, visit akash.network that’s A K A S H dot N E T W O R K. To learn more about Bittrex Global, visit global.bittrex.com or just go to bittrex.com, and it will redirect you.
Please make sure to subscribe to our podcast. You can find us everywhere you get your podcasts. Thank you for listening, and thanks for making The Bit one of the fastest-growing podcasts in the world of crypto. I’m Chris Sinkey, again, Chief Business Officer at Bittrex Global.
Team of one of the most secure trading platforms and digital wallet infrastructures in the world! https://global.bittrex.com
Publications from the most secure Bitcoin & crypto trading platform and digital wallet infrastructure in the world!
"
https://medium.com/@richardyaoipg/cloud-computing-takes-us-by-storm-caa8ed9aa486?source=search_post---------349,"Sign in
There are currently no responses for this story.
Be the first to respond.
Richard Yao
Apr 2, 2009·2 min read
April 1, 2009–There’s been a great deal of discussion lately about cloud computing. However, many people don’t understand what cloud computing really means, what it can do, and why it is so important. To address these issues, the lab has put together a handy new guide. First though, one needs to understand the ideas behind cloud computing.
About ten years ago, a number of computer engineers realized that they would soon fill the tubes connecting the Internets with things like email spam, viruses, and bittorrents. Our tubes were going to become sewers. Something had to be done.
To really understand the brilliance of cloud computing, one needs to take a moment to look up at the sky. It’s big, it’s blue, and generally speaking it’s empty, except for those big puffy cloud things. And there’s the opportunity. Rather than running data through the tubes in the ground, we can run it through the clouds. The best type of cloud computing is Nimbus. There’s tons of storage, and allows widespread access.
Cumulus is another highly desired cloud computing solution, as it can be densely packed with processing power. Thin and wispy clouds are less than ideal, and can prove problematic if relied on as primary infrastructure.
However, there are certain obstacles cloud computing needs to rise above. High winds can prove devastating to cloud arrays. Also, if clouds become too saturated, data leakage is very likely. The trick to effective cloud computing is linking in via very high towers, or on mountains. Also, maintenance of these rigs can be difficult, so finding the right talent is key. For cloud computing, it’s best to hire Care Bear sys-admins, as their overall performance is unmatched.
It’s time to put the umbrella away — cloud computing is on its way, and here to stay.
…Happy April Fools! Affectionately, the Lab.
Manager of Strategy & Content, IPG Media Lab
Manager of Strategy & Content, IPG Media Lab
"
https://medium.com/waospi/washington-state-and-amazon-web-services-to-train-2-500-k-12-students-in-cloud-computing-skills-by-8c9712c307a7?source=search_post---------350,"There are currently no responses for this story.
Be the first to respond.
Today, at an event in Seattle, the Washington Office of Superintendent of Public Instruction (OSPI) announced a strategic collaboration with Amazon Web Services (AWS) to train and certify 2,500 K-12 students in cloud computing skills within the next three years. Along with support from the Washington Training and Education Coordinating Board, this statewide initiative will provide high school students with technical training and education mapped to in-demand skills, paving the way for careers in tech.
“Our education system plays the vital role of preparing our young people for success after high school,” said Chris Reykdal, Washington’s State Superintendent of Public Instruction. “Our economy and the needs of employees and employers are evolving, and the K-12 system has to evolve with it. I’m thankful for this collaboration with AWS, which will make a difference for many of our students.”
OSPI will implement the first statewide collaboration with AWS focused exclusively on training and educating K-12 students. Through AWS education programs such as AWS Academy, participating Washington high schools will have access to ready-to-teach, cloud computing curriculum that helps prepare students for in-demand jobs and industry-recognized AWS Certifications. Secondary education institutions in Washington may receive free professional development, technical training, and a select number of certification exams for educators who will begin teaching cloud computing courses to their students starting next fall in 2022.
“Washington state has been a global leader in technology development for decades. This new initiative will help provide the next generation of Washington students with the know-how needed to take on and create the jobs of tomorrow,” said Lieutenant Governor Denny Heck. “Through this initiative, students across Washington will be able to obtain skills to make them successful and competitive in the global workforce. This initiative also provides an opportunity for equitable, substantial educational development for all students across our state — east, south, north, west, and central. I’m excited that Washington state is piloting this initiative using AWS education programs and building up the diverse, high-skill workforce we need.”
Cloud computing jobs are in high demand across the U.S. According to Economic Modeling Specialist International (EMSI), in 2020, Washington experienced 165,486 unique cloud job postings, with 88,642 of those jobs going unfilled. The Washington Training and Education Coordinating Board is a critical participating organization that will help connect students to cloud education and career pathways, while increasing the talent pipeline for employers across industries in the state.
“Washington is the world’s epicenter for cloud computing — driving cloud-related job growth almost exponentially,” said Eleni Papadakis, Executive Director, Washington State Workforce Training and Education Coordinating Board. “This innovative public-private collaboration will pave the way for many more of our young people to access high demand, well-paid job opportunities in our state’s technology-driven economy.”
Washington state will use AWS education programs to modernize and expand technical education course offerings to help provide the future workforce with the foundational skills employers need. As OSPI continues to expand career and technical education, allowing students the opportunity to earn tech industry certifications in high school will help to open doors to jobs post-graduation and ease the transition to post-secondary education. Participating schools and educators will use content and instructional tools from AWS education programs to help students prepare for several AWS certifications, including AWS Certified Cloud Practitioner, AWS Certified Solution Architect — Associate, and AWS Certified Developer — Associate, and more. Training includes a vast number of technical topics such as artificial intelligence, big data, machine learning, cybersecurity, and other cloud-related learning.
“Research tells us that when you spark a student’s interest in STEM earlier in their education journey, they are more likely to explore related careers before even enrolling at a college or university,” said Kim Majerus, Vice President, US Education, State and Local Government at AWS. “We want to inspire the next generation of tech leaders through engaging and modern cloud computing curricula that have students excited to take the next step in pursuing a tech career — whether that’s securing a job through a certification or advancing their knowledge at a higher-education institution. Washington state and OSPI have created an opportunity with AWS to empower high school students in one of the nation’s fastest growing tech hubs at an age where teens are thinking about future careers.”
All students prepared for post-secondary pathways, careers, and civic engagement.
Written by
Led by Supt. Chris Reykdal, OSPI is the primary agency charged with overseeing K–12 education in Washington state.
Except where otherwise noted, content by the Office of Superintendent of Public Instruction is licensed under a Creative Commons Attribution 4.0 International License. All logos and trademarks are property of their respective owners.
Written by
Led by Supt. Chris Reykdal, OSPI is the primary agency charged with overseeing K–12 education in Washington state.
Except where otherwise noted, content by the Office of Superintendent of Public Instruction is licensed under a Creative Commons Attribution 4.0 International License. All logos and trademarks are property of their respective owners.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@TechJobs_NYC/daily-report-cloud-computing-asserts-itself-821c1f99c816?source=search_post---------351,"Sign in
There are currently no responses for this story.
Be the first to respond.
Technology Jobs NYC
Apr 28, 2017·1 min read
Daily Report: Cloud Computing Asserts Itself By JIM KERSTETTER Amazon Web Services has become the profit engine of the internet retailer. Can it stay ahead of rivals like Microsoft and Google? Published: April 27, 2017 at 02:00PM via NYT Technology https://www.nytimes.com/2017/04/28/technology/daily-report-cloud-computing-asserts-itself.html?partner=IFTTT
The official account for https://www.technologyjobs.nyc! Follow for exclusive job opportunities, tips and tricks to help you get your next #technology #job
The official account for https://www.technologyjobs.nyc! Follow for exclusive job opportunities, tips and tricks to help you get your next #technology #job
"
https://medium.com/alfonsofuggetta-it/pa-digitale-e-cloud-computing-806d089a42d9?source=search_post---------352,"There are currently no responses for this story.
Be the first to respond.
Mio articolo su agendadigitale.eu:
Pa digitale, gli ostacoli verso la grande cloud: “In conclusione, il cloud è una grande opportunità, ma rischia di essere un’altra occasione perduta e un’ulteriore fonte di delusione. Si rischia di investire nuove risorse senza avere quei ritorni e quei benefici che oggi vengono declamati e attesi. Certamente, le PA devono fare un salto di qualità nelle proprie capacità di procurement e di evoluzione tecnologica e organizzativa. Ma è anche vitale che il mondo dell’offerta evolva e si attrezzi per essere un reale partner di innovazione per le PA. È inutile e sterile limitarsi a tradurre le presentazioni e i servizi standard offerti alle aziende di dimensioni medio-grandi dei paesi anglosassoni: è necessario saper declinare soluzioni e proposte affinché siano capaci di cogliere e soddisfare bisogni, esigenze, aspettative di un mondo complesso e articolato, che richiede sempre più partner affidabili e capaci, e non solo efficienti strutture commerciali e di vendita.”
Alfonso Fuggetta
Blog
Written by
Insegno Informatica al Politecnico di Milano e lavoro al Cefriel. Condivido su queste pagine idee e opinioni personali.
Blog
"
https://medium.com/@toxsltechnologies/how-will-edge-computing-change-the-future-of-cloud-computing-68414a333c4d?source=search_post---------353,"Sign in
There are currently no responses for this story.
Be the first to respond.
ToXSL Technologies
Dec 17, 2021·5 min read
The last decade showed a rise in the paradigm of cloud computing while the main aspects included centralization of storage and networking management in the cloud. A growing trend of mobile devices such as smartphones, computers, wearable devices is advancing the IoT and revolutionizing mobile applications.
Businesses run most of their applications in the cloud and therefore the end-user devices need computing power to support the rigorous activities as per the behavior and need. This creates an issue related to high network latency. To support the end-user activities Edge Computing was introduced.
Combining the vision of 5G and IoT edge computing is one of the leading and emerging technologies that is receiving huge attention in the market. Gartner, a research firm predicted that by 2025, around 75% of the enterprise data would be generated and processed at the edge. It is defined as a practice of processing and storing data where it is created or close to where it is generated.
It focuses on reducing the latency and the time it takes for an application to run a command.
Current State of Cloud Computing
1. The cloud Architecture
Cloud computing relies on cloud architecture. Cloud services are taken differently by the user. From the current cloud architecture, when the user requests, it gets transferred from the device to the data center via a network connection. Here the cloud is capable of handling work seamlessly so that there is no failure.
It functions as a network and even if one component fails, services are delivered from other active components.
2. Service Latency and Network Delay
Latency refers to the delay between the service request of a client and the cloud service provider response. Numerous factors affect latency that could ultimately lead to delay on network communication sides.
The cloud service data center can be located anywhere in the world, the network delay can vary dependently and increase based on the distance from the main data center. The integration of 5G technology is laying foundations for edge computing and offering solutions for latency issues.
The Emergence of Edge Computing
Introduction of 5G Technology
There is a boom in the IoT as most of the services are integrating and accompanying it. From smart homes to appliances and AI, there are enormous changes that need to be handled carefully in such a diverse environment.
5G networks offer higher flexibility, adaptability, and connectivity, therefore are becoming the cloud future. It requires low latency and high reliability, reduced cost, and support of different devices and apps. Many challenges need to be taken care of.
Edge computing is the latest trend in the cloud that potentially helps to overcome all the major challenges.
Edge Computing- a solution for the latency problem
Relying on a cloud data center is not a piece of appropriate advice for the applications that need the delays to be managed and controlled tightly. With the joined efforts experts are doing experiments with edge computing to solve the latency problems and bring the idea to life.
Technologies that make Edge computing more Powerful
1. Extended Reality (XR)
It offers a truly immersive and emerging umbrella term for the users to work in a virtual environment. It helps to reduce costs, increase revenue and productivity while offering a better customer experience. Adding edge helps to create an even more interactive experience for the users.
2. Privacy-preserving technology
It includes the techniques that allow the data to be analyzed not having been exposed to other aspects. Generally, the data are encrypted when stored and transmitted but with the privacy-preserving technology, the data is protected via the computing stage. This makes it efficient for the other businesses and partners especially when it needs to be on the edge.
3. Robotics
While the surgical control directly happens on the robots, the edge helps to coordinate with the cloud and help determine what all controls are deployed on the robot. It also offers information on what data is used, and what information is transmitted back.
Advantages of Edge Computing
Portability
The software is portable to different hardware platforms. This relieves them from being tied to a particular vendor’s hardware or software.
Operational Autonomy
It helps to make the solutions autonomous in their operations as edge computing can have local storage and local computation. It allows them to continue to function seamlessly even if it is not connected to the network.
Offers Security
From permission-based access control to encrypt communication- Edge computing improves the security and privacy of an IoT application.
It is cost-effective
The relevant data can be sent to the cloud and filtered based on threshold applications. Therefore, it reduces the cost of data transmission over the network, cost of cloud storage, and processing cost.
Eliminates Latency
Sensor data from the critical industrial operating and control system are processed with the edge gateways that eliminate latency and help achieve a quicker response time.
Also Read: What Are The Top Managed Cloud Services Challenges In 2021?
How will the edge computing drive cloud computing
1. Extend AI and IoT
Computing today is already happening on the edge in industries such as hospitals, the retail industry, and more. It operates on the most sensitive data and powers the most critical operations that need to be done safely. Edge can help to drive such decisions on the core system. It gives you an opportunity for AI and IoT tapping into these systems, there is an opportunity for an edge.
2. Creates a unique value
Controlling edge means controlling the data access at the closest point. This unique position creates differentiated services that can be used across businesses and among partners.
3. Differentiation with 5G, robotics, and more
To maximize the returns of next-generation technologies such as 5G, robotics and more edge computing is a must. Their collaboration helps and enables new features like voice assistance, working remotely via teleoperation, and more. Edge computing now offers the control that is required for any business.
Winding-up
The future of edge and cloud computing is quickly changing. As storage is becoming cheaper, network access is improving, there are tons of opportunities that are changing the world.
Edge computing is one of the major steps in the 5G era that allows serving while reducing network delays. It also helps to accelerate the processing time in the cloud, while improving the different aspects of current cloud computing.
Want to learn more about edge and cloud computing services and how it helps your organization, reach out to us. We help you strategize and build software with ease!
ToXSL Technologies is a leading Web and Mobile App Development company in India. Incorporated in the year 2012, we are ISO 27001 and ISO 9001:2015 certified.
ToXSL Technologies is a leading Web and Mobile App Development company in India. Incorporated in the year 2012, we are ISO 27001 and ISO 9001:2015 certified.
"
https://medium.com/alfonsofuggetta-it/dove-vanno-i-servizi-di-cloud-computing-37cf5bfb48cf?source=search_post---------355,"There are currently no responses for this story.
Be the first to respond.
Sto usando e provando diversi servizi su Internet:
Uso Flickr e Evernote da tempo. Ho fatto l’upgrade alla versione professional di entrambi. Evernote è comodo per prendere appunti e raccogliere materiale, docs, … Ovviamente, uso Flickr per le foto (anche se ultimamente non ne sto facendo molte).Bluehost è il servizio che fa l’hosting del mio sito e del mio blog. Offre una marea di servizi addizionali, ma non ha il servizio più ovvio: un web disk efficiente e facile da usare.
Adesso vorrei provare un servizio di document sharing (un disco di rete, appunto). La prima scelta sarebbe MobileMe, visto che offre anche una serie di servizi integrati con Mac OsX. Ma molte funzioni di MobileMe sono fornite anche da altri e meglio: penso appunto a Flickr, per esempio. Ma anche tutti i servizi di posta e sincronizzazione.
Per questo motivo, anche se ha meno funzioni (sostanzialmente disco di rete e document sharing), il migliore mi pare Dropbox: è facile da usare, gestisce i delta e quindi è più veloce, stanno sviluppando una API per permettere ad altri di basare le proprie applicazioni su Dropbox. Permette anche di avere URL per singoli documenti, così da facilitarne lo sharing (MobileMe ha una funzione simile ma meno comoda e immediata).
iwork.com è un servizio di Apple per condividere e commentare documenti. Per certi versi, anche se su scala molto minore, va direttamente in competizione con servizi come DimDim, WebEx similari. Funziona bene, ma in diverse cose si sovrappone a MobileMe.
Mi pare di poter dire che in realtà servizi come Dropbox potrebbero essere l’infrastruttura su cui creare servizi applicativi di document sharing, web conferencing, photo sharing e similari.
Il punto è che tutti questi servizi/sistemi hanno funzioni che si sovrappongono tra loro, a partire dallo spazio di disco offerto all’utente. Credo che anche qui si dovrà andare verso una segmentazione orizzontale tra servizi infrastrutturali come Dropbox e servizi applicativi come Flickr o iwork.com: noi utenti non possiamo permetterci il costo e anche la confusione di avere tre o quattro siti, con relativi spazi di disco, dove memorizzare e gestire porzioni, magari sovrapposte, di nostre informazioni e applicazioni.
Certo, questa segmentazione orizzontale è ciò che va bene a noi utenti e favorisce la concorrenza e la competizione. Ma ovviamente, non è ciò che piace alle imprese che vorrebbero catturare l’utente per tutti i servizi.
Vedremo come andrà a finire.
Alfonso Fuggetta
Blog
Written by
Insegno Informatica al Politecnico di Milano e lavoro al Cefriel. Condivido su queste pagine idee e opinioni personali.
Blog
"
https://medium.com/@TechJobs_NYC/cloud-computing-an-idea-with-old-roots-1938f95e1cfd?source=search_post---------356,"Sign in
There are currently no responses for this story.
Be the first to respond.
Technology Jobs NYC
Jul 20, 2016·1 min read
Cloud Computing, an Idea With Old Roots By JIM KERSTETTER Long before tech companies were competing for cloud customers, start-ups were calling themselves “application service providers.” Same thing. Published: July 20, 2016 at 06:00PM via NYT Technology http://www.nytimes.com/2016/07/21/technology/cloud-computing-an-idea-with-old-roots.html?partner=IFTTT
The official account for https://www.technologyjobs.nyc! Follow for exclusive job opportunities, tips and tricks to help you get your next #technology #job
The official account for https://www.technologyjobs.nyc! Follow for exclusive job opportunities, tips and tricks to help you get your next #technology #job
"
https://medium.com/@nutanix/top-6-challenges-in-cloud-computing-that-still-prevail-across-industries-d24a836ddd4f?source=search_post---------357,"Sign in
There are currently no responses for this story.
Be the first to respond.
Nutanix
Sep 16, 2016·4 min read
Enterprises across industries have already started to leverage the incredible benefits of cloud computing such as agility, flexibility, elasticity, consumption­-based pricing, cost, quality of service, and resilience. Cloud Platform Vendors like Amazon are unleashing torrents of new infrastructure and platform innovations like AWS Lambda every year. Ironically, these cloud innovations have opened up new challenges in cloud computing that IT decision makers have to factor in before finalizing their cloud strategy.
Let us look into the some of these key cloud computing challenges which IT decision makers have to factor in before finalizing their cloud strategy.
1) Service Governance in New Model of Ownership
To achieve business goals, lines of businesses are independently provisioning services by leveraging external cloud services and even managing their own infrastructure. Their successes have showed them the pace of innovation and agility that can come from outside of IT, and now there is no going back. Today, IT does not have full control over the provisioning, de-provisioning and operations of infrastructure. This de-centralized ownership has increased the complexity for IT to provide the governance, compliance and risk management required to protect their businesses.
IT needs to find new ways to exert soft controls to protect the business, while not inhibiting the agility their internal customers expect now from the cloud. Unfortunately, all the IT operations management tools available today are built to take advantage only of a centralized model. It is cloud management platform vendors who are working to find solutions on how to help IT to marry the traditional operations management tools with the de-centralized cloud platforms.
2) Service Provisioning
Service provisioning is related to service governance. Here, you allocate resources (such as cloud services) for use by an app or web-service for a period of time. And, the current approaches and mechanisms around service provisioning vary from vendor to vendor.
IT teams need a strategy for this very important aspect of cloud computing, which ideally would be a standard set of APIs for automating the entire cycle of service provisioning from allocation to de-allocation. While most cloud platform users expect a simplicity level closer to copy/paste, the reality of service provisioning is way too complex for the average users. And assistance of a cloud specialist is often recommended here.
Additionally, the user interface provided by cloud platform vendors for administrators cannot simultaneously address the needs of a single server instance customer and a large MNC having hundreds of server instances.
3) API Standardization
Today, the dependency of business on IT systems are very high due to digital economy. Most businesses do not want to rely exclusively on a single vendor for their cloud platform. And many enterprises also want to keep some of the key computing workloads in house as internal cloud environments and want a single standard platform to manage both the outsourced cloud platforms and internal cloud platforms (hybrid environments). For this to work, major cloud platforms have to standardize their APIs, which appear to be a major challenge at this stage of evolution in cloud platforms.
4) Security Standards and Frameworks
Security has been one of the prime concerns right from the start of the cloud computing technology. Incidents like data breaches, compromised credentials and broken authentication, hacked interfaces and APIs, account hijacking, etc. keep occurring. User Identity Management, authentication and access control mechanisms that are simple yet secure and that are acceptable to entire customer base of the cloud platforms can happen only through industry wide standards.
Another challenge lurking is that there are different tools to develop applications for each community cloud application platform. This makes it very difficult to build applications that span across multiple cloud providers. Common authentication across cloud providers is just as difficult with a myriad of different protocols and standards, including SAML and OAuth. At a higher level, service-level security refers to the ability to provide secure access to fine-grained cloud services by defining who and how each service is accessible. Currently it is not possible to manage service identities, just clusters of virtual or physical servers. There are some niche solutions through new emerging product categories such as Cloud Access Service Broker (CASB), but there is still a long way to go.
5) Specialized Knowledge for Cloud Administration
Cloud computing needs people who can build, run, and design cloud infrastructure. There is a dearth of talent specialized in cloud architecture and administration. As the cloud computing market grows, today’s niche training programs and certifications for specialized cloud skill sets have to get into mainstream so that the talent pool does not become a constraint for enterprises to adopt cloud platforms.
6) DevOps Tools for the Cloud
For many enterprises, a need to have cloud specialists in the IT teams would be prohibitively costly. Most of the common tasks performed by the cloud specialists must be automated. To address this challenge, Cloud Management Platform vendors like Botmetric are providing DevOps tools like monitoring usage patterns of resources, automated backups at pre-defined time periods, helping optimize the cloud for cost, governance, and security.
The Bottom Line
Cloud technology is rapidly evolving to address the above mentioned challenges. While cloud technology is a re-imagination of server infrastructure, there is also a need for re-invention of many server technologies due to need to adaptation of those technologies to cloud platforms.
While platform developers like Amazon, Google and Microsoft are busy innovating at the base level, cloud Management platforms vendors like Botmetric are at the forefront of innovation in creating comprehensive cloud management tools. Do read our blog by our experts on Cloud Integration Still A Challenge, Despite High Adoption Rates.
Tweet us @BotmetricHQ and share your thoughts with us, or tell us what challenges you’re facing in operating your cloud infrastructure. Connect with us on Facebook and LinkedIn. Also take up a 14-day free trial of Botmetric today, and see how it can help with AWS cloud automation.
We make infrastructure invisible, elevating IT to focus on the applications and services that power their business.
1 
1 
1 
We make infrastructure invisible, elevating IT to focus on the applications and services that power their business.
"
https://medium.com/free-code-camp/the-best-ways-to-test-your-serverless-applications-40b88d6ee31e?source=search_post---------358,"There are currently no responses for this story.
Be the first to respond.
Top highlight
Serverless is more than a cloud computing execution model. It changes the way we plan, build, and deploy apps. But it also changes the way we test our apps.
Meet Alex. Alex is an ordinary JavaScript developer, focused on Node.js lately.
Over the last couple of months, his good friends, Anna and Jeff, are always talking about that serverless thingy. Even through they are annoying from time to time, he likes the idea of serverless apps. He even deployed few simple functions to AWS Lambda and Azure at some point.
At some point, Alex and his team got a new project. After some analysis, Alex thought that it would be the perfect fit for serverless. He presented the idea to his team. Some of the team members were excited, one of them didn’t like it, but most of them didn’t have a strong opinion. So, they decided to give it a try — the project wasn’t too big, and the risk was low.
The team read about serverless, and they got an idea how to structure their new app. But no one was sure how they should fit serverless into their common development process.
At that moment, their process looks like this:
They decided to start step by step, and then solve the problems as they encountered them.
They picked a small feature, and as it was simple, they started with the code. When the coding part was ready, they hit the first roadblock: how do you run serverless applications locally?
With serverless apps, you don’t manage the infrastructure. Sounds great, but how do you then run your application locally? Can you even do that?
Depending on your app and serverless vendor, you can run some parts of your app locally. To do so, you can use some of the following tools and techniques:
Of course, the list is not complete — there are more tools, and we see new tools almost every day now.
Most of these tools have certain limitations. They can simulate serverless functions and a few other services, such as API Gateway. But what about permissions, auth layer, and other services?
Local testing helps with quick validations to make sure your function works. But is there a better way to make sure your serverless app is working as intended? Yes there is. The first and most important step is: write tests.
So Alex and his team tried their first function locally, and as it seemed to be working. Then they went to the next step.
Alex and his team just switched to Jest for testing their Node.js applications. They still do a lot of front end, so they want to use the same tools for the full stack whenever they can. Can they use Jest for testing serverless apps too? And what should they test?
After a quick investigation, they realized that they can use their favorite Node.js testing tools. Jest, Jasmine, Mocha and others work fine with serverless.
With their Node.js apps, Alex and his team follows the three-tier test automation pyramid. The test pyramid was first mentioned by Mike Cohn in his book “Succeeding with Agile”.
As the test pyramid defines, they have:
Besides these, they also have manual session-based testing, done by their QA team.
How does serverless affect the test automation pyramid?
The answer depends on the tier. But the test pyramid looks less like the Egyptian pyramids, and more like the Mayan pyramids.
The unit tests layer is not affected a lot. Unit tests are still the cheapest to write and run, but the units can be smaller.
Integration tests layer becomes more important than ever, because serverless apps relies heavily on integrations. It is also cheaper, because having a serverless database just for testing is cheap. So, in a serverless “test pyramid” you need to have more integration tests.
GUI tests layer is also cheaper and faster, because of cheaper parallelization.
Manual testing layer stays the same. But serverless can help you to improve it slightly. We’ll go into the details on that later.
Alex and his team finally had some idea where to focus. The next problem was how to write a function to test them more easily.
You need to think about the following risks while you are writing a serverless function:
To confirm that your serverless function is working correctly, you need to test all these risks.
You could test each of these as you did for the integration tests. But setting up and configuring the service each time you want to test for one of these risks isn’t optimal. As my friend Aleksandar Simovic loves to say:
Imagine if testing automobiles was done that way. That would mean that every time you wanted to test a single screw or even a mirror in a car, you would have to assemble and then disassemble the whole car.
To make the app more testable, the clear solution is to break up your function into several smaller ones.
One of the great ways to do so is applying Hexagonal Architecture to your serverless functions.
Hexagonal Architecture, or Ports and Adapters, is a form of application architecture that promotes the separation of concerns through layers of responsibility. As its creator, Alistair Cockburn, explains:
Allow an application to equally be driven by users, programs, automated test or batch scripts, and to be developed and tested in isolation from its eventual run-time devices and databases.
So, how does that apply to serverless functions?
As Alex and his team use AWS, they ended up with a structure like the following:
Alex and his team were happy that they were moving forward. But before we move on, let’s see how Hexagonal Architecture affects each tier of the test pyramid.
Unit tests stayed the same. But it’s easier to write unit tests because of Hexagonal Architecture. They can simply use a local adapter or mock as an adapter to test the function business layer in isolation.
Integration tests benefited a lot from Hexagonal Architecture. They were able to fully test integrations that they own. Third-party integrations are simulated with other adapters.
How does that work in practice?
Each of their serverless functions has lambda.js and main.js files. The main file contains the business logic of a serverless function. And the lambda.js file is in charge of wiring the adapters and invoking the main.js file.
The main file has its own unit and integration tests. But its integration tests don’t test full integration with end services, such as AWS S3, because that would slow them down. Instead, they use an in-memory adapter to test the function with file storage integration.
AWS S3 integration is done through the FileRepository, which has its own unit and integration tests. Integration tests checks use AWS S3 to be sure that the end integration actually works.
As opposed to main.js, the lambda.js file doesn’t have tests, because most of the time it has just a few lines of code.
This approach is like the technique the MindMup team is using for testing serverless functions. With it, you can easily test integrations of your functions, and still make your integration tests faster.
As Alex and his team were building a back end for the app, the GUI tests tier was not relevant. But as they learned more about serverless, they realized that they could use it to improve the GUI tests tier for the other apps they were working on.
UI tests are expensive and slow, because they run in the browser. But, serverless is cheap and it scales fast.
If they could run a browser in AWS Lambda, they would gain cheap parallelization. That would make their UI tests cheaper and faster.
But, can you run a browser, such as Chrome, inside a serverless function?
Yes! And it is easy with the help of a tools such as Serverless Chrome, Chromeless, and Puppeteer.
A combination of serverless and headless browsers can bring us a new generation of UI testing tools. We can already see and try some of them, such as Appraise.
As Alex and his team tested their first serverless function, it was time to deploy the code to the testing environment. That brought up a new question: how can they use CI/CD tools to deploy their serverless app?
The answer is simple: they can use a CI tool to run the tests and deploy the app. To deploy the app, use any popular tool, such as Claudia.js, AWS SAM, and Serverless Framework.
You can still use your favorite CI tool (like Jenkins, TravisCI or SemaphoreCI), or if you want to stick with AWS, you can try AWS CodeBuild.
Even through manual testing is not directly affected by serverless, the team found a way to improve their QA process.
Stages and deployments of serverless app are cheap and often fast to setup. Also, with serverless, you don’t pay for the app if no one is using it.
This means that having a testing environment has never been cheaper!
Also, with serverless, you can often promote the function from one stage to another. This means that your QA team can test a function, and when they confirm that it works, you can promote the same function to production.
Alex and his team shipped their first serverless function to pre-production, and the team was happy that they learned how to test serverless apps.
They continued using serverless on the project, and introduce it to few other projects. Alex joined his friends Anna and Jeff, as a third, sometimes annoying, serverless preacher. And they lived happily ever after.
But even though their app was well-tested, something happened overnight.
After an investigation, they found out that one of the integrations changed. They learned that testing is important for serverless apps, but it’s not enough.
As serverless apps heavily depend on integrations, the risk shifts from your code to the integrations. And, to be able to catch integration changes and react fast, your app needs proper monitoring.
Fortunately, there are more and more serverless monitoring tools on the market every day. Some of the good and popular options are IOpipe, Thundra, Dashbird, and Epsagon.
But, serverless apps often have a thick client, which means that back end monitoring is not enough. You need a similar tool for your front end. This market has a lot of nice tools too, such as Sentry and Rollbar.
But in the spirit of serverless, we created an open source error-tracking app called Desole. It is a serverless app you can install in your AWS account. It enables organisations to track application exceptions and errors without having to choose between the convenience of software-as-a-service and the security of a self-hosted solution. You can check it out here: https://desole.io.
All illustrations are created using SimpleDiagrams4 app.
If you want to learn more about testing and building serverless apps using Node.js and AWS, check out “Serverless Applications with Node.js”, the book I wrote with Aleksandar Simovic for Manning Publications:
www.manning.com
The book will teach you more about serverless testing, with code examples, but you’ll also learn how to build and debug a real world serverless API (with DB and authentication) using Node and Claudia.js. And you’ll learn how to build chatbots, for Facebook Messenger and SMS (using Twilio), and Alexa skills.
We’ve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
4.4K 
12
4.4K claps
4.4K 
12
Written by
Programmer. CTO of Cloud Horizon and JS Belgrade meetup organizer. Co-author of https://www.manning.com/books/serverless-apps-with-node-and-claudiajs.
We’ve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
Written by
Programmer. CTO of Cloud Horizon and JS Belgrade meetup organizer. Co-author of https://www.manning.com/books/serverless-apps-with-node-and-claudiajs.
We’ve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://towardsdatascience.com/key-kubernetes-concepts-62939f4bc08e?source=search_post---------359,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jeff Hale
Apr 10, 2019·12 min read
Cloud computing, containerization, and container orchestration are the most important trends in DevOps. Whether you’re a data scientist, software developer, or product manager, it’s good to know Docker and Kubernetes basics. Both technologies help you collaborate with others, deploy your projects, and increase your value to employers.
In this article we’ll cover essential Kubernetes concepts. There are a lot of Kubernetes terms, which can make it intimidating. I’ll help you make a mental model to speed your understanding of the technology.
"
https://towardsdatascience.com/10-machine-learning-algorithms-you-need-to-know-77fb0055fe0?source=search_post---------360,"Sign in
There are currently no responses for this story.
Be the first to respond.
Sidath Asiri
Jan 1, 2018·6 min read
We live in a start of revolutionized era due to development of data analytics, large computing power, and cloud computing. Machine learning will definitely have a huge role there and the brains behind Machine Learning is based on algorithms. This article covers 10 most popular Machine Learning Algorithms which uses currently.
These algorithms can be categorized into 3 main categories.
Following algorithms are going to be covered in this article.
Linear Regression algorithm will use the data points to find the best fit line to model the data. A line can be represented by the equation, y = m*x + c where y is the dependent variable and x is the independent variable. Basic calculus theories are applied to find the values for m and c using the given data set.
Linear Regression has 2 types as Simple Linear Regression where only 1 independent variable is used and Multiple Linear Regression where multiple independent variables are defined.
“scikit-learn” is a simple and efficient tool using for machine learning in python. Following is an implementation of Linear Regression using scikit-learn.
This belongs to classification type algorithm. The algorithm will separate the data points using a line. This line is chosen such that it will be furthermost from the nearest data points in 2 categories.
In above diagram red line is the best line since it has the most distance from the nearest points. Based on this line data points are classified into 2 groups.
This is a simple algorithm which predicts unknown data point with its k nearest neighbors. The value of k is a critical factor here regarding the accuracy of prediction. It determines the nearest by calculating the distance using basic distance functions like Euclidean.
However, this algorithm needs high computation power and we need to normalize data initially to bring every data point to same range
Logistic Regression is used where a discreet output is expected such as the occurrence of some event (Ex. predict whether rain will occur or not). Usually, Logistic regression uses some function to squeeze values to a particular range.
“Sigmoid” (Logistic function) is one of such function which has “S” shape curve used for binary classification. It converts values to the range of 0, 1 which interpreted as a probability of occurring some event.
y = e^(b0 + b1*x) / (1 + e^(b0 + b1*x))
Above is a simple logistic regression equation where b0, b1 are constants. While training values for these will be calculated such that the error between prediction and actual value become minimum.
This algorithm categorizes the population for several sets based on some chosen properties (independent variables) of a population. Usually, this algorithm is used to solve classification problems. Categorization is done by using some techniques such as Gini, Chi-square, entropy etc.
Let’s consider a population of people and use decision tree algorithm to identify who like to have a credit card. For example, consider the age and marital status the properties of the population. If age>30 or a person is married, people tend to prefer credit cards much and less otherwise.
This decision tree can be further extended by identifying suitable properties to define more categories. In this example, if a person is married and he is over 30, they are more likely to have credit cards (100% preference). Testing data is used to generate this decision tree.
This is an unsupervised algorithm which provides a solution for clustering problem. The algorithm follows a procedure to form clusters which contain homogeneous data points.
The value of k is an input for the algorithm. Based on that, algorithm selects k number of centroids. Then the neighboring data points to a centroid combines with its centroid and creates a cluster. Later a new centroid is created within each cluster. Then data points near to new centroid will combine again to expand the cluster. This process is continued until centroids do not change.
Random forest can be identified as a collection of decision trees as its name says. Each tree tries to estimate a classification and this is called as a “vote”. Ideally, we consider each vote from every tree and chose the most voted classification.
This algorithm is based on the “Bayes’ Theorem” in probability. Due to that Naive Bayes can be applied only if the features are independent of each other since it is a requirement in Bayes’ Theorem. If we try to predict a flower type by its petal length and width, we can use Naive Bayes approach since both those features are independent.
Naive Bayes algorithm also falls into classification type. This algorithm is mostly used when many classes exist in the problem.
Some datasets may contain many variables that may cause very hard to handle. Especially nowadays data collecting in systems occur at very detailed level due to the existence of more than enough resources. In such cases, the data sets may contain thousands of variables and most of them can be unnecessary as well.
In this case, it is almost impossible to identify the variables which have the most impact on our prediction. Dimensional Reduction Algorithms are used in this kind of situations. It utilizes other algorithms like Random Forest, Decision Tree to identify the most important variables.
Gradient Boosting Algorithm uses multiple weak algorithms to create a more powerful accurate algorithm. Instead of using a single estimator, having multiple will create a more stable and robust algorithm.
There are several Gradient Boosting Algorithms.
The specialty of Gradient Boosting Algorithms is their higher accuracy. Further, algorithms like LightGBM has incredible high performance as well.
Thanks for reading.
Cheers!
Associate Technical Lead at SyscoLABS
3.6K 
5
3.6K 
3.6K 
5
Your home for data science. A Medium publication sharing concepts, ideas and codes.
"
https://towardsdatascience.com/algorithmic-trading-bot-python-ab8f42c37145?source=search_post---------361,"Sign in
There are currently no responses for this story.
Be the first to respond.
Rob Salgado
Nov 24, 2019·8 min read
The rise of commission free trading APIs along with cloud computing has made it possible for the average person to run their own algorithmic trading strategies. All you need is a little python and more than a little luck. I’ll show you how to run one on Google Cloud Platform (GCP) using Alpaca. As always, all the code can be found on my GitHub page.
The first thing you need is some data. There are a few free sources of data out there and of course sources that cost money. I’ll be using the TD Ameritrade API which is free. The next thing you need is a trading platform where…
"
https://medium.com/crowdbotics/how-to-build-a-serverless-backend-with-aws-lambda-and-nodejs-e0d1257086b4?source=search_post---------362,"There are currently no responses for this story.
Be the first to respond.
Top highlight
Serverless architecture is a cloud computing execution model where a cloud provider like AWS, Azure or Google Cloud is used to deploy backend or server-side code. In comparison to traditionally deployed web applications, in serverless architecture, the developer does not has to maintain the servers or the infrastructure. They only have to pay a subscription to the third party vendor whereas the vendor is responsible to handle the operation of the backend logic of a server along with scalability, reliability, and security.
There are two ways a serverless architecture can be implemented in order to deploy your server-side code. First one is Backend as a Service or BaaS. A good example of this is Firebase which you can often see in conjunction between a web or a mobile application to a database or providing user authentication.
What we are going to focus in this article is called Function as a Service or FaaS. With FaaS, the server code is run inside containers that are usually triggered by common events such as HTTP requests from the client, database operations, file uploads, scheduled events and so on. The code on the cloud provider that is deployed and getting executed is in the form of a function.
In FaaS, these functions are deployed in modular form. One function corresponds to each operation, thus eliminating the rest of the code and time spent on writing boilerplate code for setting up a server and data models. These modular functions can further be scaled automatically and independently. This way, more time can be spent on writing the logic of the application that a potential user is going to interact with. You do not have to scale for the entire application and pay for it. Common use cases of FaaS so far have been implemented are scheduled tasks (or cron jobs), automation, web applications, and chatbots.
Common FaaS service platform providers are:
In the following tutorial, we are going to create a demo to deploy on a serverless infrastructure provider such as AWS Lambda.
In order to build and deploy a backend function to handle a certain operation, I am going to start with setting up the service provider you are going to use to follow this article. AWS Lambda supports different runtimes such as Node.js, Java, Python, .NET Core and Go for you to execute a function.
The function runs inside a container with a 64-bit Amazon Linux AMI. You might be thinking, ‘why I am telling you all of this?’ Well, using serverless for the first time can be a bit overwhelming and if you know what you are getting in return, that’s always good! More geeky stuff is listed below.
The execution duration here means that your Lambda function can only run a maximum of 5 minutes. This does mean that it is not meant for running longer processes. The disk space is the form of a temporary storage. The package size refers to the code necessary to trigger the server function. In case of Node.js, this does mean that any dependencies that are being imported into our server (for example, node_modules/ directory).
A typical lambda function in a Node.js server will look like below.
In the above syntax, handlerFunction is the name of our Lambda function. The event object contains information about the event that triggers the lambda function on execution. The context object contains information about the runtime. Rest of the code is written inside the Lambda function and at last a callback is invoked with an error object and result object. We will learn more about these objects later when are going to implement them.
In order to setup a Lambda function on AWS, we need to first register an account for the access keys. Use your credentials to login or signup a new account on console.amazon.com and once you are through the verification process you will be welcomed by the following screen.
To get the keys and permissions in order to deploy a function, we have to switch to Identity and Access Management (IAM). Then go to Users tab from the left hand sidebar and click on the button Add user. Fill in the details in the below form and do enable Access Type > Programmatic Access.
Then on the next page, select Attach Existing Policies Directly and then select a policy name AdministratorAccess.
Click Next: Review button and then click Create User button when displayed. Proceeding to the next step you will see the user was created. Now, and only now, will you have access to the users Access Key ID and Secret Access Key. This information is unique for every user you create.
We are going to use install an npm dependency first to proceed and scaffold a new project. Open up your terminal and install the following.
Once installed, we can run the serverless framework in the terminal by running the command:
Or use the shorthand sls for serverless. This command will display all the available commands that come with the serverless framework.
After installing the serverless dependency as a global package, you are ready to create your first function. To start, you will need to configure your AWS registered user credentials. AWS gives you a link to download access keys when creating a user.
You can also visit your username and visit Security Credentials like below.
Now let us configure AWS with the serverless package.
If the above command runs successfully you will get a success message like below
The good thing about using serverless npm package is that it comes with pre-defined templates that you can create in your project using a command and also creates a basic configuration for us that is required to deploy our Lambda function. To get started, I am going to use aws-nodejs template inside a new directory.
The -p flag will create a new directory with name aws-serverless-demo. The -t flag uses the pre-defined boilerplate. The result of this will create three new files in your project directory.
The default handler file looks like below.
In the above file, hello is the function that has two parameters: event, and context. module.exports is basic Nodes syntax as well as the rest of the code. You can clearly see it also supports ES6 features. An event is an object that contains all the necessary request data. The context object contains AWS-specific values. We have already discussed it before. Let us modify this function to our needs and add a third parameter called thecallback. Open handler.js file and edit the hello function.
The callback function must be invoked with an error response as the first argument, in our case it is null right now or a valid response as the second argument which is currently sending a simple Hello World message. We can now deploy this handler function using the command below from your terminal window.
It will take a few minutes to finish the process. Our serverless function gets packed into a .zip file. Take a notice at the Service Information below. It contains all the information what endpoints are available, what is our function, where it is deployed and so on.
You can try the invoke attribute like following to run the function and see the result.
The output will look like below.
Take a look at the configuration in serverless.yml.
In this part of the tutorial, I will show you how to hook up a MongoDB database as a service to a Serverless REST API. We are going to need three things that will complete our tech stack. They are:
We already have the first two, all we need is to setup a MongoDB cloud database called Atlas. MongoDB Atlas is a database as a service developed by the team behind the MongoDB itself. Along with providing a free/paid tier for storing your data on the cloud, MongoDB Atlas provides a lot of analytics that is essential to manage and monitor your application. MongoDB Atlas does provide a free tier that we will be using with our serverless stack.
We will start by creating a database on the MongoDB Atlas. Login to the site and create an account if you do not have it already. We just need a sandbox environment to get hands-on experience so we must opt for free tier. Once you have your account set up, open up your account page and add a new organization.
Now, after entering the name, proceed further and click on Create Organization.
You will be then prompted to the main screen where you can create a new project. Type in the name of your project and proceed further.
MongoDB Atlas is secured by default. You need to set permissions before we leverage its usage in our app. You can name the database at the pointed field below.
Now, we can add our free sandbox to this project. It is called a cluster.
After all that, just add an admin user for the cluster and give him a really strong password. As you can see the price for this cluster will be $0.00 forever. Your cluster will take a few minutes to deploy. While that is underway, let us finally start writing some code.
Next, we install all the necessary dependencies in order to create the API.
After that, we configure the serverless.yml and add the other handler functions that we need to deploy.
The CRUD operations that will handle the functionalities of the REST API are going to be in the file handler.js. Each event contains the event information of the current event that will be invoked from the handler.js. In the above configuration file, we have defined each CRUD operation along with an event and the name. Also notice, when defining the events in above file, we are associating an HTTP request with a path that is going to be the endpoint of the CRUD operation in the API, the HTTP method and lastly, cors option.
I am going to demonstrate a simple Note taking app through our REST API. These CRUD operations are going to be the core of it. Since our API is going to be hosted remotely, we have to enable Cross-Origin Resource Sharing. No need to install another dependency on that. Serverless configuration file has support for it. Just specify in the events section like cors: true. By default, it is false.
If you are familiar with Node.js and Express framework you will notice there is little difference in creating a controller function that leads to the business logic of a route. The similar approach we are going to use to define in each handler function.
The context contains all the information about the handler function. How long it has been running, how much memory it is consuming among other things. In above, every function has the same value of context.callbackWaitsForEmptyEventLoop set to false and starts with connectToDatabase function call. The context object property callbackWaitsForEmptyEventLoop value is by default set to true. This property is used to modify the behavior of a callback.
By default, the callback will wait until the event loop is empty before freezing the process and returning the results to the invoked function. By setting this property’s value to false, it requests the AWS Lambda to freeze the process after the callback is called, even if there are events in the event loop. You can read more about this context property at the official Lambda Documentation.
We need to create a connection between the database and our serverless functions in order to consume the CRUD operations in real-time. Create a new file called db.js in the root and append it with following.
The is common Mongoose connection that you might have seen in other Nodejs apps if using MongoDB as a database. The only difference here is that we are exporting connectToDatabase to import it inside handler.js for each CRUD operation. Modify handler.js file and import it at the top.
Next step is to define the data model we need in order for things to work. Mongoose provides this functionality too. Serverless stack is unopinionated about which ODM or ORM you use in your application. Create a new file called notes.model.js and add the following.
Now import this model inside handler.js for our callbacks at the top of the file.
Protecting our keys and other essentials is the first step to a secured backend application. Create a new file called variables.env. In this file, we will add our MONGODB connection URL that we have already used in db.js as a process.env.DB. The good thing about environment variables is that they are global to the scope of the application.
To find out our MongoDB URL, we need to go back to the mongodb atlas, to out previously created cluster. Click the button Connect and then you will be prompted a page where you can choose how to access the application. Click Allow Access From Anywhere.
Copy the mongodb URL from above and paste it in the variables.env file.
Replace the user and password field with your credentials. Now to make it work, all we have to add the following line in our handler.js.
All you have to do is run the deploy command from the terminal.
Since we have connected our Lambda function, this command will prompt us with a different endpoints. Each handler function is deployed as a separate REST endpoint.
You can test your API using CURL command from the terminal like below.
You can find the complete code for this article in the below Github Repository 👇
github.com
Thanks to William Wickey for help editing.
Crowdbotics is the fastest way to build, launch and scale an application.
Developer? Try out the Crowdbotics App Builder to quickly scaffold and deploy apps with a variety of popular frameworks.
Busy or non-technical? Join hundreds of happy teams building software with Crowdbotics PMs and expert developers. Scope timeline and cost with Crowdbotics Managed App Development for free.
The fastest way to build your next app.
1.7K 
5
1.7K claps
1.7K 
5
Written by
👨‍💻Developer 👉 Nodejs, Reactjs, ReactNative | Tech Blogger with 2M+ views at Medium| My weekly dev newsletter 👉 https://www.getrevue.co/profile/amanhimself
The fastest way to build your next app.
Written by
👨‍💻Developer 👉 Nodejs, Reactjs, ReactNative | Tech Blogger with 2M+ views at Medium| My weekly dev newsletter 👉 https://www.getrevue.co/profile/amanhimself
The fastest way to build your next app.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-developer-experts/building-your-next-serverless-application-the-complete-guide-98e48f85bd3c?source=search_post---------363,"There are currently no responses for this story.
Be the first to respond.
I bet you’ve already heard of Serverless architectures: The next evolution of Cloud computing. The term “Serverless” is actually an umbrella term of two areas in Cloud computing: Backend as a Service (BaaS) and Functions as a Service (FaaS).
With BaaS we’re breaking up our applications into smaller pieces and implementing some of those pieces entirely with external services. This is usually done with a call to an API (or gRPC calls). One of the most popular Backends as a Service is Google’s Firebase, a realtime database (with a bunch of other cool features) for mobile and web applications.
Functions as a Service on the other hand, is another form of Compute as a Service: FaaS is a way of building and deploying server-side code, by simply deploying individual functions (hence the name) on the vendor-supplied FaaS platform.
Now that we agree on the correct definition of what a Serverless architecture actually is, let’s build a complete “Serverless application”.
The application we’re going to build is a chatbot that’s capable of extracting a text content from a picture (optionally translating it to different languages) and sending back the result to the user via SMS (or a phone call). Such application could be used to extract other useful information from a given image or even a video stream and sends SMS notifications to the user or a group of users.
I’m sure you are now thinking of a more interesting use case. If that’s the case, please let me know!
Let’s dive in…
For our use case, we would like to start a conversation with our agent (aka. “chatbot”) and provide it with something containing some text to be extracted and later analysed (a page from a book or a newspaper maybe?).
Since the “chatbot” part is not our main focus in this post, we will “Keep it simple, stupid” and design a quick conversation in DialogFlow, as follows:
Let’s now code the logic for our agent that will actually take the picture.
First, we’ll need two utility functions:
Here the implementation of the captureImage function. This function is using a system utility imagesnap available on MacOS to actually access the camera, capture the image and store the image file under /tmp/google-actions-reader-${Date.now()}.png . This function then returns both the name and the file content in base64 :
The next function uploadImage will simply upload that image to GCS in the cloud-function-ocr-demo__image bucket:
Please note the name of the bucket cloud-function-ocr-demo__image, we will need it later.
Now that we have our two utilities functions captureImage and uploadImage ready, let’s use them inside the read intent logic (remember this intent in the dialog from above?):
This readIntent will basically capture and then upload the image to GCS.
Now that we have all of the agent’s logic implemented, let’s create the main Cloud Function that will process DialogFlow’s requests:
The assistant Cloud Function will be triggered from an HTTP call. This call will be made by DialogFlow if the user says, for example, “read this text” (as mentioned above) which is an expression defined in the read intent.
This section will serve as an example for the rest of this guide.
In order to deploy a Cloud Function, we can use the gcloud command with the following arguments:
You also can use a Google Cloud Storage bucket to host the source code of your function. But we’ll not cover this here.
Oh, by the way…
Hosting your source code in a Google Cloud Repository (a Git repo) is a good idea if you have a continuous delivery strategy in your organisation.
In our case, here is the full command:
In case you are wondering, the Google Cloud Repository source has the following format:
Once deployed, your function should be ready to be triggered:
You also will be given a public URL, which looks like this:
That’s the URL we will use in our DialogFlow project.
If you’ve been following carefully, you may have noticed that the captureImage function needs… well, access to a camera! This means that we won’t be able to deploy this specific function to the Google Cloud Platform. Rather, we will host it on our specific hardware, say a Raspberry PI (to make it simple), and use a different URL (obviously).
You can use the Google Cloud Function Emulator to run your Cloud Functions locally. Please keep in mind that this is only for development purposes. Don’t use this for production applications.
Let’s then add the fulfilment URL, which points to the assistant Cloud Function that will process the agent requests:
Now, we’re done with the first part of our application which essentially consists of getting our image uploaded to the GCS.
Until now, we’ve only talked about Cloud Functions—FaaS. Let’s jump to the Backends as a Service (or BaaS) part.
We want to be able to extract some content from the image, a text in our case. We have tons of open source libraries to do that—OpenCV or Tensorflow just to name a few. Unfortunately, these libraries require us to have some sort of expertise in Machine Learning and Image (or Sound) processing. Hiring these expertise is not easy! Also, ideally, we don’t want to maintain this code and we want our solution to be able to automagically scale in case our application becomes popular. Said simply, we don’t want to manage this feature. Luckily, the Google Cloud Platform got us covered:
Here is the sub-architecture of this feature:
In order to be able to process the image, we need two functions:
Here is the implementation of processImage :
The implementation of detectText function is straightforward (we will improve it later):
We now need to deploy the processImage Cloud Function and we want it to be triggered whenever a new image is uploaded to the GCS in the cloud-function-ocr-demo__image bucket:
Let’s now add some translations…
Translating the extracted text will be triggered by a specific Google Cloud Pub/Sub topic TRANSLATE_TOPIC and will consist of two operations:
Let’s improve our existing processImage Cloud Function with the language detection feature:
Let’s explain the new extra code we’ve added:
We first added a call to the Google Translation API in order to detect the main language of the extracted text translate.detect(text);. Then, in the next block, we basically iterate over the config.TO_LANG array we have in the configuration file, and publish a TRANSLATE_TOPIC with a specific payload containing the text content (text), the source language (from) and the target language we want to translate to (lang). If the source language is the same as the target language, we just publish RESULT_TOPIC.
For convenience purposes, we’ve also included a new utility function publishResult whose responsible of publishing a Pub/Sub topic. It essentially uses the Google Cloud Pub/Sub API to create (if needed) and publish the given topic:
Let’s then create the translateText Cloud Function that will translate the extracted text:
The implementation of this function is self explanatory: We call basically call the translation.translate(payload.text, options); and once we get the result, we publish the RESULT_TOPIC with a translated content.
It’s time now to deploy the translateText Cloud Function using the same command as before. This function will be triggered by the TRANSLATE_TOPIC topic, so we make sure to use that as a trigger type:
So far so good, we have now managed to capture the image, upload it to the GCS, process it and extract the text and then translate it. The last step would be to save the translated text back to the GCS.
Here is the implementation of such function:
The saveResult is triggered by the RESULT_TOPIC which is the topic holding the translated text. We simply use that payload and call the Google Cloud Storage API to store the content in a bucket called config.RESULT_BUCKET (which is cloud-functions-orc-demo). Once this is done, we publish the READ_TOPIC topic will trigger the next Cloud Function (see next section).
Time to deploy the saveResult Cloud Function using the same command as before. This function will be triggered by the TRANSLATE_TOPIC topic, so we make sure to use that as a trigger type:
Finally, we are now ready to read the translated text from the GCS and send it via SMS to the user’s phone.
Reading the file from the GCS is, again, a straightforward operation:
In the readResult function, we are using another utility function readFromBucket which, as the name suggests, reads the content from a given GCS bucket. Here is the detailed implementation:
That’s simply it. Now, let’s deploy the readResult Cloud Function and make it triggers from the READ_TOPIC topic:
b- Sending SMS notifications
When it comes to sending an SMS to the user’s phone, we use the awesome Twilio service, which… just works!
Using Twilio services require you create a developer account.
c- Making phone calls (BONUS)
Sending the translated content via a phone call back to the user is a bit tricky as you need to provide two functions:
To demonstrate how this process would work, let’s first have a look at the twilioCalls implementation:
The twilioCall function is responsible of reading the file from the bucket and sending back an XML response built thanks to the Twilio Markup Language (TwilioML).
You will then need to deploy this Cloud Function in order to get the public URL required by the call function:
Once deployed, you will get a public URL like this:
Next, we’ll use that URL in the call function:
Done! Now, you Twilio HTTP endpoint is ready for incoming calls.
In this guide, we implemented a bunch of Cloud Functions that performed different tasks:
Here is a recap of all the deployed Cloud Functions:
And here is again the complete architecture:
In order to test the application, we first need to deploy the DialogFlow agent. We choose to deploy it on the Google Assistant since our assistant Cloud Function is meant to process the Google Assistant requests. If you’d like to deploy to other services (Slack, Facebook, Twitter…etc) you just need to provide and deploy other Cloud Functions.
From the integration tab, choose the Google Assistant and click the TEST button:
This will open up the Actions on Google simulator, allowing you to test your agent directly in the bowser. Alternatively, you can your phone or Google Home devices as well:
Note that the we gave our agent a name: shakespeare. We did that from the same simulator, in the overview panel.
As a sample text, we will use the following quote (by Ziad K. Abdelnour):
And… here is the SMS sent by our readResult function:
Here is the complete source code:
github.com
Follow me on Twitter @manekinekko to learn more about the Web and Cloud platforms.
Experts on various Google products talking tech.
1.1K 
6
1.1K claps
1.1K 
6
Experts on various Google products talking tech.
Written by
#javascript @ Microsoft ★ GDE for Action On Google, GCP teams at Google ★ Member of Node.js Foundation ★ Ambassador for Auth0 ★ Follow me @manekinekko
Experts on various Google products talking tech.
"
https://codeburst.io/software-architecture-architect-your-application-with-aws-52d938603a32?source=search_post---------364,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Nowadays, cloud computing has become a central part of any tech company, that includes every company now since most of them can be categorized under “Software as a Service” (SaaS). In this post, I will try to simplify the most important Amazon cloud/web services known as AWS.
This post will help you to understand different services by Amazon and their different capabilities. and to discover the new opportunities that come with using cloud computing instead of self-managed infrastructures.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@mlabouardy/how-we-reduced-lambda-functions-costs-by-thousands-of-dollars-8279b0a69931?source=search_post---------365,"Sign in
There are currently no responses for this story.
Be the first to respond.
Mohamed Labouardy
Aug 6, 2019·5 min read
Serverless Computing or FaaS is the best way to consume cloud computing. In this model, the responsibility for provisioning, maintaining, and patching servers is shifted from the customer to cloud providers. Which allows developers to focus on building new features and innovating, and pay only for the compute time that they consume.
In the last 7 months, we started using Lambda based functions heavily in production. It allowed us to scale quickly and brought agility to our development activities.
We were serving +80M Lambda invocations per day across multiple AWS regions with an unpleasant surprise in the form of a significant bill.
It was very easy and cheap to build a Lambda based applications that we forgot to estimate and optimize the Lambda costs earlier during development phase, so once we start running heavy workloads in production, the cost become significant and we spent thousands of dollars daily 💸
To keep Lambda cost under control, understanding its behavior was critical. Lambda pricing model is based on the following factors:
In order to reduce AWS Lambda costs, we monitored Lambda memory usage and execution time based on logs stored in CloudWatch.
We’ve updated our previous centralized logging platform to extract relevant metrics (Duration, Billed Duration and Memory Size) from “REPORT” log entry reported through CloudWatch and store them into InfluxDB:
medium.com
You can check out the link above for a step-by-step guide on how to setup the following workflow:
Next, we created dynamic visualizations on Grafana based on metrics available in the timeseries database and we were able to monitor in near real-time Lambda runtime usage. A graphical representation of the metrics for Lambda functions is shown below:
You can also use CloudWatch Logs Insights to issue ad-hoc queries to analyse statistics from recents invocations of your Lambda functions:
We leveraged these metrics to set Slack notifications when memory allocation is either too low (risk of failure) or too high (risk of over-paying) and to identify the billed duration, memory usage for the ten most expensive Lambda functions. When performing heuristic analysis of Lambda logs, we gain insights into the right sizing of each Lambda function deployed in our AWS account and we avoided excessive over-allocation of memory. Hence, significantly reduced the Lambda’s cost.
Memory allocation can make a big difference in your Lambda function cost. Too much allocated memory and you’ll overpay. Too little and your function will be at risk of failing. Therefore, you want to keep a healthy balance when it comes to memory allocation.
To gather more insights and uncover hidden costs, we had to identify the most expensive functions. Thats where Lambda Tags comes into the play. We leveraged those metadata to breakdown the cost per Stack (project):
By reducing the invocation frequency (control concurrency with SQS), we reduced the cost up to 99% and CO2 emissions footprint of our B2C app Cleanfox 🚀💰
At a deeper level, we also breakdown the cost by Lambda function name using a secondary tag which is Function tag:
Once the target functions were identified, we reviewed the execution flow and applied some optimisation in our code to shorten the running time and resources needed (Memory and CPU)
By continuously monitoring increases in spend, we end up building scalable, secure and resilient Lambda based solutions while maintaining maximum cost-effectiveness. Also, we are now configuring Lambda runtime parameters appropriately at the sandbox stage and we’re evaluating alternative services like Spot Instances & Batch Jobs to run heavy non-critical workloads considering the hidden costs of Serverless.
Drop your comments, feedback, or suggestions below — or connect with me directly on Twitter @mlabouardy.
github.com
We’re sharing this because we’re looking for people that want to help us solve some of these problems. There’s only so much insight we can fit into a job advert so we hope this has given a bit more and whet your appetite. If you’re keeping an open mind about a new role or just want a chat — get in touch or apply — we’d love to hear from you!
CTO & Co-Founder @Crew.work — Maker & Indie Entrepreneur @Komiser.io, Writer/Author and Speaker — Blog: https://labouardy.com
1.2K 
12
1.2K 
1.2K 
12
CTO & Co-Founder @Crew.work — Maker & Indie Entrepreneur @Komiser.io, Writer/Author and Speaker — Blog: https://labouardy.com
"
https://betterprogramming.pub/10-must-know-concepts-of-modern-web-architecture-9ecbefef8bc?source=search_post---------366,"Sign in
What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Shalitha Suranga
Jul 20, 2021·6 min read
After Tim Berners-Lee invented the WWW (World Wide Web) in 1989, almost all types of physical services started moving to the cloud ecosystem. Before the rapid evolution of the internet, there were remotely connected desktop applications. Users…
About
Write
Help
Legal
Get the Medium app
"
https://betterprogramming.pub/5-great-data-engineering-online-courses-784e62f57ddd?source=search_post---------367,"Sign in
There are currently no responses for this story.
Be the first to respond.
SeattleDataGuy
Apr 1, 2020·6 min read
According to a recent survey by Statista, the data market is expected to grow 175 zettabytes in volume by the year 2025. In order to dive in further into big data, we need to understand how one can work on it — and that’s where data engineering comes in.
"
https://medium.com/snips-ai/thanks-for-breaking-our-connected-homes-amazon-c820a8849021?source=search_post---------368,"There are currently no responses for this story.
Be the first to respond.
More than a few folks got back from lunch today to find that their lightbulbs weren’t working.
They couldn’t play that post-lunch smooth jazz …
Nor could they turn off their …oven.
Or … open their garage.
Amazon’s s3, a cloud-based storage service that powers many gadgets, most of the Internet of Things, and a very large number of websites ….was down.
A fail whale that lands not just online, but … in your home.
This awkward moment was brought to you by the fact that all cloud-based IoT running on top of Amazon depend on both an internet connection as well as operational servers. This debacle could have been avoided if connected devices could run offline, and weren’t dependent on the cloud.
Right now if you have a cloud-based IoT lightbulb, every time you switch it on, the control hub sends a request to the cloud, gets a response back, and then sends a radio signal to the lightbulb. It travels thousands of miles, sending data around, just to turn on a light that’s a few meters away. That’s the modern equivalent of having to call your electricity provider each time you want to turn your physical lights on or off!
So is everyone with connected lightbulbs somehow run through s3 just … in the dark right now? No cloud. No light. That’s how fragile we’re becoming with all these unnecessary layers of complexity.
What are the chances that this corporation will make it all the way to your house, turn the key and …turn your lights off permanently? The chances are small. But still — you shouldn’t be forced to hand over the keys.
Today’s events definitely opened our eyes to what a dependency on cloud services means to our lives. It showed us how vulnerable we have become.
Your IoT services could run on-device, meaning they aren’t reliant on an internet connection, or the cloud. This also allows them to be Private by Design, meaning none of your personal data needs to be sent server side (like your info that flies to Amazon after every query to Alexa…).
A private-by-design IoT is one that would be much more impervious to cloud issues. More likely to protect you, the user.
No talking to cloudy servers, just you, your IRL IoT and … a functioning home.
If only there were a team of smart folks working on on-device tech … oh wait.
There is.
Device running Snips are currently working just fine while s3 is down.
This could be you, IoT community!
But we’re not that far from a future where IoT is more ubiquitous. And therefore, more likely to put us in danger. We’re not far from a future where IoT isn’t just inside fancy connected homes, but inside the infrastructure of our cities, our schools, our highways, our airplanes … you get the picture.
Yes, the s3 outage is a doofy internet blooper. But if we’re not careful about creating a stronger future for IoT, that doofy blooper has the danger of scaling to a larger nightmare.
Right now IoT is contained to banal household items — lightbulbs, locks, ovens. But imagine a world where IoT and artificial intelligence are more ubiquitous.
Imagine a future where the AI assistants that are integrated in to your cloud-based devices — and into your life — break down. What does that mean to your home? Your family? To you?
The s3 outage today taught us that there are real problems with cloud-based IoT and AI. The future could be dark (literally). But it doesn’t have to be.
Here’s hoping it signals the beginning of innovation. The real beginning of an era of consumer IoT and AI products that think deeply about user privacy, security and lifestyle — products that don’t just work for you, but work with you.
If you enjoyed this article, it would really help if you hit recommend below :)
Follow us on twitter @randhindi & @snips
And if you are so inclined, signup to our weekly bullet-point newsletter about AI, Privacy and IoT!
This publication features the articles written by the Snips…
201 
11
201 claps
201 
11
This publication features the articles written by the Snips team, fellows, and friends. Snips started as an AI lab in 2013, and now builds Private-by-Design, decentralized, open source voice assistants.
Written by
CEO @zama_fhe . Angel investor in 30+ startups across #Cybersecurity, #Blockchain, #Psychedelics, #MedTech. I share my dealflow on my substack.
This publication features the articles written by the Snips team, fellows, and friends. Snips started as an AI lab in 2013, and now builds Private-by-Design, decentralized, open source voice assistants.
"
https://medium.com/arpa/arpa-ask-me-anything-with-chainnode-e1cdd03db190?source=search_post---------369,"There are currently no responses for this story.
Be the first to respond.
*the following content has been translated to English from Chinese
Since the beginning of this year, IEOs have become the hot topic in cryptocurrency markets. By the numbers, over 50 IEO projects have occurred in just the first four months of 2019. So far, 35 IEOs have completed, 10 are still in progress, and 16 planned worldwide has reached 50, with 35 completed, 10 in-progress and 16 IEO exchanges planned for the near future.
The first China-Korean joint IEO project to launch was ARPA, which was co-founded by Felix Xu (CEO) and Jiang Chen (Tech Lead). On April 23rd they jointly conducted an AMA at ChainNode, one of the most influential crypto news and communities in Chinese-speaking parts of the world.
The name ARPA comes from the APRAnet network program developed by the United States Department of Defense in 1969. APRAnet was the predecessor of the TCP/IP protocol, and is considered a forerunner of the modern internet.
The ARPA project aims to provide businesses and individuals with private computing power and secure data flow solutions. Based on the advanced secure multi-party computing (MPC) cryptosystem, the ARPA secure computing network can be used as a protocol layer (Layer 2) to provide privacy computing capabilities for any public blockchain, enabling developers to build efficient, secure computing networks on ARPA computing networks, while also protecting the data privacy of business applications. Enterprise and personal data can be safely analyzed or utilized on ARPA computing networks without worrying about exposing data to any third party.
The entry point of ARPA is enterprise-level privacy data sharing. This includes: multi-party joint credit information; data leasing; secure data analysis and other scenarios in the financial industry; multi-source data joint risk control in the insurance industry; pricing; sensitive information querying; joint user portraits within the Big Data marketing field; delivery efficiency tracking. In the future, applications will exist for corporate finance, marketing, medical applications, and even artificial intelligence.
The following are excellent community questions and answers, collected and edited by ChainNode:
Felix: This is an interesting question. I gave up my investor identity to work as an entrepreneur, mainly because of the long-term development of the security data exchange market. My investment experience gives me the opportunity to learn from many excellent entrepreneurs. Investment and post-investment management gave me a deep understanding of not only the industry at large, but also the daily management of enterprises.
Previously, I mainly invested in the direction of financial technologies and Big Data. I often encountered enterprises faced with data silos, unable to effectively use external data to solve business problems. At the same time, the data intermediaries, which control data flow, greatly increase the cost of using their data. Additionally, data ownership and the use of the data were not clear. Such pain points have been present for a long time.
So, in early 2018, my partner and I developed a more secure computing technology built for blockchains to solve privacy data exchange problems. Over the past year, our team has reached a firm foothold in this field, where the technology threshold is extremely high.
Jiang: The secure multi-party computing (MPC) technology and the ARPA cryptographic protocol of the ARPA privacy computing network are our unique advantages. We can ultimately realize the separation in data of the right to use and the right of use, and directly calculate results on multi-source and heterogeneous ciphertext data. Computational operations do not need to decrypt the original data, as technical barriers are extremely high.
ARPA is becoming an emerging leader in general MPC computing. We have implemented an agreement to support the participation of any party, and as long as there is an honest node in it, it can ensure the security of the data. Either of these two points is a breakthrough. As far as we know, the vast majority of projects can only support the involvement of two parties.
On the other hand, connecting privacy computing to blockchains is also be a big innovation. Privacy computing within a network, of course, can also be used within organizations or enterprises. But from a logical point of view regarding the technology itself, it should be mainly used in cases which involve multiple parties. Now that there is multi-party collaboration, the introduction of blockchain is very natural. We sometimes hear people say they want to have their own Facebook, with their own data, so this needs to use ARPA-supported blockchains.
Jiang: The ARPA project was launched in April 2018. At present, the test network based on safe multi-party computing (MPC) has been released ahead of the roadmap. The user can publish the calculation request to the ARPA security computing network through an Ethereum smart contract. The privacy calculation is done by the ARPA node, and many kinds of MPC computing functions are supported at present.
ARPA’s main chain is expected to be available in the fourth quarter of 2019, when Ethereum, IOST and other mainstream blockchain and alliance chains will be supported. At present, ARPA has reached strategic cooperation with more than 20 large enterprises at home and abroad, including large trading institutions, financial and insurance institutions, Big Data marketing agencies, and son on, several of which have been converted into orders.
ARPA products at present include multi-party joint risk control, supplier joint KYC, blacklist sharing, security model analysis, and other solutions. Their focus is on vertical fields such as finance and transactions, so as to extract data collaborative value for enterprises. ARPA is also participating in the development of national standards for safety multi-party computing, and has been recognized by authorities and professional peers.
“At present, we are the only project which can meet the requirements of enterprise-level computing MPC.”
Felix: At present, there are few privacy computing projects based on cryptography for competition. We are the only project in the world that can support multi-party MPC computing in the test network phase. Other MPC-related projects include: Keep Network, which uses secret sharing for storage; PlatON, which supports two-way secure computing (2PC). In the field of non-cryptographic computing, security hardware projects such as SGX have advantages of Enigma and Oasis Labs — speed, but centralized — and there are some engineering problems as well.
We’re the only multi-party secure computing (MPC)-based project, which means that we can meet the needs of enterprise computing. We work closely with many of the world’s leading cryptography professors, including those at Harvard, Carnegie Mellon, Tsinghua University, and the University of Michigan. They are also responsible for engineering technology implementation and location information from Google, Amazon, Huawei, Blackstone, Fidelity Investment and other international senior engineers, architects and consultants.
Felix: The key to privacy data sharing is to make the data “available but not visible”. At present, many projects in the market claim to have data sharing, but actually do data transactions. Even if the transaction process is encrypted/co-chained, once the data is decrypted by the user, it can be copied and shared off-blockchain, and the blockchain itself, alone, cannot solve the data privacy problem.
We also discussed this issue with GXC team, which has just released a white paper based on trusted hardware — something which I think is a good first step. At present, GXC mainly focuses on two & three elements querying, though function and shared data types are limited.
We cut into To-B vertical industries such as finance, insurance, and marketing, to conduct joint analysis of high-value privacy data. Our long-term goal is privacy cloud computing based on personal data, using secret sharing to store fragments of personal full-dimensional data in the cloud, where advertisers and financial institutions can call and analyze the MPC at any time, function can be customized, and individuals receive a return on data leases without disclosing any privacy.
“The IEO model is not new in South Korea.”
Felix: We think China and South Korea are the two most important crypto markets, and our partners are BISS and Tokenman. BISS is the world’s first crypto-to-crypto plus crypto-to-securities exchange, and Tokenman is backed by the Bitman community, which is South Korea’s largest and oldest blockchain community. We will invite more than 450,000 people who hope to be more deeply involved in the crypto world on Naver (a South Korean version of Baidu). In addition, we will also cooperate with the leading public chains in South Korea, and more project commercialization and business developments will be conducted in the future in South Korea.
Jiang: From an industry point of view, rapid development needs financial support, and the growth of good projects will bring high returns to investors. Unlike previous ICOs, IEOs helped investors screen projects through exchanges, eliminating a large number of unqualified projects and scams — which means a lot to investors looking to protect themselves.
At the same time, for high-quality projects, exchanges can also help improve the visibility of the project, and improve access to users. Project teams and founders can focus on developing their products and accumulating underlying technologies, rather than spending a lot of time dealing with VC, pitching, marketing, and so on. This contributes to the growth of high-quality projects and ultimately contributes to the development of the industry;
Felix: Information of our partners can be seen on our official website arpachain.io. Currently we have strategic cooperation with more than 20 large enterprises at home and abroad. With regard to our team, we have 10 full-time team members and 5 scientific researchers, who all have long-term overseas working experience in big enterprises. The team has outstanding scientific research, engineering, and BD capabilities, which forms the core of our competitiveness.
Jiang: We will make a profit by providing technical solutions for enterprises, building secure data sharing platforms, providing secure data analysis products, and so on.
Jiang: In the future, our MPC network will be accessible for participants, and provide enforceable files, where users can download and run nodes.
Stay in the loop with ARPA by following us on Telegram, Twitter and Medium.
Website: www.arpachain.io
Telegram: https://t.me/arpa_community
中文电报群： https://t.me/arpachinese
Medium: https://medium.com/@arpa
Twitter: https://twitter.com/arpaofficial
To get more information about ARPA, or email at about@arpachain.io
The Official ARPA Medium Blog
2.5K 
32
2.5K claps
2.5K 
32
Written by
ARPA is a privacy-preserving blockchain infrastructure enabled by MPC. Learn more at arpachain.io
The Official ARPA Medium Blog
Written by
ARPA is a privacy-preserving blockchain infrastructure enabled by MPC. Learn more at arpachain.io
The Official ARPA Medium Blog
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/quick-code/terraform-an-answer-to-cloud-scalability-844c3182e9a?source=search_post---------370,"There are currently no responses for this story.
Be the first to respond.
It can be said with certainty that, at some point, you’ve interacted with the Cloud. If not, then you currently are! As you’re reading this article on Medium’s cloud servers, you are…
"
https://medium.com/@duhroach/improving-cloud-function-cold-start-time-2eb6f5700f6?source=search_post---------371,"Sign in
There are currently no responses for this story.
Be the first to respond.
Colt McAnlis
Mar 18, 2018·5 min read
Any time you are working with responsive cloud computing systems, cold-boot performance is something you’re going to have to consider. As we saw with Compute Engine, Container Engine, and App Engine, any time a user response comes into a cloud resource, it runs the risk of needing to do a boot-from-scratch, or cold-boot. This is problematic, since, in these scenarios, the response time is increased.
That’s why, when CAPITAL FUN approached me, asking about how to improve their cold-boot performance for Cloud Functions, I wasn’t surprised at all.
Functions are considered a “serverless” architecture, but nonetheless can still be at the mercy of cold-start times. The best way to figure out when you’re going to have to pay the overhead of a cold boot is to consider that you have one of two situations you face when calling your serverless function:
The “Hot” scenario your function has run recently, and the same machine instance on which it last ran is available to run it again. This results in the shortest round-trip time.
The “Cold” scenario the next call to your function will result in a new instance being instantiated to run your serverless function. This can happen for a handful of reasons:
In general, “Hot” functions will already exist, and be ready to handle your requests as fast as possible. “Cold” functions on the other hand, will have a longer response time due to the need to provision function resources prior to executing the code.
In order to get a baseline for the difference between these two, I created a new completely blank GCF, deployed it, and pinged it a few times. In the image below, you can see the red area is the “Cold” boot time for the function (12ms), while the blue box is the general response time for the “Hot” scenario (4ms).
Hands-down, the #1 contributor to GCF cold-boot performance is dependencies. When a module boots, node.js resolves all require() calls from the entry point using synchronous I/O, which can take a significant amount of time if the number of dependencies are large, or if the content itself requires a lot of linking.
So when I saw that CAPITAL FUN’s GCF module had 50 Dependencies (!!!!) I knew right where the problem was, and our profiling proved my point:
Getting this back down to something manageable took a few steps, and a lot of negotiation
As a first step, we worked with CAPITAL FUN to trim out dependencies. Including a module may involve a subset of other modules. Its’ critically important to understand what sub-linking is occurring, as it can vary the startup performance of your function significantly. After a lot of negotiation, digging, and spaghetti code cleaning, we were able to get down to 8 packages, including google-cloud. This resulted in ~ 2x improvement in cold-boot performance.
One of the influencers of dependency resolution is weather or not the dependency exists in the cache or not. The dependency cache is shared across all GCF dependencies, as such, the most popular versions of a module can be reused across a lot of users and deployments.
Looking at CAPITAL FUN’s dependencies, their version numbers were all over the place. In some cases, their system was changing the version number between deployments, depending on various code and staging factors.
To address this, we unified the production code to use the most popular module versions requested by external users, since, these versions are expected to be present in the dependency cache, making deployments faster.
Further digging through the Node.js code for CAPITAL FUN, we realized that not all the dependencies needed to be loaded at the boot-phase of the function. Although it’s a non-standard practice, we’ve seen before how putting requires within the function body, and pulling them out only when necessary can help the performance of cold-boot situations.
This allowed us to require only what is needed at start, and then switch to an async version of require later on for specific request handling, bringing down the cold boot times even further.
Working with CAPITAL FUN, we were able to reduce the number of dependencies, optimize the version numbers to increase dependency caching, and then lazy load certain modules. The result? Going from 20sec cold-boot time to 1.83sec, which is amazing considering their warm-response time is around 800ms or so.
DA @ Google; http://goo.gl/bbPefY | http://goo.gl/xZ4fE7 | https://goo.gl/RGsQlF | http://goo.gl/4ZJkY1 | http://goo.gl/qR5WI1
875 
11
875 
875 
11
DA @ Google; http://goo.gl/bbPefY | http://goo.gl/xZ4fE7 | https://goo.gl/RGsQlF | http://goo.gl/4ZJkY1 | http://goo.gl/qR5WI1
"
https://medium.com/@dbclin/aws-just-announced-a-move-from-xen-towards-kvm-so-what-is-kvm-2091f123991?source=search_post---------372,"Sign in
There are currently no responses for this story.
Be the first to respond.
David Clinton
Nov 13, 2017·9 min read
Tied up in a recent AWS announcement about a new EC2 high-end instance type (the C5) is a strong suggestion that Amazon’s cloud computing giant has begun to shift its hundreds of thousands of physical servers away from the open source Xen hypervisor that’s run them until now, to the open source alternative, KVM.
Whether you’ve got your career and/or home mortgage deeply invested in the future of Xen or whether you never knew it existed, you might be interested in learning more about KVM. So here’s a general introduction adapted from my book, Teach Yourself Linux Virtualization and High Availability: prepare for the LPIC-3 304 certification exam.
Need some more basic background on either Linux servers or AWS? My Linux in Action and Learn Amazon Web Services in a Month of Lunches books from Manning might help, along with my Linux server administration courses at Pluralsight. There’s also a hybrid course called Linux in Motion that’s made up of more than two hours of video and around 40% of the text of Linux in Action.
Like Xen, KVM (Kernel-based Virtual Machine) is an open source hypervisor technology for virtualizing compute infrastructure running on x86 compatible hardware. Also like Xen, KVM has both an active user community and significant enterprise deployments.
A KVM host actually runs on the Linux kernel along with two KVM kernel modules (the kvm.ko module and either kvm-intel.ko or kvm-amd.ko). Through its tight kernel integration — including the I/O connectivity with kernel block and network drivers provided by Virtio — KVM can offer its guests more seamless access to all the complex hardware and networking profiles that they might encounter.
Hardware virtualization extensions built into modern CPU designs and required for KVM deployments mean that, right out of the box, KVM guests can safely access only those hardware resources they need without the need to worry about leakage to the larger system.
Where exactly does QEMU fit in with all this? Besides being able to act as a hypervisor, QEMU’s strength is as an emulator. KVM, in its hypervisor virtualization role, can tap on to QEMU’s emulation powers to compliment its own hardware acceleration features, presenting its guests with an emulated chipset and PCI bus. The whole, as they say, can be greater than the sum of its parts.
A great deal of management functionality for KVM is often actually provided by Libvirt. Therefore, you might sometimes want to refer to the detailed information about KVM-related features like networking, storage, and file system layouts that’s found in the fifth chapter of the Teach Yourself Linux Virtualization and High Availability book (“Libvirt and Related Tools”).
Before anything else, you’ll need to make sure that the physical machine you’re planning to use as a KVM host supports hardware virtualization. Besides the BIOS setting and the contents of /proc/cpuinfo (which we discussed in Chapter One), you can also quickly check this from a running Linux system using kvm-ok:
It’s also a good idea to be sure which hardware architecture — 64 or 32-bit — you’re working with:
But even if your hardware profile is up to the task, you’ll have to let the Linux kernel in on your plans. If they’re not already there, you should add the kvm and either kvm-intel or kvm-amd kernel modules.
If those modules fail to load (and there’s no /dev/kvm device in the file system), then there’s a good chance your CPU just isn’t up to the job you’d like it to to. However, if all that worked out, you’re ready to install the qemu-kvm package (and, if necessary, libvirt, virt-install, and bridge-utils as well).
It’s no secret that virtualization platforms have a well-deserved reputation for being complicated. But there are two things that can make getting started with KVM just a bit more challenging than some of the others:
I’ll introduce you to Libvirt Tools and vmbuilder in Chapter Five, but here, we’ll discuss the KVM tool kit.
Building new guests using what we’ll call the “KVM” way is a two step process. First, you’ll use qemu-img to create a new image — or modify or convert an old one. Then you’ll use qemu-kvm to set up a virtual machine that will start up the installation.
Did I just say “you’ll use qemu-kvm…”? Silly me. qemu-kvm was merged into qemu a long time ago and has been replaced by qemu-system-x86_64. In the meantime, some systems offer you kvm as a wrapper that executes qemu-system-x86_64 -enable-kvm — although you shouldn’t confuse the kvm wrapper with the old kvm binary that used a somewhat different syntax.
So let’s see how these two steps work. You create a disk image with qemu-img (which, by the way, can be used very effectively for other hypervisors as well), where “my-disk” is the name of the image you’d like to create, the maximum size of the image will be 6 GB, and qcow2 is the file format. qcow, by the way, stands for “QEMU Copy On Write”.
Choosing a file format will depend on your specific needs. If you need greater compatibility and flexibility — including the ability to generate sophisticated snapshots — then qcow2 is probably going to be your best choice.
The qcow disk image format permits disk space allocation to grow only as needed, meaning the use of space is always as efficient as possible. Changes to a qcow read-only image can be saved to a separate file, which refers internally to the original image. qcow2 added the ability to create multiple image snapshots.
We’re now ready for step two. Here’s how we’ll build our VM:
A new SDL window will often (although not necessarily for all distributions) pop up where you can complete the operating system installation process. Regaining control of your mouse from the Qemu terminal requires pressing CTRL+ALT.
To explain: using “kvm” (although the precise command you’ll need for your version may differ), we’ll call our new guest “my-VM”, designate the my-disk.img file as hda (“hard drive a”), point to the location of the operating system ISO (Ubuntu 16.04 server, in this case), and set 1024 MB as the maximum memory alloted to the VM.
By default, KVM will configure your guest for user-level networking (as though the parameters -netdev user,id=user.0 -device e1000,netdev=user.0 were specified). This will provide the guest with an IP address through KVM’s own DHCP service and access to your host, the Internet, and to LAN-based recourses. While the default configuration is simple, it may be overly restrictive for some scenarios, as there are often some performance and feature limitations.
Besides these, you can use command line flags to control various VM configuration parameters, including:
The -M flag will assign a specific machine type hardware emulation. pc. For example, will provide a standard PC profile. For a complete list of available machine types, you can run kvm -M ?:
While working with QEMU, you can open a monitor console and interact with your clients in ways that might be difficult or even impossible using a regular headless server. You can launch the KVM Monitor by pressing CTRL+ALT, and then SHIFT+2, and a new console will open on your desktop. SHIFT+1 will close the console. You can also access the console from the command line using something like:
You will probably NOT be able to launch the monitor as root (i.e., via sudo). Naturally, your version may require “qemu-system-x86_64” rather than kvm. This approach allows you to add command line arguments (like that -monitor which specified a console target). Consult man qemu-system-x86_64 for details on the kinds of operations the monitor allows.
This example (borrowed from en.wikibooks.org/wiki/QEMU/Monitor) will list all the block devices currently available to your system, and then point one of them to an ISO file you want to use:
By default, a KVM guest will receive an IP address within the 10.0.2.0/24 subnet, and have outgoing access (including SSH access) both to its host, and to the wider network beyond. By that same default however, it won’t be able to host services for network clients. If you need to open up incoming network connectivity, you’ll probably want to create a network bridge on your host that’s similar to the one we used for Xen in the previous chapter. As before, you will install bridge-utils on the host and, assuming you’re running a Debian-based system and you want your host to receive its IP from a network DHCP server, edit the /etc/network/interfaces to look something like this (on CentOS machines, edit files in the /etc/sysconfig/network-scripts/ directory):
On CentOS, you’ll need to create an ifcfg-br0 file in the /etc/sysconfig/network-scripts/ directory to look something like this:
…And then add a line reading BRIDGE=br0 line to your primary network interface file (which will often be: /etc/sysconfig/network-scripts/ifcfg-eth0).
You will then stop and restart your network services (or reboot).
Looking for a solid introduction to Linux or AWS administration? Check out my Linux in Action and Learn Amazon Web Services in a Month of Lunches books and the Linux in Motion text-video hybrid course from Manning. Prefer your tech learning in video? I’ve got Linux administration courses at Pluralsight just waiting to be watched.
Linux system admin and tech training content provider. Known to hang out at https://bootstrap-it.com.
545 
545 
545 
Linux system admin and tech training content provider. Known to hang out at https://bootstrap-it.com.
"
https://blog.paytm.com/we-have-launched-an-ai-cloud-for-india-816f85434281?source=search_post---------373,"Download Paytm App
"
https://medium.com/@aallan/deep-learning-at-the-edge-on-an-arm-cortex-powered-camera-board-3ca16eb60ef7?source=search_post---------374,"Sign in
There are currently no responses for this story.
Be the first to respond.
Top highlight
Alasdair Allan
Jul 4, 2018·4 min read
It’s no secret that I’m an advocate of edge-based computing, and after a number of years where cloud computing has definitely been in ascendency, the swing back towards the edge is now well underway. Driven, not by the Internet of Things as you might perhaps expect, but by the movement of machine learning out of the cloud.
Until recently most of the examples we’ve seen, such as the Neural Compute Stick or Google’s AIY Projects kits, were based around custom silicon like Intel’s Movidius chip. However, recently Arm quietly released its CMSIS-NN library, a neural network library optimised for the Cortex-M-based microcontrollers.
Machine learning development is done in two stages. An algorithm is initially trained on a large set of sample data on a fast powerful machine or cluster, then the trained network is deployed into an application that needs to interpret real data. This deployment stage, or “inference” stage, is where edge computing is really useful.
The CMSIS-NN library means it’s now much easier to deploy trained networks onto much cheaper microcontrollers. This is exactly what OpenMV have done with their OpenMV Cam, which is built around a Cortex-M7 processor.
The OpenMV Cam is a small, low-powered, microcontroller-based camera board. Programmed in MicroPython, it’s well suited for machine vision applications, and part of a generation of accessible boards now appearing on the market. Based around the STM32F765VI Arm Cortex-M7 processor running at 216 MHz, it has 512KB of RAM and 2 MB of flash memory. It features a micro SD Card socket for local storage, and both full-speed USB (12Mb/s) and an SPI bus (54Mb/s) for streaming images. The board has both a 12-bit ADC, and 12-bit DAC.
The image sensor on the board is a OV7725 capable of taking 640x480 8-bit Grayscale images, or 640×480 16-bit RGB565 images at 60 FPS when the resolution is above 320×240 and 120 FPS when it is below.
The sensor is mounted behind a 2.8mm lens using a standard M12 lens mount, also sometimes known as an “S-mount,” commonly used for CCTV and cheap webcams. That means if you want to use a different or more specialised lens with the board you can swap out the default lens and attach it.
We’ve seen trained networks on edge devices before, I’ve talked about them extensively here on the Hackster blog, and out in meat-space at O’Reilly’s AI and Strata Conferences, as well as Crowd Supply’s Teardown conference.
However the OpenMV walkthrough of how to train a model in Caffe, then quantize it for use with the CMSIS-NN library, and deploy it to low-powered hardware, is the first time I’ve seen the entire process wrapped up with a bow. If you’re interested in running trained networks nearer your data, this is a good place to start.
The ability to run trained networks nearer the data — without the cloud support that seems necessary to almost every task these days, or even in some cases without even a network connection — could help reduce barriers to developing, tuning, and deploying machine learning applications. It could potentially help make “smart objects” actually smart, rather than just network connected clients for machine learning algorithms running in remote data centres. It could, in fact, be the start of a sea change about how we think about machine learning and how the Internet of Things might be built. Because now there is — at least the potential — to allow us to put the smarts on the smart device, instead of in the cloud.
The recent scandals and hearings around the misuse of data harvested from social networks has surfaced long standing problems around data privacy and misuse, while the GDPR in Europe has tightened restrictions around data sharing. Yet the new generation of embedded devices, the arrival of the IoT, may cause the demise of large scale data harvesting entirely. In its place smart devices will allow us process data at the edge, making use of machine learning to interpret the most flexible sensor we have, the camera.
Interpreting camera data in real-time, and abstracting it to signal rather than imagery, will allow us to extract insights from the data without storing potentially privacy and GDPR infringing data. While social media data feeds provides ‘views,’ lots of signal, it provides few insights. Processing imagery using machine learning models at the edge, on potentially non-networked enabled embedded devices, will allow us to feedback into the environment in real time closing the loop without the large scale data harvesting that has become so prevalent.
In the end we never wanted the data anyway, we wanted the actions that the data could generate. Insights into our environment are more useful than write-only data collected and stored for a rainy day.
Scientist, Author, Hacker, Maker, and Journalist.
602 
2
602 claps
602 
2
Scientist, Author, Hacker, Maker, and Journalist.
About
Write
Help
Legal
Get the Medium app
"
https://towardsdatascience.com/byod-build-your-own-dataset-for-free-67133840dc85?source=search_post---------375,"Sign in
There are currently no responses for this story.
Be the first to respond.
Rishav Agarwal
Jul 19, 2018·8 min read
Ten second takeaway: Learn how to create your own dataset with simple web scraping that mines the entire Metacritic website for game reviews using python’s beautiful soup and hosted for free on Google Cloud Platform (GCP) micro (always free tier)
Aspiring data scientists are always confused about the first step after learning the theory. There are very few places where they can apply the accrued knowledge. Sure there are plenty of datasets available but the free ones never give you a pragmatic insight into solving actual problems or sometimes they are too small to use for deep learning applications.
One way to get robust datasets is either to pay for them or enrol in expensive courses, and another is web scraping. Here I tell you how to scrape large datasets using python for free!
In my case I did not find a good dataset for game reviews that was fairly new, as Metacrtic has the largest game repository and gets updated fairly regularly I decided to go with that.
All you need to do is iterate over a list of URL, identify the containers for the data, extract the data and store it in a csv.
1. Libraries used
2. Understanding the flow of the website
Metacritic layout is pretty simple. All the data is structured as follows
http://www.metacritic.com/browse/games/release-date/available/pc/metascore?view=detailed&page=1
lets break it down:
Next we decide on the html elements which have the data. For this we use the inspect tool in Chrome. We select on the elements to and highlight the subsection to get the html element and its class .
Now we know what element we need to extract let’s go ahead and extract them.
3. Making URL requests
Metacritic has a simple site layout with a static URL where page number changes for each page.
we use the urllib2.Request to request for the page and Urllib2.urlopen to read the page data
Tip: There are 53 pages so I put my counter to max out at 54 however you can simply include all this in a try except which exits on encountering an error.
4. Extracting data
We then read the data
We use BeautifulSoup(content, ‘html.parser’) which does all the heavy lifting of parsing the vast amount of HTML.
Now we saw in the previous section that each game data is in a div with a class called product_wrap. So we extract all such divs and iterate over each div to get to the data. Here we store the following data:
Tip: HTML is unreliable so it’s better to use try:except witch each extraction
5. Saving data
We use pandas to convert the list of list into a table and then write the data to a csv. Here we use the | as a delimiter as the genre column contains , (comma)
6. Running the code on Google cloud
I’m assuming you would know how to setup a GCP account. If now please follow this blogpost on how. You need to create an instance as follows.
Once it is running you need to install python on it and copy the code from your local machine to the instance. Here spheric-crow is the project name and instance-2 is the instance name. You need to also specify the time zone to google.
Tip: Remember to update your ubuntu once you ssh to the instance.
scp copies the file from the local machine to the GCP instance. Next you need to install the aforementioned libraries on your instance. Next install byobu which is a text based windows manager. This will keep the session intact even if your ssh connection breaks.
Finally run the code using the following command and you are done.
Tip: You can scp or simply git pull from my github account.
7. Retrieving the mined data
You can get the mined data from the cloud using scp again and now you have a really cool dataset to play with!
you can then access the data using python as
and your dataset will look like
See how nicely structured this is!
Tip: You can automate this too!
2. Caught in a Captcha: Some websites really don’t want you scraping their data and they put a captcha in place. If it’s a simple 4–5 alphanumeric captcha you can try to work it using Python Tesseract and this technique. If it the google re-captcha you have to manually solve each time just like these guys.
3. Exception Handling: HTML is very unreliable and site may not follow a strict pattern all the time so the best practice is to include each element in a try:except statement.
4. Saving periodically: Web scraping is risky business and if you don’t save data regularly you may risk losing the entire dataset mined so far. A simple solution to this is to save data regularly in a csv (like I do) or use SQLite (like the smart people do). A introduction to sqlite can be found here.
We can use the same code to mine Metacritic for loads of other content just by changing the base url:
I extended my code to mine for data all ~100k user reviews for ~5k PC games and I had mentioned I will be using it for a game recommendation engine (blog posts to follow). If you want a slice of the action email me and I’ll send you a part of the dataset!
Metracritic’s site layout is pretty easy to follow and is replicated by a simple for loop but for mining more complicated sites like Amazon we use a browser automation tool called Puppeteer, which simulates clicks to generate the next page and so on.
Check out this great blog by Emad Eshan on how to use Puppeteer.
The entire code for this blog can be found on my git. The entire code for the user review scraping can also be found here.
In the next blog, I’ll be doing some data wrangling and deep learning on this great dataset. Stay tuned!
This is my first post so if you liked it, do comment and clap :)
CS graduate student. ML/Deep Learning/Data Science Enthusiast. Hobby Photographer
343 
4
343 
343 
4
Your home for data science. A Medium publication sharing concepts, ideas and codes.
"
https://medium.com/rigetti/introducing-rigetti-quantum-cloud-services-c6005729768c?source=search_post---------376,"There are currently no responses for this story.
Be the first to respond.
By Chad Rigetti
Quantum computing is approaching a pivotal milestone called quantum advantage. This is the inflection point where quantum computers first begin to solve practical problems faster, better, or cheaper than otherwise possible. The first demonstration of quantum advantage will be an extraordinary achievement, but it will only be the beginning. Ultimately, quantum advantage will be reached over and over again in new markets and new domains, changing the ways in which problems are solved across industries.
Three key capabilities are essential to achieving quantum advantage. First, users need more qubits with lower error rates. Second, users need computing systems designed to run the hybrid quantum-classical algorithms that offer the shortest path to quantum advantage. Finally, these capabilities must be delivered alongside a real programming environment so users can build and run true quantum software applications.
In August, we announced that we are building 128-qubit quantum computers with the low error rates needed to achieve advantage. These systems are based on our scalable 16, 32, and 128-qubit Aspen quantum processors. And today, to deliver the final key capabilities, we are excited to introduce Quantum Cloud Services.
Quantum Cloud Services is the only quantum-first cloud computing platform. With QCS, for the first time, quantum processors are tightly integrated with classical computing infrastructure to deliver the application-level performance needed to achieve quantum advantage.
Users access these integrated systems through their dedicated Quantum Machine Image. The QMI is a virtualized programming and execution environment designed for users to develop and run quantum software applications. Every QMI comes pre-configured with Forest 2.0, the latest version of our industry-leading software development kit.
We will be granting early access to Quantum Cloud Services in the coming weeks. You can sign up to reserve a QMI today at rigetti.com.
Partnerships with leading startups We’re partnering with visionary startups who are building the first generation of practical quantum applications. These companies include 1QBit, Entropica Labs, Heisenberg Quantum Simulation, Horizon Quantum Computing, OTI Lumionics, ProteinQure, QC Ware, Qulab, QxBranch, Riverlane Research, Strangeworks, and Zapata Computing. They will use QCS to develop groundbreaking applications and as a channel to distribute these applications to the broader community, putting even more tools into the hands of developers and researchers.
The Quantum Advantage Prize Quantum Cloud Services has been designed from the bottom-up to accelerate the pursuit of quantum advantage. We don’t know when the first demonstration of quantum advantage will be achieved, or what shape it will take, but one thing is certain: it will dramatically accelerate progress in unlocking the power of quantum computing for everyone. Recognizing the significance of this achievement, Rigetti Computing is offering a $1 million prize for the first conclusive demonstration of quantum advantage on QCS. More details of the prize will be announced on October 30th, 2018. Stay tuned!
Rigetti Computing
532 
532 claps
532 
Rigetti Computing
Written by
On a mission to build the world’s most powerful computer.
Rigetti Computing
"
https://medium.com/softkraft/aws-lambda-architecture-best-practices-e2ef23b85abf?source=search_post---------377,"There are currently no responses for this story.
Be the first to respond.
With the evolution of technology from mainframe computers to personal computers and cloud computing, the one thing that is constant is the need to make technology more efficient, convenient and affordable.
The introduction of serverless architecture has gained ground all over the world and is now a favoured option by most companies. Backend as a Service (BaaS), such as the authentication services…
"
https://medium.com/techmagic/serverless-vs-docker-what-to-choose-in-2019-80cb80f4b680?source=search_post---------378,"There are currently no responses for this story.
Be the first to respond.
Top highlight
Let’s start with the fact that cloud computing is growing exponentially. Businesses are continually migrating from traditional data centres and inefficient physical servers to innovative cloud technologies and microservices architectures. What are the key factors driving businesses to build or migrate their architectures to the cloud? Based on a recent survey we highlighted 4 benefits of cloud computing that are considered to be vital for most businesses.
Today there is an increasing number of different cloud services that enable developing more cost-efficient applications with higher performance and more effortless scalability such as Infrastructure as a Service (IaaS), Platform as a Service (PaaS), Function as a Service (FaaS) and Software as a Service (SaaS). If you’ve been looking for a full-service framework solution for your business needs, then it’s likely that you’ve come across the concepts of Serverless and Docker Containers, that represent FaaS and PaaS models, respectively. Many experts now consider these solutions as the best practices in cloud computing as they offer simplicity and flexibility in application development and deployment. However, they have some unique differences that must be taken into consideration, and today we are going to cover it.
To start with, it’s worth saying that both — Serverless and Docker Containers point out an architecture that is designed for future changes, and for leveraging the latest tech innovations in cloud computing. While many people often talk about Serverless Computing vs Docker Containers, the two have very little in common. That is because both technologies aren’t the same thing and serve a different purpose. First, let’s go over some common points:
Although Serverless is more innovative technology than Docker Containers, they both have their disadvantages and of course, benefits that make them both useful and relevant. So let’s review the two.
Serverless allows you to build and run applications and services without provisioning, scaling, and managing any servers. You can build them for virtually any type of application or backend service, and everything required to run and scale your application with high availability is handled for you. Let’s have a look at the benefits and drawbacks of Serverless.
The four major services that are being utilized for the Serverless orchestration are Amazon Step Functions, Azure Durable Functions, Google Cloud Dataflow, and IBM Composer.
Here are a few companies and startups that use Serverless right now: Netflix, Codepen, PhotoVogue, Autodesk, SQQUID, Droplr, AbstractAI.
To get more information about Serverless, refer here:
Docker is a containerization platform that packages your application and all its dependencies together in the form of a docker container to ensure that your application works seamlessly in any environment.
The most popular tools that are being utilized for Docker containers orchestration are Kubernetes, Openshift, Docker Swarm.
Here are a few companies and startups that use Docker right now: PayPal, Visa, Shopify, 20th Century Fox, Twitter, Hasura, Weaveworks, Tigera.
To get more information about Docker, refer here:
Are Serverless and Docker the competing platforms? Hardly. They are mutually supporting parts of the dynamic world of cloud computing. Both services used to develop microservices but work for different needs. If you want to reduce application management and don’t care about the architecture, then Serverless is the best option. If you want to deploy an application on specified system architecture with having control over it, then Docker containers are the best option. So when comparing Serverless vs Docker, it comes down to choosing what is better for your particular needs.
TechMagic works a lot with Serverless architectures utilizing JavaScript stack and AWS or Google infrastructure. We are a certified AWS Consulting Partner and an Official Serverless Dev Partner.
To receive more information about TechMagic’s cloud computing capabilities and services, contact us at hello@techmagic.co or through the contact form.
techmagic.co
All about JavaScript, AWS, and Serverless in one place.
316 
5
316 claps
316 
5
Written by
TechMagic is an AWS Consulting Partner with a narrow technological focus on JavaScript, Serverless, Salesforce, and Native Mobile. https://www.techmagic.co/
TechMagic is a tech consulting company focused on JavaScript, AWS, and Serverless.
Written by
TechMagic is an AWS Consulting Partner with a narrow technological focus on JavaScript, Serverless, Salesforce, and Native Mobile. https://www.techmagic.co/
TechMagic is a tech consulting company focused on JavaScript, AWS, and Serverless.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/capital-one-tech/serverless-is-the-paas-i-always-wanted-9e9c7d925539?source=search_post---------379,"There are currently no responses for this story.
Be the first to respond.
In the early days of Cloud computing, there was a really simple picture we would share to explain the Cloud Service Model, sometimes with a diagram like the pyramid above. I’m sure you’ve seen a version of it — a way of showing the different service types as a stack, highlighting the relationships between each. When I gave training for new associates beginning to use Cloud, the instruction would cover the different service layers, and the definition of each one would be spelled out.
Infrastructure as a Service — the foundation to the new Cloud computing model. I’d articulate how using this model differed from owning your own datacenter, including no longer having to focus on managing facilities, and the per unit cost benefits that these major providers have in scale for racking commodity servers and top of rack switches.
Platform as a Service was well — a work-in-progress. There were some real world examples, but much of it was still describing what was possible vs. what was being used in the marketplace. The advantages were clear as we needed more than just a provider providing compute capacity, and I would cite this reference from NIST that carried great appeal.
Cloud Platform as a Service (PaaS). The capability provided to the consumer is to deploy onto the cloud infrastructure consumer-created or acquired applications created using programming languages and tools supported by the provider. The consumer does not manage or control the underlying cloud infrastructure including network, servers, operating systems, or storage, but has control over the deployed applications and possibly application hosting environment configurations. — source: National Institute of Standards and Technology, ca 2009.
Software as a Service — is where speed and agility would really be unlocked. There were great examples to highlight the success and growth of Office365 and Workday, as well as many industry specific services. The model and benefits were easy to articulate given the time to market savings, and it fit nicely into the microservices discussions that were also being introduced at the same time.
Given the need to build custom servicing systems and leverage standard engineering patterns, the concept was great, but of the three service types, the PaaS market was slowest to develop, and without great success stories, harder to explain the value proposition to others.
Okay, fast forward to today, and let’s check in on our options for building infrastructure in the Cloud. How are they similar and how are they different from when I first gave my training?
Given that it’s 2017, let’s use a common contemporary investment project as an example — building a voice chat-bot. This hypothetical chat-bot will be a friendly voice-driven assistant that can operate on the Amazon Alexa platform and help walk individuals through finding and booking their ideal vacation getaway. Marketing says it will be a big hit this winter as people seek to escape the cold weather. Therefore, we should assume it can handle thousands of requests per second; and when running our ad campaigns, potentially service millions of prospects each day. Let’s also assume we’re looking to push the envelope for Amazon Alexa development so the returned content will not just be spoken words but will also include the ability to play sounds from exotic locales and use the card feature to show pictures of possible destinations.
The first alternative is to use the foundational parts that a Cloud provider like AWS offers, similar to a traditional on-premise datacenter. We will benefit by having all of this infrastructure pre-provisioned for us, but will need to automate the provisioning of the different parts, as well as the assembly. The solution might end up looking like this.
Within the new platform are a series of stacks:
Each stack will have an ELB in each region, it’s own cluster of EC2 instances (complete with auto-scaling groups), as well as requisite EBS volumes, subnets, and security groups. We will automate it all via CloudFormation.
How much infrastructure is needed? Well it depends on what our non-functional requirements, but assuming extensive use of auto-scaling, running workloads across zones and regions to provide a HA solution, blue-green deployments for code rollouts, and non-production environments, around 60–80 EC2 instances. Let’s say half of those are on-demand, and the others we can convert to Reserved Instances. Throw in a dozen or so ELB’s, 120–150 EBS volumes, some with high IOPS for the analytics. Sizing matters, and while the web layer we might be able to get down to small general purpose instances, the analytic and content management components may be specialty types with large capacity.
The first option has many pieces and parts, and is going to take multiple engineers familiar with the domains to go off and provision each stack. One of the aspirations we had in going to the Cloud was that we were getting out of the complexity in managing infrastructure, so how else can we accomplish this? Let’s try and use the new serverless approach, and see how it compares.
Rather than building “stacks” like in the first option, we’re using different services in the AWS catalog (numbers match stacks for option #1 above).
Now let’s go further in comparing the two options.
One key concept to understand between the options is what the pricing model is, and if we’re paying for capacity (option #1) or consumption (option #2). For example, the chart below describes the typical utilization of a servicing application that peaks during the day. In option#1, we’re paying for everything in the box, but in option #2, we’re just paying for what is consumed — the “blue area”. There are features available with IaaS like autoscaling where capacity can be dropped during off hours (see dark line), but even with this done really well, there still will be some amount of excess capacity being paid for that’s not used. If it’s not done well or the workload isn’t a great fit for auto-scaling, the delta between the two areas is huge — potentially 3–5x.
As long as we have AWS experts, it’s far quicker to provision infrastructure in option #2. It’s getting us out of managing the low level components that can be time consuming to orchestrate, and each service exposes the key parts for us to tune. Also there’s a labor impact with option #1 when we focus cycles to configure and test the auto-scaling rules. While this might be time well-spent, it drives up the labor effort (thus isn’t free). In option #2, this complexity is taken on by the cloud provider and included into the price of the service.
Now after reading this, you might be inclined to improve on these design options using containers and swarm/kubernetes/mesos, etc. to orchestrate. They are a good fit for stack #1, a moderate fit for stacks #4 & #5, and a poor fit for stacks #2 & #3. Containers are a great technology as they can simplify application provisioning and deployment, and drive up the utilization of our infrastructure — narrowing the gap between what’s been provisioned and what’s being consumed in the chart above.
First, let’s clarify our assumptions that if we use ECS, it’s just a variation of option #2 as the Cloud provider is taking on the base infrastructure and tooling. As a customer, if I build containers on top of the base infrastructure, that is a new approach with merits over option #1, but let’s ask ourselves a clarifying question on what our broader goals are as an organization.
Where do you want your engineering talent to spend time on? Do you want them to become experts at building infrastructure abstraction layers, or do you want them to be creating product features?
For me, I don’t work for a Cloud hosting company, and building tools and infrastructure abstraction layers isn’t our core competency. I do want my engineers to spend time focusing on customers, and building features to improve their financial lives. While my team could be building tools to manage containers, this effort takes away from features/cycles that could be dedicated to product development. From my perspective, abstractions on top of infrastructure are great works of engineering, but isn’t that what the Cloud provider is supposed to be doing for me?
Serverless patterns are still maturing, and while products are evolving quickly, they still have limitations which can require workarounds to mimic the features that can be built with frameworks from scratch on top of IaaS. Over time, I’m assuming these limitations will go away as these services become more mature. There are also some significant shifts in infrastructure approaches that groups must adopt for this new model to be a success. These include:
One of the biggest mindset shifts when deploying the serverless model is in how the network is managed, as well as how different components communicate. Most current security models assume a private network namespace that mimics a traditional datacenter. Small blocks of address space are allocated up-front into subnets based on size estimations for the applications. Modeling tools for managing the software defined network are robust, facilitating the complexity built up in legacy firewalls, but there still is an assumption of doing a local network design for the application where IP address ranges are king.
The serverless model assumes role-based access as the authentication model between components, with a network space that is largely hidden from the application. Whomever is provisioning the infrastructure will need to understand the relationships between the components (i.e. which components can talk to others), and be able to author policies that enable this communication. For example, in Option #2, the execution role for the Lambda function will need a policy enabling it to write and read only to relevant S3 buckets and DDB tables. This is how we get to the principal of “least privilege” in information security.
When building highly redundant platforms that can deploy applications across regions, we need to engineer how the persisted data will be replicated in support of any RTO/RPO service levels. While S3 can easily be configured to transport data across regions, products like DynamoDB & RDS are designed to provide capability across zones (datacenters) and require extra tools and patterns to replicate across regions. When building applications using IaaS, the assumption is that you’re doing it all yourself, and in some cases there are frameworks or tools to install that enable this to be done. Long-term I believe that these features will get built into the services as features by the Cloud providers, and new features are being released every day.
Building efficient infrastructure using a Cloud provider requires good engineering skills and a training plan for associates when they’re getting started. Building robust platforms with just the IaaS services does get complex, requiring an in-depth understanding of proper EC2 instance types, security groups, EBS options, autoscaling, and configuration of ELB’s. It may be easier to convert a legacy sysadmin over to IaaS given that many of the patterns are the same (although most that I’ve met are gapped on the required networking skills).
In the serverless model, the level of detail needed to get started with an individual service is less as the provisioning tooling provides the framework for how to get started, and defaults are provided to simplify the model. It’s only when we start combining many services do catch-up to the level of complexity needed to learn the IaaS model that assumes significant depth on how to assemble a very fungible set of tools to support a specific pattern.
If I were to update the training dialogue from my Cloud Computing class to reflect the state of Cloud Service Models in 2017, the examples I would cite in the PaaS section offerings from AWS including Lambda, DynamoDB, and RedShift. They are an excellent model “using programming languages and tools supported by the provider” while not requiring me to “manage or control the underlying cloud infrastructure including network, servers, operating systems, or storage” that the NIST definition laid out to define PaaS years ago. Getting out of managing the underlying infrastructure enables engineers to focus on the most important part of solution building — being responsive to the customer’s needs.
For more on APIs, open source, community events, and developer culture at Capital One, visit DevExchange, our one-stop developer portal. https://developer.capitalone.com/
The low down on our high tech from the engineering experts…
118 
2
118 claps
118 
2
The low down on our high tech from the engineering experts at Capital One. Learn about the solutions, ideas and stories driving our tech transformation.
Written by
Technology enthusiast, Amazon Alexa Champion, EV enthusiast. Always learning how to make new things with the latest tech.
The low down on our high tech from the engineering experts at Capital One. Learn about the solutions, ideas and stories driving our tech transformation.
"
https://medium.com/@satortoken/sator-announces-partnership-with-new-tv-series-hodl-and-cudos-39153c00bcac?source=search_post---------380,"Sign in
There are currently no responses for this story.
Be the first to respond.
SATOR
Aug 17, 2021·4 min read
Hollywood, La La Land, the glitz and the glam, decentralized cloud computing… wait.. What??
One of those seems out of place, doesn’t it?
Cudos has signed an agreement with HODL, a cryptocurrency themed TV drama series airing in 2022 and Sator, a new TV engagement platform set to launch very soon. HODL (Hold On for Dear Life) follows the journey of a young entrepreneur launching a new blockchain project and cryptocurrency, Aveer, named after the best friend who has disappeared.
So where does Cudos fit into this? Cudos’ parent company Cudo Ventures has spent the past 4 years developing and scaling a global distribution of computing power, with cloud service providers, gamers and miners contributing their spare computing power to the network in return for payments in multiple different cryptocurrency options including BTC, ETH, XMR and ALGO. With a mission to build the world’s largest distributed cloud computing platform by making better use of existing and often un-utilized hardware, Cudos has successfully created a profit making sustainable model, where any one or business can earn from the network. The Airbnb of computing power.
Next up on Cudo’s mission is the Cudos Network, a layer 1 blockchain and proof-of-stake network, providing decentralization to the overall ecosystem. With inherent scalability and cost issues within the blockchain space, use cases have thus far been limited to small data jobs such as DeFi transactions, NFT issuance and data oracle feeds. Compute intensive jobs such as zkSnarks and price prediction analysis still have to be processed on centralized hyperscale services, going against the grain of a decentralized technology and community. This is where Cudo differs, merging blockchain and cloud computing, marrying the benefits of blockchain with the significantly lower cost of cloud.
“There are many industries that are eager to utilize blockchain technology for security, efficiency and immutability reasons, but it is simply not affordable yet. The movie industry’s post production houses need huge CPU and GPU rich server farms to render the latest Hollywood blockbusters and the expense of buying, maintaining and managing these environments is often what drives them to use the hyperscale clouds instead. They pay more for hyperscale services than smaller cloud service provider environments, but they save time with the convenience of near unlimited scale. Decentralizing these enormous workloads and distributing the computing task out to suitable service provider environments federated across the network, provides the ideal solution for businesses not only in the movie world, but also in sciences, AI and video content streaming.” Matt Hawkins, CEO, Cudo.
When HODL airs in 2022, Cudo will showcase their mining application, Cudo Miner, in-episode and paying out in new TV fan engagement token, SAO, the native token of the Sator platform. Viewers of HODL will be able to download Cudo Miner and earn SATOR tokens from their spare computing power. These SAO tokens can then be used throughout the Sator platform and used to buy digital memorabilia from the series in the form of NFTs.
“Blockchain and crypto is on the cusp of mainstream adoption and in collaboration with HODL and Sator, we are determined to accelerate that adoption providing an upwards target of 50m people globally, with the tool to embrace the technology. Owning a unique or scarce digital collectible of your favorite show actor or maybe even the directors chair is extremely enticing to fans and the fact we now have the tech to be able to pay for that using your computing power is incredible. This is all about fan engagement and that is why we had no hesitation in partnering with HODL and Sator.” Pete Hill, VP of Sales, Cudo.
The collaboration goes even further with Cudo providing further ideas to fit into the HODL storyline. “We’re thrilled to integrate Cudo given our show’s authenticity. HODL reflects actual crypto life — all it’s drama and comedy. This stuff writes itself. Including real crypto brands, like Cudo, adds to show’s tapestry and truth,” reflects HODL writer and EP, Chris Martin.
What will this mean for the Cudos Network? No spoilers, this is Hollywood, so you’ll have to wait and see!
About HODL
For Dear Life (“FDL”) token is issued by the U.S. television comedy series, Hold On For Dear Life (the “Series”). FDL tokens represent a pro rata share in net profits of the Series. www.watchHODL.tv
About Cudos
The Cudos Network is a layer 1 blockchain and layer 2 computation and oracle network being designed to ensure decentralized, permissionless access to high performance computing at scale and enable scaling of computing resources to 100,000’s of nodes. Once bridged onto Ethereum, Algorand, Polkadot, and Cosmos, Cudos will enable scalable compute and Layer 2 Oracles on all of the bridged blockchains.
https://t.me/SatorSAO
https://satortoken.medium.com
https://instagram.com/satorsao
https://twitter.com/SatorSAO
https://discord.gg/mqcpTKngx7
https://tiktok.com/@satorsao
https://sator.io
https://watchHODL.tv
https://twitter.com/watchhodl
https://instagram.com/watchhodl
https://cudos.org
https://twitter.com/CUDOS_
https://youtube.com/c/CUDOS
https://instagram.com/cudoscast
https://t.me/cudostelegram
www.Sator.io is a dApp that aligns the interests of content providers and content viewers over blockchain, providing engagement, staking and series-based NFTs.
388 
388 
388 
www.Sator.io is a dApp that aligns the interests of content providers and content viewers over blockchain, providing engagement, staking and series-based NFTs.
"
https://levelup.gitconnected.com/aws-vs-azure-vs-google-detailed-cloud-comparison-b075a35fc8b8?source=search_post---------381,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Cloud adoption is taking over multiple regions and industries. Companies figured out that after adopting cloud computing, they become more agile in the competitive marketplace. Businesses now don’t have to spend time and money maintaining and buying their own servers, rather, they can use the off-the-shelf professional…
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/mit-technology-review/meet-americas-newest-military-giant-amazon-69f4e7b5284a?source=search_post---------382,"There are currently no responses for this story.
Be the first to respond.
"
https://medium.com/bloomberg-businessweek/the-most-valuable-company-for-now-is-having-a-nadellaissance-be9fa358e21f?source=search_post---------383,"There are currently no responses for this story.
Be the first to respond.
By Austin Carr and Dina Bass
"
https://marker.medium.com/why-amazon-wants-to-invest-in-the-worst-ipo-of-2020-6db9725891b2?source=search_post---------384,"Sign in
There are currently no responses for this story.
Be the first to respond.
Mario Gabriele
Aug 20, 2020·5 min read
Welcome to The Ticker, a series that examines everything you need to know about companies going public.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/hackernoon/how-can-my-business-benefit-from-aws-pt-1-214220b996ce?source=search_post---------385,"There are currently no responses for this story.
Be the first to respond.
I spend a few days answering questions about Cloud Computing on Quora. This question caught my interest. I decided to write about this since it can benefit a lot of people. Since AWS has got a lot of tools, I decided to cover them in parts. This is part one of the series. Here is part two, part three.
Make sure you follow Hackernoon and me (Febin John James) . If you need help with any specific topic on cloud computing, you can use this form to make requests.
If you are interested in making apps on cloud , do checkout my book Cloud Is a Piece of Cake on Amazon.
In part one of the series. I will cover the compute section of AWS. If you have any doubts or find terms difficult to understand, please comment here. I can improve the story and this can benefit a lot of people.
If your service needs to be accessible to customers on the internet, you will need to host it on a computer or a server. Earlier we used to buy or rent these servers. If your service gets less traffic than expected, then you will have huge losses. If the traffic spikes you have a time consuming process to scale it up.
You don’t have to worry about these issues with EC2. Here you can start with a instance of your need, and configure it to scale up according to the traffic.
AWS allows you to choose the OS (Linux) of your choice for your instance. You get complete ownership of the instance/computer. You can install the softwares of your choice in it.
You can also use these instance to perform heavy process. People use AWS P2 Instances to run deep learning algorithms. Since they have got High-performance NVIDIA K80 GPUs.
In Ec2 you get a computing instance with an Operating System. Linux Operating Systems comes with a lot of tools. Some of them are not necessary for your application. This also consumes resources.
Hence we made containers. Containers can be build to only have the necessary libraries or tools to make your service work. Hence, this is lightweight and efficient.
AWS EC2 Container Service allows you to host your container on the cloud. They also provide tools to scale it.
AWS Lightsail is a simplified version of AWS Ec2. AWS EC2 can be complicated for beginners. Because you will need to configure SSD Storage, DNS Management, Static IP etc . You have also got to pay additional charges for bandwidth.
In Lightsail these come as a pre-configured package. Also the charges are fixed. There are pre-configured images for wordpress, LAMP, MEAN etc. Hence, it’s easy for deployment.
In EC2 , there are a lot of steps before you deploy your app on cloud. You will need to choose the configuration, set up autoscaling, security groups etc.
Using AWS Elastic Beanstalk developers only have to upload the code(Java, .NET, PHP, Node.js, Python, Ruby, Go, Docker). The other procedures are automated. AWS will choose the EC2 Instance, Setup Load Balancing, Auto Scaling, etc.
In EC2, we have a server running all the time. This can be inefficient for some use cases.
Consider this, you have setup a EC2 server which is configured to resize an image whenever it is uploaded. What if there is only one image upload a day? You are still paying for the running hours of EC2 instance. Pretty inefficient rite?
AWS Lambda is build for these type of use cases. Here you go server less by configuring a trigger in lambda. In our example the trigger is image upload. You can write a function in AWS Lambda to process the uploaded image. So you only pay when your computing resources are used.
AWS Batch is used to run batch processing jobs efficiently. Consider genome sequencing. There are lot of procedures which needs to be performed step by step. Each step is dependent on the previous data. Each step may have different computing requirements.
AWS Batch automatically take cares of provisioning the computing requirements and does the heavy lifting. So you can focus more on solving the problem and worry less about running it efficiently.
In order to calculate prices you can use this tool.
You can also join my mailing list Cloud Computing Stories. I will use this to notify you if I write new stories or books on Cloud Computing.
#BlackLivesMatter
61 
1
61 claps
61 
1
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
Author of Cloud Is a Piece of Cake
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
"
https://towardsdatascience.com/top-12-ways-to-learn-aws-for-free-1113af329d06?source=search_post---------386,"Sign in
There are currently no responses for this story.
Be the first to respond.
Anna Geller
Oct 20, 2020·5 min read
When you look at technical job ads these days, many require some knowledge about Amazon Web Services. Regardless of whether you are a programmer, data scientist, data engineer, DevOps, database administrator, data analyst, or a manager, knowing AWS is an important skill. Even if some company uses a different cloud…
"
https://netflixtechblog.com/four-reasons-we-choose-amazons-cloud-as-our-computing-platform-4aceb692afec?source=search_post---------387,"One year ago, none of Netflix’s customer traffic was supported out of AWS, Amazon’s cloud services. Today, most of our member traffic is supported by software we’ve built and deployed in AWS. This includes much of our member website at Netflix.com as well as the software supporting many Netflix Ready Devices, for example: the Xbox, PS3, Wii, AppleTV, iPhone, and iPad.
We’ve built and deployed search engines in AWS, recommendation systems, A|B testing infrastructure, streaming servers, encoding software, huge data stores, caching architectures, various SQL and NoSQL solutions, all in 2010 for the first time.
This has been a Herculean effort by our engineering teams, of re-architecture, quick learning, nimble coding, incredible effort and can-do attitude. The teams have managed this feat while at the same time supporting massive business growth throughout the year, adding thousands of new titles to our streaming library, and launching many new Netflix Ready Devices.
Our move to AWS will be the subject of various future posts to this blog, I wanted to kick the subject off with an explanation for why we chose AWS as a platform. This is a question that comes up fairly often when we interview engineering candidates.
1. We needed to re-architect, which allowed us to question everything, including whether to keep building out our own data center solution.
Netflix has been fortunate to experience incredible growth in recent years, both in terms of customers and streaming devices. We started to hit that point where every layer of our software stack needs to be able to scale horizontally. With the shift to streaming, our software needs to be much more reliable, redundant, and fault tolerant.
After many fruitful years of growth on our previous three tier, database oriented architecture, it was time for us to go back to the drawing board. We could have chosen to build out new data centers, build our own redundancy and failover, data synchronization systems, etc. Or, we could opt to write a check to someone else to do that instead.
2. Letting Amazon focus on data center infrastructure allows our engineers to focus on building and improving our business.
Amazon calls their web services “undifferentiated heavy lifting,” and that’s what it is. The problems they are trying to solve are incredibly difficult ones, but they aren’t specific to our business. Every successful internet company has to figure out great storage solutions, hardware failover, networking infrastructure, etc.
We want our engineers to focus as much of their time as possible on product innovation for the Netflix customer experience; that is what differentiates us from our competitors.
3. We’re not very good at predicting customer growth or device engagement.
Netflix has revised our public guidance for the number of subscribers we will end 2010 with three times over the course of the year. We are operating in a fast-changing and emerging market.
How many subscribers would you guess used our Wii application the week it launched? How many would you guess will use it next month? We have to ask ourselves these questions for each device we launch because our software systems need to scale to the size of the business, every time.
Cloud environments are ideal for horizontally scaling architectures. We don’t have to guess months ahead what our hardware, storage, and networking needs are going to be. We can programmatically access more of these resources from shared pools within AWS almost instantly.
4. We think cloud computing is the future.
One year ago, our cloud computing expertise was limited to research and some prior experience at other companies. Today Netflix is running one of the highest volume cloud computing deployments in the world. Engineers who choose to work at Netflix are developing skills that will be increasingly relevant over the years to come as cloud computing becomes the dominant platform for growing internet companies.
We believe this transition will take place. It will help foster a competitive environment for cloud service providers which will help keep innovation high and prices dropping. We chose to be pioneers in this transition so we could leverage our investment as we grow, rather than to double down on a model we expect will decline in the industry. We think this will help differentiate Netflix as a place to work, and it will help us scale our business.
Next up I’ll post on some of the mistakes we’ve made and lessons we’ve learned through our transition to AWS.
— John Ciancutti.
Originally published at techblog.netflix.com on December 14, 2010.
Learn about Netflix’s world class engineering efforts…
184 
1
184 claps
184 
1
Written by
Learn more about how Netflix designs, builds, and operates our systems and engineering organizations
Learn about Netflix’s world class engineering efforts, company culture, product developments and more.
Written by
Learn more about how Netflix designs, builds, and operates our systems and engineering organizations
Learn about Netflix’s world class engineering efforts, company culture, product developments and more.
"
https://medium.com/swlh/everything-you-need-to-know-about-serverless-architecture-5cdc97e48c09?source=search_post---------388,"There are currently no responses for this story.
Be the first to respond.
Top highlight
Since the 1960s, cloud computing has allowed us to take advantage of its manageability and elasticity. It gave us the power to own a new server. It opened a whole new door to higher level platform services like API gateways, queues, authentication and more. Are we to embrace serverless next? Most think and talk of serverless in the same breath as FaaS (functions-as-a-service) products, understandably so, and since they were disappointed with those, they overlook serverless entirely. Well, they’ve missed out. Seeing as you are here, your view isn’t as narrow.
It’s two key features are centered around invisible infrastructure in place of configured VM images and billing based on invocation, rather than the usual hourly fee. Also, it isn’t as nebulous as you believe, most of the cloud is already serverless. The part which makes many programmers uncomfortable is the idea of their code being serverless. How are they supposed to debug and monitor the environment? What’s the plan for hardening the server? Is “serverless” merely a natural evolution of platform as a service (PaaS) and infrastructure as a service (IaaS), or a paradigm shift?
True. The hardware is very much still there, so when they say serverless, it’s a level-of-service abstraction, an illusion meant for the developer’s benefit. Serverless architecture shifts the responsibility for running the environment from the user to the cloud provider, and therein lies a compelling reason for you to embrace this assistance. Here, the responsibility for managing and provisioning the infrastructure lies entirely on the service provider. Hence, the developer can focus solely on writing code.
In a traditional approach, once the developer implements services or functions that communicate amongst themselves and the outside world, these services are combined into some distributed processes when installing, running, monitoring and updating. These processes run or operate on virtual machines. In the serverless approach, the service provider is responsible for everything from the processes to the operating systems to the servers. Meaning, you do not need to purchase a dedicated server anymore. At the same time, the service provider has the freedom to decide how to use their infrastructure efficiently to serve requests from all clients.
Serverless architecture is often considered the evolution of Platform-as-a-Service. In a serverless architecture, applications either depend on third-party services known as Backend as a Service or “BaaS” or on custom code run in ephemeral containers under Function as a Service or “FaaS.”
The rising number of third-party service providers implementing required functionality, providing server-side logic and managing their internal states has led to applications that don’t have application-specific, server-side logic and use third-party services for it all. In this vein, these applications are serverless.
AWS Lambda and Azure Functions happen to one of the first to offer serverless computing with broad usage. Both host bits of code which can be executed on demand. Here instead of writing applications, you write pieces of applications with event rules which trigger your code when needed. You don’t concern yourself about the server since your code executes. And you’re billed by fractions of a second, measuring memory and CPU usage, in the functions-land. You just scale invisibly, whether you’re getting one or a million invocations a day, it doesn’t matter. So yes, we are serverless today.
But FaaS comes with its issues. All workloads do not lend themselves to an event-based single-operation triggering model, and neither can all code be cleanly separated from its dependencies, and some dependencies can require quite intensive installations and configurations. And migrating an existing application to a functions-as-a-service model is usually complicated enough to be financially impossible. Moreover, enough tooling issues are making FaaS products painful to use for certain scenarios. FaaS ideally fits with microservices architecture since FaaS provides everything an organization requires for running microservices.
What differentiates PaaS from Serverless is that traditional PaaS providers like OpenShift and Heroku do not come with the automatic scaling feature. If you’re using PaaS, you have to specify beforehand how much resources your application will require. Well, it’s still possible to scale the application up, manually or down by changing the number of assigned resources, but the thing is, this is the developer’s responsibility. Secondly, a traditional PaaS is designed for long-running server applications. With such a design, the application is always running to serve incoming requests. If you bring down the application down, it will obviously not serve the requests. With FaaS, your function begins to serve a request and terminates it the moment it is processed. Ideally, when there is no incoming request, your functions should not be consuming the resources of the service provider.
The rise of Docker primarily marked the advent of the container technology. These containers set the standard for the next generation of the Virtual Machine. See the thing about containers is, it isn’t merely a replacement for VMs by a more lightweight abstraction. It’s also acting as a general-purpose packaging mechanism for applications. Simply put, containers are packaged mini-servers running on a container host, which is basically yet another server. The problem with containers is you have to get your hands dirty. One of the tenets of serverless is allowing the programmer to focus on the code, not the plumbing. Containers are a great tool, but with them, you have to deal with a technical payload of maintaining the contained environment. For containers to go truly serverless, they need to do something about the VMs waiting for work. As in they need to adopt invocation-based billing based on detailed consumption measuring. Nevertheless, there is an inherent similarity between serverless and containers.
Serverless architecture is a new method of writing and deploying application allowing you to focus on the heart of your application, the code. This approach reduces the time to market, operational costs and system complexity. While serverless leverages third-party vendors to eliminate the need to setup and configure physical servers or virtual machines, it also locks in your application and its architecture to the particular service provider. In the coming days, we expect more movement towards frameworks that will help us avoid vendor lock-in and enable us to run our serverless applications on different cloud providers or even on-premises. So many of the cons mentioned above will mostly be solved shortly.
It’s time to shrug off the responsibility onto mightier shoulders. Cloud computing has brought enormous change to the world of applications. In fact, much of the innovation in Information Technology over the past decade has been thanks to cloud computing. Some go as far as to say we’re on the verge of another cloud revolution, the move to serverless computing. It promises to change application paradigms and holds out a possibility of moving to a post-virtual machine, post-container world. Of course, it also goes without saying that serverless computing will break a lot of existing practices and processes.
Originally published on CognitiveClouds: Top Node js Development Company
Get smarter at building your thing. Join The Startup’s +750K followers.
343 
Get smarter at building your thing. Subscribe to receive The Startup's top 10 most read stories — delivered straight into your inbox, twice a month. Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
343 claps
343 
Written by
See every interaction with a customer — across all digital channels — & quickly determine how to delight your audience with personalization and recommendations.
Get smarter at building your thing. Follow to join The Startup’s +8 million monthly readers & +750K followers.
Written by
See every interaction with a customer — across all digital channels — & quickly determine how to delight your audience with personalization and recommendations.
Get smarter at building your thing. Follow to join The Startup’s +8 million monthly readers & +750K followers.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://itnext.io/kubernetes-multi-cloud-open-service-broker-api-service-catalog-b0a563077614?source=search_post---------389,NA
https://medium.com/@narinder/appirio-lessons-from-the-untold-stories-ef8482280f48?source=search_post---------390,"Sign in
There are currently no responses for this story.
Be the first to respond.
Narinder Singh
Jan 24, 2017·8 min read
In November of 2016, when Appirio announced its acquisition by Wipro, I had the normal range of rollercoaster of emotions you’d expect from a founder. I know all of the founders feel proud of what we accomplished and a large part of that is because of how we got there. We started as a scrappy underdog: we hustled, we irritated industry leaders, we hid behind and even slyly managed our friends at giants like Salesforce. After we grew by riding the tidal wave of cloud fever, we had to confront our own dose of harsh reality — the same tricks didn’t work, growth made us less scrappy, and we still didn’t have the market power of the industry leaders.
Never shy to voice an opinion, now that i’m completely unshackled I am using that freedom to reflect on some never told stories of Appirio, and broader lessons I learned from them. I still won’t go full on tabloid. I decided to save stories like “which cloud provider has a terrible technical platform and knows it”, “who has the most unethical sales team”, and “which industry founder was the biggest @$$#!^# defender of private SaaS until he got fired” for nights off the internet…
Nevertheless, there are many stories from the history of Appirio that have never been shared broadly. In them are lessons that could apply to many startups. In this post, I’ve shared ten of the most memorable lessons of Appirio. Hopefully they move beyond many of the normal platitudes you here about success after the fact. They are most relevant to those trying to navigate a trend much bigger than you are (in our case the cloud). In subsequent pieces, I’ll drill into each of the lessons and share some of the more provocative stories of Appirio and the cloud industry we grew up in.
Below are ten of the top, hopefully non-obvious, lessons we learned at Appirio. Follow on posts will stand alone and be linked from here and contain the stories beyond the story.
Before continuing, the one piece of advice you should absolutely listen to? Understand every lesson shared here (and everywhere) could be the exact wrong advice for your circumstance. Universal truths are few and far between and you’ve almost certainly heard them all already. Enough disclaimers, let’s get to it.
1. Taunt the bully — if you have a difference to flaunt — Appirio was known for our constant taunting of Accenture over the years. We were reprimanded by salesforce, cheered by its employees and even once contacted by Tiger Wood’s agent in response. No, we didn’t hate Accenture, we just loved the way the attention drew a contrast between what we were doing and everyone else. [Read the Full Story]
2. Be the pirate and the navy — Steve jobs celebrated Apple as the pirates to the industry’s navy. With two Naval Co-Founders, Appirio took a different tact. With strong discipline, we knew exactly where we stood and the landscape around our business. Even when our actions appeared spontaneous, erratic or even angry, they came from well established principles.We were able to “wing it” on strategy and decisions because we had meticulously planned. We made decisions over a handshake that our competitors took months to even contemplate. [Read the Full Story]
3. Learn from Sun Tzu and High School Musical — You’ll find the Art of War on the bookshelves of many a business leader. Entrepreneurs should supplement it with a dose of insight from High School Musical (or for those of us who remember the eighties: The Breakfast Club, Say Anything, or Can’t Buy Me Love). At Appirio we were too small to outspend our competitors; instead we spent selectively — on lavish parties that fortune 500 CIOs lobbied for invitations to. The power of FOMO isn’t just for high school. [Read the Full Story]
4. Founder’s tales: rediscover the lost art of keeping your mouth shut — With enough time conflict amongst founders is inevitable. Avoiding hard issues and allowing them to fester is nearly always the wrong decision. Gossiping about them freely is equally dangerous. Disputes, issues and disagreements should remain with only with those who are needed to resolve them. Sharing beyond that in the company creates a culture of rumors and an in and out crowd — forcing people in the company to choose sides. Discretion allows you and your co-founders / executives to address your challenges without turning the company into reality television. [Read the Full Story]
5. Growing up is hard to do: face it directly — Early stage startups can be intoxicating. You move fast, have the eyes of the industry on you and grow in leaps and bounds. Yet that growth adds people and eventually you need two things seldom associated with fun — process and management. Face it head on — because losing your culture through this phase is more dangerous than any competitor. [Read the Full Story]
6. Move beyond “kum ba yah” and acknowledge conflict between customers and partners — Your customers may come first, but if your business revolves around partners who are 800lb gorillas there is a conflict of interest. Your reputation with customers’ demands honesty that can’t be compromised. But that runs the risk of antagonizing gorillas. Navigating all of this with honesty requires partner relationships founded on interpersonal relationships between people, not the firms. [Read the Full Story]
7. Treat everyone like they matter, but invest in people like you’re handing them your money — When you first start your company it’s in your own interests to treat everyone at your partners and customers like they are the CEO. It’s important to put ego aside and retain that approach even when you no longer have to. Yet you still must prioritize. Play the long game and invest in relationships not by title but based on who you believe “gets it” and will drive change. If you can’t trust your own judgement on this –inside and outside of your company- you’re probably in the wrong role.
8. Make decisions based on your strategy and self-reflection of who you are — In 2010 Appirio sold our professional services management product (PSA) to Financial Force. They turned it into a success worth magnitudes more than what we gained from the sale. Ultimately, it may help make their company substantially larger than Appirio was. Yet it was still the right decision. We organized Appirio around trying to push the cloud overall. A single app wasn’t consistent with our strategy or culture. Our company had become a next generation services firm and even with our talent, we knew we wouldn’t do as good as job with the PSA app as Financial Force. For them it was a part of their strategy and aligned with their culture. Ultimately the combination must remain in harmony.
9. Success attracts a crowd; evolve or find a new home — Startups are all about convincing the world there is a new or better way to do something. We fight so hard to do that. It’s easy to miss considering how you will react once the world believes you. For Appirio we helped the industry see the vast potential of the cloud. Eventually all the Global SIs sounded a lot like us when they spoke of it. That became our defining problem. Sure, we were nimbler, could execute better and had happier customers — but ultimately we were now on the same playing field. That’s a dangerous place to be with competitors who are 200x your size. Ultimately we were not able to establish a new playing field fast enough — we needed a partner that could help us bulk up so we could win on the one we originally invented.
10. Recognize the privilege — Most startups fail. We all know that. Its why taking a risk to start a company can have such outsized rewards. Yet every day you grow as a company your individual impact is much less than the collective impact of those who decided to join you. They often work just as hard and care about the success of the company and their coworkers just as much as you do. Yet the gains from your collective work will still flow disproportionately to those that took the original risk. I don’t consider that unfair, but do believe it’s critical that entrepreneurs recognize this as a privilege. Too often founders and early executives start to believe their own hype — forgetting all the luck, recognizing but not fully appreciating the contributions of others, celebrating their best decisions without mourning their poor ones, and forgetting the day to day struggles of the people who work at “their company.” Even if living in self serving bubble works, is that who you want to be as a leader?
I’ll take the advice of my last lesson and say thank you to everyone who is a part of the Appirio story. My co-founders, our amazing partners, visionary customers and — most of all — the nearly three thousand people who worked at Appirio over the years. I’m most proud of what you created for one another. I know not every moment was perfect and I’m sure there are those who didn’t have an amazing experience. But I’ve seen first-hand how so many have worked so hard; cared about co-workers and the community; taken an opportunity and turned it into a new direction for their career — and helped those who came after do the same. That spirit of wanting to make things better for each other, our customers, the industry and those who aren’t as fortunate is something I will always remember and cherish. Thank you.
Being a part of Appirio was a life changing experience. I’m hoping my next chapter can take that good fortune and experience to help change medicine and healthcare with technology. Since I ignored that industry for the first twenty years of my career you’ll find me trying to catch up with an amazing group of people (many half my age!) at UCSF and Berkeley in the Masters of Translational Medicine program.
Exploring the union of healthcare & AI. Co-founder @appirio @sikh_coalition Former president @topcoder Married to my favorite economist @supkaur
51 
4
Some rights reserved

51 
51 
4
Exploring the union of healthcare & AI. Co-founder @appirio @sikh_coalition Former president @topcoder Married to my favorite economist @supkaur
"
https://itnext.io/packer-machine-image-as-a-code-3d02729d82ca?source=search_post---------391,NA
https://medium.com/why-2015-wont-suck/1-you-will-build-your-own-google-7db10e9cddd0?source=search_post---------392,"There are currently no responses for this story.
Be the first to respond.
By Paul Ford
This is pretty nerdy and buzzwordy—but in 2015 I’m most excited about… the cloud. I know. Even if you don’t know what “the cloud” is, you know it’s been around for years and refers to hundreds of thousands of computers that anyone can rent on the internet for any purpose.
But 2015 is going to be an awesome cloud year because everything is stupidly cheap. The downward pricing pressure is intense. You can have a lot—dozens, hundreds, thousands—of little computers doing your bidding; all you need to do is come up with something interesting for them to do. Not only are computers getting very cheap (mega-powerful Linux servers for $10/month, where a few years ago they cost $100), I’m also seeing companies like Amazon launch services that basically make computers invisible; they just run your code without making you set up a “computer” to run it; you never even think about what the computer is doing. So we’re getting ever closer to this point where a big chunk of the things we do on the internet are getting super-abstract and super-free.
And that means that people can do weird things and try all sorts of stuff. Start your own Google! Your own Facebook! Launch a thousand new magazines and shut them down the next day. Run weird experiments with human genetic data. Whatever, no big deal. It’s 2015, computer cycles are a commodity, like water or power. Think big.
Paul Ford is a writer for The Message.
39 reasons why 2015 will be less terrible than 2014, which…
40 
40 claps
40 
39 reasons why 2015 will be less terrible than 2014, which was garbage.
Written by
Co-founder, https://postlight.com, a digital product studio in NYC. Also writer, Medium advisor, programmer. Any port in a storm, especially ports 80 and 443.
39 reasons why 2015 will be less terrible than 2014, which was garbage.
"
https://medium.com/@neal-davis/are-aws-certifications-worth-it-in-2021-57883bb3ac69?source=search_post---------393,"Sign in
There are currently no responses for this story.
Be the first to respond.
Neal Davis
May 10, 2021·6 min read
The adoption of cloud computing accelerated over the last year as the COVID-19 pandemic caused massive disruption to businesses around the world. In uncertain times the ability to quickly innovate with agility and to align costs with demand has been a boon to many businesses. It’s therefore no surprise that cloud computing experts such as those with Amazon Web Services (AWS) certifications are in increasingly high demand.
So, if one of the overarching questions in your mind is, “are AWS certifications worth it in 2021?” the answer is a resounding yes! In this article, we are going to discuss AWS certifications in detail and why you should get at least one certification as soon as possible. Read on!
Before we dive into the benefits of getting an AWS certification before the end of 2021, it is important to be clear on what AWS certification is. AWS certifications can be attained by learning AWS through training courses or on-the-job experience and then taking an examination.
IT professionals get AWS certifications to showcase and validate technical skills and knowledge. AWS offers various certification exams for cloud engineers, architects, and administrators. There are hundreds of testing centres across the globe, but test takers can choose to take an AWS certification exam at home.
AWS provides six core certifications and five specialty certifications. The core certifications are grouped into four main paths based on job role — Foundational (Cloud Practitioner), Architect, Operations, and Developer and these are described in more detail below.
This is a certification path that is designed for people who want to build and validate their overall understanding of the AWS cloud. This path is beneficial for those in managerial, sales, technical, finance, and purchasing roles working with AWS cloud. There is a single exam in this path which is the AWS Certified Cloud Practitioner exam.
This AWS certification path is designed for solution design engineers, solutions architects, and everyone else who wishes to learn how to develop applications and systems on the AWS platform. This path has two sub-paths:
This path has a single certification though the DevOps Engineer Professional in the Developer path also sits across this path.
This path is designed for software developers who are interested in learning how to create cloud applications on AWS. It has two sub-paths that include:
There are also five specialty AWS certifications:
Now that you understand different types of AWS certifications, you are probably wondering whether it’s worth taking any of them in these challenging times. Below are some of the reasons why AWS certifications are super important in 2021:
One of the main reasons why AWS certifications are worth it in 2021 is because cloud computing is inevitable for IT experts. Revolutionary technologies such as Artificial Intelligence and Machine Learning are driving all industries towards the cloud. According to Forbes, a majority of IT professionals believe that these cutting-edge technologies play a significant role in the adoption of cloud computing.
Acquiring expertise in cloud computing, especially in AWS services, is a necessity for IT experts today and in the foreseeable future. Therefore, if you want to gain a competitive edge and stick out from the crowd, you cannot overlook AWS certifications in 2021.
Finding individuals with cloud computing skills has become difficult for employers across the globe. According to a recent report, about 60% of cloud computing job postings require AWS-related skills. These numbers indicate that skills and knowledge in AWS are in high demand and will continue to be in the future. Learning AWS is, therefore, one of the best ways of advancing your career and ensuring you get a top-dollar job in your dream company.
Another reason why you should kick-start your cloud computing journey with AWS is that AWS controls about a third of the IaaS (Infrastructure as a Service) Market. This is more than the next two providers (Google Cloud Platform and Microsoft Azure) combined. If this trend continues — which experts believe it will, we are likely to see exponential growth in AWS in the next few years. Therefore, getting an AWS certification in 2021 is a safe career choice for IT professionals and those who want to grow their business and skills on this highly-regarded platform.
SMEs, as well as well-established organizations across all industry verticals, are increasingly migrating to the cloud. Cisco projects that the global data centre IP traffic will hit 20.6 Zettabytes (ZB) by the end of this year. This translates to about 1.7 ZB per month! Additionally, it is expected that the worldwide public cloud services market will reach 17.5%.
The massive enterprise migration doesn’t come without its fair share of challenges. To solve them, there is a dire need for professional, customized solutions. This has led to increasing demand for cloud experts who can assist businesses ease their transition from conventional IT infrastructure to the cloud. In-depth expertise and know-how in cloud platforms, especially AWS, has become a valuable asset for organizations across the board, and this is one of the reasons why you should consider an AWS certification today.
Trust, credibility, and expertise are significant factors when it comes to offering value to potential clients and employers. These are the factors that help AWS practitioners stand out and build long-lasting relationships. A great way to establish credibility, trust, and expertise is to produce a certificate that shows you have skills in a particular area. By possessing AWS certifications, professionals tell their clients and potential employers that they have gone through rigorous training and have the capacity to successfully implement tasks given. With proof of expertise, there is no reason why you should not land a lucrative job in your dream organization. Don’t make a mistake of overlooking the importance of AWS certifications in 2021.
Individuals with AWS certifications rank high in the list of the highest-paid professionals. Many AWS jobs offer a six-figure salary. As you think of enhancing your skills in these challenging times, choose the AWS certification path you want to take, find great AWS Certification Training courses, study hard, and pass your exam and you too can become a cloud computing expert.
Check out our AWS Certification Courses.
Founder of Digital Cloud Training, IT instructor and Cloud Solutions Architect with 20+ year of IT industry experience. Passionate about empowering his students
54 
5
54 claps
54 
5
Founder of Digital Cloud Training, IT instructor and Cloud Solutions Architect with 20+ year of IT industry experience. Passionate about empowering his students
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@vechainofficial/vechain-attends-amazon-web-services-aws-global-beijing-summit-showcased-vechain-toolchain-and-985862655d52?source=search_post---------394,"Sign in
There are currently no responses for this story.
Be the first to respond.
VeChain Foundation
Jul 31, 2019·3 min read
The AWS Beijing Summit, a grand gathering for the Chinese cloud computing community hosted by AWS, was held on Wednesday. VeChain attended the Summit as a technology partner of AWS in the blockchain industry and a member of AWS Partner Network (APN). The Summit brought together over 200 APN Partners and over 150 guest speakers, such as the experts from IBM, Xiaomi, and Deloitte, to have in-depth discussions on the best practices of cutting edge cloud technology. More than 15,500 attendees from across China attended the Summit.
The cooperation between VeChain and AWS kick-started this year. In February 2019, VeChain announced the launch of an integrated solution into the AWS Cloud platform to enable one-click deployment of VeChainThor Blockchain Nodes upon VeChainThor Blockchain. Since then, VeChain has received strong support from AWS in terms of operations, technology development, and marketing. So far, there’re over 20 enterprise projects running on the ToolChain deployed on AWS, such as BMW, Latitude 28 and SBTG. In addition to that, the Partner Version of VeChain ToolChain is ready to be deployed on AWS. With the introduction, partners of VeChain are allowed to set up their own ToolChain rapidly, further accelerating the pace of VeChainThor Blockchain adoption.
Mr. Gu Jianliang, Chief Technology Officer at VeChain, attended the Summit as a keynote speaker. In his speech, Mr. Gu introduced VeChain ToolChain, a revolutionary one-stop Blockchain-as-a-Service solution. ToolChain integrates the strengths of NFC/RFID chips, IoT sensors, and mobile devices to lower the threshold for adopting blockchain technology. The solution comes with a variety of general-purpose interfaces, with which anyone, even those without any tech background, can build their own blockchain-based solutions for their businesses.
Thanks to the joint efforts made by VeChain and its partners, VeChain ToolChain-powered solutions, such as the Digital Carbon Ecosystem, Liquefied Natural Gas Solution and Consumer Confidence Index have been successfully integrated into various industries and sectors including FMCG, Food and Beverage, Wine, Luxury, Automobile, Healthcare, and more. Through the application of the above-mentioned solutions, the participating enterprises streamlined their business workflow and enjoyed greater transparency, enhanced security, and reinforced traceability. The adoption of blockchain technology created real value for the real economy.
The partnership between the two parties proved to be a win-win for both. As Mr. Gu said: “On the one hand, it allows enterprises to deploy quick, secure, and compliant blockchain applications, which is essential to promote the mass adoption of blockchain technology. Other than that, the blockchain technology provided by VeChain can be leveraged to create value for AWS’s clients. More importantly, the interaction and exchange between the VeChainThor ecosystem and that of AWS are giving rise to new business models and application scenarios, which brings opportunities for the two.”
About Amazon Web Services (AWS) Global Summits
AWS Global Summits are free events hosted by Amazon to bring the cloud computing community together to connect, collaborate, and learn about AWS. At the AWS Summits, you can learn how to choose the right database, modernize your data warehouse, and drive digital transformation using AI. Summits are held in major cities around the world and attract technologists from all industries and skill levels who want to discover how AWS can help them innovate quickly and deliver flexible, reliable solutions at scale.
The VeChain Foundation is a non - profit entity established in Singapore.
816 
1
816 
816 
1
The VeChain Foundation is a non - profit entity established in Singapore.
"
https://medium.com/@ariya114/mengenal-iaas-vs-paas-vs-saas-menghindari-salah-kaprah-teknologi-cloud-5358dd843312?source=search_post---------395,"Sign in
There are currently no responses for this story.
Be the first to respond.
Ariya Hidayat
Jan 13, 2020·7 min read
Karena dangkalnya pemahaman sesungguhnya soal teknologi komputasi di awan, atau yang lazim dikenal sebagai Cloud Computing, banyak pengamat dan pengguna teknologi yang malah terjerumus ke beragam kesalahpahaman tentang pemakaian Cloud. Mana yang lebih murah? Apa yang lebih aman? Harus pakai cloud yang mana? Dan rentetan kepeningan lainnya.
Hampir 99% kejadian, kalau ada tim/organisasi/perusahaan yang mengklaim telah “hijrah ke cloud”, yang dimaksud adalah sudah menggunakan Infrastructure-as-a-Service, disingkat IaaS. Namun demikian, masih banyak dan luas lagi ranah pemanfaatan cloud, termasuk juga Platform-as-a-Service (PaaS) dan Software-as-a-Service (SaaS). Bagaimana kesamaan dan perbedaan ketiganya?
Yang jelas, karena terhimpun dalam keluarga “cloud”, ketiganya punya karakteristik yang sama: yakni menjadi cara untuk menghantarkan sebuah layanan (service) melalui Internet publik (yang dikonotasikan dengan “cloud”). Contohnya, dulu tatkala kita mesti menulis laporan, meramu lembar kerja, atau menyusun slide untuk presentasi, andalannya adalah Microsoft Word, Excel, and PowerPoint (dipaket sebagai Microsoft Office) yang diinstal di komputer masing-masing. Akan tetapi, di zaman now, sebagian besar dari kita lebih memilih untuk menuangkan kreatifitas kita melalui Google Docs/Sheets/Slides yang semuanya notabene diakses secara online melalui layanan Google. Bisa dikatakan Google Docs ini adalah manifestasi dari paket Office ala Cloud. Tidak perlu lagi memburu dan memasang aplikasi secara lokal.
Dalam konteks IaaS/PaaS/SaaS, kesamaannya adalah tidak adanya infrastruktur fisik yang mesti dikelola oleh kita sendiri. Lawannya adalah “on-premises”, ditandai dengan aplikasi yang jalan di server fisik, biasanya diletakkan di ruang khusus (server room) atau pusat data (data center), dan disambungkan ke Internet. Di jaman sebelum ada teknologi cloud, inilah yang harus kita kerjakan: beli CPU dan disk, pasang sistem operasi, koneksikan ke Internet, dan konfigurasi aplikasinya.
Pun dalam keseharian, ada pekerjaan untuk memantau server tersebut, baik untuk memastikan bahwa perangkat keras sehat wal afiat (tidak kepanasan, kabel masih tertancap baik, dll) sampai sistem operasinya juga terus lancar (terus dimutakhirkan, tidak kekurangan swap, dll). Bila terjadi masalah, dari RAM yang jahil atau hard disk yang jebol, mesti diluangkan juga waktu dan tenaga untuk membenahinya.
Apakah bedanya IaaS dengan PaaS ataupun SaaS? Paling mudah kita lihat ilustrasi analoginya, Pizza as a Service, buah karya Albert Barron yang bekerja untuk IBM.
Dalam proses perjalanan sepotong pizza, dari awal hingga masuk ke lahapan kita, maka “on-premises” dapat digambarkan sebagai kegiatan yang dikerjakan sendiri, dan benar-benar dari awal. Kita mesti membuat adonan dan menambahkan saos tomat, keju, serta topping lain yang dikehendaki. Terus kita harus panggang sendiri juga (perlu oven dan api). Baru akhirnya menghidangkannya (lagi-lagi sendiri), barangkali juga bersama air limun segar sebagai pelengkapnya.
Sementara itu, IaaS, bisa dilukiskan sebagai modifikasi besar-besaran dari mekanisme on-premises, yaitu dengan pizza mentah yang sudah disiapkan oleh sang vendor (atau yang bisa dibeli di swalayan, seringnya masih beku). Memang potongan pizza tersebut belum matang. Adalah masih menjadi kewajiban kita untuk memanggangnya (masih perlu oven dan api/gas) serta menghidangkannya (meja, minuman pelengkap). Akan tetapi, bisa dilihat di sini bahwa kerja kita sendiri sudah sangat berkurang karena keribetan mengolah adonan pizza sudah bukan tanggung jawab kita.
Bagaimana bila pizza lezat ini dihantarkan dengan model PaaS? Dalam kasus ini, artinya pizzanya sudah jadi, alias tinggal dimakan. Entah caranya bagaimana, bisa nelpon pizzeria lalu minta diantar, atau ala PhD (Pizza Hut Delivery), ataupun dengan GO-FOOD, Grab Food, dan sebangsanya. Kerjaan kita minim sekali, hanya tinggal menghidangkan pizza tersebut sesuai selera dan langsung melahapnya (dan bersih-bersih, begitu sudah kelar). Tidak ada kerepotan bikin adonan dari nol. Tidak perlu punya oven, microwave, kompor, ataupun alat masak lainnya.
Akan halnya dengan SaaS, ini diibaratkan dengan makan di luar. Tidak perlu ada pengantaran. Kita cukup pilih kedai makan favorit, pesan dari menunya (misalnya pizza tersebut, bersama minuman pelengkap), lalu nikmati sajiannya. Sebagai bonus, kadang juga ada live music, baik dalam bentuk pengamen jalanan ataupun musisi serius, sebagai penghidup suasana. Selesai makan? Bayar tunai atau gesek kartu. Beres dah!
Di sisi lain, ada juga kombinasi lain yang juga ajib, seperti misalnya hybrid cloud atau malah multicloud (kita akan ulas di lain kesempatan).
Jawabannya: tergantung situasi. Tidak bisa dipukul rata.
Contohnya begini. Kalau Anda ingin makan nasi goreng (kearifan lokal: beralih dari contoh pizza di atas), murah mana antara bikin sendiri di dapur atau beli dari tukang nasgor. Dilihat dari harga bahan-bahannya, jelas lebih menghemat kala masak sendiri dong. Tapi terkadang, masak sendiri tidak memungkinkan. Barangkali Anda tinggal di kamar kost tanpa dapur. Atau juga, belum punya keahlian memasak nasi, apalagi meracik bumbu maupun menggorengnya hingga sedap. Atau malah, sedang buru-buru sehingga tidak bakal sempat untuk masak sendiri. Karena berbagai alasannya ini mungkin Anda lebih memilih pesan nasgor dari tukang nasgor di depan jalan atau lewat pengantaran GO-FOOD/Grab Food.
Apalagi untuk Anda yang sibuk sekali dan senantiasa dikejar waktu. Waktu Anda mungkin jauh lebih berharga untuk difokuskan ke pekerjaan, bercengkrama dengan keluarga, ataupun kegiatan yang lain. Walaupun perlu merogoh saku untuk membeli nasgor, dari perhitungan TCO (Total Cost of Ownership), masih lebih menguntungkan.
Makanya, kalau memikirkan soal harga murah atau nggak, tidak melulu hanya masalah harga beli. Ada juga ongkos waktu, pemeliharaan, dan komponen lainnya. Perhatikan TCO itu tadi.
Di sisi lain, walaupun masak sendiri itu menghabiskan ongkos lebih murah, belum kita dengar ada pasangan pengantin yang menyiapkan sendiri seribu porsi makan untuk tamu kondangannya. Selain sibuk dengan persiapan tetek bengek, tentu ada faktor scaling yang menyebabkan kemustahilan pasangan pengantin tersebut memasak untuk ribuan tamu kehormatan (gimana, kebayang capeknya nggak?). Di sini, lagi-lagi ada peran catering, sejalan dengan konsep SaaS, sehingga si mempelai (atau panitia yang didelegasikan) cukup membayar vendor untuk meramu selusin rupa-rupa hidangan, bahkan lengkap dengan alat makan, pelayanan, urusan kebersihan, dll. Tidak perlu pusing. Lebih mahal daripada masak sendiri? Sudah pasti. Mau mengerjakan sendiri? Jangan harap.
Orang yang benci on-premises tapi punya kendaraan sendiri itu berarti nggak konsisten (lho kenapa nggak selalu pakai ojol dan taksi, transportasi-as-a-Service?). Sama juga, mereka yang 100% bermusuhan dengan SaaS tapi masih suka beli kopi/Indomie/nasgor juga nggak taat doktrinnya sendiri (harusnya senantiasa masak sendiri dong!).
Jadi kalau ada pihak yang secara absolut sangat benci on-premises atau ada kubu lain yang 100% bermusuhan dengan cloud, jangan didengar. Pendapat seperti itu amat berbahaya dan menjerumuskan.
Jawabannya pun serupa: tergantung situasi. Tidak bisa dipukul rata.
Contohnya begini. Anda bernasib mujur dan memperoleh uang selembar 20 ribu. Bakal disimpan di dompet saja atau langsung disetor ke bank? Sebagian besar dari kita akan menyimpan lembaran tersebut di dompet saja. Ribet amat kalau hanya uang segitu terus harus repot ke bank segala. Nggak worth it, cetus anak millennial.
Lain halnya, kalau tiba-tiba ada yang menyetorkan uang kepada Anda, 100 juta rupiah, tunai. Wuih, mana berani disembunyikan di ransel lama-lama (apalagi di saku atau di dompet atau di bawah bantal). Pastinya lekas kita bergerak mencari cara untuk menjebloskan rejeki nomplok ini ke rekening bank. Gercep dong. Itupun sambil was was, siapa tahu ada penyamun di siang bolong manakala kita lagi beranjak ke kantor cabang bank tersebut.
Menyimpan data di sistem on-premises atau IaaS memberikan beberapa kelebihan, dari kontrol akses fisiknya hingga kesempatan kita menggunakan berbagai macam teknologi keamanan (kriptografi, penyandian data dari hulu ke hilir, dan lain sebagainya). Tetapi tidak selalu mutlak seperti itu, karena semua pekerjaan ini perlu ditangani ahlinya. Salah menggunakan algoritma kriptografi bisa berakibat fatal. Tidak melakukan analisis pemodelan ancaman (threat modeling) dapat menghasilkan ilusi keamanan. Ada mekanisme key rotation? Lantas, adakah review secara berkala untuk memastikan kalau semua personil yang terlibat tidak mudah ditipu oleh kampanye phishing atau social engineering lainnya? Dan selusin detil-detil lain yang mesti diperhatikan.
Sementara itu, kalau kita delegasikan urusan keamanan tersebut ke tim Google/Amazon/Microsoft karena kita menggunakan IaaS/Paas/SaaS yang mereka tawarkan, ada sejumlah hal yang berkaitan dengan kriptografi yang tidak lagi bisa kita kerjakan sendiri. Walaupun semua provider di atas menawarkan fitur untuk enkripsi (misalnya via KMS, Key Management System), tidak selalu semuanya bisa dimutlakkan. Di sisi lain, kita hendak mendapatkan manfaat dari keseriusan tim mereka untuk mengurusi keamanan fisik (triple biometrics), backup via redundancy, threat modeling, review berkala, dan beragam pekerjaan SecOps lainnya.
Ini namanya trade-off. Menimbun 100 juta rupiah di bank itu berarti kita nggak akan selalu bisa pakai kapan saja. Tetapi gila dong kalau kita pikir mengamankan 100 juta rupiah itu gampang. Bank dijaga beberapa satpam. Ada brankas lapis baja juga di dalamnya. Di rumah kita sendiri? Kadang ikan asin aja raib digondol kucing tetangga tanpa ketahuan!
Makanya, kalau berkaitan dengan isu keamanan dan ada pihak yang secara absolut sangat benci on-premises atau ada kubu lain yang 100% bermusuhan dengan cloud, juga jangan didengar. Sama-sama berbahaya dan menjerumuskan.
Mau naik sepeda atau naik mobil? Nah, tergantung ke mana dong. Tidak masuk akal kalau kita pilih moda transportasinya dulu sebelum jelas arah tujuannya. Menggenjot sepeda untuk pulkam dari Jakarta ke Semarang akan menghasilkan kepenatan tiada tara. Menyetir mobil untuk ke Indomaret depan gang juga buang-buang bensin/emosi/waktu.
Mau pakai on-premises, IaaS, PaaS, atau Saas? Cermati dari apa yang mau dibangun. Jabarkan kebutuhan dan calon-calon solusinya, lalu hitung TCO (jangan cuman harga beli), dan tinggal tentukan mana yang cocok. Tidak ada satu pilihan yang cocok untuk semuanya. Jauhi fanatisme dan jangan main pukul rata. Solusi yang manjur untuk startup kecil yang baru punya ribuan user akan sangat beda dengan apa yang bakal dipilih oleh Unicorn dengan jutaan pemakai.
Yang jelas, ketika membahas tajuk cloud, selalu gunakan kesempatan tersebut untuk menguraikan dengan rinci. Jangan hanya sebut cloud, tapi arahkan ke IaaS, atau PaaS, atau SaaS. Dengan lebih detil seperti itu, segala macam kesalahpahaman dan kerancuan dapat dihindari dan diskusinya bisa mengalir lebih cerdas.
Nah, siapa yang mau tinggal di Negeri di Awan?
Nggak bisa move on dari open-source.
283 
283 
283 
Nggak bisa move on dari open-source.
"
https://medium.com/hackernoon/real-world-applications-of-cryptocurrencies-food-traceability-3c46bc121b92?source=search_post---------396,"There are currently no responses for this story.
Be the first to respond.
As part of my series “Real World Applications of Cryptocurrencies”, and follow-up from my previous post on Cloud Computing & Golem, which you can find here, I will be discussing how the Food Traceability industry will be disrupted by the emergence of the blockchain and cryptocurrencies.
Coincidentally, after I had started writing this post, I attended a steak course, where the Chef was criticizing how hard it is to find out exactly where beef has come from, how it’s butchered, whether it’s organic etc. — a perfect example for the need of food traceability.
Food traceability is probably something you never thought of before but something extremely important for your health. Regardless, there are a number of issues in this sector and especially in emerging markets, which are also major exporters of food. Some of these issues include:
Enter TE-FOOD.
TE-FOOD is a successful farm-to-table livestock and fresh food traceability solution, focusing on emerging markets. Since 2016, TE-FOOD has been implemented in Vietnam. As a farm-to-table solution, TE-FOOD tracks the items through the whole supply chain (farm, slaughterhouse, wholesaler, retailer) and provides tools to both consumers and authorities to gain food history and food quality insights.
TE-FOOD is an existing company that has been established in Vietnam since 2016. It currently serves over 6000 business customers, tracking 12,000 pigs, 200,000 chickens, and 2.5 million eggs daily.
TE-FOOD has created a centralized ecosystem in which each step in the food supply chain can be tracked.
However, according to their whitepaper “even data coming from a third party like existing traceability system seems suspicious to the participant”. The solution to this, is introducing the blockchain. By moving the TE-FOOD ecosystem on the blockchain, TE-FOOD will be able to provide a completely transparent, unmodifiable environment (and tools) for food supply chain companies, authorities and consumers.
The diagram above is just a general representation of the food supply chain. Within the TE-FOOD ecosystem, this can be further customized for specific uses cases.
TE-FOOD focuses on solving the issues discussed in the previous section, in the following ways:
The TFOOD Token has a multitude of applications; it is used in the TE-FOOD ecosystem in the following ways:
I would also like to point out that TE-FOOD has a number of competitors in the supply chain area. Similar projects include Ambrosus (AMB), Modum (MOD), Provenance, WaBi (WABI) and Walton (WTC). If you would like to see a post comparing and contrasting these projects, feel free to drop me a comment below or a tweet.
TE-FOOD has not launched their token yet. You can only purchase TFOOD Tokens during their Initial Coin Offering (ICO) which is dated for the 22nd of February. You can sign-up for their ICO here.
Make sure you give the post a clap and my blog a follow if you enjoyed this post and want to see more.
You can also show your support by donating here:
BTC : 395JpxqaQLVYP2cP4uVMDBPPArdtdKBfZkBCH : 181FSPLrFWVK3Tpfmev678pLrUa2KPeoFhLTC : LgJw5vJo2ExXFTQaWuLJVbRtqDiscXNG7UETH: 0x4c7195E074cf0Ab6F77Bdb7C97Fd2567066Bb712NEO/GAS : Af1igVZ5GP6VDBE1MWdM9ovSeVq7wCs3zAIOTA : QNRFWZROPTRTZRGOYGAPXCKOFMNANIZIMYJASSDEMUIGZXUSB9EYDAJM9EFDGZZDOGOBQPTGRCLQIXPAI
Disclaimer : All information and data on this blog post is for informational purposes only . I make no representations as to the accuracy, completeness, suitability, or validity, of any information. I will not be liable for any errors, omissions, or any losses, or damages arising from its display or use. All information is provided as is with no warranties, and confers no rights.
#BlackLivesMatter
466 
2
how hackers start their afternoons. the real shit is on hackernoon.com. Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
466 claps
466 
2
Written by
Cryptocurrency Enthusiast expert circa 2012. Visit my website at https://ermos.io or reach out via e-mail on contact@ermos.io
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
Cryptocurrency Enthusiast expert circa 2012. Visit my website at https://ermos.io or reach out via e-mail on contact@ermos.io
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/machines-school/data-science-on-the-google-cloud-platform-%E0%B8%97%E0%B8%B3%E0%B8%84%E0%B8%A7%E0%B8%B2%E0%B8%A1%E0%B8%A3%E0%B8%B9%E0%B9%89%E0%B8%88%E0%B8%B1%E0%B8%81%E0%B8%81%E0%B8%B1%E0%B8%9A-google-cloud-platform-gcp-7605b4560fb8?source=search_post---------397,"There are currently no responses for this story.
Be the first to respond.
“Cloud Computing” หรือแปลเป็นภาษาไทยว่า “การประมวลผลแบบกลุ่มเมฆ” เป็นลักษณะของการทำงานของผู้ใช้งานคอมพิวเตอร์ผ่านอินเทอร์เน็ต ที่ให้บริการใดบริการหนึ่งกับผู้ใช้ โดยผู้ให้บริการจะแบ่งปันทรัพยากรให้กับผู้ต้องการใช้งานนั้น
โดยสถาบันมาตรฐานและเทคโนโลยีแห่งชาติของสหรัฐอเมริกา (NIST) ได้ให้จำกัดความไว้ว่า “เป็นรูปแบบที่ให้บริการเพื่อใช้ทรัพยากรคอมพิวเตอร์ร่วมกับผู้อื่นผ่านทางเครือข่ายเน็ตเวิร์ค เช่น โครงข่ายเน็ตเวิร์ค เซิร์ฟเวอร์ หน่วยเก็บสำรองข้อมูล แอพพลิเคชั่นและเซอร์วิสต่างๆ เป็นต้น โดยขึ้นอยู่กับความต้องการของผู้ใช้ ”
Google Cloud Platform หรือเรียกย่อๆว่า GCP เป็นกลุ่มทรัพยากรในการประมวลผล (computing resource) ของ Google ที่เปิดให้ทุกคนสามารถเข้าใช้งานได้ในรูปแบบของ Public Cloud
Google Cloud Platform สามารถใช้งานได้ทั่วโลก โดยเราสามารถเลือกใช้งานได้ตาม “regions” และ “zones” ทั้งนี้ขึ้นอยู่กับ latency, availability, durability
regions จะขึ้นอยู่กับลักษณะทางภูมิศาสตร์ เช่น US East, US West, US Central, Europe, East Asia, Southeast Asia, South Asia หรือ Australia เป็นต้น และในแต่ละ regions จะประกอบด้วย zone เช่นใน regions ของ Southeast Asia ที่ตั้งอยู่ในสิงค์โปร์ก็จะมี zone A และ zone B เป็นต้น
Google Cloud Platform จะแบ่งตามลักษณะการใช้งานดังนี้ (ข้อมูลเมื่อวันที่ 7 มิถุนายน 2561) แต่บาง region ก็ไม่สามารถใช้งานได้ในบาง serivice
*หมายเหตุ: ข้อมูลเมื่อวันที่ 7 มิถุนายน 2561
บทความเรื่อง “Data Science on the Google Cloud Platform”ตอนที่ 1: ทำความรู้จักกับ Google Cloud Platform (GCP)ตอนที่ 2: วัฎจักรข้อมูล (Data Lifecycle) บน Google Cloud Platform
Machines School
106 
106 claps
106 
Machines School
Written by
Founder of Humaan.ai—The AI as a tools for unleash human capabilities. 🧠 🚀
Machines School
"
