story_url,bodyText
https://medium.com/@amirhermelin/im-leaving-google-and-here-s-the-real-deal-behind-google-cloud-1b86513be01b?source=search_post---------0,"Sign in
There are currently no responses for this story.
Be the first to respond.
Top highlight
Amir Hermelin
Oct 5, 2018·8 min read
Note: this is my first post on Medium — be gentle :). I’m staring at my badge that I’ll be turning in tomorrow, and decided I must share my thoughts before my next gig consumes 100% of my time. These thoughts are my own personal opinions, and do not reflect or represent Google’s opinions or plans.
Let’s start from the end: after almost 6.5 years, I’m leaving what’s IMO the best company in the world to work for. This was my longest stint at any one company. I’m leaving to pursue a high-risk high-reward opportunity with a company that’s disrupting personal finance.
I joined Google Cloud before it was even a platform, and I’m one of the PMs that has spent the longest tenure in Cloud. I’d like to share what I’ve seen over the years and what I expect to see in years to come.
But first, I’d like to tell you why leaving Google is so difficult.
Many people mention great perks and benefits, like free food and four month maternity leaves (unfortunately my 3 boys were born before I’ve joined Google — I only had a couple of days to bond with them!). Not to mention great salaries and a stock that’s rock solid. Young professionals might place a larger weight on these, but they get so much more “free” stuff that’s going to greatly impact their development and future careers. I wish on all my boys to start their careers at Google — because Google provides accelerated learning and development into the most important fields of tech.
1.Heaven for engineers and product managers: the engineering and PM levels at Google are very high. Don’t get me wrong — not everyone is a rockstar. But on average, you can rely on your colleagues to tackle tough problems together, and get hard sh*t done. This is because:
2. The best culture money can’t buy: the first weeks after I joined two things struck me:
In addition, the culture rewards excellence and innovation, encourages saying “thank you” in public, and promotes great ideas and efforts.
There’s a feeling that people are generally “good”, which why I was completely blindsided (like many others) by the infamous memo of last year. Regardless of where you stand in the conversation, Google treats its employees with unbelievable fairness, especially when compared to other tech firms. Trust me, I’ve been around the block.
3. Innovation and scale all wrapped into one: I crack up whenever I read that some people think Google isn’t innovative anymore. Firstly, the most important field in tech today — Machine Learning — is led by Google, which is several years ahead of its nearest competition. I don’t want to put other companies down — they’re also very innovative. But in the field of AI/ML, nobody comes close. Not in technology and not in the raw numbers of quality engineering talent. Now think of all the things Google applies ML to, like self driving cars, assistant, search, etc. If that’s not innovation — what is?
To find scale, all I need to do is look at the main apps I use throughout my day: Maps, Photos, Chrome, YouTube, Gmail, Search. AFAIK all of these are irreplaceable. Yes, there are alternatives* — but for me, switching means negative impact on my day-to-day.
*I don’t really think there’s an alternative to YouTube. It’s unique. I’ve also spent a short stint working on YouTube data infrastructure, and I can say that the org culture/vibe and the people are pretty amazing.
So no room for improvement? All is perfect in Google-Land? Of course not.
In some areas across Google the execution could’ve been better. Google is willing to bet big on success, and also assumes the risk of big failures. Messaging efforts of past didn’t take off like they should’ve. Google+ was a huge effort that didn’t succeed (but did give birth to the amazing Photos app). There are other examples. But the organization is a learning one, and I’m hopeful that future efforts will integrate these lessons learned.
Another hurdle is due to the (good) fact that there’s a lot of openness to ideas and opinions. Sometimes discussions and debates feel like they’re taking too long, and decisions need to be made sooner. Even if consensus wasn’t achieved or not all opinions were heard. Once we’re done understanding the data, and we’re only left with opinions — time to march on.
Lastly, promotion and perf are areas that finding the right balance is an ongoing process. Every cycle, complaints surface around promo fairness and perf overhead. There’s an entire team working on improving the process, but I feel there’s still work to do. To be fair, it’s tough to find balance in an 80k+ sized company. Google is no longer a startup.
Which brings me to Cloud.
Google Cloud — No Longer a Startup
My first PM task at Google was to launch Monarch — Google’s planet scale monitoring service for Google’s apps and services (Maps, Gmail, etc). Talk about the opposite of “easy early wins”! But with the help of others (see above “Heaven for..”) I managed to successfully launch it, and found my way into the cloud org circa early 2013.
Only back then, it really felt like a startup. We were pressed to find product market fit amidst fierce competitors that had years of head start (AWS) and armies of sales and marketing teams (Azure). And users still had questions of whether or not we were here to stay.
Not surprisingly we’ve found success with customers that were similar to Google. When I first engaged with Snapchat I believe they were less than 10 people, but the scale and automation they were looking for were not unlike what we knew in other parts of Google.
So we’ve made some mistakes. Two meaningful ones to be precise.
Our first — taking too long to recognize the potential of the enterprise. We were led by very smart engineering managers — that held tenures of 10+ years at Google, so that’s what grew their careers and that’s what they were familiar with. Seeing success with Snapchat and the likes, and lacking enough familiarity with the enterprise space, it was easy to focus away from “Large Orgs”. This included insufficient investments in marketing, sales, support, and solutions engineering, resulting in the aforementioned being inferior compared to the competitors’.
The second mistake was chasing the competition. For example, AWS was having great success with EC2 (VMs). And customers were asking for it on GCP. So our native internal way of running things — containers — was to take a backseat for a few years, until a small startup by the name of Docker managed to hype up containers enough to make them relevant. Google took notice, and the rest is history. Another example is App Engine — predating today’s “serverless” hotness by a few years, and arguably a successful business even back then. Neither AWS or Azure had anything like it, but we had to divert too many resources to satisfy customers that were asking for features similar to what our competitors offered at the time.
But all that is in the past. Over the last ~3 years, things have changed substantially. With the current leadership in place (grounded with the right experience), strong focus on enterprise, and marketing/sales/support teams that are correctly leveled and staffed, we’re left with the meat and potatoes: the products.
My friends ask me if I think Google Cloud will catch up to its rivals. Not only do I think so — I’m positive five years down the road it will surpass them. Because today, Cloud is about helping other companies build software like Google does. All those great things about working at Google? Making them available to other companies — that’s the product market fit.
Take Kubernetes as an example. It broke many adoption records and is very successful, and not because of any new concepts that were introduced. Kubernetes is really an externalization of container orchestration that’s been battle hardened inside of Google for many years (called Borg), and built to scale the largest, most distributed web services. And of the three cloud vendors, Google is best positioned to leverage these technologies and innovate first with (much) better offerings.
Security is another area. If I asked you which company had the fewest breaches, or where you feel your information is safest — what would your answer be? And so, it’s no wonder that Google Cloud leads and will continue to lead in this area. Externalizing security features and practices to our customers will prove of significant value — sometimes too valuable to pass for any other reason.
Machine Learning is the third strong pillar. Helping companies utilize advanced ML technologies the way Google does can give businesses an unfair advantage. And so, pretty soon every industry will have to deploy and use it, one way or another.
There are other things, like externalizing what we know about running production at scale (think monitoring, logging and SRE practices), CI/CD, externalizing previously internal services such as Spanner, BigQuery, and BigTable. The list goes on, and the not-yet-realized opportunity is huge.
Fast forward 5 years. What company wouldn’t want to build their service <X> with the same agility, scale, and security that Google does? Scale it to billions of users with minimal disruption and rock solid stability and reliability?
It’s almost midnight, and the post reads more like a love story than someone who’s leaving the company :) I’ll end here and not venture into my next chapter yet — I’ll leave that for my next post. For now I’ll just say that, starting next week my commute is about to get 3x longer ;)
Update: I’ve finally got around to writing about what’s exciting about the next chapter, here: https://medium.com/@amirh1/and-heres-why-i-m-joining-sofi-3bb71ded10c
📝 Read this story later in Journal.
👩‍💻 Wake up every Sunday morning to the week’s most noteworthy stories in Tech waiting in your inbox. Read the Noteworthy in Tech newsletter.
Father, husband, product person, mountain biker, ex-Google
See all (6)
23K 
112
23K claps
23K 
112
Father, husband, product person, mountain biker, ex-Google
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@serverpunch/why-you-should-not-use-google-cloud-75ea2aec00de?source=search_post---------1,"Sign in
There are currently no responses for this story.
Be the first to respond.
Top highlight
Punch a Server
Jun 29, 2018·4 min read
Update (18-July-2018): GCP has updated their account management policies to be more friendlier to GCP customers. https://cloudplatform.googleblog.com/2018/07/improving-our-account-management-policies-to-better-support-customers.html
UPDATE (2-July-2018): Thanks to the people from GCP support team who have reached out and assured us these incidents will not repeat. Here’s a direct message from them … “there is a large group of folk (within GCP) interested in making things better, not just for you but for all GCP customers.”
Follow discussions here. HACKERNEWS: https://news.ycombinator.com/item?id=17431609REDDIT: https://www.reddit.com/r/programming/comments/8v4wrh/why_you_should_not_use_google_cloud_this_is_about/
Note: This post is not about the quality of Google Cloud products. They are excellent, on par with AWS. This is about the “no-warnings-given, abrupt way” they pull the plug on your entire systems if they (or the machines) believe something is wrong. This is the second time this has happened to us.
We have a project running in production on Google Cloud (GCP) that is used to monitor hundreds of wind turbines and scores of solar plants scattered across 8 countries. We have control centers with wall-to-wall screens with dashboards full of metrics that are monitored 24/7. Asset Managers use this system to monitor the health of individual wind turbines and solar strings in real time and take immediate corrective maintenance. Development and Forecasting teams use the system to run algorithms on data in BigQuery. All these actions translate directly to revenue. We deal in a ‘wind/solar energy’ — a perishable commodity. If we over produce, we cannot store and sell later. If we under produce, there are penalties to be paid. For this reason assets need to be monitored 24/7 to keep up/down with the needs of the power grid and the power purchase agreements made.
Early today morning (28 June 2018) i receive an alert from Uptime Robot telling me my entire site is down. I receive a barrage of emails from Google saying there is some ‘potential suspicious activity’ and all my systems have been turned off. EVERYTHING IS OFF. THE MACHINE HAS PULLED THE PLUG WITH NO WARNING. The site is down, app engine, databases are unreachable, multiple Firebases say i’ve been downgraded and therefore exceeded limits.
Customer service chat is off. There’s no phone to call. I have an email asking me to fill in a form and upload a picture of the credit card and a government issued photo id of the card holder. Great, let’s wake up the CFO who happens to be the card holder.
“We will delete your project unless the billing owner corrects the violation by filling out the Account Verification Form within three business days. This form verifies your identity and ownership of the payment instrument. Failure to provide the requested documents may result in permanent account closure.”
What if the card holder is on leave and is unreachable for three days? We would have lost everything — years of work — millions of dollars in lost revenue.
I fill in the form with the details and thankfully within 20 minutes all the services started coming alive. The first time this happened, we were down for a few hours. In all we lost everything for about an hour. An automated email arrives apologizing for ‘inconvenience’ caused. Unfortunately The Machine has no understanding of the ‘quantum of inconvenience’ caused.
I understand Google’s need to monitor and prevent suspicious activity. But how you handle things after some suspicious activity is detected matters a lot. You need a human element here — one that cannot be replaced by any amount of code/AI. You just can’t turn things off and then ask for an explanation. Do it the other way round.
This is the first project we built entirely on the Google Cloud. All our previous works were built on AWS. In our experience AWS handles billing issues in a much more humane way. They warn you about suspicious activity and give you time to explain and sort things out. They don’t kick you down the stairs.
I hope GCP team is listening and changes things for better. Until then i’m never building any project on GCP.
12.2K 
49
12.2K claps
12.2K 
49
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@yesdeepakverma/how-i-cleared-all-3-google-cloud-certifications-in-3-weeks-f5591aa22572?source=search_post---------2,"Sign in
There are currently no responses for this story.
Be the first to respond.
Deepak Verma
Dec 11, 2018·6 min read
Yes, you read it right!
I have got my 4th Certificate Google Cloud Developer and 5th Certificate Google Cloud Network Engineer each in 1 week again.
I was able to clear all 3 Google Cloud certifications in 3 weeks. In this post, I will be sharing the resources and my approach to help you prepare for GCP certifications.
A bit about me:
I am a Python+Django developer with hands-on experience of Python-based scripting and Google Cloud Platform working with MediaAgility. I have worked on designing and developing projects to production scale from scratch. At MediaAgility, I am in the Machine Learning team. Since my organization is a Google Cloud Premier Partner, I have also worked on GCP migrations — setting production grade networking, Kubernetes, and GCP infra automation (IAM, networking, Kubernetes, Dataproc, VM, and more).
The prerequisites…
Google Cloud certifications need a right balance of both theory and practical skills. For the theory, Google Cloud documentation is a great resource and to increase your practical skills, you will have to practice frequently and extensively. You can use free $300 credit from GCP and try different services or join Coursera Google Cloud Architect or Data Engineer course and watch all the videos and complete all the Labs.
I started with Coursera’s Architecting with Google Cloud Certification. This exam checks your ability to provide a solution on the Google Cloud. It is almost 80% conceptual and 20% practical. You must know beforehand -
I watched all the videos in just 2–3 days and skipped the concepts that I knew already owing to my experience on Google Cloud. Sharing the important topics below -
This certification tests you for the ability to deploy a solution on Google Cloud. You must have a strong practical experience with Google Cloud — working knowledge of GCloud SDK and Google cloud console.
More details about the exam can be found here.
You must be able to perform the below actions -
This certification tests your ability to design big data solutions on GCP. This exam expects that you are familiar with the big data products (storage, processing, display) and their open source alternatives as well. This is required because some of the questions expect you to answer GCP alternatives for open source big data products.
BigQuery is the product that you must understand clearly. If you understand BigQuery, you can answer 40% of this exam.
Sharing an overview of other important topics and a few more resources:
My Schedule -
I studied from 10 p.m. to 2 a.m during those 3 weeks of my preparation. This needed me to take care of my health so that I remain focused. So, I would suggest you maintain a healthy diet and not let the stress get the better of you!
If you thoroughly understand the ‘How-to’ and ‘Concepts‘ sections of Google Cloud documentation, you easily have 70% of what it takes to clear GCP certification; remaining 30% is your practice, experience, and your state-of-mind during exams. So, take the exam with a relaxed state of mind.
All The Best!
Here are the links to my certificates.
Professional Data Engineer, Associate Cloud Engineer, Professional Cloud Architect
P.S. Don’t hesitate to click on clap button as many time as you can. :-)
Data Analytics | Kubernetes | Cloud Architect | Data Architect | Python
7K 
24
7K 
7K 
24
Data Analytics | Kubernetes | Cloud Architect | Data Architect | Python
"
https://medium.com/@steve-yegge/dear-google-cloud-your-deprecation-policy-is-killing-you-ee7525dc05dc?source=search_post---------3,"Sign in
There are currently no responses for this story.
Be the first to respond.
Steve Yegge
Aug 15, 2020·23 min read
God dammit, I didn’t want to blog again. I have so much stuff to do. Blogging takes time and energy and creativity that I could be putting to good use: my novels, my music, my game, and so on. But you get me riled enough, and I have to blog.
Let’s get this over with, then.
I’ll begin with a small but enlightening story from my early days at Google. For the record, I know I’ve said some perhaps unkind things about Google lately, because it’s frustrating when your corporate alma mater makes incompetent business decisions on the regular. But Google’s internal infrastructure is truly extraordinary, and you could argue that there is still none better today. The people who built Google were far better engineers than I will ever be, as this anecdote should serve to illustrate.
First a wee bit of background: Google has a storage technology called Bigtable. Bigtable was a remarkable technical achievement, being one of the first (if not the first) “infinitely scalable” key-value stores: the beginning of NoSQL, basically. These days Bigtable still holds up well in the rather crowded space of K/V stores, but back in the day (2005) it was breathtakingly cool.
One fun bit of trivia about Bigtable is that they had these internal control-plane entities (as part of the implementation) called tablet servers, which had large indexes, and at some point they became a scaling bottleneck. So the Bigtable engineers scratched their heads over how to make it scale, and realized that they could replace the tablet servers with Bigtables to unblock the scaling. So Bigtable is part of its own implementation. It’s Bigtables all the way down.
Another cool bit of trivia is that for a time, Bigtables became popular and ubiquitous inside Google and everyone and their dog had one. So at one Friday’s TGIF all-hands, Larry Page casually asked in passing, “Why do we have more than one Bigtable? Why isn’t there just one?” Because in theory, one Bigtable should have sufficed for all Google’s storage needs. Of course they never did migrate to just one, for practical software engineering reasons (e.g. blast radius), but the theory was interesting. One database for the whole universe. (Side note: Anyone know if Sable does this at Amazon?)
Anyway, here’s my story, to get us started on my rant.
One day, after I’d been working at Google for just over 2 years, I got an email from the Bigtable engineering team. It said something along the lines of:
Dear Steve,
Greetings from the Bigtable team. We wanted to let you know that you are running a very, very old Bigtable binary in the [some data center name] data center. That version is no longer supported, and we would like to work with you to help you upgrade to the latest version.
Please let us know if you can schedule some time to work with us on this.
Best,
Bigtable Team
You get a lot of email at Google, as you can imagine, and when I glanced at this one, this is what I first perceived it to be saying:
Dear RECIPIENT,
Greetings from Some Team. We wanted to let you know blah blah blah blah blah blah blah. Blah blah blah blah blah blah blah BLAH, and blah blah blah blah immediately.
Please let us know if you can schedule some of your precious time to blah blah blah.
Best,
Some Team
I almost deleted it on the spot, but there was this lingering, nagging feeling that it didn’t quite feel like a form letter, even though it obviously wasn’t for me, since I didn’t have a Bigtable.
But it was weird.
For the rest of the day, as I was alternating between working and deciding which species of gummy shark to try next in the micro-kitchens, of which there were at least three close enough to hit from my seat with a well-aimed biscuit, I thought about that email with a growing sense of mild anxiety.
They had used my name specifically. And the email had been sent to my email address and nobody else’s, and not by cc: or bcc:. The tone was very personal and pointed. Maybe it was some sort of mistake?
Curiosity finally got the better of me, and I went to look at my Borg console in the data center they’d mentioned in the email.
And sure enough, I was running a Bigtable there. Whaaaaat? I looked at its contents, and lo! It was from the Bigtable codelab I’d run back in my first week as a Noogler, in June 2005. The codelab had you fire up a Bigtable so you could programmatically write some values to it, and I had apparently never shut it down afterwards. It was still running there, over 2 years later.
There are several remarkable aspects to this story. One is that running a Bigtable was so inconsequential to Google’s scale that it took 2 years before anyone even noticed it, and even then, only because the version was old. As a point of comparison, I considered using Google Cloud Bigtable for my online game, but it cost (at the time) an estimated $16,000/year for an empty Bigtable on GCP. I’m not saying they’re gouging you, but in my own personal opinion, that feels like a lot of money for an empty fucking database.
Another remarkable aspect is that it was still running after 2 years. WTF? Data centers come and go; they experience outages, they undergo routine maintenance, they change all the time. Hardware gets upgraded, network switches swap out, everything is constantly being improved. How the heck were they able to keep my software running for 2 years in the face of all that change? It may feel like a humdrum achievement in 2020, but in 2005–2007 it was pretty impressive.
And then there is in my opinion the most remarkable aspect of all, which is that an unrelated engineering team in some other state was reaching out to me, the owner of some tiny mostly-empty Bigtable instance which had had zero traffic for the past 2 years, asking if they could help me upgrade.
I thanked them and shut it down and life went on. But I still think about that letter, thirteen years later. Because I sometimes get similar letters from the Google Cloud Platform. They look like this:
Dear Google Cloud Platform User,
We are writing to remind you that we are sunsetting [Important Service you are using] as of August 2020, after which you will not be able to perform any updates or upgrades on your instances. We encourage you to upgrade to the latest version, which is in Beta, has no documentation, no migration path, and which we have kindly deprecated in advance for you.
We are committed to ensuring that all developers of Google Cloud Platform are minimally disrupted by this change.
Besties Forever,
Google Cloud Platform
But I barely skim them, because what they are really saying is:
Dear RECIPIENT,
Fuck yooooouuuuuuuu. Fuck you, fuck you, Fuck You. Drop whatever you are doing because it’s not important. What is important is OUR time. It’s costing us time and money to support our shit, and we’re tired of it, so we’re not going to support it anymore. So drop your fucking plans and go start digging through our shitty documentation, begging for scraps on forums, and oh by the way, our new shit is COMPLETELY different from the old shit, because well, we fucked that design up pretty bad, heh, but hey, that’s YOUR problem, not our problem.
We remain committed as always to ensuring everything you write will be unusable within 1 year.
Please go fuck yourself,
Google Cloud Platform
And the thing is, I get these about once a month. It happens so often and so reliably that I have been inexorably pushed away from GCP, towards cloud agnosticism. I no longer take dependencies on their proprietary service offerings, because it actually winds up being less DevOps work, on average, to support open-source systems running on bare VMs, than to try to keep up with Google’s deprecation treadmill.
Before I return to shitting on Google Cloud Platform, because boyo I am nowhere near finished yet, let’s go visit how software engineering works in some other domains. Google engineers pride themselves on their software engineering discipline, and that’s actually what gets them into trouble. Pride is a trap for the unwary, and it has ensnared many a Google team into thinking that their decisions are always right, and that correctness (by some vague fuzzy definition) is more important than customer focus.
I’m going to pick a few somewhat arbitrary examples from other big software systems, but hopefully you’ll be able to start spotting the pattern everywhere; that pattern being: Backwards compatibility keeps systems alive and relevant for decades.
Backwards compatibility is a design goal of all successful systems that are designed for open use; that is, implemented as open source, and/or guided by open standards. I feel like I’m stating something that’s so obvious that we should all be awkwardly embarrassed, but no. It’s a political issue, so I need examples.
The first system I’ll pick is the oldest: GNU Emacs, which is a sort of hybrid between Windows Notepad, a monolithic-kernel operating system, and the International Space Station. It’s a bit tricky to explain, but in a nutshell, Emacs is a platform written in 1976 (yes, almost half a century ago) for writing software to make you more productive, masquerading as a text editor.
I use Emacs every single day. Yes, I’m also using IntelliJ every day, and that has grown into a powerful tooling platform in its own right. But writing software extensions for IntelliJ is a much more ambitious and complex undertaking than writing extensions for Emacs. And more importantly, stuff lasts forever on Emacs.
I’m still using software that I wrote for Emacs back in 1995. And I’m sure there are people who are still using software they wrote for Emacs from the mid-80s, if not earlier. Every once in a while it might require a minor tweak, but it’s really quite rare. I’m not aware of anything I’ve ever written for Emacs (and I’ve written a lot) that was ever forced into re-architecture.
Emacs does have a deprecation facility called make-obsolete. Emacs’ terminology for fundamental software engineering concepts (like what is a “window”) are often different from the industry conventions, because Emacs invented them long, long ago. The perils of being before your time: Your names are all wrong. But Emacs does indeed have deprecation, called obsolescence in their lingo.
However, the Emacs folks seem to have a different working definition. A different underlying philosophy, if you will.
In the Emacs world (and in many other domains, some of which we’ll explore below), when they make an API obsolete, they are basically saying: “You really shouldn’t use this approach, because even though it works, it suffers from various deficiencies which we enumerate here. But in the end it’s your call.”
Whereas in the Google world, deprecation means: “We are breaking our commitments to you.” It really does. That’s what it ultimately means. It means they are going to force you to do some work, possibly a large amount of rework, on a regular basis, as punishment for doing what they told you to do originally — as punishment for listening to their glossy marketing on their website: Better software. Faster! You do everything they tell you to do, and you launch your application or service, and then, bang, a year or two later it breaks down.
This is like selling you a used car that they know is going to break down in under 1000 miles.
These are two very, very different philosophical definitions of “deprecation”. Google’s definition reeks of planned obsolescence. I don’t believe that it’s actually planned obsolescence in the same sense that, say, Apple perpetrates. But Google definitely plans to break your stuff, in a roundabout way. I know because I worked there as a software engineer for 12+ years. They have loose internal guidelines about how much backwards compatibility to bake into service offerings, but in the end it’s up to each individual team or service. There are no corporate-level or engineering-level guidelines, and the longest anyone has the courage to recommend, in terms of deprecation cycles, is “try to give your customers 6–12 months to upgrade before you drape them over the barrel.”
This is hurting them far more than they realize, and it will continue to hurt them for years to come, because it’s not part of their DNA to care about customers. More on this below.
For the moment, I’m going to make the bold assertion that Emacs is successful in large part, perhaps even mostly, because they take backwards compatibility so seriously. In fact, that’s the thesis of this essay. Successful long-lived open systems owe their success to building decades-long micro-communities around extensions/plugins, also known as a marketplace. I’ve ranted about Platforms before, and how important they are, and how Google has never once in their entire corporate history ever really understood what goes into making a successful open Platform, not counting Android or Chrome.
Actually I should talk about Android briefly, because you’re probably thinking, hey, what about Android?
First, Android is not Google. They have almost nothing to do with each other. Android is a company that was purchased by Google in July 2005, and that company has been allowed to run more or less autonomously, and in fact has remained largely intact through the intervening years. Android is an infamously hairy tech stack, and a just-as-infamously prickly organization. As one Googler put it, “One does not simply walk into Android.”
I’ve done my share of ranting about how bad some of Android’s early design decisions have been. Heck, at the time I was doing that ranting, they were busy rolling out shit like Instant Apps, which is now (surprise!) deprecated, and haha on you if you were dumb enough to listen to them when they told you to port all your stuff to Instant Apps.
But there’s a difference here, a material difference, which is that the Android folks actually DO understand how important Platforms are, they go well out of their way to prevent breaking your old Android code. In fact, their efforts to keep backward compatibility are so extreme that even I, during my brief stint in Android-land a few years back, found myself trying to convince them to drop support for some of the oldest devices and APIs. (I was wrong, as I’ve been about many other things past and present. Sorry Android folks! Now that I’ve visited Indonesia, I see why we need them.)
The Android folks take backwards compatibility to almost unimaginable extremes, which piles on massive amounts of legacy technical debt in their systems and toolchains. Oh boy, you should see some of the crazy stuff they have to do in their build system, all in the name of compatibility.
For this, I award Android the coveted You’re Not Google award. You really don’t want to be Google. They don’t know how to build platforms that can last, whereas Android does know how to do it. And so Google is being very wise in one respect: letting the Android folks do things their way.
Instant Apps was a pretty dumb idea, though. You know why? Because it required you to rewrite and re-architect your application! As if people are just going to up and rewrite 2 million apps. I’m guessing Instant Apps was probably a Googler’s idea.
But you see the difference here. Backwards compatibility comes with a steep cost, and Android has chosen to bear the burden of that cost, whereas Google insists that you, the paying customer, bear that burden.
You can see Android’s commitment to backwards compatibility in their APIs. It’s a sure sign, when there are four or five different coexisting subsystems for doing literally the same thing, that underlying it all is a commitment to backwards compatibility. Which in the Platforms world, is synonymous with commitment to your customers, and to your marketplace.
Google’s pride in their software engineering hygiene is what gets them into trouble here. They don’t like it when there are lots of different ways to do the same thing, with older, less-desirable ways sitting alongside newer fancier ways. It increases the learning curve for newcomers to the system, it increases the burden of supporting the legacy APIs, it slows down new feature velocity, and the worst sin of all: it’s ugly. Google is like Lady Ascot in Tim Burton’s Alice in Wonderland:
Lady Ascot: Alice, do you know what I fear most?
Alice Kingsley: The decline of the aristocracy?
Lady Ascot: Ugly grandchildren.
To explore the tradeoff of Pretty vs Practical, let’s take a peek at a third successful platform (after Emacs and Android) and see how it fares: Java itself.
Java has tons of deprecated APIs. Deprecation is super popular with Java programmers, more so than for most programming languages. Java itself, the core language and libraries, deprecates APIs all the time.
To take just one of thousands of examples, killing threads is deprecated. It’s been deprecated since Java 1.2, released in December 1998. It’s been 22 years since they deprecated it.
My live production code still kills threads every day. Is that a good thing? Absolutely! I mean, obviously if I were to rewrite the code today I’d do it differently. But my game code, which has been able to make hundreds of thousands of people happy over the past 2 decades, was written to kill worker threads that take too long, and I’ve never had to change it. I know my system best, and I have literally 25 years of experience with running it in production, and I can tell you: In my use case, killing these particular worker threads is harmless. It is not worth it to focus my time and energy on rewriting that code, and praise be unto Larry Ellison (I guess), since Oracle has never made me rewrite it.
I guess Oracle understands Platforms too. Go figure.
You can see evidence all through Java’s core APIs of waves of deprecation, like glacier lines in a canyon. There are easily five or six different keyboard focus managers in Java Swing. In fact it’s hard to find a Java API that isn’t deprecated. But they all still work! I think the only time the Java core team will actually remove an API is if it causes a blatant security problem.
Here’s the thing, folks: We software developers are all super busy, and we are also faced with competing alternatives in every software domain. At any given time, programmers in language X are looking at language Y as a possible replacement. Oh, you don’t believe me? What about Swift? Everyone’s migrating *to* Swift, not away from it, right? Oho, how little you know. Businesses are taking a mercenary’s accounting of their dual mobile teams (iOS and Android), and starting to realize that those phony-sounding dog-and-pony-show cross-platform development systems like Flutter and React Native have real teeth, and using them could cut their mobile team sizes in half, or alternately, make them twice as productive. There’s real money at stake here. Yes, there are trade-offs, but on the other hand, mooooooooney.
Let’s say hypothetically that Apple was dumb enough to pull a Guido van Rossum, and declare that Swift 6.0 is backwards-incompatible with Swift 5.0, much in the way that Python 3 is incompatible with Python 2.
I probably told this story ten years ago, but about fifteen years ago I was at O’Reilly’s Foo Camp with Guido, sitting in a tent with Paul Graham and a bunch of other at-the-time mucky-mucks, waiting for Larry Page to fly out in his personal helicopter, and Guido was droning on tonelessly in the sweltering heat about “Python 3000”, which he had named in honor of the number of years it would take everyone to migrate to it. We kept asking him why he was breaking compatibility, and he’d answer, “Unicode”. And we’d ask him, hey, if we have to rewrite our code, then what other benefits are we going to see? And he’d answer, “Yoooooooooooooouuuuuuuniiiiiiicoooooooode”.
If you install the Google Cloud Platform “gcloud” SDK, you’ll get this notice:
Dear RECIPIENT,
We would like to remind you that support for Python 2 is deprecated, so fuuuuuuck yoooooooooooooooooooouuuuuu
…and so on. The Circle of Life.
But the thing is, every single developer has choices. And if you make them rewrite their code enough times, some of those other choices are going to start looking mighty appealing. They’re not your hostages, as much as you’d like them to be. They are your guests. Python is still a very popular programming language, to be sure — but golly did Python 3(000) create a huge mess for themselves, their communities, and the users of their communities’ software — one that has been a train-wreck in progress for fifteen years and is still kicking.
How much Python software was rewritten in Go (or Ruby, or some other alternative) because of that backwards incompatibility? How much new software was written in something other than Python, which might have been written in Python if Guido hadn’t burned everyone’s house down? It’s hard to say, but I can tell you, it hasn’t been good for Python. It’s a huge mess and everyone is miserable.
So let’s say Apple pulls a Guido and breaks compatibility. What do you think will happen? Well, maybe 80–90% of the developers will rewrite their software, if they’re lucky. Which is the same thing as saying, they’re going to lose 10–20% of their user base to some competing language, e.g. Flutter.
Do that a few times, and you’ve lost half your user base. And like in sports, momentum in the programming world is everything. Anyone who shows up on the charts as “lost half their users in the past 5 years” is being flagged as a Big Fat Loser. You don’t want to be trending down in the Platforms world. But that’s exactly where deprecation — the “removing APIs” kind, not the “warning but permitting” kind — will get you, over time: Trending down. Because every time you shake loose some of your developers, you’ve (a) lost them for good, because they are angry at you for breaking your contract, and (b) given them to your competitors.
Ironically, I played a role in helping Google become the deprecation-happy prima donnas that they are today, when I built Grok, which is a source-code understanding engine that facilitates automation and tooling on source code itself — similar to an IDE, but as a cloud-based service that stores materialized representations of Google’s entire multi-billion-line source graph in a big datastore.
Grok provided Googlers with a powerful foundation for doing automated refactorings across the entire code base (literally all of Google). Grok can figure out not just your upstream dependencies (who you depend on), but also your downstream dependencies (who depends on you), so when you change an API, you know everyone you’re breaking! So you can make a change and know that every consumer of your API is being updated to the replacement version; in fact, often, via a tool they wrote called Rosie, you can automate it entirely.
This permits Google’s code base internally to be almost preternaturally “clean”, as they have these automated mice scurrying about the house, There Will Come Soft Rains-style, cleaning up the dust balls as they rename SomeDespicablyLongFunctionName to SomeDespicablyLongMethodName because someone decided it was an ugly grandchild and it needed to be euthanized.
And honestly it works pretty well for Google… internally. I mean, yes, the Go community within Google does get some good-natured laughs at the expense of the Google Java community over the latter’s habit of gratuitous continuous refactoring. If you keep twiddling with something N times, then it implies that not only did you fuck it up N-1 times, but after a while it’s pretty clear you’ve probably fucked it up on the Nth try as well. But by and large, they stay on top of it, and stuff stays “clean”.
The problem begins when they take that attitude and try to impose it on their Cloud customers and other API users.
I’ve walked you a bit through Emacs, Android, and Java; let’s look at one last successful long-lived platform: The Web itself. Boy, HTTP sure has gone through a lot of iterations since 1995 when we were all using blink tags and under-construction signs in our handwritten HTML pages.
But it still works! And those pages still work! That’s right, folks, browsers are some of the world champions at backwards compatibility. Chrome is another example of a rare Google Platform that has their heads screwed on straight, and, you guessed it, Chrome acts effectively like a separate company from the rest of Google.
I’ll also give a shout-out to our friends in the Operating Systems business: Windows, Linux, NOT APPLE FUCK YOU APPLE, FreeBSD, and so on, for doing such a great job of backwards compatibility on their successful platforms. (Apple gets like a C-minus at best, since they break shit all the time for no good reason, but somehow the community papers over it on each release, and so far, containers haven’t completely obsoleted OS X… yet.)
But wait, you say. Aren’t you comparing apples to oranges, with standalone single-machine software systems like Emacs/JDK/Android/Chrome to multi-machine systems and APIs like Clouds?
Well, I tweeted this yesterday, but as a Larry Wall “sucks/rules”-style yardstick, I searched for “deprecated” on Google and Amazon’s developer sites, respectively, and even though AWS has hundreds more service offerings than GCP, Google’s developer docs mention deprecation around 7x as often.
At this point, if anyone at Google is reading this, they’re probably ready to pull out charts, Donald Trump interview style, showing me how they’re actually doing really well, and that I can’t do unfair comparisons like “deprecation mentions as a function of number of service offerings”.
But after all these years, Google Cloud is still #3 (I still haven’t written my “How to Aim For #2 and Miss” blog post about this), and according to some internal sources, there’s some concern that they may sink to #4 soon.
I don’t have a slam-dunk silver-bullet argument for you here, to “prove” my thesis. All I have are the colorful examples I’ve shared, which I’ve accumulated over 30 years as a developer. I’ve alluded to the deeply philosophical nature of this problem; in a sense, it’s politicized within the software communities. Some folks believe that platform developers should shoulder the costs of compatibility, and others believe that platform users (developers themselves) should bear the costs. It’s really that simple. And isn’t politics always about who has to shoulder costs for shared problems?
So it’s political. And there will be angry responses to this rant.
As a user of Google Cloud Platform, and also (at Grab) of AWS for 2 years, I can tell you that there’s a world of difference between the philosophies of Amazon and Google when it comes to priorities. I’m not actively developing on AWS, so I don’t have as much of a sense for how often they sunset APIs that they have previously dangled alluringly before unwitting developers. But I have a suspicion it’s nowhere near as often as happens at Google, and I believe wholeheartedly that this source of constant friction, and frustration, in GCP, is one of the biggest factors holding it back.
I know I haven’t gone into a lot of specific details about GCP’s deprecations. I can tell you that virtually everything I’ve used, from networking (legacy to VPC) to storage (Cloud SQL v1 to v2) to Firebase (now Firestore with a totally different API) to App Engine (don’t even get me started) to Cloud Endpoints to… I dunno, everything, has forced me to rewrite it all after at most 2–3 years, and they never automate it for you, and often there is no documented migration path at all. It’s just crickets.
And every time, I look over at AWS, and I ask myself what the fuck I’m still doing on GCP. They clearly don’t want customers. They want shoppers. Do you see the difference? Let me show you.
Google Cloud has a Marketplace in which people can offer their software solutions, and in order to avoid the empty-restaurant effect, they had to populate it with some offerings, so they contracted with a company called Bitnami to create a bunch of “click to deploy” solutions, or perhaps I should write “solutions”, because they don’t solve a fucking thing. They’re just there as checkboxes, as marketing filler, and Google never gave a shit whether any of them worked from Day One. I know the PMs who were driving it and I can assure you, those men do not give a shit.
Take click-to-deploy Percona, for instance. I was getting fed up with Google Cloud SQL’s shenanigans, and started looking into setting up my own Percona cluster as an alternative. And for once, Google was going to have done something right, and they were going to save me some time and effort with the click of a button!
Go ahead, I dare you. Follow the link and click the button. Choose “yes” to get all the default parameters and deploy the cluster to your Google Cloud project. Haha, joke’s on you; it doesn’t work. None of that shit works. It’s never tested, starts bit-rotting the minute they roll it out, and it wouldn’t surprise me if over half the click-to-deploy “solutions” (now we understand the air quotes) don’t work at all. It’s a completely embarrassing dark alley that you don’t want to wander down.
But Google is straight-up encouraging you to use it. They want you to buy it. It’s transactional for them. They don’t want to support anything. It’s not part of Google’s DNA. Yes, the engineers support each other, as evidenced by my Bigtable anecdote. But for their customer-facing products and services, they have always been ruthless in shutting down any offering that doesn’t meet their money bar, even if it has millions of users.
And this presents a real problem for GCP, because that DNA is behind all their Cloud offerings. They aren’t committed to supporting anything; it’s well-known that they refuse to host (as a managed service) any third-party software until after AWS has already done the same thing and built a successful business around it, at which point their customers hold them at gunpoint. But that’s the bar, to get Google to support something.
This lack of a support culture, combined with a “let’s break it in the name of making it prettier” deprecation treadmill, is alienating their developers.
And that’s not a good thing if you want to build a long-lived platform.
Google, wake the fuck up. It’s 2020. You are still losing. It’s time to take a hard look in the mirror and answer for yourselves whether you really want to be in the Cloud business.
If you do, then stop breaking shit. You guys are rich. We developers are not. So when it comes to shouldering the burden of compatibility, you need to pay for it. Not us.
Because there are at least three other really good Clouds out there. They are beckoning.
And now I’ll get back to trying to fix all my broken stuff. Sigh.
Tune in next time!
p.s. An update, after having read some of the discussions (which were great, btw). Firebase is not deprecated, nor are there any plans that I know of. However, they have a nasty threading bug that causes the Java client to stop in App Engine, which one of their engineers helped me with while I was at Google, but they never actually fixed it outright, so I have a crummy workaround to restart my GAE app every day. Four years later! Now they have Firestore, which will take work to migrate to as it’s totally different, and the Firebase bug’s never gonna be fixed. What can we learn from this? You can get help from them if you work there. It’s frightening that I appear to be the only one using Firebase on GAE, since I’m literally writing fewer than 100 keys in a 100% vanilla app, and it stops working every couple days from an acknowledged bug. What can I tell you, except use it at your own risk. I’m switching to Redis.
I’ve also seen some folks more experienced with AWS saying that AWS basically never deprecates/sunsets anything, SimpleDB being a great example. My speculation about AWS not having Google’s deprecation disease seems to have been justified.
It was also brought to my attention that 20 days ago, Google’s App Engine team broke a critical Go library hosting service by deprecating and killing a GAE app being run by one of the core Go engineers. Egg on face indeed.
Finally, I’ve heard that Googlers are busy discussing this already, and are by and large agreeing with me (love you guys!)—with the caveat that they seem to think that it’s not fixable because Google’s culture has never had the right incentive structure in place. I think it would be good if I could carve out some time to discuss the absolutely astonishing experience I had working with AWS engineers while I was at Grab. Sometime soon, I hope!
Oh yeah, and they really did have different species of gummy sharks in a giant self-serve bin in Building 43 in MTV in 2005, hammerhead being my favorite flavor. Larry & Sergey got rid of all the unhealthy snacks by 2006 though. So at the time of my Bigtable story in 2007, there were indeed no gummy sharks and I’m a big fat liar.
When I looked at Cloud Bigtable 4 years ago (give or take), that was the cost. It seems to have come down a bit, but is still an awful lot for an empty datastore, especially given that my first story shows how inconsequential an empty Bigtable is in their grand scheme.
Sorry for offending all the Apple folks, and for not saying enough nice things about Microsoft, etc. I’ve read all the threads online and nobody has said anything unfair, in my opinion. I appreciate all the discussion this has generated! But sometimes you have to make a big splash to kick the discussions off, you know?
Thanks for reading.
Update 2, Aug 19 2020: Stripe does it right! https://stripe.com/blog/api-versioning
Update 3, Aug 31 2020: A Google engineer in Cloud Marketplace who happens to be an old friend of mine contacted me to find out why C2D didn’t work, and we eventually figured out that it was because I had committed the sin of creating my network a few years ago, and C2D was failing for legacy networks due to a missing subnet parameter in their templates. I guess my advice to prospective GCP users is to make sure you know a lot of people at Google…
Steve Yegge is ex-Geoworks, ex-Amazon, ex-Google, and ex-Grab, with nearly 30 years of tech industry experience. Nowadays he’s pretty much retired.
7.9K 
62
7.9K 
7.9K 
62
Steve Yegge is ex-Geoworks, ex-Amazon, ex-Google, and ex-Grab, with nearly 30 years of tech industry experience. Nowadays he’s pretty much retired.
"
https://towardsdatascience.com/running-jupyter-notebook-in-google-cloud-platform-in-15-min-61e16da34d52?source=search_post---------4,"Sign in
What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Top highlight
Amulya Aankul
Sep 8, 2017·5 min read
Recently, while I was doing my research project on Computer Vision using Convolutional Neural Network, I found out that my 8GB RAM laptop is useless. It took me an hour to learn from just 1 epoch. Therefore, rather than spending 1500$ on a new GPU based laptop, I did it for free on Google Cloud. (Google Cloud gives 300$ credit, and I have 3 gmail accounts and 3 credit cards :D)
So lets not waste anymore time and move straight to running jupyter notebook in GCP.
For this step, you will have to put your payment information and verify your account. It’s the most simple step. If you fail this step, close your laptop and think where you are going in life.
Click on the three dots shown in the image below and then click on the + sign to create a new project.
Click on the three lines on the upper left corner, then on the compute option, click on ‘Compute Engine’
Now click on ‘Create new instance’. Name your instance, select zone as ‘ us-west1-b’. Choose your ‘machine type’. (I chose 8v CPUs).
Select your boot disk as ‘Ubuntu 16.04 LTS’. Under the firewall options tick both ‘http’ and ‘https’ (very important). Then, choose the disk tab and untick ‘ Delete boot disk when instance is deleted’.
If you click on ‘customize’, you will be able to find options for using GPUs. You can choose between 2 NVIDIA GPUs.
Some firewall settings:-
Now click on ‘Create’ and your instance is ready!
Your new VM instance should look something like this. Note down the External IP.
IMPORTANT : DON’T FORGET TO STOP YOUR GPU INSTANCE AFTER YOU ARE DONE BY CLICKING ON THE THREE DOTS ON THE IMAGE ABOVE AND SELECTING STOP. OTHERWISE GCP WILL KEEP CHARGING YOU ON AN HOURLY BASIS.
By default, the external IP address is dynamic and we need to make it static to make our life easier. Click on the three horizontal lines on top left and then under networking, click on VPC network and then External IP addresses.
Change the type from Ephemeral to Static.
Now, click on the ‘Firewall rules’ setting under Networking.
Click on ‘Create Firewall Rules’ and refer the below image:
Under protocols and ports you can choose any port. I have chosen tcp:5000 as my port number. Now click on the save button.
Now start your VM instance. When you see the green tick click on SSH. This will open a command window and now you are inside the VM.
In your SSH terminal, enter:
and follow the on-screen instructions. The defaults usually work fine, but answer yes to the last question about prepending the install location to PATH:
To make use of Anaconda right away, source your bashrc:
Now, install other softwares :
Open up a SSH session to your VM. Check if you have a Jupyter configuration file:
If it doesn’t exist, create one:
We’re going to add a few lines to your Jupyter configuration file; the file is plain text so, you can do this via your favorite editor (e.g., vim, emacs). Make sure you replace the port number with the one you allowed firewall access to in step 5.
It should look something like this :
To run the jupyter notebook, just type the following command in the ssh window you are in :
Once you run the command, it should show something like this:
Now to launch your jupyter notebook, just type the following in your browser:
where, external ip address is the ip address which we made static and port number is the one which we allowed firewall access to.
Congratulations! You successfully installed jupyter notebook on GCP!
Lets connect : https://www.linkedin.com/in/aankul
Follow me on medium: https://medium.com/@aankul.a
Edit :All those facing ‘ERR_CONNECTION_TIMED_OUT’ error. Please try Bastardized Eloquence’s solution in the comments.
Data Engineer at Amazon
See all (170)
5.2K 
80
Some rights reserved

Every Thursday, the Variable delivers the very best of Towards Data Science: from hands-on tutorials and cutting-edge research to original features you don't want to miss. Take a look.
5.2K claps
5.2K 
80
Your home for data science. A Medium publication sharing concepts, ideas and codes.
About
Write
Help
Legal
Get the Medium app
"
https://towardsdatascience.com/passing-the-google-cloud-professional-data-engineer-certification-87da9908b333?source=search_post---------5,"Sign in
There are currently no responses for this story.
Be the first to respond.
Daniel Bourke
Apr 27, 2019·10 min read
*Note: This article is dedicated to the Google Cloud Professional Data Engineer Certification exam before March 29, 2019. After this date, there were some updates. I have included these in the Extras section*
"
https://medium.com/google-developers/whats-the-relationship-between-firebase-and-google-cloud-57e268a7ff6f?source=search_post---------6,"There are currently no responses for this story.
Be the first to respond.
If you’re a mobile app developer, I imagine there’s a good chance that you know something about Firebase, Google’s mobile application development platform. Or, if you’re an enterprise systems developer, you might know something about Google Cloud Platform (GCP), a broad suite of products and services that host your data and code at planetary scale, and more. While both platforms can be used without knowledge of the other, there are some ways in which they overlap. Knowledge of this relationship is important in two circumstances:
Even if these situations don’t (yet!) apply to you, understanding how the tools and services are related should help reduce the friction you might encounter with some tasks, especially regarding your Firebase app.
So, what do you need to know? Let’s start with the most important detail.
When you go to create a Firebase project, this fact is mostly hidden. To get started with Firebase, it’s simply not necessary to know anything about GCP. The onboarding path is optimized to get you to a working solution with minimal effort. I know that many developers appreciate this!
Here’s a screenshot of the Firebase console after you’ve created a new project:
You can see a scrollable list of products on the left, organized by top-level categories that expand and collapse. In the middle, you have some buttons that help you get started adding your app to the project. It’s pretty clear what you’re expected to do next. Later, after you add an app and start using some of the products, the main area changes into a dashboard that shows you some stats on the products you use.
By contrast, here’s a screenshot of the same newly-created project in the Google Cloud console:
The look and feel is almost completely different. Here, I’ve also opened the hamburger menu in the upper top left, and it’s hovering over the main content. This menu also scrolls, and there are a LOT more products and options here than you see in the Firebase console.
You can think of a GCP project as a virtual container for data, code, configuration, and services, regardless of how it was created. When you create a Firebase project, you are actually creating a Google Cloud project behind the scenes. This means that you can view and manage many aspects of your Firebase project in the Cloud console.
In some cases, the Firebase console actually delegates to the Cloud console in order to handle some common tasks, such as billing management and administrative user management (known as Identity and Access Management, or IAM in the Cloud console). So if you’re working with the Firebase console, then somehow click through to something with a blue and white UI theme, you’ve just been sent to the Cloud console. Same project, different console and UI. Here’s what it looks like if you try to click on a billing account from the Firebase console. It’s definitely the Cloud console, even if it says “Firebase” at the top:
There’s one super-important thing to know about these project containers. Since the underlying project is the same for both Firebase and GCP, if you delete the project using either Firebase or the Cloud console, you will delete everything in that container, no matter where it was configured or created. So, if you created a project with the Cloud console, then add Firebase to it, then delete the project in the Firebase console, all your Cloud data will also be deleted.
Now let’s imagine, instead, that you’ve created a project in the Cloud console. At the outset, your project won’t have anything directly related to Firebase configured in it. After all, the Cloud console doesn’t know if you intend to build a mobile app, so why set that up? But if you have an existing Cloud project, you can very easily add Firebase to it.
To add Firebase services to an existing project, go to the Firebase console, click the “add” button. When it asks for your project name, you have the opportunity to choose an existing project from the dropdown that shows your existing projects that don’t have Firebase added.
When you select a project and proceed from this point, all the APIs and services that power Firebase products will be automatically enabled in your project, and you’ll be able to use the Firebase console to work with those products.
If you’re wondering what exactly I mean by “APIs and services”, this is a GCP concept that’s only visible in the Cloud console. Here’s a screenshot of the APIs and services dashboard from the Cloud console after Firebase has been added to a project:
Here, you can see a number of APIs (enabled by default), along with some Firebase product APIs highlighted in the red box. This detail of enabled APIs is hidden from developers in the Firebase console, because it’s not really necessary to know. However, knowledge of GCP APIs and services gains importance as an app’s backend becomes more sophisticated. For example, an app developer might want to make use of the Cloud Vision API to extract text from images captured by the device camera. And then, go further and translate the text discovered in that image using the Cloud Translation API. To use these APIs (and get billed for them), you have to enable them in the Cloud console. Once enabled, you can call them from your backend code (deployed to Cloud Functions, for example).
As you dig around in each console, one thing you might notice is that the set of products you can manage in the Firebase console has three items in common with the set of products in the Cloud console. These products are Cloud Storage, Cloud Firestore, and Cloud Functions. While each product is the same at its core, regardless of where you’re viewing it, they are each organized and managed in very different ways between the Firebase console and the Cloud console. This leads me to my next point.
As you might guess from their names, Cloud Storage, Cloud Firestore, and Cloud Functions are Google Cloud products. Technically, they are not Firebase products, even though you can work with them in the Firebase console and manipulate them in your app using Firebase SDKs and tools. First, some quick definitions:
Without Firebase in the picture, these Cloud products are typically used in enterprise environments, where data and processes are mostly controlled within Google Cloud, or some other backend. To work with these products programmatically, Google Cloud provides client APIs meant for backend code, along with the command line tools gcloud and gsutil.
With Firebase in the picture, these three products are enabled to work seamlessly with mobile apps by providing additional SDKs for mobile clients, additional tooling with the Firebase CLI, and a way to configure security rules to control access to data through the provided SDKs. I’ll talk about some of the specifics of these Firebase additions in future posts.
(Since I mentioned Cloud IAM earlier, I should also mention that Firebase offers additional IAM roles for some Firebase products that give other members of your team granular access to those products, without the risk of them making a dangerous change elsewhere in your project.)
Note that the names of these three Cloud products don’t change from a Firebase perspective. I know it’s tempting (and natural!) to say things like “Firebase Storage” and “Firebase Functions”, but these names aren’t accurate. Am I being pedantic about this? Perhaps, but you won’t find these names anywhere in formal documentation! However, you will see names like “Cloud Storage for Firebase” and “Cloud Functions for Firebase” when dealing with the Firebase add-ons for these Cloud products.
If you’re a Firebase app developer, you probably created your project in the Firebase console. But, at some point, you might need to jump over to the Cloud console for some administrative tasks, to expand your cloud infrastructure, or make use of Cloud APIs. The Firebase console is just the beginning to build out the infrastructure of your mobile app.
If you’re a Cloud infrastructure developer, and you want to build mobile or web apps against the data you’ve already stored, you’ll need to jump into the Firebase console to deal with configurations and tasks that are unique to the Firebase additions to some Cloud products.
In fact, Actions on Google projects are also GCP projects (if you’re working with DialogFlow). These projects have Firebase enabled by default, so that’s another way you could end up with a new perspective on a GCP project. In any case, no matter how your project came into existence, the console you started with might not end up being the only console you use. Thinking of a project primarily as a container for services and APIs makes this transition easier. Each console is just giving you a view of those services and APIs in a different way.
Read more about the differences between Firebase and Google Cloud with respect to these products:
Engineering and technology articles for developers, written…
3.2K 
15
3.2K claps
3.2K 
15
Engineering and technology articles for developers, written and curated by Googlers. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
firebase-consultant.com, Firebase GDE, engineer, developer advocate, Xoogler
Engineering and technology articles for developers, written and curated by Googlers. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/@ste.grider/serverless-showdown-aws-lambda-vs-firebase-google-cloud-functions-cc7529bcfa7d?source=search_post---------7,"Sign in
There are currently no responses for this story.
Be the first to respond.
Stephen Grider
Apr 24, 2017·11 min read
June 8 Update: Jason Polites from Google (https://www.linkedin.com/in/polites/) helpfully clarified a couple issues around my analysis of Google Cloud Functions. These updates are added in line with the article text below. Thanks Jason!
If 2016 was the year of microservices, 2017 is shaping up to be the year of serverless computing, most notably through AWS Lambda and Google Cloud Functions created through Firebase.
Cloud Functions for Firebase were announced a month ago, bringing them into direct competition with AWS’s offerings. This, of course, inevitably invites benchmarks and comparisons between AWS’s and Google’s offerings. Let’s walk through the two.
Wait, what is serverless computing?
Ah, the requisite explanation.
Traditional backends have been created using monolithic servers, where a single server may have several different responsibilities under a single codebase. Request comes in, server executes some processing, response comes out. The same server might be responsible for authentication, handling file uploads, and keeping track of user profiles. The key mechanic is that if two different requests come in for two different resources, it gets handled by a single codebase. This server might run on dedicated or virtualized machinery (or several machines!), and persistently runs over the span of days, weeks, or months.
More recently, we’ve seen the introduction of microservices as a popular architectural decision. With a microservices approach, there are still distinct servers, but many different servers, which of which handles a single purpose. A single service might be in charge of user authentication, and another one may handle file uploads. Microservice architectures are characterized by many separate codebases and incremental deployments of each individual service. The idea here is that a service which isn’t modified often is less likely to break, along with providing a more logical separation of responsibilities. Like monolithic deployments, microservices are traditionally long-running processes being executed on dedicated or virtualized machinery.
Finally, serverless architectures. Think of them as a natural evolution or extension to microservices.
This is a microservice architecture driven to the extreme. A single chunk of code, or ‘function’ is executed anytime a distinct event occurs. This event might be a user requesting to login, or a user attempting to upload a file. These functions are traditionally very short running in nature — the function ‘wakes up’, executes some amount of with a duration of 10 milliseconds to 10 seconds, and is then terminated automatically by the service provider. No persistence, no dedicated machinery — in effect, you have no idea where your code is running at any given time. The benefit to serverless architectures shares some of the benefits of a microservices based approach, where each function has some distinct responsibility and logical separation.
The Test App
To compare the two services, I wrote a small React Native application with the intent of providing one-time-password authentication.
Rather than expecting a user to enter a tedious email and password combination, the user is expected to enter just their phone number. Once we have their phone number in hand, we generate a short six-digit token then text it to the user via SMS. The user then enters the code into our app, after which we expect them to enter the code back into our app. If they enter the correct code, great, they are now authenticated.
Given that the code is the key authenticating factor, its something that clearly shouldn’t be generated or stored directly on the user’s mobile device. Instead, we should generate and store the code somewhere else, somewhere that the user doesn’t have any type of read access to. Enter our serverless functions!
Its always important to plan out the different cloud functions that will be created. In this case, I see three clear phases of the login process where some amount of logic must be executed in a secure environment:
Each function we create is assigned a unique name, usually to identify its purpose. I followed a simple nomenclature, opting for ‘createUser’, ‘requestOneTimePassword’, and ‘verifyOneTimePassword’.
With these three functions in mind, let’s walk through the deployment process
Creation of functions with Lambda can take two forms, either direct access of the Lambda Console or through the Serverless framework. I chose to use the Serverless framework, as it made deployment (later) much easier.
Serverless encourages centralizing all configuration of your functions into a single YML file. The YML file requires the function name as it will be displayed on the Lambda console, the name of the function in your code base, and some configuration on when to execute the function. In our case, we wanted to execute the function on an incoming HTTP request with a method of POST.
Here’s the relevant snippet of config from the YML file for creating a new user:
One of the interesting aspects of AWS Lambda is that it is truly built assuming that you’ll have any type of event driving a function invocation, not just an incoming HTTP request issued by a client device. Other valid triggers might be a file upload to S3, or a deploy to some other service on AWS. Even though its clear to you and me that we only want to run the function with an incoming HTTP request, we still have to be awfully explicit.
I found writing the actual function to require a little more boilerplate than I’d like:
You will notice a reference to firebase in here; I am still using Firebase for user management, even though the app is hosted on AWS infrastructure.
Yep, the request body has to be manually parsed. You’ll also notice that I made some ‘handleSuccess’ and ‘handleError’ helpers, to avoid some otherwise awful boilerplate. Here’s ‘handleSuccess’:
Again, don’t expect Lambda to handle JSON encoding or decoding for you, this is all manual.
Project creation with Cloud Functions was clearly easier. Its clear that the managers around this project assume that the most common use case is handling incoming HTTP requests, so there wasn’t a tremendous amount of configuration to route a particular event to a particular function.
Generation of the initial project was done by using the firebase CLI, which I hadn’t been previously familiar with. The CLI generates an entire Firebase project, which allows hosting important configuration like your security rules in a VCS, rather than relying entirely upon the console rule editor.
Definition of the functions took place inside of a Javascript file, where each export is essentially assumed to be a deployable function. For example:
The actual function creation was far more straightforward.
Fans of Express JS will immediately be at home with the req, res function signature. The request and response objects use an identical API to Express’, which makes for a straightforward learning curve. Also notice no need for complicated boilerplate around handling responses.
Winner: Google Cloud Functions
Creating functions with Firebase is a clear winner. There’s less upfront configuration required, along with a far more palatable API. Of course, the caveat is that Firebase’s amount of configuration is smaller because there are fewer function triggers available on Firebase. No need to specify that a function should be executed on an incoming HTTP request when there are only six different ways of triggering them
Certainly not much to say here, as the deployment process is nearly identical on both platforms. Having set up the initial project with Serverless, deployment on the AWS side was as easy as a terminal command:
Firebase deployment was similar by using the Firebase CLI
In both cases, the time from initiating the deployment to seeing the function go live was about forty seconds. Nothing to lose sleep over.
Winner: Tie
If function creation was easier on Firebase, I can confidently say that testing your functions in a staging environment is far easier on AWS.
For the above project, I spent around two hours from start to finish on AWS, whereas the same exact project took around five hours, simply because of of the atrocious debug cycle. It all comes down to the presence of a simple tool on the AWS side — the beautiful blue Test button.
Once your function has been deployed, you can create a ‘test’ event, by manually creating a request to be sent directly to your function. In this case, I wanted to manually test the creation of a new user by providing a unique phone number. Using one of the sample templates, I manipulated the body of the request to include a phone number, then saved the test event.
Once your test event is created, that beautiful blue Test button will execute your function instantaneously and immediately show output from the execution in plain text, including not only the function’s request response, but also any log output coming from the function.
June 8 update: There is a testing mechanism for Cloud Functions, but it’s not (currently) available in the Firebase console. If you access the “Cloud Console” (https://console.cloud.google.com) you’ll see Cloud Functions there with a range of capabilities, including quick testing. There is also a local emulator which allows you to debug functions locally, and Cloud Platform also has a (free) Cloud Debugger which actually lets you put a breakpoint on live code!
Original writeup: Let me be clear: manual testing of Cloud Functions is a pain, stemming from two aspects:
To the first point, manual testing of Cloud Functions revolves around your favorite HTTP request utility, be it curl or Postman. If your function fails to execute due to some hidden typo, rest assured that you’ll get a 50x status code without much more information, rather than any helpful debug output.
If you do want to get information out, you’ll be using Firebase’s Function console.
At the console, you’re limited to seeing only logged information, as opposed to AWS’s console which shows both log statements and function response bodies.
But the biggest gripe I have is how long it takes to see logs appear here. With stopwatch in hand, it would take one to five minutes of waiting to see any log information pop up from a single request. That terrible feedback loop lead to a lot of confusion as I tried to keep the order in which I’d execute test requests in mind. Let’s face it; when you have a long feedback loop like that, you may immediately execute one to five manual tests, then try to decipher the output you receive a few minutes later. Not fun.
Winner: AWS Lambda
In general, you can count on paying for function invocations based on two metrics: the number of invocations, and the amount of time each invocation takes to execute, modified by the hardware that the function is executed upon.
June 8 Update: I have neglected to include Amazon’s API Gateway price, which is $3.50 per million requests and is necessary if you want to have HTTP invocation of the function. Cloud Functions includes this for no extra charge. So the 19,193,857 requests you quoted for AWS would actually cost ~$65, not $1, which is a pretty large difference.
Original: At the time of this writing, Cloud Functions cost $0.40 per million invocations (after two million that are free), while Lambda clocks in at $0.20 per million invocations (after one million that are free).
Execution environment refers to the hardware that is used to run the function. More powerful hardware, more cost. Its a bit of an exercise in engineering economics, however. If you’re running a computation heavy function that takes some non-zero amount of time to execute, you might think to use a less powerful machine, as it costs less money per millisecond of execution time. But its a double edged sword; the slower the machine, the more milliseconds you’re spending! I’d love to do some followup work to figure out the sweet spot in machine size for compute-heavy tasks.
Google Cloud Function’s invocation time pricing is a function of the CPU plus RAM size, whereas AWS is a function of the RAM size only.
For example, a function that takes 100ms to execute on a 256mb memory machine with a 400mhz cpu would cost the following on Google :
Or, put another way, you’d get 432,432 requests for $1 on Google, not including the free tier or flat cost of invocation.
On AWS Lambda, a similar setup would cost
Or, put another way, you’d get 19,193,857 invocations for $1, not including the free tier or flat cost of invocation. A factor of four, really? Someone check my math, please.
Winner: AWS
At this point, AWS Lambda is head and shoulders above Google Cloud Functions. The testing cycle feels much tighter, and the pricing is currently no-contest. Function creation is a bit easier with Google Cloud, but as soon as you get that boilerplate down you’re good to go.
Officially, Google Cloud Functions are still in beta, so we might see price reductions at some point in time, or better tooling, but for now I can’t help but point friends over to AWS Lambda.
2.6K 
10
2.6K 
2.6K 
10
"
https://medium.com/@earlg3/google-cloud-architect-exam-study-materials-5ab327b62bc8?source=search_post---------8,"Sign in
There are currently no responses for this story.
Be the first to respond.
Earl Gay
Jul 18, 2017·4 min read
(Update: I’ve added a new post highlighting my re-certification: Google Cloud Architect Exam Study Materials — Updates for 2019 Re-Certification. Be sure to check it out for some additional notes and prep material!)
After recently completing the Google Cloud Architect certification, I wanted to share the preparation materials that I used. Due to the newness of the exam, one challenge is there is not the same abundance of preparation material as there is for other Cloud exams. The official Exam Guide leaves a bit to be desired and there is no official Practice Exam at the current time, so I hope this material is helpful for folks preparing.
I prepared through a combination of methods outlined below (in addition to real-world GCP usage):
This is probably a little overkill, but none of the aforementioned alone went into the depth in all of the areas that I had hoped. The answer was using the combination, and skipping areas in each that may have been redundant.
This is the just the official Google Cloud Platform documentation: https://cloud.google.com/docs/.
Side-note: One thing I liked about the exam is that I didn’t feel like any of the questions asked me for point-in-time questions (i.e. what did this feature do when the exam was released versus what it may do now as of July of 2017). As a result, you can prepare well by reviewing the most up-to-date documentation without a fear that your knowledge will be too accurate. That’s kind of a silly thing to say, but other exams from other vendors do have questions where you have to answer them based on a previous point in time, even if the version of the exam and product still match.
Most of these documents are overviews or FAQs. You may want to branch off of them into deeper areas, but I felt the Product Overview and FAQs were solid. I reviewed the product documentation last of all of the study material I used, and mostly used it to fill in gaps of knowledge, though; if you review this material first, you may want to go deeper.
The sessions from Google Cloud Next are on YouTube now (217 of them), and if there are any areas where you feel you’d like a little more depth (e.g. App Engine, Cloud Storage, Datastore, Stackdriver), these sessions can be helpful. They’re also just all really good sessions in general. Even if you’re not preparing for the exam, I’d recommend watching as many of the videos as you can. I like to watch them at 2.0x speed for maximum productivity.
The Coursera courses are made available by Google to partners, but are available for anyone. They have a couple of options for courses, but if you are a current AWS Certified Architect Professional then there is a course available based on that, which is what I took. It compares GCP products with AWS products and is slimmed down from the other option.
I thought it was a valuable course, but lecture videos were fairly short. Total lecture time was 121 minutes, but there are quizzes and labs. The non-AWS specific course appears to be a little longer.
I subscribe to Linux Academy because they have a ton of great courses in general even outside of GCP, but they also have several Google Cloud Platform courses available. Honestly, I only skimmed through the course because I had done all of the other preparation prior, but the material seems solid.
Overall, I thought the exam was done well. It’s not too long, and the questions are good Architect-level questions: it’s about being able to architect solutions, not necessarily memorizing every low-level command. With that said, a few final thoughts as you prepare for the exam:
Customer Engineer, @GoogleCloud | Mobility, Cloud, and Random Technology | Posts are mine and don’t represent my company.
1.7K 
9
1.7K 
1.7K 
9
Customer Engineer, @GoogleCloud | Mobility, Cloud, and Random Technology | Posts are mine and don’t represent my company.
"
https://medium.com/google-developers/firebase-google-cloud-whats-different-with-cloud-functions-612d9e1e89cb?source=search_post---------9,"There are currently no responses for this story.
Be the first to respond.
In my prior post about Firebase and & Google Cloud Platform (GCP), I talked about some of the ways that Firebase relates to GCP. The main points were:
I promised to expand more on the third point for each of the products that Firebase augments. Today, it’s Cloud Functions!
If you’re not familiar with Cloud Functions (Firebase, GCP), it’s Google’s serverless compute product that lets you deploy code that responds to HTTP requests and events from other Google products in your project. Event sources include Cloud Storage, Cloud Firestore, Firebase Realtime Database, and many others. So, for example, when a file gets uploaded to your Storage bucket, your code can receive an event for that. And when a document changes in Cloud Firestore, you can respond to that as well.
I, for one, love working with Cloud Functions. In my early app-building days, I always found it to be a pain to try to manage a backend for my app, even if it didn’t have to scale. Today, if I want to make a web API or some other backend, I just write and deploy code to Cloud Functions, and it all Just Works and scales. Now, without the responsibility of managing a backend, I get to spend more time working on the app.
Since my world of development exists primarily in the mobile space, I also spend a bunch of time with Firebase. This means that I prefer a Firebase-centric view of my software development tools. So, when I work with Cloud Functions, I typically do so with the tools and APIs provided by Firebase. However, I find it’s helpful to keep in mind that Cloud Functions is actually a Google Cloud product, visible from Firebase.
I have a project where I’ve already deployed some functions with the Firebase CLI (more on that later). Here’s what the Cloud Functions dashboard looks like in the Firebase console:
You can see all the deployed functions in this project, along with some tabs at the top for more diagnostics. Between all these tabs, you can do all your typical work with Cloud Functions. For the function where the mouse pointer is hovering, there’s an overflow menu with additional options. One of those options is called “Detailed usage stats”, and you can see an icon there that indicates that you’re going to leave the Firebase console if you click it. Clicking the link opens a tab and takes you to the Cloud console (easy to spot with its blue and white UI theme), with even more detailed diagnostics for just that one function. There are also tabs for viewing the source code and other files deployed with the function, as well as a way to test the function.
If you click the “Edit” button at the top, you can change some of the properties of the function:
It’s worth noting that this console page is the only place you can set retries on failure for a background function that was deployed with the Firebase CLI. This is an important configuration that helps make sure your functions are as reliable as possible, assuming you’ve coded it correctly. (The checkbox for retries isn’t visible here — you have to scroll down a bit further.) This console page is also the only place where you can change the memory and timeout configured for your function after it’s already been deployed.
Essentially what we have here is a situation where the Firebase console delegates to the Cloud console for some deeper information and tasks that aren’t normally required during development. From here you can also navigate to the Cloud Functions dashboard inside the Cloud console, which is similar to the equivalent Firebase screen:
If you want to navigate to this dashboard directly, you can always navigate to the Cloud console directly, select your project from the dropdown at the top, then choose the Cloud Functions product in the left menu.
The Firebase console typically seeks to give you a simplified view of Cloud products. As you perform more advanced tasks, you’ll likely end up spending more time in the Cloud console. I find that it’s common to switch between them frequently, depending on my task at hand. Your typical daily work with Cloud Functions won’t be in either console, however. The most common work is writing code and deploying it with a CLI.
As I mentioned before, my view of software development is mostly through the eyes of a mobile developer who uses Firebase for most backend functionality. As such, I tend to prefer the Firebase CLI to deploy to Cloud Functions. You should probably know that there is also a Cloud CLI called gcloud that you can also use to deploy functions. Both are effective at deploying functions.
The Firebase CLI offers some unique conveniences:
gcloud gives you these benefits:
These two CLIs serve mostly different needs, and you can’t use one CLI to deploy all the same code as the other, simply because their deployment configurations are different. However, there’s nothing really preventing you from using both CLIs in tandem, as each provides its own advantages. For example, if you enjoy working with the customized APIs provided for each event source by firebase-functions and the Firebase CLI, you are almost certainly going to use the Firebase CLI for deployment (the APIs don’t work with gcloud). And did I mention those TypeScript type bindings? 😁 However, gcloud gets you closer to the Cloud Functions product itself and lets you perform more power-user actions. So give them both a try and see how they could work well for you.
It’s very common for mobile apps to invoke backend APIs via an HTTP request. Firebase makes this easier for developers by providing a special kind of “callable” function, which is a wrapper around normal HTTP functions. For client apps, Firebase provides a library that makes it easy to directly invoke callable functions without having to manage the details of an HTTP client library. When you use the Firebase SDK to invoke a callable function, these tasks are performed automatically, reducing the amount of boilerplate code you have to write on both the client and backend:
Since Cloud Functions offers HTTP triggers, you can easily use them for webhooks and REST APIs. What your API consumers may not appreciate, however, is the URL that your function is given by default. Assuming that your Cloud project name is “your-project”, and you’ve deploy to region “us-central1”, a function called “helloWorld” will look like this:
https://us-central1-your-project.cloudfunctions.net/helloWorld
It’s not exactly the most memorable URL. If you have a public API for other developers to consume, you might want to attach that to your domain instead. With Firebase Hosting, you can do that. Firebase Hosting is normally used to distribute static web content around the world, but you can also use it as a proxy for Cloud Functions. All you have to do is connect your domain with Firebase Hosting, then connect Firebase Hosting with Cloud Functions and rewrite a path forward to your Cloud Function endpoint. With this, you can have a more strongly branded endpoint:
https://api.your-domain.com/helloWorld
On top of that, you can configure Firebase Hosting to enable edge caching of your function’s response so that it’s served faster, and you don’t pay the cost of a function invocation each time.
Firebase offers unique tools and SDKs for mobile developers who want to use Cloud Functions, building on top of the core product. So, if you do any mobile development, be sure to check out both the Firebase and Cloud perspectives of Cloud Functions.
Engineering and technology articles for developers, written…
1.6K 
5
1.6K claps
1.6K 
5
Engineering and technology articles for developers, written and curated by Googlers. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
firebase-consultant.com, Firebase GDE, engineer, developer advocate, Xoogler
Engineering and technology articles for developers, written and curated by Googlers. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/@davidmytton/how-much-is-spotify-paying-google-cloud-ebb3bf180f15?source=search_post---------11,"Sign in
There are currently no responses for this story.
Be the first to respond.
David Mytton
Mar 7, 2016·4 min read
Two weeks ago, Spotify announced it was migrating from its own datacentres to Google Cloud Platform.
This is a huge win from Google because Spotify is the first major service running at huge scale that is deploying across many of its cloud products (and talking about it). We all know that Snapchat has been running on Google for a while, but since it is primarily on App Engine, Google needed a credible use case for its other services. Now it has one. This is similar to AWS’s Netflix.
"
https://medium.com/google-cloud/every-google-cloud-product-described-in-4-words-or-less-4d3f37f4567b?source=search_post---------12,"There are currently no responses for this story.
Be the first to respond.
28 new products — the Github link shows which ones are new
Download PDFs, text, and hi-res PNGs from https://github.com/gregsramblings/google-cloud-4-words
‪Includes Google Cloud, Firebase, Apigee, Google Maps Platform, and G Suite APIs
Also tweeted at https://twitter.com/gregsramblings/status/1151881674361204738
Check my blog for other resources — https://gregsramblings.com
Google Cloud community articles and blogs
1.3K 
16
1.3K claps
1.3K 
16
Written by
Google Developer Relations — Living in San Francisco https://gregsramblings.com | https://cloud.google.com | http://instagram.com/gregsimages
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Google Developer Relations — Living in San Francisco https://gregsramblings.com | https://cloud.google.com | http://instagram.com/gregsimages
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://lugassy.net/why-we-moved-from-amazon-web-services-to-google-cloud-platform-726c412fd667?source=search_post---------13,"TL;DR: AWS’s awesome, but Google is Googol awesome.
Participate in any AWS re:Invent conference ($1,600 admission) or follow head evangelist Jeff Barr and you’ll fall in love instantly with Amazon Web Services.
100s of new features every year and an all-you-can-eat, elastic, no-ops bouffe of on-demand services. Well, until you get to actually taste the food…
Amazon’s awesome, but Google Cloud is built by developers, for developers, and you see it right away.
GAE just works, has auto-scaling, load-balancers and free memcache all built in. Want to connect to Cloud SQL? use the virtual linux socket /cloudsql. Want to write custom logs and see them instantly? Append /var/log/app_engine/custom_logs or simply console.log() and console.error(). Want to profile and debug your application IN PRODUCTION? put breaking points in StackDriver or SSH to the managed instance.
Under the hood you’ll see GAE is 100% docker. Use it to run your 20 microservices with *.appspot.com service discovery or run one mamooth application at scale.
Update: I’ve received horror stories about what it was like with GAE at the early days, especially with the built-in datastore. I had no experience with GAE or Google Cloud back then so can’t really comment. As before any blind date, be prepared to leave anytime. Code a wrapper around your cache, data and message stores so you can switch technologies/providers.
Google lets you create custom machine types with any cpu/memory configuration. They let you opt for cheaper, preemptible (a-la spot) instances with a single click and no bidding/auction code whatsoever.
Google connects each and every VM to its super-fast, low-latency networking. Amazon requires you to buy expensive 10G-capable instances and/or enable enhanced networking.
Google lets you set up simple Firewall rules. Amazon gives you VPC, security groups, network access control lists and a big, fat headache.
Update: Some have commented AWS VPC is great and lets you tightly secure instances, create sandboxes and control internal networking. I still found it confusing since I rarely need anything beside basic ports/home IP rules. However if you’d like to investigate every connection issue like a murder mystery go ahead.
Google bills by the minute (not hour) and apply AUTOMATIC DISCOUNTS for long-running workloads, with absolutely no reserved pricing nonsense (warning: AWS EC2 pricing page might crash your browser).
Want to run a message bus? AWS will make your head spin with SNS, SQS, Kinesis, Kinesis Streams and Kinesis Firehose. GCP has only Pub/Sub which just works and is insanely scalable.
Update: I realize SQS (~Hosted RabbitMQ?) and Kinesis (~Hosted Kafka?) are two different buses, but getting GCP to work with one messaging product regardless of volume/velocity sounds better for me.
Google BigQuery is nicely priced by the GB stored and TB queried, has day partitioner built-in, 50% reduction in price for unmodified partitions (so you can keep data for longer) and full SQL support.
Google DataFlow is an amazing framework for consuming and processing data in batch or streams, with windowing, automatic triggering/speculative data and easy to use transformations.
Update: AWS started adding “Streams” to each service and spun them as new products, further increasing confusion. Dataflow is easier to comprehend as it treats all I/O targets as either sources and/or sinks.
Amazon has one of the most confusing IAM. While it is nice to set up a role to only allows usage for a particular resource from a specific device and times of day, you end up spending most of your time debugging policies.
Google security is more leaned back, assumes all resources are allowed within each trusted “project”.
Moreover, people you invite to projects must have a Google Account which are secure by default and usually already set up.
Update: Apparently you can have a Google Account with any email address, but if you are like me and using Google for Work (Google Apps) for SSO, you are already set. Amazon also supports MFA but you have to create a new set of users.
We moved to GCP because we wanted to work on infrastructure that runs YouTube, Gmail and Google Analytics. We moved because Google is fair, much more tech-savvy and launch products that just works.
Update: It is unclear if Google (the search engine, youtube, gmail) is using GCP. It has certainly created it and happily released it as they are working internally on the next big thing.
AWS is still fantastic. I just hope they would close issues before releasing new, half-baked features in time for re:invent.
Someone wrote: “I guess nobody ever got fired for using AWS. It’s the IBM of the cloud” and I couldn’t agree more, but if you are your own boss and like to look further, go with GCP.
Your mileage may vary.
Follow me here or on Twitter as I plan to write deeper posts battling specific products (i.e Pub/Sub vs. Kinesis, Google CDN vs. CloudFront).
Updated based on some love and hate from HackerNews.
Thoughts about Startups, Development & Ops by Michael…
636 
20
636 claps
636 
20
Written by
Co-Founder, HashUnited.com. Built 7 other companies, sold 3, worked with dozens more. Writing about startups, dev & ops.
Thoughts about Startups, Development & Ops by Michael Lugassy
Written by
Co-Founder, HashUnited.com. Built 7 other companies, sold 3, worked with dozens more. Writing about startups, dev & ops.
Thoughts about Startups, Development & Ops by Michael Lugassy
"
https://medium.com/@howkhang/ultimate-guide-to-setting-up-a-google-cloud-machine-for-fast-ai-version-2-f374208be43?source=search_post---------14,"Sign in
There are currently no responses for this story.
Be the first to respond.
How Khang
Jan 3, 2018·9 min read
course.fast.ai
– – – – – – – – – – – – – – – – – – – – – –
The guide below is archived and no longer works.
Note 1: Please note that some people have reported that Google Cloud does not accept debit cards.
Note 2: If you are unable to find an Upgrade button at this point, just continue with the next steps. The Upgrade button should appear at Step 5 below when you request for an increase in quota.
Compute Engine → Settings
IAM & Admin → Quotas
Compute Engine → Marketplace
It is up to you to decide the number of cores and the amount of memory to allocate for your VM instance but if you have the budget for it, it is a good idea to go with 4 cores and > 12 GB memory, as the Tesla K80 GPU comes with 12 GB memory.
There appear to be warnings due to a deprecated image being used but so far I have not encountered any downstream problems so it may be safe to just ignore them for now.
There are a few methods of connecting to your Debian VM according to Google Cloud Platform’s official documentation (which can be found here). Two of those methods will be mentioned briefly.
While this method is the most convenient, it comes with a list of known issues that are officially documented here, which is why I recommend using the Google Cloud SDK command line method below.
Step a. Download and install Cloud Tools for your OS to use the Cloud SDK. (Windows users: (1) verify that the option to install bundled Python is checked and (2) ensure that you have PuTTY installed).
Step b. Run “gcloud init” to link your account, and select project and region.
Step c. Run “gcloud compute ssh <your instance name> -- -L 8080:localhost:8080”
Note: You should always stick to the same method of connecting to your VM because your login usernames for SSH from browser and for Cloud SDK may be different. This may result in confusion as you will end up in completely different home directories after connecting to your VM. If you intend to alternate between the two methods, first configure your SSH from browser and Cloud SDK login usernames to be identical.
*To switch users when connecting via Cloud SDK, use the command: “gcloud compute ssh <your user name>@<your instance name>”
Launch your browser and connect using “localhost:8080”.
Compute Engine → VM instances
— — — — — — — — — — — — — — — — — — — — — — — — — — — — — —
VPC Network → Firewall rules
Create a firewall rule if your project does not already have one for tcp:8888.
You may choose some other unused port number but the typical number is 8888. It is permissible to specify a range of ports such as in the above case.
Compute Engine → VM instances → Create
It is up to you to decide the number of cores and the amount of memory to allocate for your VM instance but if you have the budget for it, it is a good idea to go with 4 cores and 26 GB memory (ideally > 12 GB, as the Tesla K80 GPU comes with 12 GB memory).
Again, it is up to you to decide the type of persistent disk to use as the boot disk but SSD is recommended for its higher speed if you have the budget for it. (For the fast.ai version 2 course, you minimally need 20 GB.)
VPC network → External IP addresses
**Following this step results in ongoing charges for the static IP address. Skip this step if you wish to save on the cost of a static IP address (US$0.01/hour at time of writing).
There are a few methods of connecting to your Ubuntu VM according to Google Cloud Platform’s official documentation (which can be found here). Two of those methods will be mentioned briefly.
While this method is the most convenient, it comes with a list of known issues that are officially documented here, which is why I recommend using the Google Cloud SDK command line method below.
Step a. Download and install Cloud Tools for your OS to use the Cloud SDK. (Windows users: (1) verify that the option to install bundled Python is checked and (2) ensure that you have PuTTY installed).
Step b. Run “gcloud init” to link your account, and select project and region.
Step c. Run “gcloud compute ssh <your instance name>”
Note: You should always stick to the same method of connecting to your VM because your login usernames for SSH from browser and for Cloud SDK may be different. This may result in confusion as you will end up in completely different home directories after connecting to your VM. If you intend to alternate between the two methods, first configure your SSH from browser and Cloud SDK login usernames to be identical.
*To switch users when connecting via Cloud SDK, use the command: “gcloud compute ssh <your user name>@<your instance name>”
curl https://raw.githubusercontent.com/howkhang/fastai-v2-setup/master/setup.sh | bash
You will be automatically disconnected after the script has finished running in order for the VM to do a reboot.
Use this method only if you went through step 9 to obtain a static IP address.
Launch your browser and connect using your VM’s static IP address followed by the port number (in this case, “:8888”). For the token, copy the long string of characters shown after the “token=” .
Use this method if you skipped obtaining a static IP address in step 9 or if you encounter difficulties connecting to your Jupyter notebook using method 1.
Step a. Open another terminal (or Google Cloud SDK Shell) on your local machine and run the following command for port forwarding:
gcloud compute ssh <your instance name> --ssh-flag=“-L” --ssh-flag=“8888:localhost:8888”
You should see a new VM terminal pop up. You should now have a total of two local terminals (both showing your gcloud compute ssh commands) and two VM terminals (one showing the jupyter notebook server running and the other just showing the command line prompt).
Step b. Connect to Jupyter notebook using localhost:8888
Launch your browser and connect using “localhost” followed by the port number (in this case, “:8888”). For the token, copy the long string of characters shown after the “token=”.
And we are good to go! Let me know if you face any issues or have any feedback on this guide.
Lawyer + Coder
See all (44)
1.5K 
41
1.5K claps
1.5K 
41
Lawyer + Coder
About
Write
Help
Legal
Get the Medium app
"
https://towardsdatascience.com/how-to-extract-the-text-from-pdfs-using-python-and-the-google-cloud-vision-api-7a0a798adc13?source=search_post---------15,"Sign in
There are currently no responses for this story.
Be the first to respond.
Silvia Zeamer
Feb 14, 2021·10 min read
This winter, I discovered that Wellesley College, where I am currently a senior studying Media Arts and Sciences, has an archive of over a hundred year’s worth of course catalogues, admissions guidelines, and yearly bulletins. I was immediately electrified by the potential for fascinating data which could be drawn from these documents, but the first step would have to be converting them to text, as there are not many analytical methods which can be run on scans of old, browned PDFs.
Thus began my search for a way to quickly and effectively run OCR on a large volume of PDF files while retaining as much formatting and accuracy as possible. After trying several methods, I found that using the Google Cloud Vision API yielded by far the best results of any of the publicly available OCR tools I tried.
As I could not find any single, comprehensive guide to using this amazing tool to run simple OCR applications, I decided to write this one, so that anyone with a little programming knowledge can put this wonderful tool to use.
In order to run optical character recognition using Google Cloud Vision, you first need to have a Google account. This will allow you to login to Google’s dashboard for cloud services. One of the many services which are accessible from this dashboard is file storage, which we will be using to host the PDF file we will be converting to text.
Because the advanced machine learning algorithms which we will be accessing via the Cloud Vision API run in the cloud, we will need to upload our PDF to a “bucket” of files hosted by Google, so that it will be accessible.
This tutorial will show you how to write the end result, a text file containing all the text in your PDF, to a location on your computer.
3. Click on the dropdown menu just to the right of the logo which says Google Cloud Platform. Mine says “OCR Test”, which is the name of my currently open project, but yours will say something different. A window will pop up with a list for recent projects and a “New Project” button in the top right corner. Click the button to make a new project. Give your project a name which will help you remember what you’re using it for. You don’t need to worry about any of the other fields. Click “Create”. Once your project has been created, make sure to select it by opening the window again and selecting it from the list of recent projects.
4. You should now see the Project Info, APIs, and other information panels for your newly created project, as in the screenshot above. In the “Getting Started” panel on the bottom left, click “Explore and Enable APIs”. This will allow you to choose the Google APIs you want to be able to use for this project.
5. In the menu bar at the top of the screen, click “Enable APIs and Services”. This will take you to the API Library. Search for “Cloud Vision API” and select it.
6. Click “Enable” to make the API available to your project. This will take you to your overview page for the Cloud Vision API. In the top right corner of the screen, click “Create Credentials”.
7. Choose “Cloud Vision API” from the drop down menu under “Which API are you using?” and under “Are you planning to use this API with App Engine or Computer Engine,” select “No, I’m not using them”. Click the blue “What Credentials Do I Need?” button.
8. Now you will be able to create a key so that you can authenticate yourself when you try to connect to the Cloud Vision API. Choose a service account name you will remember, and set your role to “Owner”. Set the key type to JSON. Click continue. You will now be able to download a JSON file containing your credentials.
You now have a project on the Google Cloud Platform, which will be able to use the Cloud Vision API. The next step is to upload your PDF document so that it is stored in the cloud. Then, you can write the script to convert it to text.
9. If it is not already open, click the navigation menu on the left side of the Google Cloud Platform, and scroll down until you see “Storage”. Click on it — this will open a drop-down menu. Select “Browser” from the dropdown menu. At this point, you will need to enable billing if you have not done so already. If you have Google Pay, you can use it here — otherwise, you will need to enter external payment information. This will vary depending upon how you pay, so I will not give instructions. Once you’re done, you should see a dialogue with the option to “Create a Bucket”.
10. Give your bucket a unique name. This is a storage repository within the project you created earlier. Set where to store your data to “multi-region” and the default storage class for your data to “standard”. Click “Create”.
You now have a bucket set up, where you can upload files so that they can be accessed by any APIs which are enabled for the current project. You can upload the PDF file you would like to transcribe by dragging and dropping it from wherever you keep it on your computer.
You are ready to write a program which can access both this file and the Cloud Vision API by connecting to Google Cloud services and providing the key you downloaded earlier.
Now that you have everything you need set up on the Google Cloud side of things, we will move to installing the necessary tools on your computer and using them to extract text from a PDF file.
First, you may need to make some installations. Open your terminal and navigate to a folder where you will keep the python script you write. Enter the following commands.
pip install google-cloud-vision
pip install google-cloud-storage
These use pip to install two Python libraries with tools for interacting with the Google Cloud Vision and Cloud Storage APIs, respectively. Next, run
pip freeze
This will check if you’ve installed everything you should have. You should have the following, although most will likely be newer versions.
If you don’t have any of them, use pip to install the ones missing.
Finally, you need to set your Google Application Credentials — that is, you need to register where you’re keeping the json key you downloaded earlier, so that when you run programs using Google Cloud services, your computer can authenticate itself as belonging to your Google account.
You can find excellent instructions on how to do this on any platform here. Once you have done this, you will be able to run programs which use Google Cloud Services from the command line.
Now we get to the fun part — writing a script to actually perform optical character recognition on our chosen PDF! Make a new Python file and open it with your preferred code editor. I will explain each part of the script I used so that you can understand it as you substitute in your information. You can find the whole of the script here, on my Github, as well. Try to follow along with each step before downloading it to tinker with.
We need to import json so that we can handle Cloud Vision’s outputs. re is a library which will allow us to use regular expressions to match particular patterns in strings.
Vision and storage from google.cloud will allow us to use the Google Cloud Vision and Google Cloud Storage APIs.
2. The next step is to write a function to detect all the places in our PDF file where there is readable text, using the Google Cloud Vision API. Make sure to read the comments in this function, so that you understand what each step is doing.
In addition to the comments explaining this function, here are some things to note. You may expect that when we run Google’s amazing OCR tools on a document, we will get a text file in return. Actually, this function will just output a json file — or several, depending on the size of your PDF — containing information about where there is text in the file. Actually getting the text so we can read it is the next step.
This function takes two inputs. The first, gcs_source_uri is the location of your PDF file in Google Cloud storage. The second, gcs_destination_uri is the location in Google Cloud Storage where you want the json files containing your file annotations to go.
URI is the term for a file location in Google Cloud storage. You can think of it as a URL within Google Cloud Storage, or like a path on your computer. It describes where, in the hierarchy of files you keep on google cloud, a particular file can be found. To find the URI of a file, you can double click on it to see details about it and copy the URI from the table of data you will thus open.
To generate your annotations, you will write a line at the bottom of your Python file calling the async_detect_document function. Mine looks like this.
The first URI is the path to a PDF document stored in my google cloud storage bucket, from which I want to read. The second leads to a folder in which I am saving all of my document annotations.
3. Now that we have annotated our PDF, we can finally use Cloud Vision to go to each location where there is text and read it into a text file! My code for doing this follows. Again, be sure to read the comments.
This function takes just one argument: the URI of the location where we stored our annotations. It will output the results of the transcription into a text file in your currently active directory, in addition to printing them in your terminal.
Here’s how I called it, using the same directory as before.
Congratulations! If all went well, you should now be in posession of a text file containing a line-by-line transcription of all the machine-readable text in your PDF. You may be surprised by how much could be read — it even works on some handwriting.
Here is a side-to-side comparison of some of my results. This is a page from a course catalogue I drew from the Wellesley College archives, dating from 1889. Despite the fact that I used a totally un-pre-processed PDF as an input file for this test, the results are highly accurate, even for names and foreign words.
In my next article, I will demonstrate some methods for pre-processing old text files in order to increase accuracy even more, so stay tuned. If you have any trouble or just want to chat, please get in touch — I love to talk!
Senior at Wellesley College studying Media Arts and Sciences. Future research scientist in HCI and security. Here for human connection <3
See all (7)
554 
10
Every Thursday, the Variable delivers the very best of Towards Data Science: from hands-on tutorials and cutting-edge research to original features you don't want to miss. Take a look.
554 claps
554 
10
Your home for data science. A Medium publication sharing concepts, ideas and codes.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/build-a-weather-station-using-google-cloud-iot-core-and-mongooseos-7a78b69822c5?source=search_post---------16,"There are currently no responses for this story.
Be the first to respond.
Top highlight
Ok, there is a lot of tutorials teaching how to build a Weather Station because there many ways of doing it. It’s a simple project so I will try to focus on building an end to end solution, from collecting data to doing analytics on your data. All of it will use managed Google Cloud services, giving an overview on how to build a complete IoT solution. At the end, you could build reports on your data and access it through the web. Here you can see how it will look:
Our finished WebApp : https://weather-station-iot-170004.firebaseapp.com /
Data Studio report: https://datastudio.google.com/reporting/0B0w5dnm9bD8sdy1OR1lZQ0l4Vmc
In this tutorial we will build a weather station using a WiFi microcontroller running MongooseOS, that sends data securely via Cloud IoT Core using MQTT protocol, then the data is processed in an event-based way using Firebase Cloud Functions, that save the raw data in BigQuery and update the device current state in Firebase Realtime Database. The data then can be accessed through DataStudio and via a simple WebApp hosted on Firebase Hosting. It’s many products, but I will show how each one can be easily connected to deploy a product that scales on demand. Our architecture will look like this:
For ease of development, I’ll use MongooseOS, that already have a connector for Cloud IoT Core and helps with the process of provisioning devices with certificates, WiFi configuration and others custom configurations.
What we will learn:
So enough talk, let’s get started 🚀.
Google recently launched in public beta Cloud IoT Core, a managed service to securely communicate with your IoT devices using common protocols (MQTT and HTTP) and to manage those devices in an easy way. Basically, with this service, you can plug with many others Google services to process, store and analyze all the data generated by your devices. Here we can see an example of a recommended architecture using Cloud IoT Core.
Cloud IoT Core have a concept of registry of devices, wherein our project we will group a series of similar devices and associate with this registry. To get started with Google Cloud you can do all on the Cloud Console web interface, but the command line tools is a more powerful tool and it’s the one that I choose to use on this project.
To use the gcloud command line tools, follow the instructions here to download and install it.
cloud.google.com
After installing the SDK, you should install the beta tools to have access to the Cloud IoT Core commands. Also after this you should authenticate and create a project to use in this tutorial, exchange YOUR_PROJECT_NAME with a name that you want for this project:
Now on the Cloud IoT Core side, you first should configure some components related to Cloud PubSub, one of the main components used by Cloud IoT Core. In the commands below you will do the following:
If you access the Google Cloud Console you can validate that it’s all created and configured.
For this project I’ll be using the newest ESP32 WiFi microcontroller, for those who don’t know it yet, it’s the sucessor of the largely famous ESP8266 from ExpressIf, but now with much more capabilities, like built-in Bluetooth LE, dual-core processor clocked at 240MHz, touch sensor and support for flash encryption, so no one can get access to your code. One hell of an upgrade.
Adafruit sells an awesome kit to get started with the ESP32 and Google Cloud, it contains all you need for this project and many others, so if you want to go the easy way, you can buy one of this. (Just an idea for you adafruit industries, I do not have one of these, just saying …)
This project also works on an ESP8266, so the code and schematic provided here have a configuration to run on both microcontrollers. The circuit for this project is very simple, just connect the DHT sensor to the ESP32/ESP8266 like the following diagram:
To program the board we will use MongooseOS, that is an Operating System with many awesome features and made for commercial products. It has support for some microcontrollers like CC3200, ESP32 and ESP8266. One cool feature of it is the possibility to quickly prototype your embedded apps using Javascript and it has a tools called mos that make programming, provisioning and configuration really easy on those supported boards.
To use it we need to download and install it from the official website. Follow the installation instructions on https://mongoose-os.com/docs/quickstart/setup.html.
mongoose-os.com
With the tools installed, download the project code on Github repository linked here, so you can build and deploy it on the device.
github.com
The repository consists of 3 sub-projects:
Here some description of the firmware project:
To program the hardware, enter the firmware folder and run the following instructions to flash the firmware, configure WiFi and provision the device on Cloud IoT Core:
That’s it, your device will begin to send data to Cloud IoT Core. The projects come configured to send data each minute, but you can changes this later on the fs/init.js file or you can create a custom configuration variable to change the time. I will leave this as a homework. You can see whats happening on the device using the mos console tool. You will see it trying to connect with mqtt.googleapis.com.
To see the data on PubSub you can use gcloud command to query the subscription that we created:
If you see the data on the console, you can start celebrating, we are on the right path 🎉🏆.
Getting directly from the official website definition:
BigQuery is Google’s low-cost, fully manageable petabyte scalable data storage service. BigQuery is stand-alone, there is no infrastructure to manage and you do not need a database administrator as it scales with your data.
Here we will use it to store all of ours collected sensor data to run some queries and to build reports later using Data Studio. To start let’s create a Dataset and a Table store our data. To do this, open the BigQuery Web UI, and follow the instructions:
Now to insert data on BigQuery we will user Firebase Cloud Functions, that can be configured to execute based on many different triggers and events. One of those triggers are new data inserted on a PubSub Topic, so we will listen to our Topic associated with our Device Registry and with each data that arrives we execute a function that store the data in BigQuery and maintain the last device data on Firebase Realtime Database.
Firebase Realtime Database is a technology really useful to maintain realtime data, giving free and automagically sync between all connected clients. Even Google recommends it to maintain realtime state from IoT devices like we can see in the here.
The code for our function can be seen above, it basically react to PubSub events and insert into BigQuery then update the current state on Firebase.
The Firebase Command Line Tools requires Node.JS and npm, which you can install by following the instructions on https://nodejs.org/. Installing Node.js also installs npm.
Once Node and NPM is installed, run the following command to install Firebase CLI.
Now to configure firebase with our project and deploy the functions, in the project root folder, follow the above instructions:
With the deployed functions you have all setup to ingest the telemetry data sent by the device and store in both storages solution. You can see all deployed resource on the Firebase Console.
You can see the code for the functions above:
The firebase-tools have also a builtin server, you can start it on the project folder just running firebase serve, it will start a web server on port 5000 by default.
The webapp can be seen directly on the public directory, the logic is at public/app.js and the frontend at public/index.html. It’s pretty basic, just Javascript, Web Material Component and Chart.JS the charts.
If all it’s correctly setup, then you can celebrate again, because you developed an end to end solution for IoT without touching an advanced server setup.
Data Studio is a really intuitive tool and I will not explore it here so this tutorial became less extensive, but to let you know, Data Studio has a BigQuery connector, so just import your table and play with the different visualizations provided by this awesome tool. Go to the datastudio.google.com and play with it.
That’s it for this tutorial, hope that you got interested in Google Cloud IoT Core, it’s an awesome service that you can do powerful things with it. The post got a little bit longer than I expected, but I believe that gives a great overview of a lot of tools on Google Cloud Platform.
The code for this project can be found on my Github and some interesting are linked in the section bellow to read later:
github.com
Do you like this post ? So don’t forget to leave your clap on 👏 below, recommend and share it with your friends.
Did you do something nice with this tutorial? Show in the comments section below.
If you have any questions, post in the comments that I will try to help you.
Google Cloud community articles and blogs
1.3K 
26
1.3K claps
1.3K 
26
Written by
Product Engineer @Leverege & Google Developer Expert for IoT. When I'm not cooking, I'm building fun stuff or helping my local dev community. GDG organizer.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Product Engineer @Leverege & Google Developer Expert for IoT. When I'm not cooking, I'm building fun stuff or helping my local dev community. GDG organizer.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/what-are-the-google-cloud-platform-gcp-services-285f1988957a?source=search_post---------17,"There are currently no responses for this story.
Be the first to respond.
See Also: What is Google’s Cloud Platform (described using on the 1,000 most commonly used words)?
Google Cloud Platform (GCP) offers dozens of IaaS, PaaS, and SaaS services. Sadly, the Wikipedia entry for GCP is garbage, and while the official docs are pretty good, the marketing-dust sprinkled on them gives me a toothache. For my own reference, I pulled together an objective description for each of the services available on GCP.
Big hat-tip to Greg Wilson, the Google Cloud Developer Relations team, and the Google Cloud Developer Experts for the 4-word descriptions.
Something missing or wrong? Add a comment!
Google Cloud community articles and blogs
774 
6
774 claps
774 
6
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Developer Advocate @ Google, software engineer, and author of “Professional Android” series from Wrox. All opinions are my own.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/@sathishvj/writing-and-passing-the-google-cloud-associate-engineer-certification-a60c2f6d99c2?source=search_post---------18,"Sign in
There are currently no responses for this story.
Be the first to respond.
sathish vj
Nov 15, 2018·6 min read
Subscribe to my YouTube channel that teaches you to apply Google Cloud to your projects and also prepare for the certifications: youtube.com/AwesomeGCP. Check out the playlists I currently have for Associate Cloud Engineer, Professional Architect, Professional Data Engineer, Professional Cloud Developer, Professional Cloud DevOps Engineer, Professional Cloud Network Engineer, and Professional Cloud Security Engineer.
Associate Cloud Engineer Playlist — https://www.youtube.com/playlist?list=PLQMsfKRZZviRwqJwNmh1eAWnRMvlrk40x
I wrote the GCP Cloud Associate Engineer exam and passed. Yaay! Here are my immediate impressions and notes. Hope it is useful to future test takers.
www.credential.net
For those appearing for the various certification exams, here is a list of sanitized notes (no direct question, only general topics) about the exam.
Overall notes across all GCP certification exams
Notes from the Professional Cloud Architect exam
Notes from the beta Professional Cloud Developer exam
Notes from the Professional Data Engineer exam
Notes from the Associate Cloud Engineer exam
Notes from the beta Professional Cloud Network Engineer Exam
Notes from the beta Professional Cloud Security Engineer Exam
Notes from the Professional Collaboration Engineer Exam
Notes from the G Suite Exam
Notes from the Professional DevOps Engineer Exam
Notes from the Professional Machine Learning Engineer Exam
Main Link — https://cloud.google.com/certification/cloud-engineer
Topics Outline — https://cloud.google.com/certification/guides/cloud-engineer
Practice Exam — https://cloud.google.com/certification/practice-exam/cloud-engineer
A collection of posts, videos, courses, qwiklabs, and other exam details for all exams: https://github.com/sathishvj/awesome-gcp-certifications
I’ve collected here a bunch of free Qwiklabs codes which are awesome to get lots of hands-on practice. Use them well.
medium.com
Check the FAQs here: https://medium.com/@sathishvj/frequently-asked-follow-up-questions-on-google-cloud-gcp-certifications-438e1addb91d.
Wish you the very best with your GCP certifications. You can reach out to me at LinkedIn and Twitter, especially for training for the certifications, short term consulting on GCP, and anything related to GoLang.
tech architect, tutor, investor | GCP 12x certified | youtube/AwesomeGCP | Google Developer Expert | Go
918 
8
918 
918 
8
tech architect, tutor, investor | GCP 12x certified | youtube/AwesomeGCP | Google Developer Expert | Go
"
https://medium.com/automation-generation/algorithmic-trading-automated-in-python-with-alpaca-google-cloud-and-daily-email-notifications-422b7c6b7c53?source=search_post---------19,"There are currently no responses for this story.
Be the first to respond.
This is your last free member-only story this month. Sign up for Medium and get an extra one
Top highlight
The purpose of this article is to provide a step-by-step process of how to automate one's algorithmic trading strategies using Alpaca, Python, and Google Cloud. This example utilizes the strategy of pairs trading. Please reference the following GitHub Repo to access the Python script.
Alpaca is a commission-free* brokerage platform that allows users to trade via an API. Once you have created an account you will be given an API Key ID and a secret key which you will reference in the Python script. This will create the bridge to automate your trading strategy.
*Commission-free trading means that there are no commission charges for Alpaca self-directed individual cash brokerage accounts that trade U.S. listed securities through an API. Relevant SEC and FINRA fees may apply.
Get started with Alpaca
Google Cloud Platform (GCP) is a top-tier cloud computing service. They offer hundreds of cloud products. However, for the purpose of this project, you will only need to use two GCP services. If you’re new to Google Cloud, you can take advantage of the free trial for new users that comes with $300 credit (more than you will need to automate this process). Leveraging a cloud service, such as Google, means that you won’t have to manually run your script — or be worried about your computer being on at the correct time each day.
Get started with the Google Cloud Platform.
Once you have created your account, start the free trial. Do this by clicking on “Activate” in the top right corner.
Fill out the necessary information, then create a “New Project”. In this example, it will be labeled, “Algo-trading”.
The next few steps will go over how to structure the Python script, attach the Alpaca API, send an email notification, and an example of how to build trading logic. The first thing to remember with the Python script is that you will need to create only one function. That function will then be called in Google Cloud. This is crucial to automate the script.
You can access your Alpaca API keys from the Alpaca Dashboard, once your account is set up. This example will be shown using the paper trading keys. These can be found on the right side of the dashboard, and below the API Key ID is your very own secret key.
You will need to import the following packages: os, and alpaca_trade_api as tradeapi. Follow Alpaca’s documentation on how to download the alpaca_trade_api package. Below is a snippet of the code found in the GitHub Repo.
The os.environ section allows you to specify which environment you are connecting to — paper trading or live trading.
The first blacked out area is where you will place your API Key ID, the second is where you will place your secret key.
The account variable is making sure that you have an account with Alpaca and that it is active. This variable will be used later on.
Adding email notifications to your trading script are subjectively awesome—they enable you to know when your script is running and what the outcome is based on the trading strategy.
To get started with adding notifications, please check out this article by Samuel Sam!
For security reasons, you’ll likely want to create a new email account (in this example Gmail is used, and named “Trading Bot”). The most important step, whether you create a new account or not, is turning on access to less secure sites. Account Settings → Security → Less secure app access ON.
The code snippet below includes the packages necessary to send emails. The subject of the email will be your trading strategy, which will allow you the capability of running multiple trading strategies while staying informed on their success.
The following example code snippets show a very sloppy way of accessing the free data on Alpaca through IEX Exchange. Just create the needed variables for the trading logic, such as 5-day moving averages and certain spreads.
This example is using ADBE and AAPL. This code is available in the GitHub Repo.
Next, add the trading logic with the desired text for the email. The portfolio variable checks to see what your current position is—which is important to the trading algo logic.
The clock variable allows you to easily check if the market is open. If it isn’t, you’ll receive an email that says, “The Market is Closed”. In the future, it might be more beneficial to run a script prior to this to check if it is open or closed, rather than correspond via email. You can have this script run in the cloud, saving computing time and money.
The mail_content variable is written throughout the trading algorithm so that it catches whatever occurs dependent on the day. This is then placed in the email, the information for the login is fulfilled and the email is sent.
The last variable is created which is “done” and that is what the function spits out. Your script is ready to go!
Next, open up Google Cloud console. Make sure that you are in your algo-trading project and then navigate to Cloud Functions on the left side panel, found under compute. You can always pin it for ease (shown below).
Click “Create Function” at the top. This will take you to a page that looks like this.
Name the function whatever you would like. You can keep all the preset settings, but change the Runtime to Python 3.7. Then, copy and paste the Python script you have created into the main.py. Make sure to put the name of the function in the Function to Execute box.
The last step is to add the necessary packages in the requirements.txt. This makes it so the cloud knows what packages you need for your Python script to run. Go ahead and deploy the function.
**Note: You may need to add other packages to this list if you add them to your main.py. Your function will not deploy if there are any packages missing from the requirements.txt.
To test the function, navigate to the Trigger tab within the function.
Click the URL and see what happens. If it runs correctly, the window should display, “Mail Sent” and you will receive an email. If this works properly, copy the URL.
Navigate to the Google Scheduler on the left panel. It is located under “Tools”.
Create a new job.
Your screen should look like this:
Name it whatever you would like. For frequency, this sample function runs every weekday at 8:30 AM MST (an hour after the market opens).
Paste the URL that you copied from the Cloud Function. After this, you’re set! This will run, as you have specified, on its own. You can test the job by clicking on “Run Now”.
If it runs successfully, you will have a nice email sitting in your inbox similar to the one earlier, and you’ll be making trades (and maybe money**!) in Alpaca.
This step-by-step example displayed how to automate your favorite algo trading strategies—hope you enjoy the email notifications! If you have any suggestions on how to improve this process or the trading strategy itself, send a message or leave a comment.
***All investments involve risk and the past performance of a security, or financial product does not guarantee future results or returns. There is always the potential of losing money when you invest in securities, or other financial products.
Technology and services are offered by AlpacaDB, Inc. Brokerage services are provided by Alpaca Securities LLC (“Alpaca”), member FINRA/SIPC, a wholly-owned subsidiary of AlpacaDB, Inc.
News and thought leadership on the changing landscape of…
833 
5

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
833 claps
833 
5
Written by
Learn | Build | Elevate
News and thought leadership on the changing landscape of automated investing. Changing the market one algorithm at a time.
Written by
Learn | Build | Elevate
News and thought leadership on the changing landscape of automated investing. Changing the market one algorithm at a time.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://towardsdatascience.com/machine-learning-with-tensorflow-on-google-cloud-platform-code-samples-7c1bc07cd265?source=search_post---------20,"Sign in
There are currently no responses for this story.
Be the first to respond.
Lak Lakshmanan
Feb 24, 2018·4 min read
Over the past few months, my team has been working on creating two 5-course specializations on Coursera called “Machine Learning on Google Cloud Platform” and “Advanced Machine Learning on GCP”. The full 10-course journey will take you from a strategic overview of why ML matters all the way to building custom sequence models and recommendation engines.
These courses provide an interactive, practical, pragmatic way to get started doing ML quickly and effectively. While there are many theoretical machine learning courses, my goal with this specialization is to provide practical training, so that you can hit-the-ground running. In order for you to get that jump start, the courses come with lots of open-source, example TensorFlow applications that you can take and train/deploy immediately.
The first course in the Machine Learning on GCP series. How Google Does ML, is now live on Coursera. Please go take the course, and take the remaining courses as they show up once every few weeks.
But even as you are waiting for the awesome team of presenters to teach the courses, the source code for the labs in the specialization is already available. In this blog post, I’ll list what is available in each folder of the GitHub repo:
data_analysis.ipynb shows you how to do data analysis on huge datasets:
mlapis.ipynb shows you how to invoke pre-trained ML models:
repeatable_splitting.ipynb illustrates the importance of repeatable splitting of data
create_datasets.ipynb shows you how to explore and create datasets using Pandas and BigQuery
a_tfstart.ipynb shows you how to use TensorFlow as a numeric software package
b_estimator.ipynb shows you how to write simple ML models in TensorFlow
c_batched.ipynb shows you how to work with large datasets in TensorFlow
d_traineval.ipynb shows you how to do distributed training with TensorFlow
debug_demo.ipynb shows you how to debug TensorFlow programs
e_cloudmle.ipynb shows you how to deploy TensorFlow models and predict with them in a serverless way, with Cloud ML Engine
a_features.ipynb illustrates the importance of representing features correctly
dataflow shows you how to use Apache Beam on Cloud Dataflow for preprocessing
taxifeateng shows you how to implement feature engineering in TensorFlow models
a_handtuning.ipynb shows you how to change a variety of parameters associated with TensorFlow models to gain greater accuracy
b_hyperparam.ipynb shows you how to autotune your TensorFlow models on Cloud ML Engine so that you don’t have do hand-tuning.
c_neuralnetwork.ipynb shows you how to train and predict with distributed neural network models in TensorFlow the easy way.
d_customestimator.ipynb shows you how to take a model that you find in a paper and implement in a way that is distributed and scaleable.
This set of notebooks:
1_explore.ipynb shows you how to explore data using Pandas and BigQuery
2_sample.ipynb shows you how to repeatably split the data
3_tensorflow.ipynb shows you how to build an Estimator model on the data
4_preproc.ipynb shows you how to preprocess data at scale using Dataflow
4_preproc_tft.ipynb shows you how to preprocess data at scale using TF.Transform
5_train.ipynb shows you how to train a TensorFlow model on Cloud ML Engine
6_deploy.ipynb shows you how to deploy a trained model to Cloud ML Engine
serving shows you how to access the ML predictions from web applications and from data pipelines.
Together the above 7 labs summarize the lessons of the previous six courses on a realistic problem, taking you from data exploration to deployment and prediction.
No labs with this one — this is about design and architecture considerations around ML models.
mnist_estimator.ipynb shows you how to build an image classifier using the Estimator API
mnist_models.ipynb shows you how to build a custom estimator with all the tricks (convolutional layers, augmentation, batch normalization, etc.) that go into a good image classification model
flowers_fromscratch.ipynb shows you how to apply the image model in the previous notebook to “real” images.
Labs on Transfer learning and AutoML are not in GitHub since they don’t involve any coding — just point & click!
sinewaves.ipynb show you how to build a time-series forecasting model in TensorFlow using a variety of techniques including CNNs and LSTMs.
temperature.ipynb illustrates how hard LSTMs can be to get right
txtcls1.ipynb shows you how to build a from-scratch text classification model using a variety of techniques including CNNs and LSTMs
txtcls2.ipynb shows you how to use pretrained word embeddings in a text classification model.
word2vec.ipynb shows you how to create a word embedding from your own dataset.
poetry.ipynb shows you how to use Tensor2Tensor to solve your own text problems, whether it is text summarization or text generation
content.ipynb shows an example of a content-based recommendation system
wals.ipynb shows you how to build a collaborative filtering recommendation system in TensorFlow
wals_tft.ipynb makes the collaborative filtering model production-ready by adding in a tf.transform pipeline to map unique user-ids and item-ids automatically.
As ML matures, there will be more labs, of course, and some of the labs above may prove unnecessary. The GitHub repo is a living repository and we plan to keep it up-to-date and reflective of recommended TensorFlow practice. So, if you are building your own ML models, the notebooks (and their corresponding model directories) are a great starting point.
Happy exploring!
Data Analytics & AI @ Google Cloud
826 
7
826 
826 
7
Your home for data science. A Medium publication sharing concepts, ideas and codes.
"
https://medium.com/@sathishvj/on-passing-all-google-cloud-certifications-54b2cc1e428c?source=search_post---------21,"Sign in
There are currently no responses for this story.
Be the first to respond.
sathish vj
Jan 8, 2019·15 min read
Subscribe to my YouTube channel that teaches you to apply Google Cloud to your projects and also prepare for the certifications: youtube.com/AwesomeGCP. Check out the playlists I currently have for Associate Cloud Engineer, Professional Architect, Professional Data Engineer, Professional Cloud Developer, Professional Cloud DevOps Engineer, Professional Cloud Network Engineer, and Professional Cloud Security Engineer.
My experience with and topics for the Profession Data Engineer, Professional Cloud Architect, beta Professional Cloud Developer, Associate Cloud Engineer exams, beta Professional Network Engineer, the beta Professional Security Engineer, the Professional Collaboration Engineer and the G Suite exams. Edit: it’s not “All” certifications anymore as some new ones have been released.
If there was one caption to describe my experience writing each of the google cloud certification exams, it is this: confident before, doubtful during, relieved after. I would also prefix a ‘very’ to each of those parts. That should give you a view into what the exam is like. There’s quite a bit to study before, for sure. Having covered that reasonably, I walk into the exam with confidence. But the exam is not easy — non-direct questions, multiple areas combined into one question, “out-of-syllabus” questions on general concepts, questions/options about obscure sub-services and details that I’ve never heard of before, and finally, answer options that are all right but we have to pick the ‘best or the recommended way’. During each exam, I honestly wasn’t sure if I would pass. And it was a big relief to see a “Provisional Result: PASS” at the end each time. Whew!
But let’s step back a bit. And let me take you through my experience in a little more detail. Today, I have all of the Google Cloud certifications: Professional Data Engineer, Professional Cloud Architect, and Associate Cloud Engineer. I also wrote the beta Professional Cloud Developer but the results will be out only a few months from now. (Update on Jan 24th 2019: the Professional Cloud Developer exam is out of beta now and I passed!)(Update on March 13th 2019: the Professional Network Engineer exam is out of beta now and I passed!)(Update on March 29th 2019: the Professional Cloud Security Engineer exam is out of beta now and I passed!)(Update in January 2020: I got the Professional Collaboration Engineer and also the G Suite certification.)
I wrote the associate engineer exam in mid November, 2018. Then bunched up the remaining 3 for the two weeks when I got time to myself to prepare around Christmas and New Year. It worked for me, luckily. And i think there is a reason this worked. Read on.
I don’t think so. I don’t think it is easy to pass all exams without wide knowledge of software development and systems in general and some knowledge of almost all offerings from Google Cloud — and there are over a hundred of them. The exams don’t go very deep in all of them (a mile wide and a few inches deep), but you will need to understand enough to know the concepts and best/recommended practices. People write blog posts only about their passing, but I really wonder how many failed. I’ve seen only a few people openly discussing how they flunked their exam the first time and how they are preparing again for it.
Without realising it, for quite a while. The certifications were not in my sights until the last quarter of 2018. However, I’d been using GCP for projects for a few years now. A bunch of my hackathon projects, personal projects, and static web sites (all from the early part of this decade) were made on Google AppEngine Standard. I had completed almost every Coursera specialisation on GCP starting from end 2017. I’d also used Firebase for a few of my Ionic/Angular projects. I owned domain names that I’d setup with GSuite and created AppEngine projects within them. I’d also done some work on Kubernetes and Docker during my stint at Red Hat. From early on in 2018, I had also gluttonously devoured (went super fast through) Andrew Ng’s machine learning courses on Coursera followed by Google’s specialisation in Machine Learning and also the Google Machine Learning Crash Course. So I had been learning merely for the sake of figuring out these technologies and using GCP for my projects for more than a year. The certifications weren’t yet a goal.
My original plan somewhere around October, probably, was to write the Professional Cloud Architect (PCA) only. Then I found the Associate Cloud Engineer (ACE) certification and thought it might be a simpler first step and digressed to write that first. Since the time I decided to attempt the certification, I prepared across two+ months for all four certifications. Within that, I actually got less than a month because I was traveling around visiting various clients. For the Architect exam, I had been preparing little by little with a burst of activity just prior to the exam. I hadn’t done too much prior work in the Professional Data Engineer (PDE) subjects, so I spent about two focused weeks, studded with a few Christmas and New Year parties, on preparing for the Data Engineer exam. I didn’t really prepare for the beta Professional Cloud Developer (bPCD) because there wasn’t any specific material to prepare with.
One for all, all for one.
An aspect that clearly stood out for me was the overlap across all the certifications. If you want to pass any one exam, you have to study for all of them. IAM, GCS, and Stackdriver are universal — every exam had these sections prominently. (Stackdriver questions were a little less in my Data Engineer exam, but there still were a few.) Beyond that, every exam had a piece of every other one. My Professional Architect exam had quite a few questions on Datastore. Usage and setup of Dataproc, Dataflow, and Pub Sub was everywhere. Firebase is not part of the exams, but they featured in some of the options. Dataprep and Datastudio featured in the developer exams. You might not fully understand the Data* products, but you have to know how they are deployed and the use cases where they are best employed. BigQuery was there in the ACE, PCA, PDE, bPCD, but probably a little less in the PCA. Kubernetes was there in all except for the PDE.
Given the considerable overlap, my key advice is to watch all tutorials/training videos for all certifications, and then focus on practice questions for the specific exam you intend to take.
To pick up from where I left off earlier, taking all certifications wasn’t my goal. Having skimmed through the courses though, I realised that there was way more to GCP than just the parts I was repeatedly using. So to get a better understanding of the possibilities and arm myself with a wider repertoire of tools, I decided that I’d try the Professional Architect exam. I then realised that there was considerable overlap between the exam content. I checked the ACE exam and realised that because I’d coded GCP projects already, maybe the AE exam would be an easier intermediate step combined with the learning I’d done for the PCA. So the ACE certification happened. Later when I got time, I wrote the PCA and passed that. Meanwhile, a client of mine wanted some help with data engineering on GCP and I wondered if I could do the PDE also. In learning for that, I again realised that I could, with the knowledge from ACE+PCA+PDE and maybe just a little more, I could probably attempt the bPCD exam. So that happened quite suddenly — I just booked it one day and went for it the next. For the bPCD, there wasn’t any material or questions or courses online yet, so there was nothing I could specifically prepare for. Finally, I also took the PDE certification, which required the most learning for me. But having written all the other exams, the PDE was way less daunting.
Edit 23rd April 2020: It has been more than a year since I wrote this. I see more courses now and more practice exams. These didn’t exist when I was writing the exam. Even the courses that I took at the time might have improved or become worse with time. So, please take this in the context of the time.
I went all out on courses, because I also had an agenda to evaluate them and suggest them to clients I’m working with. Coursera is the closest to the exam. My wild guess is that the original exams were created/reviewed by some of those who created the Coursera training material. However, often they barely skimmed the surface on some topics. But then, no course I have seen so far comprehensively covered all the material related to the exam questions. All of them were ‘introductory’ in nature. On the exam, every once in a while the questions were from a remote corner of the documentation. GCP products and services that were not even mentioned in the courses were part of the options. The documentation, therefore, is the most comprehensive but you clearly can’t go over it all. I also went through the Linux Academy and Cloud Academy courses. Both were useful; incrementally so from the Coursera one. Yet, some of those modules were very useful because the instructors there have taken the exam and they give quite a few tips based on their experience. (For tips based on my experience, follow the links at the end.) The Linux Academy course also has a good readout and analysis of the case studies which were very useful. The practice questions at the end of these courses were too simplistic and unlike the actual exam questions. The exam questions are convoluted and long, while the practice tests are straightforward. If you go into the exam with only that practice, you’re going to be badly surprised. The practice test on the google certification site is the closest to the real one. Take that and get used to those lengthy questions. I had to spend quite some time reading and re-reading some of the 50 questions in the real exam and took the full 120 minutes to finish, whereas I could finish a 50 question practice test on some of these online courses in 15 minutes. I tried a few courses and practice tests on Udemy. Almost all of them were derivative or ridiculously bad. There was one PCA practice test set which was reasonable. A bunch of questions in this were copied from other places and sometimes (I felt) the answers were wrong, but still.
After all the certifications, my best guess would be - below average. I thought I knew GCP reasonably well, but now that I know more, I realise I know less. There is so much more to GCP than I will ever be able to grasp and they are releasing new products and services and updates to them regularly. Some parts of the internal GCP engineering is nothing but astounding, but we’re not even talking about internals of that sort. Even at a high level, my ignorance is substantial. I haven’t personally worked on Hadoop+Spark=Dataproc except in small doses. I haven’t worked with Apache Beam yet, but I like it now and I’m keen to work on it especially since it also has a Go library now. My knowledge of Machine Learning is middling with only a little knowledge of Tensorflow, but I have coded full fledged applications that use Cloud Vision and AutoML. I’ve done projects on AppEngine standard but not Flexible. I’ve done a little bit of DataPrep and DataStudio, but not much. This list goes on and on. Then there are products that I’ve never used in any of my projects yet — like Cloud Armor, BigTable, Build. My hope is only to continue learning, work on small projects, and keep trying out what I see in blogs and tutorials. What the certifications have given me though, apart from a slap of humility, is a wide view of what is possible with GCP. Instead of a hammer that I used everywhere previously, I now have a wider array of tools and better knowledge about tools (if not in-depth knowledge of tools) to make architectural choices from.
What happens if the time runs out and I haven’t submitted the responses?
I didn’t know the answer to this, so I asked the Google Certifications support. I got this response: “your responses will be automatically be recorded and submitted. Please allow 7 days for Google to confirm receipt of your exam results. Upon receiving your record, Google will complete an evaluation of your exam record including compliance with the Terms and Conditions.”
I got a provisional result immediately and it’s been a few days. Why haven’t I got the mail with the certification?
I got some certification mails immediately (of course, this does not apply for any of the beta tests), while some others took a few days. As seen in the response to the above question, the final certificate could take as long as a week. When I didn’t receive the mail for a few days, I started questioning myself whether I had actually seen a provisional Pass result or whether I had just imagined it. If you want to calm yourself, you can login to the webassessor site where you registered for the exam and see your provisional result.
Edit 2019/11: Which exam should I take first?
I would strongly recommend that you take the Associate Engineer exam first.
I’ve seen people feeling devastated and demotivated after taking one of the Professional exams and flunking. My suggested path for them is to try the ACE first and then come back to the suitable professional exam.
How long will it take for me to study for the exam?
It depends. There are people who have apparently passed it with a week of study and others who have flunked after months of study. So it depends on how much you already know, whether you have the relevant experience, how much you prepare, and your ability to take multiple choice tests well. All of that, as you can see, depends on you.
For those appearing for the various certification exams, here is a list of sanitized notes (no direct question, only general topics) about the exam.
Overall notes across all GCP certification exams
Notes from the Professional Cloud Architect exam
Notes from the beta Professional Cloud Developer exam
Notes from the Professional Data Engineer exam
Notes from the Associate Cloud Engineer exam
Notes from the beta Professional Cloud Network Engineer Exam
Notes from the beta Professional Cloud Security Engineer Exam
Notes from the Professional Collaboration Engineer Exam
Notes from the G Suite Exam
Notes from the Professional DevOps Engineer Exam
Notes from the Professional Machine Learning Engineer Exam
What next for me on GCP? Keep using GCP and keep learning. Apart from that, I’m hoping to train more people in using Google Cloud, getting their certifications, and do short term consulting. If there are such opportunities, do reach out to me on LinkedIn.
p.s. for those writing their own recap after the exam, please note that you can only discuss the topics as given in the exam outline. I got a mail from the Google Certification team not to disclose points like the number of questions per topic or paraphrasing the questions. I edited out the few instances I’d mentioned the number of questions I got, even if it was descriptive phrases like “many questions”, “hardly any questions”, “no questions”.
A collection of posts, videos, courses, qwiklabs, and other exam details for all exams: https://github.com/sathishvj/awesome-gcp-certifications
Collection of free QwikLabs codes for practice: https://medium.com/@sathishvj/qwiklabs-free-codes-gcp-and-aws-e40f3855ffdb
Check the FAQs here: https://medium.com/@sathishvj/frequently-asked-follow-up-questions-on-google-cloud-gcp-certifications-438e1addb91d.
Wish you the very best with your GCP certifications. You can reach me at LinkedIn and Twitter. If you can support my work creating videos on my YouTube channel AwesomeGCP, you can do so on Patreon or BuyMeACoffee.
tech architect, tutor, investor | GCP 12x certified | youtube/AwesomeGCP | Google Developer Expert | Go
871 
16
871 claps
871 
16
tech architect, tutor, investor | GCP 12x certified | youtube/AwesomeGCP | Google Developer Expert | Go
About
Write
Help
Legal
Get the Medium app
"
https://servian.dev/google-cloud-data-engineer-exam-study-guide-9afc80be2ee3?source=search_post---------22,"Sign in
Guang X
Jan 17, 2019·2 min read
I recently studied for and successfully passed the Google Cloud Professional Data Engineer certification/exam. This is a 12-page exam study guide that I personally compiled and used in the hope of covering all the keys points. Below you’ll find a preview of the study guide, but you can click here to download full PDF file.
I hope this helps anyone looking to sit their certification!
Note: This is a good way to recap, but you may still need to read original documentation for each product to learn new concepts etc. And remember — there’s no substitute for hands-on experience with the tools. 😀
To find out more about Servian’s GCP capabilities, see here.
Pdf File: https://github.com/xg1990/GCP-Data-Engineer-Study-Guide/blob/master/GCP%20Data%20Engineer.pdf
Thanks Graham Polley for helping prepare my first Medium article!
Data Platform Engineer @ New Aim Pty Ltd. https://linkedin.com/in/xu-guang/
824 
6
824 claps
824 
6
At Servian, we design, deliver and manage innovative data & analytics, digital, customer engagement and cloud solutions that help you sustain competitive advantage.
"
https://levelup.gitconnected.com/running-a-scraping-platform-at-google-cloud-for-as-little-as-us-0-05-month-6d9658982f04?source=search_post---------23,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
I was recently faced with the problem of finding an apartment in Berlin. Following my previous experience in this same effort, I decided to automate the task and write a software to send me an alert of the best deals. In this article, I explain how I built the foundations of this platform.
The platform I've written is a Go application deployed to Google Cloud using Terraform. Also, it has Continuous Deployment from a private GitHub repository.
After a quick research, I came to the following list of platforms to monitor:
A few hours later, I have a Go binary that does everything I need to run the application locally. It uses a web scraping framework called Colly to browse all the platforms listings, extract basic attributes, and export to CSV files in the local filesystem.
Since I didn’t want to maintain the application running locally, my first choice would be to get a cheap instance at Google Cloud. Once I had this rented virtual machine, I could write a startup script to compile the app from GitHub, and set up a crontab to scrape the platforms on a daily basis.
Probably the best decision for this specific project, but could I use this personal problem as an opportunity to explore the integration of Google Cloud services?
Since, in the past, I was involved in multiple projects involving some sort of scraping application, I believed it was worth the effort. I could easily reuse this setup in the future.
My architecture started with a few premises:
My hypothesis was that I didn't need a virtual machine running 24/7; thus, it should not cost the same as a full month price. In fact, my application was able to download all the properties I was interested in under 3 minutes, so I expected something significantly lower.
My exploration through the latest Google Cloud services resulted in finding Cloud Run, a service that “run(s) stateless containers on a fully managed environment or in your own GKE cluster.” Still classified as a beta product by Google Cloud, it is built on top of Knative and Kubernetes. The key proposal is its pricing model: it charges in chunks of milliseconds rather than hours of runtime.
With a few tweaks, my Go application was wrapped in a Docker container to be runnable by Cloud Run. Once it gets a HTTP POST request, it collects attributes from all the advertised properties and publishes as CSV files to a Google Storage bucket. For my use case, I created two possible ways to hit this endpoint: an Internet-accessible address so I can trigger it whenever I want, and through Cloud Scheduler, which is configured to hit it once a day.
The application is fairly simple: it consists of an HTTP server with a single endpoint. On every hit, it scrapes all the platforms and saves results in CSVs inside a Storage bucket.
Other application files can be found in this Gist. All the feedback is appreciated, as this is one of my first Go projects.
Now with permissions already given, use Terraform to set up the rest of the infrastructure.
$ cd deployment$ terraform init$ terraform apply
The initial deployment may take about five minutes since Terraform waits for Cloud Run to build and start before configuring Cloud Scheduler.
Since Cloud Run is still in beta — with API endpoints in alpha stage —I was not able to declare all the infrastructure in Terraform files. As a temporary workaround, I’ve written a couple of auxiliary bash scripts that trigger the Cloud API through its CLI command. Fortunately, all this happens in background when a developer triggers terraform apply.
Every day, without any human interaction, Cloud Scheduler creates a new folder with a number of CSV files with the most recently available apartments in my city.
Not all the services in use are available in the official calculator. Either way, I've made a rough estimation for my personal use, considering an unrealistic number of one deployment each day.
For comparison, an f1-micro instance — with 0.6GB of RAM — running over a full month on Google Cloud, is included in the free tier; a g1-small instance, with 1.7GB, would cost US$ 13.80 per month. Also, it is reasonable to consider the cost could decrease or increase depending on how accurate were my initial assumptions and further optimizations.
Coding tutorials and news.
1K 
7
Public domain.

A monthly summary of the best stories shared in Level Up Coding Take a look.
1K claps
1K 
7
Written by
Everyday software engineer, forever mathematician. https://iriomk.com/blog/
Coding tutorials and news. The developer homepage gitconnected.com && skilled.dev
Written by
Everyday software engineer, forever mathematician. https://iriomk.com/blog/
Coding tutorials and news. The developer homepage gitconnected.com && skilled.dev
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/no-localhost-no-problem-using-google-cloud-shell-as-my-full-time-development-environment-22d5a1942439?source=search_post---------24,"There are currently no responses for this story.
Be the first to respond.
I recently switched to a Chromebook full time (the awesome Pixelbook), and the first question I get asked is “How do you do any development work on it?”
While Chromebooks are getting more powerful local development (a la Crostini), I’ve actually been using Google Cloud Shell as my primary development environment. In fact, I haven’t opened a local shell for months now.
So far, the experience has been awesome, and I’ve learned a bunch along the way. I wanted to share some tips, here they are!
Cloud Shell is a free terminal that you can use for whatever you want, and it runs 100% in a web browser.
Click this link to open Cloud Shell: https://console.cloud.google.com/cloudshell
Seeing how most of my work requires the internet, I’m completely fine with being tied to a browser.
Cloud Shell comes with most of the tools I use on a daily basis right out of the box. These include gcloud, node, kubectl, docker, go, python, git, vim, and more.
There is 5GB of persistent storage that is tied to your $HOME directory, and this is also completely free.
The other really nice thing is that you are automatically authenticated to the current Google Cloud project you’re working in. This makes setup super easy, everything just works!
Most people developing cloud apps usually run a web server of some sort. Usually, you would run it locally and just type in “localhost” into your web browser to access it.
This is not possible with Cloud Shell, so the team created a neat “web preview” function that creates a URL on the fly to point to your local server.
You can open up any port from 2000 to 65000, and your web traffic will come through!
Warning: There is some transparent auth done behind the scenes, so the URL the Cloud Shell opens might not work if you give it to someone else. I’d recommend a tool like ngrok if you want to share your “localhost” connection remotely.
By default, Cloud Shell runs on a g1-small VM, which can be under-powered for some tasks. You can easily upgrade to a n1-standard-1 by enabling “boost mode.” It’s like the TURBO button on an old PC, but this time it actually works :)
Yes yes, vim and emacs and nano are great and all. But sometimes you just want a nice, comfortable GUI to work with.
Cloud Shell ships with a customized version of the Orion editor.
While its not as good as VS Code or Eclipse, its actually a fully featured editor and I feel quite productive with it!
If you have files locally you want to upload to cloud shell, just click the “Upload” button in the menu and choose your file.
To download files, run this inside Cloud Shell:
And it will download the file!
Because Cloud Shell only persists your $HOME directory, if you install things that don’t come out of the box with Cloud Shell, chances are it will be gone the next time you use it.
If you install things with apt-get, there really is no good solution. However, if you are downloading the binary directly or are compiling from source, you can create a path in your $HOME directory (for example, /home/sandeepdinesh/bin) and add that to your PATH. Any binaries in this folder will run like normal, and they will be persisted between reboots.
If you have a git repository in a public place (like a public GitHub repo), you can actually create a link that will open the end user’s Cloud Shell and automatically clone the repo to their $HOME directory.
cloud.google.com
Because it is all free, it is a great way to get someone up and running with your project without having to worry about their local machine!
I’ve had a ton of success using Cloud Shell as my primary dev environment. If you are using a Chromebook or if you just want a free Linux shell that you can access from any browser, I’d definitely check out Cloud Shell.
There are a ton of new features coming down the line as well, I’ll be sure talk about them when they come out!
Google Cloud community articles and blogs
803 
11
803 claps
803 
11
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@sathishvj/notes-from-my-google-cloud-professional-cloud-architect-exam-bbc4299ac30?source=search_post---------25,"Sign in
There are currently no responses for this story.
Be the first to respond.
sathish vj
Jan 8, 2019·6 min read
Subscribe to my YouTube channel that teaches you to apply Google Cloud to your projects and also prepare for the certifications: youtube.com/AwesomeGCP. Check out the playlists I currently have for Associate Cloud Engineer, Professional Architect, Professional Data Engineer, Professional Cloud Developer, Professional Cloud DevOps Engineer, Professional Cloud Network Engineer, and Professional Cloud Security Engineer.
Immediately after the exam I do a memory dump as notes. Hence it is also quite unordered. This is a sanitized list that gives general topics and questions I encountered. The intention is not to give you the questions, but to give you topics that you can be prepared for. I was often stumped by some questions; hopefully you can be more prepared based on my experience. Wish you the very best!
This was the exam I originally planned to take first, but then I completed the Associate Cloud Engineer first. The notes I have on this seem to be fairly thin. So it’s kind of automatically sanitized and doesn’t divulge much details.
Google Cloud Certified — Professional CloudArchitect
For those appearing for the various certification exams, here is a list of sanitized notes (no direct question, only general topics) about the exam.
Overall notes across all GCP certification exams
Notes from the Professional Cloud Architect exam
Notes from the beta Professional Cloud Developer exam
Notes from the Professional Data Engineer exam
Notes from the Associate Cloud Engineer exam
Notes from the beta Professional Cloud Network Engineer Exam
Notes from the beta Professional Cloud Security Engineer Exam
Notes from the Professional Collaboration Engineer Exam
Notes from the G Suite Exam
Notes from the Professional DevOps Engineer Exam
Notes from the Professional Machine Learning Engineer Exam
Main Link — https://cloud.google.com/certification/cloud-architect
Topics Outline — https://cloud.google.com/certification/guides/professional-cloud-architect/
Practice Exam — https://cloud.google.com/certification/practice-exam/cloud-architect
A collection of posts, videos, courses, qwiklabs, and other exam details for all exams: https://github.com/sathishvj/awesome-gcp-certifications
I’ve collected here a bunch of free Qwiklabs codes which are awesome to get lots of hands-on practice. Use them well.
medium.com
Check the FAQs here: https://medium.com/@sathishvj/frequently-asked-follow-up-questions-on-google-cloud-gcp-certifications-438e1addb91d.
Wish you the very best with your GCP certifications. You can reach me at LinkedIn and Twitter. If you can support my work creating videos on my YouTube channel AwesomeGCP, you can do so on Patreon or BuyMeACoffee.
tech architect, tutor, investor | GCP 12x certified | youtube/AwesomeGCP | Google Developer Expert | Go
712 
21
712 
712 
21
tech architect, tutor, investor | GCP 12x certified | youtube/AwesomeGCP | Google Developer Expert | Go
"
https://medium.com/google-cloud/serverless-on-google-cloud-platform-an-introduction-with-serverless-store-demo-41992dec085?source=search_post---------26,"There are currently no responses for this story.
Be the first to respond.
This document is the opening piece of the Serverless on Google Cloud Platform: an Introduction with Serverless Store Demo How-to Guide. It discusses briefly serverless computing and its patterns in the context of Serverless Store, a web e-commerce demo app for showcasing serverless products and services on Google Cloud Platform. Serverless Store is not an official Google product.
Serverless Store is featured in Google Cloud Global Digital Conference 2019. View the keynote and breakout recordings to learn more.
Deploying a service used to be a tremendous commitment: to get everything up and running, developers and operators have to build a server, setup OS and network, install a variety of dependencies and prepare for years (if not longer) of upgrade and maintenance ahead. Nowadays many developers choose to use virtual machines (VMs) provided by Google, Amazon, and other cloud service providers instead, leveraging their experience in hardware and networking for better, more secure and more reliable service deployments. Nonetheless, VMs are still servers; the fact that they are running on the cloud in a virtualized form does not eliminate heavy server management workload. Additionally, VM deployments require careful capacity planning and they are charged per-second; in other words, every idling core and unit of RAM are a waste of budget.
Serverless computing promises a pay-as-you-go future with (almost) no server management at all. Serverless platforms take the code from developers and perform all the deployment tasks (networking, dependencies, maintenance, etc.) automatically behind the scenes. The greatest advantage serverless computing provides is that the deployment scales itself without additional configuration; your apps will always have exactly what they need computationally (within an understandable margin of error, of course) when running.
This is not saying that serverless computing is an ideal solution for every use case. Going serverless implies that you trust Google Cloud Platform, AWS, or other cloud service providers to manage a large portion of your computing stack for you and expect them to perform as you hope, which may not be the case in some scenarios. More importantly, the architecture of your app has a great impact on how well serverless computing keeps its no-server-management easily-scalable promise: for example, if you use a VM-based database backend solution, such as a Cloud SQL instance, with your serverless function deployment, its scalability will be heavily limited by the performance of the SQL instance, and unexpected errors may occur if you are not careful enough.
The ever growing popularity of AWS Lambda and Google Cloud Functions leads many to believe that FaaS (Function as a Service) is a synonym for serverless computing. FaaS platforms take a function from developers, build it into an app, and deploy it in the cloud; it advocates a quite special pattern for app development, where an app is broken down into a collection of functions, grouped by a (managed) gateway, and accessed by users via static HTML files served online:
Effectively transforming the backend into an API service, this signature architecture enjoys all the benefits of serverless computing (little to none server management, scalability, low cost), and enables team collaboration on a different level (UI design, for instance, is now decoupled; team members may each work on their own functions as opposed to a full app). Many developers have successfully adopted the pattern and created compelling experiences with minimal amount of effort.
But there is a catch. This distinct design is, as with serverless computing in general, not perfect for all use cases, and bears the risk of gruesome fragmentation: without careful coordination developers may end up with a large number of functions difficult to assess, monitor, maintain, orchestrate, and keep a complete picture of, in many ways akin to a 100,000-piece jigsaw puzzle. The architecture is also fairly new; it takes some time for developers and teams to adapt to, and they may need to overhaul their workflows to accommodate.
Serverless and FaaS are not necessarily the same, and it would be dangerous to force a function mindset. It is recommended that developers experiment with serverless solutions and integrate them into their apps to the extent they are most comfortable with. For teams with a comprehensive knowledge of serverless computing, it might be OK to build a service from ground up using nothing but functions; those who are interested in but not yet very comfortable with serverless computing should, however, consider taking a hybrid approach and migrating some select compatible workload to serverless platforms first. Serverless is, after all, a flexible misnomer.
Generally speaking, code in an app is executed sequentially. The sequence (or execution path) is essentially a contract crystalized in the deployment, triggered by an input (request) and finishes with an output (response). The input, output, and the sequence are bundled inseparably together in the contract; once deployed, it cannot be modified. Developers will have to modify the code and re-deploy.
Event-driven computing plans to break the contract and free all the parts involved in the sequence. Parts now emit an event (a piece of message with execution contexts) at the time of completion; they cares little about what happens next, and leaves the following step at the discretion of whatever delivers the event, which, for cloud-native applications, is usually a data/event streaming solution, such as Google Cloud Pub/Sub and Apache Kafka. Parts that send events are event publishers, and those that receive events become event subscribers.
This publisher/subscriber pattern grants greater development freedom and easier team collaboration. It is now possible to hot plug/swap blocks of code without redeployment, and developers can easily add multiple subscribers to the same event stream, creating a fan-out structure. One of the most prominent use cases of such structures is real-time data analytics:
Traditionally, data analysts process data in batch, usually via auto-generated log files. The application executes a sequence, writes the details to logging agents (as a step of the sequence), and data analytics team extracts insight from them, with an understandable delay. In event-driven systems, however, once connected to the event stream as one of the subscribers, data analytics team can get the data they need in real time without interrupting normal operations. If connected to real-time data processing and warehousing solutions such as Google Cloud Dataflow and Google BigQuery, developers can have analysis done in real-time as well.
The data/event streaming solutions themselves also offer great help in application development. You can set up Google Cloud Pub/Sub, for example, to reattempt delivery automatically when one of the subscribers is experiencing temporary technical difficulties. Cloud Pub/Sub also supports snapshot, enabling developers to go back in time and replay events, which can be particularly helpful when testing new pieces of code.
Event-driven computing works particularly well with serverless computing. Events are natural triggers for serverless deployments; more importantly, the (almost) infinite scalability of serverless computing platforms allows message queues to pass events around as fast as possible, saving the trouble of capacity planning commonly witnessed in VM-based deployments.
Breaking the contract of sequential code execution has some serious complications, with the most important being the dissociation of inputs (requests) and outputs (responses). In the world of event-driven computing, requests and responses arrive in two separate dimensions; whoever sends the request are not naturally guaranteed a response. For asynchronous operations it is usually fine: when people post a new photo in their social feeds they usually do not expect it to show up in the feeds of their friends immediately; quick feedback is appreciated but not required. Synchronous operations, however, are a different story. When people open a website, they expect the page to load as quickly as possible; no other actions can be performed until the contents show up. Event-driven computing, unfortunately, does not work well in this scenario, as the request is considered served when the event is published; developers have to manually retrieve the response.
There are many solutions that address this issue, though none of them is perfect. Common strategies include using statically generated pages throughout the app, polling the system for results right after event publishing, and setting up dedicated asynchronous routines for synchronous operations when required (for example, when people request a webpage, prepare it synchronously first, then publish an event to the message queue so that event subscribers can track the action). Once again, it is up to developers themselves to decide how far they would like to go with event-driven computing; for some apps it may be a heavenly solution.
As introduced earlier, Serverless Store is an e-commerce app designed to showcase serverless products and services on Google Cloud Platform. Note that it is for demonstration purposes only and does not necessarily reflect best practices in the app development.
Serverless Store features a fully serverless architecture. It runs on two Google provided serverless computing platforms, Google App Engine and Google Cloud Functions, with all of its components, storage, data analytics, machine learning/AI, etc., supported by managed services. There are no VMs involved at all. The app is scalable, and you pay only what the app uses.
Serverless Store consists of both app and functions. It is a hybrid, featuring a standard (traditional, if you may) Python flask web app as the centerpiece, with many critical and supplementary features served by Cloud Functions. For people who prefer the FaaS pattern, with a relatively small amount of effort it is possible to make Serverless Store purely function-based; you can also easily revert it back to a web app with no Cloud Function workers at all, ready for VM deployment.
Additionally, Serverless Store adopts event-driven computing for all asynchronous operations in the app. Synchronous operations, such as listing products, are still performed in a sequential manner. It is hoped that this approach can help interested developers ease into event-driven computing, a fairly new pattern, and better discover its benefits and limitations.
You can view live demos of Serverless Store in these keynotes.
Download the project here.
Guides below introduces briefly each product and service used in the Serverless Store demo app and explains how they are integrated in the app. Follow them to set up Serverless Store:
See Further Discussions for some tips and notes about Serverless Store.
Watch Google Global Digital Conference 2019.
Google Cloud community articles and blogs
734 
3
734 claps
734 
3
Written by
Developer Relations @ Google Cloud Platform
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Developer Relations @ Google Cloud Platform
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/google-cloud-functions-for-go-57e4af9b10da?source=search_post---------27,"There are currently no responses for this story.
Be the first to respond.
In January 2019, Google Cloud Functions finally announced beta support for Go. Check out the official blog post for more details.
Let me start with a simple “hello world” to introduce you to the overall build+deploy experience. GCF expects an http.HandlerFunc to be the entry point. Create a package called “hello” and add a trivial handler:
In order to deploy, use the following command. It will create a new function called hello and will use HelloWorld as the entry point. The Go runtime to be used will be Go.11.
Deploying may take a while as it is noted. Once deployed, you will be able to see the HTTP endpoints on your terminal. You can also view your functions at the Cloud Console.
On the console, you can see “hello” function is deployed. You can access to logs and basic metrics such as number of invocations, execution time and memory usage.
If you have external dependencies, go.mod file will be used to get the dependencies. You can also vendor them under the function module. I imported the golang.org/x/sync/errgroup package as an example. See GCF guideline on dependencies if you need more information.
The dependency is going to be retrieved when I redeploy the function again.
Function is redeployed at https://us-central1-bamboo-shift-504.cloudfunctions.net/hello. See it yourself. You can also call the function from command line:
I also generated some load from my laptop to the function to provide you a more realistic response time data. I made 1000 requests, 10 concurrently at a time. You can see that there are some outliers but most calls fall into the 213 milliseconds bucket.
In Go, we organize packages by responsibility. This also fits well with serverless design patterns — a function is representing one responsibility. I create a new module for each function, provide function-specific other APIs from the same module.
The main entry point handler is always in fn.go, this helps me to quickly find the main handler the way main.go would help me to find the main function.
Common functionality lives in a separate module and vendored on the function package because GCF CLI uploads and deploys only one module at a time. We are thinking about how this situation can be improved but, currently a module should contain all of its dependencies itself.
An example tree is below. Package config contains configuration-related common functionality. It is a module and is imported and vendored by the other functions (hello and user).
Unlike other providers, we decided to go with Go idiomatic handler APIs (func(ResponseWriter, *Request)) as the main entry point. This allows you to utilize existing middlewares available in the Go ecosystem more easily. For example, in the following example, I am using ochttp to automatically create traces for incoming HTTP requests.
For each incoming request, an incoming trace span is created. If you register an exporter, you can upload the traces to any backend we support including Stackdriver Trace of course.
Google Cloud community articles and blogs
859 
19
859 claps
859 
19
Written by
See rakyll.org for more.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
See rakyll.org for more.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/stackblitz-blog/google-cloud-meet-stackblitz-da13f4e4fc29?source=search_post---------28,"There are currently no responses for this story.
Be the first to respond.
Big news! Today at Google Cloud Next in San Francisco we announced an exciting new partnership with our friends at Google Cloud. Starting with Google Cloud Platform’s (GCP) latest serverless offerings, our integration enables developers to go from idea to production — in just one click.
That’s a pretty tall order- spinning up production grade apps is a lengthy & painful process. You have to scaffold a new project, create a repo, install CLIs, configure & provision a deployment service, create build & deploy scripts, cross your fingers... and then spend hours tearing your hair out on StackOverflow.
Instead, wouldn’t it be great if you could just click a stack, select a cloud provider, choose a repo name and boom- you’ve now got an app that’s deployed instantly & scales automatically?
No configuration, no installations, no hassles. With one click on StackBlitz.com your new app is created, a production build is instantly deployed live to GCP, and a new GitHub repo is initialized for you to start working in:
And with GCP’s brand new Cloud Run, a managed serverless execution platform, you can now use any language & any framework to write serverless applications. So whatever your dream stack is, you can instantly spin up production ready, infinitely scalable apps with it on StackBlitz- in a single click.
Getting your local dev environment set up can be a high barrier, especially if you’re just starting out on GCP. That’s why we automatically spin up a preconfigured, finely-tuned dev environment powered by vscode that’s already booted & running w/ hot reloading for you—right in your browser:
Once you’ve put the final touches on your app, just click “deploy” and your app is instantly built into a production container image, uploaded to the Container Registry and is live on prod in mere seconds—no CLI installs/logins required:
Commit changes, create branches, push to GitHub, and even open pull requests- your same exact git workflow “just works”:
Along with serverless, we’ll also be bringing this entire end-to-end experience to other core GCP offerings like Kubernetes Engine, AI & Machine Learning, and Compute & App Engine. The barrier to entry for these tend to be a bit higher, which makes that instant experience feel even more like magic.
We’re working around the clock to launch this experience to all GCP developers. Drop your info in the beta signup form and we’ll send you an invite as soon as we’re out of alpha!
News and Engineering from the StackBlitz Team
1.98K 
7
1.98K claps
1.98K 
7
Written by
Software engineer & builder of @StackBlitz, @GoThinkster (acq) & https://realworld.io 🤘 Code the future!
News and Engineering from the StackBlitz Team
Written by
Software engineer & builder of @StackBlitz, @GoThinkster (acq) & https://realworld.io 🤘 Code the future!
News and Engineering from the StackBlitz Team
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/linux-gui-on-the-google-cloud-platform-800719ab27c5?source=search_post---------29,"There are currently no responses for this story.
Be the first to respond.
Top highlight
Having servers on the cloud is great: you can access them from anywhere and at anytime! You can easily ssh into your server and do whatever you want, right?
Well, what if you want to browse the web? I want to use Chrome! Or you could use Lynx … but I’ve heard it’s not CSS3 compatible. This is a little tutorial that will take you through all the steps to have GUI access to a Google Compute Engine instance.
Important: if you start an instance you’ll be charged per minute. Go to the end of this post to see how to stop it and get $300 in credits!
Update: My next post will discuss how to make this connection secure by using VNC over VPN.
Visit the developers console, log in, and create a project if needed clicking on the Create Project button. Navigate the left menu to list the VM instances running on your project: Compute > Compute Engine > VM instances. If it’s the first time you do this for this project it might take a bit longer, since it’s setting some stuff up, don’t worry this happens only once.
Click on the Create Instance button to access the instance creation form. Choose a name for your instance. Any instance type and linux distribution would work, but if you want to go with something safe choose n1-standard-1 with backports-debian-7-wheezy-v20150423.
Choose a zone close to you to have the best latency in your connection.
If you’d like to use Windows the instances already come with support for RDP (Remote Desktop Protocol) so you don’t need any extra steps.
Once your instance is running you can SSH directly into it by simply clicking the SSH button. This also handles the authentication for you, really handy!
Once connected let’s update our sources list and install some extra packages:
Before we continue configuring the VNC server that will allow us to access our instance through a desktop environment we should install one. You can install your favorite one:
a) If you like Gnome and are not in a hurry:
Gnome is beautiful … and heavy! So be patient while everything gets installed, two to five minutes is completely normal.
b) If you prefer something faster to install you might like Xfce:
Now that our instance has a desktop environment let’s make it accessible via VNC. Start the vncserver, and follow the directions to create a password
Note: this password will grant access to your instance, so make it strong.
If everything went fine your VNC server is now running and listening on port 5901. You can verify this with netcat from the Google Compute Engine instance:
There’s many options available, my favorite one is RealVNC Viewer. Install one but don’t try to connect to your server just yet: it will fail as the firewall rules don’t allow it.
In order to communicate with our instance we need its external IP. You can find it on the Developers Console.
Let’s try to connect to it using netcat again:
Regardless of the tool you use the connection will fail, this is expected as the firewall rules block all communications by default for security reasons.Let’s fix that.
Navigate to the configuration for the default network “Compute > Compute Engine > Network” and then click on default. Or you could also click here and choose your project.
We’re going to add a new firewall rule, pressing the corresponding button.
Choose a descriptive name for the rule.
We will allow traffic coming from any source, which is why we use 0.0.0.0/0, the IP mask equivalent to a wildcard.
The traffic will be on the port 5901 for protocol TCP and going to instances tagged as vnc-server.
The last step is to tag our instance as a vnc-server, for that go back to the VM description page and click on “add tags”
Let’s first of all make sure that the connection is now allowed by the firewall:
Great! Everything seems ready for our VNC client to connect. Open your VNC viewer and connect to the IP of your Compute Engine instance on port 5901.
To connect you’ll need to provide the password you gave at the beginning of this tutorial.
And voilà! You can now use your favorite Desktop environment on your Google Compute Engine instance.
If you still cannot connect to VNC after you have created a firewall rule you should make sure that your IP has not been banned by sshguard.
To see if this is the case you can run:
If your output differs from this one flush the table and retry:
An instance running on the cloud has a cost but the good news is that you can simply stop it and restart whenever you need it again. Click on the Stop button and you’ll be charged only for the associated disk which at the moment of writing of this article is 40¢ per month. I dare you finding a cheaper cup of coffee in San Francisco!
Finally, if you’re new to the Google Cloud Platform make sure to get into the Free Trial to access $300 in credit so you can try it out and have some fun!
I hope this was useful. Feel free to add any comments for feedback or questions on twitter.
Google Cloud community articles and blogs
1K 
41
Some rights reserved

1K claps
1K 
41
Written by
VP of Product at Dgraph 🏳️‍🌈 Catalan
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
VP of Product at Dgraph 🏳️‍🌈 Catalan
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@LightspeedHQ/google-cloud-spanner-the-good-the-bad-and-the-ugly-5795f37a7684?source=search_post---------30,"Sign in
There are currently no responses for this story.
Be the first to respond.
Top highlight
Lightspeed HQ
Sep 27, 2018·15 min read
Originally published on the Lightspeed HQ Blog.
As a company that offers multiple cloud-based POS solutions to retailers, restaurateurs and ecommerce merchants around the globe, Lightspeed employs several different types of database platforms for a myriad of transactional, analytical and search use cases. Each of these database platforms have different strengths and weaknesses. So, when Google introduced Cloud Spanner to the market — promising features unheard-of in the space of relational databases, such as virtually unlimited horizontal scalability and 99,999% service-level agreement (SLA) — we couldn’t pass up the opportunity to get our hands on it!
To provide a comprehensive overview of our hands-on evaluation of Cloud Spanner, as well as the evaluation criteria we used, we’ll cover these main topics:
Before delving into the specifics of Cloud Spanner and its similarities and differences with other solutions on the market, let’s talk about the principal use cases we had in mind when considering where to deploy Cloud Spanner within our infrastructure:
Note: For simplicity and ease of comparison, this article compares Cloud Spanner against MySQL variants of the GCP Cloud SQL and Amazon AWS RDS solution families.
In a traditional database environment, when database query response times get close or even exceed pre-defined application thresholds (mostly due to an increase in the number of users and/or queries), there are several ways to bring response times down to acceptable levels. However, the majority of these solutions involve manual intervention.
For example, the first step one would initiate is to look at different performance related settings of the database and tweak them to best fit the applications’ use case patterns. If that proves insufficient, one may have the option to scale the database vertically or horizontally.
Vertically scaling an application entails upgrading the server instance, usually by adding more CPUs/cores, more RAM, faster storage, and so on. Adding more hardware resources translates into increased database performance, measured mostly in transactions per second and transaction latency for OLTP systems. Relational database systems (which benefit from a multithreading approach), such as MySQL, scale decently well vertically.
This approach has a few flaws, but the most obvious one is the maximum size of a server on the market. As soon as one reaches the limit of the biggest server instance, there is only one way to go: horizontal scaling.
Horizontal scaling is an approach where one adds more servers to the cluster, in order to ideally increase the performance linearly with the number of servers added. Most traditional database systems don’t scale well horizontally, or not at all. For example, MySQL can scale horizontally for read operations, by adding read slaves, but is unable to scale horizontally for write operations.
On the other hand, due to its nature, Cloud Spanner can easily scale horizontally with minimal intervention.
A fully featured RDBMS as a Service system needs to be evaluated from multiple angles. We took the most popular RDBMS on the cloud as our baseline — for Google, the GCP Cloud SQL and for Amazon, the AWS RDS. In our evaluation, we focused on the following areas:
Although Google does not explicitly claim that Cloud Spanner is designed for analytical processing, it does share some attributes with other engines, such as Apache Impala & Kudu and YugaByte, which are designed for OLAP workloads.
Even if there was a only small chance that Cloud Spanner included a consistent horizontally scalable HTAP (Hybrid transactional/analytical processing) engine with a (somewhat) usable set of OLAP features, we felt it was worth looking into.
With that in mind, we looked at the following areas:
Google Spanner is a clustered relational database management system (RDBMS) that Google uses for several of its own services. Google made it publicly available to Google Cloud Platform users in early 2017.
Here are some of Cloud Spanner’s attributes:
“Cloud Spanner chooses an index automatically only in rare circumstances. In particular, Cloud Spanner does not automatically choose a secondary index if the query requests any columns that are not stored in the index.“https://cloud.google.com/spanner/docs/secondary-indexes
Note: The Apache Tephra project adds extended transactional support to Apache HBase (also implemented within Apache Phoenix in Beta now).
Ok, so we’ve all read Google’s claims about Cloud Spanner’s advantages — almost limitless horizontal scaling while maintaining strong consistency and very high SLA. Although these claims, by any means, are an extremely hard thing to achieve, it was not the purpose of our evaluation to refute them. Instead, let’s focus on other things most database users are concerned with: feature parity & usability.
Both Google Cloud SQL and Amazon AWS RDS, the two most popular OLTP DBMS on the cloud market, have very large feature sets. However, to scale out those databases beyond the size of one node, you need to do application sharding. This approach creates additional complexity on both application and administration fronts. We looked at how Spanner fits in the scenario of unifying multiple shards into a single instance and the features (if any) that may need to be sacrificed.
Firstly, when starting with any database, one needs to create a data model. If you think that you can plug Spanner’s JDBC into your favourite SQL tool, you’ll discover that you can query your data with it but cannot use it to perform table creation or able alteration (DDL), or any insert/update/delete operations (DML). Google’s official JDBC supports neither.
“At present, the drivers do not support DML or DDL statements.”Spanner documentation
The situation isn’t any better with the GCP console, where you can submit only SELECT queries. Fortunately, there exists a community JDBC driver with the support of DML and DDL including transactions github.com/olavloite/spanner-jdbc. While this community driver is extremely valuable, the absence of Google’s own JDBC driver is surprising. Fortunately, Google is offering fairly broad support of client libraries (based on gRPC): C#, Go, Java, node.js, PHP, Python, and Ruby.
The almost mandatory use of Cloud Spanner’s custom APIs (due to the lack of DDL and DML in the JDBC) result in some limitations to related areas of the code, such as connection pools or database linking frameworks (for example, Spring MVC). Typically, while using JDBC, one has the liberty of grabbing a favourite connection pool (for example, HikariCP, DBCP, C3PO,…) that is production tested and performs well. In the case of Spanner’s custom APIs, we have to rely on connection/session pools/frameworks that we built in-house.
Primary-Key (PK) oriented design permits Cloud Spanner to be very fast when data is accessed via the PK, but also results in some query challenges.
Google’s Cloud Spanner has built-in support for secondary indexes. This is a very nice feature not always present in other technologies. Apache Kudu currently doesn’t support secondary indexes at all and Apache HBase doesn’t support indexes directly, but can add them through Apache Phoenix.
It is possible to simulate indexes in Kudu and HBase as a separate table with a different composition of primary keys, but atomicity of operations made to the parent table and linked index-tables needs to be done at the application level, and is not trivial to implement properly.
As mentioned in the Cloud Spanner overview, its indexes may have a behavior different from MySQL indexes. So, extra caution should be put on query building and profiling, to ensure appropriate index use where intended.
A very popular and useful object in a database is views. They can be used for a large number of use cases; my two favorites are as a logical abstraction layer and as a security layer. Unfortunately, Cloud Spanner does NOT support views. This is particularly limiting since there is not a column level granularity for access permissions, where views can be a viable workaround.
In the Cloud Spanner documentation, in the section that details quotas and limitations (spanner/quotas), there is one, in particular, that may be troublesome for some applications: out of the box, Cloud Spanner has a limit of a maximum of 100 databases per instance. Obviously, this can be a major setback for a database that is designed to scale out beyond 100 databases. Fortunately, after talking to our Google technical contact, this limit can be increased to almost any value via Google support.
Cloud Spanner offers fairly decent programming language support to operate with its APIs. Officially supported libraries are in C#, Go, Java, node.js, PHP, Python, and Ruby. The documentation is reasonably detailed, but similarly to other cutting-edge technologies, the community is quite small compared to the most popular database technologies, which may cause more time spent when less common use cases or issues needs to be addressed.
We didn’t find a way to create a Cloud Spanner instance in a local environment. The closest we got was a docker image of CockroachDB, which is similar in principle, but very different in practice. For example, CockroachDB can use PostgreSQL JDBC. As it is imperative for a development environment to be as close a match as possible to production, Cloud Spanner is not ideal as one needs to rely on a full Spanner instance. To save costs you can select a single region instance.
The creation of a Cloud Spanner instance is very easy. One just needs to choose between creating a single region or a multi-region instance and specify the region(s) and the number of nodes. After less than a minute, the instance is up and running.
A few rudimentary metrics are directly available on the Spanner page in the Google console. More detailed views are available via Stackdriver, where you can also set up metric thresholds and alerting policies.
MySQL offers vast and very granular user permissions/roles settings. One can easily set up access to a specific table or even just to a subset of its columns. Cloud Spanner uses Google’s Identity & Access Management (IAM) tool, which only allows the setting of policies and permissions on a very high level. The most granular option is a permission on a database level, which doesn’t fit a large chunk of production use cases. This limitation forces you to add extra security measures in your code, infrastructure or both, in order to mitigate unauthorized use of Spanner’s resources.
Simply put, backups are non-existent in Cloud Spanner. While Google’s high SLA claims may guarantee you will not lose any data, due to hardware or database failures, there’s no coming back from human error, application defects, and so on. We all know the rule: high availability doesn’t replace a sound backup strategy. For now, the only way you can back up data is to programmatically stream them out of the database to a separate storage environment.
For data loading and query testing, we used Yahoo! Cloud Serving Benchmark. The table below presents the YCSB workload B with a ratio of 95% read and 5% write.
*The load test was running on a compute engine n1-standard-32 (32 vCPUs, 120 GB memory) and the test instance was never the bottleneck in the tests.
** The maximum number of threads within a single instance of YCSB is 400. Total of six parallel instances of YCSB benchmark had to be run to get the total of 2400 threads.
Looking at the benchmark results, particularly the combination of CPU load and TPS, we can clearly see that Cloud Spanner scales quite well. More load generated by more threads is compensated by more nodes in Cloud Spanner’s cluster. While the latency looks fairly high, especially when run with 2400 threads, to get more accurate numbers it may be worth re-testing with 6 smaller compute engine instances. Each instance would each run one YCSB benchmark instead of one big CE instance with 6 benchmarks in parallel. This way it may be easier to distinguish between Cloud Spanner query latencies, and the latency added by the network connection between Cloud Spanner and the CE instance running the benchmark.
Splitting data into physically and/or logically independent segments, called partitions, is a very popular concept inherent in most OLAP engines. Partitions can greatly improve query performance and database maintainability. Delving further into partitions would be an article(s) on its own, so let’s just mention the importance of having a partitioning and sub-partitioning scheme. The ability to split data into partitions and even further into sub-partitions is key to the performance of analytical queries.
Cloud Spanner doesn’t support partitions per-se. It divides data internally into splits based on the primary key ranges. The splitting is done automatically to balance the load across the Cloud Spanner cluster. A very handy Cloud Spanner feature, is load base splitting of the parent table (the table that is not interleaved with another). Spanner automatically detects if a split contains data that is read more frequently then the data in other splits and may decide to further split it. This way more nodes can be involved in the querying and this effectively also increases bandwidth.
Cloud Spanner’s way to bulk data is the same as a normal load. To achieve maximum performance you’ll need to follow some best practices, including:
Loading data this way makes use of all of Cloud Spanner nodes.
We used the YCSB workload A to generate a 10M rows data set.
* The load test was running on a compute engine n1-standard-32 (32 vCPUs, 120 GB memory) and the test instance was never the bottleneck in the tests.
** 1-node setup is not recommended for any production load.
As mentioned above, Cloud Spanner automatically handles the splits based on their loads, so the results improved after several consecutive reruns of the test. The results presented here are the best results we obtained. Looking at the numbers above we can see how Cloud Spanner scales (well) with the increased number of nodes in the cluster. The numbers that stand out are the extremely low average latencies which are in contrast with results of mixed workloads (95% read 5% write), as described in the section above.
Scaling up and down the number of Cloud Spanner nodes is a one-click task. If you want to quickly load data, you may consider boosting the instance to the maximum (in our case it was 25 nodes in the US-EAST region) and then scale down to the number of nodes suitable for your usual load, once the data is in the database, while keeping in mind the 2TB/node limit.
We were reminded of this limit even with a much smaller database. After several runs of load tests, our database was about 155GB and when scaling down to 1 node instance we received the following error:
We managed to downscale from 25 to 2 instances, but were stuck with the 2 nodes.
Scaling up and down the number of nodes in a Cloud Spanner cluster can be automated through its REST API. This can be particularly useful to alleviate increased load on the system during busy hours.
Initially, we planned to put a significant amount of time into this part of our Spanner evaluation. After just a few SELECT COUNTs, we immediately knew that the benchmarking would be short and that Spanner is NOT an OLAP suitable engine. No matter the number of nodes in the cluster, a simple selection of a number of rows on a 10M row table took between 55 to 60 seconds. Additionally, any query that required a bigger amount of memory to store intermediate results failed with an OOM error.
SELECT COUNT(DISTINCT(field0)) FROM usertable; — (10M distinct values)-> SpoolingHashAggregateIterator ran out of memory during new row.
Some numbers for TPC-H queries can be found in Todd Lipcon’s article Nosql-kudu-spanner-slides.html, slides 42 & 43. The numbers are consistent with our own findings (unfortunately).
With the current state of Cloud Spanner features, it’s hard to think of it as an easy replacement for an existing OLTP solution, especially once your needs outgrow it. One would have to invest a significant amount of time to build a solution around Cloud Spanner’s shortcomings.
When we started the Cloud Spanner evaluation, we expected its management features to be on par with, or at least not so far from, other Google SQL solutions. But, we were surprised by the complete lack of backups and very limited resource access control. Not to mention its lack of views, no local development environment, unsupported sequences, JDBC without DML and DDL support, and so on.
So, where does this leave someone who needs to scale-out a transactional database? There doesn’t yet seem to be one solution on the market that fits all use cases. There are plenty of closed and open source solutions (a few of which are mentioned in this article), each with its strengths and weaknesses, but none of them offer SaaS with 99.999% SLA and strong consistency. If high SLA is your primary objective and you’re not inclined to build your own multi-cloud solution, Cloud Spanner may be the solution you are looking for. But, you should be aware of all of its limitations.
To be fair, Cloud Spanner was only released for general availability in the Spring of 2017, so it’s reasonable to expect that some of its current shortcomings may eventually disappear (hopefully) and when that happens, it may be a game changer. After all, Cloud Spanner is not just a side project for Google. Google uses it as the backbone for other Google products. And when Google recently replaced Megastore on Google Cloud Storage with Cloud Spanner, it allowed Google Cloud Storage to become strongly consistent for object listing on a world-wide scale (which is still not the case for Amazon’s S3).
So, there’s still hope…we hope.
Originally from Prague, Czech Republic, Ales Penkava has been a Data Architect in Montreal for almost ten years. For the past year and a half he has led Lightspeed’s Data Pipeline team. When not obsessing about big data solutions, Ales helps fledgling dancers grow at the Broadway Academy school that he co-founded with his wife.
Originally published on the Lightspeed HQ Blog.
Founded in 2005, Lightspeed builds tech that helps independent retailers, restaurateurs, and eCommerce merchants start, market, manage, and grow their business.
See all (51)
950 
4
950 claps
950 
4
Founded in 2005, Lightspeed builds tech that helps independent retailers, restaurateurs, and eCommerce merchants start, market, manage, and grow their business.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@jryancanty/stop-downloading-google-cloud-service-account-keys-1811d44a97d9?source=search_post---------31,"Sign in
There are currently no responses for this story.
Be the first to respond.
Ryan Canty
Jul 27, 2020·6 min read
TL;DR: Generating and distributing service account keys poses severe security risks to your organization. They are long-lived credentials that are not automatically rotated. These keys can be leaked accidentally or maliciously allowing attackers to gain access to your sensitive GCP resources. Additionally, when used actions cannot be attributable back to a human. You don’t actually have to download these long-lived keys. There’s a better way!
For some background, almost every change you want to make in Google Cloud from creating a GKE cluster to reading from a GCS bucket is handled using an API. This API is authenticated using the OAuth2 protocol, which basically means there’s a short lived (1 hour default) access token attached to every authenticated request. If you’re familiar with the whole “Sign in with Google” popup, that’s OAuth2 hard at work authenticating you with your Google credentials. Once you’re authenticated, an access token is attached to all your API requests whether you’re using gcloud, terraform, SDKs, or the console. In Google Cloud, we use a lot of automation and web services which similarly need those tokens, but robots aren’t very good at opening browsers and typing in passwords so they need some sort of verifiable identity. Enter Service Accounts.
Service accounts allow automated users to prove their identity using a public/private key pair in the form of a JSON file. A service account also has the same ability as users or groups to bind to IAM roles to do things in GCP. To make an API request, a service account will sign a JWT token with its private key and the Google authentication system will verify that signature with the public key, granting an access token. This basic (and oversimplified) concept is important for later parts of this post. If you’d like to read more about this flow, check out RFC 7523.
You should never need to generate and download a service account key to use a service account within Google Cloud infrastructure.
Service accounts are very easy to use within Google Cloud. Most, if not all, compute resources (i.e. GCE instances, GKE Pods, Cloud Functions, etc.) support the ability to attach a service account. This allows these resources to act as the service account, call Google SDKs and APIs within the bounds of permissions granted to the service account. You should never need to generate and download a service account key to use a service account within Google Cloud infrastructure. A risk emerges when developers think they need a service account to accomplish a task, so they generate and download a key.
I cannot tell you how often I see documentation or tutorials instruct folks to download these service account keys and use them indefinitely or worse, store them in their source code working directory. Doing this, you’re literally one line in a .gitignore from committing this highly sensitive secret to Github and getting breached.
Remember how I said that if you have the Service Account’s private key, you can sign a JWT token and be granted an API access token? Well there is a way to do that without ever needing to download the key.
Let’s say I have a service account that is used for GKE so it has the role roles/container.developer. We’ll call this service account k8s@project.iam.gserviceaccount.com. Let’s further say that my user ryan@example.com isn’t allowed to download the key to this service account and doesn’t have direct permissions to mess with GKE but what I do have is the magic role roles/iam.serviceAccountTokenCreator. Now all I have to do to setup my GKE credentials with the gcloud command is:
This is great because it allows this command to use a service account without actually having the key! Not only that but you always know you are impersonating because a warning message pops up letting you know.
But if you’re running multiple commands with the same service account, this can be annoying to type over and over. Instead, let’s set this with gcloud config.
This is much better! Now we can impersonate a user for multiple commands without having to constantly add that --impersonate-service-account flag. But what if, like me, you constantly switch back and forth between service accounts for different projects. We can write a very simple bash script to simplify typing and remembering service accounts that you use frequently.
Now you could execute this with the following before using gcloud:
You could make this more robust by reading from a config file if you like, but I think a single-file script gets the point across. Now for every new service account you want to use, simply add it and a short name for it to this script and you’re off.
We’ve spent all this time talking about a better way to consume service accounts through gcloud but what about the very common use case of Terraform? For development purposes, we need to test our infrastructure as code somehow. Thankfully, the google terraform provider supports directly passing an OAuth2 token as an environment variable. All you have to do to get this token and tell Terraform about it is this:
You could further simplify this by wrapping it in a Makefile such as this (special thanks to emalloy for putting this together):
Now you have a token that will only live in your environment for 1 hour and be useless to an attacker after that!
The SecOps folks may be thinking, how do I attribute and audit actions taken by a user impersonating a service account? In Cloud Logging, every API call executed by a service account that has been impersonated has the following structure within it:
You can also get a higher level of detail if you enable Data Access logs. The below Cloud Logging filter will include every API call that ryan@example.com made while impersonating. I’m sure you can find other clever filters as well.
I’ll also point out that this level of attribution is impossible if you allow users to download service account keys since they are effectively using shared credentials, assuming more than one person has access to download the same key. To illustrate, this is the only info you get in the logs if I could download the k8s service account key and used it.
If I’m a forensic analyst or an auditor, there’s no way I could figure out definitively what human executed this API request unless each user has their own service account or key and that’s defined somewhere. Even still, that is very difficult to trace back.
There are some use cases where downloading a service account key to your workstation is necessary, but they are not the norm. Keeping secrets like this short-lived locally should be the goal, the same way we should enable MFA and not use hunter2 as our password. For the obvious cases where service account keys must be downloaded to use GCP resources from your datacenter or another cloud, I’d recommend taking a look at HashiCorp Vault which has a plugin to checkout short-lived service account keys.
So now that you know, please stop downloading service account keys! :)
Cloud Security Engineer at Google Cloud http://github.com/onetwopunch
646 
11
646 
646 
11
Cloud Security Engineer at Google Cloud http://github.com/onetwopunch
"
https://medium.com/google-developers/firebase-google-cloud-whats-different-with-cloud-storage-a33fad7c2b80?source=search_post---------32,"There are currently no responses for this story.
Be the first to respond.
In my previous posts about the relationship between Firebase and Google Cloud Platform (GCP), I talked about what’s different between the two for both Cloud Functions and Cloud Firestore. In this post, the topic will be Google Cloud Storage (Firebase, GCP), a massively scalable and durable object storage system. In this context, an “object” is typically a file — a sequence of bytes with a name and some metadata.
It’s tempting to think of Cloud Storage like a filesystem. In many ways it’s similar, except for one important fact: there are actually no directories or folders! Here’s how it really works. The top level of organization is called a bucket, which is a container for objects. Each bucket is effectively a big namespace full of objects, each with unique names within that space. Object names can look and feel like they have a directory structure to them (for example, /users/lisa/photo.jpg), but there are no directories or folders behind the scenes. These path-like names are helpful for organization and browsing in both the Cloud and Firebase consoles. Sometimes, we just use the word “folder” to make it easy to describe these paths.
Here’s what the Cloud console looks like when you’re browsing the contents of a storage bucket:
And here’s what the Firebase console looks like for the same bucket:
You’ll notice that the Cloud console puts a lot more data and functionality on the screen than the Firebase console. The Cloud console exposes the full power of Cloud Storage, while the Firebase console exposes only those features that are likely to be important to Firebase developers. That’s because the use cases for Cloud Storage tend to be a little different, depending on your perspective.
Cloud developers are often also enterprise developers. And, as you might guess, enterprise developers have broad requirements for object storage. They’re using Cloud Storage for backups and archives, regional storage, hosting static content, controlling access to objects, versioning, loading and saving data in BigQuery, and even querying data directly. Cloud Storage is very flexible! A majority of the access to an object comes from backend systems, often other Cloud products and APIs, and the SDKs. But for Firebase developers, the use cases are typically more narrow.
The most common use case for using Cloud Storage in a mobile or web app is handling user-generated content. A developer might want to enable users to store images and videos captured on their devices, then let the users view the files on other devices or share them with friends. Firebase provides SDKs (Android, iOS, web, Unity, C++) for uploading and downloading objects in storage, directly from the app, bypassing the need for any other backend components. To help with privacy and prevent abuse, Firebase also provides security rules, paired with Firebase Authentication, to make sure authenticated users can only read and write the data to which they’ve been given permission. Collectively, these SDKs and security rules are referred to as “Cloud Storage for Firebase”.
When you create a Firebase project, or add Firebase to an existing GCP project, a new storage bucket is automatically created for that project. This helps reduce the amount of configuration required to get started with Cloud Storage in a mobile app. Since all Cloud Storage bucket names across the entire system must be unique, this new bucket is named “[YOUR-PROJECT-ID].appspot.com”, where YOUR-PROJECT-ID is the globally unique ID of your project. You normally don’t even need to know this bucket name, as it’s baked into your Firebase app configuration file. When Firebase gets initialized, it’ll automatically know which bucket to use by default. In fact, this bucket is usually referred to as the “default storage bucket” for Firebase.
All that said, Firebase developers are not limited to using only Firebase SDKs, and Cloud developers can opt into using Firebase to read and write existing buckets. If you started out with Firebase, you can use any of the Cloud-provided tools, SDKs, and configurations at any time. And if you have existing data in Cloud Storage to use in a mobile app, you can certainly add Firebase to your project to make that happen.
Everyone who wants to access a Cloud Storage bucket from the command line can use gsutil, provided their Google account is authorized to do so. gsutil provides commands to upload and download files locally, and to configure a storage bucket. It’s often more convenient to use this tool rather than either of the web consoles, especially for bulk operations.
There’s one interesting difference between Firebase and Cloud. As you’ve just read, they both provide access control to objects in storage buckets, but those rules are mutually exclusive from each other. You use Cloud IAM to control access to an object only from backend systems and SDKs, but you use Firebase security rules to control access only from mobile applications using the Firebase client SDKs. These access control mechanisms don’t overlap or interfere with each other in any way. Note that the Firebase Admin SDK is actually a server-side SDK, which can also be used to access Cloud Storage. In fact, the API it provides is actually just a wrapper around the Cloud SDKs, which are controlled by IAM.
How are you using Cloud Storage? I’ve used it in a couple experimental projects. There’s a connected video doorbell that stores images of guests who ring the doorbell and a “universal translator” that translates speech recorded on a mobile device. Check those out, and let me know what you’ve built as well!
Engineering and technology articles for developers, written…
932 
7
932 claps
932 
7
Engineering and technology articles for developers, written and curated by Googlers. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
firebase-consultant.com, Firebase GDE, engineer, developer advocate, Xoogler
Engineering and technology articles for developers, written and curated by Googlers. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/google-cloud-functions-python-overview-and-data-processing-example-b36ebde5f4fd?source=search_post---------33,"There are currently no responses for this story.
Be the first to respond.
Serverless, FaaS (Functions-as-a-Service), and Python are important knowledge areas for anyone building or utilizing cloud services in 2019. Cloud Functions are a lightweight managed service that you can use to increase agility while using cloud. You will want to consider Cloud Functions for new architectures or when modernizing existing workflows using multiple cloud platform services.
Cloud Functions are serverless, so you run code without having to worry about managing or scaling servers. Cloud Functions integrate easily with Google Cloud Platform (GCP) services and you pay for resources only when your code runs. They are invoked by triggers that you specify and they standby waiting for specific events or HTTP endpoint calls.
Serverless does not mean that you take existing code chop it up into functions to receive a lower cost and instant autoscaling. Adopting a serverless approach means utilizing managed services and allowing your provider to handle base functionality of your service or application then use Cloud Functions to act like the glue between those services to deliver business value.
Cloud Functions support microservices architectures, data management pipelines, and allow you to easily integrate AI into applications. As this article will further demonstrate, they can act like glue pulling together multiple services in Google Cloud Platform to deliver a service, build insights through ML, or help data flow to BigQuery.
Since Cloud Functions execute in about 100ms, they can enable you can have a near real time streaming pipelines. They should be quick snippets of code that can fail easily if necessary. The main prerequisite is that you have a moderate skill level at either node.js or Python. In this article, we will focus on the Python runtime.
There are two different types of Cloud Functions: HTTP functions and background functions. HTTP functions are triggered by a HTTP trigger endpoint; and, background functions are triggered by event triggers from GCP services.
What can you use Cloud Functions for?
Triggers determine how and when the function executes. When you deploy a cloud function you must select a trigger that will invoke your code. These are the unique events that you will identify and plan for actions to occur after.
Event Trigger Types
Other triggersYou can get creative with GCP services native functionally to further extend triggers with services that support Pub/Sub or any service that provides webhooks.
Make a trigger from any service that supports Cloud Pub/SubBecause Cloud Functions can be invoked by message sent to a Pub/Sub topic, its easy to integrate Cloud Functions with other Google services that support Pub/Sub. For example you can export Stackdriver Cloud Logging to a Cloud Pub/Sub topic as a sink destination, once a message is posted to that Pub/Sub topic you can execute a function. This could be interesting for integrating with a service such a PagerDuty for staying ahead of critical logging notifications.
Gmail Push Notifications can be configured to send a message to a Pub/Sub topic when there are changes in Gmail inboxes. Are you using a Gmail inbox to manage invoices or activity for a department? Now you can kick off secondary workflows with Cloud Functions based upon events (messages received) in Gmail.
One example I like for data processing workflows is using a Cloud Function to perform serverless data warehouse updates. I have only been able to find this scenario done in node.js by Asa Harland, so here it is for you in Python.
The use case: Say you have a Python proficient data science team working in Jupyter notebooks or Cloud Datalab. Your organization has a data warehouse in BigQuery that you are working on via notebooks. You need fresh updated data as the sample set you are working on is static and hasnt been updated in awhile. By using Cloud Storage as your source, a Cloud Function in between, and BigQuery as your destination you have a basic data pipeline to give you fresh data for your analysis and insights.
Using the object.finalize trigger whenever a new CSV or AVRO is uploaded to a specified Cloud Storage bucket the Cloud Function then uses the BigQuery API to append rows to the specified table in the function code.
Here is how the architecture looks for this data processing pipeline:
Here is the function code to append new CSVs or AVROs in Cloud Storage to a BigQuery table:
Now, with your function set on your source bucket whenever a new CSV delta is uploaded the rows in that CSV will be automatically appended to the table you specified.
The goal of this article is to help you understand serverless options such as Cloud Functions when building out new modern architectures on Google Cloud Platform. Today there are many managed options that can give you different types of agility when building a new application or services. Cloud functions are the glue code that can make your managed service backed applications or workflows more efficient and insightful.
Credit to Zack Kanter for an excellent post on the business case for serverless. Some concepts and descriptions used in this post were adapted from his article.
Google Cloud community articles and blogs
638 
7
638 claps
638 
7
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Customer Engineer, Google Cloud. All views and opinions are my own. @mkahn5
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://towardsdatascience.com/how-to-pass-the-google-cloud-professional-data-engineer-exam-f241d7191e47?source=search_post---------34,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jonathan Moszuti
Dec 13, 2019·15 min read
So you’ve probably googled the title above and now you’re here. Don’t worry though, you’re not the only one. I also did the same thing.
In this guide, I will give you an idea of what the exam is like and I how I prepared for it (and passed!)
"
https://towardsdatascience.com/10-days-to-become-a-google-cloud-certified-professional-data-engineer-fdb6c401f8e0?source=search_post---------35,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jeff Hale
Jun 20, 2019·11 min read
I recently took the updated Google Cloud Certified Professional Data Engineer exam. Studying for the test is a great way to learn the data engineering process with Google Cloud.
I recommend studying for the exam if you want to use Google Cloud products and:
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/gps-cellular-asset-tracking-using-google-cloud-iot-core-firestore-and-mongooseos-4dd74921f582?source=search_post---------36,"There are currently no responses for this story.
Be the first to respond.
One of the biggest problems logistics industry faces today is the tracking of assets and vehicles. We have a bunch of ways to resolve this problem, GPS and Cellular technologies have been around for a long time and in the most cases are enough to track a vehicle in almost real-time. Our project will use both of these technologies to achieve this goal. Check out how our app will look like at the end of this tutorial:
You can access the WebApp at https://asset-tracker-iot.firebaseapp.com/
In this tutorial we will build an Asset Tracker using an ESP32 microcontroller running MongooseOS, that sends data securely via Cloud IoT Core using MQTT protocol over mobile network, the data is processed in an event-based way using Firebase Cloud Function, saves the data and current device state in Firestore. The data then can be accessed through a Progressive Web App hosted on Firebase Hosting, with the possibility to even configure the device remotely, showcasing Cloud IoT Core bi-directional communication.
If you are confused by the many terms, I recommend you to read another post that I made about using Google Cloud for an IoT project. In this post, I explain in more details most of these products. Here I’ll be more succinct in the configuration of the project on Google.
medium.com
Here we’ll be focusing on building an end to end architecture that handles the location data coming from our sensors in a secure way, store the data, show our fleet on a map in realtime. Our architecture will look like this:
Making a project that relies on mobile network and with security in mind is not trivial. I’ll show how easy is to do this using MongooseOS and their awesome PPPoS library, that abstracts the network over GPRS/Cellular serial modules, making easier for us to build our products that depend on this kind of communication.
The security channel to send data using MQTT will be handled by Cloud IoT Core. All data received will be stored in Firestore database. Here is the summary of what we will learn:
To use the gcloud command line tools, follow the instructions here to download and install it.
cloud.google.com
After installing the SDK, you should install the beta tools to have access to the Cloud IoT Core commands. Also after this you should authenticate and create a project to use in this tutorial, exchange YOUR_PROJECT_NAME with a name that you want for this project:
Create a Cloud IoT Registry where the devices will be registered to send data.
If you access the Google Cloud Console you can validate that everything is created and configured.
In this section, I’ll show how to setup the asset tracker, all modules needed and how to program the board with our code.
The heart of our project will be an ESP32 WiFi microcontroller, it has many flavors out there and as far as I known, any model will work.
Here is the project component list:
Our schematic is the following:
The schematic, in general, is pretty simple. Maybe the only weird part is the GSM module connections. I put a MOSFET to switch on and off the module because it consumes a lot of energy. Making this I can use less energy and put the device to sleep when I’m not sending data. Another detail is the usage of a diode to drop the voltage to power the GSM module with 4.2v, because powering it with 3.3v or 5v is considered under-voltage and over-voltage respectively by the module.
To program the board we will use MongooseOS. It has a tools called mos that make programming, provisioning and configuration really easy on the supported boards. To use it we need to download and install it from the official website. Follow the installation instructions on https://mongoose-os.com/docs/quickstart/setup.html.
mongoose-os.com
With the tools installed, download the project code on Github repository linked here, so you can build and deploy it on the device.
github.com
The repository consists of 3 sub-projects:
I will detail a little more each project in the next sections.
Here some description of the firmware project:
Now take a look a the device code, most of its functions have a little comment. Basically the device flow is the following:
To program the hardware, enter the firmware folder and run the following instructions to flash the firmware, configure WiFi and provision the device on Cloud IoT Core:
That’s it, your device will begin to collect location data, connect to the cellular network and send data to Cloud IoT Core. You can see whats happening on the device using the mos console tool. You will see it trying to connect to the cellular network and to mqtt.googleapis.com. Here is some output from mos console, I omitted some junk messages:
I recommend taking a walk with the device to generate data and also sometimes I have problems to get GPS and GPRS to connect inside my house. To see the data on PubSub you can use gcloud command to query the subscription that we created:
If you see the data on the console, you can start celebrating, we are on the right path 🎉🏆.
The Cloud Firestore is like a merge of Google Datastore and Firebase Realtime Database, with some features like document and collections definitions, real-time data synchronization, data replication and SDK for many languages. One of the most exciting news to me is that you have the same ease-of-use of all the Firebase products and in comparison with Firebase Database now we have a much better support for advanced queries and data modeling.
For this project I choose Firestore because of these features:
This project uses Firebase Cloud Functions to handle all the custom rules and backend services that our application need. The code can be seen below, but there are some settings that can be changed in the project:
Here are some details on each function that is running on the Cloud in reaction to our events:
To deploy our functions we need the Firebase Command Line Tools, it requires Node.JS and npm, which you can install by following the instructions on https://nodejs.org/. Installing Node.js also installs npm. Once Node and NPM is installed, run the following command to install Firebase CLI.
Now to configure firebase with our project and deploy the functions, in the project root folder, follow the above instructions:
With the deployed functions you have all setup to store our received location data sent by the device and execute our custom rules on how to store it. You can see and monitor all deployed resource on the Firebase Console.
Now it’s time to deploy our WebApp.
For this part of the project, I’ll not enter into too many details because it’s not the main subject for this post, but I’ll highlight some methods that make access to Cloud Firestore.
Here are some of the most important functions for the WebApp, where it access data on Firestore and call our endpoint to update device configuration:
For this app, I used an awesome tool called create-react-app that configures a really good project structure and tools to work with modern Javascript, in this case using React. For those who don’t know it, I recommend to check it out on their official blog.
reactjs.org
In the last section we already installed NPM, so now we only have to install all project dependencies and build it to production. Here are some commands that come configured with create-react-app :
After building our project we can now run firebase deploy and have all of our project deployed on Firebase infrastructure.
If all it’s correctly setup, then we have another end to end solution created using many awesome tools and better yet: Without touching an advanced and boring server setup.
That’s it for this project, we have a really cool asset tracker prototype that uses many new features of Google Cloud and some IoT tools. Hope you guys enjoy this project and I’ll continue to explore more projects using those tools.
The code for this project can be found on my Github and some interesting are linked in the section bellow to read later:
github.com
Do you like this post ? So don’t forget to leave your clap on 👏 below, recommend and share it with your friends.
Did you do something nice with this tutorial? Show in the comments section below.
If you have any questions, post in the comments that I will try to help you.
Google Cloud community articles and blogs
1.1K 
16
1.1K claps
1.1K 
16
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Product Engineer @Leverege & Google Developer Expert for IoT. When I'm not cooking, I'm building fun stuff or helping my local dev community. GDG organizer.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-developers/5-aha-moments-with-the-google-cloud-platform-14b44b7ecdc3?source=search_post---------37,"There are currently no responses for this story.
Be the first to respond.
Posted by: Jerome Poudevigne, Startup Architect, Google Cloud
Throughout the past couple of years, I have helped a good number of companies, big and small, migrate their systems to the Google Cloud Platform (aka GCP). During the course of these migrations, there are always a few of those moments where people look at a specific Google Cloud feature and say, “now, that’s cool!”.
More often than not, it is because, coming from other platforms, they have gotten used to some features requiring multiple steps, or some operations being complicated, etc. And often they find out that in GCP you can do this specific operation in a couple of clicks, or by setting up a simple text-based configuration. Then you see that light bulb turning on in their head, and there you go… happy customer.
A few of these happen so often that I compiled them in a list to share with others who might also benefit from these “aha!” moments. You could say these are the five things I wish they told me when I started using Google Cloud.
A project is a namespace where resources live. Every resource you instantiate in GCP, from load balancers to Kubernetes clusters to virtual machines, belongs to a single project, and has no access (by default) to resources in other projects. User roles and authorisations can be defined per-project and trickle down to everything in it. This has two immediate benefits: you can group things that belong together in neat logical units, and things that don’t belong together are isolated from each other (and isolation is a Good Thing)
This is powerful and quite simple, but it often takes new users off-guard. I’ve had many clients call me and ask me “How can I make sure my developers cannot access the production machines? What’s the best way to create access policies? ”
The answer to this is actually super-simple:
Every machine/other resource in the production project won’t be accessible to developers.
Of course there is a lot more to it, and you can refine roles and permissions to a much greater degree using Organizations, Folders, etc. Not to mention all the crazy things you can do with per-project billing. But at least you can say “hey, if it’s a machine in the staging environment then it can be found in the “staging” project”.
Imagine you are using a Cloud provider and that you have servers in the US, and servers in Singapore, and that they need to communicate.
So you create a VPC (Virtual Private Cloud) network in the US data center, another one in the Singapore data center, and then you will connect them by setting up inter-region VPC peering or a VPN (Virtual Private Network) or a transit VPC or other routing magic.
Lots of work, right? And many moving parts, so lots of opportunities for things to break.
With GCP, however, what makes my clients go “aha!” is when they realize that in GCP a single VPC network covers the entire planet. Only subnets are attached to a geographic location, and virtual machines communicate between subnets on private IPs (good old RFC1918 addresses) — no extra routing needed.
So, to make your server communicate across continents on GCP, here are the steps:
That’s all there is to it. Your VPC network spanning 2 continents is ready to use. Below is a screenshot of how it looks on my account, for a VPC network called ‘my-global-network’ with 2 subnets. The first column (“us-central1” and “asia-southeast1”) contains the name of the GCP regions (read: data centers). The second column is the subnet name that I picked when I created them.
A machine in the US (on the “us-central” subnet) with IP 10.0.0.5 can communicate directly with a machine in Singapore (on the “singapore”) subnet with IP 10.10.0.8.
Nothing else to set up.
And thanks to the way these networks work, the Google Cloud Load Balancer can present a single IP to the world, and forward traffic to the instances that are the closest to you geographically without having to setup a tedious DNS-based load balancing. But that’s worth an entire blog. I’ll save it for another day.
There is no network security without a firewall so unsurprisingly GCP comes with one built-in.
Now, I don’t know about you, but nothing makes my brain hurt like a list of firewall rules displaying IP ranges and addresses and ‘Allow/Deny’ directives. It looks a bit like this:
If you imagine a normal network with a few dozen (hundred?) servers, you can quickly see how this can get out of control. You’d better have a solid printout of your network layout to refer to when you start adding and changing rules. And good luck debugging things!
Wouldn’t it be nice if, instead, you could just tell the firewall: “the HTTP traffic from outside can only reach the HTTP servers and the MySQL database is only reachable by the HTTP server(s) on the same network?”
Turns out it’s pretty simple on GCP by using a little thing called network tags. As the documentation says:
“Network tags are text attributes you can add to Compute Engine virtual machine (VM) instances. Tags allow you to make firewall rules and routes applicable to specific VM instances.”
So let’s see how it works. Firewall rules in GCP are defined in terms of source and target (the traffic flows from the source to the target). You can define filtering rules that apply to the source or the target, and in both cases you can use tags.
This is simpler shown with an example. The rule below states that on the default network, the traffic to the VMs with the tag mysql-server can come from the VMs with the tag http-appserver. Any other traffic is “Deny”-ed by default.
All you have to do is to tag your machines properly, and they will automatically be covered by the rule. You don’t need to enter their IP range.
That’s neat if you ask me. It makes it a lot simpler to grasp what’s happening.
Of course, there’s a TON more to firewalls in GCP. Tags also apply to routes and you can mix and match IP-based rules with tag-based rules. Not to mention that thing called service accounts, but I’ll leave those for another day.
The bottom line is that you can create most rules by just expressing a business need and not having to remember complicated network layouts. I have no hard stats, but I’m pretty sure this has saved me hours of work.
Easily access virtual machines (VMs) from the Google Cloud console was one of my first “aha!” moments when I started using GCP.
This is a screen capture of my Google Cloud console, with a virtual machine and its internal IP.
The last column has a header that says “Connect” and when you click on the word “SSH” a separate windows pops up. You wait for a few seconds, and… this is what you get. Your personal shell access — in a browser popup no less.
You are connected through ssh to the virtual machine of your choice. You did not have to download ssh keys and put them in the ~/.ssh directory, do the correct chmod command and run a long-winded ssh -i ~/.ssh/somekey me@<it-took-me-forever-to-copy-paste-the-address-here>
In addition, you have access to a few nifty features such as uploading and downloading files, changing the user etc. Just use the menu behind the cog icon at the top right.
In truth, you should not need to connect directly that often, but when you have to, this is a godsend.
The Google Cloud console has a cool trick: you can actually connect to a virtual environment that is managed by the Google Cloud console itself. It serves a bit as a jump host. You can access most resources from the projects from it, and you can activate it directly from the top menu with, no particular setup on your side. It’s called the Cloud Shell.
This is how it looks at the top right of the console:
When you activate the Cloud Shell, the session opens at the bottom of the console. You get a command line prompt and it’s fully configured with the gcloud command line tool (the jack-of-all-trades of Google Cloud scripting).
You can do a great many things from there, and this even includes uploading and downloading files, editing code or deploying it, a web preview for your AppEngine application, and more.
So you can get access to a fully configured shell environment in your project from any laptop where you can connect with your credentials. On top of this, it persists between connections so you can fine-tune it to your needs and have these changes available the next time you re-connect.
This has saved me many times during my previous life as a traveling consultant!
Did I say 5 “Aha!” moments ? Well, you’ve been patient reading all the way to here, so here’s one more for free.
Google Cloud has an amazing way to literally “teleport” a running virtual machine between physical hosts without stopping it. It’s called Live Migration. It allows Google to move your virtual machine away from a defective host, or a host that needs a patch or an upgrade, or for any other infrastructure related reason.
It’s all done in the background, and is totally transparent, so you never really see it happening. Unless you look VERY closely. I once did a demo to a client, where a machine was live migrated while he was simulating a solid network load — and we did not lose a single packet, with no noticeable degradation in latency.
So there you go. These are 5+1 things that made me go “Aha!” when I became more familiar with the Google Cloud Platform, and that still make my clients do the same.
There is a lot of depth to the platform, and my examples above only scratch the surface of our features. I encourage you to try it yourself. There is a generous free tier, and when you are ready to take the plunge and create that new company, please contact us at Google Cloud for Startups. We’ll get you up and running in no time.
Jerome is a Startup Architect at Google Cloud. Based in Singapore, he helps startups make the most of the Google Cloud Platform.
Engineering and technology articles for developers, written…
668 
3
668 claps
668 
3
Engineering and technology articles for developers, written and curated by Googlers. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
News about and from Google developers.
Engineering and technology articles for developers, written and curated by Googlers. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/uploading-resizing-and-serving-images-with-google-cloud-platform-ca9631a2c556?source=search_post---------38,"There are currently no responses for this story.
Be the first to respond.
Top highlight
More and more applications today require their users to upload photos. From simple things like updating a profile photo, to more complicated services like Snapchat where you share a bunch of them. These applications are running on all kind of devices with different resolutions and at different network conditions. In order to do it right we have to deliver the images as fast as possible, with the best available quality, taking into account the targeted device and screen resolution.
One option is to create an image service yourself, where you can upload the images, store them and possibly resize them in few different sizes. Unfortunately, doing so is usually very costly in terms of CPU, storage, bandwidth and can end up very pricy. It is also quite a complicated task and many things can go wrong.
By using Google App Engine and Google Cloud Storage though, you can easily achieve this seemingly difficult task by using their API. Start by completing a simple tutorial on how to upload files into the cloud and read the rest if you want to see it in action to understand why it’s one of the coolest things ever.
App Engine API has a very useful function to extract a magic URL for serving the images when uploaded into the Cloud Storage:
get_serving_url()
Returns a URL that serves the image in a format that allows dynamic resizing and cropping, so you don’t need to store different image sizes on the server. Images are served with low latency from a highly optimized, cookieless infrastructure.
In practice it’s the same infrastructure that Google is using for their own services like Google Photos. The magic URLs usually have the following form: http://lh3.googleusercontent.com/93u...DQg
If all that wasn’t enough for what’s coming out of the box, there are also no charges for resizing the images and caching them when using that Google’s magic URL. Yes, you read it correctly, this is a free of charge service and you pay only for the actual storage of the original image.
You don’t have to worry about anything when it comes to serving images for your next big idea today. All you need to do is to upload your images once, extract the magic URL and then use it directly on the client-side by updating the arguments depending on the environment. Prototyping applications similar to Snapchat or even more complicated ones could be implemented over a weekend.
Besides the already mentioned tutorial from the documentation, you can play with a live example on gae-init-upload (which is based on the open source project gae-init).
The photo in the example is taken by Aleksandra Kiebdoj.
Google Cloud community articles and blogs
713 
20
713 claps
713 
20
Written by
The world chico.. and everything in it...
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
The world chico.. and everything in it...
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/building-a-real-time-quant-trading-engine-on-google-cloud-dataflow-and-apache-beam-841a909d2c12?source=search_post---------39,"There are currently no responses for this story.
Be the first to respond.
Google Cloud has fully managed services that allow end users to build big data pipelines for their analytical needs. One of them is called Dataflow. It allows developers to build data pipelines based on Apache Beam SDK. It is fully managed in that gCloud takes care of auto scaling of distributed worker resources as well of redistributing work loads amongst available nodes. Together with Apache Beam it provides a unified programming model for both batch and streaming data alike. It also integrates conveniently with other gCloud offerings such as Pub/Sub and BigQuery.
In this post, we are going to build a data pipeline that analyzes real time stock tick data streamed from gCloud Pub/Sub, runs them through a pair correlation trading algorithm, and outputs trading signals onto Pub/Sub for execution.
Disclaimer: this post is intended to experiment with building real time data analytics on Dataflow, not to teach how to code quant trading strategies. Do not use this trading strategy for real trading without proper risk management.
A little background on the trading strategy before going into details of the pipeline. Our trading algorithm is based on the assumption that similar stocks move in tandem in the long term. For example, large cap tech stock prices like google, apple, facebook move in a correlated way. If for any reason they are not correlated any more, it might be caused by an unusual market event. We have an opportunity to open a long/short position of the pair of stocks and close that position when correlation reverts to mean. It’s a simple mean-reversion strategy for trading pairs of stocks.
For example, using a sliding window, we can calculate correlation of stock A and B’s most recent X tick prices every Y minutes . Under normal market condition, A and B should be positively correlated, which means their correlation should be close to 1. If for any reason their correlation becomes negative and falls below a threshold, it is our trading signal to open a position to long stock A and short stock B. We will continue to monitor A and B until they become positively correlated again, then we can close the position. As in any long/short trading strategy the pair can also move in adverse ways. Strict risk management triggers are put in place to prevent unconstrained loss, but that’s tangential to the focus of this post.
Our Dataflow based trading pipeline looks like this
Apache Beam SDK allows us to construct a data pipeline on gCloud Dataflow using a unified programming model. At the heart of it is the concept of PCollection. PCollection is an immutable container of data sets. It is the input and output of every transformer in the pipeline. It can be bounded which means the size of the data set is known, e.g. file IO from a text file, jdbc IO from a database query. It can also be unbounded, which is the case with streaming data, e.g. Kafka IO from messages off of a queue.
Beam SDK also offers several generic Transforms that work with PCollection. It’s based on the Map/Reduce programing model. In this post we will use
Now let’s take a look at some code.
First we defined our universe of stock symbols and pairwise combination. Let’s initialize our Beam pipeline instance
Then for each stock we have tick market data streamed in via gCloud Pub/Sub on topics in the form of “input_<symbol>”. Each tick data is just a tuple of timestamp and price. We then construct the pipeline by applying transforms one after another.
There is a lot to unpack here. Let’s go through one pipe at a time.
2. We then apply a Map transform and a Filter to decode the bytes into strings and filter out invalid data, if any
3. Then we apply a ParDo transform to add timestamps to each data point. The timestamp is the same as the timestamp of the tick data. This is necessary to make sliding windowing work in an unbounded PCollection. The AddTimestampDoFn extends beam.DoFn and overrides process function.
4. Then we apply a sliding window to the data stream. The sliding window is calculated every minute for 10 minutes worth of tick data for each symbol. This windowing affects any grouping by key operations downstream in the pipeline
5. Now comes the interest part. We apply another ParDo to pair the individual tick data stream up and output multiple streams of paired data. Our ParDo function has the same template as the add timestamp ParDo in step 3. Note in the output streams, the key is no longer individual stock symbol, but a pair of symbols
6. Now that we have paired up our input streams, we can group the values by the pair key so that all the market data for each symbol is in the value list. In this case our key is one of (‘goog’, ‘aapl’), (‘aapl’, ‘fb’) or (‘goog’, ‘fb’). The values are the iterator of tick data for the pair.
7. Now we are ready to calculate pairwise correlation in the sliding window. We construct Pandas Series out of the tick data and calculate correlation for each pair. The output is the pair key and correlation coefficient tuple.
8. The last step in the pipeline is to add a Filter for the pair that has negative correlation below a threshold and output that pair as a trading signal onto gCloud Pub/Sub. Again we use type hints to ensure Python uses binary type to publish data onto Pub/Sub.
Now if we want to run our awesome quant trading pipeline in Google Cloud Dataflow, we can follow this Quickstart to set up our GCP project, authentication and storage. Then all we need is to run the pipeline on Dataflow using this command
In conclusion, Google Cloud Dataflow together with Apache Beam offers an expressive API for building data pipelines and transforms. Apache Beam programming model has some learning curve in terms of getting used to the transforms and windowing semantics. It forces users to think through each stage of the data pipeline and the input/output data types, which is a good thing in our opinion. Google Cloud manages a lot of magic under the hood in terms of parallelize processing, distributing units of work and auto-scale. Dataflow integrates seamlessly with other GCP products like BigQuery, Pub/Sub and Machine Learning, which makes the ecosystem very powerful for big data analytics.
As always you can find the full code discussed in this post on Cloudbox Labs github.
Google Cloud community articles and blogs
552 
7
552 claps
552 
7
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Software engineer, data infrastructure, blog @CloudboxLabs.com
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/@imrenagi/serverless-lambda-architecture-with-google-cloud-platform-35cb3123206b?source=search_post---------40,"Sign in
There are currently no responses for this story.
Be the first to respond.
Imre Nagi
May 29, 2018·7 min read
I’ve been thinking about writing a technical blog post since I join Traveloka about 4 months ago. However, because of so many things went around: work’s stuff, commencement stuff (YES! I finally graduate and become #CMUGrad),etc. , I barely had enough time to work on small POC and write several things about it.
This work actually was inspired by my Big Data AWS training couple months ago with AWS Solution Architects in Traveloka Jakarta office. On that training, they proudly told us that they now have full set of serverless solution for Big Data. It includes AWS Kinesis Firehose (AWS Messaging Service solution), AWS Glue (AWS Serverless Batch Processing Solution), AWS S3 (Cloud Storage), AWS Firehose Analytics (AWS Realtime Streaming SQL Solution) and their new product named AWS Athena used for running Adhoc query to data stored in AWS S3. Go to this link to read the complete explanation about those things. Then, I’m thinking, why not doing the same thing in GCP?
First thing first. If you are not familiar with Lambda Architecture, you might need to read some articles on internet about it. Here is the some key concepts cited from http://lambda-architecture.net/ about lambda architecture:
I started this project by using public streaming data available on the internet and by picking one simple user story. The decision comes to Meetup.com RSVP streaming API. Fortunately, meetup.com provides free public streaming API that we can use to get all RSVPs that have been made world-wide. This should be enough for us because we just want to create speed layer and batch layer consuming those data. Then the user story chosen is:
As an user, given the range of date, I should be able to get the number of RSVP created for every 15 minutes.
You must be asking why it is for every 15 minutes while we can actually put the data somewhere, run the query and get the data for every minutes. Don’t think too fast. This is just for simplifying my further explanation.
Techinically in lambda architecture, the speed layer and batch layer are made so that they can provide (near) realtime analytics to the business decision maker or business analysts. In the real world, running batch job is expensive in terms of money and time consumed by the application. On the other hand, business stakeholder simply can’t wait to get the current data until the next batch job runs on the cluster. However, it is worth to note that batch processing should be the source of the most accurate data a company or organization can have. What we can do to solve this dilema?
Streaming or speed layer comes to the resque. Speed layer provides us with the estimated data in (near) realtime manner. Yes! It is estimated! It’s really hard to get the accurate data by using the speed layer. On the other hand, speed layer can provide the user with the current data easily. So, what we can do to get the realtime data? The answer is simply by combining the data from the batch job and realtime streaming job. Yes we need to take the trade off.
Let me give you example. Assume that now is 8 AM in the morning and CEO meetup.com wants to know the number of RSVP have been made up until now since yesterday midnight. However, the last batch job run 12 AM on last night so we clearly dont have accurate data from 12 AM until 8 AM. In this case, we need to combine the accurate data from the last batch job with the estimated data from the straming job runs from 12 AM until 8AM. Once we run the next batch job and get the accurate data for today, we can simply rewrite the result written by the streaming layer in the serving layer. The idea is simple, right? Don’t forget that we sacrifice the accuracy of the data a bit in this case, but to save the costs and for faster data driven decision, it is worth investment, IMHO.
I know that you have been waiting for the buzz words. Here we go.
As visually described on the diagram above, we can breakdown the component into several parts:
This is a simple java application I wrote to pull the data from RSVP Streaming API of meetup.com and push the data to GCP Pubsub. This application runs in Kubernetes Pod and deployed in Google Container Engine. For more detail, check in on my github in event-delivery/MeetupRSVPPublisher.java.
Google Cloud Pubsub is a centralized messaging system like Apache Kafka, RabitMQ, etc. If you are familiar with Topic and Consumer Group concept in Apache Kafka, it will be easier for you to understand the concept owned by Cloud Pubsub. It has topics (equivalent to Kafka’s Topic) and Subscription (equivalent to Kafka’s Consumer Group).
In Cloud Pubsub, all consumer should pull the message from the subscription instead of directly to the topic partition like what Kafka does. Once the subscription is created, the message will start flowing from the Pubsub and are ready to be consumed by the consumer subscribing the pubsub subscriptions.
In this case, Pubsub layer is not part of Lambda Architecture. However, to help getting clearer picture and creating scalable architecture, Cloud Pubsub has a very important role to achieve it.
From the data above, it is easy to see that the rate of published message and consumed message is not that huge. Pubsub only received about 3 messages per seconds and get about 3 pull operations per seconds.
Cloud Dataflow is used as the streaming engine in our implementation of speed layer. There are two responsibilities of the speed layer in our use case. First is to write the data pulled from the Pubsub to Google Cloud Storage by using TextIO so that the Batch layer can consume these data later and run batch processing on top of it. The second is to aggregate the number of RSVP comes to the system for every 15 minutes window. Once it gets the number, it will store the result to Google NoSQL technology named Cloud Bigtable by using BigtableIO. We can also dump the data to Google BigQuery by using BigqueryIO, however we don’t really need it in this use case.
You can go to the streamprocessor/RSVPStreamPipeline.java to see what is happening. :D
The DAG is pretty simple. First, rsvpParser used to serialize String given by Pubsub to Java Object. Then, every object parsed will be grouped by using 15 minutes fixed window in rsvpGroupWindow . In order to group the RSVP, I use rsvpKeyify and rsvpGroupByKey to give every RSVP a key representing the time window of its arrival timestamp. Then to aggregate the number of RSVP within the same fixed window, I used rsvpReducer to simply accumulate the count. Then transformation to a Hbase Put object is done and then the result is stored in our CloudBigtable using BigtableIO plugin.
Dataflow jon running for batch processing is not that different with the one for batch processing. The different is only the data source used, which is the Google Cloud Storage. Other than that, the batch processing as we know are not the long running process. It only runs once in a particular range of time. For instance once a day, a week or even a month.
To get the full picture of the code, you can take a look in streamprocessor/RSVPBatchPipeline.java.
We simply decided to use NoSQL technology for the serving layer provided by GCP called Bigtable.
In designing NoSQL schema, we need to think about how we gonna query the data. Since we want to query the data based on its date, we can simply use date yyyyMMdd as the partition key of the table. To get the data for every 15 minutes, we can create a column family called count containing many columns for storing the count for every 15 minutes. The way I do it is by using string 0000, 0015, 0030, 0100 and so on as column name to represents the time window of 15 minutes. By using this schema design, we can get additional benefits if:
Both of batch and speed layer will write to the same partition and to the column name and family. The speed layer will write the estimation count and the batch layer will write the corrected count of the data.
Once the data are stored in the bigtable, the other application such as backend RESTful API will be able to read the data and exposed it to the outside world.
All works mentioned in this blog post are made available in my github repository. Feel free to take a look, submit issues or even submit Pull Request for any kind of advancement.
Google Developer Expert, Cloud Platform Engineer @gojek
513 
513 
513 
Google Developer Expert, Cloud Platform Engineer @gojek
"
https://codeburst.io/how-to-build-a-command-line-app-in-node-js-using-typescript-google-cloud-functions-and-firebase-4c13b1699a27?source=search_post---------41,"Scotch.io taught me much of what I know in web development. Earlier this year, I came across an article “Build An Interactive Command-Line Application with Node.js” by Rowland Ekemezie. I was struck by the amount of knowledge I got from the article. I came to understand how much of these cli-apps like angular-cli, create-react-app, yeoman, npm works, So I decided to replicate his work and add more technologies to it here.
In this article, we are going to build a command-line contact management system in Node.js using TypeScript, Google Cloud Functions and Firebase.
We will use commander.js for command line interfaces, inquirer.js for gathering input, Node.js as the core framework, Google Cloud Functions as FaaS (Function as a Service) that executes our functions and Firebase for data persistence.
Make sure you have Node.js version ≥ 6. Let’s create a project directory and initialize it as a Node app.
As stated earlier we will be using TypeScript instead of the normal JavaScript. TypeScript is a strict syntactical superset of JavaScript and adds optional static typing to the language.
As you can see, we installed typescript and also installed ts-node globally. ts-node is an executable, which allows TypeScript to be run seamlessly in a Node.js environment.
As you can see with ts-node, we can actually run TypeScript files (*.ts) without first compiling it to plain JavaScript, then use node contact.js to run the compiled file.
The presence of a tsconfig.json file in a directory indicates that the directory is the root of a TypeScript project. The tsconfig.json file specifies the root files and the compiler options required to compile the project. A project is compiled in one of the following ways:
When input files are specified on the command line, tsconfig.json files are ignored.
Make your tsconfig.json look like this
We will need various Node modules to achieve our goal.
After the commands above are done, our package.json will look like this.
Now, TypeScript is now configured in our Node.js app. Let's create our project files.
The project directory contact-manager should look like this.
Going back to our tsconfig.json, you will see that we targeted the es5, es6, and es8 in the lib property of our tsconfig.json file.
We need to configure our TS to use the ES2107 library, and since ES6 and ES8 are not yet well supported by all browsers we definitely want a polyfill. core-js does the job for us. We installed core-js and its @types/core-js earlier on, so we import the module core-js before our app is loaded. Let's put the following line in our polyfills.ts file.
contact.ts is the entry point of our app, so we open it up and import our polyfills.ts file.
Now, we are set to use any ES8 or ES6 features. Before, we define our app logic. Let's first setup our Cloud Functions and Firebase.
Firebase Cloud Functions run in a hosted, private, and scalable Node.js environment where you can run JavaScript code. You simply create reactive functions that trigger whenever an event occurs. Cloud functions are available for both Google Cloud Platform and Firebase (they were built on top of Google Cloud Functions).
Here, we will be using the HTTP trigger. Visit Google Cloud Platform to read more about Cloud Function triggers. Before we begin creating Cloud Functions, we have to install the Firebase tools.
To begin to use Cloud Functions, we need the Firebase CLI (command-line interface) installed from npm. If you already have Node set up on your machine, you can install Cloud Functions with:
npm install -g firebase-tools
This command will install the Firebase CLI globally along with any necessary Node.js dependencies.
Let’s create a folder that will hold our Cloud Functions.
To initialize your project:
After these commands complete successfully, your project structure looks like this:
Now, our Cloud Functions are set. They are written in plain JavaScript but we want to write it in TypeScript, then compile to JavaScript before deploying it to the Cloud.
Rename index.js to index.ts, then move into the functions folder.
Install typescript
Create tsconfig.json
Make it look like this
Open package.json and modify the scripts tag section
We need two node modules: Cloud Functions and Admin SDK modules (these modules are already installed for us). So go to the index.ts and require these modules, and then initialize an admin app instance.
Now that the required modules for our project have been imported and initialized, let’s write our Cloud Functions code. As stated earlier, we are going to write functions that will be fired when an HTTP event occurs. We are going to write functions that will handle adding, updating, deleting and listing contacts.
Lets create the barebones of the following functions listed above
In the code above, each of the functions will execute when the corresponding names are called using cURL, an HTTP request or a URL request from your browser. Let’s try out a basic Cloud Function to see how it works.
Open file index.ts and insert the following implementation:
This is the most basic form of a Firebase Cloud Function implementation based on an HTTP trigger. The Cloud Function is implemented by calling the functions.https.onRequest method and handing over as the first parameter the function which should be registered for the HTTP trigger.
The function which is registered is very easy and consists of one line of code:
Here the Response object is used to send a text string back to the browser so that the user gets a response and is able to see that the Cloud Function is working.
To try out the function we now need to deploy our project to Firebase.
npm run deploy
Note: The above compiles the index.ts to index.js, then deploys the JavaScript file ‘index.js’.
The deployment is started and you should receive the following response:
If the deployment has been completed successfully and you get back the function URL which now can be used to trigger the execution of the Cloud Function. Just copy and paste the URL into the browser and you should see the following output:
Note: Google Cloud Functions is a Node.js environment, that means you can run npm install --save package_name and use whatever package you want in your functions.
If you’re opening up the current Firebase project in the back-end and click on the link Functions you should be able to see the deployed helloWorld function in the Dashboard:
Let’s add some flesh to our functions.
Wow… We did a whole lot of things here. If you noticed, Express was brought into the play to handle RESTful requests. This is possible because as stated earlier Google Cloud Function is like a Docker container with a Node.js environment.
Let’s deploy our Cloud Function. Run this command for deployment:
npm run deploy
We are done with our Cloud Functions. Let’s move back into the contact-manager folder.
In this section, we define our controller functions that handles user input and calls the corresponding Cloud Function.
Basically, we defined the URL of our Cloud Function, which will be used based on the type of action that is to be performed. Axios is used to send request alongside the payload and to receive a response from our Cloud Function. We see here that our Cloud Function is the heart of our logic. It does the actual adding, updating, deleting, etc work and all our app does is to print the result on our console.
We need a mechanism for accepting user inputs and passing it to our controller functions defined in the step above.
Commander.js comes to the rescue. commander.js is the complete solution for node.js command-line interfaces, inspired by Ruby’s commander.
.command() Initialize a new Command.
The .action() callback is invoked when the command 'a' or 'addContact' is specified via ARGV, and the remaining arguments are applied to the function for access.
When the command arg is ‘*’, an un-matched command will be passed as the first arg, followed by the rest of ARGV remaining.
Next issue after we complete the above is how do we get user inputs. Inquirer.js solves the issue for us.
Inquirer.js strives to be an easily embeddable and beautiful command line interface for Node.js (and perhaps the ""CLI Xanadu"").
Inquirer.js should ease the process of
Note: Inquirer.js provides the user interface and the inquiry session flow. If you're searching for a full blown command line program utility, then check out commander, vorpal or args.
contact.ts
The controller functions in questions.ts are imported here. The inquirer.prompt() launches the prompt interface (inquiry session) presenting to the user the questions passed to the inquirer. It returns a Promise, answers which is passed to our controller function addContact.
Now that our tool is complete, it is time to make it executable like a regular shell command. First, let’s add a shebang at the top of contact.ts, which will tell the shell how to execute this script.
Now, let’s configure the package.json to make it executable.
We have added a new property named bin, in which we have provided the name of the command from which contact.js will be executed.
We need to compile our scripts to JavaScript, we will modify our package.json.
Run npm run build .
Now for the final step. Let’s install this script at the global level so that we can start executing it like a regular shell command.
Before executing this command, make sure you are in the same project directory. Once the installation is complete, you can test the command.
This should print all of the available options that we get after executing node contact --help. Now you are ready to present your utility to the world.
One thing to keep in mind: During development, any change you make in the project will not be visible if you simply execute the contact command with the given options. If you run which contact, you will realize that the path of contact is not the same as the project path in which you are working. To prevent this, simply run npm link in your project folder. This will automatically establish a symbolic link between the executable command and the project directory. Henceforth, whatever changes you make in the project directory will be reflected in the contact command as well.
We’ve barely scraped the surface of what’s possible with command line tooling in Node.js. As per Atwood’s Law, there are npm packages for elegantly handling standard input, managing parallel tasks, watching files, globbing, compressing, ssh, git, and almost everything else you did with Bash.
The source code for the example we built above is liberally licensed and available on Github .
If you found this useful, found a bug or have any other cool Node.js scripting tips, drop me a line on Twitter (I’m @ngArchangel).
You can find the full source code in my Github repo.
Feel free to reach out if you have any problems.
Follow me on Medium and Twitter to read more about TypeScript, JavaScript, and Angular.
Bursts of code to power through your day.
881 
5
881 claps
881 
5
Written by
I am available for gigs, full-time and part-time jobs, contract roles | 📦:kurtwanger40@gmail.com | Author of “Understanding JavaScript” https://gum.co/LikDD 📕
Bursts of code to power through your day. Web Development articles, tutorials, and news.
Written by
I am available for gigs, full-time and part-time jobs, contract roles | 📦:kurtwanger40@gmail.com | Author of “Understanding JavaScript” https://gum.co/LikDD 📕
Bursts of code to power through your day. Web Development articles, tutorials, and news.
"
https://medium.com/google-cloud/using-a-gpu-tensorflow-on-google-cloud-platform-1a2458f42b0?source=search_post---------42,"There are currently no responses for this story.
Be the first to respond.
Warning before reading this: I am very happy this blog has been useful to lots of people. Since its now over a year old some of the commands are based on older versions of software. For an easier setup you may now want to use the Google Cloud optimized compute images:
I recommend reading this blog by Viacheslav Kovalevskyi instead of continuing with this one.
This blog was written for:
Google has some nice documentation here but there were a few additional steps I needed to take. So, lets start from the beginning:
There are two ways to set the instance up: 1) you use the command-line interface that Google Cloud offers or 2) you use their incredibly friendly web-ui to help you along. Since I am a big fan of the Google Cloud web interface this is what I’ll do. Setting up a server is very simple.
When in the Google Cloud platform make sure you have created a project and then navigate to the Compute Engine. There you will be asked if you want to create a new instance and once you get the popup dialog shown here on the left you can configure the number of cores, the memory (RAM) and a little option saying “GPU”. Click on this and the additional options show up that will allow you to indicate if and how many GPU’s you want to use. I then selected Ubuntu 16.04 as a boot disk, left all the other options the same and then click Create to start the instance.
Once the instance is ready you can connect to it by either using the web-shell Google Cloud offers or by copying the gcloud command to connect from your own command line.
Now that we are in we need to install some drivers for the GPU. The GPUs on Google Cloud are all NVIDIA cards and those need to have CUDA installed. To install CUDA 8.0 I used the following commands for Ubuntu 16.04 (taken from the Google Cloud documentation):
To verify its all working properly run the command below which will show you that the GPU is recognized and setup properly.
Yay! It exists and is being recognized. We’ll also need to set some environment variables for CUDA:
NVIDIA provides the cuDNN library to optimize neural network calculations on their cards. They describe it as:
The NVIDIA CUDA® Deep Neural Network library (cuDNN) is a GPU-accelerated library of primitives for deep neural networks. cuDNN provides highly tuned implementations for standard routines such as forward and backward convolution, pooling, normalization, and activation layers. cuDNN is part of the NVIDIA Deep Learning SDK.
Deep learning researchers and framework developers worldwide rely on cuDNN for high-performance GPU acceleration. It allows them to focus on training neural networks and developing software applications rather than spending time on low-level GPU performance tuning.
Summary: they have done a lot of work to make your life easier… You will need to register for the NVIDIA Developer Program and then you can download the latest version of the software. In this case I downloaded version 5.1 for CUDA 8.0 (I just noticed a newer version 6.0 is available as well). Once downloaded move it over to the instance using SCP or via Google Cloud Storage.
Once its on the instance install it using:
So the GPU instance is running and the drivers are in place, all that is left is to get TensorFlow installed to function for the GPU. You can see Google is trying to make it super simple to get all this working because you literally need to lines to get this last step done:
Installing tensorflow-gpu ensures that it defaults to the GPU for the operations where its needed. You can still manually move certain things to the CPU whenever you want to. Lets test if it all works…
Now to test if it was all successful you can use the python code below. It assigns two variables and one operation to the cpu and another two variables and an operation to the GPU. When starting the session we are telling it via the ConfigProto to log the placement of the variables/operations and you should see it printing out on the command line where they are placed.
Thats all…
based on feedback from ChrisAMancuso I have replaced the line
with
to ensure that it installs CUDA 8.0 (CUDA 9.0 is/was not supported by TensorFlow).
Google Cloud community articles and blogs
794 
12
794 claps
794 
12
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Co-founder / CTO of @orbiit_ai. Data (Scientist) junky. All views my own.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://towardsdatascience.com/how-i-could-achieve-the-google-cloud-certification-challenge-6f07a2993197?source=search_post---------43,"Sign in
There are currently no responses for this story.
Be the first to respond.
Antonio Cachuan
Nov 5, 2019·6 min read
After following the 12 weeks of preparation recommended by Google, I passed the exam for Associate Cloud Engineer, here is what I’ve learned that could help you.
This story began 3 months ago when I was like every day checking my Linkedin feed and I saw a post from Google Cloud about the Certification Challenge. The first time I read I was considering getting…
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/swlh/10-great-courses-for-aws-google-cloud-and-azure-ec89bef8a078?source=search_post---------44,"There are currently no responses for this story.
Be the first to respond.
As 2019 comes to an end, it’s that time of year when we look to setting new goals, and focusing on what we want to learn next year.
As engineers many of those goals revolve around keeping up with every new technology and framework. In particular, technologies such as AWS, Azure and Google Cloud all…
"
https://medium.com/free-code-camp/continuous-deployment-for-node-js-on-google-cloud-platform-751a035a28d5?source=search_post---------45,"There are currently no responses for this story.
Be the first to respond.
Google Cloud Platform (GCP) provides a host of options for Node developers to easily deploy our apps. Want a managed hosting solution like Heroku? App Engine, check! Want to host a containerized app? Kubernetes Engine, check! Want to deploy serverless app? Cloud Functions, check!
Recently at work, I’ve been enjoying using our in-house continuous deployment service that quickly builds, tests, and deploys new commits pushed to GitHub. So when I read about Google’s new Cloud Build service, I wanted to take it for a spin and see if I could recreate a similar seamless deployment experience for myself. Further, in a conversation with Fransizka from the Google Cloud team, she identified this as an area where a tutorial would be helpful. So here we go…
Cloud Build is a managed build service in GCP that can pull code from a variety of sources, run a series of build steps to create a build image for the application, and then deploy this image to a fleet of servers.
Cloud Build works well with Google’s own source code repository, Bit Bucket or GitHub. It can create a build image using a Docker configuration file (Dockerfile) or Cloud Build’s own configuration file (cloudconfig.yaml). It can deploy applications (and APIs) to App Engine, Kubernetes Engine, and Cloud Functions. A really cool feature is Build Triggers. These can be setup to watch for a new commit in the code repository and trigger a new build and deploy.
This post shares the detailed steps and code to setup the continuous deployment for Node apps on GCP. It assumes that you’re familiar with developing simple Node applications, working with the command line, and have some high level understanding of deploying apps to cloud services like Heroku, AWS, Azure or GCP.
For each of the sections, a companion GitHub code repository is provided for you to follow along. Don’t sweat it though — feel free to skim over the article to learn about the high level ideas, and you can bookmark it and come to it later if you plan to set this up. The real fun of having a setup like this is that you get to deploy applications quickly.
Deploying a Node app to App Engine is quite simple. Create a new project in Google Cloud Console, add an app.yaml configuration file in our code directory (which describes the node runtime we want to use — I’ve used Node 8), and run gcloud app deploy on our terminal — and done!
If you want to try this out for yourself, here are a couple of resources:
So, what we’ve done so far by following the quickstart guide above:
….now how can we automate setup such that code changes get deployed automatically on push to GitHub?
Here is what we need to do:
2. Enable Cloud Build
3. Create a Cloud Build configuration file
This configuration has three build steps (each line starting with a hyphen is a build step) that will run npm install, then npm test and if all looks good then deploy our code to App Engine.
Each build step is just like a command we run on our machine. But in this case, since this file is in yaml and each step is split over 2 lines of name and args, it can look like a bit of a mind-bender.
Let’s try this: for the line starting with “name”, read its last word and then read the values in the “args” line. I hope this file makes more sense now!
4. Run a Build manually (optional, just for verification)
5. Create a Build Trigger
6. Run a Build automatically by pushing a commit to GitHub
Here is a screenshot for builds being triggered through a GitHub push for our app:
Too good to be true?? Run this last step a few times times to test it out a few more times. Our first application now gets deployed to App Engine on every commit to master 👏
Great, so we’ve setup our application to deploy to App Engine on GitHub push, but what if we wanted the same setup for our containerized applications? Let’s give it a spin!
At a high level, deploying a Node app to Kubernetes engine has two main tasks. First, get our app ready: Containerize the application with Docker, build it, and push the Docker image to Google Container Registry. Then, setup things on the GCP end: create a Kubernetes Cluster, create a Deployment with your application image, and then create a Service to allow access to your running application.
If you want to try this out for yourself, here are a few resources:
So, what we’ve done so far by using the guides above:
…but what we want is an continuous deployment setup such that a new commit kicks off a build and deployment.
Here is what we need to do:
2. Enable Cloud Build
3. Create a Cloud Build Configuration file
This configuration has five build steps that will run npm install and then npm test to make sure our application works, then it will create a Docker image and push to GCR and then deploy our application to our Kubernetes cluster. The values my-cluster, my-deployment and my-container in this file refer to resources in the Kubernetes cluster we have created (as per the guide we followed above). $REVISION_ID is a variable value that Cloud Build injects into the configuration based on GitHub commit that triggers this build.
4. Run a Build manually (optional, for verification)
We’re also passing the revision id in this command, since we are manually running this build vs it being triggered by GitHub.
5. Create a Build Trigger
Here is a screenshot for a build being triggered through a GitHub push for our app:
The steps in this section were pretty much the same as the App Engine section. The main differences were that we had to containerize our application with Docker, spin up our Kubernetes cluster, and then have a Cloud Build configuration with just a few more steps.
But at its core, Cloud Build and its Build Triggers work pretty much the same and give us a seamless deployment experience. Our second application now gets deployed to Kubernetes Engine on every commit to master 👏👏
Sure, App Engine and Kubernetes Engine are great, but how about automated deployments for our Serverless app? I mean, having no servers to manage at all is really the best, right? Let’s do this!
Deploying a Node app to Cloud functions will require us to create a new project. No configuration files are needed, and once GCloud functions deploy on our terminal, our functions are deployed!
If you want to try this out for yourself, here are the resources you will need:
If you’ve been following along, you can probably already picture what steps we need to do:
2. Enable Cloud Build
3. Create a Cloud Build Configuration file
Similar to the App Engine configuration, this configuration has 3 steps to install. Then test the build, and if all is good, then deploy it to Cloud Functions.
4. Run the Build manually (optional, for verification)
5. Create a Build Trigger
6. Run a Build automatically by pushing to GitHub
Here is a screenshot for build being triggered through a GitHub push for our sample app:
Cloud Functions were super easy to setup with automated builds and makes the “code → build → test → push → deploy” workflow really really fast! Our third application now gets deployed to Cloud functions on every commit to master 👏👏👏
Phew! We covered a lot of ground in this post. If this was your first time trying out GCP for Node, hopefully you got to see how easy and straightforward it is to try out the various options. If you were most eager to understand how to setup continuous deployment for apps on GCP, I hope you weren’t disappointed either!
Before you go, I just wanted to make sure that you didn’t miss the fact that all the sections had a sample app: Hello World for App Engine, Hello World for Kubernetes Engine and Hello World for Cloud Functions.
That’s it for now! Let’s go ship some code! 🚢
Thanks for following along. If you have any questions or want to report any mistakes in this post, do leave a comment.
If you found this article helpful, don’t be shy to 👏
And you can follow me on Twitter here.
We’ve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
1K 
7
1K claps
1K 
7
We’ve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
Written by
Engineering Manager at Facebook. Posts about software engineering, mobile app reliability and performance.
We’ve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
"
https://medium.com/google-cloud/how-to-run-visual-studio-code-in-google-cloud-shell-354d125d5748?source=search_post---------46,"There are currently no responses for this story.
Be the first to respond.
Did you know you can run Visual Studio Code in a browser? It’s so cool, it even works with extensions and all that. I’ll show you an example using github.com/cdr/code-server
Note that you don’t need authentication since the Google Cloud Shell proxy already handles that for you.
If you get a 404, remove ?authuser=0 from the url. So for example, justhttps://8080-dot-YOURID-dot-devshell.appspot.com/instead ofhttps://8080-dot-YOURID-dot-devshell.appspot.com/?authuser=0
Google Cloud community articles and blogs
719 claps
719 
15
Written by
Google Cloud Customer Engineer
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Google Cloud Customer Engineer
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://towardsdatascience.com/90-second-setup-challenge-jupyter-tensorflow-in-google-cloud-93038bcac996?source=search_post---------47,"Sign in
There are currently no responses for this story.
Be the first to respond.
Cassie Kozyrkov
Apr 30, 2020·5 min read
Data science enthusiasts, how fast can you go from zero to Google Cloud Jupyter notebook? Let’s find out!
If you’re in the mood to ultra-customize your setup, Google Cloud gives you dizzying granularity. That’s a fabulous thing for teams starting an enterprise-scale project, which scales up the consequences of every corner you cut.
Do beginners have to trudge through a forest of options to get started?
But what if you’re a beginner who just wants to play around with a data science notebook? Do you have to trudge through that forest of options to get started? Great news: you don’t. You can get up and running in under 90 seconds! (Feel free to skip to the walk-through below.)
You can get up and running in under 2 minutes! (Walk-through below.)
If this is your first time, I bet you’re probably especially keen to get to hello world as quickly as possible. You’ll want to skip the control panel and use someone else’s setup solution. If so, you’ll love the Google Cloud Marketplace!
Welcome to a collection of prebaked machine images that you can use as a springboard for your own work. And yes, you can customize these third party templates later if you need to (learn more here).
A collection of prebaked templates.
Enough preamble! Let’s see if it is possible to go from zero to hero in under 2 minutes. I’ll break down the steps below — this is just the proof that it can be done. Here’s we go:
First, what didn’t we watch? Steps 0.1–0.3.
You’ll need a Google account to use Google Cloud. I’ve had mine for a while, so that’s not shown.
My video starts on the Google Cloud Dashboard screen, which you’ll get to by clicking console.cloud.google.com. First-time users will be prompted through a few housekeeping items.
I’m using a project I made called ItsTimeForDemos. To make a new project, I clicked on this part of the dashboard:
Now that we’re ready, let’s examine what we saw on my screen.
To skip reinventing your own wheel, you’ll head to Google Cloud Marketplace.
In the search bar, I’ll enter the keywords “ai cpu.”
Why AI? I felt like doing this demo with an AI-geared notebook. If you’re looking for something else, simply ask the search bar. There are almost 2000 solutions here.
Why CPU? It’s the vanilla option. More powerful hardware — like GPUs — is more powerful… which usually also means it’s more expensive and will burn through your $300 free credits faster. Unless you already know why you need the beast option, it’s probably a good idea to start with its lightweight cousin.
Among the solutions, I picked the TensorFlow Jupyter notebook. You pick whatever you like.
Why notebook? Data scientists like notebooks because they combine the best of interactive data analysis with pretty report-making.
Why Jupyter? It’s the notebook solution you’re most likely to have heard of.
Why TensorFlow? It’s a powerful deep-learning framework; get more info here.
Hit the big blue button. Repeat.
A randomly-generated password will be created for you. Copy its text while you wait patiently for a few seconds, then open the Jupyter notebook when that option becomes available.
Paste in your copied password to open Jupyter, then create a new notebook.
Once you’re in, you’ll see that TensorFlow is installed and you’re ready to start exploring.
Getting up and running took less than 90 seconds!
Is this the best data science environment you can have? No, because creating the perfect solution for you means customizing everything to your personal needs. One day, that’s probably what you’ll choose to do. But in the meantime, this was the shortest path to getting up and running that I’ve found. My goal with this blog post was simply to show you that there are super-quick options out there.
Can you do it faster than me?
Can you do it faster than me? Maybe! Consider it a challenge and let me know how many seconds it takes you.
To avoid charges, don’t forget to turn out the lights when you’re done; use the search bar to find the Deployment Manager page where you can delete/stop whatever you don’t need anymore.
This blog post was written in April 2020, so if you’re from The Future, things might look different in your world.
If you had fun here and you’re looking for an applied AI course designed to be fun for beginners and experts alike, here’s one I made for your amusement:
Head of Decision Intelligence, Google. ❤️ Stats, ML/AI, data, puns, art, theatre, decision science. All views are my own. twitter.com/quaesita
702 
6
Every Thursday, the Variable delivers the very best of Towards Data Science: from hands-on tutorials and cutting-edge research to original features you don't want to miss. Take a look.
702 claps
702 
6
Your home for data science. A Medium publication sharing concepts, ideas and codes.
About
Write
Help
Legal
Get the Medium app
"
https://towardsdatascience.com/giving-vertex-ai-the-new-unified-ml-platform-on-google-cloud-a-spin-35e0f3852f25?source=search_post---------48,"Sign in
There are currently no responses for this story.
Be the first to respond.
Lak Lakshmanan
May 19, 2021·7 min read
The Google Cloud AI Platform team have been heads down the past few months building a unified view of the machine learning landscape. This was launched today at Google I/O as Vertex AI. What is it? How good is it? What does it mean for data science jobs?
The idea is that there are a few key constructs in machine learning:
Yet, depending on how you do your ETL (do you store your data in CSV files? TensorFlow records? JPEG files? In Cloud Storage? In BigQuery?), the rest of the pipeline becomes very different. Wouldn’t it be nice to have the idea of an ML dataset? That any downstream model (TensorFlow, sklearn, PyTorch) can use? That’s what it means to unify behind the concept of a dataset.
Also, the way you deploy a TensorFlow model is different from how you deploy a PyTorch model, and even TensorFlow models might differ based on whether they were created using AutoML or by means of code. In the unified set of APIs that Vertex AI provides, you can treat all these models in the same way.
Vertex AI provides unified definitions/implementations of four concepts:
The key idea is that these artifacts are the same regardless of the type of dataset or training pipeline or model or endpoint. It’s all mix and match. So, once you create a dataset, you can use it for different models. You can get Explainable AI from an endpoint regardless of how you trained your model.
This is clearly visible from the web UI:
Okay, enough of the why. Let’s take it for a spin.
It’s always good when you are training an ML model using new technology to compare it against something that you know and understand.
I’ll use the Airbnb New York City dataset on Kaggle to predict the price of an apartment rental.
First, we load the data into BigQuery. This data set is a bit unwieldy and required cleanup before I could load it into BigQuery (see this blog post for details), but I made it public, so you can simply try out the query:
Having verified that the data look reasonable now, let’s go ahead and train an xgboost model to predict the price. This is as simple as adding two lines of SQL code:
We get a model that converges quickly
…and achieves a mean absolute error of $64.88.
How good is this? What’s the mean price of a New York city apartment on Airbnb?
That is $153. So, we are within a 40% off. Maybe not a model to go marching into production with … but you wouldn’t expect a public dataset to have the more proprietary and personalized data that would help improve these predictions. Still, the availability of this data helps show us how to train an ML model to predict the price.
Next, let’s try Auto ML Tables in Vertex AI.
Let’s start the with CSV file that is on Kaggle so that we are not stuck with the data transformations I did to load the data into BigQuery. Go to https://console.cloud.google.com/vertex-ai/datasets/create
About 3 hours later, the training finished, and we can examine the model and deploy it if we’d like:
We get a Mean Absolute Error of $55, almost 20% better than what we got with xgboost. So, a more accurate model, with less code. With evaluation, feature importance, explainability, deployment, etc. all provided out-of-the-box.
Both BigQuery ML and AutoML Tables are easy-to-use and awfully good. It helps to understand the fundamentals of the domain (why are reviews missing?), and of machine learning (don’t use the id column as an input!), but they are quite approachable from the perspective of a citizen data scientist. The coding and infrastructure management overhead have been almost completely eliminated.
What does this mean for data science jobs? The move from writing Raw HTML to using weebly/wix/etc. hasn’t meant fewer web developer jobs. Instead, it has meant more people creating websites. And when everyone has a basic website, it has driven a need to differentiate, to build better websites, and so more jobs for web developers.
The democratization of machine learning will lead to the same effect. As more and more things become easy, there will be more and more machine learning models built and deployed. That will drive the need to differentiate, and build pipelines of ML models that outperform and solve increasingly complex tasks (not just estimate the price of a rental, but do dynamic pricing, for example).
Democratization of machine learning will lead to more machine learning, and more jobs for ML developers, not less. This is a good thing, and I’m excited about it.
And, oh, Vertex AI is really nice. Try it on your ML problem and let me know (you can reach me on Twitter at @lak_gcp) how it did!
Data Analytics & AI @ Google Cloud
See all (63)
451 
2
Every Thursday, the Variable delivers the very best of Towards Data Science: from hands-on tutorials and cutting-edge research to original features you don't want to miss. Take a look.
451 claps
451 
2
Your home for data science. A Medium publication sharing concepts, ideas and codes.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/continuous-delivery-in-google-cloud-platform-cloud-build-with-app-engine-8355d3a11ff5?source=search_post---------49,"There are currently no responses for this story.
Be the first to respond.
Agile and DevOps continue spreading among IT projects — and I would say: at a pace that we have never seen before! As the interest in these matters increases, the set of processes and tools which make it possible to deliver better software also increases.
At the end of the software development lifecycle is the delivery “phase”. And if the delivery is not fast, the whole Agile development process may be broken. Modern software should be designed to be fast delivered, using appropriate automation tools. In this context, I’ll write about Continuous Delivery in the Google Cloud Platform. It's a series of 3 articles, covering deployment to App Engine, Compute Engine, and Kubernetes Engine.
Starting with App Engine, GCP’s fully managed serverless application platform, I will show the steps to set up a development + automated build +continuous delivery pipeline. To keep it practical, we’ll create a very simple front-end application and deploy it to App Engine Standard Environment. Please remember we will focus on delivery and not on app development itself.
Before proceeding, make sure you have created a GCP Project and installed Google Cloud SDK in your machine if you wish to run the examples. Don’t forget to run gcloud auth login and gcloud config set project <your-project-id> for proper gcloud CLI usage.
A simple way to create a front-end web application is by using Angular CLI. The steps to install this tool are out of the scope of this article and can be found here. Once installed, cd to your preferred folder and type ng new <app-name>. Wait a few seconds. After the app is created, type cd <app-name> and ng serve. Point your browser to http://localhost:4200 and make sure the app is running. As we have a fully running app, we stop the development here ;). It’s time to think about delivery!
An Angular app is a set of HTML, CSS, JS, images, and other web-related static files. So, what we need to do now is to build the app and deploy it to an HTTP server in order to make it available to the users. To build the Angular app, we run npm install and npm run build --prod in the <app-name> folder. Angular CLI then creates a new dist folder, where it places the ready-to-deploy content. We could figure lots of ways to copy these files to the webserver, but following this path we are adding complexity to our build process, don’t you agree? As we desire simplicity and automation instead of complexity and manual tasks, we’ll learn how to use Cloud Build.
Cloud Build is a Google Cloud Platform fully-managed service that lets you build software quickly across all languages, counting on containers to get the job done. Having that said, let’s prepare our project to use Cloud Build. Add a file named cloudbuild.yaml to your project’s root folder, with the following content:
This simple file instructs Cloud Build on how to build and deploy the app, similar to a Docker multi-stage build. When we invoke Cloud Build using the command gcloud builds submit --config cloudbuild.yaml ., it will compress the project’s source files (a .gitignore file, if present, will be considered in order to determine which files shall be skipped), and copy the tarball to a managed Cloud Storage Bucket. It happens in your development machine.
Then, running on GCP servers, Cloud Build will uncompress the sources and execute npm install in a container of Cloud Build’s built-in image gcr.io/cloud-builders/npm. The second step will use another npm container to run npm run build --prod. The third one will run a container of another Cloud Build’s built-in image, named gcr.io/cloud-builders/gcloud, to execute a gcloud app deploy command.
I've already talked about the 1st and 2nd steps, but not about the 3rd one, gcloud app deploy, so let me do it. This command is responsible for deploying the built content to App Engine and finish the whole delivery process. In order to succeed it requires an app.yaml file in the project’s root folder (see sample content below), the <your-project-number>@cloudbuild.gserviceaccount.com Service Account must have the App Engine Admin role, and the App Engine Admin API must be enabled for the GCP Project in which the app will be deployed. This step replaces copying the built files to an HTTP server — mentioned before — with copying them to GAE Python 2.7 Standard Environment, where they can be served as static content.
Point your browser to https://<your-project-id>.appspot.com or click a service's name in Cloud Console to see the app running on App Engine! Also, visit https://console.cloud.google.com/cloud-build/builds?project=<your-project-id> to check your project’s build history.
Ok, now we have an almost fully automated deployment process.
— Hey, Ricardo, what do you mean by “almost fully automated”?
— I mean GCP can help us doing better than this :).
We have seen how Cloud Build interoperates with Cloud Storage and App Engine, but there’s a missing piece: Source Repositories.
Git is widely used as a source control management tool nowadays. Source Repositories allow you to use GCP as a Git remote repository or to automatically mirror repositories from Github or Bitbucket. Visit https://console.cloud.google.com/code/develop/repo?project=<your-project-id>, click CREATE REPOSITORY, and follow the steps — it’s pretty straightforward.
Once the repository is set, navigate to Cloud Build’s triggers page: https://console.cloud.google.com/cloud-build/triggers?project=<your-project-id>. Click ADD TRIGGER, select a source, a repository, and proceed to trigger settings. Give a name to the trigger, let's say Push to master branch, select Branch as the Trigger type, type master as the Branch (regex), and select cloudbuild.yaml as the Build configuration. Type /cloudbuild.yaml for the cloudbuild.yaml location and save. And here we go: every time a new commit is pushed to master, the build process is automatically triggered. We don’t need to manually execute gcloud builds submit … to trigger the build anymore. The deployment process is now fully automated!
The steps described in this article can be used or customized to deploy any kind of application supported by Google Cloud Platform’s App Engine Standard Environment. Deploying to Flexible Environment requires changes in folders' structure.
The sample code is available on Github: https://github.com/ricardolsmendes/gcp-cloudbuild-gae-angular. Feel free to fork it and play.
Happy deploying :)
2019–08–04: I created a GitHub repository to demonstrate how a similar deployment might be done to App Engine Flexible Environment. Please refer to gcp-cloudbuild-gae-flex-angular for details.
This is the 1st of a 3-article series on Continuous Delivery in Google Cloud Platform:
App Engine | Compute Engine | Kubernetes Engine
Google Cloud community articles and blogs
515 
4
515 claps
515 
4
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
head of data @ ciandt.com • hobbyist tech writer • dad • birder
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/how-to-run-deep-learning-models-on-google-cloud-platform-in-6-steps-4950a57acfa5?source=search_post---------50,"There are currently no responses for this story.
Be the first to respond.
Top highlight
Using Deep Learning Virtual Machine
Google Cloud Platform is a tool provided by Google which one can leverage to build large scale solutions. This platform has recently gained a lot of popularity because of their easy access to GPUs. Additionally, they also give you $300 worth of credits for free with a year of validity which depending on the kind of processing you need to do can last up to a year.
However, some of the instances can get really challenging since there isn’t a proper GUI or many packages won’t get installed. In this blog I am going to talk of an easy way to deploy a marketplace solution for running Deep Learning model. Moreover, this would also create a Jupyter Notebook GUI that can be used to view quick results.
The first thing you need to go is setup a google cloud account.
Go to https://cloud.google.com/ and sign in using your Gmail account. If you have a school or organization account it may lead to some collaboration issues in the future so I would strongly recommend to create an account using your personal Gmail account. If you don’t have a Gmail account, now maybe a good time to create one.
While signing in, Google will ask you to share your credit card details. You can put them in but your credit card wont be charged unless you have run out of all your $300 worth of credits, so you need not worry.
Once you have signed in, it will take you to a console screen which should look something like this.
If you don’t get an automatic assignment of a project, you should go ahead and create one. Click on the “Create New Project” icon and the banner and create a new project. The project ID is assigned automatically, however you can change it if you like. I decided to stay with the default project ID assigned to me.
Now that you have an account and project, you can deploy a marketplace solution.
Your billing will only start once you deploy the solution.
To set up a deep learning marketplace solution, search for “Deep Learning VM” in the search bar. This should take you to the landing page for “Deep Learning VM”.
The advantage of using Deep Learning VM is that we don’t have to install python or tensorflow since it is a part of a pre-packaged image developed by Google. Once you’re on this page, you just hit the “Launch Compute Engine” button. This page also shows the number of past deployments you have for this engine. It is 3 in my case.
Once you launch the compute engine, you will be taken to the configuration page, where you can set a name for the environment, select the zone for the machines and select the number of CPUs and GPUs you would want.
It is important to note the Zone you select for your deployment since the machine configuration you choose will depend on it. For example, there maybe restrictions in some zones on the number of CPUs and GPUs you can access.
Depending on the kind of machine that you choose, you can see the billing amount on the right hand side change accordingly.
For example, if you choose 16 CPUs and 0 GPUs, you can see that you will be charged $392.36 per month if you use 730 hours per month. If you hit “Details” it will give you the break up for the billing. Generally, GPUs are more expensive than CPUs, so if you don’t need GPUs, it is better to skip them altogether. You also need to request for GPU quota in the zone of your deployment (which I will talk about in detail in Step 6).
For now, choose Zone: us-west1-b, 16 CPUs and GPUs as “None”. The next thing to choose is the size of your hard-drive. “Standard Persistent Disk” should be good for any project, but if you want you can expand the memory if you have a lot of data. Keep in mind larger disk sizes will lead to bigger bills, so best to be parsimonious about the requirements. They can always be modified later on if required (covered in Step 5).
Once you have selected the config hit “Deploy”. Based on your selection, it may take 5 to 10 mins for the deployment to set up. If you get an error after deploying, check to see if you selected GPUs by mistake. If you select GPUs without having an assigned quota, it may lead to an error. Just create another deployment without any GPUs and you should be good to go.
Now there are 3 different ways of running code on this VM. The easiest is using a GUI of Jupyter Notebook which runs on localhost:8080 on your machine. To access this, you need to install Google SDK to SSH this VM.
You can install Google SDK here. Initialize Google SDK and connect to your google account and project that you created. Initialization options should show up automatically after installation, if it doesn’t you can run the command : gcloud init and make sure that you connect with the same email and project ID as before.
Once you have Google SDK installed and configured, you just copy the SSH link that shows up on your deployment page and paste it on to the Google SDK. The SSH link will be under the header “ Create an SSH connection to your machine” (see image below)
If you have successfully created a SSH connection, a PuTTY screen will pop-up (image below)
Once you have your SSH setup, you are just once click away from your Jupyter GUI. Go back to the deployment manager and hit the localhost:8080 button
Voilà!! That will take you to the Jupyter Notebook instance that is deployed on 16 CPUs. You can use this like any machine.
Additionally you can also run python batch jobs on the PuTTY terminal or by hitting the SSH terminal in the Compute Engine VM. More on that in the next step.
Before we add GPUs we need to request GPU quota in the same zone our instance is deployed on.
Just search for “Quotas” in the search bar and that should take you to the Quotas page under “IAM & admin”.
Here under “Metrics” first, select “None” and then search for GPU. Based on the GPU present in your zone you can select the name of the GPU and we also need to select “GPUs (all regions)”. For example, since our zone is us-west1-b, you can select the “NVIDIA P100 GPUs” and “GPUs (all regions)”
Once you select both the GPUs, you hit the “Edit Quotas” button on top.
Make sure that the GPU you select is in the same zone as your instance, or else your deployment will not be able to access it.
This will generate a form where you need to share your personal phone number and reason for this request. As soon as you submit this form, you will get an email from Google saying that your request is under process and it will take 2–3 business days.
Although their email says 2–3 business days, the request is approved within a couple of hours. Once your quota request is approved, you can edit your virtual machine to include more GPUs.
To add the requested GPUs, you need to edit the instance that is created on the Compute Engine page. Go to the menu on your google console and then hit “Compute Engine”.
The VM instances pages gives a list of the VMs that we have installed across various solutions on google cloud platform. An important thing to note here is that we should stop all instances if we do not want to be billed for the machines. Even if we don’t run any code, google charges us for the instances.
Hence it is important to stop all instances when we aren’t running anything.
Once you have stopped the instance, you can edit it. If your quota request is approved, you should be able to add more GPUs and deploy the solution again in no time.
Another neat trick is to “Enable connection to serial ports” and “Allow full access to cloud APIs” (under Access scopes) to enable your instance to talk to buckets and vice versa.
Once your config has been modified by adding another GPU, you can either run a deep learning model on the Jupyter Lab UI or the PuTTY terminal. You will notice that it will be much faster as we have added GPUs to our system. This also means our bill is higher so make sure to keep checking the “Billing” page to ensure that you don’t run out of credits.
Below is a short video on how I accessed the Jupyter Notebook GUI on the cloud to run models.
You can copy data from your bucket to the instance that you just created using “gsutil” feature of GCP.
You can either use the PuTTY terminal or the SSH on the Compute Engine to write this command.
More details on gsutil here.
Sometimes we end up storing large files or objects on the virtual jupyter notebook. This might lead to memory constraints. However just deleting them doesn’t free up memory. Instead we need to manually go and clear the trash from the jupyter location in the VM to free up memory.
To check the available memory run, you can use du -sh * in the PuTTY terminal. Once you navigate to the jupyter folder in PuTTY, you can force delete it to free up memory. In my case the command:
rm -rf /home/jupyter/.local/share/Trash
worked. By running du -sh* again, I could observe that the available memory in dev/sda1 environment went up after running this command.
I am sure there are many other ways to run Deep learning models on Google Cloud Platform without investing too much time in setting up an environment.
What has been your experience with GCP? Please share your comments and let me know if I can help solve your queries in any way.
Google Cloud community articles and blogs
509 
7
509 claps
509 
7
Written by
End to end problem solving. Data nerd. Facebook, UT Austin
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
End to end problem solving. Data nerd. Facebook, UT Austin
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/using-google-cloud-ai-platform-notebooks-as-a-web-based-python-ide-e729e0dc6eed?source=search_post---------51,"There are currently no responses for this story.
Be the first to respond.
This post contains instructions and advice on how to set up and use Google Cloud AI Platform Notebooks as a development environment. It is targeted to Software Engineers who want to know how to use and find common IDE features, but is relevant to anyone interested in learning how to use Google Cloud AI Platform Notebooks or JupyterLab.
Google Cloud AI Platform Notebooks are built on JupyterLab. JupyterLab is web-based development environment that includes a Jupyter notebook editor, a file browser, a terminal, a text editor with code highlighting, customizable themes, a drag-and-drop tiled layout, and support for custom themes and extensions. All of these elements work side-by-side in an extremely responsive webpage.
A year ago I switched from PyCharm to JupyterLab (running on Google Cloud Deep Learning VM Images, which form the basis of AI Platform Notebooks) as my main development environment and I have not looked back.
In my day-to-day work as Machine Learning Engineer working with Google Cloud customers, I continually had challenges with my previous setup of PyCharm running on a Macbook. Most of my work requires customer data, which cannot leave the cloud. This meant I spent a lot of time in vim on a VM, relied heavily on managed services (which are great for large jobs but too slow when you just want to poke around), or struggled to create suitable synthetic data. Even when I could fully utilize PyCharm I’d run into RAM problems if a dataset was more than a couple gigabytes, or I’d encounter security problems as I traveled and made API calls to cloud projects from mysterious hotel IP addresses.
I also work a lot with data scientists, who have their own preferred tooling and workflows. Trying to get data scientists to use software engineer tools and workflows is a struggle (and hurts their productivity). But asking software engineers to switch to data science tools is absurd — you cannot build software with notebooks. And forget about trying to get a development team to unify their workflows if they are using fundamentally different tools.
There are some PyCharm features I miss, mainly advanced code navigation features. But I gained far more than I lost:
To run AI Platform Notebooks, you need a Google Cloud Platform project (with an attached billing account if you want to use GPUs) with the Compute Engine API enabled. Instructions are here. If you’re already a Google Cloud user you can probably skip this step.
Many of the commands in this post use the Google Cloud gcloud command-line tool. Most of these steps are possible with the AI Platform Notebooks UI, but using gcloud gives more control and better reproducibility.
You can install gcloud on your local machine, but if you don’t already have a machine with gcloud just use Cloud Shell, a web-based terminal session with everything you need already installed. If you already have a Google Cloud project and are logged in, click this link to launch Cloud Shell.
If you are using gcloud for the first time, run gcloud init to authorize. You can specify project/region/zone defaults if you wish, but be aware AI Platform Notebooks and GPUs are not available in all regions yet. Currently, the best way to see what zones everything is available in is to look at the regions available when creating a new Notebooks instance.
First, create an AI Platform Notebooks VM instance. You can do this in the Notebooks UI but using gcloud gives more options:
You may want to change the command to meet your specific needs.
gcloud compute instances create $INSTANCE_NAME: all VM creation gcloud commands start with this.
--zone=$ZONE: the zone you want your VM created in. Only some zones have GPUs, but any zone in a region that supports AI Platform Notebooks is fine. Currently, the best way to see what zones everything is available in is to look at the regions available when creating a new Notebooks instance.
--image_family=$IMAGE_FAMILY: specifies the image to use to create the VM. An image family is a group of related VM images; pointing to an image family instead of a specific image ensures the most up-to-date image in the family is installed. The tf-latest-gpu image family contains VM images for running TensorFlow on GPUs — all the necessary tools (including JupyterLab) are preinstalled and key binaries are custom built for increased performance. This image family is part of Google Cloud Deep Learning VM Images, the product Notebooks is based on. Deep Learning VM image families are available for popular deep learning frameworks, no deep learning framework at all, CPU-only, etc. If you want to use a specific version of an image, use --image instead of --image-family and specify a valid Deep Learning VM image version.
--machine-type=$INSTANCE_TYPE: determines the RAM and cores of your VM. Many different configurations are available. Note you can easily change this later.
--image-project=deeplearning-platform-release: in what project to find the specified --image or --image-family . The deeplearning-platform-release project holds images provided by Google Cloud Deep Learning VM Images, don’t change this if you’re creating a Notebooks VM.
--maintenance-policy=TERMINATE: what happens to your VM during a maintenance event. Most VMs can be live migrated but if GPUs are attached to your VM live migration does not work. If GPUs are not attached to your VM (and don’t intend to ever attach GPUs) you can leave this line out.
--accelerator='type=nvidia-tesla-v100,count=2':the type of GPU to attach and how many to attach, see the documentation for available GPUs/counts. If you are not using GPUs, you can leave this line out.
--no-boot-disk-auto-delete: the default behavior on VM deletion is to delete the boot disk, this overrides that default and does not delete the boot disk when the VM is deleted. This means if you accidentally delete your VM you can still recover your work. But it also means you need to delete the disk separately from your VM if you want to remove everything.
--boot-disk-device-name=$INSTANCE_NAME-disk: creates a disk based on the name of the VM.
--boot-disk-size=500GB: adjust to your taste, 100GB or above is best.
--boot-disk-type=pd-ssd: makes your disk an SSD for better performance.
--scopes=https://www.googleapis.com/auth/cloud-platform: gives your VM the ability to connect to Google Cloud Platform APIs. This means you will be able to use services like BigQuery, Cloud Storage, AI Hub, etc. from your VM. It is also required to create a direct connection URL to your VM. You can name only the scopes you need, use service accounts, or leave this line out and use the Compute Engine default service account. But be aware with reduced scopes you will not get a direct connection URL to your VM, and then you’ll need to use an SSH tunnel to connect.
--metadata=’install-nvidia-driver=True,proxy-mode=project_editors’: metadata entries are used by Deep Learning VMs to pass parameters to installation and startup scripts. If you want to use GPUs install-nvidia-driver=True installs the driver, you can leave this out if you are not using GPUs. proxy-mode=project_editors creates a direction connection URL to your VM and adds it to the Notebooks UI. You can forgo this line as well and use an SSH tunnel to connect to your VM.
One optional flag to add, especially if you want a beefy VM now or later and are not using GPUs, is --min-cpu-platform=Intel\ Skylake. This ensures your VM is on the Skylake CPU platform (if your zone supports it), which allows VMs with more cores and RAM than other platforms. You can also use other CPU platforms.
Your VM will be created a few minutes after the gcloud command finished. A few minutes after that, your VM is assigned a URL for direct connection and appears in the AI Platform Notebooks UI:
You can use the Notebooks UI to connect to your VM by clicking “OPEN JUPYTERLAB”, but you can also get the URL using gcloud:
You’ll see something like this: numbersandletters-dot-datalab-vm-region.googleusercontent.com.
This URL provides a secure web connection to your VM. Just put it in your web browser and you’ll connect to your JupyterLab session. Be aware other project users with high privileges can use this URL as well — it is not accessible only by the VM creator.
If you created your VM without proxy_mode=project_editors or did not set the https://www.googleapis.com/auth/cloud-platform scope, you need to connect to your VM via an SSH tunnel.
Once you’re connected to JupyterLab, you’ll see the default AI Platform Notebooks JupyterLab UI:
The Jupyterlab interface is powerful and highly customizable:
Out of the box, JupyterLab comes with a Jupyter notebook viewer/editor, a terminal, viewers for many file types, a text editor with syntax highlighting, and a file browser. It’s worth browsing the entire JupyterLab User Guide to learn about the features, but at least check out the basics of the interface.
AI Platform Notebooks include additional functionality beyond base JupyterLab thinks to a few pre-installed extensions: git, tensorboard, and nbdime (notebook diffs).
When you are not actively working on your VM, stop it to save money and project resources. When your VM is stopped your settings, environment, data, and work are still intact and waiting for you when you start the VM again.
You also need to stop your VM to resize it. Resizing your dev VM is a quick and dirty way to run something that would otherwise take too long or not fit in memory, or to attach more GPUs for a large training job. You can change your VM machine type with gcloud:
To attach, unattach, or change GPUs, you need to use the Notebooks UI.
Once you’ve changed your VM configuration, start your VM:
The ability to resize and respec stopped Notebooks VMs is extremely useful. You can prototype your model training on a pair of cheap GPUs and then switch to eight P100s once you know everything works. You can work effectively with datasets up to 100s of GB with standard Python packages (just make sure your packages can take advantage of multiple CPU cores; for Pandas on multiple cores check out modin). And when you no longer need the extra power you can save money by reducing the specs of your VM.
Note there are restrictions on VM size. Different CPU platforms support different machine types, and there are additional limits on cores/RAM when GPUs are attached to a VM.
Update 4/27/19: the latest VM images should allow the jupyter user to sudo from the JupyterLab terminal.
When you’re working on your VM, you are logged in as user jupyter. For better security, this user has the permissions you’ll need for day-to-day development work but has no root access to the VM or sudoer privileges.
At some point you will need sudoer privileges. You can get sudoer privileges by connecting to the VM through an SSH connection. You can do this with the Compute Engine UI, or with gcloud:
When you connect via SSH you connect as your default Google Cloud Platform user. This gives you sudoer privileges.
Depending on how your project is set up, you may not be able to SSH to your VM because port 22 is closed by default — this is a good security practice. To enable SSH, you’ll need to open port 22 by creating and applying a firewall rule.
First, create a high priority firewall rule that opens port 22:
allow-ssh is the name of the rule, --allow=tcp:22 opens TCP port 22, priority=0 makes this firewall rule take precedence over other firewall rules (lower priority=high precedence), --description is how your firewall rule is described (optional but useful), and target-tags are the VM tags that identify what VMs this rule applies to. If you’re extremely security conscious, consider using the --source-ranges parameter to only open port 22 to select IP addresses.
Next, apply the firewall rule to your instance by tagging your instance appropriately:
Now you are able to connect via SSH. Once you’re done, keep your VM more secure by untagging your VM until you need SSH access again:
Python environment management is an important consideration in a Python developer workflow. When using AI Platform Notebooks, you may not want to independently manage your environment. There are some under-the-hood optimizations you will lose (including optimized binaries of some packages) and you may run into problems with version compatibilities as you install the packages you need. You may find it easier to maintain separate VMs for each of your projects rather than separate environments.
But sometimes you need a tightly controlled environment, and the default Notebooks environment has many packages pre-installed (run pip list to see).
The most popular tools for managing Python environments are pipenv, virtualenv, and virtualenvwrapper (which extends virtualenv). If you have a preferred tool, you can use it with Notebooks (but you may require SSH to set it up properly, see Step 5 above) and you may want skip the content on virtualenv basics and skip down to “Creating a Jupyter Kernel for a Virtual Environment”.
virtualenv is the entry-level tool for environment management, and is pre-installed when a Notebooks VM is created. The general idea of virtualenv is you create a “virtual environment” with its own version of Python and its own installed packages. When you want to work in that environment you “activate” it, and then whenever you usepip only the active environment changes. When you’re done working you “deactivate” your environment. Virtual environments live on your disk in folders. You can have as many virtual environments as you want and you manage them yourself.
Creating a virtualenv virtual environment is as simple as running the virtualenv command followed by a target directory where your virtual environment will live:
This example creates a virtual environment in the venv directory.
Activate the virtual environment by running bin/activate in the environment directory:
You’ll see the shell prompt changes to have (venv) in front of it. This lets you know you’re in the virtual environment. Now anything you do with pip only changes the activated virtual environment.
When you’re done working in the environment, run the deactivate command to leave the environment:
A popularvirtualenv usage pattern is to create a virtual environment in every project you work on in a venv directory in the project root. If you do this, consider putting venv in your .gitignore file so you do not check it into a remote git repo. Another popular approach is to keep a single ~/envs directory with subdirectories for your different environments. And once you understand the basics of virtual environments, its worth checking out virtualenvwrapper which automates virtual environment management.
If you want to remove your virtual environment permanently delete the directory:
virtualenv also supports the creation of virtual environments with different Python versions:
Note that whatever comes after -p is the path to an executable Python interpreter. On a Notebooks VM python2 and python3 are in the path and point to the latest versions compatible with the pre-installed packages, but to create an environment with a very specific version of Python you may need to install that version yourself.
To make your code portable, create a requirements.txt file from inside your environment with pip freeze and share it with your code.
You may want a Jupyter kernel associated with your virtual environment. This lets you run notebooks and consoles from your virtual environment. Run this code in your virtual environment to install the ipykernel package and create the kernel for the jupyter user. KERNEL_NAME is the internal iPython name, but DISPLAY_NAME is what you’ll see in the interface:
You’ll need to refresh the JupyterLab web page before notebook and console icons for the new kernel appear in the launcher.
Be aware of your TensorFlow flavor when using virtual environments. TensorFlow has separate packages for GPU vs. non-GPU. Installing either version into your virtual environment should work, but make sure to install the right package to match your configuration (GPU vs. no GPUs). This also means you should keep separate virtual environments for both GPU and non-GPU if you plan to switch between the two. Cloud AI Platform Notebooks should handle switching automatically in the default environment (and install the correct binaries and Nvidia stack as appropriate), but you’ll need to handle it yourself if you are managing your own environments.
JupyterLab supports custom extensions (and themes) and provides detailed documentation to developers to promote the creation of new extensions. Extensions are a rapidly evolving aspect of the JupyterLab experience and are prone to instability. Other than the pre-installed extensions, they are not officially supported by AI Platform Notebooks. Also note that the JupyterLab built in extension manager is currently not available in AI Platform Notebooks
There is a simple extension worth adding called go-to-definition which jumps to the definition of a variable or function if the definition is in the same file. This is especially useful when trying to work through long or disorganized notebooks or .py files. Note this extension is not officially supported, and like all 3rd party code you should review it before installing.
You will need an SSH connection for sudoer access(see step 5). Once connected, run the following:
After the extension is installed refresh the JupyterLab webpage. Then you can hold ALT in any notebook or .py file and click to jump to a definition in the same file. ALT+O jumps backwards from the definition to where you first clicked.
There are many more extensions available, in various states of completion. There is also a tutorial and excellent documentation if you are interested in making your own extensions or fully customizing your theme.
I want to thank Viacheslav Kovalevskyi for answering my technical questions about AI Platform Notebooks and championing an excellent product, and Wesley Turner and Praneet Dutta for reviewing my post and offering excellent feedback.
Google Cloud community articles and blogs
647 
10
647 claps
647 
10
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Machine Learning Engineer at Google
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://blog.doit-intl.com/dont-get-the-google-cloud-bill-shock-465907d5347c?source=search_post---------52,"With Google BigQuery ML you can now predict your Google Cloud spend in just a few minutes and without leaving your BigQuery Console UI.
Linear Regression, although very simple, can be used to generate accurate predictions for various real world problems efficiently. Due to its simplicity, linear regression training is easy to configure and benefits from fast convergence.
In this post, I will explain how to analyze Google Cloud billing data and build a simple prediction model to estimate the expected overall monthly expenditure. To make it more interesting, I will use only Google BigQuery, thereby keeping all the billing data in the data warehouse ecosystem.
In this exercise, I am going to use Google Billing Exports. Billing export to BigQuery enables customers to export daily usage and charges automatically throughout the day to a BigQuery dataset you specify. You can read about Google Billing Exports here.
The data and code samples are available here: https://github.com/doitintl/BigQueryML-Examples
The table below lists the billings for various services consumed by two Google Cloud billing accounts we own.
The goal is to estimate the current month’s total bill based on all the billings received until a given day.
The model will enable the client to not only estimate overall expenditure, but will also detect anomalies and create alerts for overcharges.
The model assumes that the monthly bill is linearly dependant on 3 variables:
The first variable represents the current daily consumption trend. The other two variables are the number of days until the end of the month and the current balance. Together, the model can use these variables to estimate the remaining monthly expenditure.
Since the model requires data at the daily resolution, we will use BigQuery to aggregate the data by day.
The resulting scheme looks like this:
account_name — the account id day — remaining days to the end of the monthmonth — billing monthyear — billing yeardaily_cost — the total cost paid for all services during the billing daymonthly_cost — the label, which is the sum all billings made during current month, including future billing
Now that the data is aggregated at daily resolution, I save it as a new table and use it to generate an ML dataset.
The next step is to calculate how much has been billed to each account from the beginning of the month until the current day. We will use an aggregate window function to do this. This function will also enable us to calculate the month’s average daily spending, until the current day.
The syntax for the aggregate window function can be found here: [1]
I use the function on our billing_daily_monthly table as follows:
Plotting the data shows that the monthly bill compounds in a somewhat linear manner:
The results above give us confidence that a linear model is an appropriate choice for the expenditure problem. In addition, the gradient has little variance between months, which suggests that the selected features are sufficient statistics with reference to the independent variable.
Obviously, more features and more complex models will likely produce more accurate predictions — those can be built with other tools like Google Cloud ML. But for now it looks like we are done preparing the data.
(WOO HOO!)
Once the data are ready, I can use the new (as of August 2018) BigQuery ML tool to fit a Linear Regression model to the data.
Fitting the model to our dataset is insanely simple !
Once saved, the model can be used to make predictions. To do so, I use the following query, which both estimates the final monthly expenditure and calculates the Relative Absolute Error of the predictions per day:
The results demonstrated in the following table can now be saved and serve other components of the system including monitoring and alerting applications. The model’s Mean Relative Absolute Error is approximately 3.0%, which isn’t bad. (Note, the data were generated especially for this demo. With real data, I achieved around 2.0% error)
Want more stories? Check our blog on Medium, or follow Gad on Twitter.
Acknowledgments: Vadim Solovey — Editingamiel m — Technical Review
Keywords: BigQueryML, BigQuery ML tutorial, BigQuery ML example
Software & Operation Engineering. Written By Engineers.
683 
3
683 claps
683 
3
Written by
ML Tech Lead @DoiT International Machine Learning Google Developer Expert.
Here, our SRE and SWE teams share the solutions to some of the most interesting challenges we encounter during our daily work at DoiT International. If you like what we do, check out our https://careers.doit-intl.com page and join the team!
Written by
ML Tech Lead @DoiT International Machine Learning Google Developer Expert.
Here, our SRE and SWE teams share the solutions to some of the most interesting challenges we encounter during our daily work at DoiT International. If you like what we do, check out our https://careers.doit-intl.com page and join the team!
"
https://medium.com/google-cloud/building-a-go-web-app-from-scratch-to-deploying-on-google-cloud-part-1-building-a-simple-go-aee452a2e654?source=search_post---------53,"There are currently no responses for this story.
Be the first to respond.
Go short for golang, is a statically compiled language. It’s language structure resembles that of C/C++ with heavy emphasis placed on terse statements and removal of boilerplate most older languages carry around. Wikipedia article describes more motivations behind go:
"
https://medium.com/google-cloud/running-jupyter-notebooks-on-gpu-on-google-cloud-d44f57d22dbd?source=search_post---------54,"There are currently no responses for this story.
Be the first to respond.
This post is clearly inspired by this tweet from @fchollet.
Surprisingly, I have not found similar instructions on getting this set up on Google Cloud. Their Cloud Datalab product currently does not seem to run directly on a GPU enabled machine. Hope that changes soon.
Here are the steps I followed. These steps assume you have your gcloud tools already updated (or you could use Cloud Shell).
Step 1: Confirm your GPU quota and zone
Not all GCP zones currently support GPU. Also, you may have to request an increase in your GPU limit.
As per https://cloud.google.com/compute/docs/gpus/, currently these zones support GPU — us-west1-b, us-east1-d, europe-west1-b, asia-east1-a.
If the limit is <1.0, then please request an increase in the limit at https://console.cloud.google.com/compute/quotas . In my experience, Google has approved the request instantly.
Step 2: Create GPU enabled instance
This creates an instance named gpu-deep-learner in us-east1-d zone with 1 GPU and Ubuntu 16.04 (persistent disk size 50GB).
Step 3: Install CUDA
ssh to the instance (e.g. using gcloud)
Install CUDA 8.0 driver and toolkit:
Confirm:
Set environment variables:
Step 4: Install cuDNN
Register at https://developer.nvidia.com/cudnn and download cuDNN. Then, scp the file to your new instance. e.g.
Now, back on the remote instance, unzip and copy these files:
At this point, all the NVIDIA/CUDA setup is complete. You can choose your favorite way of installing Python and any deep-learning libraries that use GPU. Anaconda is one popular way to do it.
Step 5: Install Anaconda and your favorite deep-learning frameworks
Follow the prompts. I also chose “yes” to ‘adding anacoda to your PATH’. It would be good to exit and log back in.
Use conda or pip to install your library. (pip worked better for TensorFlow):
Step 6: Configure Jupyter
Edit /home/ubuntu/.jupyter/jupyter_notebook_config.py to include the following at the top:
Step 7: SSH tunnel forwarding
Set up a tunnel from your local machine to access Jupyter over ssh.
Then, on the server, start Jupyter.
Step 8: Start using Jupyter on local browser
Navigate to http://localhost:8899/ and create a new notebook. Verify by importing keras or tensorflow. The remote log will also confirm whether you are using the GPU/Cuda libraries.
Once you are done, please remember to stop your instance to save costs. These GPU instances are not cheap. Enjoy.
Notes:
Google Cloud community articles and blogs
540 
13
540 claps
540 
13
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Code, Run, Loop
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/hackernoon/aws-vs-google-cloud-platform-which-cloud-service-provider-to-choose-94a65e4ef0c5?source=search_post---------55,"There are currently no responses for this story.
Be the first to respond.
Top highlight
While AWS is undoubtedly the benchmark of cloud service quality, it has some drawbacks.Today we compare Amazon Web Services (AWS) with Google Cloud Platform (GCP).
AWS definitely is the leader of the cloud computing services, due to being the pioneer in the IaaS industry since 2006 and being 5 years ahead of other popular cloud service providers. However, this leads to certain inconveniences and drawbacks that can be exploited by the competition. Essentially, the sheer amount of AWS services is overwhelming.
While Google Cloud Platform does not boast such an ample list of services, it rapidly adds new products to the table. The important thing to note is that while AWS does offer a plethora of services, many of them are niche-oriented and only a few are essential for any project. And for these core features, we think Google cloud is a worthy competitor, even a hands-down winner sometimes, though many of essential features, like PostgreSQL support are still in beta in GCP.
Look for yourselves. Google Cloud can compete with AWS in the following areas:
Customer loyalty policies are essential as they help the customers get the most of each dollar, thus improving commitment. However, there is an important difference here: AWS provides discounts only after signing for a 1-year term and paying in advance, without the right to change the plan. This, obviously, is not the perfect choice, as many businesses adjust their requirements dynamically, not to mention paying for a year in advance is quite a significant spending.
GCP provides the same flexibility, namely the sustained-use discounts, after merely a month of usage, and the discount can be applied to any other package, should the need for configuration adjustment arise. This makes long-term discount policy from GCP a viable and feasible alternative to what AWS offers, and rather an investment, not an item of expenditure. Besides, you avoid vendor lock-in and are free to change the provider if need be, without losing all the money paid in advance.
AWS is definitely the leader for building Big Data systems, due to in-depth integration with many popular DevOps tools like Docker and Kubernetes, as well as providing a great solution for serverless computing, AWS Lambda, which is a perfect match for short-time Big Data analysis tasks.
At the same time, GCP is in possession of the world’s biggest trove of Big Data from Google Chrome, which supposedly deals with more than 2 trillion searches annually. Having access to such a goldmine of data is sure to lead to developing a great kit of products, and Bigquery is definitely such a solution. It is capable of processing huge volumes of data rapidly, and it has a really gentle learning curve for such a feature-packed tool (it even produces real-time insights on your data). The best thing about it is that Bigquery is really user-friendly and can be used with little to none technical background, not to mention $300 credit for trying out the service.
As we explained in our article on demystification of 5 popular Big Data myths, cloud computing can be more cost-efficient as compared to maintaining on-prem hardware. Essentially, this really goes down to using the resources optimally and under the best billing scheme. AWS, for example, uses prepaid hourly billing scheme, which means running a 1 hour and 5 minute-long task would cost 2 full hours.
In addition, while AWS offers a plethora of various EC2 virtual machines under several billing approaches, these configurations are not customizable. This means if your task demands 1.4GB RAM, you have to go with the 2GB package, meaning you are overpaying. Of course, there are several ways to save money with Amazon, from bidding for Spot instances to lending Reserved instances and opting for per-second billing. Unfortunately, the latter option is currently available only for Linux VM’s.
GCP, on the contrary, offers per-second billing as an option for ALL their virtual machines, regardless of the OS’s they run on, starting 26th of September 2017. What’s even more important, their instances are fully configurable, so the customers can order 1 CPU and 3.25GB RAM, or 4.5GB, or 2.5GB — you get the meaning.
As The Washington Post told us, NSA has infiltrated the data center connections and eavesdropped on Google once (many more times, supposedly). This breach has lead to Google opting for full-scale encryption of all their data and communication channels. Even the stored data is encrypted, not to mention the traffic between data centers.
AWS is still lagging in this regard. Their Relational Database Service (RDS) does provide data encryption as an option, yet it is not enabled by default and requires intense configurations if multiple availability zones are involved in the equation. The inter-data center traffic is also not encrypted by AWS as of now, which poses yet another potential security threat.
All things considered, GCP is actually a serious contestant for both AWS and MS Azure. Yes, AWS leads in terms of the numbers of customers and products, due to 5 years of head start. At the same time, GCP already provides all the needed functionality and offers competitive pricing and configuration models, backed up by serious traffic privacy and security measures. With time, as more and more businesses accept the AI-first approach to doing business, GCP’s immense power in Big Data analytics and Google Chrome’s leading position amongst the browsers will allow Google Cloud Platform become an even more serious counterpart for AWS.
Just keep in mind many of GCP features are in alpha or beta stage, so their behavior and API might change. It means that using GCP in conjunction with long-term projects may require GCP connectors upgrade during the project lifetime.
We highly recommend evaluating the actual requirements of your project and think GCP is the best choice for development and staging environments. It might turn out you will suffice to go for GCP to meet all your needs and avoid opting for multiple AWS services required to use the platform efficiently but not actually needed to run your project. Feel free to contact us with any questions regarding the project requirements, we are always ready to assist!
#BlackLivesMatter
746 
3
how hackers start their afternoons. the real shit is on hackernoon.com. Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
746 claps
746 
3
Written by
DevOps & Big Data lover
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
DevOps & Big Data lover
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@hellomondaycom/how-we-built-the-google-cloud-infrastructure-webgl-experience-dec3ce7cd209?source=search_post---------56,"Sign in
There are currently no responses for this story.
Be the first to respond.
Hello Monday
Oct 5, 2018·13 min read
Google Cloud wanted to explain the vastness of their world wide infrastructure. How their data centers and regions work, and how incredibly close to end users Google Cloud has edge nodes and points-of-presence to enable fast, low-latency access to the Google Cloud backend infrastructure. Complex systems are often hard to describe with words alone, so we decided to map out the entire system and make it tangible by building an interactive world in 3D.
These are our most important learnings.
The site is built as a single page app, utilising the History API for navigation and deep linking.
Most of the site’s javascript is authored in TypeScript which, in addition to weeding out common bugs and typos, provided critical refactoring support during the development process.
Various libraries are in use, and without these it would not have been possible to build the site in a reasonable timeframe, so a big THANK YOU to the authors and contributors of these projects:
The site is hosted on Google Cloud Platform’s own App Engine (obviously): https://cloud.google.com/appengine/
All 3D models were created in Cinema4D with Blender used for importing/exporting to glTF.
The entire website clocks in at 2.9MB gzipped, 2.22MB without audio.
We knew we wanted a fairly minimal 3D look with emphasis on subtle greyscale variations as well as a few main colors from the Google Cloud color scheme. The first thing that came to mind was to try and go for a classic ray-traced clay-style with nice beautiful soft shadows, a look well known from architectural renderings. So even before we knew what the final models where gonna be, we started exploring if this was possible.
Given that render times in Cinema 4D (for this look) way exceeded anything remotely close to real-time rendering, even on a machine equipped with 4 x NVIDIA GTX 1080 Ti graphic cards, it was clear from the start that real Ray Tracing was not a viable approach for something that would eventually need to run on just a regular consumer laptop, newer smartphones etc.
From previous projects we’ve learned that this is often the case when moving from 3D software like Cinema 4D into the browser. What we often do is to try and explore different visual styles through renderings from Cinema 4D and then figure out how to recreate or mimic that look in WebGL. It’s a back-and-forth process, and while it’s time-consuming it’s also what makes it fun to develop these kinds of projects.
It’s not always clear just where we’re gonna end up.
The clay look trough Ray Tracing is mostly defined trough the soft shadows and light bouncing around multiple times to pick up the colors of the materials they bounce off of. Light bouncing multiple times is computationally very costly and so we quickly focused on getting the shadows right, foregoing true Ray Tracing.
We explored adding Ambient Occlusion using a shader pass known as Screen Space Ambient Occlusion (SSAO) — however once again it led to another trade-off. When setting up post-processing shaders in Three.js, it’s done by setting up a new offscreen render target making the browsers built in anti aliasing unavailable.
Having lost anti aliasing we looked at ways to bring back the sharp straight lines. So we explored adding custom anti aliasing shaders, like the FXAA shader suggested in this issue: https://github.com/mrdoob/three.js/issues/568.
While it enabled us to disable the browsers’ built in anti aliasing — giving quite a performance boost — the custom anti aliasing shaders weren’t quite up to par quality-wise with the browsers built in anti aliasing.
Given that the site and models are very minimalistic — almost to the point of looking like vector graphics — any low-res shadows or aliased lines stood out quite glaringly as opposed to a more complex/colorful 3D scene where the eye has a tendency not to focus so much on these little imperfections in render quality.
The first thing that was modeled (and re-modelled multiple times) was a Region from the outside and the inside of a Data Center. A Region consists of multiple Data Centers. Through many iterations and Hangouts with Google engineers we landed on a representation that fitted close enough to how an actual Google Cloud Region is laid out.
The initial export from Cinema4D was 100.4 MB OBJ-file. No way was that gonna fly for a site even though it did actually load (slowly). For quick model testing, Three.js’ editor is highly recommended: https://threejs.org/editor/
When looking at 3D model formats it can be quite a jungle to narrow down what the best option is. It’s not a one-size-fits-all, but the Khronos Group has put a lot of effort into the glTF format, striking a fine balance between low file sizes, good export options, fast browser parsing time and high consistency between 3D authoring tools and the model loaded with Three.js’ GLTFLoader. All our models where exported as glTF binaries (.glb) giving a boost in browser parse times. With other formats it’s often not just the actual network transfer that extends load times, but also the amount of time the browser uses to parse the downloaded models.
Switching to the glTF format gave us a significant reduction in file-size and browser parse time, but the file was still too big, not only in size but also in complexity. We simply had too much detail in our model. So by working in a tight iteration cycle of trying to reduce model complexity by removing vertices and testing how it looked in the browser, we finally got the model down to the size it is now: ~716 KB (gzipped). We also stripped away the normals data in the exported model, opting to calculate them at load using Three.js’ Geometry.computeVertextNormals();
The real trick though was not just to optimize the model, but to identify identical objects.
Our model consisted of many identical objects like fans, server racks, trees, cars, trucks, windmills, cooling towers etc. The only difference being each instance’s individual position, scale and rotation (PSR), the actual geometry of each instance was exactly the same, so no need to load that geometry more than once, and most importantly for render times, no need to upload the geometry to the GPU more than once. The Three.js repo has many examples of this technique, which combines rendering multiple objects with individual PSR in one draw call.
In addition to PSR instancing, we needed to maintain the ability to animate each object individually, so for example all the fans wouldn’t be rotating in unison etc. To accomplish this on the GPU involves writing a custom vertex shader and putting the individual animation information onto custom BufferGeometry attributes.
All in all not the easiest to work with solely within Three.js standard APIs, so we opted to add the THREE.BAS library to make it a bit easier to work with these GPU based animations while maintaining the ability to use Three.js materials and their support for lights.
Offloading the animation calculations from the CPU to the GPU comes with speed improvements at the cost of a little flexibility, but overall this was okay for the simple animations needed on the fans, windmills etc. However, we still needed to be able to have some over the animations, so we exposed a single uniform in the vertex shader that then acts as an overall progress variable for the whole set of animations.
Our model contains only one source geometry per object that’s instanced, so for instance for a tree, we just defined one tree as the source, and used Instance Objects in Cinema 4D to place copies of the source tree all around the scene with variations in PSR. We wanted to be able to place these objects visually and preview it within it Cinema 4D and have an easy way to export the instanced copies’ PSR information embedded within the model. This way, to preview new changes, we only needed to do one export of the model from Cinema 4D to glTF (more on that in the next section).
Turns out it’s quite easy, we simply landed on a naming convention where we name source geometry objects: source_object_name and then all the instanced objects are named: replace_object_name_X. Here’s a screenshot of how that looks for the trees:
Now there is one extra step, sadly. Because Cinema 4D can’t export directly to glTF we needed to roundtrip through Blender using FBX as an intermediary format. When exporting Instanced Objects from Cinema 4D to FBX all the Instanced Objects are replaced with actual geometry — not what we wanted. The way we solved it was to convert all the Instanced Objects to Cinema 4D Null objects, thus maintaining the Instanced Objects PSR info on export, but without replicated geometry:
Sadly there is (as of yet) no direct way to export/import glTF files in Cinema4D so we needed to go through Blender with the glTF import/export extension installed.
The flow ended up looking something like this: Cinema4D → FBX Export → Blender → Import FBX → Export glTF Binary
Here’s a short screen recording of the flow which also shows some of the import/export settings we ended up using to compensate for the differences in world-scale and world-orientation between Cinema4D and Three.js:
The above video ends up in the Three.js Editor which is a great tool for previewing models and playing around with materials, lights, geometry etc. Sadly by default there’s no way to have this “more visual approach” to playing around with the various properties of geometries, lights, materials etc. in your own project. That said, it’s very much needed, and it also what gives applications like Unity3D their great edge. In some cases you just need to play around and do tiny tweaks to lights and materials to get everything right, or to compensate for any annoying z-fighting bug etc. And while it won’t bring the power of Unity3d into the browser the Three.js Inspector Chrome Extension does add some much needed features to do just this kind of in-browser tweaking — no need to constantly change tiny code parameters and wait for a full reload of your site:
Just remember to add your Three.Scene to the window object:
At this point we had pretty much been forced to cut all realtime shadows in favor of a sharp look, but we also wanted to bring back some of the original look. A low cost way of adding shadows and other lighting effects is to bake them into a texture directly from Cinema4D and load them onto a flat plane in Three.js. We baked two textures, one for the outside of the Region and one for the inside of the Data Center. And we used two textures to have higher resolution shadows inside the Data Center.
Another goal was to be able to make smooth transitions from one scene to another. Now the most important thing to remember when dealing with GPUs and 60FPS is that any new information that needs to get uploaded to the GPU takes time and can cause frames to drop. Especially textures can be heavy to upload. There are various formats (PVRTC, DDS, ETC…) optimized for quick decompression and low memory usage on the GPU, however the native support on different platforms varies wildly and the file-size compression settings in our testing couldn’t match regular JPGs. So we decided to use JPGs because of their small network load, and sacrifice some runtime performance.
This is one of the areas we’d like to explore more, but for this project we opted to try and mitigate some of the runtime overhead by pre-uploading all textures and precompiling all materials (shaders) needed for the entire site on initial site-load. At a high level this basically means that we tried to make the cameras see all the different scenes at load by making everything visible, forcing a render once and then resetting everything to their original visibility:
Building a WebGL site that needs to run 60FPS is no small feat. It’s a constant compromise between look, feel, and responsiveness. Many factors come into play such as not knowing what the capabilities of the device it’s gonna run on is, what resolution it will run at and so on. A visitor might have a Macbook hooked up to a 4K display — that’s just never gonna fly for anything but a fairly static site. We tried to set sensible defaults, but also explored adding a quality switcher to allow visitors to choose themselves. Admittedly, this is probably not the most used feature of the site, but building it gave some insights into which parameters matter in terms of performance, and how to change these at runtime without reloading the full site. We did not want to have the user choose a quality setting when they entered the site.
To keep it simple and to avoid adding a ton of conditional code based on the settings, we chose to tweak the rendering size (pixel ratio) and toggle anti aliasing on/off.
This resulted in three quality settings:
The tricky part was to toggle anti aliasing on/off as this currently requires us to setup a new THREE.WebGLRenderer(); and clean up the old one. Pseudo code:
For further tips on how to optimize performance the responses in this Twitter thread by @mrdoob gives some good quick hints.
Since we now had a way to tweak the device requirements we wanted to explore if there was a way to automatically detect and set the quality setting based on how powerful a visitors device is. Long story short —kinda, but it’s a bit of a hack and there’s too many false positives.
EPIC Agency describes the approach in this article under “Adaptive quality”.
The approach relies on sniffing out a specific GFX card name, using the WebGL extension WEBGL_debug_renderer_info and then correlating it to a performance score.
However their project was limited to mobile devices where it may be more reasonable to link certain GFX cards to a specific performance value, since mobile devices’ GPU capabilities are usually better matched to the devices’ screen resolutions, whereas on desktop devices this may not necessarily be the case.
It is however possible to use this information if there’s a known GFX card that is underpowered for your project even at very low resolutions —think of it like minimum requirements for a game. It would have to be a GFX card that we know is used on a lot of devices to make it worthwhile the effort of testing all these GFX cards’ performance, not to mention the regular expressions needed to detect them all.
On top of that iOS 11 introduced throttling requestAnimationFrame(); to 30FPS when in Low Power mode, probably a sensible feature to save battery, but they left out a way for a site to differentiate between intentional system-wide throttling or if we are simply running on a slow device.
We decided that for the time being the best option was to not try and be too smart about auto-changing quality settings, not even based on just a simple average frame rate tracking technique.
It can be hard to track how small changes affect the performance of a WebGL project if you’re well within the limits of your GPU and browser. So let’s say you wanted to figure out how adding a new light to a scene impacts performance, but all you’re seeing is a smooth 60FPS — with and without the new light. We found it helpful to disable Chrome’s frame rate limit of 60 FPS and just let it run as fast as it can. You can do this by opening Chrome from the terminal:
open -a ""Google Chrome"" --args --disable-frame-rate-limit
Be mindful that other tasks running on your laptop or in other tabs may affect the FPS …looking at your Dropbox!
This is more of an observation than a learning, but Safari’s WebGL performance simply blows away Chrome’s on 5K displays (and probably also on lower res displays). We found that we could easily increase the pixel ratio to almost native display resolution in Safari and still have 60 FPS, sadly this is not the case with Chrome as of today.
The biggest pain point right now for us when developing a WebGL heavy site is all the intermediary steps between having an idea in Cinema 4D to actually previewing how that idea carries over into Three.js and the browser — a lot of good work has gone into solving some of these pain points, but we’d still like to find easier workflows to enable higher parity between how materials, lights and cameras look and behave in Cinema 4D vs. their Three.js counterparts. PBR materials and the work put into glTF goes a long way to smoothen these things out, but there’s still room for improvement. Biggest wish for now would be direct export to glTF from Cinema 4D.
Written by Lasse Moos (Creative Developer at Hello Monday)
A Digital Creative Agency on a mission to turn the worst day of the week into the best one. A smørrebrød of experiments, inspiration, and creativity.
See all (123)
1K 
5
Thanks to Lasse Moos. 
1K claps
1K 
5
A Digital Creative Agency on a mission to turn the worst day of the week into the best one. A smørrebrød of experiments, inspiration, and creativity.
About
Write
Help
Legal
Get the Medium app
"
https://servian.dev/exploring-beam-sql-on-google-cloud-platform-b6c77f9b4af4?source=search_post---------57,"Sign in
There are currently no responses for this story.
Be the first to respond.
Graham Polley
Jan 24, 2019·10 min read
Did you know that a new feature was recently rolled out for Apache Beam that allows you to execute SQL directly in your pipeline? Well, don’t worry folks because I missed it too. It’s called Beam SQL, and it looks pretty darn interesting.
In this article, I’ll dive into this new feature of Beam, and see how it works by using a pipeline to read a data file from GCS, transform it, and then perform a basic calculation on the values contained in the file. Far from a complex pipeline I agree, but you’ve got to start somewhere, right!
<tl;dr> You can now inject SQL directly into your Apache Beam/Dataflow pipeline (using the Java SDK only), which means having to write a lot less code. It’s still very new and probably not ready for production workloads just yet. But, it’s certainly something to keep an eye on as it quickly matures. Full source code for below example here.
After seeing Beam SQL pop up in the docs a few months ago, I’d been meaning to test drive it for a while. But, being summer here in Australia, I was finding it rather hard to pull myself away from lounging around in my hammock all day. I was also really busy baby proofing the house (again) for the arrival of our latest ankle-biter in April.
But this week I found some time between commuting to work and meetings to finally have a proper poke around with Beam SQL, and put it through its paces. So, what’s it all about it then? Well, a good place to start is the official documentation blurb:
Beam SQL allows a Beam user (currently only available in Beam Java) to query bounded and unbounded PCollections with SQL statements. Your SQL query is translated to a PTransform, an encapsulated segment of a Beam pipeline. You can freely mix SQL PTransforms and other PTransforms in your pipeline.
Let’s break that statement down y’all:
In addition to that blurb, here’s another one that tells us Beam SQL is essentially based on Apache Calcite:
Calcite provides the basic dialect underlying Beam SQL. We have added additional extensions to make it easy to leverage Beam’s unified batch/streaming model and support for complex data types.
So, you’ll need to use Calcite’s dialect of SQL when using it in Beam. Check out the Apache Calcite SQL docs here. However, for all intent and purposes, you can simply write SQL in your pipelines in order to work on your data coming in it.
Got it? Great. Let’s move on.
Now that you know what it’s all about, let’s have a look at the code needed to get a simple pipeline that uses Beam SQL up and running. To test it, I devised a cunning data pipeline:
The first thing we need is some sample data to push through the pipeline. I usually turn to the public datasets in BigQuery whenever I need some dummy data, or if I want to test something. One of the entries in the public datasets is the Wikipedia benchmark page view tables.
These tables are great tables for doing quick tests and demos on GCP. They are simple to understand and easily relatable to the audience. Also, the size of the tables range from a 1K rows, all the way up to 100B rows (6TB). Wowsers. So, you can easily scale and test your workloads with absolutely no changes needed.
To start with, we simply export the smallest of the tables (1K rows) to GCS as a CSV file from the BigQuery console. Of course, we could just read the table directly from BigQuery in the pipeline (using BigQueryIO), but I can’t think of a use case that would require using Beam/Dataflow to pull data from BigQuery in order to run SQL over it. You’d be much better off just querying it directly in BigQuery! 😀
The first part of the code constructs the pipeline and creates the read from GCS into a PCollection<String>. Simples:
To work with Beam SQL, you must convert the type of your PCollection to a Row. Don’t confuse this with a BigQuery TableRow object though. They are different things altogether. So, the next thing we nee do is to run a ParDo over the PCollection<String>, which we obtained from the initial file read from GCS, and turn it each element into a Row object and then finally set the values for each element:
Now that we have a PCollection<Row> object, we can finally run some sweet, sweet SQL over the contents of that collection. Who doesn’t love a bit of SQL, huh?
Remember, that the elements will be distributed across all the workers in the Dataflow cluster. But, we don’t need to care about that because it’s all abstracted away. Using SQL also means we don’t need write boilerplate’y GroupByKeys, Combines, etc. in Java. Using SQL is much easier to express what we’d like to do with the data:
In this example, I’m simply aggregating by language and summing the views for each one. As simple as this sounds, under the hood a lot of work needs to be done to make this happen on a distributed cluster. And it’s something I’m not smart enough to understand. But once again, the complexity of this is hidden from users like us. You don’t need to worry about it — until you have a bug and you need to debug that is! Mwaaaah!
We’ve now got the bulk of the pipeline written. The last thing we need to do is collect the results of the SQL statement, and store them somewhere. This could be anywhere really (e.g. BigQuery, Datastore, Memorystore etc.), but for brevity’s sake I simply writes the results back as a CSV file to GCS.
To do this we need to convert the SQL Row object back to String so we can write it out to GCS. Just hitting it with a simple ParDo makes this a walk in the park. Then one more apply to write the result file out (without sharding so we get just one file) to GCS.
And you’re done folks!
Putting it all together, here’s the pipeline in all its Java glory:
The final piece of the puzzle is actually running the pipeline on GCP. Now, you could run it locally using the DirectRunner, but where’s the fun in that? Actually, snark aside, executing it locally is a great way to develop and debug your Beam/Dataflow pipelines. Don’t be like me and run your pipelines on GCP just for fun — your boss won’t like you for very much longer when they get the bill.
Anyway, after kicking off the pipeline with the DataflowRunner this is what happens:
Drilling down into the transform_sql step, it’s clear to see the steps that the SQL generated for us in the pipeline. Remember, I didn’t need to code these steps. I just wrote some simple SQL to express my intent:
With the pipeline run, and the output written to GCS, it’s time to validate the results of the magical SQL code. Once I downloaded the output.csv file from the GCS bucket, I dutifully hit it with some wonderfully dodgy Bash. Yes, yes, I know I could have federated out to the file in GCS from BigQuery and, wrote some SQL to check the results. But, doing it with Bash makes me look like I know how this computering thing works.
Firstly, let’s check the 10 most popular languages line up between what the pipeline produced, and what BigQuery spits out. The left terminal is the Beam SQL results, and the right terminal in hitting the original table in BigQuery:
That looks good. Now, let’s make sure Beam SQL calculated the corrected number of aggregations for the languages field i.e. distinct values:
Finally, let’s double check the total sum of views to make sure it’s 100% correct:
Everything lines up with the original data that I exported from BigQuery into GCS.
I like it when things work.
As awesome as all that is however, 1K rows isn’t really that impressive, now is it. Instead, let’s run the exact same pipeline on 1 billion rows (~100GB) and let the Dataflow cluster scale up to 10 workers. That’s much more fun!
The pipeline took about 25 minutes to process 1B rows. That’s not bad at all. Now, we need to perform some checks on the results just like before:
Whoopsie! The calculation for the en value was much higher in BigQuery than it was in the result of my pipeline, whereas the other values were just fine. Luckily, it didn’t take me that long to figure out the problem (thanks again Stack Overflow). Looking at the number — this time with commas — it’s much easier to see why it all went pear-shaped:
4,990,236,853
Remember our pipeline was written in Java. Yes, JAVA! Got it yet? That’s right. I had bootstrapped my pipeline by copying the example from the Beam documentation, and that sample code calls addInt32Field(). Then, I had unwittingly used Integer.valueOf() when setting the value.
An Integer in Java is 2³¹-1, so the max value is 2,147,483,647. As such, because the sum for the en value was so large, the JVM was wrapping back around to -2,147,483,648 when it topped out at the max value. Ahh, gotta love Java!
Alas, the fix was simply to use addInt64Field() and instantiate aLong (2⁶³-1) object to store the values in Java instead of an Integer. This gives you just a tad more head room to play with as a Long has a max value of 9,223,372,036,854,775,807.
So, a quick fix to the code and it was time to run the pipeline again.
This time the results looked much better, and the value for en is appropriately sized number. Wonderful.
We just took a look at Beam SQL and how it works. It was relatively easy to get going, and put it through its paces. That said however, this was a very simple example using basic aggregation and summing. It would be interesting to see how it does joins and more complex operations. Finally, I only tested it using a batch workflow. I’d be curious to see how it works with a streaming source e.g. PubSub or Kafka.
It’s also import to note that it’s a very new feature and still maturing. For that reason, I ‘d be wary of using it in production workloads just yet — unless you thoroughly test it for your use case(s). Also, being so new, there’s very little material on it aside from the Beam documentation itself.
However, this is certainly something to keep on eye on if you’re using Beam/Dataflow as part of your data analytics workloads. It may help solve a lot of problems and prove useful to many developers and data engineers. It’s got a lot of potential, but most importantly, the KSQL folks can’t say Beam is missing this functionality anymore. 😀
Cause trouble on that cloud thing that everyone is talking about. I like BigQuery. Work @weareservian and tweet nonsense @polleyg. Moving blog to polleyg.dev
364 
7
364 claps
364 
7
At Servian, we design, deliver and manage innovative data & analytics, digital, customer engagement and cloud solutions that help you sustain competitive advantage.
About
Write
Help
Legal
Get the Medium app
"
https://lugassy.net/goodbye-google-cloud-hello-digital-ocean-a4ca1c8b7ac8?source=search_post---------58,"Launching cloud instances should be fun. Like invoicing customers.
GCP used to be fun too. I’ve actually written how fun it was just a year ago.
But things have changed. GCP become cumbersome and slow to manage. It is still a great provider to trust your business with, has excellent networking, unlimited compute power and of course — BigQuery, but for most of my day-to-day projects — I’ve found a new home.
If you haven’t already — meet Digital Ocean (aff link, gives $10 free).
DO is easy to fall in love with. I call it the hacker’s cloud since it doesn’t get in your way and packs serious power. In no particular order:
Good combination of RAM, CPU and Storage to choose from. SSD by default and price ranges from $5 to $1680 monthly. Really fast boot time.
US, Canada, UK, Germany, Netherlands, Singapore and India. Each zone is clearly labeled as NYC1, SFO2, LON1, etc.
Each instance includes free 1–10TB of combined in/out traffic (which alone would cost $120 to $800 at GCP). Private network is available, unmetered.
One of the fastest DNS providers, apparently. Incredibly easy and free.
Crispy fonts, big buttons and checkboxes along with smooth animations gives you a warm fuzzy feeling. Everything is a click away. I’m in control. Cloud is fun again.
Compare this instance creation wizard, for example:
To this:
Or this firewall console:
To this:
And, am I the only one freaked out by these cryptic policies, service workers and roles which GCP keeps creating automatically?
Digital Ocean is all about simplicity and basic offerings and rounded prices. This is so refreshing because cloud providers are such a Paradox of Choice. They keep asking you:
This is wrong. Cloud should be simple by default and advanced by choice.
It was trying to be AWS. Too complex and feature-rich instead of simple and feature-complete. Specifically, here’s what I would change:
Discussion: https://news.ycombinator.com/item?id=15795793
Thoughts about Startups, Development & Ops by Michael…
565 
6
565 claps
565 
6
Written by
Co-Founder, HashUnited.com. Built 7 other companies, sold 3, worked with dozens more. Writing about startups, dev & ops.
Thoughts about Startups, Development & Ops by Michael Lugassy
Written by
Co-Founder, HashUnited.com. Built 7 other companies, sold 3, worked with dozens more. Writing about startups, dev & ops.
Thoughts about Startups, Development & Ops by Michael Lugassy
"
https://medium.com/@manus.can/serverless-platform-comparison-google-cloud-function-vs-aws-lambda-8e060bcc93b4?source=search_post---------59,"Sign in
There are currently no responses for this story.
Be the first to respond.
Can Tepakidareekul
Nov 26, 2018·7 min read
Serverless computing is a new trend in software development, it is used to build your application by deploying application’s functions separately. It reduces lots of steps in designing software architecture and deploying the application.
Although, you know you can take lots of advantages from the serverless computing but there is one more question that you have to answer yourself before starting to use this technology. The question is which provider you should use their service.
Google Cloud has Google Cloud Function and Amazon Web Service has AWS Lambda. So, which one is the best? Let’s find the answer in this article.
In this article, we are going to share about what we did to compare the serverless platform on both providers by comparing functionalities of them which include supported language, ease of deployment, availability, scalability, network performance & stability, CPU throughput, memory management, and pricing.
Programming language is very diverse. Many languages are used in API service development. To compare this functionality, we listed the supported languages on both providers.
We found that there are several supported languages on AWS Lambda while there is only one version of Node.js which fully supported and there are the beta versions of Node.js and Python on Google Cloud Function. So, AWS Lambda is better than Google Cloud Function, if the variety of supported language is concerned.
Ease of deployment is one of the most important functionalities because easy deployment means how fast you can deliver the product to your users.
In this part, we did an experiment by comparing the number of deployment steps which used to deploy a basic function on AWS Lambda and Google Cloud Function from the first step to the last.
From the table above, there are a lot of steps on AWS Lambda while Google Cloud Function has only one step to deploy the function. It is because AWS Lambda provides many options which the users can config their function.
Moreover, we counted the time on both providers use to deploy the functions and update the functions.
Although, Google Cloud Function has fewer steps than AWS Lambda but AWS Lambda can deploy and update the function faster than Google Cloud Function can do.
We did an availability test on both AWS Lambda and Google Cloud Function by keep sending a request to the function every 5 minutes for an entire week. We created a Microsoft Azure Instance to act as a middleman.
We found that there is no downtime in both AWS Lambda and Google Cloud Function. Maybe from there is too much gap in between test iteration, in the further experiment we should reduce the gap down to around every minute.
Scalability is an ability to scale which every cloud feature should have because the main purpose of using the cloud provider is to serve every response to every request from all of the clients.
We did an experiment on this functionality by sending the specific number of request to the functions that have the same purpose at the same time on both providers. Then, we observed the result and limitation that happen on both providers.
We used Apache Bench to send 1000 requests to AWS Lambda and Google Cloud Function at the same time.
We found that there is no failed request from our test on both providers. They can response to all of our requests. So, we can say that AWS Lambda and Google Cloud Function have the efficient scalability and no one wins for this round. It is very interesting that there is no option to config about scalability. It seems like both providers are able to do an auto-scaling without any extra settings needed.
We compared the network performance between AWS Lambda and Google Cloud Function by finding time used until the request is responded.
The average time used of AWS Lambda is 117.16 ms and Google Cloud Function is 176.80 ms. According to the result, AWS Lambda is slightly faster and more stable.
Performance is the most concerned functionality for choosing the serverless provider because no one needs slow computing performance, right?.
CPU Throughput is the number of executions per time unit without network constraints. We did an experiment on this functionality by writing a script with a loop for counting the number of iterations and limiting the execution time to the same list of specific number when running on both providers.
From the result, AWS Lambda is faster than Google Cloud Function. The throughput of AWS Lambda is 1.02 million executions per second and the throughput of Google Cloud Function is around 0.9 million executions per second.
For this functionality, we did an experiment by try to find an actual maximum memory size that we can allocate on AWS Lambda and Google Cloud Function at the same memory capacity.
Before testing memory management, let’s have a look at the memory setting on both providers.
As you can see, memory setting form of AWS Lambda is a slider which provides more options to the users to select their desired size between 128 MB to 3008 MB. On Google Cloud Function, memory setting form is just a dropdown selector which limits the available options to 128 MB, 256 MB, 512 MB, 1 GB, and 2 GB.
However, the input type is not an indicator to judge which one is better on memory management. So, we wrote a script to allocate memory on AWS Lambda and Google Cloud Function with 128 MB memory capacity for both providers, then maximized the number of memory size that they are able to allocate.
The result is we could allocate 6.29145 MB on AWS Lambda and 6.99999 MB on Google Cloud Function. So, Google Cloud Function wins on this round. Its memory management is better than AWS Lambda.
There is a slight difference in pricing between AWS Lambda and Google Cloud Function. All of the information is in the tables below.
Invocation is measured from how many requested is made to the function.
Compute time is measured from the time your function receives a request to the time it complete.
Fees for compute time are variable based on the amount of memory and CPU provisioned for the function. Units used in this calculation are:
GB-Seconds : 1 GB-second is 1 second of wall-clock time with 1GB of memory provisioned
GHz-Seconds :1 GHz-second is 1 second of wall-clock time with a 1GHz CPU provisioned (Google Cloud Function Only)
The total fee = Invocation cost + Compute time costCompute time cost = GB-Seconds + GHz-Seconds
* Note: The price above is an average price based on 128 MB memory but the real price varies by the amount allocated to the function.
To conclude, AWS lambda seems overall better than Google Cloud Function. However, choosing the right server less provider is depend on the objectives of your work. AWS Lambda works properly with the users who want flexibility and more freedom to setting their own functions. While Google Cloud Function works properly with the users who want simplicity in their work and the users who have few experience with this kind of work such as students.
** Note: The test was using Google Cloud Function on “asia-northeast1” region and AWS Lambda on “ap-northeast-1” region which are placed at Tokyo, Japan.
*** Note: This article is a part of the course named “2110498 - Cloud Computing Technologies” taught in Department of Computer Engineering, Chulalongkorn University.
Authors: Arin Trongsantipong & Manussawee Tepakidareekul
medium.com
medium.com
medium.com
a Thai software engineer working in Singapore — www.linkedin.com/in/manus-can
616 
5
616 claps
616 
5
a Thai software engineer working in Singapore — www.linkedin.com/in/manus-can
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@philipplies/transferring-data-from-google-drive-to-google-cloud-storage-using-google-colab-96e088a8c041?source=search_post---------60,"Sign in
There are currently no responses for this story.
Be the first to respond.
Philipp Lies
Jul 10, 2019·3 min read
Google Colab is amazing for doing small experiments with python and machine learning. But accessing data can be tricky, especially if you need large data such as images, audio, or video files. The easiest approach is storing the data in your Google Drive and accessing it from Colab, but Google Drive tends to produce timeouts when you have a large amount of files in one folder.
More robust and scalable is Google Cloud Storage, where you can also more easily share the data with colleagues. But unfortunately there is no native way to transfer data from Google Drive to Google Cloud Storage without having to download and upload it again. However, with Google Colab we can transfer files quite easily.
Mounting your own Google Drive is fairly easy. Just import the drive tools and run the mount command. You will be asked to authenticate using a token that you create using Google Auth API. After you pasted the token your drive is mounted to the given path.
Next we need to create a Google Cloud Storage project. Go to the Resource Manager and create a new project.
After the project is created (and you need to have billing enabled, as the storage will cost you a few cents per month) click on the menu in the upper right corner and select Storage (somewhere way down the menu). Next you need to create a bucket for the data.
The name of the bucket must be globally unique, so not only for your account but for all accounts. Just be creative ;-). There you can also estimate the cost for the bucket, which is around 0.60 EUR per month for 10 GB with 10,000 uploads and 1,000,000 downloads per month.
Once your bucket is set up you can connect Colab to GCS using Google Auth API and gsutil. First you need to authenticate yourself in the same way you did for Google Drive, then you need to set your project ID before you can access your bucket(s). The project ID is shown in the Resource Manager or the URL when you manage your buckets.
This will connect to your project and list all buckets. Next you can copy data from or to GCS using gsutil cp command. Note that the content of your Google Drive is not under /content/drive directly, but in the subfolder My Drive. If you copy more than a few files, use the -m option for gsutil, as it will enable multi-threading and speed up the copy process significantly.
That’s it. Now the process is running and you can check from time to time if it’s completed. I created a Colab notebook with the example code given here: https://colab.research.google.com/drive/1Xc8E8mKC4MBvQ6Sw6akd_X5Z1cmHSNca
Further information can be found in the Colab documentation here and the gsutil documentation here.
For future projects just authenticate the Colab notebook and transfer the files from the bucket to the local file system. Then you can run all experiments on the local copy.
Machine learning and neuroscience | Coding python (and recently JS and Kotlin) | Building apps you love
745 
19
745 
745 
19
Machine learning and neuroscience | Coding python (and recently JS and Kotlin) | Building apps you love
"
https://blog.percy.io/tuning-nginx-behind-google-cloud-platform-http-s-load-balancer-305982ddb340?source=search_post---------61,"Percy is a visual testing and review platform that helps you deploy every UI change with confidence. Learn more at https://percy.io ✨
This is a consolidation of learnings from weeks of tuning and debugging NGINX behind the Google Cloud Platform HTTP(S) Load Balancer.
There is an unfortunate lack of documentation around the web for some of this, so I hope it helps you! But remember, tuning is always specific to different environments and conditions—your mileage may vary.
By default, NGINX does not compress responses to proxied requests (requests that come from the proxy server). The fact that a request comes from a proxy server is determined by the presence of the Via header field in the request.- NGINX Admin Guide: Compression and Decompression
Google’s load balancer adds the “Via: 1.1 google” header, so nginx will not gzip responses by default behind the GCP HTTP(s) Load Balancer. This happens because nginx does not think the proxy can handle the gzipped response.
To re-enable gzipped responses, configure gzip_proxied in nginx.conf (in http, server, or location blocks):
Traffic from the load balancer to your instances has an IP address in the range of 130.211.0.0/22. When viewing logs on your load balanced instances, you will not see the source address of the original client. Instead, you will see source addresses from this range.- https://cloud.google.com/compute/docs/load-balancing/http/
For security reasons, you can force all HTTP(S) traffic to flow through the load balancer and block direct access to your instances (from port scanners, for example). You just need to know this specific CIDR range:
When making a GCE firewall rule, just set Source IP ranges to this range.
Update: Google has added more ranges that load balance traffic might come from. As of Jan. 31, 2018, you’ll also need to have this range allowed:
These ranges only apply for the HTTP(S) Load Balancer and SSL Proxy. If you are using Network Load Balancing, see the docs for the applicable ranges.
This was a hard one. Many hours tuning sysctl settings, running tcpdump, re-architecting flows, and trying to recreate a rare and intrusive error before figuring out what was happening.
Summary: the default nginx keepalive_timeout is incompatible with the Google Cloud Platform HTTP(S) Load Balancer!
You must increase nginx’s keepalive_timeout, or risk intermittent and sporadic 502 Bad Gateway responses to POST requests.
The “650 seconds” here is not arbitrary, see below for justification of why we picked this specific timeout. Notably, this is opposite of the advice that most articles will give you, but most of them are configuring nginx for direct connections and not for sitting behind a global load balancer.
Several times a day, POST requests to our API would return a 502 Bad Gateway response, with no backend log of the error. Long ago we added client-side retries to our API libraries to handle these cases, but I decided to finally root-cause this bug once and for all so we didn’t have to keep patching it across libraries.
In Google Cloud Logging, you can see if you are experiencing these particular 502 responses using an advanced filter:
Now it gets tricky. There were no other logs that correlated with this rare error—no logs from nginx itself, nothing from the API app, nothing related to any syslog or kernel message—nothing, except rare 502s in load balancer logs.
That looks very much like a problem with the GCP HTTP(S) Load Balancer itself. But, as an ex-Googler who has configured services behind Google load balancers in the past, I knew it was very unlikely that my site was a special snowflake that was hitting some new bug uncaught by the billions of requests that flow through those LBs every day. It was much more likely I had a bad config somewhere.
Digging further, we see more info:
Ah ha. So, this was not a problem finding a backend machine (which would have been “failed_to_pick_backend”). The docs have a tiny description of this error:
backend_connection_closed_before_data_sent_to_client: The backend unexpectedly closed its connection to the load balancer before the response was proxied to the client.- Google Cloud Platform: Setting Up HTTP(S) Load Balancing
Meaning, a TCP connection is being established from the load balancer to the GCE instance, but the instance is terminating the connection prematurely.
Some things were common to the error:- It only happened on POST requests, never on GET requests.- It only happened on our nginx + API services, not on our nginx + static.- It only happened under moderate traffic load (but the server was not overloaded, still 2–5% CPU usage and plenty of memory). This made it feel like a race condition or timeout problem of some sort.
I finally found the right knob to turn. It turns out that there is a race condition between the Google Cloud HTTP(S) Load Balancer and NGINX’s default keep-alive timeout of 65 seconds. The NGINX timeout might be reached at the same time the load balancer tries to re-use the connection for another HTTP request, which breaks the connection and results in a 502 Bad Gateway response from the load balancer.
Let’s dig deeper.
I was actually able to reliably reproduce this error using a simple curl POST load test script, combined with an aggressive nginx timeout:
And here’s a tcpdump of what happened:
So, why doesn’t the load balancer just retry the request?
Well, that’s where the POST part of this whole thing becomes important—the Google Cloud HTTP(S) Load Balancer will retry failed GET requests, but will never retry failed POST requests. These retry behaviors are completely undocumented as far as I can tell, but relatively standard practice—GET requests are assumed to be idempotent, and POST requests are not.
So what timeout should we use to fix this? Unfortunately, the load balancer timeout is also undocumented on the actual Load Balancer docs. After some tcpdump research, I found that it matches the same ten minute TCP timeout noted elsewhere in GCE.
If you’re familiar with GCE, you might be thinking that we missed the tcp_keepalive_time tuning mentioned in Compute Engine: Tips, Troubleshooting, & Known Issues. We didn’t. :) This error happens regardless of the fact that we had tuned sysctl net.ipv4.tcp_keepalive_time long ago based on the docs. I haven’t completely figured out why, but I assume this is because the connection is inbound not outbound, and NGINX probably sets custom socket options that override the system defaults and then enforces it’s own keepalive_timeout regardless.
To fix this race condition, set “keepalive_timeout 650;” in nginx so that your timeout is longer than the 600 second timeout in the GCP HTTP(S) Load Balancer. This causes the load balancer to be the side that closes idle connections, rather than nginx, which fixes the race condition! (This is not a 100% accurate description for how closing TCP connections works, but it’s fair enough for here).
After setting this, we have not seen a single 502 Bad Gateway response from our APIs in weeks.
There are more nobs we turned and tested to support higher concurrency and number of connections, but I’ve gone on long enough. :)
Check out these additional resources:
Ultimately, Google Cloud Platform has provided us with a fantastically scalable and configurable platform, and these and other system tweaks helped us dramatically reduce our app machine footprint while making our APIs more reliable.
Percy is a visual testing platform that helps you deploy your UI with confidence. Learn more at https://percy.io ✨
Words, news, and stories from the people building Percy
801 
24
801 claps
801 
24
Written by

Words, news, and stories from the people building Percy
Written by

Words, news, and stories from the people building Percy
"
https://medium.com/google-cloud/gcp-the-google-cloud-platform-compute-stack-explained-c4ebdccd299b?source=search_post---------62,"There are currently no responses for this story.
Be the first to respond.
Google Cloud Platform (GCP) offers a myriad of services, one particular set of services is its compute stack that contains, Google Compute Engine (GCE), Google Kubernetes Engine (formerly Container Engine) (GKE), Google App Engine (GAE)and Google Cloud Functions (GCF). These services all have pretty cool names, but can get somewhat confusing with regards to their function and what makes them…
"
https://medium.com/@ben11kehoe/the-good-and-the-bad-of-google-cloud-run-34455e673ef5?source=search_post---------63,"Sign in
There are currently no responses for this story.
Be the first to respond.
Top highlight
Ben Kehoe
Apr 13, 2019·5 min read
This week, at Google Cloud Next, GCP announced an interesting new service: Cloud Run. My thoughts about the new Cloud Run service are a bit more complicated than this Twitter thread, so I’ve expanded on them in this blog and welcome your comments.
In this article, I’ll compare Google’s Cloud Run with AWS Lambda and API Gateway because I am most familiar with those services and provider. But my thoughts below are a general critique of Cloud Run relative to FaaS including Google Cloud Functions and managed API services in general — regardless of the provider.
Google’s Cloud Run allows you to hand over a container image with a web server inside, and specify some combination of memory/CPU resources and allowed concurrency. The logic inside your container must be stateless.
Cloud Run then takes care of creating an HTTP endpoint, receiving requests and routing them to containers, and making sure enough containers are running to handle the volume of requests. While your containers are handling requests, you are billed in 100 ms increments.
This sounds a lot like Lambda. How is it different? You are handling multiple requests within a single container. At a fundamental level, Cloud Run is just serving as a very fancy load balancer.
All of the web logic is inside your container; auth*, validation, error handling, the lot of it. But instead of just measuring resource utilization or other metrics that are a proxy for load, Cloud Run understands requests and uses that directly as a measure of load to know how to scale and route.
Note: if you’re using GCP IAM and you’re running on the managed version of Cloud Run, the service can do the auth for you. There’s no custom authorizers, nor do I expect there to be in the future, because why not just put it in your web server?
In Lambda, while you can set up API Gateway to do no validation or auth and pass everything through to your Lambda, you’d be missing out on a wealth of managed features that perform these functions for you.
So what’s good about Cloud Run?
So what’s bad about Cloud Run? Inside your container is a fully-featured web server doing all the work!
API Gateway allows you to use custom authentication. That means that your code, in a Lambda that does nothing else, can reject the request.
API Gateway allows you to perform schema validation on incoming requests. If the request fails validation, your Lambda doesn’t get invoked. Your code doesn’t have to worry about malformed requests.
Note: The API Gateway model validation is sadly a little more complicated than I’ve described above. Expect a post from myself and Richard Boyd on this topic in the near future.
The FaaS model is that each request is handled in isolation. Some people complain about this. I’ve even heard someone claim that AWS is pushing Lambda because users’ inability to optimize resource usage across requests is lucrative for them — which is about the most outlandish conspiracy theory I’ve heard this side of flat-earthers.
But the slightly less efficient usage model comes with benefits: you never have to worry about cross-talk effects. In Lambda, I don’t have to think about whether one request might have an impact on another. Everything’s isolated. This makes it easy to reason about, and removes one more thing I need to think about in the development process.
Security is hard, and the ability to scope your code’s involvement with it as small as possible is a huge win. Beyond the security implications, it’s also fewer moving parts that are your responsibility.
Cloud Run is also not the same as Lambda’s custom runtimes. Beyond the fact that custom runtimes should be a last resort, they don’t require running a server. Instead, you only need an HTTP client, which makes it more clear that what your code is doing is not acting as a tiny web server.
All this is to say that Cloud Run should not be seen as equivalent, or even analogous, to pure FaaS — Cloud Run fundamentally involves significantly more code ownership. Cloud Run is still a valid rung on the serverless ladder, but there are many more above this service.
And that gets to my biggest concern. Cloud Run, and GCP in general, are providing people with a system that is going to make them complacent with traditional architecture, and not push them to gain the immense benefits of shifting (however slowly) to service-full architecture that offloads as many aspects of an application as possible to fully managed services.
Google’s strategy is to push Kubernetes as the solution to cloud architecture. And for good reason: Kubernetes is really good at solving people’s pain points while staying within the familiar architecture paradigm. And Google is doing a great job creating a Kubernetes layer on top of every possible base infrastructure.
But Kubernetes keeps us running servers. It removes the infrastructure notion of server, but encourages us to keep running application servers, like the ones inside Cloud Run containers.
Google’s ability to put Kubernetes on-prem is going to satisfy developers, and this will potentially come at the cost of delaying organizational moves to the public cloud. The difference from an application development perspective will be less apparent and will hide the higher total cost of ownership for being on-prem.
While Cloud Run is going to enable better usage of existing web server infrastructure, it’s also going to provide a safety blanket for developers intimidated by the paradigm shift of FaaS and service-full architecture. This will further delay the shift to the more value-oriented approach to development.
Cloud Robotics Research Scientist at @iRobot
See all (209)
714 
13
714 claps
714 
13
Cloud Robotics Research Scientist at @iRobot
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/deploying-a-node-js-app-on-google-cloud-8419de45e5dc?source=search_post---------64,"There are currently no responses for this story.
Be the first to respond.
For those looking for a way to deploy their app, the Google Cloud Platform (GCP) may not be the first choice that comes to mind. Heroku is simple and easy to use while Amazon’s AWS is by far the most popular platform in the industry. Nevertheless, GCP offers some unique tools that are worth exploring (and is offering a $300 trial credit in addition to its free tier).
It is in the best interest of software companies to offer a low barrier to entry for their products; the easier it is for someone to get started, the faster a user can explore what lies beyond the gate. Offering clear and easy-to-read documentation would go a long way to lowering these barriers. Google has provided excellent documentation for several of their APIs — however, the cloud department can be a bit lacking in clarity.
Deploying my first Node.js app on GCP presented significant frustrations. Part of it is my fault: I chose to play around with a lot of tools that aren’t completely developed. When they’re fully working, they will be awesome additions to the ecosystem. For now, one has to be aware of potential pitfalls, some of which I’ll cover below.
What drew me to GCP was the potential for a combination of Heroku’s ease of deployment (straight from Github with no CLI needed, even though one is provided) with AWS’s firepower. (Oh also, I used the Google Prediction API, which requires the Google Cloud SDK.)
GCP makes it possible to run a Cloud Shell CLI interface, edit files, and deploy an app with App Engine entirely in the browser — features that make it easier for someone to explore GCP without too much investment and our topic of exploration.
Go ahead, log into the GCP console, and create a new project!
On the right hand of the navigation bar that spans the page, you’ll find an icon to launch the Cloud Shell. This will provide you with a Cloud Engine virtual machine that runs Linux and is equipped with plenty of normal goodies (git, npm, and support for Node.js) and some that we’d otherwise have to install locally (Google App Engine SDK, Google Cloud SDK). A complete list of provided tools can be found here. On first appearance, the shell will appear on the bottom of your screen but there’s an option to pop it out into its own window.
Keep in mind that this VM is unique to your Google account, not your project. If you download a repo for one project and re-visit Cloud Shell for another project, you’ll see your old repos. This Compute Engine instance is also distinct and separate from the App Engine instance that will host your deployed app.
Go ahead and clone your repo into the VM. This is your chance to make sure you have everything before deploying! If you’ve forgotten anything below, feel free to launch Google’s experimental online file editor from the cloud shell toolbar (personally, I’m not a huge fan of VIM and the like).
What you’ll need:
2. Make sure that your package.json contains a “start” script. After your VM sets up your App Engine instance (e.g. installing any node modules you may need), it will run the this script. Don’t forget to start your server with something like node server.js.
3. (optional) If you choose to launch your server on port 8080 (or 8081–8084) in development, you can get a quick preview from the Cloud Shell. Run the start script and click the first icon in the toolbar to open the preview in a new window.
A word of caution here: the preview is unable to connect to any database, hosted on GCP or elsewhere. Since all my content was behind a login screen, it didn’t particularly help that the preview kept throwing an error whenever I tried to sign in or create a new account. If there are no errors locally, it should (hopefully) work in deployment.
4. If you choose to make any changes with the file editor, I recommend following this step. Go to the hamburger menu inside the GCP console, scroll down to the Tools subsection, hover over Development, and click Repositories.
The list of repositories will initially be empty and the option for Source Code will only be available once the app has been deployed. Eventually, the Source Code tab will show the code that is currently being deployed on App Engine.
For now, let’s create a new repository and title it “default”. This will be repo from which the app will be deployed. Since it’s empty, we can push our finalized code from the Cloud Shell VM after we’ve added our default repo as a remote.
Now we’re ready to deploy our app! Go ahead and run the magic command inside the repo folder: gcloud app deploy. You’ll be guided through a series of prompts. First, a location:
Then, confirm:
The build log begins to take shape:
If all goes well, this message will be displayed after a few minutes:
You can visit your app by running gcloud app browse or going to https://[PROJECT-ID].appspot.com (if you haven’t changed it).
Google Cloud community articles and blogs
662 
6
662 claps
662 
6
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/@sathishvj/notes-from-my-google-cloud-professional-data-engineer-exam-530d11966aa0?source=search_post---------65,"Sign in
There are currently no responses for this story.
Be the first to respond.
sathish vj
Jan 8, 2019·6 min read
Subscribe to my YouTube channel that teaches you to apply Google Cloud to your projects and also prepare for the certifications: youtube.com/AwesomeGCP. Check out the playlists I currently have for Associate Cloud Engineer, Professional Architect, Professional Data Engineer, Professional Cloud Developer, Professional Cloud DevOps Engineer, Professional Cloud Network Engineer, and Professional Cloud Security Engineer.
Immediately after the exam I do a memory dump as notes. Hence it is also quite unordered. This is a sanitized list that gives general topics and questions I encountered. The intention is not to give you the questions, but to give you topics that you can be prepared for. I was often stumped by some questions; hopefully you can be more prepared based on my experience. Wish you the very best!
Tough exam. I assumed this one would be easier because I spent more time preparing and I had the experience of the previous certifications. After the exam I went over the questions again to remind myself later what areas were covered — the answer is, everything. Zero direct questions. Every question was embedded in a situation/use case.
The Data Engineer exam was refreshed on March 29th. These are some extracted key points and links that others have posted. From what I am reading of others’ notes
Notes
Posts
Google Cloud Certified — Professional Data Engineer
For those appearing for the various certification exams, here is a list of sanitized notes (no direct question, only general topics) about the exam.
Overall notes across all GCP certification exams
Notes from the Professional Cloud Architect exam
Notes from the beta Professional Cloud Developer exam
Notes from the Professional Data Engineer exam
Notes from the Associate Cloud Engineer exam
Notes from the beta Professional Cloud Network Engineer Exam
Notes from the beta Professional Cloud Security Engineer Exam
Notes from the Professional Collaboration Engineer Exam
Notes from the G Suite Exam
Notes from the Professional DevOps Engineer Exam
Notes from the Professional Machine Learning Engineer Exam
Main Link — https://cloud.google.com/certification/data-engineer
Topics Outline — https://cloud.google.com/certification/guides/data-engineer/
Practice Exam —https://cloud.google.com/certification/practice-exam/data-engineer
A collection of posts, videos, courses, qwiklabs, and other exam details for all exams: https://github.com/sathishvj/awesome-gcp-certifications
I’ve collected here a bunch of free Qwiklabs codes which are awesome to get lots of hands-on practice. Use them well.
medium.com
Check the FAQs here: https://medium.com/@sathishvj/frequently-asked-follow-up-questions-on-google-cloud-gcp-certifications-438e1addb91d.
Wish you the very best with your GCP certifications. You can reach me at LinkedIn and Twitter. If you can support my work creating videos on my YouTube channel AwesomeGCP, you can do so on Patreon or BuyMeACoffee.
tech architect, tutor, investor | GCP 12x certified | youtube/AwesomeGCP | Google Developer Expert | Go
449 
3
449 
449 
3
tech architect, tutor, investor | GCP 12x certified | youtube/AwesomeGCP | Google Developer Expert | Go
"
https://medium.com/@imrenagi/ekstraksi-informasi-e-ktp-dengan-google-cloud-function-dan-cloud-vision-api-4655db21d084?source=search_post---------66,"Sign in
There are currently no responses for this story.
Be the first to respond.
Imre Nagi
Dec 26, 2018·6 min read
Semenjak satu atau dua tahun terakhir, alat pembayaran digital seperti seperti Go-pay, OVO, Traveloka PayLater, Kredivo dan lain-lain mulai digandrungi oleh banyak pengguna smartphone di Indonesia. Alasannya jelas karena lebih simple <coret> dan banyak promo-promo menarik </coret>. Namun sayangnya untuk mendapatkan fitur-fitur premium dari metode pembayaran ini pengguna harus melakukan verifikasi dengan cara mengunggah identitas diri mereka ke layanan-layanan tersebut.
Verifikasi ini pada umumnya dilakukan dengan menguggah identitias diri seperi foto Kartu Tanda Penduduk Elektronik (e-KTP), Surat Izin Mengemudi (SIM), atau passport. Jika seorang pengguna ingin melakukan verifikasi, tentu perusahaan pembayaran digital tersebut harus memastikan apakah data yang diunggah benar atau salah. Jika benar, maka pengguna akan mendapatkan fitur premium dan jika salah, mohon maaf Anda belum beruntung dan silahkan coba lagi. :p
Kini bayangkan bahwa dalam satu hari ada ratusan ribu pengguna yang ingin melakukan verifikasi dan mengunggah identitasnya. Jika dibalik layar ada seseorang atau beberapa orang yang harus melakukan verifikasi dan mencocokan data yang terdaftar dengan data identitas tersebut, bayangkan berapa man-hour yang dibutuhkan untuk menuntaskan semua verifikasi tersebut? Bisakah kita mengautomasi pekerjaan ini?
Tulisan ini adalah ringkasan demo yang saya lakukan di acara Google DevFest 2018 di Jakarta dan Semarang. Demo yang dilakukan adalah bagaimana cara mengekstrak informasi e-KTP seperti Nomor Induk Kependudukan (NIK), nama, alamat, nama provinsi dan kabupaten atau kota, dari gambar e-KTP yang diunggah ke sebuah sistem.
Slide presentasi bisa diunduh dari Slideshare. Presentasi bisa ditonton di Youtube.
Dalam demonstrasi ini dua teknologi utama yang saya gunakan adalah serverless dengan Google Cloud Function dan Optical Character Recognition dengan Google Cloud Vision API. Mari kita bahas satu persatu.
Berdasarkan definisi Martin Fowler,
Serverless architectures are application designs that incorporate third-party “Backend as a Service” (BaaS) services, and/or that include custom code run in managed, ephemeral containers on a “Functions as a Service” (FaaS) platform. By using these ideas, and related ones like single-page applications, such architectures remove much of the need for a traditional always-on server component
Dari definisi diatas, kita bisa mengambil intisari bahwa arsitektur serverless pada dasarnya memungkinkan kita untuk dapat menjalankan baris kode tanpa harus peduli dengan kebutuhan akan sebuah server. Jika sebelumnya, sebuah aplikasi berbasis web-server harus dijalankan di sebuah server yang dikelola oleh seorang system administrator, kini dengan teknologi serverless, seorang pengembang piranti lunak dapat melakukan deployment secara langsung tanpa harus menyewa sebuah server di salah satu penyedia layanan cloud. Yang perlu dilakukan hanyalah membuat sebuah fungsi dan menjalankan sebuah perintah untuk menjalankan fungsi tersebut di cloud (Functions as a Service).
Sementara itu, Cloud Function merupakan salah satu layanan FaaS yang disediakan oleh Google Cloud Platform. Saat ini Google Cloud Function baru mendukung bahasa pemograman Javascript (Node.JS) dan Python (versi beta). Disamping itu, Cloud Function juga dapat diintegrasikan dengan berbagai macam layanan lain milik GCP seperti Google Cloud Storage, Cloud Pub/Sub, dan lain-lain. Pada demonstrasi kali ini, kita akan menggunakan Google Cloud Function untuk membuat beberapa fungsi, antara lain:
Percayalah bahwa sebenarnya hal-hal tersebut dapat dilakukan hanya dengan menggunakan satu fungsi. Tapi untuk membatasi cakupan dan tanggung jawab dari masing-masing fungsi, maka TS berpikir akan lebih baik jika fungsi-fungsi tersebut dibuat terpisah.
Optical Character Recognition atau OCR merupakan salah satu teknik pengolahan citra digital yang sudah umum digunakan untuk mendapatkan teks yang terkandung dalam sebuah citra atau gambar. Namun, sayangnya OCR bukanlah sebuah teknik atau algoritma yang dapat dipahami dalam satu malam.
Namun, beruntunglah kita karena GCP memiliki Cloud Vision API yang memiliki fitur untuk melakukan OCR. Selain OCR, Google Cloud Vision API juga memiliki fitur lain seperti Label Detection, Multiple Object Detection, dan lain-lain. 😊
Berikut adalah diagram rancangan sistem yang digunakan untuk mengekstrak informasi e-KTP:
Dalam rancangan diatas, ada beberapa komponen dengan peran yang beerbeda-beda seperti Cloud Function, Cloud Storage dan Cloud Pub/Sub.
Fungsi untuk mengunggah e-KTP ini adalah satu-satunya tatap muka yang digunakan langsung oleh pengguna. Fungsi ini menggunakan sistem http-trigger yang berarti bahwa eksekusi dari fungsi ini akan terjadi jika pengguna melakukan request call ke http endpoint fungsi tersebut.
Fungsi ini memiliki tugas sebagai berikut:
Berikut baris kode yang digunakan:
Untuk melakukan deployment fungsi berbasis http-trigger, kita cukup menambahkan argument --trigger-http ketika fungsi tersebut di deploy ke google cloud. Sebagai contoh:
Apabila deployment berhasil, maka Google Cloud Function akan menghasilkan sebuah URL (e.g. “https://REGION-PROJECT_ID.cloudfunctions.net/uploadFile”) yang dapat digunakan untuk mengunggah foto e-KTP.
Fungsi kedua sedikit berbeda dibandingkan dengan fungsi sebelumnya. Fungsi ini memiliki dua tugas utama:
Berikut baris kode yang digunakan:
Fungsi diatas dapat di deploy dengan menggunakan perintah berikut:
Dengan perintah tersebut, fungsi ini akan dieksekusi ketika ada penambahan berkas baru di dalam GCS (event google.storage.object.finalize). Jika kita tertarik dengan event-event lain, berikut beberapa opsi lain yang bisa digunakan:
Untuk lebih detail, silahkan baca disini.
Sayangnya hasil pemrosesan dari Cloud Vision API belum bisa menentukan teks mana yang merupakan data penting dari e-KTP. Berikut hasil pemrosesan yang dilakukan oleh Cloud Vision API:
Dari hasil diatas, bisa dilihat bahwa hasil ekstraksi hanyalah berupa sebuah string dengan separator tertentu seperti line break. Oleh karena itu, fungsi ini memiliki tugas utama untuk mengambil informasi seperti nama, tempat tanggal lahir, NIK dll dari hasil pemrosesan Cloud Vision API yang dikirim melalui Google Cloud Pub/Sub.
Pada fungsi diatas, ekstraksi informasi dilakukan dengan cara membuang semua label e-KTP dengan regular expression. Setelah label dihapus, maka nama provinsi, kota, NIK, nama dan tanggal ulang tahun akan berada pada posisi yang berututan. Simpel?
Deploy fungsi dengan perintah berikut:
Sample e-KTP yang kita gunakan 😂
Hasil pemrosesan:
TIL:
Google Developer Expert, Cloud Platform Engineer @gojek
473 
6
473 
473 
6
Google Developer Expert, Cloud Platform Engineer @gojek
"
https://blog.kovalevskyi.com/deep-learning-images-for-google-cloud-engine-the-definitive-guide-bc74f5fb02bc?source=search_post---------67,"Co-author of the article: Mike Cheng
Google Cloud Platform now provides machine learning images designed for deep learning practitioners. This article will cover the fundamentals of the Google Deep Learning Images, how they benefit the developer, creating a deep learning instance, and common workflows.
Disclaimers:
* At the time of writing, the product is still in Beta, therefore it is not covered by any SLAs.
* We will update this guide as new features are available. Please see the bottom of the page to see when the article was updated.
The Google Deep Learning images are a set of prepackaged VM images with a deep learning framework ready to be run out of the box. Currently, there are images supporting TensorFlow, PyTorch, and generic high-performance computing, with versions for both CPU-only and GPU-enabled workflows. To understand which set of images is right for your use case, consult the graph below.
We also have experimental families:
All of the images are based on Debian 9.
All images come with python 2.7/3.5 with pre-installed core packages:
Jupyter environments (Lab and Notebook) for doing quick prototyping.
Nvidia packages (GPU images only):
The list is constantly growing, so I would advise the reader to keep an eye out for any updates.
Let’s say that you want to train some models with Keras and TensorFlow. You care about performance, so you want to attach GPUs. To see any benefit from the GPUs, you will need to install the Nvidia stack (Nvidia driver + CUDA + CuDNN). Not only is this tricky in and of itself, but you will also need to consider compatibility with the ML framework binaries. For example, the official TensorFlow 1.10 binary is compiled with CUDA 9.0, therefore a machine with CUDA 9.2 or 10.0 will NOT work with the official TensorFlow binary. As anyone who has set up this stack can tell you, matching dependencies is a nontrivial pain.
Now, let say that after some sleepless nights you’ve finally installed required CUDA stack. Now we can consider the question of optimization. Performance is key, since it means less time to convergence (less thumb-twiddling and a lower total cost). Is the CUDA + framework combination the fastest stack for the hardware that one will be using? For example, will a GCE instance with a SkyLake CPU, one Volta V100 GPU, and CUDA 9.0 show the highest possible performance for TensorFlow 1.10? With improvements coming in constantly from Nvidia, the framework, and even the platform itself, it’s hard to know for certain.
In order to be sure, you will have to compile TensorFlow yourself with different compilation keys and different versions of the Nvidia stack, run measurements, and pick the best one. All of this trial and error will require GPU instances, which are quite pricey. To top it off, you will have to do this all over again as each new version of Nvidia stack or TensorFlow is released. Safe to say, this process is not something you really want to handle yourself.
This is where the Google Deep Learning images come into play. The TensorFlow images have a custom build of TensorFlow that is optimized precisely for the hardware that we have on Google Cloud Engine. We’ve also tested each configuration of Nvidia stack and packaged the one that has the best possible speed. And, on top of this, almost all of the important packages you will need over the course of your research come pre-baked in the image
There are two ways of creating an instance with our image:
Our UI is very simple, so I would just show to you how it looks like:
You can start using it right away by going here: https://console.cloud.google.com/mlengine/notebooks/instances
Since this part is simple I will leave you with the official documentation for the UI and we will move for the CLI part for the users who either need to do an automation or more flexibility.
Before starting, be sure that you have installed the gcloud CLI on your local machine. Alternatively you can also use Google Cloud Shell; however, beware that WebPreview from Google Cloud Shell is currently not supported.
Now, you will need to pick the family of images you want for your VM instance. For ease of reference, I’ve duplicated the non-experimental images families graph:
Let’s say you want a TensorFlow GPU image. You would select the family tf-latest-gpu, which will reference an up-to-date image with the most recent release of TensorFlow. We also have family tf-latest-cu100 that will have latest TF with the CUDA 10.0. We will be using this family later on across the article. However, beware that after migration to the newest CUDA (like CUDA 10) this group will not be supported, so I would suggest to use tf-latest-gpu.
Let’s say you have a project that requires TensorFlow 1.11, but TensorFlow 1.12 has already come out, and the tf-latest images have already moved to 1.12. For this scenario, we provide image families that refer directly to the framework version (in this case, tf-1–11-cpu and tf-1–11-gpu). These images will still be updated with the necessary security patches, but the framework is fixed.
I do understand that there might be plenty of cases where you might want to reuse the exact same image again and again. There are many cases where this is actually preferred. For example, if you are spinning up a cluster it is NOT recommended to reference the images by image family in your scripts, because if the family is updated while your script is working, you will end up having different images (therefore different version of the software) on some instances in the cluster. In these cases, you’ll want to get the name of the latest image in the family directly:
Unfortunately, I do not have a fancy bash function for you for this one, simply because it is not something that I use very often. All the deep learning images are public and can be listed from the project “deeplearning-platform-release” where they are hosted.
Now you have the EXACT image name: e.g. tf-latest-gpu-20181023 that you can reuse wherever you want.
To create an instance from an image family:
If using an image name, you should replace “ — image-family=image-family” with “ — image=image-name” in the command.
Several things to note here:
Pick right instance type. Example of the command is using “n1-standard-8”, which has 8 vCPUs and 30 GB of RAM. You might want a cheaper instance, or more powerful. All available instance types can be found here.
Pick right disk size. You probably do not want to find out that disk is not big enough to host all your data for the training so make a good decision about the size upfront.
If you are using an instance with GPUs, there are a few points that you should be aware of:
Pick a valid zone. If you are creating an instance with a certain GPU, be sure that the GPU is available in the zone you’ve selected. See the documentation that gives a list of zones with GPUs. As you can see us-west1-b is the ONLY zone that have all 3 different GPUs.
Verify that you have quota to create an instance with GPUs. Even if you have picked the right region it does not mean that you have quota to create a GPU instance. By default, quotas for GPU are zero, therefore any attempts to create an instance with GPUs would result in a failure. A good explanation how to request increase of the quotas can be found here.
Verify that the zone has enough GPUs to fulfill your request. Even if you have picked the right region and you have quotas, it does not mean that there available GPUs in the zone that you have picked. Unfortunately, I’m not aware of any simple way to check the availability of the resource in any other way other than actually trying and creating the resource.
Pick the right GPU type and amount. The “accelerator” flag in the command controls the GPU type and the amount of GPUs that will be attached to the instance: e.g “— accelerator=’type=nvidia-tesla-v100,count=8'”. Each GPU has certain valid counts. Here are the supported types and counts that can be used with them (in order of least to most powerful):
Give permission for Google Cloud to install the Nvidia driver on your behalf. The Nvidia driver is required for your instance to interface with the GPUs correctly. Due to reasons that are out of the scope of this article, images do NOT come preinstalled with the Nvidia driver. However, it is possible to authorize Google Cloud to install this driver on your behalf. This is done via the flag “ — metadata=’install-nvidia-driver=True’”. If you do not opt into the automatic install using the metadata flag, the VM will prompt you to install the driver on your first SSH.
Unfortunately, the process of installing the driver affects the startup time for the first boot, since it must download the files from Nvidia, install the driver. This should take no more than 1 minute, but it might affect certain use cases. We will discuss how to reduce time to boot time later in this article.
If you are not planning to use GPU you might want to add the following key to your command:
this will guarantee that you will have the instance with the fastest possible CPU by the time for writing. However, you need also be sure that this CPU is available in your region. You can check it here. So the overall command for creating a CPU-only instance would be:
There is a way to create Deep Learning VM with the special HTTPS link. The link will give you access to the Jupyter Lab that is running on the VM. With the link, you will not have to use SSH (unless you want to) and port mapping.
This feature is currently in Beta and is supported ONLY for instances in US/EU and Asia.
In order to use this feature, you need to change the following line:
to
So the final command will look like this:
This will show your instance in the new Notebooks UI that we have shown before now you either go directly to the UI to start using your instance here: https://console.cloud.google.com/mlengine/notebooks/instances or if you want you can can get the url for accessing Jupyter on the VM by calling the following command:
If everything is ok you will see URL that can be used in the browser to access Jupyter Lab.
It might take a minute for the URL to show up
This can be done via simple command:
gcloud will propagate your SSH keys and create your user, so you don’t have to worry about that part. If you want to make the process even easier, I actually have several bash helper functions that simplify things for me. I prefer to ssh like this:
BTW, you can find many of my gcloud related functions here. Before we jump to questions like how fast is the image, or what can be done with it, let me address one last question related to image start time.
This is very simple. You can do the following:
We already have covered how to create the instance, ssh to it, and verify that driver is installed. Now, let me tell you how to stop instance from CLI and how to create your own image.
To stop your instance, you can use the following command (from your local machine, not on the instance):
To create your own image:
After this command finishes, you will have your own image with Nvidia drivers pre-installed that can be used to start new VMs.
First of all, if you do not know what is “preemptible”, first familiarize yourself with the official documentation here. And now the good part, in order to create a preemptible instance the ONLY change you need to do to the creation command is to add the following flag in the end: “preemptible”.
I would strongly advise you to use the access method from the section “Create Instance With Simplified Jupyter Access Feature. Beta.”. Use this section only if that method did not work or can NOT be used.
As soon as your VM is up next step probably would be to start the Jupyter Lab and do some DL :) This is very simple with images. Jupyter Lab is actually already running (also there is a user “jupyter” that is created in the system and is used by the Jupyter Lab), you just need to connect to the instance with port mapping. By default, it will be on port 8080.
Now you can open your browser on your local machine on http://localhost:8080 and this is it!
This is a very important question. However, the answer to this question probably would be even longer than everything written here up to this point. Therefore, my friends, you will have to wait for the next article :)
However, just a very quick estimation shows a speed of training on ImageNet around 6100 images/sec (ResNet-50). I did not have a personal budget in order to finish the training but I guess it is safe to assume that with such speed one can train the model to 75% accuracy in slightly more than 5 hours.
If you need anything, please do not hesitate to file question on the stack overflow with the tag google-dl-platform. Even if you think that the question is small, just file it:)
Also, you can now write to the public Google Group.
If you have any feedback, please do not hesitate to contact me by any possible means. In fact, if you want to say thank you for the article, the best way is to play with the images and give your feedback! It would be really-really nice to hear what can be improved!
These new images are bridging the gap between the software stack for deep learning (TensorFlow) and the GCE hardware (like Nvidia Volta). And since Google is behind TensorFlow they definitely have the expertise required to provide the best possible performance on the hardware.
Last updated: Mar 6 2019
follow me on twitter
Articles about the #DeepLearning (mostly Google Cloud with…
645 
7
645 claps
645 
7
Written by
#ML/#NLP researcher. Making Things Work! #TensorFlow. In the past #MXNet. All opinions are my own.
Articles about the #DeepLearning (mostly Google Cloud with #TF).
Written by
#ML/#NLP researcher. Making Things Work! #TensorFlow. In the past #MXNet. All opinions are my own.
Articles about the #DeepLearning (mostly Google Cloud with #TF).
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/continuous-delivery-in-google-cloud-platform-cloud-build-with-compute-engine-a95bf4fd1821?source=search_post---------68,"There are currently no responses for this story.
Be the first to respond.
This article is the 2nd part of a series written to tackle Continuous Delivery in Google Cloud Platform. In the 1st article, Google App Engine was the main actor. In this one, Google Compute Engine — or simply GCE — will come into the scene. GCE is the Infrastructure as a Service component of GCP, which is built on Google’s global infrastructure and allows its users to launch virtual machines on demand.
As we had already seen, GAE fits perfectly for continuously delivering projects. However, there are some limitations to use the platform: the code must be written in specific languages/versions, mainly if your team aims to use the Standard Environment, which is less expensive (more info at https://cloud.google.com/appengine/docs/the-appengine-environments); and surely will be fully compatible if the project was designed to run on GAE, but may have troubles if trying to migrate legacy code to the platform, even if choosing the Flexible Environment.
After a pros and cons analysis, your team may conclude that GAE is not the best option for a project. Maybe because the project uses some language or tool that is not supported by the platform; maybe because they want more control or customizability over the execution environment; maybe because they want just to migrate their workload from existing servers to GCP, to take immediate advantage of scaling up the app without changing so much the codebase. For all of these cases, consider Google Compute Engine.
What we will see in the next lines is how to use a set of tools available in GCP that allows your team to set up a development + automated build + continuous delivery pipeline using GCE. Also, using an example Angular app, it will be possible to compare the solution with the one proposed to use with GAE in the 1st article. The key point here is that the application must be shipped as a Docker image. If you are able to do it in your project, you have great chances to automate the whole deployment process as described in this article.
Before proceeding, make sure you have created a GCP Project and installed Google Cloud SDK in your machine if you wish to run the examples. Don’t forget to run gcloud auth login and gcloud config set project <your-project-id> for proper gcloud CLI usage.
For the sake of simplicity, the Angular app will be served by Nginx. So, create your new Angular app (follow the steps described here) and cd <app-name>. In order to pack the app plus the Nginx server as a Docker image, let’s create a file named Dockerfile in its root folder, with the following content:
Basically, it’s a multi-stage container build. The lines from 1 to 5 (stage 1) use a NodeJS 8 image just to build the app. The lines from 7 to 11(stage 2) copy the result of the build process — a set of HTML, CSS, and JS files — to an Nginx image and replace the default server’s home page with app’s content. Finally, line 12 starts the server.
Let’s also create a second file,.dockerignore, in the same folder, with the below content. It will prevent Docker from copying unnecessary files to the built images, decreasing their sizes.
If you trigger a docker build -t <app-name> . command followed by docker run -d --name <app-name>-container -p 80:80 <app-name>, and point your browser to http://localhost:4200, you’ll see the application running with Docker.
What we have seen so far is just Angular and Docker setup stuff. From now on GCP will be in action, making things more interesting!
In typical non-automated or semi-automated deployment scenario, an Engineer could (1) set up a CI tool such as Jenkins to monitor a Git repository for new pushes, (2) trigger the build process when new content is pushed and (3) find a mystic way to update all VMs that are running the Docker containers to update the new version. Steps 1 and 2 are simple. Step 3 maybe not... At this point GCP offers a much more sophisticated approach: instead of creating VMs, installing Docker on them, and manually managing the containers, what about creating VMs that exclusively run a specific container and automatically update themselves when new versions of the images are published? Sounds good, right? So, let’s see how to do it.
First of all, we need to add one more small file to the project root folder, named cloudbuild.yaml, as follows:
This file will be used by Cloud Build to build the Docker image and create a repository to store it in the Container Registry, using the specified name — at build time, Cloud Build automatically replaces $PROJECT_ID with your project ID.
Ops… a new GCP component was mentioned in the previous paragraph: Container Registry. According to the official documentation, it’s more than a private container repository: is a single place for your team to manage container images, perform vulnerability analysis, and decide who can access what with fine-grained access control.
Coming back to the code: after adding the file, run gcloud builds submit --config cloudbuild.yaml . in the project’s root folder, wait a little bit, and access https://console.cloud.google.com/gcr/images/<your-project-id> after the building has finished. You’ll see the new repository there (similar to the picture below). This manual step is required only once. And you may copy the full repository name, we will need it later ;).
We're almost done with Docker image building. Just need to set up a trigger that will start the build process automatically every time new code is pushed to a monitored Git repository, and this is a Source Repositories job, as we saw in the first article of this series. It will be set up exactly as it was for delivery with GAE. Bear in mind the main difference will be the result of Cloud Build processing: for GAE, it publishes a new version of the app; for the current process, it only pushes an image in the Container Registry.
Now, let’s create some VMs to run the containers. In GCP Navigation Menu, click Compute Engine > Instance Templates. Click on CREATE INSTANCE TEMPLATE. In the next screen, select Deploy a container image to this VM Instance, type (or paste) the full Container image — aka Repository — name, select Allow HTTP traffic, and click on Create.
In the Navigation Menu, select Compute Engine > Instance Groups. Click on Create instance group. In the next screen, select the Instance template you have just created and set the Minimum number of instances to 3. Click on Create.
After the Instance Group has been created, select Compute Engine > VM Instances in the Navigation menu. Click on instances’ External IP links to make sure the application is running in each of them. It may take a while to return the home page for the first time…
Finally, grant the Compute Instance Admin (v1) and Service Account User roles to the <your-project-number>@cloudbuild.gserviceaccount.com Service Account (Navigation menu > IAM & admin > IAM).
So, what if we push a new version of the application to the Git repository? Which steps do we need to repeat in order to have it running in production? Just restart the VMs! There are many ways to do it in GCP, including publishing a message to some PubSub topic to invoke a Cloud Function that restarts the VMs, or use advanced options for update managed instance groups (https://cloud.google.com/compute/docs/instance-groups/updating-managed-instance-groups). But, to show a simple example, let’s just add the following lines to cloudbuild.yaml:
They will include a new step in our build process that consists of running the gcloud compute instance-groups managed rolling-action restart command. The instances that belong to the group we created a few steps before will be restarted after the build, one by one, in a process that ensures there will always be available machines at any given time. It may take some minutes depending on how many instances the group has, but works perfectly. To see it in action, change something in your code, the title in app.component.ts for example, and push the new code to a Git repository that is monitored by Cloud Build. Wait a few minutes and refresh the HTML pages served by each instance (external IPs may change after restart).
Well, that’s pretty much it for this topic. As demonstrated here, setting up a CI environment to GCE is usually more complex than to GAE, but is a valid option if your project requirements don’t fit GAE. And sure, this is one solution, but there are others.
Below picture shows the main GCP components mentioned in the article:
The sample code is available on Github: https://github.com/ricardolsmendes/gcp-cloudbuild-gce-angular. Feel free to fork it and play.
Hope it helps!
This is the 2nd of a 3-article series on Continuous Delivery in Google Cloud Platform:
App Engine | Compute Engine | Kubernetes Engine
Google Cloud community articles and blogs
586 
9
586 claps
586 
9
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
head of data @ ciandt.com • hobbyist tech writer • dad • birder
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/airflow-for-google-cloud-part-1-d7da9a048aa4?source=search_post---------69,"There are currently no responses for this story.
Be the first to respond.
You know Big Data… it’s a dirty business. All the literature shows you how powerful all those data crunchers and query engines are, but it all assumes that all the data is ready to be consumed. In reality a lot of automated Dataflow, Spark and BigQuery ETL processes are glued together with bash or Python.
Well it’s time to change that… and to take a look at Apache Airflow. Airflow is a workflow engine that will make sure that all your transform-, crunch- and query jobs will run at the correct time, order and when the data they need are ready for consumption. No more writing a lot of fragile boilerplate code to schedule, retry and wait: just focus on the workflow.
A lot of work has been purred into Airflow in 2016 to make it a first class workflow engine for Google Cloud. This Medium series will explain how you can use Airflow to automate a lot of Google Cloud products and make it so that your data transitions smoothly from one engine into another. I won’t dive into how to construct DAG’s into Airflow, you should read the docs on that. Basically all DAG’s are build up though Python objects… I will be focusing on the Google Cloud integration.
In this first part we’ll explain how you can automate BigQuery tasks from Airflow.
BigQuery is a very popular for interactive querying very large datasets, but I also love to use it for storing a lot of temporary data. I do prefer it over files on Cloud Storage because you can do some ad-hoc exploratory queries on it. So let’s get start using Airflow to get data in and out of BigQuery.
The first BigQuery integration is executing a query and having the output stored in a new table, this is done with the BigQueryOperator. The operator takes a query (or a reference to a query file) and an output table.
If you look at the destination_dataset_table you will notice the template parameter. Airflow uses the power of jinja templates for making your workflow more dynamic and context aware. In our example it will fill in the ds_nodash with the current execution_date. Execution date in Airflow is the contextual date of your data. For example if your calculation some metrics for the 4th of July the execution_date will be 2017–07–04T00:00:00+0.
You can even use jinja templates in your referenced queries. This is powerful tool because in general the BigQuery API doesn’t support parameters, but with the template engine you can simulate this.
For a full list of whats possible and what kind of parameters are available in the context have a look at the Airflow macro documentation.
In our example the above query file lives on the filesystem next to the example DAG.
Next to querying you want to get your data in and out of BigQuery. Let’s start with extracting to Cloud Storage.
Extracting is a very common scenario and done with BigQueryToCloudStorageOperator. People familiar with the BigQuery API will recognise a lot of the parameter, but basically you have to specify the source table and the destination storage object pattern. It’s important that you specify a pattern though (example: …/part-.avro). The pattern will make sure if you have huge tables that you have multiple storage objects per extract.
Notice again the use of the jinja template parameter. In this case we read variables defined in Airflow, very useful for differentiating between different environments like production or test.
But before doing queries and extraction you need to get the data into BigQuery. That’s done with the GoogleCloudStorageToBigQueryOperator.
Compared to extraction, the load operator does have a few more parameters. The reason is that you need to tell BigQuery a bit of metadata of the imported object are, like the schema and the format. You do this by specifying the schema_fields and source_format.
Another important parameter to specify is the create_disposition and write_disposition. I like my operations repeatable so you can run them again and again, so CREATE_IF_NEEDED and WRITE_TRUNCATE are good defaults. Just make sure your tables can handle it, so use partitioned tables. For now you’ll have to use the classic partitioning with the date suffixed in the name. In a next later release of Airflow we will add full support for the new BigQuery partitioning.
Next in this series we’ll have a look a Airflow driving Dataproc, see you then.
The examples are taken from the Airflow for Google Cloud integration test and examples: https://github.com/alexvanboxel/airflow-gcp-examples
Google Cloud community articles and blogs
303 
8
303 claps
303 
8
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Google Developer Expert for Cloud
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/top-13-google-cloud-reference-architectures-c8a697546505?source=search_post---------70,"There are currently no responses for this story.
Be the first to respond.
👋 Hi Cloud Devs!!Last year I created #13DaysOfGCP mini series on Twitter which you all loved. So, here I compiled 13 more common Google Cloud reference architectures. If you were not able to catch it, or if you missed a few days, here I bring to you the summary!
Those the 13 hand picked Google Cloud architectures. I am thankful and really humbled by your participation, engagement and topic suggestions that made this series so much fun, not just for me but also for the rest of the Google Cloud Community members!
Thanks again and 👋 until later 🙂
Google Cloud community articles and blogs
294 
2
294 claps
294 
2
Written by
Developer Advocate @Google, Artist & Traveler! Twitter @pvergadia
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Developer Advocate @Google, Artist & Traveler! Twitter @pvergadia
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://itnext.io/google-cloud-advantages-over-aws-28751469e570?source=search_post---------71,"This post is not about GCP vs. AWS. There are a lot of articles comparing both cloud providers. I’m mainly an AWS user but recently I have been working with GCP and, although AWS is much more mature and has a lot of more services, GCP has a couple of services and few advantages that make it a better provider for certain use cases. This post is a short summary of the advantages of GCP over AWS.
If you are an AWS user, Google has a nice article explaining the similarities and differences, so I will focus on where I think GCP…
"
https://medium.com/@jamsawamsa/running-a-google-cloud-gpu-for-fast-ai-for-free-5f89c707bae6?source=search_post---------72,"Sign in
There are currently no responses for this story.
Be the first to respond.
James Lee
Dec 13, 2017·10 min read
EDIT* This guide was written for fastai version 1, which at the current date and time (Jan 2018) is in the midst of transitioning to the a newer version, dubbed fastai v2. An updated guide will be coming soon.
As a deep learning enthusiast in Malaysia, one of the biggest issues I have is securing a cheap GPU option to run my models on. If you’re like me and come from a country where paying $80-$100 every month for AWS GPUs is too expensive, I’ll show you how I set up a my GPU on GCP without incurring any cost at all.
In this article I’ll walk you through setting up a google cloud computing instance with a 500gb SSD, a 3.75gb ram Broadwell CPU and a Nvidia Tesla K80 GPU. All of this can be done for free at the start.
I’ll be setting up my instance in the Asia(Taiwan) servers since I’m pretty sick of the high latency I get when using servers in America or Europe, but you can change this on your own later.
Google Cloud Platform is a cloud computing infrastructure which provides secure, powerful, high-performance and cost-effective frameworks. It’s not just for data analytics and machine learning, but that’s for another time. Check it out over here.
Google is promoting usage of their platform right now and are giving away $300 dollars of credit and 12 months as a free tier user.
Pssss…you get $300 for each google account you have 😉 😉
So what are you waiting for?
Seriously go get them though, you’ll need these credits to continue.
To start off you’ll need to upgrade your account to a paid account. Free trial accounts aren’t allowed any quota for GPUs so this is a mandatory step.
Don’t worry you won’t be charged anything as of yet. You’ll only start getting charged once you run out of credits and $300 can last you up to a month if you’re wise about it.
Go to your GCP console here and click on the menu button on the top left.
Look for the option billing and click it. If you haven’t upgraded to a paid account yet, you should see a small button near the top that says upgrade to a paid account. Go ahead and do that.
Smash that menu button again. This time you’ll be looking for the Compute Engine option instead, smash that too. 😃
Next look for three hexagon-ish dots at the taskbar on top.
Smash it and it should bring up a window that lists all your projects. Hit the + button to add a new one. Name your project and move on.
First off you’re gonna have to do a little reading and deciding. You’ll need to figure out which region and zone you want your cloud computer to be at.
Basically each region has a bunch of zones and not all zones offer the same services. Some have SSDs and GPUs, some don’t.
Pay attention to which zones have Broadwell CPUs, as GPUs can only be attached to Broadwell generation or higher CPUs.
This link shows you what each zone offers and this link shows you which zones GPUs are available in.
Since I’m based in Southeast Asia, the nearest region with GPUs is asia-east1-a so I’ll be using that for my set up.
The names of regions and zones are concatenated in GCP. For example in asia-east1-a, the region is asia-east1 and the zone is a. Pretty confusing huh. 😓
Next you’ll have to request for an increase of our GPU quota. Most projects start with a quota of 0 GPUs available so you’ll have to get on our knees and ask for some love from big daddy G.
Smash that menu button once more and look for IAM Admin. Smash.
Once you’re at the IAM & admin page, look for the option Quotas on the side menu. Smash.
Okay it’s gonna get a little tricky here. You’ll be brought to a page filled things that are all called Google Compute Engine API. It looks like this:
Remember that region-zone you decided on earlier? You’ll need to use it now. Smash the regions drop down and select your region only. You’ll now see the Google Compute Engine APIs for that region only.
Ctrl+F and search for “k80”, check it and smash Edit Quotas on top. Fill up your details, the number of GPUs you want to request for and a justification. I only requested for 1 but you can ask for more if you want. Just be aware that more GPUs will cost more $$$ per hour so be prepared for that. We’re done here, moving on.
Requested for one? Good. It should take under 5 minutes for an email to pop up in your inbox saying your request has been approved. Give thanks to big daddy G.
Next you’ll have to create the VM instance. I did it through my terminal on a Ubuntu computer, so that’s what I’ll be using in this article. You can do it from the console just as easily too. To use the console to, go to Compute Engine from the menu button and click on VM Instances. Be sure to set up your instance with a Broadwell CPU. Read the docs on setting it up here.
Anyway, you’ll have to install google-cloud-sdk on your local machine. The docs have a pretty good guide on setting it up for different OS’ so check it out.
Once you’re done, run this snippet on your terminal:
You can get the snippet here. “jamsa-dl-fastai” in the first line is the name of the instance, change it to whatever you fancy.
Once you’re instance is set up, head to your GCP console >> Compute Engine >> VM Instances.
You’re new instance should be listed there, along with it’s IP.
Before you can go into the instance you’ll need to edit some settings. Click on the instance name and you’ll be brought to the instance settings.
Click edit at the top and scroll down to the Firewalls. Check both “Allow HTTP traffic” and “Allow HTTPS traffic”. This is so that you can connect to Jupyter Notebook which you’ll be using in the course.
Next add the tag “jupyter” without the double quotes into the Network tags. Save and go back.
Every time you spin up your instance, a new IP address will be assigned to it. This gets pretty annoying if you’re a a frequent user. A static external IP address is an external IP address that is reserved for your project until you decide to release it. Let’s go ahead and request for one. Go back to the main button, look for VPC Engine >> External IP addresses.
Click reserve a static address at the top. Name it, check IPv4, Regional, select your region and attach it to a project. It should look something like this:
Go ahead and reserve it.
Back at your VM Instance console. Check your instance and click on start at the top. Once it’s done starting up you can ssh into your instance from your local machine. Fire up your terminal and enter:
Make sure to change username to your GCP account username and instance_name that you chose for yourself.
So if your username is “i-is-smarts” and the instance is called “smarts-pants-land”, you’ll enter this:
Now that your’re in your instance, you’ve got a little bit more of housekeeping to do.
First run:
If your GPU was set up properly during instance creation it should look like this:
If not, you can reinstall it with:
Next you’ll be getting all the course files from fast.ai, enter:
Make sure it’s all in one line, the command is too long for the Medium’s code snippet block.
The file you just got is a nifty little script that you can use to install all the dependencies like anaconda2, keras, configures your jupyter notebook and git clones the fast.ai lesson materials. Run it with:
The password for sudo on your instance is an empty field, unless you set one up already. Reboot your instance either through the GCP console or:
Jupyter Notebook is the IDE you’ll be using throughout the course so we’ll need to configure it beforehand. It was installed earlier when you ran the install-gpu.sh script with anaconda. If it’s not go ahead and install anaconda again.
On your local machine (not the VM instance), add a firewall rule to allow access to port 8888, which is what you’ll be using for Jupyter:
Be sure to edit “project_name” and “external_ip_of_your_local_machine” to the corresponding values. Make sure that “project_name” is the project and not instance name (I admit I mixed that up at first 😅).
Once your instance has restarted, ssh into it again. Then start up Jupyter Notebook:
EDIT: Some people have been getting the ERR_CONNECTION_TIMEOUT whilst trying to connect to your jupyter notebook. Make sure that you give yourself ownership of your .jupyter and your anaconda directories with:
Where username is your GCP username that appears in your alias when you ssh into the instance (username@instance_name; so the part before the ‘@’).
END EDIT
You may be prompted to set up a password for Jupyter if it’s your first time. Do so if you wish to, you can always set it later. But for now you should see a token when you fire it up. Something like:
Remember it for now.
Go back to your GCP console >> VM Instances. Get the external IP of your instance.
To connect to Jupyter Notebook on your local machine’s browser, go to external_ip:8888 and you should be in. You’ll need to use the token you got earlier here to login if you didn’t set up a password.
IMPORTANT — ALWAYS, ALWAYS TURN OFF YOUR VM INSTANCE WHEN YOU ARE NOT USING IT. IF YOU DON’T YOU’LL CONTINUE BEING CHARGED ON AN HOURLY BASIS IT IS UP.
Congrats! You’re all down with setting up your GPU cloud instance on GCP. Kudos goes to Nok for beating me to the punch and writing an article about this first (I burned a whole day reading the GCP docs before finding that article), the folks at the fast.ai forums for all the resources and eshvk for coming up with a similar method too.
Alright folks, that’s it for now. If there’s any questions you’ve got you can ping me on twitter James Lee or drop me a response down here and I’ll get back to you ASAP.
— — — — — — — — — — — — — — — — — — — — — — — — — — — — — — —
Find this useful? Feel free to smash that clap and check out my other works. 😄
James Lee is an AI Research Fellow at Nurture.AI. A recent graduate from Monash University in Computer Science, he writes about interesting papers on Artificial Intelligence and Deep Learning. Find him on Twitter at @jamsawamsa.
Future Tech. Ai, Blockchain and game design enthusiast. AI Research Fellow at Nurture.Ai & moderator of the FB group Awesome AI Papers
606 
15
606 claps
606 
15
Future Tech. Ai, Blockchain and game design enthusiast. AI Research Fellow at Nurture.Ai & moderator of the FB group Awesome AI Papers
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/how-to-deploy-a-static-react-site-to-google-cloud-platform-55ff0bd0f509?source=search_post---------73,"There are currently no responses for this story.
Be the first to respond.
Top highlight
Looking at the few guides out there, I was pretty confused when trying to find out how to deploy my app.
I looked at a few guides from Google’s site and some Stack Overflow posts but didn’t find them too helpful.
I found a great resource which got me 70% there on MDN:
developer.mozilla.org
Filling in a few gaps I was able to deploy my static React site. So here’s my guide, in a few easy steps.
Note: we’ll be deploying a static site which uses React to render the Front End. No backend/server is hooked up in this example.
3. Create a bucket in GCP. You can keep the default values, just click create.
We’ll be using this bucket to easily upload our build folder files into GCP. Afterwards we’ll transfer the files to our GCP project. Take note of the bucket name as we’ll be using this later. This bucket’s name is straight-veld-8658
4. Once the bucket has been created, click into it and select upload files. Browse to your project directory and upload the entirebuild folder.
5. Another thing we need is an app.yaml file. This file is a config file that tells the App Engine how to map the URLs to static files. I’ve used the same app.yaml file as provided in the sample-app of MDN’s tutorial (but have changed the website directory to build. It looks like this:
Upload this file to the bucket as well.
Your bucket should now be populated with the following files:
6. On the same page find the icon that lets you open a Google Cloud Shell to your app instance. Click on it and open the shell.
Now we’ll upload the build directory and app.yaml file that we uploaded into the straight-veld-8658 bucket into our instance so we can launch the app. Use the following commands:
Note: the format of the gsutil rsync command is as follows : gsutil rsync -r [source] [destination] (-r means sync files recursively)
You can make sure that the files are there. When you cd test-app and ls you should see both app.yaml and build
7. Deploy the app by running gcloud app deploy in the shell. You should see some sort of success message indicating the app is served. It should also provide you with the url you can visit to see the app.
Generally this url is in the following format unless you’ve linked a custom domain name to it: https://[app_name].appspot.com
Google Cloud community articles and blogs
694 
25
694 claps
694 
25
Written by
Learn more about me here: https://dddotcom.github.io/ !
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Learn more about me here: https://dddotcom.github.io/ !
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://towardsdatascience.com/how-to-use-jupyter-on-a-google-cloud-vm-5ba1b473f4c2?source=search_post---------74,"Sign in
There are currently no responses for this story.
Be the first to respond.
Lak Lakshmanan
Apr 11, 2019·8 min read
Note: The recipes in this article will still work, but I recommend that you use the notebook API now. Do:
gcloud beta notebooks --help
The simplest way to launch a notebook on GCP is to go through the workflow from the GCP console. Go to AI Platform and click on Notebook Instances. You can create a new instance from the user interface:
Once the instance is launched, you can click on a link to open JupyterLab:
When the instance is launched, it has a persistent disk. That disk will hold your notebooks. You can stop and restart the VM (from the GCP web console) without losing those notebooks.
Note that you can attach a GPU to a notebook instance from the user interface:
Enjoy!
This article is a collection of a few of my “recipes” for working with Notebook instances.
The instance is a Compute Engine image, so if you want to script things out, customize the machine, change its firewall rule, etc. you can use Compute Engine capabilities. The notebook instance is a Deep Learning VM, which is a family of images that provides a convenient way to launch a virtual machine with/without a GPU on Google Cloud. It has Jupyter Lab already installed on it and you can access it without the need for proxies or ssh.
The simplest approach is to specify an image family (see the docs for what the image families are available). For example, you can get the latest image in the tensorflow-gpu family with a P100 GPU attached using:
The URL to access Jupyter Lab is part of the metadata of the VM that you just launched. You can get it using:
Here’s a script that will do steps #1 and #2, waiting until the Jupyter notebook server has started:
Simply navigate to that URL and you’ll be in JupyterLab.
Click on the last icon in the ribbon of icons in the left-hand pane and you will be able to git clone a repository. Use the one for my book:
https://github.com/GoogleCloudPlatform/data-science-on-gcp
Navigate to updates/cloudml and open flights_model.ipynb. You should be able to run through the notebook.
You can also open up a Terminal and use git clone, git checkout, git push, etc. I tend to find it easier than using the built-in Git UI. But your mileage may vary!
You can specify a set of operations to run after Jupyter launches. These will be run as root.
In general, use the image-family approach for development (so that you are always developing with the latest of everything), but pin down to a specific image once you move things to production. The reason you want to pin down to a specific image in production is that you want to run on a version that you have actually tested your code with.
Get the list of images and find the one you were using (the latest in the image family you specified above):
Then, specify it when creating the Deep Learning VM (lines you might want to change are bolded):
The key aspect here is to launch papermill with a startup script and exit the notebook VM using TERMINATE without a restart-on-failure once papermill is done. Then, delete the VM. See this blog post for more details.
To create a Deep Learning VM attached to a TPU, first create a Deep Learning VM and then create a TPU with the same TensorFlow version:
The only difference when creating the Deep Learning VM is that you are specifying the TPU_NAME in the startup script.
If you create a Deep Learning VM and you specified a GCP login name (all my examples above, except for the production one did so), then only you (and project admins) will be able to ssh into the VM.
All Jupyter notebooks will run under a service account. For the most part, this will be fine, but if you need to run operations that the service account doesn’t have permission to do, you can have code in Jupyter run as you by doing the following:
Note: Do not use end-user credentials unless you started the machine in ‘single user mode’.
Creating an image in the tf-latest family uses the latest stable TensorFlow version. To work with TF-nightly (e.g. this is how to get TensorFlow 2.0-alpha), use:
Restarting Jupyter: Usually, all you need to do is to restart the kernel by clicking on the icon in the notebook menu. But once in a long while, you might completely hose the environment and want to restart Jupyter. To do that, go to the Compute Instances section of the GCP Console and click on the SSH button corresponding to your Notebooks instance. In the SSH window, type:
Startup logs: If Jupyter failed to start, or you don’t get a notebook link, you might want to look at the complete logs (including startup logs). Do that using:
The TensorFlow images use pip, but the PyTorch images use conda. So, if you want to use conda, the PyTorch images are a better starting point.
If you want to develop using the Deep Learning VM container image on your local machine, you can do that using Docker:
If you have a GPU on your local machine, change the image name from tf-latest-cpu to tf-latest-cu100.
__________________________________________________
I’m interested in expanding on these recipes. Contact me if you have a suggestion on a question/answer that I should add. For your convenience, here’s a gist with all the code.
Data Analytics & AI @ Google Cloud
417 
9
Every Thursday, the Variable delivers the very best of Towards Data Science: from hands-on tutorials and cutting-edge research to original features you don't want to miss. Take a look.
417 claps
417 
9
Your home for data science. A Medium publication sharing concepts, ideas and codes.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/avmconsulting-blog/kubernetes-ci-cd-using-jenkins-on-google-cloud-5b10da6147a6?source=search_post---------75,"There are currently no responses for this story.
Be the first to respond.
This is our new blog and its about setting up CI/CD pipeline for Kubernetes using Jenkins in Google Cloud .
Sample code GitHub link . This is simple python application with backend PostgreSQL as database. This is just sample code we use this as reference and later you guys can modify same for your code. In repository we have Dockerfile and Kube YAML scripts as part of main…
"
https://medium.com/google-cloud/serverless-continuous-integration-and-ota-update-flow-using-google-cloud-build-and-arduino-d5e1cda504bf?source=search_post---------76,"There are currently no responses for this story.
Be the first to respond.
Adding Over The Air (OTA) updates is an important factor for IoT applications to succeed. It’s a mechanism to ensure that devices are always up to date with new settings, security fixes and also adding new features to the hardware, making the customer who brought the device happy with the hardware improvements and at the same time feeling safer.
There are two important parts on an OTA architecture:
Here I’ll show how to setup an initial OTA mechanism using some Google Cloud tools, deploying the updates to ESP8266 and ESP32 board using the Arduino platform.
PlatformIO will be used for building the images, as it has a set of command line tools that enables us to automate the process of generating binary images for the devices. On Google Cloud we are going to use Google Cloud Build, that is a managed Continuous Integration environment, Google Cloud Storage for storing the binary images on the cloud and Cloud Functions to handle HTTP requests querying for current firmware versions and managing them.
PlatformIO is a set of cross-platform tools for developing for embedded devices. It supports a lot of different platforms and frameworks for IoT development and also a huge set of libraries made by the community that can be easily used on your project.
I recommend installing the Visual Studio Code (VSCode) IDE and the PlatformIO plugin to get started using it. Just follow the step on the link below:
platformio.org
The code for this project is available on the following Github link. Clone or download the project the code and open it on the IDE.
github.com
The platformio.inifile contains all the configuration to build the project on the ESP32 and ESP8266 boards. Also, the project dependencies are listed here. An important configuration is the build flag VERSION, that is compiled on the project code to mark which version the device is running currently. So, every time that we create a new firmware version, this code should be bumped so the device will be able to properly check if it needs to download a new version.
The device code makes an HTTP query to the backend, sending the current version, to check if it should download a new one. Also, there is a device internal HTTP handler to display the current version. To handle Wifi connectivity, the project uses the WifiManager library, that creates an access point to setup WiFi on the device.
To deploy to the board you can use the “Build” and “Upload” buttons on PlatformIO Toolbar:
To get started with Google Cloud you can do all on the Cloud Console web interface, but the command line tools is a more powerful tool and we’ll need later to deploy the cloud function. To use the gcloud command line tools, follow the instructions here to download and install it.
console.cloud.google.com
cloud.google.com
Also after this you should authenticate and create a project to use in this tutorial, exchange YOUR_PROJECT_NAMEwith a name that you want for this project:
Now let’s create the Cloud Build configuration and also the Bucket to store the binaries. Follow the steps:
Cloud Build Setup :
Cloud Storage Setup :
Our repository contains a cloudbuild.yaml file, that contains all the configuration to build the firmware and push to Cloud Storage. Cloud Build uses Docker for building artifacts, so I used an image that contains all the PlatformIO tools for building embedded projects using our platformio.ini file.
Now, every time that you push a new tag to your repository, it will trigger a build on Cloud Build. You can create the tag on the UI of your git provider or you can do this using the following git commands.
And if everything is working correctly you should start seeing some builds on the History tag on Cloud Build page when you push a new tag. We’ll revisit this at the end of the post to see how to push new versions.
To control the OTA process, we basically need two things: Store firmware metadata in a database, so we can query later for the latest version and a way to check if given the current device version, check if there is the need to update the device. For achieving this I built two cloud functions:
Now you will need the gcloudtool that we installed in the beginning to deploy the functions. Alter the project ID on the file deploy-prod.sh and run it to deploy both functions. On the first time, it will probably ask to enable the cloud functions API. Just confirm that and continue with the process.
With this command, all the functions are deploying and reacting to events in our architecture.
To push and build a new version that can be download by the devices is simple, with just a couple of git commands we can trigger a new Continuous Integration build. So, let’s say that you add a new feature to the device, like a blink on our loop function.
Now to create a new release, change the version on platformio.inifile, from v1.1.0 to v1.2.0 for example, commit the changed files, tag the new commit and push all to the repository. Here are the commands:
This will trigger all of our processes and when it’s all done, you can reset your device with an older version and see in the logs that it will download the new version and reset itself. If everything is right, you should start seeing you a LED blinking.
I hope that this tutorial gave you an overview of what can be done to automate this process for achieving better IoT deployments. We went though a lot of pieces involved in an OTA deployment process, but of course, is too simple yet. We can improve by adding much more features, like :
References :
Google Cloud community articles and blogs
505 
14
505 claps
505 
14
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Product Engineer @Leverege & Google Developer Expert for IoT. When I'm not cooking, I'm building fun stuff or helping my local dev community. GDG organizer.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/google-cloud-pub-sub-ordered-delivery-1e4181f60bc8?source=search_post---------77,"There are currently no responses for this story.
Be the first to respond.
The Google Cloud Pub/Sub team is happy to announce that ordered delivery is now generally available. This new feature allows subscribers to receive messages in the order they were published without sacrificing scale. This article discusses the details of how the feature works and talks about some common gotchas when trying to process messages in order in distributed systems.
Ordering in Cloud Pub/Sub consists of two properties. The first is the ordering key set on a message when publishing. This string — which can be up to 1KB — represents the entity for which messages should be ordered. For example, it could be a user ID or the primary key of a row in a database. The second property is the enable_message_ordering property on a subscription. When this property is true, subscribers receive messages for an ordering key in the order in which they were received by the service.
These two properties allow publishers and subscribers to decide independently if messages are ordered. If the publisher does not specify ordering keys with messages or the subscriber does not enable ordered delivery, then message delivery is not in order and behaves just like Cloud Pub/Sub without the ordered delivery feature. Not all subscriptions on a topic need to have the same setting for enable_message_ordering. Therefore, different use cases that receive the same messages can determine if they need ordered delivery without impacting each other.
The number of ordering keys is limited only by what can be represented by the 1KB string. The publish throughput on each ordering key is limited to 1MB/s. The throughput across all ordering keys on a topic is limited to the quota available in a publish region. This limit can be increased to many GBs/s.
All the Cloud Pub/Sub client libraries have rich support for ordered delivery. They are the best way to take advantage of this feature, as they take care of a lot of the details necessary to ensure that messages are processed in order. Ordered delivery works with all three types of subscribers: streaming pull, pull, and push.
Ordered delivery has three main properties:
Let’s examine what these properties mean with an example. Imagine we have two ordering keys, A and B. For key A, we publish the messages 1, 2, and 3, in that order. For key B, we publish the messages 4, 5, and 6, in that order. With the ordering property, we guarantee that 1 is delivered before 2 and 2 is delivered before 3. We also guarantee that 4 is delivered before 5, which is delivered before 6. Note that there are no guarantees about the order of messages across different ordering keys. For example, message 1 could arrive before or after message 4.
The second property explains what happens when messages are redelivered. In general, Cloud Pub/Sub offers at-least-once delivery. That means messages may be sent to subscribers multiple times, even if those messages have been acknowledged. With the consistent redelivery guarantee, when a message is redelivered, the entire sequence of subsequent messages for the same ordering key that were received after the redelivered message will also be redelivered. In the above example, imagine a subscriber receives messages 1, 2, and 3. If message 2 is redelivered (because the ack deadline expired or because the best-effort ack was not persisted in Cloud Pub/Sub), then message 3 is guaranteed to be redelivered as well.
The last property defines where messages for the same ordering key are delivered. It applies only to streaming pull subscribers, since they are the only ones that have a long-standing connection that can be used for affinity. This property has two parts. First, when messages are outstanding to a streaming pull subscriber — meaning the ack deadline has not yet passed and the messages have not been acknowledged — then if there are more messages to deliver for the ordering key, they go to that same subscriber.
The second part pertains to what happens when no messages are outstanding. Ideally, one wants the same subscribers to handle all of the messages for an ordering key. Cloud Pub/Sub tries to do this, but there are cases where it cannot guarantee that it will continue to deliver messages to the same subscriber. In other words, the affinity of a key could change over time. Usually this is done for load-balancing purposes. For example, if there is only one subscriber, all messages must be delivered to it. If another subscriber starts, one would generally want it to start to receive half of the load. Therefore, the affinity of some of the ordering keys must move from the first subscriber to this new subscriber. Cloud Pub/Sub waits until there are no more messages outstanding on an ordering key before changing the affinity of the key.
One of the most difficult problems with ordered delivery is doing it at scale. It usually requires an understanding of the scaling characteristics of the topic in advance. When a topic extends beyond that scale, maintaining order becomes extremely difficult. Cloud Pub/Sub’s ordered delivery is designed to scale with usage without the user having to think about it.
The most common way to do ordering at scale is with partitions. A topic can be made up of many partitions, where each stores a subset of the messages published to the topic. When a message gets published, a partition is chosen for that message, either explicitly or by hashing the message’s key or value to a partition. The “key” in this case is what Cloud Pub/Sub calls the ordering key.
Subscribers connect to one or more partitions and receive messages from those partitions. Much like the publish side, subscribers can choose partitions explicitly or rely on the messaging service to assign subscribers to partitions. Partition-based messaging services guarantee that messages within the same partition are delivered in order.
A typical partition setup would look like this:
The green boxes represent the partitions that store messages. They would be owned by the messaging servers (often called “brokers”), but we have omitted those servers for simplicity. The circles represent messages, with the color indicating the message key and the number indicating the relative order for the messages of that color.
One usually has a lot fewer partitions than there are keys. In the example above, there are four message colors but only three partitions, so the second partition contains both blue and red messages. There are two subscribers, one that consumes from the first partition and one that consumes from the second and third partitions.
There are three major issues a user may have to deal with when using partitions: subscriber scaling limitations, hot shards, and head-of-line blocking. Let’s look at each in detail.
Within a set of subscribers across which delivery of messages is load balanced (often called a “consumer group”), only one subscriber can be assigned to a partition at any time. Therefore, the maximum amount of parallel processing that can occur is min(# of partitions, # of subscribers). In the example above, we could load balance across no more than three subscribers:
If processing messages suddenly became more expensive or — more likely — a new consumer group was added to receive messages in a new pipeline that requires longer processing of the messages, it may not be possible to gain enough parallelism to process all the published messages. One solution would be to have a subscriber whose job is to republish the messages on a topic with more shards, which the original subscribers could consume instead:
The downside is that now both of these topics must be maintained or a careful migration must be done to change the original publisher to publish to the new topic. If both topics are maintained, then messages are stored twice. It might be possible to delete the messages from the first topic once they are published to the second topic, but this would require the migration of any subscribers receiving messages from the original topic to the new topic.
The next issue is a hot shard — the overloading of a single partition. Ideally, traffic patterns across partitions are relatively similar. However, it is possible that there are a lot more messages or much larger messages hashing to one partition in comparison to messages hashing to other partitions. As a result, a single partition can become overloaded:
What can be done to deal with this hot shard? Typically, the solution is to add partitions. However, maintaining order during a repartitioning can be very difficult. For example, if we add a new partition in the case above, it could result in related messages going to completely different partitions:
With this new set of partitions, purple messages now publish to the first partition, blue messages to the third partition, and yellow and red messages to the fourth partition. This repartitioning causes several problems. First of all, the fourth partition now contains messages for keys that were previously split among the two subscribers. That means the affinity of keys to subscribers must change.
Even more difficult is the fact that if subscribers want all messages in order, they must carefully coordinate from which partitions they receive messages when. The subscribers would have to be aware of the last offset in each partition that was for a message before adding more partitions. Then, they need to consume messages up to those offsets. After they have processed messages up to the offsets in all the partitions, then the subscribers can start to consume messages beyond that last offset.
The last difficult issue is head-of-line blocking, or the inability to process messages due to the slow processing of messages that must be consumed first. Let’s go back to the original scenario:
Imagine that the red messages require a lot more time to process than the blue ones. When reading messages from the second partition, the processing of the blue message 2 could be needlessly delayed due to the slow processing of red message 1. Since the unit of ordering is a partition, there is no way to process the blue messages without processing the red messages. One could try to solve this by repartitioning in the hopes that the red and blue messages end up in different partitions. However, the processing of the red messages will block the processing of others in whichever partition they end up. The repartitioning also results in the same issues discussed in the Hot Shards section.
Alternatively, the publisher could explicitly assign the red messages to their own partition, but it breaks the decoupling of publishers and subscribers if the publisher has to make decisions based on the way subscribers process messages. It may also be that the extra processing time for the red messages is temporary and doesn’t warrant large-scale changes to the system. The user has to decide if the delayed processing of some messages or the arduous process of changing the partitions is better.
Cloud Pub/Sub’s ordered delivery implementation is designed so users do not need to be subject to such limitations. It can scale to billions of keys without subscriber scaling limitations, hot shards or head-of-line blocking. As one may expect with a high-throughput pub/sub system, messages are split up into underlying partitions in Cloud Pub/Sub. However, there are two main properties of the service that allow it to overcome the issues commonly associated with ordered delivery:
By taking advantage of these properties, Cloud Pub/Sub brokers have three useful behaviors:
These behaviors allow Cloud Pub/Sub to avoid all three major issues with ordered delivery at scale!
Ordered delivery doesn’t come for free, of course. Compared with unordered delivery, the ordered delivery of messages may slightly decrease publish availability and increase end-to-end message delivery latency. Unlike the unordered case, where delivery can fail over to any broker without any delay, failover in the ordered case requires coordination across brokers to ensure the messages are written to and read from the correct partitions.
Even with Cloud Pub/Sub’s ability to deliver messages in order at scale, there are still subtleties that exist when relying on ordered delivery. This section details the things to keep in mind when building an ordered pipeline. Some of these things apply when using other messaging systems with ordered delivery, too. In order to provide a good example of how to use ordering keys effectively, the Cloud Pub/Sub team has released an open-source version of its ordering keys prober. This prober is almost identical to the one run by the team continuously to verify the correct behavior of this new feature.
On the surface, publishing in order seems like it should be very easy: Just call publish for each message. If we could guarantee that publishes never fail, then it would be that simple. However, transient or permanent failures can happen with publish at any time, and a publisher must understand the implications of those failures.
Let’s take the simple example of trying to publish three messages for the same ordering keys A: 1, 2, and 3. The Java code to publish these messages could be the following:
If there were no failures, then each publish call would succeed and the message ID would be returned in the future. We’d expect the subscriber to receive messages 1, 2, and 3 in that order. However, there are a lot of things that could happen. If a publish fails, it likely needs to be attempted again. The Cloud Pub/Sub client library internally retries requests on retriable errors. Errors such as deadline exceeded do not indicate whether or not the publish actually succeeded. It is possible that the publish did succeed, but the publish response wasn’t received by the client in time for the deadline, in which case the client may have attempted the publish again. In such cases, the sequence of messages could have repeats, e.g., 1, 1, 2, 3. Each published message would have its own message ID, so from the subscriber’s perspective, it would look like four messages were published, with the first two having identical content.
Retrying publish requests is complicated even more by batching. The client library may batch messages together when it sends them to the server for more efficient publishing. This is particularly important for high-throughput topics. In the case above, it could be that messages 1 and 2 are batched together and sent to the server as a single request. If the server fails to return a response in time, the client will retry this batch of two messages. Therefore, it is possible the subscriber could see the sequence of messages 1, 2, 1, 2, 3. If one wants to avoid these batched republishes, it is best to set the batch settings to allow only a single message in each batch.
There is one additional case with publishing that could cause issues. Imagine that in running the above code, the following sequence of events happens:
The result could be that messages 2 and/or 3 are successfully published and sent to subscribers without 1 having been sent, which would result in out-of-order delivery. A simple solution may be to make the calls to publish synchronous:
While this change would guarantee that messages are published in order, it would make it much more difficult to publish at scale, as every publish operation would block a thread. The Cloud Pub/Sub client libraries overcome this problem in two ways. First, if a publish fails and there are other messages for the same ordering key queued up in the library’s message buffer, it fails the publishes for all those messages as well. Secondly, the library immediately fails any subsequent publish calls made for messages with the same ordering key.
How does one get back to a state of being able to publish on an ordering key when this happens? The client library exposes a method, resumePublish(String orderingKey). A publisher should call resumePublish when it has handled the failed publishes, determined what it wants to do, and is ready to publish messages for the ordering key again. The publisher may decide to republish all the failed messages in order, publish a subset of the messages, or publish an entirely new set of messages. No matter how the publisher wants to handle this edge case, the client library provides resumePublish as a means to do so without losing the scaling advantages of asynchronous publishing. Take a look at the ordering key prober’s publish error logic for an example of how to use resumePublish.
All of the above issues deal with publishing from a single publisher. However, there is also the question of how to publish messages for the same ordering key from different publishers. Cloud Pub/Sub allows this and guarantees that for publishes in the same region, the order of messages that subscribers see is consistent with the order in which the publishes were received by the broker. As an example, let’s say that both publishers X and Y publish a message for ordering key A. If X’s message is received by Cloud Pub/Sub before Y’s, then all subscribers will see the messages in that order. However, publishers do not have a way to know in which order the messages were received by the service. If the order of messages across different publishers must be maintained, then the publishers need to use some other mechanism to coordinate their publishes, e.g., some kind of locking service to maintain ownership of an ordering key while publishing.
It is important to remember that ordering guarantees are only for messages published in the same region. Therefore, it is highly recommended that all publishers use regional service endpoints to ensure they publish messages to the same region for the same ordering key. This is particularly important for publishers hosted outside of GCP; if requests are routed to GCP from another place, it is always possible that the routing could change if using the global endpoint, which could disrupt the order of messages.
Subscribers receive messages in the order they were published. What it means to “receive messages in order” varies based on the type of subscriber. Cloud Pub/Sub supports three ways of receiving messages: streaming pull, pull, and push. The client libraries use streaming pull (with the exception of PHP), and we talk about receiving messages via streaming pull in terms of using the client library. No matter what method is used for receiving messages, it is important to remember that Cloud Pub/Sub offers at-least-once delivery. That means subscribers must be resilient to receiving sequences of messages again, as discussed in the Ordering Properties section. Let’s look at what receiving messages in order means for each type of subscriber.
When using the client libraries, one specifies a user callback that should be run whenever a message is received. The client libraries guarantee that for any given ordering key, the callback is run to completion on messages in the correct order. If the messages are acked within that callback, then it means all computation on a message occurs in order. However, if the user callback schedules other asynchronous work on messages, the subscriber must ensure that the asynchronous work is done in order. One option is to add messages to a local work queue that is processed in order.
It is worth noting that because of asynchronous processing in a subscriber like this, ordered delivery in Cloud Pub/Sub does not work with Cloud Dataflow at this time. The nature of Dataflow’s parallelized execution means it does not maintain the order of messages after they are received. Therefore, a user’s pipeline would not be able to rely on messages being delivered in order. To ensure that one does not use Pub/Sub in Dataflow and expect ordered delivery, Dataflow pipelines that use a subscription with ordering keys enabled fail on startup.
For subscribers that use the pull method directly, Cloud Pub/Sub makes two guarantees:
The requirement that only one batch of messages can be outstanding at a time is necessary to maintain ordered delivery. The Cloud Pub/Sub service can’t guarantee the success or latency of the response it sends for a subscriber’s pull request. If a response fails and a subsequent pull request is fulfilled with a response containing subsequent messages for the same ordering key, it is possible those subsequent messages could arrive to the subscriber before the messages in the failed response. It also can’t guarantee that the subsequent pull request comes from the same subscriber.
The restrictions on push are even tighter than those on pull. For a push subscription, Cloud Pub/Sub allows only one message to be outstanding per ordering key at a time. Since each message is sent to a push subscriber via its own request, sending such requests out in parallel would have the same issue as delivering multiple batches of messages for the same ordering key to pull subscribers simultaneously. Therefore, push subscribers may not be a good choice for topics where messages are frequently published with the same ordering key or latency is extremely important, as the restrictions could prevent the subscriber from keeping up with the published messages.
In summary, ordered delivery at scale usually requires one to be very careful with the capacity and setup of their messaging system. When that capacity is exceeded or message processing characteristics change, adding capacity while maintaining order is a time-consuming and difficult process. With the introduction of ordered delivery into Cloud Pub/Sub, users can rely on order in ways they are accustomed to in a system that still automatically scales with their usage.
Google Cloud community articles and blogs
451 
5
451 claps
451 
5
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Technical Lead on Google Cloud Pub/Sub
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/@hbmy289/how-to-set-up-a-free-micro-vps-on-google-cloud-platform-bddee893ac09?source=search_post---------78,"Sign in
There are currently no responses for this story.
Be the first to respond.
HBMY 289
Nov 21, 2018·5 min read
While there are a lot of providers that offer smaller VPS you can test for free for a certain time range, there are not many that are permanently free. This article will focus on setting up an f1-micro VPS on Google Cloud Platform that is advertised as “always free”. Google offers a very broad range of possible VPS configurations and in contrast to other providers, the free tier is rather hidden in all the available options. This article will guide you through the setup process step by step.
When testing Google cloud computing for the first time you will currently also get a voucher that allows renting a wide range of non-free VPS configurations up to a total amount of 300$ in the first 12 months.
If you already have a valid account at Google Cloud Platform (either in test phase or upgraded) you can directly go to Set up an f1-micro instance. Otherwise, you first need to get a new test account.
Visit the Google cloud console (https://console.cloud.google.com/getting-started) and press Try For Free.
Fill out all necessary information on the displayed dialog. You will be required to enter valid credit card information. Google will not charge your credit card even if you use up all of the 300$ that is part of the test phase.
After entering all the required information, the free-trial phase is started.
The f1-micro VPS is part of Google’s “always free” products. If the 12 month test period has ended or if you used up all the credit of your voucher, you will not be able to use these free products anymore. In this case, you will have to upgrade your account, enabling Google to charge your credit card. However, the f1-micro VPS will still be free. Be aware that you might be credited in this case if you use more than the free network egress traffic of 1 GB per month (excluding Australia and China).
Although setting up an f1-micro VPS as shown in this article will not generate any cost for you, it could happen that due to some misconfiguration you are actually using a non-free product.
I cannot take responsibility for any cost that might be generated on your account, especially when exceeding the allowed traffic volume of 1 GB per month. Be sure to use the correct configuration and check your balance repeatedly to avoid any unwanted charges!
Visit the compute engine section of the Google Cloud Platform (https://console.cloud.google.com/compute/) and create a new instance.
The next dialog will allow you to define the configuration of your VPS. You can also choose a much more powerful setup, but only the correct F1-micro settings will result in a free VPS.
Choose a meaningful name for your new VPS and select micro (1 shared CPU) as the machine type for the N1 series. Only instances located in certain US regions will be part of the “always free” deal. At the time of writing this included the regions us-central1 (Iowa), us-east1 (South Carolina) and us-west1 (Oregon).
IMPORTANT!
Check the text next to your settings. It has to look like below and show a free usage time that is equivalent to the number of days in the current month times 24 hours (720 hours for November with 30 days).
Only if this text is displayed your VPS instance will be free of charge!
Click on Change in the Boot disk section. Optionally you can change the pre-installed operating system you want to use. More importantly, you can increase the amount of persistent disk space in this dialog. Increase the value to 30 (included in the “always free” package) and leave the dialog using the Select button.
The firewall section allows to automatically add exceptions for port 80 and 443 if you plan to use your VPS for a web server. Firewall rules can also be added and edited later.
Finally, finish the process by hitting Create.
After a few seconds, you will have a running f1-micro instance. You can access it via the SSH drop-down menu.
Use the first option in the menu to open an ssh-shell in a new browser window to access the VPS.
I hope you liked this short article and now can start using your micro-VPS instance for a lot of cool stuff.
HBMY289
607 
4
607 
607 
4
"
https://medium.com/google-cloud/google-cloud-products-described-in-4-words-or-less-7776af0552cd?source=search_post---------79,"There are currently no responses for this story.
Be the first to respond.
Download PDFs, text, and hi-res PNGs from https://github.com/gregsramblings/google-cloud-4-words
‪Includes Google Cloud, Firebase, Google Maps Platform, G Suite APIs
Welcome Looker!
Also tweeted at https://twitter.com/gregsramblings/status/1248268470308851712
Check my blog for other resources — https://gregsramblings.com
Google Cloud community articles and blogs
227 
3
227 claps
227 
3
Written by
Google Developer Relations — Living in San Francisco https://gregsramblings.com | https://cloud.google.com | http://instagram.com/gregsimages
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Google Developer Relations — Living in San Francisco https://gregsramblings.com | https://cloud.google.com | http://instagram.com/gregsimages
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@austinhale/building-a-node-api-with-express-and-google-cloud-sql-9bda260b040f?source=search_post---------80,"Sign in
There are currently no responses for this story.
Be the first to respond.
Austin Hale
Jul 28, 2018·5 min read
Note: This is an ongoing series of blog posts detailing how we made an iOS & Android mobile app in 10 days. Feel free to follow me here or on Twitter to get updated when the other articles are published.
"
https://medium.com/google-cloud/13-most-common-google-cloud-reference-architectures-23630b46326d?source=search_post---------81,"There are currently no responses for this story.
Be the first to respond.
👋 Hi Google Cloud Devs!!
I am asked multiple times to compile a list of most common Google Cloud reference architectures. That got me thinking and I started #13DaysOfGCP mini series on Twitter. If you were not able to catch it, or if you missed a few days, here I bring to you the summary!
Thank you to all who provided the topic ideas and shared their thoughts. This has been instrumental in helping me pick the topics that you all were interested in seeing the over the course of 13 days.
I am thankful and really humbled by your participation, engagement and topic suggestions that made this series so much fun, not just for me but also for the rest of the Google Cloud Community members!
Thanks again and 👋 until later 🙂
Google Cloud community articles and blogs
393 
1
393 claps
393 
1
Written by
Developer Advocate @Google, Artist & Traveler! Twitter @pvergadia
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Developer Advocate @Google, Artist & Traveler! Twitter @pvergadia
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alfianlosari/serverless-node-js-rest-api-with-google-cloud-function-firestore-d7b422f58511?source=search_post---------82,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alfian Losari
Jun 6, 2018·5 min read
You can also read this article in my Xcoding With Alfian blog website using the link below.
www.alfianlosari.com
Serverless Application Architecture has been gaining massive popularity in recent years because as a software engineer we do not have to worry about configuration, deployment, and scalability of the application. Instead of spending time doing those activities, we can focus our time on building and testing the application logic for better user experience.
In this article, i am going to focus on demonstrating how to build a simple CRUD node.js Express REST API using Google Cloud Function and store the data using Firestore NoSQL database. Both of them are fully serverless and provided by Google Cloud as service with free tier.
What we will build:
Our note API is a straightforwared Express router object with methods for GET all notes, GET single note by ID, POST to create note, PUT to update note by ID, and DELETE to delete note by ID.
To access Firestore we import admin from firebase-admin module that we add in our package.json NPM dependency. Because our app runs on Google Cloud Platform with the same project as our Firestore, we can just initialize the admin with default credential. If we run our app in other server, we can create and import JSON credential and pass it to initalize the Firestore. We store the firestore db object in db constant to access later.
How do Firestore model and store data?, according to Google Firestore documentation:
Following Cloud Firestore’s NoSQL data model, you store data in documents that contain fields mapping to values. These documents are stored in collections, which are containers for your documents that you can use to organize your data and build queries.
To sums it up, basically Firestore stores JSON document inside a collections, the JSON type can be String, Number, Nested objects. The cool things about Firestore we can also store subcollections within documents to build hierarchical data structures and it scales as the database grows.
For our note app we will store the note inside collection we called “notes” and our document JSON will only contain text with the value of String.
We can access the collection by passing the name of the collection, then we can use the add method passing the JSON data we want to add. This automatically generates unique ID to our document.
To query all our notes document inside the notes collection we simply call get() on collection object. It returns a snapshot of document reference that we can iterate.
To query a single note document inside the collection we can use the doc method passing the ID of the document.
While the query is not as powerful as mongoDB, Firestore provide many quite powerful complex queries such as compound queries and many others. See the documentation about querying complex data.
To update note we can use the doc method passing the ID and set method passing the data. We are passing the merge true options so if the document has another fields we are not overriding another fields, just the text field.
To delete note we can use the doc method passing the ID and delete method.
Here is the snippet of all the code inside our Router:
The Express app is very simple and straightforward, it assign some middlewares like cors, express 4.1+ json parser, and error handler. We also import our note Router and assign it to the /api route.
To deploy our Express app to Google Cloud Platform, we need to create an index.js file that will be used by the Google Cloud CLI as an entrypoint when deploying the function to GCP. We need to export all the function we want to expose, the function must accept 2 parameters, which are Request and Response object, they are compatible with Express request and response object so we can import our Express app and use it right away. It is assigned to a note constant because each function deployed must have a name.
One important note, Google Cloud Function currently only runs on Node .JS 6.1+ environment so we can’t deploy ES7 based source because Node 6 does not support many features like async await. Make sure to transpile all of the sources using Babel transpiler before deploying.
Make sure to Install Google Cloud CLI and beta components because Cloud Function is still in Beta, then type:
It will take about 2 minutes to deploy, after it completes, the url to access the function will be provided or you can access it from the Google Cloud Console. In this case to access the API we call https://us-central1-alfianlosari-cd236.cloudfunctions.net/note/api. We can also access the Google StackDriver Logger and Error Reporting from the Console to see all the log activities and error traces on our API.
There are many ways that we can use to deploy our functions, that you can refer in the deploying documentation.
I hope this article serves as a guide for all of you that want to deploy a simple backend service without knowledge of dev-ops and its complexity. You can access all the source code in the project Github Repository.
Serverless and Microservices are really the future for backend development because of the simplicity and rapid development productivity. There are still many improvements that need to be addressed, but i really believe Serverless is the future!.
Mobile Developer and Lifelong Learner. Currently building super app @ Go-Jek. Xcoding with Alfian at https://alfianlosari.com
532 
4
532 
532 
4
Mobile Developer and Lifelong Learner. Currently building super app @ Go-Jek. Xcoding with Alfian at https://alfianlosari.com
"
https://medium.com/@olamilekan001/image-upload-with-google-cloud-storage-and-node-js-a1cf9baa1876?source=search_post---------83,"Sign in
There are currently no responses for this story.
Be the first to respond.
Olalekan Odukoya
Oct 30, 2019·11 min read
In this relatively short tutorial, I’ll be showing you how to upload an image to Google Cloud Storage and also how you can resize the image.
As developers, one of the things we shouldn’t do is to always store an Image to our database as this could open up our application to all sort security vulnerabilities, it also eats up unnecessary space in our database since they are stored as a blob file, and finally, it could lead to an increase in the cost of backing up data. Solutions to these problems could either be to Store our images in the file system, and create links/pointers to these images in the database but this could be slow when we need to fetch those images. Another way this problem can be solved is to serve our files with a CDN (Content Delivery Network). A Content Delivery network is often used to accelerate the effectiveness and efficiency of serving contents from a server to the clients by caching them and making them available to the clients as quick as possible. However, serving just images with a CDN can be very expensive, and the thing is, since CDN tends to thrive by caching contents, you might have uploaded a new image or file but it wouldn’t be available to the users until you clear the cache. The best option is to use an online file storage web service on the cloud.
The Cloud is just a remote server sitting somewhere, where you can store your content and access them quickly, easily, securely, and from anywhere in the world. One of the advantages of using the Cloud is because of how elastic, accessible, and secure it can be. A lot of companies offer these cloud-based services including Tech Giants like Amazon and Google.
In this tutorial, we will be using Google’s cloud-based service known as Google Cloud Platform. Google Cloud platforms offer a range of cloud-based services but the one we are going to be focusing on is going to be Google Cloud Storage. Google cloud storage is a service on GCP that allows you to store static files like images and files. The advantage of using this is that instead of storing say an image in your database, you instead, upload the image to GCS and get a link/url to that file in which you can then store into your database hence, saving disk space in your database and reducing time spent in trying to backup your data.
Before we can start uploading images or any files, we need to first create an account, to do so, copy and paste the URL below into your browser and click Enter. This takes you the Google Cloud platform’s website.
After the page loads, you should see something similar to what I have below.
Click on the “Go To Console” Button. This takes you the Cloud Console. Which looks something similar to what we have below.
You’ll notice that I painted a part of the picture, click on that part on your screen. This redirects us to a page where we are being asked to create a project. Give it a name and click on the create button. This action creates a new project for us in which we can associate our GCS project with.
Yours might be way different from mine if it’s your first using GCP.
In the search bar, type Cloud Storage.
After doing so, you should be redirected to a page similar to what we have below.
You’ll notice that the “Create Bucket” button isn’t active, and also there is a button that is asking us to “Signup for free”. Click on the “Sign Up Button” and fill all the necessary information. You might also be asked to enter your Credit/Debit Card Details before you can be able to complete the “Sign Up exercise”.
After you are done, the “Create Bucket” button should be active, click on it, you should see a modal similar to what we have below.
If not, just give your bucket a name. Then, keep hitting the continue button until you get to the last select box. Finally, click on the create button.
After, completing the clicking exercise, you should be redirected to a page similar to what we have below.
Click on the create button to finally create a Bucket. A Bucket is basically a place on the cloud where all of your data are being stored. According to the Google Cloud documentation, a Bucket is said to be a “Container that holds your data.” To read more about Buckets visit the link below.
That being done, we can now move to our development environment and start writing some code!
To set up your dev environment for this little project, open your terminal or console and type in the commands below.
if you are using Visual Studio Code as your Code Editor you can type the command below after the initial ones to open up the Editor.
Once you are in your directory and have confirmed, type in the command below to initialize your project.
The command below is often used whenever a new project is to be started in Node.js. This allows us to keep track of all install dependencies when have used in developing the project. The result of running the command is shown below. A json file with some meta-data about our project.
The next thing we need to do is to install all necessary dependencies our app needs for it to function. In your terminal, enter the command below to install a couple of dependencies we need to start our app’s server.
We need express for creating a server that is going to power our application, body-parser for parsing requests whenever it hits our server, and multer for parsing files which must have been part of the request object.
After successfully completing the action, a node_modules directory is being included in your project directory. This folder contains all the dependencies our application needs for it to be able to run.
After you are done, in your root directory create a file and name it index.js. This is where we are going to put the code which is going to create a server for our application. At this rate, your application’s project structure should look similar to the picture below.
In your index.js file, paste the code below into it.
After you’re done with that, run the command below to start your server.
This would create a server that listens for requests on port 9001. To kill the process, click on ctrl + c at the same time.
Starting our app with node directly would not be efficient as when we make changes to the app when we have to manually kill the server, and then restart before it can be able to notice any changes in our application. A solution to this problem is to install nodemon which we can use to automatically start our project anytime we make a change to any file in our project. To install it run the command below in your terminal.
This installs it as a dev-dependency in the project. Now, go to your package.json file and change it’s contents to what is in the snippet below.
You’ll notice that in the script key, a new set of key-value pair has been added to it. Now, go to your terminal and run the command below to start your application server with nodemon.
You should see something similar to what we have below.
Now that we have a server running, we can now start making requests in order to push an image or file into our bucket. But to do so, we need a dependency and also some certain credentials from our bucket to do so.
In your terminal, type in the command to install the google cloud storage dependency which is going to help us with the image upload.
Create a folder in the root of your project and name it assets. This is where we are going to put the files and images we want to push into our bucket. Once you are done with that, put a random picture in it and go back to the GCP console. We need to create a service account key which gcs is going to use to authorize us to put a file in our bucket.
On the GCP console, search for API, click on the api & services option. You should be redirected to a new page. Scroll down and click on the storage link
After clicking on the link, you will be redirected to a new page. At the left section of the page, click on the credentials button, afterward, click on the add credentials button as shown below.
Then, click on the service account option. Fill in the forms and then click on the create button as shown below.
You should be redirected to the create role section. Scroll down or search for storage. Select it and also select API KEYS ADMIN.
Once that’s done, you are once again being redirected to another page. Just click on the create key button as shown below.
Click on create
After it is being created, an automatic download of the account key is being automatically downloaded. You need to keep this file very safe.
In the previous section, we created a service account key and also downloaded it on our machine. So, we need to make use of this file before we can be able to do anything. In your root directory create a folder and name it config. Copy and then paste the JSON file we downloaded into it. Also, in the config directory, create an index.js file and paste the code below into it.
What we have done here is basically a configuration of the @googale-cloud/storage dependency we installed earlier on in the project.
After doing this we can now start with the core functionality of our application, which is image/file upload.
In the root directory create a folder and name it helpers. Then, create a helpers.js file in it. We are going to use this file to create some helper functions that would help us with the image upload. However, replace the index.js file in your root directory with the code snippet below.
You’ll notice that our app has changed. We actually haven’t done anything much, all we have done is just to use the other dependencies as a middleware in our application.
Proceed to the helpers.js file and paste the code snippet into it.
What we have done here is to first import the @google-cloud/storage we initially configured and then we proceeded in linking it to our bucket. Afterward, we created a function that returns a Promise. The function expects an object which contains the blob object. We then extracted the file name and the file (buffer) from the object. We clean it up, then, create a read stream. Once it is done writing the stream, it returns a URL to the image.
Once that is done, go back to your index.js file and replace the existing code with the code snippet below.
Open up POSTMAN, click on form-data, then, type file (it is important that the key is named file since we already configured multer to grab our file in the files key) under the key column, and select a random file on your system then, finally, make a POST request to…
if that’s successful, you should see a link to the image.
However, at this rate, the image would not be publicly available. You need to go back to the gcs console and edit permission of the file.
Go to permissions => Add Members => storage object viewer.
Once that is complete, the file can now be publicly available.
In this relatively short tutorial (lol) we can see that gcs have made file upload easier. With few lines of code, you already have your file uploaded. It also has a very secure system for authorized users who can have access to the files in your bucket.
The application’s source code can be found…
Also, if you’d like to ask me further questions, feel free to hit me up on Twitter.
To learn more about gcs, check out the docs here
You can also try this awesome library for the file upload in case you do not want to go through the stress of writing the code all over again.
Arigato!
Data Science, Tech, and Finance.
902 
15
902 
902 
15
Data Science, Tech, and Finance.
"
https://medium.com/skooldio/%E0%B9%80%E0%B8%81%E0%B9%87%E0%B8%9A%E0%B8%82%E0%B9%89%E0%B8%AD%E0%B8%A1%E0%B8%B9%E0%B8%A5-real-time-%E0%B8%88%E0%B8%B2%E0%B8%81-facebook-page-%E0%B8%94%E0%B9%89%E0%B8%A7%E0%B8%A2-google-cloud-functions-%E0%B9%81%E0%B8%A5%E0%B8%B0-bigquery-66782e289f0f?source=search_post---------84,"There are currently no responses for this story.
Be the first to respond.
เมื่อเดือนสิงหาคมที่ผ่านมา ผมได้รับเกียรติให้ไปเป็น speaker ในงาน TEDxBangkok โดยใน TED talk ของผมนั้น ผมได้ทำ interactive data visualization ขึ้นมาอันหนึ่ง แสดงจำนวนคนที่เข้ามากด Reaction ต่างๆ (👍🏻, ❤️, 😲) ให้กับโพสต์เปิดตัวของผมตลอดระยะเวลาเกือบหนึ่งสัปดาห์หลังจากที่โพสต์ได้ถูกปล่อยออกมา เพื่อสร้างสีสันเล็กๆ น้อยๆ ให้กับท่านผู้ชม
เนื่องจากใช้เวลาในการเขียนโค้ดเพื่อเก็บข้อมูลและทำ data visualization ไปค่อนข้างมาก (มากกว่าเวลาที่ใช้ซ้อมพูด 😂) เลยอยากจะถือโอกาสนี้ เขียน tutorial สั้นๆ สอนการเก็บข้อมูล real-time จาก Facebook Page เผื่อจะเป็นประโยชน์แก่ผู้ที่สนใจอยากจะทำอะไรคล้ายๆ กัน
เราสามารถดึงข้อมูลการกด reaction ต่างๆ บนโพสต์ได้ง่ายๆ โดยการส่ง GET Request ไปที่ /{post-id}/reactions
👉 ตัวอย่างการเรียกใช้ API: แนะนำให้ลองเปลี่ยนจาก reactions เป็น likes หรือ comments ดู หรือใครอยากลองเปลี่ยน parameters ใน query ก็สามารถอ้างอิงได้จากที่นี่
จะเห็นได้ว่าข้อมูลของแต่ละ reaction หรือ like นั้น จะไม่มี created_time ติดมาด้วย (แต่ถ้าเป็น comment จะมี) อย่างงี้ถ้าเราอยากรู้ว่าคนเข้ามากด like กันตอนไหนเยอะ? มากันรัวๆ หรือมากันกะปริดกะปรอย? เราควรจะทำอย่างไรดี
หลายๆ คนอาจจะยังไม่รู้ว่า จริงๆ แล้ว เราสามารถสร้าง Facebook application ขึ้นมา เพื่อ subscribe การแจ้งเตือนแบบ real-time เมื่อข้อมูลของผู้ใช้งานหรือเพจมีการอัพเดตได้ ผ่านทาง webhook (หากคุณมีสิทธิ์ในการเข้าถึงข้อมูลนั้น! เช่นสำหรับเพจ คุณอาจจะต้องได้สิทธิ์ manage_pages จากการเป็น admin ของเพจ เป็นต้น)
และด้วยวิธีนี้เอง เราก็จะสามารถเก็บข้อมูลได้ว่ามีใครมากด reaction อะไรให้เราตอนไหนบ้าง 👏🏻👏🏻👏🏻
คุณสามารถอ่านรายละเอียดการติดตั้ง subscription ได้จากลิงก์ด้านล่างนี้ อย่างไรก็ตาม ตอนนี้คุณจะยังไม่สามารถทำตามขั้นตอนทั้งหมดได้ เพราะเรายังไม่ได้ทำการสร้าง callback endpoint ขึ้นมารอรับการแจ้งเตือน เดี๋ยวเราจะมาทำในส่วนนี้ในหัวข้อถัดไป
developers.facebook.com
หากเราทำการเชื่อมต่อ webhook สำเร็จแล้ว เราก็จะสามารถไป subscribe ข้อมูลแต่ละ field ของเพจ (หรือผู้ใช้งาน) ได้ตามภาพด้านล่าง โดยในตัวอย่างนี้เราต้องการ subscribe ข้อมูล feed ของเพจ เพื่อดักข้อมูลว่ามีใครมาโต้ตอบ หรือกด reaction อะไรบนโพสต์ต่างๆ ของเพจเราบ้าง
หลังจากที่เราสร้าง Facebook application เพื่อ subscribe ข้อมูลจากเพจ(ใดๆ)เรียบร้อยแล้ว ขั้นตอนสุดท้าย ก็คือการติดตั้งแอปให้กับเพจที่เราต้องการจะเก็บข้อมูล โดยการส่ง POST request ไปที่ /{page_id}/subscribed_apps ด้วย access token ของเพจที่ต้องการติดตั้งสำหรับแอปของเรา (ดูรายละเอียดเพิ่มเติมได้ที่นี่) ถ้าหากเราต้องการให้แอปของเราเก็บข้อมูลจากหลายๆ เพจที่เราดูแลอยู่ เราก็สามารถทำซ้ำแบบเดียวกันนี้กับเพจอื่นๆ ได้เช่นกัน
Access token ของเพจเพจหนึ่งสำหรับแต่ละแอปพลิชันนั้นจะไม่เหมือนกัน ใน Graph API Explorer เราสามารถสร้าง access token ที่ถูกต้องได้โดย
สุดท้าย ถ้าเราส่ง GET request ไปที่ /{page_id}/subscribed_apps ก็จะได้ผลดังภาพด้านล่าง
หากเราติดตั้งแอปให้เพจเราได้สำเร็จด้วย POST request เราก็จะได้ response กลับมาดังภาพด้านล่าง
ถ้าเราลองส่ง GET request ไปอีกที เราก็จะเห็นว่าแอปพลิเคชันของเราเข้าไปอยู่ในลิสต์เรียบร้อยแล้ว
เมื่อสักครู่เรายังค้างกันอยู่ที่ callback endpoint สำหรับรอรับการแจ้งเตือน ในหัวข้อถัดไป เราจะมา implement ในส่วนนี้กันบน Google Cloud Platform
Google Cloud Function คือ ฟังก์ชันที่เราเขียนขึ้นมาเพื่อตอบสนองการ trigger จาก event อะไรบางอย่าง เช่น มีคนส่ง HTTP(S) request เข้ามา หรือมีการเพิ่มไฟล์ใน Cloud Storage โดยฟังก์ชันนี้จะถูกนำไปรันด้วย Node.js บน infrastructure ของ Google ทำให้เราไม่ต้องกังวลกับการเปิดเครื่องตั้ง server อันใหม่ (serverless) ปัญหาเรื่องความปลอดภัย (security) หรือการขยายระบบเพื่อรองรับจำนวนผู้ใช้งานที่มากขึ้น (scalability)
ใครที่มาจากฝั่ง Amazon ก็อาจจะคุ้นเคยกับ AWS Lambda ที่มีความสามารถคล้ายๆ กันอยู่แล้ว
เมื่อมีคนมากด reaction ให้กับโพสต์บนเพจที่เรากำลังดักเก็บข้อมูล Facebook ก็จะส่ง HTTPS POST request พร้อมกับ JSON payload ว่าใครมากด กด action อะไร ฯลฯ ไปที่ callback URL ที่เรากำหนดไว้ตอนติดตั้ง webhook
สิ่งที่เราจะทำในขั้นตอนต่อไปก็คือ การเขียน HTTP function หรือ cloud function ที่สามารถ trigger ผ่าน HTTP request ได้นั่นเอง
นี่คือตัวอย่างฟังก์ชันที่มีการจัดการกับ HTTP request มาตรฐาน ซึ่งก็จะมีการตรวจสอบว่า request ที่เข้ามาเป็นประเภทไหน GET หรือ PUT หลังจากนั้นก็จะมีการส่ง response กลับไป พร้อมกับ status code ที่เหมาะสม (200, 403, หรือ 500)
เพื่อให้รองรับการทำงานร่วมกับ webhook ของ Facebook เราจะต้องรับมือกับ HTTP request 2 ประเภทด้วยกัน
เมื่อเราได้รับข้อมูลใหม่ผ่านทาง webhook เราก็จะต้องทำการเก็บข้อมูลลงในฐานข้อมูลเพื่อนำไปวิเคราะห์ในภายหลัง โดยในตัวอย่างนี้ผมเลือกใช้ Google BigQuery
Google BigQuery คือ บริการ data warehouse ของ Google ที่ออกมารองรับการจัดเก็บและประมวลผมข้อมูลปริมาณมากๆ (Big data!) โดยเราสามารถจัดการกับข้อมูลด้วยภาษา SQL ที่ทุกคนน่าจะคุ้นเคยกันดีอยู่แล้ว
จุดเด่นของ Google BigQuery ก็คือ ความเร็วในการประมวลผล มันสามารถประมวลผลข้อมูลขนาด terabytes ภายในไม่กี่วินาที (1 TB = 1,024 GB) หรือขนาด petabytes ภายในไม่กี่นาที (1 PB = 1,024 TB) นอกจากนี้เรายังไม่ต้องกังวลกับการเปิดเครื่องตั้ง database server ขึ้นมาใหม่ (serverless อีกแล้ว!) และปัญหาต่างๆ ในการดูแลระบบ (database administration)
เราสามารถเชื่อมต่อ cloud function ของเราเข้ากับ BigQuery ได้ง่ายๆ ด้วย library @google-cloud/bigquery ลองดูตัวอย่างจากโค้ดด้านล่างในฟังก์ชัน insertRows()
ก่อนที่โค้ดนี้จะใช้งานได้จริง คุณจะต้องทำการสร้าง dataset ไว้ใน BigQuery ให้เรียบร้อยเสียก่อน ผ่าน Web UI หรือ BigQuery command line tools:
เมื่อเรามีฟังก์ชันที่พร้อมใช้งานแล้ว ขั้นตอนสุดท้ายก็คือการ deploy ขึ้น Google Cloud Platform ด้วยคำสั่งหน้าตาประมาณนี้
Note: ผมข้ามรายละเอียดเล็กๆ น้อยๆ ในการ setup project บน Google Cloud Platform หรือการติดตั้ง cmd tool ต่างๆ ไป เนื่องจากผู้อ่านส่วนใหญ่น่าจะพอมั่วๆ เองได้ แต่ถ้าหากใครต้องการแบบละเอียดๆ ลองทำตาม official tutorial เหล่านี้ดูครับ
cloud.google.com
cloud.google.com
หรือถ้าใครไม่ถูกกับภาษาอังกฤษ แบบไทยๆ ก็มีครับ 😆http://www.somkiat.cc/deploy-function-on-google-cloud-platform/
เมื่อเรา deploy cloud function ของเราเสร็จสมบูรณ์แล้ว เราก็จะได้ URL สำหรับไว้ใช้ trigger ฟังก์ชันของเรา ตามภาพด้านล่างนี้
เราก็สามารถนำ URL นี้ ไปกำหนดเป็น Callback URL เพื่อติดตั้ง webhook ของ Facebook application ของเรา
ถ้าคุณทำตามทุกขั้นตอนได้อย่างถูกต้อง ข้อมูล interactions บน feed ของเพจที่คุณดักข้อมูลอยู่ก็จะถูกจัดเก็บลงใน Google BigQuery รอให้คุณนำไปวิเคราะห์ต่อไป!
Master today's most in-demand skills
343 
343 claps
343 
Written by
Co-founder @ Skooldio. Google Developer Expert in Machine Learning. A data nerd. A design geek. A changemaker. — Chula Intania 87, MIT Alum, Ex-Facebooker
แหล่งความรู้ในการ Upskill / Reskill สำหรับคนทำงานในยุคปัจจุบัน พัฒนาและเสริมสร้าง Skill ของคุณ ด้วยคอร์สเรียนออนไลน์ และ เวิร์คชอปด้าน Technology / Design / Data / Business
Written by
Co-founder @ Skooldio. Google Developer Expert in Machine Learning. A data nerd. A design geek. A changemaker. — Chula Intania 87, MIT Alum, Ex-Facebooker
แหล่งความรู้ในการ Upskill / Reskill สำหรับคนทำงานในยุคปัจจุบัน พัฒนาและเสริมสร้าง Skill ของคุณ ด้วยคอร์สเรียนออนไลน์ และ เวิร์คชอปด้าน Technology / Design / Data / Business
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/running-a-mean-stack-on-google-cloud-platform-with-kubernetes-149ca81c2b5d?source=search_post---------85,"There are currently no responses for this story.
Be the first to respond.
In my most recent post, I talked about running a MEAN stack with Docker Containers.
Manually deploying Containers is all fine and dandy, but is rather fragile and clumsy. What happens if the app crashes? How can the app be updated? Rolled back?
This post is going to be a longer deep dive into the world of containers. I am assuming you have read / completed part one. There is a lot of content, but if you just skim through, it should take only around 15 minutes for all the steps!
Thankfully, there is a system we can use to manage our containers in a cluster environment called Kubernetes. Even better, Google has a managed version of Kubernetes called Google Container Engine so you can get up and running in minutes.
At Google, it is rare that someone actually logs into a production machine to perform updates. With millions of servers, you can see how this would become impossible. Manually logging into servers also leaves room for human error.
Instead, we use a system called Borg to do the management for us.
In a nutshell, you tell Borg to run X copies of your job, and Borg will do it. Job dies? Borg will start a new one. Health check failing? Borg will shut that job down and start a new one.
We took the lessons learned from Borg, and made an open source project called Kubernetes. Kubernetes lets you manage a cluster of machines and run containers on top of it. Pretty cool stuff, and it just reached version 1.0!
Before we jump in and start kube’ing it up, it’s important to understand some of the fundamentals of Kubernetes.
Why would you want to have a group of containers instead of just a single container? Let’s say you had a log processor, a web server, and a database. If you couldn't use Pods, you would have to bundle the log processor in the web server and database containers, and each time you updated one you would have to update the other. With Pods, you can just reuse the same log processor for both the web server and database.
In my previous post, I used off-the-shelf containers to keep things simple.
I had a stock MongoDB container and a stock Node.js container. The Mongo container ran fine without any modification. However, I had to manually enter the Node container to pull and run the code. Obviously this isn't ideal in Kubernetes land, as you aren't supposed to log into your servers!
Instead, you have to build a custom container that has the code already inside it and runs automatically.
To do this, you need to use more Docker. Make sure you have the latest version installed for the rest of this tutorial.
Getting the code:
Before starting, let’s get some code to run. You can follow along on your personal machine or a Linux VM in the cloud. I recommend using Linux or a Linux VM; running Docker on Mac and Windows is outside the scope of this tutorial.
This is the same sample app we ran before. The second line just moves everything from the EmployeeDB subfolder up into the app folder so it’s easier to access. The third line, once again, replaces the hardcoded localhost with the mongo proxy.
Building the Docker image:
First, you need a Dockerfile. This is basically the list of instructions Docker uses to build a container image.
Here is the Dockerfile for the web server:
Dockerfiles are pretty self explanatory, and this one is dead simple.
First, it uses the official Node.js image as the base image.
Then, it creates a folder to store the code, cds into that directory, copies the code in, and installs the dependencies with npm.
Finally, it specifies the command Docker should run when the container starts, which is to start the app.
Right now, the directory should look like this:
Let’s build.
This will build a new Docker image for your app. This might take a few minutes as it is downloading and building everything.
Side Note: It’s good practice to put a user id in front of the image name, for example:
But I'm going to ignore this practice in this tutorial.
After that is done, test it out:
At this point, you should have a server running on http://localhost:3000 (If you are on Mac or Windows, this won't be so simple). The website will error out as there is no database running, but we know it works!
Now you have a custom Docker image, you have to actually access it from the cloud.
As we are going to be using the image with Google Container Engine, the best place to push the image is the Google Container Registry. The Container Registry is built on top of Google Cloud Storage, so you get the advantage of scalable storage and very fast access from Container Engine.
First, make sure you have the latest version of the Google Cloud SDK installed.
Windows users click here.
For Linux/Mac:
Then, make sure you log in and update.
Now you can push the container. We need our Google Cloud Project ID (we made one in part one).
After some time, it will finish. You can check the console to see the container has been pushed up.
So now you have the custom container, let’s create a cluster to run it.
Currently, a cluster can be as small as one machine to as big as 100 machines. You can pick any machine type you want, so you can have a cluster of a single f1-micro instance, 100 n1-standard-32 instances (3,200 cores!), and anything in between.
For this tutorial I'm going to use the following:
There are two ways to create this cluster. Take your pick.
After a few minutes, you should see this in the console.
Three things need to be created:
To create the disk, run this:
Pretty simple, just pick the same zone as your cluster and an appropriate disk size for your application.
Now, we need to create a Replication Controller that will run the database. I’m using a Replication Controller and not a Pod, because if a standalone Pod dies, it won't restart automatically.
Pretty straightforward stuff. We call the controller mongo-controller, specify one replica, and open the appropriate ports. The image is mongo, which is the off the shelf MongoDB image.
The volumes section creates the volume for Kubernetes to use. There is a Google Container Engine specific gcePersistentDisk section that maps the disk we made into a Kubernetes volume, and we mount the volume into the /data/db directory (as described in the MongoDB Docker documentation)
Now we have the Controller, let’s create the Service
Again, pretty simple stuff. We “select” the mongo Controller to be served, open up the ports, and call the service mongo.
This is just like the “link” command line option we used with Docker in my previous post. Instead of connecting to localhost, we connect to mongo, and Kubernetes redirects traffic to the mongo service!
At this point, the local directory looks like this
First, let’s “log in” to the cluster
Now create the controller.
And the Service.
kubectl is the Kubernetes command line tool (automatically installed with the Google Cloud SDK). We are just creating the resources specified in the files.
At this point, the database is spinning up! You can check progress with the following command:
Once you see the mongo pod in running status, we are good to go!
Now the database is running, let’s start the web server.
We need two things:
Let’s look at the Replication Controller configuration
Here, we create a controller called web-controller, and we tell it to create two replicas. Replicas of what you ask? You may notice the template section looks just like a Pod configuration, and that's because it is. We are creating a Pod with our custom Node.js container and exposing port 3000.
Now for the Service
Notice two things here:
At this point, the local directory looks like this
Create the Controller.
And the Service.
And check the status.
Once you see the web pods in running status, we are good to go!
At this point, everything is up and running. The architecture looks something like this:
By default, port 80 should be open on the load balancer. In order to find the IP address of our app, run this command:
If you go to the IP address listed, you should see the app up and running!
And the Database works!
By using Container Engine and Kubernetes, we have a very robust, container based MEAN stack running in production.
In my next post, I'll cover how to setup a MongoDB replica set. This is very important for running in production.
Hopefully I can do some more posts about advanced Kubernetes topics such as changing the cluster size and number of Node.js web server replicas, using different environments (dev, staging, prod) on the same cluster, and doing rolling updates.
Google Cloud community articles and blogs
246 
14
Some rights reserved

246 claps
246 
14
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/nooblearning/2019-google-cloud-professional-data-engineer-certification-exam-6a5d6581e507?source=search_post---------86,"There are currently no responses for this story.
Be the first to respond.
That’s pretty intense exams! (•̀ᴗ•́)و I thought I wouldn’t pass but it turns out well so if I can do it you can do it too! Let’s start!
Try to get the voucher by complete the challenge which is just 2 Qwick labs! For me I got $120 voucher which save me a lot from a fear to lose money and get nothing. 😆
Here’s a list that you can’t skip reading, and you should keep revisit them as much as possible.
Many of them are a bit outdated but still worth reading
I did my own brain dump version via draw.io and it help a lot
There’s only Professional Data Engineer 10 in Thailand which is seem to not accurate. At least 2 of my known friends didn’t list there.
The exam just get updated so other blog I read is somehow outdated already I would recommend this one for your preparation.
medium.com
Good luck!
UPDATE 2020 : Data Engineering Learning Path now include Data Fusion and Cloud Composer. More labs on advanced BigQuery, BigQuery ML, and Bigtable streaming
cloud.google.com
It’s good to be noob so we can learn and have fun ;)
305 
3
305 claps
305 
3
Written by
DLT & ML Debugger
It’s good to be noob so we can learn and have fun ;)
Written by
DLT & ML Debugger
It’s good to be noob so we can learn and have fun ;)
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://levelup.gitconnected.com/dockerizing-and-autoscaling-node-js-on-google-cloud-ef8db3b99486?source=search_post---------87,"This post will cover how to dockerize a Node.js server for hosting in Google Cloud (specifically Google Computer Engine) and configuring your VM instances to auto scale proportionally with traffic. Part 2 will go over how to connect your server to a secure MongoDB deployment.
We will set up a Node.js server, create a Dockerfile and deploy a Docker image of our server to the Google Container Registry. After that, we will create an instance template, which defines the specifications of our VMs. Then, we’ll set up a managed instance group, which will boot up and scale a group of VMs based on our instance template. We’ll configure our managed instance group to create new instances once traffic increases past a certain threshold. Finally, we will set up a load balancer to redirect and balance incoming traffic between our VM instances.
Feel free to use an existing Node.js application of yours, although it may be easier to complete this tutorial with a simple application to avoid any unnecessary errors. For the purpose of this tutorial, I’m going to use a Node boilerplate script I’ve written - https://github.com/francescov1/node-boilerplate-script.
Note: if you are using your own Node.js code, ensure it is set up to run on port 3000 as I will be using that port for all Docker and Google Cloud configuration.
If you are using your own Node.js app, skip this step. We’re going to clone the node-boilerplate-script repo, make a new folder and run the script.
After a few moments, it should be complete. Run npm start and navigate to http://localhost:3000/api/examples/hello, where you should see “hello world”.
Next we are going to set up Google Compute Engine. If you don’t have a Google Cloud account, check out https://cloud.google.com/free to learn more and for instructions on creating your account. Upon signup, you are given $300 of free credits, so don’t worry about needing to pay for the resources you create in this tutorial.
Create a new project in the Google Cloud console. If you’ve never done this before, see https://cloud.google.com/resource-manager/docs/creating-managing-projects. We will call our project gcloud-docker-node.
Next head over to https://cloud.google.com/sdk/docs/#install_the_latest_cloud_tools_version_cloudsdk_current_version to download the Google Cloud CLI (if you haven’t already), which we will use to deploy new code changes to our VMs.
Once it is installed, run gcloud init. This will guide you through authenticating the CLI and choosing your project.
We can now setup Docker for our Node.js server before deploying it. Docker allows us to containerize our application in an easily portable and scalable format. I won’t go into detail on Docker or its benefits, so if you are interested check out https://dzone.com/articles/top-10-benefits-of-using-docker.
I’ll be using Atom, but feel free to use any other text editor. Create the Dockerfile and .dockerignore and open the working directory in your text editor.
Add the following code to your .dockerignore:
The above defines files and folders to ignore when building your Docker image. Add the following code to your Dockerfile:
Pm2 is a process manager that we will use to boot up our server. It comes bundled with a ton of cool features such as clustering and load balancing. Learn more here https://pm2.io/runtime. The CMD [ “pm2-runtime"", “start"", “index.js"", “-i"", “max ]line starts as many instances of the Node.js server as possible. Node.js can only run one process per CPU core, so in our case, this will only run one Node.js process per VM instance as our defined instance templates use a single core CPU. If you were to upgrade your VM instances to run more cores, this would ensure that they are all being used.
We also need to remove the line **/.envfrom the .gitignore file as Google Cloud Container Registry will use this file on top of .dockerignore to figure out which files to ignore (this will be especially important for Part 2 where will put our MongoDB connection string in the .env). Make sure that if you end up putting this code into a git repository that you use a tool like git-crypt to ensure any secrets are not committed directly into git.
Now go find the project ID for the Google Cloud project we created earlier. To do this, simply click the project name at the top left of the Google Cloud Console. To the right of your project name will be the ID, in the format gcloud-docker-node-XXXXXX. Note: your project ID may or may not contain random digits after your project name.
To build and deploy our Docker image to the Google Container Registry, run:
replacing <project-id> with your own project id. Don’t forget the . at the end of the command! If it asks you if you would like to enable the Cloud API, say yes.
The process will go through each step that we defined in our Dockerfile. If everything goes well, you should finish with information on the ID, time created, duration, source, images, and SUCCESS status.
Head over to the Google Container Registry -https://console.cloud.google.com/gcr to see your newly deployed image.
We will now add a simple firewall rule to allow all traffic into our VM. Once we have setup our load balancer, we will close off this traffic to ensure it only comes from the load balancer. Head over to https://console.cloud.google.com/networking/firewalls and click Create firewall rule.
Name it vm-firewall-rule. Under Target tags, type vm-firewall-tag and hit tab. We will specify this same tag when setting up our VMs to link them to this rule. Under Source IP ranges, enter 0.0.0.0/0 and hit tab, thus allowing all incoming traffic. Under Protocols and ports, check tcp and enter 3000 in the field beside. Click Create.
Now we are going to setup our instance template. Head over to the Compute Engine — https://console.cloud.google.com/compute and click Instance Templates in the left menu bar, then click Create instance template.
We’ll name our template gcloud-docker-node-template. Check the Deploy a container image to this VM instance option and enter gcr.io/<project-id>/docker-image:latest into the form. This is the path to the Docker image we just uploaded.
Click the Management, security, disks, networking, sole tenancy dropdown and navigate to the Networking tab. In the Network tags form, enter vm-firewall-tag and hit tab. This establishes the link I previously mentioned to our firewall rule.
Leave all other settings the same and click Create.
Navigate to https://console.cloud.google.com/compute/instanceGroups and click Create instance group.
Name it gcloud-docker-node-group. Under Instance template, select gcloud-node-docker-template. Under Target CPU usage, enter 75, and under Maximum number of instances enter 5. This configuration will automatically add or remove VM instances to try and maintain a CPU usage of 75% for each instance. Leave everything else as their defaults and hit Create.
If you head over to the VM instances tab, you should now see at least one VM instance spinning up. Once it finishes booting, we’ll SSH into that instance to ensure its running properly with our Docker image.
Once you see a green checkmark beside the instance name, click the arrow beside the SSH button and click View gcloud command. Copy the command shown and enter it into your terminal.
If SSH succeeds, you should see the following message;
Enter docker ps to view the current active containers. There will be two containers, the first being our Node.js server and the second will have an image name of the form gcr.io/stackdriver-agents/stackdriver-logging-agent:XXXXX.
To view the logs of our container, run docker logs <container-id>, where <container-id> is the ID of our server container (specifying the first few characters will do).
The last line of the logs should read:
If you used my Node boilerplate script, you can see it in action by grabbing the External IP value of your VM from the Compute Engine page (see the image above) and navigating to http://<external-ip>:3000/api/examples/hello in a browser, where you should see “hello world”.
Congrats! You’ve now got your Node.js server dockerized and running on a Google VM!
Now we’ll need to add a load balancer to redirect incoming traffic to the least busy VM. Navigate to https://console.cloud.google.com/net-services/loadbalancing, click Create load balancer and select HTTP(S) Load Balancing. For our load balancer, we will need to configure its frontend, which will receive all incoming traffic, and the backend, which will redirect the incoming traffic to our VMs.
Enter gcloud-docker-node-lb for the name and click Backend configuration. Select Create or select backend services & backend buckets, backend services and finally Create a backend service.
Name it lb-backend. Under New backend, open the Instance group dropdown and select your managed instance group. Enter 3000 under Port numbers and click done (not Create). Click the Health check dropdown and click Create a health check. Name it lb-health-check, set the protocol to HTTP and the port to 3000. Hit Save and continue then hit Create.
Now all we have left to do is set up the load balancer’s frontend. We could setup HTTPS quite easily on the user-facing frontend using Google’s managed SSL certificates, but that would require us to obtain a domain, so for the purpose of this tutorial we will use HTTP. Click Frontend configuration and name it lb-frontend. Click the IP address dropdown, selecting Create IP address. Name it lb-ip and click Reserve. Finally, click Done.
Hit Review and finalize, ensure all the values are correct and click Create.
And that’s it! Under the Instance groups section (still on the load balancer page), after a few minutes, 1/1 should be shown under Healthy, indicating that the health checks are passing.
Once our health checks are passing, grab the value under IP:PORT in the Frontend section of the load balancer page and navigate to http://<ip:port>/api/examples/hello, where you should again see the “hello world” message, although this time you’ve accessed your server through the load balancer!
Now that our load balancer is hooked up, we can close off all incoming traffic to our VM, with the exception of our load balancer and health checks. Navigate back over to the firewall rules page and click on the rule we previously made, vm-firewall-rule. Click Edit, and in the Source IP ranges, replace 0.0.0.0/0 with 130.211.0.0/22 and 35.191.0.0/16 (hit tab after entering in each IP). Click Save. Now if you try and access the server through the VMs external IP address, the browser will just hang. All traffic will need to come in through our load balancer.
So what if you need to make changes to your code? Although we haven’t setup a build server or automation with git, we can write a simple script to update our VM instances with a new deployment. This includes two steps, the first being to build and deploy our Docker image, as previously done. After that, we need to perform a rolling restart on our managed instance group to restart the VMs. Upon booting up, they will pull the newest Docker image in the Container Registry, which will be built with our newly updated code. Note: the rolling restart feature is still in beta, but I have never ran into issues with it.
To perform a rolling restart, we first need to install the gcloud beta CLI tools. Run gcloud components install beta and follow the prompts.
Once that’s done, we can start a rolling restart with the following command:
Note: if you used a different zone then the default, you will need to adjust it above.
This command will start the process of a rolling restart. To check on its status, use:
Rather than needing to memorize and type out both these steps every time we update our code, we’ll write a simple deploy.sh script which will do it automatically.
Create the file using touch deploy.sh and copy both commands into it:
Make sure to replace <project-id> with your own project ID and change the zone if you did not use the default. Although our project is automatically set by our CLI, I am adding the --project flag in case you need to configure deployments for multiple projects at once.
Run chmod +x deploy.sh to enable permissions. Now if you make any changes to your code, simply run:
to deploy the new code.
And that’s it! 🤙 I hope you found the tutorial helpful. I would love to get your feedback, so leave a comment below if you enjoyed it or ran into any issues. Make sure to check out Part 2! If you are interested in doing something similar but with Kubernetes, check out my post on that here.
gitconnected.com
Coding tutorials and news.
632 
4
632 claps
632 
4
Written by

Coding tutorials and news. The developer homepage gitconnected.com && skilled.dev
Written by

Coding tutorials and news. The developer homepage gitconnected.com && skilled.dev
"
https://medium.com/google-developers/firebase-google-cloud-whats-different-with-cloud-firestore-40f1fc3e6d1e?source=search_post---------88,"There are currently no responses for this story.
Be the first to respond.
Previously in this blog series, I talked about the relationship between Firebase and Google Cloud Platform (GCP). In the last post, you learned about some of the specific differences with Cloud Functions. In this post, I’ll do the same with Cloud Firestore (Firebase, GCP). It’s a really good time to talk about Cloud Firestore, since it’s now generally available (Firebase, GCP) with a production quality service level agreement. I’m sure many of you have tried out Cloud Firestore already and are eager to use it in your apps!
Cloud Firestore is a massively scalable, cloud-hosted, NoSQL, realtime database. In order to structure your data, you define collections (similar to tables in SQL) which contain documents (similar to rows). Each document contains fields that contain the actual data. You can reference an individual document using its unique path, or you can query a collection for documents whose fields contain the data you’re looking for.
Cloud Firestore is unlike some of the other Cloud products that have a Firebase SDK for mobile development (such as Cloud Functions and Cloud Storage). Instead, the Cloud Firestore product is a collaboration between the Cloud and Firebase teams, combining the expertise with realtime data, learned from Firebase Realtime Database, and the scalability of Google Cloud Platform.
Cloud Firestore has SDKs for mobile apps and server components (which I will discuss later), but the underlying database product is the same. If you’re a Google Cloud developer using Firestore as part of your backend architecture, or you’re a Firebase developer whose users access it directly from your client app, Cloud Firestore stores data and scales in exactly the same way. However, there are some differences you should be aware of.
Using the Cloud console, you can browse data in the Cloud Firestore database in your project. It looks like this, where the left column lists all the top-level collections. When you select a collection, the middle column lists the collection’s documents by their unique IDs, and finally the right column shows the fields in a selected document.
This is similar to the view of the same data in the Firebase console:
Both consoles have a way to switch between showing data and showing indexes. In the Cloud console, you switch between the two in the navigation panel on the far left, whereas in the Firebase console, you switch between tabs above the content area.
Unique to the Firebase console, you’ll notice additional tabs for Rules and Usage. Usage shows a graph of reads, writes, and deletes for the current quota period:
This is convenient for Firebase developers, who might try to get a sense of what the per-user usage is like for the app that they’re working on.
If you look carefully, you can also see where your usage stands against the free quota per day. Also notice the link at the very bottom, which sends you off to the Cloud console for more detailed billing metrics:
Data, indexes, and billing are all essentially the same in the consoles for both Firebase and Cloud. But the tab labeled “Rules” is special to Firebase. I’ll get to that in a bit. First, let’s talk about the client libraries that Firebase adds to Cloud Firestore.
GCP provides SDKs for working with Cloud Firestore in server-side code. There is support for Python, Node.js, Java, C# (.NET), Go, PHP, and Ruby. These SDKs are expected to run in a trusted environment, where you have control over execution. For web and mobile apps, where you must ship the code to the end user to run directly, Firebase provides additional libraries for iOS, Android, web (and likely, in the future, for games running on Unity and C++).
One important optimization that the client SDKs give you is the ability to cache documents on the mobile device. As long as offline persistence is enabled, all documents previously read are automatically cached on the device. The next time the client queries for a document, if the cached version is up to date with the version on the server, the cached version will be used, saving you money and your users time. It also works while the device is offline. This caching feature is available only with the client SDKs provided by Firebase, but not with the server SDKs.
Another difference between the mobile and server SDKs for Cloud Firestore is the assumption about where the code is executing. Server code typically runs in a low latency environment, especially within other Google Cloud products. In this situation, other optimizations are in place to help database operations (especially transactions, which must round-trip between the caller and the database) complete more quickly.
Firebase also provides the Firebase Admin SDK (for Java, Python, Node.js, and Go), which is often used to implement some of the backend of your mobile app. Like the Cloud SDKs, the Admin SDK is meant for use exclusively on the backend and never for use in code that ships to end users. When working with Cloud Firestore using the Admin SDK, you should know that it effectively repackages, or wraps, the underlying Cloud SDK. The Admin SDK doesn’t offer any new APIs for Firestore — it simply gives you access to the existing Cloud API. As such, if you’re using the Firebase Admin SDK, you will end up consulting Cloud API documentation to understand how things work.
Backend code is typically considered to be “privileged” and normally operates without restrictions, because it’s assumed that you (the developer) have full control over the execution environment. When you initialize one of the Cloud Firestore server SDKs, you must provide a service account that gives permission to the SDK to perform actions in your project. So, if you need code that credits virtual money to your game’s players, your backend code can (and should) do that with no problems because your backend is authorized by a service account with access to your project. However, it’s highly unlikely that you want your end users to be able to modify the document that contains the record of their credits, intentionally or not! When you ship client code that accesses your database, you should assume that the code can’t be trusted, nor can the data it produces. Code running on end user devices can be modified and compromised to do whatever the attacker wants (after all, they may control the end user’s device entirely!). Since you can’t trust the end user and their device, you should implement appropriate security rules to protect your database. And it’s not even just malicious users — it’s also important to protect yourself from your own buggy code (we’re all guilty of this one)!
Cloud Firestore security rules are used to limit read and write access to Cloud Firestore coming from your mobile clients that are using one of the Firebase SDKs. This is unique to Firebase, and there is no equivalent on the Google Cloud side. The rules are enforced on the server, so they’re impossible for a client to bypass them.
By default, all access to Cloud Firestore coming from one of the mobile SDKs, or the Cloud Firestore REST API (when it’s presented with a Firebase Authentication user ID token) is rejected. To allow access to document, security rules typically employ some combination of:
So, you could easily write a rule that only allows access to a document if it has the same ID as the user’s authenticated user ID. And you could check to see if the fields being written have valid values. Note though, that security rules don’t implement a full programming language. You’ll notice that there are no loops, assignments, or procedural statements, but you have the ability to write complex expressions that evaluate to a final true or false value.
Bear in mind, however, when you access Cloud Firestore using one of the server SDKs, it always bypasses the rules. This goes back to the expectation that server code always runs in a trusted environment. If you fully control the code and the environment, then it’s assumed that you aren’t going to do anything harmful (again, excluding any bugs on your end!).
Lastly, if you don’t have end users working directly with Cloud Firestore in a mobile or web app, then you don’t need to be concerned about security rules. Firestore is perfectly usable on the backend without a client app. But if you are shipping an app, I’d strongly recommend integrating Firebase Authentication first, and you should think carefully about structuring your data and rules from there. There can be times when your data structure must follow security restrictions, such as when you want to store both public and private data for a user.
I’m curious to know how you’re making use of Cloud Firestore! Feel free to tweet the team @Firebase and ask questions to us on firebase-talk. And if you aren’t using Firestore yet, perhaps you could try one of the codelabs for Android, iOS, or the web. There’s no charge to get started with the free daily quotas.
Engineering and technology articles for developers, written…
568 
10
568 claps
568 
10
Engineering and technology articles for developers, written and curated by Googlers. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
firebase-consultant.com, Firebase GDE, engineer, developer advocate, Xoogler
Engineering and technology articles for developers, written and curated by Googlers. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/@mkahnucf/i-highly-recommend-establishing-an-enterprise-relationship-with-google-cloud-e97f4ac5d69?source=search_post---------89,"Sign in
There are currently no responses for this story.
Be the first to respond.
Mike Kahn
Jun 30, 2018·1 min read
Punch a Server
I highly recommend establishing an enterprise relationship with Google Cloud. It seems you are running a mission critical application on a consumer account and this issue could have been avoided. Reach out to the support team and let them know you want to discuss enterprise options to ensure you have done everything possible to ensure your account is never impacted like this in the future. Ping me if you have any trouble getting through.
Customer Engineer, Google Cloud. All views and opinions are my own. @mkahn5
See all (10)
759 
9
759 claps
759 
9
Customer Engineer, Google Cloud. All views and opinions are my own. @mkahn5
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/5-principles-you-need-to-know-before-using-google-cloud-dataprep-for-data-preparation-8b639bd844b5?source=search_post---------90,"There are currently no responses for this story.
Be the first to respond.
Google Cloud Dataprep is an intelligent data service on Google Cloud Platform for exploring, cleaning, and preparing structured and unstructured data.
There are 5 principles important to know before your data preparation with Dataprep.
Before you get started cleaning your dataset, it is helpful to create a virtual profile of the source data…
"
https://rominirani.com/google-cloud-functions-tutorial-series-f04b2db739cd?source=search_post---------91,"Update : November 8, 2018
— Added new tutorial on Cloud Scheduler, an Enterprise grade Job Scheduler available in Beta now. The scheduler can be used to trigger your function at specific regular intervals.
Update: October 23–24, 2018 — Updated series for Node.js v6 and Node.js v8 runtimes.  — Added new tutorial on running headless Chrome via Puppeteer in Google Cloud Functions. — Added new tutorial on deploying Cloud Functions from Cloud Source Repositories — Added new tutorial on using Environment variables in Cloud Functions
This is a tutorial series on Google Cloud Functions, an Event-driven Serverless Compute Platform.
This offering lies in the Functions as a Service (FaaS) computing offering.
The series will cover the following topics:
All companion code for the tutorial series is available here. Go ahead and clone the repository or better still, Open it up in Google Cloud Shell via the button provided on the Github project page below. Do look at the README.md file in the root folder, it contains instructions on how to run tutorials in Google Cloud Shell itself.
github.com
As Google Cloud Functions continues to add more event providers and language runtimes, I hope to keep this tutorial tutorial series updated to reflect the changes and the new features.
If you have any specific feedback on the existing content and/or would like me to possibly explore covering a topic around Google Cloud Functions, do let me know in the comments or email me.
Next Part : Overview of Computing Options
Technical Tutorials, APIs, Cloud, Books and more.
640 
4
640 claps
640 
4
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
Technical Tutorials, APIs, Cloud, Books and more.
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
Technical Tutorials, APIs, Cloud, Books and more.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/free-code-camp/how-to-perform-crud-operations-using-blazor-and-google-cloud-firestore-52890b06e2f8?source=search_post---------92,"There are currently no responses for this story.
Be the first to respond.
In this article, we will create a Blazor application using Google Firstore as database provider. We will create a Single Page Application (SPA) and perform CRUD operations on it. We will use Bootstrap 4 to display a modal popup for handling user inputs. The form also has a dropdown list, which will bind to a collection in our database. We will also implement a client-side search functionality to search the employee list by employee name.
Take a look at the final application.
Get the source code from GitHub.
The first step is to create a project in google Firebase console. Navigate to https://console.firebase.google.com and sign-in with your google account. Click on Add Project link. A pop up window will open as shown in the image below. Provide your project name and click on Create project button at the bottom.
Note the project id here. Firebase project ids are globally unique. You can edit your project id while creating a new project. Once the project is created you cannot change your project id. We will use this project id in next section while initializing our application.
Click on the project you just created. A project overview page will open. Select “Database” from left menu. Then click on “Create database” button. A popup window will open asking you to select the “Security rules for Cloud Firestore”. Select “Start in locked mode” and click on enable.
Refer to the image below:
This will enable the database for your project. Firebase project have two options for database — Realtime Database and Cloud Firestore. For this application, we will use “Cloud Firestore” database. Click on “Database” dropdown at the top of the page and select “Cloud Firestore”.
Refer to the image below:
We will create cities collection to store the city name for employees. We will also bind this collection to a dropdown list in our web application from which the user will select the desired city. Click on “Add collection”. Set the collection ID as “cities”. Click on “Next”. Refer to the image below:
Put the Field value as “CityName”, Select string from the Type dropdown and fill the value with city name as “Mumbai”. Click on Save. Refer to the image below:
This will create the “cities” collection and insert the first document in it. Similarly, create four more documents inside this collection and put the “CityName” value as Chennai, New Delhi, Bengaluru and Hyderabad.
We will use “employees” collection to store employee data, but we will not create it manually. We will create “employees” collection while adding the first employee data from the application.
To access database from our project, we need to set the GOOGLE_APPLICATION_CREDENTIALS environment variable to point to a JSON service account key file. This will set an authentication pipeline from our application to cloud Firestore.
To generate the service account key file follow the steps mentioned below:
Step 1: Navigate to https://console.cloud.google.com/iam-admin/. Login with the same google account, you have used to create Firestore DB.
Step 2: Select Project from the drop down in menu bar at the top.
Step 3: Select “Service accounts” from the left menu. Select the service account for which you want to create the key. Click on more button in the “Actions” column in that row, and then click Create key.
Refer to the image below:
Step 4: A popup modal will open asking you to select the key type. Select “JSON” and click on create button. This will create private key for accessing your Firestore account and downloads a JSON key file to your machine. We will use this file to set GOOGLE_APPLICATION_CREDENTIALS environment variable in later part of this article.
Open Visual Studio and select File >> New >> Project. After selecting the project, a “New Project” dialog will open. Select .NET Core inside Visual C# menu from the left panel. Then, select “ASP.NET Core Web Application” from available project types. Put the name of the project as BlazorWithFirestore and press OK.
After clicking on OK, a new dialog will open asking you to select the project template. You can observe two drop-down menus at the top left of the template window. Select “.NET Core” and “ASP.NET Core 2.1” from these dropdowns. Then, select “Blazor (ASP .NET Core hosted)” template and press OK.
Now, our Blazor solution will be created. You can observe that we have three project files created in this solution.
We need to add the package reference for Google cloud Firestore, which will allow us to access our DB from the Blazor application. Right click on BlazorWithFirestore.Shared project.
Select “Edit BlazorWithFirestore.Shared.csproj”. It will open the BlazorWithFirestore.Shared.csproj file. Add the following lines inside it.
Similarly add these lines to BlazorWithFirestore.Server.csproj file also.
We will create our model class in BlazorWithFirestore.Shared project. Right click on BlazorWithFirestore.Shared and select Add >> New Folder. Name the folder as Models. Again, right click on Models folder and select Add >> Class to add a new class file. Put the name of you class as Employee.cs and click Add.
Open the Employee.cs class and put the following code into it.
We have decorated the class with [FirestoreData] attribute. This will allow us to map this class object to Firestore collection. Only those class properties, which are marked with [FirestoreProperty] attribute, are considered when we are saving the document to our collection. We do not need to save EmployeeId to our database as it is generated automatically.
While fetching the data, we will bind the auto generated document id to the EmployeeId property. Similarly, we will use date property to bind the created date of collection while fetching the record. We will use this date property to sort the list of employees by created date. Hence, we have not applied [FirestoreProperty] attribute to these two properties.
Similarly, create a class file Cities.cs and put the following code into it.
Right-click on BlazorWithFirestore.Server project and then select Add >> New Folder and name the folder as DataAccess. We will be adding our class to handle database related operations inside this folder only. Right click on DataAccess folder and select Add >> Class. Name your class EmployeeDataAccessLayer.cs.
Put the following code inside this class.
In the constructor of this class we are setting the GOOGLE_APPLICATION_CREDENTIALS environment variable. You need to set the value of filepath variable to the path where the JSON service account key file is located in your machine. Remember we downloaded this file in the previous section. The projectId variable should be set to the project id of your Firebase project.
We have also defined the methods for performing CRUD operations. The GetAllEmployees method will fetch the list of all employee document from our “employees” collection. It will return the employee list sorted by document creation date.
The AddEmployee method will add a new employee document to our “employees” collection. If the collection does not exist, it will create the collection first then insert a new document in it.
The UpdateEmployee method will update the field values of an already existing employee document, based on the employee id passed to it. We are binding the document id to employeeId property, hence we can easily manipulate the documents.
The GetEmployeeData method will fetch a single employee document from our “employees” collection based on the employee id.
DeleteEmployee method will delete the document for a particular employee from the “employees” collection.
GetCityData method will return the list of cities from “cities” collection.
Right-click on BlazorWithFirestore.Server/Controllers folder and select Add >> New Item. An “Add New Item” dialog box will open. Select Web from the left panel, then select “API Controller Class” from templates panel and put the name as EmployeeController.cs. Click Add.
This will create our API EmployeeController class. We will call the methods of EmployeeDataAccessLayer class to fetch data and pass on the data to the client side.
Open EmployeeController.cs file and put the following code into it.
We will create the component in the BlazorWithFirestore.Client/Pages folder. The application template provides the Counter and Fetch Data files by default in this folder. Before adding our own component file, we will delete these two default files to make our solution cleaner. Right-click on BlazorWithFirestore.Client/Pages folder and then select Add >> New Item. An “Add New Item” dialog box will open, select “ASP.NET Core” from the left panel, then select “Razor Page” from templates panel and name it EmployeeData.cshtml. Click Add. Refer to the image below:
This will add an EmployeeData.cshtml page to our BlazorSPA.Client/Pages folder. This razor page will have two files – EmployeeData.cshtml and EmployeeData.cshtml.cs.
We will be using a bootstrap modal dialog in our application. We will also include a few Font Awesome icons for styling in the application. To be able to use these two libraries, we need to add the CDN references to allow the JS interop.
Here, we have included the CDN references, which will allow us to use the bootstrap modal dialog and Font Awesome icons in our applications. Now, we will add codes to our view files.
Open EmployeeData.cshtml.cs and put the following code into it.
Here, we have defined the EmployeeDataModel class, which is inheriting from BlazorComponent. This allows the EmployeeDataModel class to act as a Blazor component.
We are also injecting the HttpClient service to enable the web API calls to our EmployeeController API.
We will use the two variables — empList and cityList — to hold the data of our Employee and Cities collections respectively. The modalTitle property, which is of type string, is used to hold the title that will be displayed in the modal dialog. The value provided in the search box is stored in the searchString property which is also of type string.
The GetCityList method will make a call to our web API GetCities method to fetch the list of city data from the cities collection. The GetEmployeeList method will send a GET request to our web API to fetch the list of Employee Data from the Employee table.
We are invoking these two methods inside the OnInitAsync method, to ensure that the Employee Data and the cities data will be available as the page loads.
The AddEmployee method will initialize an empty instance of the Employee object and set the modalTitle property, which will display the title message on the Add modal popup.
The EditEmployee method will accept the employee ID as the parameter. It will send a GET request to our web API to fetch the record of the employee corresponding to the employee ID supplied to it.
We will use the SaveEmployee method to save the record of the employee for both the Add request and Edit request. To differentiate between the Add and the Edit requests, we will use the EmployeeId property of the Employee object. If an Edit request is made, then the EmployeeId property contains a string value, and we will send a PUT request to our web API, which will update the record of the employee. Otherwise, if we make an Add request, then the EmployeeId property is not initialized, and hence it will be null. In this case, we need to send a POST request to our web API, which will create a new employee record.
The DeleteConfirm method will accept the employee ID as the parameter. It will fetch the Employee Data corresponding to the employee ID supplied to it.
The DeleteEmployee method will send a delete request to our API and pass the employee ID as the parameter. It will then call the GetEmployeeList method to refresh the view with the updated list of Employee Data.
The SearchEmployee method is used to implement the search by the employee name functionality. We will return all the records of the employee, which will match the search criteria either fully or partially. To make the search more effective, we will ignore the text case of the search string. This means the search result will be same whether the search text is in uppercase or in lowercase.
Open EmployeeData.cshtml page and put the following code into it.
The route for our component is defined at the top as “/employeerecords”. To use the methods defined in the EmployeeDataModel class, we will inherit it using the @inherits directive.
We have defined an Add Employee button. Upon clicking, this button will invoke the AddEmployee method and open a modal dialog, which allows the user to fill out the new Employee Data in a form.
We have also defined our search box and a corresponding search button. The search box will bind the value to searchString property. On clicking the search button, SearchEmployee method will be invoked, which will return the filtered list of data as per the search text. If the empList property is not null, we will bind the Employee Data to a table to display it on the web page. Each employee record has the following two action buttons corresponding to it:
We have defined a form inside the bootstrap modal to accept user inputs for the employee records. The input fields of this form will bind to the properties of the employee class. The City field is a drop-down list, which will bind to the cities collection of the database with the help of the cityList variable. When we click on the save button, the SaveEmployee method will be invoked and the modal dialog will be closed.
When user click on the Delete button corresponding to an employee record, another bootstrap modal dialog will be displayed. This modal will show the Employee Data in a table and ask the user to confirm the deletion. Clicking on the Delete button inside this modal dialog will invoke the DeleteEmployee method and close the modal. Clicking on the Cancel button will close the modal without performing any action on the data.
Before executing the application, we will add the navigation link to our component in the navigation menu.
Open the BlazorWithFirestore.Client/Shared/NavMenu.cshtml page and add the following navigation link:
Hence, we have successfully created a Single Page Application (SPA) using Blazor with the help of cloud Firestore as database provider.
Press F5 to launch the application.
A web page will open as shown in the image below. The navigation menu on the left is showing navigation link for Employee data page.
You can perform the CRUD operations on this application as shown in the GIF image at the start of this article.
We have created a Single Page Application (SPA) using Blazor with the help of Google cloud Firestore as database provider. We have created a sample employee record management system and performed CRUD operations on it. Firestore is a NoSQL database, which allows us to store data in form of collections and documents. We have also used a bootstrap modal popup to handle user inputs. We have also implemented a search box to search the employee list by employee name.
Please get the source code from GitHub and play around to get a better understanding.
Get my book Blazor Quick Start Guide to learn more about Blazor.
Preparing for interviews? Read my article on C# Coding Questions For Technical Interviews
Originally published at https://ankitsharmablogs.com/
We’ve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
304 
1
304 claps
304 
1
Written by
SDE III @cisco | GDE for Angular | Microsoft MVP | Author | Speaker | Passionate Programmer https://ankitsharmablogs.com/
We’ve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
Written by
SDE III @cisco | GDE for Angular | Microsoft MVP | Author | Speaker | Passionate Programmer https://ankitsharmablogs.com/
We’ve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/going-serverless-on-google-cloud-platform-b770803c5ee7?source=search_post---------93,"There are currently no responses for this story.
Be the first to respond.
Calling something serverless is so hot right now. Every developer tools company found out and suddenly everything is #serverless.
In fact, in the Google Cloud Next 17 Keynotes, so many things were called serverless. These products were amazing in their own right, but the word serverless was thrown around so loosely that it was cringeworthy at times.
As expected, people are upset about this.
So what the hell is serverless? Again, everyone has their own opinions on what serverless is.
So let me define my definition.
I’m going to break down “cloud” products into four different categories. The names are basically made up, but this is what makes sense to me.
SaaS products are ready to use out of the box by end users. There is nothing to provision. You either pay for what you use, or purchase bulk credits of some sort.
Serverless products are used by developers to build higher order products. There is nothing to provision, but there may be some knobs to tune the infrastructure. You pay for exactly what you use, nothing more.
Managed products are used by developers to build higher order products. Developers need to provision capacity and tune things, but don’t need to maintain the infrastructure. You pay for excess infrastructure when it is not being used.
IaaS products mirror on-prem solutions, except you can provision resources in seconds/minutes instead of days/months. They require explicit provisioning and maintenance. You pay for excess infrastructure when it is not being used.
Do you agree with this categories? Am I TOTALLY WRONG IN EVERY WAY!!?Let me know in the comments!
I am going to map most of Google Cloud’s products to one of these four categories. Of course, arguments can be made to fit products in multiple categories, but I’m going to make the tough calls and put them in only one. I think this will make it very clear how I think about these categories.
G SuiteThe obvious entry into this category. G Suite is ready to be used by the end user, no need for developers to do anything. Of course, Apps Script can be considered serverless, but as it depends so much on the rest of G Suite, it feels right in this category.
Data StudioData studio lets you build dashboards for your Google Analytics, Cloud SQL, BigQuery, etc data. There is a lot of drag and drop involved, and end users can be productive without developer support.
Cloud FunctionsThe obvious entry into this category. AWS Lambda started the FaaS revolution, and Cloud Functions follows a similar pattern. You write your functions, specify when they trigger, tune how much memory/cpu is allocated per call, and deploy. Everything else is handled for you. You pay per function invocation, so it is exactly matched to your traffic.
Cloud RunGoogle’s serverless container offering, Cloud Run is similar to Cloud Functions or App Engine, except you deploy a container instead of code. Everything else is handled for you. Cloud Run charges per incoming request, so you pay for exactly what you use.
You can also use Cloud Run on GKE, and in that case it becomes more managed than serverless. See the GKE section for more details.
Cloud StorageCloud Storage is similar to Google Drive, but isn’t usable by end users. It requires a developer to build higher order systems on top. You don’t provision anything, and pay for exactly what you use. You can tune the latency and price by choosing the bucket type (Nearline, Multi-Regional, etc).
App Engine StandardThe first Google Cloud product. Write your code, tune how much memory/cpu you want, and deploy. Everything else is handled for you. App Engine Standard scales to exactly match your traffic and scales to zero when you have none, so you pay for exactly what you use.
Cloud DatastoreThe first NoSQL database at Google Cloud. You don’t provision anything, just read and write data. You pay per read/write and for the exact storage you use, and it scales automatically.
Firebase Realtime Database / HostingSimilar to Cloud Datastore, but used directly on the frontend. You don’t provision anything, just read and write data. You pay for data transfer and for the exact storage you use, and it scales automatically.
Cloud FirestoreThis next generation database brings the best from Datastore and Firebase Realtime Database together. You can use it directly from the frontend like the Realtime Database and from the server like Datastore, and get advanced query support and strong consistency. You don’t provision anything, just read and write data. You pay per read/write and for the exact storage you use, and it scales automatically.
BigQueryBigQuery is Google’s data warehouse service, letting users query TB of data with SQL. There is nothing to manage, just dump/stream your data in and query. You pay to store data and query data, no need to provision machines or storage or set up indexes.
Pub/SubPub/Sub is Google’s many-to-many async messaging bus (think Apache Kafka or RabbitMQ). No need to provision anything, and it scales to millions of messages a second instantly. Pub/Sub charges per message, so you pay for exactly what you use.
Stackdriver (Logs, Monitoring, Debug, Tracing)The Stackdriver tools let you access powerful tools without needs to set anything up. They are all have a free tier, but you can pay to monitor more resources and other clouds like AWS. There is nothing to provision or tune and you pay per resource monitored so there is no extra spending.
Cloud DataflowCloud Dataflow uses Apache Beam to create fully managed ETL, batch processing, and streaming analytics pipelines. It autoscales to process your pipeline, and scales back to zero when there is no more work left.
Kubernetes EngineGoogle Kubernetes Engine creates a Kubernetes Cluster for you in one click. Google manages the Master and Nodes and auto-updates them for you. You have to provision a cluster ahead of time(though there are some auto-scaling capabilities) so you are paying for unused resources.
App Engine FlexibleApp Engine Flexible is similar to Standard, but runs on VMs instead of a sandbox. While this gives it more “flexibility” over what it can run, you lose the “serverless” magic of standard. Auto-scaling and deploying is not as fast, but the biggest difference is you have to always have one VM instance running, meaning you are paying for unused resources.
Cloud SQLCloud SQL gives you managed MySQL and PostgreSQL instances. Google worries about backups, failover replication, etc, which reduces the operational overhead of running a database. While there are some auto-scaling capabilities, you need to provision a machine and disk ahead of time, which means paying for unused resources.
Cloud BigtableCloud Bigtable is Google’s HBase compatible NoSQL database. While storage scales automatically (you pick SSD or HDD), you need to choose how many nodes you want to tune performance. More nodes = more performance, but of course you can over/under provision.
Cloud SpannerCloud Spanner is similar to BigTable except it is globally consistent and relational instead of NoSQL. This is pretty magical! You need to choose how many nodes you want to tune performance. More nodes = more performance, but of course you can over/under provision.
Cloud Load Balancing (L4/L7)Both the L4 and L7 load balancers offered by Google Cloud are fully managed services that require no provisioning, pre-warming, or tuning. The only reason this is not in the Serverless category is you have to pay a flat rate to launch the load balancer regardless of how much traffic it handles.
Cloud DataprocCloud Dataproc launches a managed Spark/Hadoop cluster. You need to specify how many VMs you want and tune them, but after that you can use the cluster without any additional setup. You pay for the VMs even when they are not being used.
Cloud Machine Learning EngineCloud Machine Learning Engine creates and manages a distributed TensorFlow cluster for you to train and serve your models on. You have to provision a cluster, but the cluster is fully managed and you can just submit jobs to it.
Compute EngineThese are Linux and Windows VMs. While you can autoscale them using Managed Instance Groups, and you can do cool things like have the disks automatically grow, at the end of the day you are 100% responsible.
Cloud LauncherCloud Launcher lets you deploy pre-configured apps on Compute Engine. Though the initial setup is automatic, you are responsible for maintaining the servers and pay for unused resources.
What do you think? Does this make sense? Is this whole debate worth it? Is there something I missed? Please let me know!
Google Cloud community articles and blogs
206 
8
206 claps
206 
8
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/theta-network/theta-labs-announces-google-cloud-as-enterprise-validator-and-launch-partner-for-theta-mainnet-2-0-8f765096f2a9?source=search_post---------94,"There are currently no responses for this story.
Be the first to respond.
San Jose, CA, May 27, 2020 — Theta Labs, Inc., a leading video delivery network powered by an innovative new blockchain and distributed ledger technology, announced today that Google Cloud has joined its Enterprise Validator Program along with Binance, Blockchain Ventures, and gumi. Theta’s Enterprise Validator Node program allows enterprises to validate transactions in accordance with Theta’s underlying consensus protocol. Google Cloud is also now the company’s preferred cloud provider, and users around the world will have the ability to deploy and run Theta nodes directly from Google Cloud Marketplace (GCP Marketplace) with just a few clicks. The turn-key cloud solution went live on GCP Marketplace today, coinciding with the launch of Theta Mainnet 2.0.
Theta Network is quickly advancing towards full decentralization of its security and governance. Google has joined the Theta enterprise validator program by running a validator node, and is providing the stability, reliability and security offered by the Google Cloud Platform to the Theta Network. Theta and its partners in the media & entertainment, telecom, technology and gaming industries thus provide a high-performance decentralized micropayment network that scales to millions of concurrent video viewers.
“Distributed ledger technology enables new business models that potentially transform the global digital economy, including the media & entertainment industry,” said Allen Day, Developer Advocate for Google Cloud. “We’re impressed by Theta’s achievements in blockchain video and data delivery. We look forward to participating as an enterprise validator, and to providing Google Cloud infrastructure in support of Theta’s long-term mission and future growth.”
Theta Mainnet 2.0 and the Theta Guardian network, a unique decentralized layer of security and consensus run by community members, is now available for users to access via Google Cloud Marketplace. As part of the partnership, Google Cloud will also power the cloud infrastructure for THETA.tv video platform, a fast-growing first-party esports streaming site built on the Theta Network.
“We’re thrilled to deepen our partnership with Google Cloud across a number of key strategic areas to accelerate Theta adoption across industries,” said Mitch Liu, co-founder and CEO of Theta Labs. “We welcome Google Cloud as an enterprise validator along with our other global partners to further strengthen the security and decentralization of our protocol. As we continue to build our network and streaming business, Google Cloud is the perfect partner to help us scale globally, with extensive geographical coverage offering ease-of-use, networking advantages and platform performance.”
Next generation video delivery powered by you
1.6K 
3
1.6K claps
1.6K 
3
Written by
Creators of the Theta Network and THETA.tv — see www.ThetaLabs.org for more info!
Next generation video delivery powered by you
Written by
Creators of the Theta Network and THETA.tv — see www.ThetaLabs.org for more info!
Next generation video delivery powered by you
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://rominirani.com/tutorial-flutter-app-powered-by-google-cloud-functions-3eab0df5f957?source=search_post---------95,"The Beta release of Flutter from Google has caught the attention of several developers and that includes me. I am always interested in trying out developer tools and understanding how companies are pushing the envelope in terms of productivity.
This blog post is by no means a detailed tutorial on Flutter and for that matter Google Cloud Functions, but aims to demonstrate how straightforward it was for me to put together the two.
We all love inspirational quotes .. don’t we? This is a simple “Famous Quotes” app that has just single screen and shows a random famous quote. We shall use Flutter to create our mobile application and we will deliver a random quote to the Flutter mobile application via Google Cloud Functions, which is the Serverless FaaS (Functions as a Service) available on GCP (Google Cloud Platform).
Here is a screenshot of the Famous Quotes application in action:
The Quotes are completely managed by a Google Cloud Function that we shall deploy. For the sake of keeping things simple, we will simply have about 3–4 quotes available as sample data in the function itself and it will return us a random quote.
So, when we click the Get a Random Quote button that you see in the screen, it will directly invoke the Google Cloud Function that we have deployed and then render the data.
Sounds good? Let’s get going.
Here are the prerequisites before you get going with the rest of the tutorial:
Click on that and then Enable the API.
Now, that we have the prerequisites in place, let us do this in 2 steps:
I am going to demonstrate developing our function completely in the cloud via the Google Cloud console. The only reason I am doing this is to keep things simple and our function is very compact and there is nothing much to it.
In a real world scenario, I expect that you do your development locally and either deploy it from a repository or from your local machine via the gCloud SDK.
Assuming that you are logged into to Google Cloud Console and selected your specific Google Cloud Platform project, navigate to Cloud Functions.
Click on CREATE FUNCTION as shown below:
This will bring up a form where you can provide the details for your function as follows:
7. Function to execute : In this field, the value will be getRandomQuote as you can see from the exports field in the above code.
8. Go ahead and click on the Create function. This will create the GCF function and you should see it deployed and ready in the list as shown below:
9. Let us test out our function by clicking on the function above. It will lead you to a details page for the function, where you can not just track the metrics, source code but also test out your function. You should see a screen similar to the one shown below:
Simply click on the Test the function button and you should get the output.
Please make a note of the FUNCTION_URL as mentioned earlier. You will need this in your Flutter App. You can also test out the FUNCTION_URL directly in your browser and you should get a similar JSON output for a particular quote.
Great! Now that we are done with developing our GCF, it is time to take a look at the Flutter App.
Since I have just started with Flutter, I heavily relied on the documentation to help me get started. Here are a couple of specific links that helped me understand the basics of a simple Flutter App.
I strongly recommend that you go through the above documentation links. They are worth your time to understand what is going on.
Assuming that you have setup the Flutter plugin in Android Studio, you can create a new Flutter project. You will see some sample code present in the lib/main.dart file. Simply replace it with the source code given below:
Famous Quotes Flutter App source code:
Here are some key points from the above source code:
If you have followed the previous 2 links that I provided to the documentation, this code is heavily borrowed from there, so I suggest that you study that.
You could run the application directly from Android Studio or from the Terminal, go to the root folder of the application and give flutter run command.
If all goes well, you will see the Famous Quotes App working for you, all powered by Flutter and Google Cloud Functions.
Hope you enjoyed the tutorial.
Technical Tutorials, APIs, Cloud, Books and more.
427 
4
427 claps
427 
4
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
Technical Tutorials, APIs, Cloud, Books and more.
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
Technical Tutorials, APIs, Cloud, Books and more.
"
https://medium.com/daitan-tech/what-we-learned-by-serving-machine-learning-models-at-scale-using-google-cloud-ml-faea5010d29a?source=search_post---------96,"There are currently no responses for this story.
Be the first to respond.
By Bruno Schionato, Diego Domingos, Fernando Moraes, Gustavo Rozato, Isac Souza, Marciano Nardi, Thalles Silva — Daitan Group
Following our series of articles about cloud infrastructures for solving the Machine Learning (ML) pipeline problem, this time we gave Google Cloud ML a try. We’ll also provide a comparison between…
"
https://medium.com/@davidmytton/aws-vs-google-cloud-flexibility-vs-operational-simplicity-dca4324b03d4?source=search_post---------97,"Sign in
There are currently no responses for this story.
Be the first to respond.
David Mytton
Sep 26, 2015·5 min read
I wanted to revisit a theory I first wrote about in my 15 April 2015 e-mail newsletter — how the approach of Amazon Web Services is different from Google Cloud Platform — and add that to my theory on how containers are core to Google’s Cloud Platform strategy.
On the surface, AWS and GCP are very similar, but their approach to product design is actually quite different:
"
https://medium.com/google-cloud/building-a-personal-genome-data-warehouse-with-google-cloud-23andme-and-family-tree-dna-e9869d8dc7a0?source=search_post---------98,"There are currently no responses for this story.
Be the first to respond.
This is a followup to my post from May 2017 Interpreting 23andMe Raw Genome data with Google Genomics and BigQuery.
TLDR: In this post I will use Cloud Dataprep to clean up my Family Tree DNA and 23andMe raw data, import that raw data into BigQuery for extended genome SNP identification and Cloud Datalab for working with my genomic data. Ideally I will gain a better understanding of my genotyping raw data and I will have a stronger dataset for learning more about my DNA in the future.
Our genomic data can be used to predict disease risk, side effects to pharmaceuticals and tell us more about our who we are and what our future may hold for us. After using 23andMe to explore my raw data with Google BigQuery last year I wanted more data and to be able to do more with it. I imagined a data warehouse of my DNA with multiple raw data sources. This way in the future when we know more about DNA markers (SNPs) I can easily check my genetic data taken from multiple services to verify any suspicions, risks, or concerns I may have about my health.
In my previous post interpreting 23andMe raw Genome data with Google Genomics and BigQuery I took the 23andMe raw data txt file, converted to vcf, and used the Google Genomics load variants pipeline to load into BigQuery for analysis. In this post I will prepare 2 different raw data genomes (23andMe and FTDNA) and copy directly to BigQuery for analysis.
There is a service called Promethease that will take your raw genome data and match it with SNPs found on SNPedia for $12. If you do not want to pay the $12 and the idea of having a personal data warehouse of your genome data appeals to you, you can set one up with Google Cloud quite easily.
If was going to build a personal genome data warehouse I would need more data on my DNA. This time I tried Family Tree DNA’s Family Finder service. After having a second set of raw data I would be able to cross reference any data I found in my 23andMe raw data with another data source, hopefully giving me increased confidence in anything that I found. In the future I will add Ancestry raw data, covering all 3 consumer priced genetic testing services. Ideally this data would allow me to possibly better anticipate or plan for any health issues that may occur in my future. Would this second source give me a more accurate dataset? I think so. Both sources use different sequencing:
23andMe — Uses a customized Illumina Omniexpress+ array that includes about 600,000 SNPs.
Family Tree DNA Family finder — Uses Illumina OmniExpress microarray chip. It includes 711,424 total SNPs. Only about 13,913 of these have annotations in SNPedia.
More on SNP coverage analysis/comparisons on /r/23andme
A nice write up and charge to the many options for DNA analysis Neo.life Your Guide to Getting Sequenced
FamilyTreeDNa family finder raw data download
Family Tree DNA gives you your raw data in a few forms. It can be a bit confusing.
Every cell in the human body has DNA that is tightly packed into structures called chromosomes. There are 23 pairs of chromosomes of which 22 pairs are called autosomes and the 23rd pair is called allosome or sex chromosomes. The X chromosome spans about 155 million DNA base pairs and represents approximately 5 percent of total DNA in cells (source).
The concatenated file from FamilyTree DNA combines the autosomal and X raw data file so if you are looking to gain insight into your raw data you’ll want to use the concatenated file.
A build is a reference system used to describe a kind of typical human genome fully sequenced. Build 37 is supposed to be more accurate in terms of where SNPs actually are located than build 36 (source). A build is a Genome assembly, as more is learned about the human genome, new Genome assemblies are released.
For comparing FTDNA with another raw data source like 23andMe, Build 37 Raw Data Concatenated is the one you want to use.
More on reference genome and build 37 here.
Family Tree DNA gives you a gzip with a CSV so it’s easy to load into BigQuery for analysis. My Family Tree DNA raw data was about 6.5MB compressed and about 22MB uncompressed (CSV). My 23andMe raw data was 5MB compressed and about 15MB uncompressed (txt).
Download your raw data from 23andMe and Family Tree DNA here:
Once you have the CSV of your raw data you could really import right into BigQuery. I’ll start with importing the Family Tree DNA raw data.
You can do this either via the BQ UI or via the bq CLI tool
Result: Unable to import the FTDNA CSV dataset with the position field as an integer. So this led me to start thinking that something was wrong with my data, the POSITION rows should be all numbers.
I went ahead and imported with all fields as STRING data type to move forward and see whats going on here.
Success, but ultimately I do not want all columns to be STRING data type. So let’s explore the data a bit in BigQuery.
I found some issues with the CSV that was provided from Family Tree DNA making it not fully cleansed and in need of some changes.
2. Last 2 rows in the dataset had the column names
To clean this dataset and standardize fields and columns across my 2 different genome datasets I’ll use Cloud Dataprep.
Since some duplicate rows needed to be removed as well as some other changes in this dataset I thought this would be a great time to use Google Cloud Dataprep. Since I am using two different raw data sources I should make sure they have the same columns and column types and formatting so querying both will be easier. I can easily take care of this work in Dataprep.
The first step in in Dataprep is creating a flow and adding a dataset. This is pretty simple, give it a name, then either upload a dataset or import one in GCS or an existing table in BigQuery. I will add datasets for both my FTDNA and 23andMe raw data in the same flow.
And now my second dataset (23andMe)
After the flow is created and datasets are imported add a new recipe to one of your datasets.
Now you can edit your recipe
Great. It will take a moment to load your dataset. Now that your dataset is imported and your recipe is started, you can explore your dataset a bit and add steps or transformations to the recipe.
For this first dataset, I need to make sure the position columns are INT data type. I can also explore my dataset by mousing over the categories displayed in Dataprep.
Here is the work that I will do on the first FTDNA dataset.
Here’s how the recipe looks:
Browsing the dataset with the recipe it looks clean:
Download the FTDNA wrangle file here.
Now it’s time to Run the job and apply these transformations.
You’ll want to modify some of the default configuration here if you plan to take this cleansed dataset right over to BigQuery.
Lets see what needs to be changed. Click the pencil and lets edit this job. Lets change the following:
And under more options underneath replace the file every run
Single file so we can easily import the new .csv into BQ.
Save settings and you are taken back to the run job dialog box. Lets run job.
This transform should take a few minutes to spin up a dataflow job and output our new CSV.
The nice thing here is that we did not have to write any apache beam code at all to clean up this dataset. Dataprep is basically a layer ontop of Cloud Dataflow. While the job is running you can take a look at it in Dataflow if you would like.
A few minutes after running the job (about 7 min for this one) the transformations should be complete. View results to make sure your data is clean.
Its okay to have a 1% mismatch on the CHROMOSOME field in this table as the raw data we are using here has our x chromosome (X) and autosomal values (1–22).
Looks good to me.
Now export your results.
Validate the locations you set before, and click Create
Check your cloud storage bucket and the new .csv should be there.
Now you can either download the .csv or just import to BQ right from the GCS bucket.
Success! Now my FTDNA dataset is imported into BigQuery. Next, lets get the 23andMe dataset prepared and ready for BQ.
Since the 23andMe raw data columns do not match FTDNA columns, you will want to clean the 23andMe raw data genome TXT file in Dataprep as well. Import it as a data source and make the following transformations in your recipe:
Download the 23andMe Cloud Dataprep wrangle file here.
Run the job like we did on the FTDNA dataset, no compression and single file.
Let’s view the results of our completed job to see how it came out:
This dataset should now look similar to your Dataprep cleaned FTDNA dataset and is ready to be exported to CSV then loaded into BigQuery alongside the FTDNA table. Export results the same way that I did for the FTDNA earlier.
Super easy to import to BQ after you run the job and export the results.
My BQ personalgenome dataset includes 2 tables and 2 different genomes. I have a test with Ancestry awaiting results so eventually I’ll have 3 different copies of my genome from unique personal genetic providers. This is my personal DNA dataset:
In BQ to find the total number of rows and matching rows use the following query:
Both genomes together have 1,322,236 rows, 1,009,997 of which are unique. So that gives me 312,259 matching rows. That’s a 23.6% match between my 23andMe and FTDNA genomes.
Also, try a nested wildcard query to find the total number of matching rows between the tables in my dataset
As of June 24, 2018, SNPedia has detail on only 108,485 SNPs. So do not expect to have some strong answers to your DNA, but instead some possible insights.
I had a carrier test done before I got married from a company called Counsyl. Unfortunatly Counsyl does not provide comparable data to 23andMe and FTDNA as they primarily provide a report matching your DNA with your spouse. I attempted to obtain my raw data from Counsyl and they did send me a vcf but it was considerably smaller and did not include reference genome variants. More on Counsyl vs 23andMe:
The Counsyl Foresight screen performs full next generation sequencing across hundreds of genes that can cause inherited genetic disease. This constitutes approximately a million base pair positions which are inspected for novel variation. The VCF primarily indicates positions with rare non-reference variation which gives a VCF of approximately 1,200 entries.
23andme by comparison performs genotyping with a microarray at approximately 600,000 positions with known common ancestral variation. Their VCF files likely contain the genotypes at every position so it’s expected that the file would be 500x larger.
My Counsyl test came back that I was a carrier for something called congenital amegakaryocytic thrombocytopenia. I wanted to verify the SNP for this disease found in my genome by Counsyl was also in my genome taken by 23andMe and FTDNA.
So lets query those tables in BigQuery.
Google search for congenital amegakaryocytic thrombocytopenia snp returns an SNPedia result:
rs12731981 — SNPedia
So now that we have both 23andMe and ftdna datasets in BigQuery, lets check for this SNP in my personal genomes
Since I have made sure both tables have the same column names using Dataprep, I can use a wildcard table in my query to query all tables in the dataset for the SNP. I’ll use _TABLE_SUFFIX to let me know which raw data source is displaying the SNP.
That SNP is matched on all 3 raw data sources I’ve had, Counsyl (via report), 23andMe and ftdna. No worries, my wife had the Counsyl test done as well and she is not a carrier for this disease so our future children will be fine 🙏🏼.
What I just did wasvalidate a genetic marker across 3 different DNA testing services. This gives me a good level of confidence on this genetic marker on my DNA. Pretty cool.
Randomly checking the SNPs for cancer on SNPedia
Prostate Cancer has an identified SNP with aggressive prostate cancer here:
One SNP has been found to be associated not only with prostate cancer in general, but also specifically with aggressive prostate cancer [PMID 18073375]:
Checking for this SNP across my FTDNA and 23andMe genome
Shows up on both of my raw data sources:
Some research has been done to back this up:
We performed an exploratory genome-wide association scan in 498 men with aggressive prostate cancer and 494 control subjects selected from a population-based case-control study in Sweden. We combined the results of this scan with those for aggressive prostate cancer from the publicly available Cancer Genetic Markers of Susceptibility (CGEMS) Study. Single-nucleotide polymorphisms (SNPs) that showed statistically significant associations with the risk of aggressive prostate cancer based on two-sided allele tests were tested for their association with aggressive prostate cancer in two independent study populations composed of individuals of European or African American descent using one-sided tests and the genetic model (dominant or additive) associated with the lowest value in the exploratory study.
Fortunately the allele (variant of the gene) for this marker on SNPedia is A;A with the highest risk and my allele seems to be G;T, so hopefully I may be in the clear here. But this shows you how much we know about DNA. The research that was done was only on about 500 men and SNPedia says the A;A variant has >1.36x risk for prostate cancer. So its not very strong or conclusive but it may mean something in another case to someone.
Next I’ll use Google Cloud Datalab to create notebooks for my genome data warehouse exploration.
Quickstart for Cloud Datalab
Using Cloud Datalab I can run BigQuery queries inside a notebook with %%bq query
In Datalab I can write python code to do analysis. For example in my notebook I am scraping snpedia.com for the popular SNPs using BeautifulSoup.
Once I add more raw data sources (Ancestry), I’ll write something to give me a score for matches found across multiple tables.
Download this Cloud Datalab ipython notebook here.
Today we can have genotyping done from multiple sources at a relatively reasonable cost. Services such as 23andMe, Family Tree DNA, and Ancestry the most popular today for people to have genotyping done for ancestry and health data exploration. This data can help us better understand our genetic makeup, understand disease risk, and possibly allow people to better plan their lifestyle. SNP markers are still being identified and may not be accurate for certain cases. With your genotyping raw data and Google Cloud Platform you can easily build a personal data warehouse and gain a better understanding of large datasets such as your own DNA.
Google Cloud community articles and blogs
335 
335 claps
335 
Written by
Customer Engineer, Google Cloud. All views and opinions are my own. @mkahn5
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Customer Engineer, Google Cloud. All views and opinions are my own. @mkahn5
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/google-cloud-run-for-go-ec09ddbba111?source=search_post---------99,"There are currently no responses for this story.
Be the first to respond.
Google Cloud recently launched a fully managed container execution environment called Cloud Run. It is an environment specifically for request-driven workloads. It provides autoscaling, scaling down to zero, pretty fast deployments, automatic HTTPS support, global names and more. Google Cloud Run doesn’t have language runtime restrictions as soon as the language runtime is supported on gVisor. It only requires the deployments to expose an HTTP server on a port.
In order to deploy a Go server, we will start with a minimal helloworld HTTP server. The container runtime is expecting the server to listen on $PORT:
In order to build a Docker image, create a multi-stage Dockerfile:
Build and push the image to Google Container Registry:
Then, either from the Cloud Console or terminal, you can create a new service with the built image:
You can use the provided endpoint to access to the service. See the Cloud Run console for logs and other operations.
Google Cloud Run is specifically optimized for request-driven HTTP/HTTPS workloads. The containers can be preempted and migrated, so it is not great if you are planning to use it for long running services. There is no support other than HTTP/HTTPS for now either. See also Cloud Run on GKE if you need more flexibility.
If you have any feedback, please feel free to reach out to jbd@google.com.
Google Cloud community articles and blogs
365 
2
365 claps
365 
2
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
See rakyll.org for more.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/highly-available-websockets-on-google-cloud-c74b35ee20bc?source=search_post---------100,"There are currently no responses for this story.
Be the first to respond.
A few weeks ago, @devongovett, who is still working @Storify, contacted me to share an update on the way Storify load-balances websockets. We wrote an article 2 years ago to explain how we used IP affinity in order to load-balance socket.io using HAProxy.
But balancing using Source IP affinity could lead to issues: socket.io’s polling transport breaks on some corporate networks where they load-balance traffic across multiple external IP addresses. In this case, the requests associated with a particular session id might connect to a different process, which would break the way socket.io buffers the messages for this particular session.
The answer is to switch to an application layer persistence solution with a session cookie.
Let me take you through an example.
I’m going to use a slightly modified version of the socket.io chat example, by splitting it into 2 servers.
One server will be responsible for serving the single HTML page. The other server will take care of broadcasting the messages to all chat users.
In a production context, the HTML page should be delivered by a CDN. But for the demo, I want to show how we could load-balance both HTTP and Websocket requests.
In order to have the messages passing between websocket servers, we will use a Redis adapter. When User A, connected to the instance-websocket-1 server sends a message, this message can be broadcasted to users connected to the instance-websocket-2.
Google Cloud Platform offers a TCP Load Balancer that will be used as the public entry point of our Network. It will load-balance the traffic between many HAProxy instances.
HAProxy will be responsible for redirecting traffic to the desired backend server (frontend or websocket), and to make sure the socket.io requests from the same user always go to the same process. This is critical if the client doesn’t support the WebSocket protocol and therefore falls back to polling transport.
First, HAProxy will listen to all incoming traffic on port 80, and redirect it to the websocket backend based on the subdomain (“ws.”), or to the HTTP backend otherwise.
The HTTP backend configuration is pretty straightforward. It uses a “roundrobin” strategy to load-balance the traffic. Let’s not forget to define the HTTP check url so HAProxy knows when one of the servers is failing. I find it useful to add a query string for debugging purpose.
The magic lives in the websocket backend configuration. If the WebSocket protocol is supported, there is no issue because a single tcp connection is used. But in the case of falling back to polling transport, many connections are made to the backend. On the first request, HAProxy sets a cookie specifying which server was used, then it uses that cookie to choose the same server for subsequent requests.
The TCP Load-Balancer is responsible for balancing the incoming public traffic on port 80 to the HAProxy instances.
I added some information to the chat page to make it easy to check the full system:
By refreshing the page, we can see that the html is served by either backend servers, but the websocket is always connected to the same (original) server.
In case of a failing websocket backend, the user will re-connect to another instance. In this example, our user was connected to instance-websocket-2, this instance died, and the user was reconnected to instance-websocket-1.
When a HAProxy instance dies, Google Cloud Load-Balancer redirects all the traffic to the other HAProxy instances.
This whole stack gives us a robust way to scale our service, which uses socket.io for communication.
HAProxy gives the ability to fine-tune the redirection of the traffic to the desired microservice, with the necessary option to attach a session cookie for socket.io.
Google Cloud community articles and blogs
225 
3
Thanks to devongovett. 
225 claps
225 
3
Written by
Rocket scientist, converted to software. Engineer @google ( @kaggle ). Belgian. Traveller. Musician.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Rocket scientist, converted to software. Engineer @google ( @kaggle ). Belgian. Traveller. Musician.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@sathishvj/notes-from-my-beta-google-cloud-professional-cloud-developer-exam-e5826f6e5de1?source=search_post---------101,"Sign in
There are currently no responses for this story.
Be the first to respond.
sathish vj
Jan 8, 2019·6 min read
Subscribe to my YouTube channel that teaches you to apply Google Cloud to your projects and also prepare for the certifications: youtube.com/AwesomeGCP. Check out the playlists I currently have for Associate Cloud Engineer, Professional Architect, Professional Data Engineer, Professional Cloud Developer, Professional Cloud DevOps Engineer, Professional Cloud Network Engineer, and Professional Cloud Security Engineer.
Update on Jan 24th, 2019: The Professional Cloud Developer exam is now out of beta. Also, I passed the exam!
Immediately after the exam I do a memory dump as notes. Hence it is also quite unordered. This is a sanitized list that gives general topics and questions I encountered. The intention is not to give you the questions, but to give you topics that you can be prepared for. I was often stumped by some questions; hopefully you can be more prepared based on my experience. Wish you the very best!
I didn’t really prepare for this exam. There was no material to prepare with either (at the time that I wrote the exam). But I hoped my preparation for the other three certifications would carry me through on this one. The results are out only in a few months and I cannot be certain I passed yet. Btw, because it was a beta exam, there were 102 questions for me and 4 hours to do it in. In the notes below I’ve mentioned topics from the exam outline.
Update: the exam is out of beta now. I passed and got my certificate.
Google Cloud Certified — Professional Cloud Developer
For those appearing for the various certification exams, here is a list of sanitized notes (no direct question, only general topics) about the exam.
Overall notes across all GCP certification exams
Notes from the Professional Cloud Architect exam
Notes from the beta Professional Cloud Developer exam
Notes from the Professional Data Engineer exam
Notes from the Associate Cloud Engineer exam
Notes from the beta Professional Cloud Network Engineer Exam
Notes from the beta Professional Cloud Security Engineer Exam
Notes from the Professional Collaboration Engineer Exam
Notes from the G Suite Exam
Notes from the Professional DevOps Engineer Exam
Notes from the Professional Machine Learning Engineer Exam
Main Link — https://cloud.google.com/certification/cloud-developer
Topics Outline — https://cloud.google.com/certification/guides/cloud-developer/
Practice Exam — https://cloud.google.com/certification/practice-exam/cloud-developer
A collection of posts, videos, courses, qwiklabs, and other exam details for all exams: https://github.com/sathishvj/awesome-gcp-certifications
I’ve collected here a bunch of free Qwiklabs codes which are awesome to get lots of hands-on practice. Use them well.
medium.com
Check the FAQs here: https://medium.com/@sathishvj/frequently-asked-follow-up-questions-on-google-cloud-gcp-certifications-438e1addb91d.
Wish you the very best with your GCP certifications. You can reach me at LinkedIn and Twitter. If you can support my work creating videos on my YouTube channel AwesomeGCP, you can do so on Patreon or BuyMeACoffee.
tech architect, tutor, investor | GCP 12x certified | youtube/AwesomeGCP | Google Developer Expert | Go
506 
9
506 
506 
9
tech architect, tutor, investor | GCP 12x certified | youtube/AwesomeGCP | Google Developer Expert | Go
"
https://medium.com/@ebidel/puppeteering-in-firebase-google-cloud-functions-76145c7662bd?source=search_post---------102,"Sign in
There are currently no responses for this story.
Be the first to respond.
Eric Bidelman
Aug 16, 2018·5 min read
Earlier this year, the Google Cloud team announced support for the Node.js 8 runtime in Google App Engine Standard. I had been stoked about this for a while because most of my server work these days happens in Node. More importantly, I had been eager to run Puppeteer and headless Chrome in the cloud…at Google scale!
I got Paul Kinlan excited about the idea of running the browser in a web server. He tends to get excited about many things, but this notion of the “Browser as a Service” (BaaS) really geeked us both out. Imagine being able to set up URL endpoints that use headless Chrome and leverage features in the browser. You could do all sorts of neat stuff. For example, you could create a PDF generation web service or a handler that takes screenshots of a web page by passing it a ?url parameter. The list is truly endless!
Shortly after our geek-out came coding. We put together a POC site that shows how to use headless in the cloud. It’s called “Puppeteer As A Service” https://pptraas.com/ and has lots of examples + shows off what the browser can do when you bake it into a backend. The site runs on Google App Engine Standard (Node).
Some of our endpoint examples:
Check out the full source on Github.
For the longest time, you couldn’t run headless Chrome / Puppeteer inside a Google Cloud Function. That’s because their environment was using Node 6 and the Linux runtime was missing the required OS packages needed by Chromium.
Although Puppeteer can run in Node 6 without transpiling, the missing OS dependencies made it impossible to run headless in Cloud Functions.
Fast forward to today…things have improved! Now, Google Cloud Functions (GCF) and Cloud Functions for Firebase (FCF) both use the same Node.js 8 runtime as App Engine Standard. This means that you can write cloud functions that use Puppeteer and headless Chrome, and seamlessly author serverless apps that utilize all the features in a web browser!
Simply install Puppeteer from npm (npm i puppeteer) and use it. The library comes with everything you need and works out of the box on App Engine, GCF, and FCF.
Puppeteer is an ideal way to control headless Chrome in environments like Google Cloud Functions and Cloud Functions for Firebase because you spend no time configuring Chrome (and its required dependencies) and more time writing your own code.
The key to running Puppeteer in GCF is to include it as a dependency in your package.jsonand deploy the functions using the --runtime nodejs8flag. For example:
The rest remains the same as before: write your cloud functions and deploy.
Another option is to use Cloud Functions for Firebase. It’s slightly nicer to use than GCF but for all intents and purposes, they’re the same thing under the hood.
The key to running Puppeteer in a Firebase function is to specify Node 8 as the runtime in package.json. My functions/package.json looks like this:
That’s it!
The example below sets up a little Express server and a few endpoints to handle requests. One is a screenshot service and the other simply prints the version of Chrome being used by Puppeteer.
Main functions/index.jsfile:
Lastly, you probably want to setup human readable URLs for your endpoints instead of the default ones GCF gives you. To do that, create a .firebaserc file to use Firebase Hosting and map the cloud functions to a URL path.
Here’s what that looks like:
This gives users a nicer URL to use:
https://pptr-functions.firebaseapp.com/screenshot vs.https://us-central1-pptr-functions.cloudfunctions.net/screenshot
and works great even if you’re using a custom domain with Firebase Hosting.
Full source code available on Github.
Before we depart, it’s worth mentioning that Puppeteer is well supported on a number of different platforms across Google’s cloud. They range from running on a low-level environment with complete control over a VM (GCE), to fully managed VMs (GAE Flex), to pure serverless platforms like GCF. It’s kind of daunting so here’s a breakdown:
For more information, check out these resources.
Documentation & source
My articles on headless Chrome:
Engineer at Google working on Chrome, web components, and Polymer. Uses Digital Jedi skills to push the boundaries of the web. Go Blue.
480 
6
480 
480 
6
Engineer at Google working on Chrome, web components, and Polymer. Uses Digital Jedi skills to push the boundaries of the web. Go Blue.
"
https://betterprogramming.pub/streaming-images-videos-to-google-cloud-storage-with-node-js-express-and-multer-%EF%B8%8F-7455c60f310?source=search_post---------103,"Sign in
Nick Parsons
Jan 22, 2019·2 min read
Note: I use a config.js file for holding configuration values for simplicity. I highly recommend using dotenv over this approach.
"
https://medium.com/google-developers/building-a-simple-web-upload-interface-with-google-cloud-run-and-cloud-storage-eba0a97edc7b?source=search_post---------104,"There are currently no responses for this story.
Be the first to respond.
Posted by Matt Cowger, Global Head of Startup Architects
In my time since joining Google, more than one startup has asked about the possibility of receiving files into Google Cloud Storage (GCS) so that they can be the start of a Pub/Sub based pipeline for all sorts of use cases — image recognition, data ingestion, etc.
To me, this seems like a natural fit for the product. However — there’s no available ‘shim’ layer for doing these — there’s no way to treat GCS like a FTP server, or a direct (simple) web upload target. There are work arounds with gcsfuse or gsutil, but those require end users to install and utilize command line products that are Google specific, and even then have the challenge of not being usable without direct Google credentials in the project.
However, even a naive implementation of such a system would be suboptimal (I suppose thats what its called naive). By placing a single FTP or web upload shim (likely hosted out of a single GCP zone), we negate much of the power and performance of Google’s global network and GCS’s distributed nature, as well as potentially limit performance to the network interface upon which our shim runs. Optimally, we’d want something with the following characteristics:
Using these ideas as my guide, I developed a prototype to do exactly that, using only 2 Google products (Cloud Run and Google Cloud Storage).
Ultimately the solution to the problem was to use the fact the GCS supports signed URLs — URLs that have all relevant (time limited) authentication information built into them. We can use those as a way to avoid the need to deliver standard GCP credentials. By using signed URLs we also avoid issues #1, #2 and #4 — all of the bulk upload traffic from the user to GCS is carried directly to the nearest Google Cloud edge point over HTTP, and not through a single system somewhere.
The two biggest challenges are to:
Generating signed URLs within GCP is a fairly simple process, and supported by a wide number of languages and their associated GCP SDKs. Because Python is superior to all other languages in every way, I chose to use it as my base, but it could be done in nearly any language you prefer.
After a few setup procedures detailed in the GitHub repo for this project, I centered on a key function to generate the URLs:
It’s very simple, and very short — consider this just a backend API. After parsing some incoming parameters (note: there are gaping security holes here — this function uses a client generated value with absolutely no sanity checking), we ask the API to sign a URL for this path in the bucket that’s good for 60 minutes.
It’s worth remembering that the Python itself is just an API — it does not really handle any of client client side code. However, for ease of testing and deployment, I also have the static part of the site (HTML, CSS and JavaScript) served from the same container, but that could be replaced with GCS website serving as a later optimization.
Running this small Python script is its own interesting thought process: Do we run this on a Compute Engine instance, as a AppEngine instance, as a Cloud Function, in Cloud Run or in Kubernetes (GKE)? All of these would work, but for my case, I wanted as little management as possible, making Cloud Functions and Cloud Run the top contenders. Both support scale-to-zero and per-millisecond billing, meeting requirement #5. For me — I’m most comfortable testing, deploying and managing containers, so I went with Cloud Run.
The last important component is the client side work, where the user enters the file they wish to upload with a standard form, then the request for the signed URL is made:
And then lastly the form itself is submitted on button click by the user. This is special because the target of that form is the GCS signed URL directly rather than the Python service, meaning we are only limited by client bandwidth for the upload and maximize the performance benefit of Google’s network.
Once the code has been pushed to Cloud Run (gcloud run deploy), its ready to go (note, I’m skipping the process of building a container — that process is left up to the reader, but the Dockerfile is in the repo)!
You can find the full repo on my GitHub: https://github.com/mcowger/gcs-file-uploader
Engineering and technology articles for developers, written…
342 
2
342 claps
342 
2
Engineering and technology articles for developers, written and curated by Googlers. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
News about and from Google developers.
Engineering and technology articles for developers, written and curated by Googlers. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://itnext.io/where-google-cloud-is-going-wrong-729e78f6c160?source=search_post---------105,"Despite a solid product offering, Google Cloud Platform is struggling to find its audience.
Comparing the big three cloud providers is like measuring high performance sports cars— you can argue over the subtleties of various models but ultimately they are in a completely different class to a Toyota.
Among these cloud titans, while AWS has sped off into the distant future, there’s fierce fighting for second and third place — and it’s a race Google is currently losing.
On the surface, this table makes my argument look baseless— GCP grew by 100% in 2016. But percentage comparisons only work when sizes are roughly similar. It’s a $22 billion marketplace that’s exploding and Google only has $500 million of this pie. Microsoft is 3 times the size and Amazon is 20 times larger. Even Alibaba (they have cloud!) has displaced Google in terms of revenue share.
A peek at the 2017 numbers should also make Googlers worry:
The smaller players are shrinking. Amazon’s percentages are hard to compare when their presence is so massive but among the rest, Microsoft is clearly running away for the number two prize in cloud.
In this massive market the runner-up prize in cloud is a big deal. There are some companies that simply won’t use Amazon, mainly because it’s seen as a competitor or other clouds might be a better fit for the existing tool-set. Microsoft has launched a formidable full scale assault in the “anyone but Amazon” space which has been staggeringly successful, and Google is frankly stalling.
Google was the original global scale, high-availability platform — it should easily take the crown in this business. Graphs aside, I also hear from clients and other cloud practitioners the growing pessimism about whether they will continue to use GCP. So what is going wrong?
Back in 2010, I was a huge Google Apps fan and spent much of my time convincing companies to drop Office and head to the browser. I even wrote a freaking book about Google Apps, in between foaming at the mouth at how Google Apps would soon be eclipsing Microsoft for all email and productivity.
In my blind optimism, the reasons for Google’s glorious victory in Apps were obvious compared to Office:
And, boy, how wrong was I! People looked at me like I was insane. Google Apps just never gained a serious foothold. Years later, Microsoft got mildly aggravated that Apps was stealing single-digit market share and crushed it with Office 365, which is now the lion share of SaaS office usage.
How did that happen? Office 365, if you’ve been lucky enough to use it, combines the worst of Outlook, a dramatically reduced feature-set of Office and none of the cool things about Google Apps. And yet corporate users have flocked to it in droves.
When I ask corporate execs why they went with Microsoft, the answers are always the same — 365 offered a natural upgrade path, an evolution for users, didn’t cost anything for existing licenses and it was blessed by their IT. In short, it was an enterprise upgrade, and the underlying tone was Google wasn’t really enterprise enough.
And in a nutshell this is the same problem with Google Cloud. If you go to the conferences, look at the marketing materials and check out the partner eco-system, they are simply not effectively targeting an enterprise audience. It’s techie, it’s nerdy, it’s academic, it’s esoteric, and whatever technical advantages they have over the competition are being completely ignored by the large adopters and eclipsed by competitors.
Does it matter? Emphatically, yes. You need the large corporate customers for volume, revenue, scale and ultimately the credibility that gets you more large customers.
I am 100% sure that DynamoDB is not going away in the next 10 years. But honestly I am not at all sure that FireBase will be around by next quarter. And this is a really common concern I hear from companies: “Yeah, Google <insert product name> is great but are they really committed?”
It’s a direct side-effect of overly-aggressive retirements in recent years, from great ideas nobody used (Google Wave) to okay-ish software everybody used (Google Reader) and critical software that people built businesses around (Insights for Search). Even fantastic products have no development for a decade (Google Voice). iGoogle, Google Talk, Google Health, Picnik, Google Buzz —all gone, gone, gone. It’s a virtual graveyard.
The acceleration between “This is the future” and “Nah, we’ll killing it” has reached the point where soon they’ll be launching and retiring in the same sentence. “We pleased to announce our new beta and you have until December to download your data before shutdown.”
And then there’s the zombie product problem, as shown by Google+. Is it dead, alive or what? There has been conflicting official quotes from Google executives saying they are shutting down… nope, now it will be “more focused” —wait, we haven’t given up yet, it’s still around. Is this any way to treat a platform that was officially launched to compete with Facebook? There are even articles on how to delete your Google+ account without affecting Gmail, that’s how screwed up this is.
Google at its core has a major Product Management problem that directly impacts its cloud adoption. Companies will not commit to your platform when you shut down services unpredictably. Both Amazon and Microsoft understand this and have a solid track record of supporting their cloud products.
Google’s infamous retirement ‘strategy’ makes every product look like a hobby. (If you think this is overstated, just ask anyone who built software around Google Realtime API.)
Google has the most successful advertising platform in the world but if you’ve ever used AdWords you’ll see it was designed by developers not marketers. The average company struggles to launch AdWords campaigns and use the core Google product that literally everybody wants, so much so that Google once launched AdWords Express to solve the problem (it didn’t).
Facebook, on the other hand, has shown how to sell pretty much the same product to non-techies and has built a wildly successful platform that average companies can use. Google missed this huge opportunity because it’s not a sales company and I’ve seen this over and over in various launches.
Case in point: the Google Next conference last year was the only tech conference I’ve ever seen that became cheaper as the date approached. You could buy a $1500 ticket six months ahead or wait until 2 weeks prior for an identical $500 ticket (admittedly with a widely distributed vendor discount). From airlines to music concerts, nobody operates this way. It created zero buzz for the event and flies in the face of the scarcity model of marketing. (For comparison, try finding a discount coupon for re:Invent.)
The same happened with Premier and Standard network tier pricing confusion. Instead of launching the Google-network backbone as a premium upgrade, it was presented (bizarrely) as a standard network option downgrade. You wouldn’t believe the number of people I spoke to who thought Google had increased the price of its network service and didn’t understand what it was. It would be like if AWS had announced Glacier but left customers with the impression that now you pay for data you need immediately.
In cloud, AWS and Azure have enterprise-grade sales efforts with entire vendor ecosystems ready to sell for them. Dealing with these companies, from a sales perspective at least, looks and smells just calling the big tech companies of yesteryear and, at a certain size, target customers like the familiarity. It’s all new but it feels just the same. By comparison Google’s sales effort is amateurish at best.
Google practically created the idea that software doesn’t need support — Gmail has no 1800 number and it doesn’t need one because it’s simple to use and always works. It’s easy to forget before Google, software pretty much always came with support. Unfortunately, this same approach doesn’t work with cloud.
As with many Google products, their free (read: developer) cloud support tends to focus on thinly-contributed user forums rarely visited by Google engineers. The couple of times I ventured into GCP for work projects, when I hit arcane snags (like the phantom database disconnection problem), I quickly realized I would never get an answer and went running back to AWS.
I know, I know, all cloud vendors have terrible documentation and spotty support. But when I turn to the developer’s official fire extinguisher, Stack Overflow, you can see there isn’t much support for GCP there either, which only amplifies the need for Google to provide more support.
Google Cloud does many things better than either Microsoft or Amazon, but don’t expect Google to tell you what they are.
Again, Google Next (their cloud conference) is firmly stuck in the “isn’t cloud great?” conversation of five years ago, whereas attendees should have had their eyelids taped open to be brainwashed into the awesomeness of their AI powerhouse. Instead the audience was given once of the worst tech keynotes in Silicon Valley history and I remember exactly none of the announcements.
Seriously, Google’s AI and deep learning tools absolutely demolish the competition so why isn’t this the primary message we’re hearing? “We’ve got per-second billing now,” they meekly announced — days after Amazon beat them to the punch (Google actually implemented sooner but lost the PR battle). Seriously, where is the value in the race to the bottom for cheaper virtual servers? Dammit, show us your cool toys!
Even simpler tools like FireBase aren’t promoted as differentiators. Building the equivalent of FireBase in AWS is not trivial (and why would you?). A Cloud Guru’s tutorial on building a video sharing website shows how mindlessly easy it is to use Firebase. It’s a great, great product with broad application. But frequently all I hear is that it’s too expensive (really?) and massive unexpected price changes last year screwed some devs.
I once worked with a former Google exec who said the company struggled to focus on anything not on the first page of its revenue charts, which I thought explained its casual attitude towards cloud. Seeing as advertising is upwards of 90% of the entire company’s revenue, that might make sense when their cloud sales are so small.
But as an enormous Google Fanboy, it pains me to see this. Amazon doesn’t ignore AWS because its real focus is on retail, and Microsoft isn’t making Azure a side project because of Windows and Office. It should be possible to overcome these hurdles since the technology is good and the infrastructure is there.
Unfortunately, I’m not hopeful. People are excited about AWS and there’s significant, growing chatter about Azure. Where are the people boasting about Google Cloud Certifications? Who can’t wait for the Google Next 2018 conference? GCP has the feeling of a niche provider that is going backwards at a time when the major clouds are iterating features at lightning speed.
Reluctantly I find myself taking pause with Google and more routinely using AWS for everything. With limited time to learn new tools and clients who want dependable enterprise-grade solutions, it’s not a platform I talk about much anymore. But I’d love — just love — for Google to prove me wrong because I’m still their number one fan.
ITNEXT is a platform for IT developers & software engineers…
435 
6
435 claps
435 
6
Written by
Developer advocate. Opinions here are my own. See my other blogs at http://bit.ly/jbeswick.
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
Written by
Developer advocate. Opinions here are my own. See my other blogs at http://bit.ly/jbeswick.
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/node-to-google-cloud-compute-engine-in-25-minutes-7188830d884e?source=search_post---------106,"There are currently no responses for this story.
Be the first to respond.
I’m going to show you how to configure Google’s Compute Engine server to run your Node.js project in production in less than 30 minutes.
Things I won’t be demonstrating:
For this tutorial, I’ll be working with my starter kit. It has react + express + es6 already configured.
https://github.com/ColeMurray/react-express-starter-kit.git
To get started, let’s create a new project on compute engine.
navigate to: https://console.cloud.google.com/iam-admin/projects
and click Create a New Project
After creating your project, navigate to Compute Engine in the Menu.
Here, we’ll create our VM.
Configuration:
Sweet! We have our VM configured ready for our project.
For this tutorial, I’ll be using the web ssh client.
We need to install Node.js and Npm to setup and run our project. SSH into our machine and enter these commands:
$ sudo apt-get -y update
$ sudo apt-get install -y nodejs npm
$ sudo ln -s /usr/bin/nodejs /usr/bin/node
After installing, confirm everything installed correctly. You should see output similar to below.
We need to download our source code and task runner for our project.
$ git clone https://github.com/ColeMurray/react-express-starter-kit.git
$ cd react-express-starter-kit
$ npm install
$ sudo npm install -g gulp
After a minute or two, we’ll have all our dependencies and task runner, gulp, installed. Gulp is our task running we will be using to create our builds. Read more here https://github.com/gulpjs/gulp
Nginx will serve as our reverse proxy. This allows our node application to be accessed from port 80
$ sudo apt-get install -y nginx
we can test that it installed correctly by doing
$ curl localhost
We now need to configure nginx to serve as our reverse proxy for our node server.
Navigate to Nginx’s sites-available folder. This folder contains configurations for nginx and will be where we create our new configuration.
$ cd /etc/nginx/sites-available
Optional: Backup your current default file
$ sudo mv default default.bak
Create our new default file:
$ sudo touch default
Now within our default file (/etc/nginx/sites-available/default):
Restart nginx
$ sudo service nginx restart
Great, now we’ve got nginx configured. Our next step is to setup node for production.
We want our node application to be able to handle any crashes and restart in the event of one. For this we’ll configure PM2. PM2 is a process manager that will allow our application to live forever.
Navigate back to our project:
$ cd ~/react-express-starter-kit
Install PM2:
$ sudo npm install -g pm2
We’ll be using our pm2config to start our Node project. Note the production and production port.
react-express-starter-kit/pm2config.json:
We’ve setup nginx, setup our dependencies, created a config file for pm2. Our final step: build and start up node app.
$ npm run build-prod
$ pm2 start pm2config.json
Navigate to the IP Address of your server found here:
Navigating to the YOUR_SERVER_IP_Address above in our browser, we load our Hello World example.:
Now you may be wondering, “Pm2 will restart my process if my node server crashes, but what if my compute engine instance crashes?”
We’ll enter two more commands that will allow our server to restart in the event of a reboot:
$ pm2 startup
$ pm2 save
That’s it! We’re done. Our server is now in production and will survive through any crashes / restarts. To confirm, let’s test it:
$ sudo shutdown -r now
After our server reboots, we should be able to navigate back to our IP address and see our node server up and running.
To avoid billing charges, let’s clean up this example project.
Navigate to our menu bar and click Manage All Projects:
Select our project and delete it:
In this tutorial we learned how to deploy a node application to production. We’ve used Nginx to reverse proxy our node server and PM2 to ensure it survives any crashes / restarts.
This is a great starting point into building a scalable production-ready application.
Create Todo List App and Deploy it:
medium.com
Setup a Database:
mongodb.com
Secure our production App:
SSL: https://www.digitalocean.com/community/tutorials/how-to-create-an-ssl-certificate-on-nginx-for-ubuntu-14-04
Helmet: An node module to secure our Http Headers: https://github.com/helmetjs/helmet
Redux:
redux.js.org
If you enjoyed this tutorial, please recommend, share, or comment below! As always, feel free to contact me on Twitter for problems, suggestions, chit-chat.
twitter.com
Google Cloud community articles and blogs
425 
16
425 claps
425 
16
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Machine Learning Engineer | Personalization @ Amazon | https://murraycole.com
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/data-hackers/aws-vs-google-cloud-vs-azure-o-que-cada-um-tem-de-melhor-52107174f7b7?source=search_post---------107,"There are currently no responses for this story.
Be the first to respond.
Pessoal de tecnologia já sabe: depois do “minha linguagem é mais cabulosa”, tem discussão mais comum no buteco da TI que “qual Cloud é melhor?”
Inspirado por uma discussão no Slack do Data Hackers, resolvi escrever esse texto breve sobre as MINHAS opiniões sobre o que cada cloud tem de vantagem sobre a outra. Minha avaliação, obviamente, é enviesada. Se você tem outra opinião, comenta aí pra gente trocar aqueles 5 minutos saudáveis de porrada, sem perder a amizade HAHA!
A cultura da Amazon sempre foi de fazer o possível pelos consumidores. Jeff Bezos não tem só a cara de louco, ele realmente é louco pelo mercado.
Muita das vezes, a AWS lança um produto sem ele estar nem mesmo pronto! Isso é horrível? Sim! Porém abre uma vantagem absurda: a de ouvir os primeiros clientes que aplicam para o teste da ferramenta— que são os que querem aquele produto badly!
Mas uma coisa que eu vejo que é essencial, e muita gente que tá fora do ramo de cloud as vezes se esquece, é a força da Rede de Parceiros. Um grande fornecedor, não consegue vender, nem suportar as vendas, sem uma forte rede de suporte. E a AWS valoriza, capacita e apoia muito seus parceiros.
Pensa só, um Arquiteto de Soluções da AWS (SA), por mais foda que seja, nunca vai alcançar a expertise de um bom consultor com anos de experiência numa sub-area da cloud (containeres, data, ML…). Com 2 anos, é bem provável que o SA saia do país. Fora que ele fica longe do dia a dia dos projetos nos clientes, que é onde o pau quebra de verdade. Essa é indiscutivelmente uma vantagem competitiva da AWS. A Oracle dominou (e ainda domina) o mercado de bancos de dados on-premises justamente com essa estratégia. Andy Jassy, que não é bobo, copia o que deu certo.
Todos os grandes produtos da Google Cloud são fruto de anos de pesquisa dentro da própria empresa. Podemos listar alguns que tem liderança quase que inquestionável:
GCP preza pela inovação dos seus serviços. Tais produtos são o estado da arte nas áreas de domínio, e por isso dominam o mercado com folga. GCP, por sua vez, tem uma rede de parceiros fraca e, francamente, não está nem aí pra eles. Se tivesse uma boa rede e um customer services decente, estaria despontando nessa briga, sem sombra de dúvidas.
Desde que Satya Nadella assumiu como CEO da Microsoft, a estratégia da empresa mudou muuuuito. Você deve ter percebido isso após a compra do Github, pela bagatela de U$ 7.5 bilhões. Mas o que talvez você não saiba, é que grande parte dos novos serviços da Azure são sobre plataformas open-source amplamente adotadas. Posso listar algumas:
O suporte da Azure e o apoio à rede de parceiros é bom, não é tão forte quanto da AWS, mas beeem melhor que o da Google. Com essa política, a Microsoft diminui a fricção de adoção de cloud.
Isso sem contar que Cloud Híbrida já existe na Azure desde sua fundação, principalmente para virtualização e bancos de dados.
Digital Ocean: se destaca pela facilidade de uso. É muito fácil começar um projeto na Digital Ocean. O problema vem quando se exige escala. É comum ver empresas migrando para uma das 3 anteriores, visando diminuir os custos e ter acesso a serviços mais complexos que simplesmente VMs, Redes e Storage.
Heroku: é a mais dev-friendly de todas. Claro que é mais um PaaS do que um IaaS, mas vale citar dado o tempo de mercado e grande penetração no mundo dev. Lembro ainda que a Heroku foi a primeira a disponibilizar Kafka-as-Service de todos provedores.
IBM/Red Hat: nem iria citar a IBM aqui, já que seu projeto de Cloud, o Bluemix, vinha sofrendo várias baixas no mercado. Mas um shift bem importante nessa batalha foi a compra da Red Hat, que trouxe todo o know-how da plataforma OpenShift, o melhor provedor de serviços de Containeres na minha opinião de noob.
Resumindo, as principais características que levam a adoção de cada cloud, na minha opinião são:
Claro que existem diversos casos e nem sempre isso é verdade. A melhor cloud é aquela que resolve seu problema, mais rápido e no menor custo. E essa decisão tem muito mais variáveis que essas que apontei nesse texto.
[Edit 1] — Eu não entrei em detalhes de custo. Por dois motivos: qualquer um consegue avaliar isso no primeiro contato com qualquer cloud. Não faz sentido expor isso, já que escrevo a partir da minha razoável experiência com clouds. Segundo: custo é relativo e particular de cada produto e à produtividade do time que faz o sustain/evolution da plataforma. Quanto mais um produto me entrega, melhor o meu serviço/produto. Minha organização ganha sobre isso em diversos aspectos. Logo, avaliar somente custo, não é diferencial… é tratar tecnologia como commodities.
E aí? Qual sua opinião sobre essa guerra de clouds? Você é fanboy de alguma?Comenta aí enquanto eu vou buscar minha luva de boxe 😁 Valeu e até a próxima!
Blog oficial da comunidade Data Hackers
285 
2
285 claps
285 
2
Written by
CTO | Lead Data Engineer | Co-Founder of Data Hackers and Dadosfera. Loves science, code and cats ^*^
Blog oficial da comunidade Data Hackers
Written by
CTO | Lead Data Engineer | Co-Founder of Data Hackers and Dadosfera. Loves science, code and cats ^*^
Blog oficial da comunidade Data Hackers
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@sathishvj/notes-from-my-google-cloud-professional-machine-learning-engineer-certification-exam-2110998db0f5?source=search_post---------108,"Sign in
There are currently no responses for this story.
Be the first to respond.
sathish vj
Mar 9, 2021·7 min read
Subscribe to my YouTube channel that teaches you to apply Google Cloud to your projects and also prepare for the certifications: youtube.com/AwesomeGCP. Check out the playlists I currently have for Associate Cloud Engineer, Professional Architect, Professional Data Engineer, Professional Cloud Developer, Professional Cloud DevOps Engineer, Professional Cloud Network Engineer, and Professional Cloud Security Engineer.
I have the Google Cloud Professional Machine Learning Engineer certification now! 🏆
There are 60 questions in the exam, unlike other Google Cloud certification exams which have around 50! This seems to be standard for the machine learning engineer certification. This had previously surprised some people because they were expecting only 50 questions like other exams, and, from what I hear, it wasn’t clear until towards the end. Now they seem to have updated the screen so that it is not a surprise. Earlier they just gave the current question number. Now they show it like: 1 of 60, 2 of 60, etc.
I have been reading about, doing courses, and playing around with ML for a while now. So there weren’t many ML terms that I didn’t recognize. There were a few, for sure, and that is to be expected, but for the most part I wasn’t as baffled by particular terms as much as interpreting the scenarios and applying them. As usual, I was fairly convinced I was going to flunk. When the exam session broke about half way through, I thought it probably would be better to postpone the exam. Luckily, I went through it to the end, and passed!
The exam has a fair mix of core ML topics and Ml engineering topics. Based on articles written by others who had taken the beta ML exam, I got the feeling that the questions are mostly all or at least predominantly about ML engineering. I would say that is not true — there is a good mix of core ML topics. Learn ML algorithms and TensorFlow, but don’t go too deep. And a surface level knowledge of SciPy, NumPy, and other libraries was sufficient.
I have collected posts, articles, courses, books, and learning material that I found over here: https://github.com/sathishvj/awesome-gcp-certifications. As always, please contribute with other links you find for the benefit of the community.
Topics to Study* Recall, precision — get this picture into your head permanently (https://en.wikipedia.org/wiki/Precision_and_recall#/media/File:Precisionrecall.svg). Understand what are the implications of both. How will you increase either value? If you have the picture in mind, it will help you compute what you need. — you also need to keep in mind the formula for both.* Accuracy, F-score — similar to the recall and precision, remember accuracy also, including the formula. Just know what F-score is — you probably don’t need to remember the formula for it.* Log Loss — (https://www.kaggle.com/dansbecker/what-is-log-loss)* AUC ROC curve and its use. (https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc)* Precision-Recall curve (less important but know what it is)* Based on the formulae, what needs to change to improve recall, precision, accuracy, etc. Think in terms of both the formula and the graph for things like ROC.* Pub/Sub, Dataflow and pipelines, Dataprep — If you’ve done the Data Engineer, there is no additional study to be done.* BigQuery — This is the central datawarehouse for both storing and analyzing structured data. Again, if you’ve done the PDE, you’ll be fine on this section.* Data cleaning and validation. Think about tools like Dataprep, Dataflow, Dataproc, and cleaning data with code.* How to deal with missing data — do you remove it? do you compute it? In which scenarios would you take different approaches?* Data Normalization — what is this and why would you use it? (https://developers.google.com/machine-learning/data-prep/transform/normalization)* Transforming Data — numerical and categorical (https://developers.google.com/machine-learning/data-prep/transform/transform-numeric)* Feature crossing (https://developers.google.com/machine-learning/crash-course/feature-crosses/video-lecture)* One-hot encoding, one-hot encoding hashed (https://developers.google.com/machine-learning/crash-course/feature-crosses/crossing-one-hot-vectors)* Binning (https://developers.google.com/machine-learning/crash-course/representation/cleaning-data)* Metadata management in Kubeflow (https://www.kubeflow.org/docs/components/metadata/)* Basic TensorFlow (https://www.tensorflow.org/tutorials)* TFRecords — when are they preferred over options like dataframes and csv files. (https://www.tensorflow.org/tutorials/load_data/tfrecord)* How to optimize for data input? If data is not coming in fast enough, what can you do?* How to optimize for data processing? If data is coming in fast, but processing is slow, what can you do?* Use of tf.data.Dataset.prefetch/interleave/cache (https://www.tensorflow.org/guide/data_performance)* How to work with sensitive data? (https://cloud.google.com/solutions/sensitive-data-and-ml-datasets)* DLP (https://cloud.google.com/dlp)* Different types of data encryption  — Format Preserving Encryption (https://en.wikipedia.org/wiki/Format-preserving_encryption) — encryption with AES-264/  — salting* BigQuery ML — BigQuery by itself is important. All the ML you can do with BQML is also very important. What all can you do with BQML? What are its limitations. (https://cloud.google.com/bigquery-ml/docs/introduction)* BQML — Can it work with other ML libraries? Can you import models from tf, scipy, etc.?* BQML available algorithms (https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-create)* AI Platform — what all can you do here? For what kinds of tasks would you look outside for other tools? (https://cloud.google.com/ai-platform)* Basic algorithms — regression, classification, k-means. What kind of data does it classify? When would you use them?* Containerization — learn the basics of docker and kubernetes. How do you create images? Why use containers at all?* Kubeflow pipelines — why and how do you use containerization w.r.t. machine learning pipelines? (https://cloud.google.com/ai-hub/docs/kubeflow-pipeline)* Which libraries can work with AI Platform and Kubeflow Pipelines? For other libraries, what should you do?* Regularization methods — what is L0, L1, L2 regularization? When would you use them? (https://developers.google.com/machine-learning/crash-course/regularization-for-simplicity/l2-regularization)* Develop the ability to look at a scenario and see what kind of regularization to apply.* Develop the ability to look at a scenario and see what kind of algorithms to apply. — don’t have to go too deep into any algorithm* WhatIf Tool — when do you use it? How do you use it? How do you discover different outcomes? How do you conduct experiments? (https://pair-code.github.io/what-if-tool/)* Explainable AI — what is this? When would you use this? w(https://cloud.google.com/explainable-ai)* How can Kubeflow do scheduled trainings? Using external tools (https://cloud.google.com/solutions/machine-learning/architecture-for-mlops-using-tfx-kubeflow-pipelines-and-cloud-build#triggering-and-scheduling-kubeflow-pipelines) and also using an built-in scheduler. * When do you need to bring cloud composer? (https://cloud.google.com/composer)* Which of the available options should you choose for different requirements — AutoML, pre-built APIs, AI Platform algorithms, custom algorithms?* Know a little about Tensorflow Extended — I’m expecting the importance of this could grow in the future.* Building ML pipelines — how do you construct a continuous, end to end pipelines starting from data ingestion and ending with making predictions?* There are references to scenarios for CNNs and DNNs, but not too much details. So gather a general understanding of when you would use different algorithms or training methods.* Various AutoML possibilities — Vision, Natural Language, Translation, Tables, Video Intelligence. When should you use the AutoML version and when no?* How is the AutoML offering different from the API offering. For example, there is also Cloud Vision, Video AI, Cloud Natural Language. When would you use one or the other?* Splitting training data into test and validation set. (https://developers.google.com/machine-learning/crash-course/training-and-test-sets/splitting-data)* Issues encountered when splitting is not done correctly. * Project management around machine learning — how to separate users into groups and projects. This is similar to what you encounter in ACE, PCA, and DevOps.* Why and how to ensure that your model is current? Data and the model could be getting stale. You will need to create pipelines to ensure that they are updated continuously.* Strategies for deploying models in production. This is similar to what you would do in DevOps. Know things like canary testing, A/B testing, going from model experimentation to testing to versioning to deployment.* Hyperparameter tuning and available tools (https://cloud.google.com/ai-platform/training/docs/hyperparameter-tuning-overview)* Kinds of automl edge models (https://cloud.google.com/vision/automl/docs/train-edge)* Formatting input for online prediction (https://cloud.google.com/ai-platform/prediction/docs/online-predict#formatting_your_input_for_online_prediction)* Collaborative filtering, Feature/family filtering — what are they and where are they used? (https://en.wikipedia.org/wiki/Collaborative_filtering)* Quantization * Feature Attributions — Sampled Shapley, integrated gradients, and XRAI — when do you use these? (https://cloud.google.com/blog/products/ai-machine-learning/explaining-model-predictions-structured-data, https://cloud.google.com/ai-platform/prediction/docs/ai-explanations/overview#compare-methods)
For those appearing for the various certification exams, here is a list of sanitized notes (no direct question, only general topics) about the exam.
Overall notes across all GCP certification exams
Notes from the Professional Cloud Architect exam
Notes from the beta Professional Cloud Developer exam
Notes from the Professional Data Engineer exam
Notes from the Associate Cloud Engineer exam
Notes from the beta Professional Cloud Network Engineer Exam
Notes from the beta Professional Cloud Security Engineer Exam
Notes from the Professional Collaboration Engineer Exam
Notes from the G Suite Exam
Notes from the Professional DevOps Engineer Exam
Notes from the Professional Machine Learning Engineer Exam
A collection of posts, videos, courses, qwiklabs, and other exam details for all exams: https://github.com/sathishvj/awesome-gcp-certifications
I’ve been making videos on applying Google Cloud and preparing for the exams. You can subscribe to the channel here:
www.youtube.com
Check the FAQs here: https://medium.com/@sathishvj/frequently-asked-follow-up-questions-on-google-cloud-gcp-certifications-438e1addb91d.
Wish you the very best with your GCP certifications. You can reach me at LinkedIn and Twitter. If you can support my work creating videos on my YouTube channel AwesomeGCP, you can do so on Patreon or BuyMeACoffee.
tech architect, tutor, investor | GCP 12x certified | youtube/AwesomeGCP | Google Developer Expert | Go
199 
2
199 
199 
2
tech architect, tutor, investor | GCP 12x certified | youtube/AwesomeGCP | Google Developer Expert | Go
"
https://medium.com/google-cloud/set-up-google-cloud-gpu-for-fast-ai-45a77fa0cb48?source=search_post---------109,"There are currently no responses for this story.
Be the first to respond.
This blog post is for setting up Google Cloud GPU instance for this great course fast.ai, but some of the practice are still applicable for general purpose. I have been struggled for a few days to make it works, so I want to write a post to summarize these steps.
A lot of credits go to Sabastian from this thread and Amulya Aankul ‘s blog post Running Jupyter Notebook on Google Cloud Platform in 15 min. I just combine both of them for setting up this great course.
It will take you no more 30 minutes to finish all the set up and you can enjoy the course. :)
Note: This post originally target v1, but setup for v2 is much easier, simply just run the bash script after the you connect with the instance.
In order to have GPU available, you need to apply for increasing the GPU quota, the default is 0. It takes me 5 mins to finish the process and get approved, much faster than AWS does.
Go to Quotas → you need to upgrade your account in order to have GPU available → increase the quota for the regions you want, please check that which region you would like to use here.
(Try not to be too aggressive by asking a lot of GPU quota, as from other experience, they may ask you to pay in advance. I think starting with 1 K80 is good enough)
2. Create an Google Cloud Platform(GCP) Instance
In the home page of GCP console, there are some info that you would like to copy down to a text file. Copy your Project ID, as you will need it later when you SSH to your instance. (For me, the project ID is endless-fire-xxxxxxx)
We can start creating instance now. Click “Compute Engine” which appears at the left side.
It is not easy to find where the GPU are, but they are actually hidden, click “Customise” and you will able to add GPU to your instance.
3. Change Boot Disk to Ubuntu 16.04
Make sure you have the correct image Ubuntu 16.04, and make it a SSD hard drive, the default is a HDD. The price actually doesn’t change much, so just choose a SSD! (Recommend to get at least 25GB, note that you can only increase SSD but not decrease SSD later)
4. Almost done
You just need to tick HTTP traffic because later we will access the instance with Jupyter Notebook in a browser. You may also want to un-tick “Delete boot disk when instance is deleted” to keep your storage alive even if you kill you instance. And Now you are done, click create an instance!
It is that easy to setup the GCP, if you want to have a static IP, please go to the great blog post and check what’s more you can do.
Now we need to connect our remote instance on our local machine. Since I am a Window user and I have been following Jeremy’s tutorial, so I am using Cygwin to run my bash script. If you haven’t done that, please check out the tutorial or install Cygwin directly(Make sure you have the 64 bit version and “wget“ installed!).
STEP 1 Install Google Cloud SDK
It is something similar to “aws” package, it is a command line interface to interact with Google Cloud and we will need this to SSH our instance.
I will not spend time to explain how to install it. Please follow the instruction here, the documentation is straight forward.
STEP 2SSH into the instance. Open Cygwin or you may just open the Google Cloud SDK Shell. The INSTANCE_NAME is what I asked you to copy down before, it is in your home page.
STEP 2.5
If you are doing v2. Please do this step and go to STEP 6 and you can ignore the rest.
Originally you can just run this script and you are done with Paperspace, but if you are using GCP VM, you need to change the script a little bit, I have changed the script in my Github so you can just mimic the same step:
STEP 3
These steps are copy from the forum’s thread, thanks to Sabastian again.Download the script that installs CUDA, Anaconda etc:wget https://raw.githubusercontent.com/fastai/courses/master/setup/install-gpu.sh
STEP 4Run the script:sudo sh install-gpu.shAt the end you need to pick a password for the jupyter notebook. This script also clones the course materials from https://github.com/fastai/courses/.
STEP 5reboot, either using the reboot command or the reset option on the console:sudo reboot
STEP 6Create a firewall rule for accessing port 8888 from your local machine(you will be using port 8888 for accessing your Notebook), using the console or the command line:
PROJECT is the Project ID that shown in your console’s home page, which I have asked you to copy down earlier.
YOUR_IP is the IP of your local machine, you can do cmd.exe → ipconfig to check what’s your local IP if you don’t know about it.
(Seems they remove the beta command, I haven’t test it yet. In case it fails, go to the console, VPC Network → Firewall Rules, and add a rules that allows tcp:8888 and add your own ip on it)
STEP 7Check if CUDA is installed properly:
STEP 8Run jupyter notebook:jupyter notebook --ip=0.0.0.0 --port=8888Note the token displayed in the terminal. Go to the notebook using the external IP of your instance. You can find your external IP of your instance in the console.
xxx.xxx.xxx.xxx:8888 is the link of your notebook.
You will need a token to login to your Notebook.
Your token is here:
We are all done with the setup, and you can access the notebook with GPU-powered machine. The great thing about GCP is that you have $300 free-credit, which is quite enough for you to explore the notebook(If not, just apply one more gmail and get $300 more!)
Please let me know if you have any question!
Google Cloud community articles and blogs
499 
28

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
499 claps
499 
28
Written by
Learning | data science | HK @ GitHub: https://noklam.github.io/blog | Linkedin: https://Linkedin.com/in/noklamchan
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Learning | data science | HK @ GitHub: https://noklam.github.io/blog | Linkedin: https://Linkedin.com/in/noklamchan
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/container-builder-797c0dc2c991?source=search_post---------110,"There are currently no responses for this story.
Be the first to respond.
Google Cloud Build (f.k.a Container Builder) was announced last month and I have been using it ever since. It has a few features that I really love that have gone unhighlighted. I wrote a testimony on Hacker News when it came out, so I am going to elaborate on that here. Here’s a short list of the cool features:
If you distribute your apps as containers, you don’t need to host a Jenkins instance or a third-party CI service to build/push your images. Cloud Container Builder not only does this, it can run arbitrary build steps to deploy your application, too.
Disclaimer: I work for Google Cloud, but not on the Container Builder.
If your source repository or directory has a Dockerfile(?), Container Builder can build it —period. You have two options to use this feature: Either go to Google Cloud Console and use the UI to import your repository or just build on the cloud with gcloud(?) command-line tool.
So instead of your most favorite command:
you can run:
gcloud packages your source directory and builds on cloud by executing the following steps:
If you don’t understand what I just said, it means that you do not need Docker on your machine to build and push Docker images. Here’s how it looks like:
I host all my stuff on GitHub private repositories, including my blog. It took me less than 60 seconds to set up a build for a GitHub repository I have. Google Cloud Console has a GitHub integration, so importing your repos is super easy. You just authorize the app to access your repos, then choose your repo to build.
If your repo has a Dockerfile, you don’t need to write a .travis.yml or circle.yml sort of thing.
A single Dockerfile doesn’t always get you what you need. Sometimes you need to execute multiple steps to build your image; I suspect this is why a lot people still use Jenkins.
Cloud Builder addresses this problem elegantly, by providing an option to customize build steps and environments.
Here is what you need to know:
This is exactly like Jenkins build pipelines where your artifacts are stashed/unstashed from one step of the pipeline to another.
To illustrate this better, assume you have an example application:
then Container Builder automatically pushes the tagged image to GCR. You can see all sorts of examples of this in the cloud-builders repository.
But before I finish this section, two more cool things:
You need to understand that Google has a ton of compute and network power and does not mind allocating that to stuff like Container Builder. The machines running your builds on Container Builder are really fast in terms of CPU, I/O and network.
As you can read in my CircleCI vs Container Builder testimony, I have seen my 3m30s build to come down to 1m10s (that’s 3x faster) when I switched to Container Builder.
Here are some reasons why the Google Container Builder is faster:
That said, currently you get 2 hours of build time for free and additional build minutes are subject to charge. I expect most small projects to be happy with the free tier.
I am heavily inspired by Container Builder’s simple build mode. It works out of the box if you just build with Dockerfiles. Yet, if you want to customize, it provides a clean and modular way to do that as well. I already use it to build and deploy my applications and publish my blog.
Definitely check Cloud Container Builder out and read the docs if you are interested.
Follow me on Twitter if you want to hear more about Kubernetes, Google Container Engine and other cool stuff. This article is originally published at ahmetalpbalkan.com.
Google Cloud community articles and blogs
84 
1
84 claps
84 
1
Written by
Infrastructure Engineer at @Twitter
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Infrastructure Engineer at @Twitter
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@iromin/google-cloud-functions-tutorial-what-is-google-cloud-functions-8796fa07fc7a?source=search_post---------111,"Sign in
There are currently no responses for this story.
Be the first to respond.
Romin Irani
Apr 25, 2018·6 min read
This is part of a Google Cloud Functions Tutorial Series. Check out the series for all the articles
In the first part of the series, we took a high level look at the Computing options that are available today and specifically at Functions as a Service (FaaS) features that we should expect from anyone providing that offering.
In this post, we are going to take a look at what is Google Cloud Functions? As per the official documentation:
Google Cloud Functions is a serverless execution environment for building and connecting cloud services. With Cloud Functions you write simple, single-purpose functions that are attached to events emitted from your cloud infrastructure and services. Your Cloud Function is triggered when an event being watched is fired. Your code executes in a fully managed environment. There is no need to provision any infrastructure or worry about managing any servers.
Focus on the words that I have highlighted in bold:
Here are 3 key things to keep in mind while dealing with Google Cloud Functions. Again this applies at the time of writing and is likely to change as it gets out of BETA and announces support for more languages, event providers and so on.
For this, I will take the liberty of using a diagram from the official documentation:
Focus first on the left side labeled as Cloud Services. This is the Google Cloud Platform and its various services. Think of all the services like Google Cloud Storage, Google Cloud Pub/Sub, Stackdriver, Cloud Datastore, etc. They all have events that happen inside of them. For e.g. if a bucket has got a new object uploaded into it, deleted from it or metadata has been updated. Similarly if a new statement has been added to Stackdriver logging or Cloud Pub/Sub getting a message published at a particular topic and so on. Not all events might be supported right now or in other words, not all Cloud Services or Event Providers might be supported right now in Google Cloud Functions but some of them do.
Now, when an event for e.g. Object uploaded into a Bucket in Cloud Storage happens, then an event is generated or fired or emitted. The Event data associated with that event has information on that event. If your Cloud Function is configured to be triggered by that event, then the Cloud Function is invoked or run or executed. As part of its execution, the event data is passed to it, so that it can decipher what has caused the event i.e. the event source, get meta information on the event and so on and do its processing. As part of the processing, it might also (maybe) invoke other APIs. These could be Google APIs or external APIs too. It could even write back to the Cloud Services.
When it has finished executing its logic, the Cloud Function mentions or specifies that it is done. Multiple Event occurrences will result in multiple invocations of your Cloud Functions. This is all handled for you by the Cloud Functions infrastructure. You focus on your logic inside the function and be a good citizen by keeping your function single purpose, use minimal execution time and indicate early enough that you are done and don’t end up in a timeout. This should also indicate to you that this model works best in a stateless fashion and hence you cannot depend on any state that was created as part of an earlier invocation of your function. You could maintain state outside of this framework via some other service like Shared Memcache and so on.
Let us take another look at Events, Triggers and Event Data via the diagram shown below:
We can summarize the following:
At this point, Google Cloud Functions supports the following Event providers:
More events / services should be supported over time.
There are currently two types of Google Cloud Functions that are supported.
The diagram below summarizes this:
Finally, we will touch upon the Event Object. This is the object that is passed to your function. The object depends on the type of event that is fired. There is a different schema for each type of event and
We shall cover these when we cover foreground and background functions based on each of these triggers.
This concludes our overview of Google Cloud Functions. We can now move to setting up our environment and writing our first Cloud Function.
Proceed to the next part : Write your first Google Cloud Function or go back to the Google Cloud Functions Tutorial Home Page.
My passion is to help developers succeed. ¯\_(ツ)_/¯
354 
354 
354 
My passion is to help developers succeed. ¯\_(ツ)_/¯
"
https://medium.com/@timhberry/getting-started-with-python-for-google-cloud-functions-646a8cddbb33?source=search_post---------112,"Sign in
There are currently no responses for this story.
Be the first to respond.
Tim Berry
Jul 28, 2018·5 min read
Please see the updated version of this post at https://timberry.dev/posts/python-for-google-cloud-functions/
I’ve been a fan of Cloud Functions for a while and a fan of Python since forever, so when support for Python functions was finally announced at Google NEXT’18 I did a small dance of celebration (or at least, I fist-bumped the air when no one was looking). I’m sure you know what Python is, but if you’re new to the serverless world we’re now living in, a quick re-cap on Cloud Functions:
Cloud Functions are small pieces of code that execute in an event-driven manner. They do one small thing very efficiently in reaction to a trigger — usually an HTTP request. The neat thing is you manage zero infrastructure and usually only pay for the execution of your function and a few seconds of compute. You may have heard of other Functions-as-a-Service offerings such as AWS Lambda.
Writing Python cloud functions is easy and fun, as I will now demonstrate. To follow this tutorial you’ll need a Google Cloud Project (you can sign up and get free credits here), a local Python development environment and the gcloud SDK.
We can create a simple function by creating a main.py with just 2 lines:
You can deploy this as a cloud Function with the following command. Note that the cloud function name matches the name of the function we defined in code: hello_world
After a couple of minutes, the command will return the httpsTrigger or URL for your function. Try accessing this URL and you’ll see the message “Hello, World!” returned. You can also see your function in the Cloud Console UI:
But this is pretty boring stuff, let’s make the function do something a bit more interactive:
You can use the same gcloud beta functions deploy command as before to update your function. After a couple of minutes, try accessing the URL again and you’ll get the same message. But now try sending it a POST with some data:
And you should receive a custom message in return! The runtime for Cloud Functions uses the Flask library and provides us with a Request object that we can manipulate however we like.
This is all great fun, but now let’s do something useful to show a real world example of where you might employ a Cloud Function. As we’ve seen, Functions are typically triggered by HTTPS endpoints, however you can also make use of Background triggers. Functions deployed this way have no HTTPS endpoint, and are effectively hidden from the Internet, but will instead execute in response to an event such as a file being uploaded to a storage bucket, or a message being sent to a Pub/Sub topic.
Let’s say we have a web service that writes images to a Cloud Storage Bucket. Every time an image is uploaded, we’d like to create a corresponding thumbnail image. Arguably this could be part of our main application code, but for the sake of argument let’s say we want to run this as a separate function, because there may be other ways that images end up in the bucket (maybe an external service sends them to us in batches).
Let’s write a function that reacts when a new file is written to a Cloud Storage bucket. You’ll need 2 GCS buckets for this: You’ll upload images into the first one, and your function will write thumbnails into the second one. In a new directory, create a new main.py file:
Notice that we’re importing modules, just like we would do with any other Python application. Cloud Functions uses pip, which means we can specify the dependencies we need in a requirements.txt file:
Just keep your requirements.txt file in the same location as your main.py when you deploy your function.
Don’t forget to replace <your thumbnail bucket> in the above code with the name of your own thumbnail bucket, and <your source bucket> with your source bucket name in the following example. You can now deploy the cloud function like this:
Note: You can’t use the same bucket for both. Why? Because every time the function writes a thumbnail, it would trigger a new invocation of itself!
When you deploy this function you’ll notice we don’t get given a URL. Instead we’ve told the function to run when the google.storage.object.finalize event occurs in the bucket we specified. Let’s upload a picture of Derek the Dog to the source bucket:
Check the thumbnails bucket, and you should see that the function has done its job and created a thumbnail for us!
You can inspect the magic under the hood by revisiting the Cloud Functions part of Cloud Console, where you can track the invocations of your function and even watch its logs in real-time.
Cloud Functions are perfect when you need small pieces of code to tie together larger pieces of a stack. Now that Google has added Python support, we have a huge and diverse ecosystem of functionality available to us to write functions. For more information, read about the Python runtime or follow all of Google’s How To guides. Thanks for reading!
Please feel free to follow me on Twitter!
Technical Instructor at A Cloud Guru.
389 
5
389 
389 
5
Technical Instructor at A Cloud Guru.
"
https://medium.com/google-cloud/create-and-sell-nfts-on-dshop-powered-by-origin-and-google-cloud-6bd59b66d82?source=search_post---------113,"There are currently no responses for this story.
Be the first to respond.
Non-Fungible Tokens (NFTs) are capturing the public imagination, unlike anything to come previously from cryptocurrency technology. NFTs are often characterized simply as digital collectibles, e.g. cryptocurrency versions of baseball cards, but there are many other interesting use cases for NFTs.
In this post, I will introduce you to the world of NFTs and teach you how to create and sell your own NFTs using the Dshop e-commerce platform by Origin Protocol. As a partner of Google Cloud Marketplace, Origin is excited to introduce this new blockchain technology to developers and creators using Google Cloud infrastructure.
Non-Fungible Tokens (NFTs) are cryptocurrency tokens that are unique or scarce. When something is fungible, it can be easily substituted by an equivalent. For example, a $5 bill can be substituted for another $5 bill or five $1 bills.
In contrast, NFTs can be unique objects, or 1-of-n objects, meaning only n of that object exists. NFTs have been adopted by the NBA and other sports leagues to represent digital trading cards that have video highlights attached to them.
NFTs are also used to represent ownership of physical objects, especially scarce objects such as limited-edition sneakers or fine art. Another use case is NFTs as access tokens to digital content or real-life experiences. An NFT holder could unlock exclusive or private content on the web or redeem their token as a concert ticket. One important aspect of NFTs is that future revenues or royalties to the creator of the token from sales of the NFT on the secondary market can be programmed into the NFT itself.
Origin Protocol has facilitated two high-profile NFT sales, also known as “drops”. In early 2020, we helped Brave Software, maker of the popular Brave Browser, switch from Shopify to Origin’s Dshop platform for their merchandise store. Brave now hosts their store on a decentralized platform that accepts their native cryptocurrency. Brave held a meme creation contest with their community in early 2021 and created or “minted” 30 NFTs in the form of three 10-of-10 NFTs based on the three most popular entries. This collection of meme NFTs was then listed on their Dshop for sale. All 30 NFTs sold out nearly instantly, showing there is an extremely strong demand for NFTs from their community and demonstrating a completely new form of interaction between software companies and their users.
In March of 2021, Origin broke the record for the highest-grossing NFT sale at that time when a tokenized music album was put up for auction. The sale brought in nearly $12 million and made headlines in industry and mainstream publications, including Forbes, Billboard, and Business Insider. Several different collections of NFTs with unique use cases were sold during this auction. All of these NFTs are freely tradeable and transferable by their owners.
More recently, Origin announced NFT sales with Grammy Award-winning hip hop artist Lupe Fiasco and Internet sensation and professional boxer Jake Paul. We plan to power many more NFT sales on our commerce platform in the future.
To get started, we have to set up a Dshop on Google Cloud Platform. The process is fairly straightforward but requires a few steps. We provide both a written guide and a video guide illustrating how to do this:
You can read more about how Dshop works with Google Cloud Platform here.
After setting up your own Dshop, it’s time to create or “mint” your NFTs. You’ll need a Web3-enabled wallet like MetaMask, which is an extension to your browser. You will also need some ETH in your wallet to pay Ethereum network gas fees. Minting NFTs is free on OpenSea, which is a popular NFT platform. A full guide on how to mint NFTs for free on OpenSea is available here and it is important to note that OpenSea primarily supports the ERC-1155 standard. If you want an ERC-721 NFT, you can mint one on Mintable, another NFT platform, for free using this guide. ERC-721 is the older and most established standard for NFTs but ERC-1155 supports more token types and batch transfers. The NFTs you mint are held in custody in your MetaMask wallet and you can transfer them however you like. They are not attached to your OpenSea or Mintable collections or stores.
After you have minted your NFTs, you can create listings for them on your Dshop. To learn how to do this, you can refer back to the video guide, starting at 3 minutes into the video:
www.youtube.com
We recommend putting a link to the NFT (usually in the form of an Etherscan link) in the description for your Dshop NFT listing so potential buyers can verify its existence and scarcity. Alternatively, you can post the link to the NFT from OpenSea or Mintable or another platform. A buyer will be able to purchase your NFT from your Dshop listing and complete payment with ETH or ERC-20 tokens on Dshop. The buyer will enter their Ethereum wallet address as part of the checkout flow. You will then need to transfer the NFT to them via OpenSea or Mintable to complete the sale. You can do this simply by selecting the NFT and clicking “transfer” and inputting the destination wallet address of your buyer. This transfer will require paying a gas fee to the Ethereum network and is a separate transaction from the payment from the buyer. Congratulations, you are now a successful NFT creator and seller!
Conclusion
We hope you enjoyed this guide on how to get started with NFTs. As with any new technology, there is a bit of a learning curve involved. Thankfully, early adoption of decentralized technologies like NFTs is both fun and rewarding and we encourage you to give it a try.
Learn more about Origin
Google Cloud community articles and blogs
875 
1
875 claps
875 
1
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Cofounder Origin, early YouTube, avid traveler
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/firebase-developing-a-web-service-with-admin-sdk-flask-and-google-cloud-6fb97eb38b80?source=search_post---------114,"There are currently no responses for this story.
Be the first to respond.
Firebase Admin SDKs enable accessing Firebase from trusted environments. Therefore Admin SDKs can be used to interact with Firebase from various public and private clouds. Incidentally, curated cloud platform offerings like Google Compute Engine, Google App Engine and Google Cloud Functions are excellent deployment targets for server-side Firebase apps based on Admin SDKs. These cloud environments provide developers with a wide range of options with respect to scale, convenience, DevOps automation and cost.
In this post we will develop a simple web service using Admin Python SDK and Flask. Like in most server-side development projects, we will first code and test the app locally. Then we will conduct a production drill by deploying the code in Google Compute Engine — Google’s infrastructure as as service (IaaS) cloud. We will learn how to access Firebase from a server-side Python app, and how to authorize Firebase API calls made by the code deployed in Google Cloud Platform (GCP). We will also explore some of the potential pitfalls, and come up with solutions.
To start with we need a workstation with Python 2.7 or higher. I would also recommend using virtualenv to keep your project and its dependencies isolated from everything else. Execute the following commands in a Unix/Linux shell to start a new virtualenv sandbox, and install the required modules.
We use Google Application Default Credentials (ADC) to authorize the Firebase API calls made by our application. ADC is the recommended authorization mechanism for applications deployed in GCP. It saves us from having to hardcode credentials into the application, or ship a sensitive credentials file with the code. However, in order to locally test an application that uses ADC, we need to do a couple of things. First, download the service account credentials for your Firebase project. Then set the GOOGLE_APPLICATION_CREDENTIALS environment variable to point to the downloaded file.
Finally, we need to make sure that the Google Cloud SDK is installed in the system. This gives us the gcloud command-line tool. We will use it later to interact with Google Compute Engine.
Without further ado, lets go ahead and code our Python web service. If you are following along, you can simply copy the contents of listing 1 into a file named superheroes.py. Make sure to replace the <DB_NAME> placeholder at line 10, so it points to your Firebase database.
At line 9 of listing 1 we initialize the Firebase Admin SDK by calling initialize_app(). This is done outside of the functions, to ensure that it happens only once. Note that we are not passing any explicit credentials to initialize_app(). This prompts the SDK to look for ADC. Then we obtain a reference to a Realtime Database node called superheroes. All the data managed by our web service will be stored under this node. Next we have the four functions that constitute the public interface of our web service. Notice how they are mapped to different HTTP methods and URL paths using Flask decorators. Finally we have an internal helper function named _ensure_hero().
When deployed, the superheroes web service exposes four operations corresponding to the typical CRUD operations:
Now we are all set for a local trial run. So lets fire up that Flask!
This starts a web server that listens on port 5000. Lets try to create a new superhero in the database. Listing 2 shows the JSON payload that we will use. Simply copy it to a file named spiderman.json.
Now run the following curl command to POST the spiderman.json file to our web service. If all goes well it returns a 201 Created response containing a unique ID.
At this point you should also head back to the Firebase Console, and inspect the contents of your Realtime Database. A new entry is created under the superheroes/ path. You will notice that the key of the new entry is same as the ID returned by our web service. You can send this ID back to the web service to retrieve a superhero entry.
Feel free to also experiment with PUT and DELETE requests. See how each operation mutates the contents in the Firebase database. If you send a request with an invalid superhero ID, the server responds back with a 404 Not Found response. This check is implemented in the _ensure_hero() helper function in listing 1.
There’s a lot to consider when deploying a production web service to an IaaS cloud. How many VM instances to use? What is the size of a VM instance? How to handle load balancing? What sort of monitoring to use? How do we scale with varying load conditions? The list of questions can be truly staggering. For this reason, it is often simpler and better to deploy web-facing apps in a platform as a service (PaaS) cloud like Google App Engine. They handle all that heavy lifting for us (and then some). But IaaS has its uses, and purpose of this exercise is to see how we can access Firebase from Compute Engine. Therefore we will keep things absolutely simple, and just deploy our code to a single VM instance. In a future post I will discuss porting this web service to Google App Engine.
First thing’s first. We need to ensure that the gcloud command-line tool is configured to interact with the “correct” GCP project — especially if you have multiple projects in use. Recall that our application uses ADC to authorize Firebase interactions. For this to work, we should start our VM in the same GCP project as our Firebase database. A Firebase project is actually a GCP project in disguise — meaning when you created your Firebase project, you actually created a GCP project. We need to make sure that our Firebase database, and the Compute Engine VM instance reside in the same project. Understandably, ADC in project X cannot authorize access to a Firebase database in project Y — at least not without additional configuration. To specify the project for gcloud, find out your project ID from Firebase Console, and run the following command.
Now lets start a fresh Linux VM instance, and SSH into it. I will call this instance flask-demo.
Compute Engine instances are fairly minimalist. So you will have to do some work to get all the necessary software installed. Run the following commands in the VM.
A couple of things to note here. The package google-auth-oauthlib is not really required for our app. It was installed simply to overcome a crypto module loading issue. This is being fixed as I write this post, so chances are you won’t need it. Secondly, we install Flask globally so that we can run the flask command-line tool from the shell. Note that we are not using virtualenv here, but feel free to if you wish.
Once the VM instance is fully set up, copy the web service implementation over to the VM’s file system. You can scp the source file, or simply copy-and-paste the file contents manually since superheroes.py is small. In a real world setting you will have version control systems and other tools to handle such tasks. Finally, fire up that Flask!
We haven’t configured the firewall of the VM yet. Therefore we cannot test this by sending requests remotely. But we should be able to run a curl command on the VM itself. When we do, however, we get this.
The Flask server also logs something like the following.
Oops! What happened? It seems our web service received a 401 Unauthorized response from the Firebase database. In other words, the ADC on our VM instance failed to authorize the Firebase database call. But why?
To fully comprehend what’s going on, we need to have a rough idea of what OAuth2 scopes are.
Scopes provide a way to restrict the level of access a user gains from an OAuth2 token. By scoping a token down to a few services, a service provider can ensure that the bearer of the token can only access those services. This is quite useful in an environment like GCP where there are dozens of services. Scopes constitute a layer of defense against accidental or malicious abuse of cloud services. They also facilitate authorizing clients in a more fine-grained manner. For instance you can have a user with access to only Cloud Storage, and a user with access to only Cloud Firestore in the same GCP project. This is achieved by including the appropriate subset of scopes in the tokens issued to the two users.
Various GCP services and the OAuth2 scopes required to access them are documented here. To access Firebase database, client applications must present an OAuth2 token with the following two scopes:
However, ADC in Compute Engine does not add these scopes to the issued tokens. Consequently, Firebase database rejects the requests made by our web service in the cloud. This in turns results in the error we have seen.
Each VM instance in Compute Engine has a fixed set of scopes associated with it. The ADC on a VM instance will only include these scopes in the OAuth2 tokens. Therefore the solution to our problem is to configure the VM instance with the required scopes. We can check what scopes it currently has by running the following command.
The serviceAccounts section in the output shows the OAuth2 scopes currently set in the VM. We can see that a required Firebase scope is missing. So we need to stop the VM instance, add the required scopes, and start it again.
It is also possible to create a VM with the necessary scopes by passing the --scopes flag to the instance creation command. That can help us avoid many of these additional steps.
Now we are ready for another test run in the cloud. Start the Flask app as usual, and try sending some requests. Everything should work out fine this time around. ADC in the reconfigured VM should make the required scopes available in the OAuth2 tokens.
You may also be interested in sending requests to the superheroes web service from a remote machine. To enable this, we need to do two things.
Now our superheroes web service in the cloud can be accessed from anywhere, using the public IP address of the VM instance.
We successfully developed and deployed a server-side Firebase app in Google Compute Engine. Ours was a simple web service based on Python and Flask, but it could have been anything. Possibilities include:
The development and deployment process we went through in this example was pretty straightforward. There were only two things to be mindful of, and both were related to our use of ADC:
I will soon write a post about deploying this application to Google App Engine. You will see that due to the higher level of abstraction, the deployment process is even simpler there. In the meantime you can find all the source and support material related to this mini project on GitHub.
I hope this gave you an idea of the type of things that can be built using the Firebase Admin SDKs, and how to deploy such applications in Google Cloud. As always I’m open to comments and questions. I’m also eager to learn what you are planning to build with Firebase and Google Cloud.
Google Cloud community articles and blogs
264 
1
264 claps
264 
1
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Software engineer at Google. Enjoys working at the intersection of cloud, mobile and programming languages. Fan of all things tech and open source.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://itnext.io/what-i-learned-switching-from-circleci-to-google-cloud-build-b4405de2be38?source=search_post---------115,"Earlier this month we finally hit CircleCI’s generous 1500 minute per month build time limit.
As the founder of a bootstrapped startup (see Focuster) I’m super grateful for CircleCI over the years because it has been a great build service and they were very generous with their limits.
But every new expense feels like a punch to face.
We recently migrated Focuster’s infrastructure over to Google’s Kubernetes service (GKE) recently and I noticed they had their own build service, Cloud Build (GCB).
Though it wasn’t as comprehensive in many ways, it had some clear advantages for me at this point.
I wanted to share what I learned in the migration so that others might benefit and hopefully Google will listen to some of my feedback and make some improvements.
As I mentioned earlier, CircleCI provides 1500 of minutes per month on their free tier with a concurrency limit of 1. To move to the next level of service is a flat rate of $50/mo. for 2 build concurrency with unlimited builds.
Google Cloud Build on the other hand provides a free tier of 120 build minutes per day on their standard size container which is an n1-standard-1 machine type (1 core, 3.75gb RAM).
This works out to approx. 3600 build/minutes per month. That said the time for unused days doesn’t roll over so a particular heavy day may incur some charges.
Above the free tier the pricing is only $0.003 per build/minute. My builds are averaging about 10 minutes so I’d have to do over 1667 builds a month, or 50 builds a day to hit the $50 that CircleCI charges.
CircleCI basically charges you based on your concurrency level and gives you unlimited minutes. GCB has a practical limit of 10 concurrent builds but charges you per build/minute. I like this approach at this point because I can get the benefits of the extra concurrency while on the free tier.
Both CircleCI and GCB provide tools to test your deploys locally using docker. Last time I tried this with CircleCI on OSX it was horribly slow to the point where it was unusable. I’m not sure if that problem was ever resolved or not.
GCB provides tools for local testing that are distributed with the Google Cloud SDK via the cloud-build-local command.
Using this tool I was able to iterate on my build steps quickly without using up any time on the cloud service.
I didn’t actually test that this is true or not but I assumed that being in Google’s network meant that pulling a docker image from GCR to GCB would be faster than from CircleCI.
For deployment to my Kubernetes cluster I use a tool called Keel to automatically update my deployments when the image in the Docker repository is updated.
In the past I used a polling system with Docker because the web hooks were a bit of a pain to setup. I found setup of the GCR PubSub notifications to be really easy and this made deploys up to a minute faster.
Both CircleCI and Google Cloud Build have extensive Docker support. But Google Cloud Build lets you define your own steps using Docker images.
A shared workspace (mounted at /workspace) is persisted between steps.
I found this actually a pretty nice way to develop my build steps because I could leverage small purpose-built images for each step.
I could also leverage existing Docker images at any point in the build process and I can write my build steps in any language I want.
GCB provides existing steps for many of the most common tools: docker, git, Google’s services, curl, npm, kubectl, etc.
Complete list here: https://github.com/GoogleCloudPlatform/cloud-builders
But for doing something as simple as initializing git submodules was difficult. I had to create my own build step for that. It would be great if they provided an easy to use build step that would pull the ssh key out of Google’s KMS.
CircleCI has the advantage here as they support adding SSH keys to the build account and because their build is all running in a single container its much easier to configure.
Another area where CircleCI shines compared to GCB is in the caching story.
They provide some caching primitives that are quite flexible in terms of dynamically generating a cache key based on the hash of a file’s contents for example. And allowing fallback to an older cache based on the branch name or many other criteria.
GCB on the other hand only provided documentation on storing some folder contents in Google Storage.
The good news, I was able to create my own build step that essentially mimicked CircleCI’s behavior in GCB.
(I will open source the caching steps I created if there’s interest)
CircleCI makes it really easy to SSH into your build to diagnose a problem by simply checking off a box on the build and restarting it.
With GCB I couldn’t find any comparable step.
It might be possible to use docker run in local debugging to open a shell in a particular image but it’s not clear to me how to preserve the workspace of the build in this case.
Setting up Slack and Github integration is based on manual installation of some Google Cloud Functions, rather than a 1-click that most services use.
That said, the use of the PubSub for build notifications allows for a lot of customization.
Right now you can only choose between 1 core, 8 core or 32 cores. Would be nice to have 2 and 4 cores as options. Could we get a n1-standard-2 or other sizes for build machines?
Having made the switch I am overall pretty happy with the results. I hope that Google keeps making regular updates to this service.
—
This is part of a series of articles about Focuster’s engineering practices.
Check out Focuster, the first focus app that actually helps you get things done instead of making endless lists. Automatically build a schedule from your to-do list in Google Calendar, and move things forward if they don’t get done.
Follow @focusterapp on Twitter to get updates on productivity and high performance.
ITNEXT is a platform for IT developers & software engineers…
260 
6
260 claps
260 
6
Written by
Founder @ Focuster.com–get 10x more done with an auto-scheduled to-do list
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
Written by
Founder @ Focuster.com–get 10x more done with an auto-scheduled to-do list
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
"
https://medium.com/google-cloud/google-cloud-load-balancer-setup-tweaking-and-observations-c12d704e6d52?source=search_post---------116,"There are currently no responses for this story.
Be the first to respond.
The Google Cloud Load Balancer (GCLB) is a software defined globally distributed load balancing service. It enables GCP users to distribute applications across the world and scale compute up and down with very little configuration and cost. It allows for 0 to 1 million requests per second (rps) with no pre-warming. Pricing starts at $.025/hr or $0.6/day or $18/month for global anycast (single IP) with autoscaling. It’s low cost and insanely powerful technology available for anyone to test today for free.
In this article I will test and demonstrate some of the product features highlighted on https://cloud.google.com/load-balancing/
There are a few deployment types for the GCP load balancer service.Global external:1. HTTP(S) — Public only, single or multi-region2. SSL proxy, TCP Proxy — Public only, single or multi-regionRegional external:3. Network — Public, TCP/UDP, single regionRegional internal:4. Network — Internal only, TCP/UDP. Can be used in-conjunction with #1 HTTP GCLB.
In this article we will test #1 HTTP public load balancing.
The GCLB can help improve end user latency with the power of Google’s huge network. Check this article here for more detail measuring and comparing latency with different GCLB deployment types.
Health checks, backends, and front ends are the core elements of the GCLB. Focus on these if you are looking to setup load balancing quickly.
Health checksThere are 2 types of health checks you should be aware of. #1 is required by the load balancer and #2 is extra to ensure health of instance groups (autohealing).1. Used by the load balancer that ensures that requests are only sent to instances that are up and running2. Used by instance groups to recreate instances that are unhealthy by your set criteria. Ex: HTTP health check port 80; if webserver fails, instance is terminated and recreated.
Health checks are setup within the Compute Engine of the UI as they are used in more than one place in GCP. You can run health checks on instance groups without the load balancer as well. If the healthcheck notices an instance is not healthy it will terminate and recreate.
The GCLB will manage instance health for you and the instance group will manage autoscaling so make sure you have both configured and tuned per your requirements. You can keep them basic; HTTP port 80, if the web server or instance fails the instance group will terminate and recreate a new one.
Backend Services These are the services that direct incoming traffic to instance groups. The backend service for a single load balancer can contain a number of backends or buckets. You can have 1 backend service going to multiple instance groups. Each backend is a really a link to the instance groups you wish to distribute traffic between.
So say you plan to have 3 instance groups (US, EMEA, ASIA) setup behind the HTTP(S) load balancer, they are configured 1 backend service. Here is how the back end service looks configured with 3 instance groups (backends) using the health TCP health check we created earlier.
Host and path rulesThis is where you can send subdomains or different hosts to different back ends. Example, if I had eu.mydomain.com I could send those requests directly to my autocomplete-demo-eu instance group. Host and path rules are automatically configured for * to send to the backend service you create so you do not have to worry about configuring these if you are doing basic http load balancing.
FrontendsThe front end is your virtual IP (VIP) or called anycast IP in GCP. One front end can service multiple regions (backends). In most cases you would want a static or reserved IP and not the default ephemeral. This way you can easily point an a-record on your Cloud DNS zone file to your load balancer IP.
That’s it for configuration of the GCP load balancer. Easy huh? After you review and update configuration in a few minutes you’ll have traffic going through your front end to your instance groups.
Health checksThe portal provides a ⚠️ suggestion after a health check is added to an instance group. The warning provided is that you may want to increase the check interval and unhealthy threshold as shown below. This is working to help you reduce the amount of false positives that may come with services under heavy load. Consider changing your check interval from 2s to 10s, and unhealthy threshold from 2 attempts to 6 attempts if you are just getting started.
Initial delayThis is a setting on the instance groups configuration that specifies the delay after an instance has been replaced for the health check to run again. Default is 300 sec which is 5 minutes. Hopefully your instances are light enough to boot in or around a 1 minute. If you think you may need more time for your application to come online after initial boot by the instance group autoscale action, then increase the initial delay.
RoutingIncoming client requests are sent to to the region closest to the use assuming the region has capacity. If more than one zone is configured with backends, traffic is distributed across each zone. Check my behavior observations below for more details. Within the zone requests are distributed via round robin. You can override round robin in zone by configuring affinity.
AffinityTypically the LB is going to route new requests to any instance and traffic from one connection is going to route to the same instance. Say you want to set stickiness to make sure all connections from one client go to the same instance. Configure session affinity to client IP. You can also set by cookie. The GCLB sends a cookie on the first client request and future incoming requests with that cookie will be sent to the same instance.
Scaling instances back or maintenanceEasy, just drop an instance group’s number of instances. GCE takes all instances in that group offline and you are no longer charged. Now you can scale back or perform maintenance, maybe change an instance template to a new version of software in a region. Take a region out of service by removing the instance group from the load balancer back end to perform maintenance, test, or upgrade software. Then re-add that group and push changes across other regions.
Minimum sizingI noticed the GCLB performed best with 3–4 minimum instances if using a single instance group as a backend. There is a slight lag for the LB to pickup the new instance information so its probably not the best idea to run a backend with 1 instance group with 2 instances at a very minimum. Multiple instance groups should be fine with 1–2 instances, just be aware and test the slight lag with the GCLB to pickup new instances added to groups. Tweak the initial delay threshold if you need to run a small amount of instances.
IPv6 SupportYou an attach a IPv6 Ip to a GCLB and have the same type of routing globally as you would with IPv4. One strategy would be to configure the GCLB with an IPv6 address to handle all IPv6 traffic. Just create an additional forwarding rule with the IPv6 address. Then you can associate both IPv6 and IPv4 with the same load balancer and back end instances. More on IPv6 support here.
The GCLB is a managed service and intended to make intelligent decisions on routing and traffic shaping for you. Below are a few specific service behaviors observed for basic http load balancing.
I used Apache Bench for connection testing from a US and EU instance. Apache Bench (ab) is included in the apache2 package. More info on Apache Bench testing here.
Location based routing and traffic load distributionFirst test was sending 10,000 requests with 100 concurrent from an ab-tester US instance and the GCLB routed directly to my us-east1 instance group. So the GCLB routing works great ✔️.
Increasing the requests from 10,000 to 100,000 from an ab-tester in EU this time, initially routed to the europe-west1 instance group (blue), then distributed across all 3 instance groups to better handle the load. At the same time my instance group in EU scaled up to 7 instances to accept the bulk of the traffic. Intelligent load balancing with minimal configuration and seamless autoscaling works great ✔️.
About 30 minutes after the 100,000 query spike had long passed and the instance group started to stabilize and scale down gracefully.
Scale resources up and down with intelligent autoscaling ✔️.
I was going to stress test the GCLB up to 1 million rps just to see it perform with my own eyes. After doing more testing in excess of 500,000 rps instances I found the same behavior was demonstrated: flawlessly routing and balancing of traffic across regions. I had quickly gained trust in the service and saved myself the testing costs. Saved money by trusting the platform. ✔️
FWIW, here is the diagram of the HTTP load balancing example put together showing some of the elements covered in this post and where each element lives.
If you wish to learn more about the GCLB please check out the Google Cloud Next ’17 session given by Prajakta Joshi, PM for the networking team on GCP here. She does an excellent job explaining the services background and features in detail.
Special thanks to Prajakta and Jens for providing feedback on this article.
Thanks for reading and have fun with GCP!
Google Cloud community articles and blogs
217 
6
217 claps
217 
6
Written by
Customer Engineer, Google Cloud. All views and opinions are my own. @mkahn5
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Customer Engineer, Google Cloud. All views and opinions are my own. @mkahn5
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/rar-design/%E5%8D%81%E5%88%86%E9%90%98%E7%94%A8-google-cloud-platform-gcp-%E5%85%8D%E8%B2%BB%E9%A1%8D%E5%BA%A6%E6%9E%B6%E8%A8%AD-wordpress-%E7%B6%B2%E7%AB%99-ae5413d4805a?source=search_post---------117,"There are currently no responses for this story.
Be the first to respond.
可以架設 WordPress 網站的主機有很多家，但 Google 是我體驗過效能不錯且機制友善的方案，若讀者是第一次摸 WordPress，相當推薦！
我們可以透過目前 Google 所提供的 $300 美金額度，在一年內可以體驗使用，這篇文章主要是透過圖文方式，快速地示範如何用這免費額度，來架設我們的第一個 WordPress 網站。
若對於這系列文章有興趣，可以發摟 林育正 Riven 或是透過 Facebook 臉書 聯繫我 😃
事前只要預先準備 Google 帳號，以及一張信用卡就可以囉。並點擊以下網頁開始申請：
個人使用練習的帳戶類型，選擇「未登記稅藉的個人」也會比較單純。
如果已經是公司等商業專案用途，記得確實填寫噢～
透過 Google 的快速安裝服務，立刻取得 WordPress 網站。
找到「WordPress Certified by Bitnami」然後按「在 COMPUTE ENGINE 上啟動」。
接著呢我們來開始建立這項部署作業⋯
這邊只要輸入部署名稱之後呢，按照上圖的設定即可。
其中asia-east-a、asia-east-b、asia-east-c 是位在台灣彰化的主機，較推薦。
Machine type 選擇 小型(1個共用)
Boot disk type 選 SSD ，並輸入 30GB 的容量。
完成設定之後按下「部署」就可以囉！
如此一來就完成了無痛在 GCP 上架設 WordPress 網頁啦，如果還算受用的話請可以拍手鼓勵最多 50 下！
改天有空再分享後台操作～​若對於這系列文章有興趣，可以發摟 林育正 Riven 或是透過 Facebook 臉書 聯繫我 🙂
設計攻略
959 
959 claps
959 
Written by
是數位遊牧型態的設計師💻 喜歡邊旅行邊工作的生活，逐網路、插座與咖啡而居。期待能夠將艱難的設計與開發技術，用麻瓜都能夠聽得懂的話，說給每一個人聽。更多關於我：riven.design
關於設計的技巧 × 知識 × 方法 × 教學 × 攻略
Written by
是數位遊牧型態的設計師💻 喜歡邊旅行邊工作的生活，逐網路、插座與咖啡而居。期待能夠將艱難的設計與開發技術，用麻瓜都能夠聽得懂的話，說給每一個人聽。更多關於我：riven.design
關於設計的技巧 × 知識 × 方法 × 教學 × 攻略
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/docker-swarm-on-google-cloud-platform-c9925bd7863c?source=search_post---------118,"There are currently no responses for this story.
Be the first to respond.
There are some interesting things going on with the new Docker 1.12 release. Docker is bundling Swarm into Docker itself, as well as upgrading Swarm with more mature container orchestration abilities.
Swarm now joins Kubernetes, Mesos, and Nomad as a fully-fledged orchestration engine. With these new orchestration abilities, I wanted to take another look at Swarm. I also like the name “Swarm” ;)
Swarm does not yet have an out-of-the-box option on Google Cloud. Hopefully this is added soon, but until then let’s look at how you can manually set up a Swarm cluster on Google Cloud Platform.
All in all, you should have a fully functional Swarm cluster in about 15 minutes!
In fact, Google Compute Engine is so fast at provisioning VMs that if you run the node creation and setup in parallel you can create a cluster in around five minutes! Crazy fast!
Use this script for a fully automated install!
Note: Swarm 1.12 is still very young. Documentation is sparse and still evolving. I expect things to get better very soon!
You need Docker 1.12 to get the upgraded Swarm features. Here is what I have:
You also need the Google Cloud SDK installed, as well as a Google Cloud project. Once you do that, make sure you log in:
The first step is to create the Swarm nodes.
Ideally you would use a Managed Instance Group, but I’ll save that for another tutorial.
Let us create a manager node and a worker node.
Use docker-machine to create the manager node:
Replace <YOUR_PROJECT_ID> with your project ID. Also feel free to change the zone, machine type, and disk size. The important thing to do is tag this instance with the “swarm-cluster” tag, which will let us open firewall ports later on.
In about five minutes the manager will be created.
Now we need to set up this machine as a Swarm manager
Your manager is now created!
Create a worker node the same way as the manager:
Now we need to get the IP address of the manager so we can join the Swarm.
Use the internal IP for the Swarm Manager to connect your worker to the Swarm. The default networking setting opens all ports on the internal subnet so you don’t have to mess with firewall rules.
Repeat these steps to add more workers to the Swarm.
Log back into the Swarm Manager to start executing commands.
For example, you can see all the nodes in the Swarm:
You are done with the cluster setup!
Creating a service is straightforward. It’s basically the same commands you would use in normal Docker.
For example, to start a single nginx server on port 80, run:
And we can see the process running.
Swarm will make sure that the replica is always running. We can also scale up and down the number of replicas with a single command:
Pretty cool stuff! There are more complicated things you can do, but documentation is sparse. The launch blog post has the best information so far.
Now you have nginx running in your Swarm, you have to open it up to the outside world. By default, Swarm will expose the service on the specified port on every node in the Swarm. This is very similar to creating a Service in Kubernetes with NodePort. We need to expose this port!
Side Note: I really hope Docker adds in native support for Google Cloud Platform so that these things are automatic, similar to how they function in Kubernetes.
The easiest thing to do is open the port on a node, and use the node IP address for your website or service.
I would use this option for small clusters that have a single manager and a few workers.
Open the port on the Swarm instances
Now get the external IP address of the nodes.
Use the external IP of one of the nodes to reach your service. I recommend using the manager’s IP address.
Using the same steps as option one, you can use all the external IP addresses with Round Robin DNS. This basically gives you a form of load balancing for free! The only problem with this is if you start removing or adding nodes in your cluster, you need to update your DNS settings every time. DNS clients also cache heavily, so if you scale down it is possible your users will hit nodes that no longer exist.
If you have multiple managers, I would use this method to provide a simple form of load balancing and fault tolerance.
This is the most robust, but also the most complicated, method for exposing your service. When you create a Network Load Balancer, you get a single IP address, but traffic is sent to all the nodes in the Swarm. Additionally, you can set up a health check so if a node goes down, traffic is not sent to it.
If you want the best reliability, or have larger cluster that may be spread over multiple zones for high availability, I recommend this option.
I will set up a TCP Load Balancer. You can also set up a more powerful HTTPS Load Balancer if that is appropriate for your service.
While you can do this on the command line, I find the UI to be more intuitive.
Open the Load Balancer page.
Click “Create Load Balancer”
Now click “Start configuration” for the TCP Load Balancer
Give your Load Balancer a name
Now click “Backend configuration”
Select the region your Swarm is in, then click “Select existing instances.” Add all the Swarm nodes.
Now create a health check.
Give your health check a name, and configure the numbers how you see fit. I used the defaults. We are going to ping port 80 (where the nginx service lives) every 5 seconds to make sure the node is healthy. Save and continue.
Now go to the Frontend Configuration, and specify the port for your Swarm service.
Finally, click “Create,” and the Load Balancer will spin up in a few minutes.
You can get the IP address for your Load Balancer with this command:
With the new 1.12 release of Docker, Swarm was very easy to setup and use. Once it is officially released, it will be even easier. Great job by the Docker team!
I hope Docker adds more documentation and examples, and I really hope they add support for native Google Cloud features so people don’t need to mess with Firewalls and Load Balancers!
I also plan on doing a comparison between Swarm and Kubernetes. I found there to be a lot of differences and similarities. Stay tuned for that!
Google Cloud community articles and blogs
189 
14
189 claps
189 
14
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/envoy-nginx-apache-http-structured-logs-with-google-cloud-logging-91fe5965badf?source=search_post---------119,"There are currently no responses for this story.
Be the first to respond.
Google Cloud Logging provides several plugins that allows you to easily emit structured logs for common applications.
For example, if you install the Stackdriver Logging agent, you can get logs using the following fluentd plugins
This sample demonstrates two of these plugins (apache and nginx) and how to configure them to emit not just structured JSON logs but as a specific HttpRequest protocol buffer. This article also describes how to configure Envoy proxy for similar httpRequest logging.
The script below sets up the full sample stack with Google Cloud L7 HTTPS loadbalancer and a managed instance group running the webserver. It does not demo Envoy with the L7 but you can set that up pretty easily with the template below.
In addition to the HttpRequest protocolbuffer, this example and configuration also includes the following in the emitted LogEntry
Note, Stackdriver doens’t by default support all fluentd plugins but just the ones listed above. You are free to customer and define your own plugin described in the documentation in the following article auditd agent config for Stackdriver Logging
For Reference:
First, i’ll just show what this will look like in nginx-access logs:
Now that the logs are structured, we can apply a direct filter on those fields. For example, the following shows which requests took more than 2s to respond:
Each request in this specific example traverses Google L7 loadbalancer which also get logged. What that means is you can now trace a request in the LB logs down to the specific instnace in the managed group that handled the call. For example, if you start with a traceID from the nginx logs
You can find its corresponding entry in the LB LogEntry
or run a query like this to cover both.
Now that the frontend server is logging the inital httpRequest as well as the trace/spanId, a backend application that emits the traceId in a log line will result in log grouping. What that means is the inital request and application logs are grouped in a parent/child format as described in the following articles:
And just for reference (and becasue i authored it, :), here is the same impelmented within a webframework (Flask), directly
This article does not demonstrate this capability but if its useful, i can be convinced to provide an example.
If requests traverse the Google Loadbalancer, it injects the standard X-Forwarded-For header which includs the actual source IP address as well as the proxies this request traversed. In the case for Google, it will include the incident SSL Proxy.
For example
The origin ip is actually 73.162.112.208. Currently, the parsers just uses the provided value as the clientIP which is not the derived origin value (its still the loadbalancer). If you want to parse the actual origin IP, you will need to parse out the value from the header and add it to for the remoteIP.
For apache, its something like this:
parser_apache.rb:
apache.conf
You can find the full source here:
github.com
If you prefer to use nginx, you can create the full LB->nginx managed instance group by running the following commands in sequnce:
If can use the certs provided in this repo but if you prefer to setup your own…
then finish off the step:
You should end up with an L7 LB pointing to an instance group with four servers
NOTE: it may take upto 10 minutes for the L7 LB to provide the initial provisioning so feel free to get a coffee now
Once the LB is seutp you can send traffic down
The responses above indicate how long the specific backend took. As you see, since we applied a variable delay into each instance as part of the startup script, each VM will take a minimum amount of time to respond
To delete the cluster, run the following in sequence:
The following details the configurations used. Eventually, this should also get rolled into the standard google-fluentd library so wont' be a need to add the custom code below.
If you use Apache2, you can create the full stack by running the commands below in sequence.
You can use the certs provided in this repo but if you prefer to setup your own…
then
As with the example with nginx, you can send requests directly via curl in loop and gauge response times.
The following details the configurations used for apache. Eventually, this should also get rolled into the standard google-fluentd library so wont' be a need to add the custom code below.
/etc/apache2/apache2.conf
/etc/apache2/sites-enabled/000-default.conf
/opt/google-fluentd/embedded/lib/ruby/gems/2.4.0/gems/fluentd-1.2.5/lib/fluent/plugin/parser_apache2.rb
/etc/google-fluentd/config.d/apache.conf
Envoy Access logs is fairly customizable and can write to any number of targets. For example, you can configure envoy to emit logs to remotely (see envoy_control#accesslog) or in this article, locally to a log file where Cloud Logging can do the rest of the legwork.
This article includes two separate configurations for envoy and google-fluentd: Default envoy settings and one optimized for GCP that includes the cloud-trace-context header.
Update: 1/6/19: Authored the fluentd plugin:
github.com
The steps to emit default logs is fairly easy: just set the path where the logs will write to
You can customize envoy’s logs easily by adding in fields like custom headers into the log file. The following shows a snippet on now to setup the logs that extend the default configuration and just adds %REQ(X-Cloud-Trace-Context)%
To test this, you can incorporate the envoy configurations provided into a similar VM as described above. Remember to copy
If you are on a VM with google-fluentd and the configurations settings above, restart fluentd and run envoy:
Either send in a request via the LB or directly to test:
In cloud logging, you will see the standard httpRequest payload as well as envoy specific headers:
From there, you can further customize the headers you want to caputure by modifying the fluentd and envoy configurations.
Thats all folks
Google Cloud community articles and blogs
181 
2
181 claps
181 
2
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/building-token-recommender-in-google-cloud-platform-1be5a54698eb?source=search_post---------120,"There are currently no responses for this story.
Be the first to respond.
In this article I will guide you through the process of creating an ERC20 token recommendation system built with TensorFlow, Cloud Machine Learning Engine, Cloud Endpoints, and App Engine. The solution is based on the tutorial article by Google. The data used for training the recommendation system is taken from our public Ethereum dataset in BigQuery.
The article is broken down into the following parts:
The collaborative filtering technique is a powerful method for generating user recommendations. Collaborative filtering relies only on observed user behavior to make recommendations — no profile data or content access is necessary.
The technique is based on the following observations:
The collaborative filtering problem can be solved using matrix factorization. Suppose you have a matrix consisting of user IDs and their interactions with your products. Each row corresponds to a unique user, and each column corresponds to an item. The item could be a product in a catalog, an article, or a token. Each entry in the matrix captures a user’s rating or preference for a single item. The rating could be explicit, directly generated by user feedback, or it could be implicit, based on user purchases or the number of interactions with an article or a token.
The matrix factorization method assumes that there is a set of attributes common to all items, with items differing in the degree to which they express these attributes. Furthermore, the matrix factorization method assumes that each user has their own expression for each of these attributes, independent of the items. In this way, a user’s item rating can be approximated by summing the user’s strength for each attribute weighted by the degree to which the item expresses this attribute. These attributes are sometimes called hidden or latent factors.
To translate the existence of latent factors into the matrix of ratings, you do this: for a set of users U of size u and items I of size i, you pick an arbitrary number k of latent factors and factorize the large matrix R into two much smaller matrices X (the “row factor”) and Y (the “column factor”). Matrix X has dimension u × k, and Y has dimension k × i.
To calculate the rating of user u for item i, you take the dot product of the two vectors. The loss function can be defined as Root-mean-square error (RMSE) between the actual rating and the rating calculated from the latent factors.
For our token recommender we will use the percentage of supply a user is holding for a particular token, as the implicit rating in the user rating matrix.
Check out the code and install the dependencies:
Query token ratings from BigQuery:
Run the following query in BigQuery and export the results to a GCS bucket e.g.gs://your_bucket/data/token_balances.csv
The above SQL queries top 1000 tokens by transfers count, calculates the balances for each token, and outputs (token_address, user_address, rating) triples. Rating there is calculated as the percentage of supply held by the user. This filter — where balance/supply * 100 > 0.001 — prevents airdrops appearing in the result.
Understand the code structure
The model code is contained in the wals_ml_engine directory. The code's high-level functionality is implemented by the following files:
The csv file is loaded in the model.py file:
Then the following arrays are created:
These triplets are then randomly split into test and train datasets and converted to sparse matrices:
The WALS model is created in wals_model method in wals.py, and the factorization is done in simple_train method in the same file. The result are the row and column factors in numpy format.
Train the model locally and in Google ML Engine
To train the model locally run the following command, specifying the path to the CSV file exported on the previous step:
The output should look like the following:
The RMSE corresponds to the average error in the predicted ratings compared to the test set. On average, each rating produced by the algorithm is within ± 0.95 percentage points of the actual user rating in the test set. The WALS algorithm performs much better with tuned hyperparameters, as shown in the following section.
To run it in Cloud ML Engine:
You can monitor the status and output of the job on the Jobs page of the ML Engine section of the GCP Console. Click Logs to view the job output.
After factorization, the factor matrices are saved in four separate files in numpy format so they can be used to perform recommendations:
When training locally, you can find those files under wals_ml_engine/jobs directory.
To test out the recommendations use the following code:
You can find the configuration file for hyperparameters tuning here.
To tune the hyperparameters, first change the BUCKET variable in mltrain.sh to your bucket. Then run the following command:
You can see the progress of tuning in Cloud ML Engine console. The results of hyperparameter tuning are stored in the Cloud ML Engine job data, which you can access in the Jobs page. The job results include the best RMSE score across all trials of the summary metric.
Below are the best parameters from my tuning, which you can also find in the repository:
The error though is just slightly lower comparing to the default parameters:
You can find REST API definition in Swagger format for serving token recommendations in the repository: openapi.yaml. The implementation of the API for App Engine is in the main.py file.
First, prepare to deploy the API endpoint service:
The output of this command should look like the following:
Run the provided command:
Create the bucket where the app will read the model from:
Upload the token_balances.csv file to the bucket:
Train the model and upload the model files to the bucket:
Create an App Engine application:
Prepare to deploy the App Engine application:
The following output appears:
Run the provided command:
After the app is deployed you will be able to query the api: https://${project_id}.appspot.com/recommendation?user_address=0x8c373ed467f3eabefd8633b52f4e1b2df00c9fe8&num_recs=5 (replace ${project_id} with your value)
You can try out the recommendations at similarcoins.com. Read our article where we describe how we evaluated and improved the recommendation system:
towardsdatascience.com
Google Cloud community articles and blogs
275 
1
275 claps
275 
1
Written by
Creator of https://github.com/blockchain-etl, Co-founder of https://d5.ai and https://nansen.ai, Google Cloud GDE, AWS Certified Solutions Architect
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Creator of https://github.com/blockchain-etl, Co-founder of https://d5.ai and https://nansen.ai, Google Cloud GDE, AWS Certified Solutions Architect
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/using-the-google-cloud-vision-api-with-node-js-194e507afbd8?source=search_post---------121,"There are currently no responses for this story.
Be the first to respond.
You might have heard about the new Google Cloud Vision API. If you haven’t, you should check it out. It lets you upload an image and get a TON of machine-learning based information out of it, including landmark detection, face detection, emotion detection, adult content detection, and even OCR.
My favorite feature has to be the label detection. Give Cloud Vision an image, and it will tell you what’s in the image!
This makes things that were previously impossible super easy to do!
To use the API, you need to enable the API:
The easiest was to use the Cloud Vision API is the gcloud npm module.
Be sure to create a Service Account and download the JSON keyfile.
Yeah seriously that’s it! There are functions that target all the different features of the Cloud Vision API, and there is the generic “detect” function that can do multiple types of detection at once!
Here is a sample app I made. It creates a little Express server that lets you upload images and have them analyzed by the Cloud Vision API.
github.com
Take a look at Line 52. You can specify what things the Cloud Vision API should detect, such as faces, landmarks, labels, logos, properties, adult content, and text! This enables you to save costs by only detecting the features you need.
I uploaded an image of myself from a few years ago:
Crazy!
Check out the docs for more info. The API gives you a LOT of detailed information if you want it; I only scratched the surface!
Google Cloud community articles and blogs
332 
11
332 claps
332 
11
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@andyhume/building-a-rest-api-with-google-cloud-functions-e0acdf1b2620?source=search_post---------122,"Sign in
There are currently no responses for this story.
Be the first to respond.
Andy Hume
Nov 29, 2017·5 min read
As part of my work at Brandwatch we have been evaluating Google’s serverless cloud product known as Google Cloud Functions. The bulk of testing has been with internal tooling, but more recently it has been used on a small scale in production for small components of customer facing applications.
I’m an advocate for serverless platforms like Lambda, Azure Functions, and Cloud Functions. There are a large amount of compute use-cases at a company like Brandwatch where there are excellent advantages in having a totally managed platform, providing isolated function deployments, with a pay-per-invocation pricing model.
More generally I‘m excited about serverless (functions as a service specifically) as an extension of certain cloud native principles in general: moving the unit of compute, deployment, and cost, to smaller and smaller components. Namely from bare metal, to VMs, to containers, and now to functions.
Google Cloud Functions (GCF) is raw compared to the industry leader AWS Lambda. However, Brandwatch are a customer of Google Cloud, and as such some of our infrastructure sits in their data centres, and we have an interest in evaluating the potential of GCF as a low friction FaaS option.
My team has been working on a tightly scoped internal API which seemed like a good candidate for experimenting with GCF. We’d initially planned to write an ExpressJS application and run it on AppEngine, so moving to GCF (which heavily leverages the Express APIs internally) was a low friction decision from a code point of view. Here’s what we learnt.
GCF is badly missing an HTTP gateway layer. Google has a product called Cloud Endpoints which can sit in front of raw compute instances, App Engine, or Kubernetes Engine; but it doesn’t yet integrate with Cloud Functions. It’s an obvious fit for it to do so, and I assume Google are planning something in this area.
The key aspect of this will be integration. GCF currently makes it very easy to deploy a function exposed on an HTTP endpoint, and it would be a shame for that ease of use to get tangled up with Endpoints/Swagger configuration. This complexity in Lambda is one of the reasons frameworks like Serverless and .architect exist — and it would be nice to see Google solve that with a strong integration between Endpoints and Cloud Functions.
Due to the lack of a gateway layer, routing for HTTP triggers in GCF is quite blunt. You can’t route by HTTP method, or by URL path, meaning that you end up almost completely reimplementing routing inside your functions.
It also means that to design a RESTful style API you end up implementing multiple operations inside a single cloud function, which somewhat breaks the principle of small deployable units. For example, our design for the API endpoints initially looked like
The best routing match that GCF offers for this collection of endpoints is /objects`. Which means using a single cloud function deployment to match those four operations. We may as well have deployed an express app.
In fact, for early iterations I did end up requiring ExpressJS into the function to handle method/path routing. Eventually we decided that the smaller discreet function per operation was a more important aspect of the design than the RESTful properties of the API design. The endpoints now look like…
This is far from ideal, but I’m happy with the trade-off at this point, and hope that any future Cloud Endpoints integration solves this problem.
GCF provides no native mechanism for easily loading externally managed key/values into a function. This is something that is also surely on the roadmap, as this pattern is extremely well established, particularly in the PaaS ecosystem (twelve-factor apps, etc).
It’s possible to work around this of course, reading different files off disk for different environments. But implementing this adds un-needed complexity in our own application code, when the feature should be built in at a platform level.
Another feature required here is the ability to load in secrets (e.g. secure tokens/keys) from a location outside of the deployed source code.
I would like to be able to trigger HTTP functions without going through an external load balancer. I think this is desirable mostly from a performance point of view, if calls to synchronous functions end up getting chained together. I would generally try and avoid that as a pattern, but knowing you have direct access to an endpoint on the same internal network would make it more palatable.
Supporting Node.js as an initial runtime is a good choice. It opens the service up to the widest number of developers early, and if we could only have one language at this point it would be JavaScript. On the other hand given the type of applications we’re looking to deploy to GCF, I’m hoping that support for Go and Java are on the roadmap. Go in particular would give them one-up on Lambda given its popularity in the cloud community.
More broadly I wonder if making the platform API to be at the level of a container, or more specifically a Dockerfile, would be a good fit for serverless. AppEngine Custom Runtime supports providing a Dockerfile as part of your source upload. AppEngine builds the image and starts the container to run your application. I can’t think of a reason why this wouldn’t work as well at the function level.
The functions we have tested so far do very little work, simply connecting to Google Datastore to write/read data and then returning a blob of JSON.
Start-up of cold functions seems particularly poor, often waiting for 5–7 seconds. This is clearly not an acceptable wait time, so there’s either something going wrong here, or it’s something Google badly need to improve. It’s not uncommon to implement an external service to hit cloud functions to keep them warm, but again it breaks the abstraction of not having to care about details of the underlying infrastructure and isn’t something I want to think about.
Firebase includes Google Cloud Functions with some wrappers that support some of their other services and tooling. In fact some of the problems I’ve outlined here have solutions in Firebase.
In general though I’m just confused about Firebase and how it fits alongside the wider cloud platform. It requires creating Firebase projects, which have a separate and distinct console, a separate CLI with different patterns and paradigms for creating services. Even the exposed API for cloud functions is different. This is fine if you’re building a “Firebase app”, but in terms of Google’s wider serverless platform, it really muddies the waters in terms of documentation, community support, and integrations.
We’ve chosen to ignore Firebase for now, hoping that future improvements to Cloud Functions themselves provide similar integrations without introducing the management and process overhead of what’s essentially a third-party.
Although Cloud Functions is a long way behind Lambda I’m hopeful that as the service moves into general availability, Google are able to take the best from what we’ve learnt from Lambda over the last few years, and provide a really compelling serverless platform.
Director of Engineering, Application Infrastructure at @brandwatch. Previously @twitter, @guardian, @clearleft, @microsoft, @multimap.
396 
4
396 
396 
4
Director of Engineering, Application Infrastructure at @brandwatch. Previously @twitter, @guardian, @clearleft, @microsoft, @multimap.
"
https://medium.com/techking/key-rotation-in-google-cloud-3ee8ff0a7828?source=search_post---------123,"There are currently no responses for this story.
Be the first to respond.
By The Cloud Foundations Team at King
King’s journey to Google Cloud (GCP) began last year with the aim of taking advantage of the advanced tools and management services it offers. The migration of our infrastructure to GCP has enabled a huge increase in scaling ability as well as improvements in the development cycle for our products.
Securing our platform has been a top priority since we decided to move to Google Cloud. In this post, we will explain our thoughts on some of the proposed solutions for credentials rotation, describe our requirements and introduce the solution that was implemented.
“Our infrastructure needs to support hundreds of thousands of concurrent connections per second, as well as our data warehouse, and we saw that Google has the capacity to handle our needs. At the same time, we were very excited by its focus on machine learning and artificial intelligence.”
- Jacques Erasmus, CIO, King.
Service accounts are unique identities that are used to facilitate programmatic access to GCP APIs. Each service account can have one or more keys which are used to authenticate with Google Cloud. Security best practices recommend keys to be periodically rotated which involves generating a new version of a key, distributing it amongst users and removing the old version of a key. This is done to limit the risk of service account keys being used by a malicious third-party user in the event of keys being leaked. As GCP APIs are publicly accessible, key rotation limits the impact of leaked keys being used by malicious third parties.
For our particular needs at King, the focal point was securing access to the Google Cloud API endpoints using service account keys. Even though key rotation is a simple concept, implementing it to ensure flexibility and user convenience while guaranteeing the security of the system was not a straightforward task. Solutions to overcoming the initial implementation issues are company specific. Our solution to this set of issues involved implementing a custom key rotation setup and distribution mechanism that provided our organisation the required functionality.
The Vault is an open-source implementation of a store for such things as tokens, passwords, certificates, and API keys. Vault offers a wide range of secrets engines which store, generate, and/or encrypt data. This includes stores that simply save and retrieve data from storage and dynamic stores that interface with different cloud providers’ API endpoints. The Google Cloud Vault secrets engine can be used for the provision of Google Cloud service account keys along with OAuth tokens based on vault policies. This allows users access to Google Cloud resources without needing to create or manage a dedicated service account.
“Secure, store and tightly control access to tokens, passwords, certificates, encryption keys for protecting secrets and other sensitive data using a UI, CLI, or HTTP API.”
- HashiCorp
Vault was chosen as a service account storage for its security, provisioning of client authentication and architecture. This allows highly available deployment in multiple geographically isolated locations eliminating single points of failure.
Using Google Cloud services specifically, a solution based on Vault and custom key rotation services provides our users with an ability to individually manage service account properties while enforcing maximum lifetime of their keys and providing a consistent way of retrieving up-to-date keys.
Key rotation is one of the basic rules of good security practices and Google provides some further ideas on key rotation as their documentation mentions the following method:
“A security best practice is to rotate your service account keys regularly. You can rotate a key by creating a new key, switching applications to use the new key and then deleting old key. Use the serviceAccount.keys.create() method and serviceAccount.keys.delete() method together to automate the rotation.”
- Google Cloud.
While feasible, this method didn’t seem ideal to us, as key rotation could not be centrally enforced (client application is responsible for rotating its own keys) and/or due to possible privilege escalation risk.
Therefore a service account performing key rotation currently cannot be prevented from rotating another service account’s keys.
In other words, a service account performing rotation could assume the identity of another service account with different permissions in the project — creating a new key for a different service account.
Vault, in conjunction with its storage engines, supports dynamic secret provisioning whereby credentials are created and passed over to the requesting party. The Vault server in this scenario authenticates against GCP and makes API requests to the serviceAccount and serviceAccountKey API endpoints.
In the case of using the Google Cloud engine, there are some important considerations to note: Vault will by default only create up to 10 keys for each newly created service account (a Google Cloud restriction). Furthermore, Vault policy mapping which defines service account roles needs to be specified. Mapping is ideally done dynamically by referencing the organisation’s own IAM database. This can be an issue when: no mapping source is present, complex cross-project service account permissions are required or a different set of permissions in Google Cloud and in the corporate IAM database are required.
The key rotation service consists of enrolment and operational phases. Projects that wish to use the key rotation service can enrol by setting a GCP project label. A project is enrolled into the rotation service when the Vault access token with the corresponding Vault policy is generated and sent to the project owner. In the operational phase, the owner of the project can then use the Vault token to retrieve the GCP service account key generated daily.
In our case, with Vault serving as a store of Google Cloud service account keys, we were faced with a challenge of authorising user access to those credentials. Before a client can retrieve a secret value from Vault, it needs to authenticate with it. In our case, a dedicated project owner role is assigned based on the project’s IAM roles. This responsible user is provided with Vault authentication tokens then used to access the most recent up-to-date key for the project.
The way this was eventually implemented for the key rotation service was to compile a list of all projects in the organization that needed to be enrolled (enrolment into the key rotation service is indicated by setting a custom project label) with the corresponding project owner. After information about the project and its owner have been collected, a new Vault access token and corresponding Vault policy are created and provided to the project owner. From that moment on, the project owner owns an initial set of credentials allowing him/her to access the path in Vault containing the project’s service account keys.
After project enrolment into the key rotation service, a user can retrieve service account keys by authenticating with Vault and reading the project’s path in Vault. The key rotation service in this mode is responsible for creating new service account keys and storing them in Vault for a client’s consumption. In line with our corporate policy, a list of service accounts with service account keys older than 24 hours is compiled, and for those new keys are created and stored in Vault. During this phase, a scan of all enrolled projects is performed, removing any keys older than 24 hours.
The Cleaner service is responsible for maintaining the consistency of secrets data stored in Vault and the state of Google Cloud IAM. Cases when cleanup might be required include a project being deleted (the Vault user token path is removed), a project being unenrolled from the key rotation service (the project label is removed), a service account, or all keys for a service account, being removed (the service account key and Vault path are removed).
Combining all above-described elements of the key rotation process allows us to: centrally manage and enforce GCP service account key TTL and provide users with a convenient way of retrieving up-to-date service account keys.
As already mentioned, while key rotation is a simple concept, its practical implementation can be complex and require custom logic. Various ways of rotating GCP service account keys exist and implementation should consider aspects like user experience and flexibility, the ability to centrally manage and enforce key rotation/expiration, and the cost of maintaining the chosen solution. Each individual solution to the key rotation process will come with its own set of pros and cons, and as always, the best solution for an organization will depend on its requirements.
Going beyond service account keys, when designing a secure platform on Google Cloud, it’s worth mentioning other ways of achieving high levels of security while maintaining user convenience. If an application using a service account is hosted on GCP, an option of assigning a service account to resources (Compute Engine, GKE, etc) exists, avoiding the issue of having to manage service account keys completely. This is an appealing option, as no rotation or other maintenance of the service account keys is required.
Last but not least, Google Cloud regularly introduces new and exciting improvements for existing services, this is something worth keeping an eye on for improving your organisation’s security posture. Here at King, we always try to encourage Google Cloud on ways to make its portfolio better by giving regular feedback on their services.
Related to contents of this article and security in general, we would like to see more granular permissions management being implemented across more services, particularly in the case of service account keys creation and deletion, where a service account having granted iam.serviceAccountKeys.create and iam.serviceAccountKeys.delete permissions, can delete and create a service account key for other potentially higher privileged roles in a project.
We are continually developing and expanding our tech infrastructure thanks to GCP as well as reaching new flexibility in scaling and development cycles in a secured environment.
Read some of our previous posts on our GCP journey: Talking big data at Google Cloud Next ’18 London & Benchmarking Google BigQuery at Scale.
Read the latest tech articles from King.
279 
1
279 claps
279 
1
Read the latest tech articles from King. Learn about our tech and our company culture.
Written by
King Tech Blog - awesome tech teams at King. Everything you read comes from the minds of our tech gurus and hearts of our Kingsters!
Read the latest tech articles from King. Learn about our tech and our company culture.
"
https://medium.com/ontologynetwork/you-can-now-develop-ontology-smart-contracts-on-google-cloud-aws-and-azure-4c7425e0cfb7?source=search_post---------124,"There are currently no responses for this story.
Be the first to respond.
Today the Ontology Development Platform (ont_dev_platform) was released on Google Cloud Platform Marketplace, making Ontology one of the first public blockchains to have a development platform on the leading cloud provider marketplaces: Google Cloud, Amazon Web Services, and Microsoft Azure. Using the Ontology Development Platform on one of these cloud providers allows you to play around with and develop smart contracts without having to go through the fuss of configuring and setting up an environment locally.
Ontology has also joined the Google Cloud Technology Partner program, which gives Ontology the opportunity to collaborate with Google in marketing activities. With this new relationship and the development platform releases, Ontology hopes to grow the tech community and make developing dApps more accessible for all.
Are you a developer or want to start out? Please check out the:
Are you a developer? Make sure you have joined our tech community on Discord. Also, take a look at the Developer Center on our website, there you can find developer tools, documentation, and more.
Ontology website / Ontology GitHub / ONTO website / OWallet (GitHub)
Telegram (English) / Discord
Twitter / Reddit / Facebook / LinkedIn
A high performance, open-source blockchain specializing in digital identity and data.
973 
973 claps
973 
Ontology is a high performance, open source blockchain specializing in digital identity and data. ONTO: http://onto.app/downloadpage/TW  Telegram: http://t.me/OntologyNetwork
Written by
Active project domain: https://ont.io/
Ontology is a high performance, open source blockchain specializing in digital identity and data. ONTO: http://onto.app/downloadpage/TW  Telegram: http://t.me/OntologyNetwork
"
https://medium.com/@wisdomgoody/%E0%B8%AA%E0%B8%AD%E0%B8%99%E0%B8%95%E0%B8%B4%E0%B8%94%E0%B8%95%E0%B8%B1%E0%B9%89%E0%B8%87-kong-api-gateway-with-kubernetes-k8s-and-nginx-ingress-konga-on-google-cloud-gke-d94cc6b2e965?source=search_post---------125,"Sign in
There are currently no responses for this story.
Be the first to respond.
Ploy Thanasornsawan
Sep 13, 2019·4 min read
ในบทความนี้จะสอนแค่การติดตั้ง kong แบบ kubernestes บน google cloud นะส่วนวิธีการเล่น kong พวก plugin ไม่ได้มีอะไรแปลกใหม่ ถ้าใครอยากเล่นบน docker และดูวิธีการเล่น plugin สามารถอ่านได้จากบทความก่อนหน้า
ส่วนหลักการใช้ kong ยังคงเหมือนเดิมคือต้องมี service, route, consumer และ plugin ซึ่ง plugin นี้นอกจากที่ kong provide ให้แล้ว เรายังสามารถพัฒนาเองด้วยภาษา lua ได้ และถึงแม้ kong version community จะไม่มีหน้าเว็บ admin ให้เราสามารถ install konga เพิ่มได้ หรือ kong dashboard, kong dash แล้วแต่ชอบ ส่วนที่เลือก install บน kubernetes ในบทความนี้เพราะเห็นว่า kubernetes ถูกนำไปใช้มากในการจัดการ cluster ใหญ่ๆ มีระบบ self healing มีความยืดหยุ่นสูง สามารถเอาไปรันที่ cloud หรือ server ไหนก็ได้ และ kong เองก็ตอบโจทย์ในเรื่องการทำ api gateway ซึ่งจะต้องมีการจัดการ microservice เป็นจำนวนมาก มีการทำ load balance และ monitor ดูแล services ต่างๆที่ต่อกับ kong อยู่ ถ้าหากมี service ตัวไหน unhealthy ก็สามารถใช้ kubenetes ในการ scale up/scale down ตามลักษณะงานได้เลย
เกริ่นแนะนำ kubernetes
สำหรับคนที่ไม่เคยเล่น kubernetes มาก่อน การเล่น kubernetes และเขียนสคริปไฟล์ yaml อาจจะเข้าใจยากกว่า docker เพราะ docker เราแค่ pull image ที่ต้องการแล้วก็ run ออกมาเป็น container ได้ทันที แต่ kubernetes ไม่มีการ pull image อะไรแบบนั้น service 2 ตัว ก็เขียนไฟล์ 2 ไฟล์แยกกัน เช่น konga.yaml กับ postgres.yaml แล้วพอจะเอาแต่ละ service มาเชื่อมกันก็เขียนไฟล์ใหญ่ขึ้นมาอีกไฟล์คือ Development เพื่อบอกว่ามี service อะไรบ้าง port อะไร โดยส่วนที่เล็กที่สุดของ kubernetes คือ pod แล้วแต่ละ pod ก็จะมี ip address แยกกันไป เรียกว่า cluster ip คือ ip address ที่เอาไว้ใช้สื่อสารกันเองภายใน ไม่สามารถออกไปสู่ข้างนอกได้
หลักการทำงานของ kong บน kubernetes
จากรูป diagram จะเห็นว่า kong ถูกแบ่งออกเป็น control plane กับ data plane โดยมี flow การทำงานเมื่อมีการยิง request เข้ามา มันจะถูกส่งไปที่ Data plane ซึ่งทำหน้าที่เป็นเหมือน proxy สร้าง route เส้นทางไปยัง service แต่ข้อมูลสำคัญเกี่ยวกับ plugin security, traffic control, authentication ต่างๆจะได้จากการตั้งค่าที่ kong admin (Control plane) แล้วเขียนลง database ที่อาจเป็นได้ทั้ง postgresql หรือ cassanda ซึ่งเป็น nosql ขึ้นกับการ install ของ developer
ในรูป kong entry points จะเป็นการสรุปเรื่อง port สำคัญที่ใช้ใน kong ถ้าฝั่ง proxy หรือก็คือ kong data plane จะเป็น 8000 (http)กับ 8443 (https) ส่วน Kong management ในที่นี้คือ Control plane จะเป็น 8001 (http), 8443 (https)
ภาพการทำงานของ ingress
จากที่ได้เกริ่นไปเรื่อง kubernetes ว่าส่วนที่เล็กที่สุดของ kubernetes คือ pod แล้ว pod แต่ละอันจะมี ip address เป็นของตัวเองไว้ใช้สื่อสารภายใน แต่ถ้าเราอยากจะให้มันสามารถเข้าถึง cluster ภายในได้จากภายนอก เราจะต้องทำการติดตั้ง ingress เพิ่ม โดย ingress จะทำหน้าที่กั้นอยู่ด้านหน้าเพื่อรับ external traffic (request จากภายนอก) ทำ load balancing ช่วยในเรื่อง Virtual Host (1 IP มีหลาย Domain)
ขั้นตอนการติดตั้ง kong + nginx ingress + konga
Step1: git clone https://github.com/Kong/kong-dist-kubernetes.gitStep2: cd kong-dist-kubernetesStep3: chmod a+x setup_certificate.sh(หรือจะเช็คสิทธิ์ของไฟล์ก่อนก็ได้ด้วย ls -la จะเห็นว่าสคริปสำหรับสร้าง certificate มีสิทธ์ในการทำการ execute ไฟล์รึยัง)
ถ้าเป็นแบบในรูป -rwxr-xr-x คือแปลว่าเราสามารถทำการ execute ไฟล์ได้ล่ะStep4: kubectl apply -f kong-namespace.yamlStep5: ./setup_certificate.shติดตั้ง database โดยเลือก database เป็น postgresqlStep6: kubectl -n kong apply -f postgres.yamlStep7: kubectl -n kong apply -f kong-control-plane-postgres.yamlStep8: kubectl -n kong apply -f kong-ingress-data-plane-postgres.yamlStep9: vi konga.yaml
Step10: kubectl -n kong apply -f konga.yamlเริ่มติดตั้ง nginx ingressStep11: kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/mandatory.yamlStep12: kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/provider/cloud-generic.yamlStep13: kubectl get all -n kong
Step14: vi ingress_kong.yaml
Step15: kubectl -n kong apply -f ingress_kong.yamlStep16: map host name on your local pcGo to C:\Windows\System32\drivers\etc\host
การเขียนไฟล์ ingress on kubenetes จะต้องมีการระบุ hostname, service name, port และถ้าเราไม่ได้มีเว็บ hostname เป็นของตัวเอง เราสามารถสร้างชื่ออะไรก็ได้ตามใจเรา เช่น service name ชื่อ kong-control-plane อันนี้เรากำหนดชื่อ hostname ขึ้นมาเองว่า test-kongadmin.com ดังนั้นเวลาเราต้องการให้ hostname นี้ใช้ได้เชื่อมต่อกับตัวไฟล์ service ingress เราจะต้อง map hostname บนเครื่องเรา ซึ่งทุกคนสามารถเช็ค external ip ingress ของตัวเองได้จากคำสั่ง kubectl get svc -n ingress-nginx
หรือแม้แต่การเชื่อมต่อระหว่าง kong กับ konga ก็ต้องใช้ external IP Ingress nginx ส่วน port เราต้องใช้ port ของ kong admin ที่เป็น 3 หมื่นที่ k8s ให้มา หรือก็คือ service name ชื่อ kong-control-plane สามารถเช็คได้จากคำสั่ง
จะเห็นว่าอันนี้ในรูปของเรา kong-control-plane อยู่ที่ port 31796
หลัง create connection success จะได้หน้า dashboard แบบนี้
หลัง map host บนเครื่องเราเรียบร้อยแล้ว เราสามารถเช็คการทำงานของ IP และ hostname ของ kong admin ว่าสามารถ request ข้อมูลได้ปกติ ได้จากคำสั่งcurl http://<external ip ingress nginx> -H ‘Host:<host name>’
passion coding and develop my world
See all (926)
207 
2
207 claps
207 
2
passion coding and develop my world
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/developing-a-cryptocurrency-price-monitor-using-firebase-and-google-cloud-platform-34d5538f73f6?source=search_post---------126,"There are currently no responses for this story.
Be the first to respond.
In this article I demonstrate how to build a cryptocurrency price monitoring app using Firebase and Google Cloud Platform (GCP). The app receives notifications whenever the price of Bitcoin or Ethereum change in the market. Users can configure minimum and maximum price thresholds for each cryptocurrency, and we use these settings to determine which users to notify for any given price change.
The inspiration for this app came from an article published by the Pusher community (read part 1 and part 2 here). Their cryptocurrency price alerts app uses Pusher for delivering notifications, which seems to be using Firebase Cloud Messaging (FCM) under the hood. Furthermore they employ a SQLite database and a standalone back-end server developed in Go to implement the necessary functionality. As I was reading that article I couldn’t help but think that the use case in question would make a great demo for Firebase and GCP as well. This article and the associated app are results of that notion.
The version of the app we develop does not require deploying a standalone database or a back-end server. We implement all the server-side functionality using various Platform-as-a-Service (PaaS) and serverless products readily available in the Firebase and GCP ecosystems. Specifically, our example app demonstrates the following features.
In the client-side we use the Firebase Android SDK to directly interact with Firestore and FCM. This precludes the need for implementing a separate REST API, or having to program any HTTP interactions in the Android app.
Our app uses two Firestore collections.
There’s no need to manually create these collections. They get created automatically as our app goes into action. But if you wish to test the app by adding some sample data, figure 1 shows what your Firestore database should look like.
As part of setting up Firestore, we deploy the following security rules. These essentially make the prices collection read-only, and the prefs collection read-write. Any other collections you happen to have in the same Firestore database will be inaccessible to the app.
We don’t implement user authentication in this demo, but it is fairly straightforward to do so using Firebase Auth. If you decide to explore that option, you can use the unique user IDs provided by Firebase Auth as the document IDs in the prefs collection. That would also allow you to further tighten up your security rules, allowing each user to write only to their designated documents.
Also note that Firebase security rules only apply to the client apps (Android users in this case). In the back-end we use Firebase Admin SDKs, which bypass security rules, and access the database as a privileged user.
The full source of the app can be found in my firecloud GitHub repo. The whole thing amounts to about 170 lines of Kotlin code, plus the usual Android resources and manifest files. We start a Firestore realtime listener on the prices collection when the app launches. This way the app always displays the latest cryptocurrency prices stored in Firestore. Listing 1 shows the relevant code fragment from the MainActivity.
The MainActivity layout contains two text views with the IDs btc and eth. Note that we use the same IDs for Firestore documents in the prices collection. Therefore we can employ the trick in lines 16–18 to map each Firestore document to a text view in the UI.
Tapping on a text view launches the settings dialog for the corresponding cryptocurrency. The user can specify a minimum and a maximum threshold and save the settings. The idea is that the app will notify the user whenever the price drops below the min threshold or exceeds the max threshold. Listing 2 shows the method that saves the settings to Firestore.
In addition to the threshold values, we also save app instance’s registration token to Firestore (line 6). This is used later when we want to notify the device via FCM. We use the stable Firebase instance ID as the document identifier. In an app that implements user authentication, we can use the unique user ID here instead.
The await() method used in listing 3 (lines 2 and 9) is an extension method we have added to the Android GMS Core’s Task class. This makes it easier to use the Task API with Kotlin coroutines. Listing 3 shows the implementation of the await() method.
Finally, we add the FCM Android SDK to the app, and extend the FirebaseMessagingService as shown in listing 4 in order to receive push notifications.
This service handles incoming push notifications when the app is in foreground. It displays a simple pop-up with the notification payload. When the app is in the background the notification will be delivered to the Android’s system notification tray.
That’s pretty much all the exciting bits in the client application. Now lets look at the back-end components of the app.
We implement an App Engine service in Go (v1.11) to periodically check the cryptocurrency market prices, and save the results to Firestore. Our implementation is comprised of following files:
Full source code of the service can be found in the firecloud repo. You can run the following command to directly import the code into your GOPATH:
The price checker service uses the Firebase Admin SDK for Go to access Firestore. Since our code is going to be deployed in App Engine, we can let the SDK auto discover Google Application Default Credentials (ADC) to authorize Firestore API calls. This means we can initialize the Admin SDK with a minimal configuration as shown in listing 5.
Note that we are only passing a Context to the firebase.NewApp() function. The SDK is able to auto discover authorization credentials and any other settings required to access Firestore (e.g. GCP project ID). The firestore.Client that we subsequently obtain at line 19 provides access to the same Firestore database used by the Android client apps.
We use the CryptoCompare REST API to fetch the latest prices of Bitcoin and Ethereum. Listing 6 shows how the discovered prices are saved to Firestore.
We execute a batched write on Firestore to update the prices of both Bitcoin and Ethereum in a single operation. Since our Android app is already listening to updates in the prices collection, these changes become immediately visible to the users.
We expose this service as an HTTP endpoint at the URL path /fetch. Next, in order to keep our app data up-to-date, we need to instruct Google App Engine to invoke our service periodically. This is done by writing a cron.yaml file as shown in listing 7.
To test this service locally, set the GOOGLE_APPLICATION_CREDENTIALS environment variable to point to a service account JSON file downloaded from your Firebase project. Then execute the main.go file of the service:
This starts the service on port 8080, and you can try it out by sending a request to http://localhost:8080/fetch. You will see the prices collection getting updated in the Firebase console as a result. The updates will also appear on the Android client app, if it happens to be running at the time. Note that when testing locally, you must manually invoke the /fetch endpoint of the service. The cron.yaml file only takes effect once deployed to the cloud.
The rate at which cryptocurrency prices change in the real world may not be enough to trigger many updates while testing the service. If this becomes a problem you may forego the CryptoCompare API, and get the service to produce random pricing data at each invocation. Set the following environment variable prior to launching the service to enable this feature.
To deploy the service to App Engine, install and set up the Google Cloud SDK. Make sure the gcloud command-line utility is configured to manage your GCP/Firebase project.
Then run the following commands from the same directory as the app.yaml file.
The first command deploys the service implementation. The latter starts the scheduled task that periodically invokes the service. Shortly afterwards, you will see entries similar to the following in your App Engine log.
We have also programmed our service to accept requests only from the App Engine cron scheduler. Trying to manually invoke the HTTP endpoint in the cloud will yield a 404 Not Found response. You can change this behavior by removing the CRON_ONLY environment variable from the app.yaml file.
The last piece we need to complete our app is the service that notifies interested users when the cryptocurrency prices change. We already have a service that updates the prices in Firestore. Therefore we can use Cloud Functions for Firebase to implement a serverless function that sends out notifications whenever a new price is written to the prices collection of Firestore. Listing 8 illustrates what this implementation looks like.
We define an onUpdate Firestore trigger for the documents in the prices collection. The ID of the document that is being updated (i.e btc or eth), and its price value can be obtained from the arguments passed into the trigger. We pass these values to the findTargetDevices() helper method, which queries the prefs collection in Firestore to determine the users that should be notified of the price change.
We wish to notify users whose min threshold is higher than the current price, or whose max threshold is lower than it. Since Firestore does not support disjunctive queries (i.e. OR queries), we run two separate queries (lines 33–34), and aggregate the results. This is also where we reference the device registration tokens that the Android clients have saved in Firestore. Finally we use the FCM API in the Admin SDK to send push notifications to the selected users.
Again, notice that we are initializing the Firebase Admin SDK with a minimal configuration (lines 4–5). The SDK auto discovers valid authorization credentials, and connects to the same Firestore database used by Android clients and the App Engine service.
The full implementation of the cloud function is available on GitHub. Use the Firebase functions emulator to test the code locally. Run the following commands from the functions/ directory of the project.
This launches the Firebase emulator shell. You can now directly invoke the function with some sample data as shown below.
Having verified our function works as expected, we can deploy it to the cloud using the Firebase CLI.
You can now either wait for the App Engine service to update the cryptocurrency prices, or enter some sample prices into Firestore manually. It is also possible to manually run the App Engine cron job using the GCP console. Either way, the cloud function will get triggered, and you will be able to see the corresponding logs in the Firebase console.
Figure 2 is a screenshot from the GCP console, showing both App Engine and Cloud Functions logs in the same window. All the entries shown in this figure were produced by a single run of the App Engine service.
Figure 3 shows various screens of the Android client app, including how a notification is being delivered while the app is in the background.
In this post we looked at how to develop a cryptocurrency price monitoring app using several Firebase and GCP products. We used the Firebase Android SDK to directly interact with Google Cloud Firestore and FCM. We implemented a service in Google App Engine to periodically check the market prices of Bitcoin and Ethereum. Finally, we implemented a Google Cloud Function that notifies users of price changes based on the price thresholds configured by individual users. The whole exercise took me a couple of hours, and you can find the full implementation on GitHub.
The back-end functionality of this app is split between Google App Engine and Google Cloud Functions. However, it is also possible to implement all the back-end functionality using App Engine alone. We can program our Go service to send push notifications every time it updates the cryptocurrency prices. In fact, the service I have implemented already supports this, but it is disabled by default. See if you can figure out how to enable push notifications in the App Engine service by going through the code.
Personally, I very much prefer the idea of having distinct services for checking the cryptocurrency prices, and sending push notifications. This makes separation of concerns more explicit while resulting in a more loosely coupled implementation. It also makes each service independently testable and deployable, thus bringing us closer to a microservices architecture. For instance, imagine trying to change how our app checks cryptocurrency prices. With two distinct services in place, we can simply update the price checking service, and never touch the notification sender.
I hope you find this article and the associated demo app useful. Please feel encouraged to reach out with any questions or feedback. If you have any demo app ideas that you would like me to try and implement, I would like to hear them too. As always, you are also welcome to engage with the Firebase community via various opensource Firebase repositories.
Google Cloud community articles and blogs
239 
239 claps
239 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Software engineer at Google. Enjoys working at the intersection of cloud, mobile and programming languages. Fan of all things tech and open source.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/hackernoon/google-cloud-platform-hacker-noon-2ba28a29512f?source=search_post---------127,"There are currently no responses for this story.
Be the first to respond.
This is an email from Get Better Tech Emails via HackerNoon.com, a newsletter by HackerNoon.com.
“Google Cloud Platform and Firebase give Hacker Noon the flexibility to craft a custom publishing platform optimized for technologists,” said Hacker Noon Interim CTO Dane Lyons. “Our serverless infrastructure will generate static content and pipe it into the low latency, low-cost Google Cloud CDN. We’re excited to focus on important product details and worry less about devops and cost optimizations.”
As a startup working to free ourselves from platform dependency, it’s humbling to have Google support our own infrastructure. These resources make our future more secure, our monthly burn rate more manageable, and ultimately provide a stronger partner for serving high volumes of traffic in the long-term. We’re very excited about launching the next iteration of Hacker Noon with Google Cloud Platform and Firebase!
Deploy A Backend App As An Android Engineer (& Part 2) by Adam Hurwitz
Android App Architectures: Example of MVP with Kotlin by Rohit Surwase
Can Google’s AI Make Better AI Than the Googlers? by The Next Web
Google’s AI Based AutoDraw Turns Your Rough Scribbles Into Beautiful Icons For Free by Vinoth George
Chrome Extension Development: Lessons Learned by Sam Jarman
Optical Character Recognition With Google Cloud Vision API by Evelyn Chan
Launch a GPU-backed Google Compute Engine instance and setup Tensorflow, Keras and Jupyter by Steve Domin
Google Search Analysis: Rich Search Results and Structured Data by Garrett Vorce
The Art of Searching something on the Internet. by Vikas Yadav
Building Google’s Art and Culture Portrait Matcher by Grant Holtes
70% of People Worry About Fake News — And How Google Combats It by Chhavi Shrivastava
Getting Started with Firebase ML for iOS by Mohammad Azam
Firebase to the Rescue: Dynamic Routing via Hosting + Functions Integration by Peter LoBue
How to Build a Product Loved by Millions and Get Acquired by Google: The Firebase Story by Founder Collective
Infinite Scrolling In Firebase by Linas M
Introduction to Firebase by GeekyAnts
Prototyping with Firebase by David Kerr
Flutter — 5 reasons why you may love it by Paulina Szklarska
I love you Flutter by Shalom Yerushalmy
What are the Google Cloud Platform (GCP) Services? by Reto Meier
An actionable checklist to being a Google Summer of Code student by Chhavi Shrivastava
My (slightly unconventional) path to a Google Internship by Abhimanyu
This Student Talked His Way Into The Googleplex Through Courageous Networking by John Greathouse
Deep Reinforcement Learning in Robotics using Vision by SAGAR SHARMA
Acing Your Product Manager Interview by David E. Weekly
Interview Questions Deconstructed: The Knight’s Dialer by Alex Golec
Google Search — How A Master’s Thesis Became An Idea Worth $70 Billion by Soundarya Balasubramani
Revisiting How Serge and Larry Saw Advertising in 1998 by Gennaro Cuofano
Build a Google Home App with PubNub Functions by Kaushik Ravikumar
How I set up room-cleaning automation with Google Home, Home-Assistant and Xiaomi vacuum cleaner by Muh Hon Cheng
“This Is Going to Be Huge”: Google Founders Collection Comes to Computer History Museum by Computer History Museum
Google + Machine Learning + Getting Started = Awesomeness, and Good Cucumbers by BeeHyve
Instance Segmentation in Google Colab with Custom Dataset by RomRoc
Train Your Machine Learning Models on Google’s GPUs for Free — Forever by Nick Bourdakos
Implement Google Maps in ReactJs by Mohammed Chisti
Let’s make Reusable Web Components: Google Maps by Just Chris
Contributing to Golang using Google’s Pixelbook by Daniela Petruzalek
Getting Started with Go Development in the new Google Pixelbook by Daniela Petruzalek
Google Cirq and the New World of Quantum Programming by Jesus Rodriguez
Building an automated dashboard with Google Sheets (with example) by Nick Boyce
Google Sheets As Your Database by Just Chris
Web Scraping With Google Sheets by SeattleDataGuy
A Recommendation Engine Framework in TensorFlow by James Kirk
Building a Facial Recognition Pipeline with Deep Learning in Tensorflow by Cole Murray
Creating insanely fast image classifiers with MobileNet in TensorFlow by Matt Harvey
9 Things You Should Know About TensorFlow by Cassie Kozyrkov
Query a custom AutoML model with Cloud Functions and Firebase by Sara Robinson
Meeting The YouTube CEO by Felix Josemon
Thanks Google! For giving startups a chance** at independence and prosperity.
Onwards!
David Smooke
*We decided to leave some of our more critical articles of Google, like How Google Will Collapse, off this particular list. But rest assured, this grant won’t impact our editorial line. Whatever tech or company you are in favor or not in favor of, there is always a place for well written stories by tech professionals on Hacker Noon. Contribute today.
**If you are a rapidly growing startup thinking about building on Google Cloud Platform, you can apply for credits yourself! Apply here.
#BlackLivesMatter
739 
3
739 claps
739 
3
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
https://www.davidsmooke.net/ https://hackernoon.com/ Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant,
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
"
https://medium.com/platformer-blog/ci-cd-with-gke-and-google-cloud-build-98a797ecf346?source=search_post---------128,"There are currently no responses for this story.
Be the first to respond.
Google Cloud Build is a really powerful tool. You can configure your entire CI/CD pipeline with it. In this article, I’m going to guide you through on creating a simple CI/CD pipeline with Google Cloud Build and GKE.
You need to have a GCP account. If you don’t sign up here and subscribe for the free $300 Google gives you to try out the Google Cloud Platform. Once you are logged in, you need to create a Kubernetes Cluster, which can be created here under Kubernetes engine in the main menu.
To get started, first, you need to Dockerize your application. I’m going to assume that the Dockerfile and the context will be in your root directory of the application. When it’s done, your application should look like this. In the sample Node application I’m going to use here, it would look like this.
Make sure the Dockerfile works as expected by building it and running it locally. You can bind your ports and test it in whatever way you like. Here are the basic commands.
Okay. Now you are done with the Dockerfile, let’s get to the good stuff. Create a folder called k8s. We are going to add all the Kubernetes Yaml files into this folder. Let’s add a Service and a Deployment.
Create a file called deployment.yaml inside the k8s folder.
Now let’s create the service for this in service.yaml inside the k8s folder. I’m going to use the LoadBalancer type service for this demo. But you are free to use whatever you like.
Okay. Now with this configured, we have to go to the next step of writing the Google Cloud Build Configuration. To do this, create a new file called cloudbuild.yaml in the root directory of your application and add the following lines.
This cloud build is configured to initiate a CI/CD pipeline targeting a Tag Push Trigger. If you want Branch push, you will have to replace $TAG_NAME with$BRANCH_NAME-$COMMIT_SHA.
I will explain what happens in each of the above steps.Step 1, we are trying to pull the latest existing image of the application we are trying to build so that our build will be faster as docker uses the cached layers of the old images to build new images. The reason for adding || exit 0 is in case the docker pull returns an error (When running this build for the first time there will be no latest image to pull from the repository), the entire pipeline would fail. That is why || exit 0 is added to ignore and continue with the build even if an error occurs in that step.
Step 2, we build the docker image of our application.
Step 3, we apply all the configuration yamls that exist in the k8s/ folder of our application. Kubectl is a really powerful tool and it will automatically create or update missing configs and resources in the cluster according to the yamls you have provided in the k8s folder. <cluster-zone> is the zone of your kubernetes cluster. You cluster maybe regional, but still if you go to the Kubernetes Engine in Google, you would see the default zone name of your cluster. You have to add that in. <cluster-name> is the name of your cluster which you gave when you created the cluster.
Step 4, Update the app deployment image in GKE with the latest version of the image you built.
Now on a coding perspective, you are all done. Now we need to configure a couple more resources to get this pipeline working in GCP. First is setting up IAM for cloud builder so that it has permissions to deploy the images in GKE. To do this, navigate to IAM & Admin →IAM on the side menu.
Here, you would see an entry in the format,
with the role, Cloud Build Service Account. Edit the role and add Kubernetes Engine Admin role to it as well.
Okay, now we have to connect the dots. We are going to add a build trigger to initiate our pipeline. To do this, you have to navigate to Cloud Build → Triggers. Here, click the button Add Trigger and you will be given a choice to select from either Github or Bitbucket. (Sorry Gitlab fans, it’s not supported at the moment. If you want Gitlab, try Platformer Cloud which provides a PaaS solution on top of Kubernetes with the underlying infrastructure running inside GCP).
Once you select your repository, You will be navigated to the build config page. Please fill the details as shown in the screenshot below and create the trigger.
All good! now everything is configured and you can do a Git Tag push in your repository and the pipeline will jump to action.
You can configure Integration tests too in the cloud build yaml. Just add another step with your testing configurations. Or, better yet, I recommend you use a multi stage docker file, where you run your integration tests as a stage in your Docker build.
I hope this helped you. Clap for more. Cheers!
Platformer.com
340 
8
340 claps
340 
8
Platformer.com Blog | Technical stories written by our team, partners and invited authors on Cloud, Containers, Kubernetes, Serverless, etc
Written by
Software Architect
Platformer.com Blog | Technical stories written by our team, partners and invited authors on Cloud, Containers, Kubernetes, Serverless, etc
"
https://medium.com/@brianray-7981/google-clouds-automl-first-look-cb7d29e06377?source=search_post---------129,"Sign in
There are currently no responses for this story.
Be the first to respond.
Brian Ray
Apr 24, 2019·9 min read
I’m not new to Automated Machine Learning. It should be no surprise I have something to say regarding Google Cloud’s “Cloud AutoML” [Beta] which covers 5 of the 29 announcements made last week at Google Cloud Next ’19. I’m cautiously a fan of AutoML; although, I also believe that it’s more of a highway to get there and not a final destination.
"
https://rominirani.com/google-cloud-functions-tutorial-using-the-cloud-scheduler-to-trigger-your-functions-756160a95c43?source=search_post---------130,"This is part of a Google Cloud Functions Tutorial Series. Check out the series for all the articles.
By now, you will be familiar with creating Cloud Functions (foreground and background) and associating the appropriate trigger i.e. HTTP Trigger, PubSub Trigger and others. We usually find that we have a need to trigger the Cloud Function at regular intervals.
Consider the following Cloud Functions that you might have written:
You could even have complex scheduling like run a Cloud Function every Monday, Wednesday and Friday at 10:00 AM.
In all the above situations, you have the following requirements:
The Cloud Scheduler now available in Beta helps us out in the above requirements.
As per the official documentation, Cloud Scheduler is an enterprise grade job scheduler that will help you automate your jobs across various Google Services in a standard fashion and comes with retry mechanisms that you can configure. It can also trigger your jobs in a variety of ways and currently supports invoking a HTTP endpoint, send a message to Cloud Pub/Sub and trigger an internal App Engine URL.
In our example here, we will be looking at triggering a sample Cloud Function every minute. Let’s go.
Before we begin, a few assumptions:
Click on Cloud Scheduler API and click on Enable:
Once enabled, you might see a message that says that you will need to have credentials in order to use the API. That is fine for now, since we plan to do everything via the GCP console and Cloud Functions deployed inside the same GCP Project. The API credentials would be required in case you were making any requests to this API from external Apps.
This completes our initial setup.
We have completed the initial setup and have our Cloud Function function-1 ready. All we need to do now is to create a job in the Cloud Scheduler. All the details like the schedule, what to trigger and other parameters will be configured at the time of Job creation itself and are associated with the Job.
From the GCP Console main menu, visit the Cloud Scheduler page:
Click on the Cloud Scheduler option. Since we do not have any jobs yet, we will see the message as shown below:
Click on Create job. This will prompt you to select a region in which you would like to initialise the Cloud Scheduler. Select the appropriate region. You should see a message as shown below:
Once it is initialised, you will be led to a form to create a new job and you can provide the details as shown below:
The key things to note in the Job details form are:
Click on Create button. This will create the Job and you will see it listed in the list of jobs. You can either wait for the execution to happen (look at the Result column) since it is every 1 minute or if you want to invoke it directly click on Run now.
Once it has executed, you will see the status getting updated as shown below (only the specific columns are shown):
Click on View button to see the logs. Here is the sample log statemens from one run of the job:
Note that we could also have a Pub/Sub Trigger configured Cloud Function get triggered by the Scheduler by using the same steps given above to create a job. Except that in the target, you will select Pub/Sub and provide the Pub/Sub Topic name. At the scheduler trigger time, the Cloud Scheduler will publish a message to the topic with the message body that you specify.
This completes our note on triggering a Cloud Function using the just released Cloud Scheduler service.
Technical Tutorials, APIs, Cloud, Books and more.
163 
3
163 claps
163 
3
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
Technical Tutorials, APIs, Cloud, Books and more.
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
Technical Tutorials, APIs, Cloud, Books and more.
"
https://rominirani.com/using-puppeteer-in-google-cloud-functions-809a14856e14?source=search_post---------131,"This is part of a Google Cloud Functions Tutorial Series. Check out the series for all the articles
Ever since I heard the term headless Chrome, I have been curious about what that exactly means and the kind of applications that it can help write. Recently I checked out an excellent talk by Eric Bidelman from Google IO 2018 titled “The power of Headless Chrome and browser automation”. I recommend you watch this video for the first half atleast to understand what headless Chrome is and how it works.
In summary, headless Chrome is :
For more information, you can check out the following article:
developers.google.com
When Google Cloud Functions was first released, the only runtime that it supported was Node.js version 6 and the OS was missing several packages that made it difficult to run Chrome in this fashion.
A couple of months back came the announcement that headless Chrome support was now available in App Engine standard and Cloud Functions. This was made possible by the release of Node.js 8 runtime on App Engine standard and which was the same runtime used for Google Cloud Functions too. Check out the official blog post that announced it:
cloud.google.com
To make things dead simple for developers, we have a npm package called Puppeteer that makes working with headless Chrome a breeze. The default installation even comes bundled with a version of Chromium, so that it is self-contained and has everything to get you started.
To install Puppeteer, simply use the following command:
If you would like to learn more about Puppeteer, check out https://pptr.dev/. There is even a Puppeteer playground at https://try-puppeteer.appspot.com/
While there are multiple ways in which one could have done this, I wanted to try this out with Puppeteer and see how it goes.
Our Cloud Function will return just the Comic Strip from the page at https://loveiscomix.com/random. In short it will return HTML content that will contain just the comic strip, an example of which is shown below:
Let us check out the index.js file that contains our Cloud Function:
Let us understand the key pieces of the code and before we jump to that, what I am trying to do is the following:
Steps (1) + (2) + (3) happen via the following code snippet via the Puppeteer package:
The package.json file is standard stuff and it contains the dependency for our puppeteer package.
Ensure that both index.js and package.json file that we have created above are present in the same directory.
We can use the gcloud functions command to deploy the function as shown below. Note that we will be having a HTTP Trigger for our cloud function, the runtime will be Node version 8 and we will be giving it ample memory (more on that later in the post):
Once it is deployed, you can check that the sendComic function is available via the gcloud functions list command.
You can get the details on the functions via the describe command as shown below:
$ gcloud functions describe sendComic
The output from the above command will contain the httpsTrigger property, an example of which is shown below:
In your case, the <region> and projectid above will contain the appropriate values. Simply invoke the above URL in your browser and it should invoke the HTTP Trigger based Google Cloud Function sendComic and give you a random Love Is Comic strip. This is what I got:
I hit a few blocks while trying to run this Google Cloud Function. I erred in not paying too much attention to the official blog post and learnt a couple of things about deploying Google Cloud Functions that using Puppeteer.
When I first wrote the function, I launched the headless browser as shown below:
This resulted in an exception as shown below:
This was corrected by providing the following flag--no-sandbox while launching Chrome:
I deployed the function with the default memory allocated to it i.e. 256MB and that is definitely not enough. I got the following error during function execution:
I had to deploy the function with --memory=1024MB option while using the gcloud functions deploy command.
Do keep in mind that allocating more memory to your function is definitely going to reflect in your costs.
Hope you enjoyed this article. Do share what you plan to run with Puppeteer.
Technical Tutorials, APIs, Cloud, Books and more.
328 
4
328 claps
328 
4
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
Technical Tutorials, APIs, Cloud, Books and more.
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
Technical Tutorials, APIs, Cloud, Books and more.
"
https://blog.realworldfullstack.io/real-world-app-part-13-elasticsearch-on-google-cloud-with-firebase-functions-8a24fa2b95ed?source=search_post---------132,"Set up and integrate Elasticsearch with Cloud Functions for Firebase using nginx
This is Part 13 of our Real World Angular series. For other parts, see links at the bottom of this post or go to https://blog.realworldfullstack.io/. The app is still in development. Check it out @ https://bitwiser.io.
In the previous post, we saw how Cloud functions for Firebase can be used to move some of our application’s logic to the backend. This code could be executed on events/triggers such as an http request or on a database operation.
Why Search?
However, we hit the limitation of what the Firebase database can provide when it came to implementing complex queries. For example, we needed a way to fetch a randomized question for our game, based on certain criteria. There is no good way to do that using native Firebase database operations.
In order to make such queries or to analyze data in different ways, we would need to index our data and query using a search engine.
Some of the popular search engines are Apache Solr, Elasticsearch, Algolia & Amazon Cloudsearch. Algolia and Cloudsearch are strictly managed cloud-based platforms, while Solr and Elasticsearch are open source built on Lucene, that can be installed on your own servers or on any cloud compute platforms.
Feature comparisons and other links are provided in the reference section at the end of the blog.
Since, I did not want to be tied to a cloud based platform, the choice was between Elasticsearch and Solr. I decided to go with Elasticsearch for it’s ease of setup on the cloud and as is known to work well with JSON documents.
If you aren’t familiar with Elasticsearch, I would suggest that you read their documentation (https://www.elastic.co/guide/index.html) to get a basic understanding on how Elasticsearch works in general and how it stores/retrieves documents in/from it’s indexes.
If you prefer a managed cloud solution with zero installation/setup needed, I would suggest using Algolia. Links provided in References section.
As we have our Firebase on the google cloud platform, I thought setting up and integrating Elasticsearch on google’s cloud will be a breeze and I would be up and running integrations in no time (turns out I was wrong and it took longer than I thought).
So I signed up for their free trial - https://cloud.google.com/free/ and started my setup of an Elasticsearch deployment using their cloud launcher - https://console.cloud.google.com/launcher/details/click-to-deploy-images/elasticsearch. (Please make sure you familiarize yourself with the Google Compute Engine Pricing as you’ll incur charges once the free tier is used up).
Google Cloud Platform
As the Elasticsearch platform runs on the google cloud compute engine, I would suggest getting familiarized with the google cloud compute and the cloud platform in general - https://cloud.google.com/docs/overview/.
Setup Elasticsearch on Google Cloud Compute
The basic setup for Elasticsearch was pretty easy. Using the click to deploy link, click on “Launch on Compute Engine”. You might need to select (or create) the project you want to set this up on. On the next screen, you would see configuration options for your Elasticsearch VMs. You can customize this as per your requirements. I chose to reduce the machine type configuration to lower the costs. (Please refer to their compute pricing link above)
It could take a few minutes to set this up.
Connecting from development machine using gcloud sdk
To connect to our newly created Elasticsearch VMs, we need to use the gcloud sdk. The gcloud sdk allows us to tunnel into Google’s cloud platform.
Download and install the gcloud sdk. Once installed, run the following one-time setup command for your machine
The init command will prompt you to login and select a default cloud project.
You can then run the ssh command to connect to your VM (use your project name and change the zone and VM name as needed).
We’re connecting to port 9200 as that’s the default http port for Elasticsearch.
You can test to make sure that Elasticsearch is up and running by using the curl command or test it in the browser.
You should get an output that looks like this -
Sense Chrome Extension
In order to test and execute search queries, we can use the Sense (Beta) Chrome extension https://chrome.google.com/webstore/detail/sense-beta/lhjgkmllcaadmopgmanpapmpjgmfcfig?hl=en
You can now query http://localhost:9200. Use the default query to ensure that you get results -
So, at this point I thought I was done setting up my Elasticsearch cluster and I should be able to access it using my cloud functions for Firebase, given that the cloud functions run inside the app engine in the google cloud environment. Surely, their must be some way of doing that.
Unfortunately, after several days of trying, looking and asking around there did not seem to be a way to access the cloud compute engine directly from the Firebase functions. The only way to do this was to open up a port thru the firewall to allow access to the Elasticsearch to the outside world. Since, by default, Elasticsearch was listening on http and required no authentication, we needed a more secure way to do this.
So, I finally decided to setup nginx on the servers to act as a reverse proxy to our Elasticsearch engine. (For those unfamiliar with nginx, it’s a light weight web server / reverse proxy server among other things - https://nginx.org/en/)
nginx setup
The links in the nginx section of the References below helped me setup nginx to securely access Elasticsearch on the cloud. Logon to one of your Elasticsearch servers on the google cloud.
The steps are outlined here -
2. Generate SSL keys and certificate files in “/etc/nginx/ssl” folder. (Note that we’ll be using self-signed certificates). Follow prompts to enter details while creating the cert
3. Create a username/password to be used for accessing the server using basic authentication. Note the password and keep it in a secure place. We’ll use this later to connect to our server.
4. Modify the http section of your nginx.conf (in /etc/nginx folder) -
Note: We’ve setup the server to listen on port 8443. Feel free to change this as per your preference. Make a note of this as we’ll use this port to configure our firewall/external access
5. Test to make sure we get a valid response
If this works, you can replicate these steps on other servers. You can copy the ssl key, cert and password files across your servers.
To copy files from one server to another, you would need to authenticate yourself using “gcloud auth login” command the first time. You can then use “gcloud compute scp” command to copy. Refer to the gcloud documentation for more info
By default, the Elasticsearch service will not restart after a crash or a restart. To ensure that the service keeps running even after a restart, make sure it is enabled in systemd (along with nginx service)-
At this point, you still can’t access your nginx/elasticsearch from outside your (cloud compute) network. In order to access this, we would need to open the ports on the firewall to allow traffic to port 8443 (or whatever port you’ve setup above)
Go to the google cloud console and navigate to Networking -> Firewall rules. Create a new firewall rule
This should allow external access to your servers and complete our Elasticsearch setup. You can test this using the external IP address of the compute instances -
You should also try this on the Sense extension in Chrome.
Now that we’ve completed our Elasticsearch setup, we can start integrating it in our app code (code from prior part). Let’s switch back to the functions code in our app and install the elasticsearch npm module (along with it’s types). In the functions folder -
Refer to https://www.npmjs.com/package/elasticsearch for documentation.
In order to connect to Elasticsearch, we would use a json file to store our connection config. NOTE: Keep this file secure as it contains sensitive information.
elasticsearch.config.json (sensitive information. keep secure) -
Specifying multiple hosts will allow round-robin between hosts.
Let’s test our connection using a function (add this to the existing app.ts file in functions) -
Note the path to the config file.
We’ll start with indexing all our published questions data. The index name would be questions. We’ll set the type as the category id as that’ll help us with quick lookup for a single category.
If you aren’t familiar with how Elasticsearch creates and maintains indexes, please refer to the Elasticsearch guide - https://www.elastic.co/guide/index.html
In order to be able to perform search on our Questions/Answers, we’ll need to first index our questions data. We’ll add the following features to our app:
I will not go into details of all the code, you can get it from here - https://github.com/anihalaney/rwa-trivia/tree/part-13.
However, we’ll touch upon a few snippets here -
The above function will be triggered on all inserts, updates and deletes to “/questions/published” path in our Firebase database. We can check “event.data” to determine the type of modification and add/update/delete from our index.
The code for all Elasticsearch methods is separated out into it’s own file - https://github.com/anihalaney/rwa-trivia/blob/part-13/functions/ESUtils.ts
Let’s look at the getRandomQuestion method -
The method above looks for a random question matching the category ids passed in, excluding the given question ids (one that have already been asked as part of the current game). We’ll pass in an empty seed as we need new data every time.
Finally we’ll modify our “getNextQuestion” http function to call this to get the next question for the game.
Let’s deploy the app to Firebase and test it. Complete source for this part here
Next, I would like to focus our attention to building the database of questions. We’ll add features to allow for bulk upload and fix our workflow to allow for rejection and re-submitting Questions.
Bulk uploads and questions workflow
If you enjoyed this article please recommend and share and feel free to provide your feedback on the comments section.
Series summary and index: Part X
Search Engines
Elasticsearch
Google cloud
nginx
Firebase search (with Elasticsearch / Algolia)
Full Stack development of real world applications
342 
9
342 claps
342 
9
Written by

Full Stack development of real world applications
Written by

Full Stack development of real world applications
"
https://medium.com/google-cloud/service-discovery-and-configuration-on-google-cloud-platform-spoiler-it-s-built-in-c741eef6fec2?source=search_post---------133,"There are currently no responses for this story.
Be the first to respond.
Service discovery and configuration are required for almost all distributed cloud architectures. Instead of hardcoding the IP address of a service or pushing your client secret into git (which are both terrible things to do) you rely on a secure, consistent, and decentralized system for managing these configuration and environment variables.
Many different services provide this functionality; the top three that come to mind are ZooKeeper, etcd, and Consul. All of these are really awesome, and some even provide additional services such as DNS based discovery, but they require you to maintain and run them in your project, not to mention they cost money in the form of CPU time and disk space!
So let me blow your mind. Did you know that Google Cloud basically gives you this service for free? Yes you heard that right, it’s built right into your Google Cloud project! The best part is you can use it across all of our computing platforms: App Engine, Compute Engine, and Container Engine.
It’s called the metadata server, and it’s pretty awesome.
There are two types of metadata, instance metadata and project metadata.
Instance metadata is used primarily for Compute Engine. It gives you information about your instance such as IP address, zone, machine type, networks, and more. You can also set some custom metadata when you create the instance. This environmental data can be really useful for your app.
Project metadata can be used by ANY app; it doesn’t even need to be running on Google Cloud Platform! It gives your app (or apps) a shared set of environment variables and configs that they can access in a secure fashion. It uses the built-in OAuth and service accounts you already use on App Engine, Compute Engine, and Container Engine, so it’s super easy to use! This is what I will be focusing on for the remainder of this post.
The easiest way to set metadata is to use the gcloud command line tool or the Developers Console on the web. You could also use the Compute Engine API if you wanted to automate some of this stuff.
The Developers Console is self-explanatory:
Here is how to do it with the command line:
With Compute Engine, Container Engine, and Managed VMs, there is a magic URL you can CURL to get metadata. Authentication is taken care of transparently! You just need to remember to set the “Metadata-Flavor” header.
This means you can use any programing language that can make REST API calls (i.e. all of them) to query the metadata server!
The magic URL only applies to Compute Engine, Container Engine, and Managed VMs. If you are running in App Engine or doing local development, you need to use OAuth to securely connect to the metadata server.
The easiest way to do this is use a Compute Engine API library. For everything running on Google Cloud, you should be able to use the Application Default Credentials to connect to the service. Otherwise, you can download a JSON key file (called a service account) and use that to authenticate.
Here is a great resource on connecting and authenticating to Google APIs that you can follow.
I also wrote some sample code in Go demonstrating how an App Engine app can access the metadata server. Just use the getMetadata function to get all the metadata!
Before I sign off, there is one super cool feature I want to talk about: “wait for change”. This enables you to get updates when metadata values change! Check out the details here. Due to the nature of App Engine, this probably won’t work well there, but it will work great on Compute Engine, Container Engine, and Managed VMs.
I hope you find this nugget of information useful. It’s definitely a hidden gem of Google Cloud Platform and something we don’t talk about much!
Google Cloud community articles and blogs
146 
5
146 claps
146 
5
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/basic-streaming-data-enrichment-on-google-cloud-with-dataflow-sql-a7684353119c?source=search_post---------134,"There are currently no responses for this story.
Be the first to respond.
Exist many technologies to make Data Enrichment, although, one that could work with a simple language like SQL and at the same time allow you to do a batch and streaming processing, there are few and one of them is Dataflow on Google Cloud.
Apache Beam is a unified model for defining both batch and streaming data-parallel processing pipelines, as well as a set of language-specific SDKs for constructing pipelines and Runners for executing them on distributed…
"
https://medium.com/google-cloud/tracing-google-cloud-8f0c8ba8181c?source=search_post---------135,"There are currently no responses for this story.
Be the first to respond.
We think it should be easier to debug our platform. We think latency is a critical measure to say if a service is running as expected. We think it should be easier to understand service topologies. We think it should be easy to follow all significant events/requests in the lifetime of a user request end-to-end. Our users agree.
A common need on Google Cloud Platform is to able to debug production services much better. When there are many moving parts involved, it might become complicated to pinpoint the root causes of outages.
A while ago, we introduced Stackdriver Trace. Stackdriver Trace allows us to analyze a user request end-to-end and trace all internal requests until user gets a response. We have also recently open sourced parts of our instrumentation stack at Google, called OpenCensus. OpenCensus is designed to be not tied to any Google technology. It can upload metrics and tracing data to any provider or to open source tool. For us, portability of user instrumentation is a core fundamental goal.
Even though, releasing these tools is helping our ecosystem, we think we can do more. We recently started a few initiatives to provide a more compelling and useful data from our platform out-of-the-box, so our users can see and benefit from the availability of tracing data without any work. The overall goal is to provide all traces platform can generate and allow users to participate with custom instrumentation.
Allowing visibility at the platform levels helps our users to debug and determine unexpected events quickly. One other advantage is that it becomes easier to say whether outage is rooted at the platform or at the user code. User traces can internally be traced and it gives our engineers an easily way react to the customer problems and analyze the impact on our infrastructure.
Cloud Client Libraries are trying to do the best to provide the underlying details. Above, you see the handler is trying to Apply a few insertions. For most of our users, Apply is a blackbox until they see the traces and the entire dance required to commit a transaction.
The beauty and simplicity of our smooth integrations is that you only pass the current context around and the libraries are doing their job to keep tracing.
If you want to add some custom instrumentation, it is fine. You simply use the tracing library with the current context and immediately you can participate in the current trace.
See the image_processing span is appearing in the handler trace with user’s custom annotations.
We are trying to do best to gather from language runtimes and underlying components we depend on to enrich our tracing data. For example, Go requests can be annotated automatically with the networking events.
You are seeing a cold start of a server that makes a request as soon as it handles `/`. You can get very precise data on all low-level networking events occurred in the life of the outgoing request.
A second request to the same server’s same endpoint gives us a slightly different response. You can see that DNS is entirely omitted on the second request because we have to the DNS result cached from the earlier request.
Data at this granularity level is certainly helping us and our users to debug unexpected networking events. We want to enhance our capabilities as much as the layers we depend on allows us to do.
We are willing to make this smooth experience the default. We want to make it easy by providing platform traces and allowing users to trace platform traces simply by using the current context.
We want to utilize more sources of information and work on better strategies to visualize the collected data.
We are significantly improving the Stackdriver Trace UX. Feel free to chime in here and contact me at jbd@google.com if you want to do it privately.
We want to build better connectivity between monitoring data and traces, traces and profiles, traces and logs and traces and errors. We want you to be able to analyze the topologies of your systems by looking at the trace data. See a review of these features at the GCP blog.
We want better intermediate tools to capture 95th, 99th percentile cases even if you prefer to aggressively downsample and make sure we always have traces generated for them.
We want our users to have flexibility to upload their data to their choice of vendor or tools — not just to Stackdriver.
We want our users to be able to have high visibility into their own systems and the systems they depend on such as ours.
Google Cloud community articles and blogs
156 
156 claps
156 
Written by
See rakyll.org for more.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
See rakyll.org for more.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/google-clouds-spot-instances-win-big-and-you-should-too-5b244ca3facf?source=search_post---------136,"There are currently no responses for this story.
Be the first to respond.
Happy Holidays! Things are a little slow around the office, so I’ve decided to jot down some thoughts on one of my favorite features of Google Cloud — Spot Instances. Well, they’re actually called Preemptible VMs at Google. And they absolutely rock at ephemeral workloads! Here’s how and why.
The Basics
Unless you’ve been living under a rock, you probably know that Google has been the cloud price leader for a few years now. Well, it gets better:
Now that we’ve covered the basics, time for some nuances.
Instance Type Simplicity
Google Compute Engine’s offering is not segmented across dozens upon dozens of instance types. You don’t need a “networking optimized VM” to get fast network at Google, and you don’t need a “storage optimized VM” to get lots of storage. You get fast networking with any instance, and your disk decision is decoupled from instance type. We also don’t do multiple generations of instances.
With Google Cloud you simply choose your CPU and RAM combination. Of course, as I mentioned above, you can even create your own instance types this way!
In other words, Google’s not in business of stocking their shelves with broad cereal choices, instead trying to give you a simple yet functionally rich offering. Depth of offering, not breadth.
Fixed, zero-volatility Market
Google Cloud’s Preemptible VMs are not ran on a secondary market. This is excess capacity left over from Compute Engine’s decision making around Live Migration, VM sizings, Custom VMs, and customer tenancy:
Simple Operations
In addition to simple pricing and simple choice, Google Cloud offers folks some good tooling around managing Preemptible VMs:
Google Cloud Dataproc, Google’s managed Hadoop and Spark service, takes advantage of all this goodness for you, automatically repairing your jobs in case of preemption! You can run Spark with all these benefits:
If you’ve made it this far, you are probably a fan. At the very least I hope that I made you pause and think.
Google Cloud community articles and blogs
163 
3
163 claps
163 
3
Written by
VP of PM at Firebolt. All views are my own.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
VP of PM at Firebolt. All views are my own.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/analytics-vidhya/using-titan-security-key-to-protect-your-google-cloud-account-acba8620dd31?source=search_post---------137,"There are currently no responses for this story.
Be the first to respond.
Let’s think for one minute about all the data your company storage on GCP, and recall all the security controls you have when you connect to the GCP Console (VPN, Active Directory, two-factor authentication, etc).
In this remote world where you access the Google Cloud console from your personal computer or work laptop, phishing problems are raising so to fight against that Google Cloud launched the Titan Security Key.
Let’s discover why Google had no confirmed account takeovers due to password…
"
https://medium.com/google-cloud/google-cloud-functions-scheduling-cron-5657c2ae5212?source=search_post---------138,"There are currently no responses for this story.
Be the first to respond.
In a previous article, I’ve explained 5 easy steps to deploy Google Cloud Function. In this article, I’ll explain how to set up cron for it on Google Cloud Standard App Engine Platform.
The problem is that NodeJS is only available on App Engine Flexible, but it doesn’t make sense to set up flexible instance just for kicking GCF with cron.
Instead, we can create a simple python app and deploy it on App Engine Standard Platform.
We’ll need to add 3 files to the application:
app.yaml
cron.yaml
main.py
After creating these files, steps to deploy a cronjob runner are as follow:
Enjoy! 🎉
Hi, I’m Valerii. I live and write in Amsterdam. I wrote this article, all views are my own. If you enjoyed reading it, make sure to follow me on twitter https://twitter.com/viatsko
Google Cloud community articles and blogs
205 
9
205 claps
205 
9
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Senior Engineer @ Microsoft • Views are my own
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/a-day-in-the-life-of-a-developer-advocate-for-google-cloud-platform-fe681c8645cf?source=search_post---------139,"There are currently no responses for this story.
Be the first to respond.
There’s no routine when you’re a Developer Advocate for a very rich environment like Google Cloud Platform. It’s not gonna be a long post praising GCP or anything like that, but a very quick one to give you an example of a day in my life, since I joined Google.
Sometimes, I focus on a particular topic for a little while, like preparing a talk on Google Cloud Endpoints, creating a demo or two of Cloud Natural Language API, etc. And there are days, like Monday, where I do the big leg-splits, touching many areas, in the span of a single day! So what did I play with?
I started my day being a guinea pig for my colleagues who were preparing a code-lab on Cloud Dataflow and Apache Beam, launching interesting flows to analyze billions of NYC taxi rides.
I continued my day answering an interview for an upcoming white paper on Web API documentation, where I had the chance to talk a bit about Web APIs at Google and about the new Cloud Endpoints supporting Open API specifications.
I answered an internal question about our Cloud Natural Language API, with which I recently played quite a bit, for analyzing sentiments of tweets and White House speeches.
Then, to answer a customer question, I blogged about & played with the firewall configuration of the GCP cloud networking, in order to see how one can filter access to a Compute Engine VM based on the IP address of the requester.
And to finish the day, to cool down a bit from this busy schedule, I read a bit about the paper on Spanner, Google’s “internal scalable, multi-version, globally-distributed and synchronously-replicated database”. That’s a bit of a mouthful, but really awesome tech!
There are days which are way less busy than this, but sometimes, you know, that rocks to be able to touch so many topics: Machine Learning, networking, databases, Web APIs, and more. All in a single day! That’s crazy cool!
Google Cloud community articles and blogs
169 
3
169 claps
169 
3
Written by
Developer Advocate for Google Cloud Platform, Apache Groovy programming language project VP/Chair
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Developer Advocate for Google Cloud Platform, Apache Groovy programming language project VP/Chair
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/hacking-and-slacking/mysql-google-cloud-and-a-rest-api-that-generates-itself-9872fc9dc84f?source=search_post---------140,"Sign in
There are currently no responses for this story.
Be the first to respond.
Todd Birchard
Oct 24, 2018·5 min read
It wasn’t too long ago that I haphazardly forced us down a journey of exploring Google Cloud’s cloud SQL service. The focus of this exploration was Google’s accompanying REST API for all of its cloud SQL instances. That API turned out to be a relatively…
"
https://medium.com/@iamgique/google-cloud-platform-vs-amazon-web-services-gce-and-ec2-c7ddb6b93e7a?source=search_post---------141,"Sign in
There are currently no responses for this story.
Be the first to respond.
Sakul Montha
Aug 26, 2017·4 min read
ปัจจุบัน ระบบ infrastructure ที่ได้รับความนิยมสุด ๆ ในไทย และ สากล คงปฏิเสธไม่ได้ว่าเป็น AWS ของ Amazon หรือ Google Cloud จาก Google หรือ AZURE ของฝั่ง microsoft เรามาคุยกันก่อน ว่า ทั้ง 3 เจ้าที่ผมกล่าวมาคืออะไร มันคือระบบ “IaaS” ทำไมผมถึงได้กล่าวว่ามันเป็นระบบ infrastructure as a Services ก็เพราะว่ามันมีให้มากกว่าคำว่า “เครื่อง”
แล้วทำไมมันถึงเป็นที่นิยมหละหลัก ๆ เลยก็เพราะว่า เราไม่จำเป็นที่จะต้องมีเครื่องที่เป็น Physical อยู่จริง ไม่ต้องเสียเงินค่าเครื่อง ไม่ต้อง Maintenance ไม่ต้องไปนั่งดูระบบ infrastructure ไม่ว่าจะ Hardware Physical หรือ Software พวก Security ก็จะมีคนคอยจัดการให้เรา เรื่อง Avalability ก็มีคนดูแลให้เรา (ไม่ได้หมายความว่าท่านมาใช้ Cloud แล้วระบบของท่านจะไม่ล่มนะครับ) เรื่องสุดท้ายสำหรับระบบที่มีผู้ใช้จำนวนมาก Scalability มัน set ได้ด้วยว่าจะให้มัน Scale ขึ้นเมื่อไหร่ เอาลงเมื่อไหร่ (ทำให้เราประหยัดไปอีก) เราทำเพียงแค่ คลิ๊ก คลิ๊ก คลิ๊ก แล้วก็ คลิ๊ก โดยเราจ่ายเงินตามจริงเท่าที่เราใช้เท่านั้น
เกริ่นมาซะเยอะ เอาเป็นว่า ของมันดีครับ ทีนี้เราจะมาเข้าสู่บทความกัน เราจะมาเปรียบเทียบมวยคู่ระหว่าง Google Cloud Platform กับ Amazon Web Service ว่าอะไรดีอะไรเด็ดกว่ากันเป็นส่วน ๆ
เริ่มจาก Service ของทางฝั่ง AWS กันก่อน เรียกได้ว่า มีให้เลือกใช้กันไม่หมดเลยทีเดียว
ดูรายการทางฝั่ง AWS กันไปแล้ว ทีนี้มาดูฝั่ง Google Cloud Platform กันบ้าง
GCE เป็นชื่อย่อของ Google Compute Engine ส่วน EC2 มาจาก Amazon Elastic Compute Cloudก่อนจะไปเปรียบเทียบกัน ต้องบอกก่อนว่า เจ้าสองตัวเนี่ย อยากให้มองว่ามัน คือ เครื่อง server เครื่องนึงของเราครับ ข้างในไม่มีอะไรเลย จนกว่าเราจะไป คลิ๊กให้มันเป็นอย่างนู้นอย่างนี้ ซึ่ง เราไม่รู้หลอกว่ามันอยู่ตรงไหน เพราะเรามองไม่เห็น
ของทางฝั่ง GCE จะมีอยู่ 3 types ใหญ่ คือ Standard machine types กับ High-memory machine types และ High-CPU machine types ส่วนของ AWS EC2 จะมีอยู่ด้วยกัน 3 types คือ T2, M4 และ M3 ทั้งนี้ทั้งนั้นขึ้นอยู่กับการนำไปใช้งานว่าท่านต้องการ เครื่อง ขนาดเท่าใด
เราเอาแบบเล็กที่สุด มาให้ดู ซึ่งท่านสามารถเข้าไปดูรายละเอียดต่อเองได้ที่ https://cloud.google.com/compute/docs/machine-types และ https://aws.amazon.com/ec2/instance-types/
มาถึงเรื่องของราคา AWS จะคิดราคาเราเป็นราย ชั่วโมง ตามการใช้งานของเครื่องนั้น ๆ ซึ่งท่านสามารถเพิ่ม หรือ ลดจำนวนเครื่องได้ตามต้องการ (ช่วงแรกท่านไม่ต้องกังวลว่าจะเสียเงินเยอะ เขาให้ลองเล่นฟรีก่อนได้ มีวิดีโอสอนด้วย จะสร้าง จะบึ้มเครื่องอะไรทำช่วงนั้นแปปเดียวก็เป็นครับ)
มาดูฝั่ง Google Compute Engine machine types กันบ้าง Google เหมือนจะจงใจทำตลาดมาหักกับ AWS เลยรึเปล่าผมไม่แน่ใจ AWS คิดรายชั่วโมงใช่มะ Google คิดหลังจากใช้งานไปแล้ว 10 นาที แล้วก็คิดค่าบริการเป็นนาที เศษนาที คิดเป็น 1 นาที “Pay per use” แล้วก็ยังมีส่วนลดเพิ่มอีกถ้าหาก instances นั้นถูกเปิดขึ้นคิดเป็นเวลา 25% ของเดือนนั้น ๆ ทั้งสองเจ้าคิดราคาแยกเป็นไปตาม Regions ต่าง ๆ ทั้งคู่
https://aws.amazon.com/ec2/pricing/https://aws.amazon.com/ec2/pricing/on-demand/https://cloud.google.com/compute/pricing
ฝั่ง AWS เครื่องเขามี Amazon VPC (Amazon Virtual Private Cloud) ติดมาให้เลยในด้านของ Security Amazon มี ACL (Access Control List)ในในการจัดการระบบ network มาให้ สรุปง่าย ๆ คือ Amazon มีระบบกำหนดได้ว่า จะให้ใครเข้ามาที่ instances ของเราได้บ้าง โดยแบ่งได้ระดับ IP หรือ ระดับ เครือข่ายเลย(Amazon VPC กับ ACL คืออะไรขอไม่กล่าวถึงนะครับเดี๋ยวยาว)
มาที่ฝั่ง GCE กันบ้าง GCE ก็สามารถกำหนดการเข้าถึง instances ของเราได้เช่นกัน ผ่าน iptables firewall ของ Google
AWS มี Amazon Elastic Load Balancer (ELB) ที่จะช่วยปรับการรับส่งข้อมูลระหว่าง instances ของคุณภายในโซนให้เป็นไปตามต้องการ (ได้แค่ใน regions เดียวกันเท่านั้น) โดยทำการกระจายการเข้าชมไปยัง instances อื่นของคุณโดยใช้ algorithm rounded weighted round robin นอกจากนั้น ELB ยังเทพต่อไปอีกคือ ความต่อเนื่องของ Session ที่เข้ามา ปัจจุบัน ELB รองรับ IPv4 และ IPv6 HTTP และ TCP load balancing แล้วก็มีการเก็บ log ให้ด้วย
GCP มี Compute Engine load balancer มันมีที่ ELB มาหมดเลยแต่มากกว่าในเรื่อง Scaling pattern ELB: Linear ส่วน GCP: Real-time แล้วที่เทพมากคือ Deployment locality เครื่องอยู่ regions ไหน ก็ใช้ Compute Engine load balancer ได้ ฝั่ง ELB ทำได้แค่ใน regions
จากการเปรียบเทียบต่าง ๆ ที่กล่าวมาแล้วข้างต้น Service ของทาง AWS แลดูจะเยอะกว่า GCP อยู่พอสมควร แต่ว่าทาง GCP ที่มีมาให้นั้นก็น่าเพียงต่อต่อความต้องการอยู่แล้ว
- จำนวน Regions ของ AWS นำอยู่เล็กน้อย 14 ต่อ 11 แต่คิดว่าก็ไม่ได้มีผลอะไรกับเรา- Compare EC2 and Compute Engine Capacity ถ้าดูจากหน่วยเล็กที่สุด n1-standard-1 ให้ vCpu เท่ากันคือ 1 cpu แต่ AWS เสียเปรียบมากที่ Memory AWS มีมาให้ 500mb ส่วน GCE จัดเต็มมาก 3.75 แล้วถ้าเกิดเพิ่มขนาดเครื่องไปเรื่อย ๆ Memory Google จะยิ่งนำขาดไปอีก ขอนี้ Google ชนะใส ๆ- Compare EC2 and Compute Engine Pricingในด้านของราคา AWS เก็บเงินรายชั่วโมง GCP เก็บเงินตามการใช้งานจริงเป็นนาที ด้านนี้ผมให้ GCP ชนะเล็ก ๆ- Compare Compute Engine Security Groups, Network ACLs, and Firewalls ในส่วนนี้ผมให้เสมอกันไปครับ- Compare Amazon ELB vs Compute Engine Load Balancing ในส่วนนี้ จะสังเกตุได้ว่าของ Google เหนือกว่า AWS จริง ๆ ครับ load balance อยู่คนละ regions ได้ ยอมใจ
ที่เปรียบเทียบมาทั้งหมดเป็นเพียงแค่ GCE กับ EC2 นะครับ ยังมี Service อื่น ๆ ที่ผู้ให้บริการทั้งสองเจ้านี้ มีอีกมากมายที่ยังไม่ได้ถูกกล่าวถึง
หวังว่าจะเป็นประโยชน์กับทุกท่าน หากผิดพลาดตรงไหน สามารถแจ้งได้เลยนะครับ
ที่มา: https://cloudacademy.com/blog/ec2-vs-google-compute-engine/ที่มา2: https://cloudacademy.com/blog/google-cloud-vs-aws-a-comparison/ที่มา3: https://cloud.google.comที่มา4: https://aws.amazon.com
Technology Advocacy Manager, a man who’s falling in love with the galaxy.
See all (63)
196 
196 claps
196 
Technology Advocacy Manager, a man who’s falling in love with the galaxy.
About
Write
Help
Legal
Get the Medium app
"
https://rominirani.com/google-cloud-platform-factors-to-control-your-costs-5a256ed207f1?source=search_post---------142,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
This blog post provides general guidelines to help understand various factors that you should take into consideration while trying to understand your monthly cloud bill.
This is a guide for beginners to start thinking in terms of costs and making informed decisions to keep costs in control, especially during development. The goal is to make developers think from a financial point of view too and not just assume that they have nothing to do with it.
cloud.google.com
This is by no means a complete guide but a general direction to make you aware of these recommendations/suggestions. I might be wrong on a few points too since cloud billing can get fairly complex and would appreciate you pointing it out in the comments, it will help me better inform readers.
Best practice is still to regularly analyze your monthly cloud bills and take actions based on that.
In no order of preference, here are some points to consider:
Google Cloud Platform comes along with a generous Free Trial when you sign up. This has been repeated often enough so I want to go into the details here. In summary, when you sign up with your credit card, you get $300 and 12 months to try it, whichever is consumed earlier. This is good enough to try out the cloud and if you are looking to understand what you could do with $300 of the credit, I have written an article earlier on the same : You have $700 GCP Credit — Now What? . Most of it should it apply.
rominirani.com
One thing to note here — Your credit card will not be charged if you say exhaust your $300 credit. You will be notified of the same first. This is the #1 question that I get asked from folks who want to try out Google Cloud Platform. See the 3rd point in the right infobar below, when you go to the Free Trial page.
This is a top feature on Google Cloud Platform and available on other cloud platforms too. A free tier actually means “an always free tier” and what that means in practical terms is that you are given a little bit of most services and if you are within those limits, you will not be charged anything.
The above screenshot is just a few of the services and the free tier capabilities. For e.g. you get a Google Compute Engine instance for free (f1-micro instance) and so on.
The best part of the free tier is that they provide you as an individual developer, just about enough services that will help you try out things and even run things for the while. The key thing to understand here is that the free tier is baked into the monthly costs and hence all charges will be for resources that you utilized above the free tier limits.
Do note that some services might not be available at all in the Free tier
Study these in detail and chances are you are good for a long time:
Remember that the Free tier do not come with any SLAs and that is to be expected. Also this offer could be withdrawn at any time but given the dynamics and competition between the Cloud providers to bring new customers to the platform, this is likely to stay in some form or the other.
Get familiar with the Cost Calculator provided by GCP. This should be your first step to understand what it is going to cost you. Again, the cloud is all about elasticity and the fact that you might end up using more than one service or two, while you are trying out things, which is fine.
The whole idea of the cost calculator is to give you a sense of what it will be cost you. Try your best to fill in some values. One of the things that you should immediately notice is which services end up costing more, which Virtual Machine configurations end up being costly and so on.
If you are migrating your workloads to the cloud, it is recommended that you use the Total Cost of Ownership, which is a much more accurate measure of the savings that you might incur, if you were to manage the workloads on your own.
If you would like a JSON file of the current prices, take a look here.
It is recommended that you have setup/shutdown/destroy scripts for all the resources that you create. You can script this out using the gcloud, which is a command-line interface to Google Cloud Platform. Via gcloud commands, you can manage most resources and it is recommended that you have scripts to start and stop these services.
The reason for this is simple and it applies especially to compute resources. One of the costs is how much time you keep these resources in an Active State. For e.g. if you provision a Compute Engine instance and you leave it in the RUNNING state, then you are being charged for it. So the general guideline is that during development or in general, you should optimize and keep the resources in STOPPED state if you are not using them. This is not just from a cost point of view but also you will prevent them for potentially being misused too.
The best way is to be a bit disciplined from the start and have the scripts ready to execute and get into the habit of running the startup / shutdown scripts as needed.
Google Cloud Platform have introduced pricing innovations like Sustained Use Discounts, where an automatic discount is applied if you commit to using the resources for longer period. For e.g. you would get a 30% discount if you use a Compute Engine instance for a period of 30 days. You could possibly look at these options too as you move along and be aware that the more you use , you will be given the discount automatically.
Do take a look at the Google Cloud Console Dashboard too. Click on the Activity tab to see what is going on in the project too. A sample snapshot from my console is shown below:
You are charged for the storage space that you allocate and not for the space that you have used. So if you have allocated a 100GB drive, you will be charged for that and not for the 10GB space that you are using out of it.
Storage is cheap and prices continue to fall but multiply that by hundreds of storage drives and the cost could end up quickly. Do keep in mind that at times, you might to allocate much more storage space than you actually need due to the fact that certain compute configurations and IOPS requirements might demand a bigger storage drive for better performance.
As a thumb rule, SSDs are much more expensive and you should keep that factor in mind, unless you are clear on the performance requirements of your applications.
For e.g. here is the pricing on Persistent disks (Standard and SSD) in the Northern Virginia region:
One of the things to pay attention to is to identify resources that are expensive.
One way to learn is to play with the cost calculator and see the itemized cost that is provided to you. Identify resources that you will be charged for even if you do not use them. For e.g. consider a Load Balancer. A typical charge for the Load Balancer depends on the number of rules that you set and also the traffic moving in and out across to the different VMs that you have put behind the load balancer. This could easily go into several dollars a month, even if you do not put it to use. So be careful while assigning such resources, review your rules, where the VMs are and more.
For e.g. take a look at the pricing example given here: https://cloud.google.com/compute/pricing#lb
This is a very important feature and can go a long way in ensuring that you remain friends with your manager and finance team. Google Cloud Platform allows you to set up Budgets and Alerts for your project.
You can set a monthly spending limit, which will help keep the costs under control and also protect you from any unexpected surge in traffic and even a possible misuse/external attack. Check out the documentation that details how you can set up a Budget. It is easily accessible from the Google Cloud Console via Billing → Budgets and Alerts.
But before you set some arbitrary monthly limit, get a sense of a few days of usage and what it is costing and then set the right limits. If you run out of these limits, one should expect the services to fail.
Along with a budget, you should also setup Billing Alerts. Billing Alerts will notify you if you reach a certain percentage of your spending limit. This can help alert you before things could get out of control. So definitely setup Billing Alerts.
A sample screenshot is shown below. Notice that you can set the Budget amount to be either a specific amount or limiting it to last month’s spend.
This is a difficult metric to track but the point here is that you need to understand the traffic flowing into your resources and out of them.
Google Cloud Platform does not charge for Ingress but puts a charge for Egress i.e. traffic flowing out of their infrastructure. Again it depends if you are talking to a service with the same region, different region, outside of Google infrastructure, same zone, different zone and so on.
For e.g. if you make an API call from your instances hosted in Cloud A to another Cloud Providers infrastructure, you are going to be charged for the Egress. Also be cognizant of the fact that you have different services of GCP that talk to each other but are provisioned in different zones/regions.
Here is the Network pricing table for Compute Engine at the time of writing this article and it should give you a good idea of different combinations. It is not the easiest of things to predict and a lot depends on how well you understand the different services and the interactions between them in your application.
Try to come up with a rough number for the Egress but this is best judged once you have a few billing cycles in place and can see what the data is looking at. Always provision for a bit more when estimating the cost.
One of the best features of the cloud is elasticity. Cloud providers have gone to great extents to help your application scale without you taking the trouble of managing the compute that needs to be provisioned on the fly to meet your application demand.
If you see a checkbox or two, that allows you to auto-scale, keep in mind that it means that Google is provisioning additional resources for you and you will be charged for that. As the demand goes down, it will scale things down too. But you need to understand and consider this factor.
You will have to make a decision if you would like to manually reserve a fixed number of instances or let the cloud provider auto-scale. There is a cost opportunity to everything.
Google Cloud Platform services are available across several regions. Pricing is not the same across regions. You might be able to bring down your costs by considering a region that is cheaper for that particular service. But do keep in mind that you ideally want your services to be as close as possible to your customers and latency does kick in, which could hamper the user experience. So consider cost v/s latency when choosing which regions to host your services.
Additionally, there are some services that are global, multi-region or just zonal. Take some time to understand that and the costs that could kick in.
One of the most popular services on Google Cloud Platform is Google Cloud Storage, a globally available object store. Google Cloud Platform has a class for a Storage bucket, the container in which you store your objects.
The classes at a high level, differentiate across each other via things like how frequently you may want to access the data stored in the buckets and also the latency to access the data. If you are ok with the latency and your needs for frequency of accessing that data is low, you could opt for different storage class, which can definitely reduce the cost by a large extent.
Take a look at this table with different classes for the buckets and how it can affect costs.
For e.g. if you are looking at storing data for archival purpose, log files that you might access later, etc — your best options could be Nearline Storage and you could cut your cost by half. Just look at the price column in the table above and understand the various Use Cases that are best served by the respective Storage classes.
I would not like to label this section as hidden costs but it usually surprises anyone the first time. It typically applies to services where you think you are paying for just querying the data but you often overlook that to save your data, the cloud provider has to end up using cloud storage and hence will have to charge you for the storage costs too.
For e.g. consider a service that allows you to do streaming inserts of data, stores the data for you and allows you to query that data too. What does that mean in terms of cost. Well, there could be multiple things and which at a high level could be:
Let us consider BigQuery pricing as shown below:
Hope you get the gist of what this means and why you are being charged for the different operations that helps Google deliver the BigQuery service to you. Google Cloud Platform is transparent by explaining to you the different costs and states that explicitly.
Paying attention to these factors will help you retain your sanity when you are faced with an itemized bill that does not make too much sense in terms of additional costs for using a particular service.
Managed Services are great for everyone but they come with an additional cost, which is well worth it but it is something that you need to look at. For e.g. consider the Managed Cloud SQL service that is provided. Google Cloud Platform takes care of the maintenance, patches and a lot more while all you need to do is specify the configuration, click a button and you have a Database Service abstracted and running for you.
However, if you have the operations chops, you could provision a Compute Engine instance, install MySQL on it, expose a few ports, setup incoming network restrictions and get going too. The latter will be cheaper compared to the fully managed service that is provided.
During development phase, you might not have a need for Automatic Backup, Failover and Replica features that are provided as part of the Cloud SQL offering .
On the flip side, certain Managed Services will definitely help you save costs especially those in the Big Data services provided on Google Cloud Platform. For e.g. setting up and running your own Hadoop / Spark cluster is going to be both time consuming and difficult to manage and limit costs. On the other hand with a service like Dataproc, you could focus on running your jobs and spin up and tear down the Hadoop cluster with ease, thereby just charging you for the resources used for that period of time that you kept things running.
Google Cloud Platform provides an API for accessing your Billing data and optionally exporting this information into other tools for Analysis. It would definitely help to consider this if you are having a system in production and need to continuously monitor costs.
Check out the Getting Started guide with the Billing API. The Billing API provides multiple operations that include getting information on your billing accounts, projects under that, billing data for your project and more.
It is also advisable to setup a Dashboard where you can visualize your running costs. Check out the guide to Visualize Spend over time with Data Studio, an interesting approach to exporting your Billing Data over to BigQuery and then visualizing that data via Data Studio.
We have just touched upon a few high level points that you should look at to better understand and control costs on Google Cloud Platform. This is just a start and for each service, you should ideally be spending time in the Pricing section to understand it better.
Technical Tutorials, APIs, Cloud, Books and more.
179 
2
179 claps
179 
2
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
Technical Tutorials, APIs, Cloud, Books and more.
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
Technical Tutorials, APIs, Cloud, Books and more.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/how-to-backup-a-bigquery-table-or-dataset-to-google-cloud-storage-and-restore-from-it-6ef7eb322c6d?source=search_post---------143,"There are currently no responses for this story.
Be the first to respond.
BigQuery is fully managed and takes care of managing redundant backups in storage. It also supports “time-travel”, the ability to query a snapshot as of a day ago. So, if an ETL job goes bad and you want to revert back to yesterday’s data, you can simply do:
However, time travel is restricted to 7 days. There are situations (playback, regulatory compliance, etc.) when you might need to restore a table as it existed 30 days ago or 1 year ago.
For your convenience, I’ve put together a pair of Python programs for backing up and restoring BigQuery tables and datasets. Get them from this GitHub repository: https://github.com/GoogleCloudPlatform/bigquery-oreilly-book/tree/master/blogs/bigquery_backup
Here’s how you use the scripts:
To backup a table to GCS
The script saves a schema.json, a tabledef.json, and extracted data in AVRO format to GCS.
You can also backup all the tables in a data set:
Restore tables one-by-one by specifying a destination data set
The scripts use the BigQuery command-line utility bq to put together a backup and restore solution.
bq_backup.py invokes the following commands:
It saves the JSON files to Google Cloud Storage alongside the AVRO files.
bq_restore.py uses the table definition to find characteristics such as whether the table is time-partitioned, range-partitioned, or clustered and then invokes the command:
In the case of views, the scripts store and restore the view definition. The view definition is part of the table definition JSON, and to restore it, the script simply needs to invoke:
Enjoy!
After I published this article, Antoine Cas responded on Twitter:
He’s absolutely right. This article assumes that you want a backup in the form of files on Cloud Storage. This might be because your compliance auditor wants the data to be exported out to some specific location, or it might be because you want the backups to be processable by other tools.
If you do not need the backup to be in the form of files, a much simpler way to backup your BigQuery table is use bq cp to create/restore the backup:
Yes! The data is stored in Avro format, and the Avro format employs compression. The two JSON files (table definition and schema) are not compressed, but those are relatively tiny.
As an example, I backed up a BigQuery table with 400 million rows that took 11.1 GB in BigQuery. When storing it as Avro on GCS, the same table was saved as 47 files each of which was 174.2 MB, so 8.2 GB. The two JSON files occupied a total of 1.3 MB, essentially just roundoff. This makes sense because the BigQuery storage is optimized for interactive querying whereas the Avro format is not.
BigQuery can export most primitive types and nested and repeated fields into Avro. For full details on the Avro representation, please see the documentation. The backup tool specifies use_avro_logical_types, so DATE and TIME are stored as date and time-micros respectively.
That said, you should verify that the backup/restore works on your table.
Google Cloud community articles and blogs
142 
3
142 claps
142 
3
Written by
Data Analytics & AI @ Google Cloud
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Data Analytics & AI @ Google Cloud
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/how-to-write-to-a-single-shard-on-google-cloud-storage-efficiently-using-cloud-dataflow-and-cloud-3aeef1732325?source=search_post---------144,"There are currently no responses for this story.
Be the first to respond.
In Apache Beam, when you write out a text file to a blob store using:
You will get a number of output shards, like this:
The exact number depends on your dataset, number of workers, etc. Yet, many times, you will want exactly one output file because the software you are using wants just one file.
The easy way out is to tell Beam to write out only one shard:
However, this is very inefficient. The power of a distributed system comes in being able to completely parallelize the work and having a single sink will slow down your Beam pipeline.
I recently faced this problem, and solved it using Cloud Functions (code in GitHub).
In your Beam pipeline, specify a number of shards. The “compose” feature of Google Cloud Storage that we are going to use has a limit of 32 files currently, so you don’t want your pipeline producing more than that. At the same time, for efficiency sake, you’d like to have more shards than the number of workers in your Dataflow job:
When this is done, you will have a bunch of files. How do you concatenate them?
The naive way to concatenate a bunch of files on GCS is to download them to a VM, concatenate them using Unix `cat` and upload them. Don’t do that!
Google Cloud Storage supports a nifty feature called “compose”: it lets you compose a blob out of up to 32 source blobs. You can do this from the command line by doing:
No downloading or local storage needed! You can also delete the shards after the compose to avoid clutter. So, an efficient way to create single output file is to run the Dataflow job, tell it to produce up to 32 shards, and then concatenate the output using the compose functionality.
Well, you could run the Dataflow job, wait for it to finish and then invoke the compose command. But why do that when you can set up a Cloud Function to do it for you automatically whenever the sharded files show up in the bucket?
I went to the GCP web console, navigated to the Cloud Function console, created a function to trigger on Create events in my bucket, chose Python 3.7 as my preferred language and then typed in the following Python function in main.py:
I also checked the box that said ‘retry’ on failure and specified the `google-cloud-storage` package in requirements.txt.
What does the code above do? It verifies that it was triggered on a file that has the prefix my Dataflow pipeline was writing to and that this is the last shard.
Now, the last shard is not necessarily the last one to get written out. There could be a race condition, or one of the earlier files could be very large. So, I do a bit of defensive programming and make sure that all 10 shards that I expect exist. I can do that by constructing a Blob with the expected filename and check that it exists. If it doesn’t exist, I simply throw an exception. Recall that we set up the function so that it will be retried in 60s if this is the case.
Once I verify that all the shards exist, I can use the API call to compose the blobs into one large blob, and delete the individual blobs.
So, now, when I run my Dataflow pipeline, it produces 10 blobs. As soon as the 10th blob hits Google Cloud Storage, the Cloud Function runs and concatenates all 10 blobs into a single one.
Me? I can go get tea while the job runs and the function watches the job.
Google Cloud community articles and blogs
243 
5
243 claps
243 
5
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Data Analytics & AI @ Google Cloud
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/@kevinsimper/building-docker-images-with-docker-compose-on-google-cloud-container-builder-292b1eb3fd31?source=search_post---------145,"Sign in
There are currently no responses for this story.
Be the first to respond.
Kevin Simper
Feb 27, 2018·4 min read
When you develop your application you generally want to do a full system test where you spin up your whole application with databases and different services. That was pretty easy in CircleCI version 1 but came with limitations as running many customers containers is not a small feat. They did realize that and made their version 2 that I reviewed here, but it made so it was container native, which meant that you could not use docker-compose anymore without switching to their docker-machine version which they say is slower and that we don’t want, we want speed.
docker-compose is excellent at making an application easy to bring on any computer and you don’t have to think about configuring databases to be on the right ports and network and have the correct database and password. I can’t imagine a development environment anymore where you have to do that manually. So it would be a great idea to use the same commands to ensure you bring up the application the same way, both because it is simple to do but also simple to debug when something inevitable does not work.
Google has the best solution to building and storing docker images right now with their Container Builder service. It allows you to specify a list of custom commands to execute in a yaml list file. It has an easy way to define which images you want to save after a build have a successfully run.
Along with Docker multi-stage build, you will get a really steady and fast continues integration and build environment with little overhead and nothing to manage.
I explain how we do it here in this article:
medium.com
Google does a lot of cool things that would normally be a little complicated, for example running docker in docker, where you have to mount the docker daemon inside the container, something that you would not be allowed to at CircleCI and not something that is beginner friendly.
First, a you have to connect the repo on Container Builder.
Then you can either choose the simple docker build command
Or switch to use cloudbuild.yaml that allows you to write your own steps with for example docker-compose
Then you simply put a cloudbuild.yaml file in the root of the repository and you can essentially write any docker-compose commands:
The most basic one that just builds and brings up the containers from the docker-compose.yaml file.
You can then add another line at the bottom that tells Container Builder which containers it should push to the registry and a docker tag command to give it the correct tag with the commit SHA as docker-compose will just call it latest.
Now you are both bringing up the project like in development and pushing the result of the build after a successful run to the Google Cloud Registry ready to be run on Kubernetes Engine 🎉
Read more about Google Cloud Container Builder here at https://cloud.google.com/container-builder/
I really like building stuff with React.js and Docker and also Meetups ❤
242 
7
242 
242 
7
I really like building stuff with React.js and Docker and also Meetups ❤
"
https://medium.com/net-core/deploy-an-asp-net-core-app-to-google-cloud-d5ff3ff99b2d?source=search_post---------146,"There are currently no responses for this story.
Be the first to respond.
In this post, I will show how to deploy an ASP.NET Core web application to Google Cloud Platform (GCP).
I will use the following tools:
The sections of this post will be as follows:
If you are ready, let’s get started.
First, go to Google Cloud web site and click Get started for free button. Sign up with your Gmail account and get $300 (for free) to spend on Google Cloud Platform over the next 12 months.
Now, download the Google Cloud SDK.
After installation has completed, accept the following options:
Next, a web browser will be opened and you will be requested to login to your Google account (select the one you used to register to GCP).
For future reference, the following command accomplishes the same mission:
Then, you will be prompted to use an existing project or create a new one, select the latter one and create a new project.
The project ID must be a unique name and keep in mind that at the end your application’s URL will be like:
https://[Your-project_id].appspot.com
You can also use the following command to create a project:
I created the following project:
We can verify the project was created with the following command:
Now, we will initialize our App Engine app with our project and choose its region:
Run the following command and select a region when prompted:
Now, we will enable billing for our project.
A billing account needs to be linked to your project in order for the application to be deployed to App Engine.
Go to Google Cloud Console and select the project you created from the top menu and click Open:
Next, in the search box type Billing and choose the Billing menu:
In the next dialog, select Link a billing account and then set your account.
Your flexible environment deployment will incur costs while it is active. We will clean up our project when we are finished to avoid ongoing costs at the end of this tutorial.
Open Visual Studio and go to Tools -> Extensions and Updates
Search for Google Cloud Tools for Visual Studio and Install (It’s already installed on my computer):
You should restart Visual Studio after this operation.
As a side note, Cloud Tools for Visual Studio is not supported in VS 2019. Cloud Tools for Visual Studio is an open source project and there is an issue about this subject. The last message in this issue is as follows:
So, I am using VS 2017 for this tutorial.
Now, we will create our web application. Open File -> New -> Project and select ASP.NET Core Web Application and then name the project and click OK.
In the next dialog, select Web Application:
Next, we will link our application to our GCP project.
Open Tools -> Google Cloud Tools -> Manage Accounts:
and select Add account. Login to the account that you used to register to the GCP.
You will see your account logged on the top right corner of Visual Studio. There is a No Project label near this account and click that and click Select Project:
Select the project that you created above.
app.yaml
Next, add a new file called app.yaml to the project and add the following code to this file:
The app.yaml file describes an app's deployment configuration. Here, app.yaml specifies the runtime used by the app, and sets env: flex, specifying that the app uses the flexible environment.
At this point, we should build the project.
Now, right-click on the project and select Publish To Google Cloud…
In the next dialog, select App Engine Flex:
Next, you will get the following dialog:
Enable the services link did not work for me. If you experience the same issue, then go to the Google Cloud Console and write APIs & Services in the search box and then search for app engine admin api in the library:
and Enable this API.
Do the same operation for the following API too:
Next, select Publish to Google Cloud again and click Publish in the next dialog:
I got the following error in this step:
I followed the solution in this StackOverflow post and changed InProcess in csproj file with lowercase as follows:
I published again and deployment succeeded, finally :) It took 9–10 minutes for the deployment operation to complete. After it is completed, a web browser is launched automatically :
As I mentioned above, your application’s URL is
The web page is delivered by a web server running on an App Engine instance on GCP.
To avoid incurring charges, we can delete our GCP project to stop billing for all the resources used within the project.
In the Google Cloud Console, search for Manage Resources. In this menu, select your project and click Delete:
I hope you found this post helpful and easy to follow. Please let me know if you have any questions and/or corrections.
And if you liked this post, please clap your hands 👏👏👏
If you want to find out more about Google Cloud Platform, you can have a look at this GCP Essentials quest. After completing this quest, you will have a monthly subscription for free and you can take other quests in Qwiklabs.
Bye!
UPDATE: Check the following post if you are interested in learning how to deploy an ASP.NET Core application with database access to Google Cloud:
medium.com
References
https://cloud.google.com/appengine/docs/flexible/dotnet/quickstart
Posts related to .NET Core (ASP.NET Core, MVC, Web API…)
256 
2
256 claps
256 
2
Posts related to .NET Core (ASP.NET Core, MVC, Web API…)
Written by
A software developer who loves learning new things and sharing these..
Posts related to .NET Core (ASP.NET Core, MVC, Web API…)
"
https://servian.dev/3-best-features-of-google-cloud-run-546e367242ea?source=search_post---------147,"Sign in
There are currently no responses for this story.
Be the first to respond.
James Hegedus
May 6, 2019·5 min read
Cloud Run is a new serverless compute product from GCP, best summarised by Google itself:
Run stateless HTTP containers on a fully managed environment or in your own GKE cluster.
For today we’re going to cover the 3 features I think make Cloud Run the standout choice for serverless compute.
This is a contentious aspect of Cloud Run (one to be addressed separately in more detail), but one which I feel has benefits not being being considered.
FaaS solutions restrict you to using the tools provided in the default containers and those within your runtime/language of choice. What option do you have for running a useful Golang binary if you’re using a Node runtime? With Docker you have the flexibility to bring any dependencies or binaries to your application, regardless of the language/runtime of your code.
As an example see my other post:
💻 Berglas with Node.js on Cloud RunRuntime Secret Decryption with Golang & Node.js
Serverless compute tools/CLIs make the promise of providing an emulator or production compatible environment for you to test you code locally. Invariably, these tools fail to work or restrict you to specific versions of runtimes. Containers make the testing experience and prod reproduction more consistent with code and dependencies being packaged together, resulting in fewer “Works on my machine” moments.
KNative is a primitive for serverless compute built upon Kubernetes, supporting a number of missing use cases in the serverless compute ecosystem.
The KNative serving APIs allow for traffic controls across revisions of a service and how that traffic may be split. The docs say it best:
…the [route] API supports splitting traffic on a percentage basis, and CI tools could maintain multiple configurations for a single route (e.g. “golden path” and “experiments”) or reference multiple revisions directly to pin revisions during an incremental rollout and n-way traffic split. The route can optionally assign addressable subdomains to any or all backing revisions.
Finally, easy experiments/betas and blue/green deployments with serverless compute! NB: Cloud Run itself is yet to surface these controls, but it’s coming!
KNative allows Cloud Run to be deployed to either Google’s managed paltform (Borg) or your own Google Kubernetes Engine. Ahmet describes this best:
Why would anyone want to use GKE over the managed version? Well with the managed version you only get to customise the RAM, Timeout, Concurrency and Region. Whereas GKE allows for much more:
In addition to this, some customers experience restrictions with services for data sovereignty reasons. For example, Cloud Functions cannot be used by some business to process data as it cannot be deployed to specific regions, for example, australia-southeast1. Deploying Cloud Run to a GKE instance within this region allows the use of the same event-driven programming model where it otherwise wouldn’t be usable.
GCP has overhauled it’s HTTP story with better IAM integration and converging on an open source event specification.
🔐 IAM HTTP
Cloud Run services are exposed via IAM secured HTTP endpoints, which are private by default. Any invokers must be whitelisted with the correct role. Cloud Run supports all the expected member types (pictured below):
To make an endpoint public assign the allUsers member.
Cloud Run service to service invocation requires the invoker’s Service Account to be assigned to the Cloud Run Invoker role in the receiver’s members list. You can then validate the token in the receiving service.
Cloud PubSub, Cloud Scheduler & Cloud Tasks each support push events to HTTP endpoints. GCP will insert an OIDC JWT token in the header on behalf of the sender so you can validate the IAM role & permissions in the receiver. The aforementioned services will automatically validate this token before forwarding payloads to your code.
Hopefully we see more HTTP push events from other GCP resources in the future to close the “gap of resource triggers” between Cloud Functions and Cloud Run.
Cloud Events is an open source event specification to normalise payload consumption across cloud providers enabling more portable code. Google is providing libraries that automatically unmarshal events conforming to the CloudEvents spec, supporting functions capable of executing in many environments (Cloud Functions, Cloud Run and KNative) with no code changes required for interpreting the event payloads.
Stay tuned for an in-depth look at FaaS on GCP and how Cloud Events and new libraries will effect future runtime support, development & portability.
With a consistent development & testing experience from local to prod, IAM secured communication, consistent event delivery, and the portability & traffic controls of KNative, Cloud Run is a uniquely powerful serverless compute offering. Coupled with a simple deployment path it’s sure to become the compute service of choice.
📚 Cloud Run Product Overview📯 Cloud Run Release Blog Post📹 Cloud Run Launch Next 19💻 Awesome Cloud Run⚡ Cloud Events☁️ KNative
If you found this useful, please share with your friends & colleagues.
Read more GCP insights from Servian here.
GCP, Firebase, Sveltejs & Deno fan! @jthegedus on all platforms.
194 
Some rights reserved

194 claps
194 
At Servian, we design, deliver and manage innovative data & analytics, digital, customer engagement and cloud solutions that help you sustain competitive advantage.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/running-a-mean-stack-on-google-cloud-platform-with-app-engine-and-mongolab-4bbd2040ea75?source=search_post---------148,"There are currently no responses for this story.
Be the first to respond.
In two of my previous posts, I talk about running a MEAN stack with Docker and then making it more robust by using Kubernetes.
In this post, I’m going to go on a tangent and take a different approach. Let’s say you don’t care about all that complicated stuff, you just want your MEAN application to run and scale without worrying about VMs and Pods and Docker and blah blah blah…
To do that, it makes sense to use a Platform as a Service (PaaS), which is a fancy way of saying you write the code and we run the code. There is no server or database management needed, which lets you focus on building a great product!
Google’s PaaS is called App Engine. App Engine can run Node.js and handle all the scaling and maintenance so you don’t have to.
For the database, I’m going to use MongoLab. It’s the MongoDB you know and love, but you don’t need to worry about hosting it.
Sign up for MongoLab.
Once your account is created, you should see a dashboard.
Click “Create New”.
I’m using the free plan with Google Cloud Platform.
Also, make sure the Database Name is mean-demo, because that’s what our sample app expects.
Your config should look like this:
Once you click “Create new MongoDB deployment”, you should see your new Database up and running:
Click the database, and note the connection URI.
Now, create a user so the app can connect to the database:
Done.
Go to console.developers.google.com and select (or create) your project. For the rest of this tutorial, I’m going to be using Cloud Shell, which is basically a command line in your browser that has all the tools you need already installed. It is really cool!
Now, pull in the sample code:
And fix the configuration so you can connect to your MongoLab instance:
Replace <dbuser> and <dbpassword> with what you specified in Step 1.
Replace XXXXX with your Database ID (in my case, ds049624).
Replace #### with your Database port (in my case, 49624).
All you are doing here is changing the MongoDB client to use your MongoLab instance.
With Cloud Shell, you can easily test your code just like it was running on a local machine.
You should see this:
Now, launch the Web Preview (Click the little button on your Cloud Shell):
And ta-da, it works!
If you add some data, it will add it into your MongoLab database. Try it!
While Cloud Shell is great for testing, it can’t host your app! It’s time to deploy to production!
In the code, there is a file called app.yaml, which contains all the deployment details. Let’s take a look:
Pretty self-explanatory. The vm: true option lets App Engine know it should use the Managed VM runtime (i.e. Docker).
To deploy, run:
Or you can use the shorthand (thanks to the package.json file):
And the deployment will begin.
gcloud will pull the project configuration from the Cloud Shell environment and your app.yaml file.
App Engine will copy your code, create a Docker container, spin up VMs and load balancers, and launch your code. You just sit back and relax.
Once you see this:
You are done. Go to the URL to see your MEAN app running!
That’s it! Now you don’t need to worry about servers crashing or database management. Of course, you lose some flexibility because you are not nitpicking every little detail, but for most applications App Engine works great!
For more info, check out the Node.js getting started guide and the MongoLab docs.
Google Cloud community articles and blogs
151 
9
Thanks to Jack Wilber. 
151 claps
151 
9
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/origin-launches-decentralized-commerce-on-google-cloud-marketplace-b74fb46be7d9?source=search_post---------149,"There are currently no responses for this story.
Be the first to respond.
We’re excited to announce that Origin has partnered with Google Cloud to bring our decentralized commerce platform to Google Cloud’s broad global network of developers and customers.
As part of the partnership, Origin has made its Dshop e-commerce platform available on the Google Cloud Marketplace. Merchants can deploy and create their own decentralized e-commerce store in minutes. In addition, publicly available product data from Origin’s merchant partners will now be available in Google BigQuery. Developers, data scientists, and other interested parties will be able to quickly analyze data that is generated on the Ethereum blockchain and IPFS via Origin’s network of partner Dshops.
Google Cloud Marketplace integration
Origin’s Dshop 1-click deployer is now available on Google Cloud Marketplace, allowing sophisticated developers, large-scale e-commerce merchants, and individual sellers alike to deploy their own decentralized stores within minutes.
As a Google Cloud Partner, Origin is committed to working closely with Google Cloud to market our decentralized store platform to a wide array of merchant partners while driving new customer growth and revenue for Google Cloud. We envision a future where thousands of Dshops across many different verticals (e-commerce, digital goods, rentals, other services) are powered by the Origin technology platform running on Google Cloud infrastructure. We look forward to working with the Google Cloud team to bring new technology integrations to the market in the near future.
You can launch your own Dshop on Google Cloud in just a few minutes. Check out our guide on getting started. Once deployed, the tutorial video below will show you how to finish setting up your Dshop.
Google BigQuery integration
Origin Dshop data is now also publicly available in Google BigQuery. Because Origin is built on the Ethereum blockchain and IPFS, product listing data from merchants that have opted into publishing their data on-chain is public. The blockchain is the authoritative source of truth for all product data that is generated by these merchant partners. These merchant partners want their products to be easily discovered by customers and affiliates that help promote products and drive sales. However, the blockchain is difficult to index, making data analysis difficult and time consuming.
With today’s announcement, Origin’s Dshop data will be made available in Google BigQuery by using the Ethereum ETL library that Google Cloud participates in.
Simplistically, the Google BigQuery integration is like bringing Google Search to blockchain data. This means that affiliates can easily discover products to promote, benefiting merchants with increased sales. Data scientists can perform analyses to understand what type of merchants and products are being powered by the blockchain. Developers can look at the data and build new, powerful third-party apps that utilize data from Origin’s shared public data layer.
Relevant datasets are at the origin-214503.marketplace.listings and origin-214503.dshop.products tables on Google BigQuery.
At Origin, we’re excited about generating interesting and useful data as we increase our install base of Dshops. We’re especially looking forward to seeing what our broader group of developers, affiliates, and other ecosystem participants builds on top of this open data.
Learn more about Dshop
Origin Dshops are decentralized stores that are fully customizable, censorship-resistant, and free to use. Developers and merchants that run Dshops have full control over their seller experience.
Dshops come equipped to accept cryptocurrency payments such as Ethereum (ETH) and ERC-20 tokens like Origin Tokens (OGN) in addition to traditional credit cards and PayPal, giving merchants and consumers flexible payment options. Origin’s decentralized stores also come with native integrations with select dropshipping service providers, making fulfillment of customer orders easy and painless.
Learn more about Origin
Google Cloud community articles and blogs
1K 
1K claps
1K 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Cofounder Origin, early YouTube, avid traveler
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/data-engineering-on-google-cloud-platform-coursera-courses-ca24840d2a3a?source=search_post---------150,"There are currently no responses for this story.
Be the first to respond.
I’ve been heads down the past few weeks recording a bunch of Coursera courses that together form the Data Engineering on Google Cloud Platform specialization. Please enroll in these courses to learn how to ingest, transform, and learn from data on Google Cloud Platform:
Fortunately, all I had to do was to talk into a microphone — my colleagues who are video production wizards did the rest. And of course, the product & engineering teams helped create the products that make GCP such a pleasure to use.
p.s. So, why are the courses paid? Three reasons: (1) Coursera needs to cover their costs — support their great platform by paying. (2) Think of it as a subtle nudge — we have noticed that completion rates for our paid courses are 5x more than completion rates for our free courses (75% vs. 15%). (3) The course fee will, once we iron out a few details, cover the cloud computing costs so you don’t have to use your own GCP project to try out the labs.
Google Cloud community articles and blogs
99 
2
99 claps
99 
2
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Data Analytics & AI @ Google Cloud
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://itnext.io/getting-started-with-red-hat-ansible-for-google-cloud-platform-fa666c42a00c?source=search_post---------151,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
In this post, we will explore the use of Ansible, the open source community project sponsored by Red Hat, for automating the provisioning, configuration, deployment to, and testing of resources on the Google Cloud Platform (GCP). We will start by using Ansible to configure and deploy applications to existing GCP compute resources. We will then expand our use of Ansible to provision and configure GCP compute resources using the Ansible/GCP native integration with GCP modules.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/firebase-developers/multi-tenant-applications-with-firebase-and-google-cloud-4d0d02b7d859?source=search_post---------152,"There are currently no responses for this story.
Be the first to respond.
Google Cloud Identity Platform (GCIP) allows you to add Google-grade identity and access management support to your own applications. Enterprises can use GCIP to manage the identities of their employees, customers, partners and IoT devices at scale. In November 2019 Google further extended GCIP with support for tenant management. This new feature enables enterprises to define multiple tenants…
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@kevinsimper/google-cloud-storage-cors-not-working-after-enabling-14693412e404?source=search_post---------153,"Sign in
There are currently no responses for this story.
Be the first to respond.
Kevin Simper
Aug 13, 2018·1 min read
If you experience that the Google Cloud Cors is not working even after you did “gsutil cors set cors.json gs://your-bucket/”, it may be because you are using the wrong root domain, only one of the domains will have the headers enabled on the request, so
storage.googleapis.com/your-bucket ⬅ will not have headers
your-bucket.storage.googleapis.com ⬅ will have cors headers
Something very easy to overlook and Google by default links to the storage.googleapis.com/your-bucket from the interface, so if you copied that and thought it work.
They do say it here, but that is the only place, which I think is not enough
https://cloud.google.com/storage/docs/cross-origin:
I really like building stuff with React.js and Docker and also Meetups ❤
370 
11
370 
370 
11
I really like building stuff with React.js and Docker and also Meetups ❤
"
https://medium.com/@sathishvj/notes-from-my-beta-google-cloud-professional-network-engineer-certification-exam-2b08d852aae1?source=search_post---------154,"Sign in
There are currently no responses for this story.
Be the first to respond.
sathish vj
Feb 5, 2019·7 min read
Subscribe to my YouTube channel that teaches you to apply Google Cloud to your projects and also prepare for the certifications: youtube.com/AwesomeGCP. Check out the playlists I currently have for Associate Cloud Engineer, Professional Architect, Professional Data Engineer, Professional Cloud Developer, Professional Cloud DevOps Engineer, Professional Cloud Network Engineer, and Professional Cloud Security Engineer.
Update on March 14th, 2019: In a pleasant surprise, I actually passed this exam. I’m now a …
The original notes continue below.
I messed up with this exam. And hopefully you won’t if you learn from my experience. Read on.
I recently passed the beta Professional Cloud Developer exam with hardly any preparation because there really wasn’t any solid material to prepare with. With the beta Professional Network Engineer exam though, there was a full networking specialization on Coursera. I took that course and revised it a few times prior to the exam. That was way more than what I’d done for the Developer exam. So I was very confident. I expected it to be a breeze. It wasn’t. Results should be out in a couple of months and I would be pleasantly surprised if I didn’t flunk.
I’ve hardly been in a situation over the four GCP certifications that I passed where I haven’t been able to eliminate at least two options. This one was different. But now having reviewed the questions and the subjects post the exam, I realize it was entirely my fault.
I assumed the questions would be primarily based on the Coursera course. Though that course was important to set the base, I really needed to study all the subjects on the exam guide. The only part of this that I prepared for was what was in the Coursera course. That’s less than 50%. Facepalm!
When you study for the Network Engineer exam, do the Coursera course first. Then search on the GCP docs for each of the topics mentioned in the exam guide. Do the quick labs and probably more than just that. There’s a bunch of stuff you can’t really try out, I suppose, like Peering and Interconnect. But you need to at least know the theoretical aspects well. Some of the other use cases in the exam are not straightforward — multiple orgs, on-prem situations, etc. You can still simulate them with multiple VPCs with an external IP.
tldr; do the Coursera Networking Specialization first. Then search for and study the items in the exam guide/outline.
Below are my sanitized notes from the exam. I’m not giving you questions but general topics similar to what the guide/outline gives. Matthew Ulasien from Linux Academy says that the Architect exam is a mile wide and an inch deep. Comparatively, the Network Engineer exam is about 10 metres wide and 10 metres deep. There isn’t a wide breadth of GCP products to cover in networking alone. Instead, a lot of the feature details are tested. As usual, application of the features are tested in use cases.
Since this was a beta exam, I got 95 questions to finish in 4 hours. There were multiple network issues in my exam centre and it took me a total of ~5 hours. So definitely carry some snacks and drinks if you’re going for the beta exam. Your break time is on you and the exam clock doesn’t stop.
• Protocols: study a little about protocols and networking features beyond GCP. At least know basics of some — OSPF, BGP, RIP, TFTP, SSL, SMTP, ICMP, SNMP, IMAP, etc. • Troubleshooting: troubleshooting networking issues — learn usual network troubleshooting like packet loss, non-reachability, traffic routing, etc.• Troubleshooting: How do we debug the above issue? Know some common cmd line utilities like traceroute, nslookup, netstat, etc.• Troubleshooting: Where can we find logs to troubleshoot? Stackdriver Logs, Monitoring, Flow logs, syslog on the machine, etc.?• BGP: How is this setup under different conditions like single cloud router, multiple cloud router, various types of interconnect, subnet accessibility, etc. • Cloud DNS: how do you configure one? how do you migrate one?• Cloud DNS: DNSSEC setup and troubleshooting when DNS updates don’t propagate.• GKE networking: I was woefully underprepared for this section. • IP Aliases: partly related to GKE was IP Aliases. • CIDR: know how IP ranges are defined using CIDR. Also that the first 2 and last 2 in any subnet is reserved by GCP. So if you want to assign a certain number of nodes, pods, and services, what CIDR values do you need to set.• Cloud Router: understand everything in this — https://cloud.google.com/router/docs/concepts/overview• Cloud Router: advertising routes.• Cloud VPN: understand everything in this — https://cloud.google.com/vpn/docs/concepts/overview• Cloud VPN: Policy based VPN vs route based VPN.• Cloud VPN: creating multiple tunnels.• Firewalls and Routes: know that the lower number has higher priority than higher numbers.• Firewall: logging and how it helps to identify issues.• Load Balancer: between the various load balancers, which are global and which are regional?• Load Balancer: which load balancers support ipv4 and which ipv6?• Load Balancer: how do you setup certificates? • Load Balancer: how do you set up Managed Instance Groups and Unmanaged instance groups.• Load Balancer: what are target pools? how are they used and setup?• Google Private Access and Google Private Service Access: learn the various use cases.• Cloud CDN: cache-invalidation. How to do it? What are the different ways to do it?• Cloud CDN: compression options.• Cloud CDN: how do you connect Load Balancer to CDN?• Cloud CDN: how do you connect Cloud Storage to CDN?• Network Connectivity: How to connect various networks when they are in same/different org, same or different company, same or different project but needs isolation, etc.• How to setup interconnect in a way that is reliable, with minimum latency, etc.?• Cloud NAT: What is this and why would you do this?• Cloud Armor: same, what and why?• Interconnect, Peering, VPN — know the max speeds for each.• Interconnect, Peering, VPN — know the connection type for each. Public IP or RFC1918?• Interconnect, Peering, VPN — which of them can you have multiple copies of and how high a bandwidth can you get?• Interconnect, Peering, VPN — how do you diagnose low bandwidth usage, network issues, etc.• VPC: Security Perimeter, Service Controls, Service Context.• SSH: ports to connect on. Firewall rules. Connecting to a VM via SSH or RDP and diagnosing issues.
I feel like I’ve taken one for the team by attempting this exam early. :-) I partially feel like crap for being an idiot and not studying properly, but I also feel good knowing that you all will be much better off than me now that you have my experience. #getcertified.
Others’ Notes
See Hil Liao’s additions in the comments below.
Google Cloud Certified — Professional Network Engineer
For those appearing for the various certification exams, here is a list of sanitized notes (no direct question, only general topics) about the exam.
Overall notes across all GCP certification exams
Notes from the Professional Cloud Architect exam
Notes from the beta Professional Cloud Developer exam
Notes from the Professional Data Engineer exam
Notes from the Associate Cloud Engineer exam
Notes from the beta Professional Cloud Network Engineer Exam
Notes from the beta Professional Cloud Security Engineer Exam
Notes from the Professional Collaboration Engineer Exam
Notes from the G Suite Exam
Notes from the Professional DevOps Engineer Exam
Notes from the Professional Machine Learning Engineer Exam
Main Link — https://cloud.google.com/certification/cloud-network-engineer
Topics Outline — https://cloud.google.com/certification/guides/cloud-network-engineer/
Practice Exam — https://cloud.google.com/certification/practice-exam/cloud-network-engineer
A collection of posts, videos, courses, qwiklabs, and other exam details for all exams: https://github.com/sathishvj/awesome-gcp-certifications
I’ve collected here a bunch of free Qwiklabs codes which are awesome to get lots of hands-on practice. Use them well.
medium.com
Check the FAQs here: https://medium.com/@sathishvj/frequently-asked-follow-up-questions-on-google-cloud-gcp-certifications-438e1addb91d.
Wish you the very best with your GCP certifications. You can reach me at LinkedIn and Twitter. If you can support my work creating videos on my YouTube channel AwesomeGCP, you can do so on Patreon or BuyMeACoffee.
tech architect, tutor, investor | GCP 12x certified | youtube/AwesomeGCP | Google Developer Expert | Go
193 
3
193 
193 
3
tech architect, tutor, investor | GCP 12x certified | youtube/AwesomeGCP | Google Developer Expert | Go
"
https://blog.devgenius.io/a-developers-guide-for-passing-google-cloud-associate-cloud-engineer-exam-8a95adb44721?source=search_post---------155,"One of the goals of mine was to get certified in Cloud. When they said, “Everything is moving to cloud” a few years back, I didn’t really understand, but then Microservices architecture gained…
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/internal-load-balancing-for-kubernetes-services-on-google-cloud-f8aef11fb1c4?source=search_post---------156,"There are currently no responses for this story.
Be the first to respond.
As discussed in my recent post on kubernetes ingress there is really only one way for traffic from outside your cluster to reach services running in it. You can read that article for more detail but the tl;dr is that all outside traffic gets into the cluster by way of a nodeport, which is a port opened on every host/node. Nodes are ephemeral things and clusters are designed to scale up and down, and because of this you will always need some sort of load balancer between clients and the nodeports. If you’re running on a cloud platform like GKE then the usual way to get there is to use a type LoadBalancer service or an ingress, either of which will build out a load balancer to handle the external traffic.
This isn’t always, or even most often what you want. Your case may vary but at Olark we deploy a lot more internal services than we do external ones. Up until recently the load balancers created by kubernetes on GKE were always externally visible, i.e. they were allocated a non-private IP that is reachable from outside the project. Maintaining firewall rules to sandbox lots of internal services is not a tradeoff we want to make, so for these use cases we created our services as type NodePort, and then provisioned an internal TCP load balancer for them using terraform.
Creating an external load balancer is still the default behavior, but fortunately there is now a way to annotate a LoadBalancer service so that Google Compute Engine will build out an internal load balancer with a private IP. When this feature was first released a couple of months ago it didn’t work right, and so we continued to wire this stuff up manually. It’s since been fixed and I was recently able to test it and confirm that it works, so now seems like a good time to take a quick look at what this can do for you. Here’s a sample service:
As you can see above the change is a simple one-liner annotation. Assuming we have some endpoints (pods) to handle this traffic, then when we create this service we will, after a minute or two, see an internal lb pointed at it:
The ‘EXTERNAL_IP’ in that column header means “outside the cluster,” and as you can see the allocated address is in the private address space of the project. Pretty cool. There are a couple of limitations to internal load balancers, however, that you should be aware of. They are regional devices and can’t handle traffic to/from vms in other regions. They are layer 4 and can’t handle http/s traffic at layer 7, meaning no virtual hosts, path-based routing, tls termination, etc. They can only handle a maximum of five ports. After that you’ll need to make another lb. That last one isn’t much of a restriction since as far as I know kubernetes will drive GCE to create a new internal lb for each service port anyway. That’s something I’ll try to test and report back on later.
In any case, this is nice feature to have and will make it easier for us to spec and build out the networking stack for our kubernetes services from within our helm chart repositories, rather than having to maintain a separate terraform configuration that relies on the manual discovery of backend instance groups in order to properly configure an internal load balancer. As always when it comes to configurations less is more.
Google Cloud community articles and blogs
161 
5
161 claps
161 
5
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Senior Devops Engineer at Olark, husband, father of three smart kids, two unruly dogs, and a resentful cat.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://towardsdatascience.com/access-free-google-cloud-public-dataset-with-python-42eb378be72c?source=search_post---------157,"Sign in
There are currently no responses for this story.
Be the first to respond.
Joe T. Santhanavanich
May 17, 2020·5 min read
Good news for Data Scientists! Google is making a hosted repository of public datasets, like Johns…
"
https://medium.com/google-cloud/planet-scale-microservices-with-cluster-federation-and-global-load-balancing-on-kubernetes-and-a8e7ef5efa5e?source=search_post---------158,"There are currently no responses for this story.
Be the first to respond.
I recently gave a talk at Google Cloud Next about using Kubernetes to deploy and manage microservices around the world using Kubernetes.
I’ve blogged about how you can set up MongoDB in a StatefulSet already, so now I’m going to deep dive on how you can set up cluster federation and deploy your services around the world!
I’m going to assume you know the basics of creating a project in Google Cloud and have installed and set up the Google Cloud SDK. If not, please check out my previous blog posts.
We are going to be looking at three things specifically, Cluster Federation, Federated Ingress, and cross cluster service discovery. I’m going to split these topics into three blog posts so look out for the next ones soon!
Cluster Federation is what you can use to manage multiple Kubernetes clusters as if they were one. This means you can make clusters in multiple datacenters (and multiple clouds), and use federation to control them all at once!
Federated Ingress is super sweet, and is currently only supported on Google Cloud. This allows you to spin up a single load balancer with a single global IP address that will dynamically send traffic to the nearest cluster automatically!
Cross cluster service discovery is a concept that let’s services in one Kubernetes cluster find services in other clusters automatically. We will also look at Google Cloud Pub/Sub to enable asynchronous microservices!
DNS is core to Kubernetes, as it is used for service discovery in the cluster. While running a local DNS server works inside a cluster, you need to have a central DNS service for cross cluster service discovery. You can use any programmable DNS service to do this, and we are going to use Google Cloud DNS.
The first step is to purchase a domain name (or use one you already have). In my case, I used Google Domains to buy a cheap domain name. For this example, I’m going to pretend the domain I bought is “mycompany.com”
Now, you need to create a managed DNS zone. Let’s use the “infra” subdomain to handle the Kubernetes services, so this means the domain will be “infra.mycompany.com.” I’m also going to call this zone “kfed”, but you can call it whatever you want.
If you go to the Cloud DNS console, you should see this new zone created.
Click the Zone Name, and you will see more details, including the name servers:
At this point, go to your domain name registrar and use the DNS servers from the new managed zone. Here is how it looks in Google Domains:
The fun and easy step. Let’s make two small Kubernetes clusters to federate.
Create the first cluster (let’s use the us-west1 datacenter):
And now the second (let’s use the us-east1 datacenter):
After a few minutes, both clusters should be up and running.
The federation control plane is what manages the global state of all your federated clusters. The neat thing is the control plane can be self hosted inside one of your Kubernetes clusters, so you don’t even need to set up anything extra!
Note: Even if the Control Plane cluster goes down, other the clusters themselves are independent, so they will continue to function until the control plane comes back up. You can still control the individual clusters individually! This means you don’t have to worry about a single failure taking down all your clusters!
The first the you need to find is the context names for your Kubernetes clusters. Run this command:
You should see output like this:
You can see that each cluster has it’s own context, and the little star indicates which context is currently active.
There is one extra step that you need to do here. Kubernetes Federation uses the context name to set up the federation secret, but it needs to conform to the RFC1123 spec. This means you need to rename the context. You can do this with the following commands:
Replace $(CLUSTER_ZONE) and $(PROJECT_ID) as needed.
To make the process of deploying federation easier, we will use the kubefed tool. Let’s use the west-coast cluster to host the control plane (because West Coast = Best Coast).
Note the trailing period at the end of the dns-zone-name. This is very important!
At this point, the federation control plane will deploy. Give it a minute or two to fully deploy. This command will create a “virtual” context called “kfed.” When you use the “kfed” context, you are addressing all the federated clusters.
The next step is to add both clusters to the federated control plane. You can use the kubefed tool to do this as well.
With these two commands, both clusters are now federated! You can check the state of all federated clusters with this command:
There is currently an issue with kubefed that prevents it from creating the default namespace. Run this command to fix it:
Now the federation is set up, you have to make sure the DNS servers inside each cluster know to use the DNS Zone you created to perform cross-cluster service discovery.
The first step is to create a Config Map that the Kubernetes DNS controller will automatically use to connect to the DNS Zone
Replace the domain name with yours, and save the file as kubedns-config.yaml
Now create the ConfigMap:
One important thing to remember is to use the right context when running your kubectl commands. If you run commands on the federated context, the commands usually propagate to the underlying clusters. If you run commands on an individual cluster, the commands stay within that cluster.
That’s it! You have successfully created a set of federated kubernetes clusters!
Read Part 2 where we set up Federated Ingress!
Google Cloud community articles and blogs
229 
8
229 claps
229 
8
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@sathishvj/google-cloud-platform-authorized-trainer-1b202f3eef61?source=search_post---------159,"Sign in
There are currently no responses for this story.
Be the first to respond.
sathish vj
Apr 2, 2019·15 min read
What it is, who can apply, how to apply, and the interview process.
Subscribe to my YouTube channel that teaches you to apply Google Cloud to your projects and also prepare for the certifications: youtube.com/AwesomeGCP. Check out the playlists I currently have for Associate Cloud Engineer, Professional Architect, Professional Data Engineer, Professional Cloud Developer, Professional Cloud DevOps Engineer, Professional Cloud Network Engineer, and Professional Cloud Security Engineer.
Edit May 1st 2020: There is now an official website for the Google Authorized Trainer: https://sites.google.com/view/gcptrainer/home
Edit Nov 2019: I see that there is a new logo/badge for the authorized trainer.
Edit Dec 2019: There is now an official certificate issued for authorized trainers. Mine is here: https://googlecloudcertified.credential.net/profile/40a1ddc5b98dbac0952e85b616a19c5d74c6e762
I’d been conducting workshops and trainings for many years now. When a training coordinator learnt that I had picked up a few Google Cloud Certifications, he asked me whether I would like to try out to be a Google Cloud Authorized Trainer. I replied, “Huh? What’s that?”
Unfortunately, there’s hardly any information about this online. So here are my notes on the whole process. This process involves people only and therefore potentially subjective decisions and actions. It isn’t as clear cut and objective as the multiple choice certification tests. My intention is not to pass judgement on the process or any people. I’m only putting down notes from my perspective so that you can be better prepared when you attempt to become a GCP trainer.
Process: https://sites.google.com/view/gcptrainer/process
Trainer Evaluation: https://sites.google.com/view/gcptrainer/evaluation
Trainer Onboard: https://sites.google.com/view/gcptrainer/onboard
You have to be Google Cloud Certified — so far, the Professional Architect or the Professional Data Engineer,o̵r̵ ̵t̵h̵e̵ ̵P̵r̵o̵f̵e̵s̵s̵i̵o̵n̵a̵l̵ ̵C̵l̵o̵u̵d̵ ̵D̵e̵v̵e̵l̵o̵p̵e̵r̵. (Edit: to become a trainer even for the Application Development track, apparently you require the Professional Architect certification.)
I have a question here that I do not have an answer to yet. Assume you became an Authorized Trainer based on the Professional Architect. Later you also got the Data Engineer certification. Are you now automatically qualified to also teach Data Engineering? Or do you have to go through the process again for this? I’ll try to find that out later. (I got the answer, and it is this: the authorized trainer selection process is only once. If you become an Authorized Trainer, say, for the Architect track once, and later got certified as a Data Engineer, then you are automatically qualified to teach that also. There is no second evaluation.)
As of now, there is no restriction on what course an Authorized Trainer can take — they can conduct any of the courses irrespective of the certification that they have. So, let us say you have a Professional Architect certification and no others. You can still conduct a course of Networking, Security, DevOps, etc. Of course, it would be definitely preferable that you have the corresponding certification, but it is not mandated.
Note that, this could change in the future.
Interestingly, you can’t apply to be an Authorized Trainer on your own as an individual. Google ties up with Authorized Training Partners (ATP). (Edit: This doesn’t mean that you have to be employed by that training institute. You can remain independent while you are affiliated with them, like most of us are.) I don’t know the process well, but here is the little I learnt. Google selects ATPs based on their business history in coordinating trainings. So if your firm was just starting out, the chances that you will become an ATP are slimmer than a well established training firm. The ATP itself must have a minimum of two Authorized Trainers associated with them before they are allowed to coordinate trainings.
So, the ATP fills up a form with your name and details on it and submits it to Google. You fill up additional details on the link provided. Google then contacts you via email later for the next steps of the process.
The very first email I got gave me some topics that I had to prepare to present during the evaluation. All the material was on QwikLabs. Three subjects were chosen. You get slide decks (pdf) corresponding to them.
During the evaluation, we could be asked to present any of those topics for about an hour. For me, in one of the evaluations, I primarily presented one and the evaluator wanted to do the other subject also but there wasn’t enough time. However, he did still ask questions from the other module. The questions weren’t necessarily restricted to points in the slides; so you have to know the subject way more.
Where can I find Authorized Training Partners in my region and in my domain?
You can find the full list of training partners here: https://cloud.withgoogle.com/partners/?products=TRAINING_PRODUCT
Then you can filter by your region and also your specialization.
There is no money to be paid for applying or for getting the authorization.
Once the ATP and I had submitted our details, it was a period of silence lasting about a month. I thought things might be stuck, but from what I understand now, it is a process that stretches over time.
That first email asked me to pick a few time slots in the weeks ahead to see if an evaluator was available. These evaluations are typically done on a Friday, which was mentioned in the email. So I replied with the next three Fridays as options. I suppose those dates didn’t work out. I got another mail asking for new dates. The evaluation was scheduled within the new dates. From application to first evaluation, it had already been 7–8 weeks.
There wasn’t much information on the internet about the process. So I contacted an ATP who knew somebody who’d gone through the evaluation successfully. This person's experience was seemingly very different from mine. So I’m putting this down based on what I remember from the conversation sometime around January, 2019. (Typically, I would have left out somebody else’s experience as it is not my place to say it. However, this other story seemed to contrast significantly with mine and I think it will help people to have a feel for the amount of variance in the process.)
He explained that he had done some GCP training on his own before and had prepared his own material. Instead of using the QwikLabs material, he used his own. He was asked to present the same topic as I was. If I remember correctly, he told me that he hadn’t done much work in the area in the recent past. So for example, when he was asked to show how to do something on the console, he could not figure out where it was because the UI had changed. Additionally, the way he narrated it to me, the whole process sounded super easy. He apparently went through the material, the questions were light, and even though his experience wasn’t very deep in the topic, he sailed through the entire process. (Note that this is what I interpreted it as over the phone. For all you know, he might just have been super modest about it.)
Of the modules that I was assigned, one was related to Networking and the other to Kubernetes. I’d recently written the Networking exam, thought I did badly, and had started preparing with a vengeance going deep into the subject assuming that I was going to flunk the first exam and will have to retake it. But when the results came, I’d passed. Not only that, I thought the Networking Certification was the toughest and people would need help practising, so I’d started preparing a mock test. Suffices to say, I knew GCP Networking fairly well.
On Kubernetes, I had worked on it before and had even made contributions to some projects using Kubernetes. I had also started preparing a little for the Certified Kubernetes Administrator. And of course, I’d passed the GCP certifications. Hence again, safe to say that I knew Kubernetes concepts fairly well, even though I definitely wasn’t an expert yet.
The evaluation was scheduled over Hangouts. Since time was short, I setup two GCP projects beforehand based on the topics that I had been given so that I could do a demo if and when required. As part of my regular teaching/training, I typically do a lot of illustrating on boards, which I wouldn’t have here. Instead, I screen casted my iPad on to the Mac screen and sketched on the iPad Pro with the Apple Pencil. This is definitely not a requirement, but I was more comfortable with that setup than merely droning on and speaking off the slides.
After a couple of minutes of initial pleasantries, I was asked to start on one of the topics – Kubernetes. Now, a quick flash back, I have conducted a Google Cloud Jam for Docker and GKE once. I’d also conducted a Kubernetes 101 session for the Kubernetes community in Bangalore. This was towards the last quarter of 2018 probably. Make note, therefore, that this was not new to me. Yet, I did not get through in this evaluation.
This particular evaluation process itself left me a bit flustered. Now, I want to state this again, this is not a judgement on the interviewer or the process. I’m merely stating my experience from my point of view. Therefore, some bias is to be expected.
The topic itself was straightforward. I’d run through it a few times and even timed it, so that I could complete it in time. However, right off the bat, I was asked questions. The answers were either in other slides later on or were not in these particular set of slides. Usually in trainings that I’ve conducted, this wouldn’t matter as I’d either answer them then and there if it made sense or note down the questions and inform the attendees that I’d answer that question later after having covered some other topics necessary to fully understand it. That approach didn’t work here for me. Initially, I was told something to the tune of, “Assume we are a new team and we know nothing about containers or kubernetes”. But the very next question would be an in-depth question. I definitely stuttered in answering some of those as I couldn’t square that circle on as to at what level I answer the question.
There was also the question of time. These Q&As were definitely taking time. No issues answering the questions and I was glad to do that. Except that we had a hard cut off at about an hour. There were about 50 slides and it was at the back of my mind whether it was a requirement to complete it on time, and if not, would that be held against me?
Let me answer that right now: in the end I realized that wasn’t an issue. I did not complete all the slides. In the 2nd evaluation, the evaluator asked me to skip slides as they weren’t important enough for a discussion. Neither did we finish all the slides.
Back to the interview again and the timing. At that point though, it was a concern in my mind about completion. And I did consciously and, possibly, subconsciously speed up.
Of the questions that were asked, I was able to clearly answer some and I did not know the answer to some. One of them was straightforward, 101 question but I just blanked out on a convincing answer, which honestly was embarrassing. It’s the kind of thing one knows but then struggles for a clear definition and then ends up bumbling for words.
At the end of the allotted time, the interviewer was kind enough to give me feedback, which was very helpful. The interviewer in the next round did not give me any feedback. So, there is no fixed approach or defined process to this. These are the main points in the feedback that I got and you’d do well to learn from these:
The evaluator was also kind enough to inform me directly that I did not make it in this round but he did encourage me to try again.
One thing that surprised me about the evaluation was that my facilitation skills were rated high, but I did not pass. When I read the web page for this program, I interpreted the content somewhat like this: the primary purpose of the evaluation is checking facilitation skills and not your tech skills as those were already validated via the certification. My discussion with the other person who qualified gave me the same impression. My personal experience, however, was that this is not true at all. In-depth technical knowledge on the topic is necessary.
At this point, I wasn’t very sure if and when I would try for the authorized trainer again. The upcoming months were looking very busy for me with a bunch of travel and other assignments. I thought maybe I’ll apply again after a few months. A couple of days later, however, I got a mail from the Google coordinator asking for dates for the 2nd evaluation. I don’t think this is really a process that they reach back to you for a 2nd round. I’d not heard about this before and I’d suggest you don’t expect it. In this mail she mentioned that there might be available dates within the next few weeks, and I thought, sure, why not? I was preparing for CKA anyways and also had done additional work on networking. The second evaluation was scheduled in the last week of March.
The evaluator asked me to start on either of two topics. I thought I was comfortable with both, but we picked one to start with. This time, I did apply the feedback from the last time and made it a point to speak at a slower pace. I did not worry about finishing the slides either. The evaluator told me at the beginning that he wanted to cover both modules. His approach was to make me skip the slides with less relevant content and to spend time on slides with more in-depth content. Personally, I liked this approach. The discussions were always at a consistent level and hence, from my point of view, it flowed more smoothly than the first evaluation. The questions and discussions were also deeper – think all chapters in the book Kubernetes in Action. Obviously, in the given time we couldn’t have covered all topics in detail but it did touch upon a few. Did I get all questions right? I definitely doubt it but I don’t think I did bad either.
Around the 50 minute mark, I was asked to stop on Kubernetes. There wasn’t enough time to also cover the full Networking module. Instead, he asked me a couple of questions about Networking on GCP.
I asked a couple of questions and clarifications on the Authorized Trainer process in the few extra minutes we hung on. Unlike the first evaluation, I did not receive any feedback. I thought that was a loss though, because I did benefit from feedback on the first one.
I do not know the result of the second evaluation. If I compared it to the other person’s experience, I should have probably got through the very first time itself. But very honestly, I appreciate tough processes and tight filtering in such contexts. I lose some, I win some. But the system is better for allowing in only the best. If I failed this one and had to try again later to ensure high quality, so be it.
The only anxious part of this whole process is figuring out whether anything is happening or when things are expected to happen. There are no fixed timelines, and you are definitely going to witness a different schedule. Yet, seeing my timeline might give you some calm.
12th Jan: An ATP applies for me. I get a mail with the next steps.
14th Jan: Step 1: I fill in my details and submit.
14th Jan: Step 2 is completing the certification, which I’m already done with. I receive a mail confirming this.
14th Jan: Step 3a. I receive a mail with the modules to prepare on. In step 3b, I will be assigned an interview/er.
11th Feb: I receive a mail asking for available dates on which I can attend the evaluation. I give three dates — Feb 15th, Feb 22nd, March 1st.
26th Feb: I receive another mail asking me to choose a date the following week. An interview is scheduled for March 5th.
5th March: Step 3c. Interview happens. I don’t pass.
8th March: I get a mail requesting dates for a second interview. I give a range of dates. An interview is scheduled for 29th March.
29th March: Step 3c again. 2nd interview happens. Results awaited.
Maybe in the future sometime: Step 4. Pass the evaluation and you are now a Google Cloud Platform authorized trainer.
Edit 4th April: I̶’̶m̶ ̶t̶o̶l̶d̶ ̶b̶y̶ ̶t̶h̶e̶ ̶A̶T̶P̶ ̶c̶o̶o̶r̶d̶i̶n̶a̶t̶o̶r̶ ̶t̶h̶a̶t̶ ̶h̶e̶ ̶n̶e̶e̶d̶s̶ ̶t̶o̶ ̶f̶i̶l̶l̶ ̶i̶n̶ ̶a̶ ̶f̶o̶r̶m̶ ̶a̶n̶d̶ ̶s̶e̶n̶d̶ ̶i̶t̶ ̶t̶o̶ ̶G̶o̶o̶g̶l̶e̶ ̶t̶o̶ ̶g̶e̶t̶ ̶t̶h̶e̶ ̶r̶e̶s̶u̶l̶t̶ ̶o̶f̶ ̶t̶h̶e̶ ̶e̶v̶a̶l̶u̶a̶t̶i̶o̶n̶.̶ ̶A̶n̶d̶ ̶h̶e̶ ̶n̶e̶e̶d̶e̶d̶ ̶t̶h̶e̶ ̶d̶a̶t̶e̶s̶ ̶o̶f̶ ̶t̶h̶e̶ ̶c̶o̶m̶m̶u̶n̶i̶c̶a̶t̶i̶o̶n̶.̶ ̶S̶o̶,̶ ̶y̶o̶u̶ ̶s̶h̶o̶u̶l̶d̶ ̶p̶r̶o̶b̶a̶b̶l̶y̶ ̶i̶n̶f̶o̶r̶m̶ ̶y̶o̶u̶r̶ ̶A̶T̶P̶ ̶a̶f̶t̶e̶r̶ ̶t̶h̶e̶ ̶i̶n̶t̶e̶r̶v̶i̶e̶w̶ ̶i̶s̶ ̶o̶v̶e̶r̶.̶ (So this did not prove to be true. The approval mail is sent directly to you. The ATP sending a mail has no effect on the process but they have work to do after you pass.)
Edit 22nd April: I’m a GCP Authorized Trainer! 🎉
̶I̶ ̶o̶b̶v̶i̶o̶u̶s̶l̶y̶ ̶d̶o̶n̶’̶t̶ ̶h̶a̶v̶e̶ ̶f̶i̶r̶s̶t̶-̶h̶a̶n̶d̶ ̶k̶n̶o̶w̶l̶e̶d̶g̶e̶ ̶o̶f̶ ̶t̶h̶i̶s̶ ̶s̶i̶n̶c̶e̶ ̶I̶ ̶h̶a̶v̶e̶n̶’̶t̶ ̶q̶u̶a̶l̶i̶f̶i̶e̶d̶ ̶y̶e̶t̶.̶ ̶I̶ ̶s̶h̶a̶l̶l̶,̶ ̶h̶o̶w̶e̶v̶e̶r̶,̶ ̶p̶u̶t̶ ̶d̶o̶w̶n̶ ̶a̶ ̶f̶e̶w̶ ̶p̶o̶i̶n̶t̶s̶ ̶t̶h̶a̶t̶ ̶I̶ ̶w̶a̶s̶ ̶c̶u̶r̶i̶o̶u̶s̶ ̶a̶b̶o̶u̶t̶ ̶a̶n̶d̶ ̶a̶b̶o̶u̶t̶ ̶w̶h̶i̶c̶h̶ ̶I̶ ̶e̶n̶q̶u̶i̶r̶e̶d̶.̶ I know a little better about some of this now.
I asked a few questions to the team regarding certification expiry, re-certification, and its linkage to the Authorized Trainer status. Below is that Q&A. I’ve made edits for clarity.
Is there a connection between the certification continuity and the authorized trainer status?
Is the Trainer status automatically removed if the certification expires?
Is the certification connected to the Series ID of the certificate?
If the person re-certifies after it expires, will they need to go through the interview process again?
What happens if the re-certification is done before it expires?
My notes on all my certifications — https://medium.com/@sathishvj/on-passing-all-google-cloud-certifications-54b2cc1e428c
Authorized Trainer Website: https://sites.google.com/view/gcptrainer/home
Training Partners: https://cloud.withgoogle.com/partners/?products=TRAINING_PRODUCT
Wish you the very best with your GCP certifications and qualifying for the Authorized Trainer. You can reach me at LinkedIn and Twitter. If you can support my work creating videos on my YouTube channel AwesomeGCP, you can do so on Patreon or BuyMeACoffee.
tech architect, tutor, investor | GCP 12x certified | youtube/AwesomeGCP | Google Developer Expert | Go
144 
5
144 
144 
5
tech architect, tutor, investor | GCP 12x certified | youtube/AwesomeGCP | Google Developer Expert | Go
"
https://medium.com/pytorch/pytorch-xla-is-now-generally-available-on-google-cloud-tpus-f9267f437832?source=search_post---------160,"There are currently no responses for this story.
Be the first to respond.
Authors: Ailing Zhang (FB), Joe Spisak (FB), Geeta Chauhan (FB), Vaibhav Singh (Google), Isaack Karanja (Google)
The PyTorch-TPU project was announced at the PyTorch Developer conference 2019 and originated from a collaboration among engineers and researchers at Facebook, Google, and Salesforce Research. The overarching goal of the project was to make it as easy as possible for the PyTorch community to leverage the high performance capabilities that Cloud TPUs offer while maintaining the dynamic PyTorch user experience. To enable this workflow, the team created PyTorch / XLA, a package that lets PyTorch connect to Cloud TPUs and use TPU cores as devices. Additionally and as part of the project, Colab enabled PyTorch / XLA support on Cloud TPUs. Fast forward to September 2020, and the PyTorch / XLA library has reached general availability (GA) on Google Cloud and supports a broad set of entry points for developers. It also has a fast-growing community of researchers and enterprise users training a wide range of models accelerated with Cloud TPUs and Cloud TPU Pods including researchers and engineers at MIT, Salesforce Research, Allen AI and elsewhere.
With PyTorch / XLA GA, PyTorch 1.6 is officially supported on Cloud TPUs. Other notable new features include:
PyTorch / XLA has been used to train numerous deep learning models on Cloud TPUs. Reference implementations are available for a diverse set of models such as:
In most cases, training these models on Cloud TPUs requires very few code changes. You can find official tutorials on Google Cloud here: ResNet-50, Fairseq Transformer, Fairseq RoBERTa, DLRM, PyTorch on Cloud TPU Pods. Check out the PyTorch / XLA GitHub repository for examples of other model architectures trained on Cloud TPUs.
PyTorch / XLA works using a ‘lazy tensor’ abstraction. With lazy tensors, the evaluation of tensor operations are deferred until the result of that operation is required (control/reporting). Up until that point, the operations are captured as an Intermediate Representation (IR) graph. Once results are required, these IR graphs are then compiled via XLA and sent to TPU cores for execution. This XLA compilation can also target CPU and GPUs.
Additional technical details about the approach are available on GitHub.
To start training, you need to create a Google Compute Engine VM (user VM) with the PyTorch / XLA image and a separate Cloud TPU Instance.
Once the user VM and the Cloud TPU instance are created, you can set the appropriate conda environment and set the XRT_TPU_CONFIG environment variable to point to the Cloud TPU instance:
At this point, you are ready to start training your model on a Cloud TPU! Let’s look at some sample code for training a “toy model” and notice the elements unique to PyTorch / XLA:
The lines highlighted above are: import statements for PyTorch / XLA components, the method to access the XLA device abstraction, and the parallel dataloader to facilitate overlapped data transfer and Cloud TPU execution. Also note the optimizer_step method, which performs the all-reduce operation followed by the parameters update (optimizer.step) behind the scenes. (GPU and CPU device types are also supported by PyTorch / XLA with no change in the code. Only an XRT_TPU_CONFIG variable is set differently to target these other hardware platforms.)
In contrast, here is an example code for training the same model on 4 GPU devices using PyTorch (without PyTorch / XLA):
As you compare the two code samples above, you may note the following: 1) The model code required no changes to execute on Cloud TPUs; 2) In the training loop, there are similarities between the PyTorch API and PyTorch / XLA to wrap and transfer the model object to the corresponding device abstraction (CUDA in case of GPUs and xla_device in case of Cloud TPUs). There is also an additional element for parallel data loading as described above.
PyTorch / XLA also provides utilities to scale the training you just executed on an individual Cloud TPU (v3–8, for example) to a full Cloud TPU Pod (v3–2048) or any intermediate-sized Cloud TPU Pod slice in between (e.g. v3–32, v3–128, v3–256, v3–512 and v3–1024), this scaling is done using xla_dist wrapper:
In order to set up distributed training, you create the Cloud TPU Pod slice of the desired size and a corresponding instance group with an appropriate number of VMs (matching the number of TPU cores divided by eight). Also, make sure that the training dataset is accessible to the respective virtual machines (workers) in the instance group. To start the training, launch your training script with xla_dist wrapper as shown above; xla_dist will orchestrate the Cloud TPU mesh configuration and execute the training script from each of workers.
A more detailed guide on how to get started with Cloud TPU Pods is available here. Further details are available on GitHub.
Colab notebooks provided in the official PyTorch / XLA repository are an excellent place to start exploring PyTorch / XLA on Cloud TPUs. Once you are familiar enough with the API, you can start to work with your own models following the setup provided in official examples.
This project would not have been possible without contributions from Alex Suhan (Prior affiliations with both Google and FB), Bryan McCann (Salesforce Research), Carlos Escapa (FB), Jin Young Daniel Sohn (Google), Davide Libenzi (Formerly at Google), Jack Cao (Google), Mike Ruberry (FB), Shauheen Zahirazami (Google), Shauna Kelleher (FB), Soumith Chintala (FB), Taylan Bilal (Google), Vishal Mishra (Google), Woo Kim (FB), Zach Cain (Google), and Zak Stone (Google).
The announcement from the Google team can be found here.
Cheers!
An open source machine learning framework that accelerates…
167 
1
167 claps
167 
1
An open source machine learning framework that accelerates the path from research prototyping to production deployment
Written by
PyTorch is an open source machine learning platform that provides a seamless path from research prototyping to production deployment.
An open source machine learning framework that accelerates the path from research prototyping to production deployment
"
https://medium.com/google-cloud/use-cases-and-a-few-different-ways-to-get-files-into-google-cloud-storage-c8dce8f4f25a?source=search_post---------161,"There are currently no responses for this story.
Be the first to respond.
Including AppEngine with Firebase Resumable Uploads
For this article I will break down down a few different ways to interact with Google Cloud Storage (GCS). The GCP docs state the following ways to upload your data: via the UI, via the gsutil CLI tool, or via JSON API in various languages. I’ll go over a few specific use cases and approaches for the ingress options to GCS below.
1. upload form on Google App Engine (GAE) using the JSON api Use case: public upload portal (small files)2. upload form with firebase on GAE using the JSON api Use case: public upload portal (large files), uploads within mobile app3. gsutil command line integrated with scripts or schedulers like cron Use case: backups/archive, integration with scripts, migrations4. S3 / GCP compatible file management programs such as Cyberduck Use case: cloud storage management via desktop, migrations5. Cloud function (GCF) Use case: Integration, changes in buckets, HTTP requests 6. Cloud console Use case: cloud storage management via desktop, migrations
You can launch a small nodejs app on GAE for accepting smaller files directly to GCS ~20MB pretty easily. I started with the nodejs GCS sample for GAE on the GCP github account here.
This is a nice solution for integrating uploads around 20MB. Just remember the nginx servers behind GAE have a file upload limit. So if you try and upload something say around 50MB, you’ll receive an nginx error: ☹️
You can try and upload the file size limit in the js file but still the web servers behind GAE will have a limit for file uploads. So, if you plan to create an upload form on App Engine, be sure to have a file size limitation in your UI.
Since the previous example only works for smaller files, I wondered how can we solve for uploading larger files say 100MB or 1GB? I started with the nodejs app engine storage example here.
After attempting to use resumable uploads in GCS API with TUS and failing I enlisted help from my friend Nathan @ www.incline.digital to help with another approach.
With the help of Nathan we integrated resumable uploads with firebase SDK. Code can be found here https://github.com/mkahn5/gcloud-resumable-uploads.
Reference: https://firebase.google.com/docs/storage/web/upload-files
While not very elegant with no status bar or anything fancy this solution does work for uploading large files from the web. 🙌🏻
gsutil makes it easy to copy files to and from cloud storage buckets
Just make sure you have the google cloud sdk on your workstation or remote server (https://cloud.google.com/sdk/downloads), set project and authenticate and thats it.
More details here.
gsutil makes it just easy to automate backup of directories, sync changes in directories, backup database dumps, and easily integrate with apps or schedulers for scripted file uploads to GCS.
Below is the rsync cron I have for my cloud storage bucket and the html files on my blog. This way I have consistency between my GCS bucket and my GCE instances if I decide to upload a file via www or via GCS UI.
Enjoy an client ftp type experience with Cyberduck on MacOS for GCS.
Cyberduck has very nice oauth integration for connecting to the GCS API built into the interface.
After authenticating with oauth you can browse all of your buckets and upload to them via the cyberduck app. Nice option to have for moving many directories or folders into multiple buckets.
More info on CyberDuck here.
You can also configure a Google Cloud Function (GCF) to upload files to GCS from a remote or local location. This tutorial below is just for uploading files in a directory to GCS. Run the cloud function and it zips a local directory files and puts the zip into the GCS stage bucket.
Try the tutorial:https://cloud.google.com/functions/docs/tutorials/storage
You can also use cloud functions created to display bucket logs. Below shows a file uploaded via my public upload form and deleted via the console ui. This could be handy for pub/sub notifications or for reporting.
Cloud Functions can come in handy for background tasks like regular maintenance from events on your GCP infrastructure or from activity on HTTP applications. Check out the how-to guides for writing and deploying cloud functions here.
The UI works well for GCS administration. GCP even has a transfer service for files on S3 buckets on AWS or other s3 buckets elsewhere. One thing that is lacking in the portal currently would be object lifecycle management. This is nice for automated archiving to coldline cheaper object storage for infrequently accessed files or files over a certain age in buckets. For now you can only modify object lifecycle via gsutil or via API. Like most GCP features they start at the function/API level then make their way into that portal (the way it should be IMO) and I’m fine with that. I expect object lifecycle rules to be implemented into the GCP portal at some point in the future. 😃
In summary I’ve used a few GCP samples and tutorials that are available to display to different ways to get files onto GCS. GCS is flexible with many ingress options that can be integrated into systems or applications quite easily! In 2017 the use cases for object storage are abundant and GCP makes it easy to send and receive files in GCS.
Leave a comment for any interesting use cases for GCS that I may have missed or that we should explore. Thanks!
Check my blog for more updates.
Google Cloud community articles and blogs
154 
5
154 claps
154 
5
Written by
Customer Engineer, Google Cloud. All views and opinions are my own. @mkahn5
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Customer Engineer, Google Cloud. All views and opinions are my own. @mkahn5
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@valyala/measuring-vertical-scalability-for-time-series-databases-in-google-cloud-92550d78d8ae?source=search_post---------162,"Sign in
There are currently no responses for this story.
Be the first to respond.
Aliaksandr Valialkin
Apr 11, 2019·8 min read
Time series databases usually have free single-node versions, which cannot scale to multiple nodes. Their performance is limited by a single computer. Modern machines may have hundreds of CPU cores and terabytes of RAM. Let’s test vertical scalability for InfluxDB, TimescaleDB and VictoriaMetrics on standard machine types in Google Cloud.
The following TSDB docker images were used in the test:
InfluxDB and VictoriaMetrics were run with default configs, while TimescaleDB config has been tweaked using this script for each n1-standard machine type in Google Cloud. This script tunes Postgresql for the available RAM on each machine type.
Time Series Benchmark Suite (TSBS) was used for the test. The following command has been used for generating test data:
The data has been generated for the following $TSDB_TYPE values:
VictoriaMetrics has been fed with $TSDB_TYPE=influx data, since it supports Influx line protocol.
The generated data consists of 40K unique time series. Each time series contains 25920 data points. The summary number of data points is 25920*40K=1036800000. This number may be rounded to a billion.
The following n1-standard machine types were used for both client and server machines:
We’ll see later the difference between vCPU and CPU.
Time series databases were configured to store data on a dedicated 2TB zonal standard persistent disk. The disk has the following characteristics:
The data ingestion tests were run with --workers=2*vCPUs config on the client side. This guaranteed full CPU utilization on the TSDB side.
The following notable things are visible on the chart:
Why TimescaleDB didn’t scale on machines with more than 16vCPUs? The following chart answers the question:
As you can see, TimescaleDB writes a lot to persistent disk comparing to competitors and reaches write bandwidth limit — 240MB/s — on n1-standard-8 machine. I have no idea why TimescaleDB exceeds the limit and reaches 260MB/s instead of 240MB/s.
The chart contains other notable info:
The final on-disk data sizes after ingesting 1B of test data points shows that TimescaleDB’s disk usage is far from optimal:
Let’s calculate how many data points each TSDB may store on a 2TB disk using data from the graph above:
TimescaleDB requires 77 (seventy seven) times more disk space for storing the same amount of data points comparing to VictoriaMetrics. In other words, it requires disk space costing $80*77=$6160/month for storing the same amount of data as VictoriaMetrics with a disk costing $80/month.
While 69 billions of data points look really big from the first sight, this isn’t true. Let’s calculate how many data points are collected from a fleet of 100 nodes with node_exporter installed on each node. Node_exporter exports 500 time series on average, so 100 nodes would export 100*500=50K unique time series. Let’s suppose these time series are scraped with 10s interval. This results in an ingestion stream of 5K data points per second. A month-worth data would contain 5K*3600*24*30=13 billions of data points. So, TimescaleDB would fill the entire 2TB storage in 69/13=5 months.
Both VictoriaMetrics and InfluxDB store data in LSM trees, while TimescaleDB relies on data storage from Postgresql. Charts above clearly show that LSM trees are better suited for storing time series data comparing to general-purpose storage provided by Postgresql.
TSBS queries may be split into two groups:
The following chart shows rpm (requests per minute) for double-groupby-1 queries serially performed by a single client worker:
Interesting things from the chart:
The next chart shows the maximum possible rps for double-groupby-1 query. The corresponding test runs N concurrent client workers, where N is the number of vCPUs on the server. This results in full CPU utilization on the server.
The chart shows good vertical scalability for all the contenders until n1-standard-32. TimescaleDB scales further on n1-standard-64, while InfluxDB and VictoriaMetrics have no gains in the maximum query bandwidth when switching from n1-standard-32 to n1-standard-64. It is likely TimescaleDB has better optimizations for NUMA nodes. But VictoriaMetrics without NUMA-aware optimizations serves 6.5x more queries comparing to TimescaleDB even on n1-standard-64 machine.
Vertical scalability is limited by per-machine resources — CPU, RAM, storage or network. Users have to switch to cluster solutions when single-node solution reaches scalability limits. TimescaleDB, InfluxDB and VictoriaMetrics provide paid cluster solutions for users that outgrow a single node. Guess which cluster solution is the most cost-effective from infrastructure and licensing point of view :)
Update: VictoriaMetrics is open source now!
Founder and core developer at VictoriaMetrics
235 
4
235 
235 
4
Founder and core developer at VictoriaMetrics
"
https://medium.com/google-cloud/new-google-cloud-ssds-have-amazing-price-to-performance-2a58e7d9b433?source=search_post---------163,"There are currently no responses for this story.
Be the first to respond.
Google Cloud just announced a TON of new database related features. Datastore, BigTable, and Cloud SQL Gen 2 are all out of beta!
What really caught my eye was the fact the SSD Persistent Disks just got a huge IOPS boost at no additional cost! At 25,000 IOPS, you basically start to forget that this is a network attached disk that has built-in redundancy and ultra-fast backups.
So then I got to thinking: how do Google Cloud IOPS compare to other clouds, namely AWS and Azure?
Spoiler: Google Cloud blows them out of the water!
Disclosure: I am super biased because I work for Google, but I tried to be as impartial as possible. Also, I am using the numbers stated on websites. I haven’t benchmarked anything myself. If you can make a better AWS or Azure configuration or if I am doing something wrong, please let me know!
Amazon’s EBS volumes come in a variety of flavors. The Provisioned IOPS SSD is what I’m going to be looking at. The maximum IOPS you can provision is 20,000 per volume.
With Google Cloud Persistent Disk, you can get 25,000 IOPS per volume. Just make sure you have enough CPU and disk size to drive the IOPS, and you are good to go.
So let’s look at some comparisons. I want a beefy database server with 500GB of disk space:
Wow. Just Wow.
You get more IOPS for less than 1/3 the cost!
You could literally launch three instances on Google Cloud for the price of one on AWS.
Now let’s push the limit to get the max IOPS from each provider:
Again!
You get more IOPS for less than 1/3 the cost!
With AWS, the instance does have 4 more CPU cores, but for the same price you can literally launch three 32 core machines on Google Cloud!
Even if you pay $18,000 up-front (!!), AWS is still more expensive! That’s crazy!
Two reasons:
EDIT: It was pointed out to me that you can attach Provisioned IOPS EBS volumes to smaller instances than you could on GCP. @fittedcloud attached a 20K IOPS disk to a t2.small instance, and got the full disk performance. This would cost $1,381.54 a month, and you instance only gets 1 CPU and 2 GB RAM. Again, this is because provisioned IOPS are very expensive!
Each Azure instance comes with a disk, but it is a temporary disk that will be wiped if the instance is deleted or moved. Instead, you need to use Premium SSD Storage if you want the same type of persistent network attached storage that AWS and GCP give you.
The funny thing about Premium SSD Storage is it comes in pre-set chunks. Unlike AWS or GCP, you can’t specify exactly how many IOPS or disk space you want. This also means you have to stripe together disks in RAID to actually get the same level of IOPS that GCP and AWS give you, which is super annoying. It is also confusing which instance can do what and how to set it up.
That being said, let’s take a look at the pricing:
Update: Azure pricing is a bit off, the total should be $1,456.24/month because we are using 8 disks in RAID, not 10.
Again!
You get more IOPS for about 1/3 the cost!
I think we’re seeing a trend here.
To Azure’s credit, it is cheaper and faster than the AWS instance we configured in the first section. However, Google Cloud’s price is just so much lower it’s crazy!
Two reasons:
I feel like I’m taking crazy pills. Google Cloud is consistently 1/3 the price as AWS and Azure while offering the same or better IOPS performance!
I’m not an expert on AWS or Azure. If you can configure a better machine on these platforms, let me know in the comments!
Also, GCP is not perfect. The max Persistent Disk IOPS per instance is 25k. There are certain (very expensive) instances on AWS and Azure that can go higher.
Google Cloud community articles and blogs
93 
5
93 claps
93 
5
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@retomeier/what-is-googles-cloud-platform-d92a9c9e5e89?source=search_post---------164,"Sign in
There are currently no responses for this story.
Be the first to respond.
Reto Meier
Feb 11, 2017·3 min read
Sadly, the Wikipedia entry for GCP is garbage, and while the official docs are pretty good, the marketing-dust sprinkled on them gives me a toothache.
As part of my re-self-introduction to Google’s Cloud Platform, I wrote an objective summary of GCP for my own reference. I have a conflict of interest, so I can’t fix the Wikipedia entry, but I can share what I wrote, so here it is.
Google Cloud Platform (GCP) is a collection of Google’s computing resources, made available via services to the general public as a public cloud offering.
The GCP resources consist of physical hardware infrastructure — computers, hard disk drives, solid state drives, and networking — contained within Google’s globally distributed data centers, where any of the components are custom designed using patterns similar to those available in the Open Compute Project.
This hardware is made available to customers in the form of virtualized resources, such as virtual machines (VMs), as an alternative to customers building and maintaining their own physical infrastructure.
As a public cloud offering, software and hardware products are provided as integrated services that provide access to the underlying resources. GCP offers over 50 services including Infrastructure as a Service (IaaS), Platform as a Service (PaaS), and Software as a Service (SaaS) offerings in the categories of Compute, Storage & Databases, Networking, Big Data, Machine Learning, Identity & Security, and Management & Developer tools.
These services can be used independently or in combination for developers and IT professionals to construct their own, custom cloud-based infrastructure.
GCP is hosted on the same underlying infrastructure that Google uses internally for end-user products including Google Search and YouTube.
Each of Google Cloud Platform’s services and resources can be zonal, regional, or managed by Google across multiple regions.
A zone is a deployment area for resources within a region. Zones are isolated from each other to prevent outages from spreading between them, so each zones is considered a single failure domain within a region.
Zonal resources operate within a single zone; if a zone becomes unavailable all of its resources are unavailable until service is restored. Regional resources are deployed with redundancy across zones within a region. Multi-regional services (Google App Engine, Google Datastore, Google Cloud Storage, Google BigQuery) are managed by Google to be redundant and distributed within and across regions.
All GCP service instances are configured such that maintenance events are transparent to applications and workloads via live migration. Live migration moves running virtual machine instances out of the way of maintenance that is being performed.
GCP services are available in 18 zones in 6 regions: Oregon, Iowa, South Carolina, Belgium, Taiwan, and Tokyo¹.
In 2016, Google announced plans to make 22 zones and 8 new regions available in 2017: Sydney, Sao Paulo, Frankfurt, Mumbai, Singapore, London, Finland, and Northern Virginia.
Within each region, traffic tends to have round-trip network latencies of under 5ms on the 95th percentile.
Note that like everything I publish on Medium, these are my views on GCP, and may not represent the opinions of my employer.
Something missing or wrong? Add a comment!
Developer Advocate @ Google, software engineer, and author of “Professional Android” series from Wrox. All opinions are my own.
See all (353)
214 
2
214 claps
214 
2
Developer Advocate @ Google, software engineer, and author of “Professional Android” series from Wrox. All opinions are my own.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/how-good-is-google-clouds-cdn-e181a16f0404?source=search_post---------165,"There are currently no responses for this story.
Be the first to respond.
We talked before about how the Load Balancer can be extended to take advantage of Google’s CDN. The result was less headache for you, and faster fetches for the end user.
A few social media friends reached out to say “That’s all fine, but how good is Google’s CDN, anyway?”
That’s…... Actually a really good question. Let’s take a look at how to profile Google’s CDN.
A CDN is working properly when it can cache a requested asset as close to the requestor as possible. This has two main factors : A) Does it work, and B) Does it work everywhere?
“Does it work” is easy to test. We simply set up an asset on the CDN, with caching headers, and throw a cURL command at it 500 times or so.
You know your CDN is working properly when the latency of that request looks something like the image below. You get a bunch of standard requests at the front of your timeline, (where the latency is tied to the physical distance between the requester and the server), and then latency drops significantly once the caching occurs. In this case the caching result was almost instantaneous:
Probably most important is that CDNs minimize the impact that geographical differences impose in a fetch. This is why the “Does it work everywhere” part is just as important, and for our purposes, just as easy to test.
First, place an asset on a server, say, in North America, (maybe near a BBQ restaurant) and then fetch that asset from a bunch of machines located around the world.
The graph below shows our asset being fetched from a machine in Asia, Australia and Europe.
This graph shows Google’s CDN is working properly, despite geographical distances from the requester to the server. From this test, we can see the performance is taking advantage of the power of Google’s Edge Network, where assets are moved to closer points-of-presence for later fetches.
For most developers, the test can stop at the above data. Besides nuances on features, most companies choose a CDN due to it’s performance, cost, and uptime. But, since the behavior of CDNs may vary, we want to break apart our request latency, and see each stage individually. Specifically, I want to see the DNS time, Connection time, and Wait time.
Why do these things matter?
DNS time: It is the time that it takes for the hostname to get resolved. DNS resolution is really important to understand user performance, since end users rely on DNS resolvers of their ISPs for the last mile. Some CDNs have more complex DNS setup than others, which means the time gained in Wait time was diluted by slower DNS response time.
Connect time: Connect is the time it takes to make the TCP connection. It depends on the network (end-to-end latency). For SSL add also the time it takes to establish the encrypted tunnel. Quite important here, is that if your CDN is susceptible to peak-hour restrictions, Connect time will show it.
Wait time: is the time that it takes for the remote server to process the request. IF your content is on the edge of the network (close to you) then this time should be really low, however, if the content needs to be fetched from the origin server, it can be higher. A CDN will deliver good performance if the asset is cached, due to it’s popularity.
With a handy cURL command, we can get all this data:
What’s really nice here is that we can see, firstly, that the times line up pretty well with the non-broken-apart version (this means our break-out doesn’t seem to be adding much overhead to the test), but secondly, that overall, the DNS, Connect, and pre-transfer stages are exceptionally fast, and very consistent. Exactly what we want in a CDN.
Now, a few folks pointed out that my tests might not produce the most accurate results, given that there’s a single origin point that’s requesting the same asset multiple times. No problem, let’s build a bigger test.
For this, a team used used Vegeta, running on Google Container Engine , which allowed each pod to create 1000 requests per second (RPS) and then scaled up quickly to 50,000 RPS. The following Stackdriver dashboard shows how Cloud CDN handled the load:
Note: See a similar use case on Github for load testing a serverless pixel tracking solution.
So that’s fine, but you could argue that again, it’s not enough, and that to really test the power of a CDN, you need to have billions of requests passing through it every day. Sadly, I don’t have a website that popular, and I don’t have access to all the other websites in the world (yet), but thankfully, the folks at Cedexis, do.
The Cedexis Radar Community collects over 14 billion real user measurements each day from users all around the world. This data is aggregated and delivered free to all members of the Radar Community, who can see in near real-time the state of the Internet: the availability, latency, and throughput of global CDNs, ISPs, Clouds, and their own web properties.
Their reports make it really easy to see how your favorite CDN is doing with respect to latency, uptime, and throughput
So, we’ve presented a few ways that you could move forward testing Google’s CDN. Not only can you run it yourself with a small test, but can expand to a 50k RPS system, or just take a look at the performance numbers from a few different groups who collect RUM stats.
Figure out your needs, boot up the tests, and see if Google’s CDN can help you improve performance for your end users!
There’s some great tools to help you profile your networking perf.
Don’t forget to check you’re using the right region for your GCP instance.
And make sure you’re using the right CPU counts for your networking performance.
Google Cloud community articles and blogs
212 
1
212 claps
212 
1
Written by
DA @ Google; http://goo.gl/bbPefY | http://goo.gl/xZ4fE7 | https://goo.gl/RGsQlF | http://goo.gl/4ZJkY1 | http://goo.gl/qR5WI1
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
DA @ Google; http://goo.gl/bbPefY | http://goo.gl/xZ4fE7 | https://goo.gl/RGsQlF | http://goo.gl/4ZJkY1 | http://goo.gl/qR5WI1
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/skooldio/google-cloud-smart-analytics-8d65a421a366?source=search_post---------166,"There are currently no responses for this story.
Be the first to respond.
ใน Product Innovation Keynote วันนี้ Google ได้เปิดตัวเทคโนโลยีและบริการใหม่ๆ ที่เกี่ยวกับข้อมูลออกมาเป็นจำนวนมาก โดยเฉพาะในเรื่องของ Smart Analytics ในบทความนี้ เดี๋ยวจะมาเล่าให้ฟังในส่วนของเทคโนโลยีที่เกี่ยวข้องกับการสร้างโมเดล Machine Learning ให้ฟังกันก่อน
ปีนี้ Google ยังคงโฟกัสในเรื่องของการทำให้ทุกคนในองค์กรสามารถใช้ประโยชน์จาก AI/Machine Learning ได้มากขึ้น และสามารถเริ่มต้นสร้างโมเดลได้เองแบบง่ายๆ
ปีที่แล้ว Google เปิดตัวฟีเจอร์ใหม่ BigQuery ML ที่มาช่วยให้ Data Scientist/Analyst สามารถสร้างโมเดล Machine Learning บนข้อมูลขนาดใหญ่ใน Google BigQuery ได้ง่าย ๆ ผ่าน SQL query ใครที่ยังไม่เคยได้ยิน ตามไปอ่านบทความด้านล่างนี้จากปีที่แล้วกันก่อน
medium.com
BigQuery ML เป็นหนึ่งในเทคโนโลยีที่ถูกหยิบยกขึ้นมาเป็นกรณีศึกษาเยอะมากตลอดงาน Cloud Next ในปีนี้ โดยในงาน Google ได้เปิดตัวโมเดลเพิ่มเติมอีก 3 อัน คือ
เนื่องจากสองตัวหลังยังอยู่ใน Alpha วันนี้เลยจะมาลองเล่น K-means ให้ดูอย่างเดียวก่อนโดยใช้ข้อมูล Facebook โพสต์จากเพจ Skooldio เช่นเดียวกับตัวอย่างจากบทความในปีที่แล้ว
การสร้างโมเดลก็ง่ายเหมือนเดิม แค่เปลี่ยนประเภทโมเดลเป็น kmeans และกำหนดวิธีการคำนวณระยะทางระหว่างข้อมูลสองจุด distance_type ว่าจะใช้เป็น EUCLIDEAN หรือ COSINE
พอสร้างโมเดลเสร็จ ก็จะได้ผลหน้าตาประมาณนี้ออกมา รายละเอียดของทั้งสอง metrics นี้ขอไม่พูดถึง เดี๋ยวจะยาว
หรือถ้าใครอยากเรียกดูผลด้วยคำสั่ง SQL ก็สามารถทำได้เช่นกัน
อยากจะดูผล ก็ใช้คำสั่ง ML.PREDICT ได้เลย
โดยผลที่ได้ ก็จะแจกแจงให้ว่าแต่ละจุดข้อมูลถูกจัดลง cluster ไหน และระยะทางจุดข้อมูลไปยัง centroids ของ cluster ที่อยู่ใกล้ที่สุด 5 clusters เป็นเท่าไหร่บ้าง
จากตัวอย่างข้างต้น ข้อมูลแถวที่ 2 ถูกจัดอยู่ใน Cluster 6 โดยระยะทางจากข้อมูลไปยัง centroid ของ cluster อยู่ที่ 0.54
ปีที่แล้ว Google ได้เปิดตัว Cloud AutoML เทคโนโลยีในการสร้างโมเดล Machine Learning อัตโนมัติ โดยใช้เทคนิคขั้นสูงอย่าง learning2learn และ transfer learning เพื่อช่วยให้ทุกคนสามารถสร้างโมเดล AI ที่มีคุณภาพดีได้
ในปีนี้ Google ได้เปิดตัว AutoML Tables ซึ่งเป็นการต่อยอดนำ AutoML มาใช้กับข้อมูลแบบ Structured (ตารางข้อมูลที่พวกเราคุ้นเคยกันดีนี่แหละ) เพื่อช่วยสร้างโมเดลที่มีประสิทธิภาพจากตารางข้อมูลปริมาณมากๆ ได้อย่างรวดเร็ว
มาลองดูกันหน่อยสิว่ามันทำงานยังไง? ข้อมูลที่ใช้เป็นข้อมูล Facebook โพสต์จากเพจอีเจี๊ยบเลียบด่วน กับ Drama Addict โดยเราจะมาสร้างโมเดลเพื่อทำนายว่าโพสต์ไหน มาจากเพจอะไร
เริ่มต้นจากการโหลดข้อมูลเข้าไปก่อน โดยจะโหลดจาก Table ใน BigQuery หรือโหลดไฟล์ CSV จาก Cloud Storage ก็ได้ ข้อมูลที่โหลดเข้าไปจะต้องมีอย่างน้อย 1,000 แถว และ 2 คอลัมน์ (คือต้องมี target ที่จะทำนาย กับ feature อีกอย่างน้อยหนึ่งตัว)
เมื่อโหลดข้อมูลเรียบร้อย ก็จะมี UI ให้เราระบุประเภทของตัวแปร และกำหนดคอลัมน์ที่เป็น target ที่เราต้องการจะทำนาย ในที่นี่เราต้องการจะทำนายว่าแต่ละโพสต์มาจากเพจไหน จึงตั้ง page ให้เป็น target
นอกจากนี้ เรายังสามารถระบุรายละเอียดเพิ่มเติมได้ เช่น หากต้องการแบ่งข้อมูล Train / Validate / Test ในสัดส่วนที่ต้องการ ก็ทำได้โดยการสร้างคอลัมน์ที่มีการระบุค่า TRAIN , VALIDATE, TEST เพิ่มขึ้นมา หรือหากต้องการกำหนด weight ให้กับข้อมูลแต่ละตัวไม่เท่ากัน ก็สามารถสร้างคอลัมน์ที่ระบุตัวเลข 0 ถึง 10,000 โดยค่ายิ่งมาก โมเดลก็จะยิ่งให้ความสำคัญมาก และถ้าแถวไหนมีค่าเป็น 0 ก็จะไม่ถูกนำมาใช้
อ่านรายละเอียดเกี่ยวกับการเตรียม dataset เพิ่มเติมได้ที่นี่: https://cloud.google.com/automl-tables/docs/prepare
เมื่อกำหนด Schema เสร็จเรียบร้อยแล้ว ก็สามารถมาดูผลวิเคราะห์เร็วๆ ได้ ว่าแต่ละตัวแปร correlate กับ target มากน้อยแค่ไหน มีค่าเฉลี่ย ค่าเบี่ยงเบนมาตรฐานเท่าไหร่ เมื่อเข้าใจข้อมูลและแน่ใจว่าไม่ได้ทำอะไรผิดพลาดแล้ว ก็ไปสร้างโมเดลกันเลย!
ในการ train โมเดล เราจะต้องทำการระบุ budget ที่จะใช้ในการเทรนเป็นชั่วโมง โดยค่าใช้จ่ายตกอยู่ที่ชั่วโมงละ $19.32 😱 (แต่มีให้ลองโดนป้ายยาฟรีๆ ก่อน 6 ชั่วโมง 😂) นอกจากนี้เราสามารถเลือกคอลัมน์ที่จะเอาไปใช้ในการสร้างโมเดลได้อีกด้วย เผื่อไม่อยากใช้คอลัมน์ไหน
สำหรับโมเดลประเภท Classification เราสามารถเลือก Objective ได้ 3 แบบ ขึ้นอยู่กับว่าเราให้ความสำคัญกับอะไร ขอไม่ลงรายละเอียดเหมือนเดิม เดี๋ยวจะยาว ใครที่สนใจสามารถอ่านเพิ่มเติมได้ที่: https://cloud.google.com/automl-tables/docs/models
เสร็จเรียบร้อยดีแล้วก็กด Train Model เล้ย!!
หลังจากที่รอไปครบ 1 ชั่วโมง ก็ได้ผลลัพธ์หน้าตาประมาณนี้ ระบุรายละเอียด performance ของโมเดลชัดเจนดี
ถ้าผลข้างต้นยังละเอียดไม่พอ เราสามารถมาดูรายละเอียดเพิ่มเติมในส่วนของ Evaluate ได้
ใครที่อยากลองปรับ Threshold ในการตัดสินใจว่าจะเป็น positive หรือ negative ก็สามารถทำได้ง่ายๆ ผ่าน UI ได้เลย
โดยส่วนตัว ประทับใจกับการรายงานผลมากๆ ทำได้ละเอียดและสวยงาม ซึ่งหลายๆ ครั้ง Data Scientist ก็มักจะขี้เกียจ ดูแค่ Accuracy แล้วก็จบกัน อันนี้บังคับดูให้หมดเลย 😂
นอกจากนี้ยังมี Confusion matrix เพื่อแจกแจงว่าที่เดาผิดเดาถูก ผิดเพราะอะไร และยังมี Feature Importance ที่แสดงว่าแต่ละ feature มีส่วนช่วยในการทำนายมากน้อยยังไง
ความเจ๋งอย่างหนึ่งของ AutoML คือ มันประมวลผล Text เอาไปใช้งานในโมเดลให้อัตโนมัติ! อย่างในกรณีนี้เราอยากจะแยกแยะโพสต์จากเพจอีเจี๊ยบกะจ่า ข้อมูลนึงที่น่าจะช่วยเราได้มากๆ ก็คือสำนวนการใช้ภาษาในแต่ละโพสต์ เช่น ถ้ามีคำว่า “หัวล้าน” หลายๆ คนก็น่าจะพอเดาได้แล้วว่าโพสต์นี้มาจากเพจอีเจี๊ยบ
ข้อควรระวังก็คือ การนำ Text ไปใช้ ระบบจะทำการ Tokenize หรือตัดคำด้วยเว้นวรรค ซึ่งถ้าเป็นภาษาอังกฤษ ที่มีการเว้นวรรคระหว่างคำ ก็คือจบเลย พร้อมใช้งาน แต่ถ้าเป็นภาษาไทย และอยากให้ทำงานได้ดี ก็อาจจะต้องไปผ่านกระบวนการตัดคำกันมาก่อน แต่อย่างไรก็ตาม จากผลข้างต้นที่เราได้ ถึงแม้จะไม่มีการตัดคำมาก่อน ผลก็ยังพอได้อยู่ น่าจะเป็นเพราะว่ามีคำเด็ดๆ ที่ใช้แยกแยะทั้งสองเพจได้ ที่มักจะถูกเขียนอยู่โดดๆ (คือมีเว้นวรรคนำหน้าและตามหลัง)
สุดท้าย ถ้าคิดว่าโมเดลทำงานได้ดีแล้ว การนำไปใช้ สามารถทำได้ 2 แบบด้วยกัน จะทำ Batch Prediction คือ โหลดข้อมูลเข้ามาทำนายเป็นชุดๆ จากไฟล์หรือจาก BigQuery หรือจะทำ Online Prediction คือ เปิดเป็นบริการ ไว้รอทำนายข้อมูลที่ถูกส่งเข้ามา
สำหรับ Online Prediction นั้น เราจะต้องทำการ Deploy โมเดลก่อน จึงจะเริ่มใช้งานได้
Disclaimer: โดยส่วนตัวคิดว่าเทคโนโลยีที่เล่ามาทั้งหมดมันเจ๋งมาก และจะช่วยให้หลายๆ องค์กรเริ่มใช้ประโยชน์จากข้อมูลได้มากขึ้นอย่างแน่นอน แต่เครื่องมือแนว “Automagic” นี้ มักจะเป็นดาบสองคม หากผู้ใช้งานไม่มีความรู้พื้นฐานด้าน Machine Learning ที่แข็งแรงพอ แล้วเตรียมข้อมูลมาไม่ดี หรือนำไปใช้ไม่เหมาะสม สุดท้ายอาจจะส่งผลเสียมากกว่าผลดีต่อองค์กรได้
Master today's most in-demand skills
138 
138 claps
138 
แหล่งความรู้ในการ Upskill / Reskill สำหรับคนทำงานในยุคปัจจุบัน พัฒนาและเสริมสร้าง Skill ของคุณ ด้วยคอร์สเรียนออนไลน์ และ เวิร์คชอปด้าน Technology / Design / Data / Business
Written by
Co-founder @ Skooldio. Google Developer Expert in Machine Learning. A data nerd. A design geek. A changemaker. — Chula Intania 87, MIT Alum, Ex-Facebooker
แหล่งความรู้ในการ Upskill / Reskill สำหรับคนทำงานในยุคปัจจุบัน พัฒนาและเสริมสร้าง Skill ของคุณ ด้วยคอร์สเรียนออนไลน์ และ เวิร์คชอปด้าน Technology / Design / Data / Business
"
https://blog.machinebox.io/deploy-docker-containers-in-google-cloud-platform-4b921c77476b?source=search_post---------167,
https://towardsdatascience.com/a-gentle-introduction-to-apache-druid-in-google-cloud-platform-c1e087c87bf1?source=search_post---------168,"Sign in
There are currently no responses for this story.
Be the first to respond.
Antonio Cachuan
Oct 21, 2019·9 min read
Making easy to analyze billions of rows
In order to have a clear understanding of Apache Druid, I’m going to refer what the official documentation says:
Apache Druid (incubating) is a real-time analytics database designed for fast slice-and-dice analytics (“OLAP” queries) on…
"
https://blog.doit-intl.com/gslack-9391be7c191a?source=search_post---------169,"One of the perks I really like as part of my work as CTO at DoiT International, is my day-to-day conversations with our customers. On many occasions, they are enlightening and I always learn something new.
Last week, I’ve noticed a ticket from one of our clients with a question on what would be the best way to integrate Google Cloud Platform with Slack.
Specifically, they wanted to get notified to their Slack channel when some activity took place in one of their Google Cloud projects, for example instance being started or terminated, new bucket being created or deleted and so on. Isn’t it a nice idea? Quick search with google.com didn’t reveal any immediate results so I had to figure out what would be the quickest and simplest way of achieving Slack and Google Cloud Platform integration.
Fortunately, there is an elegant, completely serverless solution for this which I will try to explain in this post. Even better, today we are open-sourcing the gSlack which you can deploy in your own GCP project in just few minutes and get instant and flexible notifications to your Slack channel.
gSlack uses Stackdriver Logging — Google’s centralized logging platform which allows you to store, search, analyze, monitor, and alert on log data and events from Google Cloud Platform (and Amazon Web Services). Most of Google’s cloud services send its logs to Stackdriver Logging. I was specifically interested in “Activity Logs” which include changes in Google Cloud Platform environment.
Here is how it looks in the Google Cloud Console:
One of the most cute features of Stackdriver Logging is its ability to automatically export new log entries to Google Cloud Storage, Google BigQuery and Google Pub/Sub. You just configure an ‘export’ and everything else just magically happens for you.
I needed a transport which could relay log entries from Stackdriver Logging & Slack so Pub/Sub looked like the best alternative for this use-case. If you are not familiar with Pub/Sub, it is Google’s fully-managed real-time messaging service that allows you to send and receive messages between independent applications.
Setting up Pub/Sub export is as easy as configuring log filter (I only need ‘activity’ based logs, hence the logName=”projects/doit-playground/logs/cloudaudit.googleapis.com%2Factivity”) and the name of Pub/Sub topic to push the messages to:
So now, every time there is a new entry in Stackdriver’s Logging, it will be automatically pushed to my Pub/Sub topic. Pretty nice, right?
Next, I needed to put some ‘glue’ between the Pub/Sub and Slack so new messages being published to Pub/Sub will be posted as Slack notifications. Luckily, Google now has (in beta) Cloud Functions — a serverless environment to build and connect cloud services. Basically, you can code a ‘function’ written in NodeJS which will be triggered by one of the supported triggers such as new file in the bucket, http request or (I bet you’ve already guessed!) — new message in a Pub/Sub topic!
Our complete flow would look like this — StackDriver Logging logs certain activity in our project, automatically sends it to Pub/Sub topic which in its turn invokes Cloud Function sending a message to Slack channel using official Slack’s NodeJS SDK.
To setup Cloud Functions, you’ll need to upload a zip file containing the code and your package.json file:
To avoid sending every activity log entry coming to Slack channel (some of them might be not that interesting) and better format messages being published to Slack, I’ve decided to use one more Google’s managed service — Google Cloud Datastore.
Google Cloud Datastore is a managed NoSQL document database built for automatic scaling, high performance, and ease of application development. It has built-in integration with Cloud Functions and also a nice UI which you can use to quickly edit entries (‘kinds’ and ‘properties’ in Datastore’s terminology).
We need something like Google Cloud Datastore to persistently store runtime configuration of gSlack, specifically the definition of which messages we want to publish and how messages published to Slack will look like.
For each set of log entries, you’ll need to configure test, message and the slackChannel where you want to publish the notification.
The test must be a valid JS expression that returns a boolean. If it returns true the test passes and the message will be sent to Slack. For example, if we want to only include messages from Google Compute Engine and track ‘start’ and ‘stop’ instance events, we can use the following test:
Similarly, the message must be a valid JS string template. It will be evaluated to produce the message. e.g.
As a result, the following notification will be sent to Slack. In my tests, it only takes about 5 seconds between the actual event and the time the message gets to the Slack channel.
You can add as many tests & messages as you’d like, parsing various Google Cloud Platform services such as Compute Engine, App Engine, Cloud Storage, BigQuery and even Billing. Actually, we include some of these examples in the gSlack repository.
The full code as well as deployment instructions are available at GitHub’s gSlack repository. Feel free to star or pull request and improve the gSlack ;-)
As always, you can reach me at vadim@doit-intl.com with your suggestions and ideas.
P.S. I’d like to thank Shahar Frank, Cloud Architect at DoiT International who actually coded the whole example in just few hours as well as helping me to prepare this post.
Software & Operation Engineering. Written By Engineers.
185 
5
185 claps
185 
5
Written by
CTO of world’s best cloud consultancy
Here, our SRE and SWE teams share the solutions to some of the most interesting challenges we encounter during our daily work at DoiT International. If you like what we do, check out our https://careers.doit-intl.com page and join the team!
Written by
CTO of world’s best cloud consultancy
Here, our SRE and SWE teams share the solutions to some of the most interesting challenges we encounter during our daily work at DoiT International. If you like what we do, check out our https://careers.doit-intl.com page and join the team!
"
https://towardsdatascience.com/a-gentle-introduction-to-the-5-google-cloud-bigquery-apis-aafdf4ef0181?source=search_post---------170,"Sign in
There are currently no responses for this story.
Be the first to respond.
Antonio Cachuan
Dec 28, 2020·7 min read
"
https://medium.com/google-cloud/hosting-a-react-js-app-on-google-cloud-app-engine-6d1341b75d8c?source=search_post---------171,"There are currently no responses for this story.
Be the first to respond.
Migrating servers to cloud has been the current technological trend with cloud services from AWS, Auzre, IBM, SAP, Salesforce etc being the frontliners. Among these long running cloud services comes Google Cloud and Oracle Cloud whom have…
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@hiranya911/firebase-using-the-python-admin-sdk-on-google-cloud-functions-590f50226286?source=search_post---------172,"Sign in
There are currently no responses for this story.
Be the first to respond.
Hiranya Jayathilaka
Aug 27, 2018·5 min read
The Google Cloud Functions (GCF) Python runtime was launched into beta at the recently concluded GCP Next. Ever since the announcement I’ve been looking forward to try it out with the Firebase Admin SDK. I finally managed to spend some time on it last week.
As my first experiment with the GCF Python runtime, I wanted to convert one of my existing Python applications into a Cloud Function. A while ago I wrote an article about implementing a simple web service in Python using Flask and Firebase Admin SDK. In that article I deployed the resulting service using Google Compute Engine. In this post I describe how to port that same service over to GCF.
You will need a billing enabled Google Cloud Platform (GCP) project, and a local installation of the gcloud command-line tool. You will also require Python 3 and the virtualenv utility. Execute the following commands in a Linux/Unix shell to get the environment set up.
These commands update the gcloud utility, and configure it to use your GCP project. Then we create a new virtualenv sandbox, and install the required Python dependencies.
You also need to set up Google Application Default Credentials in order to locally test the function. Download a service account JSON file for you GCP/Firebase project, and set the required environment variable.
Create a new directory for your code, and make it your working directory.
Then create a requirements.txt file, and specify the required dependencies for your function. We just need to declare one dependency for this example — firebase-admin. Listing 1 shows what this file should look like.
Next, create a file named main.py (file name is important for GCF), and implement the web service as shown in listing 2.
Note that this is a variation of the same web service we developed in the past for the Compute Engine environment. It exposes the same endpoints, and performs the same operations.
The heroes() function at the bottom of the file is the entry point to our service. It gets invoked by the GCF runtime, and receives a flask.Request object as the sole argument. Depending on the HTTP method and URL path, we then dispatch the request to one of the four helper methods that perform Firebase Database operations.
I didn’t find much documentation on testing Python cloud functions locally. Therefore I devised my own mechanism for that. Create another file named main_emulate.py, and save it with the contents in listing 3.
Listing 3 fires up a local Flask server that accepts all HTTP requests addressed to the /heroes path, and delegates them to the heroes() function in main.py. You can simply execute main_emulate.py from the command-line to get the test server up and running.
The server listens on port 5000. Send it a few requests to make sure everything works as expected.
Terminate the test server by hitting ctrl + c when you’re done.
Deploying to the cloud is just a matter of running the following command.
No need to spin up any VMs or install any other software in the cloud. GCF takes care of all that. Got to admire the convenience of Functions-as-a-Service — isn’t it?
The deployment may take a couple of minutes. When done, the console output shows the base URL of the newly deployed service. The base URL will have the name of our Python function — heroes — appended to it. Use curl to send a few requests and try it out. The first request may take a couple of seconds due to cold start, but subsequent requests shouldn’t take more than a couple of hundred milliseconds.
I’m calling my function over HTTPS in the above examples. Note that I’m also using the time command to time my command executions, and we can see how the latency drops from first to the second request. Even though /heroes is part of the request URL, GCF does not expose it on the flask.Request object injected into main.py. When testing locally, we accounted for this behavior of GCF by rewriting the request.path attribute in main_emulate.py.
Things are looking pretty impressive at this point. But I do have a couple of issues that I’d like to point out as well.
Python runtime in GCF is still pretty new, and I’m sure it will get better with time. For now I’ve reported my feedback to the GCP team.
All in all, my initial foray into GCF Python with the Firebase Admin SDK has been a success. I think it is a convenient, scalable and cost-effective way for developers to create Python web services that interact with Firebase. Personally, I’ve been waiting for Python support in GCF for a long time, and I’m so excited to see it finally out. Feel free to share your own experience with the new runtime, and let me know what type of things you would use it for.
Software engineer at Google. Enjoys working at the intersection of cloud, mobile and programming languages. Fan of all things tech and open source.
195 
3
195 
195 
3
Software engineer at Google. Enjoys working at the intersection of cloud, mobile and programming languages. Fan of all things tech and open source.
"
https://medium.com/google-cloud/how-to-run-serverless-batch-jobs-on-google-cloud-ca45a4e33cb1?source=search_post---------173,"There are currently no responses for this story.
Be the first to respond.
Cloud Functions, Cloud Run, AppEngine, etc. are not a good choice for long-lived functions, i.e., anything that takes longer than a couple of minutes to run (the services themselves impose a limit of 10 or 15 minutes, but that includes errors and retries, so your goal should be 2–3 minutes maximum). If you want to run a function that will take longer than this, what are your options?
What if you want to run a long-running batch job in a serverless way?
Put your code in a Docker container. Run it using AI Platform. Schedule it using Cloud Scheduler.
You can use AI Platform Training to run any arbitrary Docker container — it doesn’t have to be a machine learning job. To have some arbitrary container executed on a GPU, you’d just do:
This is just a REST API, so you have a variety of client libraries in a bunch of programming languages to invoke this from. There are no requirements for the container — just that it needs to have an entry point and that it is published in the container registry. It is possible to use custom machine types — see the documentation for details.
Being able to launch a custom container on a job-specific cluster satisfies a number of use cases for serverless functions. But not all of them. Specifically, another use case for serverless functions is concurrent autoscaling — we want to be able to receive multiple requests, and route them to the same machine and once that machine starts to get overwhelmed, we’d like to add more machines. If you need concurrent autoscaling and your tasks last longer than 2–3 minutes, the AI Platform Training solution will not work. You’ll need Kubernetes in that case, and it won’t be serverless.
Google Cloud community articles and blogs
169 
3
169 claps
169 
3
Written by
Data Analytics & AI @ Google Cloud
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Data Analytics & AI @ Google Cloud
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/express-routing-with-google-cloud-functions-36fb55885c68?source=search_post---------174,"There are currently no responses for this story.
Be the first to respond.
Underneath it all, Google Cloud Functions written in Node uses the minimalist web framework Express via the Functions Framework.
With a few lines of Node, we can use extra features of Express like routing URLs to multiple endpoints via Express routing. This means using routing requests to URLs like /users,/users/{id}. In this post, we’ll learn how to do that!
Routing refers to how an application’s endpoints (URIs) respond to client requests.
Each route can have one or more handler functions, which are executed in order when the route is matched.
Using routing helps build logical, RESTful web applications. Rather than handling service routing logic with if/else conditional statements or switch statements, we can leverage Express’s advanced yet simple routing feature!
I think it’s best illustrated with an example comparison:
For a “Hello World” application, having a large function handler is acceptable. However, after your application evolves, you might find yourself doing this…
With your URL handlers in a separate file:
As you can see, we’re creating a potentially messy situation with our req.url.split('/')[1]. In fact, this implementation is limited to only handle one level of path routing. We could have routing cases that overlap each other. Could we avoid using a switch statement?
That’s where Express Routing comes in…
Express routing comes built-in with the express module, making it easy to integrate with the Functions Framework.
To get started, first explicitly install express:
npm i express
In fact, your functions are already using express with the Functions Framework (whether you know it or not), so you aren’t adding an extra dependancy.
Then, separate out your methods into separate express apps. This is called application-level middleware.
Here, we’ll show the same code as above, but using basic Express Routing instead of a switch statement:
Here we are explicitly creating an express object and using app.use to define a routing function as middleware. We can easily specify URL path parameters like :id that will be available in our handler under req.params.id. Now that you’ve defined the express object, you can also look into using other middleware like parsing cookies, serving static content, etc.
And that’s how you can use Express Routing with the Functions Framework!
Thanks for reading.
Give us feedback by creating a new issue on GitHub! We are building more documentation on how to use the Functions Framework like using Docker and using Cloud Events.
Check out these other relevant resources:
Google Cloud community articles and blogs
234 
3
234 claps
234 
3
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Google • Explorer
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://itnext.io/you-can-now-write-google-cloud-functions-with-node-js-8-bd066ea6d4f5?source=search_post---------175,"In my previous article looking at environment variables with Google Cloud Functions I noted that one of the downsides of using them is the relatively old Node.js 6 runtime that Google provides. As the environment is fully managed there was no option to use a newer Node.js version. Things changed recently, as Google has now added beta runtimes for Node 8 and Python 3. This is a welcome move…
"
https://engineering.flosports.tv/google-cloud-firestore-document-crud-with-php-1df1c084e45b?source=search_post---------176,"There is no love for PHP with Google’s newest addition Firestore. And it is unlikely there is going to be since Firebase never got an official PHP SDK. In that case Firebase has been around a long time so there are enough open source libraries created by other users to choose from.
But Firestore is so new that I found none. So being that we will soon have the need to write to Firestore from one of our PHP applications… well I decided to create the bones of one using the REST API documentation. There were no examples online outside of that!
First thing that you need to do is get authenticated. The easiest way is to create an API Key, which you can do by logging into this section of your Google Cloud Console: https://console.cloud.google.com/apis/credentials
Now with your API key and project ID in hand, download this gist of the basic library I fleshed out and save it as firestore.php. Now let me get out of the way… this is not a mature library and it is not especially well written, full featured, or abstracted as it should be. I just roughed out this concept in a couple of hours, so get off my back!
Well you can dig into the source and find out most things, but I’ll go over a few interesting high points.
The API root is this: https://firestore.googleapis.com/v1beta1/
After that we append a path that tells Firestore what project and database we are addressing. Apparently at some point each Firestore project will have the capability to have more than one database. For now it is aways (default) and so the first part of our path is like this:
projects/{project_id}/databases/(default)/
After that we get into actually addressing our resource. In this tutorial, I am only dealing with documents and not collections or indexes or anything. So the next part is only going to be documents like so:
projects/{project_id}/databases/(default)/documents/
After that you add your collection name. In my case the collection name is people so our path is extended to be:
projects/{project_id}/databases/(default)/documents/people/
Creating a Document
If you adding a document without defining the document id then you will just use a POST to that path. And you will post a JSON formatted payload. This part tripped me up, because at first I just wanted to post something like this:
Whereas that would have probably worked with Firebase Realtime Database, that absolutely did not work here. Reason? The fields have more specific value types, so you have to actually define that like so:
To be complete all of the possible *value options are:
We will continue using that same document format throughout, no matter which method.
Updating a Document
So with an update (and all the rest of the methods below) we are actually addressing a specific document with a certain name. This update can either be creating a new document (an insert) or updating an existing one.
Here we are using a PATCH which means that if it already exists, we are overlaying these values. In other words, we’re not replacing the object (delete and then write again) we are updating it in place. And if we leave off a field, it will not remove it.
So our path just continues were we left off but adds the document id, like this:
projects/{project_id}/databases/(default)/documents/people/jason.byrne
We will simply do a PATCH with the same document format as in our POST above, and again this will be like an update.
Firestore does provide a way to make it a strict insert (meaning it would fail if it already existed) by adding
to the query string. Or if you wanted to make it a strict update (meaning it would fail if it did not exist yet) by adding, of course, the opposite,
to the query string. Leaving this property off will mean that it will create OR update the document with that name. Also known as an upsert.
Getting the Document
This is so straight forward that you don’t need any much explanation of it. You just use the same path as you would with your update above, just do an HTTP GET method instead.
What you will receive back will look something like this:
Deleting the Document
Same as a GET or a PATCH, but it would use— obviously — the HTTP DELETE method instead. There was no gotchas that I found with it.
Replacing the Document (PUT or set)
So I don’t have a good answer for this one. Logic would say that you could just do a PUT instead of a PATCH and it would replace (set) the document, rather than doing the overlay (update). However, for some reason this method doesn’t seem to be supported.
I believe if you were to do a set, you’d have to do it as a batch or something like that. But to be honest, I found it annoying that they didn’t support the PUT. And further, I am just too impatient to figure it out right now. And the PATCH should suit my needs fine… so you figure it out if you need it!
Here is a starting point: https://cloud.google.com/firestore/docs/reference/rest/v1beta1/projects.databases.documents/write
What about the REST?!? (get it? … “rest” like double meaning? #dadjoke)
This is just the basics. I didn’t even get into querying lists of data. Maybe they’ll be a continuation or maybe not. This is all I needed for now, and I just hope that it helps someone else get a place to start from!
Thatttt’s alllll folks!
Experience, ideas, half-baked thoughts, and ramblings from…
265 
11
265 claps
265 
11
Written by
CTO @ Echelon Fitness. Founder of MileSplit. Entrepreneur, technology executive, historian, writer, Christian, family man, and track & field fan.
Experience, ideas, half-baked thoughts, and ramblings from the coders, testers, devops, hackers, and geeks at FloSports. http://www.flosports.tv
Written by
CTO @ Echelon Fitness. Founder of MileSplit. Entrepreneur, technology executive, historian, writer, Christian, family man, and track & field fan.
Experience, ideas, half-baked thoughts, and ramblings from the coders, testers, devops, hackers, and geeks at FloSports. http://www.flosports.tv
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alexismp/deploying-a-quarkus-app-to-google-cloud-run-c4a8ca3be526?source=search_post---------177,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alexis MP
Apr 9, 2019·3 min read
(updated to Quarkus 1.0 RC)
If you haven’t heard of Google Cloud Run, which was announced at Cloud Next ’19, you should check out cloud.google.com/run for more details.
Essentially this is a way to deploy and scale your stateless containers on a serverless infrastructure — no infra or cluster to setup/manage, auto-scaling (including to zero), and billing as you go based on actual user requests, not on instances running.
Quarkus on the other hand is also a recent project by RedHat introduced here and meant to support cloud native Java apps with an option to compile them down to native code.
This article explores the deployment of Quarkus native app to Cloud Run (all using Cloud Shell to minimize setup steps). This should be a nice combo with fast startup time, thus minimizing any cold start issue.
I followed the Quarkus getting started and Building a Native Executable instructions, up until the “Packaging and run the application” section. I then built and packaged the sample app using these commands :$ mvn package -Pnative -Dquarkus.native.container-build=true$ docker build -f src/main/docker/Dockerfile.native .
The Java application is compiled into a native Linux 64-bit binary optimized for startup time and memory consumption using GraalVM and is containerized. Building the application into a native artifact does take a bit of time (about 5 minutes for me, but YMMV). You can think of this a trading compile time for runtime performance.
To use Cloud Build instead of building locally, you can use a multi-stage Docker build (no need to make GraalVM available to the build environment).
I then tagged and pushed the resulting docker image to Cloud Registry using a reference to the Google Cloud $PROJECT_ID that I use :$ docker tag 0b03762bad42 gcr.io/$PROJECT_ID/quarkus-quickstart$ docker push gcr.io/$PROJECT_ID/quarkus-quickstart
At this point I am are ready to deploy the container to Cloud Run :$ gcloud beta run deploy \ --platform managed \ --allow-unauthenticated \ --image gcr.io/$PROJECT_ID/quarkus-quickstart \ --set-env-vars DISABLE_SIGNAL_HANDLERS=foobar
That’s really it! Nothing else to configure!Note: the environment variable listed in the command above is required as discussed in this Quarkus issue (tl;dr: shutdown hooks are not called).
Your app is now exposed as a service on a run.app domain and it will auto-scale from 0 to multiple container instances to adapt to the incoming load.
The logs show a nice startup time indeed :
If you enjoy Quarkus, Google Cloud Run is a great option to deploy your containers. If you’d rather deploy to an existing Kubernetes cluster which you manage then try Cloud Run on Anthos and deploy that same container! The underlying Knative technology is what makes Cloud Run workloads portable.
Next steps would be to hook up the application to Cloud SQL, Cloud Datastore or other GCP databases.
Here are a few additional Cloud Run resources:
Google Cloud Developer Relations
99 
2
99 claps
99 
2
Google Cloud Developer Relations
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/hacking-and-slacking/manage-files-in-google-cloud-storage-with-python-73cbd9b1b010?source=search_post---------178,"Sign in
There are currently no responses for this story.
Be the first to respond.
Todd Birchard
Jun 19, 2019·6 min read
I recently worked on a project which combined two of my life’s greatest passions: coding, and memes. The project was, of course, a chatbot: a fun imaginary friend who sits in your chatroom of choice and loyally waits on your…
"
https://medium.com/stellarpeers/how-would-you-increase-growth-and-market-share-for-google-cloud-fa837301931f?source=search_post---------179,"There are currently no responses for this story.
Be the first to respond.
Top highlight
by Malena Mesarina, Co-founder, StellarPeers
StellarPeers is a community platform that helps professionals prepare for interviews. We think the best way to prepare, is to work through questions and practice mock interviews as much as possible. We meet weekly to discuss product management interview questions on product design, product launch, strategy, marketing, pricing, and others. Last week, we worked on a product strategy interview question.
As a product manager, one of your responsibilities is to grow your product’s revenue and market share. This product management interview question is testing whether you can think strategically about how to go about it.
The interviewer is evaluating you on the following:
INTERVIEWEE: Okay, before brainstorming about possible ways to grow revenue and market share for Google Cloud, I would like to first talk about the competitive landscape and market trends. Understanding how competition plays will help in identifying possible vectors of differentiation that Google can leverage to spur growth. And, identifying market drivers will help in identifying opportunities of adjacent or new markets to enter.
Amazon AWS, Microsoft Azure and Google Cloud are the top players in the cloud computing market. As a first mover, Amazon has had the advantage of time; AWS market share is multiple times that of Azure and Google Cloud Platform, so that is a challenge.
Amazon has the richest and most extensive IaaS and PaaS capabilities of the three. Its strengths are in deep user management capabilities; an ecosystem of open source tools which has attracted thousands of ISV partners; a network of partners that provide application development, managed and professional services; and training and certification programs. Where AWS has weaknesses is in its complex pricing model, which is too granular, and expensive customer support.
Microsoft Azure, second in market share, has several strengths: seamless integration with enterprise on-premises infrastructure, development tools, open source technologies, competitive pricing, and a large number of existing customer relationships. But in addition to not having a complete cloud solution as AWS, Azure is not very strong in API enablement, and lacks partners in managed services and professional services that have enough experience with Azure.
Google’s strength lies in its expertise in developing and managing cloud-native applications, analytics and machine learning as well as fast virtual machine provisioning and simpler billing. Google has room for improvement in a few areas though, such as user management to provide organizations more granular and customizable access control; the need for a marketplace to license third party software and the need to increase its efforts in sales, marketing, globalization and partnerships.
Let’s talk about what is driving the cloud computing market growth.
IoT security needs will be on the rise and cloud computing solutions for this type of security will be in demand. Last year’s Dyn incident, where IoT devices were used to orchestrate a distributed denial of service (DDoS) attack on a Dyn DNS server that supported major commercial websites (Twitter, Pinterest, Reddit, Github, Spotify), shows that IoT device makers and service providers will have to increase their investment in IoT security solutions.
Data Mining and query services for vertical industries will be on the rise. Industries such as healthcare, government, finance, retail and weather forecast will look for ways to optimize customer service and operational efficiencies by adopting intelligent services that can uncover patterns, provide predictions, and answer complex questions that can only be done with advanced big data analytics technologies.
AR and VR applications will be on the rise not only for the Entertainment and Gaming market but also for the Education and Training market. These types of applications will demand vast cloud computing resources.
Advanced online collaboration tools and features will be on the rise as the number of people working in separate locations becomes the norm. This will drive the need for cloud solutions that enable deeper online collaboration. Google already has put a stake on the ground with Google Suite, and it should continue to innovate.
Now that we have looked at the competitive landscape and market trends, I would like to brainstorm ideas for growth. I think Google Cloud should follow growth strategies based on competitiveness, acquisitions, and innovation.
Growth could be achieved by playing on Google’s competitiveness along its key differentiating technologies. One such technology is its enterprise APIs. The acquisition of Apigee was a step in the right direction; it put Google ahead of AWS and MSFT in the API software integration space. Google should continue to expand features on these platform or acquire additional startups in this space.
Google could also use its differentiated technologies like Machine Learning, AI, and VR/AR to expand into related markets. For example, it could expand into the AR/VR Training and Education software market by leveraging its strong relationships with schools and universities.
Google is one of the few large technology companies that has the computing power to provide data mining and query services that vertical industries with huge Big Data needs can benefit from. In healthcare, Google is already partnering with schools such as Stanford Medicine to provide a genomics service mostly for research, but it should try to expand the service for commercial purposes. Microsoft just announced its Healthcare Next initiative to bring AI to doctors and hospitals, and Google should do the same.
Partnerships are one of the weak points of Google Cloud. Partnerships with application developers, managed services and professional services are key to growing the number of customers. More investment in these efforts is needed. Furthermore, to be able to increase the number of enterprise customers, it needs to have an application marketplace, just like AWS and Azure has.
And given that AWS and Azure are complex platforms to develop on and manage, Google could strive to provide ease of use and training programs to evangelize its platform to customers and developers.
As IoT security becomes a critical issue for all enterprises, Google should start working towards becoming the leader in this space. Acquisitions in this space would make it faster to acquire the technology and skills in IoT security, which would also help promote Google IoT related businesses like Nest, Android Wearables and Google’s driverless technology.
Advanced online collaboration is an interesting market too. Google dominates in the Education space with Google Suite, and the SMB space, and needs to grow its enterprise presence. The online collaboration market, however, is a more mature market and growing slower than other markets. I think growing this market through product innovation, customer relationships and partnerships, as opposed to acquisitions, would result in a better bang for your buck. Acquisitions are expensive and I don’t think this market is growing fast enough to offer a quick return.
In summary, I think Google Cloud can grow revenue and market share through strategies that include competitive differentiation, acquisitions, partnerships and innovation. In particular, Google could leverage its strengths in Big data analytics, machine learning and AR/VR, to expand into related markets like Education & Training, industry verticals such as Healthcare, Finance and Government. Acquiring selective startups in IoT security would strengthen Google’s leadership in this market, which is likely to grow rapidly. And, it should continue to innovate in the online collaboration space. Of course Google should continue to acquire technologies to catch up with Amazon’s offerings, but faster growth will come if Google leverages its differentiating technologies and skills to become number one in areas of cloud computing that its competitors can’t easily copy.
To learn more about product management interview questions and to find practice partners, visit us at StellarPeers.com.
StellarPeers is a community platform that helps…
173 
2
173 claps
173 
2
Written by
StellarPeers is a community platform that helps professionals prepare for product management interviews. https://stellarpeers.com
StellarPeers is a community platform that helps professionals prepare for product management interviews.
Written by
StellarPeers is a community platform that helps professionals prepare for product management interviews. https://stellarpeers.com
StellarPeers is a community platform that helps professionals prepare for product management interviews.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/secure-graphql-apis-in-minutes-with-google-cloud-run-and-grand-stack-97d050dbc744?source=search_post---------180,"There are currently no responses for this story.
Be the first to respond.
Google Cloud recently announced Cloud Run at the NEXT ’19 conference, and I wanted to kick the tires. Cloud Run lets you run stateless containers easily, with Google managing infrastructure and scaling for you. You can use them in a way that is similar to Google Cloud Functions, except you can bring any runtime you like, because you’re deploying a Docker container.
I thought I’d give Cloud Run a shot as a serverless backend for a GraphQL application, and it turned out to be pretty easy!
This picture above shows what we’re going to be setting up. Starting with a schema, we want to set up a GraphQL microservice. Users issue queries, which get handled by a serverless backend running on Cloud Run. That backend uses a Neo4j database to store the data. The whole setup is protected by an OAuth2 layer.
What’s GraphQL? It is a query language for APIs and a runtime for fulfilling those queries with your existing data. GraphQL provides a complete and understandable description of the data in your API, gives clients the power to ask for exactly what they need and nothing more, makes it easier to evolve APIs over time, and enables powerful developer tools. (GraphQL.org)
And it’s going to be relatively straightforward, I promise, because Google is going to do a lot of it for us. Let’s get started!
We’re going to need a place to store our data. I launched Neo4j Enterprise on GCP Marketplace. Because Neo4j is a native graph database, it’s particularly well suited for backing GraphQL APIs, following the GRAND Stack pattern.
Setting up the database just requires filling in a few values and launching it into the GCP project of your choosing. Documentation on how to do that can be found here. After a few minutes, my deployment finishes, and I have an endpoint URL, username, and password for my database.
A note on Kubernetes: Neo4j is also available for GKE, and of course Cloud Run allows us to deploy containers into an existing cluster if we wish. This is a good option, but for simplicity of setup, we’ll go with VMs today so that we don’t need to create and configure a GKE cluster.
Let’s head over to the Cloud Run console, and click the “Create Service” button.
I’m using a small utility container I built for this purpose called Plumbline. This container is basically just a Dockerfile wrapper around a simple node.js program which uses neo4j-graphql-js to connect to Neo4j. This code manages the GraphQL layer and translates user queries into the underlying database query language (Cypher) that Neo4j uses. The container image URL we are deploying is my public version, gcr.io/neo4j-k8s-marketplace-public/plumbline:0.0.1, but of course you could build it from source yourself, or use any other container.
We only needs a list of GraphQL typedefs, and some connection information for our new database in order to work. Neo4j-graphql-js handles everything else, including automatically adding accessors and mutators to our schema.
Neo4j-graphql-js is very convenient because it lets us focus on writing a simple domain model, and most of the boilerplate gets handled automagically.
Here’s a small set of GraphQL typedefs. We’ll be starting with a totally empty graph, and will want to create our information along this model:
Simple enough; we have named people who can know other people, and who can also have hobbies that they like. A graph this simple could help us drive recommendations of who to introduce to whom, because of shared interests!
Those @relation directives are cues to neo4j-graphql-js on how to store the information in question. Our “Person” items will be nodes in the graph, and we’ll store “KNOWS” relationships in Neo4j to handle the relationships. If you want the full details on how all of this works, consult the neo4j-graphql-js documentation.
Next, in our “Create a service” screen, we need to set environment variables, which are initially hidden behind the “Show Optional Settings” button.
This is letting our container know how to connect to our database, and what set of typedefs should be used to create the GraphQL API.
Then click create! A few moments later:
Since we created a database, and we’re getting ready to both read and write data, exposing such an API publicly to the world isn’t a good idea. Cloud Run allows us to expose endpoints publicly if we want, but we didn’t take this option.
The default with Cloud Run is that you get an OAuth2 layer for free, that is backed by the same permissions that your Google Cloud project has. By generating an OAuth2 “Bearer Token” using the regular gcloud commands, you can authenticate to the endpoint we just created.
This is very convenient, because it saves us from having to have extra authentication logic in the docker container itself! Google has also provided documentation on how your container can use that token to get information about the user.
If you’re wondering how to get the token that will let you authenticate to your endpoint, it’s like this:
For simplicity, you can use curl to use our new API, which has an HTTPS endpoint. In order to work, curl needs to send the bearer token to authorize against our new service, and also needs to set a few extra headers.
Here’s a curl example of how we can use our API:
Actually formatting our GraphQL queries as JSON for curl though gets old quickly, so I used an Apollo Launchpad to issue my queries. If you’re trying this at home, you need to make sure your Launchpad, wherever you run it, includes that bearer token, or you’ll just get unauthorized errors.
Let’s create a person and a hobby. These mutators exist courtesy of neo4j-graphql-js mentioned above, and are auto-generated to go with our nice GraphQL typedefs.
Let’s create another person, “Will”, and assert that David likes Guitar.
Will likes Guitar too….
Finally, we’ve got a simple graph in place, and we can use a regular GraphQL query to check that our data is there:
Looking into the backend on the Neo4j browser side, we can see the same, using Cypher.
Cloud run is a great way to deploy a serverless backend that’s easy to manage and use. Combining neo4j-graphql-js and Neo4j on Google Marketplace, you can stand up a full-CRUD GraphQL API really quickly, and use it as a microservice as part of a larger solution.
From here, we can:
Google Cloud community articles and blogs
249 
3
249 claps
249 
3
Written by
Architect at Neo4j
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Architect at Neo4j
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/ethereum-on-google-cloud-platform-8f10c82493ca?source=search_post---------181,"There are currently no responses for this story.
Be the first to respond.
Google’s super-fast network, powerful VMs and SSDs coupled with its state-of-the-art container technologies make Google Cloud Platform (GCP) an unparalleled destination for Blockchain platforms.
Google could do better in ensuring that developers are aware of this.
Late last year, I wrote a story describing how to deploy Ethereum to GCP. I’d not used Ethereum between then and today. This story updates that previous post with easier ways to run Ethereum on GCP using: (a) Container-Optimized OS; (b) using Kubernetes Engine.
You’ll need a GCP Project and there are free options. You will need Cloud SDK (aka gcloud) installed. Let’s get started.
Container-Optimized OS permits you to deploy a Docker image to a Google Compute Engine VM. This is the simplest way to run the ethereum/client-go image:
NB To avoid exhausting disk, the container uses 500GB (the entire default Quota) of SSD Persistent Storage. Reduce the boot-disk-size or change the boot-disk-type to pd-standard as you prefer.
NB If you would prefer to use the test network add — container-arg=""— testnet” before the existing — container-arg flags. If you would prefer to use Rinkeby add — container-arg="" — rinkeby"" before the existing--container-arg flags.
The VM should be running in under 30 seconds, you can peek into its state with:
NB konlet-startup is the systemd service that corresponds to the Docker image deployment in the Container-Optimized OS.
Success looks like:
And you may confirm the container is running:
Then, you may use gcloud to start the ssh port-forward to the instance:
And — from a different shell — you can connect to the Ethereum node running on the instance:
Which should yield:
Running Ethereum on GCP leverages Google Cloud Logging. You will automatically be able to view and query logs.
https://console.cloud.google.com/logs/viewers
NB Slightly confusingly (!), you must select “Global” as the resource and you should then see “gcplogs-docker-driver” as an option. This will — as above — provide the container’s logs.
Everything you see via Cloud Console is accessible from the CLI too. In this case, you may run the following command to view the container’s logs:
You should log entries like this:
NB What you *don’t* want are log entries of the form “no space left on device” as these terminate the container. If you receive these you’ll need to increase the size of the boot-disk-size flag or grab less data.
You also get basic monitoring *but* of the VM not the container(s) specifically:
You may exit the JavaScript console then delete the Ethereum VM with:
If you would prefer to use a Kubernetes (Engine) cluster, the commands are as straightforward.
Assuming you are authenticated against a cluster, the following steps create a namespace for Ethereum, create a disk (actually a Kubernetes PersistentVolumeClaim), deploy a single Go Ethereum (aka “geth”) node and port-forward to your local workstation so that you may attach to it:
NB If you would prefer to use the test network insert ""--testnet"" (including the quotation marks) between lines 33–34. If you’d prefer to use Rinkeby network insert ""--rinkeby"" instead.
This should result in:
And you should be able to see the Persistent Volume Claim created:
https://console.cloud.google.com/kubernetes/storage
And the Deployment:
https://console.cloud.google.com/kubernetes/workload
Running Ethereum on GCP yields some useful, automatic benefits including, Console monitoring:
Logging:
https://console.cloud.google.com/logs/viewer
Some Stackdriver goodness (although this is even better with the recently-announced Stackdriver Kubernetes Monitoring — not enabled on this cluster). If you’ve not configured Stackdriver for the project, you’ll need to do that first and you may use the free tier:
https://console.cloud.google.com/monitoring
We’ll use ssh port-forwarding (through gcloud) to access the Go Ethereum node from our workstation without opening a firewall.
NB These nebulous looking commands grab one of the Kubernetes cluster’s node names; determine which NodePort the Ethereum Service is being served on; and then uses gcloud to port-forward that node’s port to the localhost’s (same) port. If you prefer you could use — ssh-flag=“-L 8545:localhost:${PORT}” and then in the subsequent command replace ${PORT} with 8545 too. Your choice!
NB In my case echo ${PORT} (this time) returned 30873.
Then, from your local workstation, run the geth JavaScript client and attach to the node running on the Kubernetes cluster:
NB In my case, the value of ${PORT} is 30873.
And you should then be put in the Ethereum JavaScript console:
When you’re done, it’s simplest to whack the Kubernetes namespace which will delete the Deployment, the Service and the PeristentVolumeClaim:
I uncovered an issue using Wallets passed to Kubernetes (Engine) as Secrets. Providing data to Pods using Secrets (and ConfigMaps) is considered a good practice with Kubernetes. However, the implementation of both Secrets and ConfigMaps when (volume) mounted into Pods is to present file content as symbolic links. This appears to be a problem with Wallets and Ethereum:
https://github.com/ethereum/go-ethereum/issues/16793
If you’d like to use existing wallet file(s) when you deploy to Kubernetes, this should be as simple as creating a Secret (you may use ConfigMaps similarly but Secrets provide opacity for… secrets and other confidential info) and then mounting the Secret as a volume mount for the Ethereum node to access. However, as above, this does not work correctly with Ethereum. So, you will need the additional step in the ‘hacky workaround’ below.
First, create the Secret:
NB Your path should end in a directory called keystore and this directory and its wallet(s) will be encoded in the Secret.
My hacky workaround is to use init containers to copy the wallet(s) from the Secret (called /keystore) into an emptyDir (called /cache) volume *before* the Ethereum node starts and configure Ethereum to look for the wallet in /cache rather than /keystore (because this doesn’t work). This works because copying the symbolic link duplicates the underlying file.
NB See the next section ‘SSD’ to configure the datadir as SSD rather than HDD.
NB Line #31–39 create an init container called init-service. init-service uses Alpine to copy files of the form UTC* from /keystore to /cache. These directories are volume mounted. The volumes are defined (for the Pod not specific containers) in lines #66–75. keystore is the Secret containing wallet file(s) mounted as symbolic link(s) that do not appear to work with Ethereum if used directly. cache is an emptyDir volume and, if Ethereum uses the wallet(s) when copied here, it does work!
NB In line #46, Ethereum is configured to use the /cache directory for wallets with --keystore=/cache.
The Kubernetes Deployment used above includes a PersistentVolumeClaim spec and line #7 defines storageClassName of standard. This corresponds to regular Persistent Disk.
If you’d prefer to use faster SSD, we must first register the new Storage class in Kubernetes Engine:
and:
Now, when you query the Kubernetes Engine storage classes, a new type pd-ssd called ssd should be added:
Then, we can revise the Deployment file’s line #7 and change storageClassName: standard to storageClassName: ssd.
NB Default “Persistent Disk SSD (GB)” Quotas for GCP Projects are 500 GB (gigabytes) per zone. If you’ve not increased these quotas, you will be unable to provision a 500 GiB (!) disk when you Apply the Deployment. For this reason, please also *reduce* the value of line #12 in the Deployment to e.g. 400Gi (~429GB) which should be sufficient *unless* you have other SSD PD in your Project in the zone. Or, request a Quota increase:
https://console.cloud.google.com/iam-admin/quotas?project=${PROJECT}&service=compute.googleapis.com&metric=Persistent%20Disk%20SSD%20(GB)
To effect this change we must delete and recreate the Deployment:
NB There are 4 Compute Engine Disks: 3 are “Standard Persistent disk” one for each of the 3 Nodes in the Kubernetes cluster. The 4th disk is 430GB (==400GiB) and is of type “SSD persistent disk”. It was created by the PersistentVolumeClaim when we applied the Deployment.
In this post, I’ve provided two straightforward ways to run a single Ethereum node on Google Cloud Platform.
Feedback is always welcome.
That’s all!
Google Cloud community articles and blogs
237 
2
No rights reserved
 by the author.
237 claps
237 
2
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/swlh/how-to-ci-cd-on-google-cloud-platform-1e631cded335?source=search_post---------182,"There are currently no responses for this story.
Be the first to respond.
Google Cloud Platform is one of the leading cloud providers in the public cloud market. It provides a host of managed services, and if you are running exclusively on Google Cloud, it makes sense to use…
"
https://medium.com/machines-school/data-science-on-the-google-cloud-platform-%E0%B8%97%E0%B8%B3%E0%B8%84%E0%B8%A7%E0%B8%B2%E0%B8%A1%E0%B8%A3%E0%B8%B9%E0%B9%89%E0%B8%88%E0%B8%B1%E0%B8%81%E0%B8%81%E0%B8%B1%E0%B8%9A-google-cloud-platform-gcp-7605b4560fb8?source=search_post---------183,"There are currently no responses for this story.
Be the first to respond.
“Cloud Computing” หรือแปลเป็นภาษาไทยว่า “การประมวลผลแบบกลุ่มเมฆ” เป็นลักษณะของการทำงานของผู้ใช้งานคอมพิวเตอร์ผ่านอินเทอร์เน็ต ที่ให้บริการใดบริการหนึ่งกับผู้ใช้ โดยผู้ให้บริการจะแบ่งปันทรัพยากรให้กับผู้ต้องการใช้งานนั้น
โดยสถาบันมาตรฐานและเทคโนโลยีแห่งชาติของสหรัฐอเมริกา (NIST) ได้ให้จำกัดความไว้ว่า “เป็นรูปแบบที่ให้บริการเพื่อใช้ทรัพยากรคอมพิวเตอร์ร่วมกับผู้อื่นผ่านทางเครือข่ายเน็ตเวิร์ค เช่น โครงข่ายเน็ตเวิร์ค เซิร์ฟเวอร์ หน่วยเก็บสำรองข้อมูล แอพพลิเคชั่นและเซอร์วิสต่างๆ เป็นต้น โดยขึ้นอยู่กับความต้องการของผู้ใช้ ”
Google Cloud Platform หรือเรียกย่อๆว่า GCP เป็นกลุ่มทรัพยากรในการประมวลผล (computing resource) ของ Google ที่เปิดให้ทุกคนสามารถเข้าใช้งานได้ในรูปแบบของ Public Cloud
Google Cloud Platform สามารถใช้งานได้ทั่วโลก โดยเราสามารถเลือกใช้งานได้ตาม “regions” และ “zones” ทั้งนี้ขึ้นอยู่กับ latency, availability, durability
regions จะขึ้นอยู่กับลักษณะทางภูมิศาสตร์ เช่น US East, US West, US Central, Europe, East Asia, Southeast Asia, South Asia หรือ Australia เป็นต้น และในแต่ละ regions จะประกอบด้วย zone เช่นใน regions ของ Southeast Asia ที่ตั้งอยู่ในสิงค์โปร์ก็จะมี zone A และ zone B เป็นต้น
Google Cloud Platform จะแบ่งตามลักษณะการใช้งานดังนี้ (ข้อมูลเมื่อวันที่ 7 มิถุนายน 2561) แต่บาง region ก็ไม่สามารถใช้งานได้ในบาง serivice
*หมายเหตุ: ข้อมูลเมื่อวันที่ 7 มิถุนายน 2561
บทความเรื่อง “Data Science on the Google Cloud Platform”ตอนที่ 1: ทำความรู้จักกับ Google Cloud Platform (GCP)ตอนที่ 2: วัฎจักรข้อมูล (Data Lifecycle) บน Google Cloud Platform
Machines School
106 
106 claps
106 
Machines School
Written by
Founder of Humaan.ai—The AI as a tools for unleash human capabilities. 🧠 🚀
Machines School
"
https://towardsdatascience.com/how-to-do-serverless-machine-learning-with-scikit-learn-on-google-cloud-ml-engine-db26dcc558a2?source=search_post---------184,"Sign in
There are currently no responses for this story.
Be the first to respond.
Lak Lakshmanan
Aug 3, 2018·4 min read
On Google Cloud Platform, Cloud ML Engine provides serverless machine learning, for training, hyperparameter optimization and predictions. Until recently, that was only for TensorFlow. Recently, though, the team has implemented all three capabilities for scikit-learn. In this article, I will take it for a spin.
The problem is to predict the weight of a baby given some information about the pregnancy. This is a problem that I solve end-to-end with TensorFlow in a bootcamp that I developed for GCP NEXT 2018. Let me now do with scikit-learn. Follow along with me by looking at this Jupyter notebook on GitHub.
The input data is in BigQuery, and I pull 1/1000th of the full dataset into a Pandas dataframe:
It is possible to use the full dataset if I use a larger VM, and I will do that in the final step, when training on the service. For developing the model, though, it is helpful to have a small dataset.
To train the model, I first wrote a function to get the x and y for training (I’m calling them features and label):
Then, I created a RandomForestRegressor and passed the x and y into its fit() method:
Evaluating the performance of the model can be done by calling predict() on the evaluation dataframe and compute the RMSE:
Once I had the code working, I added the following Jupyter magic to the cells in the notebook to write the cells out to Python files:
I also made many of the hardcoded numbers (such as the number of trees in the random forest) a command-line parameter. By making them command line parameters, I made it possible to hyperparameter tune them.
I tested the package locally, still on the 1/1000th of the dataset:
Training on Cloud ML Engine is as simple as submitting the Python package:
This time, I am training on 1/10 of the full dataset. To train on the full dataset, I will need a larger machine. I can do that by changing to a custom tier:
and passing in the above configuration file:
How did I get the max depth and numTrees? For that, I did hyperparameter tuning.
For hyperparameter tuning in ML Engine, write out a summary metric after evaluation. This is the code, assuming you have a variable named rmse which holds the final evaluation measurement:
To submit the hyperparameter tuning job, write a configuration file with the parameters you want to optimize:
Then, pass it to ML Engine with:
The outputs of the trials will start to populate and if you look at the GCP web console, the trial with the lowest RMSE will be listed, along with its runtime parameters.
Once trained, the scitkit learn model can be deployed:
The deployed model has an endpoint that you can access to get predictions:
Currently, the request has to be in the form of a text line, and has to be preprocessed data. Naturally, this imposes some caveats on how much you can expect to operationalize a scikit-learn model.
Again, my full code is on GitHub. Happy coding!
Data Analytics & AI @ Google Cloud
96 
96 
96 
Your home for data science. A Medium publication sharing concepts, ideas and codes.
"
https://medium.com/google-cloud/serverless-etl-on-google-cloud-a-case-study-raw-data-into-json-lines-d20711cd3917?source=search_post---------185,"There are currently no responses for this story.
Be the first to respond.
I’m working on a task that consists of populating BigQuery tables with Tomcat and Nginx access log data. Every day, web servers upload new log files to GCS, containing raw data from the previous 24 hours. Data need to be converted into a format that is understood by BigQuery Jobs in order to be loaded into the tables.
I decided for the JSON Lines, or newline delimited JSON, to be the target format instead of CSV due to the nature of the data I’m handling. Since one can’t predict the data that will be transformed during this ETL process (e.g. URLs can include several UTF-8 chars, including commas), I decided not to rely on commas as delimiters to avoid problems.
So the goal is set: given standard access logs generated by web servers, they need to be transformed into newline delimited JSON files in order to fit BigQuery Jobs’ input arguments. Such jobs run later in the data warehousing process.
Tomcat logs, for example, are generated in the Common Log Format:
Which means:
And BigQuery Jobs need them as input in the JSON Lines format:
What I’ll describe in this article is how I designed and coded a solution to achieve this goal using Cloud Functions and Node.js Client for Google Cloud Storage. Although processing access logs is my main goal, the overall strategy may fit a wider range of ETL processes — with specific changes, mainly in the Transform phase. I’ll explain the solution and comment on the most important parts of the code I wrote to solve the problem. Sources are available on GitHub and links provided along with the text.
Please take a look at the below picture, it illustrates the designed workflow. Two Storage Buckets are used, and two Cloud Functions as well.
Since loading a JSON Lines formatted file into BigQuery (step 5) is a well-documented process, the present article is focused on the raw data to newline delimited JSON conversion (step 3).
The source bucket — intended for long-term storage — has the Standard Storage Class, but its files’ storage class is automatically changed to Archive when they turn 2 days old in order to decrease costs. The target bucket — short-term storage — also has the Standard Storage Class, but files are deleted when they turn 2 days old because they are no longer needed after being loaded into BigQuery.
Log files are composed of several lines — each of them representing a resource delivered by the HTTP server. The client for Google Cloud Storage allows us to download all content from a given file into memory for further processing, as follows:
A promise is returned by file.download() when there are no args in the call. The promise resolves with an array of buffers, and the first element contains all file contents. We need to convert it to a string before moving forward. Since the logs are encoded in UTF-8, calling toString() is enough in this case.
File download hint: download() can also be used to store the content into a local file. Please refer to the docs for details.
Stream reading hint: there’s a second option to read data from Cloud Storage: createReadStream(). In case you have data to be processed on a streaming pipeline, you might give it a try.
My first action to tackle the problem was to develop a javascript function that converts a raw log line into a JSON object. Basically, it uses a regular expression to break apart the values and assigns them to the JSON object’s attributes. Source code is available on GitHub.
Please notice the function returns a JSON string instead of a JSON object. This is the intended behavior so the code can be adapted to run an equivalent batch ETL process using Cloud Dataflow’s Storage Text to BigQuery template.
Now, let’s take a look at another important piece of the ETL code:
Splitting the previously downloaded raw content into an array of strings is also straightforward with the help of regular expressions.
Each element means a log entry that needs to be transformed into a newline delimited JSON. I’m using the ndjson library to help with this. ndjson.serialize() returns a transform stream that accepts JSON objects and emits newline delimited JSON. The transform job is done!
PasstThrough stream hint: you may use a transform function that does not return a stream. But, since having one is very useful in the next step, please consider using the built-in class stream.PassThrough to box the transformed data before moving forward.
— Wait, wait! What’s that LogParser class you’ve used?
— Good question! It’s simply a higher-level abstraction for the function that converts a raw log line into a JSON object, mentioned at the beginning of this section. The source code is available here.
The 3rd and last step is very simple: consists of storing the transformed content into a new file.
Once we have the JSON lines available in a stream, all we need to do is read such data and then write to a GCS file.
Cloud Storage client comes with the file.createWriteStream() method. As its name says, it creates a stream to write contents into a GCS file. Being a stream it can be nested to ndjsonStream, as shown in the above code snippet. Since writing the file is asynchronous, enclosing the pipe() call in a promise scope may help the Cloud Function to return properly when the process finishes, so I recommend this.
Please notice createWriteStream() accepts multiple options. I used resumable: false because I want the target files to be uploaded completely at once. Given their sizes are usually less than 10MB, this option makes them upload with a better performance.
The steps described in this article were designed to accomplish a serverless ETL workflow powered by GCP Cloud Functions and Javascript. At the end of the process, the transformed data is pretty much ready to be uploaded to BigQuery using a second Cloud Function as described in https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-json.
Some words about performance: a 256MB-memory Cloud Function instance takes on average 8 seconds to process a 1MB file (~9,000 log entries). In this case, most of the processing time is spent by network operations as I have noticed.
I hope the article helps developers and data engineers with similar needs to design their own processes, by adjusting or improving what I’ve done so far.
The reference code is available on GitHub: https://github.com/ricardolsmendes/access-logs-dw-gcp-js (src/raw-to-ndjson-gcs-file-converter.js). A Cloud Function that uses it resides at https://github.com/ricardolsmendes/access-logs-ndjson-gcs-gcf.
Happy 2020, and that’s all ;)
Google Cloud community articles and blogs
108 
3
108 claps
108 
3
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
head of data @ ciandt.com • hobbyist tech writer • dad • birder
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/javarevisited/5-best-courses-to-learn-google-cloud-platform-gcp-in-2021-169093a3771a?source=search_post---------186,"There are currently no responses for this story.
Be the first to respond.
Hello guys, if you want to learn Google Cloud Platform and looking for the best online courses then you have come to the right place. Earlier, I have shared the best AWS courses and best courses to learn Azure, and today, I am going to share the best Google Cloud courses for beginners.
Disclosure — Btw, some of these links are affiliate links and I may get paid if you join these courses using my links.
The GCP or Google Cloud Platform is a slightly late entrant in the world of public cloud computing but it has completely changed the public cloud landscape in the last few years, particularly the monopoly of AWS on Cloud has been challenged.
It’s still neither the most popular public cloud platform like Amazon Web Services, nor the darling of the corporate world like Microsoft Azure but its Performance, Tools, and innovation Google is attracting more and more companies to Google Cloud Platform, particularly in the areas of Big Data and Machine Learning.
With tools like BigTable, BigQuery, and libraries like TensorFlow, Google Cloud Platform is quickly becoming the go-to platform for Machine learning innovations.
Because of all these, there is an increased demand for Cloud experts who are familiar with GCP and Google Cloud Platform concepts and tools. Unfortunately or fortunately, currently, there is a lot of shortage with developer knowing Google Cloud Products like Bigtable, BigQuery, Google Compute Engine, etc. Similarly, If you look for a certified Google Cloud Architects, you will find one or two in your network, compared to a handful of AWS architects. This popularity of Google Cloud Platform and shortage of skilled GCP developers means it's the perfect time to learn Cloud Computing and Google Cloud Platform, especially if you are looking to make a career in Cloud Computing. On top of that currently, Google is currently offering $300 free credit which means there won’t be a better time to learn Google Cloud Platform than now. So, how do we start?
Well, there is no better way to learn Google Cloud than joining some of the best Google Cloud online courses on Coursera and Udemy which is offered by Google Cloud itself like Developing Applications with Google Cloud Platform Specialization on Coursera.
coursera.com
You can also use these courses to prepare for Google Cloud Associates and Professional Certifications like Google Cloud Associate Cloud Engineer and Google Cloud Professional Cloud Architect, GCP Professional Data Engineer, and Google Cloud Developer.
Here are some of the best online courses to learn about the Google Cloud Platform or GCP. In these courses, you will not only learn about concepts and technologies that make up the Google cloud world, but also understand what Google’s cloud has to offer for DevOps, Developers, and Machine Learning enthusiasts. Some of the courses will also help you to prepare for the Google Cloud Associate Cloud Engineer certification exam like the second course on the list is by Ryan Kroonenburg, who is also the author of best selling AWS Certifications course like this AWS Solution Architect course. I have also included some of the best Google Cloud courses from Coursera, particularly a Specialization which will not only teach you how to design, develop, and deploy Apps on GCP. but also provides lots of hands-on labs to practice with Google Cloud components and services, which will eventually help you to build secure, scalable, and intelligent cloud-native applications. Without any further ado, here is my list of best courses to learn Google Cloud Platform online:
This is one of the best online courses to learn the fundamentals of Google Cloud and its Big Data technologies as well as to pass the Google Data Engineer and Google Cloud Experts Certification exam. In this course, you will not only learn about GCP fundamentals like Compute Engine and App Engine but also about their Big Data and Machine learning tools like BigQuery, Bigtable, DataProc, Datalab, TensorFlow, and Hadoop clusters. I highly recommend this course for beginners who wants to learn about Google Cloud Platforms as well as people who want to prepare for Google Data Engineer and Google Cloud experts platform.
Here is the link to join this GCP course — GCP: Complete Google Data Engineer and Cloud Architect Guide
Both instructors are very knowledgeable and have strong experience in Google big Data technologies which reflect in this course. They are also from Google itself and had first-hand experience of these technologies, which makes this course even more interesting. Talking about social proof this course has already taught Google Cloud Fundemantsl to well over 20,000 students and have on average 4.2 ratings from close to 3,110 rating participants, which speaks volumes about its quality.
This is one of the most complete online resources to learn the Google Cloud Platform. This Coursera specialization is a collection of 4 online courses that will teach you how to design, develop, and deploy applications that seamlessly integrate components from the Google Cloud Platform. The course is very hands-on and thorough on explanation. You will learn key GPC concepts like compute engine, cloud storage, dataflow, etc to develop a scalable and intelligent cloud-native application. The best part of this specialization is that the courses are offered by Google Cloud Training itself. I have taken this course and vouch for the quality of them. They are just too good on both content and delivery. It would take approx 1 month to complete if you can spend 14 hours/week, but the course is completely online and you can go on your own schedule. The course is also available in French, Portuguese, German, Spanish, and Japanese apart from English.
Here is the link to join this GCP course — Developing Applications with Google Cloud Platform
Ok, let me tell you that this is probably the best online training course to pass the prestigious Google Certified Associate Cloud Engineer and Architect exam, not just for content but also for presentation and delivery. The cloud guru Ryan Kroonenburg is one of the authorities when it comes to cloud and having gone through his AWS Architect certification course, I have become a big fan of him. Even though this course is not taught by him but Mattias Andersson, the quality remains the same. The course is actually a bundle of three-course — an Introduction to Google Cloud Platform, Google Certified Cloud Data Engineer course, and Kubernetes deep dive by Nigel Poulton, which is an important topic for Google Cloud certifications.
This means it's also an excellent course to learn about Google’s cloud offering for beginners or anyone who want to learn GCP. The course also includes 2 practice tests to prepare you will well for Google’s Associate Cloud Engineer certification exam.
Here is the link to join this Google Cloud course — Google Certified Associate Cloud Engineer Certification
This is one of the best but old introductory course on Google Cloud Platform or GCP for programmers and architects who want to move into the Google cloud. In this course, Google Developer Expert Lynn Langit introduces you to Google’s Cloud technologies and provides an overview of what is possible with Google Cloud. By the end of the course, you’ll know and understand essential Google cloud services like Google App Engine, Google Compute Engine, and more into your organization. No Google cloud knowledge is required, but a lot will be imparted. This course is aimed at developers and business decision-makers and is actionable for executives as well.
It also includes a ‘Hello World’ GAE demo using Eclipse (Java) which makes it ideal for Java developer who wants to learn about the Google Cloud Platform.
Here is the link to join this course — Introduction to Google Cloud By Lynn Langit
Btw, this course would require Pluralsight membership. If you already have a Pluralsight membership then this is a great introductory course about GCP. If you don’t have a membership then you can either subscribe, it cost around $29 per month or $299 per annum (currently just $179, 40% discount), or take this course for free by signing their 10-day free trial.
pluralsight.pxf.io
While this course is good for beginners, it’s a bit outdated given how fast technology is changing and Lynn Langit has also created GCP Essentials, GCP Enterprise Essentials, and GCP Machine Learning Essentials courses on LinkedIn Learning, you can also check them out. LinkedInLearning offers 1 month free so that’s good enough time to check these courses for FREE.
This is another excellent specialization from Coursera for Data Scientists who want to deploy their machine learning models on Google Cloud to take advantage of TensorFlow and the performance offered by GCP.  Like the first specialization, this one is also offered by Google Cloud which makes it a kind of official resource to learn Machine learning for Google Cloud Platform.  The Specialization is a collection of the following 5 courses — How Google does Machine Learning — Launching into Machine Learning — Intro to TensorFlow — Feature Engineering, and — Art and Science of Machine Learning All courses are 100% online which means you can learn on your own schedule. In general, it takes 1 month to complete this specialization given you spent around 15 hours/week but you can go at your convenience. The course is also available in English, French, Portuguese, Brazilian, German, Spanish, and Japanese. The online labs are provided by Qwiklabs which makes working with GCP really pleasant. You can start with whichever course you want but you need to finish all lectures and assignments a certification will be awarded to you which you can put on your LinkedIn profile or your CV.
Here is the link to join this GCP ML course — Machine Learning with TensorFlow on Google Cloud Platform
By the way, if you find Coursera courses and specialization useful then you should also join the Coursera Plus, a subscription plan from Coursera which provides you unlimited access to their most popular courses, specialization, professional certificate, and guided projects.
coursera.com
Another great course on Udemy to learn Google Cloud Platform for Beginners. It’s great to learn Google Cloud Platform from scratch. You will learn key GCP services with their pros and cons and learn when to use them in the real world.
Here is the link to join this course — Google Cloud Platform (GCP) Fundamentals for Beginners
udemy.com
If you have some knowledge about Cloud computing and you have used AWS before then learning Google Cloud Platform is not difficult for you and this course make it even easier.
Enterprises are going multi-cloud. It is NOT sufficient to know JUST one cloud anymore. You would need to understand multiple clouds.
In this course, you will learn Google Cloud by building on your AWS knowledge.
Instead of learning Google Cloud Platform the same way you learned AWS, you will learn GCP comparing GCP services with AWS services. You will learn Google Cloud Platform using AWS as the stepping stone.
By the end of the course, you will see that learning Google Cloud Platform — GCP is very easy when already know AWS!
Here is the link to join this course — Google Cloud Platform for AWS Professionals
udemy.com
That’s all about some of the best courses to learn Google Cloud Platform or GCP. I have also included some courses to prepare for Google Certified Associate Cloud Engineer Certification which is another great way to learn Google Cloud Platform and get a certificate for your skill. Something which you can put in your resume and LinkedIn profile.  This is not really a big list as I am also learning Google Cloud, so if you have a course that should be on this list or something I should take a look at, feel free to suggest in the comments.  Other Certification Resources for IT Professionals and Java Programmers
Thanks for reading this article so far. If you like this article then please share it with your friends and colleagues. If you have any questions or feedback, please drop a note. P. S. — If you are new to the world of Cloud and AWS and looking for some free courses to learn Amazon Web Service then you can also, check this list of Free AWS Courses for Beginners to start with.
medium.com
Medium’s largest Java publication, followed by 14630+ programmers. Follow to join our community.
268 
3
268 claps
268 
3
A humble place to learn Java and Programming better.
Written by
I am Java programmer, blogger, working on Java, J2EE, UNIX, FIX Protocol. I share Java tips on http://javarevisited.blogspot.com and http://java67.com
A humble place to learn Java and Programming better.
"
https://medium.com/bandprotocol/google-cloud-integration-with-band-protocol-oracles-and-deep-learning-for-real-time-crypto-price-e6e863070f74?source=search_post---------187,"There are currently no responses for this story.
Be the first to respond.
We are proud to announce that Band Protocol has integrated into Google Cloud Public Data to enable immediate and accurate analysis of financial time series data. With Band Protocol oracle data live on Google Cloud Public Data; traditional, hybrid blockchain and cloud applications can be built to leverage unique data feeds from decentralized oracle services.
At Band Protocol, our teams are empowering researchers and developers to use decentralized oracles for any external data source or type, regardless if the application is natively built on the blockchain or Web 2, through the flexible design of Band Protocol oracles.
console.cloud.google.com
Accurate and reliable price discovery mechanisms are crucial to all capital markets. Google Cloud Public Data has a wealth of data available for unique analysis. This is especially interesting for real-time financial time series data using novel approaches to Machine Learning. In particular, a Keras model implementing an LSTM neural network for anomaly detection is provided. For this article, we will refer to the Band Protocol public dataset available on Google BigQuery.
More interestingly, the data derived from the auto encoder-decoder could be considered a dataset in and of itself. This dataset can then be used to support the creation of a new decentralized oracle through custom oracle scripts on BandChain — further expanding the capabilities and offerings provided by Band Protocol.
On-chain smart contracts on any blockchains supported by Band oracles will then be able to in turn have access to pre-trained neural network and anomaly detection systems to perform complex business logic in a trustless manner without relying on other extra external parties.
Take a decentralized insurance protocol as an example, claims can be triggered and conditioned based on data anomaly detection while computation on-chain remains relatively minimal with little overhead. The idea can be further generalized to different types of software to enable smart contracts to delegate complex or expensive computations to GCP and thus creating hybrid cloud-blockchain applications.
The whole process can be broken down into two parts:
Check out Google Cloud’s article for the full breakdown of technical details:
medium.com
This article is based on the work of Reza Rokni. For more examples of time series data processing in Dataflow refer to this repository:
github.com
With Band Protocol oracles fully integrated into Google Cloud Public Data, this is the first of many use-cases we are exploring with partners to bridge traditional enterprises and blockchain applications. Our focus is to continuously and rapidly expand the support of data available on BandChain — pushing the use-cases far beyond just Web 3 alongside many enterprises.
About Google Cloud:Google Cloud provides organizations with leading infrastructure, platform capabilities and industry solutions. We deliver enterprise-grade cloud solutions that leverage Google’s cutting-edge technology to help companies operate more efficiently and adapt to changing needs, giving customers a foundation for the future. Customers in more than 150 countries turn to Google Cloud as their trusted partner to solve their most critical business problems.
https://cloud.google.com/
About Band ProtocolBand Protocol is a cross-chain data oracle platform that aggregates and connects real-world data and APIs to smart contracts. Band Protocol enables smart contract applications such as defi, prediction markets, and games to be built on-chain without relying on the single point of failure of a centralized oracle. Band Protocol is backed by a strong network of stakeholders including Sequoia Capital, one of the top venture capital firms in the world, and the leading cryptocurrency exchange, Binance.
Website | Whitepaper | Telegram | Medium | Twitter | Reddit | Github
Secure, Scalable, Cross-chain Decentralized Oracle for Web 3.0
626 
626 claps
626 
Secure, Scalable, Cross-chain Decentralized Oracle for Web 3.0
Written by
Business Development & Growth | Band Protocol | Start-Ups & Personal Development |
Secure, Scalable, Cross-chain Decentralized Oracle for Web 3.0
"
https://medium.com/@retomeier/an-annotated-history-of-googles-cloud-platform-90b90f948920?source=search_post---------188,"Sign in
There are currently no responses for this story.
Be the first to respond.
Reto Meier
Feb 11, 2017·8 min read
See Also: What are the Google Cloud Platform services?
Google Cloud Platform (GCP) has been growing rapidly as Google invests heavily in it development. Sadly, the Wikipedia entry for GCP is garbage, and it’s difficult to skim the blog as a timeline. For my own sake, I wrote this attempt at an objective timeline of significant events and launches for GCP.
Something missing? Add a comment and I’ll update the timeline.
Developer Advocate @ Google, software engineer, and author of “Professional Android” series from Wrox. All opinions are my own.
120 
4
120 
120 
4
Developer Advocate @ Google, software engineer, and author of “Professional Android” series from Wrox. All opinions are my own.
"
https://medium.com/google-cloud/planet-scale-microservices-with-cluster-federation-and-global-load-balancing-on-kubernetes-and-cd182f981653?source=search_post---------189,"There are currently no responses for this story.
Be the first to respond.
I recently gave a talk at Google Cloud Next about using Kubernetes to deploy and manage microservices around the world using Kubernetes.
I’ve blogged about how you can set up MongoDB in a StatefulSet already, so now I’m going to deep dive on how you can set up cluster federation and deploy your services around the world!
We are going to be looking at three things specifically, Cluster Federation, Federated Ingress, and cross cluster service discovery. I’m going to split these topics into three blog posts so look out for the next ones soon!
I’m going to assume you know the basics of creating a project in Google Cloud and have installed and set up the Google Cloud SDK. If not, please check out my previous blog posts.
Cluster Federation is what you can use to manage multiple Kubernetes clusters as if they were one. This means you can make clusters in multiple datacenters (and multiple clouds), and use federation to control them all at once!
Federated Ingress is super sweet, and is currently only supported on Google Cloud. This allows you to spin up a single load balancer with a single global IP address that will dynamically send traffic to the nearest cluster automatically!
Cross cluster service discovery is a concept that let’s services in one Kubernetes cluster find services in other clusters automatically. We will also look at Google Cloud Pub/Sub to enable asynchronous microservices!
Now you have a federated cluster, what can you do with it? One cool thing you can do is Federated Ingress. Let’s dive deeper into how it works.
One of the most powerful components on Google Cloud Platform is the network. This is something people commonly forget about when looking at the Cloud. Google has built one of the largest private networks in the world, which allows us to do some amazing things.
When you spin up a HTTP(S) Load Balancer on Google Cloud, you are given a single IP address. This IP address is anycast from over 100 points of presence around the world, and users automatically connect to the nearest one. Their traffic is then sent over Google’s private backbone network to the closest datacenter hosting your application!
This is all automatic. There is no DNS trickery, pre-warming, or anything complicated involved. Just one IP address. The best part is, Federated Ingress allows Kubernetes to take advantage of this without needing to perform any manual setup!
Note: If you haven’t completed Part 1, please do that first. Otherwise, make sure you know what you are doing!
The first step is to create a global static IP address for our ingress to use.
This creates a new global IP address called “k-ingress”
Before you can create the ingress, you need to create a deployment and service to back it.
Let’s create a simple Nginx deployment and scale it to have 4 replicas:
This command will create a deployment with 4 nginx replicas in the federated context. Because we have two clusters, each cluster will get 2 replicas! The federation control plane automatically distributes the pods evenly through the clusters.
Now create a service to expose this deployment:
In YAML form, the service definition looks like this:
This will create a service exposing our nginx deployment via NodePort.
You might be used to exposing a service using a LoadBalancer, but that will create a load balancer in each datacenter with its own IP address, which we don’t want.
Instead, the NodePort directly exposes the service on all the VMs in the cluster (in this case on port 30036, but you can choose any valid port). The ingress load balancer will then route traffic to this port on the VMs.
The final step is to create the ingress load balancer!
Optional: I would recommend creating a firewall rule that will let you send traffic from the load balancer to the VMs. While this is not strictly necessary, as Kubernetes will create it’s own rules, it can help prevent issues with firewall rules. Run this command to create the rule:
Save the following to a file called ingress.yaml
And create the ingress object:
The annotation in the metadata ensures that the load balancer gets created using the static IP address you created earlier, and ensures that there is only one global load balancer created. The rest looks like a standard ingress YAML. We want traffic from all paths to go to the nginx service on port 80. You can customize this and use all the normal features ingress provides, such as routing, session affinity, etc, but now it’s federated!
If you want HTTPs support, I would highly recommend using kube-lego, which will automatically add Let’s Encrypt certificates to your ingress load balancer.
The ingress load balancer will take a few minutes to spin up and make sure all the backend health checks are up and running and firewall rules are created.
If you visit your Load Balancer section in the Google Cloud console, you should see a HTTP(s) load balancer created for you. Here I’ve expanded the details on the load balancer:
You can see that there are two instance groups that are backing this load balancer, one in us-east and one in us-west. These are the two Kubernetes clusters. You can also see that all three VMs in each cluster are healthy, which means the load balancer is up and running!
To get the IP address of the service, run:
kubectl — context=kfed get ingress
When you visit the IP address, you should see the hello world from Nginx!
Depending on where you are located, your traffic will be routed from the closest datacenter to you. You can see an example of this in my demo on YouTube.
Though we only used two clusters in this demo, the ingress load balancer can easily scale to many more clusters. You can create huge clusters in each Google Cloud data center, and a single ingress load balancer can distribute your HTTP(s) traffic between all of them automatically. Two federated clusters or 20, all the steps are the same!
Of course, the anycast magic used by Federated Ingress requires you to run all your clusters on Google Cloud Platform. Currently, there is no support for other environments. If you are running a hybrid cloud, make sure your web frontends backed by the ingress load balancer all run on Google Cloud Platform.
In Part 3, I’ll show you how you can use cross cluster service discovery to access services that are not present in local clusters, which is perfect for these hybrid deployments!
Google Cloud community articles and blogs
282 
12
282 claps
282 
12
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.datadriveninvestor.com/how-to-prepare-for-google-cloud-certified-professional-data-engineer-exam-and-pass-it-on-your-a7bdef8aa8d0?source=search_post---------190,"Thinking about new year’s resolutions? How about adding Google Cloud certificate to your career portfolio? Data engineering on a cloud platform is a crucial step in machine learning pipeline nowadays. If you’re wondering how to be better prepared for Google Cloud Professional Data Engineer certification exam and pass it on your first try, this article is here to help. And I would share one of my personal notes for preparation at the end of the article.
"
https://medium.com/zrealm-ios-dev/%E4%BD%BF%E7%94%A8-python-google-cloud-platform-line-bot-%E8%87%AA%E5%8B%95%E5%9F%B7%E8%A1%8C%E4%BE%8B%E8%A1%8C%E7%91%A3%E4%BA%8B-70a1409b149a?source=search_post---------191,"There are currently no responses for this story.
Be the first to respond.
以簽到獎勵 APP 為例，打造每日自動簽到腳本
一直以來都有使用 Python 做小工具的習慣；有做正經的，工作上自動爬數據、產報表，也有不正經的，排程自動查想要的資訊或是交給腳本完成本來要手動執行的動作。
一直以來「自動」這件事，我都很粗暴直接開一台電腦掛著 Python 腳本讓他掛著跑；優點是簡單方便，缺點是要有台設備接著網路接著電；就算是樹莓派也是要消耗著微量的電費網路錢，還有也不能遠端控制啟動或關閉（其實可以，但很麻煩）；這次趁著工作空擋，研究了一下免費&上雲端的方法。
將 Python 腳本搬到雲端執行、定時自動執行、可透過網路開啟/關閉。
本篇以我耍的小聰明，針對簽到獎勵型 APP 撰寫的自動完成簽到的腳本為例，能每日自動幫我簽到，我不用在特別打開 APP 使用；並在執行完成後發通知給我。
之前有發過一篇「APP有用HTTPS傳輸，但資料還是被偷了。」的文章，道理類似，不過這次改用 Proxyman 取代 mitmproxy；同樣免費，但更好用。
「Certificate 」->「 Install Certificate On this Mac」->「Installed & Trusted」
電腦的 Root 憑證裝好後換手機的：
「Certificate 」->「 Install Certificate On iOS」->「Physical Devices…」
依照指示在手機上掛好 Proxy 並完成憑證安裝及啟用。
這時候 Mac 上的 Proxyman 就會出現嗅探到的流量，點擊裝置 IP 下想要查看的 APP API 網域；第一次查看需要先點「Enable only this domain」之後的流量才能被解包出來。
「Enable only this domain」後就能看到新攔截的流量就會出現原始的 Request、Response 資訊：
我們使用此方法嗅探 APP 上操作簽到時打了哪隻 API EndPoint 及帶了哪些資料，將這些資訊記錄下來，等下使用 Python 直接模擬請求。
⚠️要注意有的 APP token 資訊可能會換，導致日後 Python 模擬請求失效，還要多了解 APP token 交換的方式。
⚠️如果確定 Proxyman 有正常運作，但在掛 Proxyman 的情況下 APP 無法發出請求，代表 APP 可能有做 SSL Pining；目前無解，只能放棄。
⚠️APP 開發者想知道怎麼防範嗅探可參考之前的文章。
在撰寫 Python 腳本之前，我們可先使用 Postman 調試一下參數，觀察看看哪個參數是必要的或是有時效會改變；但要直接照搬也可以。
⚠️main(args) 這邊的 args 用途後面會講，如果要在本地測試直接帶 main(True) 就好。
使用 Requests 套件幫我們執行 HTTP Request，如果出現：
請先使用 pip install requests 安裝套件。
這部分我做的很簡單，僅共參考，僅通知自己。
下一步填好基本訊息後按「Create」送出建立。
有了 User Id 跟 Token 之後我們就能發訊息給自己了。因沒有要做其他功能所以連 python line sdk 都不用裝，直接打 http 發。
串上之前的 Python 腳本後…
測看看通知有沒有發成功：
Success!
小插曲，通知部分我本來是想用 Gmail SMTP 用信件來發，結果上到 Google Cloud 後發現無法使用…
前面基本的講完了，正式進入本篇重頭戲；將 Python 腳本搬上雲端。
這部分我一開始向中的是 Google Cloud Run 但用了下覺得太複雜，我實際懶得研究，因為我的需求太小用不到這麼多功能；所以我用的是 Google Cloud Function serverless 方案；實際上比較常用來做的是構建 serverless web 服務。
⚠️記下「觸發網址」
區域可選：
⚠️建立 Cloud Functions 時會需要 Cloud Storage 寄存程式碼。⚠️詳細計價方式請參考文末。
觸發條件選：HTTP
驗證：依需求，我希望我能從外部點連結執行腳本，所以選擇「允許未經驗證的叫用」；如果選擇需要驗證，後續 Scheduler 服務也要做相應設定。
變數、網路及進階設定可在變數中設定變數給 Python 使用（這樣參數有變動就不用改到 Python 程式碼）：
在 Python 中調用的方式：
其他設定都不需要動，直接「儲存」->「下一步」。
補充 main(args)，同前述，此項服務比較是用來做 serverless web；所以 args 實際是 Request 物件，你能從其中拿到 http get query 及 http post body 資料，具體方式如下：
example: ?name=zhgchgli => request_args = [“name”:”zhgchgli”]
example: name=zhgchgli => request_json = [“name”:”zhgchgli”]
如果使用 Postman 測試 POST 記得使用「Raw+JSON」POST 資料，否則不會有東西：
我們使用「request」這個套件幫我們打 API，此套件不在原生 Python 庫裡面；所以我們要在這裡加上去：
這邊指定版本 ≥ 2.25.1，也可不指定只輸入 requests 安裝最新版。
需要花約 1~3 分鐘的時間等他部署完成。
如果出現 500 Internal Server Error 則代表程式有錯，可點擊名稱進入查看「紀錄」，在其中找到原因：
測試沒問題就完成了！我們已經順利將 Python 腳本搬上雲端。
依照我們的需求，我們需要能有個地方存放、讀取簽到 APP 的 token；因為 token 可能會失效；需要重新要求並寫入共下次執行時使用。
想要從外部動態傳入變數到腳本中有以下方法：
在程式中使用相對路徑 ./ 就能讀取到，僅限讀取無法動態修改；要修改只能在控制台這修改＆重新部署。
想要可以讀取、動態修改就需要串接其他 GCP 服務，例如：Cloud SQL、Google Storage、Firebase Cloud Firestore…
按照入門步驟，建立好 Firebase 專案後；進入 Firebase 後台：
在左方選單列找到「Cloud Firestore」->「新增集合」
輸入集合 ID。
輸入資料內容。
一個集合可以有多個文件，每個文件可以有各自的欄位內容；使用上非常彈性。
在 Python 中使用：
請先到 GCP控制台 -> IAM與管理 -> 服務帳戶，按照以下步驟下載身份驗證私鑰文件：
首先選擇帳號：
下方「新增金鑰」->「建立新的金鑰」
選擇「JSON」下載檔案。
將此 JSON 檔案放到同 Python 的專案目錄下。
本地開發環境下：
安裝 firebase-admin 套件。
在 Cloud Functions 上要在 requirements.txt 中多加入 firebase-admin。
環境弄好後，可以來讀取我們剛剛新增的數據了：
如果是在 Cloud Functions 上除了可以把 身份驗證 JSON 檔一起上傳上去，也可以在使用時將連接語法改成以下使用：
如果出現 Failed to initialize a certificate credential. ，請檢查身份驗證 JSON 是否正確。
新增、刪除更多操作請參考官方文件。
有了腳本之後再來是要讓他自動執行才能達到我們的最終目標。
執行頻率：同 crontab 輸入方式，如果你對 crontab 語法不熟，可以直接使用crontab.guru 這個神器網站：
他能很直白的翻譯給你所設定的語法實際意思。（點 next 可查看下次執行時間）
這邊我設定 15 1 * * * ，因為簽到每天只需要執行一次，設在每日凌晨 1:15 執行。
網址部分：輸入前面記下的「觸發網址」
時區：輸入「台灣」，選擇台北標準時間
HTTP 方法：照前面 Python 程式碼我們用 Get 就好
如果前面有設「驗證」記得展開「SHOW MORE」進行驗證設定。
都填好後，按下「建立」。
⚠️請注意，執行結果「失敗」僅針對 web status code 是 400~500 或 python 程式有錯誤。
我們已達成將例行任務 Python 腳本上傳到雲端＆設定自動排成自動執行的目標。
還有一部分很重要，就是計價方式；Google Cloud、Linebot 都不是全免費服務，所以了解收費方式很重要；不然為了一個小小的腳本，付出太多的金錢那不如電腦開著掛著跑哩。
參考官方定價資訊，一個月 500 則內免費。
參考官方定價資訊，每月有 200 萬次叫用、400,000 GB/秒和 200,000 GHz/秒的運算時間、 5 GB 的網際網路輸出流量。
參考官方定價資訊，有 1 GB 大小容量、每月 10 GB 流量、每天 50,000 次讀取、20,000 次寫入/刪除；輕量使用很夠用了！
參考官方定價資訊，每個帳號有 3 項免費工作可設定。
對腳本來說以上免費用量就綽綽有餘啦！
東躲西躲，還是躲不掉可能被收費的服務。
Cloud Functions 建立好之後會自動建立兩個 Cloud Storage 實體：
如果剛剛 Cloud Functions 選擇的是 US-WEST1、US-CENTRAL1 或 US-EAST1 這三個地區則可享有免費使用額度：
我是選擇 US-CENTRAL1 沒錯，可以看到第一個 Cloud Storage 實體的地區是 US-CENTRAL1 沒錯，但第二個是寫 美國多個地區 ；我自已估計這項是會被收費的。
參考官方定價資訊，依照主機地區不同有不同的價格。
程式碼沒多大，估計應該就是每個月最低收費 0.0X0 元（？
⚠️以上資訊均為 2021/02/21 時撰寫時紀錄，實際以當前價格為主，僅共參考。
just in case…假設真的有狀況超出免費用量開始計價，我希望能收到通知；避免可能程式錯誤暴衝造成帳單金額報表卻渾然不知。。。
點擊「查看詳細扣款紀錄」進入。
下一步。
下一步。
動作這邊可以設定當預算達到多少百分比時會觸發通知。
勾選「透過電子郵件將快訊傳送給帳單管理員和使用者」，這樣當條件處發時就能第一時間收到通知。
點擊「完成」送出儲存。
當預算超過時我們就能馬上就能知道，避免產生更多費用。
人的精力是有限的，現今科技資訊洪流，每個平台每個服務都想要榨取我們有限的精力；如果能透過一些自動化腳本分擔我們的日常生活，聚沙成塔，讓我們省下更多精力專心在重要的事情之上！
有任何問題及指教歡迎與我聯絡。
有自動化相關優化需求也歡迎發案給我，謝謝。
「解決問題的道路上你並不孤單」整理開發上遇到問題的解決方法跟一些新鮮事。
283 
283 claps
283 
「解決問題的道路上你並不孤單」整理開發上遇到問題的解決方法跟一些新鮮事。
Written by
專職IT狗，寫了多年網頁，因緣際會踏入開發 iOS APP，求知若渴、教學相長；更愛電影、美劇、西音、運動、生活． www.zhgchg.li
「解決問題的道路上你並不孤單」整理開發上遇到問題的解決方法跟一些新鮮事。
"
https://rominirani.com/google-cloud-functions-tutorial-writing-background-functions-e651f27ddde5?source=search_post---------192,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
This is part of a Google Cloud Functions Tutorial Series. Check out the series for all the articles.
In an earlier part of the series, we covered writing Foreground Functions that were based on HTTP Triggers. In this post, we are going to look at writing background functions that can be supported by events raised by two specific event providers that are supported in Google Cloud Functions: Cloud Pub/Sub and Google Cloud Storage.
This post will not be a tutorial for Cloud Pub/Sub and Google Cloud Storage. Please refer to their respective documentation on the same.
First up, it is important to understand a few points about Background functions:
The Background Cloud Functions will be triggered as a result of the message being published to a Pub/Sub Topic or file changes in specific Google Cloud Storage buckets.
You have a choice of using Node.js 6 or Node.js 8 runtimes with Google Cloud Functions. The sections below will indicate the key differences in terms of method signatures and other points. Note that Node.js 8 brings new features to the table like async and await and so it completely depends which runtime you want to go with. Keep in mind that at the time of writing Node.js 8 is in Beta.
Node.js 6
The function signature of Background Cloud Functions will take 2 parameters:
A skeletal outline for your Background function will look like this:
The event object will contain the following properties, which can be of interest in your function logic. This is taken from the official documentation:
The event.data property will have a specific schema depending on whether the event is from the Cloud Pub/Sub event or from Google Cloud Storage event.
Node.js 8
The function signature of Background Cloud Functions will take 2 parameters:
A skeletal outline for your Background function will look like this:
Compared to the function signature for Node.js 6 where you had to extract both the data and context from the event object, in Node.js 8 function signature, the first parameter is the data object, the 2nd parameter is context , which is provided as a separate parameter as shown above.
We will cover more on this in the respective sections for two Event Provider triggered functions.
Let us take a look at writing our first Cloud Function based on the Cloud Pub/Sub Event Provider. A skeletal template for the same (taken from the default code that appears) is shown below. We are covering this for Node.js 6 version for now.
Note that the event object that we mentioned has the data property that will contain the message that was published to the Pub/Sub Topic. This message is encoded in base64 format, so you will need to decode the same in your code. Similarly, take a look at the other attributes that you might be interested in your function logic.
Let us go ahead and write a Google Cloud Function based on Pub/Sub now.
The first step is to be logged into the Cloud Console with your Cloud Project selected. Click on Cloud Functions from the main menu and click on Create Function. This will lead to the Cloud Function creation dialog as shown below:
Let us look at the form parameters:
On successful function creation, you will be led to the function screen as shown below:
Once you click on the function, this will lead to the Function details screen. Click on the Trigger tab as shown below. Notice the Trigger type and Pub/Sub topic name.
Click on Source tab. This will lead to the source for your Cloud Function as shown below:
index.js
The code is straightforward:
Let us test our function now. Click on the Testing tab and enter the sample data as shown below. Keep in mind that we are base64 encoding our data to simulate how the message will be published to the Pub/Sub topic.
Click on the Test the function button. This will invoke our Cloud function and you can see that in the logs below.
But keep in mind that we simply used the testing message to simulate a message that was being published to the Pub/Sub topic. How about actually using Pub/Sub in Cloud Console and publish the message to the topic from there. That should then invoke our function because it has a subscription to any message published on that topic. Let’s do that.
Click on Pub/Sub from the Cloud Console main menu. In the list of topics, you should find the topic that we created while creating the Cloud Function. You will also notice that there is already a subscription present for our Topic, which means that the messages that are published to this topic will be sent to the subscribers to that topic. And it is no surprise that the subscription is one of our Cloud Function, which will get invoked when a message is published to that topic.
Click on the ellipsis at the end and it will bring up a list of options as shown below. Select Publish message since we want to publish a message to this topic.
In the Publish message form, enter the message that you want to send and click on Publish button.
This will publish the message to the topic. This in turn should invoke our Cloud Function. Since we are logging the message that is received via the console.log statements in our Cloud Function code, we can visit the Stackdriver Logging and we find that our function did get invoked successfully.
This completes the task of writing our first Cloud Function based on Pub/Sub Event Provider.
Node.js 8
So far we used the Node.js 6 runtime. In case we wanted to use the Node.js 8 runtime, we would do the following:
Let us take a look at writing our first Cloud Function based on the Cloud Storage Provider. A skeletal template for the same (taken from the default code that appears) is shown below. We are covering this for Node.js 6 version for now.
Note that the event object that we mentioned has the data property that will contain information on the Google Cloud Storage Event. It contains information on Bucket name, Content-type and other metadata too. Similarly, take a look at the other attributes that you might be interested in your function logic.
Do note that always call the callback() function or return a Promise from your function to indicate that your function processing is complete.
Let us go ahead and write a Google Cloud Function based on Cloud Storage now.
The first step is to be logged into the Cloud Console with your Cloud Project selected. Click on Cloud Functions from the main menu and click on Create Function. This will lead to the Cloud Function creation dialog as shown below:
Let us look at the form parameters:
Ensure that the function name that you export is helloGCSGeneric.
On successful function creation, you will be led to the function screen as shown below:
Click on the function to view the function details (Trigger tab) as shown below. Note the Trigger type and our Bucket name that we specified.
Let us test the function now via Google Cloud Storage service in the Cloud Console. Navigate to Cloud Storage in the Cloud Console. You should see a screen as shown below:
Click on the specific bucket that we created. This will lead you to a screen as shown below:
Go ahead and click on Upload Files button. Select any file from your local machine and upload it.
Once the file is successfully uploaded, we are tracking for all changes that are happening in the gcs-function-bucket1, so our function should get invoked and we should see the required output in the Stackdriver Logging service as shown below:
This completes our task of writing a Google Cloud Function based on Google Cloud Storage.
Node.js 8
So far we used the Node.js 6 runtime. In case we wanted to use the Node.js 8 runtime, we would do the following:
As a best practice, always remember to invoke the callback from your function. This will indicate that your function processing is complete and the service can calculate the time that it has taken your function to run. If you do not invoke the callback, your function will be considered as running till it times out, which is not something that you want to do.
There are multiple ways in you can invoke the callback. They are listed below:
You don’t have to always specify a callback() to denote function completion. You could return a Promise too or alternately return a discrete value from your function or throw an Error object from your code. Cloud Functions will handle that as a callback for you. In this case, do remember to omit the callback parameter from the function signature.
Here is an example of throwing an Error object from your code or returning a discrete value. Both of these techniques are valid ways to indicate function completion.
A final way of not using the callback is to wrap your function using the async keyword (which causes your function to implicitly return a Promise).
This completes our section on writing Background Cloud Functions.
Proceed to the next part : Monitoring Cloud Functions or go back to the Google Cloud Functions Tutorial Home Page.
Technical Tutorials, APIs, Cloud, Books and more.
231 
3
231 claps
231 
3
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
Technical Tutorials, APIs, Cloud, Books and more.
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
Technical Tutorials, APIs, Cloud, Books and more.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@sathishvj/notes-from-my-google-cloud-professional-collaboration-engineer-certification-exam-5aaaaa2a4b19?source=search_post---------193,"Sign in
There are currently no responses for this story.
Be the first to respond.
sathish vj
Jan 6, 2020·11 min read
Subscribe to my YouTube channel that teaches you to apply Google Cloud to your projects and also prepare for the certifications: youtube.com/AwesomeGCP. Check out the playlists I currently have for Associate Cloud Engineer, Professional Architect, Professional Data Engineer, Professional Cloud Developer, Professional Cloud DevOps Engineer, Professional Cloud Network Engineer, and Professional Cloud Security Engineer.
I’ve been through a bunch of these exams and the experience never changes. First, there is no end to how much you can study. The material could go on and on, and you’d still never feel fully prepared. Having said that, the entire scope of the Professional Collaboration Engineer exam is not too wide+deep as compared to the Professional Architect or Professional Data Engineer. But there are lots of details, and at least some is going to catch you off guard during the exam. Second, given that there is no end to studying, one has to take a leap of faith at some point. So I did and came out with another certification. Luckily, I think.
I had also received a 50% off coupon to finish a Professional Google Cloud Certification before the end of the year and the Collaboration Engineer was the only one left. That matched my personal plan also to try and do all the available Google Cloud certifications this year.
I had started doing the Coursera specialization for the Collaboration engineer a few months ago, but other work/travel came up and I got pulled away into that. I’d also started the Coursera course on Cloud Identity. I thought I had finished that, but realized a couple of days before the exam that I hadn’t. I went through the last of that just before the exam. Since I would have definitely forgotten a good part of the G Suite Admin specialization, I paid for it again so that I could revise. Both those courses were well worth it. The questions in the exam were mostly from within those two course — G Suite Admin Specialization and Cloud Identity. Some parts were not though — like Chrome device management and Hangouts device management. As with other GCP exam preparations, I tried to spend maximum time in the docs — that’s where I have always found the most comprehensive material. Unlike many of the other Google Cloud courses on Coursera, these courses had less videos, more text, and links into the docs. I found this great to quickly revise.
Coursera — Introduction to Cloud Identity
Coursera — G Suite Administration Specialization
I’m also collecting material for this exam in my GitHub repo: https://github.com/sathishvj/awesome-gcp-certifications/blob/master/professional-collaboration-engineer.md. (I encourage you to contribute to it.)
This was as was to be expected. 2 hours, about 50 questions, all multiple choice, no case studies, almost all questions were short scenarios. I approach each question slowly, often reading and re-reading the question and options. I finished the exam with about 15 minutes to spare and reviewed the questions for the remaining time. I probably changed one or two answers during the review process. On submitting it at the end, I got a provisional PASS result.
Google Cloud encompasses a wide array of solutions today. This includes G Suite with its offerings like Mail, Calendar, Slides, Drive, Hangouts, etc. There is also the provisioning and management of devices like Chromebook, mobile phones, teleconference equipment, etc. These resources are typically accessed via user identities, either using Google as the Identity Provider (IdP) or via third party IdPs. Managing all of this usually comes under the purview of the IT department. The Collaboration Engineer validates your ability to administer users, G Suite Services, devices, and also migrate from/to alternate office solutions.
This would not be a necessary exam for you if your prime focus is architecting software solutions, planning networking, or data engineering. There is, of course, some overlap in terms of user management and IT security, but that is covered in other certifications.
If you are only a user of G Suite services, then there is the G Suite certification which I’ve covered here.
Personally, I did not find the exam too tough. Most of the G Suite options are logical. I have worked with G Suite for a few years now and managed my own G Suite accounts. So I do have some general knowledge of the subject but I’m not by any stretch of imagination an IT administrator. So it was difficult for me only because of minimum exposure to it via my work. I was, however, able to make up for it by doing the courses and studying the docs.
* Reports — It is to be expected that management and legal will expect to see multiple kinds of reports. How do you generate these reports? From where? What level of detail do you have to provide? Is it ready made or would you have to take it from somewhere and fix it up?* Audits — what actions and events are logged? What are the different types of audits? * Audits — If you had a particular type of event happening, say a data breach or an external attack, where should you be looking for investigation data?* Apps — Scheduled and Rapid Release — how quickly does G Suite features reach your users? How do you test new features?* 2SV — What are the ways in which you can secure user access? Which is better under what circumstances? Should you be using authenticator, security keys, disabling passwords, resetting sessions?* Apps — Configuring SSO and SAML for apps — you’ll have 3rd party apps that you want to use in your organization. How do you verify if they are safe? How do you limit what they can do? How do you enable only them for install by your company’s employees?* Apps — Settings for apps in own domain — is there a way you can give default access to apps written by your own employees?* Apps — Access scopes for external apps — maybe you don’t want to give full access to all apps. Ideally, follow the least privilege rule for apps and users.* Apps — Domain wide delegation — or maybe you do want to give domain wide access to apps. How do you set that up? What are the best practices here?* Users — deletion/suspension — what happens to data of suspended and deleted users?* Users — Data of deleted/suspended users — how to get information on users who are active, suspended, deleted?* Apps — When should you use apps script vs 3rd party tools? Which APIs should be enabled for this?* GCDS, LDAP, Active Directory and syncing data — you will often have to work with different organizations who have a different user management system. How do you integrate the two? This might also be the situation when users are being migrated, say after a company got acquired.* GCDS — What happens when GCDS sync fails? These systems might not work perfectly the first time you try it? How should you investigate failures and correct it? Manually using some process or automatically using some configuration changes?* GCDS — know how the different objects are mapped between Active Directory and G Suite. They don’t all have the same naming convention. * GCDS — which objects are synced always? Which are never synced? And which are synced only if chosen?* GCDS — which way are changes synced? Does G Suite allow changes to user info, passwords, etc.?* GCDS — how do you work with shared contacts and personal contacts?* Organizational Units — Inheritance of OUs. In a hierarchy of OUs, what are the permissions of an OU lower in the tree? Can it override permissions set above it?* Access Groups and OUs — Where are OUs used? Where is an Access Group a better option?* Groups — What are all the different types of groups? Which are used when?* Groups — What is a Collaborative inbox? What is the use case for a collaborative inbox?* Groups — What is a web forum? What is the use case for it?* Mail — SPF, DKIM, DMARC. What are these technologies? How are they useful? How do they make email secure?* Mail — SPF, DKIM, DMARC — how do you set it? Where do you set it? What are the configuration parameters and what do they indicate?* Mail — why has email not reached? why is email not seen? How to analyze it using email logs?* GSuite Toolbox — what is this? How do you use this to investigate issues in your G Suite setup?* Mail — interaction between different email servers. You might have a situation where you have to manage multiple mail servers, probably from different vendors and not just G Suite. How do you get them to interact correctly. Where should mail go first? Who should forward to whom?* Mail — email whitelist and blocked list. You want to be able to let through some mail and block other types.* Security — What are phishing attacks? How to identify them? How to audit and get reports?* Mail — split and dual delivery. For what use cases are they used?* Mail — Forwarding email. Admin configuration vs user configuration. * Mail — what is a recipient map? Where would you use it? * Mail — how do you ensure consistent company footers and other organisational settings for all email? Or maybe only for external mail?* Mail — how do you configure routes?* Mail — which routing settings can you set at an OU level and which at the organization level?* Mails — SMTP relay, where would you use it and what are the pre-conditions.* Calendar — Shared Calendar and Calendar resources. Can you have resources attached to only a specific calendar?* Calendar — how do you sync calendar info when migrating to G Suite.* Drive — Team Drive. How do you share information among teams?* Drive — Sharing. What happens when employees are terminated? Who owns their files? How to transfer them? Until how late after the user has been terminated can you share the data?* Vault — retention rules, searching for data, auditing, reports, exporting.* Vault — creating a matter and sharing it.* Security — DLP, and what does it solve. What are some of the pre-built rules?* Security — Methods to manage data exfiltration.* Devices — How to push wifi connection info to devices?* Security — Has there been a data leak? How could you figure it out? And if yes, what to do about it?* Releases — How to stay updated on G Suite info? There is the Release Calendar and the G Suite Blogs. Which is useful for what kind of info? Cloud Connect Community also has forums that share some information.* Users — what are onflicting accounts? https://support.google.com/a/answer/7062710* Users — how to resolve conflicting accounts with the transfer tool? https://gsuiteupdates.googleblog.com/2017/02/resolve-conflicting-accounts-with-new.html * Groups — what are the access settings for groups? Open to all? Restricted to a few?* Drive/Security/Reports — get reports about files shared externally, internally, when permissions were changed, etc.* Mail — troubleshoot mail headers. What is the direction of mail flow — bottom to top or top to bottom? * Mail — in what circumstances are mail headers useful and when not? Sometimes examining mail headers give you no worthwhile info and sometimes they will.* Mail — Configuring Inbound and Outbound Gateways. When do you use them as opposed to other routing settings?* Mail — Gateways — where do you configure them?* Mail — Gateways — in a mail header, how do you figure out which were the gateways?* Mail — How many copies of a mail are there when you do dual, split delivery, message forwarding, recipient map, etc.* Mail — Delegating access to somebody else’s email box.* Roles — how do you create custom roles?* Vault — Recovering deleted data from mail, drive, etc. Know how long data is retained when it is deleted. Where is it retained? What settings do you need for it to be retained?* Mail — What is an SMTP envelope?* Mail — Learn how to decode email headers at least to the extent of understanding mail delivery failure, which gateways’ SPF/DKIM/DMARC records are checked, etc.* Device — enrolment permissions and controls. (https://support.google.com/chrome/a/answer/2657289#device_enroll_permission)* Audits — differences between the various logs/audits — admin, saml, email, login.* Security — dashboard — what information can you get from here?* Reports — Aggregate reports — what info does it show you?* Reports — Where to find the reports? Via gmail, via security center, via reports?* Reports — Using BigQuery in combination with audit data and reports.* What is a HAR file? What is it used for?* Device — what approaches to updating Chromebook? Should you do it all together? Scatter it randomly?* Device — what kind of devices can you administer remotely?* Device — can you automatically give network connection information without sh* Apps Script — no code, but know basics of how this works and what you can do with it. You don’t have to do programming in it, but know what it is capable of.* AppsScript — Are there easier ways though than writing your own scripts? Know the tools that are available. A best practice is to use Google recommended/built tools than rolling your own.* AppMaker — know what this is and where it might be used. But doing exercises on it isn’t necessary. * Apps — what are the parts of a SAML configuration? What do you need to provide if you are a developer of the app and what if you are a user?* Alerts — how can you set alerts to notify you on certain events?* Hangouts — How do you setup and configure Hangouts hardware?* Hangouts/Calendar — what are the ways you can configure hangouts via calendar?* Hangouts/Calendar — can you give exclusive access to some users to certain resources?* Mail — what are consumer gmail accounts? How do you integrate them into G Suite enterprise accounts?* Mail — what happens to external accounts created using the same mail id on services like AdWords, Analytics, Twitter, etc.?* Mail — what are conflict accounts? How do you resolve them? How do you use the transfer tool?
Google Cloud Certified Professional Collaboration Engineer
For those appearing for the various certification exams, here is a list of sanitized notes (no direct question, only general topics) about the exam.
Overall notes across all GCP certification exams
Notes from the Professional Cloud Architect exam
Notes from the beta Professional Cloud Developer exam
Notes from the Professional Data Engineer exam
Notes from the Associate Cloud Engineer exam
Notes from the beta Professional Cloud Network Engineer Exam
Notes from the beta Professional Cloud Security Engineer Exam
Notes from the Professional Collaboration Engineer Exam
Notes from the G Suite Exam
Notes from the Professional DevOps Engineer Exam
Notes from the Professional Machine Learning Engineer Exam
Main Link — https://cloud.google.com/certification/collaboration-engineer/
Topics Outline-https://cloud.google.com/certification/guides/collaboration-engineer/
Practice Exam-https://cloud.google.com/certification/practice-exam/collaboration-engineer
A collection of posts, videos, courses, qwiklabs, and other exam details for all exams: https://github.com/sathishvj/awesome-gcp-certifications
I’ve collected here a bunch of free Qwiklabs codes which are awesome to get lots of hands-on practice. Use them well.
medium.com
Check the FAQs here: https://medium.com/@sathishvj/frequently-asked-follow-up-questions-on-google-cloud-gcp-certifications-438e1addb91d.
Wish you the very best with your GCP certifications. You can reach me at LinkedIn and Twitter. If you can support my work creating videos on my YouTube channel AwesomeGCP, you can do so on Patreon or BuyMeACoffee.
tech architect, tutor, investor | GCP 12x certified | youtube/AwesomeGCP | Google Developer Expert | Go
268 
1
268 
268 
1
tech architect, tutor, investor | GCP 12x certified | youtube/AwesomeGCP | Google Developer Expert | Go
"
https://medium.com/net-core/using-google-cloud-storage-in-asp-net-core-74f9c5ee55f5?source=search_post---------194,"There are currently no responses for this story.
Be the first to respond.
In this post, I will show how to use Google Cloud Storage (GCS) to store and serve images of an ASP.NET Core web application.
The sections of this post will be as follows:
The application that we will modify manages a database of TV Shows and its main page looks like below:
The app is an ASP.NET Core MVC web application and it uses EF Core to perform CRUD operations on a SQL Server database. You can download the source code from this Github repository.
If you are interested in the development process of this application, you can check this post.
In the current version, the image of the TV show needs to exist in the wwwroot/images folder of the project and is entered as shown below:
In the modified version, the user will select the image file to be uploaded and the file will be uploaded to Google Cloud Storage in the create action.
Let’s start by performing the necessary actions in the Google Cloud Platform (GCP).
If you don’t have a Google Cloud account, go to Google Cloud web site and click Get started for free button. Sign up with your Gmail account and get $300 (for free) to spend on Google Cloud Platform over the next 12 months.
In this section, we will do the necessary operations to use Google Cloud Storage service of the GCP for our application.
Cloud Storage provides worldwide, highly durable object storage that scales to exabytes of data. You can access data instantly from any storage class, integrate storage into your applications with a single unified API, and easily optimize price and performance.
Cloud Storage is typically used to store unstructured data.
First, we will create a new GCP project in the Google Cloud Console. (You can use an existing project too). Select the dropdown box shown in the red box and then click New Project.
Give your project a name and then click Create.
Next, we will create a bucket to store our images.
Buckets are the basic containers that hold your data in Cloud Storage.
Write Storage in the search box and then click Storage.
Click Create Bucket on the next screen. Then,
After the creation is completed, we will see our bucket as follows:
Next, we will make the bucket publicly readable so it can serve files:
Now, we will set up a service account and download credentials to run our application locally.
Enter APIs & Services in the search box of the cloud console. Then,
In the next dialog, create the new service account as shown below:
After this operation, a new JSON file is created and we download this file to our computer to use in our project in the next section.
In this section, we will perform the modifications listed below in our application so that we will be able to upload the image of the TV show that will be used in the application:
Modify the model
Change the TvShow model as seen below:
We added two new properties to the model:
Next, we will run the following commands in the Package Manager Console to update the database.
Modify the views
In the Create.cshtml, add the parts shown in red boxes below and delete the part related to ImageUrl :
Modify the Edit.cshtml as shown below:
Next, we will add style attribute to the ImageUrl to make the height and width of image standard in the other views.
In the Delete.cshtml and Details.cshtml, change the part related to ImageUrl with the following:
In the Index.cshtml, change the part related to ImageUrl with the following:
Edit the application settings
Add the following key-value pairs to appsettings.json and change the values with the values of the credential file path and GCS bucket that you created in the previous section:
Install the Google Cloud Storage package
Right-click on the project and select Manage Nuget Packages… and then install the following package:
Create a new class for GCS operations
Now, we will create a new class that will manage the file upload and delete operations in our GCS bucket using the client library that we installed above.
Right-click on the project and create a new folder called CloudStorage. Then add the following ICloudStorage interface to this folder.
Next, add a class called GoogleCloudStorage which implements the ICloudStorage under this folder:
This class gets the credential JSON file name and the GCS bucket name from the application settings that we defined previously.
As seen in the implementation, it has two methods:
We will use these methods in the Create, Edit and Delete action methods of the TvShowsController.
Edit Startup.cs
Before using the GoogleCloudStorage class in the TvShowsController, we need to register it as a service in Startup.cs to the built-in IoC container as it will be injected to the controller via constructor injection.
Add the following line to the end of the ConfigureServices method:
Modify the controller
Now, we will modify Create, Edit and Delete action methods of the TvShowsController.
First, change the following part for the dependency injection:
Second, add the following new methods to the controller class:
FormFileName method creates a new object name in the format below:
Then, this new name is passed as a parameter to the UploadFile method and it is used as the object name in the GCS bucket. This object name will also be stored in the TvShow.ImageStorageName field and will be used to delete the object from the GCS bucket if needed.
UploadFile method calls the UploadFileAsync method that we defined in the GoogleCloudStorage class previously. Notice that after the file is uploaded to GCS, the public URL to this file is returned and we will use this URL to serve the file directly from GCS. So we store this value in the TvShow.ImageUrl field to show the poster in the views of the application.
Next, change the Create method as shown below:
As seen above, when we create a new TV Show, we also upload the image file (if the user chooses one) to the GCS using the UploadFile method.
Next, change Edit method as shown in the below image:
In the Edit method, we first check if there is a related poster for this TV Show and if it exists we delete that from the GCS. Then, we upload the new image to the GCS.
Lastly, add the following code to the DeleteConfirmed method:
You can find the last version of the project in this GitHub repository.
Now, we will test the application and see if everything works as expected.
First, let’s create a new TV Show:
As you see above, we can now choose the image file from our computer. After clicking the Create button, the new record including the fields related to the image is created in the database as shown below:
We can also check that the object is created in our bucket in the GCS:
Notice that the ImageStorageName in the table and the Name field in the GCS bucket match.
In the Index view of the application, the poster of the image is served from the GCS using the ImageUrl field in the table.
Next, we will test the Delete view:
After clicking the Delete button, if we refresh the GCS bucket we see that the image object is deleted here as well.
You can check the Edit view and see that everything works as expected.
To avoid incurring charges, we can delete our GCP project to stop billing for all the resources used within the project.
In the Google Cloud Console, search for Manage Resources. In this menu, select your project and click Delete.
That’s the end of the post. I hope you found this post helpful and easy to follow. If you have any questions and/or comments, please let me know in the responses section below.
And if you liked this post, please clap your hands 👏👏👏
Bye!
https://cloud.google.com/storage/
https://cloud.google.com/storage/docs/quickstart-console
https://cloud.google.com/appengine/docs/flexible/dotnet/using-cloud-storage
Posts related to .NET Core (ASP.NET Core, MVC, Web API…)
200 
3
200 claps
200 
3
Written by
A software developer who loves learning new things and sharing these..
Posts related to .NET Core (ASP.NET Core, MVC, Web API…)
Written by
A software developer who loves learning new things and sharing these..
Posts related to .NET Core (ASP.NET Core, MVC, Web API…)
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://itnext.io/google-cloud-platform-and-flutter-mini-course-released-9193efebb8b0?source=search_post---------195,"Becoming a full-stack developer requires a hand in all layers of the application development stack. This includes Frontend, Backend, Database and Infrastructure.
Google Cloud Platform is a suite of cloud computing services used by Google and external business for running various kinds of software. This platform contains relevant services allowing us to be effective in building full-stack applications.
In this mini course we will build an Image Upload Mobile App using a couple of Dart packages primarily gcloud and googleapis_auth. These packages provide a set of helper classes for connecting to Google Cloud Platform services.
We will be using the Flutter SDK to build our mobile app since it provides a great set of features and tools for building cross-platform mobile applications. It also makes sense to be using Flutter as it’s also written in Dart 😉
We need to set up a Google Cloud Platform account. It’s free to set up and they’re currently offering $300 credit for 90 days. This will allow you enough time to use the Cloud Storage service for uploading images to.
The contents of the video are as follows:
Once you’ve set up a Google Cloud Platform account, create a Flutter project by running the following command:
Add the following packages under the dependencies key of your pubspec.yaml file:
Then run pub get to update your packages. Once you are setup, see the course for the full demonstration.
→ Watch full mini-course
If you enjoyed reading this post, please share this through your social media accounts. Subscribe to the YouTube channel for tutorials demonstrating how to build full-stack applications with Dart and Flutter.
Subscribe to the newsletter for my free 35-page Get started with Dart eBook and to be notified when new content is released.
Like, share and follow me 😍 for more content on Dart.
Originally published at creativebracket.com
ITNEXT is a platform for IT developers & software engineers…
197 
1
197 claps
197 
1
Written by
Christian | Web Developer | Egghead.io instructor | Sharing exclusive Dart content on my Blog → https://creativebracket.com | @creativ_bracket
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
Written by
Christian | Web Developer | Egghead.io instructor | Sharing exclusive Dart content on my Blog → https://creativebracket.com | @creativ_bracket
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/deploy-serverless-container-google-cloud-run-68d716af7716?source=search_post---------196,"There are currently no responses for this story.
Be the first to respond.
Cloud Run is a managed compute platform that enables you to run stateless containers.
Cloud Run is serverless: it abstracts away all infrastructure management, so you can focus on what matters most — building great applications.
It is built from Knative, letting you choose to run your containers either fully managed with Cloud Run, or in your Google Kubernetes Engine cluster with Cloud Run on GKE.
In this article, we would:
I will be using Google Cloud Shell to manage resources on Google Cloud Platform with an assumption that you have it installed on your PC.
where <PROJECT_ID> is your GCP project ID.
Our demo project files for this article would just be index.js and package.json. We'll also have a Dockerfile to build our image.
Feel free to use your own project files.
We would not be building our Docker image on our PC, Google Cloud Build allows us to build a Docker image using a Dockerfile, which we already have and then pushes the image to Container Registry 😊
Let’s use Google Cloud Build to build our Docker image and push the image to Container Registry. Both can be done by simply running the following command:
That’s it! We have our Docker image built and now on Container Registry.
Note that if you’re building larger images, you can pass a timeout parameter such as: --timeout=600s
We could either deploy from Cloud Shell or directly from Container Registry Interface.
We’ll be asked to input Service name and some other options. On success, you’ll get the Service URL 😀
Click on the Image Name and Deploy the latest by selecting Deploy to Cloud Run on the list of options.
We’ll also need to define the Service name and authentication option.
Cloud Run also allows us define additional options for our deployment such as Environment Variables , Memory Allocation.
Click Create and take a sip of that juice. Voila!Our Containerized Application Runs on Cloud Run - Serverlessly 😀
Cloud Run does not charge when the service is not in use. You can use a custom domain rather than the default address that Cloud Run provides for a deployed service.
Cloud Run also runs on Google Kubernetes Engine — this gives you more flexibility on managing your infrastructure. The tweet below gives more insights on this.
Additional Resources on Cloud Run ::📚 Cloud Run Product Overview📯 Cloud Run Release Blog Post📹 Cloud Run Launch Next 19💻 Awesome Cloud Run⚡ Cloud Events☁️ KNative
Thanks for reading through! Let me know if I missed any step, if something didn’t work out quite right for you or if this guide was helpful.
Originally posted on Mercurie Blog
Google Cloud community articles and blogs
272 
2
272 claps
272 
2
Written by
Software / DevOps Engineer | Google Developer Expert for Cloud | https://timtech4u.dev
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Software / DevOps Engineer | Google Developer Expert for Cloud | https://timtech4u.dev
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://blog.doit-intl.com/predicting-gcp-monthly-bills-with-cloudml-f5b2f2cbff9d?source=search_post---------197,"As some of you might already know, DoiT International is the engineering power behind reOptimize — Cost Discovery and Optimization SaaS for Google Cloud Platform.
With reOptimize you can get instant insights on your Google Cloud Platform billing, manage budgets, set up cost allocations and explore different cost optimization strategies.
One of the features we had from the day one is an estimation on how your monthly bill will look like at the end of the month. Here is how it looks like in reOptimize’s dashboard:
Initially, our estimation model used a very trivial linear regression — taking the current spending and the day of the month and extrapolating the value assuming spending will increase linearly. We did make some adjustments such as taking into consideration Google Compute Engine Sustained Discounts and few other things as well but still it was very naive and simplistic.
Unfortunately cloud services are cumulative, and in general, the cloud spending by itself doesn’t act linearly at all. There are seasonality patterns such as longer or shorter months, holidays seasons, marketing campaigns and so much more! The estimation given by this model usually is pretty far off. We calculated it to have a root-mean-square deviation (RMSE) of ~900.
Since many of our customers rely on reOptimize to forecast their Google Cloud Platform spendings, we started to work on Machine Learning based model to increase the accuracy of our estimations.
Naturally, we decided to use TensorFlow —an open-source software library for Machine Intelligence and specifically the Google CloudML which is Google’s managed service for TensorFlow so we would not need to take care of scaling, deployment, monitoring and other not too fascinating operational burden ;-)
There are three major parts to a our project:
All of these need to be automatically performed iteratively as new billing data arrives daily.
Google Cloud Billing API can export billing records into a variety of destinations and one of these is Google BigQuery. We are using BigQuery table as a source of data to train our prediction model.
The information in the table has the following columns:
To optimize the training process, we want to transform the raw data into a format better suitable for TensorFlow. This includes both the format and the semantics of the data.
TensorFlow supports multiple formats but the standard format is called TFRecords format. There are many built-in utilities to read and write data in this format.
The data semantics itself should be transformed into values best fitted for a DNN. The DNN or a Deep Neural Network — is a type of mathematical model that is used in machine learning and is built on the basis of the functionality of a human brain.
A DNN doesn’t work well with inputs that are not numbers and preferably numbers with real meaning. For example, product names and project names do not really mean anything on their own as well as year values and month values. On the other hand, the day numeric value has such meaning as well as the cost. Also feeding the raw cost rows would not be productive as we need to supply a target value per sample.
We are transforming the data from BiqQuery into something better suited for our DNN model.
The way we thought would be best to define our samples is to create a daily aggregation for a product and project and the sum of the costs from the beginning of the month up to that day. We also need to add the total cost for that product and project at this month and use that as the target value to train on. Finally, the product and project names must be first mapped into integers.
In our case, each sample would include the project, product, day, partial cost (from beginning of month) and total cost. In order for the DNN to be able to understand the data we want to augment the sample with additional data that mind prove useful but hard for the DNN to calculate on it’s own.
The number of days in the current month is an arbitrary constant that may affect the total cost. We can “calculate” this for the DNN, thus saving time and complexity. We can also calculate the ratio of cost per day by dividing the partial cost from the current day.
Another useful value would be the mean ratio and mean total cost for the product and project — these values would probably be used by a humans when trying to forecast the monthly bill. We can easily get BigQuery to build us a mapping table that provides these mean values per project and product and we used this table to create a static lookup table to enrich samples with.
Some other ratios were also calculated to provide more information for the DNN to use such as the estimated linear total cost (i.e. ratio * days in month)
Now that we have the data, we would feed the samples, one by one, along with the label value for the sample. We will train the model to predict the total cost from the sample input values. This way we could later use the model to feed the current spending and date per project and product and predict the total cost for the end of the month.
Now that we have our data ready, we wanted to have a way to evaluate our model against what we have today. We used the transformed data to calculate the simple linear regression performed by our code today. Each sample was calculated and then we subtracted the expected total from the prediction, squared the value and then calculated an average for this value for all samples. The square root of this value is the RMSE. We got an RMSE of ~900.
Looking at the data at this stage it was obvious that it didn’t behave in any linear fashion. Trying to create a DNN that would predict such data would be very hard — but we can help build a simpler DNN by feeding the network with some calculated values which we already know are relevant. To do that we would like to enrich the data as well as transform it.
One thing that looks helpful is to feed a value with the ratio between the current cost from start of month and the days since start of month. This value can also be averaged over each month and even globally to provide some more relevant info about spending behaviour.
And of course, it might help the DNN to know the history of total cost for previous month so we add the average total cost over each month and also a global average of the total cost.
This is the preprocessing function we built:
We then want to package it all together in TFRecord format ready to be trained. Luckily a package already exists that can help us to it. The python tensorflow-transform package uses a combination of Tensorflow and Apache Beam to process large amounts of data, transform them and write them into TFRecord files.
Another advantage of tensorflow-transform is that it creates a TensorFlow graph to process the data and runs it through a Beam pipeline — and that means you can use this graph later for the prediction — thus doing all these transformations on the data for you so you don’t need to transform the data prior to feeding it for prediction.
At the heart of the preprocessing script we run the Apache Beam pipeline:
The get_data_from_bq function reads the BigQuery data by using the SQL query shown above.
It then uses AnalyzeAndTransformDataset to perform the processing itself. The AnalyzeAndTransformDataset needs a pre-processing function to define the transformation graph. The transformed data is written using TFRecord formats into two file sets for training and testing.
In addition to the data, the pipeline also saves the metadata for the model, including the raw input metadata, the transformed data metadata and the transformation function graph. These are later used for training the model.
After the data is ready, the next step is to train a model with the data we have just prepared.
We decided to use a simple DNN and train it using the TensorFlow DNNRegressor class. The metadata created by tensorflow-transform in the pre-processing step is now used to create input functions for training, testing and serving.
Finally we use TensorFlow contrib.learn.Experiment to coordinate the training and evaluation.
To do this we define a function which generates the experiment:
And then use it to run the experiment:
The result of this process is a folder with the saved model which we can later deploy to Google CloudML to do actual predictions.
To allow reOptimize use the predictions, we obviously need to create some infrastructure that serves API requests coming from the application returning the output from the model.
Google CloudML does exactly that with TensorFlow Serving. TensorFlow Serving uses a trained model saved into a local disk or a GCS bucket to run the model and retrieve output. It exposes a RESTful API that can be used by any web or mobile client. Google CloudML takes it a step further by providing a serverless, fully managed serving infrastructure for your model so you don’t need to set up machines, autoscale them and so on..
After our model has been trained and saved, we are deploying it using the gcloud command line.
This will create a URL that can be used to predict using our model. The URL points to a fully managed backend API service created by Google CloudML.
It can both handle REST calls as well as be used by the gcloud command line tool like this:
The data/predict.json file contains information such as this:
And the results would be something like this:
By implementing machine learning to predict Google Cloud Platform bills, our forecast are now much more accurate. They are taking (indirectly) into account huge amount of signals such as seasonality, changes in pricing models, marketing campaigns and so on.
Our RMSE went down from ~900 to just ~100 so our predictions are much more credible and can be relied on by our customers.
By using Google CloudML, Dataflow and BigQuery, we could implement the machine learning infrastructure in reOptimize in just couple of weeks, while investing more time into the engineering rather than operations or maintenance.
Software & Operation Engineering. Written By Engineers.
253 
253 claps
253 
Written by
CTO of world’s best cloud consultancy
Here, our SRE and SWE teams share the solutions to some of the most interesting challenges we encounter during our daily work at DoiT International. If you like what we do, check out our https://careers.doit-intl.com page and join the team!
Written by
CTO of world’s best cloud consultancy
Here, our SRE and SWE teams share the solutions to some of the most interesting challenges we encounter during our daily work at DoiT International. If you like what we do, check out our https://careers.doit-intl.com page and join the team!
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/understanding-data-encryption-in-google-cloud-c36d9095fb38?source=search_post---------198,"There are currently no responses for this story.
Be the first to respond.
gcpcomics.com
Encryption is a process that takes plaintext as input, and transforms it into an output (ciphertext) that reveals little or no information about the plaintext. A public encryption algorithm is used, but execution depends on a key, which is kept secret. To decrypt the ciphertext back to its original form, the key needs to be used.
When you use Google Cloud, the data is encrypted at rest and in transit to protect the data.
In this issue of GCP Comics we are covering exactly that…Here you go!
Want more GCP Comics? Visit gcpcomics.com & follow me on Medium, and on Twitter to not miss the next issue!
Google Cloud community articles and blogs
204 
1
204 claps
204 
1
Written by
Developer Advocate @Google, Artist & Traveler! Twitter @pvergadia
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Developer Advocate @Google, Artist & Traveler! Twitter @pvergadia
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@gregsramblings/the-google-cloud-developer-cheat-sheet-now-on-github-cf42d9b8ecf8?source=search_post---------199,"Sign in
There are currently no responses for this story.
Be the first to respond.
Greg Wilson
Dec 4, 2018·1 min read
I’ve migrated the cheat sheet to GitHub — you can find it at https://github.com/gregsramblings/google-cloud-4-words
There are links to PDFs, PNGs, and a clean text version.
Google Developer Relations — Living in San Francisco https://gregsramblings.com | https://cloud.google.com | http://instagram.com/gregsimages
See all (727)
139 
1
139 claps
139 
1
Google Developer Relations — Living in San Francisco https://gregsramblings.com | https://cloud.google.com | http://instagram.com/gregsimages
About
Write
Help
Legal
Get the Medium app
"
https://rominirani.com/google-cloud-functions-tutorial-writing-our-first-google-cloud-function-a62de60b5c90?source=search_post---------200,"This is part of a Google Cloud Functions Tutorial Series. Check out the series for all the articles.
This post will cover how to write your first Google Cloud Function. We shall primarily be using the Google Cloud Console to create, deploy and test out the Cloud Function.
We shall be using JavaScript to write our first Google Cloud Function. As a result, you can either select the Node.js 6 or the Node.js 8 runtime for your Google Cloud Function. More on that later.
Our first Google Cloud Function will be a HTTP-Trigger based function, which means that we can directly invoke it via a HTTPs endpoint that will be assigned to our function.
Let’s get started.
The first step is to ensure that you have a Google Cloud Platform account with Billing setup. Remember that there is an always free tier when you sign up for Google Cloud and you could use that too.
Optional: If you have not created a Google Cloud Platform project, I suggest to create a new Cloud Project under which you can have all the examples covered in this series. Create the New Project from the Cloud Console and fill out the details for your project. A sample screenshot is shown below. Please note down your project-id.
Once you have setup your project, the next step is to enable the Google Cloud Functions API for your project. From the Cloud Console, click on the Main Menu in the Top Left corner and navigate to Menu → APIs and Services → Dashboard as shown below:
Click on Enable APIs and Services. This will bring up a screen as shown below, where you can enter Cloud Functions in the search field.
Click on Google Cloud Functions API and then click on Enable. This will enable the API for use in your Google Cloud Platform project.
We are now primarily going to be using the Google Cloud Functions menu option in the main menu as shown below. You will find that in the Products → Cloud Functions option.
I strongly suggest to use the Pin feature to pin this option to the top of the Cloud Console menu for easy access.
This completes our Project setup. We are now ready to explore writing our first Google Cloud Function.
Click on Cloud Functions in the main menu. This will lead to a screen shown below:
Click on Create function.
You will see an intimidating form come up next as shown below:
Fill out the details as given below.
Click on Create to create the function. This will take a while at times, so please be patient.
Once the operation is successful, you will see your function listed in the list of functions as shown below. Note the Runtime column value, which is Node.js 6 that we selected.
If you click on the ellipsis at the right corner to see additional options as shown below:
This will let you test and view logs for your function execution. We shall come to that in a while.
Click on the function name i.e. function-1 above to proceed further.This will bring up the function details as shown below. You can do several things from here. You can manage your source code, edit your function details, test your function, view your logs, view metrics and more.
Click on Trigger and you will notice that the trigger type for our function is HTTP trigger. The interesting thing to note here is the Unique HTTPs URL that is generated for our function. Notice how it uses the region, projectid and your function name provided to form this url. This URL is directly invocable and you can try that in your browser.
The next thing to observe is to click on Source. You can chose to edit the source from here too if you want and deploy a newer version.
The source code is simple for now and we are using the standard template that it has provided. Some high level details are given below:
Let us go ahead and test our function. Click on the Testing tab. This will present a screen as shown below:
Notice that our code was parsing out the message attribute via the ExpressJS req.body.Messagecall. Here in the Testing form, we provide a JSON formatted message for the same as shown below. Go ahead and provide that. Click on Test function.
This will invoke our function and the output that our function returned will be shown to us. Notice the Logs for our function execution. It will contain some metadata in terms of the time that it took our function to execute, the status code that it returned. In addition to that, it will also have any messages and errors that our function throws in terms of the console messages.
I suggest that you test out the functions for some 5–10 times manually. Once done, go ahead and visit the General tab. It will show you various metrics for execution time, latency, errors and more.
This completes the tutorial for writing our first Google Cloud Function.
Proceed to the next part : Setting up a Local Development Environment or go back to the Google Cloud Functions Tutorial Home Page.
Technical Tutorials, APIs, Cloud, Books and more.
235 
1
235 claps
235 
1
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
Technical Tutorials, APIs, Cloud, Books and more.
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
Technical Tutorials, APIs, Cloud, Books and more.
"
https://rominirani.com/tutorial-write-a-hangouts-chatbot-powered-by-google-cloud-functions-890db447bceb?source=search_post---------201,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
This is part of a Google Cloud Tutorial Series. Check out the series for all the articles.
Google recently announced general availability of Hangouts Chat for G Suite customers. The announcement mentioned the release of the Hangouts API to write custom Chatbots that can be made available to users in your organization.
This is a step by step tutorial on writing a sample bot with full instructions on setting it up in your Hangouts Chat and seeing it work. The Bot that we are going to write is a simple one but a very apt one for the times that we live in: A Bitcoin Hangouts Chatbot. It will provide you the current price in a currency of your choice : USD, EUR and GBP. If you have invested in Bitcoin, I hope you keep calm with the price fluctuations, if any. Let’s get going.
Let us see what we are going to build first. This will help to understand the pieces.
The screen below shows my Hangouts space via chat.google.com.
I click on the Bitcoin Price Bot that was added to my space. And it opens up a direct conversation channel with the Bot. I provide the currency for which I would like to know the current price of 1 Bitcoin in. The Bot responds with the price.
At a high level, this is what we did and we shall see the steps for the same in a while but it is sufficient to know what is going on here:
Let us first develop our function. Before we begin that, let’s perform the following steps:
Click on that and then Enable the API.
Assuming that you are logged into to Google Cloud Console and selected your specific Google Cloud Platform project, navigate to Cloud Functions.
Click on CREATE FUNCTION as shown below:
This will bring up a form where you can provide the details for your function as follows:
7. Function to execute : In this field, the value will be bitcoinPrice as you can see from the exports field in the above code.
8. In the package.json section, please use the following:
I am using a 3rd party npm module axios that makes it easy to make HTTP Requests.
9. Go ahead and click on the Create function. This will create the GCF function and you should see it deployed and ready in the list as shown below:
It is important to understand what is going on in the code. I will skip any discussion around Javascript and Promises, since that will take the focus away. Even if you are not a Javascript person, you should be able to follow along.
A few points are given below:
It would be good for you to test out your function. But if you want to do that, please ensure that you comment out the line for verifying the webhook, which does a token validation as explained.
Once the function is saved, please make a note of the FUNCTION_URL as mentioned earlier. You will need this when we configure the Chatbots API in our Hangouts space next.
Now that we have written our function that were power the Hangouts Chatbot, all we need to do is publish (a step of configuration tasks) the bot. The steps are given below:
3. Click on the Hangouts Chat API and enable it. Once you have enabled it, click on Manage and go to the Configuration tab for the same as shown below:
4. In the Configuration tab, you will see several fields to fill out.
This completes the configuration/publishing of the Hangouts Chatbot.
3. Click on Message a Bot as shown below:
4. This will bring up a Find a Bot form, where you can enter “Bitcoin” to locate our published Bitcoin Bot as shown below:
5. That’s it. This will bring up a direct message channel with the Bot. Provide any one of the currencies (USD, EUR or GBP) and get the current price of 1 bitcoin in that currency.
Hope you enjoyed the tutorial. Now go ahead and energize your G Suite Hangouts space.
This is part of a Google Cloud Tutorial Series. Check out the series for all the articles.
Technical Tutorials, APIs, Cloud, Books and more.
222 
3
222 claps
222 
3
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
Technical Tutorials, APIs, Cloud, Books and more.
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
Technical Tutorials, APIs, Cloud, Books and more.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/apache-airflow/a-simple-guide-to-start-using-apache-airflow-2-on-google-cloud-1811c2127445?source=search_post---------202,"Sign in
There are currently no responses for this story.
Be the first to respond.
Antonio Cachuan
Mar 30, 2021·7 min read
If you are wondering how to start working with Apache Airflow for small developments or academic purposes here you will learn how to. Well, deploying Airflow on GCP Compute Engine (self-managed deployment) could cost less than you think with all the advantages of using its services like BigQuery or Dataflow.
This scenario supposes you need a stable Airflow instance for a Proof of Concept or for learning space.
Why not Cloud Composer?Cloud Composer is a fully managed workflow orchestration service built on Apache Airflow. I suggest going with this if you or your team require a full production or development environment since Composer demands a minimum of 3 nodes (Compute Engines) and other GCP services so the billing could be an obstacle if you are starting your learning path on Airflow.
ImportantGoogle Cloud offers $300 in credits for first time users.
First, let's start with some concepts and then go deep with a simple Airflow deployment on a compute engine instance.
What is Apache Airflow?
Apache Airflow is an Open Source Platform built using Python to program and monitor workflows.
DAG
A DAG is a collection of all the tasks organized in a way that reflects their relationships and dependencies.
Tasks
A Task is a unit of work within a DAG. Graphically, it’s a node in the DAG. Some examples are the implementation of a PythonOperator which executes a piece of Python code, or BashOperator, which executes a Bash command.
What is new in Apache Airflow 2?
Airflow 2 was launched in December 2020 with a bunch of new functionalities here are some important changes:
For more details and changes regarding authoring DAGs in Airflow 2.0, check out Tomasz Urbaszek’s article for the official Airflow publication, Astronomer’s post, or Anna Anisienia’s article on Towards Data Science.
Remember that this scenario supposes you need a stable Airflow instance for a Proof of Concept or for a learning environment.
Important: The following guide is not recommended for production environments. I suggest you to visit this document for more details about deploying Airflow in production
You’ll need to create a Service Account, so your Airflow instance can access the different GCP services in your project.
First, go to IAM & Admin then Service Accounts
Then enter the service account name and click on Create
Give minimum access to BigQuery with the role of BigQuery Job User and Dataflow with the role of Dataflow Worker.
Click on Done, and after that look for your new Service Account and then click the three dots and go to the Manage keys.
Click Add Key/Create new key/Done. This will download a JSON file.
Finally, keep this JSON file and change the name to key-file.json. It’s the key to work with BigQuery and Dataflow.
2. Create a Compute Engine instance
Let’s deploy a Debian instance with the minimum requirements for this case.
Additionally, allow HTTPS and HTTP traffic and select the Service Account created
3. Installing Airflow
On the console click on SSH to start a terminal.
On the terminal let's install python and update the catalog
I’ll use miniconda to be able to create a virtual environment
Close your terminal, and open a new one.
Create our virtual environment, and activate it
Install Airflow and extra libraries
4. First-time setup
Only once you need to init your metadata database and register at least one admin user.
5. First execution
Yes! We are near to have an Airflow instance on GCP, We need to whitelist our IP for port 8080.
Go to Firewall/Create Firewall Rule
Create the airflow-port rule
Go to Compute Engine/VM instances and click on airflow-poc
Add the firewall rule
Go back to the terminal and start the Web Server
Open another terminal and start the Scheduler
Go to your Google Cloud Console and copy the external IP to your clipboard.
Finally, on your browser go to https://COMPUTE-ENGINE-IP:8080 and login with the user and password you have created when the DB was initialized.
It’s done! Our Airflow 2 instance is running!
6. Next executions
For future executions, we want that our Airflow starts immediately after the Compute Engine start.
Create a Cloud Storage Bucket
Create a script, upload the file and keep it as a backup
Only once copy your start file from your bucket.
Now, each time you need to start your server just run it.
7. Set up access to GCP resources
It’s time to upload our key-file.json to our instance and move to the location
Then set up the connection on the Airflow website
Complete with the file type and with your GCP project id and then click on save
That's all! You have a basic Airflow environment ready to orchestrate processes on BigQuery or Dataflow.
Apache Airflow is a fantastic orchestration tool and deploying it on GCP enables the power to interact with services like BigQuery, Dataproc.
CaveatsLike I said at the beginning, this article is for development or simplistic environments. As you develop more pipelines Airflow would need more resources triggering scaling problems.
On the other hand, since you only use a compute engine you don’t need to keep the machine running all day as a consequence the billing is cheaper, in this case, $25 per month than Cloud Composer around $300 minimum.
PS if you have any questions, or would like something clarified, ping me on LinkedIn I like having a data conversation 😊
Google Cloud Professional Data Engineer (2x GCP). When code meets data, success is assured 🧡. Happy to share code and ideas 💡 linkedin.com/in/antoniocachuan/
77 
2
Thanks to Tomasz Urbaszek, RNHTTR, and Leah Cole. 

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
77 claps
77 
2
Apache Airflow (or simply Airflow) is a platform to programmatically author, schedule, and monitor workflows. If you are interested in adding your story to this publication please reach to us via #blogposts channel on Airflow slack.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/net-core/deploy-an-asp-net-core-app-with-ef-core-and-postgresql-to-google-cloud-be8a06978eb0?source=search_post---------203,"There are currently no responses for this story.
Be the first to respond.
In this post, I will show how to deploy an ASP.NET Core web application with EF Core and PostgreSQL to App Engine on Google Cloud Platform (GCP). Besides, I will show how to use Cloud SQL for PostgreSQL instance from this application.
I will use the following tools:
The sections of this post will be as follows:
In the previous post, I demonstrated how to deploy an ASP.NET Core web application starter project to GCP. As it has no database access, it is a simpler version of this one and I recommend you start from there if this is your first deployment to GCP.
If you are ready, let’s get started.
The application that we will deploy manages a database of TV Shows and its main page looks like below:
The app is an ASP.NET Core MVC web application and it uses EF Core to perform CRUD operations on a PostgreSQL database.
You can download the source code from this GitHub repository.
If you don’t have a Google Cloud account, go to Google Cloud web site and click Get started for free button. Sign up with your Gmail account and get $300 (for free) to spend on Google Cloud Platform over the next 12 months.
Next, go to Google Cloud Console and type App Engine in the search box and click Create in the dashboard of the App Engine to create a new project:
Name your project and then select your region and enable billing in the next page:
Now, we will create our PostgreSQL instance in the Cloud SQL.
Cloud SQL is a fully-managed database service that makes it easy to set up, maintain, manage, and administer your relational databases on Google Cloud Platform. You can use Cloud SQL with either MySQL or PostgreSQL.
First, select APIs & Services -> Library
and in the next page search for Cloud SQL API and Enable this API:
Next, search for Cloud SQL and click Go To Cloud SQL in the following page:
Click Create Instance in the next page:
and then choose PostgreSQL.
On the next page, give an ID to your instance and set your password for the default (postgres) user.
We will use this Id and password to connect to this instance later in the tutorial.
Next, the Compute Engine API creates the instance and it takes a few minutes.
After you see the green tick on the instance ID, click on the instance to go to Instance Details page:
In the Overview tab, you can see the IP and connection name for the instance:
Next, go to the Connections tab and click Add Network:
Security Issues
As you see, we gave access to public IPs to make login attempts to the instance. SSL encryption is recommended when using Public IP to connect to the instance.
For the sake of simplicity, I will not implement this part in this tutorial. If you want to implement this, please follow the instructions here. Besides, you will need to modify your app too. If you need help in this part, let me know in the comments.
In Visual Studio, open the application and modify the connection string in appsettings.json:
If you haven’t created your migration file already, run the following command in the Package Manager Console:
and then run the below command to create the database on the instance:
Now, we will check the database created on Cloud SQL using pgAdmin.
Launch pgAdmin and then select Servers -> Create -> Server:
and then fill in the fields shown in the red box below:
As you see below, our database and tables were created on the Cloud SQL:
Now, we will test the application on our local computer.
In this test configuration, our application server is on localhost and the database server is on the cloud.
Run the application on the Visual Studio and click TVShowsApp on the web page and then Create New on the next page.
After creating our first record, it is shown below on the main page:
We can check the record created using pgAdmin as well:
Now, we will publish our application to Google Cloud.
First, install Google Cloud Tools for Visual Studio if you don’t have it already.
Open Visual Studio and go to Tools -> Extensions and Updates. Search for Google Cloud Tools for Visual Studio and click Install (It’s already installed on my computer):
As a side note, Cloud Tools for Visual Studio is not supported in VS 2019. So, I am using VS 2017 for this tutorial.
Next, we will link our application to our GCP project.
Open Tools -> Google Cloud Tools -> Manage Accounts:
and select Add account. Login to the account that you used to register to the GCP.
You will see your account logged at the top right corner of Visual Studio. There is a No Project label near this account and click that and click Select Project:
Select the project that you created above.
Enable the following APIs from Google Cloud Console:
Next, right-click on the project in Visual Studio and select Publish To Google Cloud. Then select App Engine Flex in the next dialog.
After the deployment operation is completed, the web page is launched automatically and the application’s URL is as follows:
I created a new record as we did in the previous section and the main page of the application running on the cloud looks like below now:
Again, we can check the new record from the database using pgAdmin:
To avoid incurring charges, we can delete our GCP project to stop billing for all the resources used within the project.
In the Google Cloud Console, search for Manage Resources. In this menu, select your project and click Delete:
That’s the end of the post. I hope you found this post helpful and easy to follow. If you have any questions and/or comments, please let me know in the responses section below.
And if you liked this post, please clap your hands 👏👏👏
Bye!
https://cloud.google.com/appengine/docs/flexible/dotnet/using-cloud-sql-postgres
Posts related to .NET Core (ASP.NET Core, MVC, Web API…)
152 
3
152 claps
152 
3
Posts related to .NET Core (ASP.NET Core, MVC, Web API…)
Written by
A software developer who loves learning new things and sharing these..
Posts related to .NET Core (ASP.NET Core, MVC, Web API…)
"
https://medium.com/google-cloud/using-google-cloud-vision-api-with-golang-830e70323de7?source=search_post---------204,"There are currently no responses for this story.
Be the first to respond.
(Exploring LABEL_DETECTION and TEXT_DETECTION)
Google Cloud Vision API enables developers to understand the content of an image by encapsulating powerful machine learning models in an easy to use REST API.
The Cloud Vision API is an easy to use REST API that uses HTTP POST operations to perform data analysis on images you send in the request. The API uses JSON for both requests and responses. A typical Vision API JSON request includes the contents of image(s) on which to perform detection, and a set of operations (called features) to run against each image.
Of the many feature that the API gives us, we are going to explore “LABEL_DETECTION”. A “LABEL_DETECTION” request annotates an image with a label (or “tag”) that is selected based on the image content. For example, a picture of a barn may produce a label of “barn”, “farm”, or some other similar annotation. A label request is one of the most common use cases for the Vision API. A “TEXT_DETECTION” request finds and reads printed words contained within images.
The Google Cloud Vision API is in general availability and there is a free tier, where you are allowed 1,000 units per Feature Request per month free. Beyond that there is a tiered pricing model based on the number of units that you use in a month.
My friend Romin Irani has written an excellent article “How to Build a Monitoring Application With the Google Cloud Vision API” using Python. Please refer to pages 2, 3 and 4 of this article and execute the steps 1 to 4. These are necessary to use the Google Cloud Vision API.
After you have completed steps 1 to 4, let us start writing our Go code.
Get Dependencies
Our Go program depends on the following packages. Before getting started, be sure to get them.
label.go
I have created a folder “cvision” which will hold my Go source code “label.go”, “text.go” and an image “dog.jpg” to analyze. The Go code for “label.go” is the same as the sample code for Google Cloud Vision with minor changes.
We shall be running our program at the command prompt in the folder “cvision” as follows:
First draft of our program “label.go”
Let us understand the program so far.
First “import” the libraries necessary to run the program.
The package “flag” implements command-line flag parsing. Usage is a variable that holds a function. It is called when an error occurs while parsing flags. Let us run the program written so far as:
I get the above error since I did not give the name of the image after “label.go”.
Package “os” provides a platform-independent interface to operating system functionality. “Stderr” points to the standard error file.
Package “filepath” implements utility routines for manipulating filename paths in a way compatible with the target operating system-defined file paths. “filepath.Base” returns the last element of path. Trailing path separators are removed before extracting the last element. If the path is empty, “Base” returns “.”
“flag.Parse” parses the command-line flags from os.Args[1:]
“flag.Args” returns the non-flag command-line arguments.
“os.Exit” causes the current program to exit with the given status code. Conventionally, code zero indicates success, non-zero an error. The program terminates immediately; deferred functions are not run.
Second draft of our program “label.go”
We pass the name of the image file to a function “run()”.
Authenticate your Service
Package “context” defines the Context type, which carries deadlines, cancellation signals, and other request-scoped values across API boundaries and between processes. “context.Background()” returns a non-nil, empty Context. It is never canceled, has no values, and has no deadline. It is typically used by the main function, initialization, and tests, and as the top-level Context for incoming requests. Do not store Contexts inside a struct type; instead, pass a Context explicitly to each function that needs it. The Context should be the first parameter, typically named “ctx”.
Before communicating with the Vision API service, you will need to authenticate your service using previously acquired credentials. Within an application, the simplest way to obtain credentials is to use Application Default Credentials (ADC). By default, ADC will attempt to obtain credentials from the GOOGLE_APPLICATION_CREDENTIALS environment variable, which should be set to point to your service account’s JSON key file (Step 4 of Romin Irani’s article).
Package “google” provides support for making OAuth2 authorized and authenticated HTTP requests to Google APIs. “google.DefaultClient” returns an HTTP Client that uses the “DefaultTokenSource” to obtain authentication credentials. It looks for credentials in a JSON file whose path is specified by the GOOGLE_APPLICATION_CREDENTIALS environment variable.
Package “vision” provides access to the Cloud Vision API. “vision.CloudPlatformScope” is a constant that can view and manage your data across Google Cloud Platform services.
“vision.New” returns a “Service”.
We now have a Vision API service with which we can make API calls.
Read the image and create a request, encoding the image in base64
We first read our image data into a variable. ioutil.ReadFile reads the whole file named by filename and returns the contents. Requests to the Google Cloud Vision API are provided as JSON objects. However, JSON does not support the transmission of binary data, so we will need to escape our binary data into text by encoding it in Base64. Variable “StdEncoding” is the standard base64 encoding. “EncodeToString(b)” returns the base64 encoding of b.
Currently, the Vision API consists of one collection (images) which supports one HTTP Request method (annotate). The annotate request passes a JSON request of type “AnnotateImageRequest”. An example is shown below:
“requests” — An array of requests, one for each image.“image” — The image data for this request.“features” — The array of features to detect for this image“type” — The feature type example LABEL_DETECTION“maxResults” — The maximum number of results to return for this feature type. The API can return fewer results.
“vision.AnnotateImageRequest” is a request for performing Vision tasks over a user-provided image, with user-requested features.
“vision.Image”: Client image to perform Vision tasks over.
“vision.Feature” indicates what type of image detection task to perform. Users describe the type of Vision tasks to perform over images by using Features. Features encode the Vision vertical to operate on and the number of top-scoring results to return. We use LABEL_DETECTION. In our case we ask for 5 results, which will be given to us in increasing order of probability.
Submit the Requests on Batched
“vision.BatchAnnotateImagesRequest” — Multiple image annotation requests are batched into a single service call.
“Annotate”: Run image detection and annotation for a batch of images. “Do” executes the “vision.images.annotate” call.
A POST request has now been made.
JSON Response Format
The “Annotate” request receives a JSON response of type “AnnotateImageResponse”. Although the requests are similar for each feature type, the responses for each feature type can be quite different.
“LabelAnnotations” if present, label detection completed successfully. “Description” is the Entity textual description.
Run the program
The output
The results are not very accurate for some of the labels that it found but you can see where things stand today and the possibilities that this opens up.
text.go
This program is very much similar to “label.go” with some minor changes as shown below.
Run the program
The output
You can now easily write a Go program that can use “LOGO_DETECTION” and “FACE_DETECTION”.
That’s it!
Google Cloud community articles and blogs
138 
2
Some rights reserved

138 claps
138 
2
Written by
Senior Technical Evangelist at GoProducts Engineering India LLP, Co-Organizer GopherConIndia. Director JoshSoftware and Maybole Technologies. #golang hobbyist
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Senior Technical Evangelist at GoProducts Engineering India LLP, Co-Organizer GopherConIndia. Director JoshSoftware and Maybole Technologies. #golang hobbyist
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/google-cloud-storage-what-bucket-class-for-the-best-performance-5c847ac8f9f2?source=search_post---------205,"There are currently no responses for this story.
Be the first to respond.
One of the most common performance questions we get with respect to Google Cloud Storage is related to “What type of bucket I should use for the best performance?”
Well, to help figure that out, let’s describe the buckets, and run some tests to figure out the best situations.
GCS asks you to create “buckets” in order to place your assets, where the first question is what type of service bucket you want:
Regional, Multiregional, nearline or cold-storage.
Nearline and Coldline are not intended for high-performance systems, so we’ll ignore those right now, and look at the performance of the Regional and Multiregional buckets.
First off, let’s clarify some terminology:
There’s a common scenario where you want to have some client, or on-prem system uploading data to a cloud environment to do compute work, and then returning the information back to the client. Or, in sysadmin terms : “You want your VMs close to your data source to maximize throughput”
Regional GCS buckets guarantee all data in this bucket lies in the specified region, for this exact reason.
Now, to be fair, you don’t have fine grained control what subregion in the region your data is in; over time it can migrate and move. As such write latency is replicated to 2 locations, where there’s different metadata listings for each location it’s copied for fault tolerance. This can cause a remote-round-trip sync write to all the other meta data locations on a write (due to strong read-after-write consistency).
This means Regional buckets are great for data processing since their physical distance is fairly tight, and the overhead of write consistency is low.
Multiregional Storage, on the other hand, guarantees 2 replicates which are geo diverse (100 miles apart) which can get better remote latency and availability.
More importantly, is that multiregional heavily leverages Edge caching and CDNs to provide the content to the end users.
All this redundancy and caching means that Multiregional comes with overhead to sync and ensure consistency between geo-diverse areas. As such, it’s much better for write-once-read-many scenarios. This means frequently accessed (e.g. “hot” objects) around the world, such as website content, streaming videos, gaming or mobile applications.
To provide real numbers, we set up a test: upload a 2MB file to a bunch of regional and multiregional buckets, and then fetch that asset (with caching disabled) from a VM in us-west1.
This data appears to show that multiregional buckets perform significantly better for cross-the-ocean fetches, however the details are a bit more nuanced than that.
Looking back at the logs, the reason that the Multiregion buckets are performing better in those scenarios, is that the data was duplicated to region (of the multi-region) which provided a better access point (and lower latency) to our fetching client. (To confirm this, I ran the same exact test between us-west1 and europe-west1 directly, and got about ~175ms.)
What these tests show us is that there’s no specific performance difference of the classification of the buckets themselves. Rather the performance is dominated by the latency of physical distance between the client and the cloud storage bucket.
As such, we get a handy little rule here:
Google Cloud community articles and blogs
154 
1
154 claps
154 
1
Written by
DA @ Google; http://goo.gl/bbPefY | http://goo.gl/xZ4fE7 | https://goo.gl/RGsQlF | http://goo.gl/4ZJkY1 | http://goo.gl/qR5WI1
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
DA @ Google; http://goo.gl/bbPefY | http://goo.gl/xZ4fE7 | https://goo.gl/RGsQlF | http://goo.gl/4ZJkY1 | http://goo.gl/qR5WI1
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-developer-experts/trimming-down-the-cost-of-running-google-cloud-dataflow-at-scale-1c796f72c002?source=search_post---------206,"There are currently no responses for this story.
Be the first to respond.
Google Cloud Dataflow is one of the products provided by Google Cloud Platform which helps you ingest and transform data coming from a streaming or a batched data source.
At Roobits, we extensively use Dataflow pipelines to ingest events and transform them into desirable data that is to be used by our customers.
Dataflow is also serverless and auto-scales based on the input load, which is an added bonus to the flexibility it already provides.
Dataflow essentially requires you to write the logic that’s to be performed on the incoming events from a source (which could be PubSub, Apache Kafka, or even a file!) and then deploy that logic on Google’s servers.
Dataflow allows you to write this logic either in Java, Kotlin or Python.
A very simple example of a Dataflow Pipeline that takes an input paragraph and counts the words in it, is as follows :
While the code here might look complicated, you can go to the documentation page of Apache Beam to know more about what’s happening here.
To deploy this code on your Google Cloud Project, you can do so as follows :
While it looks good, there are certain concerns when it comes to pricing as you plan on scaling this pipeline as it is.
Let’s look at them one by one.
By default, the disk size for the dataflow pipeline is set to 250GB for a batch pipeline and 400GB for a streaming pipeline.
If you are processing the incoming events in memory, this is mostly a wasted resource, so instead, I’d suggest reducing this parameter to 30GB or less (the min recommended value is 30GB but we faced no issues while running the pipeline at 9–10GB of PD)
You can do so by specifying the disk size as follows while deploying your pipeline :
Now looking at Google Cloud Pricing calculator, reducing this value saves us around 20$ per month per worker.
Micro batching a streaming pipeline helped us cut down on the number of writes our dataflow pipeline made into BigQuery, thereby reducing the cost of BigQuery writes.
You can look at the article below for more insights on how to do this :
medium.com
By default, Dataflow supports the n1 machine types for the pipeline and while these machines cover a variety of use cases, however, you might often want to use a custom machine of your own with either a powerful CPU or a large RAM.
To do this, you can add the following parameter while deploying the pipeline :
The value above would correspond to 8 cores and 7424 MB of memory and you can tweak this according to your will instead of being locked into using the presets.
Streaming Engine is a new addition to the Dataflow family and has several benefits over a traditional pipeline, some of them being :
As of now, the streaming engine is only available in the regions mentioned in the list here, but more regions will be added as the service matures.
To enable Streaming Engine, just pass the following flag to your pipeline execution and that’s it!
By default, the Dataflow service assigns your pipeline both public and private IP addresses.
Now if you don’t want your data to be made available to the general public, it’s a good idea to disable public IPs as that not only makes your pipeline more secure but might potentially also help you in saving a few bucks on your network costs.
Adding the following flag to the pipeline execution disables public IPs :
While it might be a no brainer for some, but I see a lot of people (including myself) paying extra for data that is transferred between the GCP services, just because they are not in the same region.
For instance, we ended up paying around 500$ in a week one of our projects, because the dataflow pipeline and the source AppEngine were in different locations (US and Europe)
Not only AppEngine and Dataflow, but a lot of GCP services have free ingress/egress from/to the same region!
To set the region while deploying your Dataflow pipeline, you can add the following execution parameter :
The supported regions by Cloud Dataflow are listed here :
cloud.google.com
And that’s it!Using a combination of the tips mentioned above, we were able to save a substantial amount from our spendings on Dataflow.
You can visit my Medium profile to read more blogs around Dataflow and Google Cloud; starting with this one that I wrote last week!
medium.com
Thanks for reading! If you enjoyed this story, please click the 👏 button and share to help others find it! Feel free to leave a comment 💬 below.
Have feedback? Let’s connect on Twitter.
Experts on various Google products talking tech.
349 
1
349 claps
349 
1
Written by
Has an *approximate* knowledge of many things. https://aftershoot.co
Experts on various Google products talking tech.
Written by
Has an *approximate* knowledge of many things. https://aftershoot.co
Experts on various Google products talking tech.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://blog.appsecco.com/using-google-cloud-platform-to-store-and-query-1-4-billion-usernames-and-passwords-6cac572f5a29?source=search_post---------207,"Recently it came to our attention that there was a combined password dump which contained passwords cracked to plaintext.
The dump, said to be one of the largest, was 42 GB in size. That is a lot of usernames and passwords! Woah!
The username and password dump came conveniently sorted alphabetically and with simple scripts to query for email addresses. It also had scripts to count the total number of entries etc. On any decent laptop/virtual machine with an SSD, the query time is mere 4–5 seconds. But we wanted to do dig a bit deeper. We wanted to count things like:
Since the dump files were formatted in the format of username and password separated with a colon it seemed like a great opportunity to try and use Google BigQuery to query for these questions and more.
Google BigQuery enables super-fast SQL queries using the processing power of Google’s infrastructure.
It was a fairly compelling offer, whilst none of us at Appsecco are data scientists, we are definitely familiar with using SQL for querying databases.
We started at /r/netsec sub-reddit which gave us the magnet link for the torrent to this dump.
(Internally we now refer to this password dump as treasure trove!)
The post linked to a gist on Github with a magnet link to download the sorted list as a torrent (magnet link), the torrent didn’t start since it didn’t have any tracker information. But a redditor (Thank you CiNXNppjlK) posted the magnet link with tracker information.
Permalink to the comment for magnet link comment.
Since we planned to use Google BigQuery and wanted the data to be in Google Storage, we decided to download the torrent directly in a virtual machine on GCP. We are using a Debian Stretch available from the console.
We used aria2 for downloading magnet links on the command line.
aria2 is a complete bittorrent downloading solution. More information about aria2 https://aria2.github.io/
Since we planned to download at least 42 GB of data, it made sense to add an additional SSD to the machine.
We created a folder called torrents on the SSD
Starting a new bittorrent download using aria2 client with a magnet link is as simple as
The double quotes in BASH ensure that anything that needs to be escaped which is part of the link is escaped.
Since this was a torrent being downloaded (and we assumed it would take some time) we went out for a cup of coffee. By the time we came back 30 minutes later, the download was done!
A nice touch to every compute engine instance is that some of the command line tools required for working with other Google cloud services are already installed.
Before we could copy files to a Google Storage Bucket we needed to run the config command
Apart from some random output, we got a message saying
We copied the link given in message and got an authorization code
We knew it worked when we saw the following line of output
We created a bucket for our treasure trove
If you plan to do this as well you would ideally want to add some random hexadecimal string suffixed with a hyphen as well.
Optional, but looks cool kind of command
This is just a hangover of how we create AWS S3 buckets. You can chose to ignore this if you prefer your bucket names to be simple words
For any reason if you are unhappy with the bucket created, here is the command to remove it
Now we were ready to copy the data.
The gsutil utility had an option to parallelise copying of data by using the -m flag. Since we have to copy a lot of data, we used it. The entire copying got done in about 5 minutes or so.
If you plan to use this flag, if something does go wrong while copying you will not be able to use the retry feature. Therefore it makes sense to run this command inside tmux.
Now we just had to be inside the data directory
and execute the command
All this typing made us thirsty. That and the fact that we are copying about 42 GB of data, so time for another cup of coffee.
Now that we had the data in storage, we could if required, trash the compute engine instance so as to not incur any more charges.
So far all that we have done was to get ready for the main event. This is roughly what our plan was:
We started by creating a new dataset
Clicking on Create new dataset opened up a new dialog box
The new dataset name was visible and we clicked on the really small plus sign which opened up this
These were the options we decided to go with
Since we used a CSV file format, we needed to let BigQuery know about the following
With some trial and error we figured out that:
At this point we were ready to create the table. As soon as we clicked on create table, data started loading in a job.
Once the loading of data was successful we could see table details and that is a lot of data!
Compared to this the rockyou dump (an all time favourite for most security practitioners and penetration testers was about 32 million records in a 140 MB file
All this hard work would only be useful if we could get some interesting answers from the database which would be difficult scripting in BASH on the virtual machine where we downloaded the database in the beginning.
So it was time to start asking the query editor our questions in SQL
If you recall at the beginning of this post we listed down the kind of questions we wanted to answer
The answer is 15,511.
This doesn’t mean that these long passwords are sufficiently random but we can assume that not everyone of these 15000+ users would be typing these passwords by hand. So they may be using a password manager (clearly a subjective opinion at this point).
The answer is 119,340,157.
There are a whopping 119 million+ email addresses from Gmail in this dump!
The two queries required to get this to work
Result of this query need to be stored in a table named temptable
The main corporate domain that we were interested in was ours. For the query
The answer thankfully was zero.
The easy way was to allow different users access to the project where this particular BigQuery dataset is residing. But we saw that BigQuery had a nodejs client API.
We quickly wrote up a script to query for email addresses and uploaded it as a Google Cloud function. Since we are not developers, at this point we aren’t very keen to release the entire source code but snippets at this point.
This was a great learning experience for us. With breaches becoming so common and widespread, OSINT is already an integral part of our security assessment services.
While we were already searching password dumps for client email addresses to keep them secure, using services like BigQuery we can do this faster and reduce errors than do creep in with ad-hoc scripts.
If there are any gotchas in our approach or if you have a better one, please do let us know in the comments.
At Appsecco we provide advice, testing, training and insight around software and website security, especially anything that’s online, and its associated hosting infrastructure — Websites, e-commerce sites, online platforms, mobile technology, web-based services etc.
Making sense of application security for everyone.
171 
1
171 claps
171 
1
Written by
Author Burp Suite Essentials;Co-Founder+Director — http://appsecco.com , Community Manager @ null0x00; Ex-Chapter Lead OWASP BLR
Blog posts from the Security Testing Teams and DevSecOps Teams at Appsecco. Covering security around applications, Cloud environments like AWS, Azure, GCP, Kubernetes, Docker. Covering DevSecOps topics such as Secrets Management, Secure CI/CD Pipelines and more
Written by
Author Burp Suite Essentials;Co-Founder+Director — http://appsecco.com , Community Manager @ null0x00; Ex-Chapter Lead OWASP BLR
Blog posts from the Security Testing Teams and DevSecOps Teams at Appsecco. Covering security around applications, Cloud environments like AWS, Azure, GCP, Kubernetes, Docker. Covering DevSecOps topics such as Secrets Management, Secure CI/CD Pipelines and more
"
https://medium.com/google-cloud/all-you-need-to-know-about-google-cloud-dataproc-23fe91369678?source=search_post---------208,"There are currently no responses for this story.
Be the first to respond.
If you are using Hadoop ecosystem and want to make it easier to manage then Dataproc is the tool to checkout.
Dataproc is a managed Spark and Hadoop service that lets you take advantage of open source data tools for batch processing, querying, streaming, and machine learning.
Dataproc automation helps you create clusters quickly, manage them easily, and save money by turning clusters off when you don’t need them. With less time and money spent on administration, you can focus on what matters the most — your DATA!
In this video I summarize the what Dataproc offers in 2 mins.
Erin and Sam are part of growing data science team using Apache Hadoop ecosystem and are dealing with operational inefficiencies! So, they are looking at Dataproc which installs a Hadoop cluster in 90 seconds, making it simple, fast and cost effective to gain insights as compared to a traditional cluster management activities. It supports:
To move you Hadoop/Spark jobs, all you do is copy your data into Google Cloud Storage, update your file paths from HDFS to GS and you are are ready!
It disaggregates storage & compute. Say an external application is sending logs that you want to analyze, you store them in a data source. From Cloud Storage(GCS) the data is used by Dataproc for processing which then stores it back into GCS, BigQuery or Bigtable. You could also use the data for Analysis in a notebook and send logs to Cloud Monitoring and Logging.
Since storage is separate, for a long-lived cluster you could have one cluster per job but to save cost you could use ephemeral clusters that are grouped and selected by labels. And finally, you can also use the right amount of memory, CPU and Disk to fit the needs of your application.
If you like this #GCPSketchnote then subscribe to my YouTube channel where I post a sketchnote on one topic every week! And, if you have thoughts or ideas on other topic that you might find helpful in this format, please drop them in comments below!
Here is the website for downloads and prints👇
thecloudgirl.dev
Google Cloud community articles and blogs
202 
202 claps
202 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Developer Advocate @Google, Artist & Traveler! Twitter @pvergadia
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/google-cloud-storage-large-object-upload-speeds-7339751eaa24?source=search_post---------209,"There are currently no responses for this story.
Be the first to respond.
Medic7 was seeing a problem uploading their large video files. Their clients would make a video of some genetic test, and then upload it, and it was taking forever. They needed help.
To get a sense of their problem, I uploaded a bunch of 100MB 200MB and 500MB files to see the performance. You can see in the graph below, that the current upload performance seems to cap out at about 200M object size.
So if we’re uploading a bunch of 4 gig files (say, video editing) we need a better plan.
The answer to this problem smacks you in the face while using gsutil. Any time you try to upload a “large file” you’ll see the following message.
Breaking this down, gsutil can automatically use object composition to perform uploads in parallel for large, local files being uploaded to Google Cloud Storage. This process works by splitting a large file will into component pieces that are uploaded in parallel and then composed in the cloud (and the temporary components finally deleted).
You can enable this by setting the `parallel_composite_upload_threshold` option on gsutil (or, updating your .boto file, like the console output suggests)
gsutil -o GSUtil:parallel_composite_upload_threshold=150M cp ./localbigfile gs://your-bucket
Where `localbigfile` is is a file larger than 150 MiB. This will divide up your data into chunks ~150MiB and upload them in parallel, increasing upload performance. (Note, there’s some restrictions on the # of chunks that can be used. Refer to the docs for more information)
Here’s a graph showing 100 instances of uploading a 300MB file regular, and with composite.
Using parallel composite uploads presents a tradeoff between upload performance and download configuration: If you enable parallel composite uploads your uploads will run faster, but if you’d like to fetch the object using gsutil (or other python apps), then the client will need to install a compiled crcmod (see gsutil help crcmod) in order to download the file properly.
To be clear, this restriction for crcmod is temporary, and mostly there to protect the integrity of the data and ensure you your client doesn’t end up freaking out that things might look different. (CRC values and HTTP ETAG headers might show some difference.)
However, if this doesn’t work for your setup, you’ve got three options:
1) Maybe turn it off? Modify the `check_hashes` option of your config files to disable this step. NOTE: It is strongly recommended that you not disable integrity checks. Doing so could allow data corruption to go undetected during uploading/downloading.
2) Don’t use gsutil? To be clear, this isn’t an endorsed recommendation. However if you download the composite object using cURL, wget or http, then the fetch will work (you get the composited object). However, it’s strongly advised to still do crc checking, it’s just your responsibility to do it now.
3) Machine-in-the-middle? Another way to reduce this problem is to use a cloud-side instance to download the file (since crcmod can be installed there), and then re-upload it to a bucket in it’s entirety. To be clear, this takes time, and is more expensive (in terms of transaction costs), however this completely removes the crcmod restriction, and it might be a net-win, time wise, since GCP can easily get ~16 Gbits / sec upload speed from an internal VM.
For Medic7, putting CRCmod on each of their internal clients was not an issue, since uploaded videos had to be fast, and were then processed internally before being moved to another GCS bucket for distribution, so the machine-in-the-middle approach was almost de facto for them. The use of composite objects resulted in a 50% performance improvement for their clients, which is pretty great!
Google Cloud community articles and blogs
169 
2
169 claps
169 
2
Written by
DA @ Google; http://goo.gl/bbPefY | http://goo.gl/xZ4fE7 | https://goo.gl/RGsQlF | http://goo.gl/4ZJkY1 | http://goo.gl/qR5WI1
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
DA @ Google; http://goo.gl/bbPefY | http://goo.gl/xZ4fE7 | https://goo.gl/RGsQlF | http://goo.gl/4ZJkY1 | http://goo.gl/qR5WI1
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/flutter-community/mobile-app-that-speaks-with-deepmind-wavenet-google-cloud-text-to-speech-ml-api-and-flutter-48abf0992cb9?source=search_post---------210,"There are currently no responses for this story.
Be the first to respond.
You can also read this article in my Xcoding With Alfian blog website using the link below.
www.alfianlosari.com
Text to Speech synthesis is the process of text transformation into human speech audio using computer. Many Operating System had built the feature for text to speech since 1990s. The advancement in Machine Learning and Artificial Intelligence in recent years results in many new advance voice synthesis technologies such as WaveNet Deep Learning Neural Network from DeepMind. According to Google:
DeepMind has made groundbreaking research in machine learning models to generate speech that mimics human voices and sounds more natural, reducing the gap with human performance by over 50%. Cloud Text-to-Speech offers exclusive access to multiple WaveNet voices and will continue to add more over time.
Google has provided Cloud Text to Speech API as a GCP service for developers to use in their application and it also provides exclusive access to WaveNet voices from DeepMind. In this article, we are going to build a simple mobile App using the Cloud Text to Speech API where user can select the voices to use and the text to convert into speech. It uses Flutter so the app can run both on iOS and Android. You can access the project source code through the GitHub repository here.
To start using the Google Cloud Text to Speech API you need to sign up into Google Cloud Platform. Follow the quickstart guide in here, make sure to enable the API for your project as shown in the screenshoot below.
After enabling the Cloud Text to Speech API, we need to create API Key to use in our project when making REST call. Go to API then Credentials from the sidebar and generate one API Key for the project as shown below the screenshot below.
Our Flutter project pubspec.yaml will be using 2 external dependencies:
Google Cloud Text To Speech does not provide native client libraries to use for Android and iOS. Instead we can use the provided REST API to interact with the Cloud Text To Speech API. There are 2 main endpoints we will use:
TextToSpeechAPI class is the class we build to wrap the Cloud Text To Speech HTTP REST API with dart so we can provide public method for the client to perform the get voices and synthesise operation. Don’t forget to copy the API Key from the GCP console into the _apiKey variable. All the HttpRequest will set the X-Goog-Api-Key HTTP header with the value of the _apiKey.
The getVoices method is an async method that perform the GET HTTP request to the /voices endpoint. The response is a JSON containing the voices that we will map into list of simple Voice dart object. The Voice Class contains the name, languageCodes, and gender of the voice.
The synthesizeText method is an async method that receives text to synthesize, name and languageCode to use as the voice, and the audioConfig for the audio file. Based on the parameter we wrap it into a JSON object, for the audioEncoding we set the value to MP3. At last, we set the json object as a request body of the HttpRequest and perform the POST request to the /text:synthesize endpoint. The response will be a json object containing the audioOutput in Base64Encoded String.
The app consists only of one main screen where user can select the voice to perform synthesize using DropdownButton and TextField where they can enter their text. The synthesize process will be performed when the user tap on the FloatingActionButton located at the bottom right corner.
There are 4 internal states:
As the Widget is created, the getVoices async method is invoked, inside it uses the TextToSpeechAPI getVoices method to get the list of voices from the network. After the response is received, setState is invoked and _voices is assigned with the response to trigger widget render.
The Widget itself uses a SingleChildScrollView as the root widget. Then it uses the Column Widget to stacks the children vertically on its main axis. The first children is the DropdownButton that uses the _voices array and _selectedVoice as its datasource builder and selected value. The TextField widget is configured so it can accept multiline text.
When the FloatingActionButton is pressed, the onPressed callback check if the TextField is not empty and the selected voice is not null, then the synthesizeText async method is invoked passing the TextField text. Inside the method, the TextToSpeech API synthesizeText method is invoked passing the text, selected voice name, and the languageCode.
After the synthesize response is returned, we check if the response is not null. Because the response from Google Cloud Text To Speech API is a Base64Encoded String, we will decode it into bytes using the Base64Decoder class passing the string. After that we will use the path_provider getTemporaryDirectory and create a temporary file for the audio so we can play it using the AudioPlayer plugin passing the temporary file path.
Google Cloud Text To Speech API powered by WaveNet DeepMind is a really amazing technology that can be used to synthesise and mimic real person voice. There are many real world project that can use this technology to empower user experience and interaction with computer such as call center automation, iOT embedded devices that responds to user, and media text transformation to audio. The new era of speech synthesise has only just begun with the era of AI and ML. Happy Fluttering!
Articles and Stories from the Flutter Community
141 
2
141 claps
141 
2
Written by
Mobile Developer and Lifelong Learner. Currently building super app @ Go-Jek. Xcoding with Alfian at https://alfianlosari.com
Articles and Stories from the Flutter Community
Written by
Mobile Developer and Lifelong Learner. Currently building super app @ Go-Jek. Xcoding with Alfian at https://alfianlosari.com
Articles and Stories from the Flutter Community
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/simplifying-big-data-with-google-cloud-dataproc-9b3bf844b48e?source=search_post---------211,"There are currently no responses for this story.
Be the first to respond.
My former boss, Larry Ellison (Oracle's founder and current CTO) used to say that there is no ""Cloud"", that it's just a computer attached to a network. He was not completely wrong, but cloud computing is not only about that.
What are you talking about? It’s not water vapor. It’s a computer attached to a network!” — Larry Ellison, Oracle, 2009
Cloud computing, as defined by the National Institute of Standards and Technology (NIST), is composed by five essential characteristics:
Also, it defines three service models: Infrastructure as a Service (IaaS), Platform as a Service (PaaS) and Software as a Service (SaaS).
There is an interesting analogy on the web that I like to use to explain these service models. It is called ""Pizza as a Service"". Imagine you want to eat pizza… you actually have 4 different ways you can use to fulfill you desire:
The main advantage of using a cloud service is to be able to use a high end environment without having to pay for it's full price. Like, you don't need to purchase all the hardware and keep a staff just for wiring and supporting everything, while on-premise you have to pay the full price upfront even if you don't use it 24/7.
Also, you have the flexibility to upscale or downscale the resources as you need. With an on-premise setup you may never downscale.
Like most of the clouds out there, the Google Cloud offers all service levels, but for the rest of this article I'll be focusing on their data processing PaaS offer: Google Dataproc.
I've chosen this technology to talk about because there is no better example of complex environment than the Big Data technology stack. It soon will become evident the benefits of running this type of workload on the cloud.
But in order to understand Dataproc, first let's dig a bit deeper in the Big Data world.
Big Data, as the name implies, is about processing huge amounts of data. The first time the term appears on literature was at a paper written by Michael Cox and David Ellsworth from NASA, published in October, 1997:
We call this the problem of big data. When data sets do not fit in main memory (in core), or when they do not fit even on local disk. — Michael Cox and David Ellsworth, 1997
The term started gaining momentum after the publications of ""Google Filesystem"" in 2003 and ""MapReduce: Simplified Data Processing on Large Clusters"" in 2004, both papers written by Google's engineers. They described how did Google build an engine to process huge amounts of data using clusters made of commodity hardware.
The answer to those articles was the creation of Hadoop in 2006, an open source framework that implemented a distributed filesystem like Google's, called HDFS (Hadoop Distributed File System), and had the capabilities of running MapReduce workloads.
Besides being able to run workloads on a cluster, the MapReduce model had it's limitations because it implied that complex workloads would need to stage intermediate results on disk.
Running over Hadoop and designed to lift this disk staging limitations, rises Apache Spark as a new distributed computation framework. It uses the concepts of Resilient Distributed Datasets, datasets that exists only on memory, to allow the processing of iterative workloads without necessarily staging data to disk.
This change in architecture gave a performance boost of up to two orders of magnitude on the average case and soon Spark became the new gold standard of the industry.
But still, in order to run Spark workloads we need a setup similar to this one:
Spark is there in the middle, but there are so many other Big Data tools... It looks like a zoo so much that it even have a tool called ""Zoo Keeper"" to keep everything in place.
Seems like a lot of trouble, right? It is! But luckily we can now leverage Dataproc to do this crazy setup for us so we only need to worry about eating the pizza… I mean, running our workloads! :)
The sales pitch of Dataproc says we can setup an entire cluster in less than 90 seconds. If there's any company that can make a crazy statement like this is Google, but I must say that in my personal experience it sometimes took about 2 to 2 and half minutes to create it. But hey, look at all that craziness above, it's still pretty impressive!
Google Dataproc allows you to run Spark jobs from the command line (using the Google Cloud SDK's gcloud command) or the Cloud console. It also has the option to run jobs written in both Scala and Python (PySpark).
It also has connectors to other Google Cloud Platform products, like for instance, BigQuery. That means you can run a data processing or machine learning Spark job and output it to BigQuery for further analysis, or you can use BigQuery data as input to your workloads.
By the way, if you are interested in this scenario you should definitely check out the documentation: Using the BigQuery Connector with Spark.
In summary, by using Google Cloud Platform you can simplify the deployment of the Big Data stack to the level that with a few clicks and a couple of minutes you will have a cluster ready for processing your workloads. Also, if you ever need to scale it up or down it can be done in the same fashion, so you don't need to worry about long term sizing or paying for the infrastructure upfront.
GCP is definitely the way to go if you want to play around with Big Data without paying high entry costs. It allows practically any kind of company to have access to the disruptive power of Big Data.
P.S.: This article is a modified version of my talk on Google Cloud Platform given at Google Launchpad Build Porto Alegre on August 5th, 2017.
Google Cloud community articles and blogs
144 
144 claps
144 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Executive Director at JPMC, Google Developer Expert: Go & GCP, TEDx speaker, blogger, traveler(?) and cat lover =^.^=
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/serverless-a-journey-to-a-no-ops-data-architecture-on-google-cloud-a3b019a27799?source=search_post---------212,"There are currently no responses for this story.
Be the first to respond.
Serverless is really a new and fancy term in the data world. A promise of minimum worries in infrastructure (no-ops) and pay only for what you use seems a dream, that invites me to go further and review all the serverless products available on GCP and then build an architecture that covers the basics of Data Orchestration, Data Processing, and Data Warehousing for a simple project.
To keep this architecture reproducible I’ll be using the free COVID dataset taken from Google…
"
https://medium.com/google-cloud/accessing-google-cloud-apis-though-a-proxy-fe46658b5f2a?source=search_post---------213,"There are currently no responses for this story.
Be the first to respond.
> updated on 12/3/21
Proxy servers are not uncommon and I found some of my customers accessing Google Cloud APIs thorough them.
Most of the time, its a simple forward proxy with no authentication and never with SSL interception (which IMHO, is a really questionable, see link)
Fortunately, many of our APIs piggyback off of the native language proxy configuration settings (eg. https_proxy env variable) so that part makes it easier.
This article describes the various proxy configurations you can use while accessing google APIs and I thought i’d consolidate some of my findings and issues with configuring google libraries into a doc to share.
For background, please see the two types of libraries available to access Google APIs though in this article, the focus is google cloud client libraries.
The recommended library set to use is the idiomatic “Cloud Client Library” which itself comes in two different transport mechanisms: HTTP and gRPC. At the time of writing (10/17), some APIs support both transports, some HTTP only (GCS, BigQiery), while other gRPC (PubSub). How users enable and configure proxy support is slightly different for these transports.
One other complication to account for is that the authentication step (i.,e getting a GoogleCredential()) uses HTTP even if the underlying RPC is using gRPC. For example, with java, you have to account for the credential via HTTP and the rpc call via gRPC.
The following lists the tests i ran using a squid proxy below. Each tests verifies two modes of for the proxy
within those, i tested Application Default Credentials while a user was active and ADC when an service account was active
As you’ll see, some languages uses the proxy for all traffic, some partial traffic (ie., auth or GCS is omitted), while some reuire a lot of fiddling
The outcome of each test and permutation is described in the comments in code…which means, to use this, you need to read the comments and understand which mode you are using (basic auth enabled proxy or not).
For most uses, you don’t have to have basic auth enabled. If you need a proxy server to test with , see the appendix or here
my ask: If you have any updates or fixes, please file an bug or better yet, a PR.
You can find the full source here
with out basic
with basic
without basic
with basic
If you need a local, containerized Squid proxy in various modes to test with, please see the following gitRepo and image
To use this image, simply enter a shell:
for a basic proxy, run from within the shell:
Your can verify the proxy is being used by running the following in a shell command
to view the accesslogs in the container:
Note this is only for amusement!
start the proxy server dockerfile with HTTPS intercept:
on your laptop, setup virtualenv:
edit the two files and disable SSL checks verify=False (you can also set the REQUESTS_CA_BUNDLE env variable as described here..)
Create main.py:
then export the proxy env var
and run the sample from withing the virtualenv
the access logs now shows actual path requested (within the SSL session!)
Google Cloud community articles and blogs
150 
5
150 claps
150 
5
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://netflixtechblog.com/netflix-security-monkey-on-google-cloud-platform-gcp-f221604c0cc7?source=search_post---------214,"Today we are happy to announce that Netflix Security Monkey has BETA support for tracking Google Cloud Platform (GCP) services. Initially we are providing support for the following GCP services:
This work was performed by a few incredible Googlers with the mission to take open source projects and add support for Google’s cloud offerings. Thank you for the commits!
GCP support is available in the develop branch and will be included in release 0.9.0. This work helps to fulfill Security Monkey’s mission as the single place to go to monitor your entire deployment.
To get started with Security Monkey on GCP, check out the documentation.
See Rae Wang, Product Manager on GCP, highlight Security Monkey in her talk, “Gaining full control over your organization’s cloud resources (Google Cloud Next ‘17)”:
We released Security Monkey in June 2014 as an open source tool to monitor Amazon Web Services (AWS) changes and alert on potential security problems. In 2014 it was monitoring 11 AWS services and shipped with about two dozen security checks. Now the tool monitors 45 AWS services, 4 GCP services, and ships with about 130 security checks.
We plan to continue decomposing Security Monkey into smaller, more maintainable, and reusable modules. We also plan to use new event driven triggers so that Security Monkey will recognize updates much more quickly. With Custom Alerters, Security Monkey will transform from a purely monitoring tool to one that will allow for active response.
More Modular:
Event Driven:
Custom Alerters:
We’ll be following up with a future blog post to discuss these changes in more detail. In the meantime, check out Security Monkey on GitHub, join the community of users, and jump into conversation in our Gitter room if you have questions or comments.
We appreciate the great community support and contributions for Security Monkey and want to specially thank:
By: Patrick Kelley and Mike Grima
Learn about Netflix’s world class engineering efforts…
154 
154 claps
154 
Written by
Learn more about how Netflix designs, builds, and operates our systems and engineering organizations
Learn about Netflix’s world class engineering efforts, company culture, product developments and more.
Written by
Learn more about how Netflix designs, builds, and operates our systems and engineering organizations
Learn about Netflix’s world class engineering efforts, company culture, product developments and more.
"
https://medium.com/google-cloud/hosting-web-applications-on-google-cloud-an-overview-46f5605eb3a6?source=search_post---------215,"There are currently no responses for this story.
Be the first to respond.
“Get Cooking in Cloud” is a blog and video series to help enterprises and developers build business solutions on Google Cloud. In this series we plan on identifying specific topics that developers are looking to architect on Google cloud. Once identified we will create a mini series on that topic.
In this first mini series we will cover, how to create websites on Google Cloud. If your business revenue and customer satisfaction depends on the availability and scalability of your website, then you are at the right place. In the next few blogs, we will elaborate on creating websites:
In this article, we will see four tools that help you scale from small to large website, depending on the needs. So, read on!
At a high level, there are four recipes to build a website or an application on Google Cloud. Depending on where you are in your Google Cloud journey, your business needs, and the maturity of the development and infrastructure team, one of these options should fit your needs.
Static websites are a good option for sites like blogs — where the page rarely changes after it has been published, or where there isn’t any dynamically-generated content. All you need to set up a static website on Google Cloud is a cloud storage bucket connected to your domain name and that’s it!!
As your business starts to mature and customers are interested in buying things from your website, you might need to generate dynamic content and enable payments. But if your company is still small, you want to be able to grow your website without worrying about scaling the website based on the increase in demand.
In such a scenario, Google Cloud’s managed and serverless offerings like App Engine or Cloud Run would be apt, this allows you to focus on delivering features and let Google worry about operating and managing the infrastructure.This provides a wide range of features that make scalability, load balancing, logging, monitoring, and security much easier than if you had to build and manage the website yourself.
When you use Cloud Run, you can code in any programming language, because your application is deployed as a container, and Google will seamlessly launch and scale your application for you. So give it a try!
For websites with higher complexity, you probably want more options and control than a managed platform offers. Whether it’s configuring your servers or virtual machines, or if it’s a need for specific memory, SSDs, and GPUs, it makes sense to use Compute Engine.
Compute Engine provides a robust computing infrastructure, but you must choose and configure the platform components that you want to use. Google ensures that resources are available, reliable, and ready for you to use, but it’s up to you to provision and manage them.
The advantage of using compute engine is that you have complete control of the systems and unlimited flexibility. The easiest way to deploy a complete web-serving stack on Compute engine is by using Google Cloud Marketplace. With just a few clicks, you can deploy any of over 100 fully-realized solutions.
For a more detailed explanation of how to set up Compute engine to serve scalable and resilient websites, stay tuned for upcoming articles.
Finally, for a larger business with more developers and more complicated problems, it makes sense to containerize your application. You will notice that it becomes really hard to manage feature roll outs if the website is one big monolith, which makes it difficult to keep up with the increase in demand and pace of business.
Containerizing web applications provides three key advantages:
Using Container to deploy web apps on GKE has further advantages because:
Using Kubernetes Engine makes sense in following situations:
Whether you are a small blogger looking to grow your community, or a huge, multi-scale eCommerce site, hopefully this has been helpful in identifying which tools within the Google Cloud Platform are right for your specific web use case.
Google Cloud community articles and blogs
173 
2
173 claps
173 
2
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Developer Advocate @Google, Artist & Traveler! Twitter @pvergadia
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/@sathishvj/notes-from-my-google-cloud-professional-devops-engineer-certification-exam-60d23aca37f5?source=search_post---------216,"Sign in
There are currently no responses for this story.
Be the first to respond.
sathish vj
Nov 2, 2020·7 min read
Subscribe to my YouTube channel that teaches you to apply Google Cloud to your projects and also prepare for the certifications: youtube.com/AwesomeGCP. Check out the playlists I currently have for Associate Cloud Engineer, Professional Architect, Professional Data Engineer, Professional Cloud Developer, Professional Cloud DevOps Engineer, Professional Cloud Network Engineer, and Professional Cloud Security Engineer.
Taking this exam showed me that it becomes easier the more time you spend learning. I was originally supposed to take this exam around Feb/March 2020 and had started preparing then. But then the pandemic hit and I couldn’t go to the test center. When there was news that the exam is going to be available online, I prepared a little again but I couldn’t take the exam because I got extremely busy with my work. Finally, I was able to take it a few days ago. I didn’t take much time to prepare this time — just skimmed parts of a Coursera Google Cloud course (see git repo here), did a Qwiklabs lab or two, read through a handful of docs. And I passed! I won’t say it is an easy exam, but I was able to get through without much immediate effort since I had already put in effort towards it multiple times.
I refactored this section into a separate post here because it is applicable to all exams: https://medium.com/@sathishvj/taking-the-google-cloud-certification-online-exam-a8d5a8d18550
As I mentioned, I had prepared for this a couple of times. The first time around, I created videos of the practice questions on my YouTube Channel (AwesomeGCP). I reviewed all of them again, and that really helped. I also skimmed over a more recent course on Coursera that I hadn’t fully gone through before. I read through a few posts written by others to see what they’d faced. I’ve collected all those links in this git repo: https://github.com/sathishvj/awesome-gcp-certifications
And now, based on the exam areas that I encountered, here are some of the topics that I too recommend that you study based on the study guide.
2. Building and implementing CI/CD pipelines for a service
3. Implementing service monitoring strategies
4. Optimizing service performance
5. Managing service incidents
For those appearing for the various certification exams, here is a list of sanitized notes (no direct question, only general topics) about the exam.
Overall notes across all GCP certification exams
Notes from the Professional Cloud Architect exam
Notes from the beta Professional Cloud Developer exam
Notes from the Professional Data Engineer exam
Notes from the Associate Cloud Engineer exam
Notes from the beta Professional Cloud Network Engineer Exam
Notes from the beta Professional Cloud Security Engineer Exam
Notes from the Professional Collaboration Engineer Exam
Notes from the G Suite Exam
Notes from the Professional DevOps Engineer Exam
Notes from the Professional Machine Learning Engineer Exam
A collection of posts, videos, courses, qwiklabs, and other exam details for all exams: https://github.com/sathishvj/awesome-gcp-certifications
I’ve been making videos on applying Google Cloud and preparing for the exams. You can subscribe to the channel here:
www.youtube.com
Check the FAQs here: https://medium.com/@sathishvj/frequently-asked-follow-up-questions-on-google-cloud-gcp-certifications-438e1addb91d.
Wish you the very best with your GCP certifications. You can reach me at LinkedIn and Twitter. If you can support my work creating videos on my YouTube channel AwesomeGCP, you can do so on Patreon or BuyMeACoffee.
tech architect, tutor, investor | GCP 12x certified | youtube/AwesomeGCP | Google Developer Expert | Go
76 
76 
76 
tech architect, tutor, investor | GCP 12x certified | youtube/AwesomeGCP | Google Developer Expert | Go
"
https://blog.kovalevskyi.com/how-to-train-a-chatbot-with-the-tensorflow-and-google-cloud-ml-3a5617289032?source=search_post---------217,"There are currently no responses for this story.
Be the first to respond.
In our previous article we discussed how to train the RNN based chatbot on a AWS GPU instance. Today we will see how we can easily do the training of the same network, on the Google Cloud ML and with the Google Cloud Shell. With the Cloud Shell you will not need to do literally anything on your local machine! For the sake of the experiment, exactly the same network as we did in our previous article will be used. However, one can use any TensorFlow network.
Special thanks to my patrons from my Patreon page who made the article possible:
Aleksandr Shepeliev, Sergei Ten, Alexey Polietaiev, Никита Пензин, Igor German, Mikhail Sobol, Карнаухов Андрей, Sergei, Matveev Evgeny, Anton Potemkin.
Even though the material in the article is self-contained, I am strongly advising you to go through each and every link in order to understand what is going on under the hood.
There is only one pre-requirement that the reader must satisfy in order to perform all the steps here: to have a Google Cloud Account with billing enabled.
Let’s start our journey by answering 2 main questions:
In order to understand what it is, let’s look into the official definition:
Google Cloud Machine Learning brings the power and flexibility of TensorFlow to the cloud. You can use its components to select and extract features from your data, train your machine learning models, and get predictions using the managed resources of Google Cloud Platform.
I don’t know about you, but for me, this definition does not say much. So, let me explain what it actually can do for you:
The focus of the current article will be primarily on the first 3 items. In the following articles, we will see how to deploy the trained model to the Google Cloud ML and how to predict data using your cloud-hosted model.
Again, let’s start with the official definition:
Google Cloud Shell is a shell environment for managing resources hosted on Google Cloud Platform.
And, again, it does not say much to me. So, let me explain what this is actually about. The Cloud Shell is, basically, a provisioned instance in the cloud that:
Yes, you got it right, you have a 100% free instance with the shell, that you can have access to from anywhere via the Web.
But nothing comes for free, in the case of the Cloud Shell — one can access it only via the Web with some limitations (personally, I hate to use any other terminals but iTerm). I have asked on the StackOverflow if it is possible to use the Cloud Shell with your own terminal. But for now, there is a way to make your life easier by installing the special Chrome plugin that, at least, enables a terminal friendly key bindings.
You can read more about Cloud Shell features here.
The overall process includes following steps:
Now, it’s the best time to open the Cloud Shell. It is super easy, you just need to:
In case of any problems, here is a small page, that describes how to start the Shell with more details.
All the following examples will be executed in the Cloud Shell.
Also, if this is the first time when you are going to use the Cloud ML with the Cloud Shell — you need to prepare all the required dependencies. That can be done by just executing the one-liner:
It will install all the required packages. Also, you need to update your PATH variable:
One can check whether everything is installed successfully or not by running one simple command:
Now it’s time to decide which project you will use in order to train the network. I have the dedicated project for all my experiments with the ML. Anyway, it’s up to you, to choose the project.
Let me show you my commands that allow me to switch between projects easily:
Now, if you would like to use the same magic, here is what you need to add to your .bashrc/.zshrc/other_rc file:
Okey, so now we have prepared the Cloud Shell and have switched to the desired project. What next? If this is the very first time when you are using the Cloud ML with the project, you will be required to initialize it. And again, this can be done with just one line:
Finally, the Cloud Shell can be considered as one that has been prepared. We can move on to the next step.
First of all, let me explain, why would we need cloud storage? Since we are going to train the model in the cloud it will not have any access to the local file system of your current machine. It means that all the required input needs to be stored somewhere in the cloud. As well as, we will need to store the output — somewhere.
Let’s create a brand new bucket that would be used for the training:
Here I need to tell you something, if you look on the official guide you will find the following text:
Warning: You must specify a region (like us-central1) for your bucket, not a multi-region location (like us). Learn more in the development environment overview.
However, if you try to use the regional bucket instead of a multi-regional, the script will fail to write there anything (do not worry, the issue has been filed).
In the perfect world with ponies where everything works as expected, it is very important to set the region here, and the region should be consistent with the region that will be used during the training. Otherwise, it might have a negative impact on a speed of the training.
Now we are ready to prepare the input data for the training.
This time (compared to the previous article) we will use a slightly modified version of the script that prepares the input data. I would encourage you to read how the script is working and what it is doing in the README. But for now, here is how you can prepare the input data (you might replace “td src” with “mkdir src; cd src”):
One might be wondering, what is “td”? It is actually a short form of “to dir” and it’s one of the most frequently used commands. In order to use it you need to update your rc file with the following code:
This time we will improve quality of our model by splitting our data set into 2 groups: training and test. That’s why we can see 4 files instead of 2, how it was during the process described in the prev article.
Ok, now we have data, let’s upload it to the bucket:
At this moment, we can prepare the training script. We will use the translate.py, even though, the current implementation does not allow to be used with the Cloud ML, so we will need a small refactoring. As usual, I have created a feature request and provided you with a branch with all the required changes. Let’s clone it:
Again, pay attention that we are not using the master branch!
Since the remote training costs money it might be a good idea to test the training locally. The problem here is that local training of our network definitely will kill the Cloud Shell instance. And you would have to restart it. Do not worry, nothing will be lost in the case if this happens, but still, it’s probably not something that we would want. Luckily, our script includes a self testing mode that we can use. Let’s start the training locally in a self-test mode:
Pay attention to the folder from which we are executing the command.
Looks like that a self-test has been finished successfully. Let’s talk a little bit about keys that we are using here:
This is the most exciting part. Before we start a training we need to prepare all the required buckets that will be used during the process and set all the local variables:
The job’s name needs to be unique each time when we start the training. Now let’s change the current folder to translate (do not ask =) ):
At this moment we are ready to start the training. Let’s first create a command that we will need to execute in order to discuss the details, before the actual execution:
Let’s first discuss some new flags of the training command:
Now let’s discuss the new flags that will be passed to our script:
Looks like everything is set to actually start the training, so let’s rock(be patient, it will take some time for the execution to be started)…
You can monitor the state of your training. You just need to open another tab in your Cloud Shell (or the tmux window), create the required variables:
Now we can actually stop the job and restart it with the default amount of steps per checkpoint (200). The updated command should look like:
Since you can start using the latest checkpoint from any other machine without interrupting or impacting the training process, this is probably the biggest advantage of using the Cloud Storage.
Now, for example, I’m going to show you how to chat with the bot, that is still bing trained, after only 1600 iterations.
Also, this is the only step that can’t be done in the Cloud Shell. Why? If you are really asking me — Cloud Shell was never designed to run a heavy task on it, so it will gloriously die with honor and with the OutOfMemory error.
Okay, so here is how you can start chatting from your local machine:
The TRAIN_PATH should point to the “tmp_data” folder, and you need to be in the “models/tutorials/rnn” folder of the model repo.
As you can see chatbot is not super smart after 1600 iterations. If you want to know how the conversation will look like after 50+k iterations please have a look at the prev article, since the main purpose of this one was not to train the perfect chatbot, but to explain how to do it, so anyone can train it better.
I hope that my article helped you to learn what the Google Cloud ML is and how you can use it in order to train your own NN. I also hope that you have enjoyed reading it, and if so, you can support me on my Patreon page and/or by liking/sharing the article.
If you have spotted any problems while executing the steps, please write a comment to me so I could update the article so no one else would have the same problem.
last modified: Jan 10 2017
Articles about the #DeepLearning (mostly Google Cloud with…
59 
4
59 claps
59 
4
Written by
#ML/#NLP researcher. Making Things Work! #TensorFlow. In the past #MXNet. All opinions are my own.
Articles about the #DeepLearning (mostly Google Cloud with #TF).
Written by
#ML/#NLP researcher. Making Things Work! #TensorFlow. In the past #MXNet. All opinions are my own.
Articles about the #DeepLearning (mostly Google Cloud with #TF).
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://towardsdatascience.com/hyperparameter-tuning-on-google-cloud-platform-with-scikit-learn-7d6155195efb?source=search_post---------218,"Sign in
There are currently no responses for this story.
Be the first to respond.
Rob Salgado
May 20, 2019·10 min read
Google Cloud Platform’s AI Platform (formerly ML Engine) offers a hyperparameter tuning service for your models. Why should you take the extra time and effort to learn how to use it instead of just running the code you already have on a virtual machine? Are the benefits worth the extra time and effort?
One reason to use it is that AI Platform offers Bayesian Optimization out of the box. It is more efficient than a grid search and will be more accurate than a random search in the long run. They also offer early stopping…
"
https://medium.com/google-cloud/calling-google-cloud-machine-learning-apis-from-batch-and-stream-etl-pipelines-9a789ac6f972?source=search_post---------219,"There are currently no responses for this story.
Be the first to respond.
Google Cloud AI has some very handy “building block” APIs for Natural Language Processing, Vision (e.g. OCR, image classification, logo identification, etc.), and Video Intelligence.
You will often to need call these APIs on a bunch of documents, images, or videos. Sometimes, this is on already collected data (“batch processing”) and sometimes, it is on streaming data. Invoking online APIs one-at-a-time from batch and stream pipelines requires quite a bit of care so that you don’t hit networking, throughput, or throttling limits.
Fortunately, Apache Beam 2.20 now provides a handy PTransform that does all the heavy lifting for you. To use it, first install Apache Beam:
Here’s a complete program that will run Apache Beam on three sentences locally:
These are the steps in the above pipeline:
The output of the NLP API looks like this:
That’s why I’m able to extract the pieces I want as response.sentences[0].text.content and response.document_sentiment.score. Here’s what the output of the pipeline looks like:
In the above snippet, I ran the pipeline on an in-memory set of sentences and used the DirectRunner, which runs locally. Let’s change the input to BigQuery and run it in Cloud Dataflow:
https://gist.github.com/lakshmanok/a07d488a0b8006c26bdee0a7effd6245
The pipeline now runs on Dataflow off the hackernews comments table in BigQuery:
The result looks like this:
How easy was that, to get the sentiment of a boatload of comments?
Enjoy!
Google Cloud community articles and blogs
83 
83 claps
83 
Written by
Data Analytics & AI @ Google Cloud
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Data Analytics & AI @ Google Cloud
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://rominirani.com/google-cloud-functions-tutorial-setting-up-a-local-development-environment-8acd394a8b76?source=search_post---------220,"This is part of a Google Cloud Functions Tutorial Series. Check out the series for all the articles.
In this post, we shall take a look at setting up a local development environment for Google Cloud Functions.
The set of tools that we shall deploy will help us in multiple areas like development, managing our Cloud Functions deployment, Local Testing and Debugging and more.
It is advisable that you have the following tools setup to enable you to make best use of all the posts present in this tutorial series since some of the posts later on in the series will assume that you have these tools installed.
Keep in mind that at the time of writing, Google Cloud Functions support multiple runtime environments that allow you to write your Cloud Functions in JavaScript, Python or Go (Alpha). We are using JavaScript in this tutorial and depending on Node.js v6 or Node.js v8 that you prefer, you will need support for that in your local environment.
The first step is to ensure that you have a Google Cloud Platform account with Billing setup. Remember that there is an always free tier when you sign up for Google Cloud and you could use that too.
The Always Free tier in Google Cloud Platform is extremely generous when it comes to Google Cloud Functions. Take a look at the following:
Not bad, isn’t it ?
Optional: If you have not created a Google Cloud Platform project, I suggest to create a new Cloud Project under which you can have all the examples covered in this series. Create the New Project from the Cloud Console and fill out the details for your project. A sample screenshot is shown below.
Please note down your project-id, which is unique across all Google Cloud Platform projects
Once you have setup your project, the next step is to enable the Google Cloud Functions API for your project. From the Cloud Console, click on the Main Menu in the Top Left corner and navigate to Menu → APIs and Services → Dashboard as shown below:
Click on Enable APIs and Services. This will bring up a screen as shown below, where you can enter Cloud Functions in the search field.
Click on Google Cloud Functions API and then click on Enable. This will enable the API for use in your Google Cloud Platform project.
Let us take a look at an overall diagram that explains the tools that we will use across this set of tutorials:
You would be familiar with the Google Cloud Console (Web UI) that you use to manage services across Google Cloud Platform. In addition to using that, we can use the gcloud SDK , that we can deploy on our local machine to both deploy/monitor/manage our Google Cloud Functions directly. Finally, we setup our Local Development Environment in the next few sections.
Google Cloud SDK is a set of tools to interact with various Google Cloud Platform Services. It is available to use from the Command Line (CLI).
Go ahead and install this via a download from https://cloud.google.com/sdk/ . Choose a download for your platform and go ahead with the standard setup.
Once installed successfully, you can try out the functions component via some sample calls. Google Cloud Functions are available via gcloud functions command.
Try the following to list out the Google Cloud Platform regions in which Cloud Functions is currently available:
Now that we are set with gCloud SDK, we need to configure our Google Cloud Platform Project to use with the gCloud SDK. To do that, you need to execute the following command:
Do a login first via:
Finally, set the project id via the following command. Please use your project id instead of the PROJECT_ID value below.
Finally, validate that you are logged in with your project and the project id is also correctly setup via the following command:
Check the accountand projectvalues and ensure that they were set correctly.
For your local development environment, you will need to have the Node and npm tools setup. Please install them from https://nodejs.org/en/download/ and install it for your environment.
On my machine, I have the following setup:
Note: At the time of writing, a couple of Node.js runtime versions are supported on Google Cloud Functions : v6.14.0 and v8.11.1 , so you should make sure to have the appropriate Node.js version on your local machine.
Once you have Node.js installed, you need to setup the Local Functions Emulator. This is a tool made available by Google to help you test your functions locally without the need to deploy to the live environment every time. We will cover this in a post later in the series. For now, install the Local Functions Emulator via the command given below:
Finally, I have Visual Studio Code on my machine. It is a great and fast IDE that helps me develop and test stuff locally. I recommend it but you are free to use an IDE of your choice.
That completes our setup for our local development and management of Google Cloud Functions.
Proceed to the next part : Writing Background Cloud Functions or go back to the Google Cloud Functions Tutorial Home Page.
Technical Tutorials, APIs, Cloud, Books and more.
94 
4
94 claps
94 
4
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
Technical Tutorials, APIs, Cloud, Books and more.
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
Technical Tutorials, APIs, Cloud, Books and more.
"
https://medium.com/google-cloud/google-cloud-storage-exploder-221c5b4d219c?source=search_post---------221,"There are currently no responses for this story.
Be the first to respond.
A colleague asked whether Cloud Functions could provide unzip functionality to Cloud Storage. Another colleague pointed me to a solution — similar in principle — for Firebase Functions that provides image transformation; he also suggested “exploder” as a more evocative title ;-)
My interest was piqued and I developed the following as a proof-of-concept (it works) but I urge you to not use this in production.
It’s an excellent and quite common request from customers that Google Cloud Storage (GCS) should include common file processing tools. The goal would be that these transformations happen service-side to save the complexity of downloading an object from GCS, processing it and returning the result(s) to GCS. For those that are not familiar with GCS, it’s a service in Google Cloud Platform (GCP) that provides effectively limitless “object” aka “BLOB” aka “unstructured” file storage.
You may get started for free with Google Cloud Platform but I suspect, if you’re reading this, you’re likely already a beloved GCP customer and may be seeking a template solution for this problem. Read on, dear customer.
You’ll need a GCP project, at least 2 GCS buckets, and Cloud Functions enabled:
Customarily, I’m all command-line but I find the Console experience for Cloud Functions to be mostly excellent and I find myself preferring it. Let’s do this both ways, starting with the Console:
Click “Create Function” and
For convenience, I’m going to use the inline editor.
Replace index.js with:
NB Replace [[YOUR-ROOT]] in lines 13+14 with the value of ${ROOT}. Or whatever names you used to create the buckets.
and replace package.json with:
Click “Create” to deploy the Cloud Function to GCP.
All being well:
Our Cloud Function should be triggered on *all* object changes to our GCS “receive” bucket. You may read more about so-called Object Change Notification (OCN) here. In practice, our Function should be more prudent in filtering the OCNs for those that are intended for our exploder.
I recommend you file a reasonably small zip file (${TESTZIP}) to test.
and, to confirm you can run the following command although the cp command should provide sufficient confirmation of success or failure.
Browsing GCS buckets and objects is facilitated with the Console’s Browser:
which should show something similar to:
If you drill into the “receive” bucket, you should see your uploaded ${TESTZIP} file:
And, if you wait a few seconds and navigate to the “explode” bucket, you should see:
GCS stores objects in a flat namespace (one per bucket). As a convenience, GCS presents objects that contain / in their object name as if these formed a conventional directory hierarchy. In the code, you will see that I prefixed unzipped files with a Linux epoch value (in this case 1510952315489) and a /. The Browser presents the bucket’s contents as if there were a directory called 1510952315489 (your value will differ but it will be unique and corresponds to the epoch value) but the implementation is that all these image files image-1.jpg, image-2.jpg are actually named: 1510952315489/image-1.jpg, 1510952315489/image-2.jpg…
That worked…. my test.zip in the “receive” bucket is now exploded in the “explode” bucket.
And, you should see our console.log output in the Cloud Logging logs:
and you can enumerate the bucket using the gsutil CLI:
Cloud Functions is fully supported by the Cloud SDK (“gcloud”). Assuming you have index.js and package.json in the current directory. You do *not* need to npm install the packages. Just the two files:
Hopefully this will trigger some ideas around other uses of Cloud Functions. Once again, this is a proof-of-concept only and needs work before it would be usable for anything more.
You can delete Cloud Functions individually:
You may delete buckets after recursively delete all their objects — please be VERY careful using this command:
Alternatively you can simply delete the project which will delete everything within it too:
Thanks!
Google Cloud community articles and blogs
111 
7
No rights reserved
 by the author.
111 claps
111 
7
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/getting-the-best-out-of-google-cloud-shell-3d6ca64bc741?source=search_post---------222,"There are currently no responses for this story.
Be the first to respond.
When it comes to accessing google cloud resources or giving ssh access or doing some actions on cloud, we think of creating a standalone VM aka jump server. Running dedicated instance or getting full access to local system could also be option. And what about mobile when we need quick access? All these hurdles can be avoided by one solution: Google Cloud Shell.
Google Cloud Shell is a command line machine which is available for every google cloud account user for free and provides shell access from browser from where we can access and manage cloud resources and projects. This secured shell with 5GB storage is pre-installed with favorite tools like: MySql client, Kubernetes, and Docker. The shell is account wide so by staying on same shell, we can connect to cloud resources of multiple project with access.
Let’s see how you can make life easier by getting the best of Google Cloud Shell.
The machine pre-loaded with debian starts in few seconds in the same browser by clicking on “Activate Cloud Shell” icon on top right of the google cloud console. If you haven’t been using the shell since long time or trying out for first time, it might take half a minute or so. The shell has built-in authorization with gcloud credential, so you can access any cloud resources for which you have IAM access in any project.
Google Cloud Shell is not limited to browser access but from any gcloud sdk installed and account activated terminal. Authenticate your local system with gcloud IAM and access the google cloud shell:
From the Cloud Shell, one with SSH access can easily connect to compute engine instance using gcloud command.
Even if the compute instance is not assigned with public ip, the instance can be accessed through cloud shell.
Its not only shell but is a real tool for debugging. We can start any service or run docker and preview the output on nice URL with “Web Preview”.
By running multiple service on background or on different tab of the shell, we can preview them on different port.
By default, cloud shell runs on g1-small machine type which provides 1 vCPU short periods of bursting and 1.70 GB memory. Sometimes, this isn’t enough if you have to work with multiple tabs and run few services.
The newly introduced power boosting feature of cloud shell allows upgrade of the shell n1-standard-1 VM instance which offers 1vCPU and 3.75GB memory for 24 hours.
To activate the boost mode, click ‘Enable Boost Mode’ option under the ‘More’ menu of the cloud shell.
Google Cloud Shell’s integrated code editor based on Theia IDE makes it easy to edit files and folders in the shell from browser.
Clicking on Launch Editor(edit icon) on the top of the cloud shell opens a new tab on browser with all access to files and folder in the root of shell.
Also, we can run VS Code with Cloud shell. By simply running cdr/code-server through docker or shell inside cloud shell, we can preview nice VS code UI on http port.
Whether it be creating mysql user, database or any operation on Cloud SQL instance, using Cloud Shell makes it easier.
The cloud shell access is not limited from web browser and computer but also from mobile device. We can connect to shell from Cloud Console Mobile App apart from getting alert notification, manage GCP resources and access compute engine instances.
Deploying containerized serverless application to Google Cloud Run button is possible with cloud shell. The newly introduced “Run on Google Cloud” button makes it super easy to deploy new image to cloud run service.
Read more about the application structure for the setup from GoogleCloudPlatform/cloud-run-button repo.
Apart from above usage of cloud shell, we can use it for deployment to cloud function, GKE cluster, App Engine and other services. The Google Cloud Shell doc might interest you to explore more.
Happy Cloud Shell-ing !!
Get connected with me on Twitter and Linkedin where I keep on sharing interesting things.
Google Cloud community articles and blogs
73 
73 claps
73 
Written by
DevOps | SRE | #GDE
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
DevOps | SRE | #GDE
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@wilsonhuang/%E5%8F%83%E5%8A%A0-google-cloud-onboard-2017-%E6%98%AF%E6%80%8E%E6%A8%A3%E7%9A%84%E9%AB%94%E9%A9%97-f7820c8d202c?source=search_post---------223,"Sign in
There are currently no responses for this story.
Be the first to respond.
Wilson Huang
Apr 20, 2017·8 min read
昨天出席了一整天的 Google Cloud Onboard 的課程，獲得了一張證書和 T-shirt，還有一些使用 GCP 入門的知識和概覽，按照慣例要來寫點心得和介紹。
這篇文主要還是描述我參加整場活動的一些體驗和想法，可能沒辦法詳細介紹整場議程所教的課程內容，有些詳細的技術名詞可以看 iThome 的這篇文和那篇文。我也不是雲端或 DevOps 神人，用到什麼知道什麼而已，偶爾聽到一些名詞會去查查大概是做什麼的，僅此而已。
所以這場課程教的內容，我會的大概也就 30% 差不多，其他的 30% 則是「哦！以前常常看到，原來是這樣！」，最後 40% 大概是「哇這看起來好厲害，但還是不確定是什麼東西或能做什麼用？」
前一天寄信來說，似乎因為報名的人數爆多，原本早上九點報到改成八點就可以入場，所以我大概八點半就到達。報到流程我覺得設計得不錯，似乎有國際會議中心的專業幫忙，有好幾台名牌列印機，輸入報名得到的序號後，下一秒名牌就印出來，工作人員把書、吊牌、名牌貼紙跟午餐卷直接給你，整個流程不到 10 秒，比上次我去的 FbStart 快很多，因為 FbStart 的名牌不是當場列印，是已經做好精美的厚紙板，所以還要從一大疊名牌中花時間尋找。FbStart 的名牌可以帶回家紀念，而 GCP 的結束時會收回，也是有利有弊。
可能是我大學時期辦太多種活動，其中也不乏一些技術相關的，所以每參加一場活動都會從一開始就觀察它們的整個流程、動線設計、以及有沒有準備開發者喜歡的食物和紀念品等等，這場當然也是沒讓我失望，有吃有喝也有衣服貼紙可以拿，相當不錯了！
當然也不是只關注吃、喝、紀念品這麼膚淺，這些只是最基本的參考點而已，活動最重要的當然是內容。整天的主題圍繞在 Google Cloud Platform 上面，從早上的平台介紹，以及 Google 最近的發展方向，為何往這方向發展，取得了哪些進步等等。
圖片為 Eric Schmidt 講的，另外一張是 Google CEO 說 Google 從 Mobile First 轉變為 AI First，一切戰場都轉向 Machine Learning、Deep Learning、AI 相關領域的服務應用。照片中開場人叫做田哲禹，據說是負責在台業務的經理，分享了一些 Google 最近 ML 的進展：用 ML 幫著判斷人類的眼球糖尿病病變，那是人眼以往不容易發現的區塊；一家公司 Airbus 也透過 Google Cloud Platform 的 ML 服務提高了不少的 Accuracy；Google 自己的Data Center 也透過 ML 減碳了 40%，篩出一些不必要的浪費。大公司當然也要說一下自己有在做環保。
接著就繼續講說 Google Cloud Platform 為何如此強？為何不只會場外四家公司攤位 Partner 在用，各國也一堆 Partners 在用？這種自問自答的方式帶出了想強調的重點，那是因為這些功能幾乎都是 Google 內部像是 Youtube、Gmail 這種使用者的體量超過十億的產品在使用的，現在開放出來給大家用（當然綽綽有餘）應該沒什麼問題。
再來零星地前情提要一下幾個產品名稱，Cloud Storage、Cloud BigTable、當然還有據說超猛的 Cloud Spanner，號稱克服了 Relational 的 Horizontal Scalability 弱勢，又有 ACID 的能力，可以短時間全球同步，不像 NoSQL 的 eventually 同步。
最後再誇一下 Google Cloud 的 VM 開啟速度有多快，十幾秒而已，不像某 A 的要等好幾十分鐘，在 Performance 上做了很多優化等等，而且還不會自動扣錢，不像某 A 先收了再說，也會 Auto Resizing，用 ML 偵測並善意提醒客戶可能不需要那麼多 Memory 或 CPU，開一整個月或整年還會自動打七折，總之就是一個狂打競爭對手的節奏。
開場結束後，就提供早餐了，會場人真的滿多，有一千多人，所以一宣布休息我就趕快衝出去外面了，因為據我以往參加活動的經驗，這種通常動作要快一點，不然人多真的會排到死，把很多時間浪費在排隊上面。果不其所然，我吃完吃飽後的半個小時，還看到一堆人在排隊還沒吃到，真的很辛苦，那時候我已經在跟攤位的人聊了，也認識了一些新朋友。
吃完早餐後繼續講 Google App Engine 和 Datastore 的部份，接著就吃午餐了。憑著午餐卷換便當，一人一個，也是還不錯的菜色，免費的最好吃，而且還有開放宴會的場地，讓與會者找合菜桌一起坐著進食，順便有機會交流，認識彼此。
比較技術的內容是由兩位 GCPUG.tw 的大大來講解，一位講解技術和服務內容，一位比較搞笑（但大家好像沒笑出來），負責講解實際操作畫面和流程，讓大家可 step by step 的體驗 Google Cloud 的服務內容。
step by step 的包括開 Cluster 怎麼選 CPU 和規格？哪些按鈕在 console 畫面的左上角？如果找不到 quota 相關資訊，要去哪裡搜尋比較快？CDN 上傳要在哪？細節到這種程度的實際操作。
讓我比較印象深刻的是編寫 app.yaml 檔，然後直接 deploy docker 到雲端上面那段，感覺實在是太方便了，我本身就寫一堆 Node.js 和 Express.js，所以看了特別有感覺。Web shell 的測試機也不錯，事先裝好一堆環境，啟動 server 馬上就能從外部連上。接著就是一堆 Cloud Storage、Cloud SQL、Cloud xxx 的介紹，很多細節我就不展開講了，滿多我有聽也不一定懂，也許之後回來補上。
下午末段的主題換到 Machine Learning 的部份，提到 Google 提供了哪些與 ML 有關的 API，有 Vision API、Speech API、Cloud ML、Translate API、NL API。其實就是在說大家都不用自己做 ML 了，Google 都幫你做好了，請直接來用的意思。後面還介紹了 Google Cloud Datalab，我的解釋是一個 Google 推出的 Jupyter/iPython，讓你調用 Google 的各項 API，可以 Python 和 SQL 混寫開發，所寫即所得的一個服務。當然最後也要秀一下最近推出的 AutoDraw，讓還沒看過的人驚豔一番。
居然一下就來到結尾了，因為課程真的太多技術細節了，我是有做一些筆記，但在這篇文章一個一個提到會寫太長。
整場課程內容，我的結論大概就是：Google Cloud Platform 把現在互聯網公司所需要的技術服務都包下來了，從基礎建設、到應用層的行動裝置、分析輔助開發等等，尤其機器學習方面更強更完整。如果公司想要迅速成長，省下一堆 DevOps 的維護成本，專心衝刺業務需求和產品功能的話，非常建議整套使用 Google Cloud Platform。但是這樣也是有點風險，哪天出問題了，你公司整碗都在這邊，GCP 一垮就全倒，基本上公司也無能為力，只能看著自己的服務爆炸，這就是要自己評估的風險了。
後面附上一些無聊亂拍的會場照片，這場活動也是學到滿多的，值得參加，讚！
Founder of Steaker. VP of Product & Engineering at Mithril. 
See all (310)
113 
113 claps
113 
Founder of Steaker. VP of Product & Engineering at Mithril. 
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/hackernoon/snaps-2-billion-gcp-purchase-is-strategic-e6ce3ef5f5e1?source=search_post---------224,"There are currently no responses for this story.
Be the first to respond.
Top highlight
Snap’s announcement that it will be using the Google Cloud Platform (GCP) for its infrastructure, to the tune of $2 billion over the next five years, drew much attention from the tech community.
It even made it to the front page of Hacker News TWICE with two different stories in the same day!
A day later, still sits in the Top 10.
Most of the attention has been paid to the buy vs. build argument. Always a question for companies of any size. Should Snap build out its own infrastructure or outsource it to a big provider? And $2B? Seems like a lot.
But it’s also about alliances, even more than the tech.
Given Snap’s computing and bandwidth needs and potential for future growth, there would really only be three options, if you take the view that you HAVE to go with one of the big boys: Amazon, Google, or Microsoft.
But, Google is building platforms. In a way not seen since Microsoft gained domination of PCs in the 1980s and 90s. Yes, Apple owns a huge client footprint, but G is poised to move on the corporate world much better than Apple.
Snap’s decision to go with Google makes much more sense as a strategic business decision than just deciding on an infrastructure provider.
Take into consideration:
Google is having tremendous success on the client side. Not just Android (1.5–2 billion users), but Chrome (> 1 billion users). Google is integrating its client and server side technologies.
Look at JavaScript technology Web push. This is a push notification technology, and in Chrome, MUST be used with Google Messaging service. How much longer until Chrome natively supports saves to Firebase?
Google is linking their client and server offerings. In short, AWS is a powerhouse, but among AWS and Microsoft, Google is the real force in modern computing. They set standards and own consumer platforms. Amazon does not.
So, go from the infra-arch perspective to the business management perspective. Who would you rather align with? Who would you want to give a *stake* in your success? Snap has the mound of cash right now.
In one move, Snap becomes one of GCP’s biggest customers. G has a stake in Snap’s success. Alliances.
GCP will be massive in the years to come. If you work in IT, look at how much infra is still maintained in-house.
As far as the buy vs build — I would argue —
IT staffs like running in-house servers, network, and managementCIOs HATE running in-house servers, network, and management
Snap’s decision makes more sense from the suits’ perspective than the techs’.
Hacker Noon is how hackers start their afternoons. We’re a part of the @AMI family. We are now accepting submissions and happy to discuss advertising & sponsorship opportunities.
If you enjoyed this story, we recommend reading our latest tech stories and trending tech stories. Until next time, don’t take the realities of the world for granted!
#BlackLivesMatter
32 
how hackers start their afternoons. the real shit is on hackernoon.com. Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
32 claps
32 
Written by
Opinions are my own and not those of Google, Inc.
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
Opinions are my own and not those of Google, Inc.
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/hosting-a-static-website-on-google-cloud-using-google-cloud-storage-ddebcdcc8d5b?source=search_post---------225,"There are currently no responses for this story.
Be the first to respond.
In this mini series we are covering, how to create websites on Google Cloud. This is the second article in the series.
Let’s say you’re a small company looking to set up a basic web presence. You don’t expect a lot of traffic, and don’t want to pay a lot to get up and online. So, read on to find how to start small by deploying a static website.
Static websites are a good option for sites like blogs — where the page rarely changes after it has been published, or where there isn’t any dynamically-generated content. Static web pages can contain client-side technologies such as HTML, CSS, and JavaScript. They cannot contain dynamic content such as server-side scripts, like PHP.
Before we begin, we need to make sure we are properly set up in Google Cloud Console. Ensure the following:
Now that we’ve covered the logistics, there are four steps to set up a static web app.
First step is to connect your domain to cloud storage. To do that, create a CNAME record that points to c.storage.googleapis.com. Your domain registration service can help with this.
www.example.com CNAME c.storage.googleapis.com.
Second step is to create a Google cloud storage bucket. To do this, browse to the google cloud console, click on cloud storage, and click create bucket. Make sure the bucket name matches the CNAME created for your domain. In this case, the storage bucket should be named www.example.com
Third step is to upload the files you want your website to serve. We can do this in one of two ways.
First, by directly uploading files using the google cloud console.
Second, is using gsutil command line, which is great for when you have an existing website.
gsutil rsync -R local-dir gs://www.example.com
Once the files are uploaded, we need to make sure they are properly shared for access. You can either make all files in your bucket publicly accessible, or you can set individual objects to be accessible through your website. Generally, making all files in your bucket accessible is easier and faster.
At this point, we essentially have a functioning static website but it is recommend to assign an index page suffix and a custom error page to guide the users better.
This helps in scenarios when say you have no file named apple in your bucket www.example.com. In this situation, if a user requests the URL http://www.example.com/apple, Cloud Storage attempts to serve the file www.example.com/apple/index.html. If that file also doesn’t exist, Cloud Storage returns an error page with 404 response.
You can learn more about setting this up here.
The last step is to test our static website. Verify that content is served from the bucket by requesting the domain name in a browser.
One important thing to note is that GCS ONLY supports HTTP. In order to serve HTTPS, and get all that security goodness, we need to either using direct URIs using a CNAME redirect, or migrate to using a load balancer as a front-end as shown in the picture.
In this article we learned how to start small and deployed a static web application on Google Cloud platform. We used Google cloud storage to host the static content and pointed the Domain name to the storage bucket.
Google Cloud community articles and blogs
205 
2
205 claps
205 
2
Written by
Developer Advocate @Google, Artist & Traveler! Twitter @pvergadia
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Developer Advocate @Google, Artist & Traveler! Twitter @pvergadia
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/hackernoon/deploy-react-application-using-docker-and-google-cloud-platform-4bc03f9ee1f?source=search_post---------226,"There are currently no responses for this story.
Be the first to respond.
In this article, you will learn how to deploy applications on GCP. We will deploy a create-react-app.
Link to the Repo — https://github.com/Harshmakadia/react-docker
Before we get started with the actual steps of deploying the React App using GCP and Docker. First, let’s understand what docker actual is?
Docker is a tool which is designed to make the creating, deploying and running of applications easier with the help of containers. Containers are something which allows the developer to bundle the application with all the necessary ingredients like different libraries, dependencies and ship is as only a single package.
We will go step by step
Create react app is a lot easier using the create-react-app (CRA)
We will use create-react-app package to install and configure simple react application from NPM, Open your terminal and install react app.
For more info on creating a react app
github.com
once you run the application using command$ npm start
After that, it’s time to create a build the app, run$ npm run build
creating a docker file is a cup of your tea. The is no rocket science in creating a docker file.
Just Create a new file with name DockerfileNow once the file is created we will add some command to that which will help us to create, run, deploy the application.
Here the content of Dockerfile for react app. Note I’m using Nginx to as a server.
Once the docker file is created. I’m creating a new folder named deployment within the app directory which has a nginx.conf file
The content of nginx file, note that this is default configuration you may not need to alter this file unless you have some special requirements.
Head to this link below and download it for your respective operating system
www.docker.com
Once it is installed run open your terminal and run below command to check it is installed successfully
docker — — version
Now that we have the docker setup on our machine it’s time to create the first image using the following command
docker build -t first-docker .
more info about different command can be found here.
Once you run this command it will execute all the command listed down in the Dockerfile.
we have successfully created the image. Let’s proceed to next step
Download SDK from the below link and setup on your machine
cloud.google.com
Now that we have the gcloud SDK setup on our machine
Next step is to create a new project in the GCP where we will be pushing our docker images to the containers.
Configure Docker to use gcloud as a credential helper or are using another authentication method. To use gcloud as the credential helper, run the command:
It’s time to push the image to the registry
Push the tagged image to Container Registry by using the command:
This command pushes the image that has the tag latest. If you want to push an image that has a different tag, use the command:
When you push an image to a registry with a new hostname, Container Registry creates a storage bucket in the specifiedmulti-regional location. After pushing your image, you can:
Navigate the GCP console and search of Container Registry, you will be able to see the image which we push.
It’s time to create cluster now inside Kubernetes Engine in GCP
Create deployment under workloads
Select the image which you want to deploy and finally click on Expose to expose the deployment.
Set port to 80 in Target Port since we had EXPOSED our application to 80 in the Dockerfile
Once you have exposed the port you will get IP Adress where your react application will be running live.
And that it’s you are all set with docker. Whenever you want to push new image to the container first build the image with above-specified commands and then push the image to container registry and finally make that image live by going to rolling update option.
Please note down your questions in the comment section below if you have any doubts & I’ll be happy to address them.
That’s the end 🔚 I hope you have learned something new. Happy Learning! 💻 😃
#BlackLivesMatter
226 
3
226 claps
226 
3
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
I talk about JavaScript, Web Development, No-code to help you stay ultra-modern. See you on Twitter — https://twitter.com/MakadiaHarsh
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
"
https://medium.com/@DazWilkin/google-cloud-sql-6-ways-golang-a4aa497f3c67?source=search_post---------227,"Sign in
There are currently no responses for this story.
Be the first to respond.
Daz Wilkin
Aug 20, 2017·6 min read
I spent some time yesterday exploring ways to connect from Golang to Google Cloud SQL using the Cloud SQL Proxy. This post summarizes 6 ways to connect Golang code to Cloud SQL:
I recommend you always default to use the Proxy. It removes security hassles and removes the need to expose Cloud SQL database instances to the Internet. The Proxy is customarily run as a “companion process” and accessed from your applications via UNIX or TCP sockets. Because the Proxy is written in Golang, if you’re writing Go code, you can also reference the Proxy as a library directly in your code.
The tutorial assumes you have a working knowledge of Google Cloud Platform (GCP), a Google account and that you’ve got billing setup. Unfortunately the free tier does not include Cloud SQL.
For convenience, I’m going to assume that you have the following constants. When you see these occur in the instructions below, please replace them with your actual values:
Please ensure you enable the (Cloud) SQL Admin API in your project. Either, from the command-line:
Or via the Cloud SDK:
Create a MySQL database instance and a PostgreSQL database instance. You can do this either from the Cloud Console or the Cloud SDK. Here are the Cloud SDK commands for MySQL:
and for PostgreSQL:
You may use the default databases created with each of the above flavors but, for consistency, I recommend you create a new database in each server called $DBNAME. The Cloud SQL tools do *not* fully support database management as this is database-specific and is functionality well-served by the databases’ own tools. However, you are able to create databases and create users using the Google Cloud Console. Here are the URLs — you’ll need to replace the ${INSTANCE}, ${PROJECT} and the correct “m” for MySQL and “p” for PostgreSQL:
That’s it!
In order to use the Proxy via UNIX sockets, ensure that you also:
You may use a location other than /cloudsql but, if you do, be careful to ensure you correctly replaces the references to this directory in the following instructions.
Application Default Credentials (ADCs) provide a very useful mechanism for authorization. Using ADCs you can run code unchanged on a local workstation, on Google App Engine, Compute Engine etc.
The following examples assume you are running the Golang code and the proxy on your local workstation and that you have authenticated using ADCs. To authenticate using ADCs, issue the following command and accept the “Google Auth Library” prompt:
The “6 ways” use the following boilerplate Golang code. This code is based on a Spring example “Accessing Relational Data using JDBC with Spring”. More about that in the next post!
You may either create 6 copies of this or — as I’ve been doing — you can comment in/out appropriate sections. You will need to correct the imports, add the correct dns/db lines and correct the Prepared Statement for each way:
Ensure you pull the relevant libraries on first use:
Then you should be able to run the code with:
For Prepared Statements, MySQL uses “?” to denote parameters, please ensure you use this Golang statement:
When using the Cloud SQL Proxy in-process, your code imports the Cloud SQL Proxy for MySQL:
In this scenario, you do *not* need to run the cloud_sql_proxy command. That command provides the “companion process” (out-of-process) solution that’s described in “companion process” section below.
Golang:
Run the code!
When using the Cloud SQL Proxy as a “companion process”, your code uses a regular MySQL driver and treats the Cloud SQL MySQL instance as if it were running on localhost. I use this Golang MySQL driver:
Proxy:
Golang:
Run the code!
Proxy:
Golang:
Run the code!
For Prepared Statements, PostgreSQL uses $ prefixed parameters, please ensure you use this Golang statement:
When using the Cloud SQL Proxy in-process, your code imports the Cloud SQL Proxy for PostgreSQL. Do not omit the prefixing _:
In this scenario, you do *not* need to run the cloud_sql_proxy command. That command provides the “companion process” (out-of-process) solution that’s described in “companion process” section below.
Golang:
Run the code!
When using the Cloud SQL Proxy as a “companion process”, your code uses a regular PostgreSQL driver and treats the Cloud SQL PostgreSQL instance as if it were running on localhost. I use the pq pure Golang PostgreSQL driver, you must import the following. Don’t forget the prefixing _:
Proxy:
Golang:
Run the code!
Proxy:
Golang:
Run the code!
Don’t forget to delete the project or the databases when you’re done investigating. You will be incurring charges. The easiest way is to delete the project:
If you choose not to delete the project, you may just delete the databases:
We demonstrated 6 ways to connect Golang code to Google Cloud SQL.
OK…. so 8 ways if you include running the Proxy as “companion process” in a Docker container. You will need to create a service account and give it the role “Cloud SQL > Cloud SQL Client”. A JSON key file will be produced and stored on your workstation:
MySQL
PostgreSQL
That’s all!
116 
1
No rights reserved
 by the author.
116 
116 
1
"
https://medium.com/@sathishvj/notes-from-my-beta-google-cloud-professional-security-engineer-certification-exam-bff02c93d757?source=search_post---------228,"Sign in
There are currently no responses for this story.
Be the first to respond.
sathish vj
Feb 27, 2019·7 min read
Subscribe to my YouTube channel that teaches you to apply Google Cloud to your projects and also prepare for the certifications: youtube.com/AwesomeGCP. Check out the playlists I currently have for Associate Cloud Engineer, Professional Architect, Professional Data Engineer, Professional Cloud Developer, Professional Cloud DevOps Engineer, Professional Cloud Network Engineer, and Professional Cloud Security Engineer.
Update on March 29th, 2019: Results are out and I passed! I’m now a …
And now, back to the original programme …
I wrote the beta Google Cloud Security Engineer certification exam. I think I did fairly ok, but I won’t be convinced that I passed until I see the result. Checking my answers post the exam, I definitely got a few wrong, but will I pass despite those? Have I made way more errors than I thought? I’ll just have to wait and see.
The exam was easier compared to the Network Engineer — but then, according to me, the Network Engineer exam was the toughest of the six so far. On the Security exam, many of the questions were straightforward. I sometimes doubted if they could be that straightforward and I re-read the questions a few times to ensure that I wasn’t missing some catch. Nope, they actually were fairly direct. Then again, not all of it was easy.
In the other GCP exams, I was often able to come to a logical conclusion based on the use case. You look to eliminate a few obviously wrong ones, then search the question for further clues on what is the requirement in the use case to arrive at the answer. Here though, sometimes, the questions were a little too straightforward — the kind that you have to mug up for. Imagine if somebody questioned you on capitals of countries. There is no use-case there, no logic. It’s just straight up by-hearting. There were at least 10 questions like that.
On a personal note of judgement, I wasn’t very appreciative of the Security exam. The Networking exam was challenging and I felt it takes a certain amount of merit to pass that. The bar for the Security exam was lower. If somebody came in with a GCP Networking Certification, I would consider them at a higher level in their area than one who had the GCP Security Certification in their own area. I feel this exam needs to be different; not just needlessly difficult but more relevant with use cases and questions that you have to reason through given all the security tools in the GCP ecosystem.
Let me also mention one exam question I got annoyed with. And if the exam creators are seeing this, please fix it. There was a similar one on the networking exam too. It’s a question about firewall rules priority. Here’s a made up similar one. A default rule is created with priority 1000. To overrule this with another, a) set a higher priority or b) set a lower priority. It’s impossible to answer this question with confidence because effective priority and numerical priority value are inversely related. One could interpret it either ways. The options would be clearer as a) set a priority value of 999 or b) set a priority value of 1001.
For preparation, I did the Coursera GCP Security Specialization. But I didn’t delve on it too much. For other exams I’d watched the related Coursera videos at least a couple of times. But not this time. This time I went mostly for documentation. The Linux Academy GCP Security course is currently free btw. And while it is free, you might as well take it. I watched about one or two videos on that but then abandoned it. There’s no denying that they are competitively good compared to the Coursera ones, but once you’ve done one of those, the relative benefit of the other is only trivially incremental. Moreover, my experience with the Networking exam changed my approach. As I’d mentioned in my notes on that exam, I was underprepared. I learnt way more in the exam than prior to it. Duh. After the exam I was strongly motivated to learn more about GCP networking, and learn I did. Primarily from the documentation only. Similarly, I was really keen to learn about GCP Security (not just for the certification) and I found greater depth and breadth in the documentation.
If you are preparing for the Networking or Security certification, my suggestion is that you prepare for both together. A large part of Security is also about Networking and therefore there is considerable overlap. Between the two, the Security exam is easier, so you might want to take that first.
Onwards to the questions I got. Unfortunately, because many of the questions were straightforward, I have less to give you from this exam as mentioning them might be revealing them. So I apply my own Data Loss Prevention API on the questions I can remember and am redacting anything that is Question Identifiable Information. I got 113 questions for 4 hours. Because of time spent switching between questions and general network delays, plan to be in there for about 4.5 hours if you are taking the beta. That by itself is going to wear you down and make you lose focus. A regular exam is more reasonable at 2 hours and about 50 questions.
IAM — questions covering Folders, Organizations, IAM Permissions, Organizational Constraints, Google Groups.
IAM — managing users can be via GSuite or Cloud Identity. There were questions on GSuite and I wasn’t expecting those. I haven’t done the GSuite course, but I wonder now if there would have been value in skimming those topics.
Networking — shared VPC, VPN, VPC peering, interconnect, Private Google Access. Here, you are better off doing the full Networking specialization. There is significant overlap.
DLP — Some straightforward and some were that by-hearting type. So, if you find lists of items related to this, spend some time on it.
DLP — what are the various ways to de-identify data? How can you completely redact them and never get it back? And how can you get it back? What are the various algorithms you can use?
DLP — using custom dictionaries and regex. Creating custom infoTypes.
DLP — how to manage data when in BigQuery and on Cloud Storage.
PCI DSS — What solutions are compliant with PCI and what requires additional work?
DNSSEC — how to protect your domain to the extent possible?
GCDS — How do you sync users, groups, third party tools, etc. There were mentions of LDAP and Active Directory, but you don’t need to really know them.
SIEM — how to connect, export, etc.
KMS — An important topic that is covered well in the courses. Do the exercises to really get a hang of what’s happening. One of the QwikLabs exercises on working with Cloud Storage and KMS was what really made me understand some parts of this.
KMS — know all about the process of how DEKs, KEKs, Key Rings, etc. are used within GCP. Where are they stored? Where are they retrieved from when used? There was some post on how keys are managed even above that. It isn’t relevant for the exam, but it was good reading.
KMS — Google managed keys, CMEK, CSEK, Application Security with keys.
Compliance — Know what these are about at a high level: GDPR, HIPAA, COPPA, FIPS 104, PCI-DSS.
Cloud Build — what are the best practices on ensuring secure builds and safe images?
Cloud Build — what base images do you start off with when you do your build? How do you ensure those are safe?
Cloud Security Scanner — where do you use this? What kind of situations is it used in? What kinds of issues does it catch? What are the downsides of using it?
Firewalls — when is it better to use firewall rules as opposed to other options? Priority values on rules. What are the default rules?
GKE —Aliases and GKE with private access.
Shared Responsibility Model — know what you are responsible for beyond what Google takes care of.
Stackdriver — there was something about capturing and viewing logs. Can’t remember where or what that was.
Forseti — under what circumstances is Forseti an appropriate choice? What are the various components of Forseti and when are they used?
Google Cloud Certified — Professional Cloud Security Engineer
For those appearing for the various certification exams, here is a list of sanitized notes (no direct question, only general topics) about the exam.
Overall notes across all GCP certification exams
Notes from the Professional Cloud Architect exam
Notes from the beta Professional Cloud Developer exam
Notes from the Professional Data Engineer exam
Notes from the Associate Cloud Engineer exam
Notes from the beta Professional Cloud Network Engineer Exam
Notes from the beta Professional Cloud Security Engineer Exam
Notes from the Professional Collaboration Engineer Exam
Notes from the G Suite Exam
Notes from the Professional DevOps Engineer Exam
Notes from the Professional Machine Learning Engineer Exam
Main Link — https://cloud.google.com/certification/cloud-security-engineer
Topics Outline-https://cloud.google.com/certification/guides/cloud-security-engineer/
Practice Exam-https://cloud.google.com/certification/practice-exam/cloud-security-engineer
A collection of posts, videos, courses, qwiklabs, and other exam details for all exams: https://github.com/sathishvj/awesome-gcp-certifications
I’ve collected here a bunch of free Qwiklabs codes which are awesome to get lots of hands-on practice. Use them well.
medium.com
Check the FAQs here: https://medium.com/@sathishvj/frequently-asked-follow-up-questions-on-google-cloud-gcp-certifications-438e1addb91d.
Wish you the very best with your GCP certifications. You can reach me at LinkedIn and Twitter. If you can support my work creating videos on my YouTube channel AwesomeGCP, you can do so on Patreon or BuyMeACoffee.
tech architect, tutor, investor | GCP 12x certified | youtube/AwesomeGCP | Google Developer Expert | Go
See all (32)
125 
4
125 claps
125 
4
tech architect, tutor, investor | GCP 12x certified | youtube/AwesomeGCP | Google Developer Expert | Go
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/hackernoon/hosting-a-free-static-website-on-google-cloud-storage-d0d83704173b?source=search_post---------229,"There are currently no responses for this story.
Be the first to respond.
This guide walks you through setting up a free bucket to serve a static website through a custom domain name using Google Cloud Platform services.
Sign in to Google Cloud Platform, navigate to Cloud DNS service and create a new public DNS zone:
By default it will have a NS (Nameserver) and a SOA (Start of Authority) records:
Go to you domain registrar, in my case I purchased a domain name from GoDaddy (super cheap). Add the nameserver names that were listed in your NS record:
PS: It can take some time for the changes on GoDaddy to propagate through to Google Cloud DNS.
Next, verify you own the domain name using the Open Search Console. Many methods are available (HTML Meta data, Google Analytics, etc). The easiest one is DNS verification through a TXT record:
Add the TXT record to your DNS zone created earlier:
DNS changes might take some time to propagate:
Once you have verified domain, you can create a bucket with Cloud Storage under the verified domain name. The storage class should be “Multi-Regional” (geo redundant bucket, in case of outage) :
Copy the website static files to the bucket using the following command:
gsutil rsync -R . gs://www.serverlessmovies.com/
After the upload completes, your static files should be available on the bucket as follows:
Next, make the files publicly accessible by adding allUsers entity with Object Viewer role to the bucket permissions:
Once shared publicly, a link icon appears for each object in the public access column. You can click on this icon to get the URL for the object:
Verify that content is served from the bucket by requesting the index.html link in you browser:
Next, set the main page to be index.html from “Edit website configuration” section:
Now, we need to map our domain name with the bucket we created earlier. Create a CNAME record that points to c.storage.googleapis.com:
Point your browser to your domain name, your website should be served:
While our solution works like a charm, we can access our content through HTTP only (Google Cloud Storage only supports HTTP when using it through a CNAME record). In the next post, we will serve our content through a custom domain over SSL using a Content Delivery Network (CDN).
Drop your comments, feedback, or suggestions below — or connect with me directly on Twitter @mlabouardy.
#BlackLivesMatter
192 
3
how hackers start their afternoons. the real shit is on hackernoon.com. Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
192 claps
192 
3
Written by
CTO & Co-Founder @Crew.work — Maker & Indie Entrepreneur @Komiser.io, Writer/Author and Speaker — Blog: https://labouardy.com
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
CTO & Co-Founder @Crew.work — Maker & Indie Entrepreneur @Komiser.io, Writer/Author and Speaker — Blog: https://labouardy.com
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/aws-to-google-cloud-platform-service-mapping-5a1689d41f01?source=search_post---------230,"There are currently no responses for this story.
Be the first to respond.
UPDATE 9/1/2016: We now have a very comprehensive mapping that goes far beyond this post. You can read it here.
As a Developer Advocate on the Google Cloud Platform team, I am frequently asked what services we provide. If the person I’m talking to is familiar with Amazon Web Services (AWS), the quickest way to jump start an explanation of Google Cloud Platform is to start with a comparison to AWS’s similar services, then cover the differences.
Below is a simple map between some of the major services in AWS and Google Cloud Platform. This is not intended to be a complete mapping. It would be unfair to both platforms to list every service because Google and Amazon are taking different approaches in many areas, making direct comparisons practically impossible. I’m only listing the services where the comparison is helpful.
EC2→ Compute EngineEC2 Container Service → Container EngineElastic Beanstalk → App Engine **
S3 → Cloud StorageGlacier → Cloud Storage NearlineCloudFront → Cloud Storage (edge caching is provided for public buckets)
RDS → Cloud SQLDynamoDB → Cloud Datastore and Cloud Bigtable
Redshift → BigQuerySQS/Kinesis → Cloud Pub/SubEMR → Cloud Dataflow
CloudWatch → Cloud Monitoring and Cloud Logging
Route53 → Cloud DNS and Google DomainsDirect Connect → Cloud Interconnect
CloudFormation → Cloud Deployment ManagerSES → SendGrid (partner)WorkMail → Gmail (also see Google for Work)WorkDocs → Google Docs (also see Google for Work)
** AWS Elastic Beanstalk and Google App Engine are often described as similar offerings, but there are significant differences in their approaches. Both offer auto-scaling, load balancing, monitoring, etc., but unlike App Engine, Elastic Beanstalk requires the typical system administration that raw VMs require (OS updates, etc.). Google App Engine is a PaaS, meaning that it’s fully managed, so all of these administrative tasks are handled by Google. The basic App Engine setup includes built-in services such as Task Queues, Memcache, Users API, and more.
If you require unmanaged VMs, Google also has auto-scaling, load balancing, and monitoring of unmanaged VMs as features of Google Compute Engine. There is also an alternative hosting model now available as part of Google App Engine called Managed VMs.
My advice is to do your homework and understand these models thoroughly before diving in on either platform. Each has unique advantages.
I’ll have more posts in the near future with more specifics on several of the offerings. Stay tuned!
Originally published at gregsramblings.com on May 12, 2015.
Google Cloud community articles and blogs
65 
1
65 claps
65 
1
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Google Developer Relations — Living in San Francisco https://gregsramblings.com | https://cloud.google.com | http://instagram.com/gregsimages
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://towardsdatascience.com/building-serverless-python-data-apis-with-dockers-on-google-cloud-24d4f15cf81?source=search_post---------231,"Sign in
There are currently no responses for this story.
Be the first to respond.
Antonio Cachuan
May 2, 2020·6 min read
In this article, I’ll show you a simple way to build in minutes a few Data APIs for exploiting data from a BigQuery dataset. These APIs will be deployed with dockers using a GCP serverless service called Cloud Run.
The idea behind is to work with serverless components. First, let’s understand these services…
"
https://betterprogramming.pub/how-to-save-money-on-google-cloud-platform-22bf4c302d32?source=search_post---------232,"Sign in
There are currently no responses for this story.
Be the first to respond.
Gaurav Agarwal
Jul 3, 2020·5 min read
Google Cloud Platform is one of the fastest-growing cloud platforms, maintaining a steady position at a number three, behind AWS and Azure. It prides itself for its network quality and the edge on data science and engineering.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/techking/king-collaborates-with-google-cloud-for-next-generation-analytics-and-machine-learning-8488af0adbab?source=search_post---------233,"There are currently no responses for this story.
Be the first to respond.
By Åsa Bredin, FVP Technology and Jacques Erasmus, CIO
Every month, more than 270 million people around the world play a King game. Our mission — which we share with our parent company Activision Blizzard — is to bring those players moments of magic in their everyday life.
We believe it is a harmony of art, craft, science, marketing and technology which allows us to do this. At the intersection of science and technology is our world-class data analytics capability, which we use every day across the business to understand how our players interact with our games — and then to make those games even more fun and engaging for our players.
Today, King has one of Europe’s largest Hadoop clusters, with a double-digit number of petabytes online for our data science and engineering communities to query. In the last couple of years, we started to ask ourselves whether a monolithic on-premise Hadoop environment best set us up for the challenges of the future, particularly in a world where public cloud is becoming an ever-greater part of the IT landscape, and — especially important for us — when much of the innovation in data science and data engineering is now taking place in public cloud. At the same time, a number of our teams had started to do more work with machine learning and AI — again, increasingly a public cloud story — so this was also a factor for us to consider.
After increasing amounts of investigation, discussion, benchmarking, and, finally, sign off from King’s leadership, we’ve made the decision to move our core data infrastructure and AI/ML platform to Google Cloud, and we are very pleased to be able to announce that today.
We believe that our move to Google Cloud Platform (GCP) — a transformational change for King that is already under way — will amplify our own talented engineers’ efforts, allow us to focus more on differentiation — areas of effort where King needs unique capabilities not provided by the market — and reinvigorate King’s reputation as one of the most innovative and future-facing tech companies in Europe.
We’re also super happy to work with Google Cloud on this move, and consider our relationship — or rather the many relationships which our teams have already established across the Google Cloud organisation — to be a genuine partnership that is a win-win for both companies.
“King has long been a leader in delivering delightful mobile gaming experiences to millions of players around the world. King will continue to innovate and demonstrate its leadership position as a global innovator by utilizing our big data, AI and machine learning capabilities to give its engineers the next generation of tools to build great experiences. We look forward to seeing King deliver even richer gaming experiences for players worldwide through our deep collaboration and the unique products and services Google Cloud offers game developers.”
Sunil Rayan,
Managing Director, Gaming, Google Cloud.
We now look forward to completing our migration to GCP, leveraging the reliability, scalability and agility which the platform provides as the foundation on which our teams can continue to do great things, and to deliver for our players.
Originally published at techblog.king.com on August 20, 2018.
Read the latest tech articles from King.
158 
158 claps
158 
Read the latest tech articles from King. Learn about our tech and our company culture.
Written by
King Tech Blog - awesome tech teams at King. Everything you read comes from the minds of our tech gurus and hearts of our Kingsters!
Read the latest tech articles from King. Learn about our tech and our company culture.
"
https://medium.com/google-cloud/how-can-google-cloud-help-with-security-of-your-apps-8f5692f56177?source=search_post---------234,"There are currently no responses for this story.
Be the first to respond.
gcpcomics.com
At this point in the evolution of cloud computing it is fair to say that you have at least some apps in the cloud, or are planning to have a few in the near future. So, you may be wondering about the kind of security measures available to you. In this issue of GCP Comics we are covering exactly that!
We will go over cloud security fundamentals including the three very simple security concepts.
Here you go! Read on and please share your thoughts in the comments below.
Google Cloud provides protection from threats through a secure foundation. It offers the core infrastructure that is designed, built and operated to help prevent threats. How is it done? Here are a few of the ways!
Defense in depth
Google’s infrastructure doesn’t rely on any single technology to make it secure. Rather, builds security through progressive layers that deliver true defense in depth.
Other cloud providers may describe a similar stack of capabilities, but the way Google Cloud approaches many of these is unique. Here is how:
If this is intriguing, here is a white paper on Google infrastructure design that goes into all of these areas in significant details.
End-to-end provenance & attestation
Google’s hardware infrastructure is custom-designed by Google “from chip to chiller” to precisely meet their requirements, including security.
Google’s servers and Operating Systems(OS) are designed for the sole purpose of providing Google services.
Understanding provenance from the bottom of the hardware stack to the top allows Google Cloud to control the underpinnings of the security posture. Unlike other cloud providers, Google has greatly reduced the “vendor in the middle problem” — if a vulnerability is found, steps can be taken immediately to develop and roll out a fix. This level of control results in greatly reduced exposure.
Private backbone
Google operates one of the largest backbone networks in the world. There are more than 130 points of presence across 35 countries — and there is a continuous addition of more zones and regions to meet customers’ preferences and policy requirements.
Google’s network delivers low latency but also improves security. Once customers’ traffic is on Google’s network it is no longer transiting the public internet, making it less likely to be attacked, intercepted, or manipulated.
Encryption at rest by default
We will cover this one in more details in the upcoming comics but in short, all data at rest or in motion is encrypted by default on the Google network. And some services offer the option to supply or manager your own keys.
Update at scale without disruptions
Google has the ability to update the cloud infrastructure without disrupting customers using a technology called Live Migration.
Updates add functionality, but from a security standpoint, they also are required to patch software vulnerabilities. No one writes perfect software, so this is a constant requirement.
Keeping ahead of threats
Security landscape rapidly evolves and many organizations struggle to keep pace. Because Google runs on the same infrastructure that is available to the customers, customers can directly benefit from those investments.
The global footprint across enterprises and consumers gives Google an unprecedented visibility into threats and attacks. As a result, solutions can be developed before many other organizations even see the threats, reducing exposure.
In the cloud there can be a lot of control options to make sure the app, the data and the services you deploy are secure. The most important thing to understand is that “cloud security requires collaboration”
Your cloud provider (Google Cloud) is responsible for securing the infrastructure.
You are responsible for securing your data.
And.. Google Cloud provides the best practices, templates, products and solutions to help you secure your data and services.
Keeping this section short because I am planning on doing another comic issue on this topic, there is a lot more to learn here, so stay tuned! 😊
In order to protect the sensitive data that you store in Google Cloud, it maintains and goes though compliance including complex regulatory, frameworks and guidelines. For example HIPPA, FedRAMP, SOC etc.
Read about the detailed compliance standards and certifications here.
To learn more about security fundamentals on Google Cloud, check out this link to the detailed security whitepaper.
Want more GCP Comics? Visit gcpcomics.com & follow me on Medium, and on Twitter to not miss the next issue!
Google Cloud community articles and blogs
291 
291 claps
291 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Developer Advocate @Google, Artist & Traveler! Twitter @pvergadia
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/all-you-need-to-know-about-mongodb-on-google-cloud-28adc359af37?source=search_post---------235,"There are currently no responses for this story.
Be the first to respond.
Google Cloud offers a marketplace with many stacks & service solutions with Click to Deploy. The solutions includes database solutions(mongodb, cassandra, mysql) to Blog & CMS(wordpress, joomla) along with Developer tools(gitlab, jenkins). They are not a fully managed solution and google doesn’t offer support for any but the stacks decreases time to set up solutions and one can enjoy the services by paying only for the GCP resources but not for the solutions*.
Among the deployments offered on Google Cloud Launcher, mongodb is one which can be deployed at a click with creation of primary, secondary instances (compute engines) in a replica set along with arbiter node.
Head over towards MongoDB Launcher on Google Cloud Console for deployment of the NoSQL database in cluster of instances. Click on “LAUNCH ON COMPUTE ENGINE”.
Configure deployment, replica set name and disk type on the console. Choose the number of nodes among which one will be primary replica and rest secondaries. Also, choose a arbiter node which can be a small instance as it is utilized just for choosing the primary replica by voting. Its recommended to choose SSD for the data storage which has high IOPS & throughput. Grab details about the performance difference from official optimization guide.
Now, within few minutes, the clusters are up not only with the number of replicas specified and arbiter but also with a separate disks for each node which has naming standard of mongodb-servers-vm-0-data that are attached to respective instance. The data on those disk are synced with each other. Also, by selecting External IP option None, we can hide the nodes from outside the specific subnet.
There are different ways of connecting to the mongodb replica set:
Connecting to each node: On the Compute Instances page on console, hit the SSH button and new window opens with ssh connection. Also, SSHing into google cloud instances is easy with gcloud sdk: gcloud compute ssh [Instance Name] --zone [zone] --project [project]
On arbiter node:
On primary node:
On secondary:
Connect to ReplicaSet
As we have primary and secondary replicas running, to establish mongodb connection with the replica set rather than each one, enter this command on any of the node:
This way, we get connected to the primary replica/node at first and in case of fail over, the arbiter node chooses another node as primary and the connection is established to new primary.
First, we connect to the replica set from arbiter node
Here, the instance mongodb-servers-vm-0 is primary and mongodb-servers-vm-1 is secondary. Now, lets restart mongod service on the primary node
On the arbiter connection:
Now, we are connected to the instance mongodb-servers-vm-1 which has become the primary replica now. Check the replica set status:
We get the status:
MongoDB ReplicaSet Connection from Compute Engine Instances
From any of the Google Cloud compute engine instances, the mongodb connection can established by passing the replicaset host name: mongodb://mongodb-servers-vm-0,mongodb-servers-vm-1/myDB?replicaSet=rs0
But we cannot get connected to the set by providing the ip address of any of the nodes. The name of the replicas can be found on /etc/hosts of respective instance.
MongoDB ReplicaSet Connection from Localhost
Sometime, we may need to connect to the replicaSet launched on Google Cloud from local computer, for that we need to create SSH tunnel:
By that way, we can connect to replicaSet even from MongoDB Compass for exploring and manipulating data.
As they say Make life easier with Ops Manager, its one of the best solution provided by MongoDB which offers performance visibility to query optimization to backup and alerts out of the box. Ops Manager can be downloaded for various platforms but I faced issues with ubuntu and later succeeded for CentOS without any hurdle.
Google Cloud community articles and blogs
229 
4
229 claps
229 
4
Written by
DevOps | SRE | #GDE
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
DevOps | SRE | #GDE
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://rominirani.com/build-a-serverless-online-quiz-with-google-cloud-functions-and-cloud-firestore-1e3fbf84a7d8?source=search_post---------236,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
This post will take a look at how you can build a Serverless online quiz using a couple of services on the Google Cloud Platform, namely Cloud Functions and Cloud Firestore.
If you are new to Google Cloud Functions, I suggest that you take a look at my tutorial series on Google Cloud Functions. It gets you up-to speed on the basics, types of functions and there is a Github repository to support it too.
Cloud Firestore, as the documentation states, is a NoSQL document database built for automatic scaling, high performance, and ease of application development. It is currently in Beta and I decided to give it a go and demonstrate how we could power some of our functionality wrapped inside Google Cloud Functions via a persistence layer in Cloud Firestore.
Note that I could have used Firebase Functions too, but I prefer to show this via the plain vanilla Cloud Functions in Google Cloud Platform.
Before we jump into how we build the online quiz, it is nice to see it in action. Since we are powering our quiz via Cloud Functions, all we will have is a HTTP URL to launch in the browser that will trigger the Cloud Function.
Assuming that you will visit that URL, you will get a screen that looks like this in your browser. The quiz that we have created is “Countries and their Capitals”. A sample screen is shown below:
You can select one of the options as the answer and click on submit. It will provide you with the results as shown below, along with an option to continue taking the quiz.
That is all there is to it but you get the picture. All of the above will be driven via Cloud Functions and the set of quiz questions, along with the 4 options and answer will be there in Cloud Firestore. No servers to run for you at all.
Let’s get started.
Here are a few steps to complete to begin our journey:
If you would like to see detailed screenshots on how to do the above, please follow my tutorial on Setting up a Local Environment for Google Cloud Functions development, which is part of my Google Cloud Functions tutorial series.
Now that we have the setup in place, let us move to the first part of our Online Quiz application, a place to hold our questions and answers or in other words, build our bank of questions and understanding how we can invoke it via any other application.
Firestore is a NoSQL document database built for automatic scaling, high performance, and ease of application development.
Having said that, the best way of think about it is a document store. Think of a document store as a collection of different kinds of documents or types. Examples of kinds or types can be a questions that will hold a list of question or employees, which will hold a list of employees.
So there are 3 things to keep in mind, if I have to simplify : Collection, Document and Attributes (Properties). So a quiz can be modelled as follows:
Now that we have got that out of our way, let us go setup Firestore for our project.
The first step is to visit the Firebase Console and sign-in with the same account. Once signed in, click on Add Project as shown below:
This will bring up a screen, where you should select the same GCP Project that you created in the earlier section and then click on ADD FIREBASE button as shown below:
This completes your step to add Firebase database to your GCP Project. It will lead you to the Firebase console as shown below:
But we are not done yet since you have both Firebase and Firestore and by default, this project adds Firebase to your project, whereas we need Firestore.
To do that, click on the Database link, once you are logged in. You will find that in the Develop → Database section in the left menu, as shown below:
This will show you that Firebase database is selected. What we need to do is select the Firestore database. Click on the Database dropdown list at the top and select Cloud Firestore BETA as shown below:
Wait for the initializations to get done. At the end , you should see the Cloud Firestore BETA selected for your project as shown below:
Now that we have our database selected, it is time to create our Collections as described earlier.
The Cloud Firestore documentation is excellent and I suggest to read that, but for now, our minimal understanding from the earlier section will be enough.
To re-iterate, our collection will be named questions. This collection will contain documents, where each document will contain the question details (Attributes/properties) that we explained earlier.
We will create our collection and add a few sample questions as given below. This can be done via the client libraries too, but we will keep with the web console for now.
Go ahead and click on ADD COLLECTION as shown below:
This will bring up a form where you can give a name for your collection. In our case we will call it questions.
Click on NEXT. This step will give us the opportunity to also add our first record i.e. document.
As you can see, we can give a unique id to help identify our document within the collection. We can either go with the AUTO-ID presented over here or we can click on that and give our own generated id. In my case, I will go with unique ids that are in sequential order like 1,2,3… and so on.
Once you provide your id, you can then start creating the fields. Remember that each question i.e. document of ours has the following fields:
Here is a sample screen that shows the 2 questions (documents) that I created in the collection with the fields for the second question shown:
Go ahead and create as many questions as you would like. For the purpose of our demo, 2 or 3 questions in total will suffice.
Great ! We now have our Firestore collection ready for serving to applications. We shall be using the Node.js Firebase npm modules to access them from our Cloud Functions as demonstrated in the next section.
Let’s move now to the final puzzle in our project i.e. write Google Cloud Functions to serve a random question to the user and check the answer. In short, we shall be writing two functions:
The names of the functions are self-explanatory.
This Cloud Function will get a random question from the Firestore collection i.e. questions. It will serve a HTML form with the question and the 4 options. The form will then on submit action trigger the checkanswer function.
This function will check the answer and inform the participant if the answer is correct or not. It will also put in a link to the getRandomQuestion again, so that the participant can continue to take more quiz questions.
On your local machine, create a folder in which our files will result. Inside that folder, create a index.js file with the code shown below:
Here are a few comments on the code above:
The package.json file is shown below. Place it in the same folder as the index.js file.
To deploy the function, launch a terminal session and go to the folder in which the index.js and package.json files reside. Assuming that you installed the local gcloud tools and the beta dependencies, you can deploy the two functions via the two commands as shown below:
Ensure that both the functions are deployed correctly via the command shown below:
Since both of these functions are HTTP Trigger based functions, we can get their HTTP endpoint via the describe command as shown below. Remember that we just need the HTTP Trigger Endpoint url for the getRandomQuestion function . The property httpsTrigger is important.
Copy the value of the url and you are all set to invoke that from the browser and see it in Action as given below:
Enjoy your own Quiz ! :-)
You can monitor your Cloud Functions via Stackdriver Logging. It is integrated into both the Google Cloud Console and also Firebase console.
From Google Cloud Console, you can visit Stackdriver Logging and can see the Cloud Functions execution as shown below:
Alternately, if you prefer the Stackdriver logging to be visible via the Firebase console, go to Functions option in the main menu as shown below:
You can then click on the Logs tab, to see the Functions execution and any log statements for your function execution.
This concludes our tutorial on building out a serverless online quiz application via Cloud Functions and Cloud Firestore. There are definitely improvements that we can make to this and here are a few suggestions to work on:
Hope this gives you a good idea on how to use Cloud Functions along with Firestore. Thank you for reading this tutorial. Hope you enjoyed it. If you have any feedback, please let me know.
Technical Tutorials, APIs, Cloud, Books and more.
89 
1
89 claps
89 
1
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
Technical Tutorials, APIs, Cloud, Books and more.
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
Technical Tutorials, APIs, Cloud, Books and more.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/debugging-node-google-cloud-functions-locally-in-vs-code-e6b912eb3f84?source=search_post---------237,"There are currently no responses for this story.
Be the first to respond.
The Google Cloud Functions Framework (FF) is a powerful developer tool that allows developers to debug serverless functions within their favorite IDE!
In this blogpost, we’ll learn how to debug Google Cloud Functions locally using breakpoint support in VS Code via the V8 Inspector Protocol (--inspect).
From your terminal, create a new Node project with the FF using the npm CLI:
Open VS Code with code . and create a new file called index.js with the following stubbed content:
VS Code comes with a built-in Node debugger! But we still need to create config files.
To create a bare-bone project, create a.vscode directory with the following launch.json and settings.json files:
.vscode/launch.json
Note: port 9229 is the default port number for node --inspect.
.vscode/settings.json
This option will automatically attach the Node debugger to VS Code. Useful.
Open the Terminal in VS Code using the menu item View > Terminal. Run the Functions Framework with:
Let’s break that down:
You’ll see something like this:
You’re running the Functions Framework locally with the debugger attached. Go to localhost:8080 to run the function locally!
You should {""sum"":45}. But how do we debug? Read on!
While your node process is running, use VS Code’s Debugger tool by clicking:
Note: If you didn’t create a .vscode/settings.json file, you need to manually attach the debugger using the top-left ▶ icon after running the FF.
To add a breakpoint, click left of the line number to kindle a bright red 🔴.
Then run the function with http://localhost:8080/ to see useful information in the debug side panel. Hover over variables to see their values or look left.
Sure, you could console.log these values, but now you can see every variable on the stack, copy values, and even inject you own variable values. It’s really helpful when trying to solve issues.
OK. So a for loop is something, I guess. But let’s build something more impressive.
The Google Discovery API is a meta-API that lists Google APIs. Calling the API does not require auth (not even a Google account is needed!). Let’s query the API and list out all Google (discoverable) APIs:
npm install googleapis
2. Replace index.js with the following code.
3. Now re-run the function. You should see a large JSON response with a list of Google services.
Cool beans!
Deploying this function to Google Cloud is easy. Just run the following gcloud command:
Then after a bit, you’ll get a httpsTrigger URL like:
Easy peasy (lemon squeezy).
We can even do fancy things like conditional debugging:
Tired of accidentally committing console.log test code in your PRs? Add a Logpoint and keep your code clean. Just right-click to the left of a line number.
Thanks for reading! Check out these related posts:
Google Cloud community articles and blogs
204 
1
204 claps
204 
1
Written by
Google • Explorer
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Google • Explorer
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/ontologynetwork/ontology-is-now-an-official-google-cloud-partner-as-three-ontology-projects-are-accepted-to-the-935edeef3e64?source=search_post---------238,"There are currently no responses for this story.
Be the first to respond.
We are delighted to announce that Ontology has been accepted as an official Google Cloud Partner. The program will see the Ontology development team granted unique access to cutting-edge tools and technology, while also leveraging the support of the wider Google Cloud Partner ecosystem. Three of Ontology’s projects have been accepted into the Google Cloud ecosystem, outlining new, innovative ways that Ontology has been working to integrate Google’s cloud computing services.
Ontology will also take part in Google’s Next On Air event. Starting today, the nine-week virtual event will provide opportunities for Ontology to take part in breakout sessions, digital demos, 1:1s with Google experts, and explore the ecosystem’s wealth of resources for learning and development.
Andy Ji, Co-founder of Ontology said, “Collaborating with Google Cloud offers us the opportunity to demonstrate the real-world business value of integrating blockchain technology with other technological disciplines such as cloud computing, which is a vital step in bringing blockchain to mainstream use. Ontology’s contribution to a leading global project run by Google is yet another high-profile validation of our technological acumen and broad industry experience. We can’t wait to get started.”
As a new official Google Cloud Partner, Ontology will continue to improve upon the full utilization of Google Cloud in the following three Ontology-led projects.
Ontology’s solution for OGQ, a leading global social creator platform for connecting content creators with fans, to protect content copyright ownership based on Google Cloud’s infrastructure.
Ontology’s tech support for Kaiyun, a comprehensive logistics service provider focusing on urban transportation and terminal-end deliveries, to accelerate the formation of a more efficient and easy-to-use transport capacity model utilizing Ontology and Google Cloud.
Ontology’s Dangerous-Goods-Delivery solution for LANXESS, a German specialty chemicals company, integrating Google Cloud Platform benefits, blockchain technology, and IoT technology.
This is yet another milestone moment for Ontology, as we further succeed in integrating our technology with other mainstream disciplines. We look forward to furthering our collaborations with Google Cloud through this new partnership.
Ontology website / ONTO website / OWallet (GitHub)
Twitter / Reddit / Facebook / LinkedIn / YouTube / NaverBlog / Forklog
Telegram Announcement / Telegram English / GitHub / Discord
A high performance, open-source blockchain specializing in digital identity and data.
394 
394 claps
394 
Ontology is a high performance, open source blockchain specializing in digital identity and data. ONTO: http://onto.app/downloadpage/TW  Telegram: http://t.me/OntologyNetwork
Written by
Active project domain: https://ont.io/
Ontology is a high performance, open source blockchain specializing in digital identity and data. ONTO: http://onto.app/downloadpage/TW  Telegram: http://t.me/OntologyNetwork
"
https://medium.com/free-code-camp/decentralize-your-application-with-google-cloud-platform-7149ec6d0255?source=search_post---------239,"There are currently no responses for this story.
Be the first to respond.
When first starting a new software project, you normally choose a certain programming language, a specific framework and libraries. Then you begin coding. After 2 - 3 months you end up with a nicely working single application.
But, as the project grows and more functionalities are added, you quickly realize the disadvantages of a centralized system. Difficult to maintain and unscalable are some of the reasons which will make you search for a better solution. Here is where Microservices come in help.
Microservices are independently built systems, each running in their own process and often communicating with REST API. Representing different parts of your application, they are separately deployable and each part can be written in any language.
You can easily see how, by dealing with the problems of a monolithic system, Microservices have become a requirement for any state-of-the-art software.
I strongly recommend reading Microservices (by James Lewis) and On Monoliths and Microservices if you want to understand more in depth what are the key concepts in this architectural style.
This article will walk you through the process of implementing a Microservice using Google Cloud Platform.
Imagine you’re developing an application that accepts a text input from a user and determine the category of the key words within the input.
We’ll use an example to illustrate the functionality of the App. Consider the sample text below from the GCP Cloud Natural Language API website:
“Google, headquartered in Mountain View, unveiled the new Android phone at the Consumer Electronic Show. Sundar Pichai said in his keynote that users love their new Android phones.”
Our web App would accept the text above as input, and return the category that the key words belong to, as in the figure below:
This feature is quite likeable and people use it hundreds of times each day. Now, if you’re going to offer this functionality as a service that receives a high amount of daily traffic, you want to respond with a stable and reliable system.
That’s why we’ll build a lightweight Flask App, hosted on Google App Engine. Integrating it with Google Cloud Pub/Sub will help us handle all the asynchronous requests we receive and help us assure that users don’t wait too long for a response.
Let’s first start with the Flask app (you can also choose Django, Node.js, Go or anything used to build server-side applications). If you’re not very familiar with developing a Flask App, this Flask Series can show you step-by-step how to set up an application.
For the purpose of this tutorial we will use this simple example:
First you need to install the dependencies pip install Flask gunicorn. You will be using gunicorn to run the application on Google App Engine. For local access you can run python text.py in the console and find the app on port 8080.
To deploy the app to Google App Engine, you need to take these steps:
The app.yaml file looks like this:
Line 3 is important, where you use gunicorn to tell Google App Engine to run the application app from a file called text.py (the Flask app). You can learn more about the .yaml file structure here. After deployment you should be able to access your project from https://[YOUR_PROJECT_ID].appspot.com.
When building production ready applications, you often want to test your code before pushing it live. One way to do this is to run your App within a server locally. A better approach is to have a development version of the app which can be tested not only from your local machine but also from a hosted environment. You can use Google App Engine versions for this.
Just deploy your App with gcloud app deploy -v textdev (for development) or gcloud app deploy -v textprod (for production).
Then navigate to https://textdev.[YOUR_PROJECT_ID].appspot.com or https://textprod.[YOUR_PROJECT_ID].appspot.com to access the specific version.
So far so good. You have a working application, hosted on the Google Cloud Platform. Now you need to add Google Cloud Pub/Sub and Google Natural Language API.
But first, let’s explain the architecture.
Once a request is received, the Flask app will publish a message with the text to a topic (created below). Then a subscriber (Python script) will pull this message and apply the Google Natural Language API to each text. Finally, the result will be saved to a database.
For multiple requests, the app asynchronously publishes them to the topic and the subscriber starts executing the first one. When ready, it picks the second one and so on.
Now you need to modify text.py file:
The code on line 15 and 16 creates the publisher. On line 18 it publishes a message containing the user email and text input.
You only need to fill in the project_id and topic_id (line 6 and 7).
Since the project_id was used earlier, just add it here.
For the topic_id you need to do the following:
Wonderful! Now you have a working publisher.
Let’s jump into setting up the subscriber. There are two files that need to be created: worker.py and startup-script.sh.
The worker.py looks like this:
The file is slightly larger but we will examine it step-by-step, starting from the bottom.
When the file is executed, the code on line 44 runs main(). This function sets the subscriber with your project_id and subscription_id and assigns a callback to it.
The callback (initialized on line 7) is going to receive all messages and perform the required task (to determine the category of a text). If you follow the code from the callback, you can easily see how the Google Natural Language API is being used.
The interesting line is 11 where message.ack() acknowledges the current message. You can see this is as if the worker is saying: “I am done with this message and ready to handle the next one”.
Now, you need to implement startup-script.sh.
This is a shell script with several commands:
Before explaining the code above, I need to clarify the process.
Basically, Google Cloud Compute Engine gives you the ability to scale an application by providing as many virtual machines (VM) as needed to run several workers simultaneously.
You just need to add the code for the worker, which you already have, and set the configurations of the VM. Together with the worker.py, you also need to add a startup-script.sh which will run every time a new VM boots up.
New VM instances are booted up to prevent delay in responses when a high number of messages is received.
For a deeper and more technical explanation of this process check out the documentation.
Now, let me walk you through the script:
Now you need to upload worker.py and startup-script.sh to your storage and set up the VM. To upload the files just go here and create a new bucket with the same name as your project id. Create a folder called workers and upload the scripts inside. Make sure to change the worker.py to a ‘Public link’ and edit the permissions of the startup-script.sh to have your service account as an owner.
The final step is to set up the configurations of the VM and test the system. Just follow the ‘Create an instance template’ instructions from the documentation and you are good to go!
Once the VM boots up, you can try sending requests to your application and examine how it reacts by checking the logs.
Going through Google’s documentation may help you a lot. Also check out this tutorial - you may find it useful while implementing some of the steps above.
I want to express my gratefulness to Logan Allen for helping me better understand this process. I hope you find it useful.
Leave any questions or suggestions in the comment section.
We’ve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
286 
3
286 claps
286 
3
We’ve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
Written by
Obsessed with creating a positive impact. Love blogging about AI and reading books. For more content, follow me 👉 https://www.linkedin.com/in/simeonkostadinov/
We’ve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
"
https://levelup.gitconnected.com/google-cloud-bigquery-vs-aws-redshift-a4d7238f1867?source=search_post---------240,"For those of us that have to manage large data sets across massive organizations with diverse needs, managing your own set of servers and applications has proven to be beyond difficult.
Thus the rush to the cloud.
Google and Amazon are some of the largest providers for cloud services! Many…
"
https://medium.com/hacking-and-slacking/google-cloud-platform-creating-an-instance-and-configuring-dns-7043875fd7d4?source=search_post---------241,"Sign in
There are currently no responses for this story.
Be the first to respond.
Todd Birchard
Jul 15, 2018·7 min read
Compare the advantages of Google’s Compute Engine to other cloud providers.
I’ve been a big fan of Google Cloud Platform from the beginning, and they aren’t even paying me to say that. A lot of critics place GCP far enough behind AWS and Azure that most people haven’t bothered to consider the “third option,” but this leaves much of the story untold.
"
https://medium.com/dos-network/dos-network-joins-google-cloud-partner-advantage-program-as-a-build-partner-903384f0e2e9?source=search_post---------242,"There are currently no responses for this story.
Be the first to respond.
Happy New Year to our DOS Network community!
As part of our effort to achieve the DOS 2.0 vision, we are extremely proud to announce that DOS Network is now an official Google Cloud Build Partner in in Google Cloud’s Partner Advantage Program. The DOS Network team has begun planning to migrate various blockchain, backend, and database solutions onto the Google Cloud Platform.
DOS Network provides a decentralized, layer 2 oracle service capable of connecting all mainstream public blockchain networks and on-chain applications. The oracle service offers DOS Network’s customers with reliable off-chain data integration through a fast, secure data channel. As a Google Cloud Partner, DOS Network will immensely benefit from Google Cloud’s product suite, which will unlock efficient cloud computing, big data analysis, and machine learning capabilities. The DOS Network team will also have access to Google Cloud’s team of cloud computing experts, enabling us to innovate and iterate at unprecedented speed.
“Google Cloud has a vibrant community of industry professionals and the trustworthiness and reliability associated with the Google Cloud name are well-documented.” Jerry Liu, Chief Technology Officer at DOS Network, said. “The relationship we have with Google Cloud will allow us to meet our technical needs as part of our mission to bridge the gap between blockchain and real-world data and push for more blockchain adoption.”
DOS Network’s membership in the Google Cloud Partner Advantage Program represents our commitment to building future-proof decentralized data services. Our team is now better equipped to help our customers solve their oracle problems. Specifically, we are planning to implement the following objectives:
The off-chain component of DOS Network is a client implementing the core protocol run by third party users (as known as node operators), which together form a layer 2 distributed network serving oracle requests in a decentralized manner. Node operators are the most important part of the DOS Network ecosystem and their node setup requires stable and reliable servers. Google Cloud Platform is the ideal infrastructure provider, as it has a 99.9% availability service-level agreement (SLA), making it one of the preferred choices for DOS Network node runners. Interested DOS Network node operators are invited to contact us and join the Node Runners Telegram group.
As a data middleware service, DOS Network will be able to explore and unlock brand new use cases for our customers with the vast amount of data available in the Google Cloud ecosystem. More than 90% of searches worldwide are driven by Google’s cloud infrastructure. Google Cloud offers valuable public and commercial datasets across different industries. More specifically, Google Cloud has a public dataset marketplace, BigQuery public datasets, and Cloud Storage public datasets. These datasets allow DOS Network team to connect our service users with the data they want.
The DOS Network team is excited to be a Google Cloud Build Partner. We will work tirelessly in our pursuit to connect on-chain and off-chain services and promote the global-scale adoption of decentralized applications. The future is bright.
Boost Blockchain Usability with Real World Data and…
706 
2
706 claps
706 
2
Boost Blockchain Usability with Real World Data and Computation Power.
Written by
Boost Blockchain Usability with Real World Data and Computation Power.
Boost Blockchain Usability with Real World Data and Computation Power.
"
https://medium.com/google-cloud/simple-google-api-auth-samples-for-service-accounts-installed-application-and-appengine-da30ee4648?source=search_post---------243,"There are currently no responses for this story.
Be the first to respond.
I’ll be adding updates to my new blog here: https://salrashid123.github.io/
This article describes the various mechanisms to access GCP Services using our APIs. I find it pretty confusing to keep track of all the various ways to access a service and that coupled with the changes in the library set accross languages, i often lose trac….so I’ve kept this repo as a running reference. Hope you find some of the samples useful.
This article will only describe the libraries in general terms but point back to a gitRepo for all the code samples. What i’ll describe is the following:
Revisions:
github.com
As an introduction, here is a quick summary of the various libraries:
The samples use Application Default Credentials which uses credentials in the following order as described in the link. Set the environment variable to override.
You can always specify the target source to acquire credentials by using intent specific targets such as: ComputeCredentials, UserCredentials or ServiceAccountCredential.
There are two types of client libraries you can use to connect to Google APIs:
The basic differences is the Cloud Client libraries are idomatic, has gcloud-based emulators and much eaiser to use.
It is recommended to use Cloud Client Libraries whereever possible. Although this article primarily describes the API Client libraries, the python code section describes uses of Cloud Client libraries with Google Cloud Storage.
For more information, see
This article also describes how to use IAM’s serviceAccountActor role to issue access_tokens, id_tokens and JWT. For more information on that, see Using serviceAccountActor for impersonation.
The following examples use the Oauth2 service to demonstrate the initialized client using Google API Client Libraries. The first section is about the different client libraries you can use.
Cloud Client Libraries and API Client Libraries
→ Cloud Client Libraries
→ API Client Libraries
→ serviceAccountActor role for impersonation
For more information, see:
This code samples contained in this article can be found on my github repo.
As described in the introduction, this section details the two types of libraries you can use to access Google Services:
These libraries are idiomatic, easy to use and even support the gcloud-based emulator framework. This is the recommended library set to use to access Google Cloud APIs.
For more information, see:
The following example describes various ways to initialize a service account to list the Google Cloud Storage buckets the account has access to. It also shows listing the buckets using the default account currently initialized by gcloud.
To use the mechanisms here, you need to initialize gcloud’s application defaults:
See:
medium.com
Conclusion
Navigating GCP libraries is pretty confusing at the moment. I hope the git repos cited here help atleast bootstrap. If you find this useful, +1 this article…or better yet, if you find something i’ve missed in the git repo or should add, please let me know or file a PR!. thanks
Google Cloud community articles and blogs
81 
2
81 claps
81 
2
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/@wkrzywiec/how-to-publish-a-spring-boot-app-with-a-database-on-the-google-cloud-platform-614b88613ce3?source=search_post---------244,"Sign in
There are currently no responses for this story.
Be the first to respond.
Wojciech Krzywiec
Oct 15, 2018·9 min read
I’ve found myself in such situation many times. I was done with my web service project and I wanted to share it with my friend, but my only options were either send a link to my GitHub repository (to deploy it on a local machine), or I bring my laptop to her/him. Not so convenient, right? In this blog post I’ll try to fix it, so everyone could check my awesome apps over the Internet. And for that I’ll use the Google Cloud Platform.
As already mentioned I’ve recently worked on my first Spring Boot RESTful web service Library API. It’s a simple app that process few HTTP requests to access/modify resources that are in MySQL database. It works fine on a local machine, but I want to move into next level to publish it on the Google Cloud Platform.
If you already has a Google account you might be familiar with Google Drive service which offers “free” storage space for your files and it can be accessible from any part of the planet. It works, as we call it Software-as-a-Service (SaaS), which means that Google hosts Google Drive application on their servers and allows users to access it from the Internet.
Other delivery model, in cloud computing world, is called Platform-as-a-Service (PaaS), and on the contrary to SaaS the provider gives the infrastructure to the customers, so they can deploy and run their application in the Cloud. Google Cloud Platform (GCP), along with Amazon Web Services (AWS) and Microsoft Azure, provides couple services that could be assigned to this category.
One of them is Cloud SQL which is used for MySQL and PostgreSQL database management. Which provides not only the storage, but also other features like backup.
Another one is Google App Engine (GAE). It provides an easy way to deploy an app that is written in languages like Java, Python, C#, PHP, Node.js, Ruby and Go. It also supports Docker images deployment.
With Google App Engine we don’t need to worry about:
If you want to read more about Cloud SQL and App Engine capabilities check their official documentation here and here, respectively. Except explaining how they works they also guide how to make a use of each feature with step-by-step approach.
Before I jump into next section, make sure that you have go thru all these steps:
Once we are done with basic set up we can add MySQL instance to the project. Therefore go to you project dashboard and select SQL from Navigation menu (top left corner).
You should get a page with a single window. Click on Create instance button and then choose MySQL as database engine. On a next page select MySQL Second Generation (my database is MySQL 5.7, which is supported only by this generation).
Finally we need to configure the database. My application won’t be handling big traffic on run, so I decided to pick the lowest set up I can get (pricing for such are much lower).
Below there are screenshots of my config. To see the whole configuration options click on Show configuration options. I’ve kept almost all properties as defaults, except for Machine type and storage and Backup feature.
After couple of minutes the Cloud SQL will be up and running.
Next, enter SQL instance dashboard, which can be picked from the list of instances. Then, move to Users tab and click Create user account button, where you can provide username and password.
Next go to Databases and click Create database button. In the pop up provide database name.
My project has Flyway implemented which will create all necessary tables during app deployment, so I don’t need to run any script at this point. But if you would like to run them manually, here are the instructions.
The database is now set up. The only thing that we’ll need from this point is the Instance Connection Name, which can be read from the SQL Instance Dashboard, and is required to establish connection between app and database.
The base project is a Spring Boot app which cannot be simply packed into WAR file and copy-paste into Google App Engine. It requires some modifications, but luckily not so much.
First of all, GAE service uses Jetty webserver/servlet container, but Spring Boot per default uses Tomcat. Therefore we need to update build.gradle file.
With the first line we tell Gradle that we want to exclude the embedded Tomcat dependencies (they will conflict with Jetty), in second we explicitly add Java Servlet dependencies.
Last thing to do is to remove JUL to SLF4J bridge that interferes with App Engine’s log handler that’s provided through Jetty server.
The app is now running on Jetty server, so we can move on to add Google Cloud dependencies. Below code snippet presents all required plugins and dependencies that needs to be added to Gradle build file. They enable Gradle tasks for GAE deployment and add dependencies that allows internal connection between the GAE app and Cloud SQL. More information about App Engine Gradle tasks can be found here.
Next we need to add app.yaml configuration file into the project (into src/main/appengine directory). It contains information about the URL, destination GAE service where it will be deployed or general settings for scaling. In my app it looks like as follows:
Note. Most of the tutorials over the Internet doesn’t include lines with resource settings. I’m doing it because I’ve faced some difficulties during start up when they have not been provided. But if you work on your own project you probably won’t need that.
Last thing is to create a class that extends SpringBootServletInitializer class. It is required for traditional WAR file deployment and which is not generated automatically by Spring Boot Initializr. I’ve decided to not create a new class but to extend the main.
Project is set up, so the only thing to do is to deploy it on GAE. To do that we need to first authenticate on ourselves with the Google Cloud Platform using Google Cloud SDK Shell (installed on a locally) and typing following command:
Above command will trigger web-based authentication process, after which you will be able to access GCP from the command line and be able to deploy an app.
As a GCP user you can have multiple projects within it so there is last thing to — we need to explicitly decide into which project we want to deploy a software. Try this:
Second command prints the default project to operate on.
Finally we can run the appengineDeploy Gradle task (from your IDE). You’ll need to wait several minutes but after that your app will be successfully deployed 😄.
So you think that all of these came up really quick for me? No at all ;)
It was first time when I was playing around Google Cloud and I must say that for a first project it was quite challenging.
At the beginning I thought that the whole transition, from local Spring Boot app to Google Cloud deployed one will go really smooth and will be done in a day or two. But nothing goes as it was planned as it should be (but when it does?😏).
The problems were that even though appengineDeploy Gradle task says that build has been successful but it doesn’t run. There could be several reasons for that, but here are the problems I’ve stomp
As usual here is the link to the entire project on GitHub.
github.com
cloud.google.com
cloud.google.com
github.com
github.com
github.com
cloud.google.com
codelabs.developers.google.com
github.com
cloud.google.com
cloud.google.com
dzone.com
docs.spring.io
stackoverflow.com
Java Software Developer, DevOps newbie, constant learner, podcast enthusiast.
151 
4
151 
151 
4
Java Software Developer, DevOps newbie, constant learner, podcast enthusiast.
"
https://medium.com/google-cloud/google-cloud-storage-tutorial-part-1-aee81f9d3247?source=search_post---------245,"There are currently no responses for this story.
Be the first to respond.
During the past few days, I’ve been diving into Google Cloud Storage (GCS). You might assume that this is a boring aspect of Google Cloud Platform because cloud storage has been around for awhile, but I found some pleasant surprises in my exploration. Let’s start with a brief overview:
Below is a simple example of storing and retrieving files. First we need to create a bucket — the basic container used to store files. Since all bucket names share the global name space, it needs to be unique across all of Google Cloud. You’ll see why this is the case in a later post when we start sharing files publicly via http. For this example, I’ll use a bucket named “gwbucket”. When referencing a bucket you need to specify the URL — e.g. gs://[bucketname].
Nothing too exciting yet, but you quickly see that the commands are familiar. You can add “-r” to most commands to make it recursive, as expected. Detailed help is available for all commands at https://cloud.google.com/storage/docs/gsutil or from the command line with gsutil help cp
Here’s another example using rsync and versioning:
Learn more about object versioningLearn more about the -m option
Note: The above examples use the default bucket class of “standard” and the default bucket location of “US”. To learn about other options, see these docs.
I used the last example above (except for the versioning line) to backup my 120,000+ image library to Google Cloud Storage. Anytime I add/modify/delete images, I simply repeat the gsutil rsync.
(Be very careful using the -d option on rsync as it deletes any files from the destination that have been removed from the source. I suggest using the <strong>-n</strong> option if you have any doubts. The -n option causes rsync to run in “dry run” mode, i.e., just outputting what would be copied or deleted without actually doing any copying/deleting .)
In my next post, I’ll show how to set up bucket object lifecycle management to configure automatic object deletion when it reaches a certain age, or to simply keep the last n versions of a specific object. This becomes a key feature when doing regular backups such as archiving log files each night, etc. Then I’ll show you how to share objects publicly via HTTP and how to utilize Google’s worldwide edge caching to provide very fast downloading of your files.
Google Cloud community articles and blogs
81 
2
81 claps
81 
2
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Google Developer Relations — Living in San Francisco https://gregsramblings.com | https://cloud.google.com | http://instagram.com/gregsimages
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/@jaychapel/4-ways-to-get-google-cloud-credits-c4b7256ff862?source=search_post---------246,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Feb 24, 2020·4 min read
Google Cloud credits are an incentive offered by Google that help you get started on Google’s Cloud Platform for free. Like Amazon and Microsoft, Google is trying to make it easy and in some cases free to get started using their Cloud Platform or certain services on their platform that they believe are “sticky” — which is beneficial if you’d like to try the services out for personal use or for a proof-of-concept. There is both a spend and a time limit for Google’s free credits, but then they also offer “always free” products that do not count against the free credit and can be used forever, or until Google decides to pull the plug, with usage limits.
The most basic way to use Google Cloud products is the Google Cloud Free Tier. This extended free trial gives you access to free cloud resources so you can learn about Google Cloud services by trying them on your own.
The Google Cloud Free Tier has two parts:
The Google Cloud 12-month free trial and $300 credit is for new customers/trialers. Be sure to check through the full list of eligibility requirements on Google’s website. (No cryptomining — sorry!)
Before you start spinning up machines, be sure to note the following limitations:
Your free trial ends when 12 months have elapsed since you signed up and/or you have spent your $300 in Google Cloud credit. When you use resources covered by Always Free during your free trial period, those resources are not charged against your free trial credit.
At the end of the Free Trial you either begin paying or you lose your services and data, it’s pretty black and white, and you can upgrade at any time during your Free Trial with any remaining credits being applied against your bill.
The Always Free program is essentially the “next step” of free usage after a trial. These offerings provide limited access to many Google Cloud resources. The resources are usually provided at monthly intervals, and they are not credits — they do not accumulate or roll over from one interval to the next, it’s use it or lose it. The Always Free is a regular part of your Google Cloud account, unlike the Free Trial.
Not all Google Cloud services offer resources as part of Always Free program. For a full list of the services and usage limits please see here — a few of the more popular services include Compute Engine, Cloud Storage, Cloud Functions, Google Kubernetes Engine (GKE), Big Query and more. Be sure to check the usage limits before spinning up resources, as usage above the Always Free tier will be billed at standard rates.
Google is motivated to get startups to build their infrastructure on Google Cloud while they’re still early stage, to gain long-term customers. If you work for an early-stage startup, reach out to your accelerator, incubator, or VC about Google Cloud credit. You can get up too $100,000 in credit — but it will come at the price of a large percentage of equity.
Options that don’t require you to give up equity include Founder Friendly Labs, StartX if you happen.
Google offers several options for students, teachers, and researchers to get up and running with Google Cloud.
There are also several offerings related to making education accessible without associated credits. See more on the Google Cloud Education page.
Various vendors that are Google Cloud partners run occasional promotions, typically in the form of a credit greater than $300 for the Google Cloud Free Trial, although we’ve also seen straight credits offered. For example, CloudFlare offers a credit program for app developers.
Also check out events that might offer credit — for example, TechStars startup weekends offers $3,000 in Google Cloud credits for attendees. Smaller awards of a few hundred dollars can be found through meetups and other events.
Google Cloud Credits do offer people and companies a way to get started quickly, and the Always Free program is a unique way to entice users to try different services at no cost, albeit in a limited way. Be sure to check out the limitations before you get started, and have fun!
Further reading:
Originally published at www.parkmycloud.com on February 18, 2020.
CEO of ParkMyCloud
133 
1
133 
133 
1
CEO of ParkMyCloud
"
https://medium.com/@jaychapel/aws-vs-azure-vs-google-cloud-market-share-2019-what-the-latest-data-shows-dc21f137ff1c?source=search_post---------247,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Jul 12, 2019·5 min read
Q1 earnings are in for the ‘big three’ cloud providers and you know what that means — it’s time for an AWS vs Azure vs Google Cloud market share comparison. Let’s take a look at all three providers side-by-side to see where they stand.
Note: a version of this post was originally published in April 2018. It has been completely rewritten and updated for 2019.
To get a sense of the AWS vs Azure vs Google Cloud market share breakdown, let’s take a look at what each cloud provider’s reports shared.
Amazon reported Amazon Web Services (AWS) sales of $7.7 billion, compared to $5.44 billion at this time last year. AWS revenue grew 41% in the first quarter — at this time last year, that number was 49%.
Across the business, Amazon’s growth rates are slowing down — which perhaps is all that can be expected at their mammoth size. However, their profit margins are increasing, giving investors a boon of $7.09 earnings per share compared to the projected $4.72.
AWS has been a huge contributor to this growth. This quarter, AWS revenue makes up 13% of total Amazon sales, up from 10% in the fourth quarter. AWS only continues to grow, and bolster the retail giant time after time.
In media commentary, AWS’s numbers seem to speak for themselves:
While Amazon breaks out revenue from AWS separately, Microsoft has a more nebulous “commercial cloud business” — which includes not only Azure, but Office 365, Dynamics 365, and other segments of the Productivity and Business Processes Division. This fact frustrates many pundits as it simply can’t be compared directly to AWS, and inevitably raises eyebrows about how Azure is really doing. Microsoft reported that the commercial cloud business grew 41% in the first three months of 2019, to $9.6 billion.
What Microsoft reported for Azure specifically is the growth rate: 73%. However, Microsoft did not specify what that growth actually represents. This time last year, the Azure growth rate was reported at 93%. Supposedly, analysts say that Azure is growing at a faster rate than AWS was at a similar size, but without specific numbers, it’s hard to say what this actually means.
Here are a few headlines on Microsoft’s reporting that caught our attention:
Like Microsoft, Google avoided reporting specific revenue numbers for its cloud business yet again. Parent company Alphabet reported $36.34 billion in revenue for the quarter, up 17% from $31.15 billion for the same quarter last year. Google Cloud Platform revenue is included in Google’s “other” revenue category, alongside G Suite, Google Play, and hardware such as Nest. That category reported revenue of $5.45 billion for the quarter, up 25% from the same quarter last year when it was $4.25 billion.
According to Google and Alphabet CFO Ruth Porat, “Google Cloud Platform remains one of the fastest growing businesses in Alphabet with strong customer momentum reflected in particular in demand for our compute and data analytics products”. But without specifics, it’s hard to say what this means.
Further reading on Google’s quarterly reporting:
When we originally published this blog last year, we included a market share breakdown from analyst Canalys, which reported AWS in the lead owning about a third of the market, Microsoft in second with about 15 percent, and Google sitting around 5 percent.
This year they report an overall growth in the cloud infrastructure market of 42%. By provider, AWS had the biggest sales gain with a $2.3 billion YOY increase, but Canalys reports Azure and Google Cloud with bigger percentage increases.
Ultimately, it seems clear that in the case of AWS vs Azure vs Google Cloud market share — AWS still has the lead.
Bezos has said, “AWS had the unusual advantage of a seven-year head start before facing like-minded competition. As a result, the AWS services are by far the most evolved and most functionality-rich.”
Our anecdotal experience talking to cloud customers often finds that true, and it says something that Microsoft and Google aren’t breaking down their cloud numbers just yet.
Others have made their own estimates. In November, a Goldman Sachs report stated that AWS, Azure, Google Cloud, and Alibaba Cloud made up 56% of the total cloud market, with that projected to grow to 84% this year. The report shows AWS far, far in the lead with 47% of the market projected for this year, with Azure and Google trailing at 22% and 8% market share, respectively.
AWS remains far in the lead for now. With that said, it will be interesting to see how the actual numbers play out, especially as Google positions itself for multi-cloud and Azure continues rapid growth rates. Perhaps this time next year will report revenue numbers broken out and we’ll be able to say for sure.
Originally published at www.parkmycloud.com on April 30, 2019.
CEO of ParkMyCloud
41 
2
41 
41 
2
CEO of ParkMyCloud
"
https://medium.com/firebase-developers/using-the-firebase-admin-sdk-for-net-in-google-cloud-run-e44b12758bf7?source=search_post---------248,"There are currently no responses for this story.
Be the first to respond.
Recently I was reading Patrick Martin’s article on deploying a C# backend in Google Cloud Run, and I figured it would be fun to write a follow up piece featuring the Firebase Admin SDK for .NET. Hence, in this post I show how to implement a custom authentication service in C#, and deploy it as a serverless API using Cloud Run. This service validates user credentials, and uses the Admin SDK…
"
https://levelup.gitconnected.com/how-to-parse-forms-using-google-cloud-document-ai-68ad47e1c0ed?source=search_post---------249,"Many business processes, especially ones that involve interacting with customers and partners, involve paper forms. As consumers, we are used to filling out forms to apply for insurance, make insurance claims, specify healthcare preferences, apply for employment, tax withholdings, etc. Businesses on the other side of these transactions get a form that they need to parse, extract specific pieces of data from, and populate a database with.
Google Cloud Document AI comes with form-parsing capability. Let’s use it to parse a campaign disclosure form. These are forms that every US political campaign needs to file with the Federal Election Commission. An example of such a form:
The form(s) need to be loaded to Cloud Storage for Document AI to be able to access it. The full code can be found in this notebook on GitHub.
To invoke Document AI, we need to create a request JSON structure that looks like this:
We then send it to the service:
The response is a JSON document that we can parse to pull out the data that we want. The response has a text field that contains all the extracted text. You can get that by:
For example, the text includes the words “Cash on Hand” because it appears in the form:
Let’s say that we want to find the actual amount ($75,931.36) that the campaign currently has on hand. This information is available on the second page of the form. Because of this, we can look for page=1 (it starts at page=0) and look at either the text blocks or at the extract form fields.
The text blocks is more low-level; the form fields is a higher-level abstraction. Let’s look at both.
This, for example is how to get block=5 on page=1:
The block itself is a JSON struct:
We can parse it in turn to get the textSegment’s start and end index:
Using the start and end index into allText:
gives us:
Well, that was block=5. What is block=6?
Yup, $75,931.36.
Document AI understands that this form consists of name-value pairs. So, we can parse the JSON response at that level. Let’s write a helper function first:
To get the third form field on the 2nd page, we’d do:
This gives us the following structure:
So, we can extract the field name and field value using:
Enjoy!
Coding tutorials and news.
129 
2
129 claps
129 
2
Written by
Data Analytics & AI @ Google Cloud
Coding tutorials and news. The developer homepage gitconnected.com && skilled.dev
Written by
Data Analytics & AI @ Google Cloud
Coding tutorials and news. The developer homepage gitconnected.com && skilled.dev
"
https://read.acloud.guru/will-spotify-be-to-google-cloud-as-netflix-is-to-aws-9efb49e59022?source=search_post---------250,"Most-read and must-read. Dig into news, insights, and assorted awesomeness around all things cloud, certifications, and the pursuit of modern tech skills.
Ahh, that new blog smell. Read up on the most recent updates for individuals and businesses, plus what’s new from ACG.
Say hello to the ACG blog crew — a mix of instructors, industry pros, and cloud enthusiasts. We’re write up your alley.
ACG Co-founder + O.G. AWS Guru
AI/ML Maestro
Cloud Adoption Whiz
 We like cloud. You like cloud. Let’s keep in touch.
Cybersecurity Junkie
Azure Geek Extraordinaire
Enterprise Talent Transformation Sage
"
https://medium.com/google-cloud/asynchronous-code-execution-with-google-cloud-tasks-9b73ceaf48c3?source=search_post---------251,"There are currently no responses for this story.
Be the first to respond.
No application is an island — every non-trivial application has dependencies and exists alongside other services, sharing resources and data. Over the years, messaging has evolved from static, synchronous point-to-point models to asynchronous mechanisms.
In this blogpost, we’ll learn what Google Cloud Tasks, an asynchronous point-to-point queuing system for code execution, where you could benefit from using Cloud Tasks, and how to get started.
Cloud Tasks is a queuing system service for Google Cloud Platform for managing the execution, dispatch, and delivery of a large number of distributed tasks.
With Cloud Tasks, you can add millions of “Tasks” in a queue. You may also create many queues. The queue processes Tasks in the queue, sending an HTTP request to the target URL or executing an App Engine application handler, depending on your setup.
An example diagram of this relationship is below.
Using Cloud Tasks can increase the reliability, manageability, and responsiveness of your applications. For example, Cloud Tasks works particularly well with decoupled services built with Cloud Functions and Cloud Run.
Cloud Tasks includes these features:
Dispatch tasks that reach any target within GCP and on-prem systems.
Offload heavyweight, background and long running processes to a task queue, allowing an application to be more responsive to users.
Control the rate at which tasks are dispatched to your service to ensure your microservices doesn’t get overwhelmed.
With each task is persisted in storage and retried automatically, your infrastructure is resilient to intermittent failures.
Microservices often don’t talk to each other via direct request/response, but rather asynchronously, allowing services to scale independently. Cloud Tasks helps you better structure and scale your application via dedicated independent, configurable Task queues.
This allows you to do things like pause a Tasks queue, or configure independent retry policies.
Other features include scheduling a Task for up to 30 days, Queue rate limiting, configurable retries, Task deduplication, and Task handler auth.
Cloud Tasks can be used to call Google App Engine, Cloud Run, Cloud Functions, or any HTTP server.
Here are some example use-cases:
So why would you use Cloud Tasks versus Pub/Sub?
The core difference between Cloud Pub/Sub and Cloud Tasks is the notion of implicit vs explicit invocation.
Cloud Pub/Sub supports implicit invocation: a publisher implicitly causes the subscribers to execute by publishing an event.
By contrast, Cloud Tasks is aimed at explicit invocation where the publisher retains full control of execution.
A detailed comparison and handy side-by-side comparison can be found here.
How can you use Cloud Tasks? To control Cloud Tasks, you have a variety of tools, for manual and programmatic use:
At console.cloud.google.com/cloudtasks, you are presented an interface for viewing all queues and controls for resuming/pausing the queue, deleting the whole queue or individual Tasks, as well as purging/deleting all Tasks.
One of the easiest ways to use Cloud Tasks is by using the command line.
With gcloud, you can easily create Tasks with a command like such:
Ignore the warning about queue.yaml and queue.xml.
This creates a new queue which can contain thousands of Tasks.
Programmatically, we can use Tasks with Google API clients, for example, the Node client:
Thanks for reading! That’s a brief introduction to Cloud Tasks.
Google Cloud community articles and blogs
61 
61 claps
61 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Google • Explorer
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/how-to-test-google-cloud-services-locally-in-docker-d74196147841?source=search_post---------252,"There are currently no responses for this story.
Be the first to respond.
The “gcloud” command-line tool is amazing, it is super simple and works without problems and you authenticate with that it opens a browser window that then sets the correct values, something that is much easier than generating access and secret token that you have to manually set in your bash config.
However with that comes the question, how do you get that auth inside a docker container running locally.
Especially now that “gcloud auth login” says:
We need to run this command instead that will write down the authenticating information down into a file
That saves it to a file here:
Now you need to change your docker-compose.yml file to have this:
This will make the google-cloud package look for the right place for authentication.
Success, you can use gcloud components inside docker.
Google Cloud community articles and blogs
150 
3
150 claps
150 
3
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
I really like building stuff with React.js and Docker and also Meetups ❤
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/how-reliable-is-google-cloud-4d219a4f7e56?source=search_post---------253,"There are currently no responses for this story.
Be the first to respond.
Nothing is 100% reliable. When designing application architecture, you have to assume there will be failures. Historically, this has meant deploying across racks, rooms and data centres to ensure that local switch, power and geographic incidents do not affect your entire infrastructure.
When deploying on the cloud, this translates to deploying across zones and regions. A zone is supposed to be isolated from…
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://heartbeat.comet.ml/creating-a-tensorflow-lite-object-detection-model-using-google-cloud-automl-d83f997c1848?source=search_post---------254,"Sign in
What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Top highlight
Harshit Dwivedi
Jan 13, 2020·7 min read
Following up on my last blog post on training an image labeling model using Google Cloud AutoML (linked below), in this second blog post in the series; we’ll look into how to train yet another model to identify and locate objects within an image instead—an object detection model!
If you haven’t read my blog on image labeling, you can read it here:
heartbeat.comet.ml
Contrary to image labeling (or image classification), where the model labels an input image according to certain classes or categories, an object detection model will instead detect the objects (that you have trained) from an image along with their location(s).
Here’s an image showcasing the differences between these two techniques:
As you can see, on the right image, we not only get the detected object (a dog), but we also get the bounding box around the area containing a dog.
Such models are extremely useful when you want to do manipulate the detected objects—for example, maybe you want to extract the image of a dog from the image and replace it with something else. For such scenarios, this kind model and ML task can be extremely useful!
Training such a model might sound difficult. But thankfully, to help democratize machine learning, we have Google’s Cloud AutoML Vision, which is a tool provided by Google that can help make the process of training such a machine learning model much easier, without having to write a single line of code!
Using AutoML also eliminates the need for a high-end PC to train your model. You can quickly offload the training process to Google‘s servers and then export the trained edge flavor of the model as a tflite file to run on your Android/iOS apps.
I recently used this product to train a custom object detection model for my AfterShoot app that can identify human beings from a given image. Here’s how the model performs:
Let’s now explore how we can train a similar model of our own in less than 30 minutes :)
Go to https://console.cloud.google.com and log in with your account or sign up if you haven't already.
All your Firebase projects use parts of Google Cloud as a backend, so you might see some existing projects in your Firebase Console. Either select one of those projects that are not being used in production or create a new GCP project.
Once in the Console, open the sidebar on the right and navigate to the very bottom till you find the Vision tab. Click on that.
Once here, click on the Get Started button displayed on the Object Detection card. Make sure that your intended project is being displayed on the top dropdown box:
You might be prompted to enable the AutoML Vision APIs. You can do so by clicking the Enable API button displayed on the page.
Lastly, click on the New Dataset button, give it a proper name, and then select Object Detection in the model objectives.
Once the dataset is created, you’ll be asked to upload some images to be used in the training process, along with the location of the Cloud Storage Bucket used to store those images.
Since this model will be used to detect humans, I’ve already prepared a dataset containing images of humans. If you want to prepare your own dataset easily, I’ve explained how to do so in a previous blog post, which you can find here:
heartbeat.comet.ml
Once you have your dataset prepared locally, click on Select Images and choose all the images that you need your model to be trained on. After this, click on the Browse button next to the GCS path and select the folder named your-project-name.appspot.com:
Once done, press Continue and wait for the import process to complete.
Once the importing is finished, click on the Images tab and you should see all your imported images there. From here, click on the Add New Label button and name the label(s) that you want to identify in the image. I personally want to identify humans from a picture, so I will create a single label called “human”:
Once you’ve created a label, the next step is to teach the model what a human looks like! To do this, open the imported images and start drawing bounding boxes around humans in each image:
You need to do this for at least 10 images, but Google recommends that you do it for 100 images to have better accuracy. Once you have enough images labeled, we then move on to the next and final step.
Once you’ve annotated enough images, it’s time to train the model. To do this, head over to the Train tab and click the button that says Start Training.
You will be prompted with a few choices once you click on this button. Make sure that you select Edge in the first choice as opposed to Cloud-Based if you want tflite models that you can run locally.
Press Continue and choose how accurate you need the model to be. Note that more accuracy means a slower model and vice versa:
Lastly, select your preferred budget and begin training. For Edge-based models, the first 15 hours of training is free of cost :
Once model training has started, sit back and relax! You’ll get an email once your model has been trained.
Once the model has been trained, you can proceed to the Test & Use tab and deploy the trained model.
Once the model has been deployed, you can upload your images to the Cloud Console and test the model’s accuracy.
You can also download the tflite file on your local system and load it in your app to implement the same functionality there:
Note: You are billed for the time that your model is deployed on Google Cloud, so be sure to remove your deployment from the Test & Use tab once you’ve downloaded the .tflite file to prevent incurring extra charges. For more info, have a look at the Google Cloud AutoML pricing page : https://cloud.google.com/vision/automl/pricing
And that’s it! You can add more images to your dataset, annotate them, and retrain the model if you want to detect more objects or want a more accurate model.
I’ll be writing another blog post soon on how to use the obtained tflite model to detect objects in an Android app—so keep an eye out for that!
Thanks for reading! If you enjoyed this story, please click the 👏 button and share it to help others find it! Feel free to leave a comment 💬 below.
Have feedback? Let’s connect on Twitter.
Editor’s Note: Heartbeat is a contributor-driven online publication and community dedicated to providing premier educational resources for data science, machine learning, and deep learning practitioners. We’re committed to supporting and inspiring developers and engineers from all walks of life.
Editorially independent, Heartbeat is sponsored and published by Comet, an MLOps platform that enables data scientists & ML teams to track, compare, explain, & optimize their experiments. We pay our contributors, and we don’t sell ads.
If you’d like to contribute, head on over to our call for contributors. You can also sign up to receive our weekly newsletters (Deep Learning Weekly and the Comet Newsletter), join us on Slack, and follow Comet on Twitter and LinkedIn for resources, events, and much more that will help you build better ML models, faster.
Has an *approximate* knowledge of many things. https://aftershoot.co
See all (33)
303 
1
303 claps
303 
1
Helping data scientists, ML engineers, and deep learning engineers build better models faster
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/the-2-limits-of-iam-service-on-google-cloud-7db213277d9c?source=search_post---------255,"There are currently no responses for this story.
Be the first to respond.
The security is paramount in cloud environments and IAM (Identity and Access Management) service helps on any cloud provider (you can find IAM service on Azure, AWS and GCP for example).
On Google Cloud, this IAM service uses OAuth and OpenID protocols. It allows to authenticate and to authorize the account (user account or service account). The authorization is performed only for Google Cloud components; you can’t add your custom authorization/permissions for your own app in IAM service.
The authentication part uses OAuth protocols to generate a credential. You can use your own credential (user account, with interactive authentication in a browser) or a technical credential (service account).
Service account credential are automatically loaded on Google Cloud environment (I explain how later). However, you can also generate a service account key file to use this credential anywhere. And it’s an ugly practice especially to keep the secret…secret!
Service account key files are useful in some cases, but they are also ugly when bad used. First, think about what is it: a file. A simple file.
A file is the most common object in computing. You can copy it, send it by email, commit it into a Git repository. Sometime this repository is public and you receive an email from Google Cloud that informs you about the leak of your secrets file.
In my company, we have had 2 times this kind of issue with bad actors that silently created VMs with bitcoin miner. Hopefully we fixed it quickly and we have limited the cost. Additionally, it was dev projects without more dramatic issues (confidential data exfiltration for example). I will release another article to present how we tackled the problem and increase the security.
Additionally, this file can be shared between the developers, more focused on the features’ development than on the keeping safe this secret file. You can also have external developers that work for a while in your company and go elsewhere with the key file still in their computer.
So, if the users aren’t aware of the confidentiality of this key file, you quickly loose the control of it. That leads to the second problem: Google recommend to regularly rotate the service account key file, at least every 90 days.
How do you perform this key rotation is you don’t control the key files?
So, in summary, the service account key files are a nightmare to manage. They are useful in a specific cases, when the Application Default Credential isn’t enough to solve your authentication. In all other cases…
To avoid any leaks of secrets, never have stored secrets
Based on this, IAM service allows to never have to store secrets, like service account key file, and to work seamlessly in your local environment and on Google Cloud.
Therefore, the principle is to use the environment context to be authenticated by IAM service. This strategy is called ADC (Application Default Credential) and the Google Cloud client libraries support this authentication mode in several languages. The library retrieves, according with the environment, the credential, and use it as default credential in the application.
In Python, for example, it’s the google-auth library that allows you to do this
You never mention your environment or an account, you let the library doing its job. It also works for component libraries (like Google Storage): default constructor() or keyword defaultCredential are used in that case.
You can use your user credentials, the same that you use in the Google Cloud console.
In both cases, you will have to go in you browser, to select your Google account, if many. You might have to re-authenticate yourselves, maybe with 2 factors mechanism. And finally authorize the use of your account with gcloud SDK.
Only a refresh token is stored locally, never your authentication credential login/password or secrets.
All Google Cloud services have access to Metadata server. This internal server provides informations on the environment, included the service account used for the service. And thus, the Google Cloud client library can detect this sever and get credential through this Metadata server.
Some products allow you to customize the service account that you want to use in the service, such as Cloud Functions and Cloud Run. Others not, it’s a default service account which is used. But, in any case, a service account is loaded and usable.
ADC works well when you use your user credential or when you run your code on Google Cloud products.
What happens when you have to connect your on-prem environment to Google Cloud?
Your CI/CD (gitlab CI or Github Action for example)?
Or other applications hosted on other Cloud Provider?
There is no magic solution, you need a credential file to be authenticated, with a secret (a private key) store in it: the service account key file. The service account key files are mainly designed and useful in this context.
However, even with local user account credentials or service account credentials on Google Cloud environment, there is still 2 main limitations;
For this 2 cases, service account impersonation is preferred (to avoid key file generation), but it’s not always the safest solution.
App Engine has been the first product of Google Cloud and have more than 12 years old! It allows you to deploy a set of (micro)services to serve a web application. However, as old and first product, there is some legacies that limits you in the future.
App Engine has 2 limitations:
The nicest way is to use service account impersonation (and thus to avoid the service account key file). In other words, you won’t use directly the ADC to access to the service, but you will use them to generate a credential on behalf of another service account. With IAM service, you can manage which service account can be impersonated or not.
Additional issue: Python and Java Google Auth library natively include impersonation methods. It’s not the case for the other languages.
When you use impersonation on App Engine, you can also use it with your user credential in your local environment. Therefore, your code on App Engine and on your local is the same and the code that you test locally is exactly the same as this one that will be run on App Engine.
You can also generate service account key file per service and load them with your code. It’s useless to store key files inside Secret Manager and to retrieve them, at runtime, with the App Engine default service account credential, because we go back to the #1 tradeoffs. And in this case, impersonation is a far more better solution.
In my company, we sadly use service account key file for App Engine critical services only, not for all.
If you are deploying on another component than App Engine, like Cloud Functions and Cloud Run, you can use ADC. However, you will have scope issue with your personal user account in your local environment.
To solve this, scope your credential when you create it by defining the scope like this
The second issue is when your local code try to call a service on Cloud Functions or on Cloud Run deployed in private mode. This mode implies the caller to present an signed identity token.
To call the privately deployed services with a simple curl, you can do this
Gcloud SDK has the capability to generate a signed identity token with your user credential. However, the Google Auth client libraries doesn’t implement this feature for user account credential to reach the deployed service directly from your app code (for example a service being developed locally that call another service deployed privately on Google Cloud).
Impersonation can work as previously here. But, I don’t like this solution because you have to perform a hook in your code
And thus, your code doesn’t run exactly in the same way in local and in the Cloud. Therefore, you could have issues on Google Cloud that you havent detected locally. It’s not really safe (a bug point-of-view, not at security point-of-view)
Of course, you could impersonate service account in any case, but that increases your app complexity (and thus decreases the maintainability).
The latest option is to use service account key file, only for local development, and with only the role run.invoker to limit the impact in case of leak.
I create an open source project, and an article, on this topic to help the local test, with a service account key file.
IAM service offers lot of possibility, and also lot of cases to do the wrong things with security.
One solution can work for any use cases: generate a service account key file every time. It’s also the worst and the most dangerous solution.
Security implies tradeoffs and to understand what we want to achieve. It’s not always easy and automatic. There isn’t a unique solution.
The best starting point is to think ADC for all the use cases except for 3 situations:
This is true even if some examples, tutorials, even on Google Cloud documentations present code samples with a service account key files!
Google Cloud community articles and blogs
141 
4
141 claps
141 
4
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
GDE Google Cloud Platform, scrum master, speaker, writer and polyglot developer, Google Cloud platform 3x certified, serverless addict and Go fan.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://blog.originprotocol.com/origin-launches-dshop-on-the-google-cloud-marketplace-5ba9652b1a38?source=search_post---------256,"We’re excited to announce that Origin has partnered with Google Cloud to bring our decentralized commerce platform to Google’s global network of developers. You can read all the details in our guest post on the official Google Cloud blog:
medium.com
Today, you can visit the Google Marketplace to launch your own store on Google Cloud with just a few clicks. In addition, publicly available product data from Origin’s merchant partners will now be available for free in Google BigQuery.
This is just the beginning of our partnership with Google. We have deep ties between our two companies. Our co-founder Matt, and two senior engineers, Yu Pan and Franck, all worked at Google prior to joining Origin. We look forward to continuing to work with Google to bring new technology integrations to the market in the near future.
Learn more about Origin
Bringing NFTs and DeFi to the masses
1.3K 
1.3K claps
1.3K 
Written by
Co-founder at Origin. Previously founded EventVue, Torbit & Forage. Engineer & rock climber.
Origin Protocol is bringing NFTs and DeFi to the masses. Origin’s flagship products are our NFT platform, which has supported numerous high-profile NFT drops and Origin Dollar (OUSD), the first stablecoin that earns a yield automatically in users’ wallets. Origin’s  token is OGN.
Written by
Co-founder at Origin. Previously founded EventVue, Torbit & Forage. Engineer & rock climber.
Origin Protocol is bringing NFTs and DeFi to the masses. Origin’s flagship products are our NFT platform, which has supported numerous high-profile NFT drops and Origin Dollar (OUSD), the first stablecoin that earns a yield automatically in users’ wallets. Origin’s  token is OGN.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@chroniclesec/chronicle-joining-google-cloud-c29037ee2d89?source=search_post---------257,"Sign in
There are currently no responses for this story.
Be the first to respond.
Chronicle
Jun 27, 2019·2 min read
In late 2015, when the earliest idea for Chronicle was formed, we had a vision to help companies find and stop cyber attacks before they cause harm. In the following years, we created Backstory, our security telemetry platform within X, the moonshot factory, as a way to combat cyber threats that have the potential to disrupt the lives of billions of people around the world. Backstory joined VirusTotal, one of the world’s largest malware information services, to help organizations of any size better understand and defend against cyber threats.
At the core of our mission was the ability to look for the biggest way to have impact, and to bring the technology and services needed to “Give Good the Advantage.” Our goal was centered around delivering a way to 10X the capabilities of security teams around the world who are struggling to stay ahead.
Today, we are excited to announce that Chronicle will be joining Google and become part of Google Cloud. Combining our efforts will allow us to take the next step on our journey, and will significantly accelerate our impact globally, together.
As CEO of Chronicle, I’d like to say thank you to our valued employees (Chroniclers!), customers, and partners. Without our global team’s passion and dedication, we could not have built something so compelling and so well received in the market.
To our customers, who worked with us in our early days through to a full product launch, thank you on behalf of our entire team!
Google Cloud has built its own products for detecting threats and securing data, and these products are very complementary to Backstory and VirusTotal. By combining our solutions, we can give customers a unique, single platform for securing their systems both on premise and in the cloud. Customers from each of our organizations have asked about using these solutions together, and combining our efforts will enable this.
We will be joining Google Cloud soon and expect the integration to be completed sometime this fall. We will work on accelerated product integrations and roadmaps immediately. Stay tuned for more information, and once again, thank you to everyone who’s helped Chronicle get to this point. We are excited to work with you on the next phase of our journey to Give Good the Advantage!
Stephen GillettChronicle CEO and Co-Founder
61 
1
61 claps
61 
1
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/serverless-daily-reporting-notification-service-with-google-cloud-scheduler-cloud-functions-d3cc42390006?source=search_post---------258,"There are currently no responses for this story.
Be the first to respond.
You can also read this article in my Xcoding With Alfian blog website using the link below.
www.alfianlosari.com
Task scheduling is a very important tools for developers to run various automated tasks in a system such as performing database maintenance, perform daily big data batch operations, and overall system maintenance.
Building the infrastructure to perform task scheduling are challenging tasks, there are several important things that need to be considered carefully such as:
At the beginning of November 2018, Google Cloud Platform finally released Cloud Scheduler, a fully managed, scalable, and fault tolerant cron job scheduler that allows developer to automate all their scheduled tasks in one place. They provide many amazing features such as:
cloud.google.com
In this article, we will build a simple notification service to setup daily scheduled data reporting using several Google Cloud Services and notification using Slack HTTP Webhook.
!!! Make sure to clear and delete all of the created resources to avoid recurring cost in GCP after finishing the tutorial !!!
You can download the Project source code in the GitHub repository link below.
github.com
First, we need to create a new bucket in Google Cloud Storage. This bucket will be used to store the CSV file for the data that we will be querying in BigQuery inside our Cloud Function later. We will be using the dashboard to create the bucket:
Make sure to store the bucket name somewhere as it will be used later inside the Cloud Function to refer to the bucket when saving the CSV.
The daily data that we will be using as daily report is the public dataset from Global Historical Climate Network (GHCN) inside the BigQuery. The query will retrieve the maximum temperature for GHCN stations in the radius area within the Blok M, Jakarta for previous week. You can try to query the data using the BigQuery dashboard and hardcoded values:
3. Click on more and query settings.
4. Set SQL dialect to legacy.
5. Run the query.
Make sure to create your own Slack Workspace, channel, and app before. Then, create an incoming webhook URL associated with the workspace channel. You can follow the documentation below:
api.slack.com
This is the most interesting part, we will create Cloud Functions running node.js 8. There are several npm dependencies we will use:
Inside our index.js file, here are the key important things we will perform:
To deploy the function to the Cloud Function, we will use the Cloud Function Dashboard:
Copy the endpoint of the url deployed, then open your browser or terminal to navigate to the url providing the secret key as the url parameter
This will trigger the function and send the notification to the Slack Channel!. You can open the CSV url link to download the CSV.
At last, we will create a Cloud Scheduler job that will be scheduled to run daily at 00:00:00 UTC to hit our Cloud Function endpoint.
After the job is created, click on the Run now button to test the job manually. You can customize the schedule frequency of the job using the unix cron syntax.
!!! Make sure to clear and delete all of the created resources to avoid recurring cost in GCP after finishing the tutorial !!!
We finally build and deploy serverless daily reporting notification service without managing the infrastructure by ourselves. As developer, i really love serverless because it abstracts all the infrastructure management into an interface that we can automate easily to deploy our app without worrying about scalability and distribution. We can just focus on what we love, writing code and solving problem. This is really the beginning of a new era for Cloud computing and i am pretty pumped up to create solution that solve problem in real world with all of this technologies 😋.
Google Cloud community articles and blogs
177 
2
177 claps
177 
2
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Mobile Developer and Lifelong Learner. Currently building super app @ Go-Jek. Xcoding with Alfian at https://alfianlosari.com
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/neo4j/running-neo4j-on-google-cloud-6592c1b4e4e5?source=search_post---------259,"There are currently no responses for this story.
Be the first to respond.
Recently, Neo4j made available VM-based images for running both community and enterprise in Google Cloud. Before this, plenty of people were already doing it, but generally had to roll their own and figure out how to do the cloud-specific configuration bits themselves. With these new GCP images though, the process just got a lot easier.
This article shows just how easy this has become. We’ll set up a neo4j instance on GCP, configure firewall rules to allow us to access it, and connect Neo4j Desktop to show how to use a cloud instance with other tooling. In this article we will be working with single-node neo4j deploys. If you’re looking for multi-node clusters, neo4j has separate instructions for those.
All we need is two commands. The fact that it’s so easy to automate this means you can include it in scripts or CI/CD pipelines, as needed.
The first command configures firewall rules according to the instructions on neo4j’s site. Port 7473 is for HTTPS access to Neo4j Browser (the web-based shell we use to interact with the database). Port 7687 is for the binary Bolt protocol, to send queries to the database. The “source ranges” allows all traffic from the entire internet, and the “target tags” simply indicates that this rule applies to any VM in your google project that is tagged “neo4j”.
The second command creates a neo4j instance from the image named “neo4j-community-1–3–3–5” in the “launcher-public” project, and tags that image with “neo4j”. Because we’re tagging the image neo4j, the firewall rules will apply, and we’ll be able to access it via those two ports. This will use an n1-standard VM, if you’d like more or less hardware you can consult the gcloud documentation.
If all goes well, you’ll see output like this:
After waiting a minute or two to allow the system service on the VM to come up, we can go to https://YOUR_VM_IP:7473 and log into neo4j using the default username and password neo4j/neo4j. Make sure to change the password immediately. Don’t worry if your browser gives you an SSL warning, this is expected since we haven’t configured an SSL certificate.
If you prefer, you can use all of the existing tools like cypher-shell and others to work with this database directly by its public IP and 7867 bolt port.
In the commands above, we launched neo4j-community-1–3–3–5. If we wanted enterprise, we could simply instead launch neo4j-enterprise-1–3–3-5. Both are work fine. If you have an evaluation or commercial license, or you’re in the Neo4j startup program, you can use enterprise.
After starting Neo4j Desktop, click on “New Project” in the left pane. Edit the project name like below, and click on “Connect to Remote Graph”.
You can then enter in the details of how to connect to your instance:
Following this screen, you’ll be asked for your username and password. Because we changed it from the default in the last step, make sure to use the one you changed it to here.
When finished with the configuration dialog, hit the “activate” button.
Once activated, this database in neo4j can be treated as if it was local.
Using the gcloud utility, it’s just one last command:
Developer Content around Graph Databases, Neo4j, Cypher…
79 
4
79 claps
79 
4
Developer Content around Graph Databases, Neo4j, Cypher, Data Science, Graph Analytics, GraphQL and more.
Written by
Architect at Neo4j
Developer Content around Graph Databases, Neo4j, Cypher, Data Science, Graph Analytics, GraphQL and more.
"
https://medium.com/google-cloud/translate-a-book-with-google-cloud-translate-api-938d16396911?source=search_post---------260,"There are currently no responses for this story.
Be the first to respond.
I had a autobiography from a family member in another language that I wanted to translate for awhile. You know, one of those projects on your list for a long time that you just cant seem to complete. Finally the day came and it was time to get this done and get hands on with the translation API.
Upon initial testing I noticed that there was a per character limit per request (5000 chars) using the translation API. My book happens to be over 500,000 chars and over 1000 lines of text.
I knew I would have to split the text by line and send multiple requests to the translation API. At first I thought I would split the file and then send 5000 char pieces to the translation API, but then I realized it would make more sense to split the requests by new line or ‘.’, then call the translation API, store response, and loop through. So thats what we did. Its only around 30 lines of python without the progress meter and comments and pretty easy to understand. Find the code on my github.
github.com
This script is only tested with Spanish and Hebrew languages and it worked well.
Did you Google has been doing translation services for over 10 years?
The cost for the Translation API is $20 per 1,000,000 characters. For my testing this week I am at the following cost:
As long as you send text through the translation API by new lines and breaking text by period you should be able to preserve most formatting in source text files. Some post processing manual corrects may be required (see below). Output language is set to English but can be changed by modifying target_language in the script.
Thanks for reading and have fun with Google Cloud Translation API!
Google Cloud community articles and blogs
94 
3
94 claps
94 
3
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Customer Engineer, Google Cloud. All views and opinions are my own. @mkahn5
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/@goangle/deploy-go-allciation-google-app-engine-11db42aeb773?source=search_post---------261,"Sign in
There are currently no responses for this story.
Be the first to respond.
Teerapong Singthong 👨🏻‍💻
Jun 30, 2018·4 min read
App Engine เป็นเครื่องมือช่วยเหล่านักพัฒนา สามารถ Deploy Application ที่ถูกเขียนขึ้น ให้ไปอยู่บนระบบ Google Cloud ที่รองรับ Scalability และ Reliable โดยที่นักพัฒนาไม่จำเป็นมีความรู้ด้าน Infrastructure มาก ช่วยให้นักพัฒนาสามารถส่งมอบงานได้รวดเร็วขึ้น และไม่ต้องกังวลเรื่องการดูแลรักษา โดยที่ App Engine เองรองรับภาษามากมาย เช่น Node.js, Java, Python, PHP, Go, Ruby และ .NET
Serverless คงเป็นคำที่จำกัดความได้ดี สำหรับ Google App Engine ช่วยให้ นักพัฒนา ได้โฟกัสส่วนของการเขียนโค๊ดอย่างเดียว ส่วนการสร้าง Infrastructure นั้น ก็ให้ Code เป็นตัวกำหนดเช่นกัน ซึ่งจะกล่าวในขั้นตอนช่วง Deployment
ทาง Google Cloud Platform ไม่ได้มีแค่ Google App Engine เท่านั้น ยังมี Product & Service อีกมาก ให้เราเลือกใช้งาน ซึ่งผู้อ่านสามารถดูเพิ่มเติมที่ลิงค์นี้ https://cloud.google.com/docs/
ไปที่ https://console.cloud.google.com/ และ เข้าสู่ระบบด้วย Google Account
นักพัฒนาหลายท่านคงทราบดีว่าทาง GCP ให้เครดิตกับนักพัฒนา หรือ ผู้ใช้ระบบ $300 เหรียญในการใช้งานระบบ สำหรับ 1 Account ของเรา
จากนั้นทำการสร้างโปรเจคใหม่
จากนั้น รอประมาณ 2–3 นาทีระบบจะทำการสร้างโปรเจคให้
เมื่อ GCP ทำการสร้างโปรเจคของเราเสร็จเรียบร้อย เราสามารถเข้ามาดู Dashboard ของโปรเจคได้ ซึ่งจะเห็นได้ว่า มันเป็นเพียงโปรเจคว่างเปล่า ไม่มีอะไร
เลือกภาษา Go หลังจากนั้นระบบจะแสดง Promp ขึ้นมาให้เราระบุ Region ที่เราต้องการ ซึ่งเงื่อนไขในการเลือกนั้นขึ้นอยู่กับกลุ่มผู้ใช้ของเราว่าอยู่โซนไหน ในที่นี้ผมเลือก asia-south1 เนื่องจากผมทำการทดสอบอยู่ที่ประเทศไทย
หลังจากนั้นรอประมาณ 2–5 นาที GCP จะทำการสร้าง VM Instance ให้เราขึ้นมา คู่กับโปรเจคของเรา ซึ่งเราสามารถแก้ไขค่าของ VM Instance ของเราได้ภายหลัง
ในขั้นตอนนี้ท่านผู้อ่านสามารถดูวิธีการตั้งติดได้ ที่นี่
สร้างโปรเจคใหม่
ทำการสร้างไฟล์เหล่านี้
main.go
app.yaml
app.yaml ทำหน้าที่เป็น configuration file ของโปรเจค ที่เกี่ยวข้องกับ Infrastructure as code / Static file / Runtime Application Setting / Handler ตลอดจนไปถึง Scaling config ถ้าผู้อ่านคุ้นเคยกับ k8s ก็จะง่ายเลย สำหรับส่วนนี้
หากผู้อ่านอยากเข้าใจโครงสร้าง configuration file ในโปรเจคให้มากขึ้น สามารถตามอ่านได้ ที่ https://cloud.google.com/appengine/docs/standard/go/configuration-files
ผู้อ่านสามารถ Clone Project ทั้งหมด ผ่านทาง command
ทำการทดสอบรัน Go app ของเราผ่านทาง dev_appserver.py ซึ่งเป็น package ที่มากับ google-cloud-sdk ที่เราได้ดาวน์โหลดมา ตามขั้นตอนก่อนหน้า
ทันทีที่เรารันคำสั่งสำเร็จ local server จะ provide ให้เรา 3 อย่าง ดังภาพ
หากการทดสอบขึ้นแบบนี้ ยินดีด้วยครับ เราได้ API ง่ายๆ ด้วยภาษา Go แล้ว และพร้อม Deploy ขึ้น Google App Engine แล้ว
คำสั่งนี้จะทำการแสดง Dialog เพื่อขอสิทธิในการเข้าถึง Profile ของ Google Account ของเรา ระบบจะขอให้เราทำการ Login และ Grant Permission ตามลำดับ
คำสั่งนี้ คือ การเลือก project ที่เราสร้างไว้ใน GCP
คำถาม แล้วเราจะหา project id ได้จากไหน?
เมื่อ gcloud ตรวจสอบ project id / app.yaml ถูกต้อง ระบบจะแสดงผลลัพธ์การ deploy ให้เราทราบ และแสดง target url ซึ่งเป็น public url ของโปรเจค
หากเราต้องการดู runtime log เราสามารถใช้คำสั่งนี้
Dashboard ช่วยให้เราสามารถดู ภาพรวมของระบบเราได้ เช่น จำนวน User ที่ Visited ในช่วงเวลาที่เราระบุ / Current Load ในแต่ละ URL / Billing / Bandwidth Usage เป็นต้น
บทความนี้เป็นเพียงตัวอย่างง่ายๆ ในการ Deploy Go application ซึ่งในโลกของ Cloud Engine ยังมี Product & Service มากมายที่เกี่ยวกับเว็ป ให้เราใช้ เช่น Real-time database, Authentication, OIDC, Logging, ACL และอื่นๆ อีกมากมาย
เอกสารที่อยู่ในลิงค์นี้ จะช่วยให้เราเข้าถึง ความสามารถ App Engine กับภาษา Go ได้มากขึ้นครับ https://cloud.google.com/appengine/docs/standard/go/how-to
App Engine เป็นเครื่องมือที่ช่วยให้เหล่านักพัฒนาสามารถ Deploy แอพพลิเคชันที่สร้างขึ้น ให้ไปอยู่บน Cloud แทนที่จะทำแบบ On-premise ด้วยตัวเอง ช่วยให้เราโฟกัสเรื่องการเขียนโค๊ดไปเลย ไม่ต้องห่วงเรื่อง Infratructure ต่างๆ
ซึ่งการ Deployment นั้น เราสามารถใช้ผลิตภัณฑ์อื่นของ Google Cloud Platform ได้ เช่น Google Compute Engine ในการสร้าง VM Instance ขึ้นมา หรือ จะใช้การสร้าง Dockerfile แล้วใช้ Kubernetes ก็ยังได้ครับ
สุดท้ายก็อย่าลืม!!! เรากำลังใช้ Credit $300 ในระยะเวลา 1 ปี ถ้าเกินโควต้า ก็อย่าลืม Delete Project นะครับผม เดียวค่าใช้จ่ายในบิลจะบาน :)
https://cloud.google.com/docs/overview/
https://cloud.google.com/go/home
https://cloud.google.com/go/docs/
https://cloud.google.com/go/getting-started/hello-world
LINE Engineer x Software Craftsmanship
38 
1
38 
38 
1
LINE Engineer x Software Craftsmanship
"
https://medium.com/@duhroach/google-cloud-storage-sequential-file-names-f1ba1205c6a8?source=search_post---------262,"Sign in
There are currently no responses for this story.
Be the first to respond.
Top highlight
Colt McAnlis
Oct 26, 2017·4 min read
BUMBLECAM is a nursery camera company, and their application uploaded snapshots of the nursery cam on a regular basis to the owners of the camera. Their setup was pretty simple: Every half second, the camera would snap a photo, and store it locally. In battery saver / bandwidth saver mode, a bunch of pictures would be batched up on the device before uploading to the Google Cloud Bucket for the owner.
The problem they were seeing was that their upload times for the images was painfully slow. During the course of 20 minutes, this slowdown would result in a serious backlog of images that needed uploading, eventually resulting in the camera running out of memory, and stop working.
So let’s run down the checklist here.
First, we ran perfdiag on a sample bucket they created, and verified it had a high throughput from the source. Way higher than what the developer was seeing. This meant that the bucket itself was performing properly in terms of connection and upload speed from the client; the problem had to be in the type of data being uploaded, or how it was being organized.
Since the cameras were embedded hardware, we knew they weren’t using GSUTIL to do the uploads, but rather the native Python APIs to upload files. As such, we knew that the uploads were going through the fastest possible API.
Next we checked the sizes of the files, they were about 100k each. So, they weren’t big enough to use the composite file upload, and the developer was properly using parallel upload API, so we should be seeing max throughput there.
Basically, all of the most common cases for performance, things were already set up in the ideal manner.
At this point, I needed to do research myself, so I turned to Michael Yu’s NEXT 2017 talk, where he provided details on how GCS works behind the scenes. Here’s a generalization of how things work:
When uploading a number of files to GCS, the frontend will auto-balance the connections to a number of shards to handle the transfer. This auto balancing is, by default, done through the name/path of the file; Which is very helpful if the files are in different directories, since each one can be properly distributed to different shards.
Which means that how you name your files could have an impact on your upload speed.
When the files are co-located in the directory structure, and the names are sequential, the requests are constantly shifting to a new index range, making redistributing the load harder and less effective.
And that was exactly our issue. The developer was using the timestamp in the file path. (e.g. YYYY/MM/DD/CUSTOMER/timestamp)
One solution to this problem is to manually break up your linearly-named files into folders, and then upload the folders in parallel. For example, the foo and bar folders can be uploaded in parallel w/o stomping all over each other. However, Bumblecam wasn’t too thrilled at this option, since this would cause some new “house keeping” dependencies to show up in various parts of the pipeline, not to mention that it might cause other scaling issues down the line if they didn’t continue to create new folders.
What we finally settled on was , prepending a hash of the filename to the filename itself. There’s lots of hash functions out there, but we settled on one which generates a uniform distribution of values over a fixed range (e.g. 00000000 — FFFFFFF). This allows GCS to partition the fixed range into shards for better load balancing.
For BUMBLECAM , this was an easy fix, for a massive increase in performance.
DA @ Google; http://goo.gl/bbPefY | http://goo.gl/xZ4fE7 | https://goo.gl/RGsQlF | http://goo.gl/4ZJkY1 | http://goo.gl/qR5WI1
See all (78)
141 
3
141 claps
141 
3
DA @ Google; http://goo.gl/bbPefY | http://goo.gl/xZ4fE7 | https://goo.gl/RGsQlF | http://goo.gl/4ZJkY1 | http://goo.gl/qR5WI1
About
Write
Help
Legal
Get the Medium app
"
https://itnext.io/writing-google-cloud-functions-with-python-3-49ac2e5c8cb3?source=search_post---------263,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
This is my third and final article looking at new features in Google Cloud functions as Google starts to narrow the gap to Amazon’s AWS Lambda product. Until recently Node.js 6 was the only option for writing Google Cloud functions. That’s changed now with the addition of Node.js 8 (read my article) and Python 3 runtimes available in public beta.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/hosting-a-website-on-google-cloud-using-google-compute-engine-c6fe84d76f51?source=search_post---------264,"There are currently no responses for this story.
Be the first to respond.
In this mini series we are covering, how to create websites on Google Cloud. This is the fourth article in the series.
One of the great things about being an engineer, is that we get to have complete control over so many aspects of the things we develop. And for a lot of you out there, this extends to the websites you develop, too. If you need something more advanced than a static site, but need more control than what Google Cloud managed services offer then, you can use Compute engine.
Google Compute Engine lets you create and run virtual machines on Google infrastructure. Compute Engine offers scale, performance, and value that allows you to easily launch large compute clusters on Google’s infrastructure. There are no upfront investments and you can run thousands of virtual CPUs on a system that has been designed to be fast, and to offer strong consistency of performance.
It is important to note that a well-designed web application should scale seamlessly as demand increases and decreases, and be able to withstand the loss of one or more compute resources.
But, a truly resilient and scalable web application requires planning. Let’s see how a website works, using Google Compute Engine. Stay tuned for a codelab! (coming soon)
When a user requests your website in the browser, their request ends up on your DNS provider, in this example, we are using Google DNS which is a highly available Domain Name service.
The Network traffic is routed to infrastructure running on Google Cloud.
If you use your own DNS provider, then the request will land there first, and then the DNS provider routes the traffic to Google Cloud.
Then, if the request is for content that is cached, it is delivered by CDN. In this architecture, we are using Cloud CDN, a global network of edge locations. Requests are automatically routed to the nearest edge location, so content is delivered with the best performance. (You can integrate with any other third party CDN of your choice as well.)
Static content used by the web application is stored on Google Cloud Storage, which is a highly durable storage infrastructure designed for mission-critical and primary data storage.
HTTP requests are first handled by Cloud Load Balancing, which automatically distributes incoming application traffic among multiple Compute Engine instances.
If you use HTTPS, the SSL session is terminated at the load balancer and requires at least one signed SSL certificate.
Web apps, including the application server and web server, are deployed on Google Compute Engine (GCE) instances. Deployment and scaling is done automatically and seamlessly by using instance templates. An instance template is a specific customized configuration of the GCE instance that facilitates the reuse of instance configurations by using managed instance groups.
To learn more about instance groups, check out the next article!
To store application data, you would use a database. The relational database used in this example is Cloud SQL. Cloud SQL is a fully-managed database service that makes it easy to set up, maintain, manage, and administer your relational data in the cloud. It automates all the data backups, replication, patches, and updates.
Depending on the website use case you may want to use document based database like Datastore for user data or use Bigtable for large fully managed, highly scalable and global, NoSQL database.
Now that we know how the web application architecture is set up for Google Compute Engine, what happens if our website gets really popular and the traffic grows from 100s to millions of users? We need to make sure that our application can gracefully handle peaks and dips in traffic.
One way to increase capacity to handle peaks is to scale vertically, by adding more CPU and memory to the same instance. This means that Vertical scaling will be limited based on the capacity of a single machine and its size.
Horizontal scaling, on the other hand, is a better option for high-availability applications, because it allows you to scale the number of compute resources dynamically, as the demand increases.
The idea that the application’s resources can increase or decrease requires that it has a means by which we can add or remove instances from service. But, how does adding or removing instances automatically work? Check out the next article to find out!
Google Cloud community articles and blogs
169 
169 claps
169 
Written by
Developer Advocate @Google, Artist & Traveler! Twitter @pvergadia
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Developer Advocate @Google, Artist & Traveler! Twitter @pvergadia
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/google-cloud-storage-signedurl-resumable-upload-with-curl-74f99e41f0a2?source=search_post---------265,"There are currently no responses for this story.
Be the first to respond.
A couple days ago a colleague asked if its possible to use Google Cloud Storage Signed URL with Resumable Uploads.
SignedURLs are pretty useful in that they allow an application to issue a time-limited URL that a customer can use to upload or download a file in Cloud Storage (GCS) without needed to login.
That is, your application can simply give a self-contained URL to a user and he/she can use that URL alone without logging into upload or download a given object.
However, what if the file/object to upload is large or your network connection is flaky. Well, in GCS you can use Resumable Uploads as described here for GCS XML and JSON APIs and their corresponding libraries.
One problem though…GCS SignedURL only works with the XML endpoint and the libraries to perform the resumable upload only speaks to the JSON endpoint.
What to do? You can ofcourse mint a signedURL and reply the protocol as shown here….its certainly very tricky to do this but this article simply shows the mechanism. Hopefully, there will be library support for this within the GCS library set as they do now for the JSON API.
I would advise against implementing the protocol…there are many cases you need to consider like parallel download and handling all the appropriate retry logic….for reference, i’ve provided a link to Gmail attachment download page.
Anyway, here is the raw protocol using curl
2. Generate the signed URL and exchange it for the upload location URL.
The java and golang source for the samples here is shown at the end of the article. For java, you’ll need to crate a service account JSON file while for golang, a .p12 which you will need to convert to PEM. You will also need to grant the service account access to the bucket+object in question.
3. Submit an empty POST request with the added HEADER (x-googe-resumable:start) to get the location URL Use the signedURL from the previous step
4. Just set the Location value in an environment variable for later use (enclose the value with quotes)
5. Now start the upload and interrupt it after maybe 5 or 10 seconds to simulate network failure (i.,e click ^C to interrupt curl). If you do not interrupt it, the upload should work as normal…but we want to show
6. Now find out how much got transferred.
Remember to set the content-range header:
7. Create a temp file to transfer with the remaining bytes
response header shows that we transmitted 9699328 bytes so we have to transmit the reamaining bits…so lets create a file with that starting with the _next_ byte in the file 9699327 +1 = 9699328
8) Upload the remaining
9) Verify partial transfer:
Thats it!, we’ve uploaded the file completely by hand.
The remaining is for extra credit and if you want to generate a signedURL with canonical headers.
Appendix
The following code samples in Java and Golang issues a SignedURL with the resumable headers baked into it already.
Note: SignedURLs issued by google-cloud Java currently does not support setting Canonical Headers (see issue#2000)…which means you have to create a signUrl manually as shown below:
google-cloud goland does allow for setting the canonical header in the request:
developers.google.com
Google Cloud community articles and blogs
117 
2
117 claps
117 
2
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/how-to-reduce-latency-of-your-google-cloud-endpoints-apis-5bbe66385e32?source=search_post---------266,"There are currently no responses for this story.
Be the first to respond.
It’s been 2 years now since I’ve used Google Cloud Endpoint in order to build powerful APIs for endpoints like Android & IOS.
If you’ve never heard about Google Cloud Endpoints you’ve probably heard about Google Cloud Platform (GCP), that is basically the equivalent of Amazon Web Services, made by Google.
So, Google Cloud Endpoints (one of the services provided by GCP) helps you create easily your own APIs, to allow your endpoints (Android, IOS, Javascript, etc…) to access to your backend.
However, there are some optimisations to take full advantage of Google Cloud Endpoints and improve performances of your own APIs.
As you may know, Google Cloud Endpoints runs on Google App Engine instances. Thereby, when an instance is not used, then it shuts down itself after some time, and start up when it’s necessary (Incoming http request, etc…).
But the startup time for an appengine instance can take long . I’ve noticed that sometimes, it can exceed 7 seconds…
To handle that problem, a simple trick is to create a cron task that keep alive instance in cron.xml:
Also, you can tell Google when startup a new instance if there are too many requests to handle, with “min-pending-latency” parameter in appengine-web.xml. This value depending on your average latency time you can notice on your APIs :
I highly recommend to read this documentation that explains when using Memcache. Back in the day, I set up every of my Datastore Entities in Memcache, telling myself that it will be more faster… #Fail
Basically, if you serve datas that never (or rarely) change, use Memcache for those datas. Otherwise, don’t.
For example, on my app CookMinute, every recipe is daily downloaded and stored in user’s phone (thanks Realm). So I’ve only set up RECIPE entity to be part of Memcache.
If you have high processing on your APIs, you can increase the instance class of your appengine instances. However, this will increase the cost too.
I’ve tested those optimisations with API that sign-up a new user to my app. Stats are generated with Stackdriver Trace (Console => Stackdriver => Trace) and the instance class is F1.
As you can see, latency has decreased and is more smooth, especially because there is always an instance alive that can handle a request.
Oh, and about the cost, well, I don’t pay anything yet because I never exceeded the free quotas… (which is really huge)
If you have some advice or tips about Google Cloud Endpoints API, let me know in the comments !
Phil, Founder @ CookMinute and mobile enthusiast.
Google Cloud community articles and blogs
109 
5
109 claps
109 
5
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Android Software Engineer @backmarket 📱🏡 💚
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://codeburst.io/google-cloud-authentication-by-example-1481b02292e4?source=search_post---------267,"The article that I did not think I needed to write but did.
It turns out that understanding how to authenticate to Google Cloud on your workstation is more complicated than one would think.
Please note: This article is not about authenticating a user account to the Google Cloud Console. It is also not about authenticating a service account on a GCE instance.
First, we need to understand that there are two separate components that are authenticated separately: Google Cloud SDK Command Line Tools and Google Cloud Client Libraries.
First, we have Google Cloud SDK Command Line Tools:
The gcloud CLI manages authentication, local configuration, developer workflow, interactions with Google Cloud APIs. With the gcloud command-line tool, it’s easy to perform many common cloud tasks, like creating a Compute Engine VM instance, managing a Google Kubernetes Engine cluster, and deploying an App Engine application, either from the command line or in scripts and other automations.…A collection of command-line tools comes packaged with Cloud SDK, including gsutil, bq, and kubectl. The gsutil tool allows you to manage Cloud Storage buckets and objects using the command line. Run queries and manipulate datasets, tables, and entities in BigQuery through the command line with bq. With kubectl, you can deploy and manage Kubernetes container clusters using the command line.
— GCP — Cloud SDK Command Line Tools
And then we have Google Cloud Client Libraries:
Google Cloud Client Libraries are our latest and recommended client libraries for calling Google Cloud APIs. They provide an optimized developer experience by using each supported language’s natural conventions and styles. They also reduce the boilerplate code you have to write because they’re designed to enable you to work with service metaphors in mind, rather than implementation details or service API concepts.
— GCP — Google Cloud Client Libraries
And then we have two different types of accounts that can be authenticated: User and Service Accounts.
User accounts are managed as Google Accounts, and they represent a developer, administrator, or any other person who interacts with Google Cloud. They are intended for scenarios where your application needs to access resources on behalf of a human user. See Authenticating as an end user for more information.…Service accounts are managed by IAM, and they represent non-human users. They are intended for scenarios where your application needs to access resources or perform actions on its own, such as running App Engine apps or interacting with Compute Engine instances. See Authenticating as a service account for more information.
— GCP — Authentication Overview
Lastly, we have the situation where one account can impersonate another; will explain why this is important later.
This page describes how to allow members and resources to impersonate, or act as, an Identity and Access Management (IAM) service account. It also explains how to see which members are able to impersonate a given IAM service account.
— GCP — Managing Service Account Impersonation
If you wish to follow along, you will need:
Please note: In order to create a clean virtual environment, one might want to use the pyenv tool.
To authenticate as a user to the Google Cloud SDK Command Line Tools we execute:
Please note: There is another command that will also authenticate a user in addition to setting other common configuration values; glcoud init.
We can observe this newly created configuration with:
We can list the storage buckets in the project by executing:
Things to observe:
Here we try to run our Python application and observe that we are not authenticated. Again, this is because authentication for Google Cloud Client Libraries is separate from Google Cloud SDK Command Line Tools.
To authenticate as a user to the Google Cloud Client Libraries we execute:
Things to observe:
Now that we are authenticated to Google Cloud Client Libraries, our Python application executes as expected.
Using Google Cloud Console, we can create and download a key, a JSON file, for the service account. If we read the content of the JSON file, we will observe that the key’s expiration date is Dec 31, 9999. It is essentially a non-expiring key.
To authenticate as the service account to the Google Cloud SDK Command Line Tools we execute (changing out the account’s id and JSON file name as appropriate):
We can observe this overwrote our previous configuration with:
Please note: Here we simply chose to use a single configuration, default; we, however, could use multiple configurations if we wanted to.
We can list the storage buckets in the project by executing:
Because authentication for Google Cloud Client Libraries is separate from Google Cloud SDK Command Line Tools, we are still authenticated with the user account for Google Cloud Client Libraries.
To authenticate as a service account, we have to set an environment variable, GOOGLE_APPLICATION_CREDENTIALS, with the path to the downloaded JSON file.
Our Python application executes as expected.
Please note: If we are not convinced that we are actually authenticating as the service account, we could temporarily remove the Project / Viewer permission from it and observe that the Python application will error; it takes about 30 seconds for permissions to propagate.
As we saw earlier, the service account’s key, the JSON file, is essentially a non-expiring key which makes it a security risk.
Service accounts represent your service-level security. The security of the service is determined by the people who have IAM roles to manage and use the service accounts, and people who hold private external keys for those service accounts. Best practices to ensure security include the following:- Use the IAM API to audit the service accounts, the keys, and the policies on those service accounts.- If your service accounts don’t need external keys, delete them.
— GCP — Service Accounts
At the same time, we often want to test our applications on our workstations using the service account’s credentials. The solution, as we alluded to earlier, is to impersonate a service account.
To demonstrate this feature, let us first authenticate as our other user (one that currently has no permissions in our project) to the Google Cloud Client Libraries:
As before this overwrote our previous configuration:
We can verify that we currently do not have the proper permissions to list buckets in this project.
To enable this user account to impersonate the service account, we add the Service Account Token Creator role for the user account on the service account.
Once those permissions propagate, which takes about one minute, we can then list the buckets in our project with the impersonation option.
We can also update our configuration to always use the impersonation:
We list the buckets in the project without the impersonation flag and it still is successful.
If we wish to update the configuration to not use the impersonation, we execute:
It is this last scenario that ultimately caused me to write this article as I could not find a simple solution on how to impersonate a service account when using Google Cloud Client Libraries. I thought this was odd as this would appear to be something one might commonly want to do.
I did, however, find a reasonably simple solution that involves a slight update to the application code.
To ensure that our code is working, we need to disable our previous mechanisms to authorize Google Cloud Client Libraries. Starting with removing the environment variable.
And then revoking our user account authentication:
From the previous section, we know that the user that can impersonate the service account is still authenticated to Google Cloud SDK Command Line Tools.
The next step is to obtain a time-limited access token (which I believe expires in one hour) for the service account and store it into an environment variable, ACCESS_TOKEN.
We then need to update our application code to use this environment variable; in particular the file helloaccounts/gc_sdk.py:
With this in place, we can successfully run our application as the service account.
For an article that I did not think I needed to write, it turned out to be rather lengthy. Hope you find it useful.
Bursts of code to power through your day.
37 
1
37 claps
37 
1
Written by
Broad infrastructure, development, and soft-skill background
Bursts of code to power through your day. Web Development articles, tutorials, and news.
Written by
Broad infrastructure, development, and soft-skill background
Bursts of code to power through your day. Web Development articles, tutorials, and news.
"
https://medium.com/google-cloud/running-neo4j-with-hosted-kubernetes-in-google-cloud-b479e87b74c0?source=search_post---------268,"There are currently no responses for this story.
Be the first to respond.
Update: since this article was originally written, Google launched Kubernetes applications on GCP Marketplace. If you’re interested in a point-and-click approach specifically, please have a look at my other post on how to launch that. This post still contains accurate information, but is focused on a helm-based deploy.
Since Neo4j published docker images, there are a huge number of options for how to deploy it. Because neo4j can be run as a clustered database, it’s helpful to use a container orchestration tool such as something lightweight like docker-compose, or something more robust like kubernetes.
Kubernetes is an open-source platform for automating deployments, scaling, and operations of containers across clusters of hosts. So essentially, it’s just the sort of tool that’s useful in maintaining a cluster of neo4j instances derived from docker containers. Google’s GCP provides a hosted Kubernetes engine option, so it is quite easy to set up a kubernetes cluster and deploy applications to it, which is what we’ll do today.
The gcloud utility is a set of software that allows command line interaction with the Google Cloud. It’s equivalent to Amazon’s aws wrapper utility.
You’ll need it, so head over to that page and install it first!
You’ll also need a utility called kubectl which controls kubernetes clusters, which you can get from the kubernetes page, for just about any OS.
In the google cloud console, go to the Kubernetes clusters page, navigating like this:
Next, we’ll configure a kubernetes cluster, in this case just with 1 node (the “Size” parameter) since we don’t need a lot of extra capacity for this test instance. Remember that the cost scales with the number of nodes you need and the resources given to them. But this is a critical setting; later in this tutorial we’ll be deploying 3 neo4j pods onto a single node. Obviously this is only for demonstration purposes, as that type of deployment topology would not make sense in production, where you would want redundancy in your nodes to keep the cluster running should any one node fail.
Configuring a new cluster has many options; for our purposes I took most of the defaults, and clicked “Create” at the bottom, but of course full documentation is available from Google.
Make sure that your cluster has a minimum of 3 nodes. When deploying a causal cluster, we’ll be deploying a minimum of 3 different containers to the kubernetes cluster. (Or more, if you end up adding read replicas)
After the cluster starts, you should see a screen like below, which lets us know things are working well:
Now, we can simply click the “Connect” button, and google will give us commands we can use to execute with our Host OS and the kubectl program in order to control and deploy to this kubernetes cluster.
The kubectl proxy command sets up a local HTTP proxy so that you can talk to your google cluster as if it were local, running on your machine.
The following steps will assume that you executed this gcloud command, and the kubectl proxy command.
Next, we’ll need to apply a neo4j configuration to our kubernetes cluster. Kubernetes does this in YAML files.
Fortunately via the kubernetes-neo4j repo, we have basic defaults available that can be applied. In addition to that code, neo4j’s website also has an article describing how to deploy to kubernetes running locally.
Cutting to the chase though, these commands are necessary:
This git repo provides a set of scripts that has everything you need to deploy your neo4j cluster. The kubectl command here simply applies the out of the box configuration, which is a stateful set of 3 core nodes that can discover one another, and a DNS service.
After a minute or two of startup, on your localhost, you should be able to see your deployment.
You can look at the logs to ensure that the various pods are running correctly.
Looking good!
From here, you can follow the directions on the github repository and scale your cluster by adding read replicas, or adding new nodes overall. Most of this can be done by applying other templates which the github repo provides.
The kubernetes configuration is just a set of environment variables and a bit of shell script wrapped around the official neo4j docker images. You can see that configuration in the statefulset.yaml file. The kubernetes-specific bits deal with the details of how the nodes discover one another, and what the topology of the default cluster is (in this case, 3 core nodes). The neo4j docker image itself already provides a lot of configuration items, like the ability to pass a whole range of configuration options directly to the database via environment variables.
Using kubectl, you can already directly execute commands against the individual pods like so. Doing this doesn’t require setting up any extra networking or port permissions.
You’ll want to configure and customize your deployment template for neo4j found in the cores directory of the github repo. In particular, the default deployment as of this writing specifies a clusterIP of None, meaning that the deployment is headless, and can’t be accessed from the outside. This is an initially wise setting, since the setup also disables authentication for the purposes of ease of setup. Make sure to carefully review the neo4j and kubernetes configuration to be sure you know what you’re getting!
These configuration options are non-trivial, and have heavy consequences for production performance and security. Consult your local kubernetes expert in order to make choices that are right for your deployment.
Google Cloud community articles and blogs
77 
1
77 claps
77 
1
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Architect at Neo4j
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@duhroach/google-cloud-storage-on-a-shoestring-budget-55f054fad436?source=search_post---------269,"Sign in
There are currently no responses for this story.
Be the first to respond.
Colt McAnlis
Oct 24, 2017·6 min read
A common question I get at Meetups and conferences relate to billing optimization. Things like “what’s the cheapest way to do X” or “how do I cut my costs” or “how do I not go over my free quota”.
IMHO Google Cloud Platform has some of the most transparent, cost effective pricing structures available from cloud providers. But even with that, it’s still a challenge to properly figure out how to optimize your architecture for price.
So, let’s take a look at how to run Google Cloud Storage on a Shoestring budget.
Disclaimer : These prices are accurate as of 10/5/2017. Since time continues to move forward, these prices may not be accurate in the future. Also, these are my best attempts at “math”. All standard disclaimers about me being bad at math apply here.
A quick summary of GCS’ pricing page reveals that your GCS bill is a composite of 3 main things : Data storage, Network usage, Operations (4 if you’re using near-line, but we’ll ignore that.)
Here’s what you get for free, per month:
We will need to factor this into all of our forward facing calculations, since this quota is taken into account for your monthly services. Let’s break each one down really quick and see what the costs would be.
Once you eclipse your free storage quota, you’ll see that storage costs differ slightly per area. For example, us-central1 is 0.02 per GB, where us-east1 is 0.023 (it also changes if your in AMEA or APAC).
So, what’s it cost to store 1TB of data per month in the IOWA center (us-central1)?
(1024GB-5GB) * 0.02 = $20.38
That’s pretty cheap, I suppose. That’s basically lunch for two people to store 1TB of data.
You’re charged for outbound costs, depending on the origin, and the outbound tier. For example, if you only transfer 0–1TB, from the Iowa location, it’s $0.12 per GB to anywhere in the world (except China and Australia which have diff costs) based on a tiered structure (so if you go over 1TB, you’ll get charged differently).
So, what’s it cost to transfer 1TB of data a month to to external clients (outside of GCP) in the US or EU?
(1024GB — 1GB) * 0.12 = $122.76
Operations on a bucket/object are anything which queries or changes data on it. Ops are broken into two categories : A and B, and have different costs associated:
$0.05 per 10k ops (A — insert, patching, Listing buckets, listing objects, watching & triggers)
$0.004 per 10k ops (B — mostly everything else)
So, let’s say your 1TB of data is broken across 1k objects in GCS.
Let’s say the average user looks at 500 object listings a month (you’re a shoe site, or something), in batches of 50, so 10 calls to our API per month, per user.
What’s this cost?
If we have 10k users @ 10 API calls a month, that’s 100k Class A ops. Objects.list is a Class A operation. Meaning we get 5k of those free a month. . so:
(100k -5k)*0.05 = $4.75
Prices are highly dependent on your scenarios, so let’s take a few examples.
Scenario 1 : Data in a regional bucket, transferring to the same region.
Note: moving data from a regional bucket to a service (GCE,GAE,GCF,GKE) in the same region is free. However the cost for network egress looks to be uniform between GCS, and all services. So, there’s no upside to sending data to a Frontend and serving that to a user.
Although sending GCS->Frontend->user will incur costs if the frontend is in a different region than the GCS bucket.
Scenario 2 : Data in a regional bucket, transferring to a different region.
Let’s put our data in us-west1, and have a user in us-east4. We’d be getting charged the same $122.76 cost for network egress, but our performance wouldn’t be ideal due to distance latency.
What if we copied the data to a bucket in a closer region (us-east4)?
(1024GB-5GB) * 0.02 = $20.38 and is doubled, since us-east4 and us-west1 have the same storage cost. If you assume that xfer costs remain the same, then you’d end up with a total cost of $122.76 for network, and $40.76 for storage giving $163.52. So, a little more expensive.
What if we used a multi-region bucket instead?
Storage costs are 0.026 / gig / month for multi-regional. So, (1024GB-5GB) * 0.026 = $26.494 for storage, plus $122.76 for network gives us $149.25. So it’s about ~18 dollars cheaper to use a multiregional bucket instead.
You can specify the load-balancer to fetch from a GCS bucket directly, which would allow the data to be distributed and cached via the CDN w/o needing to set up extra buckets.
LB is 0.08 per /GB for the first 10TB of cache-egress, and 0.04 /gig of cache fill.
So, what would it cost to xfer 1TB of data between regions to a client using the LB as a front end to the bucket?
So, summary:
For cross-region fetches (in the same multi-region):
If we have 10k users @ 10 API calls a month, that’s 100k Class A ops.
Objects.list is a Class A operation. Meaning we get 5k of those free a month… so: (100k -5k)*0.05 = $4.75
Does it make more sense to list that in Datastore?
Considering DataStore is 1GB of storage free (0.18 /gig over that); it costs you 0.06 per 100k entries read.
If we had 500 object listings, let’s assume 1k of metadata each; that would be 500k of metadata storage (which is within the free tier) .
Each user is looking at 500 object listings a month; at 10k users, that’s 5000k entity reads / month. Factoring in the cost of 0.06 per 100k reads, we end up with ((5000–50)/100)*0.06= $2.97
So, Summary:
It’s cheaper to store your object metadata in data store, and fetch it there, rather than doing a GCS bucket listing.
Custom metatdata on an object is charged per character for the object. If you assume 1k metadata per object, and 100k objects, you’re looking at an additional 1k of data in your GCS bucket charges. That’s still relatively low for storage, ($9.5^e-7 cents total);
Fetching that metadata falls as a classB operation, which you get 50k of free / month. So, if you were doing 100k metadata fetches a month, that’s ((100k-50k)/10k)*.004 = $0.02.
Likewise, DataStore gives you 1GB free storage a day; so your 100k would be in the free tier, storage wise, daily.
You get 50k entity reads / day free. If we assume 100k / month, then 100k / 30 = 3k reads / day.
All within the free tier.
In order for Datastore to not make sense, you’d have to be doing >50k entity reads a day, at which point, you’re pro-rated charged $0.06 per 100k entities you read. Meaning you’d be doing 1500k+ entity reads a month before you’d jump over the free tier, but even at that point, it’s still cheaper to do the reads in Datastore rather than grabbing metadata directly from GCS.
So, Summary:
Datastore is the cheapest way to store and fetch metadata on your objects.
Absolutely! Stay tuned to my medium page for more details, and don’t forget to check out Google Cloud Performance Atlas, where we help you trim down your cloud usage and maximize your profit-to-cost structure.
DA @ Google; http://goo.gl/bbPefY | http://goo.gl/xZ4fE7 | https://goo.gl/RGsQlF | http://goo.gl/4ZJkY1 | http://goo.gl/qR5WI1
127 
2
127 
127 
2
DA @ Google; http://goo.gl/bbPefY | http://goo.gl/xZ4fE7 | https://goo.gl/RGsQlF | http://goo.gl/4ZJkY1 | http://goo.gl/qR5WI1
"
https://medium.com/@azamsharp/sharing-augmented-reality-experiences-using-google-cloud-anchors-in-arkit-for-ios-c1e1edd2f7d3?source=search_post---------270,"Sign in
There are currently no responses for this story.
Be the first to respond.
Mohammad Azam
May 22, 2018·5 min read
Google I/O was few weeks ago where Google announced a lot of amazing technologies. As an AR enthusiast, I was really excited by the annoucement of cloud anchors. Cloud anchors will let users share their augmented reality experience with other users. Cloud anchors is part of the ARCore framework but it is available for Android and iOS. In this post I will walk you through a prototype I build which showcases the power of cloud anchors.
NOTE: There is a lot of code involved in this project. Not all code is part of this post. I highly encourage you to download the code here or at the end of the post. You will need to insert your own API key for Firebase project as well as for the Google Cloud Anchors.
Instead of talking about it let me just show you the demo I created. Check out the video below:
The phone on the left is controlled by me and the phone on the right is controlled by my daughter. We are looking at the same exact cube, through different phones and interacting with the same cube by changing the color of the cube. If I change the color, she sees it and if she change the color I see it.
Create a new Xcode Augmented Reality project and setup the Podfile to download the required CocoaPods. Copy and paste the following pods in your Podfile and then run “pod install”.
This will install all the required pods for your application. Apart from the CocoaPods, you will also need to get the API key and enable Cloud Anchors in the Google Cloud dashboard. You can read about how to enable cloud anchors and setting up the Firebase project here. Also, make sure that your Google Cloud Platform billing is up to date and active.
Before we start implementing cloud anchors, it will be a good idea to understand how cloud anchors work. The position of the anchor does not depend on the latitude and longitude since latitude and longitude are not accurate enough. Latitude and longitudes also does not work indoors.
Cloud anchors begin by mapping the area where you want to drop the anchors. Once the area is mapped the cloud map is uploaded to the Google Cloud platform which then returns a unique cloud identifier. This process is called “hosting an anchor”.
Later, using the cloud identifier along with the approximate cloud map points we can retrieve the anchor. This process is called “resolving an anchor”. We will be using Firebase database to store the cloud identifier along with other information related to the physical object like current color.
NOTE: It is very important that you provide enough mapping information to cloud anchors so it knows about the environment. If you do not provide enough information then your anchors will appear few inches apart as shown in the above video.
The first step is to initialize the session related to cloud anchors. Create properties for GARSession, ARAnchor ad GARAnchor as shown below. Also don’t forget to add your apiKey for Google Cloud Anchors.
You also need to make sure that you are feeding frames to the GARSession. If you don’t feed frames to the session then cloud anchors will not be able to correctly determine raw points for that area. This can result in obtaining incorrect cloud anchors which may be located few feet apart. This is accomplished in the didUpdate function as shown below:
Next, we will add a cloud anchor where the user touch intersects with the plane.
The addAnchorWithTransform is responsible for hosting the cloud anchor to the Google server.
When the anchor is hosted successfully we get a cloud identifier back as shown in the code below:
NOTE: Cloud anchors have a lifetime of 24 hours.
Once we have the cloud anchor we can attach virtual objects to those anchors. This process is called resolving cloud anchors. In the code below we used the cloudIdentifier we received after hosting the cloud anchor and used that to resolve the anchors. Once the anchor gets resolved we simply attach a virtual 3D cube to that anchor and add it to the scene view as shown below:
As you might have guessed, cloud identifier is an integral part of cloud anchors. Using cloud identifier we can retrieve the cloud anchors and then use it to place virtual objects in our surroundings. But how can we share our cloud anchors with other users. For this we need to setup some sort of cloud storage where we can persist our cloud identifiers, along with other attributes of the virtual object that we want to share.
Luckily Firebase Realtime Database is exactly what we need. We can store cloud identifier along with other attributes we want to share in the Firebase Realtime Database which can later be accessed by other users.
The below code shows the implementation of addBlock function which is invoked when the user adds a new virtual object to the scene. The block is added at the cloud anchor position and then later persisted in the Firebase database.
As soon as the record is persisted to the Firebase database an observer is triggered which fetches the cloud identifier and the hex color saved. The code is shown below:
There is lot more code which deals with touching the virtual object and changing the colors of the virtual object. You can download the complete code here.
Google Cloud Anchors is an interested technology that allows to share the AR experience with multiple users. This opens the door for endless possibilities and I can’t wait to see what developers build with it.
If you liked this post and want to support my work then perhaps you will be interested in enrolling in my “Mastering ARKit for iOS” course.
www.udemy.com
iOS Developer, speaker and educator. Top Udemy and LinkedIn instructor. Lead instructor at DigitalCrafts. https://www.udemy.com/user/mohammad-azam-2/
143 
3
143 
143 
3
iOS Developer, speaker and educator. Top Udemy and LinkedIn instructor. Lead instructor at DigitalCrafts. https://www.udemy.com/user/mohammad-azam-2/
"
https://medium.com/google-developer-experts/micro-batching-a-streaming-input-source-using-google-cloud-dataflow-ccd30d2aabf2?source=search_post---------271,"There are currently no responses for this story.
Be the first to respond.
Often while building a data processing pipeline, your data can be either Bounded (of a defined size) or Unbounded (with an undefined size).
An example of a Bounded data source could be a text file containing all the words inside the Oxford Dictionary while the twitter stream of all the Tweets containing a specific #hashtag is an example of an Unbounded data source.
Google Cloud Dataflow allows you to handle both types of Data source by providing you with options to create either a Streaming or Batch pipeline with handle Unbounded and Bounded data source respectively.
In our use case, we had to process data coming from an Unbounded source but we didn’t want to process these events immediately; instead we wanted to do an operation on the collected events once every 2 minutes.
We tried using a Batch pipeline to do this job, but unbounded sources cannot be read by dataflow in batch mode. This is by design, as batch pipelines expect a finite amount of data to be read and to terminate when done processing it.
When we switched to a Streaming Pipeline, Dataflow expected us to operate on the events as soon as they entered the pipeline; which was something which we didn’t want to do.
Thanks for reading this blog, if you are working at a high growth company and are looking for a large scale, real-time data collection platform; take a look at https://roobits.com/.We might be what you are looking for!
Enter Sharding!Dataflow allows developers to write all the incoming events entering the Pipeline into a file for processing it at a later point of time.
Sharding is supported by almost every IO Transform present in Dataflow; for example I have used it with BigQueryIO and TextIO but it’s also available to use with KafkaIO, AvroIO and others.
Sharding the incoming events asks for 2 things :
The number of shards is essentially the number of files that Dataflow should create to write the incoming events to.So if you set the number of shards to 10, dataflow will create 10 files and write your incoming events among these 10 files. Having a high number of shards ensures that if one of the files get corruped, you have the rest of the events safe in other files which weren’t corrupted thereby reducing the risk of a single point of failure.
The Triggering frequency is essentially the time after which you want to read the above files.It can be anything that you want, but do keep in mind that a higher time means a larger file created!
And that’s it!
For our use case, we wanted to process and load the events once every 2 minutes into BigQuery, so we modified our Dataflow Pipeline to be :
from :
All it took was 2 lines of code to convert our Streaming Pipeline into a Streaming Pipeline that Micro-Batched the incoming events at the specified time.
Note : Dataflow by default stores the file into a Google Cloud Storage Bucket so using this might incur you the storage cost charged by GCS.
Thanks for reading! If you enjoyed this story, please click the 👏 button and share to help others find it! Feel free to leave a comment 💬 below.
Have feedback? Let’s connect on Twitter.
Experts on various Google products talking tech.
278 
278 claps
278 
Experts on various Google products talking tech.
Written by
Has an *approximate* knowledge of many things. https://aftershoot.co
Experts on various Google products talking tech.
"
https://medium.com/@davidcarboni/google-cloud-professional-architect-35f27d368dc8?source=search_post---------272,"Sign in
There are currently no responses for this story.
Be the first to respond.
David Carboni
Jul 12, 2019·7 min read
Ever since working with a great team at the BBC, I’ve been noticing the momentum behind Google Cloud. I’ve been looking to do a certification for a while, so decided to go with Google’s Professional Cloud Architect.
If you do some research on the certification, you’ll find words like “vast” describing the scope of the exam. They’re not wrong. From compute, storage and networking, through business requirements, capacity planning, SRE, regulatory compliance, containers, continuous deployment, even kubectl commands, this isn’t something you can study for straight out of the gate from coding bootcamp.
The range of topics and the layers of knowledge, from CIDR blocks to Continuous Deployment to cloud migration and hybrid connectivity, sets a high bar. That’s what makes it both tough and also respected. It takes a healthy and broad level of experience to tackle it. What’s nice is that experience is more than theoretical and more than rote product knowledge, and there are common-sense aspects in there too that probe real-world experience. Something I particularly like is that the answer isn’t always Google.
The advice I was given was to go for the Coursera material. That stood me in good stead. Don’t expect perfection though: this stuff is changing all the time and there are a few bloopers and “human touches” in the content. My favourite is when the instructor’s Google Home starts talking to him in the background, closely followed by the time there’s a rustling sound, as if someone is monkeying around behind the camera, and the straight-faced, earnest instructor can’t quite hold back a lovely smile.
When it comes to the cloud, over-engineered perfection isn’t the name of the game and for me these little foibles add real warmth to what is otherwise a pretty intense process of learning. I listened to much of the content on 1.5x speed, partly to get through it and partly to stop my mind from wandering. There’s nothing quite like feeling like you‘re working to keep up to keep you focused. It’s important to say you won’t get everything from the course material. You’ll get good coverage of most areas, but it’s unlikely the course content alone will get you through.
I started with the Architecting with Google Cloud Platform Specialization and Preparing for the Google Cloud Professional Cloud Architect Exam. I’ve had hands-on experience with AWS and GCP at this point, but haven’t covered their breadth of services, so those courses broadened my horizon but, perhaps more importantly, underscored how many services there are and how much there is to know about each. Having completed the courses comfortably, and hit 80% on the practice exam, I felt I had a pretty good grasp of how under-prepared I was.
I’m strongest on compute: virtual machines, functions, Kubernetes and PaaS are all familiar to me, although I needed to get into the finer detail (e.g. how are storage throughput and network capacity affected by the number of cores on a Compute Engine instance?). I decided to round out the other areas covered by the exam.
I knew I had plenty to learn about the range of storage products, their different aims, use-cases, capacities, advantages and disadvantages, so I decided to do a few courses from the Data Engineering, Big Data, and Machine Learning on GCP Specialization. I also decided, because Kubernetes is a Leviathan with hidden depths, I’d do as much of the Architecting with Google Kubernetes Engine Specialization as I could before the exam. This gave me the detail I needed to answer one or two questions I might otherwise have had to make intelligent guesses for.
As with any certification exam, there’s diminishing returns to over-studying. You’ll likely only work in depth in a few areas and all areas will change over time, so knowing it all as it stands today doesn’t add much value. Knowing enough to pass across the board, staying up to date with most things and getting really good at a couple of things is my idea of pragmatic and practical. I booked the exam and dug in for some more study.
I now felt I had compute and storage covered, plus some more detail on GKE, which got me feeling more comfortable. I have a decent grasp of networking, and by now had a good idea of GCP’s take on VPCs and load balancing. I had touched on Stackdriver both theoretically and in practice, so held off on learning more there. I felt I still needed more detail though, so I went looking for blog posts like this one by Jean-Louis (JL) Marechaux and this one by sathish vj to get some leads on where to deepen my understanding. They were particularly effective in getting me more than adequately terrified about what I was facing.
In case you think I’m made of the stuff that causes impostor syndrome in innocent bystanders, someone sliding through with ease, I’d like to share with you the sense of foreboding I felt coming up to the exam. I hope that if you’re studying for PCA and all you see on this glistening Internet are what look to be smug people who sailed through under a light breeze, a glass of prosecco and strawberries in hand, pinkies out, shades on, that I can share a real moment which I hope will bring you solace.
It’s a tough exam. The range of things you realise you don’t know will grow faster than the number of things you do know. Much like life, you can’t “win” this one, but you can show up with the best of your efforts and experience. I ended up trawling through pages and pages of documentation, gleaning details, into the nigh, hoping that just a few what felt like a smattered collection of details I was gathering would get me over the bar — a bar looked very high.
I found a practice test to take, the night before the exam, on a website somewhere. I gave it a go, just to get an “exit poll” on what I’d learned ahead of exam day. It had 10 questions, two of which I recognised from the official practice exam. I scored 50%, including the two questions I already knew the answer to. It wasn’t good. With hindsight, I think the questions were open to interpretation, or maybe I was tired, but at the time I became increasingly concerned that I was about to face-plant.
There’s no official pass mark for the exam and you get no feedback. Just a yes or no. It’s two hours and 50 questions. I was anxious. There was one timely bright spark: I’d watched this Simon Sinek talk in my worried state the night before. He explains that “anxious” and “excited” are physiologically similar. It turns out that just saying “I’m excited” rather than “I’m nervous” can materially improve your performance.
My exam was at the Pitman Training Centre in central Edinburgh. I arrived early, had my ID ready and they got me started. There’s something about these kinds of testing facilities where it feels like everything is running Windows Vista on a Pentium II and you’re never sure if the next screen is really going to load. I settled in. “I’m excited” I said out loud in my head.
I opted for a strategy of answering everything as a first guess, then review and re-review, gradually whittling down to those last few tough nut questions. The most important tip I can give you for this exam is also the most simple: read the question. And the answers. Pay attention to the language and don’t rush. The questions are well written. Don’t assume you know what’s being asked. Bookmark questions for review.
My first pass took just over an hour. In the next half hour I did a full review to eliminate the questions I was most confident in. I was left with about 15 to whittle down. I spent my last 20 minutes going over them, committing to answer one by one. I knew there’d be a few that would have be educated guesses. I wasn’t shooting for 100% but I knew I had to minimise the chance of slipping under the bar by a few points.
With two seconds left on the clock, I submitted my final answer and the exam was over. A feedback form later, I found myself staring at a white screen with writing on it, explaining what would happen next. After those two hours of intense concentration I was word-blind. I scanned the sentence back and forth, but saw no sign of a test result. Finally my eyes scanned up a little way and landed on a single word: Pass.
I’ve never been so pleased, so relieved and so thankful to see those four letters. For me achieving Professional Cloud Architect felt like stepping up to a genuine challenge. If you’re considering it, I’d certainly recommend it, but not for light entertainment.
Knowing what I do now I can say I’ve got a new level of respect for people who’ve done it. More than a learning experience, it’s the closest I’ve come to something that can assess those of us who identify as something like Architect or Tech Lead. There’s something both humbling and satisfying about putting yourself to the test and coming through.
When it comes to measuring yourself against the theory and practice of designing, building and operating modern cloud architectures, PCA is the highest and most relevant standard I’ve found. Thank you to Google Cloud and all the training course developers, it’s been a blast.
No certification post would be complete without some proof that I’m not just making this up to entertain you, so here’s the genuine article.
Hands-on culture and techology. Work hard be kind. CTO at Policy in Practice (https://policyinpractice.co.uk)
See all (186)
84 
84 claps
84 
Hands-on culture and techology. Work hard be kind. CTO at Policy in Practice (https://policyinpractice.co.uk)
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@awagonfeld/reflections-5-months-in-at-google-cloud-and-looking-out-to-the-horizon-9163596e1d8e?source=search_post---------273,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alison Wagonfeld
Oct 1, 2016·3 min read
It’s been five months since I joined Google. Yesterday we had our first event as the newly minted Google Cloud business, and today we’re launching a new advertising campaign for G Suite (rebrand of Google Apps for Work). From the moment I arrived at Google it’s been a whirlwind (just ask my family!). It feels like a good time to pause for a moment and reflect, while also looking out to the horizon.
Throughout my career in technology I’ve strived to answer the question — How can we use technology to help people do things they couldn’t do before? That’s what I did in the late 90’s at Intuit, when I helped launch QuickenLoans to let consumers shop and apply for mortgages online, and what I was trying to do at Greenlight, an early online car-buying platform. Most recently at Emergence Capital, I worked with start-ups as they tried to answer this question. But at Google, the opportunity feels so much bigger, not just because “the cloud” represents such a fundamental change in how businesses can use technology, but also because I’ve seen how people really work differently when using cloud-based productivity apps.
Working at Google completely upended processes that I took for granted: writing and collaborating. My initial exposure to Google’s productivity tools had been through commenting on my kids’ school work. My daughter would share an outline in Google Docs, I’d suggest an edit, my daughter would accept (or mostly reject!) it, end of story. Since joining Google, using all of G Suite has been game-changing. My team and I can co-create an entire presentation, from different locations, in real-time, in a matter of minutes. As an extroverted thinker, this kind of collaboration has completely changed how I work. It’s to the point where I received an email with a Microsoft Excel attachment about a soccer carpool the other day, and thought to myself, ‘How inefficient for each of us to work off of different versions of the spreadsheet…’ and quickly converted it to Google Sheets so each of the families could weigh in together, in one updated sheet, accessible on all of our phones.
G Suite is just one part of Google Cloud — yesterday at Horizon we announced new products and features highlighting the complete array of technology, including Google Cloud Platform, our Maps and Machine Learning tools and APIs, and also the Android phones, tablets, and Chromebooks for work and schools. Together, Google Cloud represents a fundamental shift that has accelerated over the last year; we’re taking the huge set of technology and infrastructure that we’ve developed over years and externalizing it. We’re offering it as a canvas upon which others can imagine and create.
In addition to sharing product news, Horizon was also an opportunity for customers to talk about what they are building. It was great to hear how Airbus is using machine learning to solve an imaging problem that has been challenging them for 20 years. I loved hearing more about how Niantic is using Google Cloud to deliver its augmented reality mobile app (Pokemon Go!) to millions and millions of users. And while listening to the Evernote CEO, I kept thinking about how cool it will be when Evernote can use Google Cloud machine learning to make sense of all of my unstructured notes.
When I looked around the room at 300+ CIOs yesterday, I thought to myself, “I wonder what all of these companies will build with Google Cloud technology.” It’s impossible to predict what solutions customers will dream up, and that is what’s so exciting for me. It goes back to the original reason I started working in tech: I want to help people use technology to do things they couldn’t do before.
As I move into month six at Google, I’m the first to say that I don’t know exactly where Google Cloud will take us, but I do know that we have thousands of Googlers who are excited to build the infrastructure, applications and devices to help customers imagine and build what’s next. Because in the end, what’s so exciting about Google Cloud isn’t how far Google has come, but how far we can go together.
CMO, Google Cloud
See all (326)
32 
1
32 claps
32 
1
CMO, Google Cloud
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@gmusumeci/how-to-create-a-service-account-for-terraform-in-gcp-google-cloud-platform-f75a0cf918d1?source=search_post---------274,"Sign in
There are currently no responses for this story.
Be the first to respond.
Guillermo Musumeci
May 24, 2020·7 min read
Before we start deploying our Terraform code for GCP (Google Cloud Platform), we will need to create and configure a Service Account in the Google Console.
In this example, we will create a master Service Account with permissions at Organization-level and Project-level.
"
https://medium.com/google-cloud/gke-with-google-cloud-single-node-filer-nfs-4c4dc569964f?source=search_post---------275,"There are currently no responses for this story.
Be the first to respond.
Sometime ago, a customer of mine asked me how they can setup a simple GKE cluster with a shared NFS filer. Back then I put together a small demo with GKE+Single Node File as provided by Google Cloud Marketplace. However, there are now two ways to do this: use the ‘single node filer’ and GCP’s managed Cloud Filestore (hint, the latter is strongly recommended!!).
Google Cloud provides an easy to deploy Single Node Filer which with a couple of clicks, you’ve got your very own NFS server. While there are some very clear issues with the single node filer such as being a single point of failure (i.,e it only runs on one VM), its much easier to setup than GlusterFS or other systems on GCP. Even GKE Persistent Volumes come with their limits where only one node can write while many can read.
However, just as easily with the same number or clicks, you can create a _managed_ NFS server on GCP as Cloud FileStore.
Ofcourse both offer a standard NFS host:/share to for your application to bind to and the steps outlined below describe how to deploy a simple GKE service that mounts the filer for read-write by all instances.
This article also describes two variations for NFS Storage on k8s and you can choose one of them during this tutorial. If you would rather just understand GKE+NFS, use the SingleNode Filer. If you’d rather setup Filestore as a managed service, use that. Finally, this article demonstrates two ways to mount the NFS type you chosse: direct Volumes or as a StorageClass.
—
A direct volume mount will simply map a voume to the root mountpoint. In this example, its 10.128.0.9/data. Any files read or written to by any deployment will be under /data.
On the other hand, a client provisioner will _carve out_ a dynamic path under /data for each persistent volument claim set. That is, if I declare the configuration below, k8s will _dynamically_ create a slice specifically for this claim under ‘/data’. For example, k8s will create something like:
The NFS provisioner is described here in more detail:
ok…the first step is to setup an NFS Server:
— — — — — — — — — — — — — — — -
→ SingleNode FIler:
For the Single Node Filer, I called my filer singlefs-1-vm.
I selected us-central1-a as the filer zone as well as the zone for the GKE cluster.
On your local system, find the ip address for the filer
for me it was:
→ Cloud FileStore
For Cloud FileStore, simply create any instance but remember to set the FileShare mount to ‘/data’ (whch is what I use in the example yaml files below).
Note the IP address provided since you will use that in the specifications later if you choose to use FileStore.
I used Cloud Shell and ran:
The following section will deploy GKE ReplicaSet with direct volume mounts on NFS:
Copy and paste the following files in:
Note: I added in the IP address of the single node filer (10.128.0.9)
SPECIFY the IP address (GCE internal DNS does not does not resolve without additional work!!!)
Now lets use a confiuration to that will mount the NFS volume in directly:
the image salrashid123/single_node_filer does the following simple steps:
Create the GKE service and replicationController:
Get a coffee until the GCP loadbalancer is setup
The IP address for the LB I got is 104.197.18.186 (yours will be different).
Invoke the endpoint to read the file:
You won’t see anything (unless you manually added a file to /data ealier manually)
Then add a file by invoking the /write endpoint
Repeat a couple of times and then read:
What that indicates is different nodes all writing to one mount point because each file shows the node name that wrote the file!
If you browse the NFS system’s filesytem, you will see the files created directly on
— — — — — — — — — — — — — — — — — — — — — — — — —
Now lets move on to using the same NFS server with a provisioner. What this does is setups a StorageClass on which k8s will dynamically parcel out subdirectories for your PersistentVolumeClaims.
Lets start off. We’re going to use a helm template to and install the NFS Provisioner:
This step shoud’ve been done arleady above. In addition, allow yourself to crate higher level objects like StorageClasses:
2. Install Helm
3. Install NFS Provisioner Helm Chart
I personally prefer to _not_ install tiller so the command set below only setsup helm as ‘client-only’
Now initialize the chart by specifying the NFS server’s IP address from the previous steps
Create the provisioner
4. Deploy your application and the PVC
5. Note that there is a new “StorageClass” called ‘nfs-client’ and our claim ‘nfs-claim’ is now bound
5. Send some /read and /write traffic to it.
Note the IP address for the loadbalancer is different (i ran this setup just recenctly so i got a different IP)
As before, the output shows multiple reads and writes creating and reading from the same mount.
If you used the SingleNodeFiler, simply ssh into that instance and navigate to the /data folder. You should see a dynamic PVC claim that the provisioner made for you. Under that folder, you’ll see the two files you just created!
This article showd a variety of ways to setup NFS on GCP:
As well as two ways to mount NFS
You can pick and choose whichever one you need though I would suggest avoiding the SingleNode filer if availabity is of concern.
— — — — — — — — — — — — — — — — -
Google Cloud community articles and blogs
254 
1
254 claps
254 
1
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/@thanachart-rit/machine-learning-%E0%B8%89%E0%B8%9A%E0%B8%B1%E0%B8%9A%E0%B9%80%E0%B8%81%E0%B9%87%E0%B8%9A%E0%B9%80%E0%B8%A5%E0%B9%87%E0%B8%81%E0%B8%9C%E0%B8%AA%E0%B8%A1%E0%B8%99%E0%B9%89%E0%B8%AD%E0%B8%A2%E0%B8%88%E0%B8%B2%E0%B8%81%E0%B8%87%E0%B8%B2%E0%B8%99-google-cloud-next18-94fa842ee7f5?source=search_post---------276,"Sign in
There are currently no responses for this story.
Be the first to respond.
Thanachart Ritbumroong
Jul 25, 2018·2 min read
Google Cloud Next เป็นงานประจำปีของ Google Cloud ที่จะมาประกาศสิ่งใหม่ๆ และมีพวก break out session ที่เป็นหัวข้อย่อยๆ เรื่องน่าสนใจเกี่ยวกับ Product ทางด้านของ Google Cloud Platform (GCP) ซึ่งสิ่งที่เราสนใจมากกว่าสิ่งอื่นใด คือ Data Analytics กับ Machine Learning (ML)
หลังจากวิ่งเข้าวิ่งออก break out session มาทั้งวันในวันแรก ก็ขอเอาสิ่งที่น่าสนใจเกี่ยวกับ Machine Learning ที่ได้ฟังวันนี้มาแชร์ให้ฟังนะครับ
งานที่เป็น repeated decisions หรือ งานที่เราต้องตัดสินใจซ้ำๆ และเกิดขึ้นหลายครั้งมากๆ ไม่ใช่สิบ ยี่สิบครั้ง แต่เป็น พันๆ หมื่นๆ ครั้ง เช่น จะ stock สินค้าเท่าไหร่ดี transaction นี้ เป็น fraud หรือเปล่า จะอนุมัติบัตรเครดิตให้ลูกค้าคนนี้ดีไหม (จะกินอะไรดีเที่ยงนี้???)
หลายคนยังมองว่า Machine Learning เป็นเหมือนปาฏิหาริย์ที่สามารถเสกได้ทุกสิ่งอย่างอัน เช่น จงใช้ ML หาลูกค้าใหม่อีกซัก 1 ล้านคนให้หน่อย หรือ เราจะไปเปิดสาขาตรงไหนดี หรือ ช่วยบอกทีว่างวดหน้าหวยจะออกเลขอะไร
งานบางอย่างก็ไม่เหมาะกับ Machine Learning อาจจะเหมาะกับงานด้าน Analysis มากกว่า เพราะต้องการ judgement ของคนช่วยประกอบ
งานด้าน Machine Learning จะเป็นการ Automate Decision Makings ที่ (ความเห็นส่วนตัว) มีความเป็น structured decision-making ในระดับนึง เพราะโมเดลจะต้องสร้างโครงสร้างการตัดสินใจ ที่อาจจะเป็น set of rules หรือ สมการที่ซับซ้อนมากมายระดับนึง เพื่อนำไปใช้ในการตัดสินใจในอนาคต
Speaker ท่านหนึ่งก็สรุปความแตกต่างของ Classical Programming กับ Machine Learning ไว้ได้เห็นภาพมากๆ ก็คือ
ถ้าเป็น Classical Programming เราก็จะเขียน Rules เพื่อประมวลผลข้อมูลในการหาคำตอบ โดย Rules ที่เราเขียนนั้น ก็จะเป็นเทคนิคต่างๆ ที่เราเล่าเรียนมา เช่น IF-THEN-ELSE หรือ การวน LOOP ต่างๆ
ส่วนถ้าเป็น Machine Learning จะเป็นการเอาข้อมูลและผลลัพธ์ที่เราทราบ มาสอนให้ Machine สร้าง Rules ขึ้นมาให้ (พวกนี้ก็คือ Supervised Learning)
ด้วยความแตกต่างนี้ ทำให้เป็นที่มาของความแตกต่างในการทำงาน รวมถึงทักษะความรู้ที่ต้องใช้ ตัวอย่างเช่น
#ช่วงขายของทำมาหากิน
นอกจากเคสตัวอย่างข้างต้น น้องๆ ฝึกงานที่ Data Cafe Thailand ก็ยังได้ลองทำ Use Case เกี่ยวกับ Object Detection ไว้หลากหลายอัน ไว้ว่างๆ จะเอามาให้ดูว่าเค้าทำอะไรกันบ้าง ตัวอย่าง ก็มีตั้งแต่ detect เหรียญแล้วคำนวนว่า กองเหรียญนี้ มีเงินกี่บาท scan รูปแล้วบอกชื่อ ขนมไทย หรือ scan ดูว่าเป็นขยะประเภท ขยะเปียก ขยะ recycle หรือ ขยะอันตราย
Machine Learning จริงๆ แล้วไม่ได้เป็นเทคโนโลยีที่มีพลังอำนาจอะไรพิเศษกว่าเทคโนโลยีอื่นใด ช่วงนี้คำนี้เป็น buzzword อย่างมาก ก็หวังแค่ว่าคนจะเข้าใจว่า Machine Learning แท้ที่จริงแล้วทำอะไรได้ ทำอะไรไม่ได้ เราจะได้เริ่มต้นเอา Machine Learning มาใช้เพื่อเสริมกำลังธุรกิจได้อย่างมีประสิทธิิภาพ
อีกเรื่องหนึ่งจาก Speaker ท่านหนึ่งที่พูดแล้ว ผมชอบก็คือ ให้นึกถึงข้อมูลที่เราไม่เคยเก็บได้มาก่อน ในปัจจุบันเราอาจจะมีเทคโนโลยีที่เก็บข้อมูลเหล่านี้ได้แล้ว และจะช่วยให้ Machine Learning ของเราเก่งขึ้นก็ได้ เพราะท้ายที่สุดแล้ว คนที่มีข้อมูลมากที่สุดจะชนะคนที่มี Algorithm ที่ดีที่สุด (Quote นี้จากเจ้าพ่อ Machine Learning Andrew Ng)
“It’s not who has the best algorithm that wins,It’s who has the most data”
— Andrew Ng
ขอบคุณทีม Google Cloud Thailand ที่เป็นธุระช่วยประสานงานให้ จะได้มาร่วมงาน Google Cloud Next’18 นะครับ ^^
Lecturer at Management of Analytics and Data Science Program, National Institute of Development Administration, Thailand and Data Analytics Consultant
See all (501)
32 
32 claps
32 
Lecturer at Management of Analytics and Data Science Program, National Institute of Development Administration, Thailand and Data Analytics Consultant
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/swlh/all-you-need-to-know-about-google-cloud-vision-api-6ae8e17969a?source=search_post---------277,"There are currently no responses for this story.
Be the first to respond.
Google Photos is the first to bring image recognition features to the public. It relies on pattern matching algorithms and image classification. This is the technology which makes it possible for us to search for photos containing a particular landmark or object. By open sourcing Cloud Vision, Google allows developers who are not in a position to build their technology, to take advantage of Google’s underlying image recognition technology to build applications which see and understand content within its images. Cloud Vision API enables your developers to build image recognition and classification features into your application, by incorporating image analytics capabilities in the form of easy to use REST APIs.
For the non-technical crowd, an Application programming interface (API) is a messenger of sorts. An API takes your requests and tells the system what you want it to do then brings the response back to you. A lot like a waiter in a restaurant who takes your order to the kitchen where it’s made and brings the response or the food back to your table. Cloud Vision API immediately categorizes images(queries), detects particular faces, objects and logos and searches for content in those images and displays them (response). This opens up a lot of avenues, like building metadata on image catalog or identifying offensive content. Companies have always encouraged their users to annotate and tag images accurately, even so, the metadata is poor or missing even. Cloud Vision uses artificial intelligence to add metadata to images automatically when they are uploaded to your system.
Phrases like “game-changer” and “”revolutionary technology” have been thrown around to describe Google’s Cloud Vision API. Skeptics will find it hard not to look past the marketing smokescreens, but some developments come with this technology which makes it worth the effort to examine the fine print.
Exposed as RESTful APIs (meaning providing the user(Cloud vision) with an interface to access and manipulate your data(images)), Cloud Vision accepts an image and categorizes it. Your developers then build rich metadata around the images to perform custom searches. Google gave good thought to the three main parameters involved in developing this technology.
Today’s users are overwhelmed by the sheer number of photos they store on their devices and in the cloud. Solving their challenges will influence their photo taking, sharing and tagging behavior.
By shifting your heavy duty to the Cloud, low-powered devices can take advantage of these services through the APIs. Even the app developers who have a homegrown image recognition technology could benefit from adding a subset of the Google Cloud Vision API functionality to complement theirs. The possibilities are immense. Door keys might die out soon if this technology is coupled with the Internet of things to open doors through facial recognition. It can even be used to describe images to visually impaired people.
Image recognition defines an image in words. It will identify objects, facial expressions, landmarks, logos, etc. Visual search is about finding visually similar images, or maybe to find visually similar objects like those identified in your image. Visual search is more of a challenge to develop. It relies a lot on domain expertise to ensure the results are relevant and not merely technically correct. While the ink is not yet dry as to Cloud Vision’s upcoming features, and future scope, the days of offering general image recognition solutions are over. The market will only allow vendors, focusing on areas that require specialized image recognition services to thrive.
Originally published on Product Insights Blog from CognitiveClouds: Top web app development company
Get smarter at building your thing. Join The Startup’s +750K followers.
110 
Get smarter at building your thing. Subscribe to receive The Startup's top 10 most read stories — delivered straight into your inbox, twice a month. Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
110 claps
110 
Written by
See every interaction with a customer — across all digital channels — & quickly determine how to delight your audience with personalization and recommendations.
Get smarter at building your thing. Follow to join The Startup’s +8 million monthly readers & +750K followers.
Written by
See every interaction with a customer — across all digital channels — & quickly determine how to delight your audience with personalization and recommendations.
Get smarter at building your thing. Follow to join The Startup’s +8 million monthly readers & +750K followers.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://rominirani.com/google-cloud-endpoints-tutorial-part-1-b571ad6c7cd2?source=search_post---------278,"December 2017 : This tutorial has not been updated for a while now and there could be breaking changes. I suggest that you instead reference the official documentation at https://cloud.google.com/endpoints/docs/frameworks/java/about-cloud-endpoints-frameworks
Welcome to a Tutorial Series on Google Cloud Endpoints.
IMPORTANT:
This series covers Google Cloud Endpoints with Eclipse.
If you are looking for using Google Cloud Endpoints with Android Studio, I recommend my series on Gradle, especially the following parts:
Public APIs are all over the place and ProgrammableWeb, the premier directory for public APIs reports that we are well over 10,000 public APIs. This is just the tip of the iceberg. The number of APIs that are private in nature or in other words undocumented and officially unavailable to us is probably 10x or 20x times that (I could be way lower in my estimates!).
Public APIs are now considered to be the most effective mechanism to integrate two applications. Typically the APIs are hosted in a Server side application and exposed to the clients to consume. The clients could be other server side applications, native mobile applications or even browser applications (mobile or desktop).
When we speak about a Public API, few things are common to them:
The above are not absolutely binding but the majority of the Public APIs have that in common.
Additionally, there has been a lot of debate over SOAP vs REST. I cannot get into the details of the two protocols and neither do I want to debate the pros and cons of both the approaches. Suffice to say that the currently predominant style is REST. REST makes use of various HTTP verbs in an effective manner to perform basic operations CRUD Operations or in simpler terms :
One key point about Web Services is that it provides a great layer of separation between the client and the server in terms of the technology used. Client applications are completely agnostic to the programming language/environment that you are using on the Server side. HTTP is the common language that binds the two together and as a result, you could write your Server side API implementation in Java but still have the ability to invoke the same HTTP API via a Java client, iOS client or even a Web client.
The series of tutorials that I wish to cover, starting with this episode is focused around Google Cloud Endpoints, which is a solution from Google to help create a Public API for your App Engine application. Cloud Endpoints also provides facilities to generate client libraries for Android and iOS, thus easing your task to integrate your backend application functionality into your mobile applications on Android and iOS.
The diagram below shows the Cloud Endpoints solution (taken from the official documentation):
The point to note is that you need to have an App Engine application. Apart from that, how you store your data or retrieve your data is completely up to you. You could opt to work with the data store to meet your API functionality or simply talk to external services too to aggregate and present the response.
The tutorial series is more about the mechanics of Cloud Points, the toolset that Google provides rather than best practices around writing your API or API backend functionality.
This tutorial series will cover the following topics to give you a complete feel of the total solution:
I might eventually end point combining one or more parts into a single blog post but we will leave that for another day. I might stumble along the way, but together we can and will cross over to the finish line.
In this part, we shall look at how we can write our API layer manually with the help of Cloud Endpoint APIs. By API Layer, we will write a REST interface that exposes our sample entity object (Famous Quotes) and provides API methods for Adding, Updating, Deleting and Searching/Retrieving Quotes.
Additionally, we will look at how to test your API locally and finally deploy it.
But why write the API manually? One could argue about why write the API manually since Cloud Endpoints does provide a mechanism to generate the API class from existing JDO/JPA annotated Entity classes. Well, it is important to understand this in detail too since it could help a lot in debugging when you face issues. And rest assured, in the next part, we will look at generating the API layer that we wrote here in an automated fashion and see the best in breed code that Google generates for the API class.
Attention-> Cloud Endpoints became GA (General Availability) in Release 1.8.7 of the App Engine SDK. The latest release as of App Engine at the time of writing is 1.8.8.
I suggest that you begin with a full download of the project source code.
Go ahead & download the code from :
Note: This is an Eclipse project that you can import directly. For the sake of reducing the code size, I have removed the App Engine SDK Jars from the WEB-INF\lib folder. So depending on the version of your App Engine SDK in the Eclipse development environment on your machine, please link to the appropriate App Engine SDK that you have for the project to build successfully.
If you are successful with importing the project into your Eclipse setup, the project directory should look something like this:
Our goal is going to be to write a public API that will allow us to manage Famous Quotes. I deliberately chose a Quote to make it easy for all of us to follow. A Quote is a famous/philosophical statement said by an eminent person.
For example:
“Be The Change That You Wish To See In This World.” — M.K.Gandhi
We will keep our Quote object simple. It will contain the following 3 attributes:
Let us keep aside the API business for the moment. We will first model the Quote as a Java class. The class is present in the com.mindstorm.famousquotes.entity package and it also contain additional methods for equals/hashcode, etc. The source code Quote.java is shown below:
[gist]8315482[/gist]
This section is not required as such but I am including it here so that we define both our interface and the implementation details for the different methods to handle Quote. So assume that you were not writing a web service or public API and had to create a Service class or Utility class to manage different operations like add, modify, delete and list for Quotes, then the class could look something like this.
The class is present in com.mindstorm.famousquotes.service package and the file name is QuoteService.java. The code is shown below:
[gist]8315485[/gist]
Let us discuss a few things here since they are important:
Hope this makes things clear on how we would have written a standard Java class to help work with the Quote entity and manage the Quote collection.
Now, let us think for a moment for what we would need to do in order to expose the above service as a REST API.
At a high level, we will need to do the following:
Given the above points, let us see first how to convert our existing QuoteService.java file to a Cloud Endpoints class that will help expose the REST API. You will find that it is all about using the correct annotations, Exception classes from the Cloud Endpoints libraries and some magic that Endpoints does behind the scene based on your method signatures.
Let us look at the converted class with the API Annotations. The Java class is present in com.mindstorm.famousquotes.service package and the file is QuoteServiceAPI.java. The source code is shown below:
[gist]8315491[/gist]
Let us go through the important points:
We are done with our API ! Simple … wasn’t it ? Yes, but how do we test out the API and see the raw HTTP Request / Responses that move back and forth.
Now, comes the fun part and indicates the amount of work the Google Developers have put in to make it easy for you.
Follow these steps:
Make sure you are connected to the Internet. If all is well, you will see an API Explorer as shown below:
How did something get exposed over the _ah/api endpoint. Well, visit your web.xml and you will find an entry created out there for you, which behind the scenes will invoke the API logic for you.
You will notice that the explorer has used our @API (name, version and description) as we provided.
Cool, now lets click on the quoteapi link, this will bring up a list of all our API methods. Notice that the method names were taken as the ones that we provided in the @APIMethod annotation.
Now, lets add a Quote. We will click on quoteapi.add link. This will bring up a form where we can provide the id, author and message for the Quote as shown below:
Clicking on the Execute button will actually make the call and what is interesting to note is that behind the scenes a HTTP POST call is made. Take a look at the HTTP Response too, where the return object type i.e. Quote is marshalled correctly into a JSON object.
Go back to the list and click on the quoteapi.list method. This will invoke a GET as shown below and return the collection as a JSON response.
Now, let us look at quoteapi.getQuote method where we need to provide the id field to retrieve the record. The API explorer brings up the form as shown below. We provide the id as 1 (for our only quote so far) and it fires a GET and retrieves the data.
Pay attention to the entire GET format too to better understand the format. You will notice that the format is _ah/api/<APIName>/<APIVersion>/<Object>/<ID>
Now, look at what happens if we provide an id that does not exist.
Since a Quote record with id = 2 does not exist, it throws a NotFoundException which behind the scenes gets converted into the correct HTTP Response 404.
If we invoke the quoteapi.update method, it will open up a form where we can enter construct the Quote object that we want to modify. We will provide the same id but different values for the message as shown below:
On executing it, we will see that it fired a HTTP PUT command now:
If we retrieve the list of quotes, we will find the updated data:
Finally, let us look at quoteapi.remove method shown below (In my sample data I added a second record too so I am deleting here with id = 2). Notice that it fired a DELETE HTTP command and since we did not return any type, it sent back a 204.
Similarly, if we try to delete an non-existing record i.e. id = 3, we will get the correct 404 responses as shown below:
This brings us to the end of this episode. In this episode, we saw how simple it was to use the Google Cloud Endpoints library to expose a public API. You can go ahead and deploy the same to a live instance too, things should work seamlessly.
In the next part of this tutorial, we are going to take a look at how we could take an existing JDO/JPA annotated Entity object and ask Google Cloud Endpoints to generate the API for us, instead of us having to write it by hand as we saw in this tutorial. But our basics here will help us understand better the code that it generates.
Hope you liked the episode. Till the next one, Happy “API”ing … !
Technical Tutorials, APIs, Cloud, Books and more.
104 
6
104 claps
104 
6
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
Technical Tutorials, APIs, Cloud, Books and more.
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
Technical Tutorials, APIs, Cloud, Books and more.
"
https://medium.com/google-cloud/terraform-assume-role-and-service-account-impersonation-on-google-cloud-ffc553863e72?source=search_post---------279,"There are currently no responses for this story.
Be the first to respond.
About two months ago, someone asked me to help them setup Terraform to automatically provision GCP projects. It was the first time I actually used it and found it capabilities pretty powerful: it’s easy to manage complex resources and maintain a picture of the state change. However, one aspect of its capabilities that struck me was its need to directly have permissions on all GCP resources it provisioned or manage. At first glance this may seem pretty obvious (“how can I create a Pub/Sub topic on project B if I don’t have access to it?”), but the issue I had with it is a bit more subtle: How can I selectively control that access in a way which I can define the conditions on the access and easily manage Terraform’s permissions as a group.
Put it another way, traditionally, if Terraform runs as service_account_A from project_A and it needs to create a GCS bucket in project_B that I own, I'd have to either give it excessive privileges by setting its service account as OWNER or directly assign a role to serivce_account_A on my project as roles/storage.admin. What if I needed to also allow it to create a Pub/Sub topic in the same script or some role with Compute Engine? Now I have to assign additional roles to service_account_A and remember to roll them back after Terraform does its provisioning. As you can see, it can get out of hand if you have to manage it all manually.
Well, what if I can create service account in the project I own and with those roles aggregated already set and then allow Terraform’s service_account_A to impersonate it? That way, I've collapsed and specified the permissions I needed to one identity (the service account I created), and then allowed Terraform access to it under any IAM conditions I wish.
If you’re familiar with AWS, another way to think about what we’re trying to do with this is to have AWS Assume Role where Terraform assumes a specific role capable of access to resources I control.
Given that objective I decided place a pull request (#3211) against the GCP provider to implement the account impersonation flows I previously added to the google-auth-library-python and google-auth-library-java library sets. For more info see Using ImpersonatedCredentials for Google Cloud APIs
The net result is the Google Access Token datasource in Terraform…and as you guessed, this article describes is usage.
This article assumes you’re already familiar with Terraform and use it to manage resource provisioning. The steps below will show how to setup the IAM permission sets and a sample template that will display the original and assumed identity that was bootstrapped.
First let’s setup the IAM permissions that allows impersonation. In the sample below, source-service-account designates the service account Terraform runs as, and impersonated-account is the service account on the target project where Terraform should conditionally assume access to.
This involves assigning the Service Account Token Creator Role:
then run
To test, I configure Terraform to use the source-service-account:
Then, I specify a configuration which uses the source-service-account email to assume the identity of the impersonated-account with an aliased provider. Any Terraform resources which reference that aliased provider will use the impersonated-account.
The sample output for the above would show
The example above doesn’t really do anything to alter actual state as the impersonated credentials but you can easily use other Terraform resources to do so (just remember to specify provider = ""google.impersonated"" in each resource). In real usage, you control the specific resources the impersonated service account has access to. For example, if all Terraform should do is create GCS buckets in your project, assign the appropriate role to the service account that will be impersonated (i.,e. assign roles/storage.admin to impersonated-account@projectB.iam.gserviceaccount.com )
The following list describes the arguments the resource accepts:
Up until now, the capability describe simply shows how to grant indefinite access to impersonate. That’s useful but in conjunction with IAM Conditions, it’s even more compelling.
IAM Conditions allows you to declare the specific criteria under which an IAM permission on a resource is allowed: you can say, “only allow Terraform to impersonate my service account during 1->2am on April 10th”. With these high level policies, you can define a type of “just in time” access controls through workflow systems: define when the Terraform script will run to provision or alter resources you control!
As of 2019-04-20, you can only setup IAM conditions that are time based for the IAMCredentials API. This means the service account impersonation capability with IAM conditions is restricted only by time. Later on, GCP will add on support for the other condition types such as restricting via VPC network or based on IP origin.
For example, future support will involve policies covering:
Note: you can actually apply IAM conditions against downstream resources the impersonated account manges itself (i.e. IAM conditions on the access impersonated-account@projectB.iam.gserviceaccount.com has against resources). If you do this, it may make debugging why Terraform isn't able to manage resources a bit more difficult to troubleshoot: you're now dealing with levels of conditional access.
You can also enable GCP Audit logging that will track the initial impersonation call. For example, if you enable Audit Logging for IAM APis:
an impersonation request will appear in the logs as such:
Note, you can also enable audit logging on the target resources such as GCS or Compute Engine as well but those logs will currently not display what the source service account was that was being impersonated (eg, the resource was mutated on behalf of source-service-account)
While this capability is relatively easy to use and setup (just grant Terraform IAM permission to act-as), the implications of acutely controlling remote resources is pretty compelling: a terraform installation can manage resources in remote projects or organizations without persistent and explicit grants...an administrator of another organization can also control and scope limit the access it grants to Terraform you manage!.
Google Cloud community articles and blogs
60 
60 claps
60 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/google-cloud-data-catalog-and-looker-integration-4ebefdef6a34?source=search_post---------280,"There are currently no responses for this story.
Be the first to respond.
The Google Cloud Data Catalog Team has recently announced its product is now GA and ready to accept custom (aka user-defined) entries! This brand new feature opens up scope for integrations and now users can leverage Data Catalog’s well-known potential to manage metadata from almost any kind of data asset.
To demonstrate how it works, I’ll share design thoughts and sample code to connect Data Catalog to market-leader Business Intelligence/Data Visualization tools, covering Looker metadata integration in this blog post. They come from the experience of participating in the development of fully operational sample connectors, publicly available on GitHub.
Disclaimer: Google and/or Google Cloud do not officially support any tool to connect Data Catalog to non-GCP systems at the time this article has been written down (Apr 2020). What you will find here is merely the result of my experience as a Data Catalog early adopter.
Let’s get started with the minimum technical requirements to make the integration happen:
Since 1 and 2 are already provided by Looker and Google Cloud, our focus has been on the development of a software component addressing the 3rd one.
One of the first steps in this kind of integration is to map business entities from the source to the target system. Five Looker types were chosen to illustrate this: Folder, Look, Dashboard, Dashboard Element (aka Tile), and Query.
We need to adjust such types to fit the Data Catalog model, which is more generic. All Looker assets of these types will turn into entries in the catalog since Entry is a first-class citizen there. They will be distinguishable by the new userSpecifiedType attribute added to the Entry class, as we’ll see next.
But Looker entities have many more attributes than catalog entries can support… In this case, we can use Tags to annotate them and avoid missing meaningful information. Tags are based on Templates, which means we will leverage three Data Catalog main types to achieve the goal. I assume you have at least a basic understanding of them, by the way. In case you don’t, please take a look at this blog post.
There’s a missing piece when it comes to entity associations and parent/child relationships. E.g., a folder contains multiple dashboards and looks; a dashboard contains multiple tiles; a given query can provide information to multiple looks and tiles. Data Catalog currently doesn’t provide native support for structuring such relationships in custom types. And, again, Tags may help us to map them.
Once the overall design decisions have been presented, it’s time to see some practical stuff. Looker has a rich REST API and the following resources are discussed in this article:
Please browse through these pages and you’ll have a clear idea of what I mean by a rich API. You’ll notice there is plenty of information on each asset.
It’s possible to access the API programmatically through pre-built client SDKs available in the most popular languages, including Python and Javascript. Both the sample connector and the code snippets presented here were written in Python.
The API client needs a configuration file, which content is described in the SDK docs. Once you have the proper file, instantiating a client is as simple as:
Sidenote: ApiAuth: API Authentication describes how to authenticate in the API. The document mentions client credentials required for the login, which must be obtained by creating an API3 key on a user account in the Looker Admin console. A shortcut for the Looker Admin console is https://<YOUR-LOOKER-INSTANCE>/admin/users/api3_key/<YOUR-USER-ID>.
Any API call can be wrapped by the client. As an example, let’s say you want to retrieve information for a given folder. The API is https://docs.looker.com/reference/api-and-integration/api-reference/v3.1/folder#get_folder. To perform the call programmatically we do:
Please notice the fields argument: it allows us to specify the fields we want in the response, instead of retrieving all information provided by the API.
We’ve explored two strategies when scraping Looker metadata — let me refer to them as eager and lazy.
Eager scraping allows reading metadata from all assets of a given type by using a single API call. It’s useful for reading common assets’ (usually user-managed) metadata. After reading all metadata the connector iterates through them and uses attribute values to reconstruct relationships among the objects, e.g. append/nest an array of dashboards to their parent folder.
Lazy scraping, on the other hand, might be useful when processing information of special (including system-managed) assets, such as the so-called lookml folder and its children, which are responsible for handling LookML stuff. It needs a bunch of subsequent API calls to retrieve nested assets information.
The next image illustrates the differences. From what we’ve seen so far, mixing them is a good practice in the sense they fill gaps left by each other.
Let me add a few words about coding decisions when implementing eager scraping. There are two main options to retrieve lists of objects: sdk.all_*() and sdk.search_*(). The all* methods return abbreviated objects. The search* methods are more flexible in the sense you can specify the fields you want in the response. In summary:
In summary, we prioritize the eager approach over the lazy. Eager implies fewer API calls and more in-memory processing, hence it’s faster. Lazy was used only to address corner cases not eagerly solvable: LookML Dashboards, for instance, are not included in sdk.search_dashboards() — eager scraping — response, so they are lazily processed.
At this point, we have all the information we need from Looker. It’s time to convert them into Data Catalog entities. Creating Entries seems to be a good starting point since everything else depends on them.
Methods starting with __ represent helper stuff, out of the scope of the snippet.
The above code is intended to be self-explanatory (even for those who don’t know Python…), but there are important points to pay attention:
As I mentioned in the Adapting distinct concepts section, Entry’s attributes cover only a small set of Looker assets’ metadata. But we can leverage Data Catalog Tags to avoid losing valuable information. First, let’s take a look at a Tag Template suggestion:
A Tag from this Template can be attached to each Look-related Entry. By doing this, we enrich the metadata available in Data Catalog as if we ""added"" four extra fields to such entries: Folder name, Query Id, Data Catalog Entry for the Query, and Excel File Url. Folder name, query id, and Excel file are typical technical metadata. Entry for the query can be fulfilled with an HTML link referring to the corresponding Query Entry in Data Catalog’s web console. This link can be generated during the Prepare phase by a quite simple algorithm.
The same approach applies when enriching metadata for other asset types. There’s an interesting example regarding queries: Looker allows us to get the SQL statement for any query through the Run Query method (passing sql as the result_format argument). So the SQL statement turns into an attribute in Query-related Tags.
This is the last stage of the integration process, where the Entries and Tags are ingested into Data Catalog. As mentioned in the previous section, every Entry must belong to an Entry Group, so let’s start by creating it:
Then we can iterate through the entries and ask Data Catalog to persist them and their Tags as well:
Once the ingestion process is finished you can see the new metadata by using the Data Catalog UI. Type system=looker and Search:
The below picture (screenshot manipulated to improve readability) shows an Entry and a Tag for a sample Query:
And the SCRAPE-PREPARE-INGEST process is DONE!
All topics discussed in this article are covered in a sample connector, available on GitHub. Feel free to get it and run according to the instructions. Contributions are welcome, by the way!
It’s licensed under the Apache License Version 2.0, distributed on an “AS IS” BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
The code snippets provided along with the text are based on the connector but simplified for readability reasons. You’ll notice actual code is instrumented with a bit more consistency checks, exception handling, and tackles not only create, but also update and delete operations for entries and tags. Required templates are created at the beginning of the ingestion phase.
Also, it’s split into two major components: datacatalog-connector-commons and datacatalog-connector-looker. There are sample connectors for other non-GCP systems such as Tableau, Hive, MySQL, Oracle, and others. The code they share, chiefly the ingestion phase-related classes, is managed in the datacatalog-connector-commons component. Ideally, by design, only this component should communicate with Data Catalog custom types API. On the other hand, all Looker communication and specific metadata manipulation code reside on datacatalog-connector-looker. The commons component knows nothing about external source systems as shown in the picture below:
Since Looker currently doesn’t have webhooks or similar technology to potentially support incremental synchronization, all assets belonging to a given server are synced on each execution.
Tests have shown that scraping 1,000 assets from Looker, preparing, and ingesting them into Data Catalog, takes about 25 minutes. An Entry and an enriching Tag are generated for each asset. Each Entry + Tag pair needs 4 API calls to be fully synced: 1 GET Entry, 1 CREATE or UPDATE Entry, 1 GET Tag, and finally 1 CREATE or UPDATE Tag. Data Catalog currently does not support batch API operations.
That’s all, folks!
Google Cloud community articles and blogs
43 
Thanks to Marcelo Silvio Miranda Alvarenga Costa. 

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
43 claps
43 
Written by
head of data @ ciandt.com • hobbyist tech writer • dad • birder
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
head of data @ ciandt.com • hobbyist tech writer • dad • birder
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://towardsdatascience.com/deploy-a-python-visualization-panel-app-to-google-cloud-cafe558fe787?source=search_post---------281,"Sign in
There are currently no responses for this story.
Be the first to respond.
Sophia Yang
Dec 19, 2021·5 min read
Related article: Python Visualization Panel App to Google Cloud Run: Google Cloud Run, Google Cloud Build, and Terraform
Is it possible to deploy a Python app or dashboard to the cloud without knowing anything about Docker or Kubernetes and without using a…
"
https://medium.com/@marian-caikovski/connect-to-a-free-virtual-machine-on-google-cloud-with-mobaxterm-3b59690b085c?source=search_post---------282,"Sign in
There are currently no responses for this story.
Be the first to respond.
Marian Čaikovski
Nov 8, 2021·6 min read
Google cloud allows using its virtual machines for free. New customers get $300 credit for trying out Google…
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@qwiklabs/integrate-kafka-with-google-cloud-pub-sub-96084d3bd2b7?source=search_post---------283,"Sign in
There are currently no responses for this story.
Be the first to respond.
Qwiklabs
Aug 14, 2019·6 min read
Did you know that Kafka was developed by LinkedIn and donated to the Apache Software Foundation? Kafka is a powerful tool for building real-time data pipelines and streaming apps. But what if you want to choreograph Dataflow jobs or use topics to trigger Cloud Functions? You can still have the flexibility of having Pub/Sub as your GCP event notifier and exchange messages between Kafka and Pub/Sub.
So how do you exchange messages between Kafka and Pub/Sub? There’s a lab for that! Streaming IoT Kafka to Google Cloud Pub/Sub will explain how to integrate Kafka with Google Cloud.
Here is a glimpse at what all you will be doing in this lab:
The set up of this lab is just like other labs. You will use the Google Cloud Shell in this lab. After opening the Google Console you have to activate your Google Cloud Shell following the instructions given.
Read though the intro section — I know, I know — this will help you in understanding the integration of Kafka with the Google Cloud Platform.
The setup instructions can be a little confusing. Here is a tip: when you search for “Kafka” in the Marketplace you will see multiple similar results. You have to pick the right one in order to go further. You will see something like this:
Confused? Choose the option highlighted in red above.
Once you make your selection, you will see a Launch on Compute Engine button. Click it! Then you will see the details of the Kafka deployment which you are about to launch. Keep all the default settings as they are.
Check the terms and conditions of the GCP Marketplace and then click Deploy.
Deploying the VM instance will take a couple of minutes. While you’re patiently waiting for the Kafka VM to be fully deployed, you will see the status as pending.
Once your VM is successfully deployed, you will see this on the right side of your screen:
Great, you now have a VM running Kafka! Now you will have to stop the Kafka instance which you have created, because you need to Allow full access to all Cloud APIs. In order to do this, click on the VM instance to reveal a control panel, then click the Stop button. This may take a minute. You cannot edit the Cloud Access Scopes till the Kafka instance completely stops.
This section is all about running and understanding commands. You will configure your Kafka VM using SSH. Make sure you are entering your project ID wherever it is instructed. Find your project ID back in the lab manual.
Use the given commands to copy the file generated in the cloud storage bucket to the Kafka VM instance. Then you will move the jar file created in the sub-directory of the Kafka application.
Change the operational directory to config, then use the nano file to add the cps-sink-connector.properties. Make sure you add your project ID according to the given instructions.
Congrats, you have successfully configured the Kafka VM Instance to use the connector!
In this section we will create a Pub/Sub Topic to-kafka from-kafka. ‘To-kafka’ will be communicating with from-kafka. In the next step we will create the subscription for to-kafka and also from-kafka. This step will ensure that communication happens between Kafka and GCP. We can say that Pub/Sub acts as the mediator between Kafka and GCP.
This is a very crucial step in the lab: start the Kafka application! Follow the steps as they are given and you will be good to proceed in the lab. Please note that you have to perform the given steps in the SSH.
First you will create a topic to exchange information through pub/sub. Then you will create another topic to receive messages from pub/sub. Now you have to make certain edits using nano. Here is a screenshot of the changes which you will make:
After this step you will move to the home directory. You have to create the run-connector.sh and add the given content in the lab instructions in the file. Use the command nano run-connector.sh
In this section you will be working on a new SSH of Kafka instance. You will enter the Kafka console and add certain elements in the console. Once you have added the elements in the Kafka console you need to check if these elements are reflected successfully in the cloud shell. For this you will run the command gcloud pubsub subscriptions pull from-kafka — auto-ack — limit=10. Once you run this command it will take some time to sync with the Kafka console. You will get the results after running this command a couple of times. Once you see the output given in the lab instructions you are good to proceed further with testing Pub/sub to Kafka.
This is a very interesting task in which you will be verifying the data exchange between Pub/Sub to Kafka. You will run the commands in the Cloud Shell and see the output in the Kafka VM SSH. Your output will look like this
Now you will be verifying the exact opposite procedure where in you will be running the command in the Kafka VM and seeing the output in the Cloud Shell. It will take some time for the output to be reflected and you may have to run the command gcloud pubsub subscriptions pull from-kafka — auto-ack — limit=10 a couple of times to see the output. Your output will look like this
In this section you will see how the IoT simulator works with Kafka. IRL (in real life) you can also use this for connecting to other devices. First you will make the set up ready in order to create a device registry. For this purpose you will be using git and cloning the repository which will help you to gain access to some specific lab tools. You will also create a cryptographic key which would allow IoT devices to connect to the Cloud Pub/Sub.
Simply follow the steps in the lab manual and you will be able to understand and complete this task. You know you have completed this task successfully when you see the list of temperatures being displayed in the SSH of the iot-device-simulator and the SSH window for Kafka is receiving the temperatures. The final output will look like this:
Iot-device-simulator SSH window
Kaka VM SSH
Hope you have enjoyed this lab. If you want more interesting labs, we have good news: You still have a chance to enroll in our 30 Day Challenge! Earn your Data Engineering Badge by 31st August, and along with the badge you will also get a second month free + an exclusive invitation to play a Data Engineering game, open only to those who complete the challenge. You’ll compete for swag and glory.
Use code 1q-thirty-14 and enroll into the challenge by today as this offer is valid just for 24 hours!!
94 
1
94 
94 
1
"
https://rominirani.com/google-cloud-functions-tutorial-overview-of-computing-options-3c27781e8ced?source=search_post---------284,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
This is part of a Google Cloud Functions Tutorial Series. Check out the series for all the articles
This is the first part of the Cloud Functions Tutorial Series. It provides a high level overview of the Computing Options that are available in the cloud today. You might be already familiar with some of these options and might want to go over it quickly.
Let us take a look at a diagram from one of the presentations given recently:
The interesting thing to note over here is the entire spectrum ranging on IaaS on the left side to FaaS on the right side. All of these computing options are relevant depending on the use case and organization. The thing to note is that on the extreme left, you have more control but you need a team that is capable of managing the infrastructure. As we move towards the right side, we find that the abstractions go to a high level and the infrastructure management is taken care of by the cloud provider.
Virtual Machines on the right side are best suited to moving your existing workloads. That is precisely what the IaaS vendors give you. With the need for portability across platforms, containers have gained mindshare and great success. The CaaS Platforms primarily help you manage the containers at scale. PaaS platforms are much more restrictive in nature since they run within a sandbox, you have to code against the APIs and so on. But they are a highly productive environment from a development perspective if it meets your requirement. The Cloud Vendor takes care of most of the infrastructure management except for a few knobs that you have to tweak.
Finally, we come to FaaS (Functions as a Service), which is the central theme for this tutorial series. Functions currently represent the highest level of abstraction and hope to make it easier for everyone to understand and use them today. You package your code in the form of a ZIP file or point to where you code resides and the tooling takes care of running the functions for you. The functions are primarily executed in response to events that the Cloud Provider Services support. Not all cloud provider services might be supported as triggers to execute your functions automatically. FaaS is also gaining in importance due to the economies in running the same. You are not charged if the functions are not running. The pricing is interesting and is based on the number of invocations and the amount of memory/CPU that your function will utilize. It changes the way you think of your application.
In summary, the move is towards Serverless Architectures. Serverless is usually referred to as “No Ops” where you focus on the functionality and with minimal effort your functionality is deployed and running for you. It utilizes services that the Cloud Providers give but you do not do any of the server management. Other characteristics of Serverless Architectures would include a pricing model based on usage, High Availability, Flexible Scaling and no idle capacity.
Specifically, FaaS which stands for Functions as a Service is available since a while. Currently it is available from the big 3 cloud vendors are AWS Lambda, Azure Functions and Google Cloud Functions. There are popular FaaS distributions available for deployment even locally like OpenFaaS, Nuclio and others.
At a recent Google Cloud Community Conference, we had a great presentation on Serverless Computing and I wish to reproduce a slide from that session here that helps you think of Serverless via 3 categories:
Specifically if we have to map the compute offerings on Google Cloud Platform, the diagram below is another manifestation of the Computing options that are available today, which range from IaaS to FaaS.
We are specifically going to focus on FaaS (Functions as a Service) over here and will zoom in on Google Cloud Functions.
To summarize this post, we can focus now on what we exactly mean by Functions as a Service. You will probably find multiple definitions on it but I have found it more useful to look at features that we expect in a FaaS. Not all FaaS offerings might give you all the features but it will help set the context to make you understand what FaaS has to offer. I have collected these points from an excellent paper on Serverless Architectures (https://martinfowler.com/articles/serverless.html) and have listed them here for you, with some additional comments from my side.
Let’s get started next with an overview of Google Cloud Functions.
Proceed to the next part : What is Google Cloud Functions or go back to the Google Cloud Functions Tutorial Home Page.
Technical Tutorials, APIs, Cloud, Books and more.
116 
116 claps
116 
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
Technical Tutorials, APIs, Cloud, Books and more.
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
Technical Tutorials, APIs, Cloud, Books and more.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/google-cloud-announcements-september-2016-4f43ae53712b?source=search_post---------285,"There are currently no responses for this story.
Be the first to respond.
Last Thursday at the Google Horizon event in San Francisco, we made a ton of announcements across a broad spectrum of topics. The announcements were coming so fast, it was hard to keep up! Now that the dust has settled, I’ve put together a recap to get you caught up.
Diane Greene, SVP, blogged Introducing Google Cloud. The new Google Cloud brand spans every layer of business and includes Google Cloud Platform, machine learning tools and APIs, enterprise Maps API, Android and Chrome devices that access the cloud, and the newly named G Suite (formerly Google Apps for Work).
Brian Stevens, VP, blogged Google Cloud Platform sets a course for new horizons — announcing eight new Google Cloud Regions: Mumbai, Singapore, Sydney, Northern Virginia, Sao Paulo, London, Finland, and Frankfurt (with more to be announced next year). Brian also covered Kubernetes’ latest feature, Cluster Federation that supports straightforward deployment across multiple clusters and multiple clouds. He also mentioned that Google Container Engine usage is doubling every 90 days! Last but not least, Brian announced a new role: Customer Reliability Engineering (CRE) that directly integrates with our customers operation’s team.
Fausto Ibarra, Product Management Director, blogged BigQuery: introducing powerful new enterprise data warehousing features announcing a bunch of new enterprise features in Google BigQuery including identity access management (IAM), monitoring through Google Stackdriver, flat rate pricing, improved compatibility with ecosystem data tools, new ODBC drivers, support for standard SQL, and the ability to update, delete, and insert rows and columns in BigQuery datasets (DML). He also describes how Coca-Cola European Partners, The New York Times, and Viant are relying on BigQuery.
Riku Inoue, Product Manager, blogged Google Cloud Machine Learning: now open to all with new professional services and education programs, announcing that Google Cloud Machine Learning is now publicly available in beta and includes a new feature called HyperTune that automatically improves predictive accuracy. The post also discusses how Airbus Defense and Space tested the use of Google Cloud Machine Learning to automate analyzing satellite images and solved a problem that has existed for decades. Riku also announced a new dedicated machine learning practice within our Professional Services team and new programs and certifications to help business learn how to use machine learning to solve real business problems.
Luke Stone, Director of Customer Reliability Engineering, blogged Bringing Pokemon GO to life on Google Cloud, providing a behind-the-scenes look into the launch of Pokemon GO. This post provides some amazing details into the explosive growth of Pokemon GO including how the app grew to 50x the original usage estimate requiring some heroic measures, and went on to become the largest Kubernetes deployment on Google Container Engine ever. This post is like reality TV for developers!
Caliah Manson, Head of Global Strategic Alliances, blogged Google and Accenture: building for the enterprise, announcing a new alliance between Google and Accenture to develop solutions that bring together cloud, mobility, and collaboration for enterprises that will help clients embrace the digital transformations technology can enable.
Prabhakar Raghavan, Vice President Apps, blogged G Suite: intelligent tools designed for teams and announced several new features with G Suite (formerly Google Apps for Work). These new features bring amazing machine intelligence to calendar, docs, spreadsheets, and presentations. Also announced was Google Drive for Teams that provides team-level content ownership and sharing, and Meetings for teams that supports up to 50 participants and allows joining of meetings with a simple link.
The momentum is exciting and more announcements are coming — we’re firing on all cylinders.
Stay tuned!
Google Cloud community articles and blogs
74 
74 claps
74 
Written by
Google Developer Relations — Living in San Francisco https://gregsramblings.com | https://cloud.google.com | http://instagram.com/gregsimages
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Google Developer Relations — Living in San Francisco https://gregsramblings.com | https://cloud.google.com | http://instagram.com/gregsimages
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/capturing-and-integrating-service-data-with-google-cloud-functions-and-neo4j-588855f1695e?source=search_post---------286,"There are currently no responses for this story.
Be the first to respond.
All code mentioned in this article can be found on Github.
Many services provide a mechanism for webhooks, or the ability to call some custom URL whenever a certain action takes place. With so many different systems and triggering needs, it’s a nice and easy way to make events on one person’s proprietary platform subject to publish and subscribe type behaviors somewhere else. It’s also very simple; a single HTTP POST, with a JSON body.
Real examples include:
More than just triggering a custom behavior, many use cases require capture first. Recently when developing some cloud-deployable images, I wanted an easy and flexible way to capture data from the cloud service, which offered me the ability to call a webhook whenever someone deployed my package. I knew I wanted to get this into a neo4j graph, because I’d eventually need to integrate this data with another graph I had.
GCP’s Cloud Functions are a way of creating stand-alone functions running on top of cloud infrastructure. They’re “serverless” because Google manages all of the provisioning and runtime parts. The developer just writes a set of functions, deploys them to the cloud, and they’re run when needed. Ideally too these functions are stateless and idempotent, which helps with reuse.
Well when you see this tagline, “respond to events in the cloud”….hey that sounds like webhooks!
For me, the advantage of this for the webhooks was also that I was expecting fairly low volume (say, hundreds per day maximum) — and I didn’t want to pay to host a VM 24/7, or bother to take care of that VM.
The code associated with this article can be found in the neo4j-serverless-functions repo on github.
Capturing data turns out to be fairly easy. Google’s cloud functions expose an API that in javascript looks like (or may be) the Express.JS API. The simplest possible function would look like this:
To capture data from a webhook, all we need to do is look at the data coming in on the request, things like its headers, it’s POST body and so forth, and then transform that data into a Cypher statement that creates data in neo4j. You can see that code here.
Suppose we had a simple POST, like this:
That then would turn into the Cypher equivalent of:
This is a simple way of capturing audit-able data from an external service, any service, any JSON schema. Of course, the property definitions in neo4j are going to be far from ideal, but this can be manipulated with Cypher after the fact.
For APIs which will allow some customization of the URL, you can use the same approach to knit together the graph. The code package includes a separate “edge” function which can be given a property name and value, and which will serve to draw new relationships if that’s appropriate for your use. Suppose on a social network someone friends someone else, you could invoke the webhook:
Which would have the same effect as cypher like:
An interesting part here is that one of the ways serverless functions are sold involves the idea of statelessness. Ideally, your cloud function is a pure function, in the sense that it produces a value and has no side effects. As usual, reality intrudes on perfection…
Under the covers, of course google has to deploy this function to an actual container or server. This can cause confusion since the reality is that “serverless ain’t serverless”. So in reality, your code at least at times is hot deployed somewhere, and it’s not spinning up from cold every time, that would be very inefficient, doubly so if you’re using a heavyweight runtime (e.g. java).
In google’s tips and tricks for cloud functions, they point out that it’s a best practice to use variables to reuse objects in future invocations. Let’s take a look at what that means in the neo4j case.
In the code module that sets up the neo4j driver, there’s a persistentDriver variable which holds a driver instance between function invocations.
Neo4j driver objects are “heavyweight” according to the documentation, and shouldn’t be created in large numbers willy-nilly, or you’ll see performance impacts. By having a persistent driver, database connections can be reused between function invocations. By exposing an accessor function, this driver gets “lazy created” when it’s needed, it’s not just lying around.
What if we did it the other way? If we didn’t have a persistent driver here, the function would still work; but we’d likely spam the database with connections. Probably OK for dozens of requests, but that function would never scale to large numbers of calls.
This article covers some simple examples of how to do a basic integration. But beyond two simple functions, you can (and people surely do) layer on quite a few more.
When you understand the model, you’ll quickly see how an entire backend can be written with only cloud functions. What, no VMs? No docker containers? That’d be a “backend-less backend”. Very zen.
Unfortunately, it’s not quite so simple. As I described with statelesness above, Google cloud functions do have both state and servers, of necessity. As result, the serverless abstraction is a bit leaky, there are some bits of a functions lifespan that are good to know as you get into them. I also found this article to be a good concise overview on some key architectural issues you’ll face in the brave new world. As with all things in engineering, it’s not what’s right or wrong, it’s what’s a good tradeoff for your use case.
In this simple example, I’m glossing over a number of things you’d want to do in a more heavy-duty use case. I’m just after capturing data from a few webhooks here and there, nothing heavy duty. For example:
Google Cloud community articles and blogs
50 
50 claps
50 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Architect at Neo4j
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://towardsdatascience.com/how-to-train-and-deploy-a-vaex-model-pipeline-on-google-cloud-platform-d5023ef46322?source=search_post---------287,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jovan Veljanoski
Mar 23, 2021·17 min read
Training a machine learning (ML) model is often a rather lengthy and computationally intensive task, especially when using larger quantities of data.
Regardless of whether you are using your favorite deep learning framework, trusted gradient boosting machine or a custom ensemble, the model training phase can easily consume most if not all available resources on your laptop or local server, effectively “freezing” your machine and preventing you to do other tasks. Or you may not even have enough resources at hand to train a sophisticated model on all of the data you have so painstakingly gathered, or use all of the features you have so thoughtfully engineered.
On top of that, curating ML models is a continuous process: one will likely need to re-train the model relatively often to account for new data, concept drift, changes in the domain, or simply to improve the model by adjusting the input features, architecture or hyper-parameters.
Therefore, it can be rather convenient, if not necessary, to outsource the heavy lifting stage of ML model creation to a service managed by a recognized cloud provider. With this approach computational resources will not be a problem. Nowadays, one can “rent” computing instances having few hundreds of vCPUs and several Terabytes of RAM attached, or provision custom cluster configurations. In addition, one can submit multiple training jobs which will run independently and not compete with each other for resources. All this means that you can spend more of your time on creating the best model possible, and virtually no time managing, maintaining and configuring your computational resources. An important note is that such services are becoming cheaper are more accessible with time.
So how does Vaex fit in all of this? There are several substantial benefits to using Vaex as the core technology for building ML solutions even in cloud environments. To start with, you can host your data on a Google Cloud Storage (GCS) or an Amazon Web Services (AWS) S3 bucket, and stream it lazily to your compute instance, on a “need-to-have” basis. This means that only the specific columns that your model needs will be downloaded, and not all of the files in their entirety. One can even choose to download only a fraction of the data, which is especially useful for testing and continuous integration.
All Vaex transformations are done by fully parallelized, efficient out-of-core algorithms. That means that you always take full advantage of the compute instance you rent out, without any additional set up. The no-memory-copy policy makes it easier to choose the type of machine you require while minimizing cost without performance trade-off.
There are a couple of more benefits that will be highlighted throughout the article. So, without further ado, let us see how one can use Vaex to build a ML solution, and how to then use GCP to make it come to life.
This article assumes some basic knowledge of GCP and how to interact with it through the Google Cloud Console, and via the gcloud command-line tool. If you would like to follow along with this tutorial, you will need an authenticated Google account, a GCP project, and a GCS bucket already set up. There are many useful guides in case you are not sure how to do this. If in doubt, the official GCP documentation is always a good place to start.
All the materials from this article are fully available here, along with a variety of other Vaex examples.
This example uses the public Heterogeneity Activity Recognition (HAR) dataset. It contains several sets of measurements captured from a group of volunteers performing one of six activities: walking, going up & down stairs, sitting, standing and biking. The measurements are captured via popular smart-phone and smart-watch devices, and comprise the triaxial acceleration and angular velocity sampled from the on-board accelerometer and gyroscope respectively. The dataset contains also the “Creation_Time” and “Arrival_Time” columns, which are the time-stamps attached to each measurement sample by the OS and mobile application respectively. The goal is to detect the particular activity performed by the wearer using just a single measurement sample. The activity itself is specified in the “gt” column which stands for “ground truth”.
The following example uses the accelerometer data obtained via the smart-phone devices. It comprises just over 13 million samples. For brevity, we will not present any exploratory analysis of the data, but will jump straight into building a production ready solution.
Let us start by creating a Python script that will fetch the data, engineer relevant features, and train and validate a model. Since we are using Vaex, fetching the data is trivial. Provided the data is in the HDF5 file format and hosted on GCS (or Amazon’s S3), Vaex will lazily stream the portions that are needed for analysis. Since we already know which columns of the data are going to be needed, they can be pre-fetched right away:
The next step is to randomly partition the data into 3 sets: training, validation and testing. The validation set will be used as a quality control during the training phase, while the test set will be the final, independent performance indicator on the already trained model.
At this point we can start to create some useful features. Let us begin by making a couple of coordinate transformations. We will convert the triaxial acceleration measurements from Cartesian to spherical coordinates, as well as to their “natural” coordinate system by the means of a PCA transformation:
Even though some of the above transformations are not overly complex, we can still choose to accelerate them by using just-in-time compilation via numba. Note that we are also using the new API available in vaex-ml version 0.11, instead of the more traditional scikit-learn “fit & transform” approach.
To capture some non-linearity in the data, we can create some feature interactions between the PCA components:
Now, we will get a bit more creative. First, let us calculate the mean and standard deviation for each Principal component per activity class. Then, we will calculate the difference between the value of each Principal component and the mean of each group, scaled by the standard deviation of that group:
Notice how we are combining the usage of both Vaex and Pandas in creating these features. While the df_summary DataFrame will not be stored, its values are “remembered” as part of the expressions defined in the for loop that follows the groupby aggregation. The above code-block is an example of how one can very quickly and clearly create new features, without the otherwise necessary exercise of creating a custom Transformer class.
Another interesting approach to feature engineering is to apply a clustering algorithm on subsets of already defined features, and use the resulting cluster labels as additional features. The vaex-ml package directly implements the KMeans clustering algorithm, so it is guaranteed to very fast and memory efficient. Using the KMeans algorithm, we create 3 sets of cluster labels: one by clustering the PCA components, two by clustering PCA interaction components:
In Vaex any model is treated as a transformer, so it’s outputs are readily available to be used as any other features in the downstream computational graph.
Finally, we can also make use of the time-stamp features, and calculate the difference between the “Arrival_Time” and “Creation_Time” columns, to which we apply standard scaling:
Once we are done defining all the features, we can gather them into a single list for convenience:
The last part of the data preparation is to encode the target column “gt” into a numeric format. Following the encoding, we will also define a inverse mapping dictionary, which we will later use to translate the predicted classes to their true labels.
At this point, we are finally ready to start training the model. You may have noticed that we have not bothered to explicitly create a pipeline to allow for all the data transformations to be propagated to the validation and test sets. This is because a Vaex DataFrame implicitly keeps record of all the transformations and modifications done to the data. Filters, categorical encodings, scaling, even the outputs of an ML model are considered to be data transformations and are part of the state of a DataFrame. Thus, in order to get the validation set up to speed so we can use it as a reference point during the model training, we only need to get the state of df_train and apply it to df_val:
Now we are ready to instantiate and train the model, which we have chosen to be a LightGBM classifier:
When used in conjunction with Vaex, ML models are also transformers. That means that the predictions can be added to a DataFrame just as if you were applying another transformation. This is incredibly useful when building ensembles, but also for performing model diagnostics. In our case, the outputs of the LightGBM model are arrays of probabilities. To make the outputs more meaningful to an end user of the model, we are going to find the most likely class, and to it apply the inverse transformation, so we can get the name of the most likely activity — yet another in the series of transformations!
Once the model is trained, we can get a sense of its performance by computing a couple of metrics on both the validation and test sets, the latter of which was completely unused in the process so far. Again, all we need to do to get the predictions, is to get the state from df_train, which now includes the model predictions, and apply it to the df_val and df_test DataFrames:
Notice usage of the log function in the above and in previous code blocks, which is an instance of the standard Python logging system. When this code is run on the AI Platform, the logs will be automatically captured and made available in the centralized Cloud Logging section of GCP. Neat!
And we are done. The final step is to save the final state file in Google Cloud Storage (GCS) bucket, so it can be deployed later. Vaex can save the state file directly to a GCS or a S3 bucket:
Now that our training script is ready, it needs to be made into a Python package so it can be installed and executed on the AI Platform. Let us call our training module “har_model”. Its constituent files should be organized in the following tree structure:
Note that we are also including an empty “__init__.py” so that Python treats the “har_model” directory as a package. The “setup.py” script installs the package along with the required dependencies:
The nice thing about the AI Platform is that we can run our package locally before submitting a job to the GCP. This is quite useful for debugging and testing purposes. The following shell command will execute the training script locally, in the same manner as in the cloud:
Since the core technology in the current solution is Vaex, one can quite easily limit it to use a small fraction of the data to make the tests run faster. Once we are sure that the training module works as expected, we can submit the training job to GCP via the following command:
Given the large number of parameters, it can be rather convenient to execute the above command as a part of a shell script. That way, it can be version controlled, or part of a CI/CD pipeline for example.
Once the above command is executed, the AI Platform training job will start and you can monitor its progress in the Logging section of GCP. With the machine type we choose in the above example (n1-highcpu-32, 32vCPUs, 28GB RAM), the entire training job takes ~20 minutes. Once the job is finished, we can examine the logs to see how model has done on the test set:
That is it - with a minimal set up we successfully trained a fully custom Vaex pipeline on GCP! The pipeline itself, which contains the full feature engineering & data processing steps, the classification algorithm and the post processing manipulations, has the form of a Vaex state file and is saved to the designated GCS bucket, ready to be deployed.
The AI Platform is a rather convenient way to deploy ML models. It ensures high availability of the prediction service, and makes it easy to deploy and query multiple model versions, which is useful for doing A/B testing for example.
Deploying a Vaex pipeline is quite simple — all a prediction server needs to do, is to convert the incoming batches or data samples to a Vaex DataFrame, and apply the state file to it.
In order to deploy a custom Vaex pipeline, we must instruct the AI Platform on how to handle the requests that are specific to our problem. We can do this by writing a small class which implements the Predictor interface:
The above VaexPredictor class has two key methods: the from_path method simply reads the state file from a GCS bucket, while the predict method converts the data to a Vaex DataFrame format, applies the state file to it, and returns the predictions. Notice that the predict method conveniently intercepts data that has been passed either as a Python list or dict type.
The next step is to package the VaexPredictor class as a .tar.gz source distribution Python package. The package needs to include all the dependencies that are needed for obtaining the predictions. Creating such a package needs a “setup.py” file:
The package is created by running the following shell command:
Finally, we need to move the prediction package to GCS, so the AI Platform can pick it up and deploy it:
It may come in handy to bundle the above two commands in a bash script for convenience, especially if one needs to iterate a few times while creating the prediction package.
For reference, the directory tree of deployment part of this example project should look something like this:
We are now ready to deploy the prediction package. It can be rather convenient to define some environmental variables in the shell first:
The model deployment is done with the following two commands. First we need to create a “model” resource on the AI Platform like so:
Then we create a “version” resource of the model, which points to the model artefact, i.e. the state file, and the predictor class:
It may take a minute or two before the above command executes. That’s it! Our Vaex model is now deployed and is ready to respond to incoming prediction requests!
We are now ready to query our model. The batches of data sent to the AI Platform need to be in a JSON format. If the input has the form of lists, each row of the file should be a list containing the features of a single sample. Care should be taken that the order of features are consistent with what is expected by the Predictor class. And example of such a file is shown below:
Prediction requests are then sent with the following command:
The input data can also be formatted as JSON objects. One can be more flexible here — one line can be a single or multiple samples:
The following is a short screen cast of querying our “har_model” with the file above:
It is that simple!
The lives of ML models are not infinite. When the time comes to undeploy a model, one needs to first delete the version resource, and then delete the model resource:
Finally, it may be important to note that even though we trained the model on GCP in this example, this is not at all a deployment requirement. All that is required is for the state file to reside in a GCS bucket, so that the prediction module can pick it up. One can train the model and create the state file locally, or using any other service that is available out there.
I hope this article demonstrated that Vaex is an excellent tool for building ML solutions. It’s expression system and automatic pipelines are especially useful for this task, while its efficient, out-of-core algorithms ensure speed and keeps computational costs low.
Using Vaex in combination with GCP brings considerable value. Vaex is able to stream data directly from GCS, and only those portions that are absolutely necessary to the model. Training ML models on Google Clouds’ AI Platform is also rather convenient, especially for the more demanding, longer running models. Since the entire transformation pipeline of a Vaex model is contained within a single state file, deploying it with the AI Platform is straightforward.
Happy data sciencing!
In mid-November 2020 Google launched the next iteration of the AI Platform, dubbed AI Platform Unified. As the name suggest, this version unifies all ML related services that GCP offers: autoML, ready to use APIs as well as options to train and deploy custom models can all be found in the same place.
One significant improvement that the new AI Platform brings is the option to train and deploy models using custom Docker containers. This brings additional flexibility compared to the “classical” AI Platform in which only specific environments are available with a limited options to install or modify their contents.
Lets see how we can use the Unified AI Platform to train and deploy the Vaex solution we built earlier in this article, now using custom Docker containers.
Training the model is rather straightforward: all we need to do is create a Docker image that when started will execute the training script we prepared earlier. We begin by creating a Dockerfile:
In the above Dockerfile, “env.yml” all the dependencies we need that can be install via either conda, mamba, or pip. The “setup.py” and “har_model” make up the model training package we defined earlier. We then install the required dependencies and the model training package, and finally set up an entrypoint so that the training process starts when the container is run. Pro-tip: if you want to build very small Docker containers check out this guide by Uwe Korn.
Now, we can build the Docker image locally and push it to Google’s Container Registry, but it is much more convenient to simply use Cloud Build and build the image right in GCP:
Then we can launch the container and start the training job simply by executing:
The training progress can be monitored via Cloud Logging, which captures any logs that come out of the custom Docker container. Some 20 minutes later the job should finish, and we can inspect it via the Google Cloud Console:
Now let us deploy the model we just trained using a custom Docker container on the Unified AI Platform. When started, the container should run a web application that will respond to requests with the predictions. The web application should implement at least two method: one that the AI Platform will use to make “health checks” i.e. make sure the web application is running as expected, and another method which will accept incoming prediction requests and respond with the answers. For more information on the requirements of the container and all available options for customization you can check out the official documentation.
We will not go into detail in how to build such a web application, as there are plenty of resources on the web focusing on this. For reference, you can see the web application we have prepared for this example here.
After building and testing the web application, we need to create a Docker container that will run it when started. This is easily done following the same steps for creating the model training container.
Once the Docker image is available in Container Registry, we need to make a model resource out of it via the following gcloud command:
The next step is to create a model endpoint used to access the model:
We can now deploy the model resource to the endpoint like this:
This step may take a few minutes to complete. Note that you can deploy several model resources to a single endpoint, as well as have a single model deployed to multiple endpoints.
The model now is finally deployed and ready to accept requests. The requests should be in JSON format and have the following structure:
Here you see how such a file would look like for this particular example. The Google Cloud Console will also give you an example of how to query the model endpoint. It will look something like this:
That’s it! When your model outlives its lifetime, do not forget to undeploy it and delete the endpoint and model resource, as to avoid unwanted costs. That can be done like this:
As a final note, one can think of training and deployment as two separate, fully independent processes. This means that one can use the “classical” AI Platform to train, and the Unified AI Platform to deploy the model, and vice-versa. Of course, one can always create the model “in house” or using any resources available out there, and just use GCP for serving.
Just another data scientist | PhD Astrophysics | co-founder of vaex.io | https://www.linkedin.com/in/jovanvel/
253 
1
253 
253 
1
Your home for data science. A Medium publication sharing concepts, ideas and codes.
"
https://medium.com/google-cloud/this-week-in-google-cloud-platform-how-google-does-ml-by-coursera-iot-core-goes-ga-multi-cloud-13bfc8a5a42a?source=search_post---------288,"There are currently no responses for this story.
Be the first to respond.
A “How Google does ML” course is now available on-demand via Coursera. This is the first course in a two 5-course specializations. Already lots of open-source example ML applications are available here.
[GA] The thing is . . . Cloud IoT Core is now generally available (with partners and customers) (Google blog). You can now publish data streams from the IoT Core protocol bridge to multiple Cloud Pub/Sub topics.
Enhanced Compute Engine instance templates allow you to create instances from existing instance templates and create instance templates based on existing VM instances. See “Managing your Compute Engine instances just got easier” (Google blog) for details. Added bonus: you can now protect your virtual machines from accidental deletion.
From the “all things Cloud Spanner” department :
From the “multi-cloud is real” department :
From the “how-to {Kubernetes|TensorFlow|Istio}” department :
From the “In case you’ve missed it (ICYMI)” department :
This week‘s picture is taken from the Cloudflare multi-cloud Kubernetes deployment blog post :
That’s it for this week!-Alexis
Google Cloud community articles and blogs
56 
56 claps
56 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Google Cloud Developer Relations
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/javarevisited/7-free-online-courses-to-crack-google-cloud-associate-cloud-engineer-ace-certification-exam-in-2cf0b297aed?source=search_post---------289,"There are currently no responses for this story.
Be the first to respond.
Hello guys, if you are aiming for the Google Cloud platform and preparing for GCP Cloud Engineer Associate certification, the first certification to get yourself going with the Google Cloud platform and looking for free online courses to start your preparation then you have come to the right place.
In the past, I have shared the best courses to learn Google Cloud as well as certification courses to pass cloud engineer, data engineer, and cloud architect certifications, and today, I am going to share free GCP Cloud engineer courses for beginners and experienced developers.
These courses are created by experts and people who have already gone through the ordeal of this prestigious google cloud certification and passed it.
They are also picked from sites like Udemy, Pluralsight, and Coursera, three of the best online learning platform for programmers and IT professionals. You can also join these courses to better prepare for this certification and pass it on the very first attempt.
Before I share these free courses with you, I must congratulate you on making a great decision by learning cloud computing and choosing Google Cloud Platform, and when it comes to learning GCP, what else can be better than a Google Certified Cloud engineer?
Google is one of the top three market leaders when it comes to cloud computing, and if you have a certification of excellence from them, then you are going to have the upper hand among your competitor candidates.
In this tutorial, I have shortlisted 7 free online courses that will help you prepare for your GCP Associate Cloud Engineer Exam within a limited span of time. Apart from this, all these courses are created by the market leaders; therefore, not even a single moment will be wasted.
By the way, if you are serious about passing this prestigious Google cloud certification on the very first attempt then I highly recommend you join the Google Associate Cloud Engineer: Get Certified 2021 course on Udemy.
udemy.com
It’s not free but it’s the best course to pass the Cloud engineer certification and created by experts like Dan Sullivan, the guy who wrote the Official Certification Guide for Google. You can also get in just $10 on Udemy sales which happens every now and then.
Here is the list of the best free online courses to prepare for Google Cloud Professional Associate Cloud Engineer certification. This certification tests your ability to work with the Google cloud console and command line. You should be able to provision resources, create servers, deploy apps, and monitor them.
This exam is similar to AWS Cloud Practitioner and Azure Fundamentals exam and should be your first step on Google cloud certification. After passing this exam, you can prepare for other advanced Google Cloud certifications like professional cloud architect, professional data engineer, and professional DevOps engineer.
Anyway, without wasting any more of your time, here are the free courses for Google Cloud Associate Cloud Engineer certification:
This is one of the best free Udemy course to learn Cloud Computing. If you are just starting with your preparation and don’t want to leave any stone unturned, then this course is going to be a base for you.
It will work as a foundation for you. Xavier Corbett created it, and it’s a 1-hour long video course. So far over 2 lakh, students have enrolled in this course, and it is rated as 4.4 stars out of 5.
In this course, you’ll learn about the followings:
Apart from this, the instructor will give you brief information about various cloud platforms. If you are looking for a foundation course, then this one’s for you.
Here is the link to join this free cloud computing course — Introduction to Cloud Computing
Once you are done with the foundation course, then you are all set to learn about the platform that you are preparing for. This course is created by Djanaji Musale and Google Cloud Platform Gurus!. So far over 40 thousand students have enrolled in this course, and it is among the top-rated courses on Udemy about GCP.
In this course, you’ll learn about the followings:
If you are looking for a course that can help you get enough information about your platform then this course is for you.
Here is the link to join this free GCP course — GCP — Google Cloud Platform Concepts
If you are out on the internet looking for a single course that can prepare you in every aspect, then this course is for you. It’s a long video course available on Pluralsight.com.
Though this is a paid platform, since it offers a trial period, you can complete this course in that period without paying a single amount to the platform.
The Google Cloud team itself created this course in partnership with Pluralsight; thus, quality won’t be a concern anymore. In this course, you will learn about the following concepts of Cloud computing and GCP:
Apart from these, the instructor will also teach you how to maintain the security of the cloud and its applications. This course is specifically designed for beginners, and that’s why illustrations and graphics are majorly used in the videos.
Here is the link to join this online course — Google Cloud Certified Associate Cloud Engineer
By the way, you would need a Pluralsight membership to join this course which costs around $29 per month or $299 per year (14% discount).
I highly recommend this subscription to all programmers as it provides instant access to more than 7000+ online courses to learn any tech skill. Alternatively, you can also use their 10-day-free-pass to watch this course for FREE.
pluralsight.pxf.io
Another course created by the Google Cloud team, but this one is more of a limited and short course than the one available on Pluralsight, and this is available on Coursera which is an entirely free platform.
In this course, the instructors have live illustrations of various implementations, and you’ll be assigned with some project that needs to be finished to move forward in the course.
Besides this, if you’ll complete the course on time, then you’ll be rewarded with a certificate of competition. So far in this course, over 32 thousand students have enrolled, and based on the rating of over 1500 students, it has 4.7 stars out of 5.
This course is specifically designed for your preparation for the GCP Associate Cloud Engineer exam. If you move as per the instructor, then it will take four weeks, but if you are running short on time, then you can attend all the videos in just 8 hours.
This course is divided into three modules:
If you are looking for a course that can quickly prepare you for the GCP Associate Cloud Engineer Exam, then this one’s for you. The next three courses are interlinked with each other, and it is not recommended to skip any of the next three courses. Pursue them in serial order.
Here is the link to join this free Coursera course — Preparing for the Google Cloud Associate Cloud Engineer Exam
By the way, If you are planning to join multiple Coursera courses or specializations then consider taking a Coursera Plus subscription which provides you unlimited access to their most popular courses, specialization, professional certificate, and guided projects. It cost around $399/year but it's completely worth your money as you get unlimited certificates.
coursera.com
So far, it is assumed that you have completed the courses mentioned above, now it’s time to prepare for the particular topics that you are going to appear for.
This course is also available on udemy, and Google Cloud Platform Gurus created it, so far over 72 thousand students have enrolled in this course. It has a rating of 4.3 out of 5 based on the reviews of 1400 students.
In this course, you will learn about tactics to prepare for your GCP Associate Cloud Engineer certification course. Though this course is focused on a different certification exam, all the topics covered in this course are directly related to your GCP Associate Cloud Engineer Exam.
Instructors have smartly used explanatory illustrations and graphics. Even if you are a beginner, understanding things won’t be a problem for you. In this 16-hour long video course, every concept of GCP is explained in detail.
If time permits you, then it is recommended to pursue all the above mentioned three courses, you’ll be prepared enough to pass the GCP Associate Cloud Exam with flying colors.
Here is the link to join this Google Cloud course — Ultimate Google Certified Professional Cloud Developer
This is another free Udemy course for Google Cloud certification including Associate Cloud Engineer. This course shares some useful strategies to prepare for Google Cloud certification and it is prepared by none other than Dan Sullivan, author of some of the best Google cloud courses and the man who designed Google’s official certification guide.
There is no doubt that Google Cloud certification exams are challenging. Even if you deep knowledge of Google Cloud services, you could fail a certification exam if you are unfamiliar with the structure of the tests.
This course will help you understand how Google Cloud certification exams are structured, the rules for taking these exams, and the kinds of questions you can expect.
Perhaps most importantly this course shows how to analyze questions and precisely identify what is being asked and how to reason about each possible answer so you can choose the best option.
The course begins with a review of Google Cloud certification topics followed by a detailed discussion about the structure of certification exams. It then looks at the limitations of certification exams and how someone can fail an exam even if they are knowledgeable about the topic.
Here is the link to join this free course — How to Pass Google Cloud Certification Exams
This one is a relatively newer course to learn essential concepts of Google Cloud Platform which is necessary to pass the Google Cloud Associate Cloud Engineer (ACE) Exam.
In this free udemy course, you will learn about what is Cloud Computing, what are different Cloud computing models, Important GCP services, and some hands-on demo for creating a virtual machine, creating a bucket, and how to use Bigquery for Machine learning.
This course will also help you are planning to build or change your career to GCP. This course will also help to plan one of highly valued Google Certifications like “Google Associate Cloud Engineer”, “Professional Cloud Architect” etc.
Here is the link to join this free course — Google Cloud Fundamentals 101
That’s all about the best free course to prepare for Google Cloud Associate Cloud Engineer certification in 2021. In this guide, you have learned about the five best free courses to pass the GCP associate cloud engineer exam.
All these courses picked after detailed research, and all these courses are top sellers on their respective platforms. If you have ample time, then it is recommended to enroll in each of these courses and then decide which one you should finish and which instructor is more understandable.
For better preparation, I also recommend solving practice questions like the ones given in Google Cloud Associate Cloud Engineer Practice Exams by none other than Dan Sullivan, the author of Google’s official certification guide. It’s not free but in $10 completely worth it.
udemy.com
Other Cloud Computing and IT Certification Courses and Articles you may like
Thanks for reading this article so far. If you find these free Google Cloud Platform and Cloud Engineer certification courses useful, then, please share them with your friends and colleagues. If you have any questions or feedback, then please drop a note.  P.S. — If you are serious about learning the Google Cloud platform and passing Cloud engineer certification then I also suggest you join the Google Certified Associate Cloud Engineer Certification by A Cloud Guru and Ryan Kroonenburg. It’s not free but it’s the best course to pass the Cloud engineer certification and created by experts.
udemy.com
Medium’s largest Java publication, followed by 14630+ programmers. Follow to join our community.
253 
253 claps
253 
A humble place to learn Java and Programming better.
Written by
I am Java programmer, blogger, working on Java, J2EE, UNIX, FIX Protocol. I share Java tips on http://javarevisited.blogspot.com and http://java67.com
A humble place to learn Java and Programming better.
"
https://betterprogramming.pub/how-to-terraform-with-jenkins-and-slack-on-googles-cloud-platform-56c5e8b3aeeb?source=search_post---------290,"Sign in
There are currently no responses for this story.
Be the first to respond.
Gaurav Agarwal
Apr 7, 2020·10 min read
Terraform is the most popular Infrastructure as Code (IAC) tool provided by Hashicorp. With the advent of cloud and self-serviced infrastructure, a groundbreaking concept called Infrastructure as Code (IaC) emerged.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/using-google-cloud-sql-e3e4c123a89f?source=search_post---------291,"There are currently no responses for this story.
Be the first to respond.
Google recently announced their next generation of managed MySQL offerings on Cloud SQL, so we wanted to take it for a spin and create a cloud-based SQL database that we could then utilize as the back-end for mobile apps, or even for advanced data analytics from our desktop.
According to Google, the two principal goals of the second generation of Cloud SQL were better performance and scalability per dollar. It seems that they succeeded in these goals: the second generation Cloud SQL is more than seven times faster than the first. And it scales to 10TB of data, 15,000 IOPS, and 104GB of RAM per instance — well beyond the first generation. So it looks like the ideal, scalable cloud-based database back-end for mobile apps.
If you follow the instructions on the the Google blog post, you will first have to create a trial account for Google Cloud. Then, once you are logged into the Google Developer Console, click on the top-left menu button and select SQL under the storage section. This will enable you to create a new second generation Cloud SQL instance:
It is highly recommended to change your root password and to only allow access to your cloud instance from authorized networks that will host your developer machines, or your back-end servers for the mobile app that you are building. For example, if a future blog post we will explore building a mobile app on top of this database, and the MobileTogether back-end server that will provide all the server functionality and workflow for our mobile app will need to be able to access this Cloud SQL instance.
In order to use the Cloud SQL instance from your desktop developer machine, you will need to download the MySQL tools for your machine, which will enable you to use the mysql command-line tool. Also, to use the Cloud SQL database from other applications on your computer, you will need to download the respective MySQL Connectors. In this blog post we’ll be using Altova DatabaseSpy to access the SQL instance, upload some data, and then perform some analysis, so you’ll want to download the MySQL Connector/ODBC for Windows. Essentially, DatabaseSpy is similar to Toad, but allows you to connect to multiple different database instances on different database servers at the same time. So you can have one connection open to Cloud SQL, one connection to a local SQL Server, and a third connection to an Oracle Database — all from within one workspace. Pretty nifty.
One important note: you’ll want to make sure that you download the right Connector/ODBC driver. If you’re using the 64-bit version of DatabaseSpy, you’ll also need the 64-bit version of the MySQL Connector/ODBC driver. Or, better yet, you can download both the 32-bit and 64-bit versions of Connector/ODBC and install them both — then you can use Cloud SQL from both 32-bit and 64-bit software on your machine.
To populate our Cloud SQL instance with some useful data for experimentation and further analysis, we downloaded the excellent Lahman’s Baseball Database, which is conveniently available as a zipped MySQL dump. After downloading this onto our computer, we had a complete SQL database table descriptions plus all the data — all in one SQL file, so it can be easily uploaded into our Cloud SQL instance.
The first step is to create a new database in your Cloud SQL instance. You can do that either via the command-line mysql utility, via the Google Developer Console, or via cURL — see link at the beginning of this paragraph for more details. We will simply call this database “lahman”.
Now we can connect to this database from DatabaseSpy by choosing the “Connect to a database” command and following these steps through the connection wizard. First we select MySQL as the database technology:
Then we confirm the use of the MySQL Connector/ODBC driver that we have downloaded previously:
And last, but not least, we specify the IP address, username, and password of our Cloud SQL instance, then hit the “Test” button to verify the connection. If that works, we can now create a data source by giving it a name, and also specify the “lahman” database that we previously created:
Now we’re ready to upload all the data into tables in our new database. To do that, we simply open the SQL dump of the Lahman Baseball Database that we previously downloaded and unpacked, which looks like this in DatabaseSpy:
As you can see, the SQL dump of that database contains all the necessary commands to recreate all the tables on our new Cloud SQL instance. So all we have to do now is hit the “Execute” button and sit back. Depending on your Internet connection speed, this may take a little while, as you’re now creating 24 tables with highly detailed baseball statistics from 1871 until 2014 — some of which have over 160,000 rows. In my case it took about 8 minutes to complete.
Now you can hit “Refresh” in your Online Database Browser in DatabaseSpy and then explore all the tables and the data contained therein, and the structure of the Lahman database. And you can use this database to calculate interesting historical data, do statistical analysis, and apply other tools often used in Sabermetrics to better understand the quality of players.
As an example, we will look at David Ortiz‘s percentage of homeruns (HR) hit for every at-bat (AB) over his entire career and will plot that as a function of his age. You will find a player’s at-bat performance in the “Batting” table and his name, birth date, etc., in the “Master” table. Joining the two tables in SQL and selecting the data for David Ortiz is easy, and we can then calculate his Career HR Trajectory simply as HR/AB and plot that against his age in a given year:
Once you hit “Execute”, this query is sent to the Cloud SQL instance and the resulting data is returned almost instantaneously as a table. You can then hit the “Graph” button in the result table view to graph the data and plot the Career HR Trajectory over his age:
As you can see, it is very easy to connect to and utilize the new Google Cloud SQL instances from Altova DatabaseSpy or, for that matter, from any of the other Altova developer tools, including XMLSpy, MapForce, MobileTogether, StyleVision, and UModel. In addition, you can easily utilize Cloud SQL instances from MapForce server for data integration and conversion projects.
In a future blog post we will connect to this Cloud SQL instance from a mobile app and explore how easy it is to build a convenient mobile app front-end on top of the Lahman baseball database.
Originally published at blog.altova.com on January 6, 2016.
Google Cloud community articles and blogs
36 
36 claps
36 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Entrepreneur. Investor. Co-founder and CEO of Altova. Co-creator of XMLSpy. Licensed captain. Interests: Drones, IoT, XBRL, XML, XSLT, XPath, XQuery, Ham Radio.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/train-test-models-in-google-cloud-ludwig-with-no-code-9d22c7d7ceea?source=search_post---------292,"There are currently no responses for this story.
Be the first to respond.
You can train and test Deep Learning models for Machine Learning, AI and Data Science without writing code and with minimum setup. The tools I used are Google Colab, which is a virtual cloud based…
"
https://m.chmarny.com/using-google-cloud-spanner-to-measure-social-media-influence-over-stock-market-b4980f0f9ca1?source=search_post---------293,"few longer thoughts, because every once in a while 140 characters is just not enough

I learn best by doing. And recently, most of the projects I’ve been building are either REST or gRPC-base services deployed as container images into Cloud Run on GCP. That means that I increasingly find myself recreating a lot of the same infra and app deployment flows.


Why not Medium My main reason for migrating off Medium was the paywall Medium introduced while back. I actually understand why they did it. The unlimited access price: $5/month ($50/year) is too high, but still, I get it.
For me though, the objective was to allow readers to easily discover and read my posts.


Increasing large amount of technical news I read come from the posts shared on Hacker News or on Twitter. While both of these services have search options, neither of these seem to be advanced enough to narrow my searches at the desired level or to setup automatic delivery.


All complexity needs to be abstracted, right? This reductionist statements misses nuance around the inherent cost/benefit tradeoffs, especially when you consider these over time.
Don’t get me wrong, there often are good reasons for additional layers to make things simpler (grow adoption, lowering toil, removing friction, etc.


I recently joined the Office of CTO in Azure at Microsoft and wanted to ramp up on one of the open source projects the team has built there called Dapr. Dapr describes itself as:
A portable, event-driven runtime that makes it easy for developers to build resilient, microservice stateless and stateful applications that run on the cloud and edge and embraces the diversity of languages and developer frameworks.


We are entering a period where custom, highly-optimized, vertical solutions are becoming viable option again. This is a good news for ISVs with proven domain expertise and skilled development resources.
Why do I think so? We now have:
Plethora of feature-rich developer frameworks, message queues, scalable data stores, and even lower-level components in the OSS community with great documentation and a large number of use-case validation Growing number of custom solution companies (more than just ISVs) with existing deep vertical/domain expertise who are also increasingly now investing in hiring and training strong development teams Virtually every Cloud provider offering either a raw Kubernetes service or managed container execution platform which (regardless how you feel about these technologies) creates ubiquitous surface area that can be addressed with a single solution Yes, there still are many ways in which these custom development efforts can fail.


When dealing with file permissions in a non-root image or building apps that include static content (like css or templates), I sometime get an error resulting from the final image content mismatch with my expectations.
Most of the time the errors are pretty obvious, simple fix and rebuild will do.


While the idea of a serverless platform and long running workloads does seem somewhat “unnatural” at first, smart people are already working on that (looking at you @Knative community). In the meantime, a simple approach is sometimes all you may need.


A co-worker recently told me about flic.io buttons. These button caught my attention because they can include triggers for single, double, or hold click and can be easily wired up to all kinds of actions.
I instantly thought of of a few really interesting applications.


Next week, April 9–11, Google will be hosting this year’s Cloud Next Conference in San Francisco. The conference is already sold out, but there will be a livestream from keynotes and video available shortly after the sessions.
This year, we have a lot of content to share, and I have the privilege of presenting in four sessions — and hope to do at least six live demos.

"
https://medium.com/cognitiveclouds/aws-vs-microsoft-azure-vs-google-cloud-cbdb03e0281e?source=search_post---------294,"There are currently no responses for this story.
Be the first to respond.
While collectively, these three cloud providers dominate the space, their approach to cloud computing is dictated strongly by their background. Amazon has immense know-how when it comes to collating and aggregating massive amounts of data, Google’s heritage stems from an analytical background, and Microsoft’s strength comes from computing. This comparison will underline their strengths and weaknesses. As we proceed, tie these activities to your business objectives to find the right fit. Also, it’s important to note that your best fit may not turn out to be a single cloud provider.
Your reasoning for picking one service over another will differ from another user. However, there are particular aspects of competing clouds that offer benefits in certain circumstances. That can always be compared. So, let’s advocate for and against each now.
AWS continues to lead the way, in very broad terms, regarding maturity and offering the widest range of functionality. However, the gap is certainly closing. Its expansive list of services and tools, along with its enterprise-friendly features make it an attractive proposition for big organizations. While its massive, continuously growing infrastructure offers economies of scale that allow aggressive price cuts.
Now it appears, Microsoft has begun to bridge that gap between the two. With its plans to strengthen ties with its on-premise software and ongoing investment in building out the Azure cloud platform, it will continue to bridge that gap. Microsoft Azure will continue to be a strong proposition for organizations already heavily invested heavily in Microsoft regarding technology and developer skills, of which there are undoubtedly many. With Google offering a slightly different proposition, it has made inroads with certain users. Even so, to become a viable enterprise choice, it has a lot of work to do. It might carve a niche for itself in advanced use cases based on machine learning and big data, but whether Google is willing to relinquish the key IaaS market to its largest competitors is another subject.
Originally published on Product Insights Blog from CognitiveClouds: Top Web App Development Company
Tips, advice and insights from our digital product…
43 
1
43 claps
43 
1
Written by
See every interaction with a customer — across all digital channels — & quickly determine how to delight your audience with personalization and recommendations.
Tips, advice and insights from our digital product strategy, design and development experts.
Written by
See every interaction with a customer — across all digital channels — & quickly determine how to delight your audience with personalization and recommendations.
Tips, advice and insights from our digital product strategy, design and development experts.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/run-your-app-locally-as-if-you-were-on-google-cloud-2722e33e5656?source=search_post---------295,"There are currently no responses for this story.
Be the first to respond.
Any applications have issues; Applications running on Google Cloud also. And, to debug them, you have to run them locally, add breakpoints, and deep dive in the code. Some issues come from security context or configuration. To reproduce them, you have to test with the exact same context locally to reproduce the exact same behavior as if you were on Google Cloud.
How to run securely and locally an app with the exact same security context as on Google Cloud?
That means 2 things:
Before exploring the different solution, let’s have a look at the Google Cloud environment security mechanism.
The Google Cloud client libraries that you use automatically detect the runtime environment to obtain the credentials. The mechanism is named ADC: Application Default Credential.
On Google Cloud, every service has a metadata server which exposes endpoints, and some to generate short lived tokens and without exposing any secrets.
By default, the default service account of the service is used to generate those tokens. However, many services offer to customise the service account to use. It’s a good practice to use a custom service account on that service and to grant only the required permissions on it for a dedicated workload. If you have several workloads that use the same service, you can have 1 custom service account per workload and narrow the granted permissions.It’s the least privilege principle.
The default service accounts have too wide permissions like Project Editor and it’s dangerous to use such powerful permissions by default everywhere!
If you use a service account key file on Google Cloud environment, I recommend you upgrade your code and application to remove it. The service account key files are only files and it’s dangerous and difficult, to manage them safely. I already wrote an article on that topic.
The most secure way to keep secrets secret, is not to have secrets!
Service account use is a bad practice, even if you watch videos, follow tutorials or read documentation EVEN COMING FROM GOOGLE CLOUD!
To reproduce the ADC locally, and running the application in the exact same security condition as on Google Cloud, there are 3 existing and not satisfying solutions.
Let’s discover which they are, and why they aren’t good.
It’s the most common way to achieve it in the Google Cloud security environment. Lot of articles and videos use, and abuse, of it. Mainly because it’s easy, and legacy. It’s also the ugliest!The principle is :
The ADC mechanism inside the Google Cloud client libraries detects automatically the environment variable and use it as credential source. No need to change the code.
The main problem of that solution is the service account key file itself. It’s a key file, a secret. Not many in the development teams are aware of that, and the impact that can have a leakage.You download a secret on your workstation. Is it well secured? Will you delete the file after your tests? Will you do that with your production environment? Are you sure enough?
In addition, because you directly use the service account identity, it’s impossible to differentiate who has performed the request: The application on your workstation or on Google Cloud?In the audit logs, there is no difference, it’s the same identity. That’s a serious problem in the case of internal security issues, it’s impossible to trace the source of the issue/attack/data thief.
That solution isn’t really acceptable in terms of security.
To prevent the service account key file download, you can use your own credentials, and grant the same permissions as the Google Cloud service account on your own user account.
The ADC mechanism inside the Google Cloud client libraries detects your user account credentials stored in the “well-known” locations (usual local directories), and loads them as source credentials. No need to change the code.
Ok, you have the same permission as the service account on the project, but YOU aren’t the service account. Your principal isn’t the same.
In addition, if the service account email has been granted on another resource, in another project (on a BigQuery dataset hosted in another project), your user account won’t, and you won’t be able to reproduce the exact same workload.
For a small app limited to 1 project, the solution can work. But we can’t rely on it for all apps.
To have the exact same permission in all the project and to have the same credential as the Google Cloud service account on your local environment,
Here comes the service account impersonation feature.
The impersonation is the ability to use your user credential and to generate OAuth tokens on behalf of the service account.
It’s a powerful feature that allows reuse of service account permissions without downloading a secret key.In addition, your principal is logged in the impersonation delegate chain to know the originator of the request. You have a clear view of WHO is using the service account.
The problem comes from the integration with the Google Cloud client libraries. The feature is supported but you need to add a piece of code to activate the impersonation.
When you want to test locally your app because you have security issues, and the first thing that you do is to change the security logic, it’s obviously absurd!
So, if we could combine all the pros without the cons, we will have the perfect solution: impersonate a service account but without changing the code and letting the Google Cloud client library manage that for us.
That feature exists!
It’s a brand new feature and few know it. The idea is to configure the account impersonation outside of the app code, with the gcloud CLI. For that, you can use that command
Pretty simple, no?! Then run your app, and it will run with the same service account credentials as your Google Cloud environment by impersonation! The Google Cloud client libraries are “awesome”!Of course you must be a Service Account User on the service account to be able to impersonate it.
Awesome? quite. Simple? Yes. Does it work? Not sure!!
Indeed, I initially tested it with a Golang app and it didn’t work. It was too new and the feature wasn’t implemented in the Google Cloud Golang client libraries. I implemented it and it was merged in October.
So now, it works for Golang (perform a go get golang.org/x/oauth2 to download the latest version on your local environment). It also works in Java.From my latest tests, it wasn’t the case in Python (feel free to contribute if you need that feature!)
The reproducibility on your local environment is really critical to quickly understand and resolve the problems, but security is certainly more important when you work with sensitive applications in production.
That table summarizes the 4 solutions
Solutions are multiple, some are legacy, others are quicker. However, keep security in mind at any stage of the development, and even after the development!
The Service Account impersonation with gcloud CLI hasn’t any red flag but is not yet perfectly supported by the client libraries of all the languages. Anyway, try it first!!
Google Cloud security products are gaining more and more features that help you to avoid bad practices and to keep your environment clean and safe!
Google Cloud community articles and blogs
100 
1
100 claps
100 
1
Written by
GDE Google Cloud Platform, scrum master, speaker, writer and polyglot developer, Google Cloud platform 3x certified, serverless addict and Go fan.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
GDE Google Cloud Platform, scrum master, speaker, writer and polyglot developer, Google Cloud platform 3x certified, serverless addict and Go fan.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/i-am-mahasak/%E0%B8%A1%E0%B8%B2%E0%B9%80%E0%B8%82%E0%B8%B5%E0%B8%A2%E0%B8%99-dotnet-core-%E0%B9%82%E0%B8%94%E0%B8%A2%E0%B9%84%E0%B8%A1%E0%B9%88%E0%B8%95%E0%B9%89%E0%B8%AD%E0%B8%87%E0%B8%A5%E0%B8%87-dotnet-sdk-%E0%B9%83%E0%B8%99%E0%B9%80%E0%B8%84%E0%B8%A3%E0%B8%B7%E0%B9%88%E0%B8%AD%E0%B8%87%E0%B8%81%E0%B8%B1%E0%B8%99-p-212aec02d9f9?source=search_post---------296,"There are currently no responses for this story.
Be the first to respond.
ห่างหายไปนานจากการเขียน blog เพราะว่าต้องไป join Engineering Bootcampm ที่ Menlo Park แล้วยังต้องเหนื่อยกับการ relocation อีกต่างหาก ตอนนี้ก็ยังย้ายของไม่เสร็จ แต่ชีวิตเริ่มมีเวลามากขึ้นบ้างแล้ว ก็เลยมาเขียน blog สักหน่อยดีกว่า
เรื่องของเรื่องคือ วันนี้ไปงาน meetup มา เป็น workshop เล็กๆ เกี่ยวกับ CI/CD และต้องใช้ dotnet core ทีนี้ผมก็เตรียม dotnet SDK ไปจากบ้านอย่างเรียบร้อยเลย ปัญหาคือ พอมาถึง workshop ก็พบว่า version ไม่ตรงกัน TADA !!! เลยทำให้ทำ workshop ต่อไปไม่ได้ อยากทำ workshop ก็อยาก แต่ไม่อยากลง dotnet SDK ใหม่ให้เป็นมลทินกับ macbook pro เครื่องใหม่ ทำไงดี !!!!
หยุดคิดไป 0.29 nanosec ก็ตัดสินใจว่า เอาวะ docker ละกัน ก็เลยลงมือเลย เนื่องจาก workshop ต้องใช้ dotnet sdk 2.0.201 เพราะว่า template ที่ใช้ในเวอร์ชันล่าสุดไม่ตรงกับที่ workshop ต้องการ ก็เริ่มจากการ pull image มาก่อน
หลังจากนั้นก็เริ่มทำการสร้าง project ได้เลย ด้วยคำสั่งนี้ครับ
เนื่องจากเราต้องการให้ dotnet CLI สร้าง project เพื่อเอามาใช้งานใน host machine เพื่อที่จะสามารถเขียน code และ run project ที่จะใช้ใน workshop ต่อได้
ทีนี้ เราก็มาทำ Dockerfile เพื่อให้เราสามารถรัน project นี้ด้วย docker ได้กัน
Protips #1: อย่าลืมเพิ่ม .dockerignore เพราะว่าอาจทำให้มีปัญหาจากการ build ได้ เช่น node_module ที่ไม่ up-to-date
ในการ build docker image นั้นก็สามารถ build ได้ตามปกติเลยนะครับ
และ เราก็สามารถรัน asp.net core website ได้เลยด้วยคำสั่งต่อไปนี้ !!
Protips #2: ถ้ามีปัญหาในการรัน docker image ที่ build ลองเพิ่ม property นี้ลงใน <PropertyGroup>
Option เสริม มาเพิ่มการ build แบบคูลๆด้วย Google Cloud Builds กัน (ในส่วนของการ enable cloud builds API ไปอ่านกันเองที่นี่นะครับ => https://cloud.google.com/cloud-build/docs/quickstart-docker
ในที่นี้เรามี Dockerfile อยู่แล้ว ก็มาลองใช้ gcloud builds กันดีกว่า ในการสั่ง build ด้วย gcloud builds นั้นก็ไม่ต่างจากคำสั่งที่เรา build เองมากนัก
และ เมื่อ build เสร็จก็รันได้เลย แต่อย่าลืมตั้งค่าให้ docker รู้จัก repo ของ gcloud ด้วยคำสั่ง
ทีนี้ก็รันได้เลย !!!
#HappyCoding
Keep It Simple Stupid
40 
Public domain.

40 claps
40 
Written by
Keep It Simple Stupid
Keep It Simple Stupid
Written by
Keep It Simple Stupid
Keep It Simple Stupid
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/google-cloud-sdk-dockerfile-861a0399bbbb?source=search_post---------297,"There are currently no responses for this story.
Be the first to respond.
The Cloud SDK is the command-line interface to the Google Cloud Platform. It is a flexible utility which uses GCPs own Cloud APIs to perform many different tasks such as deploying code on AppEngine, to creating Compute Engine VMs to checking IAM permissions and so on. It is the command line interface to pretty much every Google Cloud Platform API and service.
This article describes some more ways customers can use it by demonstrating several use cases involving containerizing the Cloud SDK. The following shows Dockerfiles targeted towards various operating systems and some sample usecases (Its nothing new, just a sample set to document the baseline dockerfiles.
EDIT 4/24: Updated blog post with changes from the source github Repo.
EDIT 9/4: Update content to point to official docker.io/google/cloud-sdk image
Dockerfile
Usecases
To use any of the images above, simply download the image directly (except alpine, you have to create that as shown later)
If you want to use a prebuilt image, see
github.com
specifically
or via tagged version of the SDK:
NOTE, with latest image set, all components are installed by default. If you would rather have a minimal image to use, try the slim image
The alpine based images do not yet exist in the official google cloud sdk but I am listing the docker generation file here incase you want to create one
The SHA256 checksum is listed on the SDK documentation page.
Note: you can pass in the ARG value of the sdk version and checksum as overrides:
The following describes building and running the Cloud SDK as an Alpine package. In other words, its a package you can use Alpine’s installer to setup instead of sourcing from a base image and then installing Cloud SDK as docker commands.
For more information see:
At the time of writing (3/16), the package is not in the official alpine community repository. Although building a version of the package is pretty straightforward, you can find it also on the private repository shown below.
Note: the package below is pinned to SDK 147.0.0. You can either update gcloud post-install or regenerate a new package.
APKBUILD
Running the SDK using an untrusted private repository (https://storage.googleapis.com/mineral-minutia-820/alpine/v3.5/community)
The following describes the abridged steps to creating the image locally in a Dockerfile for alpine
You’re an App Engine developer and you want to keep your workstation in as much of a consistent state as possible. That means you would rather not install the Cloud SDK or run dev_appserver.py directly from your laptop. What you would rather do is spin up any components for your local development without having Cloud SDK installed on the laptop.
What you’d like to do is run Cloud SDK and dev_appserver.py in a container. To do that, you need to run dev_appserver.py from the Cloud SDK docker image but map your deployable sources to that container.
For example with python, I’m mapping my current source directory to the container (under /apps) and instructing it to run the dev_appserver.py
You can also configure your container image if you’d like to use Maven You can also do this with maven but you will need to install the dependencies into the extended image itself as shown in the following Dockerfile that sets up your execution environment for maven:
Build your containerized runtime environment:
Then just launch with your app:
(Note: you’ll need to specify the host/port for the dev_appserver in the pom.xml section for the appengine-maven-plugin)
Don’t want to install, update and maintain a local SDK install? Install the SDK requires python installed locally. If you containerize the SDK, all you need to run is docker. If you pull the published google/cloud-sdk:latest image, you are guaranteed to have the latest release of the SDK. Note that once you pull an image down, it will remain cached in your local repository until you delete the local image and pull again. Alternatively, if you want to remain on a given version, you can always specify the docker image to pull. Note: we support only the last two releases of the SDK from ‘latest’.
For example, first initialize the volume to use by authorizing it with your credentials:
Then reuse that volume but now use any gcloud command:
You can also use this technique to initialize a volume with a service account. This is useful if you want to have several service accounts handy as volumes where each service account is scoped with a different IAM Role.
To setup a service account credential in a volume, first download the JSON certificate file and map a volume to the Cloud SDK container:
In the following example, my service account certificate file is stored on my local system at $HOME/certs/serviceAccountFile.json
Warning: the volume gcloud-config now has your credentials/JSON key file embedded in it; carefully control access to it!
Then run a new container but specify the volume. You’ll see that the configured credentials already exist
Now run some gcloud command on behalf of that service account.
You can continue to do this with other restricted service accounts in volumes. This will allow you to easily control which service accounts and its capabilities you use by having it already defined in a redistributable the container image (vs. using gcloud’s — configuration= parameter in each command).
Running emulators in a container provides an easy, predictable configuration for an emulator. You can always reuse a given configuration without needing to initialize the SDK with credentials. For example, the following starts up the pubsub emulator from a baseline SDK:
You can even extend and link some code you run in an container to connect with this emulator. In this mode, you run your emulator in one container, acquire the container’s internal address, then separately run your application in another container but link to the emulator via docker networking. There are several ways to do this securely too with docker custom networks.
First run the emulator:
We need to pass in the internal IP address for the emulator into your application container. The following command returns the internal IP address which we will use later.
Now run your application container with credentials but link it back to your emulator by passing in an environment variable for the emulators internal IP address
Note: you need to pass in GOOGLE_APPLICATION_CREDENTIALS because the PubSub client library tries to acquire an access token before contacting the emulator.
Suppose you want to run different automated tasks with service account with restricted access. First create a service account and initialize a volume as shown in the “Reuse service account credential” use case. Once you’ve done that, you can invoke any script that uses gcloud-cli. We described some of the scripting you can do with gcloud in a previous blog post here.
As a concrete example, suppose you have a script which lists out the service accounts and their keys:
$ svc.sh
If you want to run that script using credentials initialized and attached to a volume in a container, simply run the gcloud-sdk container, reference the volume with credentials and map your script to the running container. In this example, I initialized gcloud-config with a given service account and then mapped svc.sh script from my local workstation to the gcloud-sdk image. The entrypoint for the image is the script itself
For more info on scripting gcloud see previous blog post on this topic.
Hopefully, these basic use cases have given you some ideas on how to containerize the Cloud SDK and extend the image and customize in ways to suit your need.
Google Cloud community articles and blogs
44 
1
44 claps
44 
1
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://rominirani.com/google-cloud-functions-tutorial-pricing-9cc6dc47f7c0?source=search_post---------298,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
This is part of a Google Cloud Functions Tutorial Series. Check out the series for all the articles.
This post touches upon pricing in Google Cloud Functions. There is excellent documentation available with pricing examples and I strongly suggest to take a good look at that.
To reiterate, since Google Cloud Functions are part of the Serverless Computing option, you are not charged for resources if you are not using it. Given that the unit of code that we are executing over here is a function , the questions that should pop up in your mind could be one or more of the following:
You will be charged for the following resources that you use:
A solid diagram is available as part of the documentation that highlights how your cost is calculated. It is reproduced below:
When you provision your function to run, you select from one of several CPU + Memory Configuration that is available to you. The table is given below:
In addition to the above, you also have a Free Tier that will kick in and will apply as deductions from your usage, as part of your bill. The details for Cloud Functions in the Always Free Tier is given below:
This is obviously very difficult to comprehend, but let’s work it out via an example. This example is given in the official documentation along with another example and I suggest that you go through the same too.
Let us consider the following scenario:
A background function with 256MB of memory and a 400MHz CPU, invoked 10 million times per month and running for 300ms each time using only Google APIs (no billable egress).
If we analyze the items that I have marked in bold, we will come to teh following conclusion:
We can now work out the following total cost:
Notice that in the above table:
The Cloud Function pricing is also available in the Cloud Platform Cost Calculator available over here.
Pricing is often best understood by keeping a track of your usage, looking at a few billing cycles and then determining if you need to tweak your functions around execution time, etc.
This is part of a Google Cloud Tutorial Series. Check out the series for all the articles.
Technical Tutorials, APIs, Cloud, Books and more.
184 
2
184 claps
184 
2
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
Technical Tutorials, APIs, Cloud, Books and more.
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
Technical Tutorials, APIs, Cloud, Books and more.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@sathishvj/frequently-asked-follow-up-questions-on-google-cloud-gcp-certifications-438e1addb91d?source=search_post---------299,"Sign in
There are currently no responses for this story.
Be the first to respond.
sathish vj
Jan 8, 2020·12 min read
p.s. You’re probably here because I sent you here from a chat on one of the social media. I do this because, like you, I have quite a few people reaching out to me and I’m too pressed for time to be able to type out replies individually to each person. Or when I’m traveling, it might be a long time before I’m able to respond. So this suits us both.
Following my posts on each of the Google Cloud Certifications, people reach out to me on LinkedIn and Twitter with further questions. Most of the questions are very similar, and I want to save some time for both you and me with this listing of common questions and answers I’ve exchanged with others. Some of these responses are verbatim, while others have been rephrased.
How do I know when I am ready? How do I know I’ve covered all the required material?
That really is the toughest part. The material is very vast — both deep and wide. The more we learn, we realize that there is way more to learn. I doubt if there is any end to it. And I don’t know of any way easy way to get around it but preparing better — not just learning more, which you have to do, but also getting a sense for when to stop.
My experience has been that the GCP exam questions s are better approached in a logical, deductive process rather than a brute force method. Common sense and reasoning beats by-hearting.
Here are some tips:
I’ve done the courses and labs, studied the docs, but I’m still scared.
That is natural. All of us go through that phase. I’ve done almost every certification on GCP, but when I go for a new exam, I am nervous and doubtful whether I’ll pass. At some point though, you have to take a leap of faith.
Maybe you will fail once, but use that as a learning experience, prepare again, study harder, and go back again and do it.
There is no shame in not getting through the first time. Many, many people who have reached out to me have failed but then come back better prepared and succeeded. You can too.
I attempted the Professional Architect but failed. What do I do?
Have you done the Associate Cloud Engineer exam yet? If not, do that first and come back to the Professional exam again. The Associate Cloud Engineer sets a good horizontal base to all the other exams. Attempting and passing the Associate exam first will give you an easier first step to the other exams. You’ll get a good hands-on with the type of questions all the GCP exams have. Also, the confidence boost from passing the Associate is a very useful moral support that helps you with the other certifications.
But I don’t have a free coupon for the Associate.
My recommendation remains that you do the Associate first. If that means that you have to pay up from your own pocket for the exam, then do that.
Ok, so I passed the ACE. Which should I do next? Architect, Data Engineer, Network, Security?
Of course, this depends on which of these aligns best with your job. But if you had an equal option for any of these (after completing the Associate Engineer), my recommendation is that you do the Professional Architect next. This exam requires you to make decisions/choices from among a wide array of solutions provided by GCP. It sets a broad, solid base and understanding of GCP as a whole. It does not deep-dive into all technologies, but that’s for other certifications.
After the Associate Engineer and then the Professional Architect, I don’t believe there is a specific order which is advantageous. Depending on your job, preference, and comfort level, you could do any of the remaining in any order. Networking and Security are kind of deep-dives beyond the Professional Architect. There is some overlap in the study material between the two, but I have no preferred sequence for attempting them.
Data Engineering, Cloud Developer, Collaboration Engineer — these are all separate lines. Once you finish the Professional Architect, pick them up in any order that you like.
I have been preparing since many months. In practice tests, I’m always around 50–60%. There is some or other things in the corner of docs, I’ve missed or not understood correctly. Is it just bad preparation or am I doing something wrong?
Try the official practice test also and see how you fare. If you’re not doing well, I suggest you retake the test until you get used to the format, the way you reason through it, etc.
Also, at some point you have to take a leap of faith. We could be studying for ever and not feel confident. I’m at that stage for the next exams I’m studying for.
I have been preparing for GCP […] exam since last month but I don’t feel confident. So how can I boost my confidence?
Do the Google practice exam, try to work through all the case studies, study best practices, do more QwikLabs. All of that will help increase your confidence.
It is common not to be confident about the exam because there aren’t any similar tests to try out with. Your option is to try and cover as much material as possible and then take a leap of faith. Try the Google practice exam. Also learn all the best practices. Study the decision making flowcharts on which solutions to choose. Do a lot of qwiklabs and get hands on practice. After doing all of that, you just have to attempt it. If you don’t make it in that attempt, then note down the kind of questions you got, the subject areas that the questions were from, the wording and type of questions, etc. Practice again harder based on that and attempt again.
Can you give me any additional tips?
If I do have any, I will include it in the blog posts for the benefit of everyone. So, no, I do not have notes specifically for any person beyond what is in the notes. I’m also collecting notes from other exam takers for all GCP certifications here: https://github.com/sathishvj/awesome-gcp-certifications
Do you have any question dumps?
No. First and foremost, it is against the rules of the exam. Moreover, I highly recommend against them as you are going to be worse off with that approach than actually picking up GCP skills with courses and hands-on practice.
Is there any course that comprehensively covers the material and guarantees passing the exam?
I haven’t researched/reviewed all the courses, but my guess is that there isn’t and there never will be. The material is too vast for courses to be exhaustive — they’ll end up recreating all the documentation. The courses are great to give you the fundamentals, but you will need to pick up way more knowledge from the documentation and by doing hands-on exercises.
Which courses do you recommend for <your> certification test?
I’ve mostly taken Coursera courses created by Google, studied the docs, and done additional QwikLabs quests and labs. I have not reviewed most or pretty much any of the other courses to give you a recommendation. Since I’m already certified, neither do I have the time or motivation to review any of them. So I am unable to recommend any particular online course. But these are the course publishers, in no particular order, that I usually hear others mention: A Cloud Guru, Coursera, Linux Academy, Pluralsight.
Can you mentor me?
Though I would like to help each of you, I do not have the time for that. Therefore, I try to help multiple people together by creating material (see the AwesomeGCP channel), collecting material (see this github repo), writing my own notes, speaking at conferences, and conducting workshops and study jams.
Should I do GCP certifications or should I do AWS or Azure?
As of now, I prefer GCP, but it is not to say that the others aren’t good. I believe that there is enough space for all these public cloud vendors and more. Which of these you should take up is primarily going to be a function of your job demands. But if you are just independently picking one, here are some considerations that might help you.
Disclaimer: this entire section is mostly anecdotal evidence resting on unverifiable data
AWS has the largest market share. Azure is the next biggest. And GCP, next. However, GCP is the fastest growing among the three. There are (almost certainly) more jobs right now in AWS but there are also proportionately many more certified AWS professionals than GCP certified professionals. I would therefore guess that there is greater demand to supply ratio for GCP professionals than AWS professionals. So having a GCP professional certification could help you stand out from the crowd and even command better income.
How long should I study for the exam?
It depends. There’s no one answer to it as it depends on where you are coming from — do you work in that area regularly, do you have other cloud certifications, do you generally do well in multiple choice question exams? I know people who have studied for many months and still flunked, while I also see blog posts from people who studied just three days and passed.
Is there value in doing the beta exam?
I don’t think there is much. I’ve done three beta certification exams but I have not seen any substantial gain from the significantly higher effort. The exam itself is a torturous 4 hours — not including travel time to the location. For me, each time that is almost a full day gone. There is no food/snack break in between. Assuming that I left home an hour or so earlier, that’s about 5 hours minimum with no food. After a few hours of these tough questions, I can barely focus. And if I fail, I have to pay full price and try again. Why would I put myself through that without a significant incentive? There is a discount of about 40% on the price, but even for me who pays from my own pocket for the exams, it is not worth the effort.
Worse yet is that there is very little but the documentation to prepare with. Nobody has written any guiding notes, there is no practice test, no online courses (though Coursera usually has a high level course by Google instructors). It’s easier on your time and sanity to take the exam when it is generally available.
Are you a GCP trainer? Can you train me?
Yes, I am an Google Authorized trainer. Proof. However, this is usually for corporates. For a trainer, the time to train an individual or a group of people is somewhat similar. As an individual it wouldn’t be cost effective to pay the price that a group of people would pay. Therefore, for individuals, I recommend you take some of the online courses which work out much better. Google and other training coordinators also setup online trainings occasionally. You can see that here: https://cloud.google.com/training/courses
Can you help me with the answer to this practice question?
If it is a question from the official practice test, then yes. For others, I would rather not. That’s because I am unable to verify where you got the question from. People have reached out to me with questions that look eerily similar to the actual exam. Maybe you got it from a legitimate source or maybe you got it from an exam dump, but I’m unable to verify which. So in general no, don’t send me practice questions as I strongly disapprove of question dumps.
Update: put the question on Stackoverflow. Somebody else or I could answer it there.
Does passing the exam ensure higher salary?
A survey in 2019 showed that among the cloud certifications the GCP Professional Architect paid the highest. But that is statistical data. It doesn’t mean it will apply to every single person.
Do you know of jobs in GCP?
I do hear of them based on those who reach out to me. However, there is no central collation as yet. I am hoping to fix that gap this year. Give me some time to work on that.
QwikLabs is costly.
I have been collecting free codes that you could use: https://medium.com/@sathishvj/qwiklabs-free-codes-gcp-and-aws-e40f3855ffdb.
There might not be enough codes even with that. If you find the labs useful (and I think they are very useful), take a monthly subscription. It is worth the $55. If you work for a company that uses GCP, chances are that they will sponsor that cost.
Can you publish more QwikLabs codes? & Some of the QwikLabs codes you published have expired.
I don’t create the codes myself. I only collate what QwikLabs puts out.
Unfortunately, there is no easy way to figure out which ones have expired. Of late, when they publish the codes they’ve also been mentioning the expiry date. I have started including that for your convenience.
I don’t have QwikLabs. Where can I practice?
You can create a new gmail account, sign up for Google Cloud and get 300$ credit which should be good enough to practice with. You will need to put in a credit card number for each signup to legitimize it, but I don’t think it is automatically charged if it goes beyond the credit limit. Please check that yourself.
Kryterion/Proctor suspended my exam! Why?
During the exam, if you seem to be doing something suspicious, they will suspend the exam. They want to ensure that you are not possibly cheating. Some of the reasons I’ve heard:
If my exam gets suspended, what should I do?
First, you should chat with the person who is proctoring your exam. In the past, others and I have had our exams re-started after about 15 minutes.
If that doesn’t reach a good conclusion, you should contact kryterion support: ktnsupport@kryteriononline.com
If they are unable to give you a resolution, try reaching out to Google: https://support.google.com/cloud-certification/gethelp.
Earlier, there also used to be this email id, but I don’t know if it’s still valid: google-cloud-certification-support@google.com
There were network problems. I got a FAIL result. How do I know it was not because of the network issues?
Network issues are very common. I’ve faced it too at almost every onsite or remote proctored exam. Online exam proctors like Kryterion take that into account. The examination process is built with that contingency considered.
Were you able to do a review of the questions at the end? Did you see any differences in the answers compared to what you selected? My guess is that it was consistent with what you answered. So it’s unlikely that network problems were the issue.
These exam questions are often deceptive — it looks like the answers might be easy but it often requires that you look at the details. So the best thing you can do is to go back and check what you probably misunderstood or answered wrong. Prepare again and take the exam again.
For those appearing for the various certification exams, here is a list of sanitized notes (no direct question, only general topics) about the exam.
Overall notes across all GCP certification exams
Notes from the Professional Cloud Architect exam
Notes from the beta Professional Cloud Developer exam
Notes from the Professional Data Engineer exam
Notes from the Associate Cloud Engineer exam
Notes from the beta Professional Cloud Network Engineer Exam
Notes from the beta Professional Cloud Security Engineer Exam
Notes from the Professional Collaboration Engineer Exam
Notes from the G Suite Exam
A collection of posts, videos, courses, qwiklabs, and other exam details for all exams: https://github.com/sathishvj/awesome-gcp-certifications
Collection of free QwikLabs codes for practice: https://medium.com/@sathishvj/qwiklabs-free-codes-gcp-and-aws-e40f3855ffdb
Wish you the very best with your GCP certifications. If you still have more questions, you can reach me at LinkedIn and Twitter. If you can support my work creating videos on my YouTube channel AwesomeGCP, you can do so on Patreon or BuyMeACoffee.
tech architect, tutor, investor | GCP 12x certified | youtube/AwesomeGCP | Google Developer Expert | Go
122 
1
122 
122 
1
tech architect, tutor, investor | GCP 12x certified | youtube/AwesomeGCP | Google Developer Expert | Go
"
https://medium.com/@sathishvj/notes-from-my-google-cloud-g-suite-certification-exam-36cd881eded3?source=search_post---------300,"Sign in
There are currently no responses for this story.
Be the first to respond.
sathish vj
Jan 6, 2020·9 min read
Subscribe to my YouTube channel that teaches you to apply Google Cloud to your projects and also prepare for the certifications: youtube.com/AwesomeGCP. Check out the playlists I currently have for Associate Cloud Engineer, Professional Architect, Professional Data Engineer, Professional Cloud Developer, Professional Cloud DevOps Engineer, Professional Cloud Network Engineer, and Professional Cloud Security Engineer.
While other Google Cloud Certification exams have to be taken at an exam site, this one can be done at home. I wanted to experiment and see how that works out. So I scheduled it at home/office. Wasn’t a good idea. There were constant disturbances at my co-working space. In a phone booth/conference room next door, people were constantly speaking. Others were walking by the meeting room/phone booth. I was constantly concerned that somebody is going to walk in and say hi and start a conversation. At home I’d have to open the door for anybody that rang the bell or visited. I recommend you take this test at a testing center where these variables are not your concern. I wonder whether it costs more than the USD 75 that I was charged for taking the exam at home if I chose to take it a test center.
Edit: On thinking about it later, I realized that one of the advantages of doing this exam at home/office on your own computer would be that you know some of the shortcuts. Had I taken it at a test center, it probably would have been on Windows and some of the editing shortcuts might not have been natural for me.
Before the exam, make sure you login to the proctoru site. You will need to install a plugin for Chrome. On my Mac on the day of the exam, when I tried to give it permissions to record video, Chrome had to restart. When I installed the plugin a few days prior, it had not checked recording video, so this happened just as I started the exam. So all my open tabs were gone because I had started this in a new window. On turning back on, only this window showed up. Aargh.
I would recommend that you start this the proctoru exam session in an incognito window. That way, you will only have to deal with the account given during the exam and not your own gmail account also.
I was first asked to take a photograph of myself using the webcam. Then I had show an ID with a photograph in it to the webcam. So, keep your ID ready.
The verification started off with a person who talked to me (voice, not chat) via the website. She asked to see the ID again — I had to show it in front of the webcam. She then took a look at the room. Make sure there are no other books or devices or other material in the room. I was asked to show where I had kept the mobile phone. She asked me to keep it far away from me on the ground. Then I moved/rotated the laptop around to show her a full view of the room, including below the table.
The exam itself is divided into two general sections: a multiple choice set and a scenario set.
We get 30 minutes for the MCQ set, which I finished in less than 15 minutes. Note that any time saved here does not carry over into the scenario section. You can review the questions in the MCQ section before you submit it that section. But once you finish the MCQ section, you cannot come back to it. There is a checkbox to mark a question for later review. I marked a couple of questions for review, came back to it later before submitting the section.
We get 1 hour 30 minutes for the scenarios section. I got 6 scenarios, but I do not know if that is different for others. I was tested for Sheets, Docs, and Slides. I do not know if any of the others like Hangouts, Sites, Drive, Calendar, etc. could be part of the scenarios.
The questions itself were fairly simple. I’d be a little surprised if I got any of them wrong. I might have made some silly mistakes, but nothing where I did not know the answer. Then again, I have used the G Suite products quite a bit for a few years. If you are from a Microsoft Office background, the things you have to do are very similar. It was easy to figure out everything that was asked even if I did not already know how to do it.
One big tip: there is a “Help” menu item that contains a ‘Search the menus’ search box. The menu items are logically grouped together, so you usually don’t have to struggle much to find anything. But even then, if you can’t find something — say like ‘Data validation’ — then just search for it.
The format of the scenario is similar across all of them. If it is a Docs/Sheets/Slides scenario, you are given a link. You open that link in a new tab. Then you go back to the scenario questions and apply it on the document. For example, the scenario might say “Add the comment ‘This needs to be checked’ on the title ‘Section 1’”. So you go to the document and do that. After you are done with all questions in the scenario, you may close that particular document. Though I didn’t do it myself, in case you close a tab accidentally, you should be able to re-open it from the first page of the scenario.
I tried to set the document/sheet tab to the right half of the screen and the question tab to the left side so that I could look at both simultaneously. The test tool threw up a warning saying that I am not allowed to navigate away from the given windows and additionally warned that they would report it if I tried it again. I was obviously not navigating away from the page, but I guess the tool cannot detect that. So you will have to switch back and forth between the two tabs. That wasn’t convenient.
Unlike the Google Cloud exams, the results are not immediate. There is a wait period of maximum 7 days for the results. Because I wrote it during the Christmas-New Year season, I wonder if it will be delayed further. Edit: I wrote my exam on a Friday and I got my result by Tuesday morning.
Honestly, I didn’t prepare too much for this exam. I would recommend you take the practice exam: https://www.qwiklabs.com/focuses/4051?parent=catalog. You could also do this entire quest to get familiar with some of the features of the G Suite products: https://www.qwiklabs.com/quests/65. I was not able to do the labs because it did not open up features like Calendar, Drive, etc. for some labs. But that did not matter to me much. I just read through the steps of the labs and that was good enough for me. I did learn a few new things though — for example, that you can add specific persons to a comment by typing + and then choosing the recipient of that comment.
This exam might be useful for folks who use G Suite. If you are making a career in Google Cloud on any of the tech tracks, this certification does not further your status in any particular way. At best, the G Suite certification would be a vanity metric of an additional certificate. However, my reasoning for attempting the exam was that it might help towards the Professional Collaboration Engineer. Once I finish that, I shall see if learning for the GSuite exam was useful or not. It is a super easy exam though, so if nothing else and you merely want that vanity metric, go for it.
Edit: I finished the Collaboration Engineer exam. Looking back, there is no overlap between the questions in the two exams. There is some overlap in the content to learn, but very little.
Here are some general sections within each that you should study. And many of these are covered in the sample G Suite exam on Qwiklabs.
Docs* formatting text — italic, bold, etc.* creating tables * editing header/footer and page numbers* font and font size edits* comments and assigning comments* highlighting elements* Bulleting
Sheets* basic formulas like average, sum, etc.* creating basic charts* conditional formatting* comments and assigning comments* data validation* No pivot table or anything complex like that.* No scripting
Slides* Formatting elements* Applying themes and layouts* Aligning elements* Grouping elements* Publishing slides* Bulleting
Drive* Sharing folders and documents* Uploading and downloading files and folders* Team drives* Moving documents that are shared with you to your drive* Checking edit history/activity on the file/folder* Searching for files
Calendar* Creating events* Creating reminders* Shared calendars* Adding attendees* Marking attendees as optional
Gmail* Embedding images* Various settings* Out of office setting* Difference between labels and folders* Filtering emails* Adding tasks from gmail* Snoozing an email until a certain time
Hangouts* starting hangouts from gmail* sharing screen/windows* adding users
To the test creators, there would be some feedback. * One question asked me to add a formula in cell I36. But because of the font, it was not possible to figure out if it was i36 or l36. So maybe use non-confusing cell letters. Usually if I have an issue like that, I would copy that text and paste it into another editor where the font does not have that problem so that I can distinguish the letter. But during the exam, we can’t go anywhere else. A workaround for the test taker would be to paste it into an empty cell, change the font only for that cell to figure it out, and then delete it after.* There are questions like “The comment should be Look into this.” Working with QwikLabs which sometimes matches exact strings (case sensitive, same spelling, punctuation, etc.), I’ve become cautious of the evaluation/checks. Is the comment “Look into this.” or “Look into this”? I know it seems silly, but I’ve had issues with even minor things like this in automated checks which match for exact strings. How is the valuation done for these? Is it manual or automated? Does it match the exact string or does it allow differences? For the test taker, I suggest you do not type in these things; instead, copy-paste it.* There was one question that I got that asked me to choose 2 correct answers. And to me there seemed only one that was right. Usually I am eventually proved wrong, but how do we give feedback?* At the end there was a feedback survey. I was not able to submit it. Weirdly, it kept going in circles. There was no Submit button at any point. Only next and back buttons.
Google Cloud Certified in G Suite
For those appearing for the various certification exams, here is a list of sanitized notes (no direct question, only general topics) about the exam.
Overall notes across all GCP certification exams
Notes from the Professional Cloud Architect exam
Notes from the beta Professional Cloud Developer exam
Notes from the Professional Data Engineer exam
Notes from the Associate Cloud Engineer exam
Notes from the beta Professional Cloud Network Engineer Exam
Notes from the beta Professional Cloud Security Engineer Exam
Notes from the Professional Collaboration Engineer Exam
Notes from the G Suite Exam
Notes from the Professional DevOps Engineer Exam
Notes from the Professional Machine Learning Engineer Exam
Main Link — https://cloud.google.com/certification/gsuite
Topics Outline-https://cloud.google.com/certification/guides/gsuite
Practice Exam-https://google.qwiklabs.com/focuses/4051?locale=en&parent=catalog
A collection of posts, videos, courses, qwiklabs, and other exam details for all exams: https://github.com/sathishvj/awesome-gcp-certifications
I’ve collected here a bunch of free Qwiklabs codes which are awesome to get lots of hands-on practice. Use them well.
medium.com
Check the FAQs here: https://medium.com/@sathishvj/frequently-asked-follow-up-questions-on-google-cloud-gcp-certifications-438e1addb91d.
Wish you the very best with your GCP certifications. You can reach me at LinkedIn and Twitter. If you can support my work creating videos on my YouTube channel AwesomeGCP, you can do so on Patreon or BuyMeACoffee.
tech architect, tutor, investor | GCP 12x certified | youtube/AwesomeGCP | Google Developer Expert | Go
39 
3
39 
39 
3
tech architect, tutor, investor | GCP 12x certified | youtube/AwesomeGCP | Google Developer Expert | Go
"
https://medium.com/@duhroach/optimizing-google-cloud-storage-small-file-upload-performance-ad26530201dc?source=search_post---------301,"Sign in
There are currently no responses for this story.
Be the first to respond.
Colt McAnlis
Oct 12, 2017·4 min read
DNA SOAP is a bio-tech company focused on understanding more about the human genome.
Their latest release was a software package that allowed a distributed protein folding computing model; where internal machines and external machines could be used together to calculate protein folding operations.
While this was a great, cost effective way to tackle a very compute-heavy problem, their new software design wasn’t performing as well as they wanted. Any time they would create a new data set to be worked on, there was a 5–6 minute delay before any of the clients would start doing computation.
Their architecture was pretty straightforward. Once a new genome dataset came in, it would be mirrored across regions, and then a Data Coordinator would chop up the genome into workable blocks, and upload that data to Google Cloud Storage. Clients listening on the service would be notified of the available work via Pub/ Sub, and would grab the blocks directly from GCS to start working.
The problem, as defined by DNA SOAP, was that it took too long to upload all the files to GCS. Here was the interesting part : The data coordinator would modify the number and size of blocks based on the expected number of clients in that region. So with lots of users, there could be a lot of smaller files being sent around. The assumption then, is that there must be some correlation between file sizes and GCS upload performance.
This seems like a straightforward enough problem to duplicate. We can generate a bunch of small files, upload them to a GCS bucket, and download them individually from the GCS bucket, and see what the time difference is.
To test this, I created a small python script which generated files of various sizes, and then uploaded each file to a GCS regional bucket 100 times (random name each time to remove collisions). Below, you see the performance (bytes/sec) charted against the size of the objects.
As the graph shows, upload speed improves as the object size improves, which is mostly due to the reduction of transactional overhead per event.
The reason for this is that GCS is really strong in terms of single stream throughput, but there’s a fixed latency cost per request that’s related to ensuring that the files are replicated and uploaded properly. As such, this transactional overhead is higher for smaller files, and as that number increases, so does the amount of overhead. As the files get larger, the transactional overhead is smaller, resulting in higher throughput.
This concept of high-overhead-per-operation is not a new one. If you’ve ever done SIMD programming on the CPU, the same idea exists : batch your operations together so that the overhead of each operation is mitigated across the set.
For DNA SOAP, fixing their upload performance on GCP followed the same idea: Parallel uploads.
Gsutil provides the `-m` option which performs a parallel (multi-threaded/multi-processing) copy which can significantly increase the performance of an upload. Here’s the same test, but batch uploading 100 200k files using `gsutil -m cp <src folder> <dst folder>` rather than uploading each one individually.
Although it’s more pronounced at the right side of the graph, the log-scale version shows that -m gives significant improvements in performance at the left side (smaller file sizes) as well.
GCS is a powerhouse when it comes to upload performance, but that power comes once you hit a certain level of efficiency. The transactional overhead of tons of small files means lots of additional round-trips (to ensure consistency) and thus, less performance.
The takeaway? It’s more performant to batch upload your files with `-m` but if you have to do things one-at-a-time, make sure you’re using the direct API rather than going through GSUTIL for each one.
Which is faster, TCP or HTTP load balancers?
Did you know there was a connection between core count and egress?
Want to know how to profile your kubernetes boot times?
DA @ Google; http://goo.gl/bbPefY | http://goo.gl/xZ4fE7 | https://goo.gl/RGsQlF | http://goo.gl/4ZJkY1 | http://goo.gl/qR5WI1
113 
113 
113 
DA @ Google; http://goo.gl/bbPefY | http://goo.gl/xZ4fE7 | https://goo.gl/RGsQlF | http://goo.gl/4ZJkY1 | http://goo.gl/qR5WI1
"
https://medium.com/javarevisited/7-free-courses-to-learn-google-cloud-platform-for-beginners-cbb260fbd8e4?source=search_post---------302,"There are currently no responses for this story.
Be the first to respond.
Hello guys, If you want to learn Google Cloud Platform in 2021 and looking for some free online Google Cloud training courses, tutorials, and learning materials to start your GCP journey then you have come to the right place.
Earlier, I have shared the best Google Cloud Platform courses, and today, I am going to share free online courses from Udemy Youtube, and Coursera which you can join to learn Google Cloud platform.
If you don’t know, Google Cloud Platform is the massive Cloud platform for Google which is one of the three biggest public cloud platforms along with AWS and Microsoft Azure.
There is a huge demand for Google Cloud Professionals but there are not many skilled professionals in the market, hence more and more people are learning about Google Cloud and getting certified.
This article contains free Google cloud courses from sites like Udemy, Coursera, and Pluralsight which you can use to learn Google cloud from scratch. These Cloud services are providing great help to the technical world easing up the tasks, remote access, and security features. People are taking their business online, and the market is growing at a steady rate.
We have to keep up with the trend if you’re a business enthusiast, you know how much importance our words hold. To provide you with knowledge of the cloud and how you can work with them we have picked up a few courses that you can access for free and make your way in the market with new technology available. We have tried to provide you all the free Google cloud courses that can satisfy your curiosity and help you attain your goal whether you’re a developer or just looking to gain some Cloud computing skills.
We specifically focused on the part to provide you with the best kind of teaching that doesn’t waste much of your time and can make learning easy for you.
By the way, if you don’t mind spending few bucks on learning a valuable skill like Google Cloud and looking for more comprehensive online training courses then I highly recommend you check out the Google Cloud Platform (GCP) Fundamentals for Beginners course on Udemy. One of the best online courses to learn GCP concepts from scratch.
udemy.com
Here is the list of the best free courses that you can access online to learn the Google Cloud Platform or GCP. These courses not only cover the platform details but also get you familiar with essential GCP services and features.
This is one of the best available courses on Udemy. To begin this course you need to have little knowledge of IT. Since the Google cloud platform is the fastest growing cloud service in the world, people are taking their business online and enhancing them with services the cloud has to provide. If you are looking to be in the developer section of the industry this course will help you a lot. Also, big data concepts and machine learning concepts are introduced recently. Here are things you will learn in this course:
This course focuses on providing you with a deep understanding of google cloud concepts and working. You’ll be learning a lot about its applications in different segments of this course.
Here is the link to join this free GCP course — Google Cloud Platform Concepts
This is one of the best free courses to learn Google Cloud Platform and it’s offered by none other than Google Cloud on Coursera. You can access the course to learn it all from their official website as well. They have sincerely categorized and provided concept-wise learning, distributed among topics. All the new technology and its functioning can be accessed on the official website if you have a learner id. This course is also part of multiple Coursera specializations like Developing Application with Google Cloud and completing this course counts towards getting the certification. Here are things you will learn in this course:
All the topics are covered with examples. It is a theoretical way to teach so you have to be patient while you learn and apply concepts. Doubts can be easily addressed in the query section.
Also, real-time practice is going to provide you a detailed understanding of cloud services and functions. You can interact with other users following the same course as well. There are other premium courses suggested by Google as well.
Here is the link to join this Coursera course — Google Cloud Platform Fundamentals: Core Infrastructure
Overall a fantastic free Coursera course to learn key GCP concepts and services like Google App Engine, Google Compute Engine, Google Cloud Storage, Google Kubernetes Engine, Google Cloud SQL, and BigQuery, etc.
This is another awesome free Udemy course to learn about Google Cloud Platform from scratch. To start this course you should have a little experience with coding or software development.
This is an introductory course to get started on Google Cloud Platform (GCP). During this course, you will learn about what is Cloud Computing, what are different Cloud computing models, Important GCP services, and hands o
You can also use this free course to prepare for highly valued Google Certifications like “Google Associate Cloud Engineer”, “Professional Cloud Architect” etc. Here are things you will learn in this free Google Cloud course:
For all those people who are looking to gain application development skills in this course focus on that too.
Also, the basic knowledge of the cloud would provide you a better understanding. You will be getting a lot of theory and hands-on to prepare you for a bigger platform.
Here is the link to join this free course — Google Cloud Fundamentals 101
Google Cloud has many courses on Pluralsight and this is one of them. PluralSight uses effective tutorial learning to provide critical cloud skills. Even they’ll teach you how to operate on hybrid as well as in multi-cloud computing.
You’ll be able to arrange and understand the way digital information works in the cloud. Pluralsight has teamed up with Google to provide you with the essential skillset to make your career in the cloud.  This course designed for AWS professionals like AWS developers, solution architects, and sysops administrators who are already familiar with essential cloud computing concepts and have prior experience with AWS. Here are things covered in this course:
They start from basic and the course takes you through all the important topics that need to be learned.
Here is the link to join this course — Google Cloud Fundamentals for AWS Professionals
Google has mentioned the Pluralsight courses on its official website for cloud computing as well. This course is personally recommended by Google and since the website doesn’t support free access for a longer time, make sure you get to the learning as soon as possible. Alternatively, you can also use Pluralsight 10-day-free-trail to access this course for FREE.
pluralsight.pxf.io
This is a list of small online tutorials from a Youtube Playlist to learn about Google Cloud Platform from Google Tech itself. Learning Google Cloud Platform! GCP can be overwhelming at first, but these small tutorials will help you get started.
You will watch a short video to understand the difference between Google Cloud Platform’s (GCP) free tier and free trial and understand how they can help you test GCP or use it for development purposes at no or little cost.
There are many short videos to learn about essential services and Google Cloud Platform concepts like compute, storage, networking, security, pricing, and many more.
If you like to learn from short videos then I highly recommend this free Youtube playlist from Google Tech. You can watch them right here or on Youtube.
This is another awesome course to learn about Google Cloud Platform and it's available on Pluralsight. This course will provide an overview of the platform and a framework for diving deeper.
While the Google Cloud Platform is a more recent offering than some of its competitors, it draws on years of experience running Google’s massive internal infrastructure and exposes a streamlined set of solution-focused capabilities to help you build great systems.
Created by Howard Dierking this course will first teach the core building blocks of the platform. Next, you’ll explore the characteristics that differentiate Google’s offering from other cloud platforms.
Finally, you’ll learn the common application architectural patterns. By the end of this course, you will be able to understand how the areas fit together and provide starting points for deeper exploration.
Here is the link to join this course — Google Cloud Platform Fundamentals
By the way, this course is not exactly free as you would need a Pluralsight membership which costs around $29 per month or $299 per year. Alternatively, you can use their 10-day-free-trial to watch this course for FREE.
pluralsight.pxf.io
This is one of the best free Udemy courses available online to learn Google Cloud Platform concepts, plus learners have provided it near-perfect ratings to learn. One thing that would be beneficial is that you don’t need to learn anything to begin this course, you can even start right now. If you are willing to learn the basics of google cloud and cloud computing this is the course for you. Here are things covered in this course:
This course has a goal to provide you with a simple conceptual introduction to cloud services and computing. There is not much technical information to remember, this course will help you to have the perspective if you are starting from scratch. Plus if you choose to learn from this course you’re going to get Linux Academy’s Hands-On lab and maybe you’ll get flashcards.
Here is the link to join this course — Google Cloud Concepts
That’s all about the best free courses to learn Google Cloud Platform or GCP in 2021. Google cloud is the fastest growing cloud service in the world. Hope you found these courses useful to help you learn the new technology and implementation. We would recommend you to visit each course personally to have a better insight.
Other Cloud Computing Articles you may like
Thanks for reading this article so far. If you like these free Google Cloud Platform online courses for beginners then please share them with your friends and colleagues. If you have any questions or feedback then please drop a note.
P. S. — If you don’t mind spending few bucks for learning a valuable skill like Google Cloud and looking for more comprehensive online training courses then I highly recommend you check out the Google Cloud Platform (GCP) Fundamentals for Beginners course on Udemy. One of the better courses to learn GCP concepts from scratch.
udemy.com
Medium’s largest Java publication, followed by 14630+ programmers. Follow to join our community.
164 
164 claps
164 
A humble place to learn Java and Programming better.
Written by
I am Java programmer, blogger, working on Java, J2EE, UNIX, FIX Protocol. I share Java tips on http://javarevisited.blogspot.com and http://java67.com
A humble place to learn Java and Programming better.
"
https://medium.com/@ismailtasdelen/create-a-free-hacking-machine-with-google-cloud-9f61eff97a4f?source=search_post---------303,"Sign in
There are currently no responses for this story.
Be the first to respond.
Ismail Tasdelen
Dec 7, 2019·2 min read
Hello,Today I will tell you how to create a hacking machine using google cloud. First thing you need to do is go to https://cloud.google.com/.
The next step is to click “Console” in the picture above and wait for your virtual console to open. Google Cloud gives us this opportunity for free. We may have wanted to take advantage of this opportunity. There’s a tool we’re going to use in this name, katoolin. This tool allows us to install the tools that are installed on kali linux.
So what do we do now?
“sudo su” command with after receiving our authority “git clone https://github.com/LionSec/katoolin.git” saying we are doing cloning process to our own system. Then run the command “sudo cp katoolin / katoolin.py / usr / bin / katoolin.. We put it in / usr / bin. The reason for this is to make an executable script from each directory path. Finally, we say “sudo chmod + x / usr / bin / katoolin” and give the executable authority to the tool we copied along the directory path.
Then we run the katoolin tool with the sudo privilege as above. As you have seen, the cathoolin is ready to use. After you have done the following steps 1, you can install the tool or tool package as you would in a kali linux distribution.
Hope to see you in my next article ..
🎯 Security Researcher | Bug Bounty Hunter | Exploit Development | Chess Player | Code is Life | 0xBUFFDEAD | Advisor 🎯
See all (212)
74 
1
74 claps
74 
1
🎯 Security Researcher | Bug Bounty Hunter | Exploit Development | Chess Player | Code is Life | 0xBUFFDEAD | Advisor 🎯
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/using-google-cloud-ml-engine-to-train-a-regression-model-e2a582de389e?source=search_post---------304,"There are currently no responses for this story.
Be the first to respond.
Submit a training model job in Google Cloud Datalab
Google Cloud Platform is an useful tool to run Machine learning code and processes. The benefits are many — setup and storage on the cloud, a ML toolbox, cloud VM resources, other well known cloud benefits, and easy setup. This article is specifically about submitting a job (task) for a training model created in a…
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/javarevisited/8-free-google-cloud-data-engineer-certification-courses-and-practice-tests-594f31643b58?source=search_post---------305,"There are currently no responses for this story.
Be the first to respond.
Hello guys, if you are preparing for Google Cloud Professional Cloud Data Engineer certification in 2021 and looking for free online Google cloud Data Engineer courses then you have come to the right place.
In the past, I have shared the best courses to learn Google Cloud as well as the best online training courses to pass cloud engineer, and cloud architect certifications, and today, I am going to share free GCP Cloud Data Engineer certification courses for both beginners and experienced cloud professionals.
This is one of the most difficult and prestigious exams, similar ot AWS Data Analytics certification, once you pass this exam, you will have sufficient knowledge and skills to propose Google cloud-based solution, which is a very in-demand skill.
Every Company right now is focused on Data. Most of the companies even changed their field of work also. Earlier, they were dealing with something else, but now they are dealing with data.
In the coming years, data will be the most precious asset for a company, and to secure that data, cloud servers will be required. Therefore if you are looking forward to being a Google Professional Cloud Data Engineer, you are going to have a bright professional future.
If you are still wobbling around, then it is recommended to enroll in some courses and then decide which field will be the sweet spot for you.
In this guide, I have listed the 7 best online courses that are free of cost, and these courses can help you pass the GCP Professional Cloud Data Engineer Exam.
Btw, If you need a comprehensive online course then I also highly recommend you to join Google Cloud Professional Data Engineer: Get Certified 2021 course By Dan Sullivan, the guy who wrote official Google certification docs on Udemy.
udemy.com
This online course is not free but the most comprehensive online courses for Google cloud certifications and provide study material for all four Google Cloud certifications including Cloud Data Engineer.
In this list, every course is free of cost, and some of them are interlinked with each other. Therefore you have to enroll in all those courses to prepare properly.
Now let’s begin with the list.
The next two courses on the list are interlinked with each other. If you are looking for a course that can help you create a strong foundation of cloud computing and then want to learn about GCP, then these courses are for you.
Can you use Cloud Computing? What are the advantages of cloud computing? And what are the future aspects of cloud computing?
If you are an absolute beginner and don’t know anything about Cloud Computing, then this code will act as a foundation for you. Once you are done with this course you can start learning specifically for your GCP Professional Cloud Data Engineer Exam.
Here is the link to join this free course — Introduction to Cloud Computing
This course is available on Coursera, and it is among the best courses available on the Internet. In this course, you will be about the basics and complex concepts of GCP. Besides this, it is created by the Google Cloud Training Team. Therefore understanding the concepts won’t be a concern for you.
Over 23 thousand students have enrolled in this course, and it is rated 4.6 stars out of 5 on the platform. It’s a 7-hour long video course, and in this course, you’ll learn about the following topics:
Once you are done with this course, you’ll find it easy to handle and process data on GCP. Besides this, you’ll be able to pass the GCP Cloud Professional Data Engineer Exam with flying colors.
Here is the link to join this course — Preparing for the Google Cloud Professional Data Engineer Exam
This course will be beneficial only if you have successfully completed the above-listed code. In this one 16 minute video course over 17000 students have enrolled. This course is created by Linux Academy.
In this course, you will learn about the following:
Even though this course covers nothing related to data, but in the GCP Professional Cloud Data Engineer Exam, it is noted that most questions are asked from the basics. If you have a strong base, you can easily clear the GCP Professional Cloud Data Engineer Exam.
If you are running short on time, then you can simply enroll in this course. Even with basic knowledge, you can clear your exam.
Here is the link to join this online course — . Google Cloud Concepts
If you are looking for a course with intermediate difficulty and something that is not designed for beginners, this course is for you.
Both the above-listed courses are specifically designed for the beginner, but this course is designed for those who have prior knowledge about GCP and Cloud Computing.
In this course, the instructor will teach you directly from the topic that you are looking for. In this course, no introduction will be given to you regarding cloud computing and all.
Apart from basic knowledge about cloud computing, this course also requires basic knowledge about the database. Thus, it is not recommended to enroll in this course if you are just starting with cloud computing and never worked on databases.
Here is the link to join this free course — Google Data Engineer Exam
This course is also available on Coursera, but this one is more detailed than the previous one. This course is also created by Google Cloud Training Team, and after completing this course, you’ll be rewarded with a certificate of completion.
This is a group of courses that will teach you every bit related to Data engineering on GCP. It is one of the most detailed courses available on the internet. If you give 4 hours a week to this course, it will take you four months to complete this course.
In this course, you will learn about the basics of data, and later, the instructor will teach you different SQL statements to handle data on GCP. Over 23 thousand students have enrolled in this course, and based on reviews from over 500 students; this course is rated as 4.4 out of 5.
Graphics are used quite smartly in this course, and since it’s a long course, every particular step is illustrated multiple times. If you have an ample amount of time, then you must enroll in this case. It will give you hands-on experience on live projects too.
Here is the link to join this course — Data Engineering with Google Cloud Professional
By the way, if you find Coursera courses useful, which they are because they are created by reputed companies and universities around the world, I suggest you join Coursera Plus, a subscription plan from Coursera which gives you unlimited access to their most popular courses, specialization, professional certificate, and guided projects. It cost around $399/year but it's completely worth your money as you get unlimited certificates.
udemy.com
This is another free course from udemy to learn Google cloud and you can use this to prepare for Google cloud data engineer certification as well.
By the way, this is an introductory course to get started on the Google Cloud Platform (GCP). During this course, you will learn about what is Cloud Computing, what are different Cloud computing models, Important GCP services, and hands-on.
There are no prerequisites for this course, however, any familiarity with other Cloud computing, familiarity with IT projects will be helpful.
Here is the link to join this free course — Google Cloud Fundamentals 101
This is a comprehensive free course on Google Cloud Platform on Udemy. This 6-hour long course is as good as any paid course and I strongly recommend every beginner to join this course to learn Google cloud concepts from scratch.
Here is the thing you will learn in this course
Here is the link to join this free course — Google Cloud Fundamentals 101
This is a short free course on how to pass Google cloud certification on UDemy. This course is created by none other than Dan Sullivan who has authored Google’s official certification guide as well as created best-selling Udemy courses for Google cloud certification like Google Cloud Professional Data Engineer: Get Certified 2021, which I highly recommend if you can spend few bucks.
Google Cloud certification exams are challenging. Even if you deep knowledge of Google Cloud services, you could fail a certification exam if you are unfamiliar with the structure of the tests.
This free course will help you understand how Google Cloud certification exams are structured, the rules for taking these exams, and the kinds of questions you can expect.
Perhaps most importantly this course shows how to analyze questions and precisely identify what is being asked and how to reason about each possible answer so you can choose the best option.
Here is the link to join this free course — How to Pass Google Cloud Certification Exams
That’s all about the free online courses to pass Google Cloud Professional Cloud Data Engineer in 2021. In this guide, every sort, of course, is listed. If you are looking for a detailed course you’ll find it here, if you are looking for a simple and short course you’ll find it here.
Besides this, All the courses listed above are created by industry experts; therefore understanding won’t be a problem for you. Even though five different courses are listed above, it is recommended to enroll in all these courses. Then after completing a few videos, you can decide which course is more understandable for you.
Other Cloud Computing and IT Certification Courses and Articles you may like
Thanks for reading this article so far. If you find these free Google Cloud Platform and Cloud Engineer certification courses useful, then, please share it with your friends and colleagues. If you have any questions or feedback, then please drop a note.
P. S. — If you need more comprehensive and focused online certification courses then I also highly recommend you to join Ultimate Google Cloud Certifications: All in one Bundle (4) course on Udemy. This is not free but the most comprehensive online courses for Google cloud certifications and provide study material for all four Google Cloud certifications including Cloud Data Engineer.
udemy.com
Medium’s largest Java publication, followed by 14630+ programmers. Follow to join our community.
186 
1
Collection of best Java articles, tutorials, courses, books, and resources from Javarevisite and its authors, Java Experts and many more.  Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
186 claps
186 
1
Written by
I am Java programmer, blogger, working on Java, J2EE, UNIX, FIX Protocol. I share Java tips on http://javarevisited.blogspot.com and http://java67.com
A humble place to learn Java and Programming better.
Written by
I am Java programmer, blogger, working on Java, J2EE, UNIX, FIX Protocol. I share Java tips on http://javarevisited.blogspot.com and http://java67.com
A humble place to learn Java and Programming better.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://blog.getambassador.io/experimenting-with-a-serverless-service-mesh-with-ambassador-on-google-cloud-run-c0b0da2130d1?source=search_post---------306,NA
https://medium.com/google-cloud/disaster-recovery-on-google-cloud-for-data-part-1-9cf08782bac9?source=search_post---------307,"There are currently no responses for this story.
Be the first to respond.
“Get Cooking in Cloud” is a blog and video series to help enterprises and developers build business solutions on Google Cloud. In this second miniseries I am covering Disaster Recovery on Google Cloud. Disasters can be pretty hard to deal with when you have an online presence. In these articles, we have been elaborating on how to deal with disasters like earthquakes, power outages, floods, fires etc.
Here are all the articles in this miniseries, for you to checkout.
Since data is the most important part of any application’s recovery, I thought talking specifically about that would make sense. Hence, I am focusing this and the next article on DR for data. In the next one, you will learn to plan for data recovery when production environment is on Google Cloud and in this article, you will learn to plan for data recovery when production environment is on-premise or on another cloud. So, read on!
Mane-Street-Style is an e-commerce company that has their production environment on-premises. Imagine if they were to discover that they have lost all the recent customer orders during a disaster. It would be a HUGE loss for their business if orders are not fulfilled. Since data is a critical piece of their application, let’s dive deeper into it and help Mane-Street-Style with some strategies to avoid losing data during a disaster!
You can only recovery data if you have backed it up somewhere. But what do backups mean when it comes to Data?
The term backup in regards to “Data” covers two scenarios:
Now that we have the basic understanding of data and database backups for DR, let’s consider “Mane-Street-Style’s” scenario and how they can set up DR specifically for Data.
For Data backup and recovery, there are a few options:
Mane-Street-Style can use a number of strategies to implement a process to recover a database system from on-premises to Google Cloud. Let’s look at the two of the most common solutions.
When time comes to recover the database to DR site on Google Cloud, it’s easy for Mane-street-Style:
When production environment on premise is up and running, they would just have to just reverse the steps:
An alternative recipe is to set up a standby server on Google Cloud for data replication, which, helps achieve very small RTO and RPO values since it actually replicates data and database state in real time to a hot standby of the database server. If Mane-street-style was to set up a standby server then here is how they would do it:
In the case of Mane-Street-Style, production is set up on -premises. But if they had production set up in AWS they can use the storage transfer service within GCS to transfer objects from Amazon S3 to Cloud Storage.
They could set up a transfer job to schedule periodic synchronization from data source to the sink with advanced filters based on file creation dates, filename filters, and the times of day they prefer to import data. They can also use tools like Apache Airflow to move data between clouds
If you have a production application on-premise or in another cloud and need to set up Data recovery, then hopefully you learned some strategies to do that! Stay tuned for the next article, where you will learn to set up more DR for data on Google Cloud.
Google Cloud community articles and blogs
187 
187 claps
187 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Developer Advocate @Google, Artist & Traveler! Twitter @pvergadia
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/@duhroach/getting-google-cloud-functions-times-in-stackdriver-61ec1b33a92?source=search_post---------308,"Sign in
There are currently no responses for this story.
Be the first to respond.
Colt McAnlis
May 1, 2018·4 min read
A few weeks back, we did a small article on how to get profiling information for Google Cloud Functions. Since then a few folks reached out to me to show off a really nice way to get your GCF times in your Stackdriver dashboard which we felt was awesome enough to highlight here.
Special thanks to Mary Koes and her team for letting me use their screenshots here. If you’d like to learn other great tips for Stackdriver and Cloud functions, make sure to check out their Qwiklabs course!
Getting your GCF latency time into stackdriver involves a few steps (which we’ll walk through below)
Let’s look at each one.
A nifty feature of Stackdriver Monitoring is that it supports “logs-based metrics.” Which allow your logging data in GCP to be parsed and the resulting data to be sent to StackDriver to create graphs, dashboards, and alerts. This is really nice for some things which are automatically logged for you (like execution time in GCF) but also really handy if you’re outputting your own metrics to the logs that you’d like to see a dashboard for (size of payload, or # of RPCs, for example).
In the Logging page of GCP console, filter your logging view to only include Cloud Functions logs by selecting “cloud-functions” from the All Logs dropdown:
In the “Filter by….” field, type “function execution took” (include the quotes!!) to filter your logs to just those that contain this particular phrase (note that it’s the default phrase that GCF outputs when your function finishes a request/response set).
Then click “Create Metric” at the top of the screen.
This will pop up a small dialog asking for some data to be input. The easiest thing to do is the following:
After selecting Create Metric you’ll see your user-defined metric added to your Logs-based Metrics page.
Once you’ve created a metric, we can setup stackdriver to import that metric regularly, and update graphs / monitoring / dashboards with it.
In Stackdriver head to Resources > Metrics Explorer :
Change the Resource type to “Cloud Function.” In the “find resources type and metric” box, simply type in “execution times” and you’ll see it auto populate with “Execution times” from the suggested metrics. By default, this will show you a heatmap view of the execution time:
But you can also view different aggregations, such as the 95th percentile:
But there’s no reason to stop there. As mentioned before, there’s lots of metrics that Logging/Stackdriver auto parse and fill for you, for example, you can see the number of invocations (executions?) charted over time by changing the metric to “Executions” from the suggested metrics. If you then change the graph type to stacked bar you’ll see something like this:
Stackdriver is really an amazing one-stop-shop for many things revolving around dashboard, metrics and alerts. The fact that I can output custom log data, and with a few clicks, get it in graph form w/o having to do all the heavy lifting in between is a great thing.
If you’re interested in more information on integrating Stackdriver with GCF, checkout the Qwiklab course!
DA @ Google; http://goo.gl/bbPefY | http://goo.gl/xZ4fE7 | https://goo.gl/RGsQlF | http://goo.gl/4ZJkY1 | http://goo.gl/qR5WI1
See all (78)
100 
1
100 claps
100 
1
DA @ Google; http://goo.gl/bbPefY | http://goo.gl/xZ4fE7 | https://goo.gl/RGsQlF | http://goo.gl/4ZJkY1 | http://goo.gl/qR5WI1
About
Write
Help
Legal
Get the Medium app
"
https://blog.doit-intl.com/breaking-down-google-cloud-costs-by-location-or-anything-else-6f08918cf867?source=search_post---------309,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
One of the popular question I am getting often is “how do I break down my Google Cloud costs by location?”. Today, I am going to show how to make Iris and reOptimize — two open source projects by DoiT International, to collect and visualize this information for you.
Before we can actually get to specifics, let’s do a quick recap on Iris and reOptimize so people not familiar with these tools can be better equipped for the rest of the article.
Iris is an open source software which automatically assigns labels to Google Cloud resources, usually for better discoverability and observability. With Iris, each resource in Google Cloud gets an automatically generated labels, usually in a form of [iris_name], [iris_region] and [iris_zone]. For example, if you have a Google Compute Engine instance named `nginx`, Iris will automatically attach the following labels this instance [iris_name:nginx], [iris_region:us-central1] and [iris_zone:us-central1-f]. You can read more about how Iris works in one of our recent blog post.
Since Iris is open source and it is extensible using plugins, you can basically collect any information you’d like from Google Cloud and add it as labels to your resources. Few popular examples can be iris_instance-cores, iris_instance-memory and finally iris_ip.
reOptimize is a free cloud cost discovery and optimization platform which is also a part of Google’s Cloud Launcher. With reOptimize it’s easy to answer questions such as:
One of the reOptimize’ most popular features is ability to produce custom reports. Reports are very similar to pivot tables which you are probably familiar with from working with spreadsheets such as Google Spreadsheets or Microsoft Excel.
Recently, reOptimize has introduced a “Cost Labels”, a convenient way to enrich reports with label information. If you label your resources with something like “env:production” or “app:backend”, you can filter or group your billing reports using these labels.
As you remember, Iris automatically adds to each resource the `iris_name`, `iris_region` and `iris_zone` labels. Once these labels are populated and propagated to Google’s billing data, you can configure these labels as “Cost Labels” and produce location aware reports:
You can even track cost of individual instances, buckets or BigQuery datasets/tables using the iris_name label:
There is basically no limit in how you can analyze your Google Cloud costs. Another example can be break-down by zone and instance type over time:
Cost Labels is the not the only great feature of reOptimize. You can create “Cost Allocations” which are intersections of Service, SKU, Project and Labels. With cost allocations it’s easy to track costs of your complete applications or services, environments and so on.
As great example is tracking network egress traffic using the following Cost Allocation:
Based on cost allocations, you can configure smart budgets or setup users with limited view to track only parts of your infrastructure.
We are actively looking for contributors to help us improve our open-source stack of tools for Google Cloud such as Shamash, Iris, Zorya, kubeIP and few others.
Want more stories? Check our blog on Medium, or follow Vadim on Twitter.
Software & Operation Engineering. Written By Engineers.
323 
Your monthly dose of Google Cloud and Amazon Web Services articles  Take a look.
323 claps
323 
Written by
CTO of world’s best cloud consultancy
Here, our SRE and SWE teams share the solutions to some of the most interesting challenges we encounter during our daily work at DoiT International. If you like what we do, check out our https://careers.doit-intl.com page and join the team!
Written by
CTO of world’s best cloud consultancy
Here, our SRE and SWE teams share the solutions to some of the most interesting challenges we encounter during our daily work at DoiT International. If you like what we do, check out our https://careers.doit-intl.com page and join the team!
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/this-week-in-google-cloud-platform-cloud-tpus-gpus-on-gke-96-vcpu-and-a-serverless-landscape-2ec95d726e01?source=search_post---------310,"There are currently no responses for this story.
Be the first to respond.
This past week, GCP has unleashed key performance enablers, some will say game-changers :
Staying with Kubernetes, Pivotal Container Service has recently reached GA (pivotal.io). What make it special for GCP is its constant compatibility with GKE.
CNCF is thinking about serverless computing (via its Working Group) and has released a whitepaper and a landscape chart.
“In our genes: How Google Cloud helps the Broad Institute slash the cost of research” (Google blog). You can now run the germline GATK Best Practices Pipeline for $5 per genome (down from $45). 76k genomes processed to date and 36PB of data stored on GCP.
From our “In Case You Missed It (ICYMI)” department :
This week’s GCP Podcast (#114) is about “Machine Learning Bias and Fairness” with Timnit Gebru and Margaret Mitchell (gcppodcast.com)
This week‘s picture is CNCF’s Serverless Landscape :
That’s it for this week!-Alexis
Google Cloud community articles and blogs
115 
115 claps
115 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Google Cloud Developer Relations
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://rominirani.com/google-cloud-functions-tutorial-using-gcloud-tool-ccf3127fdf1a?source=search_post---------311,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
This is part of a Google Cloud Functions Tutorial Series. Check out the series for all the articles.
In the series so far, we have been using Google Cloud Console to deploy, test and manage Cloud Functions. If you are a command-line person or would like to script out the management of Cloud Functions, the gcloud SDK is your ideal tool.
The gCloud SDK is a set of tools that you can install locally that helps you manage various services on the Google Cloud Platform. Google Cloud Functions can be managed via the gcloud functions command.
Before you run any of these commands, please keep in mind that gCloud tool should be configured to run against your project id. In an earlier post, we had seen how to setup your local environment, where we mentioned how to setup a project against which to run your commands. To reiterate:
Do a login first via:
Finally, set the project id via the following command. Please use your project id instead of the PROJECT_ID value below.
Finally, validate that you are logged in with your project and the project id is also correctly setup via the following command:
Check the accountand projectvalues and ensure that they were set correctly.
Assuming that you have the gCloud SDK tools installed on your machine, let us first take a look at the first group of commands, especially the event-types and regions group:
Google Cloud Functions is currently supported is multiple GCP regions. If you run the following command, you get that response as shown below:
In a similar way, we can look at the current Events Providers and Types that are currently supported.
Finally, you can view all the current Stackdriver logs for your Cloud Functions via the functions below:
We will take a look at more parameters that we can pass to the logs read command in order to view logs specific to a function, number of log statements and so on.
Let us take a look at deploying and then invoking a Google Cloud Function using the gcloud utility. The Commands that we shall use are deploy and call as shown below from the documentation.
The deploy command for a HTTP Trigger function is shown below:
The above diagram shows a sample HTTP Trigger based Cloud Function. Note that the name of the function is the function name that we are exporting. Similarly the Trigger name is --trigger-http. Note that this is not the only way and there are ways to specify where the index.js file is and which function to choose, etc. Refer to --help for more details via the gcloud functions deploy --help option.
There are a couple of other parameters that we need to provide to this command.
For now, let us go to the root folder into which you have cloned the Github repository for this tutorial series.
Go to the helloworld-http folder by giving the following command:
$ cd helloworld-http
We will now deploy our function as shown below:
Now that the function is deployed, you can check the same via the following command :
We can get the details for any function by using the gcloud functions describe <function_name> command as shown below:
This will get you all information about the function like name, trigger, url, serviceaccount, version, etc.
To invoke the function, we will use the gcloud functions call method as shown below:
Notice that we were provided a unique executionId and the result from our code i.e. “Hello World!”
We can now view the specific logs for our function from Stackdriver logging. Instead of using just logs read which would give us logs across several functions, we can specify the execution id to view the logs specific to that function execution.
The command is shown below (Notice that we provided the same execution id value that we received as part of the call command execution.
In the Cloud Function that we tested above, the code was straightforward and it did not bother about the data that was passed in the request parameter, since it was simply printing out a “Hello World” message.
What if you had to pass data to the function in the request and wanted to the use the gcloud functions call command? To do that, let us look at the sample function as explained below.
For now, let us go to the root folder into which you have cloned the Github repository for this tutorial series.
Go to the hellogreeting-http folder by giving the following command:
$ cd hellogreeting-http
The index.js file has the code as shown below:
First up, we will now deploy our function as shown below:
Once the function is deployed, we can invoke it with the data as given below:
Similarly, assuming that we have the direct HTTPs URL for the function, we can use curl utility too as given below:
Replace the PROJECT_ID above with your Google Cloud Platform project id value.
We can use the gcloud beta functions deploy command to deploy a Pub/Sub based Cloud Function as given below:
Earlier we had deployed our first Cloud Pub/Sub based function pubsubfunction1 earlier in the series. Assuming that we have this function deployed on the topic pubsubtopic1 , we can invoke it as given below:
But what if we could use the gcloud pubsub command itself to test out the function. Sure, we can !
First up, we can list down the Pub/Sub topics as given below:
The next thing is to publish a message to this topic as given below:
We can now get the logs from our function execution as shown below:
We can use the gcloud beta functions deploy command to deploy a Google Cloud Storage based Cloud Function as given below:
Assuming that we have the Google Cloud Storage based function deployed , we can use the gsutil utility to upload a file to the specific bucket that we are monitoring and then view the function logs to ensure that the Cloud function got executed.
First up, let us list down the buckets via the command shown below:
You should see at a minimum the bucket name as shown above. This was the bucket that we had created for our GCS based function.
Now, let us copy a sample file package.json to the above bucket via the command shown below:
We can then inspect the function execution log as shown below:
This completes our use of the gcloud functions command/group to deploy, manage and invoke our functions. There is a command delete too to delete a Cloud Function as needed.
Proceed to the next part : Local Functions Emulator or go back to the Google Cloud Functions Tutorial Home Page.
Technical Tutorials, APIs, Cloud, Books and more.
103 
1
103 claps
103 
1
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
Technical Tutorials, APIs, Cloud, Books and more.
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
Technical Tutorials, APIs, Cloud, Books and more.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@DazWilkin/google-cloud-sql-4-ways-spring-a4f13ad32714?source=search_post---------312,"Sign in
There are currently no responses for this story.
Be the first to respond.
Daz Wilkin
Aug 21, 2017·5 min read
Yesterday, I wrote of my investigations using Google Cloud SQL from Golang. Today, I’ve a summary using Spring. I’m going to optimistically state that there are 4 ways to do this. Truthfully, as I start writing, I have only tried 2 of these! ;-)
Please refer to yesterday’s post for the steps to:
Spring provides a straightforward “Accessing Relational Data using JDCB with Spring” tutorial. We’ll use this as the basis for the 4 ways to connect to Cloud SQl databases with one very minor (!) code change. The tutorial uses H2 and it’s syntax differs from the syntax used by MySQL and by PostgreSQL MySQL and PostgreSQL both require Application.java line #32 to be:
I recommend you clone the Spring sample and run it per the instructions. I’m going to use Maven in this post:
The console output should include:
Google provides plugins to facilitate using Google Cloud Platform services from within Spring. Google Cloud SQL is now supported. The artifacts are not yet available in the central Maven repository. So, the first step is to clone Google’s repo, build and install the artifacts locally:
All being well, this should conclude:
We will reference the “Spring Cloud GCP Cloud SQL Starter” artifact below.
I recommend you comment out the database dependencies that aren’t being used. This minimizes confusion when swapping between databases. Comment out the H2 dependency in pom.xml:
MySQL uses Prepared Statements with “?” denoting parameters. Please ensure you use this statement for Application.java line #45:
You may reference the “Spring Cloud GCP Cloud SQL Starter” from your project pom.xml:
Create a file called “application.properties” in src/main/resources”. The “resources” directory must be at the same level as the “java” directory. The content of the file should be as follows. NB in this case, you refer to the Cloud SQL database by the Instance ID (${ROOT}) not the connection name (${INSTANCE}):
Run your code!
When using the Cloud SQL Proxy as a “companion process”, your code uses a regular MySQL driver and treats the Cloud SQL MySQL instance as if it were running on localhost. To use the MySQL JDBC driver, add the following dependency to the pom.xml:
Create a file called “application.properties” in src/main/resources”. The “resources” directory must be at the same level as the “java” directory. The content of the file should be:
Please replace the ${VARIABLE} values using the constants for your database name, user and password.
Now, run the Cloud SQL Proxy configured for MySQL via TCP sockets:
Run your code!
PostgreSQL uses Prepared Statements with “$” prefixing each parameter. Please ensure you use this statement for Application.java line #45:
You may reference the “Spring Cloud GCP Cloud SQL Starter” from your project pom.xml:
Additionally, we must reference the PostgreSQL JDBC Driver and Google Cloud SQL Socket Factory for JDBC Driver (for PostgreSQL):
Create a file called “application.properties” in src/main/resources”. The “resources” directory must be at the same level as the “java” directory. The content of the file should be as follows:
Run your code!
When using the Cloud SQL Proxy as a “companion process”, your code uses a regular PostgreSQL driver and treats the Cloud SQL PostgreSQL instance as if it were running on localhost. To use the PostgreSQL JDBC driver, add the following dependency to the pom.xml:
Create a file called “application.properties” in src/main/resources”. The “resources” directory must be at the same level as the “java” directory. The content of file should be:
You do not need to remove the properties for the other database drivers. These will be ignored when you comment out the driver references.
Now run the Cloud SQL Proxy configured for PostgreSQL via TCP sockets:
Run your code!
We demonstrated 4 ways to connect (Java) Spring code to Google Cloud SQL.
If you’d prefer to also run the Java code from within a Docker container, you can use the Maven image (JDK 8). You must use a service account instead of Application Default Credentials and the service account must be granted the role “Cloud SQL > Cloud SQL Client”.
To facilitate using your editor of choice etc., I recommend you create the Spring example on your localhost and map this directory into the container.
Then, within the container:
If you would prefer, you may also run the Cloud SQL Proxy as a Docker container. However, you must also use a service account instead of Application Default Credentials and the service account must be granted the role “Cloud SQL > Cloud SQL Client”.
Here’s the format for MySQL:
Here’s the format for PostgreSQL:
109 
4
No rights reserved
 by the author.
109 
109 
4
"
https://medium.com/@duhroach/google-cloud-storage-performance-4cfcec8bad72?source=search_post---------313,"Sign in
There are currently no responses for this story.
Be the first to respond.
Colt McAnlis
Oct 5, 2017·4 min read
Google cloud storage is listed as a “a unified object storage solution” … or in layman’s terms, it’s a place in the cloud to host and serve files.
The uses for GCS are pretty large. For example, some people host their static websites there, some ship game content from there, others have used it to power their back-up services, and let’s not forget, it can host and serve content just like a CDN.
That’s all fine and dandy, but what I’m concerned with is : how would you test performance of it?
As we looked at in previous posts, it’s really easy to just throw a curl command around to see how GCS performs when fetching an asset.
First, place an asset on a server, say, in North America, (maybe near a BBQ restaurant) and then fetch that asset from a bunch of machines located around the world.
for ((i=0;i<500;i++)); do (time curl http://examplecdn.cdn/asset.png) 2>&1; done | grep real
The graph below shows our asset being fetched from a machine in Asia, Australia and Europe.
Note : Caching was not enabled for these assets, so their numbers are higher than what you’d see from the CDN article. AND this bucket was intentionally set to Regional (instead of multiregional) We can talk about the perf differences, but that a topic for another article.
Now this bucket was set up to be Regional, which means it doesn’t scale properly to being accessed across regions, so you can expect that the farther away the request is, the slower the response can be.
But we’ve seen read performance be really strong with GCS in the past; Write performance, however, is much trickier to get correct. In some following articles, I’ll highlight a few gotchas that some developer friends ran into, but for now, let’s just upload the same file, under different names, to the US-CENTRAL-1 bucket from various places in the world.
Now, if you run this test yourself, be careful about how you do your timings. There’s two main ways to do upload data to GCS: via `gsutil cp` and via a language-specific API. I’ve charted the performance of both below.
Note : These tests were done on a bucket was intentionally set to Regional (instead of multiregional) We can talk about the perf differences, but that’s another article.
We can see that write performance, for a 600k file is really good over time, however we see a significant difference between the GSUTIL and Python versions. The reason for this is that our scripting process for GSUTIL will cause a new process to be created for each uploaded asset, which isn’t ideal in terms of processor usage, and may not directly be GSUTIL’s fault.
Note : There’s also a multithreaded upload flag, but we’ll talk about that in a future article.
Direct read/write performance is easy to test. But in honesty, there’s lots of variables here that influence performance, that’s a pain to write proper benchmarking for. As we will talk about later, performance can greatly vary depending on the size of your objects, or if you’re creating, deleting, uploading or downloading. Thankfully, you don’t have to write all those tests yourself. The gsutil perfdiag command runs a suite of diagnostic tests for a given Google Storage bucket:
Simply running `gsutil perfdiag -o test.json gs://<your bucket name>` can get you something that looks like this :
The perfdiag command is provided so that customers can run a known measurement suite when troubleshooting performance problems.
Now that we have a clear picture of the proper way to test GCS read/write performance, we can get on with helping some of our developers and their performance problems. Stay tuned to Google Cloud Performance Atlas (YT Video, Medium Blog), and subscribe / follow to get more great content!
DA @ Google; http://goo.gl/bbPefY | http://goo.gl/xZ4fE7 | https://goo.gl/RGsQlF | http://goo.gl/4ZJkY1 | http://goo.gl/qR5WI1
See all (78)
103 
103 claps
103 
DA @ Google; http://goo.gl/bbPefY | http://goo.gl/xZ4fE7 | https://goo.gl/RGsQlF | http://goo.gl/4ZJkY1 | http://goo.gl/qR5WI1
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/use-google-cloud-user-credentials-when-testing-containers-locally-acb57cd4e4da?source=search_post---------314,"There are currently no responses for this story.
Be the first to respond.
Container packaging is very popular today: it allows a full customization of the execution environment and is language agnostic. More and more applications use it. So now, to validate the production environment behavior, the developers need to test the containers, not only the workload with unit tests.
In some cases, the containerized app can required to access to Google Cloud API and thus, to be authenticated. When deployed on Google Cloud services, metadata server is reachable and provides the Application Default Credentials (ADC).
How to get authenticated locally with ADC for testing?
By definition, the container run in an isolated environment. That means that your local configuration isn’t known from inside the container, and thus your credentials aren’t loaded
If you go on Google search with this query google cloud container test authentication, the first link leads you to Cloud Run local test tutorial. It’s a great tutorial which explains how to load credentials in the container runtime environment for local tests.
But the suggested JSON file to use is a service account key file. It must be generated and stored locally!
This solution isn’t safe enough and, as described in my previous article, I would like to avoid service account key files. Furthermore, it’s a great question of Eran Chetzroni which has inspired this article.
When you code your app on your local environment, you use Google auth libraries according with your prefered languages. This library can be used directly in your code, or it can be used as dependency in service specific libraries, such as Cloud Storage client library.
The Google auth library tries to get a valid credentials by performing checks in this order
The “well-known” locations are
To get your default user credentials on your local environment, you have to use the gcloud SDK. You have 2 commands to get authentication:
Both commands trigger an OAuth2 authentication flow, synchronous or asynchronous, and store a refresh token locally.
Now, we have the 2 pieces of the puzzle
Therefore, you have to run your local docker run command like this
Here again, an easy solution exists to prevent the use of service account key files on a local environment.
However, even if this solution is great, don’t be too inspired for using it outside of your local environment. The JSON files, your user credentials JSON file or the service account key file, are “secrets” and need to be handled as secrets.
Thus, never add these JSON files into your container, especially if it’s public. A container is simply a packaging mode, it encrypts/hides nothing!
Don’t use your user credentials in another environment than your local and your own environment (such as production, staging,…). The API access will be perform on your behalf (and logged as-is), with your authorizations. And, because it’s a user credential, you can be blocked by the 2 IAM limits.
So, as always, when playing with security, think wisely!
Google Cloud community articles and blogs
104 
104 claps
104 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
GDE Google Cloud Platform, scrum master, speaker, writer and polyglot developer, Google Cloud platform 3x certified, serverless addict and Go fan.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/@DazWilkin/google-cloud-iap-and-gke-c773da56c3cf?source=search_post---------315,"Sign in
There are currently no responses for this story.
Be the first to respond.
Daz Wilkin
May 1, 2017·7 min read
2018–12–18 Update: Native Kubernetes support for IAP. My colleagues in the Kubernetes team have worked diligently to make this a better-integrated solution. Please see their document as the definitive way to do this: Enabling Cloud IAP for GKE.
Google has released a Beta of Cloud Identity-Aware Proxy (Cloud IAP). This post provides an overview of using Cloud IAP to help secure a Container Engine (GKE) service.
Ensure you are able to access your GKE cluster using kubectl. I recommend the increasingly awesome Kube UI too which is easily accessible after kubectl proxying:
And, if you’d like to use the Kube UI, perhaps:
Then:
We will generate a self-signed certificate using OpenSSL and upload this as a GKE Secret. The secret will then be used by GKE to encrypt traffic to the L7LB when it is created. This provides us the necessary HTTPS endpoint for Cloud IAP.
This will result in tls.crt and tls.key in the working directory. These are then uploaded to GKE as a Secret named “cloudiap” with the following command. Don’t forget the final “-”:
If you’re using Kube UI, you can view the Secret in the UI:
From the command-line:
Google provides a code-sample for Cloud IAP. The sample provides a function that validates the JWT that Cloud IAP passes our application. Auth0 provides a good overview of JWT.
github.com
We will extend the Google validate_jwt.py by adding the Flask web framework and defining a trivial web server that will reject unauthorised users and will accept authorised users. For authorised users, the site will display the Google account ID, email address and provide a copy of the JWT token. The JWT token is returned entirely to help understanding the process and should not be provided otherwise.
validate_jwt.py:
requirements.txt:
Dockerfile:
Now we can build the Docker image locally, tag it, upload it to GCP and deploy it to the GKE cluster. If you make changes to the code, you should delete the GKE Deployment, rebuild the Docker image etc. You do *not* need to delete the other GKE resources that we will create.
I recommend you version your Docker images. You can, of course number incrementally. Or you can use the hours and minutes from your current time:
Then “build”, “tag” and “push”:
You may view the images and tags in the GCP (!) Console:
Or using the Cloud SDK (not kubectl):
And:
Everything is now in place and we are able to deploy the containers to GKE, expose the solution as a Service and then create an Ingress that will create a L7LB.
The easiest way to deploy our ‘cloudiap’ container to GKE is:
Then:
Or:
Now, we’ll expose the Deployment as a Service via a NodePort. NodePorts are ports on all the cluster’s nodes (i.e the GCE VM) that provide access to the deployed service:
Then:
The result should be similar to:
Lastly, we’ll create an Ingress resource named “cloudiap” that creates an L7LB in front of the service named “cloudiap” using the Secret named “cloudiap”. Don’t forget the final “-”:
Then:
Initially, this page may not show an IP under Endpoints but, if you wait a minute or two, it will. The IP address refers to the L7LB. Where’s that?
console.cloud.google.com
For convenience, I’m using Cloud DNS to add an A(ddress) type record to the DNS record. You may use your preferred Domain management tool. You will need to add a record to provide a DNS name for the L7LB. I’ve used “iap” as the machine name and the IP from the L7LB in the previous screen.
After making the change, it may take some time to propagate to the DNS cache used by e.g. Chrome. You can check your machine’s DNS cache with nslookup. Eventually, it should resolve to the IP address of the L7LB:
You may check Chrome’s DNS cache entries with the following command. It is possible to flush these too:
Cloud IAP uses OAuth to authenticate users. Navigate to the OAuth Consent screen and, at a minimum, add a product name. Save the change.
The last step is to configured and enable Cloud IAP.
Navigate to:
All being well, the page should list the backend service created for the L7LB by the GKE Ingress resource. You can double-check this with:
NB The backend service references the NodePort that was created when we exposed the “cloudiap” service. In this case 31009.
Click the slider to enable Cloud IAP and, when prompted, add the DNS A name that you created:
By default, no-one is permitted to access the service through Cloud IAP. Click “Add” in the upper right-hand corner of the Cloud IAP and add, for example, your own Google credentials. Then navigate to the service by the DNS name e.g. iap.your-domain.com as both a permitted user and as a non-authorised user. For the permitted user, you should see an ID, your email and the JTW token.
If you’d like to decode the token, copy and paste it into jwt.io:
Using an existing GKE cluster, you’ve deployed a simple Python web app that expects a JWT token and only permits authorized users. The Python web app was packaged as a Docker image, deployed to GKE as a Deployment, exposed as a Service, and Load-balancing was configured by an Ingress and secured by a Secret. Cloud IAP secured the endpoint by intercepting traffic from the LB and only permitting authorized users.
The easiest way to tear everything down is to delete the GCP project:
If you’d prefer to just delete the resource created by the project, disable Cloud IAP then:
40 
3
No rights reserved
 by the author.
40 
40 
3
"
https://rominirani.com/creating-google-cloud-shell-tutorials-a1774cc4eb93?source=search_post---------316,"I love creating developer tutorials. A majority of the time, I end up writing them as blog posts (fairly detailed ones) and I do my best to not miss a step and give screenshots of what to expect along the way.
Given the pace at which new versions of software are being released, I often play a losing game when it comes to keeping my material updated. Is there a better experience that I can give my readers that is cognizant of the fact that they would simply like to get going quickly, have the code ready, just see it all work and it has just about enough instructions and theory. One approach can be creating videos but I find those difficult to update too.
A recently released feature on Google Cloud Platform allows you to create a tutorial that runs inside Google Cloud Shell. This definitely opens up multiple possibilities and all I need to do to keep this tutorial updated is probably minimal text changes and code updates. I think it has very good potential and I am excited about it.
This blog post will be about how to go about creating tutorials that provide a seamless experience to the user in terms of packaging all the code necessary to run your tutorial, provision that on the Cloud Shell and provide step by step instructions that helps the reader follow along, in true tutorial style.
Let us first look at what we are trying to do here. That will make things much easier to understand when it comes to writing a tutorial similar to the one that you will shortly see.
The only prerequisites to running this tutorial is to have a GCP Account.
Please visit the following URL:
When you visit the above page, you will be led to the README.md file, a snippet of which is shown below:
When you visit the above page, you will be led to the README.md file, a snippet of which is shown below:
I have tried to demonstrate how to write a tutorial that runs in Google Cloud Shell and teaches you about the Google Cloud Natural Language API via an experience that looks like a codelab but goes a step further setting up all the code for you in Cloud Shell for you to get running quickly.
You do not need to run the whole tutorial, just a few steps will do to give you a good idea of what’s going on.
Click on the blue button titled “Open in Google Cloud Shell”. What happens next is magical.
It uses your Google Cloud Platform account and launches Google Cloud Shell and asks you for permission to clone the Git Repo as shown below. Click on PROCEED.
It then moves forward to setting up a Workspace for you as shown below.
If everything goes well, you will have a Cloud Shell session that looks like this:
Give it a try and go through the next few steps and if you actually try them out in the terminal session, you will even learn about the Cloud Natural Language API :-)
Before we dive into the tutorial creation specifics, let’s remind ourselves of a quote.
No battle was ever won according to plan, but no battle was ever won without one — Dwight D. Eisenhower
I cannot stress enough the importance of planning out your tutorial. Even if you have never created a step by step tutorial before, put down a rough set of steps. For e.g. Each of the numbers below could be a step in the tutorial.
Hopefully you get the picture. What works for me is to actually put down the above points as sections and then fill them out accordingly with text, source code, instructions to build/run and so on. These are just my recommendations but you should go with your flow and how you would like to write the tutorial.
The tutorial that you shall create will go into a single (CommonMark) Markdown syntax file. Let us call this file tutorial.md. The tutorial can be thought of conceptually as having the following:
A sample tutorial.md file is shown below:
The syntax supports the H1 (#) , H2 (##) and H3 (###) tags for notifying the title of the tutorial, the steps in the tutorial and the substeps respectively. That’s all there is to it. There can only be one title tag (#) in the entire tutorial.
Now that you have the outline in place, all you need to do is fill out the details in each of the steps and substeps. Note that the title tags i.e. ## and ## will determine the title i.e. label of the steps and what follows them is what will be displayed on the that particular step page in tutorial.
It is obvious that we can put in as much text as we want. But what are the other elements that it supports. The tool creators have thought about it carefully and have come up with initial support for multiple custom elements that will keep the user experience in mind i.e. make it easier for the reader to click at certain points on the screen (icons) by highlighting them, opening up files in the editor and so on.
Let us discuss a few of these elements:
Let us take a particular step from my complete tutorial.md file. A part of the rendered tutorial step is shown below:
You should now be able to easily predict that it consists of :
The code snippet to support that is shown below:
The Title for the Step is via the H2 (##) tag. The text is standard stuff and the way you include any code block is via the ```bash ….. ``` element as shown above.
The great part about this is that it is rendered as shown below with a Copy icon at the extreme right corner. The user can click on this, which copies the command to the clipboard and you can paste that into the Shell terminal session easily. Little things like this make a big difference.
Note: You can have as many code blocks in a step as you like.
This is an interesting element which helps you to open the file directly in the Orion editor that is integrated into the Cloud Shell Workspace. Take a look at a particular step in the tutorial as shown below:
The code snippet for this is shown below:
Pay attention to the walkthrough command, the format is which is shown below:
All you need to do is provide the entire path to the file that you want to open in the editor, in our case the index.js file. And give a text label that will be rendered in the Tutorial Step. If the user clicks on it, the editor will open up with the file as shown below in the left pane.
This makes it easy for the user to focus on the tutorial and not get distracted with navigating to some folder, etc. The experience comes into play again.
There are other custom elements supported too like Spotlight, indicating a Cloud Shell icon, etc. Please refer to the documentation. These are valuable elements to support a great experience for the reader.
There are multiple ways to launching the tutorial, but I prefer the following:
4. You can add the button as follows:
[![Open in Cloud Shell](http://gstatic.com/cloudssh/images/open-btn.png)](https://console.cloud.google.com/cloudshell/open?git_repo=YOUR_REPO_URL&tutorial=YOUR TUTORIAL FILE NAME)
You just need to provide two variables:
The tutorial parameter is a hint to the Cloud Shell that once it has setup the repository, it needs to start the tutorial present at tutorial.md. This will then open up the first step of the tutorial in the right pane.
If you have never seen Google Codelabs, please do so. There are tons of tutorials to get your familiar with various GCP services. Do it now!
2. The folder structure that you see in my github repo is not the one to be strictly followed. You can organize your folders as you see fit. Just make sure that you provide the correct path when using custom elements to open the file in the editor and also for referencing your tutorial markdown file. I struggled a bit with that.
3. If you see the official documentation, you are asked to just use ``` … ``` for the code block. But I found that unless I use ```bash … ```, it did not render as expected.
4. There are a few hidden walkthrough commands like walkthrough conclusion-trophy which are not documented. It presents a nice trophy to you when you complete the stuff.
5. The experience and capabilities of the tutorials are centered around Cloud Shell. So your imagination is limited by what you can do with the Cloud shell. For e.g. I wanted to even lead the user to Cloud Console and automate some screens over there, but I believe I am crossing the line here.
6. You will notice that there are buttons to sending feedback for the tutorial. This includes a rating for your tutorial and a description by the user in case there are issues. I am not sure where the feedback is sent.
That brings this tutorial about creating tutorials to an end. Enjoy creating your tutorials for Cloud Shell !
Let a thousand tutorials bloom.
Technical Tutorials, APIs, Cloud, Books and more.
49 
49 claps
49 
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
Technical Tutorials, APIs, Cloud, Books and more.
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
Technical Tutorials, APIs, Cloud, Books and more.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/setting-up-saml-for-google-cloud-cloud-identity-for-customers-and-partners-cicp-8f27cff61ab1?source=search_post---------317,"There are currently no responses for this story.
Be the first to respond.
A little under a decade ago I wrote my first SAML IdP for the Google Search Appliance (yeah, that wonderful yellow box!). Since then, that script changed many hands and I’ve resued and adapted as an IDP for Google GSuites SSO and now finally in this article for Google Cloud.
This article circles back on that script but extends it for the Firebase Authentcation enchancements recently integrated into Google Cloud called Cloud Identity for Customers and Partners. This article is basically a picture book on how to setup your own SAML IdP and perform SSO with a Service Provider (SP) that is based off of the Firebase SDK or ‘roll your own’ Service Provider that hosts the SAML endpoint.
This is a simple ‘how to get started’ tutorial and ofcourse do not use any script contained here in production/qa,anywhere outside of yoru laptop! Also please note i’ve included public/private and API keys here. You really should regenerate the public/private keys (see Appendix).
As mentioned, there are two ways you can use this script with the same IdP:
Note: at the time of writing (11/21), you need to manually alter the SAMLConfig ACS endpoint via API. Eventually you'll be able to just edit the setting via the console...Also, I do not know much javascript so the example provided is not optimal.
You can find the full source here:
github.com
As this is just a tutorial to help you understand, we will setup the IdP and SP all locally on your workstation. The IdP used is common across both scenarios described above (as it should be) but the SP is different since one uses the Firebase SDK for CICP and the other manages the ACS. Switching between the two requires you to update the CICP ACS url (more on this later).
Since we’re running everything locally, override the /etc/hosts file
(in this example, the project I use in the example is called cicp-project)
Enable the CICP API
Setup Providers
We will setup Google OIDC too just for fun
click create on SAML Enter the following:
Under Settings
Users
Providers
Now start the commond IdP: Start SAML IDP
You might be wondering why you’re running some image called salrashid123/appssso...thats just a prebaked image with the python xmlsec libraries build in. You are free to recreate that image...here is the repo and Dockerfile
First lets start the Firebase SDK based SP:
Open Incognito window goto: https://sp.providerdomain.com:38080/
First try Login with google account (in my example, i used user@esodemoapp2.com)
If everything goes well, you’ll get redirect and see the current loged in user at the footer of the page
In a new incognito window and goto https://sp.providerdomain.com:38080/
Click Login with SAML
You’ll get redirected to SAML Login page on the IdP:
Enter (any username/password (this is because i don’t do any validation on the idP; this is just a test!)
I entered: user1@idpdomain.com
You’ll see a temp page with the SAML Response (normally, your users don’t see this and i added in this pause just to show you the decoded SAML POST data)
Once you click continue, you’ll see a login success screen on the IDP with the users credentials displayed
Note two users is provisioned per each login and the Identity Provider :
In this scenario, we will run our own IDP.
First step is to stop the SP from the previous step (SP Scenario: FirebaseSDK) if you are still running it.
Then use the API key, projectID and saml_provider for your setup and start the SP:
Ok, you still can’t run the full flow yet because we need to modify the ACS URL back to our own provider. By default, the ACSis
""callbackUri"": ""https://cicp-project.firebaseapp.com/__/auth/handler""
but we need it to point to our endpoi
""callbackUri"": ""https://sp.providerdomain.com:38080/acs""
Plese see the Appendix section on how to run through these steps.
Once youve done it, you can access the SP at: https://sp.providerdomain.com:38080/
At that point, you’ll get redirected once clicking on the button to same IDP as earlier.
Login and you’ll go back to the
Note the saml2p:Response Destination= value
By default when you setup CICP with FirebaseSDK, your ACS redirects you to a domain like ""https://cicp-project.firebaseapp.com/__/auth/handler
However, you may want to setup a custom domain hosted on firebase. If you set up a custom hosted domain, firebase will automatically manage the redirect back to your app. THis works as long as you use the firebaseSDK for the SP.
To set this up, first setup “Firebase Hosting” and setup a custom domain. For me, it was sp.esodemoapp2.com (you ofcourse need to own the domain and edit the DNS settings, proper (the /etc/hosts trick wont work)). Once you set that up and provide the IPs they specify (wait maybe 1hr)
Then edit the javascript in sp_firebase/layout.html to add on the auth domain you have
You also need to update SAML CallbackURI manually as described in the appendix to https://sp.esodemoapp2.com/__/auth/handler.
Once all that is done, if you use the firebaseSDK scenario, the IDP will redirect you back to the domain above which intun will send you back to your site with an initialized firebase user
Thats all folks.
The following procedure details how to update the SAMLConfig manually via API. At the moment, the UI does not allow you to modify the SAML ACS callback url so we need to update it via the API.
Then get the key and activate it:
Your file should look something like this:
update_config.json
Google Cloud community articles and blogs
177 
6
177 claps
177 
6
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/javarevisited/7-free-google-cloud-devops-engineer-certification-courses-f0046ac39f7e?source=search_post---------318,"There are currently no responses for this story.
Be the first to respond.
Hello guys, if you are preparing for Google Cloud Professional DevOps Engineer certification and looking for free online courses to learn DevOps and Cloud Computing then you have come to the right place.
In the past, I have shared the best Google Cloud courses, as well as the best online training courses to pass cloud engineer, data engineer, and cloud architect certifications, but my readers kept asking for free online courses on Google Cloud, particularly for DevOps engineer certification and today, I am going to share free GCP Cloud DevOps Engineer certification courses for both beginners and experienced cloud professionals.
Google is one of the leading cloud service providers; besides this, its cloud platform, i.e. Google Cloud Platform (GCP) is one of the fastest-growing cloud platforms. It is expected that in the coming years, it will outperform Microsoft Azure.
First of all, you made an excellent decision to appear in GCP Professional Cloud DevOps Engineer Exam; cloud computing is a vital part of the IT industry. Every company irrespective of its field is somewhere using cloud platforms. In the coming years, the cloud industry will become one of the largest industries.
Getting a certification in GCP could give you ample opportunities to have a successful career in cloud computing. To help you with your preparation for the GCP Professional Cloud DevOps engineer exam, I have listed seven free online courses that you can use to learn both cloud computing and DevOps and pass the GCP Professional Cloud DevOps Engineer.
Btw, If you need a comprehensive online course then I also highly recommend you to join Ultimate Google Cloud Certifications: All in one Bundle (4) course on Udemy. This online training course is not free but the most comprehensive online courses for Google cloud certifications and provide study material for all four Google Cloud certifications including Cloud Data Engineer.
udemy.com
Without wasting any more of your time, here is the list of free online courses to pass the Google Cloud Professional Cloud DevOps Engineer exam in 2021. These free courses have been created by experts and thousands of professionals have joined these free online courses. They are also offered from online learning portals like Coursera, Udemy, and other popular websites.
This is going to be our first step in preparation for the GCP Professional Cloud DevOps exam. In this course, the instructor Xavier Corbett will teach you every fundamental related to cloud computing.
This course has excellent visuals, and appropriate illustrations and these two make the understanding process quite easy for students.
In this 1-hour video course, you’ll learn about the followings:
If you are looking for something that can help you build a sturdy base for your career, then this course is the best pick for you. From here, you can pick any cloud topic you want.
Here is the link to join this free course — Introduction to Cloud Computing
So far, you have learned about cloud computing concepts; now, since you are appearing in the GCP Professional Cloud DevOps Engineer exam, you need to know about DevOps.
This course will teach you every important DevOps concept. It’s a short and simple course that directly focuses on DevOps and its fundamentals.
This course is 2 hours long and Linux academy has created it. So far, over 60 thousand students have enrolled in this course, and it has a rating of 4.4 based on the reviews of 7019 students.
If you lack DevOps knowledge, then this course is the best pick for you. It has well-explaining videos and illustrations.
Here is the link to join this free DevOps course — DevOps Essentials
So far, you have learned about Cloud computing concepts and DevOps concepts; now it’s time to move towards the platform of the topic. In this course, you’ll learn about different concepts of GCP.
This course is created by Dhanaji Mausale and Google Cloud Platform Gurus; it’s a 6-hour long video course. So far, over 39 thousand students have enrolled in this course. This course has a rating of 4.2.
Apart from the basic fundamentals of GCP, the instructors will also teach you the trick to get $300 credit in your Google cloud account. So, if you are looking for some course that can give you a live project and implementation experience, then this course is for you.
Here is the link to join this free course — GCP — Google Cloud Platform Concepts
If you are interested in directly starting your preparation with a single course, then this particular course is for you. It is a 16 hour 20 minutes long video course that will teach you everything related to cloud computing and GCP. Besides this, the instructors’ Google Cloud Platform Gurus have used some great visuals to convey the information smoothly.
In this course, over 72 thousand students have enrolled so far, and it has a rating of 4.5 based on the reviews of 1,429 students. In this course, you’ll learn about different components of GCP like Security, APIs, Deployment Manager, Cloud Repository, and several other developer tools.
Apart from these topics, the instructor will also teach you how to prepare for GCP certification exams quickly and efficiently.
Here is the link to join this course —Ultimate Google Certified Professional Cloud Developer 2021
If you are looking for a very detailed and informational course then, this is the best option for you. Google Cloud Training Team created this course, and it is available on Coursera.
This course contains five different sub courses, and each of these five courses covers specific topics. So far over 37 thousand students have enrolled in this 51-hour long course. To complete this course, you’ll have four months.
Since it is a Coursera course thus, you’ll be assigned ample quizzes, live projects that you need to submit, and once your instructor evaluates them, you can move to the next section of the course. Without completing these assignments, you won’t be able to complete this course.
Here is the link to join this course — Cloud Engineering with Google Cloud Professional Certificate
This is another free udemy course you can join to learn DevOps, Kubernetes, and Terraform. This is not related to Google Cloud but will give you knowledge of essential DevOps tools like Kubernetes and Terraform.
Kubernetes as the orchestration platform for not only Docker containers is gaining more momentum every day, especially in the DevOps world. Besides other main orchestration systems, Kubernetes can be considered as one of the most popular ones.
All big cloud providers like AWS, Google Cloud, Azure have already more or less adopted Kubernetes as a part of their portfolio and services.
Kubernetes and Docker containers altogether create an ideal ecosystem for deploying microservices — which seems to be the model for next-generation applications. Good knowledge of these essential DevOps tools is necessary to pass any DevOps certification including this one.
Here is the link to this free DevOps course — Learn DevOps Kubernetes deployment by kops and terraform
This is another great free course to learn DevOps on Udemy. This course is a must for those starting their journey into the DevOps and the Cloud world. The best thing about this course is that its created by Mumshad Mannambeth, one of my favorite instructors when it comes to learning DevOps and author of some of other Udemy best-sellers like Docker for the Absolute Beginner — Hands-On — DevOps
Most of the Cloud and DevOps courses out there require a person to know some basic concepts such as a basic web application, deploying a lab environment using VirtualBox, or on cloud platforms like AWS or GCP.
Most DevOps tools rely on languages such as JSON, XML, and YAML. The YAML language is used by automation tools like Ansible and container orchestration tools like Kubernetes.
Good knowledge of the YAML language is a must for a beginner learning these technologies. This course introduces YAML with simple and easy lectures and coding exercises that will help you practice YAML right in your browser.
This course also provides a high-level overview of common technologies in DevOps such as Orchestration tools like Ansible, Puppet, and Chef. And container technologies like Docker, Kubernetes, and OpenShift.
Here is the link to join this free course — DevOps — The Introduction Course [Free Udemy]
That’s all about the free online courses to pass the Google Cloud Professional DevOps Engineer certification. It’s one of the vast and comprehensive exams to pass and you can’t expect results just after pursuing a single course, you have to enroll in multiple courses so that you can learn about every particular concept of GCP.
It is recommended to enroll in all the above-listed sources, and once you are done with these courses, I can assure you that you’ll find the GCP Professional Cloud DevOps Engineer exam super easy.
Other Cloud Computing and IT Certification Courses and Articles you may like
Thanks for reading this article so far. If you find these free Google Cloud Platform Cloud DevOps Engineer certification courses useful, then, please share them with your friends and colleagues. If you have any questions or feedback, then please drop a note.
P. S. — If you need more comprehensive and focused online certification courses then I also highly recommend you to join Ultimate Google Cloud Certifications: All in one Bundle (4) course on Udemy.
This is not free but the most comprehensive online courses for Google cloud certifications and provide study material for all four Google Cloud certifications including Cloud Data Engineer.
udemy.com
Medium’s largest Java publication, followed by 14630+ programmers. Follow to join our community.
172 
Collection of best Java articles, tutorials, courses, books, and resources from Javarevisite and its authors, Java Experts and many more.  Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
172 claps
172 
Written by
I am Java programmer, blogger, working on Java, J2EE, UNIX, FIX Protocol. I share Java tips on http://javarevisited.blogspot.com and http://java67.com
A humble place to learn Java and Programming better.
Written by
I am Java programmer, blogger, working on Java, J2EE, UNIX, FIX Protocol. I share Java tips on http://javarevisited.blogspot.com and http://java67.com
A humble place to learn Java and Programming better.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/fabric-on-google-cloud-platform-97525323457c?source=search_post---------319,"There are currently no responses for this story.
Be the first to respond.
With thanks to the IBM engineers that wrote a Helm Chart to deploy Fabric as part of the IBM Blockchain Platform. It was relatively straightforward to port this to Kubernetes Engine although — at present — this is just a working Fabric network and I need to now get the tools (Composer) configured.
Create yourself a Kubernetes. I’m trying to pioneer and so am using a Regional Cluster and I’ve enabled the new Kubernetes Monitoring functionality.
Update: So, I’m not so pioneering, Regional Clusters are now GA :-) Yay!
This all appears to work just fine, something like:
You should be able to:
The Helm Chart requires a Kubernetes PersistentVolume that’s ReadWriteMany. This is not currently as easy as it ought be on Google Cloud Platform … cough… Google… cough…
So, we’re going to use NFS to provide the read-many capability. Ironically, the NFS solution we’re going to use is itself backed by Google Persistent Disk. Let’s create the NFS server because the Helm Chart depends upon it.
With thanks to mappedinn, I used their repo kubernetes-nfs-volume-on-gke to get this setup. This uses Google’s volume-nfs image and works great.
Create the underlying Persistent Disk:
Then apply the following Deployment and Service your Kubernetes cluster. The Deployment creates the NFS service using Google’s hosted volume-nfs image and binds the service to the Persistent Disk:
NB If you’d prefer to use SSD Persistent Disk instead of Standard (HDD) Persistent Disk, replace line #7 “default” with “ssd” in the Deployment script and before applying the Deployment, apply the following file to your cluster to register SSD as a storage class:
You should:
This will yield an NFS service that’s accessible through the Kubernetes’ Service’s DNS name: nfs.default.svc.cluster.local.
OK. You should not create a PersistentVolume or PersistentVolumeClaim for NFS because these will be created using the Helm Chart. Here’s a sneak-peek of the PersistentVolumeClaims *after* the Helm Chart has been deployed. You’ll do that in the next step:
Download and unzip the latest Helm binary (releases), add it to your $PATH and install Helm’s Tiller into the Kubernetes cluster. Assuming we’re in ${WORKDIR} and that you unzipped Helm into ${WORKDIR}/linux-amd64:
NB Helm has binaries for OSX and Windows, I’ll leave it to you to work out the specific instructions for non-Linux.
If — as is likely — you’re using an RBAC-based Kubernetes (Engine) cluster, I recommend the following steps to install Helm’s Tiller to the cluster:
This should return:
Then, clone my GitHub repo (with all credit to the folks at IBM to doing 98% of all this work for us):
But do not change into the ibm-blockchain-network directory created by the clone; remain in the parent (${WORKDIR}).
Optional: It’s good practice to run the Helm Linter over the Chart before deploying helm lint ibm-blockchain-network
Optional: It’s good practice to ‘dry run’ the deployment before applying it to your cluster. This is a useful feature of Helm and provides you with a way to see the Kubernetes specs that would be applied to your cluster: helm install --dry-run — debug ibm-blockchain-network.
When you’re confident in the results:
Helm will apply the Chart to your Kubernetes and provide you with an enumeration of its work:
You may check the Cloud Console for Kubernetes Engine Workloads:
https://console.cloud.google.com/kubernetes/workload
And Services:
There’s a bunch of logs created by the various containers that are deployed. Here’s the bootstrap container created by the ibm-blockchain-network-utils Pod:
For configtxgen:
For cryptogen:
For ca:
For orderer:
And org1peer1 which is similar to org2peer1:
You may list and delete deployed Charts with:
NB: There’s not currently an easy way to grab dynamically generated Chart names (such as solitary-possum) in this example. So, to delete a Chart, you’ll need to do the list in order to identify its name.
Don’t forget to delete the NFS Deployment when you’re done with it too:
You can whack your cluster:
Since we covered logging, I’d mentioned that I deployed the cluster enabling support for the new Stackdriver Kubernetes Monitoring.
Apart from needing to create the NFS service, the only other change that was needed for the Chart to deploy to Kubernetes Engine was a tweak to the Composer configuration. The ibm-blockchain-network-utils Workload creates 3 pods including bootstrap. bootstrap has a volume called composer-credentials that was mounted onto the Node’s (host’s) root (/) directory.
Container-Optimized OS is built to be secure and the OS includes the minimal amount of extraneous tooling. You can see here that the root (/) directory is mounted as read-only “to maintain integrity”. For this reason, the Chart was revised to use /tmp/composer instead of /composer. You can see this change in blockchain-utils.yaml lines 20–22:
and also in blockchain-debug-nfs.yaml. Wait what?
I provided some examples recently of ways you may debug Kubernetes Deployments. In this case, I wanted to ensure that the NFS service was working correctly. If it were working correctly, containers would be able to access its volume mounts.
To confirm this, I added a template to the Helm Chart. This template is called blockchain-nfs.debug.yaml. There are two advantages for including this in the Chart. The first is that it gets to use Helm’s variable replacement. The second is that it keeps everything together and ensures the debugging Deployments is created|deleted with the Chart.
blockchain-debug.nfs.yaml:
The NFS volume is referenced by the PersistentVolumeClaim defined in lines 34–36. You’ll note that there’s a second (and non-NFS) volume called composer-credentials that I added when debugging why this was failing when deployed to Kubernetes Engine (because of COS and not permitting “/” explained previously).
What’s neat is that, once deployed, we can grab the resulting Pod’s (!) name and exec into its (Alpine) shell and enumerate the contents of the directories:
This returns:
Helm is a good tool and it’s easier to use than I’d expected.
I’m working on a Helm Chart for Trillian too.
IBM’s Helm Chart is designed to deploy Fabric to IBM’s Kubernetes service but, because Kubernetes is Kubernetes is Kubernetes, as you can see, it’s trivial to convert this Chart to work on Kubernetes Engine too.
I’m hoping to learn more from the IBM folks about how to integrate Hyperledger Compose into this deployment and will update this post then.
Google Cloud community articles and blogs
41 
3
No rights reserved
 by the author.
41 claps
41 
3
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://towardsdatascience.com/working-with-google-cloud-automl-in-python-614395ab66b3?source=search_post---------320,"Sign in
There are currently no responses for this story.
Be the first to respond.
Priyanka Banerjee
Aug 9, 2019·5 min read
Its easy to have a CSV file and implement it in various ML models. But, the difficulty lies in implementing the e2e process of getting a video, extracting images, uploading them on Google cloud storage and later performing AutoML on them, entirely using Python. Nowadays, most of the companies have their own in-built models; if not, then they use Google ML models or others.
I will share with you the process of :
"
https://medium.com/georgian-impact-blog/comparing-google-cloud-platform-aws-and-azure-d4a52a3adbd2?source=search_post---------321,"There are currently no responses for this story.
Be the first to respond.
A starting point to help you choose the right platform for your ML project.
By Jing Zhang
If you’re looking for an end-to-end machine learning (ML) platform, you’re spoiled for choice. There are three main choices for cloud providers: Google Cloud Platform (GCP), Amazon Web Service (AWS) and Microsoft Azure Platform (Azure). The question is: how do you choose between the three? What functionality do they provide to build ML pipelines? We set out to answer these questions in a recent hackathon.
The R&D team at Georgian, where I work as an ML Engineer, decided to organize a hackathon to explore how each provider can help with the workflow. Our team builds machine learning software components ourselves, but we’re typically working with the growth-stage software companies in the Georgian family to deploy, so we wanted to familiarize ourselves with the different platforms and to be able to adapt to their workflows faster.
In this blog post, we’ll give a high-level overview of the ML platform solutions provided by GCP, AWS and Azure based on our experience during the hackathon. This list is not exhaustive but shows the results of our hackathon research.
Specifically we’ll cover:
We hope this can offer help as a starting point to help you choose the right platform for your ML project.
To help compare the different platforms, we chose a project that we could run on all three so we could compare apples to apples. Our project was to build a binary classification model with a given dataset and an end-to-end pipeline on a cloud provider. We used a dataset of companies that we use for our own machine learning platform, Spring. Spring helps us identify companies that fit our investment profile.The dataset contained general information about companies including their management team, financial information and office locations. The goal was to identify whether a company fit well into our investment profile (labeled 1) or not (labeled 0).
Since we used our own data for this project, we can’t share the specifics here, but to give you a sense what we were working with, here’s an overview of the dataset:
We also intentionally injected noise and error in the dataset so that we could run some tests on the data cleaning functionality. Specifically, we introduced:
One of the goals of this hackathon was to explore and assess each aspect of the end-to-end ML pipeline shown in the figure below. We decided to approach the challenge this way so that we would be able to assess the needs of a given project against the providers. For example, if explainability is important to a project, which is the best choice? Or, which provider performs the best if you want to run heavy testing?
Specifically, we wanted to assess each platform’s functionality in these areas:
Preprocess
Discover
Develop
Train
Test & Analyze
Deploy
Since we were building a model, we needed to think about model performance. The metric we tracked was Area Under the Receiver Operating Characteristic Curve (ROC_AUC). We weren’t, however, using model performance to make a judgment on which platform was better, so long as the results were roughly comparable.
Fairness in ML has emerged as an enormous issue area for policy makers, industry and the public. It’s important to have tools at your disposal to verify that your model is not treating any group and individual unfairly. To be able to address fairness, explainability tools are important too. These tools allow you to query why a model reached a certain decision. We were looking for the availability of the tools rather than assessing their relative performance.
Unlimited budgets are nice to dream about, but the reality is that cost is a constraint we should always keep in mind, so we tracked the costs for each provider.
As you would expect with these three platforms, there is a product or service for each step in the ML pipeline. The differences between them are in how well the services integrate with each other to build an end-to-end ML pipeline experience.
We have summarized the available services in the areas we’re interested in the table below. Going into detail is beyond the scope of this blog post. You can use this summary table as a starting point to see what’s available at each step for each provider.
For our team, the most important features are the model development and model deployment sections and, to a lesser extent, data visualization and QA. We were glad to see that all three providers have hosted notebook services, experiment tracking and version control, and easy deployment methods.
As for AutoML, all three providers have developed their own offerings. We used it to build our initial models and check whether there were valuable signals in the dataset. It would be great if the explainability component could be integrated so we could understand how the models are built for further model analysis and development.
Speaking of model explainability, while all three platforms all provide some tools, the functionality varies. If you have specific requirements, make sure you check if the current functionality satisfies your needs.
GCP uses a package called “the what-if tool”. You can integrate it with your notebook and play with the model by changing threshold or feature value for a given example. This allows you to check how certain changes affect the prediction result.
If you are using the Sagemaker debugger, it allows you to analyze how feature engineering and model tuning are done.
Azure provides a built-in module in their SDK, which seems to have the best integration.
So how did the models perform? As measured by ROC_AUC, performance was comparable across all three platforms. Azure and GCP scored slightly higher than AWS. This doesn’t necessarily mean one platform is better than another. It matched our expectation that the scores vary but should be close.
Cost, on the other hand, was more interesting. The cost on AWS was considerably lower than GCP and Azure. Please note this isn’t strictly an apples-to-apples comparison. The Azure team on our hackathon explored more services since Azure was a completely new platform to our team and we wanted to use the opportunity to learn more about it, so that may account for some of the cost difference.
One question we asked was: “Is it worth spending 4 to 6 times of money to get a 5% performance improvement?” The answer depends on the problem you’re addressing. For example, we’re looking for companies that could potentially generate large returns for our fund, so it may be worth spending the extra few hundred dollars. If you have budget concerns and the spending doesn’t justify the return, then it’s a different story.
Based on our observations, all three cloud providers cover the aspects of the ML workflow we care about.
Two hot topics in the industry right now are the rise of AutoML and the need for an end-to-end machine learning workflow in one place to provide a frictionless experience. As we mentioned earlier, AutoML products are already available on all the platforms but the end-to-end pipeline experience doesn’t seem to be mature yet.
On GCP and AWS, you’ll need to assemble multiple products together to get to the desired outcome. Azure, on the other hand, provides a machine learning designer service with drag and drop UI. This might imply that they are targeting different customer bases.
With the drag and drop UI interface, Azure’s machine learning designer may be more friendly to those new to data science, with little coding and technical background, to try out machine learning projects and evaluate whether it brings value to the business. Many corporations already use Microsoft products, so it might be an easy choice for teams at larger companies who are looking to try machine learning for the first time.
AWS and GCP seem to be more developer-focused. Though it’s a little more work to assemble a pipeline, they’re more customizable with the different components available. These components and the connection of the pipeline are often developed through code and configurations rather than an user interface. Companies who are more familiar with the options and know what they want to achieve may prefer this option.
We certainly learned a lot from this Georgian hackathon and it puts us in a better position to undertake projects on any of these three platforms in future. We hope that this is useful to you and helps you pick the right platform to start your next project.
A blog focused on machine learning and artificial…
139 
139 claps
139 
A blog focused on machine learning and artificial intelligence from the Georgian R&D team
Written by
Investors in high-growth business software companies across North America. Applied artificial intelligence, security and privacy, and conversational AI.
A blog focused on machine learning and artificial intelligence from the Georgian R&D team
"
https://medium.com/google-cloud/deploy-swift-http-serverless-container-to-google-cloud-run-in-5-minutes-alfian-losari-98389d34d4b8?source=search_post---------322,"There are currently no responses for this story.
Be the first to respond.
You can also read this article in my Xcoding With Alfian blog website using the link below.
www.alfianlosari.com
At Google Cloud Next 2019, Google has just introduced Google Cloud Run. It’s a serverless computing service, which means we don’t have to manage the infrastructure by ourselves. It is based on containers, so we only need to provide the Dockerfile that contains our HTTP server application and deploy. The service will be invocable via the HTTP request from the client and the payment will be using pay per use model.
There are many features that Google Cloud provides for the Cloud Run, such as:
You can learn more about the Cloud Run directly from Google with the official link below.
cloud.google.com
In this article, we will deploy a simple Swift HTTP Server app to Google Cloud Run using Dockerfile. We will use the Google Cloud SDK with Command Line for this. There are only 4 main tasks that we need to perform:
Before you begin, here are the things that you require to have:
Open your terminal/shell, create a new directory named hello-swift-cloudrun and navigate to that directory.
Inside the directory, create a new swift package.
Next, open Package.swift and copy the following code into the file. We will add the Swifter tiny HTTP server library as the dependency to run our HTTP Server in Swift.
Next, open the main.swift file from the Sources directory. Copy the following code.
Here are the things that it performs:
Try to build and run the server by typing these commands the terminal.
To test, open your browser and navigate to the address http://localhost:8080/html. You should see the text printed with the current time in your browser.
Next, we will containerize our app by creating the Dockerfile. Create the file and copy the following code below.
This will copy all the file to the container image, then run the swift build using release configuration. It will also run the server after the build has been finished.
Next, we need to upload our container to Cloud Registry. Make sure to retrieve your project id for your project. Run the command below.
Wait for the container builds process to finish and then uploaded to container registry. It will print the success message to the terminal.
You can check the list of the successfully uploaded container using this command.
At last, we need the deploy the image to the Google Cloud Run. Type these following command.
Here are several things that it performs:
You can configure other things, from memory, concurrency, and request timeout. Check the link below
cloud.google.com
After the deployment finished successfully, the terminal will print the URL endpoint of the deployed service that we can use. Open your browse and navigate to:
You can view all your deployed services to Cloud Run fromt the console dashboard.
https://console.cloud.google.com/run
In here you can also manage the custom domains, delete and create services, and view the logs of your deployed services.
!!!Make sure to delete all the resources that you have created after you finish this article to avoid billings!!!
You can clone the completed project in the repository below.
github.com
That’s it, in just a simple steps we have deployed our serverless backend using Docker to the Google Cloud Run managed autoscaling service. The serverless paradigm provide us speed and reliability to execute rapidly as the size of our application grows over time ⚡️⚡️⚡️.
Google Cloud community articles and blogs
86 
1
86 claps
86 
1
Written by
Mobile Developer and Lifelong Learner. Currently building super app @ Go-Jek. Xcoding with Alfian at https://alfianlosari.com
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Mobile Developer and Lifelong Learner. Currently building super app @ Go-Jek. Xcoding with Alfian at https://alfianlosari.com
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/this-week-in-google-cloud-platform-datastore-backups-configurable-ssl-a-new-billing-api-and-52cd3aeba9a2?source=search_post---------323,"There are currently no responses for this story.
Be the first to respond.
You can now set SSL policies, such as a minimum required TLS version, on GCP load balancers. Both predefined and custom profiles are available: “Announcing SSL policies for HTTPS and SSL proxy load balancers” (Google Blog)
A long-awaited feature for our highly-scalable NoSQL managed database service is now Generally Available: “Fully managed export and import with Cloud Datastore” (Google Blog)
Cost transparency has always been important to Google Cloud so this enhancement to the Google Cloud Billing API should come as no surprise: “Introducing Cloud Billing Catalog API: GCP pricing in real time” (Google Blog)
This was already shared last week, but here’s now the formal post: “Announcing Google Cloud Spanner as a Vault storage backend” (Google Blog). Developed as open source and supported by Google.
From the “TWINR (This Week In New Releases)” department :
From the “it takes great research to make great AI” department :
From the “In case you’ve missed it (ICYMI)” department :
From the “Customers talk best about Google Cloud” department :
A couple of networking solutions have been released by the Google Cloud team this past week (check out the latest GCP podcast about Solutions authors) :
From the “how-to” department :
From the “listen and watch” department :
This week’s picture is taken from the Google Research “Mobile Real-time Video Segmentation” blog post :
That’s it for this week!-Alexis
Google Cloud community articles and blogs
46 
46 claps
46 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Google Cloud Developer Relations
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://blog.searce.com/internals-of-google-cloud-spanner-5927e4b83b36?source=search_post---------324,"Sign in
What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Bhuvanesh
Feb 5, 2020·9 min read
We have learned a lot more internal things about Google Cloud Spanner from past two days. We read some of the portions of the Spanner white paper and the deep internal things from the Google Cloud Next event videos from Youtube. We shared the video links here, but we want to summarize all the learnings in one place. A special thanks to Deepti Srivastava(Product Manager for Spanner) who presented the Spanner Deep Dive sessions in the Google Cloud Next Event.
In 2005, 2006 Google was using the MySQL at massive scale. Google Adwords is one the biggest platform where 90+ MySQL Shards are used to store the data. Due to some maintenance, they re-sharded the MySQL Clusters. This process took 2 years to complete. Google understood that they are growing very fast and these kinds of databases will be a pain in future. That is how Spanner was born.
Once they decided to build something new with distributed, the Big Table team was the one who started working for the Spanner process. Because BigTable uses distributed process, storage and highly available(or maybe some other reasons as well).
Colossus is the distributed file system which is derived from the GFS. A high performance file system is needed for a super database. This project started by BigTable team and the BigTable is powered by Colossus. So Spanner also got the colossus as a filesystem.
The Google Adwords is MySQL based stack and the new system required to have essentials of a relational database like ACID compliance without the limitations of scale. The pain with MySQL is resharding. So they wanted the sharding features like the traditional NoSQL sharding that will take care of resharding and rebalancing. Plus more availability, Horizontal Scale and globally distributed.
Spanner is a global database system, per region we’ll get a minimum of 3 shards. Each shard will be in each zone. In Spanner terms, a shard is called Split. If your provision 1 Node Spanner cluster, you’ll get 2 more Nodes on the different zone which are invisible to you. And the Compute and Storage layers are de-coupled. Paxos algorithm is used to maintain one leader at a time and the rest of the nodes will be the followers.
Based on the partitions, we’ll have more Splits(shards) in the storage layer. Each shard will be replicated to the other Zones. For eg: if you have a shard called S1 on Zone A, it’ll be replicated to Zone B and C. The replication works based on Leader follower method. So the Paxos will help to maintain the quorum and will help to select a new Leader during the failure. If you are writing something on this Split, the Spanner APIs are aware of the Leaders. So the write directly goes to the Zone where it has the Leader Split. Each Split has its own leader zone.
when I was watching the deep dive video of Spanner, they were discussing the strong consistency. Spanner supports strong consistency across all the nodes(Globally). If you write something in US region, you can read that same data from the Asia region or any other region. How they implemented this logic? It’s called TrueTime.
Spanner synchronizes and maintains the same time across all the nodes globally spread across multiple data centers. The hardware is built with Atomic Clocks to maintain the time. If you take a look at the Server Hardware Rack, the Server is having 4 time servers. 2 Servers are connected with GPS and the remaining 2 are connect with Atomic Oscillators. There are 2 different brands of Oscillators for better failover processing. The GPS time servers will sync with Oscillators to synchronize the time across the global data centers with every 30sec interval.
Let’s now try to understand how this TrueTime helps Spanner stay consistent.
To understand the relationship between consistency and TrueTime, we have to understand how a write operation works in Spanner. During every write operation Spanner picks up the current TrueTime value and this TrueTime timestamp will create an order for the write operations. So every commit has been shipped with a timestamp.
For Eg: If you are writing a data on Node 1, it’ll commit the data with the TrueTime timestamp and replicate the data and timestamp to the other nodes. This timestamp is the same on all the nodes. Lets say we committed this data on Node 1, if you are reading the same data from the Node B, then the Spanner API will ask the leader of the Split for last committed data’s timestamp, if the timestamp is matching from the Node A’s timestamp then the data will be returned from Node B, else it’ll wait until the Node A sync the data to Node B and then it’ll return the data.
Here is the lifecycle of a single write operation. We are writing a row that will go to Split 2. Now the Spanner API will understand who is the leader node for Split 2, then the request will go to Zone B node(Blue indication refers to the leader). Then it’ll acquire the lock write it on the split. Once this write has been done, it’ll send the requests to Zone A and C Nodes to write the same. It’ll wait for the acknowledgement from the majority of the nodes. Once the leader split got the majority of the acknowledgement, then it’ll send the success response to the client.
If you are writing the data in a single transaction, but the data resides on different splits, then the spanner will handle it in a different way. For eg: we have to update 2 rows.
When we initiate the transaction, the Spanner API will understand that the rows are in the different split. And they will randomly pick a Co-ordinator zone. In our example, the API has chosen Zone C is the coordinator zone. The following steps will be performed for the multiple row operations.
While reading the data from Spanner, data will be fetched from the nearest follower split. Let’s explain this with an example. Refer the below image.
We want to read the data from MyTable, for the value 123. This value is stored in Split 2. Now once the request reached the Spanner Frontend server, then it’ll understand who is the nearest follower split and forward the request to that split. In our case, Zone A is the nearest split. Once the request reached the split, then that split will ask to the Leader split to get the last committed TrueTime. And then it’ll compare the Timestamp with its own timestamp. If both match then it’ll serve the data to the application. If the timestamps are not matched then the leader split will ask the follower to wait until it sync the data to that Zone. And then the split will serve the data.
Spanner supports MVCC. So it’ll keep the old data for some period of time. If our applications are fine to get the old data (older than X seconds) then we don’t need to wait for data sync from the leader split. For example, We have to tell the Split that we are fine with 15sec old data, then it’ll check the committed timestamp and that is less than 15 seconds, then the old data will be served to the application.
All scenarios explained above apply to clusters within a single region — zone level. But Spanner is built for global scale and multi-region deployments. The architecture and write/read operations will have a slight difference in the multi region setup. In the single region concept, we need a minimum of 3 zones to create the cluster. And the zones support both read and write. But in Multi region concept, One Continent will be act as a Leader and the rest of the Continent will be the followers. In Spanner terms, the Continent where we have more region will be the quorum. All the writes will go to any region in this continent. In the quorum continent, 2 regions will be hosting the data nodes, and 1 region will host the witness for failover. Other continents will have read only replica nodes.
In a multi region cluster, the writes are always performed on the Quorum continent. Let’s say, US region is the R/W continent, then if you are sending a write request from the US region, then the Spanner API will send it the nearest region, once the data has been committed then the success response will go to the client. If you are sending a write request from Asia region, then the Asia region’s API servers will put the request into Google’s internal network and send the request to the US region’s API server. Then that US region API server will commit the data and the success response will be send it to Asia region client.
For Reads, the process is same as single region concept, if the TrueTime matches, then the data will be served from the local region, else it’ll wait until the data sync to the local region and then served to the clients.
We covered most of the internal concepts of Spanner. But still there are a lot more things to learn in Cloud Spanner. Please refer the Google Cloud Next event videos links below.
Less Talk, More Data | https://thedataguy.in
See all (503)
38 
1
38 claps
38 
1
Searce is a niche’ cloud-native technology consulting company, specializing in modernizing infra, app, process & work leveraging Cloud, Data & AI. We empower our clients to accelerate towards the future of their business.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@everisbrasil/como-se-tornar-um-google-cloud-certified-professional-cloud-architect-em-um-m%C3%AAs-438c2acdcdb1?source=search_post---------325,"Sign in
There are currently no responses for this story.
Be the first to respond.
everis Brasil
Dec 13, 2018·5 min read
Tudo começou ao aceitar um desafio no lugar onde trabalho. O objetivo era obter a certificação como Professional Cloud Architect em um mês. Antes de chegar até mim a notícia deste desafio, eu estava pensando em fazer o track de especialização da Coursera chamado Architecting with Google Cloud Platform, mas claramente minha meta para terminar este track de especialização era, no começo, muito mais folgada. Agora, as coisas eram totalmente diferentes, eu só tinha 30 dias.
A primeira coisa que fiz foi começar com o track de especialização. O track de Architecting with Google Cloud Platform (de agora em diante, sua bíblia se vocês quiserem passar no exame) é composto por seis cursos, cada um projetado para ser concluído entre uma e duas semanas tranquilamente. Meu primeiro objetivo era terminar os cursos em uma semana para poder me preparar durante as três semanas seguintes nos casos de exemplo (outro dos fatores extremamente importantes para ser aprovado no exame).
A verdade é que eu terminei todos cursos do track de especialização em duas semanas, me dedicando pelo menos entre dois e três dias da semana após o trabalho e todo o final de semana. Ou seja, eu tinha apenas duas semanas para preparar os casos de exemplo e fazer o exame.
01) Tome notas pessoais ao ministrar os cursos do track (isso pode ser no seu próprio computador, tablet, caderno etc.), pois após concluir os cursos, vocês vão precisar fazê-lo novamente (desta vez, de maneira muito mais rápida e após seguir as dicas dois e três que mencionarei a seguir) para reforçar as áreas nas quais vocês sentem mais dificuldades. Minha maneira de tomar notas foi em meu próprio computador, tirando screenshots de seções dos vídeos que me pareciam interessantes para não precisar voltar a assistir o vídeo inteiro caso tivesse alguma dúvida, e salvando as tabelas comparativas ou árvores de decisão que apareciam.
Também é importante realizar todos os laboratórios práticos incluídos nos cursos. A tentação de ignorá-los pode ser alta por não ser obrigatório realizar os laboratórios para ser aprovado nos cursos, mas eles são realmente necessários para assimilar o lado prático da matéria.
02) Faça o exame de prova que o Google oferece.
A primeira vez que fiz esse exame de prova foi ao terminar o track de especialização. Eram 23 perguntas e a recomendação é fazê-lo em 45 minutos. Acertei apenas 13 perguntas das 23. O bom deste exame é que, no final, ele mostra quais perguntas você acertou, quais errou e quais são as respostas corretas das perguntas, o que será crucial para a próxima dica.
03) Anote todas as perguntas que vocês errarem e volte a realizar o track de especialização (desta vez de maneira muito mais geral e rápida), reforçando o conteúdo em que vocês se mais dificuldade de acordo com as áreas que erraram no teste prático. Neste ponto, é importante complementar suas anotações pessoais com o novo conteúdo, pois isso será útil no momento de repassar o conteúdo nos dias anteriores ao exame.
04) Após seguir esses passos até aqui, recomendo que façam novamente o teste prático para ver quão bem puderam assimilar o novo conteúdo.
05) Depois de todos esses passos: terminar o track de especialização com suas próprias anotações personalizadas, realizar o teste prático, voltar a fazer o track reforçando o conteúdo de acordo com as perguntas incorretas, atualizar suas anotações e fazer pela segunda vez o teste prático, vocês podem enfrentar aquela que eu diria ser a parte mais difícil e que toma mais tempo do exame: os casos de exemplo.
Como o nome diz, são quatro casos que o Google propõe e sobre os quais serão feitas perguntas que correspondem a aproximadamente 40% do exame, portanto é muito importante lê-los (não é necessário memorizá-los, pois podem ser consultados em uma tela dividida durante todo o exame). Alguns pontos a serem considerados são os seguintes:
· Estratégia de Negócio: Este ponto é importante, pois pequenas sutilezas podem fazer a diferença entre uma solução e outra. O mais importante neste caso é ver se a solução deve ser global ou regional, a quantidade de informações que precisa ser processada, a rapidez para entregar novos recursos, a otimização de recursos etc.
· Estratégia Técnica: Neste ponto, os casos mais considerados são utilizar serviços gerenciados pelo Google Cloud quando for possível, que as soluções sejam NO-OPS quando for possível (especifica-se quando não é possível devido a sistemas personalizados), reduzir a latência para soluções globais, ter soluções autoescaláveis, armazenar grande quantidade de informações para poder ter estatísticas ou análises (várias soluções fazem referência a BigQuery).
· Otimização de Custos: Qual de todas as soluções propostas é a mais eficiente para o cenário atual, obedecendo à estratégia de negócio e técnica. Nem sempre a solução mais elegante é necessariamente a solução mais eficiente para o caso de exemplo proposto. Várias perguntas fazem referência aos diferentes tipos de Cloud Storage e ao Ciclo de Vida do mesmo para reduzir custos.
· Suportar o design atual do aplicativo a ser migrado: Aqui é fundamental entender quais soluções do Google Cloud são as mais fáceis para migrar aplicativos, jobs ou BDD existentes. Por exemplo: para migrar uma BDD PostgreSQL é possível utilizar Cloud SQL, ou para migrar jobs em Hadoop pode-se utilizar Dataproc. Haverá perguntas referentes a migrar aplicativos realizando a menor quantidade de modificações.
· Como avaliar se a solução implementada foi a correta: Neste item, faz-se referência geralmente ao Stackdriver e como é possível analisar se a solução aplicada está sendo bem-sucedida na produção referente a KPI e ROI. Vocês irão se deparar com perguntas que fazem referência a quais tipo de métricas podem fornecer informações sobre o sucesso ou fracasso da saída para produção. Alguns exemplos se referem a medir a quantidade de visitas em um site, quantidade de erros, etc.
Após estudar os casos, recomenda-se estudar as soluções de referência propostas pelo Google. Por último, o que posso lhes recomendar é que reforcem as áreas em que tenham mais dúvidas consultando diretamente a documentação oficial da Google Cloud Platform.
Desejo-lhes muito sucesso neste desafio e espero que tenha um bom desempenho. Para qualquer dúvida, podem me perguntar que ficarei feliz em ajudar.
PS: Deixo também como referência um PDF com minhas anotações pessoais para que vocês tenham ideia do que é importante estudar. Cabe destacar que vocês precisam criar suas próprias anotações, pois assim é mais fácil assimilar o conteúdo dos vídeos e, posteriormente, revisar a matéria.
· Realizar o track de especialização Architecting with Google Cloud Platform.
· Tomar notas personalizadas de cada módulo.
· Realizar todos os laboratórios dos cursos
· Terminado o track, fazer o exame de prova.
· Identificar dificuldades de acordo com as perguntas que errarem ou material que não se sentem seguros para entender.
· Voltar a fazer o track de especialização, reforçando as áreas com mais ­­dificuldade e atualizando suas anotações personalizadas.
· Estudar os quatro casos de exemplo do Google.
· Estudar as soluções de referência propostas pelo Google.
· Reforçar áreas em que tenham mais dúvidas consultando a documentação oficial da Google Cloud Platform.
· Entender a diferença de usar cada solução em relação a aspectos como a estratégia de negócio, estratégia técnica, otimização de custos, suporte de design atual do aplicativo no ambiente On-premise, suporte para armazenar e processar grande quantidade de informações.
By: Sebastian Moreno E
Exponential intelligence for exponential companies
97 
97 
97 
Exponential intelligence for exponential companies
"
https://medium.com/@thanachart-rit/data-lake-%E0%B8%84%E0%B8%B7%E0%B8%AD%E0%B8%AD%E0%B8%B0%E0%B9%84%E0%B8%A3-%E0%B9%80%E0%B8%84%E0%B9%89%E0%B8%B2%E0%B8%9E%E0%B8%B9%E0%B8%94%E0%B8%96%E0%B8%B6%E0%B8%87%E0%B8%81%E0%B8%B1%E0%B8%99%E0%B8%AD%E0%B8%A2%E0%B9%88%E0%B8%B2%E0%B8%87%E0%B9%84%E0%B8%A3-%E0%B9%83%E0%B8%99-google-cloud-next18-5e6fba897b11?source=search_post---------326,"Sign in
There are currently no responses for this story.
Be the first to respond.
Thanachart Ritbumroong
Jul 26, 2018·1 min read
Data Lake เป็นเหมือนสิ่งใหม่ที่เกิดขึ้นมากับคำว่า Big Data บางองค์กรยังไม่รู้เลยว่า เค้าคืออะไร มีไว้ทำอะไร แต่รู้สึกว่ามันต้องมี คนสร้างก็สร้างกันไปแบบไม่ได้เข้าใจ ก็เทๆ ข้อมลใส่ลงไป แล้วเรียกอันนี้ว่า Data Lake
A data lake is a collection of storage instances of various data assets additional to the originating data sources. These assets are stored in a near-exact, or even exact, copy of the source format. The purpose of a data lake is to present an unrefined view of data to only the most highly skilled analysts, to help them explore their data refinement and analysis techniques independent of any of the system-of-record compromises that may exist in a traditional analytic data store (such as a data mart or data warehouse).
— Gartner
ถ้าดู definition ของ Garter ก็จะบอกว่า Data Lake นั้น คือแหล่งที่เก็บข้อมูลแบบไม่มีการเปลี่ยนแปลงรูปแบบใดๆ เลย เก็บเหมือนๆกับที่มาจากต้นทาง คำที่ผมชอบใช้คือ primitive format
สาเหตุที่เก็บแบบนี้เพราะว่า ข้อมูลต้นทางก็มีการลบหรือเปลี่ยนแปลง ข้อมูลปลายทางที่เก็บในรูปแบบของ Data Warehouse ก็ไม่ได้เก็บทั้งหมด บางทีเก็บในรูปแบบที่มีการประมวลผลแล้ว ทำให้ถ้าจะย้อนกลับมาหาข้อมูลในอดีต ก็จะสูญหายไป
แล้วเมื่อ Data Lake จะต้องเก็บข้อมูลใหญ่มหาศาลขนาดนี้ จะมีเรื่องอะไรให้ห่วงบ้าง ก็แน่นอนครับ เอาจากที่ประมวลผลที่ฟังมาในงาน ก็มีประมาณ 3 เรื่อง
สิ่งที่ Speaker พูดกัน ก็คือ การเก็บข้อมูลใน Data Warehouse นั้น มีต้นทุนสูง ทำให้เราไม่สามารถเก็บข้อมูลทั้งหมดทัั้งปวงได้ แต่ Data Lake นั้น ด้วยความที่เก็บเอาไว้เฉยๆ เก็บเยอะๆ เลย เก็บทั้งหมดก็ได้ ก็ควรจะเก็บด้วยต้นทุนที่ต่ำที่สุด
ด้วยต้นทุนของ Storage สมัยนี้ที่ราคาลดลงอย่างมากแล้ว เราก็สามารถจัดเก็บข้อมูลปริมาณมหาศาลได้ด้วยต้นทุนที่ไม่สูงนัก ไม่ว่าจะใช้เทคโนโลยีอย่าง Hadoop หรือ Google Cloud Storage (ที่เค้า claim ว่า ถูกกว่าจริงๆ เพราะว่า มีการแบ่งการคิดราคาตามประเภทของข้อมูล ถ้าเราเก็บไว้แล้วไม่ใช้เลย ก็จะยิ่งถูกลงไป)
เมื่อเราก็บข้อมูลได้ในราคาถูก เค้าก็บอกอีกว่า งั้นก็ไม่เห็นต้องคิดมากเลย เก็บไปเลยเยอะๆ ถ้ายังไม่รู้ว่าจะเอาไปใช้ทำอะไร ก็เก็บไว้ก่อน ค่อยไปคิดทีหลังได้
ข้อมูลบางส่วนใน Data Lake นี้ จึงเหมือนเก็บพักไว้ แช่เอาไว้ก่อน เผื่อในอนาคตเราจะต้องใช้ จะได้มีไว้ให้ใช้ (มีหลายกรณีเลยที่อยากได้ข้อมูลเอาไปใช้ แต่ต้นทางลบไปหมดแล้ว)
เมื่อข้อมูลไหลเข้า Data Lake แล้ว จะต้องมีความ consistency (ไม่รู้ว่าแปลว่าอย่างไรดี แนบนัย? สม่ำเสมอ?) คือ พอเข้ามาแล้ว ทุกคนจะต้องเห็นข้อมูลได้ทันที และเห็นข้อมูลที่เหมือนกัน อันนี้เป็นกรณีที่เค้าทำหลายๆ cluster หลายๆ site ถ้าแต่ละ node เห็นข้อมูลไม่ตรงกันนี่ก็พังเลยทันที ซึ่งก็จะเป็นสิ่งที่ lead ไปยังคำว่า data governance ล่ะครับ ว่าจะจัดการกับข้อมูลอย่างไรทั้งฝั่งที่เป็น technical aspect หรือ consumption aspect
Lecturer at Management of Analytics and Data Science Program, National Institute of Development Administration, Thailand and Data Analytics Consultant
35 
35 
35 
Lecturer at Management of Analytics and Data Science Program, National Institute of Development Administration, Thailand and Data Analytics Consultant
"
https://medium.com/@hoffa/connecting-africa-nomanini-gcp-and-bigquery-c613aedb6d3c?source=search_post---------327,"Sign in
There are currently no responses for this story.
Be the first to respond.
Felipe Hoffa
Aug 21, 2018·4 min read
Nomanini is a startup powered by Google Cloud that enables micro-transactions in various countries around Africa. I flew to Johannesburg to meet their CTO and learn how they use App Engine, Kubernetes, BigQuery and other Google Cloud Platform tools.
Nomanini builds point-of-sale systems to help remote villagers get access to prepaid airtime, prepaid electricity, and basic banking services like being able to withdraw or deposit cash.
Back in 2011, they started by focusing on selling prepaid mobile air time. Distributing boxes of scratchcards out into rural villages isn’t easy — so they created a wireless alternative.
Now people in the villages don’t have to travel far to go and get essential services like air time or electricity, and even banking services.
In this video, Dale Humby — Nomanini’s CTO — shares more about the custom devices they created and why they chose Google Cloud to power their backbone. Originally on App Engine, they now have most of their infrastructure running on Kubernetes.
And for their analytics they use… BigQuery!
Each of their devices reports back its own stats — thus Nomanini is able to track their status and what kind of connectivity they have. This information allows them to debug the status of the different cell networks — and even inform the mobile networks of problems before they notice them.
OpenCellID is the world’s largest open database of cell towers.
As of October 2017, the database contained almost 36 million unique GSM Cell IDs. More than 75,000 contributors have already registered with OpenCellID, contributing millions of new measurements every day to the OpenCellID database.
Thanks to open data, Nomanini can look beyond their devices and check the daily reports of thousands of contributors around the world.
In the video, we use a combination of BigQuery, Data Studio, and re:dash to visualize the status of cell networks around the world, and in the cities where Nomanini is present.
For example, let’s see what are the most popular mobile radio technologies around the world, according to the OpenCellID tables:
UMTS is the most popular radio tech according to OpenCellID, while CDMA is the least. But they’re not distributed evenly around the planet. A quick check with Data Studio can show us the actual patterns:
We also visualized how many cell towers each country in Africa has, relative to its population:
Nomanini is able to join this data with the reports from their own devices.
For example, to visualize the state of each of their devices around Mozambique (with Redash):
For the full story, watch our video!
OpenCellID is an open data project — let us know if you’d like to see it permanently updated in BigQuery through our public datasets program.
Want more stories? Check my Medium, follow me on twitter, and subscribe to reddit.com/r/bigquery.
And try BigQuery — every month you get a full terabyte of analysis for free.
Data Cloud Advocate at Snowflake ❄️. Originally from Chile, now in San Francisco and around the world. Previously at Google. Let’s talk data.
See all (1,656)
74 

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
74 claps
74 
Data Cloud Advocate at Snowflake ❄️. Originally from Chile, now in San Francisco and around the world. Previously at Google. Let’s talk data.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/mai-piu-senza/ti-serve-la-riga-di-comando-e-sei-dalla-zia-c%C3%A8-la-google-cloud-shell-e2e56ace6506?source=search_post---------328,"There are currently no responses for this story.
Be the first to respond.
Qualche giorno fa ho letto questo tweet:
È un servizio attivo da diverso tempo e ci avevo anche già “messo il naso”, ma stavolta ho guardato a questo tweet con “occhi nuovi” .Ho pensato a quei — rari — momenti in cui ho bisogno di una shell e non ho con me il mio PC, non ho le mie chiavi di accesso per accedere a una remota, eccetera: Google Cloud Shell può essere (per alcune cose) una soluzione.
È un servizio gratuito per gli utenti Google (credo che ci sia comunque da associare una carta di credito all’account) con queste caratteristiche di base (qui faccio riferimento all'offerta gratuita, perché è modificabile a pagamento):
Una nota su quest’ultimo punto: se oggi dovessi installare un’applicazione in modo standard (i.e. sudo apt-get install NomeApp), domani non sarà disponibile, perché (di solito) la cartella di installazione non è la $HOME dell’utente.
Per poter installarle nella cartella $HOME ci sono queste modalità:
La cartella /home/nomeUtente/.local/bin deve essere nel PATH del sistema operativo.
Mi sono creato il mio coltellino svizzero da usare dalla zia con dentro csvkit, ipthon, Miller, Visidata, goodtables, chardet, gron, jq, xq, yq, pup, mapshaper, sqlite, pandas, tldr, ecc., da usare all'occorrenza.
Molto comodo.
di tutto quel superfluo che è indispensabile
97 
Some rights reserved

97 claps
97 
Written by
#data #maps #GIS #baci #condivisione. Orgoglioso di essere presidente di @ondatait
di tutto quel superfluo che è indispensabile
Written by
#data #maps #GIS #baci #condivisione. Orgoglioso di essere presidente di @ondatait
di tutto quel superfluo che è indispensabile
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/hosting-a-website-on-google-cloud-using-cloud-run-a65343a98fce?source=search_post---------329,"There are currently no responses for this story.
Be the first to respond.
In this mini series we are covering, how to create websites on Google Cloud. This is the third article in the series.
Let’s say you’re a small company that got really big very quickly, and now you are seeing a lot of traffic on your simple website, but you don’t want to deal with infrastructure setup and maintenance when you could be focusing on your core business!
Cloud Run is a managed compute platform that automatically scales stateless containers. “Stateless” is important here, since container instances can be started or stopped at any time. These containers can be invoked by web requests or pub/sub events.
It is built from Knative, letting you choose to easily run your containers either fully managed with Cloud Run, or in your Google Kubernetes Engine cluster with Cloud Run on GKE.
Cloud Run is also serverless: it abstracts away all infrastructure management, so you can focus on what matters most — building great applications.
Before we begin, we need to make sure we are properly set up in Google Cloud Console. Here are some setup logistics:
Now that we’ve covered the logistics, there are five steps to set up a web app on Cloud Run.
NOTE: At the time of this blog post’s publish date, Cloud Run is in Beta.
This requires us to install the beta components of the gcloud SDK using the following commands.
“gcloud components install beta” — to install the beta features
“gcloud components update” — to update the components
You could skip this step if you already have a web app that you would like to use further in these steps. If you choose to follow along with my example, then let’s create a “hello world” web app in python using the following steps:
Now, we are ready to containerize our app and upload to container registry!
Create a new file named Dockerfile in the same directory as the source files. A Dockerfile is a text document that contains all the commands a user could call on the command line to assemble a container image.
you could use the following commands:
Now, we are ready to build using cloud build by running the build command from the directory containing the Dockerfile.
To deploy the container image on Cloud Run, we now need to execute the following command. Make sure you replace the PROJECT-ID and SERVICE-NAME with your own project-id and service name.
You have just deployed a web application packed in a container image on Cloud Run.
We created and deployed a dynamic web application on Cloud Run and learned that if you host a website on Cloud Run, you can rely on google cloud for it to scale with the demand.
Google Cloud community articles and blogs
221 
2
221 claps
221 
2
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Developer Advocate @Google, Artist & Traveler! Twitter @pvergadia
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/swlh/reorder-with-google-cloud-firestore-8e13ea9f7fb9?source=search_post---------330,"There are currently no responses for this story.
Be the first to respond.
Have you ever had the need to let your users order data as they wish to?
In one of my most recent work, one of the required feature had to do with giving the users the ability to reorder their data using Kanban boards. Likewise, in DeckDeckGo…
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://proandroiddev.com/google-cloud-anchors-with-sceneform-in-android-7b8e6e0f3128?source=search_post---------331,"This article was originally posted here.
In this post on Augmented Reality App Development using ARCore, we’ll take a look at how to use Cloud Anchors. This is the 4th post in our Augmented Reality App development articles. If you’re new to ARCore, I’d suggest taking a look at some of the articles given below.
The first article would give you a gist of what is ARCore and how it helps in developing Augmented Reality Applications.
Now that you’ve read the articles above, you’ll have an overview of what is ARCore and where Sceneform SDK comes into the picture. With this out of the way, now let’s have a look at how to use Cloud Anchors API by Google!
This post will be divided in the following sections:
To host and retrieve anchors from the cloud, we’ll need to use the Cloud Anchors API. Follow these simple steps to enable the API.
Now with the API key enabled, it’s time to fire up the Android Studio. Create a new Android Studio project with an empty activity.
Open AndroidManifest.xml and add the following permissions outside the <application> tag:
Now add the following meta-data directly below your <application> tag:
Don’t forget to add your API key to the above tag.
To develop an Augmented Reality application using ARCore, you’ll need to use the Sceneform Plugin and Sceneform SDK. The article below will show you how to enable Sceneform Plugin for Android Studio:
Check how to enable Sceneform Plugin:
https://developers.google.com/ar/develop/java/sceneform/
Once the plugin is enabled, add the following dependencies to the app level build.gradle file:
Now we’re ready to import a 3D model into our android app.
We’ll download the 3D assets from poly.google.com, it’s a great tool to download .obj files for sceneform. Select any 3D model and download it in the .obj format. You’ll get a zip donwloaded to your computer.
Right click the app package in Android Studio and click on New → Package and name it “sampledata”. Extract the .zip file in this folder.
Once you’ve extracted the files, find the .obj file and right click on it. Select “Import Sceneform Asset” and follow the steps to finish importing the 3D model in your app.
We’re now ready to code our Augmented Reality application using Sceneform SDK. Create a new class and extend it from ArFragment. Then override the `getSessionConfiguration()` method and add the following code:
Using planeDiscoveryController we set the instructionView as null. This will prevent any tutorial from blocking our view. Then we enable the CloudAnchorsMode. For this, we get the session configuration and set cloudAnchorsMode to Enabled. Finally, we return the custom configuration.
We need to add this custom ArFragment into our Activity’s layout file. We’ll also add two additional buttons. One to clear any objects on the screen and other to resolve the anchor. Apart from that we’ll have the custom ArFragment covering the entire screen.
Add the following code to your layout file:
With this we’ve completed our layout file. We’re ready to render Augmented Reality scenes on the screen.
If you’ve ready the previous articles, you’ll know that we use 2 methods to add a 3D object to the Augmented Reality scene. First method loads a 3D model from .obj file into Renderable object from Sceneform SDK. The other method actually places that renderable to our scene.
So, add the following methods to your MainAcitivity.java file
Read the above mentioned articles to find out what these methods do in detail.
We’re now ready to host our anchors to the cloud. First, we’ll add a listener to detect touches on the plane and place an anchor + 3D model to the scene.
The hostCloudAnchors method begins the process of hosting an Anchor to the cloud. It returns an Anchor with state TASK_IN_PROGRESS. Next, we add a method to set the global anchor to refer to the hosted anchor. It also set’s the anchor’s state to NONE.
To maintain the state of the Anchor, we need to create an Enum and a global variable to keep track of current state of the anchor.
Now whenever we touch the scene, we’ll start hosting the anchor. Hence, we need to update the current anchor, and it’s state as well. Add the following code in your setOnTapArPlaneListener
Next, we need to check regularly, whether the anchor was successfully hosted to the cloud. If not, then we need to notify the user of failure. To do this, we add an onUpdateListener to the scene. This method will check if the anchor is hosted or not, for each camera frame.
This method might look long, but it’s only job is to check for the current state of cloud anchor HOSTING/RESOLVING and checking whether hosting is successful or failure. Then it set’s the current state accordingly.
We’ve used a class StorageHelper to store anchor ID in sharedPreferences. You can use other methods of storage such as Firebase, FireStore, or your custom solutions. But aim of this article is to show how to store Augmented Reality anchors on cloud. Hence, for local storage we’ll use Shared Preferences.
Last thing that we need to do is, resolved the hosted anchors. To do this, we’ll add the onClickListeners to our two buttons: Clear and Resolve.
For the clear button, we’ll just call the cloudAnchors method and pass null to it.
For resolve button, we’ll show a dialog box on click. It’ll have an EditText for entering the short code of the anchor. When the user submits the shortcode, we’ll resolve the anchor and show the object!
First we’ll need a dialogFragment. I’ve created one for you:
Add this to your project and call it when user clicks the resolve button as shown below:
Here’s the onResolveOkMethod:
It get’s the shortcode and resolves the object onto our Augmented Reality scene.
Notice that we’ve also used the SnackBarHelper class a lot. It’s another class by Google to manage snackbars in Android. Here’s the link to it: SnackbarHelper
And we’re done!
This is how your final MainActivity.kt class will look like:
Here’s how the final result looks like:
This article will help you host CloudAnchors using Google Cloud Anchors API. Let me know if you’ll face any problems and I’ll be happy to help :)
*Important*: Join the AndroidVille SLACK workspace for mobile developers where people share their learnings about everything latest in Tech, especially in Android Development, RxJava, Kotlin, Flutter, and mobile development in general.
Click on this link to join the workspace. It’s absolutely free!
Like what you read? Don’t forget to share this post on Facebook, Whatsapp, and LinkedIn.
You can follow me on LinkedIn, Quora, Twitter, and Instagram where I answer questions related to Mobile Development, especially Android and Flutter.
The latest posts from Android Professionals and Google Developer Experts
84 
5
Thanks to Simon Percic. 
84 claps
84 
5
Written by
A sensible writer (https://ayusch.com)
The latest posts from Android Professionals and Google Developer Experts.
Written by
A sensible writer (https://ayusch.com)
The latest posts from Android Professionals and Google Developer Experts.
"
https://medium.com/google-cloud/hands-on-with-google-cloud-platform-tensorflow-bigquery-early-thoughts-4d379cd693fc?source=search_post---------332,"There are currently no responses for this story.
Be the first to respond.
I spent two days at Google’s Sydney office this week, using their new Google Cloud Platform (GCP) and getting some hands on experience with the GCP tool set.
I was given the opportunity to use GCP tools for querying billion-row data sets, set up virtual servers and create machine learning models.
We were encouraged to try … test … trial GCP to our hearts content.
At the end of the two days, I walked away impressed and excited. I think Google really has something here.
It’s much cleaner and easier to use than Google’s predecessor called App Engine. App Engine didn’t work that well if we’re being honest and Google will be the first to tell you that.
The Google Cloud Platform has clearly been rethought and redeveloped — complete with data analysis, compute power, machine learning, storage, data processing and more.
Overall, Google has created a great platform in GCP and its ahead of the field when compared to the market.
I spent my time using the GCP tool set, thinking about how I might perform the same tasks with the IBM BlueMix/Watson suite as well as Amazon’s platform.
The difference came down to ease-of-use and elasticity. GCP felt miles ahead of both IBM and Amazon in terms of easily creating machine, conducting data analysis and running machine learning scripts. In terms of elasticity, GCP allows you to run virtual machines for seconds at a time which is much more elastic than Amazon where you pay for machines by the hour.
Another attendee at the workshop, also comparing tools, summed it up perfectly when he said “The other tools have a high barrier to use, with this I can just make it work.”
His words echoed the same sentiment I took away from my time in Sydney. GCP was built to ‘just work’. And while there were improvements needed for various processes, Google has put out a great product and platform.
I’m excited to start throwing real data into GCP and trialling the platform in the wild. Look out for more from me as I try BigQuery and TensorFlow with real-world projects in upcoming weeks.
Google Cloud community articles and blogs
21 
2

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
21 claps
21 
2
Written by
Head of Decision Science + Analytics @ Spaceship. Ex Instagram + Intuit. PhD. Social Scientist. Conservation, paddleboards & smoothie fan. Views are mine only.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Head of Decision Science + Analytics @ Spaceship. Ex Instagram + Intuit. PhD. Social Scientist. Conservation, paddleboards & smoothie fan. Views are mine only.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@sathishvj/my-notes-on-the-google-cloud-business-professional-accreditation-ece25a18f2fe?source=search_post---------333,"Sign in
There are currently no responses for this story.
Be the first to respond.
sathish vj
Nov 30, 2020·5 min read
A course and test to take on your own time that could be a start to understanding Google Cloud.
Google seems to have removed this towards the end of December 2020. Existing accreditation links seem to be live.
Plug: Subscribe to my YouTube channel that teaches you to apply Google Cloud to your projects and also prepare for the certifications: youtube.com/AwesomeGCP. Check out the playlists I currently have for Associate Cloud Engineer, Professional Architect, Professional Data Engineer, Professional Cloud Developer, Professional Cloud DevOps Engineer, Professional Cloud Network Engineer, and Professional Cloud Security Engineer.
Over the last few months I have been conducting a few Google Cloud sales sessions across multiple geographies with attendees from Australia to western USA. The audience is primarily from a sales and pre-sales background. One of the frequent questions I got was whether there is a Google Cloud certification similar to the technical certifications, but for those from a business background. Short answer: there isn’t, but.
If I remember right, Google Cloud did experiment with a Business Essentials certificate. However, that didn’t eventually see the light of day, it seems. Some of the courses meant for that are no more available or some along those lines are still available in some form or the other on Coursera and Pluralsight.
What is there, though, is a Business Professional Accreditation. I went through it to understand what it is about and to give guidance to this audience. These are my notes on trying out this course and test.
I wouldn’t attach much value to this accreditation. Because you can try it as many times as you want. You could even have somebody else answer the questions for you. This one is hardly any better than any of the online courses with a completion certification. It wouldn’t carry much value with an employer.
However, if you are new to Google Cloud and if you are from a sales or pre-sales team, this could be a milestone at the beginning that gives you some knowledge and confidence.
If anything, you should be going through this course for your own learning and not for the final accreditation.
The questions on the exam are quite high level. They are all multiple choice questions. Product fitment from a compute and storage perspective is the primary focus. For example, what would be the product to use with containerized workloads (answer: Kubernetes, though Cloud Run and App Engine Flex are also options now). Or what product would be best for a mobile and web backend (answer: Firebase)? There are questions like what are containers and what is a Virtual Private Cloud. And others that check your understanding of the terms reliability, availability, etc. These would be very familiar and easy for any one who has some exposure to Google Cloud already. For those without that experience, the course that goes with this accreditation should be good enough. The course has been quite nicely laid out.
However, there are multiple issues with the questions. Some of the questions are outdated. In my opinion, it is also missing key topics like migration planning and cost management. It definitely hasn’t received much attention in the recent past. In a few cases, the answer options weren’t very good in my opinion. There were even some grammatical errors occasionally. Even with a fairly good knowledge of Google Cloud at this level, I got only 80% on one section. That’s 6 wrong out of 30. It’s true that I wasn’t taking the test very seriously and was just going through it quickly to find out what it is about, but even then, I consider that number quite high. My personal opinion is that it is because some of the questions and answer options were weird. You should also try it and let me know your opinion about the questions. My guess is that the questions are based exactly on some point made in the course, which, honestly, I did not go through.
You can go here to sign in for the course and exams: https://skillshop.exceedlms.com/student/path/7325-google-cloud-platform-business-professional-accreditation#
skillshop.exceedlms.com
For those appearing for the various certification exams, here is a list of sanitized notes (no direct question, only general topics) about the exam.
Overall notes across all GCP certification exams
Notes from the Professional Cloud Architect exam
Notes from the beta Professional Cloud Developer exam
Notes from the Professional Data Engineer exam
Notes from the Associate Cloud Engineer exam
Notes from the beta Professional Cloud Network Engineer Exam
Notes from the beta Professional Cloud Security Engineer Exam
Notes from the Professional Collaboration Engineer Exam
Notes from the G Suite Exam
Notes from the Professional DevOps Engineer Exam
A collection of posts, videos, courses, qwiklabs, and other exam details for all technical certification exams: https://github.com/sathishvj/awesome-gcp-certifications
I’ve been making videos on applying Google Cloud and preparing for the exams. You can subscribe to the channel here:
youtube.com
Wish you the very best with your Google Cloud learning. You can find me on LinkedIn and Twitter.
tech architect, tutor, investor | GCP 12x certified | youtube/AwesomeGCP | Google Developer Expert | Go
58 
2
58 
58 
2
tech architect, tutor, investor | GCP 12x certified | youtube/AwesomeGCP | Google Developer Expert | Go
"
https://medium.com/@serhatcan/aws-microsoft-azure-google-cloud-gibi-bulut-bilisim-servisleri-pahali-mi-e191928f89e3?source=search_post---------334,"Sign in
There are currently no responses for this story.
Be the first to respond.
Serhat Can
Dec 9, 2017·3 min read
10 - 100 milyon kullanıcı seviyesinde olan ve insanların çok vakit geçirdiği SnapChat, Slack, AirBnb gibi firmalardan tutun, devasa endüstri devi GE, Siemens gibi firmalar bile kendi sunucularını yönetmek yerine bulut bilişim servislerini kullanıyorlar, kullanmak zorundalar. Peki bu firmalar için bile bulut bilişim pahalı değilse, size neden pahalı geliyor?
Bulunduğunuz ülkenin yasa ve yasaklamalarından dolayı kullanmamanız dışında bulut bilişimin sınırsız kaynaklarından neden faydalanmadığınızı açıklamanız çok zor hale geldi.
Bulut bilişim dediğimiz zaman aynı şeyi anladığımızdan emin olmak için çok kısa bulut bilişim servislerinin genel özelliklerini şöyle sıralamak istiyorum:
Amazon’da 1 GB RAM ve 1 CPU çekirdeği sunan standard ve güçsüz diyebileceğimiz Linux sunucu bile aylık 8.5$ (30+ TL).
Haklısınız, olaya sadece böyle bakarsak pahalı gelebilir. OpsGenie’de ilk işe başladığımda, AWS’in sağladığı nimetlerin farkına varmadan önce, bulut bilişim servisleri bana da pahalı görünüyordu.
Peki aslında ne zaman pahalı:
Güvenlik ve güvenilirlikten bahsetmek istememin sebebi bulut bilişim denilince insanların aklına gelen ilk soru işaretinin bu konular ile ilgili olması. Teknik detaya girmek istemediğim için bu konularda şu servis şunu sağlar bu servis bunu sağlar diye yazmayacağım. Ama bulut bilişim servislerinin sağladığı imkanları kendiniz sağlamanız günümüzde neredeyse imkansız. Ücret açısından bakarsak bulut bilişim servislerinin sağladığı güvenlik sertifikalarını almanız veya uygulamanızın problemlere karşı daha dayanıklı olması için dünyanın değişik yerlerinde çalışmasını sağlamanız ise çok pahalı işler.
Yani bulut bilişim sanılanın aksine çok özel bir uygulama alanınız (örneğin savunma sanayi) yoksa çok daha az paraya güvenlik ve güvenilirlik sağlayabileceğiniz bir ortam.
Nasıl faydalanacağınız ve yöneteceğinizi biliyorsanız bulut bilişim servisleri çok çok ucuz ve her geçen gün daha da ucuz hale geliyor.
Ulaşmak ve iletişimde kalmak isterseniz sosyal medya hesaplarım:
Medium: https://medium.com/@serhatcanTwitter: https://twitter.com/srhtcnLinkedin: https://www.linkedin.com/in/serhatcan/
Başka yazılarımı okumak isterseniz:
medium.com
medium.com
linkedin.com/in/serhatcan/
80 
80 
80 
linkedin.com/in/serhatcan/
"
https://medium.com/techking/talking-big-data-at-google-cloud-next-18-london-474fd3b08b40?source=search_post---------335,"There are currently no responses for this story.
Be the first to respond.
By Kenneth MacArthur and Nicholas Brenwald
King’s recent attendance at Google Cloud Next ’18 London offered a stage (or several!) where we could share our knowledge and experience working with Google Cloud Platform (GCP) thus far, and our ideas for leveraging GCP in the future.
King took part in a series of sessions at Next about our ongoing GCP analytics platform migration, including Rethinking Big Data: Moving from Complex Analytics to Actionable Insights with Google Cloud, where we had the opportunity to share with Google’s Sudhir Hasbe the progress of our petabyte-scale migration to GCP.
Below is an edited version of our interview with Sudhir.
We started to notice a general industry trend towards public cloud — both for infrastructure in general and particularly in the analytics and machine learning (ML) space, where the direction of travel seemed pretty clear — hardly anyone is doing greenfield data warehouse deployments on-premise these days, for example.
At the same time as that, we started to see some themes emerging inside our business that also pointed us towards looking at public cloud — teams wanting to be able to more easily provision multiple environments, the business wanting us to be able to support multiple tenants, some of our colleagues working with machine learning wanting to have access to the latest ML hardware: GPUs, TPUs and so on. We had a generally reliable stack, but with some reliability issues here and there. And we were asking ourselves: do we want to be in the low-level infrastructure operations game, or do we want to focus higher up the stack?
Taking those two strands together — the wider industry trends, and the themes inside our business — we thought: there’s something here; we need to look at this further.
The first use case we wanted to tackle with cloud was the replacement of our on premise data warehouse. King used to store all of its data in a 20 PB, 500 node, on-premise Hadoop cluster. Most users would query this data using Hive, or perform more complex analytics using MapReduce or Spark. For performance reasons, we also used to copy a subset of our data to an in-memory analytics database that whilst quick, didn’t scale so well and really struggled with our volumes.
Over many months, we did an extensive evaluation of various cloud solutions and found that GCP provided the best analytics capabilities and performance for our needs. In addition, we really believe in Google’s future vision for the platform and when you consider Google’s contributions to the analytics and ML space with things like Beam, TensorFlow, we felt that you’re clearly leading the market.
We also felt that options such as BigQuery flat-rate pricing, auto scaling, transparent billing allowed us to present a really compelling argument for choosing GCP. The billing aspect has been really eye opening. Previously if a team needed a reserved YARN pool, or to store a large volume of data in HDFS, it was really difficult to determine the cost. Now we’re able to accurately determine this for all new initiatives and then decide if they are actually worth pursuing.
BigQuery forms the core of our new analytics platform and we are already heavily invested with over 10 petabytes stored. We love it, it takes us out of the capacity planning game and being a fully managed service, allows us to focus on use cases actually relevant to King and gaming.
Each day, we ingest around 50 billion game events into BigQuery using Dataflow. In fact, we’ve already processed over 8 Trillion game events this way and find Dataflow provides a really neat and cost-effective way to ingest data. We make heavy use of auto scaling, game launches used to be really demanding; even with the best playtesting in the world, it’s really hard to predict how successful a game will be and capacity plan accordingly. With Dataflow autoscaling if we get spikes in traffic, our jobs now auto scale out accordingly.
We’re also quite heavy Dataproc users. We were basically a Hadoop shop prior to embarking on this migration, so we have years of IP written in Hive, MR and Spark. We don’t have the time, or desire to rewrite all of these old jobs, so we use Dataproc as a really useful migration tool allowing people to migrate to the cloud quickly without much thought or attention. Once our migration is complete, we can always go back and revisit these jobs, to see if they would be better handled with a different product such as Dataflow, or ML Engine.
Adding to what Nick touched on around ML Engine, we have a team at King that is really focused on applications of AI for our business and they’ve been all-in on GCP for some time — using, amongst other things, ML Engine, GKE and Cloud Pub/Sub. They’ve built a deep learning tool on GCP that helps our creative teams design the most enjoyable content for our players, which is proving to be super useful.
More broadly, with the data teams and the machine learning folks now using GCP, we’re seeing some of our colleagues also beginning to look at the platform, and a hybrid cloud strategy starting to take shape at King.
It’s still early days — we’re just coming towards the final stages of our migration at the moment. But there are three benefits we’ve already seen that are worth calling out:
My main advice would be to do everything in your power to reduce the migration period. At King, we took the conscious decision to complete our migration within 1 year, knowing full well that this would cause a certain amount of pain and discomfort.
We felt the alternative would have been far worse. Running in parallel both on prem and in the cloud for an extended period would have cost us more financially and distracted our engineers from the more important value-add work that is so key in making King a world leader in mobile games.
Additionally, we found that once we started on the migration path, our whole engineering org got really excited about new possibilities and were really impatient to get stuck in using the new tools. Once they got a taste of working with GCP, the speed at which they could run their queries, the ease at which they could spin up new environments and track their costs, administering the old systems became the least appealing job in the company.
As Nick mentioned, we took the conscious decision to move our data warehouse to GCP relatively quickly, so as we move into 2019, we’re looking forward to spending some cycles optimizing and making more cloud-native some of the things that we lifted and shifted from our old environment this year.
Looking beyond that, we’re excited to start looking at some of the unique capabilities that drove us to GCP in the first place — BigQuery ML, for example, which we’ve got a couple of people looking at already. As Airflow users, we’re also keen to look at Cloud Composer, and whether that can allow us to work even more efficiently.
Let’s not forget, GCP isn’t only a platform for analytics and machine learning. Whilst we expect certain key parts of our business to remain on-prem for now, we absolutely see our hybrid cloud strategy broadening over time. We are already starting to identify other areas within King where it may make sense to migrate fully or in part to GCP.
“We are already starting to identify other areas within King where it may make sense to migrate fully or in part to GCP”
King has a great partnership with the Google Cloud team, and we enjoyed sharing our journey to GCP on the Next London main stage.
As we look towards the coming year, we would like to see Google continue to evolve and improve the platform — particularly in the following three areas:
We hope both King and Google Cloud will have more exciting things to share at Next San Francisco ‘19!
You can watch the full interview here.
See our latest jobs at king.com
Read the latest tech articles from King.
54 
54 claps
54 
Read the latest tech articles from King. Learn about our tech and our company culture.
Written by
King Tech Blog - awesome tech teams at King. Everything you read comes from the minds of our tech gurus and hearts of our Kingsters!
Read the latest tech articles from King. Learn about our tech and our company culture.
"
https://medium.com/@maxxsh/setup-free-ssl-certificate-for-website-on-google-cloud-platform-90a249a6bce3?source=search_post---------336,"Sign in
There are currently no responses for this story.
Be the first to respond.
Max Shestov
Jan 25, 2021·3 min read
This guide provides step by step instructions that will help you to install the auto-renewing Let’s Encrypt SSL Certificate using automated client Certbot for your site hosted on GCP.
Make sure that you have: ∘ A running VM instance with the Apache server on Ubuntu 20.04 installed.  ∘ Your domain name set up with your Google Cloud instance. ∘ A website that works over an HTTP connection. (URL of the website begins with “http://”. If you change it to “https://” it will not load the page).
Log into your Google Cloud Console and selectCompute Engine > VM InstancesOn your VM instance click SSH to open the terminal.
First, the Certbot needs snapd installed. But If you’re running Ubuntu 16.04 or later, you don’t need to do anything. Snap is already installed and ready to go. You can check it is installed by attempting to run snap version on the command line:
Ensure that the latest essential release of a snap is installed and tracked for updates.
Now, install Certbot, executing the command
Then create the symbolic link to the Certbot directory
Next, launch the Certbot to generate an SSL Certificate for your website
Provide an email address (optional), that Let’s Encrypt automatically send you expiry notices when your certificate is coming up for renewal. If the certificate is already renewed, they won’t send an expiry notice.
Agree with the Terms of Service by entering Y.
Enter Y if you are willing to share your email in order to get news from Let’s Encrypt project. Otherwise, enter N.
Type your domain with both naked domain name and www sub. If you have a subdomain enter it also.
Let Certbot edit your Apache configuration automatically.
Choose a vhost with enabled HTTPS typing its number
All done!Now you have enabled the SSL certificate provided by Let’s Encrypt with automatic renewal by cron job systemd timer.
By default, the crontab attempts to renew the certificate twice a day, but renewal will only occur if expiration is within 30 days.You may change the periodicity in the tasks file running by Cron, a time-based scheduling service.
For example, the code * 4 1.15 * * will run on every first and 15th day every month at 4:00 am. More about scheduling tasks in a crontab file see the documentation here.
Simulate automatic renewal
Verify that Certbot works by typing in your browser’s address bar “https://” at the beginning of the website’s address.
Lead Developer at Reverence Global. Husband of a wonderful wife, Entrepreneur, Dad
See all (53)
440 
1
440 claps
440 
1
Lead Developer at Reverence Global. Husband of a wonderful wife, Entrepreneur, Dad
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/using-serviceaccountactor-iam-role-for-account-impersonation-on-google-cloud-platform-a9e7118480ed?source=search_post---------337,"There are currently no responses for this story.
Be the first to respond.
I’ll be adding updates to my new blog here: https://blog.salrashid.me/
The serviceAccountActor IAM role on Google Cloud has some very useful and powerful capabilities. It is an IAM role that allows you to grant another user or serviceAccount the ability to impersonate a service Account. In this way, you can have service account A impersonate B and acquire the access_tokens or id_tokens for B.
This article covers how you can acquire id and access tokens for service account B by service account A. You can also have a end-user assigned the serviceAccountActor role too. For clarity, I’ve separated out the id and access token steps below.
Note:
One final mechanism described here is creating an arbitrary JWT using Service Account’s private key. These JWTs are signed by one of the keys associated with a given service account and can convey additional verifiable information within the claim such as the intended target (audience) and custom scopes.
For more information see
You can find the script referenced here on my github page
Update 7/1/18: GCP IAM now allows for IAMCredentials.generateAccessToken(). This feature allows you to directly genreate an ID or AccessToken and place conditionals on delegation too. While the samples below will work, I’ll rework the sample in this repo to use that API as shown in this example. In several ways, this article is obsolete given the direct API is now available. I’ll keep it posted as a reference. The full code sample for iamcredentials API is shows at the end of the article
The first step is to assign the serviceAccountActor role on B to A. That is, you are allowing A the permission to act as B. This configuration is done on the IAM Permissions page by selecting B as shown below.
In the example screenshot,
Note: serviceAccountB_ID does not even have any valid certificate keys. This means there are no valid physical, distributed certificate key files.
As mentioned, you do NOT have to use a service account to impersonate another one; you can easily assign the serviceAccountActor role to an end user (e.g. user@domain.com).
Since we are doing operations as service Account A, we need to create a client for access to IAM using Application Default Credentials:
After we assigned the role, download the certificate for A (the service you will use to impersonate B). You may also impersonate B as a user and to do that, you will need to initialize a client as an end user. For more information on, see Google Cloud Authentication Samples.
Now that we have an initialized, authenticated client for A, we need to generate a JWT with some specific claims. Since we are creating an access_token request, the JWT needs to be in the form:
Set the scopes to limit the capabilities of a given token. A list of scopes can be found here.
Now that we have an JWT claim set, we need to sign it using A’s credentials but instruct .signJWT() for B:
Now that we have the signature for the JWT, we need to append the signature to the unsigned part
Now you have an access token. With this, you can access any GoogleAPI that is scoped correctly and to which B has access.
One way to initialize a Google API client given a raw access_token is to use oauth2client.client.AccessTokenCredentials. Other languages have similar bindings.
You can also verify the access_token by interrogating the tokeninfo endpoint as shown here:
Though id_tokens and access_tokens have similar flows, they are used for very different things: as the name implies, access_tokens are used to grant access to google resources (eg. GCS, PubSub, etc) while id_tokens simply assert the bearers identity.
These id_tokens are digitally signed by google so given an id_token, a system can verify its authenticity by verifying its signature against Google’s public certificate. The id_tokens are simply convey who the caller is (not what it can do).
If you need two system to communicate securely, you can pass (via SSL), an id_token. The receiving system can locally validate the token using a cached copy of Google’s certificate. However, Google-issued ID tokens simply identify the caller and cannot assert if the receipient of the token is the intended target. For that, you can mint a Service Account id_token as described below.
See:
The first step is to create an unsigned JWT but have the scope to the service Account B’s ID:
Note the audience is different than for an access_token.
Now that we have an JWT claim set, we need to get a jwt using A’s credentials but instruct .signJwt():
Now that we have the id_token, we can transmit it to another system.
The recieving system that gets the id_token must validate its authenticity.
You can validate it using some standard libraries too:
The public certificate used by Google to sign can be found at:
Users can also generate a JWT that is signed by a given Service Account. The steps here are very simple: create a JWT and use .signBlob() to sign the header and claims. The signedJWTs can be used to verify caller identity as well as see if the recipient is the intended target. Finally, you can customize the scopes fields to convey capability for the token.
You can specify the intended target for the JWT using the aud field. In this case, the audience is ‘System C’. The scopes field is arbitrary and user-defined.
The JWT issued may look like the following:
A GCP service account has multiple keys that are rotated. Unless the current keyID is specified in the JWT header, you need to iterate the keys in the keystore URL (shown below) to verify the correct one.
If the key used to sign is: ‘cc1080d1a4c61e8cb821331a5a2652dee2c901a1’, you may add on the ‘kid’ header value to the JWT.
Once you have the JWT, you must verify the authenticity of the JWT by verifying against the public certificate. The public certs for any Google Service account is visible at URLs similar to: For serviceAccount B:
The following node sample verfies a self-signed JWT:
You can find the script referenced here on my github page. To use it, you need to download a certificate for service Account A, assign the serviceAccountActor role to it for B, then install the libraries and invoke it with service account B’s ID value
Using IAMCredentials API to issue Access and ID Tokens
Finally, I’ll rework the samples above to use iamcredentials.generateAccessToken() and iamcredentials.generateIDToken() shortly. For now, here is the sample set for that:
Google Cloud community articles and blogs
24 
24 claps
24 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/@sathishvj/notes-from-my-beta-google-cloud-digital-leader-certification-exam-e4f9fd1b119e?source=search_post---------338,"Sign in
There are currently no responses for this story.
Be the first to respond.
sathish vj
Jul 5, 2021·6 min read
Subscribe to my YouTube channel that teaches you to apply Google Cloud to your projects and also prepare for the certifications: youtube.com/AwesomeGCP. Check out the playlists I currently have for Associate Cloud Engineer, Professional Architect, Professional Data Engineer, Professional Cloud Developer, Professional Cloud DevOps Engineer, Professional Cloud Network Engineer, and Professional Cloud Security Engineer.
I have created practice tests for the Cloud Digital Leader. Check them out here: https://www.udemy.com/course/google-cloud-certified-cloud-digital-leader-practice-tests/
I took the Cloud Digital Leader beta exam on the 4th of May. And I passed!
I was supposed to take it on the 3rd of May but due to some technical glitch the proctoring team postponed it to the 4th of May. A side note: usually when there is a technical issue, they postpone the exam by about 15 minutes; but, oddly, this time they postponed it by a day and 15 minutes.
There were also another glitch with the exam. I had marked a few of them for review, but at the end of answering all the questions, the button for “Review All” did not show up. It wasn’t much of an issue for me as there was time to review all the questions. Still, I hope it will be fixed for the generally available exam.
My expectations of the exam were that it was going to be super easy. I barely studied anything thinking that the questions would be very simple for somebody who’s already got other certifications. Since this wasn’t a Professional certification or even an Engineer focused one, I assumed it will be similar to the now defunct Business Professional Accreditation, which was super easy. But, since my exam got delayed by a day, I did read some short posts from people who took the exam on May 3rd stating that the exam was tough. I realigned by expectations of the exam based on those posts.
The exam had very wide breadth but limited depth. Except the Professional Collaboration Engineer, content from all other certification exams were represented. The greatest similarity was to the Professional Cloud Architect and the Associate Cloud Engineer, with bits of Network Engineer, Security Engineer, Data Engineer, and Machine Learning Engineer thrown in. Whereas the Engineer and Professional certification exams would ask about command line tools, cli options, and deeper technical features, this one didn’t go that deep. You still need to know pretty much the entire product list, the requirements that they are a good fit for, and how to apply the main/popular products to solution a customer requirement.
The exam seems to be (as the title indicates, duh!) for leaders who are leading a technical team. Depending on the size of the company this could be a project manager, a mid-level manager, or head of engineering. It would also apply perfectly for some pre-sales engineer roles, who need some technical knowledge to quickly respond to proposal requests. Though a cloud engineer or an architect could pass this exam fairly easily, it doesn’t add significant value for them beyond the professional exams. However, I would strongly recommend it as a starting point if you are feeling overwhelmed with one of the Professional certifications. I’ve realized that it is quite convenient to pick up the certifications in increasing amounts of difficulty — you learn progressively and also build your personal confidence alongside.
I have also been recommending this exam to people in purely sales and pre-sales too with no technical knowledge of Google Cloud. It might not fully apply to those roles, but I feel that even if you don’t pass this exam, it is helpful to learn about the technologies. There is value in the preparation. So you should definitely give it a shot.
Given the 100s of products and sub-products and the vastness of the material, you are likely to be overwhelmed. If you already have other Google certifications like ACE, PCA, PDE, PCNE, PCSE, and PMLE, you could probably waltz into this exam and crack it. For the others, you might benefit from a boundary of material to primarily focus on. This is my recommendation:* Read about all the products and what they are used for. It’s enough to focus on top level features of products, a summary of which you can even find on the product overview page. * For main products you need to go a little deeper, some of which are listed below in this post. * For those, study best practices also.* Study how products fit together in a data pipeline.* You can skip Google Workspace, Gaming Servers, Maps APIs from this comprehensive list: https://github.com/gregsramblings/google-cloud-4-words/blob/master/DarkPoster-lowres.png.
* Anthos — when to choose this for migration* Apigee — knowing what this does would be important for certain migrations. Again, just get a top level idea of product features.* Architecture choices — how to ingest and process data. Which product options do you have and which will you use in a given scenario.* Bare Metal Solution — where would you use this? * BigQuery — for analytics* Billing — how are infrastructure and managed products costed?* Billing — setting up billing. * Cloud Build — usage of build pipelines* Cloud Filestore vs Cloud Firestore/Firebase* Cloud Identity — SSO and SAML* Cloud Storage features — storage classes, redundancy options, managing object life cycle* Compliance Reports — (https://cloud.google.com/security/compliance/compliance-reports-manager)* Compute options that can shut down to zero when not in use* Containers — Know advantages of containers vs VMs.* Containers, Kubernetes, Anthos — don’t go as in-depth as you would for PCA or DevOps. * Cost Management — how do you ensure expenditure is within plans and budgets? How do you keep track of it?* Creating data pipelines — from ingestion, to processing, to storage, and analysis* Data Studio — what is it used for* Data Transfer options — Transfer Appliance, BQ Data Transfer, Storage Transfer Service. When would you use which?* Database Migration Tool* Database storage technologies — different options available for different storage requirements. Know things like SQL vs NoSQL, regional vs global.* Developer options — debugging, tracing, etc.* GCE — Provisioning VMs according to need* GCE — use of pre-emptible VMs* Hybrid Networking Options* IAM — Groups* IAM — Principle of Least Privilege* IAM — resource hierarchy. How to setup org, folders, projects based on departments and teams.* ML — bqml, automl, api options, etc.* ML — predefined options vs programmable options* Migrate for Compute and Migrate for Anthos* Migration — Choosing between private data center and public cloud* Migration — how will you migrate VMs, Databases, * Networking — Cloud NAT, VPN, Cloud Armor* Networking — Private Google Access* Pricing Calculator * Security — Security Command Center* Serverless vs Serverful options* Stackdriver/Operations options and features* Storage options — what is the cost/performance difference between PDs, SSDs, and Filestore* Support — different enterprise support options available and which to use for your use case* VM flat reservation
For those appearing for the various certification exams, here is a list of sanitized notes (no direct question, only general topics) about the exam.
Overall notes across all GCP certification exams
Notes from the Professional Cloud Architect exam
Notes from the beta Professional Cloud Developer exam
Notes from the Professional Data Engineer exam
Notes from the Associate Cloud Engineer exam
Notes from the beta Professional Cloud Network Engineer Exam
Notes from the beta Professional Cloud Security Engineer Exam
Notes from the Professional Collaboration Engineer Exam
Notes from the G Suite Exam
Notes from the Professional DevOps Engineer Exam
Notes from the Professional Machine Learning Engineer Exam
A collection of posts, videos, courses, qwiklabs, and other exam details for all exams: https://github.com/sathishvj/awesome-gcp-certifications
Check the FAQs here: https://medium.com/@sathishvj/frequently-asked-follow-up-questions-on-google-cloud-gcp-certifications-438e1addb91d.
Wish you the very best with your GCP certifications. You can reach me at LinkedIn and Twitter. If you can support my work creating videos on my YouTube channel AwesomeGCP, you can do so on Patreon or BuyMeACoffee.
tech architect, tutor, investor | GCP 12x certified | youtube/AwesomeGCP | Google Developer Expert | Go
83 
1
83 
83 
1
tech architect, tutor, investor | GCP 12x certified | youtube/AwesomeGCP | Google Developer Expert | Go
"
https://medium.com/hackernoon/going-live-with-google-cloud-compute-engine-70542da89aa8?source=search_post---------339,"There are currently no responses for this story.
Be the first to respond.
In the previous article we saw how easy it’s to go live with Google’s App Engine. We also learned how costly that could get in the long run. A simpler and cheaper solution for start-ups and small/medium businesses would be Google’s Compute Engine.
Disclaimer: this topic is aimed at developers who are familiar with or don’t shy away from a terminal.
The most frequent question I get when discussing this topic is: “What’s the difference between App Engine and Compute Engine?”. The oversimplified answer is that App Engine does magic out of the box whereas Compute Engine lets you implement your own magic.
A more in-depth answer is that:
For all that manual labor, you do have some major benefits, two of which are: flexibility and cost.
Please read the previous article in order to create your Google Cloud project and install Google Cloud SDK. From this point forward let’s assume again that we’re the owners of example.com.
Go to your Google Cloud / Compute Engine / VM instances section and select Create instance. From here you should:
You should be able to see your instance in the VM instances view, like so:
You’ll see that your instance has an internal IP and an external IP. The external IP is what we care about, since we’ll be adding that to our registrars DNS Manager so we can link the domain name to this server.
Before we do that however, we need to make sure we reserve a static IP. The one we’re currently seeing is what is known as an ephemeral IP (meaning it won’t last long).
To do that, go to Google Cloud / VPC Network / External IP addresses and click on Reserve static address:
Go back to Google Cloud / Compute Engine / VM instances and you’ll see that you probably have a different external IP address. This one is static and we can add it to our registrar. We’ll do that after our server is actually displaying something.
We need to install a few things on the server, so click on the SSH button in the table with our instance name:
Based on the OS you’ve chosen, these next steps will be different but if you’ve chosen CentOS, the package manager will be yum. We need to run the following commands:
At this point, if we type our static IP address in a browser, we’ll see the default Nginx landing page. So far so good, now we need to teach it to serve our own site. We’ll configure it to server a static HTML page:
Don’t worry if VIM is tricky to use, we only need to paste this into it:
Then press ESC then :wq to save and exit the file (remember this). This is the most basic config file… you give it the root to our site, the name of the server and your done. The server_name will be localhost until we link the domain with the server IP address, after that we can write the actual domain name here.
Next we need to disable that default page that nginx is runing, so edit the nginx.conf file like this:
Press the Insert key so you can edit the file, find the http {} section and comment out the entire server {} block by adding a # in front of each line, then save and exit the file.
Almost there, we just need to restart nginx.
Open a browser tab, navigate to the server static IP address aaaaaaaand:
Why tho`, we did everything right… right? Well no… welcome to the wonderful world of SELinux. If you’re on a distribution of Linux which doesn’t have the SELinux module, like Ubuntu… you’re probably up and running already. The rest of us on Cent OS or other Red Hat distributions are stuck here.
SELinux was originally authored by NSA (yes, that NSA) and Red Hat Software which provides a security mechanism for access control. We have the configs in place but SELinux won’t let nginx access them. To make it work, simple run the following command:
Remember it, as you’ll need to run it every time you add new folders or files here. As for WHY it works, I’ll leave you with this article, which explains the basics of SELinux.
Refresh the page again and eureka! We’re serving a static HTML page. Next, let’s link our domain name with this machine.
In the previous article I mentioned I’m using GoDaddy as my domain registrar. Regardless of what you’re using, the next step should be similar. Log into your domain registrar and go to your domain DNS Manager. Assuming you’re using IPv4 for your static IP, add two new records with:
In the table above, the Name is the Host, and the Value is the static IP. If you’ve chosen IPv6, you do the same thing, but instead of A records, you’ll be creating AAAA records in the same fashion.
Edit your server config file and add the correct domain_name (which is currently localhost)
The DNS changes can take a while to propagate, but once they do, you’ll be able to access your server using your own domain name instead of the static IP.
All that’s left is to create some SSL certificates so everything is running on HTTPS and we’re done. The simplest and free option is to use the certbot utility which generates certificates issued by Let’s Encrypt. Certbot also offers utils based on your web server (in our case nginx) and you install it like so:
If you’ve checked HTTP and HTTPS when you created the VM instance, you can skip this step. If not, we need to allow access through the firewall on ports 80 and 443 so run these commands:
Once we’ve done these steps, we can ask certbot to issue an SSL certificate for our domain and sub-domain like so:
If you’re running it for the first time, certbot will ask you a few questions, one of which will be related to HTTP traffic; specifically if we allow HTTP traffic or only HTTPS, I will pick: “Secure - Make all requests redirect to secure HTTPS access”
Certbot will automatically update your nginx config file with the newly generated SSL certificates. You can test the strength of your SSL certificates here. These are free but need renewal once every 3 months.
There are 2 ways to approach this, you can:
If you create a SSL auto-renewal cron job, it’s recommended that you run this command at least once or twice per day. On CentOS 7, cronie is running by default.
If you want to use nginx as a reverse proxy (in case you’re running a NodeJS instance on another port) you only need to edit your example.com.conf config file to something like this:
Take note of the proxy_pass and add the port your NodeJS server is running on. As a side note, you should probably run your NodeJS through PM2 or a similar process manager.
You now have a very basic site running on Google Cloud Compute Engine with SSL auto-renewal.
Congratulations and happy coding!
#BlackLivesMatter
75 
2
75 claps
75 
2
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
CTO, Evozon
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
"
https://medium.com/google-cloud/i-think-google-cloud-is-the-best-best-tech-best-pricing-best-support-best-roadmap-and-best-4b4e17856505?source=search_post---------340,"There are currently no responses for this story.
Be the first to respond.
I think Google Cloud is the best — best tech, best pricing, best support, best roadmap, and best people. I work for Google Cloud, so I may be biased. So when folks ask me why Google Cloud is better than it’s competitors, perhaps it’s better for real users to speak for the platform themselves.
So I put together a list of customer opinions — folks who live and breathe cloud every day, folks who take the time to understand the differences between vendors, folks who knows performance and cost characteristics because they actually run real-life workloads and deal with scaling, billing, and maintenance. Here it goes!
(this is just a small quick gathering — send me more, and I’ll be sure to update the list!)
Quizlet compares Google and Amazon VMs based on networking, performance, and price. Guess who comes out on top!
2. May 2016 — “The future of cloud computing: Google Cloud!”
Obulpathi from Monsanto opines on Easy of Use aspects of Google Cloud Platform and Amazon Web Services.
3. May 2016- “5 unique features of Google Compute Engine that no cloud provider could match”
This Forbes editorial discusses what sets apart Google’s VM service, Compute Engine (TL;DR: sustained use discounts, preemptible VMs, custom VM shapes, online disk resizing, shared storage).
4. June 2016 — “GCE vs AWS in 2016: Why you should NEVER use Amazon!”
A rather one-sided piece by thehftguy that talks about limitations of Amazon EC2 and how Google Compute Engine’s architecture overcomes such limitations.
5. July 2016- “Why we moved from Amazon Web Services to Google Cloud”
Michael Lugassy of adtecho.com explains his reasons for migrating from AWS to Google Cloud, and some downsides of Google as well!
6. February 2016 “Announcing Spotify Infrastructure’s Googley future”
Spotify describes reasons for their move to Google Cloud — level of innovation, big data technologies, and people. And, remember folks, next time someone tells you “Spotify chose GCP because they got a good deal”: when the market is not commoditized, it’d be foolish for companies to make decisions on cost.
7. September 2016 “Top 5 Advantages of Choosing Google Cloud”
A nice quick recap, although “Google Cloud Hosting” doesn’t have the same ring to it!
When you do the math, Google’s SSDs are 10x-ing the competition on the price-performance spectrum.
2. October 2016 — “A Survival Guide for Containing your Infrastructure”
Tripstr switched from AWS to GCP, containerized, and saved 75% on their infra bill. Not bad!
3. November 2016 “Google is 50% cheaper than AWS”
A comparison of GCE and EC2 prices.
Google Cloud Storage comes out ahead on throughput, but loses on latency. Mr. Johnson correctly points out that the latency tradeoff is due to Google Cloud Storage being a multi-region service by default, unlike Amazon/Azure options, which are closer to “GCS regional buckets”.
2. October 2016 “Google Cloud Storage vs AWS S3”
Jishnu from Vuclip finds GCS from 4x to 20x faster than S3 for uploads while being 35% cheaper than S3.
This O’Reilly blog compares EMR to Google Cloud Dataproc.
2. September 2016 — “Fun with .. Google BigQuery”
Mr. Alvarez is entirely blown away by BigQuery, and leverages the GHCN Public Dataset to come up with some fun conclusions.
Google Cloud community articles and blogs
85 
2
85 claps
85 
2
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
VP of PM at Firebolt. All views are my own.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://itnext.io/writing-google-cloud-functions-in-go-fb711f33459a?source=search_post---------341,NA
https://blog.kovalevskyi.com/semi-managed-jupyter-lab-with-access-to-google-cloud-resources-cc6f9e439416?source=search_post---------342,"There are currently no responses for this story.
Be the first to respond.
Just imagine the following use case:
You are opening a link to private Jupyter Lab in the cloud that authenticates you with your GCP account. From the Jupyter Lab, you are uploading data from the private file on Google Cloud Storage to the BigQuery (right from the Lab). Later, you are executing several queries to BQ and visualizing data right from the Jupyter Lab. Finally, you are executing the training of the model with TensorFlow and the data from BQ on the GPU attached to the Jupyter Lab. Now you are uploading the trained model back to GCS.
Moreover, all this without leaving managed Jupyter Lab. Sounds like magic, doesn’t it? Now let me show the screenshot of what you will have by the time you finished reading this article:
I am going to guide you through the setup process step by step with many pictures!
**** IMPORTANT ****
THIS ARTICLE IS DEPRECATED. With DeepLearning VM M19 it is possible to get URL link for accessing Jupyter Lab without any additional work. Please look on the main article, see section “Create Instance With Simplified Jupyter Access Feature. Beta.”
First, let’s start with…
The first thing that you need to do is to create an instance with managed Jupyter Lab. Let’s go to the main GCE page (if you don’t have a GCP account yet you can sign up for a free trial here).
Your page should look like this:
Now it’s time to press the “Create” button. You should see a dialog of instance creation, and it looks like this:
I know it has a lot here, but do not worry, I am going to walk you step by step. First, you need to set a name of the instance and the region. In my case, it is “jupiter-lab” and “us-central1” (set by default). The name is something that you can specify any. The region, on the other hand, depends on the GPU that you are planning to use. BTW, if you are not planning to use the GPU with the instance, you can skip the following section and jump straight to the OS pick section.
Not all GPUs are available in all regions. General rule of thumb, region that you pick should be the region that is closest to you and has the GPU you need (and has the required quotas). List of all available GPUs on GCP with the regions where they are available is here. In my case, I want to use Tesla P100, and I’m on the west coast. Now, looking on the doc …
I can see that I need to use us-west1-b. So now I can set details for my new instance like this:
Okay, we now know that we have picked the right region. However, we also need to be sure that we have the right quotas to create an instance with P100 in that region. Quotas are protecting you from using more resources that you are intended to. To check and edit your quotas go to the following page. It should look like this:
Now, click on the “All metric” drop-down and select “none”:
In the search box write “P100” and select “NVIDIA P100 GPUs”:
Be careful, do not select “Preemptible”, unless you intend to use preemptible instances. At this moment your list of quotes should look like this(with only P100):
Now we can select the region (yes quotas are per region). Click on “Locations” and pick “None”:
From the list, scroll down and find the location that you need (in my case it’s us-west1) and select it:
At this point your screen should look like this:
First of all, if you already have quotas (in my case quota is 3, and I am using 0), you can skip the rest of this part. If your quota is 0 you need to check the checkbox next to the quota item and press the “Edit quotas” button:
On the right side you see the pop-up with the fields that you need to fill in:
Press next and fill out how much GPUs do you need:
After filling in all the information, you can submit the request and wait for the confirmation letter from Google. I think it might take ~1 business day to fulfill the request. When the request is fulfilled you will get the magic confirmation letter:
Now, when we are finally done with the quota, we can go back to our instance creation dialog and press the “Customize” button:
You should see something that looks like this:
In here we can start adjusting values, I’m going to use 4 cores instance with 15 GB of memory:
I would strongly advise you to set CPU type from “automatic” to SkyLake to get the fastest possible CPU (unless you are planning to use K80 GPU since SkyLake is NOT available with K80):
Now, as for the GPU, we are going to use one P100 GPU:
So GPU type, respectively P100:
The end screen should look like the following one:
This step is very very important. The thing is, only some of GCP OS comes with pre-configured, and instance managed Jupyter Lab that is automatically running. Default OS is not one of them, therefore pressing the “change” button:
You now should see something like this:
Scroll down until you see a “Deep Learning” images:
All of them (Base/PyTorch/TensorFlow) have pre-configured and managed Jupyter Lab. The main difference in-preinstalled DL frameworks. The Base has only necessary frameworks like numpy/sklearn/etc. TensorFlow has everything that Base have plus the TensorFlow. PyTorch, respectively has everything that Base have plus the PyTorch.
In my example, I am going to use “Deep Learning image: TensorFlow 1.10.1 m7 CUDA 9.2”. Also on the same page, I would advise you to:
so the setting should look like this:
We now ready to press select and go back to the instance creation screen. The last thing that needs to be made is permissions adjustment for the instance that we are about to create. I’m going to use my Jupyter Lab with many different resources on GCP; therefore I’m going to grant full access to GCP APIs to my instance like this:
We are finally done with preparing our instance. Here is how my instance setting looks at this moment:
So we are finally ready to press the “Create” button.
IMPORTANT: It is possible that the zone does not have requested GPUs available at the moment. In this case, you will see an error during the creation of the instance. In such case, you need to go back to the square one and re-create instance in a different region where required GPUs are available.
It will take some time for the instance to be created. Let’s wait till the instance is fully created.
As soon as your instance has been created, you can get the link to the Jupyter Lab. Here is how you can do this. First, you need to open Cloud Shell by clicking on the icon at the top-right corner:
By clicking on the button, you should see actual shell opened on the screen:
Now you need to execute only one small command in the shell:
same command for copy-pasting:
Execution might take some time. After the execution, you should see the following:
Press yes and wait. It will take 2–3 minutes to install Nvidia drivers and reboot the instance. Luckily this needs to be done only once. So let’s wait…
2 minutes later, let’s execute the same command, now it should give the different result:
We are almost ready! There is a WebPreview button in the cloud shell, let’s press it:
And press “Preview on port 8080”:
and voila, here is your Jupyter Lab running on the instance with GPU, that has access to all the GCP resources and accessible via the simple link (guarded by the Google Cloud Authentification):
You can try to open the same URL in incognito mode:
Starting with the release M7 of Deep Learning images they have out-of-the-box integration with the BigQuery. Let’s see this in action. First, I am going to create the new dataset in BigQuery that I will use later in this demo. Dataset is called “examples”. The command to do so:
Actual execution results:
Now, let’s do something more interesting, let’s create a new table in the dataset from the data that I have in CSV file stored in my Google Cloud Storage. Here is the command and execution:
Now we are ready to query data from the table and experiment with the data in the Jupyter Lab. Here is where the integration kicks in. To do so first, we need to load BigQuery module like this:
So now we can load data into the memory:
This will create the variable “house_pricing” with the data from the BigQuery:
Amazing, isn’t it? Now, let’s use TensorFlow to train a linear regressor on our instance with this data. I know it is a toy example for powerful GPU like P100, but I’m sure you will be building a way more complex model with your data.
The best part that our Deep Learning images come with different tutorials and TensorFlow source code preinstalled:
So we can look at the existing example of the linear regressor and make your own. It can be found in the following directory:
After experimenting with the example I have created a small snippet for training:
I probably have made several mistakes there since it has not converged(with a small amount of data that I had in my CSV file it is probably expected, though I might have a bug somewhere too:) ) but the point of the exercise to show how quickly you can use data from the BigQuery:
Now let’s upload our model back to GCS, right from the Jupyter Lab:
It is just amazing how simple you can use all GCP resources right from the Jupyter Lab!
Work is done, it is almost time for the beer, however first…
No need to run the instance constantly. You can stop it so it will not consume your money and restart it later (you will have to do Cloud Shell step again to get the link). So let’s stop the instance:
By stopping the instance, all your data (including the notebooks) will be safely persisted until the next time.
I’m glad you’ve asked:) After stopping the instance you can always detach the GPU. This can be done through several simple steps:
2. click the “edit” button on the top:
3. now you can select the“customize” button:
now you should see the familiar instance config dialog:
4. you can now set Number of GPUs to “none”
And press the “save” button. That is it! You now have the same instance with all your data, without the GPU attached!
I hope that you have enjoyed reading this article. If so, please clap for it, share it, follow me and/or leave a comment:)! See you at GCP!
Last modified Feb 13 2019
Articles about the #DeepLearning (mostly Google Cloud with…
43 
4
43 claps
43 
4
Articles about the #DeepLearning (mostly Google Cloud with #TF).
Written by
#ML/#NLP researcher. Making Things Work! #TensorFlow. In the past #MXNet. All opinions are my own.
Articles about the #DeepLearning (mostly Google Cloud with #TF).
"
https://medium.com/google-cloud/cold-disaster-recovery-on-google-cloud-for-applications-running-on-premises-114b31933d02?source=search_post---------343,"There are currently no responses for this story.
Be the first to respond.
“Get Cooking in Cloud” is a blog and video series to help enterprises and developers build business solutions on Google Cloud. In this second miniseries I am covering Disaster Recovery on Google Cloud. Disasters can be pretty hard to deal with when you have an online presence. In the next few articles, we will elaborate on how to deal with disasters like earthquakes, power outages, floods, fires etc. If you are interested in the prior mini series covered, checkout this.
Here is the plan for the series.
In this article, you will learn to set up a cold DR pattern for your applications that are deployed on-premises. So, read on!
Your DR plan would depend on your specific application and recovery goals. Let’s consider a few scenarios
Mane-Street Art is a company that runs their applications on-premise and are building a DR infrastructure on Google Cloud. They are now working out a DR plan, with a low budget and are okay with a bit high RPO and RTO values. This means they need to set up a Cold DR pattern.
Note: If you are unfamiliar with the terms used here (RTO, RPO, DR Patterns) checkout the previous blog to get an overview.
In cold DR pattern Mane-street art needs minimal resources in the DR Google Cloud project — just enough to enable the recovery scenario. When a disaster occurs, the failover strategy requires a mirror of the production environment to be started in Google Cloud.
In any DR pattern you need to understand what steps need to be taken before a disaster hits, what happens when a disaster hits and what needs to happen after the disaster has passed.
When the production environment is running on-premises again, and it can support production workloads, reverse the steps that you followed. Typically it goes something like this:
If you are running your application on premise, have a constrained budget, and can work with high RTO and RPO values, then a cold DR pattern is the way to go. You learned how to approach recovering the environment from failure in a Cold DR scenario. Stay tuned for upcoming articles, where you will learn to set up more DR patterns that makes sense for your business.
Google Cloud community articles and blogs
180 
180 claps
180 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Developer Advocate @Google, Artist & Traveler! Twitter @pvergadia
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/neo4j/neo4j-aura-pubsub-on-google-cloud-image-annotation-ca7104cd493?source=search_post---------344,"There are currently no responses for this story.
Be the first to respond.
Not too long ago, Neo4j announced the ability for users to purchase Neo4j Aura through the GCP Marketplace. So now there is a fully managed graph database as a service available in GCP, which is sweet.
PubSub on the other hand is a native GCP messaging service that GCP users use to send data between application components. In this post, we’re going to walk through how to make the two work together nicely; how can we take data from PubSub and get it into Neo4j Aura?
By the end of this post, we’re going to have a pipeline which will let us:
Some of this material was discussed in this NODES 2020 talk, so if you’d like to watch a video with a deeper-dive on Google Cloud services, check it out.
If you don’t have an Aura instance, you can follow these instructions to get started on GCP quickly. It’s just a few clicks through the GCP Marketplace.
We will be using Cloud Functions, which are an FaaS offering that lets us trigger code in response to an event. Google Cloud already conveniently lets you automatically trigger functions like this on Cloud Storage bucket events.
In this repo, we have all the code for this example, but let’s focus on the most important part:
The magic there is the annotateImage function, which looks like this:
This is the simplest way of using the Cloud Vision client libraries to run all of the feature detection offered by that API. What we’ll get is a list of labelAnnotations back that look like this, with scores & confidences that tell us how confident the Cloud Vision model is in its identification.
We added back the image URI to this label object in our implementation, (it didn’t come from Cloud Vision) so we know what the label is for when it moves on in the pipeline. This is important, which we’ll see in a later step.
When we deploy this image annotation function, we will do it like so:
The important parts here are the --trigger-bucket argument, which will call the function whenever a file gets uploaded to my-bucket, and the --service-account argument, which runs our function with a particular account with the correct rights. Also notice the OUTPUT_TOPIC which tells the function where to send the messages; in this case to the imageAnnotation topic in PubSub.
OK, so we have images getting uploaded, processed into an array of JSON labels, and sent to another PubSub topic. Now we need to get that data into Neo4j Aura. To do that, we’ll deploy a second Cloud Function that’s triggered by PubSub. We’ll use this code repo that contains serverless functions for working with Neo4j.
In the directions for that repository, we want to deploy a custom cypher function; this will basically listen on a PubSub topic, and use a particular cypher statement that we define to sink all of the data coming in to Aura.
First, we need to set up a list of environment variables, which will make our deploy easier. So I’ll create an env.yaml file that contains this:
The variables that deal with secrets tell the function to get Aura credentials from Google Secret Manager; this is optional, you can use regular environment variables if you prefer. The most important part is the cypher statement. When we get a list of messages via PubSub, the function will unwind that list for us as a variable called event, which we can use to access the message payload.
From within the code repo, we execute this:
This is similar to deploying the first; we need a service account. We specify a custom entry point of customCypherPubsub to get the right implementation, and name our function. The --trigger-topic effectively implements part of our workflow, because we know things coming to imageAnnotation are messages that are coming from our previous function.
Here’s what the resulting graph looks like, for a small sample of the images in it.
We can see that mammals, vertebrates, and Primates are central in this graph. This makes sense, since the image corpus I’m using is a collection of animal images.
Let’s take a particular single image and how it was tagged, and look at it together with the actual underlying image.
All driven by files in a bucket:
And just two deployed Cloud Functions:
Happy graph hacking!
References and code repos used in this post:
Developer Content around Graph Databases, Neo4j, Cypher…
88 
1
88 claps
88 
1
Written by
Architect at Neo4j
Developer Content around Graph Databases, Neo4j, Cypher, Data Science, Graph Analytics, GraphQL and more.
Written by
Architect at Neo4j
Developer Content around Graph Databases, Neo4j, Cypher, Data Science, Graph Analytics, GraphQL and more.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/hosting-web-applications-on-google-cloud-an-overview-87d0962931a3?source=search_post---------345,"There are currently no responses for this story.
Be the first to respond.
“Get Cooking in Cloud” is a blog and video series to help enterprises and developers build business solutions on Google Cloud. In this series I plan on identifying specific topics that developers are looking to architect on Google cloud. Once identified, I create a mini series on that topic. If you are interested in the prior mini series covered, checkout this.
In this second miniseries I will cover Disaster Recovery on Google Cloud. Disasters can be pretty hard to deal with when you have an online presence. In the next few blogs, we will elaborate on how to deal with disasters like earthquakes, power outages, floods, fires etc. Here is the plan for the series.
In this article, we will define some Disaster Recovery terms that are instrumental to the DR planning. So, read on!
When a natural disaster happens, we you need to make sure the impact on your business is minimal and for that, you need a robust Disaster recovery (DR) plan!
A disaster typically means a service-interrupting event. So, Disaster Recovery is the amount of impact a business can take during a disaster. It is a subset of Business continuity planning.
RTO or Recovery time objective is the maximum time your application can be offline. This usually depends on the SLAs you offer to your customers. An SLA is a promise made by you as a service provider, to your consumers, about the availability of your service and the ramifications of failing to deliver the agreed-upon level of service.
RPO or Recovery point objective is the maximum amount of time during which the data might be lost.
Typically, smaller RTO and RPO values mean that the application must recover quickly from an interruption.
How quickly a system can recover after a disaster is defined by High Availability and disaster recovery patterns. Also known as HA and DR patterns.
Let’s consider a scenario. I am making some cakes and cookies for a party, which requires a mixer. I am on my first batch of cookies and the mixer starts to make some weird noise. The manual says that the mixer will fail with such a noise, so I need to do something to carry on with the party preparations. I have three options:
Option 1: I can call the mixer company to come fix it — which is obviously going to take time and won’t really work best given the party is today
Option 2: I can try to fix it myself based on the instructions in the manual. This will mean a small pause in my preparation but WILL bring me back on track a little quicker than waiting for the Mixer repair person
Option 3: I could keep going at a slow mix option where I can’t hear the warning noise. In this case, I have to slow down, but the mixer is still working so I can continue on and get it fixed later. This option would definitely have less impact on my party preparations.
With that understanding, let’s review our options in the DR terminology now:
Moral of the story, we pick a DR pattern that makes sense for the business at hand, In this case it was a really serious business of hosting a party!
If you use Google Cloud for DR it can greatly reduce the costs that are associated with achieving both your RTO and RPO values as compared to fulfilling those requirements on-premise. For example: traditional DR planning requires you to account for a number of requirements, including capacity, security, network infrastructure, support, and bandwidth.
Google cloud has several features that help bypass most of these complicated factors and reduces the cost of managing a DR solution. Global network, redundancy, scalability, security and compliance are few such factors. Keep following the series for more on this!
Whether you are a small blogger looking to grow your community, or a huge, multi-scale application, you need to protect your application from going down during a disaster. Hopefully this has been a helpful overview of Disaster Recovery. Whether your application is deployed on-premises or on Google Cloud and whether you have small or big RTO and RPO values, stay tuned for upcoming articles, where you will learn to set up a DR pattern that makes sense for your business.
Google Cloud community articles and blogs
126 
126 claps
126 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Developer Advocate @Google, Artist & Traveler! Twitter @pvergadia
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://rominirani.com/tutorial-getting-started-with-google-cloud-identity-aware-proxy-iap-c3df5bdf10f1?source=search_post---------346,"This tutorial covers how you can get started with Google Cloud Identity-Aware Proxy (IAP), a service that was announced recently at Google Cloud Next 2017.
I reproduce the diagram from official IAP documentation below, which should make things a lot more clear.
Consider that you have multiple applications (in the above diagram, think of ERP and CRM as applications that you are running and to which you want to control access to) hosted on App Engine or on one of the services in Google Cloud Platform and you want to streamline access to these applications from outside. By access, you want to do several things, as an Administrator responsible for that (the IDENTITY module in the above diagram):
Even if you keep aside the whole VPN setup, you had to ensure that the right users are allowed to access the application. You either had to setup a Application Gateway or Proxy in front that contained a list of users allowed to access your application. Or maybe you had this information stored somewhere in the Application User Database and you controlled access via that.
This approach is error prone, with user identity being replicated across multiple applications and often not in sync with each other. Employees leaving the organization can present another headache in addressing this problem. And since security is paramount, wouldn’t it be a better way to centralize the access control lists and give your security teams one simple Identity layer to manage and even review it from time to time.
Enter the Cloud Identity-Aware Proxy (IAP).
I will assume that already have an App Engine application deployed for your Google Cloud Project and that you are familiar with navigating the Cloud Console.
The Identity-Aware Proxy is available in the IAM & Admin section as shown below:
You will also notice that it picked up my list of App Engine applications and it indicates that the IAP is not turned on. All you need to do is switch it on, it will prompt you with a list of domain names that are used to access the resource (App Engine App) as shown below:
Once you Turn it on, you should see a similar screen as shown below:
At this point, we have the IAP turned on but we have not provided list of accounts that are allowed to access this resource. To do that, you can provide that information in the Access section. Click on the ADD button. This will bring up the form as shown below. Notice that it standardizes on the IAM Users, Groups and Roles and you can specify specific accounts or Groups or even Service Accounts as the help indicates.
In my case, as a test, I add a specific account as shown below and click on ADD.
This adds the account to the list of Members allowed access to the resource.
Now, when I try to access the App Engine URL (<something>.appspot.com) , it gives me a list of accounts that I could use.
If I go ahead with the wrong account, I get the error shown above. If I select the correct account, I can access the application.
Cool … isn’t it?
Think of the next time you are prototyping an application on App Engine and need to share the working prototype with a handful of users. No VPN. No playing around with specific users inside the Application. Turn IAP on, specific the users in the access list and its game on. It can’t get simpler than this.
The nice thing about the IAP for your App Engine applications is that it is seamless in terms of the Users identity. What this means is that you can continue to the use the Users API in your App Engine application to retrieve the identity of the user. Nothing changes. Check out more details over here.
The service is currently available free of charge for both App Engine and your Apps that might be running behind a Load Balancer and other VMs on Google Cloud. This is fantastic for now but not sure if there will be charge once the service comes out of Beta.
This service is a manifestation of Google’s approach to Enterprise Security called BeyondCorp and represents some of their best practices towards achieving that. The mission is “To have every Google employee work successfully from untrusted networks without use of a VPN.”. Take a look at the BeyondCorp:
cloud.google.com
It cannot get easier than this to streamline secure access to your Google Cloud Applications via external users, and without the need for a VPN. Do note that the service is currently free and is in Beta.
Technical Tutorials, APIs, Cloud, Books and more.
33 
1
33 claps
33 
1
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
Technical Tutorials, APIs, Cloud, Books and more.
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
Technical Tutorials, APIs, Cloud, Books and more.
"
https://medium.com/@garystafford/using-the-google-cloud-dataproc-workflowtemplates-api-to-automate-spark-and-hadoop-workloads-on-gcp-95b02f54b5f2?source=search_post---------347,"Sign in
There are currently no responses for this story.
Be the first to respond.
Gary A. Stafford
Dec 18, 2018·17 min read
In the previous post, Big Data Analytics with Java and Python, using Cloud Dataproc, Google’s Fully-Managed Spark and Hadoop Service, we explored Google Cloud Dataproc using the Google Cloud Console as well as the Google Cloud SDK and Cloud Dataproc API. We created clusters, then uploaded and ran Spark and PySpark jobs, then deleted clusters, each as discrete tasks. Although each task could be done via the Dataproc API and therefore…
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@qwiklabs/introduction-to-google-cloud-pub-sub-fce6a31aea9f?source=search_post---------348,"Sign in
There are currently no responses for this story.
Be the first to respond.
Qwiklabs
Jan 10, 2020·4 min read
January is the month of resolutions and if your resolution is to get #googlecertified then Google Cloud Pub/Sub is an interesting service which you can learn about. Let’s understand first what Google Cloud Pub/Sub is.
Google Cloud Pub/Sub is a messaging service for exchanging event data among applications and services. By decoupling senders and receivers, it allows for secure and highly available communication between independently written applications. Google Cloud Pub/Sub delivers low-latency/durable messaging, and is commonly used by developers in implementing asynchronous workflows, distributing event notifications, and streaming data from various processes or devices.
If you’re curious and you need to see how Google Pub/Sub functions Google Cloud Pub/Sub: Qwik Start — Command Line is the right lab for you. Let us have a look at this lab and look at the objectives which you will be achieving here
The set up of this lab is quite simple. Make sure you are performing the lab in an incognito window and using the latest version of Google Chrome. Follow the lab instructions to get into the Google Console. You can read through the instructions to be ready for the further steps in this lab.
Have a look at the diagram below to take a look at the terms which you will be hearing often when you are dealing with this service.
Let’s now head to the first task of this lab which is to create a Pub/Sub topic. This task can be easily completed by executing the command which is given in the lab instructions. Once you have run this command successfully click on the Check my Progress button to see your score.
After completing the first task you have to create two more topics named as Test1 and Test2. Once you have successfully created these topics you will see a screen similar to this
You can also check the topics created with the command gcloud pubsub topics list
Moving on, you will now have to delete the topics which you created. This can be done simply by running the command which is mentioned in the lab instructions. Your screen will look like this once the topics have been deleted.
Now it is time to work with subscriptions. You will follow the same process which you followed above for the creation of topics. All you have to run is two commands
In this section you will publish and pull a single message on Pub/Sub. All you need to do is follow the instructions given carefully and fill in the blanks wherever necessary. Refer to the example screenshot for your reference
Instead of Qwiklabs you can mention your name. Similarly in the next message you can add your favorite Food in the message.
Then you have to run the command to pull the messages. Once you run the pull command, you will notice that only one message is pulled. Don’t worry we got you covered. In the next section of this lab, you will learn to pull multiple messages.
In the above section you saw how to pull a single message. Now, you will see how you can pull multiple messages from subscriptions. You have to run an additional command after you pull all your messages. Here is the command
gcloud pubsub subscriptions pull mySubscription — auto-ack — limit=3
In the command if you change the limit to four, you will see four messages in the output. You can set the limit as per your convenience. With this last step, you have successfully completed this lab.
Similarly you can take other interesting labs from the quest Baseline: Infrastructure to get a headstart for preparing for #googlecloudcertification. Enter code 1q-certify-34 to get 3 free credits to complete this quest (valid through January 13th).
270 
270 claps
270 
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/martinomburajr/building-a-go-web-app-from-scratch-to-deploying-on-google-cloud-part-0-intro-a6bf26972ce5?source=search_post---------349,"There are currently no responses for this story.
Be the first to respond.
I recently wrote an article describing the Google Cloud Platform (GCP) Compute stack which described how different layers of cloud, FaaS, PaaS, CaaS, IaaS are utilized in GCP (See link below).
"
https://medium.com/@qwiklabs/convert-speech-to-text-with-google-cloud-d8bd128e06dd?source=search_post---------350,"Sign in
There are currently no responses for this story.
Be the first to respond.
Qwiklabs
Feb 11, 2020·3 min read
Valentine’s day is just around the corner and we are sure you are in search of something to make your special one happy. How about learning to convert speech to text and type out your feelings automatically using Cloud Speech API? We have a lab for that: Speech to Text Transcription with the Cloud Speech API.
The setup of this lab is simple and you will be able to complete this setup once you go through the lab instructions.
The first step of this lab is very easy where you just need to create an API key. You need to follow the instructions as given in the API key. Here is what you need to look for
Copy the key and store it as you will require it again further in the lab.
Now use the API key which you have stored in order to export your API Key.
In this lab you will be using a pre-recorded file which is available on Google Storage. When you are using further you can upload a file into Google Storage and convert the speech into text.
Let us now build the Speech API in a request.json file. In order to do that you just need to run the command which is given in the lab instructions. Now you need to edit this json file. In order to that you can use your favorite command line editor. You need to open the file in the editor by running the following the command:
<name_of_cloud_editor> request.json
Once you open the the file copy and paste the request given in the lab instructions and save the file.
Now we are ready to call the speech API and we will do it with the help of the curl command. Once you run the curl command, the response will be stored in a file named as result.json. Once you run the cat command you will be able to see the output!
Everything sounds good in French. If you want to French it up, you can do that as well. All you need to do is change the language from“en-US” to “fr” and your script will be translated. The instructions are given in the lab. You can translate the script into more than 100 languages!
We hope you had fun with this lab and you will make the most use of it. You can bookmark this blog as this will definitely come in handy this week. Want to know why? Stay tuned to our social media channels for more information.
Here’s something for you for completing this lab. Enter code 1q-translate-838 to get 3 free credits on your learner journey (valid through February 14th).
69 
69 claps
69 
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/syncedreview/spotlight-on-ai-at-google-cloud-next-18-f4309c93beb9?source=search_post---------351,"There are currently no responses for this story.
Be the first to respond.
Artificial intelligence has become a sort of secret weapon in the battle to build the best cloud service platform. Google Cloud Platform is currently the underdog, trailing both Amazon Web Services and Microsoft Azure. But Google is betting robust AI will give it the edge it needs to catch up. At the annual Google Cloud Next conference which kicked off July 24 in San Francisco the company unveiled a series of AI-based product releases and enhancements for its analytics and machine learning tools, additional applications on G Suite, and new IoT products.
Earlier this week, Google parent company Alphabet reported its Q2 earnings, which were ahead of Wall Street’s expectations. The company’s “other revenue” category, which includes the Google Cloud business, rose 37 percent over last year. CEO Sundar Pichai emphasized that “machine learning has been a major focus and a key differentiator for Google, and that’s true for our Google Cloud customers as well.”
Synced is onsite at the Moscone Center in San Francisco to bring you Google Cloud Next ’18 news and updates.
AutoML is one of Google Cloud’s standout innovations: A suite of machine learning tools designed for developers who have limited machine learning expertise and resources. Powered by neural architecture search and transfer learning, AutoML can help developers build custom AI models, or more specifically, find the best neural network architecture and automatically set weights.
Earlier this year Google Cloud announced AutoML Vision, which provides pre-trained models via an API, and the ability to build vision-based custom models. Google has now introduced AutoML Natural Language and AutoML Translation, which enable model customization and integrate with existing Google Cloud APIs. All three AutoML tools are currently available in beta.
To build a custom model, developers need only feed labeled text data into AutoML Natural Language or translated language pairs into AutoML Translation. AutoML will then produce a trained and optimized machine learning model.
Google provides labeling and annotation services, a critical data processing step which ensures training data is high-quality. AutoML Vision’s in-house human team manually classifies images with labels. Natural Language API can analyze text data, such as content classification, sentiment analysis, and entity recognition. Translation API can directly convert texts in more than 100 languages.
The Tensor Processing Unit (TPU), introduced two years ago by Google, is a custom application-specific integrated circuit (ASIC) tailored for machine learning workloads on TensorFlow. In February Google made its TPU V2.0, or Cloud TPU, available in beta for researchers and developers on the Google Cloud Platform. Built with four custom ASICs, Cloud TPU delivers a robust 64 GB of high-bandwidth memory and 180 TFLOPS of performance.
At Google Cloud Next the company announced that its TPU 3.0 — a next-generation AI chip announced two months ago that is eight times more powerful than its predecessor and can achieve up to 100 petaflops performance — are now available in alpha. Google Cloud Chief AI Scientist Fei-fei Li comments, “TPUs allowed eBay to reduce the training time of their visual search model by a factor of almost 100 — from months to days.”
Google is applying its conversational AI technologies in customer service. The company announced Contact Center AI, a cloud-based machine operator that can answer customers’ questions, fulfill basic tasks, and transfer the calls to a human representative if necessary.
The Contact Center AI adopts some of the technologies behind Duplex, an advanced system announced two months ago that can for example call restaurants for reservations or schedule appointments at a hair salon.
Contact Center AI also leverages capabilities from Dialogflow, a technology that enables conversational AI and helps developers build chatbots. Google announced new Dialogflow features at Google Cloud Next, including text-to-speech capability via DeepMind’s WaveNet, the Dialogflow Phone Gateway for telephony integration, Knowledge Connectors to enrich the response database, and Automatic Spelling Correction.
Today, over four million businesses subscribe to Google G Suite, a set of cloud-based productivity and collaboration tools that includes G-mail, Google Calendar, Hangout, Google Drive, etc. Google is now using AI to promote its cloud-based enterprise businesses to the next level.
Google also announced that Smart Compose for Gmail will be soon available to G Suite customers. The program can automatically complete emails by filling in greetings or signing off, etc. It was first introduced with Gmail’s redesign this May as an experimental access feature.
Hangout, a Google communication platform that enables messaging and video chatting, added the new feature Smart Reply to help users respond to messages more quickly. Users are given automatic response suggestions on a Hangout Chat interface.
But the most exciting new feature might be the AI-powered grammar suggestions on Google Docs. Using machine learning, Google Docs can perform corrections from “simple grammatical rules like how to use articles in a sentence (‘a’ versus ‘an’), to more complicated grammatical concepts such as how to use subordinate clauses correctly.” The new feature was made available this week in Google’s Early Adopter Program.
Other announcements include a security center investigation tool (available in the Early Adopter Program for G Suite Enterprise customers), data regions (available now for G Suite Business and Enterprise customers), and voice commands in Hangouts Meet hardware (coming to select Hangouts Meet hardware customers later this year).
The Cloud and the Internet of Things (IoT) are inseparable, and Google has already invested heavily in IoT with products such as Android Things, Nest, Google Home, etc. Several months ago, the company announced Cloud IoT Core, a service that connects data from millions of dispersed devices using the Google Cloud Platform, and provides data-intensive processing, visualization, and analysis.
Without edge computing however, the back-and-forth communication between devices and the Google Cloud Platform would still cause high latency. Google took a step to fill that void yesterday with two new products: Edge TPU, and Cloud IoT Edge.
Edge TPU is a cut-down Google ASIC designed to complement Cloud TPUs, and will be embedded into gateways that bridge the Google Cloud Platform and devices such as sensors. Edge TPU empowers TensorFlow Lite machine learning models and accelerates inference at the edge so that edge devices can make local, real-time, intelligent decisions.
Cloud IoT Edge is a software that allows edge devices to run pre-trained machine learning models on the Edge TPU or on GPU- and CPU-based accelerators. Cloud IoT Edge can run on Android Things or Linux OS-based devices, and its runtime environment should include gateway class devices, the Edge IoT Core, and the TensorFlow Lite.
CTO of LG CNS Shingyoon Hyun says Google Cloud AI and IoT technologies will allow the South Korean tech company to “make a better working place, raise the quality of products, and save millions of dollars each year.”
Google will release an Edge TPU development kit, including a system on module (SOM) that combines Google’s Edge TPU, a NXP CPU, Wi-Fi, and other microchip’s elements Google has teamed up with semiconductor companies like NXP and Arm as well as gateway vendors and edge computing companies for manufacturing. The development kit will be available for order this October.
Most new products announced at Google Cloud Next ’18 so far are based on existing Google technologies such as AutoML, TPU, and other AI features. They are perfectly integrated with Google cloud business and designed to solve the pain points of businesses.
Google Cloud’s quarterly revenue crossed a billion dollars in 2017, prompting Google Cloud CEO Diane Greene to proclaim it the world’s fastest growing cloud. There is nothing to suggest that growth will slow in the future, as the cloud plays an important role in Google’s grand strategy to transition from an Internet giant into an AI mega-power.
Journalist: Tony Peng | Editor: Michael Sarazen
Follow us on Twitter @Synced_Global for more AI updates!
Subscribe to Synced Global AI Weekly to get insightful tech news, reviews and analysis! Click here !
We produce professional, authoritative, and…
141 
141 claps
141 
We produce professional, authoritative, and thought-provoking content relating to artificial intelligence, machine intelligence, emerging technologies and industrial insights.
Written by
AI Technology & Industry Review — syncedreview.com | Newsletter: http://bit.ly/2IYL6Y2 | Share My Research http://bit.ly/2TrUPMI | Twitter: @Synced_Global
We produce professional, authoritative, and thought-provoking content relating to artificial intelligence, machine intelligence, emerging technologies and industrial insights.
"
https://medium.com/techking/visualising-google-cloud-1900fda919e?source=search_post---------352,"There are currently no responses for this story.
Be the first to respond.
By Alfred Wahlforss
Google Cloud Platform (GCP) provides an incredible foundation for building modern applications and services. In fact, it’s so incredible, King has decided to move its entire data warehouse to GCP. As with all new technologies, there are some missing parts — though Google is aware of these and is actively working with customers like us to improve their platform.
One of the most evident missing components in GCP currently is the lack of a clear overview of activity and consumption at the organisational level. For example, it is easy to see BigQuery slot usage for one project (on BigQuery flat-rate pricing), but there is no simple overview of total slot usage across all projects. The same goes for BigQuery storage, Cloud Storage, and many other GCP services. At King, we have over 500 GCP projects currently, and without a good organisational overview, it is difficult to know what is going on.
We decided to fix this problem by creating our own dashboards. These dashboards give us a clear overview of our GCP organisation. We wanted to know everything from how we use BigQuery slots, to the bytes we have stored in Cloud Storage. This data needed to be fresh, ideally not older than a couple of minutes.
Luckily, Google opens up most of its cloud via APIs — although sometimes a lot of APIs need to be called to obtain useful data. Connecting these multiple APIs together sometimes felt like putting the pieces of a jigsaw puzzle together.
To minimize mess and ease future operations, we decided to use GCP’s serverless offerings to build our dashboards. Serverless means that the cloud provider handles all of the configurations of the servers. We found this approach reduced maintenance, decreased complexity, and helped us launch faster.
To aggregate data from Google’s APIs, we used the serverless Cloud Functions. Cloud Functions belongs to the new paradigm of computing called Functions as a Service, or FaaS. In essence, FaaS enables isolated functions that run in the cloud and only cost when they are called. Perfect for collecting data from APIs.
With App Engine, there is no need to upgrade or maintain any servers. Our standard approach, by contrast, requires spinning up virtual machines and configuring them. Virtual machines often require software upgrades on a regular basis. Configuration and upgrading software takes precious time away from our IT department. The fact that App Engine requires neither of these things is a major time saver for a small app like this.
Turning to the end product — the dashboards — the information in the dashboard shown above was extracted using three separate Cloud Functions. The first function fetches all projects that use BigQuery, which is a list of hundreds of projects. A second function gets all the datasets per individual project and finds all the tables contained in each dataset. The last Cloud Function fetches the bytes stored in each table and sums it for every project. Finally, after over 30,000 Google API calls, we know how much we are storing in BigQuery.
A similar process was repeated to get the data for each of the dashboards shown below.
When we launched the dashboards we noticed one immediate benefit — they acted as conversation starters. We discovered that there are some discussions that could only be properly had when everyone shares the same information.
The table in the above dashboard was an example of a great conversation starter. It spurred discussions such as whether or not we should limit slot usage per project, what time to schedule jobs, and so on. Before we knew which projects were using slots it was impossible to have productive conversations on these topics.
When we visualised BigQuery storage as above, it became clear which projects were responsible for the storage costs. That started a conversation around exactly what we should store in BigQuery vs Cloud Storage vs not at all.
A further fun effect has been that people stop in new places in the office. They might walk past a screen and see something interesting. Suddenly a discussion is sparked between people who have never spoken before. Maybe that conversation leads to other topics and all of a sudden a dashboard starts a friendship.
Originally published at techblog.king.com on October 15, 2018.
Read the latest tech articles from King.
206 
206 claps
206 
Read the latest tech articles from King. Learn about our tech and our company culture.
Written by
King Tech Blog - awesome tech teams at King. Everything you read comes from the minds of our tech gurus and hearts of our Kingsters!
Read the latest tech articles from King. Learn about our tech and our company culture.
"
https://medium.com/google-cloud/this-week-in-google-cloud-platform-1-billion-per-quarter-two-new-regions-and-wordpress-63b34a6fc50a?source=search_post---------353,"There are currently no responses for this story.
Be the first to respond.
This past week Google has started disclosing its Cloud revenue and “(Google) says its cloud now brings in $1 billion per quarter” (cnbc.com)
At the same time the investments are not slowing down with two new datacenters :
On the security front, here are the weekly updates :
Assuming security is top of mind for just about everyone, cost is probably a close second which means this detailed post by Romin Irani will come handy for many — “Factors to control and understand your costs” (rominirani.com)
#marketplace: Orbitera and MobileIron team up to make it easier to buy and sell apps in the cloud (Google blog)
From the “BigQuery remains awesome” department :
If you like to take your .NET App migration to the cloud as an opportunity to embrace cloud-native principles, here’s a post about a white paper and github repo to do just that (Google blog)
If you’re curious about Kubernetes history, its release cadence, SIGs, and a lot more, I’d encourage you to read the well-documented “The full-time job of keeping up with Kubernetes”, it covers both the software and the project (gravitational.com)
This week’s GCP Podcast (Episode #112) is a conversation with percy.io creator Mike Fotinakis.
From the “Istio in the trenches” department :
From the “In case you’ve missed it (ICYMI)” department :
This week’s pictures are celebrating our two new datacenters in Montréal and in the Netherlands :
That’s it for this week!-Alexis
Google Cloud community articles and blogs
21 
21 claps
21 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Google Cloud Developer Relations
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/google-cloud-platform-technology-nuggets-august-16-31-2021-edition-730ea8df7dfe?source=search_post---------354,"There are currently no responses for this story.
Be the first to respond.
Welcome to the August 16–31, 2021 edition of Google Cloud Platform- Technology Nuggets.
A friendly reminder that Registration for Google Cloud Next, happening on 12th-14th October, is open.
For anyone working with the cloud, I am sure that you have had requests to look at current costs and try to optimize it. Our latest guide titled Cost optimization through automated VM management, takes you to multiple methods ranging from time-based scheduling to idle recommendations to help start/stop your fleet of VMs and control costs. The interesting part of this article is that it will introduce you to multiple other managed services like Cloud Scheduler, Cloud Functions, etc.
If you are looking for Cost Optimization across all our services, check out our Cost Optimization guide for developers and operators, available in the Architecture Center. It has guides to Cost Optimization across our multiple compute offerings, storage services, data analytics and more.
Google Datasets is an ever-increasing set of databases that have been available to the public for their analytics needs. At last count, there are more than 200+ public datasets that are available ranging from Google Trends, Climate Data and more.
An interesting development is that Google Cloud Platform release notes are also available now as a dataset that you can query across releases, security bulletins and across product lines.
Check out this blog post that highlights some of the recent datasets that have been released.
Our Conversational AI platform, Dialogflow CX, has got a bunch of new features. A couple of features that are particularly interesting and which several customers have requested are private network access to Webhook targets and an ability to stream partial responses. The partial responses feature addresses the situation of getting a timeout from your Webhook after 5 seconds. It presents an elegant way to inform the user about the ongoing fulfillment process. Take a look at the sequence diagram below that shows how instead of waiting for 5 seconds, you present a message “One moment while I look that up” after 2 seconds of the process.
In a big win for Security, Dialogflow now integrates with Service Directory private network access so it can connect to webhook targets inside our customers’ VPC network.
If you have deployed any Google Cloud Project in Production, you would have experienced the challenges of setting up a strong security foundation from the grounds up. This blog post is a detailed guide on various security controls to consider while doing a GCP deployment. It is a go to guide and an absolute must if you are deploying applications in production. It is an exhaustive guide to setting up the organization hierarchy, resource controls, logging, authentication and authorization, billing and more.
As an application developer, you are spoilt for choice when it comes to various managed services available to you like App Engine, Cloud Run, Cloud Functions, etc. A key indicator of application performance is often latency and it can become challenging when there are potentially multiple services invoked to fulfill a customer request. The recommended way to determine this indicator is via an application trace. For our serverless products mentioned above, we have Traces built by default across your request as it goes through every layer of a distributed system, including the load balancer, compute, database, etc. Check out more information on Cloud Trace and how you can use it today to debug your application performance.
In keeping with the tradition to highlight a GCP Sketchnote, this edition presents All you need to learn about Google Cloud Storage (GCS), our object store for data such as images, videos, text files and other file formats. GCS has deep integration and is central to connecting multiple GCP services together and hence a good understanding of this is key to utilizing the range of services across GCP.
How about learning more about storage options on GCP other than Google Cloud Storage? We have several services ranging from SQL, NoSQL and specialized databases that can help with your specific use cases. “Take a look at which database shall I use?”.
Have questions, comments, or other feedback. Do send it across.
Looking to keep a tab on new Google Cloud product announcements? We have a handy page that you should bookmark → What’s new with Google Cloud.
Google Cloud community articles and blogs
49 
49 claps
49 
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/this-week-in-google-cloud-platform-k-is-for-kubernetes-dialogflow-goes-enterprise-and-multi-563a6a6bd67e?source=search_post---------355,"There are currently no responses for this story.
Be the first to respond.
Google’s Dialogflow (né API.AI) now offers an Enterprise Edition for a new way to build voice and text conversational apps. A Dialogflow V2 beta API is also available for testing.
Cloud Spanner now delivers five 9s with Multi-Region support. The database transactions and synchronous replication now span across regions and continents! Straight from the post — “At Google, Spanner powers apps that process billions of transactions per day across many Google services. In fact, it has become the default database internally for apps of all sizes.”
From the “where will you take Cloud Spanner today” department:
From the “Kontainer all the things” department :
From the “data gravity and cost” department :
From the “stream of portability“ department :
From the “Tensorflow, from tiny to humongous” department :
Following up on the recent Cloud DataPrep updates, users now have access to a preloaded sample dataset, in-product step-by-step walkthroughs, and guidance videos (Google blog)
From the “Customers and partners talk best about GCP” department :
What does Istio with GCP look like? The answer lies in this new page (cloud.google.com), a recent presentation by Ray Tsang (youtube.com), and this free codelab (g.co/codelabs).
You may want to add Cloud OnAir to your agenda, an online conference in a couple of weeks (December 6th) on the journey from big data to AI.
This week’s GCP Podcast #103 covered Performance Atlas with Colt McAnlis. If you’ve only watched the videos, you’ve only seen the trailers, make sure to read Colt’s detailed posts on Medium (medium.com)
This week’s picture is from the Dialogflow Enterprise announcement :
That’s it for this week!-Alexis
Google Cloud community articles and blogs
52 
52 claps
52 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Google Cloud Developer Relations
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/@SandeepDinesh/moving-ezschool-com-to-google-cloud-platform-bbb845ccbc97?source=search_post---------356,"Sign in
There are currently no responses for this story.
Be the first to respond.
Sandeep Dinesh
Jul 16, 2019·15 min read
A lot of the time, people like myself blog about greenfield apps, best practices, and the latest and greatest cutting edge technology. However, the reality is most real apps are never that easy or that clean.
Many people are not ready to go full-in on cloud-native hype technology, or they just don’t know where to start. The truth is, you can adopt a little at a time where it makes sense for you to do so.
In this blog post, I’ll share the story of how I moved EZSchool.com from a traditional LAMP stack running on a VPS to a containerized system running on GCE and Cloud SQL. I’ll share my thought process and the tradeoffs I had to make in a real world scenario.
TL;DR: EZSchool is a monolithic PHP app with a MySQL database. Ops were challenging due to lack of best practices. Moved self hosted MySQL to Google Cloud SQL to benefit from a managed solution. Dockerized the PHP and Nginx layers to simplify operations. Considered Kubernetes, but found the cost of entry too high. Considered serverless, but found the change in developer workflow too high. Settled on running containers on a single GCE VM controlled with Docker Compose and startup scripts.
Lesson Learned: You don’t have to use cutting edge tools and best practices to start your modernization journey. You can modernize slowly and only the pieces which make sense at the time.
Skip to the Modernization section if you don’t want the long history lesson.
In 1998, my mom launched EZSchool.com. EZSchool was one of the first “cloud-based” educational platforms, where teachers could manage all aspects of a classroom online. From assigning homework, give tests, and ensuring students were on the right track, the vision for EZSchool was ambitious.
Of course, being a bootstrapped startup in the early 2000s meant that the whole company ran out of two servers in the closet. One server ran Oracle DB, and the other ran Java Web Server 1.1 and has two (2!) physical CPU cores. Dual core baby. Of course, we were in the middle of Californina’s rolling blackouts, so we also had a UPS that could power both servers for about 30 minutes before falling over. I vividly remember the noise they would make when activated; a power cut to the database server could potentially corrupt everything so when the UPS activated it was a mad scramble to cleanly shut everything down before it lost all power, even at 2am.
When the dot-com crash happened and funding dried up, all hopes of raising money vanished overnight. It looked like the end, but my mom was determined to push forward even if it would be a one-woman operation.
At this point, it was clear EZSchool wouldn’t have the money to get space and servers in a colo facility, and having servers at home that went down all the time was putting massive strain on everyone. So what was the solution?
Around this time, the concept of shared PHP Hosting was becoming more and more popular. For a few dollars a month, you basically got access to a folder on a server that had Apache and CPanel installed. From here, you could FTP HTML and PHP files over to the folder, and bam you have a running website. Of course, there were tons of limitations, the biggest of which was no database support. This meant all the “cloud based” ideas like an online classroom were cut and EZSchool pivoted to a free content platform (games, worksheets, tutorials, etc) that made money from display ads. These were the days that the online advertising market was taking off, and you could make good money from modest amounts of traffic.
So let’s recap:
This also meant throwing away the existing Java codebase and starting from scratch in PHP.
Eventually, EZSchool got enough traffic that even the biggest shared hosting plan couldn’t support it. Remember, there was no way to isolate customers from each other in this model, it was all based on good faith. Our provider gave two choices, move to a “Virtual Private Server” or leave.
So once again, we had to migrate.
A VPS was a fully isolated Virtual Machine that was provisioned just for you. You got full root access to the machine and could do anything you wanted with it. Unlike shared PHP hosting, the resources for your VPS were pre-provisioned and dedicated for you.
Moving to a VPS had it’s pros and cons. More control means more flexibility, but it also means managing the OS updates and running Apache ourselves. Nothing too hard, especially since my mom was already familiar with Linux and bare metal servers.
Eventually, the little VPS we were using started to fall over as well. The biggest problem was Apache’s habit of spinning up individual processes for each request, which caused huge amounts of overhead. I started reading about a new web server called “nginx” that claimed to be a lot more performant with a lot less overhead. So along with upgrading to a bigger VPS, we moved from Apache to Nginx.
At the end of the day, we now had more computing power than we needed, and could start to do more. At this point, display ad revenue was getting worse and worse, so we decided to let people pay for memberships that would remove advertisements from the website. Because we had a VPS with full control, my mom spun up MySQL, created a PayPal integration, and implemented a membership program.
So let’s recap:
Now we had memberships and a fully functional database, it was time to come full circle and go back to the original vision of a “cloud based” learning platform. Of course by this time, this wasn’t a unique proposition anymore, but it still seemed like the right direction to go. I had just finished college, and I had a summer free before starting a full time job.
Over those three months, we designed a database driven platform, where all content was stored in MySQL or dynamically generated and then rendered by PHP and JavaScript. It was a big success, and over the next few years we moved everything to this model.
This let us remove hundreds of folders and thousands of static HTML and PHP files that were mostly all copies of each other and removed a bunch of manual, repetitive, and error prone work.
However, this “centralization” had some negative side effects. I remember a nasty hours-long outage where we overwrote a critical routing component, forcing us to literally rewrite it from scratch using ‘vim’ on the server, because we had no backups. So we finally adopted Git, yay! Still just pushed everything to master, but it was better than nothing! Another negative was increased resource usage, with more and more functions going through the database and PHP routers.
A huge issue was MySQL would randomly die, probably due to a spike in traffic causing memory issues. I say probably because monitoring consisted of looking at the default CPU/Memory graphs and our nginx logs and squinting really hard.
We also started running into ops issues. The “server setup” guide was a poorly updated Google Doc with commands copied from random blog posts I had found. While this initially worked, the production server had experienced config drift, and was getting more and more out of sync every day.
So let’s recap:
You still with me? Here is where things get interesting.
At this point, I had been working at Google Cloud for some time, and whenever I helped my mom with EZSchool it was like a blast from the ugly past. I knew we could be using a lot of new technology to simplify our lives, but my mom just didn’t have the cycles to learn and implement it.
Lucky for me, our contract with our VPS provider was coming to a close, and we were having some reliability issues with them. So, this was a good opportunity to both modernize and migrate EZSchool to GCP.
There were multiple goals for this project:
I won’t focus much on upgrading to PHP 7, except for saying that the massive improvement in performance over PHP 5 was amazing, and it was mostly backwards compatible with our PHP 5 code.
One decision I made from the start was to move everything into containers. Using containers brings a lot of benefits. Upgrading the version of Nginx or PHP is much easier and safer than using the OS package manager. They also make it easy to add the nginx config into source control, as they can be mounted into the right place in the Nginx container easily. They also make running near identical stacks in prod and dev possible. Restarting everything is just one command, and support for auto-restart makes it even easier.
The first challenge was moving the MySQL server over to Google Cloud SQL. Creating the instance was easy enough, but how would weto move the data over?
One way would be to follow the migration guide in the documentation. This involves setting up our current server as an external master for Cloud SQL, waiting for all data to be replicated over, turning off the app, promoting Cloud SQL to master, and finally restarting the app to point to Cloud SQL.
So it seems pretty complicated, and you still have to experience downtime. The downtime would be super minimal, but it’s a lot of moving pieces. Can we trade off a little more downtime for a simpler migration?
Because we only had a few GB of data, I came up with a much simpler solution.
→ Run mysql-dump to dump the data into a .sql file
→ Compress .sql file with gzip
→ Upload compressed file to Google Cloud Storage with gsutil
→ run ‘ gcloud sql import sql ‘ command to load the data into the database
All in all, these steps took about 5 minutes to finish. Not bad! I then put these four commands into a bash script, and we were good to go.
There are many ways to connect to Cloud SQL from your application. I would highly recommend using the Private IP for better performance and security. On top of that, I would also use the Cloud SQL Proxy to set up the connection.
I like the proxy for a few reasons:
To come up with the new architecture, it was important to understand the current architecture and the issues it had.
Pretty simple design, with some pretty big flaws.
So what were the goals for the new architecture:
Because my day job was working with Kubernetes, the first thought I had was to move everything to Kubernetes. It would then look something like this:
At this point I took a step back and weighed the pros and cons of this new architecture:
Good:
Bad:
Ugly:
It really seemed that Kubernetes was out of the picture for this project. The killer feature of Kubernetes, namely managing multiple microservices at scale, was basically unnecessary. We had a small monolith that really only needed one replica. We would be paying the cost of entry that Kubernetes brings of managing a complex system with none of the benefits.
My next thought was to use something serverless. These days, I would recommend Cloud Run, but that wasn’t an option when I was doing the migration. Google Cloud Functions doesn’t support PHP, so that was out. That left App Engine Standard and App Engine Flex.
App Engine Standard seemed to be an ideal choice. It should dramatically reduce ops work and give us new features like traffic splitting and rolling updates. However, there was a fundamental problem with using App Engine, and that was Nginx. We had thousands of lines of routing rules in the form of HTTP 301 and 302 redirects. built up over years of operation, Unfortunately, App Engine requires rewriting them in a proprietary format which would be very time consuming. We also do some other interesting things in Nginx to get all the routing working the way we want, have a seperate non-public directory for dynamic question generation, and use CIOPFS to make the website case insensitive due to legacy reasons.
App Engine Flexible was out due to the high price, which you can read more about here.
If we were doing the migration these days, Cloud Run would be a great choice. Because it allows us to run arbitrary Docker containers, we have the option to run Nginx and keep all the routing rules and customizations we need. Let’s look at the pros and cons for Cloud Run
Good:
Bad:
Ugly:
Cloud Run seems like a great solution with limited downsides. However, it does require a new dev workflow, which is the biggest hurdle to adoption. It would require a new container to be built and deployed on each change and would remove the ability to SSH into the machine to make hotfixes. Now, both of these things are actually features and good things to have, but it is different and thus harder for my mom to adopt. The biggest thing is it didn’t really exist when I was doing the migration! So I’m going to ignore it for now, but might take another look in a later post.
So is it possible to take the positives from the Kubernetes world and use it without the downsides? We wanted to reduce the ops overhead, but didn’t need scalability. Running on a single VM was fine.
The solution I came up with was to use Docker Compose. Docker Compose gives some of the advantages of using Kubernetes when it comes to using multiple Docker Containers together on a single machine, is super simple to set up, and just requires a simple “docker compose up” to start.
Good:
Bad:
Ugly:
The biggest issue with this setup is that it doesn’t follow best practices. Containers are best when they are “immutable”. This would mean adding all the files into the container image when building it, and every time there is a change you should rebuild and retag the container and do a redeploy. Instead, we are just using a Docker Volume Mount to add in the appropriate Nginx config and the source code. However, I’m not trying to win any awards for the cleanest setup, my goal is to decrease ops toil and this hits the sweet spot perfectly.
Coming up with this exact architecture took a little bit of time. After looking for a combination Nginx/PHP Docker container for quite a while and only finding a bunch of half baked images, I decided to run separate containers for Nginx and PHP and then link them together with Compose.
The PHP container’s Dockerfile is as follows:
These were the options we needed, but obviously your application will be different. Check out the official documentation for more configuration options.
For Nginx and MySQL, the official containers worked without any modification!
Now came the interesting part, using Docker Compose to maintain Dev/Prod parity. I wanted an easy way for the prod service to use Cloud SQL, but dev to use a local MySQL instance.
To do this, I used the extension feature of Compose to have two different MySQL configurations, but a base nginx and php configuration.
The base configuration looked like this:
As you can see, Compose runs two containers with a shared bridge network. This is very similar to a Pod in Kubernetes, which is exactly what I want!
We mount in the config files using volume mounts, and make sure logs are saved outside the container. In an ideal world, we would not be doing these things. Containers work best when they are immutable, so ideally we should be running a build step that adds in all relevant files to the container and tags it with a unique ID. However, this means we need a CI/CD pipeline and more which we don’t have. This is a great topic for a future project. Similarly, logs shouldn’t be written to disk, but rather a logging service like Stackdriver. However, because we only have one host, it was easy to set up fluentd to send logs to stackdriver.
Now for the MySQL configuration. First, we have the local config stored as a file called “docker-compose.override.yaml”
This file is a pretty standard MySQL setup. The only difference is the volume mount, which let’s us pre-populate the database with a snapshot of the data from prod in the form of a .sql file.
For production, we instead use the CloudSQL Proxy. In a file called “docker-compose.prod.yaml”:
In this case, we are using the CloudSQL Proxy docker container instead of the standard MySQL one, and use the command section to configure it to connect to the correct database. You might also notice that we are not exposing the MySQL port anymore, which makes sense. In development, you might want to connect directly to the database, but in production you can use the gcloud cli to connect to Cloud SQL.
Finally, I created a few scripts to automate basic tasks. I created the following Makefile:
Most of the commands are self-explanatory. The sync-db command is neat as it creates a new sql backup using gcloud, then copies it into the mysql_init folder using gsutil. This means you can get a fresh snapshot for local use by running “make sync-db start-local-server”
We pulled the code onto the new GCE server, and everything worked! Of course, I am glossing over the changes we had to make here and there due to slight differences in configuration and moving to PHP7, but all in all it was very minimal.
The last part was creating a startup script that would automatically start the containers on boot.
Now that everything was running, it was time to migrate. Like I said before, we were willing to deal with some downtime. So we SSH’d into the old server and stopped Nginx, ran the db migration script I had previously created, and then switched DNS to point to the new server. All in all, it took just 5 minutes!
At the end of the day, this was a great lesson in finding low hanging fruit. Did we follow all the best practices? Did we create the most optimized infrastructure? Do we have amazing automation and autoscaling? No, no and no. What we did do is cut our costs, increase our uptime, simplify ops, and establish dev/prod parity. I call that a job done well enough :P
Future Posts:
Originally published at https://blog.sandeepdinesh.com on July 15, 2019.
See all (155)
45 
45 claps
45 
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/lemmings/lemmings-partners-with-amazon-web-services-and-google-cloud-platform-ed6477e6a73d?source=search_post---------357,"There are currently no responses for this story.
Be the first to respond.
A few days ago Google’s AI project AlphaGo Zero beat the world’s leading chess engine (Stockfish) after just four hours of training on Google’s Tensor Processing Units (TPUs). Just one of the many examples of how a generic machine learning approach can within hours yield better performance than specialized software that got fine-tuned over many years.
Yet most fascinating to me is that the technology behind extraordinary achievements like these is not behind a walled garden. It is readily available to all of us.
Most of the leading machine learning tools are open source and well documented. The data needed for machine learning is more accessible than ever before and even the hardware used is becoming ever more cost effective. We all can play around with the same ingredients that big players like Google, Amazon, Facebook and Apple do.
For early stage teams this is amazing. If you have an idea, playing around with it and getting some results is fast and there are few barriers. Machine learning is an open field and there are countless problem domains to apply it to in addition to chess.
While the recent technological shift was largely about making services more accessible by making sure they are also available on mobile (“mobile first”) we are now in a shift focused on making more sense of existing data by using machine learning (“artificial intelligence first”).
“In an AI-first world we are rethinking all our productsand apply machine learning and AI to solve user problems.”
— Sundar Pichai
Having access to machine learning tools as well as to data and hardware is great but for very early stage teams (pre-product, pre-market, pre-funding …) there is still a huge barrier: infrastructure cost.
While it only takes a few hours to run an experiment similar to AlphaGo Zero’s chess training it can be prohibitively expensive for an early stage team to do so unless you have a few thousand dollars to burn.
This puts early stage teams at a disadvantage.
Imagine an ambitious painter. She is pouring all her heart into filling one canvas after the other. Yet every canvas, every brush and every pot of paint is incredibly expensive. It puts a lid on what she can express. Every mistake is costly. Every idea gets scrutinized. Every stroke is restrained.
What could the painter do if canvases, brushes, paint and storage would not only be readily available but free? What if her studio would automagically scale with her efforts? What could she do?
We were asking ourselves what could Lemmings do if their imagination would not be artificially constrained by infrastructure cost. If they would not have to restrain their thoughts to the GPUs in their laptops? What if they would not have to worry about purchasing hardware that becomes obsolete within weeks?
I’m glad that we are not the only ones who are curious to see what ambitious teams can do with machine learning on modern infrastructure.
We are partnering with Amazon and Google to provide every Lemmings team with USD 100.000 for Amazon Web Services as well as with USD 100.000 for the Google Cloud Platform.
On top of this all of our teams get access to first class 24 / 7 supportas well as access to training directly provided by each platform partner.
This not only means massive infrastructure and tech supportcost reductions for our early stage teams.
It means unleashing the mind.
https://aws.amazon.com/blogs/aws/new-amazon-ec2-instances-with-up-to-8-nvidia-tesla-v100-gpus-p3/
https://aws.amazon.com/blogs/ai/new-aws-deep-learning-amis-for-machine-learning-practitioners/
https://chess.stackexchange.com/questions/19366/hardware-used-in-alphazero-vs-stockfish-match
https://news.ycombinator.com/item?id=15556789
https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/alphago-zero-goes-from-blank-slate-to-grandmaster-in-three-dayswithout-any-help-at-all
https://cloud.google.com/blog/big-data/2017/05/an-in-depth-look-at-googles-first-tensor-processing-unit-tpu
https://news.ycombinator.com/item?id=15869083
https://cloudplatform.googleblog.com/2017/09/introducing-faster-GPUs-for-Google-Compute-Engine.html
https://cloudplatform.googleblog.com/2017/11/new-lower-prices-for-GPUs-and-preemptible-Local-SSDs.html
https://www.nvidia.com/en-us/data-center/dgx-1/
http://images.nvidia.com/content/technologies/volta/NVIDIA-Volta-GPU-Architecture_The-Future-of-AI.pdf
https://en.wikipedia.org/wiki/Tensor_processing_unit
Incubator focused on Art and Artificial Intelligence
287 
287 claps
287 
Incubator focused on Art and Artificial Intelligence
Written by
Founder at Magic, Lemmings, Blossom …
Incubator focused on Art and Artificial Intelligence
"
https://medium.com/rd-shipit/como-a-migra%C3%A7%C3%A3o-para-o-google-cloud-platform-em-2018-potencializou-o-crescimento-da-rd-station-8e2fdf744427?source=search_post---------358,"There are currently no responses for this story.
Be the first to respond.
A RD Station nasceu em 2011 como um produto SaaS (Software as a Service), 100% em cloud que tinha como base uma PaaS (Platform as a Service) chamada Heroku. O Heroku ajudou a ganhar muita velocidade e a otimizar boa parte do pipeline de desenvolvimento. Essa solução oferecia desde deploy com apenas 1 comando e gestão simplificada em produção, até um marketplace que conectava múltiplos serviços (por exemplo monitoramento e bancos-como-serviço), eliminando a necessidade de termos times dedicados de SRE e DevTools, como temos hoje. Sim, acredite, essas eram “inovações” na época :)
O crescimento do nosso produto e do número de clientes foi exponencial, porém ali por volta de 2015 percebemos que a solução que tínhamos não atendia mais as nossas necessidades. Queríamos customizar nosso uso para ter mais controle da operação e estruturar melhor nossos serviços, entretanto, isso não era transparente e nem muito configurável. Decidimos então migrar para uma IaaS (Infrastructure as a Service), que ao mesmo tempo que traria mais controle, também exigiria mais desenvolvimento e governança do nosso lado.
Em 2016 testamos pelo menos dois provedores conhecidos do mercado usando uma parcela dos seus serviços, entretanto nunca ficamos completamente satisfeitos (nossa régua é bem alta, confesso). Acontece que ou a solução era boa no aspecto técnico (ex: diversificação de serviços, escalabilidade, disponibilidade), ou era boa apenas no atendimento (suporte ativo, capacidade de resolução de problemas). Depois de muitos e muitos testes, decidimos usar o Google Cloud Platform como nosso principal provedor de serviço na nuvem (cloud), e de lá pra cá essa foi a melhor decisão que tomamos.
A seguir vou os critérios que utilizamos nessa tomada de decisão e também os aprendizados que tivemos até aqui. Espero que esse conteúdo te ajude a tomar uma boa decisão quando for necessário fazer uma escolha parecida.
Antes de tudo é preciso entender que uma decisão sobre um fornecedor cloud não é meramente por requisitos técnicos. Toda parte técnica, serviços disponíveis, investimento de P&D, capacidade e histórico de crescimento, performance, etc, é sim super importante (na verdade ela é fundamental, pois sem isso pouco importa olhar para o resto). Porém, os fornecedores atuais já entregam de forma parecida boa parte dessas coisas, quase como um celular, que você não escolhe por “fazer ligação ou não” pois isso já é esperado. Ele precisa entregar mais do que o esperado.
No entanto, o restante também é importante. No que tange isso, gosto de pensar em ao menos mais 3 itens: custo, suporte e valor ao negócio.
O fato é que o custo que sua solução tem para rodar, impacta diretamente na sua margem. Quanto menor seu custo, melhor sua margem, simples. Se você consegue crescer sua base de clientes descolada do custo de cloud, excelente, você tem uma solução eficiente. É esperado que seu COGS (custo de infra para servir seu produto por cliente), diminua proporcionalmente com o tempo.
Para isso, é importante contar com um provedor que tenha boa base de preço, ofereça descontos fixos em máquinas, possua cobrança local com menor impostos (ou hedge de dólar), e que tenha cupons para migração ou atenuação dos custos. Dica bônus: vale revisar a infra-estrutura para consumir menos e também negociar bons contratos. Você precisará negociar isso, porém só conseguirá quando atingir um tamanho mais relevante.
Por fim, tome cuidado com fornecedores menores ou menos estruturados, já vi empresas mudarem seu empacotamento de serviços e preços sem critério, afetando diretamente seus clientes e exigindo migrações a toque de caixa.
Com relação ao suporte, ao escalar nosso negócio aprendemos que é importante ter ao lado um parceiro estruturado, que possa não só ajudar a evitar problemas (por exemplo, através de serviços para evolução da arquitetura, revisão de custos etc), como também dar um suporte extra durante incidentes. Dependendo da configuração do seu time, a língua pode ser uma barreira também.
Em nossa experiência vimos que alguns fornecedores cloud não estavam preparados para oferecer esse suporte mais próximo pois atuavam em um modelo mais “faça você mesmo”, em que o cliente adotava no início da jornada empreendedora, mas depois não tinham estrutura para acompanhar. Crescer e escalar muito sem isso pode ser um risco.
Custo e valor são conceitos diferentes, porém nem sempre um provedor de serviço na nuvem consegue adicionar valor ao seu negócio, exceto quando ele ajuda seu negócio a crescer mais ou melhor. “Crescer mais” pode acontecer através da construção de novos canais ou bizdev de produtos, já “crescer melhor” é, por exemplo, quando esse fornecedor consegue potencializar o desenvolvimento das pessoas que fazem parte da sua equipe técnica. Em ambos os cenários, essa parceria agrega valor no serviço que você vende, seja em resultados institucionais, seja em capital intelectual.
Quando adotamos o GCP, há 3 anos, entendi que a proposta de valor deles era resolver de forma mais estruturada todos os pontos listados anteriormente.
Na pandemia, em 2020, pude também confirmar toda parceria que estávamos construindo e o respeito que tratam seus clientes. Diante do cenário nebuloso dos primeiros meses, onde estávamos entendendo diariamente tudo que estava acontecendo e ajudando nossos clientes com fluxo de caixa (em especial setores impactados e empresas com pouca visibilidade), o Google se mostrou um grande parceiro ao congelar o dólar para evitar ainda mais problemas. Isso reforçou bastante sua cultura e abertura pra conversar.
Recentemente fizemos uma nova negociação com GCP na ordem de dezenas de milhões, que nos reforça como um dos principais clientes do Brasil. Seguimos com nosso uso intenso, tanto no aspecto de quantidade de serviços, como no volume de dados, e suporte muito próximo. Em paralelo também estamos desenhando programas de desenvolvimento em conjunto, que irão auxiliar nossa equipe internamente a contar com toda a expertise do time deles.
Estou super animado com nosso momento e tudo que está por vir! O fortalecimento dessa parceria com o Google é uma possibilidade a mais de desenvolvimento para nosso time aprender com uma empresa referência mundial. E se você quiser conhecer mais sobre nosso modelo de trabalho e cultura, confira nossas oportunidades de carreira. Temos vagas para diferentes produtos e estágio de carreira.
Conteúdo, opinião, vivência e compartilhamento de ideias da…
42 
2
42 claps
42 
2
Conteúdo, opinião, vivência e compartilhamento de ideias da equipe de Produto e Engenharia da RD Station @RD
Written by
Co-founder / CTO @ResDigitais. Endeavor Entrepreneur. Angel. Dad.
Conteúdo, opinião, vivência e compartilhamento de ideias da equipe de Produto e Engenharia da RD Station @RD
"
https://medium.com/@maxxsh/website-on-google-cloud-platform-221d79d69976?source=search_post---------359,"Sign in
There are currently no responses for this story.
Be the first to respond.
Max Shestov
Jan 25, 2021·3 min read
This guide shows you how to set up Apache and PHP on a Compute Engine virtual machine instance on GCP. The instructions are for Ubuntu 20.04 Linux distribution (goes with PHP 7.4 in its default repositories).
Go to your Google Cloud Console and clickCompute Engine > VM Instances > CreateCreate a name for your instanceRegion — us-central1(Iowa)Zone — us-central1-c
[Update] Google introduced E2-micro Free Tier, which is a part of a second-generation VM family on August 1, 2021 Series - E2Machine type - e2-micro (2 vCPU, 1 GB memory)
Boot disc - Standard persistent disk, Image Ubuntu, 20.04 LTS (LTS — Long Term Support)
Firewall - checkmarkAllow HTTP trafficAllow HTTPS traffic
Click Create.It takes a few seconds to create the instance.
On your instance click SSH to open the SSH windowIn the opened window update packages that need upgrading typing
Now, install Apache
and install PHP and the extensions with the apt package manager
Verify PHP Version
In Google Cloud console on your instance copy external IP and type it in a browser to verify that Apache is running:
Alternatively, you can install a LAMP (Linux, Apache, MySQL, PHP) stack by Google Click to Deploy automatically.
If you don’t have a connection to your instance check this guide on how to connect to Google Cloud VM Instance.
In the Cloud Console clickVPC Network > External IP addressesIn the column Type of your instance select Static
Create an A record in your DNS Management console and copy/paste your external IP address.
Create a CNAME record for WWW subdomain and point it to your naked domain.
It usually takes effect within a few next hours.
Read this article to install auto-renewing Let’s Encrypt SSL Certificate using automated client Certbot.
Lead Developer at Reverence Global. Husband of a wonderful wife, Entrepreneur, Dad
See all (53)
369 
369 claps
369 
Lead Developer at Reverence Global. Husband of a wonderful wife, Entrepreneur, Dad
About
Write
Help
Legal
Get the Medium app
"
https://rominirani.com/google-cloud-functions-tutorial-debugging-local-functions-357c24829198?source=search_post---------360,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
This is part of a Google Cloud Functions Tutorial Series. Check out the series for all the articles.
In the previous post, we saw how we can use the Local Emulator to deploy and test our functions locally. Can we debug the functions too?
Let’s try that out as per the instructions below:
In the Github cloned directory, go to the folder hello-localfunctions. The index.js file is shown below:
This is a straightforward HTTP trigger based function. Assuming that you have the Local Emulator running (via the functions start command) deploy the function as per the command given below:
Once the function is deployed successfully, you can check if it is present in the list of Local functions as given below:
You can now use the inspect command as shown below:
Notice the last line. It says that a Debugger is started on port 9229.
Launch Google Chrome browser and type in chrome://inspectin the URL. It should give you a screen as shown below:
Notice the Remote target. Click on the inspect link in the browser window above.
Go to the Sources tab and put your breakpoint:
Now, you can invoke the function from the terminal as given below:
This will result in the code execution stopping at the breakpoint that we have placed as shown below:
Simply run the rest of the code. You will find that the terminal window via which you invoked the function will exit too with the “Success” message.
Proceed to the next part : Cloud Functions Pricing or go back to the Google Cloud Functions Tutorial Home Page.
Technical Tutorials, APIs, Cloud, Books and more.
60 
3
60 claps
60 
3
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
Technical Tutorials, APIs, Cloud, Books and more.
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
Technical Tutorials, APIs, Cloud, Books and more.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/velotio-perspectives/a-primer-on-http-load-balancing-in-kubernetes-using-ingress-on-google-cloud-platform-d45108f90ff1?source=search_post---------361,"There are currently no responses for this story.
Be the first to respond.
Containerized applications and Kubernetes adoption in cloud environments is on the rise. One of the challenges while deploying applications in Kubernetes is exposing these containerized applications to the outside world. This blog explores different options via which applications can be externally accessed with focus on Ingress — a new feature in Kubernetes that provides an external load balancer. This blog also provides a simple hand-on tutorial on Google Cloud Platform (GCP).
Ingress is the new feature (currently in beta) from Kubernetes which aspires to be an Application Load Balancer intending to simplify the ability to expose your applications and services to the outside world. It can be configured to give services externally-reachable URLs, load balance traffic, terminate SSL, offer name based virtual hosting etc. Before we dive into Ingress, let’s look at some of the alternatives currently available that help expose your applications, their complexities/limitations and then try to understand Ingress and how it addresses these problems.
There are certain ways using which you can expose your applications externally. Lets look at each of them:
You can expose your application directly from your pod by using a port from the node which is running your pod, mapping that port to a port exposed by your container and using the combination of your HOST-IP:HOST-PORT to access your application externally. This is similar to what you would have done when running docker containers directly without using Kubernetes. Using Kubernetes you can use hostPort setting in service configuration which will do the same thing. Another approach is to set hostNetwork: true in service configuration to use the host’s network interface from your pod.
Limitations:
Kubernetes services primarily work to interconnect different pods which constitute an application. You can scale the pods of your application very easily using services. Services are not primarily intended for external access, but there are some accepted ways to expose services to the external world.
Basically, services provide a routing, balancing and discovery mechanism for the pod’s endpoints. Services target pods using selectors, and can map container ports to service ports. A service exposes one or more ports, although usually, you will find that only one is defined.
A service can be exposed using 3 ServiceType choices:
Limitations:
Endpoints are usually automatically created by services, unless you are using headless services and adding the endpoints manually. An endpoint is a host:port tuple registered at Kubernetes, and in the service context it is used to route traffic. The service tracks the endpoints as pods, that match the selector are created, deleted and modified. Individually, endpoints are not useful to expose services, since they are to some extent ephemeral objects.
If you can rely on your cloud provider to correctly implement the LoadBalancer for their API, to keep up-to-date with Kubernetes releases, and you are happy with their management interfaces for DNS and certificates, then setting up your services as type LoadBalancer is quite acceptable.
On the other hand, if you want to manage load balancing systems manually and set up port mappings yourself, NodePort is a low-complexity solution. If you are directly using Endpoints to expose external traffic, perhaps you already know what you are doing (but consider that you might have made a mistake, there could be another option).
Given that none of these elements has been originally designed to expose services to the internet, their functionality may seem limited for this purpose.
Traditionally, you would create a LoadBalancer service for each public application you want to expose. Ingress gives you a way to route requests to services based on the request-host or path, centralizing a number of services into a single entry-point.
Ingress is split up into two main pieces. The first is an Ingress resource, which defines how you want requests routed to the backing services and second is the Ingress Controller which does the routing and also keeps track of the changes on a service level.
The Ingress resource is a set of rules that map to Kubernetes services. Ingress resources are defined purely within Kubernetes as an object that other entities can watch and respond to.
Ingress Supports defining following rules in beta stage:
When no host header rules are included at an Ingress, requests without a match will use that Ingress and be mapped to the backend service. You will usually do this to send a 404 page to requests for sites/paths which are not sent to the other services. Ingress tries to match requests to rules, and forwards them to backends, which are composed of a service and a port.
Ingress controller is the entity which grants (or remove) access, based on the changes in the services, pods and Ingress resources. Ingress controller gets the state change data by directly calling the Kubernetes API.
Ingress controllers are applications that watch Ingresses in the cluster and configure a balancer to apply those rules. You can configure any of the third-party balancers like HAProxy, NGINX, Vulcand or Traefik to create your version of the Ingress controller. Ingress controller should track the changes in ingress resources, services, and pods and accordingly update the configuration of the balancer.
Ingress controllers will usually track and communicate with endpoints behind services instead of using services directly. This way some network plumbing is avoided, and we can also manage the balancing strategy from the balancer. Some of the open-source implementations of Ingress Controllers can be found here.
Now, let’s do an exercise of setting up an HTTP Load Balancer using Ingress on Google Cloud Platform (GCP), which has already integrated the ingress feature in its Container Engine (GKE) service.
Ingress-based HTTP Load Balancer in Google Cloud Platform
The tutorial assumes that you have your GCP account setup done and a default project created. We will first create a Container cluster, followed by deployment of a nginx server service and an echoserver service. Then we will set up an ingress resource for both the services, which will configure the HTTP Load Balancer provided by GCP
Get your project ID by going to the “Project info” section in your GCP dashboard. Start the Cloud Shell terminal, set your project id and the compute/zone in which you want to create your cluster.
Fetch the cluster credentials for the kubectl tool:
Create a Service resource to make the nginx and echoserver deployment reachable within your container cluster:
When you create a Service of type NodePort with this command, Container Engine makes your Service available on a randomly-selected high port number (e.g. 30746) on all the nodes in your cluster. Verify the Service was created and a node port was allocated:
In the output above, the node port for the nginx Service is 30746 and for echoserver service is 32301. Also, note that there is no external IP allocated for this Services. Since the Container Engine nodes are not externally accessible by default, creating this Service does not make your application accessible from the Internet. To make your HTTP(S) web server application publicly accessible, you need to create an Ingress resource.
On Container Engine, Ingress is implemented using Cloud Load Balancing. When you create an Ingress in your cluster, Container Engine creates an HTTP(S) load balancer and configures it to route traffic to your application. Container Engine has internally defined an Ingress Controller, which takes the Ingress resource as input for setting up proxy rules and talk to Kubernetes API to get the service related information.
The following config file defines an Ingress resource that directs traffic to your nginx and echoserver server:
To deploy this Ingress resource run in the cloud shell:
Find out the external IP address of the load balancer serving your application by running:
Use http://<external-ip-address> and http://<external-ip-address>/echo to access nginx and the echo-server.
Ingresses are simple and very easy to deploy, and really fun to play with. However, it’s currently in beta phase and misses some of the features that may restrict it from production use. Stay tuned to get updates in Ingress on Kubernetes page and their Github repo.
*****************************************************************
This post was originally published on Velotio Blog.
Velotio Technologies is an outsourced software product development partner for technology startups and enterprises. We specialize in enterprise B2B and SaaS product development with a focus on artificial intelligence and machine learning, DevOps, and test engineering.
Interested in learning more about us? We would love to connect with you on ourWebsite, LinkedIn or Twitter.
*****************************************************************
Thoughts and ideas on startups, enterprise software &…
29 
1
29 claps
29 
1
Thoughts and ideas on startups, enterprise software & technology by the Velotio team. Learn more at www.velotio.com.
Written by
Velotio Technologies is an outsourced software and product development partner for technology startups & enterprises. #Cloud #DevOps #ML #UI #DataEngineering
Thoughts and ideas on startups, enterprise software & technology by the Velotio team. Learn more at www.velotio.com.
"
https://medium.com/javarevisited/10-best-free-google-cloud-professional-architect-certification-courses-practice-tests-943e75f03929?source=search_post---------362,"There are currently no responses for this story.
Be the first to respond.
Hello guys, if you are preparing for the Google Cloud Professional Architect certification exam in 2021 and looking for free and best online training courses then you have come to the right place.
In the past, I have shared the best courses to learn Google Cloud as well as certification courses to pass cloud engineer, data engineer, and cloud developer certifications, and today, I am going to share free GCP Cloud Architect certification courses for beginners and experienced cloud professionals.
This is one of the most difficult and prestigious exams, similar ot the AWS Solution Architect and Azure Technology Architect (AZ-300) exam, once you pass this exam, you will have sufficient knowledge and skills to propose a Google cloud-based solution, which is a very in-demand skill.
Cloud Industry is one of the fast-growing IT industries of this era. Soon it will be counted among the top IT industries of all time. If you are looking ahead to pursue a career in the IT industry, then cloud computing could be an excellent field for you.
When it comes to cloud computing, the first name that strikes our mind is Google Cloud Platform, like GCP. In the coming years, there will be a great demand for cloud experts in the industry. You must prepare for one such certification exam.
In this guide, I’ll share 10 free and best online courses and practice tests to pass the GCP Cloud Architect Exam with good grades. Once you are a certified expert, you can start your career working as an intern, and later you can begin your full-time job in cloud computing.
Besides this, In cloud computing, the pay scale is not a concern; most companies are offering good pay with different periodic incentives.
Now, let’s directly jump on the list of free online courses for the GCP cloud architect exam.
By the way, If you are serious about becoming a certified Google Cloud Architect then I also suggest you join the Ultimate Google Certified Professional Cloud Architect 2021 course on Udemy. It’s not free but it’s the best course to pass this prestigious Google cloud certification and it’s created by none other than Google Cloud Platform itself.
udemy.com
In this list, a few courses are interlinked with each other. Therefore you have to enroll in a set of courses to complete your GCP cloud architect exam preparation. Apart from this, all other courses are independent, and you can decide as per your choice.
This course includes a 30-day money-back guarantee, so it’s a safe bet. If you do not like the course, then you can just drop it and have your money returned to you. It has a rating of 4.2; about 64,280 students enrolled and was created by Google Cloud Platform Gurus. It is available in English, Italian, and Polish.  If you have very little knowledge about Google cloud platforms, then this course is for you as it provides very detailed explanations on Google Cloud Platform, which is required to pass this prestigious Google Cloud Certification.  The course is created by Google Platform Gurus and it has more than 24-hours of quality content to gain an in-depth understanding of Google Cloud Platform.
You will not only learn about Google cloud Compute Services like Virtual Machine (GCE), App Engine (GAE), Container Service (GKE), Google Cloud Function, but also other essential GCP services like IAM and Security, Networking, and Management and monitoring tools.
Here is the link to join this course — Ultimate Google Certified Professional Cloud Architect 2021
Overall, a great, comprehensive course to prepare for Google Cloud Professional Architect certification, and the best thing about this course is that you can buy in just $10 on Udemy sales which happens every now and then.
This is a Coursera course created by Google Cloud Training Team itself, and it is specifically designed for the candidates who are planning to appear in the GCP Cloud Architect Exam?
In this course, over 40 thousand students have enrolled so far, and it contains six different sub courses that will teach you every concept related to the GCP Cloud Architect Exam. Besides this, in this course, instructors will illustrate the implementation of various elements in the GCP dashboard.
Before you enroll in this course, it is essential for you to know that its a 3-month long video course and in this course, you’ll be assigned with assignments and task that needs to complete as per the deadlines given by the instructor.
If you are looking for a course that can teach you concepts and at the same time, help you practice those, then this course is a perfect pick for you. The course is also available for a 50% discount now.
here is the link to join this certification course — Cloud Architecture with Google Cloud Professional Certificate
Once you are done with the introductory course then it’s time to move towards the specific platform that we are preparing for, this Udemy course will teach you every minute detail related to the Google Cloud Platform.
In this 1-hour 16-minute long video course, the instructors from Linux Academy will teach you the basics of cloud computing, What is google cloud, and Why it is better than any other cloud computing platform.
In this course, you will also learn about various security aspects of GCP and different concepts related to the deployment of apps on it.
But before enrolling in this course, you must know that this course is specifically designed for beginners. If you are looking for an expert level of information, then you must avoid this course and enroll in some other course listed here.
Here is the link to join this free course — Google Cloud Concepts
If you are looking for a one-step solution, then this course is for you. It’s a 13-hour long course available on CloudAcademy.com. In this course, you’ll learn about different components of the Google Cloud platform that are linked with your certification exam.
So far, over 4 thousand students have enrolled in this course, and it has a rating of 4.5 stars out of 5. From security to deployment, every topic is explained in detail with the help of proper illustrations and graphics.
You won’t regret enrolling in this course, every second of this course is worth your time. So if you are looking for an in-depth and practical approach based course, then this one is a perfect pick for you.
First of all, since you are just starting with your preparation for this exam, it is essential to build a base. This course will help you create a strong foundation, and from this foundation, learning about GCP won’t be a problem for you.
This course is available on Udemy, and it is created by Xavier Corbett. In this course, the instructor has used the graphics and gifs quite smartly. These graphics make this course relatively easy to understand, and since we are talking about the basics here, therefore proper illustration is required. Again the instructor has taken care of this too.
In this 1 hour long course, you will learn about every basic and minute of information related to Cloud Computing. Once you are done with the course, you can easily sit in a basic level interview of a cloud computing-related job.
Here is the link to join this free course — Introduction to Cloud Computing
Another Coursera course and is also created by Google Cloud Training Team. The major difference between this course and the above one is that it’s a comparatively short course.
In this course also, first, the instructors will teach you about every concept related to the GCP cloud architect exam, and then you’ll be assigned a project.
Once you have completed the assignment successfully then as a reward, you will get a certificate of completion. Like every Coursera course, here also you have to work on a live project.
Overall, if you have completed this course with dedication, then you won’t need any other source to prepare for the GCP Cloud Architect Exam.
Here is the link to join this course — Architecting with Google Compute Engine Specialization [
By the way, If you are planning to join multiple Coursera courses or specializations then consider taking a Coursera Plus subscription which provides you unlimited access to their most popular courses, specialization, professional certificate, and guided projects. It cost around $399/year but its complete worth your money as you get unlimited certificates
udemy.com
Although most courses assume that you have a fair knowledge of Google Cloud Platform Architecture, this course checks with you first to identify just how much you understand and then provides you with more information about the exam. The course can be seen as a detailed map of Google Cloud Architecture. You can map your own fastest route to the examination with it. But not to worry, the course will help you distinguish what you know from what you don’t know. There is also Activity Tracking Challenge Labs to help you test how much your ability has developed. Despite the name of the course, this course is so much more than an exam guide. It gives you real practical skills like developing proposed solutions, cognitive skills such as case analysis, and identifying technical watchpoints. There are also a bunch of mock exams available in this course with answers, so you will know how much you are ready for the exam.
Here is the link to join this track — Preparing for the Professional Cloud Architect Exam
Btw, you would need Pluralsight membership to access this course which costs $29 per month or $299 per year, a 14% discount. This membership also gives you access to many more online courses and quizzes. Alternatively, you can also take their 10-day-free-trial to access this course for FREE.
pluralsight.pxf.io
Do you love boot camps? Because I do, especially ones for developers. If you are a boot camp enthusiast, then this course will be perfect for you. Just like most actual boot camps, this course is quite stressful but seeks to bring out the best in you. This course will prepare you for the Google Cloud Platform Architect exam. It has a student rating of 4.0 on Udemy with 4,256 students enrolled. It was created by Joseph Holbrook (The Cloud Tech Guy). It is available in English, Italian and Polish.
It also offers a summary or review to check if everyone is up to speed, so fear not; you won’t be left behind. Everything you will need to pass the Google Cloud Platform Architect exam is covered in this course. The author acknowledges the fact that the Google Cloud Professional Architect exam is not cut and dry, and not written clearly and concisely as there are many approaches to a single question. It tries to cover most of these approaches so that students can go with what they find most agreeable. The most important requirement is knowledge of Google Cloud Platform, such as familiarity with the GCP Console.
Here is the link to join this course — Google Cloud Certified Professional — Architect — Boot camp
Finally, last but not least is the GCP Architect practice test. This course has a rating of 4.3 from its students, with 1,756 students enrolled, and was created by Whizlabs Learning Center. This is one of the few professional Google Cloud Platform Architect course that does not require any prior knowledge of Google Cloud Platform.  It comes with 150 practice tests. As you may have gathered, this course focuses less on helping you acquire the Google Cloud Platform Architect work skills and more on passing the Google Cloud Platform Architect exam.
If you are an AWS Solution Architect, Microsoft Azure Architect, or a Cloud Developer who wants to understand Google Cloud Platform or Customers of Amazon, Google Cloud Platform, Azure, or any other then take this practice test, you will love it.
Here is the link to join this practice test — Google Cloud Professional Cloud Architect Practice Test
This is another fantastic practice test you can take on Udemy to prepare for Google Cloud Architect certification. This course contains 152 high-quality questions that will test your skills like the real exam.
All questions and answers in these practice exams have been carefully curated and updated to be fit for the 2021 Professional Cloud Architect Certification exam. Every question has a detailed explanation of why an option is correct and why the other options are wrong.
The scenarios covered in these practice tests and the breadth and complexity of the questions here are indicative of what you see in the real exam. The instructor also updates this course regularly to include new up-to-date content which is great, given the fluid nature of the cloud certifications.
Here is the link to join this test — Google Certified Professional Cloud Architect Practice Tests
This is another online course you can take to prepare for Cloud Architect certification. This provides a really comprehensive guide to the Google Cloud Platform — it has ~25 hours of content and ~60 demos.
Here are things you will learn in this course
Here is the link to join this course — GCP: Complete Google Data Engineer and Cloud Architect Guide
That’s all about the best online course to pass Google Cloud Professional Architect certification in 2021. In this guide, you have learned about the five best free courses to pass the GCP professional cloud architect certification exam.
Once you are done with these courses if time permits you, then it is recommended to register on GCP and try to implement the concepts that you have learned in these courses practically.
Practicing concepts practically will help you understand every minor detail of the google cloud platform.
Other Cloud Computing and IT Certification resources you may like
Thanks for reading this article so far. If you find these free Google Cloud Architect certification courses useful, then, please share them with your friends and colleagues. If you have any questions or feedback, then please drop a note. P. S. — If you are serious about learning the Google Cloud platform and passing Cloud Architect certification then I also suggest you join the Ultimate Google Certified Professional Cloud Architect 2021 course on Udemy. It’s not free but it’s the best course to pass this prestigious Google cloud certification and it’s created by none other than Google Cloud Platform itself.
udemy.com
Medium’s largest Java publication, followed by 14630+ programmers. Follow to join our community.
120 
120 claps
120 
A humble place to learn Java and Programming better.
Written by
I am Java programmer, blogger, working on Java, J2EE, UNIX, FIX Protocol. I share Java tips on http://javarevisited.blogspot.com and http://java67.com
A humble place to learn Java and Programming better.
"
https://towardsdatascience.com/deploy-a-python-visualization-panel-app-to-google-cloud-ii-416e487b44eb?source=search_post---------363,"Sign in
There are currently no responses for this story.
Be the first to respond.
Sophia Yang
Dec 25, 2021·4 min read
Related article: Deploy a Python Visualization Panel App to Google Cloud App Engine: Google Cloud App Engine and Github Actions.
In my last blog post, I wrote about how to deploy a Python visualization Panel App to Google Cloud App Engine. App Engine…
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@koistya/google-cloud-sql-tips-tricks-d0fe7106c68a?source=search_post---------364,"Sign in
There are currently no responses for this story.
Be the first to respond.
Konstantin Tarkus
Sep 6, 2020·6 min read
Ensure that the instance name is picked wisely. You may find it helpful if it remains short, and contains the Cloud SQL edition (MySQL, PostgreSQL, SQL Server) and version number. For example, pg12.
"
https://larrylu.blog/cloud-automl-vision-practice-b0d2c4377a87?source=search_post---------365,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Google Cloud AutoML Vision 是 GCP 最近新發表的服務，這次跟 GCP 專門家合作寫了一篇用 AutoML 辨識蜆、蛤蠣、海瓜子的介紹文，希望大家喜歡
首先我們用 Google Search 來收集這三種海鮮的圖片作為訓練資料，這邊可以利用 google-images-download 這個 tool 批次下載圖片。
如下圖，將下載的圖片分別整理至三個角色各自的資料夾中 (Xian: 蜆；Hali: 蛤蜊；Haiguazi: 海瓜子)。
Cloud AutoML Vision 有兩種上傳訓練圖片的方式，這邊我們使用 Cloud Storage 匯入。
首先，將收集好的訓練圖片上傳至 Cloud Storage (alpha 有限定要在 PROJECT_ID-vcm 這個 bucket 裡面，假如你的 project ID 是 test-123 那麼就要上傳到 test-123-vcm 這個 bucket)。
如下圖：
接著要建立一個 CSV 檔案描述訓練圖片 URL 和其所對應的 labels，CSV 內容節錄部分如下：
然後將該 CSV 也上傳到 Cloud Storage 中，如下：
1. 在 Cloud AutoML Vision 的 console 中，點選 「NEW DATASET」。
2. 填入 Dataset 名稱，指定好 CSV 的 Cloud Storage URL，點選 「Create Dataset」(訓練資料如果很多的話，需要稍等片刻等待資料匯入完成)
3. 匯入完成後，可以在 「IMAGES」tab 可以看到匯入圖片的縮圖
1. 在「TRAIN」tab 點選 「TRAIN NEW MODEL」按鈕
2. 選擇 Training budget (理論上 compute hour 愈多，訓練出來的 model 準確率越高，)，然後按下 「START TRAINING」按鈕 (這通常需要一些時間)
3. 沒錯，你完全不需要具備 machine learning 的背景知識，就可以訓練出一個 machine learning model。訓練完成後，可以在「EVALUATE」tab 看到訓練的結果，包括 Precision、Recall、Confusion matrix 等指標 (參照 lee 文)
到這邊我們已經完成了這個狗狗辨識器啦！切到「PREDICT」tab 上傳一張你想要辨識的圖片，這個辨識器就會告訴你這是蜆、蛤蜊還是海瓜子，並且有一個信心分數 (0 ~ 1)。
頁面下方也會給出 prediction API 的使用範例。你不需要擔心這個 API 是 host 在哪邊也不需要擔心 scaling 的問題，Cloud AutoML 會幫你代管這個 API 服務。
你也許會問，有什麼方式可以提升 model 的準確率呢？在 Cloud AutoML 當中，因為 training 和 evaluation 是由 Cloud AutoML 自動處理，因此我們只能藉由提升訓練資料的品質，來提升模型的準確率。
切換至 「IMAGES」tab，我們發現在 Xian(蜆) 的訓練資料當中，其實有些圖片並不是 Xian(蜆)，因此我們可以將這些錯誤的訓練資料刪除，提升訓練資料的品質，如下圖。
整理完各類別的訓練資料之後，我們再重新訓練一個新的 model，準確率果然有顯著的提升。
Cloud AutoML 獨家搶先體驗
結合 AI 大眾化的趨勢，Google Cloud 首席合作夥伴：GCP專門家架設了「Cloud AutoML 獨家體驗專區」，讓所有人都能即刻感受 Cloud AutoML 的威力。
若想客製化擁有自己的 Cloud AutoML 模型，GCP專門家提供以下教學文章與應用案例：
擁有專屬自己的機器學習模型
想立即擁有自己的客製化機器學習模型嗎？想訓練模型卻不知從何下手嗎？
立刻與 GCP專門家聯繫吧！
瀏覽更多 Cloud AutoML 相關文章與 Google Cloud 產品應用，詳見 GCP專門家技術部落格，最新知識均在此與您分享。
我是 Larry Lu，我熱愛技術、喜歡與人分享，專長是 Web 前後端還有 CI/CD，歡迎大家來跟我聊聊技術～
82 
1
82 claps
82 
1
Written by
我是 Larry 盧承億，傳說中的 0.1 倍工程師。我熱愛技術、喜歡與人分享，專長是 Javascript 跟 Go，平常會寫寫技術文章還有參加各種技術活動，歡迎大家來找我聊聊技術～
我是 Larry Lu，我熱愛技術、喜歡與人分享，專長是 Web 前後端還有 CI/CD，歡迎大家來跟我聊聊技術～
Written by
我是 Larry 盧承億，傳說中的 0.1 倍工程師。我熱愛技術、喜歡與人分享，專長是 Javascript 跟 Go，平常會寫寫技術文章還有參加各種技術活動，歡迎大家來找我聊聊技術～
我是 Larry Lu，我熱愛技術、喜歡與人分享，專長是 Web 前後端還有 CI/CD，歡迎大家來跟我聊聊技術～
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/a-dissection-of-google-cloud-next-17-whats-next-c32f4d803750?source=search_post---------366,"There are currently no responses for this story.
Be the first to respond.
tl;dr; What are you waiting for? Get on the plane or be left behind in dust.
Let’s start from the very beginning. When the event was announced, the branding was totally different. Rather than being GCP Next, it was now Google Cloud Next. Hmm.. time to put my thinking cap on.
It’s always amusing to watch Google’s events unfold. While Google IO used to be my favorite one, Google seems to have turned it into an end-consumer centric event. In those same efforts, they combined GSuite and Google Cloud Platform (GCP) into one Google Cloud offering, thereby making Google Cloud Next event to be everything related to Cloud. For technologies that are partly open source and have grown way beyond Google’s own infrastructure, there are now summits, most notably Chrome, Android and recently Tensorflow summit. With renaming of GCP Next to Google Cloud Next, google is now betting big on their cloud efforts and other hardware efforts in that organization. While Chrome and Android do not get a big mention in Google Next, remember that it’s the hardware part that is Google’s own, rest is majorly open sourced.
Well, you see, when you change branding, you also change what’s happening in the brand. Google started these changes by the end of GCP Next 16, it was pretty clear that Google is pushing more and more towards Enterprise. Diane Greene became SVP and started on her mission to get Enterprises on GCP. All this while, Urs Holzle been doing what he does best, building and extending the amazing infrastructure that Google runs on. But, somewhere during this time Google realized that open source efforts at Google, esp on Kubernetes and TensorFlow have grown considerably and product strategy might remain incomplete if they do not include Open Source into it. So they went ahead and got one of the best, Sam Ramji onboard. And then they were happy once again. But wait.. Come end of year, they realized that there are two types of folks in machine learning, academia and enthusiasts that they have been overlooking. Fei-Fei Li and Kaggle were acquired to complement the missing part and now the puzzle is solved. For now.
Well let’s take a flight to SF and have a closer look. And one day before the event, Sam Ramji and GCP team is having an event with Google Cloud Insiders (a group of users, evangelists and experts on GCP). What’s so special about doing this event? Well, one thing about Sam, his approach is rooted on ground up and hard truth. The event was a step forward from Google to embrace developers. Most of the developers/geeks aren’t used to getting hugged and the reactions swing, either they will love you or complain a lot. No, devs don’t hate people, they hate technologies that those folks use. Here Sam’s charisma played a major role. His answers are long and explanatory, rather than being cut short. He will hear you out and explain not only what they are trying, but also why. He connects to developers personally, he reads books that we have read and if he could, he would make all of Google open for you. He got his whole team in action. If you do not like Google’s Support, let’s talk. Not a fan of direction of Kubernetes, we are here. Confused between App Engine Standard, Flex and Cloud Functions, let’s call in experts.
Long queues are always an issue at events of such a scale. That’s why they give badged one day in advance. Plan better next time. On stage, we see a plethora of enterprises using Google Cloud, be in Compute, Data or GSuite offerings. As the keynote unfolded, it was pretty clear that Greene deliverd on her promise of making Google Cloud competitive in enterprise market as well. She made enterprise not only move from their infrastructure, but also from competitions. She not only built internal teams to help support this, but also laid foundation for partners and other providers thereby enriching the whole ecosystem around Google Cloud. But since the audience of GCP has conventionally been developers, parts of crowd was internally screaming (Show me some code!!). Well, their calls were answered by Fei-Fei Li’s arrival on stage. Not only she showed a promising roadmap for machine learning at Google, she connected to audience’s plight of machine learning being an alien concept (to implement properly). Video Intelligence API and Kaggle’s acquisition solidified the fact of google’s ongoing investment in ML and it’s democratization for masses.
Well, you see, everyone’s been complaining that GCP does not have enterprises. And it was a valid complaint. Enterprise is slow. They move because someone else is moving. And now Greene has shown a stream of some of the biggest organizations moving to Google Cloud. This will pull all the enterprises which have been on the edge to choose between Google and anything else to Google Cloud. But.. while planning this amazing show, the transition from core developer centric event to an all technology event, the folks misjudged developers love for action. Just one demo was not enough to fulfill the appetite and hopefully it will be considered appropriately next time.
You get a car, and you get a car and you too will get a car. Everyone gets something! Not Titan though. Urs Holzle and team put on a spectacular show, giving away everything (almost) folks have asked for. More zone, better security, GPUs, more machine learning APIs, even more security services, core machine learning, databases and data products, more platforms, go serverless, NoOps, no nonsense facts and amazing live demos. Everything developers hoped for was there, and rejoiced they were. Did you forget, we are not just GCP now, take some updates from GSuite team too. Make sure whether you are working with your team from Chromebook, or hanging out with them, or just jamming with them you can do all of that effortlessly and remain productive at all times. All this while, partners are coming and going off the stage, showing that these are not just Google used technologies, enterprises are gaining from them already and winning big. At the end of keynote, Urs makes the concluding statement: “And we understand it’s not just about the technology, it’s about helping everyone be successful in using it.”
Let’s hope Sam surprises us. And he does! A complete history of Open Source and Google is being played effortlessly on stage. Starting by father of internet Vint Cerf himself. Reminiscing the early days of internet he makes everyone realize how open source has been the key for internet and continues to be so. We move further in time where foundations are here to help us with open governance, trust and security of open source. Then of course how google has been involved in open source projects, not just of their own, but also the overall ecosystem. The bet google is making here is that they will take chances with Open Source since they are confident in their own innovations and speed of execution. And now when ecosystem is there, partners are welcome as well. Oh and, don’t forget, we are committed towards open machine learning and open data too. Open source and cloud have to work with each other to bring out the best in each other and Sam is on a mission to make it happen. We are building an Open Ecosystem where users will win and we welcome all of you to take part in this process. Let’s begin by extending your credits and giving you some always free services. Also, we love Startups and wanna help you build great machine learning products and give you loads of credits and funding too. Let’s do it!
It’s all about the mission. Mission is not to just be great in enterprise, or amazingly engineered platform, but it’s a collection of all such entities and at the same time, keeping open source, end developers and startups along with us. In Sam’s words: “Negative feedback is a gift, it’s a declaration of what’s missing”. Google has heard us well. They have rethought and rebuilt their products and approaches to include everyone.
Watch as the market responds to Google, how adoption happens, how quickly Google deliver on promises and be ready to get amazed by progress reports, new products and features next year.
I will never leave just like that haha. Here you go:
Disclaimer: I am not a googler, all of these are just observations.
Google Cloud community articles and blogs
28 
28 claps
28 
Written by
Building scalable web and data platforms for high availability, resilience and security.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Building scalable web and data platforms for high availability, resilience and security.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/hackernoon/trials-and-tribulations-of-google-cloud-hosting-a-tutorial-26395eb25636?source=search_post---------367,"There are currently no responses for this story.
Be the first to respond.
If you ever wanted know all of the steps that go into deploying a live site, from buying a domain, to hosting it, to running multiple sub-domains then you’re in the right place. We’ll cover the following:
Whenever I think about intuitive hosting and well written, up-to-date documentation, gcloud is not at the top of the list… hence this tutorial.
To get things started, you’ll need to purchase / be the owner of a domain. Feel free to pick whichever domain registrar you want, as long as it allows DNS management… I’ve chosen GoDaddy for my domains.
These days you can purchase a domain for as little as 9€ / year; from this point forward, let’s assume we own example.com. You’ll also need a Google Cloud account, feel free to create one here.
Create a new Google Cloud project from the Dashboard. We’ll need it later as our deployments need to be within the context of a project:
Once created, the project will be automatically selected. The next step is to go through Google’s domain verification process. This is needed to prove our domain name ownership.
Go to your Google Cloud / App Engine / Settings section Custom Domains subsection. Click the Add a custom domain and you should reach a page similar to this:
Pick the Verify a new domain option, type in your domain name and hit Verify. You’ll end up on Webmaster Central which is Google’s automated domain name verification tool:
You should select your domain name provider (in my case GoDaddy.com) and click Verify; Google will try to automatically add a TXT record in your domain name provider DNS records.
Don’t worry if your domain provider doesn’t appear here, it just means Google won’t be able to automatically create the required DNS record.
If that’s the case, simply select Other from the domain name provider dropdown and you’ll be given a TXT record which you need to copy over to your domain name DNS Manager like shown bellow:
Log into your domain name provider and select the option which allows you to manage the DNS records for your domain, then add the TXT record provided by Google (shown above). Your DNS Manager should look similar to this:
After this step is done the Google domain name verification will fail. Yes, fail. This is something you’ll get used to, as changes in DNS take a while to propagate. It can take anywhere from a few minutes to 24 hours (maybe more). In my case it took around 8 minutes. Keep clicking the Verify button once every 2–3 minutes until it says the verification was successful.
If you’ve gotten this far, then congratulations! Google now recognizes you as your domains owner. If it’s not automatically added, you should also add a www CNAME record in your DNS Manager like this (needed in order to access your domain with www.example.com):
Go ahead and install the Google Cloud SDK, we’re going to need it later. Once that’s done, run gcloud init in a terminal to setup the default configuration.
We’ll need an application which we can deploy to the App Engine; a super basic NodeJS Hello World seems appropriate. Here’s our server:
A package.json file with a start script is also needed; something along these lines:
Finally we need an app.yaml file which handles the App Engine setup. Again we’ll go for the most basic one:
Make sure you read the Google Cloud documentation on app.yaml files. Ours basically instructs App Engine to keep the automatic scaling to 1 instance, have a NodeJS runtime with a flexible environment and it targets this service (our application) as the default service. I also added an example on how to skip multiple folders which should not be deployed.
The service part is crucial as you must deploy a default service before anything else. Picture services as standalone applications or as microservices that are part of a larger application.
To better visualize this, imagine you have 2 applications: The first is your frontend and the second is your backend API. In this scenario you could deploy your frontend application as the service: default and your backend as the service: api.
Keep in mind that the more services you have, the more you’ll pay. Having multiple services is probably best suited for large applications. You can actually create a single service and point all of your domains and sub-domains to it. We’ll cover sub-domains later.
There’s our basic application, it only has 3 files: server.js , package.json and app.yaml. Open a terminal in this project folder and run: gcloud app deploy
After a few minutes your deploy will be available at a URL similar to this: https://project-000000.appspot. Congratulations, you’re live!
Next step is to link it with your purchased domain. To achieve this, start by creating a dispatch.yaml file and add the following lines:
Pretty straight forward, you specify the domain you’ve linked to your project and the service it points to. Now we need to upload it to Google cloud and that’s done by running this command: gcloud app deploy dispatch.yaml
Once this is done, if you visit your App Engine > Services page you’ll see something like this:
Every time you deploy the default service, another version will be created and all traffic will be ported to it. You can see all deployed versions in your App Engine > Versions section or by running the command gcloud app versions list. You can also choose to split traffic between two or more deployed versions in case you want more control on how many users hit a specific version of your app.
Now, you’re truly live and your SSL certificates are automatically regenerated by Google (by default) but there’s more that could be done; for instance, restricting your traffic to HTTPS only. You’d normally do this in Apache or Nginx through a 301 redirect, but it can also be achieved through a NodeJS middleware (express was used in this example):
Assuming you’d also want to run a sub-domain, it’s just as simple. First, we add the sub-domain in our App Engine > Settings > Custom Domain > Add Custom Domain by selecting the domain we want to use (example.com) then specifying the sub-domain we want to create (awesome.example.com).
We’re then instructed by Google to create a CNAME for our sub-domain in our domain registrar DNS Manager… it’s the same thing we had to do for www:
The last thing we need to do is point our newly created sub-domain to a service. Which we already know how to do… enter dispatch.yaml:
Then we have do deploy this dispatch.yaml once again by running: gcloud app deploy dispatch.yaml.
Small note here; if you skip this step, all of your sub-domains will point to the default service. To avoid costs you might want to do that, and manage the sub-domain mapping logic within your main application. ExpressJS has the express-subdomain npm package which could help you with that.
That’s quite a lot of information but it should get you up and running pretty fast… only one problem remains… how much does this all cost?
There’s no denying it… if you want flexibility, it will cost you. Google’s App Engine can get pretty expensive even for our boilerplate which does nothing. Here’s a price calculator which can give you an overview of how much you could end up paying for your hosting as well as the App Engine pricing list.
You can however reduce the cost significantly by lowering the maximum number of allowed instances to 1 and sticking to a single service (the default one).
In the 10 days I’ve been running my domain the total hosting costs have been around ~$13 for one service on one domain. Not really what you want to see while prototyping your application. Currently however Google is offering a 1 year free trial + $300 spending credit so you can get a feel of how it works and how much you’ll pay.
If you’re looking to further improve costs there’s always Google’s Compute Engine, which can be configured to go as low as $6~8 / month… but don’t expect you’ll be running a full blown application with hard core traffic on it. Probably the best choice for small businesses, personal websites and portfolios.
A small “Gotcha!” I should mention is that you can’t delete the default service once it’s created. You can only stop the versions running within it. Even if you stop all versions and delete them, one will still remain, the one with 100% traffic allocation on it. If you really want to get out… delete the entire project.
If however you still want scalability, versioning, easy and fast deploys, while maintaining humanly decent costs, I would recommend Zeit. Guillermo and his team are doing a great job over there.
Happy coding!
#BlackLivesMatter
132 
132 claps
132 
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
CTO, Evozon
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
"
https://architecht.io/google-cloud-is-winning-over-customers-starting-with-big-data-and-ai-9391a838d873?source=search_post---------368,"This is a reprint (more or less) of Thursday’s ARCHITECHT Daily newsletter. Sign up here to get it delivered to your inbox every morning.
There is not a topic in IT right now more likely to get somebody talking than whether Microsoft, Google or some darkhorse candidate can give Amazon Web Services a real challenge in cloud computing. The conventional wisdom is that Microsoft Azure is in the best position right now, but I have to say that Google continues to present a strong case for itself. It’s certainly not as mature as AWS, but — as I suggested back in March — users seem to really like the performance of the Google cloud, as well as its big data and artificial intelligence services.
Want proof? Here are three items from today alone underscore this point:
My podcast co-host, Barb Darrow from Fortune, also published a post today highlighting a handful of other customers moving to Google, including Mixpanel, Server Density, and more recent comments from Metamarkets’ Driscoll.
Even if a lot of these companies are taking a multi-cloud strategy right now — and might still host a majority of stuff on AWS or elsewhere — sometimes all it takes is getting a foot in the door. Google hasn’t had to play the scrappy startup role in a long time, but everyone knows it has a lot to offer. Experiments with BigQuery, Cloud Vision APIs or Container Engine could expand to a lot more very quickly.
architechtshow.com
In this episode of the ARCHITECHT Show, Elastic founder and CEO Shay Banon talks about the evolution of Elasticsearch — from an open source side project (the first iteration was a recipe-search app for his wife) to popular big data tool to the core of a company worth nearly a billion dollars. He also shares his thoughts and strategies on the growth of Elastic, which, somewhat under the radar, has expanded to include multiple products and employ hundreds of people around the world.
architechtshow.com
In this episode of the ARCHITECHT AI Show, Derrick Harris speaks with Jeremy Howard and Rachel Thomas of Fast.ai, where they teach popular online courses aimed to get students up and running with deep learning. Among other things, Howard and Thomas discuss the promise of deep learning and early student successes (including Hot Dog, Not Hot Dog app from Silicon Valley), as well as the threat of job losses from AI and how seriously we should take Elon Musk’s AI warnings.
techcrunch.com
AIMatter is based in Belarus and built an app, called Fabby, that lets users add effects to their selfies. One has to wonder if Google is holding onto its dreams of launching a successful social platform, or if it has other plans for the technology. Or perhaps it’s just playing games with Snap …
research.googleblog.com
Speaking of Google and digital photos … This is kind of interesting work, but Google devised a pretty sophisticated method for getting around them. Seems like there should be another use for this technology.
news.wsu.edu
Yes, an artificial neural network that’s as good as humans at mapping neural networks in brains. If AI helps us better understand neuroscience, which helps us develop better AI, that will really be something.
www.fast.ai
This is a good thing to look into if you meet the criteria (i.e., not white, straight and male) and live in San Francisco. Some people get up in arms about the push for diversity, but the truth is that it is a big problem — and will only get bigger — as we rely on algorithms more important things.
arxiv.org
I linked to the blog post about this earlier in the week, and here’s the paper. When someone ultimately does crack StarCraft II, it will probably be a smaller deal from a PR perspective than chess, Go, Jeopardy, etc., but a bigger technological accomplishment.
www.oreilly.com
Here’s an O’Reilly podcast featuring two folks working on the Ray project at UC-Berkeley’s RISELab. Ray, as you might have heard in the recent podcast with RISELab director Ion Stoica, is focused on building a general-purpose execution platform for reinforcement learning, etc.
techcrunch.com
I’m all for DigitalOcean, Cloudflare and whoever else booting Nazi websites from their services, but where’s the line on publicly shaming them for hosting them in the first place? When people can sign up with a credit card, you’re going to get some bad apples and might not even know it.
www.theregister.co.uk
It was in Amazon S3, not that that part really matters. This is just another brick in the wall of data left exposed because something wasn’t configured correctly.
coreos.com
CoreOS was funded by Google Ventures and there were rumors Google was going to acquire it, but here it is announcing stable support for Microsoft Azure. That’s really a necessity today if you’re claiming to be multi-cloud, and multi-cloud will be a big selling point for Kubernetes.
thenewstack.io
Yet another reminder that microservices environments can be as complex as containers are simple. For more on service meshes, check out the recent podcast interview with William Morgan of Buoyant, which manages the Linkerd project.
arxiv.org
A group of researchers is working on a method to help GPUs better handle memory requirements from multiple applications. They say improvements are necessary as general-purpose GPU usage ramps up, especially in multi-tenant cloud environments.
If you enjoy the newsletter, please help spread the word via Twitter, or however else you see fit.
If you’re interested in sponsoring the newsletter and/or the ARCHITECHT Show podcast, please drop me a line.
Use Feedly? Get ARCHITECHT via RSS here:
www.reuters.com
The basic pitch around ThoughtSpot is that it’s a simpler way to get information by using a Google-like interface. It’s now being marketed as AI, but that’s probably a stretch.
www.technologyreview.com
This is a nice overview of an increasingly complex market — especially, I would expect, as companies start relying on gig workers rather than just UPS and FedEx. My gut tells me it’s largely data-based right now, but I could imagine advances in AI (maybe even quantum computing) having an impact via a learning-based approach.
Enterprise IT interviews and analysis: AI, cloud-native, startups, and more
93 
93 claps
93 
Written by
Founder/editor/writer of ARCHITECHT. Day job is at Pivotal. You might know me from Gigaom - way back in the day, now.
Once a site about next-gen enterprise IT and the people building it; now a place where Derrick Harris occasionally blogs about tech-related things.
Written by
Founder/editor/writer of ARCHITECHT. Day job is at Pivotal. You might know me from Gigaom - way back in the day, now.
Once a site about next-gen enterprise IT and the people building it; now a place where Derrick Harris occasionally blogs about tech-related things.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/google-cloud-platform-tech-nuggets-june-2021-eb54079b31fe?source=search_post---------369,"There are currently no responses for this story.
Be the first to respond.
Welcome to Google Cloud Platform Technology Nuggets. We look at all the announcements in the previous month and highlight a few articles to read along with our own commentary.
Updates from Google I/O
Our annual developer event Google I/O was held on May 18–20 and saw a great set of announcements ranging from Android, WearOS, Flutter, Tensorflow and of course Google Cloud. Specifically speaking of Google Cloud, there were announcements in areas of AI, Serverless and Google Workspace. You can read the summary but more specifically, check out the Cloud Developers guide to I/O that contains links to the sessions and workshops, which are now available on-demand. If you’d like to try a hands-on lab, feel free to go for any of the Cloud codelabs from the event.
Customer Stories
Our Customer stories this month focuses on how 6 customers chose to run their SAP Workloads on Google Cloud Platform with the transformation and benefits that they are seeing today.
Check out our SAP solution site for more information on SAP on GCP and our customers.
Anthos Updates
We have published a number of articles on Anthos and we have aggregated them for you in an easy to reference blog post, titled Anthos in depth, that covers our articles published in our hybrid and multicloud development series. An interesting article that we want to highlight here tackles a common situation once you’ve bought Anthos. What do you do next to build that momentum? We cover six initiatives to start your Anthos Day-1 journey.
If you are looking to get started with learning Anthos, check out our Anthos 101 series.
Security Updates
Cloud Armor, our Distributed Denial of Service (DDoS) protection and Web-Application Firewall (WAF) service on Google Cloud, saw an update with the introduction of Google Cloud Armor Managed Protection Plus. The updates include: A machine learning powered service to detect Layer 7 attacks, Curated rules to simplify the deployment of effective access controls for your applications , a 24/7 DDoS Response support and DDoS Bill Protection, where customers will be able to open a claim to receive a credit in the amount of the bill spike due to a DDos attack.
Do check out our post that lists down best practices to protect your organization against ransomware attacks. In this article, we list down 5 pillars of protection (Identify, Protect, Detect, Respond and Recover) as identified by NIST in their Cybersecurity Framework and give you information on how various Google Cloud Services help you to achieve that.
Google Cloud has also published CISO Perspectives: May 2021, that provides a good roundup of cloud security and industry highlights.
Big Announcements from Data Cloud Summit
The Data Cloud Summit, held on May 26, brought together leading companies to share how they are using Google Cloud to build their data clouds. The summit saw key new product announcements and updates to multiple services. You can check out the list of announcements here. We’d like to highlight a few here:
We also announced Vertex AI, an AI Platform that simplifies the process of building, training, and deploying ML models at scale.
The sessions from Data Cloud Summit are available on-demand.
Bonus : A handy guide to product offerings across Cloud Service Providers and Compute Engine #Sketchnotes
We have a handy new guide to product offerings across Google Cloud, AWS and Azure. This is a revamped version of our earlier document but this time with the ability to filter in a specific area that you are looking at, e.g. Compute. Check out the blog post, which lists other useful resources like 2-minute guide to various GCP services and describing each Google Cloud Product in 4 words or less.
Our core compute offering, Compute Engine, gets its own sketchnotes. Learn about this service, key use cases, pricing and how it works.
Stay in Touch!
Have questions, comments, or other feedback. Do send it across.
Looking to keep a tab on new Google Cloud product announcements? We have a handy page that you should bookmark → What’s new with Google Cloud.
Google Cloud community articles and blogs
26 
26 claps
26 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@mpjme/how-to-archive-your-huge-final-cut-projects-to-cheap-google-cloud-cold-line-storage-a649fcb0c82b?source=search_post---------370,"Sign in
There are currently no responses for this story.
Be the first to respond.
Mattias Petter Johansson
Oct 29, 2016·2 min read
I really like offsite backup for my data, but I’ve struggled for a while to find a good place to back up my video projects. Dropbox and similar services (10$/month for 1TB-ish) are sufficient to archive the rendered projects (1–2GB). However, archiving the Final Cut projects with raw footage (30-60GB per project) reliably, and at a reasonable cost seemed like an impossible dream. However, I’ve recently learned about Google Cloud Cold Line Storage (AWS has an equivalent service) allows you to archive data at a very reasonable rate ($0.007/GB as of October 2016).
Google Cloud is a developer product, so it might be daunting for someone not familiar with it how to use it.
I will be writing these instructions with rather low specificity, without screenshots and exact names of buttons, since the Google Cloud interface is bound to change around over the years.
Creator of Fun Fun Function, a YouTube show about programming.
14 
1
14 
14 
1
Creator of Fun Fun Function, a YouTube show about programming.
"
https://medium.com/google-cloud/an-agile-architecture-for-analytics-and-ai-on-google-cloud-6415e692591f?source=search_post---------371,"There are currently no responses for this story.
Be the first to respond.
An agile architecture is one that gives you:
What does such an architecture look like on Google Cloud when it comes to Data Analytics and AI? It will use low-code and no-code services (pre-built connectors, automatic replication, ELT, AutoML) so that you get speed of development. For flexibility, the architecture will allow you to drop down to developer-friendly, powerful code (Apache Beam, SQL, TensorFlow) whenever needed. These will run on serverless infrastructure (Pub/Sub, Dataflow, BigQuery, Vertex AI) so that you get low-maintenance, autoscaling, and resiliency.
When it comes to architecture, choose no-code over low-code and low-code over writing custom code. Rather than writing ETL pipelines to transform the data you need before you land it into BigQuery, use pre-built connectors (in Data Fusion, Datastream, Data Transfer Service, Dataflow Templates, FiveTran, etc.) to directly land the raw data into BigQuery. Then, transform the data into the form you need using SQL views directly in the data warehouse. You will be a lot more agile if you choose an ELT approach over an ETL approach.
Another place is when you choose your ML modeling framework. Don’t start with custom TensorFlow models. Start with AutoML. That’s no-code. You can invoke AutoML directly from BigQuery, avoiding the need to build complex data and ML pipelines. If necessary, move on to pre-built models from TensorFlow Hub, HuggingFace, etc. and pre-built containers on Vertex AI. That’s low-code. Build your own custom ML models only as a last resort.
You will want to be able to drop down to code if the low-code approach is too restrictive. Fortunately, the no-code architecture above is a subset of this full architecture that gives you all the flexibility you need:
When the use case warrants it, you will have the full flexibility of Apache Beam, SQL, and TensorFlow. This is critical — for use cases where the ELT+AutoML approach is too restrictive, you have the ability to drop to a ETL/Dataflow + Keras/Vertex approach.
Best of all, the architecture is unified, so you are not maintaining two stacks. Because the first architecture is a subset of the second, so you can accomplish both easy and hard use cases in a unified way.
Enjoy!
Google Cloud community articles and blogs
66 
66 claps
66 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Data Analytics & AI @ Google Cloud
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/google-cloud-platform-technology-nuggest-august-1-15-2021-edition-9d053196e4ad?source=search_post---------372,"There are currently no responses for this story.
Be the first to respond.
Welcome to the August 1–15, 2021 edition of Google Cloud Platform- Technology Nuggets.
A friendly reminder that Registration for Google Cloud Next, happening on 12th-14th October, is open.
What does Application Modernization look like inside of Google? Check out this blog post, on how Google teams moved vendor managed applications, Confluence and Acronix from Compute Engine Virtual Machines to Anthos. The post highlights the pain points of running these applications on VMs and the benefit of moving them to Anthos.
We have published a number of self-service guides that help to migrate to Google Cloud. These guides are categorized into different workloads to migrate (Microsoft, VMware, SAP) to general migrations around storage and databases.
The recent GA release of our Database Management Service made it easier to migrate your SQL databases to Cloud SQL. But did you know that you could potentially use this service to even do a major version upgrade of your database? Check out this blog post.
If you are looking to deep dive into Big Query, there is a fantastic series titled “Big Query Admin Series” that has published multiple posts already starting with Resource Hierarchy to the latest post on Data Governance. Check out the deep dive articles published so far and bookmark this series for ready reference:
Speaking of Analytics, every organization wants to tap into data insights. While that is the goal that organizations want, there are still fundamental questions that arise when you build out an entire strategy for building out an analytics pipeline. Is it a Data Lake or a Data Warehouse? What tools should we be using? What if we want to model and run all stages of a typical data pipeline from Ingestion right to visualization and inference. We have written a paper that shows what it makes to build out a Unified Platform for Analytics. Check it out.
OWASP Top 10 is a list by the Open Web Application Security (OWASP) Foundation of the top 10 security risks that every application owner should be aware of. Check out this detailed guide from our Architecture section, that covers various Google Cloud Security Products and which specific OWASP Top 10 risk that it helps to mitigate.
This is a great way to understand not just the risk but to map it to the specific GCP security product. For example, Google Cloud Armor, our DDoS and WAF service, is shown below vis-a-vis the Top 10 risks and you can click on each of the risks (tick mark) to see what Google Cloud Armor does.
In another win for User Experience, we have integrated several in-context observability metrics right within the Compute Engine VM details page. This helps to troubleshoot and monitor key metrics while investigating the VM itself rather than switching context across multiple UIs and tools.
You can now click into any VM and get a rich set of visualizations acrossCPU, Disk, Memory, Networking, and live processes.
Are you looking to learn more about the GCP story around IOT and what services it provides to help you launch your next IoT product powered by GCP? Take a look at this technical note that helps you understand the core components behind IoT Core.
Finally, the first-ever Google Cloud Startup Summit will be taking place on September 9, 2021 and we encourage you to register for the same. Check out the published agenda for the event.
Have questions, comments, or other feedback. Do send it across.
Looking to keep a tab on new Google Cloud product announcements? We have a handy page that you should bookmark → What’s new with Google Cloud.
Google Cloud community articles and blogs
86 
86 claps
86 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/platformer-blog/kubernetes-sri-lanka-google-cloud-study-jam-4d7700e75bb4?source=search_post---------373,"There are currently no responses for this story.
Be the first to respond.
In this article I would like to talk about the Kubernetes Sri Lanka — Google Cloud Study Jam. It was a hands-on workshop which was organized by Kubernetes Sri Lanka…
"
https://itnext.io/using-environment-variables-with-google-cloud-functions-e9948f70f6cd?source=search_post---------374,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Google’s Cloud Functions have been around for a little while now. The product works well with other Google Cloud Platform offerings and provides a scalable, event-driven runtime for serverless applications or microservices. Developers can write functions using (a relatively old version of ¯\_(ツ)_/¯) Node.js (other language runtimes aren’t available at this point ¯\_(ツ)_/¯) and attach them to…
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://rominirani.com/google-cloud-functions-tutorial-deploying-from-a-source-repository-6cb0d4b81cd8?source=search_post---------375,"This is part of a Google Cloud Functions Tutorial Series. Check out the series for all the articles.
In the series so far, we have deployed our source code directly. What I mean by that is that we either had the source code present in a local folder and then we used gcloud to deploy the function or we used the Cloud Console Web UI to paste our source code directly. If you have taken a closer look, you would have noticed that one of the options available for deploying your Function is to use a Source Code Repository like Git to host your function code and deploy it directly from there.
The ability to host our Source code in a Git repository and use that as a basis to deploy our function code has multiple benefits. It fits with a typical development workflow that we use today. Additionally, you can maintain versions of your function code and also have automatic builds done for you via the source code checkin trigger. Let’s learn the mechanics of how to do this now.
First up, if you visit the Cloud Console Web UI for creating a Cloud Function, you can see the option for Source Code Repository as shown below (only relevant sections from the Create Function form ):
Did you know that each Google Cloud Platform project comes with a Git repository ? This is available in the Cloud Console main menu via the Source Repositories option shown below:
What we shall be doing is the following:
Let’s do it.
Google Cloud Source Repositories has been revamped significantly and a Beta version is available, at the time of writing this blog post. I shall use the Beta version and in case you see a message asking you to use the Beta version, please do so.
The Beta version available at https://source.cloud.google.com/ looks like this:
If you click on View All Repositories, you will see all your GCP Projects and the associated repositories for the same. We shall go ahead and create a new respository. To do that, click on the New repository button in the top right corner.
You have a couple of options as shown below and I am choosing the 2nd option since I would like to connect this repository to an already initialized and available repository that I have for this series at Github.
Click on Continue. This will bring up a screen below where you should select your existing GCP Project and choose the provider as Github. Give the appropriate consent and complete the formalities of connecting to Github.
Once you click on Connect to Github, you will need to authorize Google Cloud Platform to access your Github account and repositories that you have under it and select the particular repository on Github to connect to. Please do that. You should see a message similar to the following:
If all goes well, your Github repository will be available under Source Repositories as shown below:
We are able to see our README.md file and that is a good thing ! :-)
This completes our section of connecting an existing Github repository to Cloud Source Repositories. You can continue to use your standard Git workflow to checkin files into Github repository and the same will get mirrored into the Cloud Source Repositories. The synchronisation is done for you.
The important thing to note from the above screen is the repository name and the branch name. An example is shown below. This should also help you understand that a GCP Project can have one or more repositories (a default one and multiple others like these as shown below).
You can click on the Repository and Branch drop down arrows to view the details. What I particularly liked was that you could look at various branches, tags, commits and everything that has happened in Github directly from Cloud Source Repositories.
We are going to deploy a HTTP Trigger based function that is present in the helloworld-http folder of our repository as shown below:
Few things to note here:
Once ,we have this information, we can visit the GCP Console for New Function and provide the information as given above and click on Create Function. Your function should deploy successfully and ready to be invoked!
This is part of a Google Cloud Functions Tutorial Series. Check out the series for all the articles.
Technical Tutorials, APIs, Cloud, Books and more.
19 
1
19 claps
19 
1
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
Technical Tutorials, APIs, Cloud, Books and more.
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
Technical Tutorials, APIs, Cloud, Books and more.
"
https://medium.com/@imrenagi/writing-date-partitioned-files-into-google-cloud-storage-with-cloud-dataflow-50ee1d5c03ed?source=search_post---------376,"Sign in
There are currently no responses for this story.
Be the first to respond.
Imre Nagi
Jun 13, 2018·3 min read
Writing Date Partitioned Files Into Google Cloud Storage With Cloud Dataflow
If you read my previous article about Serverless Lambda Architecture With GCP, you will know that I was trying to store all events to google cloud storage by using dataflow. I intended to store it on date-partitioned basis, but it failed..
My goal was to group a collection of files into a date partitioned folder. What does it mean? It means, all streams arrived at a time should be stored in a folder named as yyyy/mm/ddof that time. For instance, If I would like to store a file namedworldcup.json collected on 25th May 2018 , to gs://imre-poc bucket, the file should end up stored on this path: gs://imre-poc/2018/05/25/worldcup.json
Here is the approached that I used with Dataflow API:
With the code above, I was trying to use .to() method of TextIO by parsing the predefined string as the parameter. Once the code run on the cloud, I can see the files immediately stored on gs://bucket/rsvp-stream/2018/05/25/file-xxxxxx.json . But, can spot the bug? If you realized, I use .to() method to set the prefix of the file, but this approach didnt work as expected when the day is changed. When the day changes into 26th May 2018, all of the streams were still written into the same bucket gs://bucket/rsvp-stream/2018/05/25/ .
So, I started digging about writing dynamic file name to dataflow and I found out that extending FileBasedSink.FileNamePolicy would solve my problem. Extending FileNamePolicy requires us to implement to methods: windowedFilename() and unwindowedFileName() . We only need to implement windowedFilename() and throw exception in another function because we can guarantee that the data stored to GCS are always bounded data (your use case might be different than mine!)
With the new implementation above, I have private function named filePrefixForWindow() which will construct a the folder path of the files every time they are written to GCS. Then, instead of passing the file name directly to .to function of TextIO, I’m trying to only pass the bucket name to .to() method and pass an instance of DatePartitionedNamePolicy into .withFilename() . Remember that, you also need to use .withWindowedWrites() to make sure that the writing will always be done in windowed time.
Finally as expected, now the result will be stored on different date partitioned folder in my GCS bucket.
I will be happy to share all of my knowledge related to Google Cloud Technology specifically for its Big Data Technology. Let me know if you are interested to learn more or just want to discuss something.
Google Developer Expert, Cloud Platform Engineer @gojek
141 
3
141 claps
141 
3
Google Developer Expert, Cloud Platform Engineer @gojek
About
Write
Help
Legal
Get the Medium app
"
https://blog.qtum.org/qtum-partners-with-google-cloud-to-launch-suite-of-developer-tools-91ef68090a4?source=search_post---------377,"Qtum has unveiled a full suite of blockchain developer tools in partnership with Google Cloud. These free-to-use tools, are designed to give developers and non-technical users alike, a simple and cost-effective way of launching nodes and building on the Qtum blockchain.
“Google Cloud is the perfect partner to help us make the blockchain ecosystem simpler and more intuitive. Where launching a node was once an intensive and complex process, Qtum’s new developer suite introduces helpful shortcuts and tools to make it faster and easier. With a more accessible technology, we hope to open up and expand the Qtum community to include people with a broader range of experience — from experts to the everyday user.” — said Miguel Palencia, Qtum CIO.
The suite of tools launched on Google Cloud are available through the Qtum compute engine. The compute engine lets anyone launch a full developer environment on Qtum and begin developing or staking in a matter of seconds, free of cost. Previously, developers had to source the necessary tools themselves to create a full Qtum node or decentralized applications (dApps). With Qtum, users now have access to the suite of tools necessary to build dApps, launch a full node, make a fork, or begin staking on the Qtum blockchain on Google Cloud. When the Qtum source code is updated, Google Cloud will automatically update the code everywhere, saving developers the need to manually re-download in order to remain on the latest version.
The Qtum developer toolkit includes Qtum Core, a Solidity Compiler, Qmix IDE, Solar (smart contracts deployment tool), Qt-dev libraries, and all other necessary libraries and tools to develop dApps.
To access the Qtum blockchain developer tool, visit this link.
Website: https://qtum.org/Telegram Group: t.me/qtumofficialTwitter: https://twitter.com/QtumOfficialFacebook: https://www.facebook.com/QtumOfficial/
Qtum — Defining the Blockchain Economy
242 
242 claps
242 
Written by

Qtum — Defining the Blockchain Economy
Written by

Qtum — Defining the Blockchain Economy
"
https://medium.com/javarevisited/5-best-gcp-associate-cloud-engineer-certification-courses-in-2021-c93d7e35228a?source=search_post---------378,"There are currently no responses for this story.
Be the first to respond.
Hello guys, If you are looking to pursue a career as a Google Cloud Engineer in 2021 or merely want to acquire a cloud certificate to add to your colorful resume, then taking the Google Cloud Engineer exam might be just the right thing for you.
Google Cloud Platform or GCP is one of the top 3 public cloud providers along with AWS and Microsoft Azure and the demand for certified Google cloud professionals is growing exponentaitonal because of increased adoption of the Google Cloud Platform.
If you already have a fair amount of experience, then you may want to go for the Professional Cloud exam. But if you are a newbie, or looking to add more certifications to add your CV then the Associate Cloud Engineer exam is the way to go.
This exam tests your ability to set up a cloud solution, plan, configure, deploy, and implement it. And it also checks the ability to configure access and security, ensuring the successful operation of a cloud solution with Google Cloud Platform services and technologies. The Google Cloud Engineer exam lasts exactly two hours and is available in Japanese, Spanish, Indonesian, and English. The registration fee is approximately 125 US dollars and can be taken remotely or in person at a test center. Apart from checking the syllabus and exam guide about Google Cloud Engineering, you should also join online training courses to prepare well before you take the exam. It is better to have more than six months of background knowledge and hands-on experience but that’s optional and not really a requirement for an exam. If you don’t have practical experience working with Google cloud you can still join these courses and practice in their hands-on lab. The Coursera GCP training course (second course in this list) has a partnership with Quiklabs for practicing on Google cloud and it just likes working in the real world. Google even has the Google Cloud Free Tier of few products for monthly use.
coursera.com
Without wasting any more of your time, here is the best course you can take to prepare well for the Google Cloud Professional Associate Cloud Engineer certification exam. These courses will help you to better prepare for different topics and keeping the exam pattern in mind.
This is one of the best courses to prepare for Google Cloud Associate engineer certification on Udemy. This course has almost 60,000 students enrolled and a rating of 4.2 out of 5.0 with an impressive success rate. It prepares students for the Cloud Engineer Certification by taking the basics of the Cloud and Google cloud platform. Because this course only covers the Associate Cloud Engineer exam, most of the sessions are focused on more theory and less practical application. Although most users are satisfied with the course, some students commented that most of the sections were repeated, making it bulkier than it would initially have been. However, most people have more positive than negative reviews, so we feel the course is worth checking out.
Here is the link to join this online course — Ultimate Google Certified Associate Cloud Engineer 2021
This is another awesome Udemy course for Google Cloud Assocaiten Engineer certification. The best thing about this course is that it's prepared by none other than Dan Sullivan, the guy who wrote the Official Certification Guide for Google.
This course is designed to help prepare you for the Google Associate Cloud Engineer exam by introducing all of the major GCP services covered on the test, including Compute Engine, App Engine, Kubernetes Engine, Cloud Functions, Cloud SQL, BigQuery, Bigtable, VPCs, VPNs, Stackdriver, and more.
In this course, you will learn how to:
The Google Associate Cloud Engineer exam is two hours in length and contains 50 multiple-choice and multiple-select questions. This course uses demonstrations as well as lectures to ensure you know how to work with GCP and understand its key design and operational concepts.
Here is the link to join this awesome course — Google Associate Cloud Engineer: Get Certified 2021
And, if you like to read the books and study guides then you can also check out the Official Google Cloud Certified Associate Cloud Engineer Study Guide from Amazon by Dan Sullivan.
www.amazon.com
This course is completely free and has more than 22,580 students already enrolled. It offers lessons on skills needed to perform a cloud engineering role and prepare for the Cloud Engineer certification.
It also teaches about the infrastructure and platform services provided by Google Cloud Platform with a basic and advanced understanding of the purpose and intent of the Associate Cloud Engineer certification and its relationship to other Google Cloud certifications.
Here is the link to join Course — Cloud Engineering with Google Cloud Professional Certificate
By the way, If you are planning to join multiple Coursera courses or specializations then consider taking a Coursera Plus subscription which provides you unlimited access to their most popular courses, specialization, professional certificate, and guided projects. It cost around $399/year but its completely worth your money as you get unlimited certificates.
coursera.com
The most amazing thing about this course that makes it stand out from other courses is that it’s offered by Google Cloud. This means that students will get a little bit of everything with enough material to pass the exam. This course takes approximately 7 days to take and guides students in familiarizing themselves by covering the structure and format of the examination, as well as its relationship to other Google Cloud certifications.
This course will make you a better Google Cloud Engineer, especially if it is your first time and you have no prior experience. However, students still need additional materials and study plans to ace the exams. Fortunately, this course offers these materials and platforms.
Here is the link to join this GCP course — Preparing for the Google Cloud Associate Cloud Engineer Exam
Btw, you would need a Pluralsight membership to access this course which costs around $29 per month or $299 per year(14% discount) but gives access to their 5000+ online courses on various cloud certifications, software development, coding, and other technical topics. I highly suggest programmers have Pluralsight membership because as a programmer and IT professional you need to constantly learn and update yourself. Even if you don’t have Pluralsight membership, you can check their 10-day free trial to access this course for free.
pluralsight.pxf.io
Apart from being one of the best sellers on Udemy with a 30-day money-back guarantee, this course also offers a hands-on experience with Google Cloud Platform (GCP) and aims at helping students pass the ACE exam and become a Google Certified Associate Cloud Engineer. It has about 25,289 students enrolled and does not require any background knowledge to take it. It also will give you all the basics you need to study other skills based on the Google Cloud Platform. So if you want to expand your skills, then this course is for you.
The course is also interactive and active so you must be willing to commit and learn. If you are very busy and cannot commit fully then you might want to skip this one.
Here is the link to join this Google cloud course — Google Certified Associate Cloud Engineer Certification
This course is a combination of four Google cloud certifications, namely Associate Cloud Engineer, Cloud Architect, Cloud Developer, and Network Engineer. It has a rating of 4.2 and about 66,000 students enrolled.
It covers beginner to advanced levels, but you don’t need any prior programming or coding experience to keep up. However, the most important thing to remember is that this course is intense. You can decide to take all four aspects of Google clouds or you can just go straight up to Google cloud engineer. It starts the course from scratch and will help you learn about Google Cloud Platform and Certification.
Here is the link to join this GCP course — Ultimate Google Cloud Certifications: All in one Bundle
This is another awesome interactive course to prepare and learn about Google Cloud Platform from Educative, a text-based, interactive online learning portal.
This course will not only help you to hone your cloud computing skills but also to ace your certification exam to help you stand apart.
Even if you’re an AWS user, you will find this course valuable and easy to pick up considering their similarities. All the topics and resources required to pass the Associate Cloud Developer certification are provided in the course, so you can think of it as a cheat sheet where all the information is in one place.
You’ll learn about the many services GCP offers and at the end, you will take a timed practice exam with 50 questions. This will give you the confidence you need to ace your exam.
Here is the link to join this course — Cracking the Google Cloud Associate Cloud Engineer Certification
And, if you find the Educative platform and their interactive courses useful in learning new tech or preparing for coding interviews then consider getting Educative Subscription which provides access to their 250+ courses for just $14.9 per month. It’s very cost-effective and great for preparing for coding interviews.
www.educative.io
This is another awesome resource for people preparing for Google Cloud Associate Cloud Engineer Exam. This practice test is prepared by Dan Sullivan the guy who wrote the official certification guide and author of the second course in this list.
Dan Sullivan is the author of the Official Study Guides for the Cloud Engineer, Architect, and Data Engineer exams and this practice test contains two tests with 50 questions to math the real exam format where you need to solve 50 questions in 2 hours.
You should take this practice test as part of your final preparation and build your speed and accuracy. You can also use this to gauge your preparation level and find your strong and week areas.
Here is the link to join this practice test — Google Cloud Associate Cloud Engineer Practice Exams
That’s all about the best courses to pass the Google Cloud Platform Associate Cloud Engineer certification exam. These courses cover most of the exam topics as given in the exam guide and good to learn both the GCP platform as well as to pass the certification. Whichever option you choose, committing to a course can give help you learn a new program or keep your Google Cloud skills sharp. Google even has the Google Cloud Free Tier of few products for monthly use.
Cloud computing technologies (like GCP) have become a fundamental requirement for most organizations as these technologies are relatively easy to use and maintain, they scale with your business, and they are affordable.
Cloud engineering is also an extremely hot market and is pretty much foundational to every industry. In order to be a cloud engineer, you have to distinguish yourself in some way. That’s where a cloud certification comes in and by cracking the Google Cloud Platform Associate Cloud Engineer exam, you will be one step ahead of your competition.   Other IT and Cloud Certification Articles you may like:
Thanks for reading this article so far. If you find these Google Cloud Associate Cloud Engineer certification courses useful then please share with your friends and colleagues. If you have any questions or feedback then please drop a note. P.S. — Apart from going through these GCP Associate Cloud engineer courses you can also check out the exam guide or take the practice exam that will help you in identifying and focusing on the main areas during your preparation.
udemy.com
Medium’s largest Java publication, followed by 14630+ programmers. Follow to join our community.
160 
160 claps
160 
Written by
I am Java programmer, blogger, working on Java, J2EE, UNIX, FIX Protocol. I share Java tips on http://javarevisited.blogspot.com and http://java67.com
A humble place to learn Java and Programming better.
Written by
I am Java programmer, blogger, working on Java, J2EE, UNIX, FIX Protocol. I share Java tips on http://javarevisited.blogspot.com and http://java67.com
A humble place to learn Java and Programming better.
"
https://medium.com/hackernoon/edge-tpu-and-cloud-iot-edge-at-google-cloud-next-e7d7d28ab7e6?source=search_post---------379,"There are currently no responses for this story.
Be the first to respond.
On stage today at Google Cloud Next, Antony Passemard, Head of Product Management, IoT, Google Cloud will debut Edge TPU and Cloud IoT Edge​:
Edge TPU ​is Google’s purpose-built ASIC chip designed to run TensorFlow Lite ML models at the edge. Google tells us these are high throughput systems built for something as demanding as inferring things from streaming video at your home (No, its not being used on Nest) to determine if actions are needed. While there is a SD card on the device so that you can store the data, we are told that this isn’t meant for training your ML models as much as making inferences based on them.
Cloud IoT Edge ​seems to be Google’s data processing machine learning heavy competitor to Amazon Web Services Lambda at edge, where lambda at edge is built to make decisions on the Cloudfront level with very quick speed, Cloud Iot Edge seems to be built to influence gateways, cameras, and end devices, this service probably built first due to Google’s work inside the device space being much larger.
By running on-device machine learning models, Cloud IoT Edge with Edge TPU provides a internet free inference, which as any computer scientist will tell you means it will be significantly faster than general-purpose IoT gateways. Even better is by not sending the data out over the internet you don’t have to give state actors access to a live video stream of your home, which is nice.
The Edge TPU development kit:
If you want one of these in early access, no need to dig further, sign up via this form.
This kit includes a system on module (SOM) that combines Google’s Edge TPU, a NXP CPU, Wi-Fi, and Microchip’s secure element in a compact form factor. It’ll will be available to developers this October. Do of course let us know what you all are able to build with this, as we all love seeing new hardware!
The Edge TPU development kit — SOM (above) and base board (below) (photo google)
#BlackLivesMatter
23 
23 claps
23 
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
Editor Hackernoon, FestivalPeak, and Keeping Stock
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
"
https://medium.com/javarevisited/my-favorite-free-google-cloud-platform-gcp-professional-cloud-developer-certification-courses-856ef69a56bb?source=search_post---------380,"There are currently no responses for this story.
Be the first to respond.
Hello guys, if you are preparing for Google Cloud Professional Cloud Developer certification in 2021 and looking for free online Google cloud courses then you have come to the right place.
In the past, I have shared the best courses to learn Google Cloud Platform as well as certification courses to pass cloud engineer, data engineer, and cloud architect certifications, and today, I am going to share free GCP Cloud Developer certification courses for both beginners and experienced cloud professionals.
This is one of the most difficult and prestigious exams, similar ot the AWS Developer Associate and Azure Developer (AZ-200) exam, once you pass this exam, you will have sufficient knowledge and skills to propose Google cloud-based solution, which is a very in-demand skill.
Being a Certified Cloud Developer could help you get more success in your professional career. In this era, basic certificates won’t work; you need to have a certificate of expertise. These days, companies themselves offer certificates to the candidates, but students have to clear their respective exams in return.
As I said, In this guide, I’ll share five free courses to pass the GCP Professional Cloud Developer Exam. You might find some courses that are only available on premium platforms, but they are included because such platforms offer a trial period. After research, I concluded that the Trial Period is more than enough to complete those courses.
Btw, If you need more comprehensive and focused certification courses then I also highly recommend you to join Ultimate Google Cloud Certifications: All in one Bundle (4) course on Udemy.
udemy.com
This is not free but the most comprehensive online courses for Google cloud certifications and provide study material for all four Google Cloud certifications like — Associate Cloud Engineer, Cloud Architect, Cloud Developer, Network Engineer. It also contains 500+ practice questions to hone your speed and accuracy skills.
Without wasting any more of your time, here is the list of best free online courses to prepare for the Google Cloud Professional Developer Associate exam.
These online training courses have been created by experts and made free for educational purposes. Thousands of developers shave already joined these free Google cloud courses and you can do the same too and pass this prestigious Cloud certification.
This is another free Udemy course you can use to prepare for the Google Cloud Developer Certification exam. This course will also help you are planning to build or change your career to GCP.
This is an introductory course to get started on the Google Cloud Platform (GCP). During this course, you will learn about what is Cloud Computing, what are different Cloud computing models, Important GCP services, and hands o
You can also use these to prepare for other highly valued Google Certifications like “Google Associate Cloud Engineer”, “Professional Cloud Architect” etc.
In short, a great free online course for beginner Google Cloud learners who are curious to learn about GCP, planning to get started with Google Cloud Certification.
Here is the link to join this free GCP course — Google Cloud Fundamentals 101: A quick guide to learn GCP
The next three courses on this list are interlinked with each other. If you don’t want to learn from the above two-course, you can enroll in these three courses.
These courses are designed for absolute beginners, and if you want to create a strong foundation in cloud computing, you can enroll in these courses.
This first course is created by Xavier Corbett, and so far, over 2 lakh students have enrolled in it. This course only talks about the basics of Cloud Computing and covers the following topics:
As mentioned above, throughout the course, only basics are covered, and the instructor has covered every aspect of cloud computing in detail and explained everything using graphics and proper illustrations.
If you are just starting to learn about cloud computing, this course will be a perfect start. More than 200K students have already joined this course.
Here is the link to join this course — Introduction to Cloud Computing
udemy.com
Once you have completed the basic course about cloud computing, you will understand How cloud computing can be used and what GCP offers. Now you are ready enough to start your platform-specific learning.
This course will teach you about various GCP concepts that will help you pass the GCP Professional Cloud Developer Exam with flying colors.
This course is created by Dhanaji Musale and Google Cloud Platform Gurus!, who has authored some of the most popular and comprehensive courses on the Google Cloud platform like Ultimate Google Certified Professional Cloud Developer 2021.
And so far, over 40 thousand students have enrolled in this course. Its a 6-hour long video course that will cover the following topics:
Apart from this, the instructors will also share a small trick through which you can get a reward of $300 in your Google Cloud Platform. Through this reward, you can practice. Throughout the course, graphics and visuals are widely used, and understanding the concepts is quite easy from the videos.
Here is the link to this free Google Cloud course — Google Cloud Platform Concepts
udemy.com
This course is available on CloudAcademy.com. It is among the newest and top-rated courses available on the internet for GCP Professional Cloud Developer Exam. So far, over 187 students have enrolled in this course.
In this course, there is a total of 13 videos, 12 Assignments. This is an 18-hour long video course that will teach you every bit of the GCP platform. One of the most exciting things about this course is that every topic is taught by experts.
For instance, In this course, a total of 5 instructors will deliver the lectures. Apart from this, at the end of the course, a sample exam is created for you. You can test your knowledge through this exam.
This course also includes three labs where you need to perform practicals like creating a Linux Virtual Machine on GCP, etc.
If you are looking for a course that can teach theory and practicals, this course is the best pick for you.
The above course is of 6 hours, but if you are running short on time and need a quick revision sort of course, then this one is for you. In this course, Linux Academy has covered major topics in-depth and briefly talked about less important topics.
It’s a 1-hour long video course, and over 17 thousand students have enrolled in it. Based on 621 Students’ reviews, this course has a rating of 4.4 out of 5 on Udemy.
This course is full of visuals and graphics, and that’s the reason why instructors have covered a wide range of topics in a limited period.
Graphics and visuals made the learning process quite easy and quick. If you are looking for a short course that can give you a brief introduction to GCP, this course is a perfect pick for you. It will deliver the best possible information in an hour.
Here is the link to join this free GCP course — Google Cloud Concepts
This is a meta course on Google Cloud Professional certification as it will provide you strategies and Tips for cracking Google Cloud Certifications in 2021.
The USP of this course is that it's prepared by Dan Sullivan, a cloud architect, systems developer, and author of the Official Google Cloud Professional Data Engineer Study Guide. He is an experienced trainer and his online training courses have been viewed over 1 million times.
The course begins with a review of Google Cloud certification topics followed by a detailed discussion about the structure of certification exams. After that, you explore the limitations of certification exams and how someone can fail an exam even if they are knowledgeable about the topic.
The final three lectures of the course focus on the Associate Cloud Engineer, Professional Data Engineer, and Professional Architect exams. In each lecture, you will analyze example questions and consider strategies for identifying key pieces of information and tips on eliminating incorrect options. Overall, a great free course for anyone preparing for Google cloud platform certification exams.
here is the link to join this course — How to Pass Google Cloud Certification Exams
This a Udemy Course with a rating of 4.3 out of 5. This is a detailed course on the GCP Professional Cloud Developer exam. It is created by Google Cloud Platform Gurus!, in every video of this course, graphics are used quite smartly to ease up the understanding process for candidates.
In this 16-hour long video course, every aspect of GCP is explained in detail. For instance, the following topics are covered in this course:
So far, over 72 thousand students have enrolled in this course, and if you are looking for a course that covers every aspect of GCP even beyond the course of the GCP Professional Cloud developer exam, this course is for you.
Here is the link to join this free course — Ultimate Google Certified Professional Cloud Developer 2021
That’s all about the free online courses to prepare for the Google Cloud Professional Cloud Developer certification exam. In this guide, I have shared 5 Free Courses that will help you prepare for the GCP Professional Cloud Developer exam.
If you think that I have missed a worthy course, then do let me know. I’ll check it out, and if it meets will meet my criteria, then no doubt, you’ll find it here in this list.
Other IT and Cloud Certification Articles you may like:
Thanks for reading this article so far. If you find these Google Cloud Platform Professional Cloud Developer certification courses useful then please share them with your friends and colleagues. If you have any questions or feedback then please drop a note.
P. S. — If you need more comprehensive and focuses certification courses then I also highly recommend you to join Ultimate Google Cloud Certifications: All in one Bundle (4) course on Udemy. This is not free but the most comprehensive online courses for Google cloud certifications and provide study material for all four Google Cloud certifications including Cloud Developer.
udemy.com
Medium’s largest Java publication, followed by 14630+ programmers. Follow to join our community.
220 
220 claps
220 
A humble place to learn Java and Programming better.
Written by
I am Java programmer, blogger, working on Java, J2EE, UNIX, FIX Protocol. I share Java tips on http://javarevisited.blogspot.com and http://java67.com
A humble place to learn Java and Programming better.
"
https://medium.com/google-cloud/building-a-slack-reminder-app-with-google-cloud-functions-and-google-cloud-scheduler-4046f4c9c19?source=search_post---------381,"There are currently no responses for this story.
Be the first to respond.
Google Cloud Platform provides awesome tools to help engineers perform automation with ease.
In this article, we will build and deploy a serverless application that sends messages to Slack by leveraging on Google Cloud Function. We will also use Google Cloud Scheduler to periodically run our application at an interval of 3hours.
Google Cloud Functions is a lightweight compute solution for developers to create single-purpose, stand-alone functions that respond to cloud events without the need to manage a server or runtime environment.
Google Cloud Functions can be written in Node.js, Python, and Go, and are executed in language-specific runtimes.
Cloud Functions can be associated with a specific trigger. The trigger type determines how and when your function executes. Cloud Functions supports the following native trigger mechanisms:
Google Stackdriver provides a suite of monitoring tools that help you understand what’s going on in your Cloud Functions. Logs for Cloud Functions are viewable in the Stackdriver Logging UI.
Cloud Function can be used for multiple cases such as Serverless application backends, Real-time data processing systems or Artificial Intelligence applications.
Google Cloud Scheduler is a fully managed, scalable, and fault-tolerant cron job scheduler that allows engineers to automate all their scheduled tasks in one place.
Google Cloud Scheduler allows you set up fully managed scheduled units of work to be executed at defined times or regular intervals with support for Unix cron format.
Cloud Scheduler can be associated with a target which can be either of the following:
In addition, Stackdriver integrates with Cloud Scheduler providing powerful logging for greater transparency into job execution and performance.
Cloud Scheduler can be used for multiple cases such as sending out a report email on a daily basis, updating some cached data every 10 minutes, or making requests to an endpoint.
Sign in to your Slack workspace and Create a new Slack app as follows:
Visit Cloud Functions and Create Function
Paste the following code snippet into the Inline editor and replace the value of url to your Webhook from Slack and Deploy your function.
Once done, you can visit the URL on your Cloud Function page to test.
Visit (Cloud Scheduler)[console.cloud.google.com/cloudscheduler] and Create Job
Great! Your should get the following message from the RestReminder Bot on your Slack Channel every 3 hours.
Thanks for reading through! Let me know if I missed any step, if something didn’t work out quite right for you or if this guide was helpful.
Google Cloud community articles and blogs
94 
94 claps
94 
Written by
Software / DevOps Engineer | Google Developer Expert for Cloud | https://timtech4u.dev
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Software / DevOps Engineer | Google Developer Expert for Cloud | https://timtech4u.dev
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/econ%E8%A8%98%E8%80%85/google-cloud%E8%88%87salesforce%E9%81%94%E6%88%90%E7%AB%8B%E5%90%88%E4%BD%9C%E5%8D%94%E8%AD%B0-19ab7349bb35?source=search_post---------382,"There are currently no responses for this story.
Be the first to respond.
Salesforce公佈，該公司正式與Google Cloud達成協議，Google Cloud將會成為Amazon Web Service以外，另一個Salesforce客戶可選用的雲端運算平台。現時三大雲端運算平台Amazon Web Service、Google Cloud及Microsoft Azure，有兩間都與Salesforce 正式合作。
作為合作協議的一部份，Salesforce亦會向用戶提供一年免費的Google網上企業服務 — -G Suit。
Source: Google and Salesforce Ink Cloud, Apps Deal
Originally published at 經濟新聞記事錄.
Econ記者嘅環球經濟記錄
101 
101 claps
101 
Econ記者嘅環球經濟記錄
Written by
專注寫經濟學同國際經濟嘅獨立記者 | 我個網站係 http://econreporter.com
Econ記者嘅環球經濟記錄
"
https://rominirani.com/google-cloud-functions-tutorial-using-environment-variables-20b4f0f82aa0?source=search_post---------383,"This is part of a Google Cloud Functions Tutorial Series. Check out the series for all the articles.
At the time of writing this article, support for Environment variables in Google Cloud Functions is in Beta.
As you start work your way through Cloud Functions, one of things that you will notice as you write them is that you often end up accessing other systems or define some constants, etc. You might want to connect to an external system for which you need to have a hostname, service name, etc.
The questions to ask are:
The answer to all of the above questions is yes ! Enter Environment Variables in Cloud Functions.
Let us take a look at the existing environment variables that are already present. This is not something you want to do too much about but it comes in handy to understand which of these are reserved environment variables and used in some way or the other by Google Cloud Functions runtime.
If you deploy the above Cloud Function, you should get an output that looks like this. It gives you an interesting amount of information.
To associate Environment Variables with your Cloud Function, you can do it in multiple ways:
In this tutorial, we will use the GCP Console, but if you would like to do the same via the gcloud functions command, you can read up the documentation here: Using environment variables.
Let us go ahead and create a Google Cloud Function via the GCP Console. I am assuming that you know that by now via this series and are able to bring up the Create Function form on your own.
If you scroll right down, you will find a More link. Click that and the Environment Variables section should show up as shown below:
Click on the Add variable button to add variables as you want. We will define four variables as shown below:
To use any specific environment variable value in your Google Cloud Function, you will have to use the process.env statement as shown below:
If you deploy and run this function, it will send back the following response text:
Using Environment Variables in Google Cloud Functions is straight forward but there are a few things to keep in mind:
4. The scope of the Environment variables is limited to your specific Cloud function. You cannot share it across multiple functions.
5. Environment variables are not the place to put sensitive data. So do not keep credentials (usernames, passwords, keys) or anything that you do not want others to get a hold off. Take a look at this article by Seth Vargo (Twitter: @sethvargo) on managing secrets in serverless.
www.sethvargo.com
Here is an excellent article on using process.env and specifically I would recommend the section “When to use it” to understand common use cases that are a good fit for Environment variables.
codeburst.io
6. In case you are providing Environment variables to your Cloud Functions via a configuration file at the time of gcloud functions deploy , do remember to add that file potentially to your .cloudignore and .gitignore files.
Official Documentation on Environment Variables → https://cloud.google.com/functions/docs/env-var#using_environment_variables
Technical Tutorials, APIs, Cloud, Books and more.
104 
1
104 claps
104 
1
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
Technical Tutorials, APIs, Cloud, Books and more.
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
Technical Tutorials, APIs, Cloud, Books and more.
"
https://medium.com/@jaychapel/5-free-google-cloud-training-resources-9410766667bc?source=search_post---------384,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Apr 17, 2019·4 min read
If you’re looking to break into the cloud computing space, there’s an abundance of resources out there, including free Google Cloud training. If you know where to look, open source learning is a great way to get familiar with different cloud service providers. Combined with our previous blog posts on free training resources for AWS and Azure, you’ll be well on your way to expanding your cloud expertise and finding your own niche. No matter where you are in the learning process, there’s a training resource for every experience level and learning type — get started now with our list of 5 free Google Cloud training resources:
For free, hands-on training there’s no better place to start than with Google Cloud Platform itself. GCP’s free tier option is a no-brainer thanks to its offerings:
And for help with navigating the platform as you use it, check out GCP’s documentation for a full overview, comparisons, tutorials, and more.
On the Google Cloud training page, you’ll find plenty of classes to get technical skills and learn best practices for using the platform. Among those options, they have also teamed up with Coursera, an online learning platform founded by Stanford professors, to offer courses online so you can “skill up from anywhere.”
Coursera includes a number of free courses, and until 1/1/19, you can sign up and get your first month free on any select Google Cloud Specialization. Courses include topics in Machine Learning, Architecting, Data Engineering, Developing Applications, and the list goes on.
In conjunction with Coursera, Google Cloud offers hands-on training with specialized labs available via Qwiklabs, a learning lab environment for developers. Choose a “quest” from their catalog to get started with 50+ hands-on labs from beginner to expert level, where you’ll learn new skills in a GCP environment and earn cloud badges along the way. Get started with GCP Essentials and work your way into more advanced, niche topics like Managing Cloud Infrastructure with Terraform, Machine Learning APIs, IoT in Google Cloud, and so on.
You can’t go wrong with YouTube. An endless amount of free videos offers an abundance of Google Cloud training for those of you who prefer to watch the movie instead of reading the book (you know who you are). Some of the most popular YouTube channels for free Google Cloud training include:
While other resources keep you learning with hands-on training, tutorials, and certification prep, blogs keep your mind flowing with new insights, ideas, and the latest on all things cloud computing. Google Cloud and Qwiklab have blogs of their own, perfect for supplemented reading with their trainings. But for a more well-rounded blog with content on other service providers, check out Cloud Academy. We also cover Google Cloud on the ParkMyCloud blog — check out this guide to Google Cloud machine types, an explanation of sustained use discounts, and introduction to resource-based pricing. And be sure to subscribe to relevant discussion forums such as r/googlecloud on Reddit and the GCP Slack.
As it becomes clear that cloud computing is here to stay, free training resources only continue to emerge. We picked the 5 above for their reliability, variety, quality, and range of information. Whether you’re new to Google Cloud or consider yourself an expert, these resources will expand your knowledge and keep you up to date with what’s latest in the platform.
More Free Training Resources:
Originally published at www.parkmycloud.com on October 23, 2018.
CEO of ParkMyCloud
See all (317)
32 
32 claps
32 
CEO of ParkMyCloud
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/serverlessguru/companies-moving-from-aws-and-azure-to-google-cloud-platform-55fe74f54bd2?source=search_post---------385,"There are currently no responses for this story.
Be the first to respond.
Amazon Web Services (AWS) is the oldest and most mature provider of cloud computing, offering the most services. However, Google Cloud Platform (GCP) and Microsoft Azure are innovating at a quick pace, and all three of these industry leaders have their pros and cons.
"
https://rominirani.com/tutorial-google-cloud-natural-language-api-910740100378?source=search_post---------386,"Google recently released the Google Cloud Natural Language API that provides powerful machine learning APIs that they have built up over the years.
This API allows you to do several things and which includes:
All the above goodness is made available via a REST API and supports 3 languages at this point : English, Japanese and Spanish.
I wrote up a tutorial on ProgrammableWeb that gives you all the details to get started with the API. This includes:
www.programmableweb.com
Take a look at the tutorial and let me know if you have any feedback/questions.
Technical Tutorials, APIs, Cloud, Books and more.
78 
4
78 claps
78 
4
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
Technical Tutorials, APIs, Cloud, Books and more.
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
Technical Tutorials, APIs, Cloud, Books and more.
"
https://medium.com/google-cloud/running-node-red-on-google-cloud-platform-under-docker-3d4185e97f28?source=search_post---------387,"There are currently no responses for this story.
Be the first to respond.
Node-RED is very interesting. The Node-RED documentation does not include instructions for running Node-RED on Google Cloud Platform (GCP). If you haven’t used GCP, there are free options.
There have been (many) changes to GCP since I wrote this post almost 2 years ago. In response to Shiv Kodam, here’s an update that will help you get Node-RED running on (a) Container-Optimized OS image (container-vm is being deprecated); (b) Kubernetes Engine.
Container-Optimized OS uses cloud-init for configuration. Here’s a tweaked cloud-init file that runs a Node-RED container:
Create the Container-Optimized OS VM using the above cloud-init file:
NB This is a cheap-cheap VM f1-micro and it’s preemptible. If you want more horse-power, choose a more powerful VM.
Give the image a short amount of time to stabilize, download Node-RED and run the container. You can either ssh into the VM and then run the — command, or — as here — run them together:
You want to see something similar to the following for the successful start of the node-red-docker container:
Once the container is running, you can curl the Node-RED endpoint remotely:
And you should see output of the form:
If you’d prefer not to create a firewall rule, you may use gcloud to create an ssh port-forward from the VM’s port 1880 to your local machine’s 1880:
And then, from your browser:
When you’re done, don’t forget to blat the VM:
Deploying Node-RED as a single Pod|Service to Kubernetes (Engine) is straightforward. Assuming you have a cluster that you’re authenticated against:
Then you can use this shortcut to grab one the of Kubernetes’ Nodes, determine the Node-Red Service’s NodePort (in the Console screenshot above you can see this is31156 in my case) and then using gcloud to port-forward to this NodePort on this Node:
Then you can assess Node-RED as before *but* using the NodePort (${PORT} not 1880):
I was using a Chromebook to revise this post yesterday. It continues to be possible to run the port-forward when using a Chromebook thanks to the excellent Cloud Shell.
Using Cloud Shell, you may run the commands as above. The one difference is that Cloud Shell has a constrained set of ports (8080–8084). So, when it comes to the gcloud port-forward, please use one of these values instead of 1880, let’s use 8083 for argument’s sake:
Then — if not already changed — click “Change port”:
And set it to 8083:
And, you should see the Node-Red console as before.
That’s all!
It’s very easy to run Node-RED on a GCP.
If you’ve not used Google Cloud Platform before, start here. Otherwise, assuming you have a project [[PROJECT-ID]] and have the Cloud SDK installed, let’s start by defining some environment variables:
To make things simplest, we’re going to run Node-RED in Docker on the VM. We provide a configuration file to GCP with the command to create the VM.
Create a file called whatever you’ve decided for [[CONTAINER_FILE]] and use the following text for its content. This tells GCP where to find the Docker image for Node-RED and to run it on port 1880:
Now, to create a VM running Docker, with Node-RED installed and configured using the [[CONTAINER_FILE]], use the following command:
When the command completes and the VM is created, it will summarize the VM details including the EXTERNAL_IP address. Keep a record of this EXTERNAL_IP address as it is the IP address you will use to browse to Node-RED. You may also find the EXTERNAL_IP for your NODE with this command:
In order to access the Node-RED VM from other machines, you must open the firewall for port 1880. Use this command:
That’s it!
You should now be able to visit Node-RED using the following URL. Replace EXTERNAL_IP with the IP address that you determined previously:
And, then with the “first flow” deployed:
To delete the VM and delete the firewall, use the following commands:
It’s very easy to run Node-RED in a container on a VM in Google Cloud Platform. Have fun!
Google Cloud community articles and blogs
66 
4
No rights reserved
 by the author.
66 claps
66 
4
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/@jwlee98/gcp-dns-%EB%B6%80%ED%84%B0-%ED%95%98%EB%82%98%EC%94%A9-%ED%95%B4%EB%B3%B4%EB%8A%94-google-cloud-%EB%A1%9C-%EC%84%9C%EB%B9%84%EC%8A%A4-%ED%95%B4%EB%B3%B4%EA%B8%B0-1%ED%83%84-cloud-dns-a77acc9350cd?source=search_post---------388,"Sign in
There are currently no responses for this story.
Be the first to respond.
이정운 (Jungwoon Lee)
Jul 28, 2019·7 min read
안녕하세요 이정운 입니다.
그동안은 Google cloud 의 조금 전문적인 기술 위주로 공부하고 해당부분을 정리해서 이야기로 다뤘는데, 처음 접근하시는 분들에게는 조금 어렵게 다가갈 수 있을 것이라는 생각에 다시 한번 기초 부분을 공부하면서 이를 이야기로 정리해보면 어떨까하는 생각으로 새로운 시리즈를 시작하려고 합니다. 그래서 하나의 간단한 운영 서비스를 Google cloud 로 적용한다는 가정에서 어떤 부분을 살펴보고 고려하면 좋을지 테스트 하는 형태로 몇가지 이야기를 연결하여 진행하도록 하겠습니다.
기초에도 여러가지가 있지만 이번 시간에는 서비스의 가장 맨처음에 필요한 도메인과 DNS 관련된 부분으로 시작을 할려고 합니다. 특히나 Google cloud 서비스 중에 유일하게 SLA 100% 를 자랑하는 Cloud DNS 를 활용해서 첫 시작을 해보려고 합니다.
Cloud DNS 는 Google 과 동일한 인프라에서 실행되어 확장성과 안정성이 높은 관리형 DNS 서비스입니다. 지연 시간이 짧고 가용성이 높으며, 사용자에게 경제적으로 서비스를 제공할 수 있는 장점이 있습니다.
https://cloud.google.com/dns/?hl=ko
그럼 지금부터 하나씩 하나씩 실제 테스트를 해보면서 살펴볼까요?
#1) Cloud DNS 설정 하기 — 네임서버 변경
우선 해당 이야기를 진행하기 전에 도메인(예:freejava.co.kr 같은)이 필요한데 여기서 도메인을 직접 구매하는 것을 다루지는 않을것이고 도메인 구매 업체를(godaddy 나 cafe24 등) 통해서 이미 도메인을 구매해서 보유하고 있다는 가정을 가지고 이야기를 진행하도록 하겠습니다.
처음으로 Cloud DNS 설정을 하려면 GCP 관리 콘솔에서 “네트워크 서비스 > Cloud DNS” 를 클릭하여 해당 메뉴로 들어갑니다. 그리고 DNS 영역 만들기를 클릭합니다.
DNS 영역은 동일한 도메인 이름(예: freejava.co.kr)을 공유하는 모든 DNS 레코드의 컨테이너로서 네임서버 집합을 가지고 있습니다. (상세 내용은 링크 참고-https://cloud.google.com/dns/zones/?hl=ko)
보유하고 있는 도메인 이름을 넣어주고 DNSSEC 사용 여부를 결정한후 만들기를 하면 하단과 같이 DNS 영역이 바로 만들어지고 Cloud DNS 를 위한 네임서버 주소를 확인 가능합니다. 참고로 네임서버란 실제로 freejava.co.kr 이라는 도메인이 들어왔을때 124.1.1.1 이라는 IP 로 변경해주는 역할을 수행하며 이 네임서버 주소는 기존 도메인 구입 업체를 통해서 변경해야 할 주소이기 때문에 따로 기억해두거나 메모해 둡니다.
다음으로 기존 도메인 구입 업체를 통해서 네임서버 변경 작업을 수행합니다. 이 작업은 업체마다 다를듯 한데 Cafe24 의 경우에는 네임서버 변경 메뉴가 있어서 하단과 같이 사용자가 쉽게 변경을 할 수 있습니다.
참고적으로 네임서버 변경에는 일반적으로 24~48 시간 정도가 소요됩니다. 특히나, 운영환경이라면 ISP Cache 가 반영되는거 까지 고려해야 되는데 넉넉잡고 1주일 정도 소요되며 이때는 만약을 위해서 기존 서버를 유지하는 형태의 접근을 권장드립니다.
Cloud DNS로 이전https://cloud.google.com/dns/docs/migrating?hl=ko
생성한 DNS 영역의 정보는 관리 콘솔 뿐만 아니라 하단과 같이 gcloud 명령을 통해서도 확인이 가능합니다.
Cloud DNS 의 네임서버에 해당 도메인이 적용되었는지 확인하기 위해서는 하단과 같은 명령을 통해서 확인 가능합니다.
또는, 하단과 같은 명령을 통해서도 해당 도메인이 어느 네임서버를 호출하는지 알 수 있습니다.
위와 같은 명령들을 통해서 정상적으로 네임서버 변경이 이루어지고 Cloud DNS 를 호출되는지 확인할 수 있습니다.
#2) Cloud DNS 에서 레코드 추가하기
네임서버가 연동이 된 후 Cloud DNS 를 이용해서 실제 서비스를 하려면 DNS 영역에 이제 레코드를 등록해야 합니다. 이때 사용할 수 있는 DNS 레코드의 종류와 설명은 하단의 링크를 참고하시기 바라겠습니다.
https://cloud.google.com/dns/records/?hl=en
우선 IP 를 직접 매핑하기 위한 A Record 를 추가해보도록 하겠습니다. 레코드 모임 만들기를 클릭해서 리소스 기록 유형을 A 로 하고 TTL 시간 설정 확인후에 실제 해당 DNS 에 매핑될 IPv4 주소를 하단과 같이 입력합니다. (해당 IP 는 서비스 체크용으로 간단하게 nginx 가 설치된 GCE 의 외부 IP 를 사용 했습니다.)
그리고 일반적으로 도메인 주소 앞에 기본적으로 www 가 사용되므로 www 값으로 alias 를 추가할 수 있는 CNAME 을 하나 더 만듭니다. CNAME 은 결국 alias 이기 때문에 www.freejava.co.kr 로 DNS 요청이 들어오면 freejava.co.kr 로 들어온 것과 동일하게 취급하게 됩니다.
CNAME
위와 같은 작업을 하시게 되면 대략 하단과 같이 작업된 것을 확인할 수 있습니다.
A Record 추가가 제대로 되어 있는지 nslookup 명령을 통해서 해당 도메인을 호출해서 지정된 IP 가 나오는지 확인합니다. (반영에 약간의 시간이 소요되기 때문이고 저는 5분정도 걸리네요.)
정상적으로 IP 반환이 확인되면 이제 실제로 브라우저를 이용해서 테스트 해보시고 하단과 같이 결과가 나오는 것을 확인할 수 있습니다.
여기까지 잘 따라 오셨다면 간단하게 미리 구입한 도메인을 가지고 Cloud DNS 에 도메인 연동 및 설정, 서비스 하는 것을 테스트 해보신 것입니다. 보시면 아시겠지만 아주 간단하고 쉽게 해당 설정이 가능하며 기언급 한것처럼 무려 100% 의 SLA 를 보장한다고 합니다. 그럼 이번 이야기는 여기서 마무리 하고 다음 이야기에서 뵙겠습니다. 휘리릭~~~
Disclaimer: 본 글의 작성자는 Google 직원이지만 Google cloud 를 공부하는 한 개인으로서 작성된 글입니다. 본 글의 내용, 입장은 Google 을 대변하지 않으며 Google 이 해당 콘텐츠를 보장하지 않습니다.
참고 자료 #1
Cloud DNShttps://cloud.google.com/dns/?hl=ko
Quickstarthttps://cloud.google.com/dns/docs/quickstart
Managing Recordshttps://cloud.google.com/dns/records/?hl=en
2.(입문서)Google Cloud DNShttps://brunch.co.kr/@topasvga/169
Technical engineer who dreams better future. (These thoughts are my own personal opinions, and do not reflect or represent Google’s opinions or plans.)
33 
4
33 
33 
4
Technical engineer who dreams better future. (These thoughts are my own personal opinions, and do not reflect or represent Google’s opinions or plans.)
"
https://medium.com/@jaychapel/google-cloud-platform-vs-aws-is-the-answer-obvious-maybe-not-c85623f7d86e?source=search_post---------389,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Dec 12, 2019·7 min read
Google Cloud Platform vs AWS: what’s the deal? A while back, we also asked the same question about Azure vs AWS. After the release of the latest earnings reports a few weeks ago from AWS, Azure, and GCP, it’s clear that Microsoft is continuing to see growth, Amazon is maintaining a steady lead, and Google is stepping in. Now that Google Cloud Platform has solidly secured a spot among the “big three” cloud providers, we think it’s time to take a closer look and see how the underdog matches up to the rest of the competition.
As they’ve been known to do, Amazon, Google, and Microsoft all released their recent quarterly earnings around the same time the same day. At first glance, the headlines tell it all:
The obvious conclusion is that AWS continues to dominate in the cloud war. With all major cloud providers reporting earnings around the same time, we have an ideal opportunity to examine the numbers and determine if there’s more to the story. Here’s what the quarterly earning reports tell us:
You can see here that while Google is the smallest out of the “big three” providers, they have shown the most growth — from Q1 2018 to Q1 2019, Google Cloud has seen growth of 83%. While they still have a ways to go before surpassing AWS and Microsoft, they are moving quickly in the right direction as Canalys reported they were the fasted growing cloud-infrastructure vendor in the last year.
It’s also important to note that Google is just getting started. Also making headlines was an increase in new hires, adding 6,450 in the last quarter, and most of them going to positions in their cloud sector. Google’s headcount now stands at over 114,000 employees in total.
The Obvious: Google is not surpassing AWS
When it comes to Google Cloud Platform vs AWS, we have a clear winner. Amazon continues to have the advantage as the biggest and most successful cloud provider in the market. While AWS is growing at a smaller rate now than both Google Cloud and Azure, Amazon still holds the largest market share of all three. AWS is the clear competitor to beat as they are the first and most successful cloud provider to date, with the widest range of services, and a strong familiarity among developers.
The Less Obvious: Google is actually gaining more ground
While it’s easy to write off Google Cloud Platform, AWS is not untouchable. AWS has already solidified itself in the cloud market, but with the new features and partnerships, Google Cloud is proving to be a force to be reckoned with.
We know that AWS is at the forefront of cloud providers today, but that doesn’t mean Google Cloud is very far behind. AWS is now just one of the three major cloud providers — with two more (IBM and Alibaba) gaining more popularity as well. Google Cloud Platform has more in store for its cloud business in 2020.
A big step for google was announced earlier this year at Google Cloud’s conference — Google Cloud Next — the CEO of Google Cloud announced that they would be coming out with a retail platform to directly compete with Amazon, called Google Cloud for Retail. What ‘s different about their product? For starters, they are partnering with companies such as Kohl’s, Target, Bed Bath & Beyond, Shopify, etc. — these retailers are known for being direct competition with Amazon. In addition to that, this will be the first time that Google Cloud has had an AI product that is designed to address a business process for a specific vertical. Google doesn’t appear to be stopping at just retail — Thomas Kurian said they are planning to build capabilities to assist companies in specialized industries, ex: healthcare, manufacturing, media, and more.
Google’s stock continues to rise. With nearly 6,450 new hires added to the headcount, a vast majority of them being cloud-related jobs, it’s clear that Google is serious about expanding its role in the cloud market. In April of this year, Google reported that 103,459 now work there. Google CFO Ruth Porat said, “Cloud has continued to be the primary driver of headcount.”
Google Cloud’s new CEO, Thomas Kurian, understands that Google is lagging behind the other two cloud giants, and plans to close that gap in the next two years by growing sales headcount.
Deals have been made with major retailer Kohl’s department store, and payments processor giant, PayPal. Google CEO Sundar Pichai lists the cloud platform as one of the top three priorities for the company, confirming that they will continue expanding their cloud sales headcount.
In the past few months, Pichai added his thoughts on why he believes the Google Cloud Platform is on a set path for strong growth. He credits their success to customer confidence in Google’s impressive technology and a leader in machine learning, naming the company’s open-source software TensorFlow as a prime example. Another key component to growth is strategic partnerships, such as the deal with Cisco that is driving co-innovation in the cloud with both products benefiting from each other’s features, as well as teaming up with VMware and Pivotal.
Driving Google’s growth is also the fact that the cloud market itself is growing so rapidly. The move to the cloud has prompted large enterprises to use multiple cloud providers in building their applications. Companies such as Home Depot Inc. and Target Corp. rely on different cloud vendors to manage their multi-cloud environments.
Home Depot, in particular, uses both Azure and Google Cloud Platform, and a spokesman for the home improvement retailer explains why that was intentional: “Our philosophy here is to be cloud-agnostic, as much as we can.” this philosophy goes to show that as long as there is more than one major cloud provider in the mix, enterprises will continue trying, comparing, and adopting more than one cloud at a time — making way for Google Cloud to gain more ground.
Multi-cloud environments have become increasingly popular due because companies enjoy the advantage of the cloud’s global reach, scalability, and flexibility. Google Cloud has been the most avid supporter of multi-cloud out of the three major providers. Earlier this year at Google Cloud Next, they announced the launch of Anthos, a new managed service offering for hybrid and multi-cloud environments to give enterprises operational consistency. They do this by running quickly on any existing hardware, leverage open APIs and give developers the freedom to modernize. There’s also Google Cloud Composer, which is a fully managed workflow orchestration service built on Apache Airflow that allows users to monitor, schedule and manage workflows across hybrid and multi-cloud environments.
Google Cloud Platform vs AWS is only one of the battles to consider in the ongoing cloud war. The truth is, market performance is only one factor in choosing the best cloud provider. As we always say, the specific needs of your business are what will ultimately drive your decision.
What we do know: the public cloud market is not just growing — it’s booming. Referring back to our Azure vs AWS comparison — the basic questions still remain the same when it comes to choosing the best cloud provider:
Right now AWS is certainly in the lead among major cloud providers, but for how long? We will continue to track and compare cloud providers as earnings are reported, offers are increased, and price options grow and change. To be continued in 2020…
Originally published at www.parkmycloud.com on December 5, 2019.
CEO of ParkMyCloud
21 
1
21 
21 
1
CEO of ParkMyCloud
"
https://medium.com/google-cloud/deploying-google-cloud-functions-in-5-easy-steps-21f6d837c6bb?source=search_post---------390,"There are currently no responses for this story.
Be the first to respond.
Recently I did a project which involved deploying Google Cloud Functions to do the stuff.
This is how to start using GCF the easy way.
First, you need a project to be created already. If not, go to https://console.cloud.google.com and create it. You also need to visit Google Cloud Functions section at least once to make sure GCF API is enabled. You’ll also need to create a storage bucket for a function — there’s no rocket science there, just pick up random name 👍
Second, you need to have Google Cloud SDK installed on your machine. You can download it from here or just go with brew:
Third, you need to set the current project for Google Cloud SDK. The directory structure doesn’t need to be there yet.
Forth step! We need a nodejs project set up. It’s enough to just do yarn init in a desired project and create index.js with a content of your new cloud function, like this:
Final step 🎉
Example:
That’s it! 😎
Hi, I’m Valerii. I live and write in Amsterdam. I wrote this article, all views are my own. If you enjoyed reading it, make sure to follow me on twitter https://twitter.com/viatsko
Google Cloud community articles and blogs
48 
1
48 claps
48 
1
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Senior Engineer @ Microsoft • Views are my own
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/faster-serviceaccount-authentication-for-google-cloud-platform-apis-f1355abc14b2?source=search_post---------391,"There are currently no responses for this story.
Be the first to respond.
A couple weeks ago I wanted to understand the AccessTokenCredentials flow that certain google cloud APIs supported.
It’s described here in the addendum of our developers oauth documentation as a specific optimization over the “normal” oauth flows providers maintain. The optimization is pretty dramatic so I thought I’d write an article describing it and how its used…and finally a quick bakeoff to demonstrate its advantage.
First, a quick background on serviceAccount oauth flows
A serviceAccount on GCP can take many shapes but it normally represents a non-user accessing a system. Think of it as machine accounts that require access to a service. When a system needs to access a GCP service (eg Pub/Sub), it needs to acquire an Oauth2 token described here
Essentially, a cryptographic private key is issued for a ServiceAccount and that key is used to sign a JSONWebToken (JWT) which includes some claims and information about the token capabilitites requested. At that point, the JWT is trasmitted to Google which verifies the claims and identity of the service account. Once google verifies the identity, it issues an access_token for the ServiceAccount with the scopes the original JWT requested and returns that token back to the client. The client application at that point has the bearer access_token to make the request to the service (Pub/Sub, in this example).
Note that this flow involves a roundtrip exchange for the JWT for an access_token ... so what if we could bypass that one roundtrip?
Allright, so how can we optimize the flow above if we already have a crypto key we can sign with? How about we create a JWT with a specific audience that is the service we intend it for? That is the optimized flow we're dealing with in this article and that is the flow that will save us this roundtrip.
The golang sample here basically reads the private key and uses it to sign a JWT with a specific aud: field that denotes the serivce its intended for.
This flow saves a round trip call but only applies to specific services within Google Cloud. These specific services utilize a different backend system which allows for this abridged flow. For example, the services listed below are the only ones that allows for this:
If you’re interested, the JWT that is signed by the service account uses the aud: field that describes the target service itself:
If you want to try this sample out, you would need to first create a service account and download its JSON private key. Once you do that, enable IAM access for that service account to Pub/Sub Viewer role as shown below:
At that point, download the JSON certificate and initialize the client:
And then run the sample:
In which response is the latency in milliseconds.
I must note: this whole procedure ONLY applies to the inital acquisiton of the access_token. In normal usecases, you can reuse an access_token or even the id_token until it expires (normally 3600s). What that means is the latency described below is only to get the first token for most usecases.
The following sample runs through the abridged flow against the standard ServiceAccount oauth flow where the full cert is loaded already and the measure is the Percentile Latency
I ran each mechanism 100 times on the same computer separately (and yah, trust me, the workstation where Iran it had lots of compute and very high network bandwith to GCP endpoints!).
As you can see, in any bracket, the lack of the additional roundtrip makes a difference in getting and making the same API call!
The following describes various other language bindings for the same abridged flow:
Additional references for oauth and service accounts:
You can find the source below or under the git repo i maintain here:
github.com
AccessTokenCredentials:
ServiceAccountCredentials:
Google Cloud community articles and blogs
96 
2
96 claps
96 
2
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/building-a-go-web-app-from-scratch-to-deploying-on-google-cloud-part-2-deploying-our-go-app-on-a07285dfa6a9?source=search_post---------392,"There are currently no responses for this story.
Be the first to respond.
In Tutorial #1, we created a simple Go application that welcomed us and displayed the time. We ran our application locally and we were able to interact with it only from our machine. In this tutorial, we share our beloved welcome app to the world using Google’s App Engine (GAE).
"
https://medium.com/javarevisited/7-best-google-cloud-professional-data-engineer-certification-exam-courses-in-2021-dd15631a6ebd?source=search_post---------393,"There are currently no responses for this story.
Be the first to respond.
Hello guys, if you want to become a Google Cloud certified professional data engineer in 2021 and looking for the best online resources then you have come to the right place. Earlier, I have shared the best Google Cloud platform courses and today, I am going to share the best online courses for Google Cloud Data Engineer Certification in 2021.
Google Cloud Professional Data Engineers are currently in demand for their ability to maintain the process of accumulating, converting, and publishing data with a certain level of expertise.
They are mostly known for having the capability to put machine learning models into operation, creating and structuring data processing systems to work accurately, and ensuring that the data is secure and scalable.
It’s a very important role to play on Google Cloud’s data science team as the skills of a professional data engineer make the data management process more efficient. It allows the rest of the data team to complete their tasks with less difficulty. A career in Google cloud data engineering is mostly pursued by those who are confident in their programming skills and wish to conduct in-depth studies in data and data technologies. If you have an interest in programming and manipulating data tech, then it’s recommended that you take a look into data engineering. As for the additional rewards of being a Google Cloud Data Engineer, you could potentially receive an average salary of more than $117,000 per year. The rates may vary depending on how much experience you possess in data engineering as data engineering is best learned with much practice.
In order to become a Google Cloud Data Engineer, one must first pass the exam that will certify them as a data engineer. This exam is conducted online by Google and tests the user’s knowledge of data engineering. There are no prerequisites necessary to qualify for this certificate but it is recommended that you have a thorough knowledge and some practice in data engineering before the exam is conducted. There are many courses online available to help you with this. Here are the top 5 recommended courses for Google Cloud Data Engineer certifications in 2021:
This is the best Udemy course to become a Google Data Engineer in 2021. Its lessons are taught by two instructors who have been working for Google as Cloud data engineers for 7 years after successfully completing Google Cloud’s data engineer certification exam. The course’s total video content measures 28 hours long and is taught in a very engaging and practical way with downloadable material available for additional information and practice. They break down complex topics in a way that the participant will find easy to comprehend and apply in the certification exam. The course can only be accessed through Udemy. Udemy is an online learning platform that uses a one-time payment for each of their courses and they also offer a refund within 30 days if you are not satisfied with your course of choice. Here is the link to join this Google Cloud course — Complete Google Data Engineer Guide
They also give very good discounts on their prices which makes their courses very affordable. Udemy also has a section where you can ask the instructors a question regarding the course. They generally answer these in 24 hours or less.
This is another awesome Udemy course to pass the Google Cloud Professional Data Engineer Exam. You will not only prepare for exams but also learn to build scalable, reliable data pipelines, databases, and machine learning applications.
This course is designed and developed by the author of the official Google Cloud Professional Data Engineer exam guide and a data architect with over 20 years of experience in databases, data architecture, and machine learning.
This course combines lectures with quizzes and hands-on practical sessions to ensure you understand how to ingest data, create data processing pipelines in Cloud Dataflow, deploy relational databases, design highly performant Bigtable, BigQuery, and Cloud Spanner databases, query Firestore databases, and create a Spark and Hadoop cluster using Cloud Dataproc.
The course also includes a 50 question practice exam that will test your knowledge of data engineering concepts and help you identify areas you may need to study more.
Here is the link to join this GCP course — Google Cloud Professional Data Engineer: Get Certified 2021
The final part of the course is dedicated to the most challenging part of the exam: machine learning. If you are not familiar with concepts like backpropagation, stochastic gradient descent, overfitting, underfitting, and feature engineering then you are not ready to take the exam.
And, if you like reading books, it is also available as a book on Amazon, you can buy the book here.
www.amazon.com
At 4.5 hours long, this Udemy course is a very short one but makes up for that in terms of the content that is focused on. It is taught by a certified Google Cloud Professional Data Engineer who has plenty of experience working with data tech. The course builds on your foundation of GCP’s products by explaining the role of each of the products as well as the applications of GCP that you will need to know to be able to pass the exam. There are quizzes at the end of each section, aiming to help you apply and retain the information you’ve learned. However, before you start the course there are some requirements which include SQL Basics and having an understanding of Relational Databases and NoSQL. These concepts are not difficult to grasp and won’t require more than a Google search or two.
Here is the link to join this course — . Google Cloud Professional Data Engineer Course
This best Coursera course for Google Cloud Platform has landed jobs in data engineering with Google Cloud for many of its students. It’s offered by the Google Cloud Team themselves and is taught by many certified instructors from top companies and universities. Upon entering this course, you will gain access to a plethora of resources structured to help you apply the google cloud technology and pass the exam for the Google Cloud Data Engineering Certificate. There is also a certificate of completion that will be given to you upon completion of the course. To access the course, you’ll need to purchase a Coursera subscription for $49 monthly but in case you would like to confirm that you are satisfied with the course, you can start a 7-day free trial and cancel any time after.
Here is the link to join this course — Data Engineering with Google Cloud Professional Certificate
And, if you find Coursera courses and certifications useful then you should also join the Coursera Plus, a subscription plan from Coursera which provides you unlimited access to their most popular courses, specialization, professional certificate, and guided projects.
It cost around $399/year but it's completely worth your money as you get unlimited certificates and it’s also very cost-effective.
coursera.com
Designed by Google Cloud, this course on Pluralsight offers Cloud Data Engineer training for all levels from beginner to advanced data engineering.
You are taught all the necessary skills needed to become a Google Cloud Data Engineer such as processing and streaming data, building data processing systems, and managing data pipelines. This is all through a series of presentations, demos, and labs. One important thing to note about Google cloud certification is that they are valid for two years from the data you passed the exam and after expiration, you need to re-certify yourself.
Here is the link to join this GCP course — Data Engineering on Google Cloud
By the way, you would need a Pluralsight membership to watch this course which cost around $29 per month or $299 per year (14% discount) but Pluralsight also offers a 10-day free trial which you can use to watch this course but you’ll have to pay a fee of $29 per month after that trial is over and if you want to continue.
click.linksynergy.com
This Udemy online course is taught by two instructors who have years of experience in Google Cloud Data Engineering and coaching others who wish to get into the field.
They take great care in making sure that the individual taking the course will get a thorough knowledge of the GCP concepts and examples to know what it is like to take on the tasks of a data engineer. Before this, they will show you how to set up the Google Cloud Platform on your computer in order to follow along with them in their demos so that you can get hands-on experience with the products while you are taking the course. The course is 8.5 hours long and contains many downloadable training material resources to help you along with the lectures. You will get all those materials without any additional costs.
Here is the link to join this course — GCP: Google Cloud Platform: Data Engineer, Cloud Architect
When it comes to IT certification, I go to Whizlabs they have the best online courses for IT and Cloud certifications. I have used their courses and practice test in the past to pass Java, AWS, and another cloud certification and this Google Cloud Data Engineer Course is another gem from Whizlabs.
This 7+ hours long training course covers all Exam topics in good detail and helps professionals to prepare themselves for the actual certification exam.
One needs to go through all the training videos & appear in all the Google Cloud Certified Professional Data Engineer practice tests to get fully prepared for the Google Cloud Certified Professional Data Engineer certification exam. This will make you confident for the exam, and thus you will be able to pass the exam on the first attempt.
Here is the link to join this Whizlabs course — Google Cloud Data Engineer Course
That’s all about the best online courses to become a Google Cloud Platform Data Engineer in 2021. These courses are specially designed keeping this prestigious cloud certification in mind and they cover all topics specified in official exam guides.
If you are preparing for Google Cloud Certification, particularly Data Engineer then you should take benefit of these courses for better preparation.   Other IT and Cloud Certification Articles you may like:
Thanks for reading this article so far. If you find these Google Cloud Platform Data Engineer courses useful then please share with your friends and colleagues on Facebook, Twitter, and LinkedIn. If you have any questions or about then please drop a note. P. S. — Apart from going through these GCP Associate Cloud engineer courses you can also check out the exam guide or take the practice exam that will help you in identifying and focusing on the main areas during your preparation.
udemy.com
Medium’s largest Java publication, followed by 14630+ programmers. Follow to join our community.
171 
171 claps
171 
A humble place to learn Java and Programming better.
Written by
I am Java programmer, blogger, working on Java, J2EE, UNIX, FIX Protocol. I share Java tips on http://javarevisited.blogspot.com and http://java67.com
A humble place to learn Java and Programming better.
"
https://medium.com/hacking-and-slacking/creating-google-cloud-functions-running-python-3-7-8034e066a130?source=search_post---------394,"Sign in
There are currently no responses for this story.
Be the first to respond.
Todd Birchard
Oct 19, 2018·6 min read
The more I explore Google Cloud’s endless catalog of cloud services, the more I like Google Cloud. This is why before moving forward, I’d like to be transparent that this blog has become little more than thinly veiled Google propaganda, where I will henceforth bombard you with persuasive and subtle…
"
https://medium.com/google-cloud/playing-with-concourseci-via-a-google-cloud-platform-free-trial-65acfbdd02d2?source=search_post---------395,"There are currently no responses for this story.
Be the first to respond.
I’ve started a practice of spending a couple of hours each weekend honing and growing my skills as a software developer. This weekend’s lesson was deploying my own ConcourseCI cluster to Google Compute Platform via the Bosh Google CPI.
I’m guessing that everyone knows about Amazon’s cloud computing platform called Amazon Web Services, AWS for short. Until recently, I hadn’t realized that Google had a competing platform called Google Compute Platform, GCP for short. One thing I like about GCP over AWS is the pricing model. Where AWS charges you for full hours of usage, GCP charges by the minute after a minimum of 10 minutes. Not to mention $300 free trial for 60 days.
Lucky for me, the folks at Google have done a great job documenting the procedure for installing Concourse to GCP.
Here are some of the tweaks I had to make to the above instructions to make it fit into the free plan (latest version numbers may be different by the time you read this post):
And that’s it. Following the directions with these minor tweaks should get you up and running quickly with your own GCP based Concourse installation.
If you are looking for tutorials to help you learn Concourse, check out the Concourse tutorials page. The flight school example is a great way to get started. You can check out my running version here: http://107.178.255.195/
Google Cloud community articles and blogs
15 
15 claps
15 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Home of my latest thoughts on software development. If you are looking for code, check out https://github.com/mikegehard.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/this-week-in-google-cloud-platform-quantum-bristlecone-interactive-clis-and-a-cloud-healthcare-e9ca40d3be01?source=search_post---------396,"There are currently no responses for this story.
Be the first to respond.
Announcing Bristlecone (Google Blog), a new quantum processor from the Google Quantum AI Lab built for researching and testing Google’s qubit technology
[Alpha] gcloud and other CLIs are getting even smarter — “Introducing GCP’s new interactive CLI” (Google Blog).
Google Cloud for Healthcare: new APIs, customers, partners and security updates (Google Blog). This includes a new API to ingest and manage HL7, FHIR, and DICOM formats for further analytics and ML processing with GCP. Read also the post for customer and partner momentum announced at HIMSS 2018.
From the “customers talk best about GCP” department :
From the “some insights about how foundations operate” department :
From the “in case you’ve missed it (ICYMI)” department :
From the “How to” department :
From the “Watch, listen, & learn” department :
From the “taking a step back” department :
The lovely terminal colors from this week’s picture are here to celebrate the release of interactive CLI tools :
That’s it for this week!-Alexis
Google Cloud community articles and blogs
35 
35 claps
35 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Google Cloud Developer Relations
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/@Kurozakizz/google-cloud-shell-%E0%B8%84%E0%B8%B7%E0%B8%AD-vm-%E0%B8%9F%E0%B8%A3%E0%B8%B5%E0%B8%95%E0%B8%B1%E0%B8%A7%E0%B8%99%E0%B8%B6%E0%B8%87-94143fe8205c?source=search_post---------397,"Sign in
There are currently no responses for this story.
Be the first to respond.
Nuttawoot Singhathom
Nov 9, 2017·1 min read
เมื่อก่อนก็งงๆว่า cloud shell มันเอาไว้ทำไรอะ ตอนนี้เริ่มเข้าใจละ สรุปมันคือ vm free ตัวนึงที่สาระพัดประโยชน์มากในการใช้งาน google cloud
จริงๆแล้ว มันคือ Debian linux มีพื้นที่ให้ใช้ฟรี 5GB เลยนะ แถมสามารถรัน Git, Docker, Node.js, Go, Java, Python ได้เลย
สร้าง google compute engine ก็ได้
download file ผ่าน wget ก็ได้
upload file ขึ้น cloud storage ก็ได้
git clone โปรเจคมาลองรันเล่นก็ได้
เอาเป็นว่าไปลองจิ้มดูแล้วกันมันทำอะไรได้เยอะเลย เพราะมันคือ linux vm ตัวนึงนี่แหละ
See all (323)
71 
71 claps
71 
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/a-beginners-guide-to-painless-ml-on-google-cloud-8453575f357e?source=search_post---------398,"There are currently no responses for this story.
Be the first to respond.
Building AI-powered apps can be painful. I know. I’ve endured a lot of that pain because the payout of using this technology is often worth the suffering.
Happily, over the past five years, developing with machine learning has gotten much easier thanks to user-friendly tooling. Nowadays I find myself spending very little time building and tuning machine learning models and much more time on traditional app development.
In this post, I’ll walk you through some of my favorite, painless Google Cloud AI tools and share my tips for building AI-powered apps fast. Let’s get to it.
One of the slowest and most unpleasant parts of machine learning projects is collecting labeled training data-labeled examples that a machine learning algorithm can “learn” from.
But for lots of common use cases, you don’t need to do that. Instead of building your own model from scratch, you can take advantage of pre-trained models that have been built, tuned, and maintained by someone else. Google Cloud’s AI APIs are one example.
The Cloud AI APIs allow you to use machine learning for things like:
The machine learning models that power these APIs are similar to the ones used in many Google apps (like Photos). They’re trained on huge datasets and are often impressively accurate! For example, when I used the Video Intelligence API to analyze my family videos, it was able to detect labels as specific as “bridal shower,” “wedding,” “bat and ball games,” and even “baby smiling.”
The Cloud AI APIs run in, well… the cloud. But if you need a free and offline solution, TensorFlow.js and ML Kit provide a host of pre-trained models you can run directly in the browser or on a mobile device. There’s an even larger set of pre-trained TensorFlow models in TensorFlow Hub.
Though you can find a pre-trained model for lots of use cases, sometimes you need to build something really custom. Maybe you want to build a model that analyzes medical scans like X -rays to detect the presence of disease. Or maybe you want to sort widgets from doodads on an assembly line. Or predict which of your customers is most likely to make a purchase when you send them a catalog.
For that, you’ll need to build a custom model. AutoML is a Google Cloud AI tool that makes this process as painless as possible. It lets you train a custom model on your own data, and you don’t even have to write code to do it (unless you want to).
In the gif below, you can see how I used AutoML Vision to train a model that detects busted components on a circuit board. The interface to label data is click-and-drag, and training a model is as simple as clicking the button “Train New Model.” When the model finishes training, you can evaluate its quality in the “Evaluate” tab and see where it’s made mistakes.
It works on images ( AutoML Vision), video ( AutoML Video), language ( AutoML Natural Language and AutoML Translation), documents, and tabular data ( AutoML Tables) like you might find in a database or spreadsheet.
Even though the AutoML interface is simple, the models it produces are often impressively high-quality. Under the hood, the AutoML trains different models (like neural networks), comparing different architectures and parameters and choosing the most accurate combinations.
Using AutoML models in your app is easy. You can either allow Google to host the model for you in the Cloud and access it through a standard REST API or client library (Python, Go, Node, Java, etc), or export the model to TensorFlow so you can use it offline.
So that, more or less, makes model training easy. But where do you get a big training dataset from?
I mean it.
When I start an ML project, I first check to see if a pre-trained model that does what I want already exists.
If it doesn’t, I ask myself the same question about datasets. Almost any kind of dataset you could ever imagine exists on Kaggle, a dataset-hosting and competition site. From tweets about COVID-19 to a list of Chipotle locations to a collection of fake news articles, you can often find at least some dataset on Kaggle that will let you train a proof-of-concept model for your problem. Google Dataset Search is also a helpful tool for finding datasets that will query both Kaggle and other sources.
Sometimes, of course, you must label your own data. But before you hire hundreds of interns, consider using Google’s Data Labeling Service. To use this tool, you describe how you’d like your data to be labeled and then Google sends it out to teams of human labelers. The resulting labeled dataset can be plugged directly into AutoML or other AI Platform models for training.
Lots of times, building (or finding) a functioning machine learning model isn’t the tricky part of a project. It’s enabling the other folks on your team to use that model on their own data. We faced this problem frequently in Google Cloud AI, which is why we decided to add interactive demos to our API product pages so you can upload our APIs and try them out fast.
Leading a successful machine learning project often comes to being able to build prototypes fast. For this, I have a handful of go-to tools and architectures:
I used this setup recently when I built a document AI pipeline
When a document is uploaded to a cloud storage bucket, it triggers a cloud function that analyzes the document by type and moves it to a new bucket. That triggers a new cloud function, that uses the Natural Language API to analyze the document text. Check out the full code here.
Hopefully that’s convinced you getting started with machine learning doesn’t have to be painful. Here are some helpful tutorials and demos to get started with ML:
Originally published at https://daleonai.com on October 19, 2020.
Google Cloud community articles and blogs
52 
1
52 claps
52 
1
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Writing about writing code, analyzing data, and building ML models. Applied AI @ Google.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/google-cloud-platform-technology-nuggets-december-1-15-2021-edition-1f11dd72668a?source=search_post---------399,"There are currently no responses for this story.
Be the first to respond.
Welcome to the December 1–15, 2021 edition of Google Cloud Platform Technology Nuggets.
We have just two weeks left in 2021 and if you are looking to boost your GCP Skills, check out the offer to register before January 10, 2022 and get access to free 30 day access to Cloud Skills Boost, where specific courses ranging from Core Infrastructure, Kubernetes and Machine Learning are available.
This edition of the newsletter contains several Top 10 Blog posts in the respective categories that have been published on the official Google Cloud Blog site.
Google Cloud launched its 29th region in Santiago, Chile and has plans for multiple other regions in 2022. Check out this blog post that gives an overview of the infrastructure investments and roadmap for the near future.
In more infrastructure news, Google Cloud has been placed as a Leader in the Forrester Wave, AI Infrastructure, Q1 2021 Report.
Since we are at the end of the year, what better way to cap off the Infrastructure segment, with top 10 Infrastructure blogs on Google Cloud Blog in 2021.
I find reading case studies a bit dry and prefer listening to customers telling us about their journey, what worked for them, what did not and more. Did you know that we have an Architecting with Google Cloud series where customers share their stories with us of how they have built innovative solutions on top of Google Cloud Platform.
Check out this video playlist where customers share their journey.
Google Fellow Eric Brewer has been in the driver’s seat to building ground breaking services at Google and helping to externalize them. Eric currently oversees multiple services ranging from Kubernetes, Istio, Serverless and more. In a 4-part video series, Eric discusses four key Kubernetes and Open Source insights that have defined the future of cloud computing.
GKE Autopilot has taken the managed GKE offering to a new level with its mode that reduces the burden of cluster configuration with optimizing and secure default configurations. It has gone a step further now by partnering with multiple Tech Partner solutions like Gitlab, Dynatrace, Splunk and more, which allows you to run these products on Autopilot without modification. Check out the post which references a full list of integrations.
You know that Anthos Config Management helps to manage an ever expanding Kubernetes footprint enabling you to set and enforce consistent configurations and policies for your Kubernetes resources. If you would like to see this in action via the “Config Sync” GitOps methodology, take a look at this blog post.
We conclude this section with the Top 10 posts from the Managed Compute offerings on Google Cloud.
Cloud Pub/Sub now allows you to retain messages sent to topics for a period of 31 days instead of the early maximum of 7 days. This gives you a bit more breathing space in case you want to use these messages to debug subscribers, transform existing messages into other event information, replay the messages if you want to and more.
Understanding your BigQuery environment has got better with the availability of Slot Estimator and Resource Charts. The Slot Estimator is an interactive capacity management tool that helps administrators estimate and optimize their BigQuery capacity based on performance, while Resource Charts allow you to monitor their slot usage, manage capacity based on historical consumption, troubleshoot job performance, and self diagnose queries, and take corrective action as needed. Check out the blog post that gives a detailed explanation of how you can use them today via examples.
It has been a hectic week for operations teams across the world with the discovery of the log4j vulnerability. The Google Cybersecurity Action team has published recommendations around these vulnerabilities and how to mitigate them. Additionally, there have been updates to Cloud Armor, Security Command Center (Premium) to help manage the risk. It is also recommended to check the security advisory page for updates.
While talking about Google Cloud Security, a discussion around Service Accounts comes up regularly. It is recommended that Service Accounts be used minimally and with the right permissions to prevent attacks. Any organization would like to know its current inventory of Service Accounts and audit them for their activity, last usage and more. This blog post goes into those specifics and helps you understand how you can look at 3 specific GCP services has help you understand:
Looking to sharpen your skills with Google Cloud Serverless services like Cloud Run, Cloud Functions and more? How about taking part in the Serverless Hackathon that has just been launched and runs till February 2022. Be one of the first to try out the latest (2nd generation) versions of Cloud Functions and additional features to pick and demonstrate how you will address the serverless challenges thrown at you in the Hackathon. Check out the blog post or directly visit the website for more information.
If you are targeting a GCP certification in the coming year, the question you might have is where to start and how to build up your readiness not just in terms of the exam content but also the tools to ensure that you are ready to use them in your role. This blog post gives a good starting path to first picking up critical skills via GCP Skill badges and then moving on to a specific exams and helping you understand what those exams cover:
​Next up, we have a SketchNote and blog post on Google Cloud DevOps Overview, where you are given an overview of the CI/CD processes and what various tools are available in GCP to make that happen.
Since we are on the CI/CD topic, what about taking it one step further and understanding that once your application is in production, how do you manage, debug and ensure that your application is performing reliably. You do that via the Cloud Operations Suite, which was formerly called Stackdriver. Check out the notes to learn more about Cloud Operations.
Have questions, comments, or other feedback. Do send it across.
Looking to keep a tab on new Google Cloud product announcements? We have a handy page that you should bookmark → What’s new with Google Cloud.
Google Cloud community articles and blogs
42 
42 claps
42 
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
