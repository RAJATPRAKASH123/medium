story_url,bodyText
https://chiefscientist.org/programming-languages-in-the-era-of-the-cloud-1b0290238b39?source=search_post---------0,"Google was the first cloud company. It is still the most important one — its open source riff on an internal cloud, called Kubernetes, is now the de facto cloud as we know it these days.
Google excited our imagination with global-scale distributed systems. Big Table begat Hadoop. Machine translation opened the Babel tower or the siloed web cultures to each other. Google App Engine, Big Query and other Google Cloud offerings changed the way we build cloud-native applications.
But something else started to happen as Google grew. The set of programming languages allowed for use was restricted to just a few, Java, C++, Python. JavaScript for the web, and Go — an intentionally uncool and restrictive language, without type gymnastics or category theory foo, but with elegant concurrency, suitable for devops. Using git as a packaging system was a harbinger of the cloud standardization to come.
Will all cloud programming converge on a set of just a few approved languages? Will our IDEs, moving into the cloud, become standardized as well? Will the testing frameworks converge?
Will we all use GitHub as our development environment, with certain IDEs and workflow tools emerging as supermajority? What will happen with niche languages, understood and enjoyed by a few now? Will developers become conveyor belt automatons, with their skills commoditized and outsourced?
Or will software craftsmanship flourish in new ways? Our panel spans the whole spectrum. Jaana Dogan is an established speaker for Go and distributed database expert from her Google tenure, now reestablishing herself at AWS. Gabriel Volpe uses Scala at Chatroulette, using Nix for reproducible developer environments, testing, and deployments. Rúnar Bjarnason is creating Unison, a new way to program where your code gets distributed and built as you write it. Baruch Sadogurski is supporting developers in all their cloud building and deployment endeavors from the glorious Artifactory.
Come join our panel and let us find the truth in the debate, together!
Data tales from the Silicon Valley
1 
1 clap
1 
Written by
Chief Scientist By the Bay. sfscala.org, sftext.org, sfspark.org, scala.bythebay.io, data.bythebay.io, ai.vision, rethink.money. Dad of four.
Data tales from the Silicon Valley
Written by
Chief Scientist By the Bay. sfscala.org, sftext.org, sfspark.org, scala.bythebay.io, data.bythebay.io, ai.vision, rethink.money. Dad of four.
Data tales from the Silicon Valley
"
https://medium.com/@wickr/the-rise-of-zero-trust-and-why-it-matters-wickr-b981215fac7b?source=search_post---------1,"Sign in
There are currently no responses for this story.
Be the first to respond.
Wickr
Jan 27, 2020·4 min read
Today, many businesses use office equipment that connects through their network into internal cloud databases. Although many CIOs, CISOs, and other corporate executives believe their existing network administration practices effectively monitor the security infrastructure, a surprising number of threat agents slip past firewalls and remain unchecked on the network.
According to a recent report by Wipro, around 29% of breaches were caused by unauthorized access to user credentials in networks that utilize a Personally Identifiable Information (PII) system. The cause of these breaches may be due to administrators allowing seemingly trustworthy applications and devices to pass through network perimeters.
As we start replacing outdated office equipment with the latest cloud technology, more businesses need to rework their cybersecurity strategy to protect their internal network infrastructures.
Around 81% of companies today use cloud-computing infrastructure, with over $6 million expected to be invested in cloud technologies annually. The focus on Zero Trust architecture has become more critical than ever.
That said, below are five strategies a business can utilize to maintain or improve their cybersecurity from external threats.
1. Multifactor Authentication of All Users
Typing in a username and password is only one direct avenue to access sensitive company details, customer database information, or controls on the network. Certain types of malware may bypass the initial authorization if user credentials are compromised, but they cannot duplicate multifactor authentication codes.
Instead of one user authorization method, a multifactor authentication module requires a user to verify the request a second time with a random code sent to an email. As long as the threat agent doesn’t have access to the system that receives the authentication code, they won’t be able to access the network.
2. Identify and Familiarize Devices on the Network
Each time your company adds a new device or user to the system, your company needs to label and identify that entity on the network. Keep a record when you add new cloud devices to your network or hire new employees in the IT department.
3. Utilize a Reverse Proxy
Once you have a familiar network infrastructure routine and a record of each device and user, you can reference these when conducting a threat check, reducing the time it takes to identify a breach of the system or trace its source.
4. Conduct Threat Checks
One way to trap unauthorized users before they have direct access to the network or sensitive database information is to set up a reverse proxy. A reverse proxy is a server that channels all external traffic into an internal application.
5. Treat Each Entity on the Network the Same
With this type of setup, users must pass through the external server before they can enter the area with more sensitive data on the network. It’s also convenient when specific employees in an IT department require minimal network access to complete tasks.
What You Can Do to Simplify Zero Trust Procedures Here are a few ways to automate the Zero Trust process:
Implement Zero Trust Strategies with Wickr Wickr understands that companies may experience significant delays and network hang-ups while updating internal software. Make Zero Trust strategies easier to implement with an expansive network-management application. Visit today to learn more about the different ways to improve Zero Trust integration and to find the right software for your company.
As a system grows, adds new devices, or new types of malware/phishing attempts come out, a company must conduct routine threat checks. What were considered threats five years ago may have become almost entirely obsolete or irrelevant to your company at its current stage.
While conducting a threat check, make sure all devices or systems on the network have the latest security updates, remove active threats, and revise employee protocols to protect against malware. By gaining an enhanced overview of the IT department and vulnerabilities that exist in the network, your company can keep up with changing cybersecurity threats.
Even if your employees or network administrators recognize an application or user on the network, that doesn’t mean that it should be trusted. Once hackers or malware infiltrates your database or network, they can gain access to usernames or mimic a trusted user/device on the system by hiding their code.
Threat agents can also connect to your network through back doors that don’t show up as abnormalities or alerts on the system. That’s why you need to verify every entity that requests access to your network and double-check that the active entities are authentic.
With established cybersecurity databases in place, a company’s network can be backed up while implementing effective Zero Trust strategies that manage internal threats during daily operations.
Each of the individual steps required to conduct Zero Trust procedures may be too much for one security team or network administrator to handle. By simplifying the process, you can keep both internal and external network infrastructures secure without IT activity delays.
More and more companies will eventually require a complete overhaul of outdated cybersecurity systems since cloud-based technologies are on the rise. With Zero Trust software, companies can use a single internal platform to manage users, password updates, monitor the network, get alerts, or update employees on changes.
Originally published at https://wickr.com on January 27, 2020.
Secure Ephemeral Communications. Built for the enterprise. End-to-end encrypted messaging. Secure rooms. Peer-to-peer encrypted file sharing. Multi-platform.
Secure Ephemeral Communications. Built for the enterprise. End-to-end encrypted messaging. Secure rooms. Peer-to-peer encrypted file sharing. Multi-platform.
"
https://medium.com/@alibaba-cloud/log-on-to-alibaba-cloud-using-internal-enterprise-accounts-with-ram-single-sign-on-301dbee6a48f?source=search_post---------2,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Jan 30, 2019·5 min read
By the Open API Team
Alibaba Cloud Resource Access Management (RAM) is an identity and access control service that enables you to centrally manage your users and securely control their access to your resources through permission levels. With RAM, you can easily create and manage users, including employees and apps developed by your enterprise. You can control the access permissions of these users for cloud resources, allowing for collaborative work while protecting your account from any unsolicited access.
The ability to protect cloud resources and mitigate risks are necessary to ensure successful enterprise cloud migration. In various cloud-native app scenarios, RAM provides customers with diversified access control mechanisms and enables enterprises to implement the principle of least privilege across full-stack systems such as DevOps, computing environment, apps, and data access. These benefits reduce the exposure to attack of cloud resources and effectively control the information security risks involved in enterprise cloud migration.
RAM has provided identity security and access management services to over 100,000 enterprise customers. Based on the Attribute Based Access Control (ABAC) security model, RAM provides customers with fine-grained access control over cloud resources and supports the following cloud-native app scenarios:
Recently, the RAM Single Sign-On (SSO) function was released to support a new scenario: logging on to Alibaba Cloud using internal enterprise accounts.
Let’s assume that your enterprise has deployed a local domain account system, such as Microsoft AD or AD FS. To meet the enterprise’s security management and compliance requirements, all employees must pass a unified identity verification of the enterprise domain account system before they can perform any operations on resources, including cloud resources. In this case, employees are prohibited from using independent user accounts and passwords to directly operate on cloud resources. To meet the security and compliance requirements, a similar security capability is required from the cloud service provider.
Alibaba Cloud RAM supports the Security Assertion Markup Language 2.0 (SAML 2.0) standard for identity federation, which is widely used by enterprise-level identity providers (IdPs). By activating the RAM user federated Single Sign-On (SSO) service under the cloud account, you can use internal enterprise accounts to log on to Alibaba Cloud.
In scenarios where Alibaba Cloud services are integrated with an enterprise identity system, Alibaba Cloud serves as the SP while the enterprise identity system serves as the IdP. Figure 1 shows how employees of an enterprise log on to the Alibaba Cloud console through their own enterprise identity system.
Figure 1. Basic process of using internal enterprise accounts to log on to the Alibaba Cloud console
After the administrator configures SAML federated SSO, enterprise employees can log on to the Alibaba Cloud console by using the method shown in Figure 1.
Note: In step 1, the employee does not have to log on from the Alibaba Cloud console. Instead, the employee can click the link on the enterprise’s own IdP logon page to send a SAML verification request to the enterprise IdP in order to access the Alibaba Cloud console.
For more information about the working principles and configuration method of SAML federated SSO, visit the official RAM documentation page for SSO Federation Logon.
For this scenario, let’s assume that your enterprise has only one cloud account, which has resources including VMs, networks, databases, and storage resources. Meanwhile, this account is used to manage RAM users and their permissions. Figure 2 shows the proposed SSO model.
Figure 2. Single-account management and SSO model for on-cloud enterprises
Recommendations: Use this account as an SP for identity federation with the enterprise’s local IdP, and use RAM to control user access to cloud resources.
In this scenario, assume that your enterprise has two cloud accounts, which are referred to as workload accounts. Both accounts host resources, such as VMs, networks, databases, and storage resources. Figure 3 shows the proposed SSO model.
Figure 3. Multi-account management and SSO model for on-cloud enterprises
Recommendations: Create an independent cloud account, which is referred to as the identity account. Under this account, you can only create RAM users. Use this account as an SP for identity federation with the enterprise’s local IdP. Then, use the cross-account access function provided by Alibaba Cloud RAM to authorize the employees to access the resources under A1 and A2.
To learn more about Alibaba Cloud Resource Access Management, visit https://www.alibabacloud.com/product/ram
Reference:https://www.alibabacloud.com/blog/log-on-to-alibaba-cloud-using-internal-enterprise-accounts-with-ram-single-sign-on_594416?spm=a2c41.12532350.0.0
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/sensual-enchantment/i-awoke-from-a-wet-dream-only-to-realize-i-had-lucid-dream-sex-77f66a6dc10a?source=search_post---------3,"There are currently no responses for this story.
Be the first to respond.
You have 2 free member-only stories left this month. Sign up for Medium and get an extra one
Top highlight
When I woke up my body was tingling all over and a tepid mist washed over me. My mind felt at peace and I had a satisfied grin on my face. I could still feel the rhythmic pulsations between my legs. I wanted to keep riding that wave. It’s spectacular how one can reach orgasm while sleeping without human touch. How’s that for a mind fuck? Imagine how glorious it would be to awaken from a lucid sex dream? But first, you need to learn how to lucid dream before you can eventually sexplore in la-la land, and maybe even break the cycle of recurrent dreams while you’re at it. In a time of global travel restrictions who wouldn’t want to take a trip to Cloud-9?
As I roused from slumber, I realized that during the sex dream I had control over the plot, and so I had been lucid dreaming. Lucid dreams occur when sleepers are aware that they are dreaming, which is associated with only one kind of sleep, known as “rapid eye movement” (REM) sleep. During REM sleep, brain activity looks similar to that seen during waking hours.
In fact, there is so much untapped energy in our dreams, and some of that power can be yours if you learn how to lucid dream. Then, once you get good enough at it, when you find yourself in a sex dream, you can direct it.
Imagine that you’re having a sex dream, and you are able to dictate what direction it goes in. Pretty cool, eh? Dare I say hot. You can be the director of your private porn as you actually participate in it at the same time. It’s a place where voyeurs can meet exhibitionists.
But before you begin to think about even having a lucid sex dream, you need to learn how to lucid dream. The state of lucid dreaming holds healing powers, but often the dreamer abruptly wakes from this level of consciousness.
But in learning to remain present in the dream, one can write the script which can be empowering and healing.
One tip for being able to remain in the lucid dream is to be present in the dream by interacting with your surroundings during the state of lucid dreaming. Therefore, that one moment where you realize that you are indeed dreaming is crucial. Instead of focusing on being excited by this discovery and waking yourself over it, try remaining calm and grounded by reaching out and touching something in your dream.
So you're first going to want to focus on the setting. A conscientious effort to interact with your environment can keep you in the moment and in the driver’s seat of the dream, rather than waking, which is what most people do when they become conscious of dreaming.
If you want to delve more deeply into the tips and tricks to help you explore in a lucid dream then visit this article 8 Insanely Powerful Lucid Dreaming Tactics For 2021at World of Lucid Dreaming.com.
Learning how to remain in a lucid dream is especially critical if you experience recurring dreams. Many individuals report having recurring dreams, such as an intruder breaking into one’s home. It’s not uncommon to have a response to such a dream where you feel paralyzed with fear. Another popular dream where one becomes frozen in a dream is while one is driving, for instance. Being frozen while driving, or losing control is a popular recurring dream.
Often the presence of recurrent dreams where we feel stuck within the dream can reflect being stuck inside a traumatic experience in your waking life. But once you begin to actually make yourself change the narrative of your dream by either getting up or leaving, one can realize that you’re indeed dreaming and thus emerge from lucid dreaming more likely to heal.
I can speak to this based on personal experience. Last year, I had a recurrent dream of a predator coming into my home. I would run around the house and try to make sure all of the doors were closed, but the predator was forcing its way in, and then I would awaken.
But eventually, I changed my dream story when I no longer became frozen with fear. I cannot only recall how at one point in the dream, I was paralyzed in my bed, but that I then willed myself to stand up and get out of harm's way. So I remember that I was lucid dreaming, since I willed myself to stand up by speaking to myself. When I woke up, I was conscious of how I hadn’t been able to do that action before — at least not in my recent past.
Therefore, it’s as though you’re breaking free of a trauma cycle, reaction, outlook, or fixed mindset. Imagine that one can use dreaming as a way to challenge self-limiting beliefs or patterns of thinking. For me, my fixed mindset was that of a victim. There were circumstances whereby I felt like a victim of life. But through hard work in my waking hours, this translated to my dreams where I was able to break free from the recurrent dream.
The healing energy of lucid dreaming is of great value. Some dreamers report a wave of emotions washing over them like healing energy. I have indeed experienced this before: it was almost like a warm ray of light penetrating my soul and my skin, like a hit of Vitamin D. This is a recent occurrence to me on my journey in healing from past trauma. Have you ever experienced this?
It turns out that most people are motivated to train themselves how to lucid dream due to being able to live out sexual fantasies, or learning how to fly. But it isn’t easy to do as it is like grasping at space objects or soap bubbles.
According to Ryan Hurd (a dream educator and author of several books) in The World of Lucid Dreaming:
This is the “carrot on the string” — one of the greatest self-limiting constructs in lucid dreamwork. Just when what you want is in reach, something yanks the string and you are left grasping at air. The reason this happens is because although we may crave lucid dream sex, the dream actually requires sexual connection.
Therefore, if interacting with your surroundings allows you to stay in a lucid dream, then training your mind to be with a person with whom you have a personal connection with rather than just lust over, can help to keep you in the lucid sex dream. Hmmm, maybe that’s why I have so many sex dreams about my own husband? I always ask myself why I don’t add some variety to my dreams, more regularly. (He assumes my sex dreams are about Greek Gods, but when I tell him they are most often about him, he doesn’t believe me.)
Hurd goes on to explain that the reason why lucid dreams are so sexually charged is that becoming aware of a dream often means becoming aware of our bodies' physiological arousal as we dream. There is also a connection between flying dreams and this physiological arousal which may explain why the two most popular lucid activities are flying and sex.
Now if learning how to lucid dream to move away from a recurrent dream, can heal your waking life, and vice-versa, imagine what tapping into lucid dreaming can do for sexual healing or repressed desires.
Perhaps sexual barriers and self-imposed limitations on your sexuality can be worked through to take out some of the kinks. Maybe taking out the kinks can help to release untapped fetishes, kinks, or barriers.
Imagine how liberating it can be to have influence over a sex dream? Maybe you want to go sexploring in your dreams. Or have an amazing orgasm that occurs simply due to your thoughts, rather than actual bodily stimulation. Because in fact, Hurd says:
Orgasms in lucid dreams can be real orgasms, accompanied by muscular responses and a quickened heart rate.
Actually, Hurd also explains, that all sensations can feel as real as the waking world, provided you have a working memory to fall back on and have felt those same sensations before. Wow, talk about some kind of muscle memory!
The main ideas from Hurd’s article in World of Lucid Dreaming stem from one of his books Big Dreams: Psi, Lucid Dreaming, and Borderland of Consciousness (Dream Like a Boss Part 2).
While there is a lot more research to support lucid dreaming beyond sexploring during a lucid dream, sex dreams are indeed one of the main reasons people want to learn this new skill. Through raising the level of consciousness in your sleep during a lucid dream to remain inside of it long enough to impact change, one can experience healing and shifts in mindset, and if nothing else, perhaps even result in a warm orgasm.
Lindsay Soberano-Wilson is on a dream tilt. Please enjoy dream-themed poetry:
psiloveyou.xyz
psiloveyou.xyz
medium.com
medium.com
medium.com
medium.com
Cited Sources
Hurd, Ryan. Everything You Want to Know About Lucid Dream Sex But Are Afraid to Ask. https://www.world-of-lucid-dreaming.com/everything-you-want-to-know-about-lucid-dream-sex-but-are-afraid-to-ask.html
Hurd, Ryan. Big Dreams: Psi, Lucid Dreaming, and Borderland of Consciousness (Dream Like a Boss Book 2). Dream Studies Press, 2014.
Resources for More Research:
www.economist.com
goeatacarrot.com
www.world-of-lucid-dreaming.com
A sex-positive community of essayists and poets
540 
4

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
540 claps
540 
4
Written by
Author of Casa de mi Corazon| Poet | Editor: Put It To Rest (Founder) & iPoetry. (BEd, MA). Sex-Positive Feminism| Mental Health| LGBTQ| lindsaysoberano.com
We publish essays, and poetry about sex, sexuality and erotic relationships.
Written by
Author of Casa de mi Corazon| Poet | Editor: Put It To Rest (Founder) & iPoetry. (BEd, MA). Sex-Positive Feminism| Mental Health| LGBTQ| lindsaysoberano.com
We publish essays, and poetry about sex, sexuality and erotic relationships.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/crustnetwork/crust-cloud-beta-open-for-testing-c64c7751008?source=search_post---------4,"There are currently no responses for this story.
Be the first to respond.
After a period of intense development, the Crust Network team is ready to launch the internal beta Crust Cloud.
Crust Cloud is a functional platform based on the Crust decentralized storage protocol. At present, it integrates an encrypted wallet, staking, cloud storage and other functions, and will also support the data market and more applications in the future.
We will start recruiting 200 testers from the Crust community today, and the Crust Cloud test will be officially launched on September 25. Interested users, please register as soon as possible, as registration will end when the applications reach 200.
Since the network and Crust Cloud applications are still in the testing phase, this test has the following restrictions:
1. User file size is limited to 100 MB
2. User storage order is valid for 3 days
3. If the user logs in to Crust Cloud from another device, the previously stored file records will not be displayed
4. Due to the small number of testnet nodes, each user will be limited to 10 actions for storage and 20 actions for download.
5. This test uses a whitelist. Participants need to go to Crust Apps to create an account, and put your account (token address) on the Google form (link). After the users are added to the whitelist, they will be able to import accounts on Crust Cloud and use Crust Cloud.
6. Each person can only claim 100 test tokens for this test.
Note: Due to the unstable test network stage, users may face file transfer failure or file loss, so please do not upload important files to the Crust Cloud beta version.
In this Crust Cloud beta, test users that report system bugs and optimization suggestions will be rewarded with CRU tokens.
User can submit their feedback to the following Google Form: https://forms.gle/QrxrqBeock1TJNiJA
Alternatively, submit your feedback to @CrustTyler on Telegram.
🔸 Official Website
🔸 Crust Twitter
🔸 Crust Facebook
🔸 Crust Telegram
🔸 Crust Discord
🔸 Crust Subreddit
Decentralized Cloud Blockchain Technology
3 
3 claps
3 
Decentralized Cloud Blockchain Technology
Written by
CRUST implements the incentive layer protocol for decentralized storage, and vision to building a decentralized cloud ecosystem.
Decentralized Cloud Blockchain Technology
"
https://medium.com/@edgecast/how-cdns-can-adapt-to-the-cloud-computing-era-cd4c41513fde?source=search_post---------5,"Sign in
There are currently no responses for this story.
Be the first to respond.
Edgecast
Dec 12, 2017·6 min read
“Build like the cloud; deliver like the cloud” is an internal mantra we have been using in our Development and Product teams at Verizon Digital Media Services for the last two years. Cloud computing, in which remote servers, virtualization, Infrastructure as a Service (IaaS), and orchestration tools make the deployment and scaling of applications easier, is quickly becoming the standard way that software is built, deployed and delivered.
This puts content delivery networks (CDNs) in something of a predicament. While customers crave the speed and ease of the cloud, they’re unwilling to compromise on the performance, reliability, security and scale advantages that purpose-built CDNs still provide. That means CDNs have to continue to meet and exceed customer expectations, while at the same time finding new ways to integrate with cloud computing platforms and tools.
Here are some ways in which we are adapting to keep pace with the cloud paradigm.
Build with cloud tools in mind.Customers have always used application program interfaces (APIs) to easily integrate their applications with third-party services. Our Edgecast Content Delivery Network differentiated itself early on with a foundation of enabling customers and partners to easily self-service their CDN accounts and configurations via portal-based tools and APIs. Increasingly though, customers are adopting cloud APIs and DevOps toolsets to integrate applications and other workloads into the cloud. As the application development life cycle progresses from development to testing to staging to production deployments, popular configuration management and orchestration tools such as Puppet, Chef, Salt, Terraform and others are used to automate and regulate these activities and stages.
One of the ways we are adapting the CDN to support these tools is by using them ourselves. While CDNs have always (and likely will always) push the envelope of performance by optimizing using bare metal servers and full-stack tuning, we have been using cloud environments for development, testing, staging, prototyping, data/analytics and other uses. This has made our own processes more agile and robust, while letting us learn the art and science of DevOps centric, testable-by-design, CI/CD-friendly development practices. In other words, it has taught us to build the way our customers build and to adapt our own configuration tools and interfaces to what our customers expect.
As we have adopted these practices, we have started building a set of interfaces and tools we are calling EdgeControlspecifically to provide this functionality to the Edgecast CDN. It was as natural as looking at what tools are popular and useful in open source and DevOps communities, looking at how they’re leveraged against compute, storage and network infrastructure, and seeing how we could fit our CDN into those tools and workflows more organically.
The cloud works in real time. CDNs should, too. It’s easy to see the cloud’s appeal to customers; it’s a faster way to deploy software than ever before. According to a Vanson Bourne report on the business impact of the cloud, there’s more than a 20 percent speed improvement in a software’s time to market when it’s hosted in the cloud. When an application is deployed into the cloud, it’s dynamic and quick, as well as adaptable; a developer can just as easily spin up one instance or multiple at the same time, often using automated orchestration and deployment tools.
A CDN needs to work just as quickly and efficiently. If a customer is launching an application in the cloud, it’s the CDN’s job to ensure it is incorporated into that custom automation and app deployment process at the same time. Testing the application and then testing the CDN shouldn’t be a two-step process for customers to consciously think about, but rather two things that can happen in tandem. The CDN can play an appropriate role in the same testing process, and when it happens concurrently, developers can get data back out of the CDN during testing for enhanced metrics and analytics monitoring. And having a responsive CDN during the initial testing step ensures that the testing environment replicates the eventual production environment as closely as possible: from how the software performs in different regions, to how it mitigates potential security issues, to how it handles load or offloads certain business logic to edge servers.
At Verizon, our increasing internal use of cloud infrastructure, tools and automation has shown us that testing the CDN as an afterthought can not only be tedious, but less efficient. EdgeControl was born out of discussing needs with our customers and partners, building to complement our own internal processes, and monitoring our competitors. It’s not enough to provide API hooks, configuration and propagation; these things also need to happen in real-time, so we’re working on making more of our components real-time as well, including configuration APIs, ingest and propagation and feedback.
Automate, automate, automate. Finally, it’s vital for a cloud-integrated, real-time, responsive CDN to have extensive automation capabilities. The more that the CDN is a secondary, manual step in the application deployment process, the less it is likely to be updated and in sync with application changes, the fewer automated-testing processes are likely to incorporate the CDN component (including any edge-deployed application logic), and the less reliable and predictable the end-to-end deployment process becomes.
From code testing to software deployment, the end goal should be to have customers do as little manually as possible. Instead, they ought to be able to make long-term configurations in the CDN to reflect a new content profile, a new application or API, or even just changes in the application environment, such as new regions being deployed, elasticity changes in deployed instances, or other characteristics of the origin/cloud environment that the CDN could or should be responsive to. Even better would be if that configuration could be tied to the customer’s existing software configuration tool so the same automation processes and testing tools work together. The ability to automate in advance doesn’t just make software deployment easier, but also more reliable, since it can be configured well before anything goes live on the CDN. This has the added benefit of fitting into the more dynamic, continuous integration deployment model that is becoming more common.
For an application that lives in the cloud, the CDN is the infrastructure that sits in front of it. That means when a customer updates the application’s code, the CDN should automatically be aware of the change, and trigger an update in its configuration if need be. Our EdgeControl toolset is being developed with an eye toward how that process can be scripted in advance through an automated deployment process to avoid having our customers do a secondary configuration. The results will be a quicker, more efficient and more reliable deployment process.
Many of these improvements are as natural as asking ourselves, “How can we build this to be more like the cloud?” The answer will be emerging in new capabilities and tools we are rolling out now and in coming months which include, improving our configuration propagation times from ~40–60 minutes to less than 5 — a more than 90 percent improvement); exposing more native configuration syntax to allow for the full power of our edge performance engine to be exposed to developers; expanding our APIs and command-line (CLI) tools to increase scripting and automation integration opportunities with the most common DevOps toolsets and frameworks; and taking the covers off of extensive metrics, analytics and other data that can provide real-time feedback on performance and utilization, etc. We’re excited about this coming transformation and the promise of enabling cloud applications and developers to interact with our platform more efficiently and natively.
To learn more about how our CDN acts like and integrates with the cloud get in touch with us today.
Formerly Verizon Media Platform, Edgecast enables companies to deliver high performance, secure digital experiences at scale worldwide. https://edgecast.com/
2 
2 
2 
Formerly Verizon Media Platform, Edgecast enables companies to deliver high performance, secure digital experiences at scale worldwide. https://edgecast.com/
"
https://medium.com/google-cloud/cloud-run-as-an-internal-async-worker-480a772686e?source=search_post---------6,"There are currently no responses for this story.
Be the first to respond.
If you’ve heard of Cloud Run, you already know that it’s great for spinning public endpoints inside stateless containers to handle HTTP request/reply type of workloads. And the best part is that you only pay for the duration of the request!
However, HTTP request/reply handling is not the only use-case for Cloud Run. Combined with Cloud Pub/Sub, Cloud Run is very well suited for internal async worker type use-cases because:
All of this means that you can have Pub/Sub push messages to an internal Cloud Run endpoint. This is perfect for an internal async worker! You can see how to set this up in detail Using Cloud Pub/Sub with Cloud Run Tutorial.
In this blog post, we will follow that tutorial but I also want to see what it takes to have one of my existing Knative Eventing samples and deploy as an internal async worker in Cloud Run.
In my Knative Tutorial, I show how to connect Cloud Storage to Cloud Vision API using Knative Eventing:
To summarize, when a user drops an image in a Cloud Storage bucket, it triggers a message to a Pub/Sub topic which in turn gets read by Knative Eventing. Once the event is read by Knative Eventing, it’s turned into a CloudEvent and passed to a Knative Service to make a Vision API call and extract labels out of the image.
What does it take to take this sample and run it on Cloud Run? Cloud Run implements Knative Serving, so it’s very easy to move a service from Knative Serving to Cloud Run. However, it does not implement Knative Eventing directly, so it’s a little more work to go from Knative Eventing to Cloud Run.
Let’s go through the steps needed to understand what’s needed. I’m assuming that you have a project ready for Cloud Run already.
First, we need to create a Pub/Sub topic for Cloud Storage notifications to go to:
Next, create a Cloud Storage bucket to store images. You can do this from Google Cloud console or use gsutil command line tool as follows:
Enable Pub/Sub notifications on the bucket and link to the previously created topic:
We’re now ready to deploy our Cloud Run Service. We need to push our image to Google Container Registry (GCR). Inside vision-csharp folder, there is already a Dockerfile. Run the following to submit it to Cloud Build:
This will build the image using Cloud Build and store to GCR:
Run the following to deploy to Cloud Run as vision-csharp service:
During deployment, respond “No”, to the “allow unauthenticated” prompt to keep the service private. In a minute or so, you should see the service in Cloud Run section of Google Cloud Console:
At this point, we have the bucket and the service ready but we need the Pub/Sub in the middle to forward messages. We need to configure Pub/Sub to push messages to our internal Cloud Run service. I’m following the Using Cloud Pub/Sub with Cloud Run Tutorial.
2. Create a service account to represent the Cloud Pub/Sub subscription identity:
3. Give this service account permission to invoke your CloudRun service:
4. Create a Cloud Pub/Sub subscription with the service account:
Admittedly, this is a lot of manual steps to setup Pub/Sub and I hope it gets easier going forward but once you go through it once, it kind of makes sense.
We’re finally ready to test our internal Cloud Run service! I go to the Cloud Storage bucket and drop an image:
I go to Cloud Run console and look at the logs of the vision-csharp service:
My service throws a NullPointerException! Turns out this is because the event types in Knative Eventing and Cloud Run is different. In Knative Eventing, you end up receiving CloudEvents and that’s what my code tries to parse. In Cloud Run, you receive a Pub/Sub push message which has a slightly different format. Once I adjusted my code to parse Pub/Sub push messages, I was able to make calls to Vision API.
In this blog post, we took a look at Cloud Run and how it can be used as an internal async worker. There are more manual steps than I expected but once setup, it works great. Combining Cloud Run with Cloud Pub/Sub opens up a lot of opportunities for Cloud Run to act like a background worker for pretty much any service.
Google Cloud community articles and blogs
46 
1
46 claps
46 
1
Written by
Developer Advocate at Google
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Developer Advocate at Google
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@bitkeepblog/how-to-use-bitkeep-cloud-wallet-to-make-an-internal-transfer-79771d2baedb?source=search_post---------7,"Sign in
There are currently no responses for this story.
Be the first to respond.
BitKeep
Nov 8, 2021·2 min read
BitKeep’s internal transfer functionality has gone live on November 8, 2021. BitKeep users may use cloud wallet addresses to transfer assets between different BitKeep Cloud wallets.
How to use BitKeep Cloud Wallet to make an internal transfer
2. Tap BKB to go to [BKB] page, then tap [Transfer] to jump to Transfer page, now select your desired chain, and input the address of receiver and transfer amount. Tap Confirm, then input your transfer password.
Note: This function is intended for internal transfers only. If the user fills in a non-Cloud Wallet address, then a dialogue box will pop up saying that the transaction cannot be completed.
How to use BitKeep Cloud Wallet to receive tokens with internal transfers
1. In Cloud Wallet, tap the token you’d like to receive. You’ll then go to the selected token page, now tap Receive;
2. Select the desired chain, and copy the address or save the QR code. Now share with other users, and you are ready to receive.
Thank you for reading!
Disclaimers: This tutorial is only a guide to BitKeep Cloud Wallet, and shall not be construed as BitKeep’s investment advice. Capital investments are subject to market risks. You may want to fully understand the related risks before making investment decisions at your own discretion.
Contact us:BitKeep: https://bitkeep.com/Twitter: https://twitter.com/BitKeepOSDiscord:https://discord.com/invite/qYTatUzNjMTelegram (Chinese): https://t.me/BitKeep_OfficialTelegram (English): https://t.me/bitkeepFacebook:https://www.facebook.com/BitKeep/Instagram: https://www.instagram.com/BitKeep/Youtube: https://www.youtube.com/channel/UCD2S03RS-Q08VdJqZpuOCFQ
BitKeep is your decentralized wallet. Providing a one-stop destination for DApps, the BitKeep Aggregate Exchange, Multichain support, Wealth Management and News
8 
8 claps
8 
BitKeep is your decentralized wallet. Providing a one-stop destination for DApps, the BitKeep Aggregate Exchange, Multichain support, Wealth Management and News
About
Write
Help
Legal
Get the Medium app
"
https://wtfeconomy.com/distributed-teams-the-next-big-economic-sea-change-14482d84fd75?source=search_post---------8,"The shift from “building your own datacenter” to “using the cloud” revolutionized how companies viewed internal infrastructure, and significantly reduced the barrier to starting your own global-scale company. WhatsApp, Instagram, github and airbnb are some famous examples, but there are many, many, others.
Like any new major change, “the cloud” went through an uphill acceptance curve with resistance from established nay-sayers. Meanwhile, smaller companies with no practical alternatives jumped in with both feet and found that “the cloud” worked just fine. And scaled better. And was cheaper to run. And was faster to setup, so the opportunity-cost was significantly reduced.
(Personally, I dislike the term “cloud” but it’s the easiest vendor-neutral term I know for describing essential infrastructure running on Amazon AWS, Google GCE, Microsoft Azure and others…)
Today, of course, “using the cloud” for your infrastructure has crossed the chasm. It is the default. Today, if you were starting a new company, and went looking for funding to build your own custom datacenter, you’d need to explain why you were not “using the cloud”.
Rethinking infrastructure from the fixed costs of servers and datacenters to rented by the hour “in the cloud” is an industry game changer. Similarly, rethinking the other expensive part of a company’s infrastructure — the physical office — is a game changer.
Deciding to setup an office is an expensive decision which complicates, not liberates, the ongoing day-to-day life of your company.
If you convince yourself that your new company needs a physical office, you quickly get distracted by crucial topics that are not about the shipping product per se, and are instead about the operational mechanics of a physical building. Deciding to have your own physical data center involves one-time-setup costs as well as ongoing recurring operational costs. Similarly, deciding to have a physical office involves one-time-setup costs as well as ongoing recurring operational costs.
There’s plenty of information available on why distributed teams and working-from-home, is better for the mental and physical health of the individual humans. Even on why reduced commuting is good for the environment. By contrast, there is relatively little about why distributed teams are also better for the organization. Thankfully, this is changing. Scott Berkun wrote a book about Wordpress/Automattic. Jason Fried and DHH wrote a book about Basecamp/37signals. Both are well-known, large, profitable, companies built with very distributed teams, and it’s great to see them talking publicly about how they did it. I’m writing my own book “Distributed” based on my experiences. (The publishers, O’Reilly are also a distributed organization!) The more people describe what did, and didn’t, work for them, the easier it will be for other companies to start growing distributed teams.
For organizations, I believe the competitive advantages are too huge to ignore:
A few years ago, “using the cloud” was new. People with “build your own datacenter” experience needed to change how they worked “in the cloud” — while people who had never worked in either environment thought “using the cloud” was normal. Now, of course, “using the cloud” has crossed the tipping point.
Similarly, working in distributed teams is still new(ish) for some people. People who have experience working and managing all-in-one-location companies may need to change how they work and lead in distributed companies — while people who have never worked in either environment think that “distributed teams” are normal.
Technology is helping with this transition. Products like Slack, Google Hangouts, Skype/Lync make complex communication technologies easily available to all. These technologies help all-in-one-office organizations be more efficient, and help make distributed organizations possible. Products like linkedin.com, upwork.com and flexjobs.com enable people to have distributed careers — recreating the trusted friend-of-a-co-worker network of loose connections that grew organically whenever people from different companies rubbed shoulders at the neighborhood deli lunch counter or conferences or cross-company projects or volunteering events.
All of these new technologies, along with the super-hot hiring crisis in some locations, is encouraging more and more companies to try “distributed teams”. This sea-change towards distributed teams has impact rippling out across multiple industries, society in general and our global economy. Literally change-the-world stuff!
With all this going on, the upcoming Next:Economy conference should be exciting.
How work, business, and society face massive…
118 
6
Some rights reserved

118 claps
118 
6
Written by
#RelEng at scale @CivicActions. Author of #distributedteams. Mentor. Ex-@ObamaWhiteHouse, @usds, @codeforamerica, @Hortonworks, @Mozilla, @Oracle.
How work, business, and society face massive, technology-driven change. A conversation growing out of Tim O’Reilly’s book WTF? What’s the Future and Why It’s Up To Us, and the Next:Economy Summit.
Written by
#RelEng at scale @CivicActions. Author of #distributedteams. Mentor. Ex-@ObamaWhiteHouse, @usds, @codeforamerica, @Hortonworks, @Mozilla, @Oracle.
How work, business, and society face massive, technology-driven change. A conversation growing out of Tim O’Reilly’s book WTF? What’s the Future and Why It’s Up To Us, and the Next:Economy Summit.
"
https://medium.com/google-cloud/internal-load-balancing-for-kubernetes-services-on-google-cloud-f8aef11fb1c4?source=search_post---------9,"There are currently no responses for this story.
Be the first to respond.
As discussed in my recent post on kubernetes ingress there is really only one way for traffic from outside your cluster to reach services running in it. You can read that article for more detail but the tl;dr is that all outside traffic gets into the cluster by way of a nodeport, which is a port opened on every host/node. Nodes are ephemeral things and clusters are designed to scale up and down, and because of this you will always need some sort of load balancer between clients and the nodeports. If you’re running on a cloud platform like GKE then the usual way to get there is to use a type LoadBalancer service or an ingress, either of which will build out a load balancer to handle the external traffic.
This isn’t always, or even most often what you want. Your case may vary but at Olark we deploy a lot more internal services than we do external ones. Up until recently the load balancers created by kubernetes on GKE were always externally visible, i.e. they were allocated a non-private IP that is reachable from outside the project. Maintaining firewall rules to sandbox lots of internal services is not a tradeoff we want to make, so for these use cases we created our services as type NodePort, and then provisioned an internal TCP load balancer for them using terraform.
Creating an external load balancer is still the default behavior, but fortunately there is now a way to annotate a LoadBalancer service so that Google Compute Engine will build out an internal load balancer with a private IP. When this feature was first released a couple of months ago it didn’t work right, and so we continued to wire this stuff up manually. It’s since been fixed and I was recently able to test it and confirm that it works, so now seems like a good time to take a quick look at what this can do for you. Here’s a sample service:
As you can see above the change is a simple one-liner annotation. Assuming we have some endpoints (pods) to handle this traffic, then when we create this service we will, after a minute or two, see an internal lb pointed at it:
The ‘EXTERNAL_IP’ in that column header means “outside the cluster,” and as you can see the allocated address is in the private address space of the project. Pretty cool. There are a couple of limitations to internal load balancers, however, that you should be aware of. They are regional devices and can’t handle traffic to/from vms in other regions. They are layer 4 and can’t handle http/s traffic at layer 7, meaning no virtual hosts, path-based routing, tls termination, etc. They can only handle a maximum of five ports. After that you’ll need to make another lb. That last one isn’t much of a restriction since as far as I know kubernetes will drive GCE to create a new internal lb for each service port anyway. That’s something I’ll try to test and report back on later.
In any case, this is nice feature to have and will make it easier for us to spec and build out the networking stack for our kubernetes services from within our helm chart repositories, rather than having to maintain a separate terraform configuration that relies on the manual discovery of backend instance groups in order to properly configure an internal load balancer. As always when it comes to configurations less is more.
Google Cloud community articles and blogs
161 
5
161 claps
161 
5
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Senior Devops Engineer at Olark, husband, father of three smart kids, two unruly dogs, and a resentful cat.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/kubernetes-routing-internal-services-through-fqdn-d98db92b79d3?source=search_post---------10,"There are currently no responses for this story.
Be the first to respond.
I remember when I was first getting into Kubernetes. Everything was new and shiny and about scale. As I continued making Cloud Native Applications running on Kubernetes I found some small paragraph that stated that Kubernetes has a built in DNS server.
Of course it does, that makes so much sense.
But now with a built in DNS server this opens up so many opportunities. Routing and masking routes in new and complex ways while still within our cluster.
In this article we are going to look at how you can make custom routes within your cluster to simplify your inter cluster communication.
If you haven’t gone through or even read the first part of this series you might be lost, have questions where the code is, or what was done previously. Remember this assumes you’re using GCP and GKE. I will always provide the code and how to test the code is working as intended.
medium.com
When calling external services you may be used to writing fully qualified domain names (FQDN), like the following.
However when you are making requests within your cluster, how would you expect it to work? The pods are ephemeral, so the URL will change as often as the pod is created and destroyed. Not a solution.
You could use the external URL that is available from the load balancer exposed by the service. But then you’d be making an extra hop and wasting time and processing.
If we were wanting to communicate with our other services without making an unnecessary hop then you just need to use the internal communication scheme built into Kubernetes. Looking at part of the service yaml file we can pull out the values for the FQDN.
For this service, due to the service name, the FQDN would be:
The other parts?
This is the namespace of the pods that we are targeting. Since I didn’t set a namespace the namespace is default.
You can also shorten the FQDN by removing the svc.cluster.local. Leaving you with:
As you will see from the example provided below, it is really simple to insert parameterized routing into your application. This is extremely helpful with Kubernetes as you might want to have slightly different routing based on environments or other rules.
I’ve created an example project to highlight this feature. For this example I used pod environment variables and a single application to inject the necessary variables into the application so we can see how one service can call another.
And in the application code the values are injected to customize the code.
In this code I am using service-1 to call service-2 through the /foreign endpoint. I also set up the reverse so that service-2 can call service-1. You can run the code by running the following command in Cloud Shell.
This will produce an IP Address for service-1 when it is ready. If you hit the /foreign end point you will see the following result.
http://[service-1 IP Address]/foreign
You’ll see that service-1 calls service-2 directly just as easy as hitting any other endpoint. This wonderful magic makes it just a little easier when building your microservices.
Before you leave make sure to cleanup your project so you aren’t charged for the VMs that you’re using to run your cluster. Return to the Cloud Shell and run the teardown script to cleanup your project. This will delete your cluster and the containers that we’ve built.
medium.com
medium.com
itnext.io
medium.com
Jonathan Campos is an avid developer and fan of learning new things. I believe that we should always keep learning and growing and failing. I am always a supporter of the development community and always willing to help. So if you have questions or comments on this story please ad them below. Connect with me on LinkedIn or Twitter and mention this story.
Google Cloud community articles and blogs
90 
3
90 claps
90 
3
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Excited developer and lover of pizza. CTO at Alto. Google Developer Expert.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://hi.stamen.com/here-xyz-bdd29e3d4a16?source=search_post---------11,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
For the last couple of months, Stamen Design has been lucky to have access to an internal beta of HERE XYZ, the powerful new mapping solution from HERE Technologies. XYZ is now available for public beta, so you can try it too! As you explore XYZ, we believe our experiences can serve as a guide to understanding the range of possibilities.
First, XYZ is built around the XYZ Hub API. One of the key concepts it offers is the notion of an “XYZ Space”. You can think of XYZ Spaces as extremely flexible geospatially enabled databases hosted in the cloud. At Stamen we really loved how easy it was to use the XYZ Hub API to connect XYZ spaces with a wide range of existing libraries, and in particular we loved how XYZ is agnostic about what front-end mapping libraries we wanted to use. In our exploration we visualized XYZ data using several of our favorite mapping tools: Leaflet, Tangram, and even a bit of flashy three.js.
Over the course of our experimentation, we built four prototypes of increasing complexity, to demonstrate the capabilities of XYZ.
First, like any good web mapping platform, XYZ includes a graphical user interface called XYZ Studio. Without any code at all, you can upload data to XYZ, manage your spaces, and make a simple map. We tested out XYZ Studio by uploading some recent earthquake data. Within a matter of minutes, we had a shareable map.
Querying XYZ spaces is organized around the concept of “tags”, which you can define at upload time or afterward, based on any attributes of your data. For our next map we wanted to test out this ability to query slices of data using arbitrary tags. We ended up with a Leaflet map of air pollution measurements for the city of Madrid. Try out the live map and see what happens when you select a new date or a different air quality indicator.
You’re also not limited to a single XYZ Space for all of your data. You can create multiple spaces (analogous to separate databases or different database tables) and access each of them with their own API endpoints. As we scaled up our experiments, we ran into limitations on the number of points Leaflet can render and on the size of our requests. So, for our next map, we tried out XYZ’s capability to serve up data as vector tiles, which we displayed in a multi-layer Tangram map. In this map, you can explore the solar power generation in various neighborhoods in Amsterdam, or draw a bounding box and count the number of trees in any part of the city.
Finally, we wanted to push the limits of what was possible with XYZ, both on the back-end and with our front-end visualization. We set up a script to continually query a public feed of aircraft locations, and to push these automatically into an XYZ Space. Then we used THREE.js to draw planes in three dimensions above a spinning Earth. Take the app for a spin, and if you keep it open long enough you should see the positions of the plane icons update in near-real-time.
We also created four tutorials explaining how you can make your own maps like these. Yes, you!
In all, we’re pretty impressed with XYZ Spaces and we are eager to see what new features HERE keeps adding. So, what are you waiting for? Try XYZ for yourself now!
Designers and technologists specialized in data…
122 
1
122 claps
122 
1
Written by
Lead Cartographer at @stamen / election reformer @FairVoteWA / founder @LocalgroupBham. Maps, networks, visualization, code. 15 min of fame: @pop_vs_soda
Designers and technologists specialized in data visualization and maps since 2001. Visit us @ stamen.com.
Written by
Lead Cartographer at @stamen / election reformer @FairVoteWA / founder @LocalgroupBham. Maps, networks, visualization, code. 15 min of fame: @pop_vs_soda
Designers and technologists specialized in data visualization and maps since 2001. Visit us @ stamen.com.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/aws-activate-startup-blog/three-internal-software-tools-every-startup-should-use-596a5290efb2?source=search_post---------12,"There are currently no responses for this story.
Be the first to respond.
Guest post by Vanessa Kruze of Kruze Consulting
At Kruze Consulting, we’re a CPA-led team that handles the Accounting, Finance, and HR for over 160 startups. We’ve seen the good, the bad, and the ugly of startup operations, and we have come up with a simple solution: We believe in software.
By leveraging best of breed software providers, startups can run smoothly and focus on building their business. It sounds so simple, but every day we meet startups that haven’t internalized these systems and are a complete mess operationally. That’s ok though, because it’s never too late to set up your startup for success.
We recently led an evening discussion at the NYC AWS Pop-up Loft: “A Prescription for Startup Financial Health — 8 Financial Best Practices for Startup Executives.” In this post, we continue that conversation and share three of our favorite software systems for startups.
We’ve used almost every payroll processor and can confidently say that Gusto (formerly Zenpayroll) is the best, for a number of reasons:
We absolutely love Bill.com. It does what bank payment systems should be doing. We always recommend Bill.com because it can:
The only drawback with Bill.com is that the QuickBooks sync is critical and sometimes breaks. If sync breaks (which is rare, we’re talking a 5% chance), you’ll be spending some time with customer support. However, Bill.com recently beefed up its customer support, so service levels have vastly improved.
Zenefits is Kruze Consulting’s favorite HR Management solution. The company is automating Healthcare Benefits, one of the most difficult and complex offline services any startup has to touch. Zenefits is a young company and is still improving its service offering, but it has already made huge strides in reliability and ease of use. Zenefits has made our clients lives much easier:
The only drawback with Zenefits is that they are tackling a very complicated problem and sometimes the insurance carrier systems don’t talk to Zenefits’ systems as seamlessly as everyone would like. Both the carriers and Zenefits are investing a lot of time, energy, and capital into improving the whole experience though.
At Kruze Consulting our experience has taught us an important principle: All startups seek ways to maximize efficiency and cut costs, so they can focus on business growth while they develop their next big idea. Automating internal processes with help from these tools will further simplify operations, giving you time to do what you do best: grow your business.
Kruze Consulting will be presenting “Startup Financial Best Practices for the New Year” at the San Francisco AWS Pop-up Loft on January 21st. You can register here for this event.
For startups building on AWS.
25 
2
25 claps
25 
2
For startups building on AWS.
Written by
Amazon Web Services Startup Program. Follow @AWSstartups.
For startups building on AWS.
"
https://medium.com/@duhroach/internal-ip-vs-external-ip-performance-76f15a650356?source=search_post---------13,"Sign in
There are currently no responses for this story.
Be the first to respond.
Colt McAnlis
Jul 27, 2017·4 min read
“We’re not getting the throughput on our VMs. I think the bald guy in those videos is lying.”
Calls with customers are fun.
Google has a fantastic crew of dedicated people to help you get to the bottom of your cloud problems. I was lucky enough to sit in on a call with “Gecko Protocol” a B2B company offering a custom, light-weight networking protocol built for gaming and other real-time graphics systems.
They reached out to our fantastic support team since they were seeing lower-than-expected throughput for their backend machines which were responsible for transferring and transcoding large video & graphics files.
Here’s the graph they shared with us:
Truth is, yes, those are a lot smaller numbers than I’d expect. Let’s dig in a bit more and see what’s going on.
I grabbed the configuration data from their engineering director, and duplicated a test on my side of the fence. Since they were testing throughput, we can test it with iPerf. Following the same thing we did in the other article, we simply need to setup iPerf on the boxes, designate one of them as a server, and point the 2nd one at it.
1.95GB / sec was much higher than what Gecko Protocol was seeing in their graphs. Just to sanity check some things, I jumped on a quick video chat with their engineering team, and tried to get them to reproduce this test.
After about 20 minutes of “I still don’t see the same numbers” the reason for the problem suddenly appeared.
While setting up their tests, the engineer driving the setup made one disconnect in our discussion. He was testing the external IP of the server, while I was testing the internal IP.
I switched over to testing the external IP in my tests, and got the same results as Gecko Protocol was, much slower.
We see the difference is 1.066 gb / sec between using internal vs. external IPs in this test.
At this note, the team quickly scrambled : One of their engineers realized they were using external IPs for all their backends, even when transferring data within the same zone; and with this difference in throughput, it’s clear to see a bottleneck.
While Gecko Protocol was fixing up their target IPs, I decided to run another test. Something was wrong with the numbers we were looking at, since I know from experience that the networking latency between instances on the same zone should be significantly higher; not to mention that the Gecko Protocol group was seeing about ~1.7 gb/ sec, but our tests were topping out at ~1.06 gb / sec.
Having just found a throughput problem with the Dobermanifesto group, I decided to check a higher core instance and see if that would get us closer to the numbers they were seeing.
Sure enough, running a 16vCPU machine, doing same-zone transfer, on an external IP showed exactly the bandwidth that GeckoProtocol was seeing:
When we switched over to using the internal IP for the same test, on the larger machine, the bandwidth went through the roof:
The difference was 14.21 Gbits/sec between the internal & external IPs, using the right CPU configuration and same-zone transfer.
For Gecko Protocol, this small debugging session resulted in their video and graphics data transfer improving by ~14x. Which is immense, considering their offering is built on performance backend services for high-performance compute scenarios.
DA @ Google; http://goo.gl/bbPefY | http://goo.gl/xZ4fE7 | https://goo.gl/RGsQlF | http://goo.gl/4ZJkY1 | http://goo.gl/qR5WI1
See all (78)
94 
94 claps
94 
DA @ Google; http://goo.gl/bbPefY | http://goo.gl/xZ4fE7 | https://goo.gl/RGsQlF | http://goo.gl/4ZJkY1 | http://goo.gl/qR5WI1
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/istio-grpc-loadbalancing-with-gcp-internal-loadbalancer-ilb-1b058b517640?source=search_post---------14,"There are currently no responses for this story.
Be the first to respond.
A couple weeks back I started looking at how to setup and expose an Istio service on GKE through a GCP Internal (and external) LoadBalancer. I did work a fair amount on gRPC with GKE (example at the end of this article), and my experience so far with Isito was just HTTP-based traffic. Coincidently, a different customer asked about how to setup a generic GCP-GKE Internal LoadBalancer for ingress traffic with Istio. After i got that bit working, i started tackling how to augment that with gRPC.
This article covers how to setup Istio on GKE, then expose an Internal (and External) LoadBalancer for gRPC traffic.
After setting this up, the gRPC traffic was (expectedly) automatically LoadBalanced _between_ pods in cluster. I expected this but to see Istio automatically loadbalance individual RPCs sent via one channel was really nice.
I’ve documented the steps here:
github.com
Anyway, lets get started (you can follow the git link above or to repeat here inline:
and most critically, set
(its not enabled by default on Istio 1.0.5 atleast)
What that’ll do is just give you the external and internalLB IP addresses (note them down; you’ll also find it on the GCP console as above)
The source code for the sample application is in the apps/ folder for this repo. Thats a simple app that creates one gRPC Channel to the server and on that one connection, sends 10 RPC requests.
The image you’re deploying is salrashid123/grpc_backend…you ofcourse don’t have to deploy that app as-is; you can look at the source then build and upload to your own repo!
Now that w’ere setup, lets test
You should see responses from different service addresses:
Each response shows the hostname/pod that handled the request. In this example, the responses come from different pods, round-robin (gRPC Loadbalancing!)
Ok, now we need to verify that we can connect via internal LB.
First create a GCP VM within the same VPC. SSH in and run.
Once in the VM, install Docker and run the following (remember to set the environment variable for $ILB_GATEWAY_IP
You should see responses from different service addresses:
If you deployed and setup Kiali, you should see traffic inbound from the ILB and external gateway:
So..thare you have it, gRPC loadbalacing from external and internal traffic to a service inside Istio. If you’re interested in generic GKE gRPC loadbalancing setups, please see the examples below.
Appendix
medium.com
medium.com
medium.com
medium.com
Google Cloud community articles and blogs
51 
51 claps
51 
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://levelup.gitconnected.com/internal-dns-using-route53-and-wireguard-a4d6196f9481?source=search_post---------15,"You have 1 free member-only story left this month. Sign up for Medium and get an extra one
Wireguard is fantastic. It's easy to use, easy to configure, and blazing fast. Wireguard has been rising in popularity since it was first introduced and with its inclusion into the Linux kernel, usage is only going up.
If you’re using Wireguard at home or with a small office, it's easy to manage endpoints and hosts across the tunnel by hand. If you’re implementing Wireguard in a larger enterprise you would likely want to use some form of internal DNS to handle many hostname entries.
Using a local DNS server like Bind or Windows Server DNS is fine for those on-premises resources, but what if you don’t have on-premises servers?
Implementing an internal DNS resolver with Wireguard in AWS is straightforward and makes life a whole lot easier managing all those resources. Let’s take a look at how to get setup using Wireguard with AWS Route53 DNS.
This article assumes that you have already implemented Wireguard on client hosts and on at least one instance in AWS EC2. You should be able to reach the EC2 instance and the VPC that instance resides in across the Wireguard tunnel.
If you’re currently only able to reach the EC2 instance terminating the other end of the tunnel, you may need to enable routing on that instance. You can do this on most Linux distributions by editing /etc/sysctl.conf and adding:
If you need help setting up Wireguard from scratch the Wireguard team has a great quick start available here.
In order to add internal DNS entries in Route53 you’ll need to make a private hosted zone. Visit the Route53 console and select Hosted Zones to get started.
Choose a new domain name (internal only) and give it a TLD that differentiates it from other public zones. The “.internal” domain is frequently used for this purpose. This is the fun part, you get to choose any internal domain name you want because it won’t resolve publicly.
Ensure you’ve selected “Private Hosted Zone for Amazon VPC” as the type and then enter your VPC ID you want the zone associated with.
Once the new zone is created feel free to add some records to it. You can add some “A” records or “CNAMEs” pointing towards instances or services inside the VPC.
More details on configuring a private hosted zone are available from Amazon here.
In order to actually resolve your newly created DNS names, you’ll need to point towards a resolver IP. In order to resolve internally, you need an “Inbound Resolver Endpoint”.
The endpoint is essentially an IP address inside your chosen VPC that acts as a resolver for the private zone. This is similar to when you change a client’s DNS settings towards Google’s 8.8.8.8 resolver, but instead you’re pointing towards your own cloud resolver across the tunnel.
Go ahead and create a new inbound endpoint (inbound means resolver requests will come from your private network towards the VPC).
After naming the new endpoint, selecting the VPC it should reside in and assigning security groups we need to provide IP addresses for the endpoint. You do have the option to generate IPs automatically, but since we want to always point clients at the same resolver let’s choose some static IPs here.
Make sure you use valid IP addresses from within each subnet and Availability Zone and don’t overlap with other in-use addresses.
After completing the IP address section you should be ready to complete the endpoint setup process. After clicking “submit” you will need to wait until the endpoint becomes fully operational.
Once you’ve got the green light its time to move on and configure the Wireguard client to resolve using the new endpoint.
If you’re working on a Mac and using the Wireguard application available in the App Store (which you should) configuration is simple. Open up the Wireguard application and click “Edit” on your existing VPN tunnel.
You will need to add a DNS entry under the Interface section. Replace the <INBOUND_RESOLVER_ENDPOINT> with the IP address of your new endpoint like so:
Once the configuration is updated go ahead and save. The tunnel should re-establish connections automatically.
If you’re working on Linux the configuration can vary depending on how you establish tunnels. To use the DNS key you’ll need to use wg-quick to establish the connection. If you are working on a Debian-based distribution the directions available here outline the necessary steps. Referencing Step 2: Alternative D should provide the correct connection method.
Once the Wireguard configuration has been updated and the tunnel restarted you should now be able to perform queries. Try running dig or ping on a DNS record in the private zone to ensure resolution is happening properly.
Alternatively, you can also try connecting to instances directly using their FQDN via SSH.
If everything worked you should be able to resolve DNS records from your private zone across the Wireguard tunnel. Now you can add any internal-only resources to that zone and clients will only be able to resolve them when connected to the tunnel.
Thanks for reading! If you enjoyed learning more about configuring Wireguard check out wireguard.com for the latest news and updates.
Coding tutorials and news.
55 
1
55 claps
55 
1
Written by
Software Engineer @mixhalo & die-hard Rubyist. Amateur Radio operator with a love for old technology. Tweet at me: https://twitter.com/@Tate_Galbraith
Coding tutorials and news. The developer homepage gitconnected.com && skilled.dev
Written by
Software Engineer @mixhalo & die-hard Rubyist. Amateur Radio operator with a love for old technology. Tweet at me: https://twitter.com/@Tate_Galbraith
Coding tutorials and news. The developer homepage gitconnected.com && skilled.dev
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/crustnetwork/crust-decentralized-storage-market-technology-internal-testing-has-started-come-and-store-your-296f211ffbc4?source=search_post---------16,"There are currently no responses for this story.
Be the first to respond.
In order to let users become familiar with the Maxwell testnet, the Crust development team has integrated support for IPFS in Crust Apps. Users can perform IPFS related operations on the corresponding page of Crust Apps. At the same time, the decentralized storage market is expected to go live in early March, when the storage market function will be integrated with the IPFS function of Crust Apps.
We will begin recruiting internal testing users in the Crust community from today, and the number of places is unlimited. The deadline for the internal testing activity is by the end of February, and depends on the testing situation. We welcome everyone to register for the activity.
For users that participate in the internal testing of storage market tech, please submit Issues on GitHub (https://github.com/crustio/crust/issues/new). After that, please send a screenshot of the Bug to the Telegram admin.
After the activity, Crust will distribute 3 levels of rewards depending on the situation:
1. For users that find simple errors, such as typos, graphic discrepancies, etc. (referring to a page), they will earn a reward of 1,000 Candy.
2. For users that find more complex errors, such as application layer operation process issues, they will earn a reward of 5,000 Candy.
3. For users that find issues about the protocol layer, they will earn a reward of 10,000 Candy.
Submit a registration form (https://forms.gle/LDjAgafQWwo2jZXA7), we will review and contact you within 5 working days.
Take the MacOS version as an example to test
1. Download IPFS Desktop
1) Crust Apps website: https://apps.crust.network/#/explorer
Login to your Crust account, and then click on “IPFS”.
2) Download IPFS desktop, choose your system.
3) You will see this page when you successfully download and log in IPFS.
2. Import a file
1) Click on “Files”, choose “Import”-“File”
2) Pick up the file you want to upload
3. Place an order (only accounts in the whitelist can place orders now)
1) Select the file you uploaded and click “Order”.
2) Click “Confirm”.
3) Enter your password, click “Sign and Submit”.
4) After successfully placing an order, you will see this prompt in the upper right corner.
4. Choose “IPFS” — “Storage Market”, you can find your order here.
The Crust network will store files after a few blocks, but due to the state of the chain, it will be updated every 30–60 minutes.
Waiting patiently and you will see the number of copies that Crust has stored in the figure below.
At this time, Crust has completed the storage work.
5. Download files.
1) Select the “Storage Market”, you can just download your file or share with other people. Copy the CID and share it.
2) After the other one receives the CID, click “Files” -”Import”-”From IPFS”
3) Paste the CID into the blank space and click “Import” to finish
4. Your CID is QmSjo9f26ETHWyjcEEtXdLMnQUe3HWhfRCooKqEbhh8cUx, you can also access it through the link: ipfs.io/ipfs/QmSjo9f26ETHWyjcEEtXdLMnQUe3HWhfhRCookUxb.
Decentralized Cloud Blockchain Technology
57 
57 claps
57 
Written by
CRUST implements the incentive layer protocol for decentralized storage, and vision to building a decentralized cloud ecosystem.
Decentralized Cloud Blockchain Technology
Written by
CRUST implements the incentive layer protocol for decentralized storage, and vision to building a decentralized cloud ecosystem.
Decentralized Cloud Blockchain Technology
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/the-internal-startup/a-guide-on-cloud-infrastructure-administration-2a6e58dd9416?source=search_post---------17,"There are currently no responses for this story.
Be the first to respond.
As more technology projects are being migrated into and being built in the cloud today, there needs to be a team managing the cloud infrastructure. I was part of…
"
https://medium.com/@alibaba-cloud/kubernetes-persistent-storage-process-97372f86089d?source=search_post---------18,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Sep 2, 2020·14 min read
By Sun Zhiheng (Huizhi), Development Engineer at Alibaba Cloud
Before explaining the Kubernetes storage process, let’s review the basic concepts of persistent storage in Kubernetes.
Kubernetes introduces PVs and PVCs to allow applications and developers to request storage resources properly without concerning storage device details. Use one of the following ways to create a PV:
Let’s use the shared storage of a network file system (NFS) as an example to explain the differences between the two PV creation methods.
Statically Create a PV
The following figure shows the process of statically creating a PV.
Step 1) A cluster administrator creates an NFS PV. NFS is a type of in-tree storage natively supported by Kubernetes. The YAML file is as follows:
Step 2) A user creates a PVC. The YAML file is as follows:
Run the kubectl get pv command to check that the PV and PVC are bound.
Step 3) The user creates an application and uses the PVC created in Step 2.
The NFS remote storage is mounted to the /data directory of the NGINX container in the pod.
Dynamically Create a PV
To dynamically create a PV, ensure that the cluster is deployed with an NFS client provisioner and the corresponding StorageClass.
Compared with static PV creation, dynamic PV creation requires no intervention from the cluster administrator. The following figure shows the process of dynamically creating a PV.
The cluster administrator only needs to ensure that the environment contains an NFS-related StorageClass.
Step 1) The user creates a PVC and sets storageClassName to the name of the NFS-related StorageClass.
Step 2) The NFS client provisioner in the cluster dynamically creates the corresponding PV. A PV is created in the environment and bound to the PVC.
Step 3) The user creates an application and uses the PVC created in Step 2. This step is the same as Step 3 of statically creating a PV.
The following figure shows the process of Kubernetes persistent storage. This figure is taken from the cloud native storage courses given by Junbao.
Let’s take a look at the steps involved in the process.
1) A user creates a pod that contains a PVC, which uses a dynamic PV.2) Scheduler schedules the pod to an appropriate worker node based on the pod configuration, node status, and PV configuration.3) PV Controller watches that the pod-used PVC is in the Pending state and calls Volume Plugin (in-tree) to create a PV and PV object. The out-of-tree process is implemented by External Provisioner.4) AD Controller detects that the pod and PVC are in the ‘To Be Attached’ state and calls Volume Plugin to attach the storage device to the target worker node.5) On the worker node, Volume Manager in the kubelet waits until the storage device is attached and uses Volume Plugin to mount the device to the global directory `/var/lib/kubelet/pods/[pod uid]/volumes/kubernetes.io~iscsi/[PVname]` (iscsi is used as an example).6) The kubelet uses Docker to start the containers in the pod and uses the bind mount method to map the volume that is mounted to the local-global directory to the containers.
The following diagram shows a detailed process:
The persistent storage process varies slightly depending on different Kubernetes versions. This article uses Kubernetes 1.14.8 as an example.
The preceding process map shows the three stages from when a volume is created to when it is used by applications: Provision/Delete, Attach/Detach, and Mount/Unmount.
Provisioning Volumes
PV Controller Workers
PV Status Changes (UpdatePVStatus)
PVC Status Changes (UpdatePVCStatus)
Provisioning Process (Assuming a User Creates a PVC)
Static volume process (FindBestMatch): PV Controller selects a PV in the Available state in the environment to match to the new PVC.
Dynamic volume process (ProvisionVolume): The dynamic provisioning process is initiated if no appropriate PV exists in the environment.
claim.Annotations[""volume.beta.kubernetes.io/storage-provisioner""] = storageClass.Provisioner.
""pv.kubernetes.io/bound-by-controller""=""yes"" and ""pv.kubernetes.io/provisioned-by""=plugin.GetPluginName()"".
Deleting Volumes
The deleting volumes process is the reverse of the provisioning volumes process.
When a user deletes a PVC, PV Controller changes PV.Status.Phase to Released.
When PV.Status.Phase is set to Released, PV Controller checks the value of Spec.PersistentVolumeReclaimPolicy. If it is set to Retain, it is skipped. If it is set to Delete, then either of the following options is executed:
Attaching Volumes
Both the kubelet and AD Controller perform the Attach and Detach operations. These operations are performed when kubelet if — enable-controller-attach-detach is specified in the startup parameters of the kubelet. Otherwise, these operations are performed by AD Controller. The following section explains the Attach and Detach operations using AD Controller as an example.
Two Core Variables of AD Controller
The Attaching Process
AD Controller initializes DSW and ASW based on the resource information in the cluster.
AD Controller has three components that periodically update DSW and ASW.
Detaching Process
a) In-tree Detaching: 1) AD Controller implements the NewDetacher method of the AttachableVolumePlugin interface to return a new detacher. 2) AD Controller calls the Detach function of the detacher to perform the Detach operation on the volume. 3) AD Controller updates ASW.b) Out-of-tree Detaching: 1) AD Controller calls the in-tree CSIAttacher to delete the related VolumeAttachement object. 2) External Attacher watches the VolumeAttachement (VA) resource in the cluster. If a data volume needs to be deleted, External Attacher calls the Detach function and calls the ControllerUnpublishVolume interface of the CSI plug-in through gRPC. 3) AD Controller updates ASW.
Volume Manager
It has two core variables:
The mounting and unmounting processes are as follows:
The global directory (global mount path) is a block device mounted to the Linux system only once. In Kubernetes, a PV may be mounted to multiple pod instances on a node. A formatted block device is mounted to a temporary global directory on a node. Then, the global directory is mounted to the corresponding directory of the pod by using the bind mount technology of Linux. In the preceding process map, the global directory is /var/lib/kubelet/pods/[pod uid]/volumes/kubernetes.io~iscsi/[PVname].
VolumeManager initializes DSW and ASW based on resource information in the cluster.
VolumeManager has two components that periodically update DSW and ASW.
UnmountVolumes ensures that the volumes are unmounted after the pod is deleted. All the pods in ASW are traversed. If any pod is not in DSW (indicating this pod has been deleted), the following operations are performed (VolumeMode=FileSystem is used as an example):
1) Remove all bind-mounts by calling the TearDown interface of Unmounter, or calling the NodeUnpublishVolume interface of the CSI plug-in in out-of-tree mode.2) Unmount volume by calling the UnmountDevice function of DeviceUnmounter, or calling the NodeUnstageVolume interface of the CSI plug-in in out-of-tree mode.3) ASW is updated.
MountAttachVolumes ensures that the volumes to be used by the pod are successfully mounted. All the pods in DSW are traversed. If any pod is not in ASW (the directory is to be mounted and mapped to the pod), the following operations are performed (VolumeMode=FileSystem is used as an example):
1) Wait until the volume is attached to the node by External Attacher or the kubelet.2) Mount the volume to the global directory by calling the MountDevice function of DeviceMounter or calling the NodeStageVolume interface of the CSI plug-in in out-of-tree mode.3) Update ASW if the volume is mounted to the global directory.4) Mount the volume to the pod through bind-mount by calling the SetUp interface of Mounter or calling the NodePublishVolume interface of the CSI plug-in in out-of-tree mode.5) Update ASW.
UnmountDetachDevices ensures that volumes are unmounted. All UnmountedVolumes in ASW are traversed. If any UnmountedVolumes do not exist in DSW (indicating these volumes are no longer used), the following operations are performed:
1) Unmount volume by calling the UnmountDevice function of DeviceUnmounter, or calling the NodeUnstageVolume interface of the CSI plug-in in out-of-tree mode.2) ASW is updated.
This article introduces the basics and usage of Kubernetes persistent storage and analyzes the internal storage process of Kubernetes. In Kubernetes, all storage types require the preceding processes, but the Attach and Detach operations are not performed in certain scenarios. Any storage problem in an environment can be attributed to a fault in one of these processes.
Container storage is complex, especially in private cloud environments. However, through this process, it’s possible to seize more opportunities while braving more challenges. Currently, competition is fierce in the storage landscape of China’s private cloud market. Our agile PaaS container team is always looking for talented professionals to join us and help build a better future.
1) Source code of the Kubernetes community2) kubernetes-design-proposals volume-provisioning3) kubernetes-design-proposals CSI Volume Plugins in Kubernetes Design Doc
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
4 
4 
4 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/work-your-way/we-re-not-selling-stuff-we-re-on-a-mission-to-create-the-futureofwork-22fb5ec3dc6e?source=search_post---------19,"There are currently no responses for this story.
Be the first to respond.
This post was an internal paper about ‘why we do what we do’ sent to our crew members in summer 2015. Since we believe in sharing our ideas we decided to also share this with clients, partners, friends or anyone who would like to know what we are up to.
As founders we’ve made our own experience how critical a solid operation can be for the success and the growth speed of a business. When we hit 100 people at my last company we were glad to rely on fast internal communications tools, that were far more superior than just sending emails and working with static documents. We’ve started implementing that tool architecture when we were a team of 10 people only, because we knew that it would be harder to change the way people work once the company was bigger. And we also knew that we wanted to work with tools, which are fun to use on a daily basis.
“Some 80% of your life is spent working. You want to have fun at home; why shouldn’t you have fun at work?” — Sir Richard Branson
Dealing with overflowing mailboxes, non searchable archives on local servers, VPN clients while travelling, is such a poor user experience but still everyday life in many businesses. Since we are used to simpler, faster and more fun ways of communication in our private lifes, our admins frequently freak out about confidential business infomation being sent via unsecured channels — it’s simply not as complicated as using the existing, super secure but nasty tools. Another extreme occurs when a company grows fast. More and more tools are added for internal communication, CRM, project management or file transfer. Skype, Mail, Trello, Slack you name it. It’s not uncommon that we count 30+ tools in a company of only 15 people.
But group communication is far more crucial for scale today than it was before. We live in the century when speed is a major resource as well as being able to find important information within seconds like we know it from the web. But searching an old presentation or paper througout the endless file archives can cause a headache and a very bad end of the workweek. The same is true for communication within a team. The faster you can connect teams without missunderstanings the better you will be able to outperform your competition. It’s like an engine that runs smooth and reliable.
Thanks to companies like Salesforce, Amazon or Google the “Cloud” has evolved from a crazy idea to a reliable software and tool infrastructure that not only millions of companies use today but also many IT departments cannot beat in terms of security, data compliance and most importantly: performance. Additionally thanks to tools like Whatsapp, Hipchat or Slack the term “group communication” is finally a term that people are aware of. There are simply better ways than just sending Word documents to a company mailing list. And finally we are used to work mobile and across multiple devices more than ever before and cannot understand that our company tools cannot keep up with that.
So there is a need to change the way we work and find better ways together with our IT departments to build what we call the “Future of Work”. This of course includes scalable as well as fun to work with infrastructure without compromising on IT security or compliance, but also tools that help users/employees to communicate faster and more efficiently in order for the leader to run and grow the company without heaving a ‘heart attack’.
That’s why it is our job to support making that transition by identifying, implementing and leveraging the best cloud tools for them. There are just too many tools and too many providers that it’s almost impossible to keep up to date. The setup is easy, but building a cloud tool architechture that can scale from 5 to 100+ employees and works across different timezones and countries, needs effort.
Teams that partner with us to transform their way of working, will experience a major shift in how communication happens, how information is organized, how time is spent and how people collaborate with more fun in a much more intense level. We call this super-productive, transparent and scalable work environment a „heart attack free“ work environment - less misunderstandings, less inefficiency and with all information just one search away.
That’s what we do every day. We show them the impact of this new way of working, help them cleaning up the tool architecture and support the transformational process that will create the new work environment. We do not just sell cloud licences or monthly service agreements. We make an impact on how their teams work and perform. We sell less mail overflow and data archive madness. We sell a workspace where your coworker is just one click away. We sell a way to shift towards the “Future of Work” before the competition gets ahead.
“We save people up to 2 hours a day at work that they can be used for more important stuff.”
What do we exactly achieve for our clients:
Since we’ve made this experience on our own we always put ourselves in our customers perspective and rely on analytics tools to develop a deep understanding for their communication and information flow. We are at the heart of what’s new. We’re constantly reviewing new tools and methods to be able to leverage these for our clients. We’ve curated, implemented and still run thousands of cloud accounts in all kinds of industries such as fintech, ecommerce, advertising, media or gastronomy what helps us to build a better understanding for everyone’s individual needs.
We do not know today what Blackboat will look like in 5 years from now. But what we aim for is a major impact on how people collaborate and communicate in the future. We know that is a fairly large goal but we also know that the goal is always the first step to deliver incredible useful services and build an outperforming company.
Update Jan 2016: Learn more about the impact that the cloud has on every business. The team behind Bettercloud and Dave Politis has recently published a The State of Cloud IT Report based on sources like Gartner, PwC, Google, Microsoft and 1,500 IT professionals.
About New Work and Digital Transformation
5 
5 claps
5 
About New Work and Digital Transformation
Written by
On a mission to free people from the old way of working... #OntheWaytoNewWork - also publishing via YOUTUBE.blackboat.com (sometimes…but min 2x a week)
About New Work and Digital Transformation
"
https://faun.pub/what-is-an-internal-developer-platform-dc89e9e1edaa?source=search_post---------20,"Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://scientya.com/all-about-btrdb-berkeleys-tree-database-68c78e0711cf?source=search_post---------21,"By Jiao Xian
This article studies and introduces the internal implementation details of BTrDB, an open-source time series database for Internet of Things (IoT) applications.
A practical project is introduced in the BTrDB paper, which can clearly explain the original design intention and applicable scenarios of BTrDB:
A large number of sensors of certain types are deployed in a power grid. Each sensor generates 12 timelines with 120 Hz frequency (that is, 120 points per second) and 100 NS time accuracy. For various reasons, problems, such as delays, and disorder (time), often occur in these sensors’ data transmission. In this scenario, BTrDB can support 1000 similar sensors in a single unit, with a write rate of about 1.44 MB points/s.
This project has the following features:
(1) The timelines are invariant over a long period of time, and their life cycle are consistent with those of (IoT) devices.(2) The data frequency is very high (120Hz) and fixed.(3) The time resolution of the data is very high (100 NS level). In general, the time accuracy of time series databases, such as Druid and TimescaleDB, can reach MS level at most.(4) Data transmission is often disordered.(5) The number of timelines is limited.
To adapt to the preceding scenarios, BTrDB designs and proposes the “time-partitioning version-annotated copy-on-write tree” data structure, in which a tree is built for each timeline (refer to B+Tree). Data is sorted by the time stamp in the tree, and the leaf nodes store the time series data for a certain time period in an orderly way.
It is conceivable that the life cycle of this tree is directly linked to the life cycle of the device, so as time goes by, even if the tree contains only one timeline, it also occupies a considerable storage space (about 10 MB points/day). In addition, based on the tree structure and the version-annotated concept, BTrDB can support disordered data and time series data with arbitrary precision well.
The data structure is different from the previous variants of time series databases based on LSM-Tree, so BTrDB also provides a new time series data query interface, which is convenient for building various algorithms and applications on the upper layer of BTrDB.
When writing data, BTrDB first modifies the data blocks in the memory (create new blocks or modify existing blocks using CoW mechanism), and writes the data back to the underlying block storage when the data reaches a certain threshold. Because of the CoW mechanism, and because the underlying block storage (Ceph is used by default) cannot overwrite the update, only a new version of the data block can be created.
Because of the fixed frequency of the data sent from IoT devices, the space occupied by these leaf nodes is basically the same.
When the leaf node has not been persisted to the underlying storage, the timestamp and double-precision floating point values are stored in memory as an array, respectively. When serialized to the underlying storage, delta-delta compaction is performed on the timestamp and values. Before the double-precision floating point values are serialized, the number of floating point data is split into the mantissa and the exponent, and delta-delta compaction is performed respectively.
An intermediate node is divided into multiple buckets. Each bucket contains the link (with version number) pointing to the child node, and the statistical data of the child node:
When processing a query, if the time precision of the intermediate node meets the query requirement, the query operation no longer reads the lower-level child nodes, so that the precision function is naturally realized. This implementation method can effectively process disordered and duplicate data, as well as delete operations, and can ensure data consistency better than other existing implementations.
A new tree (corresponding to a new timeline) has only one root node. In the BTrDB implementation, the time span of the root node is about 146.23 years, thus the time span of each bucket is 146.23/64 ~ = 2.28 (years). According to the default configuration, the year 1970 is on the 16th bucket in the root node.
It can be seen that the root node has limited the time range of data when it is created, and subsequent data insertion is split layer by layer from top to bottom. When the timeline is incomplete due to data loss or other reasons, the depth of some nodes may be different, so it is not a strictly Balanced tree. The data insertion process is as follows:
If the number of points to be inserted in the current bucket exceeds the maximum number of leaf nodes (1024 by default), an intermediate child node is directly created;
Otherwise, a leaf node is directly created;
As can be seen from the above process, a node may be split in two places during insertion. One is a top-down split from the root node. The other is an upward split from the leaf node.
Although this tree is not a balanced tree, for IoT projects, the timeline life cycle and the frequency of data collection of the device are very stable. In most scenarios, the data in the nodes are evenly distributed.
By default, a leaf node can store up to 1024 data points, and a intermediate node can store up to 64 child node pointers. Therefore:
The following schematic diagram shows the correlation between check points in WAL and the time series buffer. After the buffer is emptied, BTrDB writes the deleted check points into the metadata (block attributes) of the block file corresponding to the current WAL:
When all check points in a WAL block file are marked as deleted, the file can be deleted from Ceph. When the size of the current WAL file exceeds 16 MB, a new block file is created. Ideally, the block file can be promptly deleted. However, if some timelines are abnormal, and as mentioned earlier, the buffer cannot be recycled until 8 hours later, then the WAL files responsible for recording these timelines can only be recycled 8 hours later.
These retained WAL files are only 16 MB in size, and the number of these files is linearly related to the number of abnormal IoT devices. Therefore, more IoT device operation statistical data is required to measure the impact.
After being persisted, the BTrDB tree structure generates two types of data files. One is called the superblock, which records the basic information of the current tree, such as the latest version, the update time, and the root node location. The other is called the segment, which uniformly contains the data of leaf nodes and intermediate nodes of the tree.
A superblock has a version. Each version of superblock only occupies 16 Bytes. The format is as follows:
The addressing method of the superblock in Ceph is as follows:
When the BTrDB tree structure is persisted, leaf nodes and intermediate nodes are serialized into the segment file together. The addressing method for each node is as follows:
The WAL file, the superblock block file and the segment block file are all 16 MB in size. In addition, in BTrDB, the compaction operation is not performed, and the data of the expired version is not cleaned. Only the WAL processing described above is performed, so the write amplification is obvious.
Here, I list and briefly introduce the new query semantics provided by BTrDB. These query semantics is closely related to the BTrDB data structure, either to take advantage of some tree structure features or to avoid some tree structure defects:
The Resolution parameter in the interface above has a significant impact on the performance of the interface As mentioned above, the time resolution of the root node is 2.2 years. From the root node to the underlying node of the tree, the time resolution of data in the node is getting higher and higher. When querying, low-resolution data has a high degree of aggregation, and few data blocks need to be scanned. High-resolution data has a low degree of aggregation, but many data blocks need to be scanned.
In BTrDB, the data structure is built for a single timeline, and a tree is built to store time series data based on the stability of IoT device data. The tree structure solves some problems that traditional TSDBs face with aspects such as disordered insertion, deletion, and pre-precision reduction.
www.alibabacloud.com
The digital world publication
53 
53 claps
53 
Written by
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
Scientya is all about creating and sharing knowledge in the digital world. Switzerland’s fastest growing Medium publication  covers various topics including Blockchain, Artificial Intelligence, Cloud Computing, Health Care, Marketing and more.
Written by
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
Scientya is all about creating and sharing knowledge in the digital world. Switzerland’s fastest growing Medium publication  covers various topics including Blockchain, Artificial Intelligence, Cloud Computing, Health Care, Marketing and more.
"
https://medium.com/@alibaba-cloud/best-practices-for-physical-connection-internal-network-bgp-configuration-using-express-connect-5aa528454c1d?source=search_post---------22,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Feb 27, 2019·4 min read
Join us at the Alibaba Cloud ACtivate Online Conference on March 5–6 to challenge assumptions, exchange ideas, and explore what is possible through digital transformation.
This document provides guidance to help you use dynamic routing to quickly build a hybrid cloud with Express Connect.
For more information, see the first three steps in this guide: https://www.alibabacloud.com/help/doc-detail/44844.htm
The configuration process can be summarized as follows:
On the console, go to Express Connect > BGP > BGP Groups. Click “Create BGP Group” in the upper-right corner of the window.
Set the BGP parameters.
After the preceding steps are completed, an available BGP group is listed on the BGP Groups page.
To configure a BGP group for your IDC, you need to enter the Alibaba Cloud ASN, 45104. This is unique and remains unchanged. Refer to https://www.alibabacloud.com/help/doc-detail/52410.htm for more information.
On the console, choose Express Connect > BGP > BGP Peers, and click Create BGP Peer in the upper-right corner of the window.
Set the BGP peer parameters.
After the preceding steps are completed, available BGP peers are listed on the BGP Peers page.
Configure a corresponding BGP peer for your IDC.
On the console, choose Express Connect > Physical Connections > Virtual Border Routers (VBRs).
Locate the row that contains the target VBR and click its name or click Manage.
On the Manage page, click Add Route.
In Destination CIDR Block, enter the CIDR block of the VSwitch in the VPC.
In Next-Hop Direction, select “To VPC”.
In Next Hop, select Ri-B.
On the console, choose Express Connect > Physical Connections > Virtual Border Routers (VBRs).
Locate the row that contains the target VBR and click its name or click Manage.
On the Manage page, click BGP CIDR Block on the right side of the page.
Advertise the CIDR Block: The static route CIDR block created in the preceding step is also a VSwitch CIDR block in the VPC.
This CIDR block communicates with your IDC network through BGP routing.
On the console, choose Express Connect > VRouter Interface.
Select the VRouter interface on the VPC VRouter side (connection role: Receiver). Click Route Configuration on the right side of the page.
On the displayed VRouters page of the VPC console, click Add Route on the side of the page.
Destination CIDR Block: IDC CIDR block
In Next Hop Type, select VRouter Interface.
In VRouter Interface, select the ID of the API connected to the VBR. In this example, this is Ri-C.
EC is a connection between two networks. After a connection is established, each network generates a VRouter interface. In this example, this is Ri-B on the VBR side and Ri-C on the VPC side. The function of EC is similar to that of an NIC.
Use the server in your IDC to ping the ECS instance in the Alibaba Cloud VPC.
If the ECS instance is pinged, the BGP communication has been deployed in the internal network.
Ensure that the host address of your IDC has been advertised in the BGP CIDR block.
The ECS instance address in the Alibaba Cloud VPC has been advertised in the BGP CIDR block of the VBR.
For more information about the performance tests of physical connections, see the following tutorial:https://www.alibabacloud.com/help/doc-detail/58625.htm
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
8 
8 claps
8 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.datadriveninvestor.com/proxima-a-vector-retrieval-engine-independently-developed-by-alibaba-damo-academy-baafd3c95fca?source=search_post---------23,"By Wang Shaoxuan (Dasha) and Xiao Yunfeng (Hechong), senior technical experts at Alibaba DAMO Academy’s System AI laboratory
What kind of retrieval technology is used by Taobao’s search recommendation and video search? What problems are solved by unstructured data retrieval, vector retrieval, and multi-modal retrieval?
In this article, we invited scientists at Alibaba DAMO Academy, to address these questions and elaborate on the internal technologies of Alibaba DAMO Academy. In particular, we will discuss the vector retrieval engine Proxima, and also describe the status quo, challenges, and future of the vector retrieval.
Artificial Intelligence (AI) is a technical field that existed way back when the computer was first invented. One of its core features is that it works like a human brain. Through a series of mathematical methods, such as probability theory, statistics, and linear algebra, algorithms can be analyzed and designed for automatic learning in computers.
AI algorithms can abstract all kinds of unstructured data, such as voice, pictures, videos, languages, behaviors generated by people, objects and scenes in the physical world into multidimensional vectors. These vectors, like space coordinates in mathematics, mark the relationships between entities. Such a procedure is called embedding, as shown in the following figure. Unstructured retrieval indicates the retrieving of these generated vectors to find corresponding entities.
Unstructured retrieval is essentially a vector retrieval technology. Its main application includes image recognition, recommendation systems, image search, video fingerprint, speech processing, natural language processing, and file search. Due to the widespread use of AI technology and the continuous growth of data, vector retrieval has gradually become an indispensable part of the AI technology procedure. It also serves as a supplement to traditional search technology with the multi-modal search capability.
Vector retrieval is applied firstly to retrieve the most common unstructured data that can be accessed by humans, such as voices, images, and videos. Traditional search engines only index the names and descriptions of these multimedia without interpreting and indexing the contents of these unstructured data. Therefore, the retrieval results produced by traditional search engines are quite narrow.
As AI developed, it allowed us to analyze unstructured data in a quick and cost-effective way. Thus, it is now possible to directly retrieve the unstructured data, among which Vector retrieval is a very important part.
In the following figure, image search is used as an example. First, machine learning and analysis are performed on all historical images offline. Each image (or the figure in an image) is abstracted into multi-dimensional vector features, which are then constructed into efficient vector indexes.
When it comes to a new query (image), the same machine learning method is used to analyze it and generate a characterization vector for finding the most similar result in the formerly built vector indexes. In this way, an image retrieval based on image content is completed.
Vector retrieval has been used in common full-text searches for a long time. The address retrieval is used as an example to briefly introduce the application and importance of vector retrieval technology in text retrieval.
For the example on the left of the following figure, a user searches for “The first hospital of Zhejiang” in the standard address library. However, the keyword “The first hospital of Zhejiang” is not in the standard address library, since the standard address of “The first hospital of Zhejiang” is “The First Affiliated Hospital of Zhejiang University School of Medicine”. If only “The first hospital” and “Zhejiang” are searched separately, the relevant results cannot be found in the standard address library. This is because the address “The first hospital of Zhejiang” does not exist. However, through the analysis based on people’s historical languages or even the previous click associations, a model of semantic correlation can be established to express all addresses with high-dimensional features. By doing so, the “The first hospital of Zhejiang” and the “The First Affiliated Hospital of Zhejiang University School of Medicine” can be highly connected, and thus “The first hospital of Zhejiang” can be retrieved.
Another example is also about address retrieval, as shown on the right side of the following figure. If a user searches for “Hangzhou Alibaba” in the standard address library, it is almost impossible to find a similar result when using only text recall. After analyzing massive users’ click behaviors, high-dimensional vectors can be formed combined with address text information. Therefore, texts that are most clicked can be recalled and placed in front.
In e-commerce scenarios such as search, recommendation, and advertisement, it is commonly required to find similar products of the same type and recommend them to users. Most of these requirements are implemented through product collaboration and user collaboration. The new generation of search recommendation system, absorbing the embedding capacity of deep learning, realizes quick retrieval through the vector recall method, including Item-Item (i2i), User-Item (u2i), User-User-Item (u2u2i), user-Item-Item (u2i2i).
Through the abstraction of browsing and purchasing by users, as well as the similarities and correlations between items, algorithm engineers characterize the items into high-dimensional vector features and store them in the vector engine. In this way, similar items of an item (i2i) can be quickly and efficiently retrieved from the vector engine.
In fact, the vector retrieval scenarios are far more diverse than the preceding types. The following figure covers most business scenarios where AI can be applied.
The essence of vector retrieval is to solve problems of k-Nearest Neighbor (KNN) and Radius Nearest Neighbor (RNN). KNN searches for the K points that are closest to the query point, while RNN searches for all points or N points within a specified radius of the query point. When dealing with a large amount of data, the cost of 100% solving the KNN or RNN problem is relatively high, so the method of Approximate Nearest Neighbor (ANN) is introduced. In conclusion, retrieving large amounts of data is to solve the ANN problem.
Many retrieval algorithms have been proposed for solving the ANN problem. The commonly used algorithm can be traced back to the KD-Tree proposed in 1975. Based on Euclidean space, the algorithm uses a multidimensional binary tree data structure to solve the ANN retrieval problem.
At the end of the 1980s, the spatial coding and ideas of Hash were produced, mainly represented by fractal curves and Locality-Sensitive Hashing (LSH). Both the algorithms belong to the ideas of spatial coding and transformation, and similar algorithms include Product Quantification (PQ) and so on. These quantization algorithms map high-dimensional problems to low-dimensional problems for solutions, so the retrieval efficiency is improved.
At the beginning of the 21st century, using the neighbor graph to solve the ANN problem also emerged. The neighbor graph is mainly based on the assumption that “a neighbor’s neighbor may also be a neighbor”. Namely, the neighbor relationships in the data set are established in advance to form a neighbor graph with certain characteristics. During retrieval, the graph is walked through all over, and finally the result is generated through convergence.
Vector retrieval algorithms are numerous without generality. Different algorithms are available in different data dimensions and distributions. However, the vector retrieval algorithms can be divided into three categories, spatial partitioning, spatial coding and conversion, and neighbor graph. Spatial partitioning is represented by KD-Tree and clustering search. These small collections can be quickly located during retrieval to reduce the number of data points to be scanned and improve retrieval efficiency.
The spatial coding and conversion, such as p-Stable LSH, PQ and other methods, re-encode or transform the data set and map it to smaller data space, thus reducing the calculation of scanned data points. Neighbor graphs include Hierarchical Navigable Small World (HNSW), Space Partition Tree and Graph (SPTAG), ONNG and so on. This idea can improve the retrieval efficiency by establishing relationship graphs in advance to speed up convergence during retrieval and reduce the number of data points to be scanned.
During the development of vector retrieval, some excellent open source products have emerged, such as Fast Approximate Nearest Neighbor Search Library (FLANN) and Facebook AI Similarity Search (Faiss). They provide unified implementation and optimization for some commonly used and effective ANN algorithms in the industry, and form some engineering retrieval solutions through runtime libraries. Based on these runtime libraries and improvements, some service-oriented engineering engines were invented in the industry, such as Milvus and Vearch.
Although vector retrieval has been developed for many years and has gradually become the mainstream method of unstructured search, there are still many technical challenges and problems.
Massive Index Precision and Performance
Vector retrieval is designed to support large-scale data retrieval since there is a large quantity of complicated unstructured data. However, many retrieval algorithms are still facing challenges in scenarios with hundreds of millions, or even billions of data. What’s more, there are also some problems in engineering implementation, such as huge construction costs and low retrieval efficiency.
In addition, the increase of dimensionality causes the efficiency decrease of some vector retrieval methods. Vector retrieval methods are useless in high-dimensional space, increasing the data calculation and storage cost in engineering. Secondly, the algorithms lack complete generality, which means extensive consistency of data retrieval cannot be achieved. In other words, the retrieval algorithms are effective in any data distribution.
At present, the industry still cannot cope with the billions of high-dimensional data. Multiple indexes and separate retrieval are often combined to deal with data, increasing the actual computing cost.
Distributed Construction and Retrieval
Currently, most vector retrieval uses data sharding for horizontal scaling. However, too many shards easily lead to an increase in the computing workload, thus decreasing the retrieval efficiency. In distributed mode, it is still difficult to merge algorithms in the vector retrieval. Therefore, once data is sharded, a more efficient index cannot be generated by combining it with the MapReduce computing model.
Online Update of Streaming Indexes
It is easy to implement Create, Retrieve, Update, and Delete (CRUD) operations with traditional retrieval methods. Vector retrieval relies on data distribution and distance measurement, and some methods also need data set training. Even a slight change on a single data point would cause massive changes. Therefore, algorithms and engineering still face some challenges to realize the full-streaming construction of vector indexes from 0 to 1, fast retrieval with addition, instant disk writing, and real-time dynamic index update.
Currently, non-training retrieval methods can easily support online dynamic addition and query of full memory index. However, in terms of instant disk writing, insufficient memory, online dynamic vector update and deletion, it is costly and non-real-time.
Joint Retrieval of Tags and Vectors
In most business scenarios, both tag retrieval and similarity retrieval are required, such as querying similar images under certain attribute conditions. This type of retrieval is called “vector retrieval with conditions”.
Currently, the K-way merge method is adopted in the industry. That is, the results are merged after tags and vectors are retrieved respectively. Through this method, some problems can be solved, but the results are not ideal in most cases. The main reason is that there is no precise scope for vector retrieval, while the goal of vector retrieval is to ensure the accuracy of TOPK as much as possible. When TOPK is large, the accuracy may decline, resulting in inaccurate or even empty merging results.
Complex Multi-Scenario Adaptation
Through vector retrieval is universally applied, no universal algorithms can adapt to any scenarios or data for now. Even if the same algorithm works with different data, the parameter settings may vary. For example, for the multi-layer clustering search algorithm, clustering algorithm, number of layers and classes, and convergence threshold differ when dealing with diverse scenarios and data. It is the hyper-parameter tuning that greatly increases the user threshold.
To make it more user-friendly, scenario adaptation needs to be considered. It mainly includes data adaptation and requirement adaptation. Data adaptation includes data scale, data distribution, data dimensions and so on. Requirement adaptation includes recall rate, throughput, latency, streaming, and real-time performance, etc. Based on different data distribution, appropriate algorithms and parameters are selected to meet the actual business needs.
Proxima is an exclusive vector retrieval kernel of Alibaba DAMO Academy. At present, its core capabilities are widely used in many business scenarios of Alibaba Group and Ant Financial, such as Taobao search and recommendation, Ant Financial face payment, Youku video search, and Alimama advertising search. Proxima is also deeply integrated into a variety of big data and database products, such as Alibaba Cloud Hologres, Elasticsearch and ZSearch, MaxCompute (previously known as ODPS), to provide vector retrieval capabilities.
Proxima is a universal vector retrieval engine. It implements high-performance similarity search for big data in multiple hardware platforms such as ARM64, x86, and GPU. Moreover, Proxima supports embedded devices and high-performance servers, covering from edge computing to cloud computing. It also supports high-accuracy and high-performance index construction and retrieval of a single index at the billion level.
As shown in the preceding figure, Proxima has the following core features:
Currently, the vector retrieval library — Faiss (Facebook AI Similarity Search) is commonly used in the industry, invented by the Facebook AI team. Faiss is excellent and is the core of many service-based engines. However, Faiss still has some limitations in large-scale universal retrieval scenarios, such as real-time streaming computing, offline distributed computing, online heterogeneous acceleration, joint retrieval of tags and vectors, cost control and servitization.
For example, for the public data set ANN_SIFT1B (from corpus-texmex.irisa.fr) at one billion scales on the server of Intel(R) Xeon(R) Platinum 8163 CPU and 512GB memory, the computing resources required by Faiss are too large to implement the construction and retrieval of a billion of indexes. However, Proxima can easily perform index construction and retrieval of a billion of indexes by using a single machine under the same environment and data volume.
Considering the feasibility of the test, DAMO Academy team compared Faiss and Proxima in index construction and retrieval of 200 million data. Besides, based on 20 million data, the team also compared the heterogeneous computing capabilities of Faiss and Proxima single card. For the one billion data volumes, Proxima is tested separately, and the results are as follows.
Retrieval Comparison
Proxima is several times better Faiss in terms of the retrieval performance, and it can achieve higher precision of recall. In addition, the retrieval technology for Topk is even better. In addition, Faiss also has design defects in some algorithm implementations, such as the HNSW implementation, and low retrieval performance for large-scale indexes.
Construction Comparison
For 200 million data, the index construction of Faiss takes 45 hours, which can be shortened to 15 hours using HNSW optimization. Proxima can finish the construction within two hours under the same resources with smaller index storage and higher precision (see retrieval comparison).
Heterogeneous Computing
Proxima adopts a GPU computing method different from Faiss. It optimizes online retrieval scenarios that suffer from “small batches, low latency, and high throughput”.
Proxima shows amazing advantages in small batch scenarios. It features small batches, low latency, and high throughput, and makes full use of GPU resources. At present, this retrieval solution is widely used in Alibaba’s search and recommendation scenario.
A Billion Scale of Data
Proxima supports streaming indexes and half-memory retrieval construction. This creates indexes at a billion of scale and implements high-performance and high-precision retrieval under limited resources. The high-performance and low-cost capabilities of Proxima provide strong and basic support for AI large-scale offline training and online retrieval.
With the wide application of AI technology and the continuous growth of data, vector retrieval, as a mainstream method in deep learning, will be further developed with its extensive retrieval and multi-modal search capabilities. The entities and features of the physical world are characterized and combined through vectorization technology to map to the digital world. Computers are used to calculate and retrieve, thus digging out the potential logic and implicit relationships for more intelligent services to human society.
In the future, vector retrieval will not only face the continuous increase of data scale, but also needs to solve the problems of hybrid spatial retrieval, sparse spatial retrieval, ultra-high dimension, and generalized consistency in algorithms. In engineering, vector retrieval will have to deal with more and more extensive and complex scenarios. How to form a powerful and systematic system that runs through scenarios and applications will be the key to the future development of vector retrieval.
www.alibabacloud.com
empowerment through data, knowledge, and expertise.
4 
4 claps
4 
Written by
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
empowerment through data, knowledge, and expertise. subscribe to DDIntel at https://ddintel.datadriveninvestor.com
Written by
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
empowerment through data, knowledge, and expertise. subscribe to DDIntel at https://ddintel.datadriveninvestor.com
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/store-operations-optimization-search-acceleration-over-postgresql-arrays-json-and-internal-tag-a9eed0339c8b?source=search_post---------24,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Feb 12, 2020·10 min read
The PostgreSQL arrays, JSON data, and other data types offer a lot of convenience to sellers for managing store operations. However, the optimization of these data types becomes increasingly complicated. For example, if a seller stores the information of some products in the database and attaches multiple tags in JSON format to each product. Then, the data structure may appear as follows.
Consider the following sample data, where:
Assume that the seller wants to search for data, where “gid=?”, the data contains a certain ID, and the ID score is within a certain range. The purpose is to find data that meets certain tag conditions in a store.
Now, write a User Defined Function (UDF) to implement the query of the specific class.
The final SQL statement is shown below.
The preceding method only uses the Group Identification (GID) index. Therefore, it requires a large amount of CPU computing which inevitably becomes a bottleneck during high concurrency.
The critical question is how to accurately perform index retrieval on data and improve the performance when a known property has a maximum of N JSON elements.
In fact, the SQL statement can be changed as follows.
Or
The SQL statement contains many OR conditions, but it does not affect the performance because the GID, ID, and score are all set to fixed values, and the ID is unique in the same record. Although multiple trees are scanned, the I/O or CPU usage does not increase upon the final retrieval considering the same record can only be retrieved from one tree.
Follow the steps listed below to optimize the performance:
Step 1. Set the number of JSON elements in the prop field to a fixed value.
Step 2. Construct a composite expression index for each JSON element.
Step 3. Rewrite the SQL statement.
Step 4. Use UDF to splice dynamic SQL statements or splice dynamic SQL statements at the program end.
The following statement shows the dynamic query splicing.
The index is correctly used as shown below.
Step 5. Add a composite express index if the number of elements increases.
Note that this method involves many indexes, so it might affect the Data Manipulation Language (DML) performance. However, there is a significant improvement in query performance.
If you do not want to use a composite index, use a single column expression index instead.
You can apply this approach to implement searches by equivalent and range. Consider the following example.
Integrate GID and prop-> ID into the index.
LPAD function is also critical. Scores may vary by the number of digits, and therefore, the query result in the TEXT range may not meet expectations. In this case, LPAD can be used to ensure that the scores have the same number of digits to achieve the same sequence effect as numerical values.
The effect after padding is as follows.
Implement the following steps to test the performance.
Step1. Create a table.
Step 2. Construct data by creating 10 thousand GIDs, consisting of 100 thousand items each.
Make each prop have 10 JSONBs, and set the ID range of each JSONB to 0 to 1000 and the score range to 0 to 100.
Now, write 1 billion records as shown below.
Step 3. Next, you need to perform the indexing.
Step 4. Perform the pressure test by using the original method.
Step 5. Perform the pressure test by using the optimized method.
1. When you use the multi-tree APPEND method described in this document, there is no wastage of CPU resources and the performance improves by N times.
Maximum data capacityCaseTPSRTCPU usage1 billionOriginal method96583 ms100%1 billionOptimization method17,0003.3 ms100%
2. We recommend you to perform indexing by partition to optimize the database kernel. At the kernel layer, one BTree index corresponds to multiple trees, which solves the problem related to the multi-value point query and interval query in the array.
3. Currently, PostgreSQL GIN and GiST indexes only support the retrieval of the multi-value data, array, and JSON data in the inclusion and overlapping relations. It does not support query by value range. To support query by value range, you need to improve the inverted tree and develop the corresponding OPS.
The RUM index API can realize this function to some extent.
Build your own PostgreSQL solution on Alibaba Cloud with ApsaraDB for RDS PostgreSQL.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
4 
4 
4 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/@techgenyz/why-cloud-telephony-system-is-the-best-for-business-communication-f7f89abb08b3?source=search_post---------25,"Sign in
There are currently no responses for this story.
Be the first to respond.
TechGenyz
Sep 24, 2017·1 min read
Communication is a vital part of any business, whether for sales or for internal communication. We all know the mishmash it creates when communication goes wrong. For a company that is widely spread around the world, right communication is the priority. Businesses have started using cloud telephony system for their external and internal communication because of its superb benefits. Apart from connecting sales reps to overseas customers, cloud telephony system also integrates your teams well.
Here are some tremendous benefits of using a virtual phone system. You will definitely find the reasons why most of the businesses are using it.
TechGenyz is a leading source of latest technology news, updates on future tech stories, news on Virtual Reality, Augmented Reality, gaming, apps and more.
55 
1
55 
55 
1
TechGenyz is a leading source of latest technology news, updates on future tech stories, news on Virtual Reality, Augmented Reality, gaming, apps and more.
"
https://medium.com/@ibmcloud/empowering-the-in-house-digital-marketer-c6480ba57238?source=search_post---------26,"Sign in
There are currently no responses for this story.
Be the first to respond.
IBM Cloud Stories
Nov 10, 2016·3 min read
With Nanigans software running on IBM Bluemix infrastructure, internal marketing teams optimize online advertising and save big
By: Ben Tregoe, Senior Vice President of Business Development, Nanigans
In the world of digital marketing, change is the constant. But at Nanigans, we see one clear trend among today’s top performance marketers: bringing online advertising efforts in-house. Our customers believe that advertising performance is critical to their business success and too important to be left to third parties.
When a marketing organization outsources its online advertising efforts, ad spend can be quickly eaten up by agency fees and the ad tech tax, which can range from 15 percent to as high as 70 percent. Plus, engaging external parties means funneling data to any number of third-party players, introducing concerns about data leakage and an overall lack of reporting transparency.
But, most importantly, no one knows a company’s products and services like the internal team involved in marketing them on a daily basis. Our customers want to be able to capitalize on their in-house expertise, channeling it into an online advertising strategy that’s agile, responsive and highly effective in driving revenue.
Nanigans created a comprehensive suite of software tools to help in-house marketing organizations do just that. To deliver these services to customers worldwide, we needed support from a cloud partner we could count on for performance, scalability and international reach.
When we initially began hosting data-intensive workloads for our e-commerce and performance advertising customers, we quickly hit capacity with our data center services provider.
We subsequently turned to an IBM Bluemix cloud hosting solution, which has been a tremendous asset as our company has grown over the years.
The switch to Bluemix has allowed us to accommodate our workloads and scale internationally as our business demands. And we don’t have to worry about low-latency service delivery in an industry where every fraction of a millisecond matters. As one of the largest ad-buying partners within the Facebook Marketing Partner ecosystem, that confidence is essential for us.
For example, an iconic apparel retailer wanted to drive new and repeat online purchases by deploying its Facebook ad spend more strategically. Such large global companies may be investing hundreds of millions of dollars in online advertising, but without tools to help them understand how to turn ad spend into revenue, these investments can be largely ineffective.
With cloud-based Nanigans software, including functionality designed to dynamically test, identify and scale top-performing ad units across target audiences, the retailer increased its Facebook return on ad spend (ROAS) by a whopping 801 percent in just two years.
While the marketer is always in control, these kind of results are a credit to Nanigans’ best-in-class engineering and data infrastructure. Backing our offerings with IBM Bluemix means that we can deliver the services our customers need to take control of their online advertising efforts and drive success.
To read 2017 IBM Cloud Stories visit http://ibm.co/2iKoD4d .
4 
4 
4 
To read 2017 IBM Cloud Stories visit http://ibm.co/2iKoD4d .
"
https://medium.com/@renebuest/microservice-cloud-and-iot-applications-force-the-cio-to-create-novel-it-architectures-96e317eefe7a?source=search_post---------27,"Sign in
There are currently no responses for this story.
Be the first to respond.
Rene Buest
Aug 2, 2016·8 min read
The digital transformation challenges CIOs to remodel their existing IT architectures providing their internal customers with a dynamic platform that stands for a better agility and fosters the companies’ innovation capacity. This change calls for a complete rethink of the historically implemented architecture concepts. Even if most of the current attempts are to migrate existing enterprise applications into the cloud, CIOs have to empower their IT teams to consider novel development architectures. Because modern applications and IoT services are innovative and cloud based.
Typical application architectures, metaphorically speaking, remind of a “monolith”, a big massive stone that is made of one piece. The characteristics of both are the same: heavy, inflexible and not or not easy to modify.
Over the last decades, many, mostly monolithic applications have been developed. This means that an application includes all modules, libraries, and independencies that are necessary to ensure a smooth functionality. This architecture concept implicates a significant drawback. If only a small piece of the application needs to change, the whole application has to be compiled, tested and deployed again. This also implies for all parts of the application that don’t experience any changes. This comes at big costs taking manpower, time and IT resources and in most cases lead to delays. In addition, a monolith makes it difficult to ensure:
CIOs can meet these challenges by changing the application architecture from a big object to an architecture design composed of small independent objects. All parts are integrated with each other providing the overall functionality of the application. The change of one part doesn’t change the characteristics and functionality of other parts. This means that each part works as an independent process, respectively, service. This concept is also known as microservice architecture.
A microservice is an encapsulated functionality and is developed and operated independently. So, it is a small autonomous software component (service) that provides a sub-function within a big distributed software application. Thus, a microservice can be developed and provided independently and scales autonomous.
Application architectures based on microservices, are modularized and thus can be extended with new functionalities easier and faster as well as better maintained during the application lifecycle.
Compared to traditional application architectures, modern cloud based architectures are following a microservice approach. This is because of the cloud characteristics cloud native application architectures have to be adapted to. This means that issues like scalability and high-availability have to be considered from the very beginning. The benefits of microservice architectures are related to the following characteristics:
Another benefit of microservice architectures: A microservice can be used in more than one application. Developed once it can serve its functionality in several application architectures.
Today, a number of providers already understood the meaning of microservice architectures. However, in particular the big infrastructure players have their difficulties with this transformation. Startups respectively cloud native companies show how it works:
The providers, based on their microservice portfolios, are offering a programmable modular construction system of ready services that are accelerating the development of an application. These are ready building blocks (see hybrid and multi-cloud architectures), whose functionalities don’t have to be developed again. Instead they can be used directly as a “brick” within the own source code.
Netflix, the video on demand provider, is not only a cloud computing pioneer and one of the absolute role models for IT architects. Under the direction of Arian Cockroft (now Battery Ventures) Netflix has developed an own powerful microservice architecture to operate its video platform high scalable and high available. Services include:
All microservices, Netflix encapsulates within its “Netflix OSS” that can be downloaded as open source from Github.
An example from Germany is Autoscout24. The automotive portal is facing the challenge to replace its 2000 servers that are distributed over 2 data centers, and the currently used technologies based on Microsoft, VMware and Oracle. The goal: a microservice architecture supported by a DevOps model to implement a continuous delivery approach. Thus, Autoscout24 wants to stop its monthly releases and instead provide improvements and extensions on a regular basis. Autoscout24 decided for the Amazon AWS cloud infrastructure and already started the migration phase.
Despite the benefits, microservice architectures come along with several challenges. Besides the necessary cloud computing knowledge (concepts, technologies, et.al.) these are:
Today, standard web applications (42 percent) still represent the major part at public IaaS platforms. By far mobile applications (22 percent), media streaming (17 percent) and analytics services (12 percent) follow. Enterprise applications (4 percent) and Internet of Things (IoT) services (3 percent) are still playing a minor part. The reason for the current segmentation: websites, backend services as well as content streaming (music, videos, etc.) are perfect for the public cloud. On the other hand enterprises are still sticking in the middle of their digital transformation and evaluate providers as well as technologies for the successful change. IoT projects are still in the beginning or among the idea generation. Thus in 2015, IoT workloads are only a small proportion on public cloud environments.
Until 2020 this ratio will significantly change. Along with the increasing cloud knowledge within the enterprises IT and the ever-expanding market maturity of public cloud environments for enterprise applications the proportion of this category will increase worldwide from 4 percent to 12 percent. Accordingly, the proportion of web and mobile applications as well as content streaming will decrease. Instead worldwide IoT workloads will almost represent a quarter (23 percent) on public IaaS platforms.
These influences are challenging CIOs to rethink their technical agenda and thinking about a strategy in order to enable their company to keep up with the shift of the market. Therefore they have to react to the end of the application lifecycle early enough by replacing old applications and look for modern application architectures. However, a competitive advantage only exists if things are done differently from the competition and not only better (operational excellence). This means that CIOs have to contribute significantly by developing new business models and develop new products like an IT factory. The wave of new services and applications in the context of the Internet of Things (IoT) is just one opportunity.
Microservice architectures support IT departments to respond faster to the requirements of specialty departments to ensure a faster time-to-market. In doing so, independent silos need to be destroyed and a digital umbrella should be stretched over the entire organization. This includes the introduction of the DevOps model to develop microservices in small and distributed teams. Modern development and collaboration tools are enabling this approach for worldwide-distributed teams. This helps to avoid shortage of skilled labor at certain countries by recruiting specialist from all over the world. So, a microservice team with the roles product manager, UX designer, developer, QA engineer and a DB admin could be established across the world, which accesses the cloud platform via pre-defined APIs. Another team composed of system, network and storage administrators operates the cloud platform.
Decision criteria for microservice architectures are:
However, the introduction of microservice architectures is not only a change on the technical agenda. Rethinking the enterprise culture and the interdisciplinary communication are essential. This means that also the existing IT and development teams needs to be changed. This can happen either by internal trainings or external recruiting.
Originally published at analystpov.com.
Gartner Analyst covering Infrastructure Services & Digital Operations. These are my own opinions.
2 
2 
2 
Gartner Analyst covering Infrastructure Services & Digital Operations. These are my own opinions.
"
https://medium.com/the-internal-startup/migrating-on-premise-applications-to-the-cloud-8d15e385f22e?source=search_post---------28,"There are currently no responses for this story.
Be the first to respond.
Obtained from the AWS Value Proposition:
"
https://medium.com/google-cloud/twigcp186-e24ccafd458f?source=search_post---------29,"There are currently no responses for this story.
Be the first to respond.
Here are the links from the latest This Week in Google Cloud’s video series :
Top GCP stories for this past week include :
From the “from the people using Cloud Functions and Cloud Run” department :
From the ever-growing “How-to” department :
From the “lovely community contribution” department :
From the “lovely graphs for nasty violations” department :
From still my favorite “Customers and partners talk best about GCP” department :
From the “Beta, GA, or what?” department :
From the “all things multimedia” department :
That is all for this week, the next recap will come late August!-Alexis
Google Cloud community articles and blogs
5 
5 claps
5 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Google Cloud Developer Relations
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/@alibaba-cloud/how-to-execute-mars-in-a-distributed-manner-775d68f3cf9f?source=search_post---------30,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Apr 3, 2019·11 min read
We have already introduced what Mars is in a previous article, and have made it open source on GitHub after testing it on our internal systems. This article introduces the distributed execution architecture implemented by Mars.
Mars provides a library for distributed execution of tensors. The library is written using the actor model implemented by mars.actors and includes schedulers, workers and web services.
The graph submitted by the client to Mars Web Service consists of tensors. The web service receives the graphs and submits them to a scheduler. Before submitting the job to each worker, a Mars scheduler compiles the tensor graph into a graph composed of chunks and operands, and then analyzes and splits the graph. Then, the scheduler creates a series of OperandActors that control the execution of a single operand in all schedulers based on the consistent hash. Operands are scheduled in a topological order. Once all operands have been executed, the entire figure is marked as completed, and the client can pull the results from the Web. The entire execution process is shown in the following figure.
The client submits jobs to the Mars service through the RESTful API. The client writes the code on the tensor, and then converts the tensor operation into a graph composed of tensors through session.run(tensor) and submits it to the web API. After that, the web API submits the job to the SessionActor and creates a GraphActor in the cluster for graph analysis and management. The client starts to query the execution state of the graph until the execution ends.
In the GraphActor, we convert the tensor graph into a graph composed of operands and chunks according to the chunks settings first. The process enables the graph to be further split and executed in parallel. Afterwards, we conduct a series of analyses on the graph to obtain the priority of operands and assign workers to the starting operand. For details about this part, see the section “Prepare the execution graph”. Then, each operand creates an OperandActor to control the specific execution of the operand. When the operand is in the READY state (as described in the Operand state section), the scheduler selects the target worker for the operand, and then the job is submitted to the worker for actual execution.
When an operand is submitted to a worker, the OperandActor waits for a callback on the worker. If the operand is executed successfully, the successor for the operand is scheduled. If the operand fails to execute, the OperandActor tries several times. If it still fails, the execution is marked as failed.
Clients can cancel running jobs using the RESTful API. The cancel request is written to the state storage of the graph, and the cancel interface on the GraphActor is called. If the job is in the preparation phase, it ends immediately after the stop request is detected, otherwise the request is sent to each OperandActor and the state is set to CANCELLING. If the operand is not running at this time, the operand state is directly set to CANCELLED. If the operand is running, the stop request is sent to the worker and causes an ExecutionInterrupted error, which is returned to OperandActor. At this time, the operand state is marked as CANCELLED.
When a tensor graph is submitted to the Mars scheduler, a finer-grained graph composed of operands and chunks is generated according to the chunks parameters contained in the data source.
After the chunk graph is generated, we scale down the size of the graph by fusing the adjacent nodes in the graph. This fusing also enables us to make full use of accelerated libraries, such as numexpr, to accelerate the computation process. Currently, Mars only fuses operands that form a single chain. For example, when the following code is executed:
Mars fuses the ADD and SUM operands into a FUSE node. Rand operands are not fused, because they do not form a simple straight line with ADD and SUM.
It is critical to allocate workers to operands for the performance of the graph execution. Random allocation of initial operands may result in huge network overhead and imbalance of job allocation among different workers. The allocation of non-initial nodes can be easily determined according to the physical distribution of the data generated by its precursor and the idle state of each worker. Therefore, in the preparation phase of the execution graph, we only consider the allocation of initial operands.
We should follow several principles for initial worker allocation. First, the operands allocated to each worker needs to be balanced as much as possible. This enables the computing cluster to have a higher utilization rate during the entire execution phase, which is especially important in the final phase of execution. Second, the initial node allocation requires minimal network traffic when subsequent nodes are executed. In other words, the initial node allocation should fully follow the locality principle.
It should be noted that the principles above may conflict with each other in some cases. An allocation solution with minimal network traffic may be very skewed. We have developed a heuristic algorithm to obtain the balance between the two goals. The algorithm is described as follows:
When a graph composed of operands is executed, an appropriate execution sequence reduces the amount of data temporarily stored in the cluster, thus reducing the possibility of data being dumped to disk. An appropriate worker can reduce the total network traffic during execution.
An appropriate execution sequence can significantly reduce the total amount of data temporarily stored in the cluster. The following figure shows an example of the Tree Reduction. The circles represent operands, the squares represent chunks, red represents the operand is being executed, blue represents the operand can be executed, green represents the chunks generated by the operand have been stored, and gray represents the operand and its related data have been released. Assuming that we have two workers and each operand uses the same amount of resources, each figure below shows the state after 5 time units of execution under different policies. The figure on the left shows that the nodes are executed according to the hierarchy, while the figure on the right shows that the nodes are executed in order of depth-first priority. The data of 6 chunks needs to be temporarily stored in the left graph, while the data of only 2 chunks needs to be stored in the right graph.
Our goal is to reduce the total amount of data stored in the cluster, so we have set a priority policy for operands in the READY state:
When the scheduler is ready to execute the graph, the worker of the initial operand has been determined. We allocate workers for subsequent operators based on the worker where the input data is located. If a worker has the largest input data size, the worker is selected to execute subsequent operands. If there are multiple workers with the same input data size, the resource state of each candidate worker plays a decisive role.
Each operator in Mars is separately scheduled by an OperandActor. The execution process is a process of state transition. In the OperandActor, we define a state transition function for the process of entering each state. The initial operand is in READY state during initialization, while the non-initial operand is in UNSCHEDULED state during initialization. When the given conditions are met, the operand is transitioned to another state and the corresponding operations are performed. The process of state transition can be seen in the following figure:
The following describes the meaning of each state and the operations performed by Mars in these states.
A Mars worker contains multiple processes to reduce the impact of GIL on execution. The specific execution is completed in an independent process. To reduce unnecessary memory copying and inter-process communication, a Mars worker uses shared memory to store execution results.
When a job is submitted to a worker, it is first placed in a queue waiting for memory allocation. When the memory is allocated, the data on other workers or the data that has been dumped to the disk on the current worker is reloaded into the memory. At this point, all the data required for computing is already in the memory, and the actual computing process is ready to start. When the computation is completed, the worker puts the job into shared storage. The transition relationships of the four execution states are shown in the figure below
A Mars worker controls the execution of all operators in the worker through an ExecutionActor. The actor itself is not involved in actual operations or data transfers, and only submits tasks to other actors.
The OperandActor in the scheduler submits jobs to the worker through the enqueue_graphcall on the ExecutionActor. The worker accepts the submission of operands and caches it in the queue. When the job can be executed, the ExecutionActor sends a message to the scheduler, which determines whether to execute the operation. When the scheduler determines to execute the operand on the current worker, it calls the start_executionmethod and registers a callback through add_finish_callback. This design allows execution results to be received at multiple locations, which is valuable for fault recovery.
The ExecutionActor uses the mars.promise module to process execution requests from multiple operands simultaneously. The specific execution steps are linked through the ""then"" method of the Promise class. When the final execution result is stored, the previously registered callback is triggered. If an error occurs in any of the previous execution steps, the error is passed to the handler function registered by the catch method and processed.
All operands in the READY status are submitted to the worker selected by the scheduler. Therefore, for most of the execution time, the number of operands submitted to the worker is usually larger than the total number of operands that a worker can handle. The worker needs to sort the operands and then select some of them to execute. This sorting process is performed in the TaskQueueActor, which maintains a priority queue that stores information about the operands. At the same time, the TaskQueueActor regularly runs a job allocation task, allocating execution resources for the operands in the top of the priority queue until there are no additional resources to run operands. This allocation process is also triggered when a new operand is submitted or when the execution of an operand is completed.
A Mars worker manages two aspects of memory. The first part is the private memory of each worker process, which is held by each process itself. The second part is the memory shared by all processes, which is held by plasma_store in Apache Arrow.
To avoid process memory overflow, we have introduced a worker-level QuotaActor to allocate process memory. Before an operand starts executing, it sends a batch of memory requests to the QuotaActor for input and output chunks. If the remaining memory space can meet the request, the request is accepted by the QuotaActor. Otherwise, requests are queued to wait for free resources. When the relevant memory is released, the requested resources are also released. At this point, the QuotaActor can allocate resources to other operands.
The shared memory is managed by plasma_store, which usually takes up 50% of the total memory. There is no possibility of overflow, so this part of memory is allocated directly through the relevant plasma_store method without going through the QuotaActor. When the shared memory is used up, the Mars worker tries to dump some unused chunks to the disk to free up space for new chunks.
The data in chunks dumped from the shared memory to the disk may be reused by subsequent operands, while reloading the data from the disk into the shared memory may be very IO resource-intensive, especially when the shared memory is exhausted and other chunks need to be dumped to the disk to accommodate the loaded chunks. Therefore, when data sharing is not needed (for example, the chunk is only used by one operand), we load the chunk directly into the private memory of the process, instead of the shared memory. This can significantly reduce the total execution time of the job.
Mars is currently undergoing rapid iteration. We are considering implementing the worker-level failover and shuffle support in the near future, and scheduler-level failover is also in planning.
Reference:https://www.alibabacloud.com/blog/how-to-execute-mars-in-a-distributed-manner_594633?spm=a2c41.12715028.0.0
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
1 
1
1 clap
1 
1
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/crustnetwork/crust-storage-market-internal-testing-has-successfully-ended-d2fa570a22ff?source=search_post---------31,"There are currently no responses for this story.
Be the first to respond.
As of February 24, the three-week Crust Decentralized Storage Market technology internal testing activity has ended. According to the background data, there were more than 100 users registered for the Crust internal testing activity in the community. We can feel the enthusiasm and expectation of the community users for the Crust storage market.
At present, the collection of Issues submitted on GitHub (link: https://github.com/crustio/crust/issues/new) has been stopped as well. We will mark the eligible issues with cool-proposal tags before 18:00 on February 26th according to the following reward standard. At that time, the community users who have submitted an Issue need to login and check, and also reply with your Maxwell address. Rewards will be distributed within 3 working days. If you have any questions, feel free to contact Crust admin in the Telegram.
As shown in the image
Reward Distribution Standard:
1. For the users who find simple errors, such as typos, graphic discrepancies, etc. (referring to a page), will get a reward of 1,000 Candy.
2. For the users who propose that the application layer operation process is not smooth, etc., will get a reward of 5,000 Candy.
3. For the users who find out issues about the protocol layer, will get a reward of 10,000 Candy.
The user-oriented decentralized storage market will be officially launched soon. At the same time, the storage market function will be integrated with the IPFS function of Crust Apps. Please pay attention to subsequent announcements.
🔸 Official Website
🔸 Crust Twitter
🔸 Crust Facebook
🔸 Crust Telegram
🔸 Crust Discord
🔸 Crust Subreddit
Decentralized Cloud Blockchain Technology
Written by
CRUST implements the incentive layer protocol for decentralized storage, and vision to building a decentralized cloud ecosystem.
Decentralized Cloud Blockchain Technology
Written by
CRUST implements the incentive layer protocol for decentralized storage, and vision to building a decentralized cloud ecosystem.
Decentralized Cloud Blockchain Technology
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/altoros-blog/t-mobile-slashes-production-time-from-7-months-to-days-with-cloud-foundry-37fd14ddfc90?source=search_post---------32,"There are currently no responses for this story.
Be the first to respond.
Aspiring for a major transformation with the “uncarrier” initiative, T-Mobile has introduced a lot of internal technical innovations. The blog post explores how Cloud Foundry helped the company to speed up the release cycle, eliminate the scalability issues, and improve configuration management. In addition, the article overviews the role of a new DevOps culture and APIs in the transformation processes.
Read the full article on our blog:
bit.ly
Driving digital transformation with cloud-native platforms, blockchain, ML/RPA.
Written by
Altoros provides consulting and fully-managed services for cloud automation, microservices, blockchain, and AI&ML.
Driving digital transformation with cloud-native platforms, such as Kubernetes and Cloud Foundry. Helping to disrupt industries with blockchain and ML/RPA.
Written by
Altoros provides consulting and fully-managed services for cloud automation, microservices, blockchain, and AI&ML.
Driving digital transformation with cloud-native platforms, such as Kubernetes and Cloud Foundry. Helping to disrupt industries with blockchain and ML/RPA.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/best-practices-for-building-secure-global-networks-internal-and-external-10aab7e6f123?source=search_post---------33,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Jul 20, 2020·4 min read
By Alibaba Cloud Network
One of the major challenges to cybersecurity comes from access between networks in different regions. When connecting Alibaba Cloud network products in different regions, Cloud Enterprise Network (CEN) serves as a secure, global private network that provides high performance and low latency within Alibaba Cloud. By using CEN, you can establish private network connections between Virtual Private Cloud (VPC) networks in different regions, or between VPC networks and on-premises data centers. CEN supports automatic route distribution and learning, which speeds up network convergence, improves the quality and security of cross-network communications, and interconnects all network resources. With these benefits, CEN can help you build an extended enterprise-level network with cross-network communication capabilities. As the basic component for enterprise connectivity, CEN provides outstanding security. By using typical access control policies combined with cloud services such as Cloud Firewall and PrivateZone, CEN provides enterprises with comprehensive security protection.
As a cloud network, CEN first builds channels for private network intercommunication. Cloud enterprise networks built through CEN are fully private networks that do not need to expose public network entries. This significantly reduces their vulnerability to attacks from public networks, greatly decreasing security risks. You can define strict access control policies and customize rules to permit or deny specific traffic flows. Then, you can apply these access control policies to instances to achieve trusted communication. By implementing routing policies, you can filter route information and modify route attributes. This allows you to define cloud network intercommunication capabilities and configure a wide range of route control capabilities.
CEN access links support encrypted transmission to minimize the risks posed by intermediate links. The cloud network uses Smart Access Gateway (SAG) and establishes private encrypted channels between Alibaba Cloud access points. By rigorously preventing replay attacks and periodically updating keys, this ensures that user traffic is not tampered with or listened to on public network transmission paths. Cloud Firewall allows you to implement access control, traffic analysis, and post-event auditing in scenarios that require intercommunication over public networks and cross-VPC access.
PrivateZone is a private DNS resolution and management service based on Alibaba Cloud VPC environments. By accessing PrivateZone through CEN, you can prevent your business DNS from being exposed to a public network. This helps prevent DNS hijacking and domain name pollution.
Customers in any region around the world can access the same services over the Internet, but the access quality and experience vary greatly from one customer to another. Access links on the Internet are uncontrollable. Access requests hop between multiple nodes to reach the destination server, the intermediate nodes are not under control, and the request and response paths may be different. Each node a request passes through can affect performance by introducing delays or jitters. These nodes can also affect service quality due to congestion and packet loss.
To solve these problems, Alibaba Cloud’s Apsara Cloud Network Management launched Global Accelerator (GA), which is a global network acceleration service. Based on Alibaba’s high-quality BGP bandwidth and global network, this service can direct user traffic to nearby acceleration nodes and deploy applications across regions. This can reduce the negative impact of network issues such as latency, jitter, and packet loss on service quality, providing global users with a high-availability and high-performance network acceleration service.
GA integrates scheduling and acceleration capabilities to provide high-quality, high-performance, high-availability, secure, reliable, and easy-to-deploy network acceleration services. The integration of Anti-DDoS Pro and Web Application Firewall (WAF) provides tiered security protection for enterprise applications.
GA provides free protection against DDoS attacks with a rate of up to 2 to 5 Gbit/s. Linked with Anti-DDoS Pro, GA can defend against attacks from hundreds of Gbit/s. Requests from terminals are cleansed of DDoS traffic before entering the acceleration network, ensuring the continuous availability of Internet application services. This provides a highly secure cross-region acceleration solution for global mobile Internet service providers.
For web applications, GA integrates WAF. Linked with WAF, GA provides a highly secure cross-region acceleration solution for global web application providers based on cloud security and big data capabilities.
Finding ways to empower government and enterprise users with intelligent network capabilities will be a key strategy and breakthrough point for future network development. After cloud migration, users must adopt completely new network construction services and operation methods. This means many existing network service tools and systems need to be completely reconstructed. This also represents many new opportunities for the networking industry. Improving efficiency by using intelligent network services will be a major future trend. Given the ongoing development of its digital economy, China is gradually establishing its position as the center of the global digital economy. Whether Chinese enterprises go overseas or multinational enterprises run business in China, networks will become the basic infrastructure that connects branches inside and outside the country. This will make networks the factor that most directly affects productivity. For the Alibaba Cloud Network team, our ultimate mission is to simplify the networks.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/@IBMDeveloper/ibm-cloud-hyper-protect-services-protect-your-organization-from-internal-and-external-threats-d7af9af88a18?source=search_post---------34,"Sign in
There are currently no responses for this story.
Be the first to respond.
IBM Developer
Jun 14, 2019·1 min read
As a developer, you probably understand how important data security is — and this holds true whether you are a founder of the next great tech startup or part of a large enterprise team. Barely a month goes by without a high-profile story in the news about a data breach at a major company, and those are just the ones that were discovered and worth reporting on.
Regardless of company size, data protection is as relevant now as it’s ever been. More recently, we’ve even heard of creative compromises where organizations believed their sensitive data was secure, but since they didn’t secure data they didn’t believe was sensitive, they found themselves vulnerable to attack. Even beyond specific incidents, many organizations are bound by compliance requirements (such as PCI DSS, GDPR, and HIPAA). As more and more countries evaluate and implement their own requirements, conversations around keeping sensitive data secure continue to evolve. Why rely on policy when you can rely on technology?
You can read the full blog post on IBM Developer.
Originally published at https://developer.ibm.com.
Open source, code patterns, tutorials, meet-ups, challenges, mentoring, and a global coding community — all in one place.
Open source, code patterns, tutorials, meet-ups, challenges, mentoring, and a global coding community — all in one place.
"
https://medium.com/@IBMDeveloper/ibm-cloud-hyper-protect-services-protect-your-organization-from-internal-and-external-threats-7cf9f00216c0?source=search_post---------35,"Sign in
There are currently no responses for this story.
Be the first to respond.
IBM Developer
Jun 14, 2019·2 min read
As a developer, you probably understand how important data security is — and this holds true whether you are a founder of the next great tech startup or part of a large enterprise team. Barely a month goes by without a high-profile story in the news about a data breach at a major company, and those are just the ones that were discovered and worth reporting on.
Regardless of company size, data protection is as relevant now as it’s ever been. More recently, we’ve even heard of creative compromises where organizations believed their sensitive data was secure, but since they didn’t secure data they didn’t believe was sensitive, they found themselves vulnerable to attack. Even beyond specific incidents, many organizations are bound by compliance requirements (such as PCI DSS, GDPR, and HIPAA). As more and more countries evaluate and implement their own requirements, conversations around keeping sensitive data secure continue to evolve. Why rely on policy when you can rely on technology?
This is where a solution that provides data-at-rest and data-in-flight protection can help developers easily build applications with highly sensitive data. To meet this need, IBM Cloud offers a suite of services collectively known as IBM Cloud Hyper Protect Services, which are powered by LinuxONE. These services give users complete authority over sensitive data and associated workloads (even cloud admins have no access!) while providing unmatched scale and performance; this allows customers to build mission-critical applications that require a quick time to market and rapid expansion.
You can read the full blog post on IBM Developer.
Originally published at https://developer.ibm.com.
Open source, code patterns, tutorials, meet-ups, challenges, mentoring, and a global coding community — all in one place.
Open source, code patterns, tutorials, meet-ups, challenges, mentoring, and a global coding community — all in one place.
"
https://medium.com/google-cloud/loadbalancing-grpc-for-kubernetes-cluster-services-3ba9a8d8fc03?source=search_post---------36,"There are currently no responses for this story.
Be the first to respond.
Sample application demonstrating RoundRobin gRPC loadbalancing on Kubernetes for internal cluster services (eg, type: ClusterIP and headless services type: None)
gRPC can stream N messages over one connection. When k8s services are involved, a single connection to the destination will terminate at one pod. If N messages are sent from the client, all N messages will get handled by that pod resulting in imbalanced load.
One way to address this issue is insteaad define the remote service as Headless and then use gRPC’s client side loadbalancing constructs.
In this mode, k8s service does not return the single destination for Service but instead multiple destination addresses back to a lookup request.
Given the set of ip addresses, the grpc client will _itself_ send each rpc to different pods and evenly distribute load. To emphasize, gRPC clients have buit in capability to do this LB…you just have to specify or override the scheme!
Update 1/25/21:
Add GKE Traffic Director (xDS-based Loadbalancing). See https://github.com/salrashid123/gcegrpc/tree/master/gke_td_xds
Update 5/4/20: the Headless service will seed an initial set of POD addresses directly to the client and will not Refresh that set. That means once a client gets the list of IPs it will not know about _new_ pods that kubernetes spins up. It will only keep the existing set (you can remove stale/old clients using the keepalive but that willnot inform you of new pods)
References: gRPC Load Balancing on Kubernetes without Tears
One option maybe to create a connection pool handler that periodically refreshes the set: from @teivah : sample pool implementation
The other is to use xds balancer but that is much too experimental now
You can find the source here:
github.com
for xDS Loadbalancer
github.com
or without Traffic Director (the following is very limited and experimental)
github.com
In the repo above either create the image in cd ~app/http_frontend
or use the one i’ve setup docker.io/salrashid123/http_frontend
The image provides two endpoints: one using a ‘normal’ Cluster service and one “Headless” service.
Wait ~5mins till the Network Loadblancer IP is assigned
Normally, you connect to a k8s service using DNS SRV or directly via provided serviceName. In the example below, its be-srv.default.svc.cluster.local
type: ClusterIP
k8s will return a single IP representing the backend. A grpc client that connects to that IP will terminate the connection at ONE pod. So if the client sends 10 requests over that connection, they will all terminate at the same node.
The following shows one connection and the responses that come back all from one host.
Note: all the responses are from one node
Now connect and test with a “Headless” Service which returns a set of PodIPs to connect to directly.
Given the set, gRPC client will automatically keep track of the destination IPs and periodically refresh the list (so to keep aware of pod health)
Now if you invoke the /backendlb endpoint, you’ll see the responses from different pods!! :)
Note: responses are distributed evenly.
Some other References you maybe interested in:
There are several other modes for grpc on GKE and GCP. Below find some additional blog posts in this series
Google Cloud community articles and blogs
89 
2
89 claps
89 
2
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/design-ibm/ibm-receives-an-honorable-mention-at-the-2018-fast-co-innovation-by-design-awards-6f4b8dd99f1a?source=search_post---------37,"There are currently no responses for this story.
Be the first to respond.
We’ve had a busy year here in the Hybrid Cloud arm of IBM Design, and I’m excited to announce that another one of our projects has been recognized by the design community. Our new internal Hybrid Cloud Design website has been awarded an Honorable Mention by Fast Company’s Innovation by Design Awards in the Web Design category. The Innovation by Design award honors creative work at the intersection of design, business, and innovation.
The awards, which can be found in the October 2018 issue of Fast Company, recognize people, teams, and companies solving problems through design. This is exciting news for our team, and follows other design awards we’ve won in the last year for our product work, including the Red Dot Award, IF Design Award, and more.
Just doing good design in itself isn’t good enough. As a design practice we need to constantly evangelize, demonstrate innovation, and lead by example — both internally and externally. This internal IBM Hybrid Cloud Design website was designed and built by the IBM Hybrid Cloud designers, to bring focus on our design work, educate our stakeholders about our design practice, and display the impact and benefits that design can bring to the overall success of products and offerings. The site is used on a daily basis by various types of users, including designers and researchers of the organization, as well as engineering, product management and executive stakeholders.
The website is an educational resource raising awareness about the design process, the team’s various deliverables, and the design disciplines within the organization (user research, UX, visual design and prototyping). It’s also a management system for our team to track the execution of the various design initiatives and to share project work with each other and our various stakeholders.
We designed the website with several types of users in mind:
Each section combines specialized productivity for a primary audience, yet also includes informative content for other groups of users who would benefit from learning more about the practice that is outside of their usual role. All the information is seamlessly presented to users so that they can transition throughout the website.
Another goal and focus of our website is to feature a strong brand and voice for the IBM Hybrid Cloud Design team, as well as a promote a singular voice and tone to our users via our products. We used this opportunity to express our personality and introduce our perspective of diversity in creativity.
Again, we aimed to bring the focus on design within our organization, while displaying all the various practices and projects that we work on. Each main page of the website features detailed graphics, micro interactions and animations that go along with the topic of that page.
Stylized icons and illustrations accompany design directories, research personas, and more, helping create that strong voice and brand for our organization. With the effort put into the design thinking of our website, we were able to distinguish ourselves as a design team of force within IBM and within the software industry.
An important facet of this website was to enable a connection and transparency between designers and engineers or managers within the software development lifecycle. One of our challenges was to integrate the role of design and user experience at a large scale into our organization. This website, as part of the overall effort, is a great example of fostering that integration by inviting everyone at IBM to find out about the ongoings of the designers in the Hybrid Cloud org. I’m thrilled that our team’s hard work has been recognized and this is evidence that our mission to focus on design awareness, design execution and design transparency in the enterprise is integral to long term success.
Oh and for some icing on the cake, IBM was a finalist in the Fast Co Apps and Games category and also won two more Fast Co honorable mention awards: Design Company of the Year and General Excellence for IBM Plex. Congrats to Phil Gilbert and all IBM designers, and Todd Simmons and team. IBM Design rocks.
Arin Bhowmick (@arinbhowmick) is Vice President, Design at IBM based in San Francisco, California. The above article is personal and does not necessarily represent IBM’s positions, strategies or opinions.
Stories from the practice of design at IBM
78 
78 claps
78 
Stories from the practice of design at IBM
Written by
Global VP Design | Chief Design Officer, @IBM |Cloud, Data and AI I UX Leadership| UX Strategy| Usability & User Research| Product Design
Stories from the practice of design at IBM
"
https://medium.com/google-cloud/what-is-community-when-everyone-is-isolated-2a789ef290ff?source=search_post---------38,"There are currently no responses for this story.
Be the first to respond.
What does community mean in the middle of a crisis that’s isolated us all so profoundly from each other? What happens to company culture when a physical community isn’t around to support it?
That’s what Google — already well-known for its rich, open culture, and creative office spaces that enable thousands of connections — has had to ask itself…
"
https://medium.com/google-cloud/how-to-query-ether-supply-in-bigquery-90f8ae795a8?source=search_post---------39,"There are currently no responses for this story.
Be the first to respond.
Recently we announced beta availability of transaction traces (aka internal transactions) in our public Ethereum dataset in BigQuery: https://bigquery.cloud.google.com/table/bigquery-public-data:crypto_ethereum.traces (updated daily). They allow querying all Ethereum addresses with their balances, as well as total Ether supply.
Below is the query that retrieves total Ether supply on every date:
Run it in BigQuery
I filter out only the traces with type genesis, which include the initial Ether allocactions in block 0; and thereward type, which include both block and uncle rewards.
Below are the query results visualized in Data Studio:
From the graph you can see that on October 16th of 2017 (block 4370000) Ether supply growth rate decreased. This is associated with the block reward reduction from 5 Ether to 3 Ether, according to this EIP https://github.com/ethereum/EIPs/blob/master/EIPS/eip-1234.md
It’s possible to query running Ether balances for every address on every date. Try to compose a query that will return the number of addresses with non-zero balance, and plot it over time. Post your SQL in the comments. (Solution)
Also read:
Google Cloud community articles and blogs
67 
67 claps
67 
Written by
Creator of https://github.com/blockchain-etl, Co-founder of https://d5.ai and https://nansen.ai, Google Cloud GDE, AWS Certified Solutions Architect
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Creator of https://github.com/blockchain-etl, Co-founder of https://d5.ai and https://nansen.ai, Google Cloud GDE, AWS Certified Solutions Architect
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/google-compute-engine-metadata-server-emulator-fe0fb1e5a8b5?source=search_post---------40,"There are currently no responses for this story.
Be the first to respond.
This script acts as a GCE’s internal metadata server for local testing/emulation.
It returns a live access_token that can be used directly by Application Default Credentials transparently.
This is useful to test any script or code locally that my need to contact GCE’s metadata server for custom, user-defined variables or access_tokens.
Another usecase for this is to verify how Application Defaults will behave while running a local docker container: A local running docker container will not have access to GCE’s metadata server but by bridging your container to the emulator, you are basically allowing GCP API access directly from within a container on your local workstation (vs. running the code comprising the container directly on the workstation and relying on gcloud credentials (not metadata)).
For more information on the request-response characteristics:
The script performs the following:
The endpoints that are exposed are:
You are free to expand on the endpoints surfaced here..pls feel free to file a PR!
Overall, the proxy works to emulate the link-local address 169.254.169.254 that is used by the metadata server. Once a route to that is established (via iptables or socat), you can run a simple http webserver that responds back with the request details.
NOTE: this post is a copy of my github repo.
github.com
Updated: 2/10/16: Added in container->container networking; minikube access
Update: 8/28/19: refactored from python->go, removed a lot of confusing stuff
This script runs a basic webserver and responds back as the Google Compute Engine’s metadata server. A local webserver runs on a non-privleged port (default: 8080) and optionally uses a gcloud cli wrapper to recall the current contexts/configurations for the access_token and optional live project user-defined metadata. You do not have to use the gcloud CLI wrapper code and simply elect to return a static access_token or metadata.
You can run the emulator either:
The following steps details how you can run the emulator on your laptop.
1. Reconfigure the /etc/hosts to resolve the metadata server
Note: dockerimages/metadatadns/ contains a DNS server with the google.internal as a zone. You can use that as well (described at the end of this document).
2. Create metadata IP alias
GCE’s metadata server’s IP address on GCE is 169.254.169.254. Certain application default credential libraries for the metadata server by IP address. The following steps creates an IP address alias for the local system.
You can veirify the alias was created by checking ifconfig
(on windows)
3. Run socat
You need to install a utility to map port :80 traffic since REST calls to the metadata server are HTTP. The following usees ‘socat’:
Alternatively, you can create an OUTPUT iptables rule to intercept and redirect the metadata traffic.
4. Downlaod JSON ServiceAccount file
Create a GCP Service Account JSON file
You can assign IAM permissions now to the service accunt for whatever resources it may need to access
5. Run the metadata server
or via docker
6. Test access to the metadata server In a new window, run
If you run an app inside a docker container that needs to access the metadata server, there are two options:
Add bridge networking to the running Container ( — net=bridge)
To use bridge networking, you need to first
You may need to drop existing firewall rules and then restart the docker daemon to prevent conflicts or overrides.
Add host networking to the running Container ( — net=host)
This will allow the container to ‘see’ the local interface on the laptop. The disadvantage is the host’s interface is the containers as well
NOTE: using — net=host is only recommended for testing; For more information see:
Since GCE’s metadata server listens on http for :80, this script relies on utilities like ‘socat’ to redirect port traffic. socat has pretty basic connection handling so you’d be better with iptables, gunicorn. You are free to either run the script on port :80 directly (as root), or use a utilitity like iptables, HAProxy, nginx, etc to do this mapping.
The following example of an iptables route 80 -> 8080 on the local interface
You can extend this sample for any arbitrary metadta you are interested in emulating (eg, disks, hostname, etc). Simply add the routes to the webserver and handle the responses accordingly. It is recomended to view the request-response format directly on the metadata server to compare against.
Instead of explictly setting routes, use the local filesystem to return the strucure for non-dynamic content or attributes. In this way, the metadata server just returns the directory and files that mimics the metadata server structure.
eg: create a directory structure similar to:
and access the path using a handler like this
However, that currently returns HTML content, the metadata server new-line text as Content-Type: application/text
TODO: figure out how to return text payload similar to the metadata server
2 Redirects
Metadata server currently redirects path from root to one with a /.
TODO: account for the redirect.
Google Cloud community articles and blogs
33 
1
33 claps
33 
1
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://towardsdatascience.com/the-first-nine-primary-concepts-and-code-examples-for-using-kubernetes-productively-d2aabccc0380?source=search_post---------41,"Sign in
Bruce H. Cottman, Ph.D.
Aug 19, 2021·11 min read
An explanation and usage of Kubernetes should not discuss internal architectural components. Instead, I discuss Kubernetes basic concepts with code examples. In the end, as a bonus, Kubernetes has a glaring hole in its design.
"
https://medium.com/world-wcloud-baby/facing-pain-346e64009827?source=search_post---------42,"There are currently no responses for this story.
Be the first to respond.
What drives us to start developing new a product?
In case of our “baby” app, the starting point was the “PAIN” — not the kind of pain we feel when we can’t hail a taxi (Uber) or can’t find a place to stay (Airbnb), but the psychological pain when we cannot talk to anyone even when we have an urge to do so.
When I think back, it was a good way to start, starting with my own internal pain. That’s because the opportunities to think about a concrete solution will come every time we feel a pain.
Also, if we think of a pain as an inquiry, the solution for the pain would be the answer to that question. Once we find a solution, that also means that we have found a way to use it and people who want to use it.
When we start something through our own pain, we gain the opportunity to think the unique and only one solution.
If we come up with a good solution, we can recommend it to others with a deep sense of satisfaction. This is a really good thing from the marketing point of view. We are not good at recommending anything to others unless we ourselves are fully convinced of its value.
When we think about application, it might be not good way to think about successful existing application.
It’s because it was the specific pain for that particular person, and the solution was for the person’s specific pain only. It is not easy for others to understand the internal pain and the solution that are specifically geared towards dealing with the person’s internal pain.
In case of our “baby” app, we were trying to come up with a solution so that we can talk to someone whenever we want to. There were some focal points to consider: first thing to consider was “with whom,” and second thing was that this would deal with “the very act of talking.”
Even from the beginning, we were pretty clear about the first point: we want to talk to ANYONE, anonymous strangers we do not even know. Encountering complete strangers anonymously sounded like a great idea.
The point is that we felt a huge possibility in encountering others for a chat (the first conversation with a stranger is often, albeit not always, the most exciting conversation you will have with them.)
For the second point, we thought about the very act of talking. We started to think of the way we talk not like the way we do in a text chat, but like we do in a telephone chat.
Then we noticed that the pain we feel when we make a phone call has not been eased even with the emergence of digital phone apps.
“Someone could be an anonymous stranger”- it includes a solution for those who shut themselves up in the social media domain because of the difficulties of human relationship. We cannot escape or hide from SNS. But we doubted if we can talk whatever we like since the SNS has obstructive in one side.
By focusing on the very act of talking, we realized that this application includes the solution for the difficulties and pain people feel when making a phone call. The phone call or other digital chatting applications inflict unbearable pain on us. In other words, the pain of using a phone gave birth to our “baby” app.
We came to realize that the solution of “baby” app includes the solution for the serious pain of phone call or digital chat. This painful condition (of when I want to talk but cannot) was the cue to redefine the phone.
The verification or hypothesis always requires assembling the pieces of that theory ourselves. First you need to try and test it yourself. Then you need to ask others to try it. I found, through developing this app, that we cannot move forward without having first tested and experienced ourselves. This is the invariable principle.
It is really difficult to estimate the effectiveness of the solution without any first-hand experience. In other words, if you feel satisfied with the solution and feel that the pain has been eased by it, the solution probably is the right one.
I think it is meaningful to think of an application as a solution for the pain we face internally. We hypothesize, test the hypothesis, and brush up on the theory. Rinse and repeat. Repeating these processes is the essence of application development. Stay tuned!
All you need is a conversation
7 
7 claps
7 
Written by
DOKI DOKI, INC. CEO & Telepathy Fellow
All you need is a conversation
Written by
DOKI DOKI, INC. CEO & Telepathy Fellow
All you need is a conversation
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@nutanix/introducing-botmetric-smart-reserved-instance-management-for-aws-powered-by-aiops-c22b3b52cf3?source=search_post---------43,"Sign in
There are currently no responses for this story.
Be the first to respond.
Nutanix
Mar 10, 2017·7 min read
AWS users using more than 75% RIs in AWS realize the best savings, reveals an internal customer survey by Botmetric. One of the most expensive mistakes that AWS customers make is not using Reserved Instances (RIs) aptly. Just reserving instances does not reap optimum cost savings. A well maneuvered AWS Reserved Instance management plays a major role in reducing the overall cloud spend. But it takes a lot of effort and time when done manually. Don’t worry. Botmetric has your back with its AWS RI Planner.
With much delight, I’d like to introduce Botmetric Smart RI. Using Algorithmic IT Ops (AIOps), it analyzes, modifies, and monitors RIs automatically without you logging into AWS console, thus saving you time and effort that goes in manually modifying the RIs. It does all the heavy-lifting for you using automation so that you save cost optimally, which would have been impossible if done manually.
Simply put, by configuring this app on Botmetric, you can reduce the effort involved in modifying unused RIs, automate modification of RIs as soon as the unused RIs are found (that occur multiple times a day), and above all save cost that could have been wasted due to unnecessary on-demand usage, along with wasted RIs.
Team Botmetric’s motto is to save your cloud cost as much as possible, and also help you govern your RIs and the cost associated with it optimally. Working day-in day-out on RIs, Botmetric team has observed that RIs go unused for two main reasons:
1. Changing Infrastructure Needs: Because, usage patterns change. At times, new instances are added, or few instances need upgradation from time to time as new modules, new features, or new projects are being launched. Sometimes, due to changing business strategy RIs go unused. This leads to wasted RIs and unnecessary on-demand usage and cost.
2. Miscommunication: Most of the times, the engineer who purchased the RI and the engineer who is launching them and monitoring them are different. In such cases, there are high chances of miscommunication between both of them. When instances are launched without the alignment with the purchased RIs, it leads to RI mismatch.
To get the discounted reserved pricing, it is pivotal that it matches the defined rules. For EC2, these rules include Availability Zone (AZ), Scope of Reservation, Platform, Instance Size, and Instance Type. Botmetric scans through your infrastructure for any on-demand instances that are matching two of the three rules required to take price discount of any unused RIs and highlights those instances.
One amazing thing about AWS RIs is you can modify AWS EC2 RIs any number of times at no cost. AWS has introduced this new flexibility in reservations by providing the support for modifying reserved instances, so that you get better savings.
On the other hand, the only solution for unused reservations (RI discount not being applied) is to: reallocate the RI by modifying the RIs to different zones, changing the scope of RIs, or selling the reservations on the marketplace at loss. If you have purchased reserved instances as the newly launched Convertible RI, then you have the ability to convert the instance types as well as platforms, but it may involve fresh cost.
Essentially, modifying RIs involves changing the factors about your reservation with no cost, unless you have new Convertible RI and you are upgrading/downgrading the reservation. In a typical day-to-day AWS reserved instance management, instances needs to be checked one by one and then change the factors accordingly. Few of you might even follow the scheduled monitoring and analysis of RI from time to time too.
It’s critical that you check and evaluate hourly data and create the right modifications. To this end, you will have to create a list of unused reservations and possible modifications. In simple words, you need to create an analysis report of RI in some form and then act upon it.
It is, however, easier said than done!
Here’s why: you need to look for all the probabilities by having the context of list of instance types you are running within a family, then figuring out if there are unused reservations, then figuring out the combination for modifications matching with running on-demand instances. To ease this, Botmetric built Smart RI.
Botmetric Smart RI automatically modifies the reservation as soon as there is a modification available. It smartly manages RIs, which are aligned for discounts, by analyzing the usage, planning, helping with modifications, and monitoring them from time to time.
Essentially, Botmetric Smart RI helps reduce the effort involved in modifying the unused RIs. It automates modification of RIs that occur multiple times a day as soon as the unused RIs are found. It saves you that much amount of cost that could have been wasted due to unnecessary on-demand usage, along with wasted RIs.
To leverage these benefits, you can easily enable Smart RI by providing the permission of “ec2:ModifyReservedInstances” in the policy you attached with the cross account role created for Botmetric.
Apart from helping with AWS Reserved Instance Management, Botmetric also provides the capability to track the modified RI details from “Modify History.” You can get details like the time when the reservation was modified, the new combinations created by modification, etc.
Further, Botmetric AWS RI Planner also features other cool capabilities, which not only saves time and weeks of effort but also adds cloud intelligence that further helps you with apt RI management, evaluates your cloud utilization, and further saves RI cost.
Here they are below, if you’d like to explore them:
Using Botmetric AWS RI Planner, you can quickly identify what you should plan in regards to RI from your existing infrastructure, with key data available on one single console. Botmetric performs intelligent AIOps on your RI usage, on-demand utilization and existing reserved instances. For each AWS account, Botmetric runs various analysis for all relevant EC2 or RDS machines, and provide you with recommendations on what kind of RI purchase should be done for a specific server.
Using Botmetric AWS RI Planner, you can get a complete view of all the reservations across multiple accounts and multiple AWS regions. It also suggests whether a particular reserved instance is recommended to be renewed or not. Among the list of reservations, you can also know about the RIs that are expiring soon. Additionally, Botmetric also sends a RI expiry alert with the list of reservations expiring in the next 45 days. Furthermore, Botmetric also throws warning messages for RIs that are going to expire in the near future.
Botmetric, using machine learning (ML), keeps track of your RI utilization on a daily basis using your billing data and metadata of current infrastructure. If you have any unused RIs in your accounts, Botmetric detects them and provides recommendations on how these unused RIs of EC2/RDS can be utilized, essentially to reduce your monthly RI spend. Above all, it facilitates you govern and manage unused RIs aptly for maximum savings using Smart RI.
With CSV export option, you can download the reports of Plan RI recommendations, so that you can share it with your team. This will allow you to analyze what should be planned for RI purchase based on your utilization patterns and business needs.
With the EC2 RI Change History, you can get a list of all the EC2 RI modifications performed on Botmetric. Apart from the details associated with each modified EC2 RI, the console also allows you to filter data by Linked Accounts, status, and action initiated, for better understanding of modified EC2 RIs.
Using Botmetric AWS RI Planner, you can quickly analyze your entire RI portfolio and utilization from single graphical view in a matter of minutes. Botmetric recommends you to purchase RI for more machines to reduce your monthly spend, and the top RI recommendations, along with a visualization on on-demand vs. reserved hours.
You can access the Smart RI app and RI Planner from Cost & Governance product in the Botmetric Cloud Management Platform. If you are looking for that one AWS Reserved Instance Management that can help you save optimally, then Botmetric AWS RI Planner is the one. Do take the 14-day trial to experience it first hand. If you are already a Botmetric customer, then do try it and share your feedback below in the comment section or tweet to us @BotmetricHQ . We’d be happy to know how it’s working for you and what we can do to make it better to suit your needs.
A free copy of RI guide is up for a grab, if you’re looking to know about RIs in-depth. Hurry!
We make infrastructure invisible, elevating IT to focus on the applications and services that power their business.
3 
3 
3 
We make infrastructure invisible, elevating IT to focus on the applications and services that power their business.
"
https://towardsdatascience.com/the-second-seven-primary-concepts-and-code-examples-for-using-kubernetes-productively-7170642c4094?source=search_post---------44,"Sign in
There are currently no responses for this story.
Be the first to respond.
Bruce H. Cottman, Ph.D.
Aug 25, 2021·14 min read
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@efipm/data-bezos-and-feng-shui-in-financial-services-62ab22a4d31?source=search_post---------45,"Sign in
There are currently no responses for this story.
Be the first to respond.
Efi Pylarinou
Mar 18, 2021·5 min read
To leverage internal Data & Data Analytics, it is paramount to become internally an API-first organization. Such a digital transformation for any size organization will enable efficient data management, will unlock value through data analytics, and will add resilience. It is never late to start this journey.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@GiantSwarm/why-knowledge-sharing-is-important-ac38576b0811?source=search_post---------46,"Sign in
There are currently no responses for this story.
Be the first to respond.
Giant Swarm
Nov 13, 2020·6 min read
We recently had an internal discussion about documentation — too much, too little, not needed, crucial — back and forth, and back again. It’s a hard subject. Something drove me a little bit crazy in that discussion and I needed to revisit my MBA course for clues and possibly better ways to express my frustration.
In short, there’s a reason that we as Giant Swarm are structured the way we are structured. It hasn’t just grown by itself unattended. The structure was developed on purpose and is being continuously refined. We need to make sure that we do not lose sight of this structure because many people have joined since it was first developed.
Or, why not everything can or should be documented and why we need to include other ways of conveying information and sharing knowledge.
In the Industrial Age, you built a process and optimized that process until you threw up.
The Ford Motor Company introduced the assembly line and optimized each step, bringing cars to the middle class. Cars had never been that cheap before. Toyota then changed things when cars, and the problems building them, became more complex and invented something around continuous improvement and open communication systems.
Building upon that Toyota way, Nonaka and Takeuchi, who studied Toyota and other companies, built the SECI model: Socialization, Externalization, Combination, Internalization.
Some say that Nonaka and Takeuchi’s Book The New New Product Development Game from 1986 is what led to the Scrum process that we know today. Their definition of Scrum is “cross-functional teams engaging in the dynamic conflict of ideas that generates ‘ba’ the energy flow that surfaces the knowledge that forms new products”.
In a system where you want continuous optimization alongside continuous knowledge building, the idea is you need to first socialize (tacit to tacit knowledge) and learn by working with a master and building up shared experiences. Through externalization (tacit to explicit) you create explicit concepts (documents, images, and procedures) for that implicit knowledge. Through combination (explicit to new explicit), new knowledge is born. Then you internalize (explicit to tacit) the shared knowledge and a new cycle starts.
One thing to note is that while in Japan, you might work for a company your entire life. In Germany, culturally, what is not written down, does not exist. In the USA, people change jobs even more quickly than in Europe. We in Europe and the USA are afraid of knowledge not becoming explicit, while in Japan, companies are much more comfortable with the implicit knowledge; knowledge that to a certain extent is locked into people’s minds and out of reach of the corporation.
Or, why in many cases, the one true way does not exist.
We always talk about being in a knowledge economy but seldom integrate what this actually means. The point is that you cannot create a process or a set of documentation and be done with it. Knowledge is created, becomes old, outdated, and superseded by new knowledge constantly, and for that, a lot of people need to interact in constant loops and with chance encounters.
For Giant Swarm, this is especially true as we are a different kind of managed service provider that needs to be experienced to be truly understood. Of course, we try to explain it to people, but once the first problem occurs in a cluster, and the customer sees the team and the process coming to life, they then see that we are truly in this together and move mountains for our customers. In short: through interactions, you see the true knowledge within Giant Swarm that is available at your fingertips.
The important point is that the ‘one true way’ of doing things does not exist anymore and as we are talking about tacit knowledge, moving it from people to the organization (documentation that can be followed) is futile. Instead, you need to facilitate the exchange between people inside and outside of the organization. And some knowledge, like how you deal with customers, can only be learned in a master and apprentice-like model.
That’s the reason we implemented certain processes for our company. As an example, the onboarding for Solution Engineers (SE) has a special process where initially any new SE hops from customer to customer in two week periods. We do this so that they can get to know all of our different customers in person and understand their individual requirements and challenges and is the reason why we now have a primary and secondary (shadow) SE per customer. The shadow SE tries to attend as many meetings as possible and to function as a backup if the primary SE isn’t available.
Once you understand this, once you really understand it, you will stop trying to document certain explicit knowledge as you have learned that too much of the tacit knowledge will be lost. Rather, we need to allow for freedom to reign and enable a large number of interactions.
At Giant Swarm, we do this through our intricate web of teams, chapters, Special Interest Groups (SIGs), and Working Groups (WGs) that are explained in this article. Additionally, it means that in customer interactions we need to ensure that they understand that we are there to drive them forward, not to tread water. While the latter might work with documentation, the former will need a constant exchange of knowledge. A setup for learning in a network of companies.
Remember though that these groups of diverse people are susceptible to groupthink as time passes. You need to employ double-loop learnings and allow for disagreements through a sense of belonging.
We want everyone to be free to share and that’s why we want honesty and are constantly working hard on having and keeping a no-blame culture. One way of sharing knowledge and building bonds within the team is our regular onsites, where we meet in person for 5 days with strong focus on enabling peer to peer interactions. Our Giant Summit Events and our Engineering Days are aimed to extend that sense of belonging towards customers.
However, considering this always; “To maximize variety everyone in the organization should be assured of the fastest access to the broadest variety of necessary information, going through the fewest steps.” — Nonaka and Takeuchi, 1995
Hence, we need clear responsibilities that are findable to be able to find the tacit knowledge that is locked in people’s minds.
Silo culture, blame culture, a heavy emphasis on risk and reward, bureaucracy, and poor communication are just some of the things that have been shown to work against knowledge distribution. To a large extent, Giant Swarm is different here because (as visible above) we need to share to succeed. Through our SIGs, we enable cross-departmental pollination. In our special #meta-listening channel, we actually share some of our deepest personal problems and have managed to move together with customers to a non-blame culture even in light of critical infrastructure issues. As a company, we focus on the happiness of the individual and try to limit the bureaucracy.
Or, the next level of mastery is trusting the unconscious.
To go further in the deep end, it’s now understood that your unconscious can often go further than your conscious and process things faster. It seems that we might be learning faster than we can think or even articulate. In fact, professionals sometimes make decisions that they cannot articulate.
By now, management studies have found that often the decisions successful managers make are not based on numbers but based on intuitive thinking. For this to really work, you need to be totally open to all alternatives because you have to take into account that your own biases are involved. Your own frame of reference that allows you to process the world, but it also means that you pay more attention to the things you know. To break out of your own frame of reference, you can play mind games with yourself, like saying “Yes and” instead of “Yes but”.
This topic has a lot more depth to it which might trigger a separate blog post, but for now, it’s important to show that mastery can unlock things that documentation can’t.
Read on at Our Way of Working with Customers and Partners.
Written by Oliver Thylmann Co-Founder and COO of Giant Swarm
Giant Swarm is a leader in cloud-native infrastructures and provides managed Kubernetes clusters to run containerized applications on-premises and in the cloud.
14 
14 
14 
Giant Swarm is a leader in cloud-native infrastructures and provides managed Kubernetes clusters to run containerized applications on-premises and in the cloud.
"
https://medium.com/building-an-open-startup/create-an-enterprise-social-discussion-space-in-3-minutes-c3369799d23d?source=search_post---------47,"There are currently no responses for this story.
Be the first to respond.
Over 90% of organizations have some kind of social network running internally. As the co-founder of Elgg, one of the first internal social networking solutions, I find this gratifying: 12 years ago, few people saw the potential for social technology beyond personal blogging and sharing.
Unfortunately, too many organizations choose to purchase an enterprise social solution with a Field of Dreams mentality: “if you build it, they will come”. On the contrary, as Harvard Business Review noted recently:
The reality is that the landscape is littered with failed technology deployments. Altimeter’s research shows that less than half of the enterprise collaboration tools installed have many employees using them regularly.
When we work with higher education, we’re keen to point out that technology should support teaching, and not the other way around (a curriculum shouldn’t be moulded around the capabilities of your technology). The same is true in organizations: the way your enterprise functions should be in tune with your business goals, rather than the software you happen to be using.
Forcing people to use an internal social platform is counter-intuitive. Instead, it makes sense to provide a platform that supports you when you need it, in a flexible way. That means it’s got to be simple, support ad-hoc use, but allow you to keep your activity around in case you need to refer to it later. Many organizations also require the ability to audit activity.
If you just need to create a one-off simple space to bring up to 200 people in to discuss a project either privately or publicly, you can use Known Pro. $10 for up to 200 users with unlimited storage and bandwidth is hard to beat. Signing up and inviting your users in takes less than three minutes. (I know: I’ve timed it countless times.)
If you’re the kind of firm that needs to run social spaces on your own infrastructure, for compliance or legislative reasons, Known can provide this too. We can also run Known on a fully-managed private cloud for you.
We can also create a system for your organization that allows you to create unlimited social discussion spaces, all linked to your existing accounts via Single Sign On (in education, we use LTI; in enterprise, LDAP and Shibboleth are available). You can easily keep track of all the project spaces that have been created, and you can control access individually — which means you can invite people from outside your organization to help with a project, on a project-by-project basis.
It’s completely customizable, flexible, easy-to-use, and mobile-first. We’re proud to give you full control over your data and discussions.
To learn more about how Known can help your enterprise, get in touch.
Originally published at stream.withknown.com on August 26, 2015.
Building respectful software: stories from the ground.
1 
1 clap
1 
Building respectful software: stories from the ground.
Written by
Writer: of code, fiction, and strategy. Trying to work for social good.
Building respectful software: stories from the ground.
"
https://medium.com/@Technews/ibm-sets-a-target-for-its-softlayer-f7b83342f776?source=search_post---------48,"Sign in
There are currently no responses for this story.
Be the first to respond.
Tech News
Dec 7, 2015·2 min read
International Business Machines Corporation is aiming to expand in the industry. The New York based company has planned to obtain an internal sales revenue target of $1 billion next year for its cloud- infrastructure enterprise SoftLayer, a person aware of the issue stated. That is an increase in revenue from $700m to $800m for the quarter, the division would probably earn this year as stated the person, who requested that his or her identity shouldn’t be disclosed because the fiscal details are private.
SoftLayer, which the organization acquired two years ago, had earned sales revenue worth of $355m before the acquisition. The main goal underpins the exigency of wringing expansion of the cloud enterprise where the facilities and application are delivered across the web- as clients have shunned other gadgets produced by 104 years old enterprise. IBM, an organization known for selling cloud expertise later has been able to post decreasing revenue’s for 14 straight quarters throughout its transition.
The company’s public and private cloud sales revenue reached $9.4 billion in the year that ended on 30th Sept. “This puts IBM at the forefront of the cloud market,” stated the organization’s spokesperson, refusing to share his or her views regarding the revenue earned by SoftLayer. Of the enterprise’s $92.8bn in sales revenue earned a year ago, $7bn was contributed by software and cloud facilities.
IBM news exclaimed that it is not clear whether revenue of SoftLayer encompasses just infrastructure as a service, or if there is an inclusion of facilities alongside an additional application. Sales of SoftLayer increased by at least 10% in the 3rd quarter, IBM’s CFO Martin Schroeter stated in a conference call that was held in October.
The cloud technology business has proved to be one of the Corporation’s biggest bets, which has invested $2bn in it. IBM news today affirmed that the initiative was taken to boost the company’s public cloud offers and play a role in helping it compete with Microsoft Corporation and Amazon in offering remote computation facilities.
IBM stated last year it would invest $1.2bn to develop 15 supplementary data centers. According to 2016 technology outlook of Bloomberg Intelligence, in the mean times, competitors such as Google, the ecommerce giant and Microsoft would make the cloud computation business more competitive in 2016 by lowering prices and making organizations encompassing IBM find others means to earn money.
IBM Breaking news revealed that the Corporation presently is not officially dividing revenue earned by its various cloud facilities and application, encompassing SoftLayer.
The online trading lately initiated the practice of reporting sales revenue for Amazon Web Services, a pattern that Microsoft and Search engine developer would follow in 2016.
Tech News is like a comprised platform for users to find all technology news on one blog.
Tech News is like a comprised platform for users to find all technology news on one blog.
"
https://medium.com/@eFileCabinet_57957/how-document-management-software-optimizes-activity-based-costing-5c74cfc3579e?source=search_post---------49,"Sign in
There are currently no responses for this story.
Be the first to respond.
eFileCabinet
Jul 12, 2017·4 min read
Document management software optimizes activity based costing efforts by gauging the efficacy and impact of internal controls at the document level — a space in the information management realm that is often left unconsidered in most costing models of accountancy.
Essentially, so many office items relied on today prevent companies and accounting departments from effectively gauging any form of activity based costing, as the inefficiencies of these traditional office items are often left out of the equation of production and volume.
The Chartered Institute of Management Accountants (CIMA), defines activity based costing as the approach to the costing and monitoring of business activities as they pertain to resource consumption. This model seeks to make sense of overhead expenditures rather than direct costs.
They may understand accounting. But they don’t have the efficiency thing down just yet, and eFileCabinet is an industry leading expert in that category.
This lack of inefficiency is one of the many reasons so many workers belonging to the Big 4 are unhappy; they’re overworked for the sake of being overworked — not for the sake of theirs or the company’s excellence. And paper-based processes are entirely to blame for this matter. They breed apathy and inefficiency.
Document management software optimizes activity based costing because it gives accountants the time they need to exercise their skill set effectively.
This ensures that accountants are actually utilizing the skills they received education to acquire — making their output more valuable to the departments, organizations, and firms for which they work.
The Big Four accounting firms in the world could learn a thing or two from the smaller practices and firms when it comes to efficiency, and here’s why.
Overhead costs no longer correlate perfectly with machine hours or direct labor hours.
That, and the demands of the accounting client have grown significantly in the past decade. Now more than ever, accountants are required to do more with fewer resources, and become trusted experts on the subjects ranging from human resources to financial operations.
Technology that allays injurious accounting demand will always optimize outcomes in the activity based costing paradigm, and document management software is one of the
Cloud document management software optimizes activity based costing and other accounting methodologies because it reduces the space between activities and the costs associated with them.
If we look at any traditional cost-volume-profit (CVP) analysis, document management software optimizes activity based costing by reducing the costs associated to produce the same volume of products and services within an organization.
The technology accomplishes this by eliminating paper-dependency and, therefore, inefficient project management document practices within an organization or accounting department.
Document management software’s ability to streamline operations is what makes it the preeminent technology in helping accountants adapt to the ever-changing contours and difficulties of the role.
The major concern for any organization, and one of the main reasons document management software optimizes activity based costing, is because the impact of varying levels of product sold and product price are expected to produce an operating profit outcome.
The fewer number of dollars a company spends to get to its breakeven point in sales, the less time it takes to reach the point of breakeven — mitigating the impact of accounting considerations like opportunity costs and other deleterious expenses.
If there are human beings employed by organizations, which will always be the case, costing models will be inseparable from workplace behavior, productivity, and the interstice of psychology and behavioral economics.
Workplace technology has a more scalable impact on this interstice than any other phenomenon that has preceded it — including 19thcentury industrialization and the invention of hunting tools during the earliest time of our species’ existence.
However, it is only in examining atavistic characteristics of human behavior that we can shed light on how technology proliferation has profoundly altered present day work and office environments.
Document management software optimizes activity based costing because of the role it plays in improving internal processes for organizations with intense accounting strictures.
As an easier way of tying activities to costs, document management software makes it possible to make accurate forecasts and company planning procedures, and isolates variables in a way that will help accounting managers ensure accounting quality controls for the organizations they work in.
With incredibly fast-paced businesses sprouting up left and right all over the United States, the need to reduce the margin of error in all accounting practices is more important than ever.
Only in seeing how document management software optimizes activity based costing can accountants understand the exactitude with which document management technologies operate.
However, we want to take this opportunity to knock down a current myth regarding contemporary accounting practices.
Although it’s important for accountants to be tech-savvier than they’ve been in previous generations, they do not actually need IT skills to embolden their practice and remain relevant in competing with self-service technologies.
That is what the cloud is for, and that is why more accountants are turning to cloud-based technologies than ever before. In fact, eFileCabinet has 5 very good reasons that accountants’ cruising altitude is in the cloud.
That’s why the best document management vendors will be able to do those jobs for you at a one-time, flat software subscription fee, whereas laggard accounting practices are spending money on interns and filing clerks to handle these matters.
However, there’s also a reason you’re more than a document scanner and data entry specialist as it pertains to document management software: because document management software optimizes activity based costing.
To learn more about why document management software simplifies activity based costing in the accounting industry, please fill out the form or talk with one of our business efficiency experts.
Originally published at www.efilecabinet.com on July 12, 2017.
Paperless Solution Provider - Over 156,000 users worldwide work smarter and more efficiently with eFileCabinet
Paperless Solution Provider - Over 156,000 users worldwide work smarter and more efficiently with eFileCabinet
"
https://medium.com/@GiantSwarm/how-giant-swarm-enables-a-new-workflow-80cfea793f7?source=search_post---------50,"Sign in
There are currently no responses for this story.
Be the first to respond.
Giant Swarm
Sep 21, 2018·4 min read
By now we all know that Amazon AWS changed computing forever and it actually started as an internal service. The reason for the existence of AWS is pretty easy to understand once you understand Jeff Bezos and Amazon. Sit tight.
Jeff and his team deeply believe in the two pizza team rule, meaning that you need to be able to feed a team with two pizzas or it is too big. This is due to the math behind communication, namely the fact that the number of communication links in a group can be calculated based on the members of the team n: n
* (n-1)
— — — — —
2
In a team of 10, there are 45 possible links, possible communication paths. At 20, there are 190, at 100 people there are 5.000. You get the idea. You need to allow a small team to be in full control and that is really where DevOps comes from: You build it, you run it and if you want to make your corporate overlord fully tremble in fear, you need a third point: you decide it.
The problem Amazon had though, was that their teams were losing a lot of time because they had to care for the servers running their applications and that part of the equation was just not integrated into their workflow yet. “Taking care of servers” was a totally separate thing than the rest of their work, where one (micro-)service simply talked to another service when needed. The answer, in the end, was simple. Make infrastructure code and give those teams APIs to control compute resources, creating an abstraction layer to the servers. There should not be a difference between talking to a service built by another team, the API for a message queue, charge a credit card or start a few servers.
This allows for a lot of efficiency on both sides and is great. Developers have a nice API and the Server Operations people can do whatever needs to be done as long as they keep the API stable.
Everything becomes part of the workflow. And once you have it internally as a service there was no reason to not make it public and hence have better utilization of your servers.
Now think about how Kubernetes has started to gain traction within bigger companies. It actually normally starts out with a team somewhere that installs Kubernetes however they want, sometimes as a strategic DevOps decision. Of course, these teams would never think about buying their own servers and building their own datacenter, but as K8s is code, it is seen as being more on the developer side. This means you end up with a disparate set of K8s installations until the infrastructure team gets interested and wants to provide it centrally.
While the corporation might think that with providing a centralized K8s they are actually doing what Amazon did with providing the API to K8s, being API driven, but that is not what the Amazon way is. The Amazon way, the right way, is to provide an API to start a K8s cluster and abstract all other things, like security and storage provisioning, away as far as possible. For efficiency, you might want to provide a bigger production cluster at some point, but first and foremost, this is about development speed.
This is where the Giant Swarm Platform comes in, soon including more managed services around it. Be it in the cloud or on-premise, we offer you an API that allows teams to start as many of their own K8s clusters, in a specific and clear-cut version, as they see fit, integrating the provisioning of K8s right into their workflows. The infrastructure team, or cluster operations team as we tend to call them, makes sure that all security requirements are met, there is some tooling around the clusters like CI/CD, possibly supply versioned helm chart templates and so on. This is probably worth a totally separate post.
At the same time, Giant Swarm provides you with fully managed Kubernetes clusters, keeping them always up to date, with the latest security fixes and in-place upgrades, so you are not left with a myriad of different versions run by different teams in different locations. Giant Swarm clusters of one version always look the same. “Cloud Native projects at demand at scale in consistently high quality”, as one of our partners said.
Through Giant Swarm, customers can put their teams back into full control, going as far as allowing them to integrate Giant Swarm in their CI/CD pipeline and quickly launch and tear down test clusters on demand. They can give those teams the freedom they planned by letting them launch their own K8s clusters by themselves, not having to request it somewhere, while keeping full control of how these clusters are secured, versioned and managed, so that they know that applications can move easily through their entire K8s ecosystem, in different countries and locations.
Giant Swarm is the Amazon EC2 for Kubernetes in any location. API-driven Kubernetes, where the teams stay in control and can really live DevOps and API-driven as a mindset and way of doing things. Request your free trial of the Giant Swarm Infrastructure here.
Written by Oliver Thylmann — Co-Founder and COO @ Giant Swarm
twitter.com
Giant Swarm is a leader in cloud-native infrastructures and provides managed Kubernetes clusters to run containerized applications on-premises and in the cloud.
Giant Swarm is a leader in cloud-native infrastructures and provides managed Kubernetes clusters to run containerized applications on-premises and in the cloud.
"
https://medium.com/@jshuey/with-microsoft-your-customer-is-your-partner-and-vice-versa-15f9815c1401?source=search_post---------51,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jeff Shuey
Feb 2, 2018·6 min read
What if I told you that Microsoft wants to help you sell your internal solutions?
Meaning, your intellectual property that your IT teams, your research & development teams, your financial teams, your manufacturing teams, and any other departments have created.
Not just with partnering organizations, but also with customers.
Well, guess what?
They do!
Microsoft Beyond IT
Most of us that have been in the Microsoft Partner Ecosystem for any period of time know that traditionally Microsoft has sold to the Information Technology (IT) department. They didn’t talk to the other parts of the business. That’s changing.
With XaaS that’s not possible anymore.
Speed is of the Essence.
Microsoft Knows That “Time is Precious” and “Billable” via Stev Rubio of Intuit
Why?
We can’t spend the next five years paving the cow path with yesterday’s processes
We can’t even spend the next one year doing that.
It needs to happen now.
The faster trains are leaving the station.
No longer is the $1B business good enough. This is a $20T opportunity.
As I wrote about in the somewhat cryptically named The POTF is intertwined with The COTF there is a need to understand and appreciate that the market is moving fast. Scale is going to be the secret to get there. Speed is the other factor.
While you may have a 100 year plan you will need to be read to adapt to the zigs and zags.
What’s Microsoft Saying about it?
Gavriella said this with an infographic and a spot on quote in on the MPN blog.
$20 trillion. With a T.
In the above post from the Microsoft Partner Network it speaks mostly about partners.
In another article from Forbes that profiled Gavriella Schuster spoke more about the customer engagement opportunity. Given this is relatively new idea for most at Microsoft and will take time (not much time … as noted in the article … incentives are in place today and already working).
Why Now Microsoft?
It’s simple … it’s a classic First Mover effort.
Microsoft is betting on being early. (Which hasn’t always Microsoft’s model)
Microsoft is betting that those who don’t seize the opportunities will fall behind and may find never be able to catch up.
Which prompts the question … Are you in?
Key Point: You don’t need to be the surgeon to make things happen.
Surgeons have support teams. The support teams are much larger that the “thing” they are supporting. I’m not disparaging surgeons or any other highly visible role — whether it’s a CEO, professional athlete or performer, or other “lead” characters. I’m just saying that the bigger the lead the bigger the support team. There is a lot of work to be done and, to be candid, a lot of money to be made in “support” roles.
The Surgeons AND Administrators Analogy (via Harry Brelsford)
Many of us want to be The Surgeon … the tip of the spear … saving lives every day.
However, the surgeon is nothing without the organization behind them. Scheduling, sterilizing, and supporting the surgical effort.
In a sense … everyone can be a surgeon … at least more often … by using the cloud and by working with partners. By sharing their best IP everyone can contribute their best to the quest.
Some partner(s) will be expert at scheduling and running the facilities (not to mention building said facilities) while others will be experts at the logistics to keep it running smoothly. There is a lot of room for Surgeons and Administrators
Key Point Corollary: Everyone can be The Surgeon
Where lifestyle Businesses meet growth businesses
It’s time to embrace partnership with a child like vigor.
It’s time to be the partner (or customer) you want to be.
It’s time to consider working more with Microsoft to get your IP out in the world.
It’s time to be compensated for your great work … beyond your four walls.
It doesn’t matter what “type” of business you want to be. If you have IP … you can partner.
One Throat to Choke
This is an old line from my early days at Microsoft. Except it was used against us from the likes of Sun, IBM, Oracle, and other large monolithic players. Now it’s Microsoft’s turn to take that line and use it to help customers, partners, and yes … themselves.
A collection of best of breed solutions can often be better and more flexible that a single monolithic platform. The rise of cloud computing and the adoption of XaaS makes this more of a reality. It’s not perfect, but it’s getting better. Microsoft wants to help make this more possible with the ability to pull IP — — and I mean IP in the broadest sense of Intellectual Property of People, Processes, and Technologies.
Microsoft wants to make it possible to being smaller vendors with their IP into the mix. Note: this could be a division or group of people within a Global 2000 company.
This is where the BIG play comes in. Microsoft is betting on the fact that a collection of inter-connected partners can help each other AND help bring customers IP to life.
And, as Gavriella Schuster mentioned … this is not a 100% altruistic effort. Microsoft wants customers and partners to build solutions on Azure and to consume Office 365 and/or Dynamics 365 products and services. No one should be surprised about this.
Microsoft wants to embrace you … the customer. Are you ready?
And, make ’em earn it. Microsoft is new to this. Microsoft has to earn your business.
I’m not suggesting you make it hard for them. In fact, I’m advocating just the opposite.
If you have a great idea … Microsoft will help you get it to market.
Caveat: This is NOT a lifelong commitment. You MUST continue to deliver … and vice-versa.
Which brings us back to the question … Are you in?
And, what will you build?
— -
Follow on posts will explore more about this new model for Microsoft. In the meantime please let me know your thoughts and if you would like to collaborate on ideas. This market is huge. This is just the beginning. Time is of the essence.
Image Credits: Microsoft
— -
Jeff is business advisor, mentor and community engagement expert. He brings over 20 years of Channel Sales, Partner Marketing and Alliance expertise to audiences around the world in speaking engagements and via his writing. He has worked for Microsoft, Kodak, and K2.
Connect with me on Twitter @jshuey
Or connect on LinkedIn, Facebook, or Google+
He is a contributing author to Entrepreneur, Elite Daily, Yahoo, US News and tothe Personal Branding Blog with over 250 articles published.
Originally published at jshueywa.blogspot.com.
Storyteller, Business Adviser, Author & Speaker on The Partner Journey. Mtn Biker, Wake Surfer, SUP’r, Proud Dad; Ellie’s Human, IAMCP & Social Media Club board
See all (6,378)
Storyteller, Business Adviser, Author & Speaker on The Partner Journey. Mtn Biker, Wake Surfer, SUP’r, Proud Dad; Ellie’s Human, IAMCP & Social Media Club board
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/theburningmonk-com/serverless-observability-its-easier-than-you-think-48801abf2d66?source=search_post---------52,"There are currently no responses for this story.
Be the first to respond.
Observability is a measure of how well the internal state of a system can be inferred from its external outputs. It helps us understand what is happening in our application and troubleshoot problems when they arise. It’s an essential part of running production workloads and providing a reliable service that attracts and retains satisfied customers. It is even more important when it comes to serverless because we rely on managed services that we can’t simply “run on your machine”.
In my work as a consultant, many clients ask me about how to build observability into their serverless applications. If it’s better to build a custom solution using the native AWS services or to pay for a 3rd-party service such as Lumigo.
AWS offers a range of services that help you build observability into your serverless application, such as CloudWatch, CloudWatch Logs and X-Ray. These services cater for your basic needs cheaply, but they lack the premium developer experience that specialized vendors such as Lumigo can offer.
For example, when compared with X-Ray, Lumigo offers a significantly better developer experience, including:
While the native AWS services have plenty of limitations and don’t offer the best developer experience out-of-the-box, you can work around these and still create a solution that gives you a lot of observability into your serverless application. For example:
All of these are doable but they take a significant amount of engineering time and often require coordination and agreement between different parts of the organization. Teams have to follow the same conventions such as log message format and the approach towards correlation IDs (their format, and how to propagate them) for the solution to work.
If the build vs buy decision is purely based on capabilities and what will make your developers more productive, then I think the most sensible approach is to use a 3rd party service that ticks most of your boxes and supplement them with the AWS services. This is the approach I have used in all of my recent projects and it has allowed me to focus on solving the client’s business challenges and deliver the desired outcomes on time and on budget.
In my projects, I use Lumigo as the main entry point when I try to understand what’s going on in my application and to troubleshoot issues. I seldom write custom log messages anymore because Lumigo captures most of the information I need to be able to infer the internal state of my applications.
Take the Lumigo dashboard, for instance. It gives me a lot of insights into the activities in my system. I can see at a glance if there is a high number of Lambda invocation errors and if so, which functions are failing. I can see hot spots and identify functions that are invoked frequently, these are good candidates for optimization (e.g. right-sizing the memory setting to reduce cost).
I can see Lambda functions where cold starts are an issue. Lambda functions that have a high frequency of cold starts are also good candidates for Provisioned Concurrency.
And I can also see which of the services (3rd party APIs, AWS services, or internal APIs) my application depends on are performing poorly.
The dashboard alone saves me hours of engineering time to collect, analyze and visualize this (very useful) information. The information it gives me helps me quickly find and investigate problems that arise in production. Problems that used to take me hours to find can now be identified in minutes!
In fact, when a problem does arise, I would usually receive an alert through Slack or PagerDuty and I will find the problem waiting for me on the Issues page in Lumigo.
From here, I can navigate to the erroneous transactions and interrogate them. Because every outbound request from the Lambda function is recorded in Lumigo, I can see how long they took and what was said in those communications. Together with the invocation event and the environment variables that were used in the invocation, I can quickly infer the internal state of the function when it was invoked.
As you can see, that gives me a lot of observability and it takes only a few minutes to set up, and I didn’t have to make any changes to my code. Instead, all of my energy and time can be better spent on tackling the problems that actually matter to my clients and delivering value to their businesses. Which is very much aligned with the spirit of serverless — to move away from undifferentiated heavy-lifting and use the most precious resources we have (engineering time) to solve the most valuable problems.
Building custom solutions to solve the problem of serverless observability is fun and it can be challenging in the best possible way! But they are also not what differentiates your business from your competitors.
Observability is an essential part of running a production application, and the good news is that with platforms like Lumigo, observability for serverless applications is easier than you think.
To learn more about how to overcome common challenges with building production-grade serverless applications, join us on our next webinar on Friday, 19th November.
I will be speaking alongside Ryan Jones, the founder of ServerlessGuru, and share many of the lessons that we have learnt running serverless applications in production over the last few years.
Hope to see you there!
If you wanna try out Lumigo, it also has a generous free tier (no expiration) that lets you trace 150k Lambda invocations per month, for free. Sign up now.
Originally published at https://lumigo.io on November 11, 2021.
the personal blog for Yan Cui
2 
2 claps
2 
the personal blog for Yan Cui
Written by
AWS Serverless Hero. Independent Consultant https://theburningmonk.com/hire-me. Author of https://productionreadyserverless.com. Speaker. Trainer. Blogger.
the personal blog for Yan Cui
"
https://medium.com/expedia-group-tech/the-inside-scoop-on-primer-expedias-internal-cloud-deployment-tool-8a3e16ec0300?source=search_post---------53,"There are currently no responses for this story.
Be the first to respond.
By Kuldeep Chowhan
In early 2013, Expedia decided it was time to more heartily explore cloud solutions. At the time, the company’s web services structure was a monolith. The team was nearing the end of a major platform overhaul, changing the way the entire core of Expedia operated. The cloud was becoming too interesting a tool to ignore. But how does a company move a proverbial rock uphill?
A small group of us came together to look at moving a microservice to the cloud. We focused on an application within the shopping path — starting on a segment of the business that wouldn’t immediately impact the booking path. We knew we wanted to run everything through an automated path. For us, if developers had to SSH to the boxes, then automation had failed.
To kick things off, we used Chef as a provisional cloud server deployment tool. It served as a model for moving over to Amazon Web Services (AWS) as a longer-term solution. As we started building microservices we also built modules to help teams with things like monitoring, bot-detection, site resolution, and site experimentation. By observing these modules and making small tweaks along the way, we were able to effectively deploy our first set of microservices to AWS. This enabled us to provision an Elastic Cloud Compute (EC2) instance and deploy Chef’s Cookbook on it with monitoring and logging infrastructure. This was our first iteration of deployment automation to the cloud.
After few months we added capability to our automation to do blue green deployments. Using these two techniques, teams were able to deploy to AWS successfully — we rolled out about 12 services that first year. However, it was still a time consuming process: in order to take advantage of AWS and its automation, teams had to wire in platform modules, write a Chef Cookbook, and create infrastructure files and deployment configuration files. Since we recognized much of this work could be automated itself, in May 2014 we decided to build Primer. Primer is a tool that automates creation of a HelloWorld service. In just five minutes, it wires in a platform module, creates a Chef Cookbook, builds infrastructure configuration files and deployment configuration files, and creates monitoring dashboards.
We also created Jenkins Jobs for continuous integration and continuous delivery (CI/CD) for the teams. Every time a check-in happens to a GitHub repository, the service is deployed automatically to AWS.
Here’s a glimpse at our CI/CD tooling pipeline for cloud:
The workflow involves:
The workflow for CloudFormation/Chef based Primer template is illustrated below.
Primer supports the following app types:
With Primer in place, cloud adoption within Expedia skyrocketed. We currently have more than 1000 services we’ve tested and created. We continue to learn from our efforts in the cloud space and are currently working on a next generation version of Primer, revamping how the deployments work so we can make every part of the process be a REST API call. I will share the detailed architecture of Primer in a later post. We’ve expanded our footprint of cloud developers within Expedia, so there is a large community who understands how the underpinnings work.
We’re pleased by the early success from moving some of the smaller parts of our business to the cloud. Step by step, we’re exploring the right ways to continue this journey.
In a later post, I will share how we deploy docker containers on a cluster of machines using Primer.
Stories from the Expedia Group Technology teams
9 
9 claps
9 
Stories from the Expedia Group Technology teams
Written by

Stories from the Expedia Group Technology teams
"
https://medium.com/@ansumanv/internal-cloud-storage-architecture-cbec5346db0d?source=search_post---------54,"Sign in
There are currently no responses for this story.
Be the first to respond.
Anshuman Verma
Sep 15, 2018·3 min read
Many enterprise organisations are switching to internal clouds to increase the operation efficiencies which also drive down the cost and increase business agility. Storage requirements for internal cloud are different. It requires different storage model than the traditional data centres.
- Architect the internal cloud storage structure
- Architect the storage integration to IaaS cloud
- Finding key storage considerations organisations need to consider to ensure storage is ready for transformation
Organisations face many integration challenges while transitioning to the internal cloud as it requires a different storage model than traditional data centres. Now days clouds are
more than compute services as they also require storage services to be part of a viable
infrastructure as a service cloud offering. For organisations to build an efficient internal cloud, storage must transform to support a service-oriented infrastructure To accomplish this, storage must transform from traditional silos of technologies to a tightly integrated set of storage services that can meet the demanding, self-service, automated, mobile-workload nature of cloud computing.
To deliver services more efficiently and at reduced cost, professionals are under increasing pressure. Many IT professionals would like to be more flexible and efficient in how they deliver infrastructure services, but they question how to get there. When complete, internal IaaS cloud will be a core infrastructure technology critical to the automation of IT service delivery. It is critical for the IT decision maker to understand how to design storage for the internal cloud. Investment choices and missteps that lead to instability, security breaches, or poor service levels, for example, could have a disastrous and costly impact on the IT organisation. Transforming the traditional data centre to an internal cloud is essential for enabling the IT organisation to make intelligent use of external and hybrid cloud alternatives in the future.
- Storage for the internal cloud requires extensive initial work to gather the business and technical requirements for developing a sound storage strategy for the internal cloud.
- Storage transformation begins with consolidation and virtualisation. Once mastered, process automation and service integration will enable automation of IT service delivery.
- Building out storage services and infrastructure is a repetitive and tenuous process. Defining application affinity to storage workload will ensure delivery of recovery point objective/recovery time objective expectations.
- Building out storage services and storage infrastructures ensures that storage for the internal cloud can service application requests as new business requirements emerge and the IaaS cloud matures over time
The steps required to transform existing storage or to integrate new storage into the private cloud include:
2. To prepare storage to service the individual layers of the IaaS cloud model, IT organisations must begin by consolidating storage and implementing storage virtualization. The pre-work accomplished in Step 1 will be instrumental in accomplishing this step. IT organizations must begin with:
3. Critical to integrating storage for use within the IaaS cloud is to begin to automate many of the manual storage functions typically found in traditional data centre environments. The purpose of this step is to remove as much of the human element as possible from the day-to-day storage operations. To accomplish this step organisations should define and implement:
4. Building out storage services and storage infrastructures ensures that storage for the internal cloud can service application requests as new business requirements emerge and the IaaS cloud matures. Organisations must:
Storage for the internal cloud is about storage transformation and integration with the
IaaS cloud. It must be a well thought out, pragmatic journey and requires an extensive amount of initial work prior to designing storage for the internal cloud. This three- to five-year journey will transform traditional silos of storage into service-oriented, virtualized, elastic, scalable, shared storage infrastructures that will be capable of on-demand provisioning and transparently servicing the storage requirements of the internal cloud.
"
https://medium.com/@danfhern/self-service-platform-development-made-easy-with-pulumi-4e2cf150e93c?source=search_post---------55,"Sign in
There are currently no responses for this story.
Be the first to respond.
Dan Hernandez
Nov 13, 2020·4 min read
Over the last five years I’ve worked on and consulted with nearly a dozen internal platform teams. In almost every case, the goal was to provide self-service infrastructure capabilities to development teams within the organization.
This is a desirable pattern because it improves the efficiency of development teams by reducing context switching and the need to understand infrastructure in a deep way. It’s also useful for governance and ensuring that infrastructure is properly architected for security across an organization.
Puppet recently released the State of DevOps Report 2020 and, not surprisingly, there is a whole section dedicated to “Scaling DevOps practices with internal platforms.”
The report claims that “High DevOps evolution correlates strongly with high use of internal platforms. Highly evolved firms are six times as likely to report high use of internal platforms as firms at a low level of DevOps evolution.”
The report also notes that common challenges to building an internal platform are “lack of time”, “lack of standardization” and “lack of technical skills within the team”.
This is right in line with what I’ve seen. It’s hard for platform teams to properly build and manage an internal platform.
In an attempt to surmount these challenges, it’s common to see teams cobbling together bash scripts with ticketing systems like ServiceNow or Jira Service Desk. Platform teams are usually comprised of former system administrators so it makes sense that they would use tools they are familiar with.
Unfortunately this is not how developers want to interact with infrastructure. Developers want to interact with APIs and code. Another point in the report from Puppet: “When we compare low-evolution companies to those at a high level of DevOps evolution, we see expanded use of raw APIs, GitOps, ChatOps and command line interfaces (CLIs)”.
That said, I’ve also worked with some teams that have software engineering capabilities and the problem isn’t what they are capable of building, it’s a lack of focus on their customers and product.
Because of this it’s not uncommon to see these teams spend months if not years building without working with a specific “customer”. So they end up building the wrong product or they spend so much time building that the underlying patterns and assumptions they had when they started are no longer relevant and the product ends up getting scrapped.
Again, a relevant point in the report: “Focus on developer experience and flow. We can’t stress enough that empathy is a critical skill set. Empathy means understanding someone’s position, and it’s impossible to build a good product without having empathy for your user.”
So how can a recent offering from Pulumi make building an internal platform easier?
Well, that offering is called “Automation API” and basically it’s a way to programmatically create, run and manage Pulumi programs.
It doesn’t solve all the aforementioned challenges but it does provide building blocks that make the actual development of an internal self-service cloud platform much easier.
And to prove it, I built a simple prototype of what this would look like called “cloud platyform”.
So what does it do?
Cloud platyform provides an easy way for a platform team to create self-service.. services.. for developers or other users to consume. Services are authored as Pulumi programs in a Github repo, so they are really simple to create and manage.
And to provision those defined services, users can click a few buttons and get a service that includes all of the best practices for that organization while hiding any unnecessary complexity. See the demo videos below.
Service creation:
Service instance creation:
It’s basically like your own AWS, GCP, Azure, etc.
It’s important to mention that beyond interacting with the cloud platyform console, users could also interact with the API that backs it. So it’s easy to use for developers and creating automation.
To take that a step further, you could create a Pulumi provider for this API so developers have a really slick way to consume services put together from a platform team, next to other services that they may be using now or in the future.
Of course, there are a whole host of features that could be added to a platform like this. Here are a few ideas:
That said, I had fun putting together this prototype. It took minimal effort considering what it can do. I can only imagine what a properly staffed product team could do with this.
Pulumi has done a great job getting the ball rolling and I’d love to see a solution like this from them or from the community as an open-source project.
Here’s a link to a blog post that introduces the Automation API from Pulumi: https://www.pulumi.com/blog/automation-api/.
I’ve open-sourced the code and can be found here: https://github.com/danfhernandez/cloud-platyform
(The project is a quick prototype and was only deployed locally. If there’s enough interest I’d consider working on it further.)
Software Engineer, Cloud Architect, DevOps Practitioner, dan.dev. http://linkedin.com/in/danfhernandez https://twitter.com/danfhernandez
58 
58 
58 
Software Engineer, Cloud Architect, DevOps Practitioner, dan.dev. http://linkedin.com/in/danfhernandez https://twitter.com/danfhernandez
"
https://blog.doit-intl.com/multi-cluster-istio-1-5-with-private-gke-clusters-and-google-cloud-internal-load-balancer-2cbf9f98d590?source=search_post---------56,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Top highlight
This tutorial shows you how to build a private multicluster service mesh solution, with Istio 1.5 and Service Mesh Hub for mesh Federation. This is achieved by using an Internal Load Balancer (ILB) to connect Istio workloads running in multi-region Private Google Kubernetes Engine (GKE) clusters.
Istio 1.5 was released on March 5th and with this major release, comes several important changes, however, support for Hashicorp Vault as External CA is still in progress. For this reason, I started to investigate what other options there are for managing the common root CA in a secure way. I then came across solo.io’s open-source project, Service Mesh Hub. Service mesh hub is a solution that can help you to unify the root identity between multiple service mesh installations. This means that any intermediates are signed by the same Root CA and end-to-end mTLS between clusters and services can be established correctly.
In this tutorial:
github.com
we will build the following architecture:
There are several reasons to isolate your Google Kubernetes Engine (GKE) clusters from internet access, the primary one being security. In a private cluster, nodes only have RFC 1918 private addresses and communicate with the master’s private endpoint via private networking.” This prevents any access authorized or no from the outside internet. “
The TCP/UDP Internal Load Balancer has a less well-known beta feature, called Global Access. This feature allows clients from any region in your VPC network to access the internal TCP/UDP load balancer. Without global access, traffic originating from clients in your VPC network must remain in the same region as the load balancer. Global access is enabled, in your private Kubernetes clusters, on a per-Service basis by using the following annotation:
In the example implementation, we will annotate the ingressGateway Kubernetes Service directly to provision a GCP ILB with global access. After Global access has been applied we now have network connectivity in place between our private clusters running in different regions.
Success! Notice we have the service A (sleep.foo.cluster-1) that needs to talk to service B (httpbin.bar.cluster-2) in another mesh. If a service in one mesh requires a service from another, you must federate identity and trust between the two meshes. To do this you must exchange the trust bundle between the meshes.
Service mesh hub allows you to group together multiple meshes into an object called a VirtualMesh. Creating this resource will instruct the Service Mesh to establish a shared root identity across the clusters in the Virtual Mesh as well as federate the services.
With the creation of the VirtualMesh resource:
Once trust has been established, Service Mesh Hub will start federating services, automatically generating service entries for you! so that they are accessible across clusters.
For this tutorial, the enforceAccessControl flag is disabled and this will be the subject for one of my next articles. Where we will see how Istio can provide security features for a strong identity, powerful policy, and authentication, authorization, and audit (AAA) tools to protect your services and data.
In this tutorial, I have demonstrated the ease with which you can implement a service mesh solution with intra-mesh communication. Service to service communication can happen internally via secure mTLS traffic communication, without leaving the Google Cloud’s Global Network.
The step by step instructions, for this tutorial, can be found here:
github.com
Software & Operation Engineering. Written By Engineers.
106 
Your monthly dose of Google Cloud and Amazon Web Services articles  Take a look.
106 claps
106 
Written by
Senior Cloud Architect - Interested in #GCP, #k8s, #servicemesh, security and distributed systems.
Here, our SRE and SWE teams share the solutions to some of the most interesting challenges we encounter during our daily work at DoiT International. If you like what we do, check out our https://careers.doit-intl.com page and join the team!
Written by
Senior Cloud Architect - Interested in #GCP, #k8s, #servicemesh, security and distributed systems.
Here, our SRE and SWE teams share the solutions to some of the most interesting challenges we encounter during our daily work at DoiT International. If you like what we do, check out our https://careers.doit-intl.com page and join the team!
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@deborah.martin/putting-your-cloud-to-work-integrating-public-cloud-and-internal-it-resources-829817a4c08d?source=search_post---------57,"Sign in
There are currently no responses for this story.
Be the first to respond.
deborah.martin
Mar 17, 2016·4 min read
Public cloud resources are a key enabler for today’s agile enterprise. Cloud services allow a company to quickly respond to market opportunities and competitive pressures while helping drive down costs. There are many well documented use cases for public clouds that are ideal for certain functions. However, one of the biggest issues enterprises must address, when brokering these services to their employee base, is the integration and management of the public cloud assets with their own internal IT resources.
While public cloud resources can be used independently of an enterprise’s own IT assets, business outcomes can be maximized by integrating the public cloud resources in such a way that they almost become transparent to the user. Ideally, users should be able to seamlessly move back and forth between public cloud and private cloud assets with little to no effort. For example, if your DevOps team wants to use a public cloud to develop and test code that will ultimately run internally, that code should be able to run unchanged when it is deployed to production. If data needs to be stored in a public cloud service, you need to ensure it meets regulatory and country-specific compliancy requirements for privacy and security. Controls also need to be in-place to ensure that the correct integrated services are used to meet those requirements.
However, making public cloud resources and internal hybrid IT assets work together seamlessly requires considerable thought and planning of your private cloud environment — particularly considering that the largest public cloud providers are not standards-based. You can start with these three key considerations for your hybrid cloud integration:
The resulting choices in services made available to your user base will require policy-based deployment tools to ensure the right integrated service is used, based on the requirements of the data.
The key point is, if your users can’t move their workloads easily between your hybrid IT and the correct public cloud resources you end up increasing your costs, wasting time and losing much of the agility that cloud computing offers. Even worse, your user base might even circumvent your private/public cloud service choices and deploy on unauthorized services that are simply easier for them to use.
The solution to providing an agile, seamless, private/public cloud environment is to start by closely adhering to open standards whenever possible. If you deploy your own private cloud, consider standardizing on an open cloud operating system like HPE Helion OpenStack. If you use a public service, like Amazon AWS, make sure you have tools on your private cloud that ensures it works seamlessly with AWS, like HPE Helion Eucalyptus. If Microsoft Azure is your primary public cloud service, deploy a private cloud service based on Windows Azure Pack. You can start with something as simple as an HPE hyper converged Azure-consistent cloud in a box. Implemented properly, the final public and private cloud offerings in your service catalog should be so easy to use and move between them that your user base would not consider using anything else.
Selecting the right global and regional public cloud partners that will mesh with your private cloud system is critical. Proper planning and implementation of your private cloud will ensure that you’re ready for this public cloud hybrid integration from the start — yielding the best possible return on investment from your private cloud resource. Getting the proper level of integration of your hybrid cloud could take years of expertise and an army to keep up with all of the cloud providers and private cloud technology choices, but you don’t have to do it yourself — and it would be cost ineffective to do so. There are turn-key solutions focused on delivering this type of open integration. For example, take a few minutes to watch the overview on the latest HPE CloudSystem to see how to make your private cloud and hybrid cloud management work seamlessly across all your different cloud providers and technologies.
Getting an open agile hybrid cloud implemented doesn’t have to be difficult. However, it does require a well-designed strategy along with the right tools, products and technology partners to make it happen. Hewlett Packard Enterprise has what you need to make it happen faster.
Cloud Solutions at Hewlett Packard Enterprise; with 20 years at multiple technology companies. Passionate about the use cases cloud can enable for business.
1 
1 
1 
Cloud Solutions at Hewlett Packard Enterprise; with 20 years at multiple technology companies. Passionate about the use cases cloud can enable for business.
"
https://medium.com/@carvercloud/internal-resistance-to-cloud-adoption-might-be-your-biggest-barrier-a498280d7e45?source=search_post---------58,"Sign in
There are currently no responses for this story.
Be the first to respond.
carvercloud
Jan 5, 2017·6 min read
In a recent article on Infoworld, David Linthicum asked the question, “Why do cloud projects fail?” The answer he came up with is often “resistance from your colleagues.” While there are many technical challenges that can become barriers such as vendor lock in, security, and compliance, cloud project stakeholders are often blindsided by another set of barriers that they don’t expect, internal resistance. There are lots of changes that must take place when an organization moves to the cloud. Changes in software, changes in service, changes in escalation, the list goes on. The scope of these changes, coupled with an organization’s existing culture, can create internal resistance to cloud adoption that is difficult to overcome. The issues surrounding internal resistance must be carefully, and thoughtfully, managed to ensure they don’t stop cloud projects before they start. In this article we’ll lay out some common problems, and some solutions for managing your organization’s culture to facilitate cloud adoption.
IT Staff May Push Back
People are often resistant to change, and for IT staff the shift to the cloud is seismic. Servers, applications, and data will no longer be on premises. As stated in a CIO Online article, IT staff “become service brokers rather than operators of their own tangible IT assets.” That’s a big change, and it comes with, at the very least, the perception of loss of control. In fact, with cloud monitoring, management, and security tools, administrators may have more control than ever. However, the fact that service provider resources will have their hands on virtual servers to patch, reboot, and apply other updates creates the perception of lost control. In public cloud scenarios, multi-tenant access can further that perception
In addition to that adjustment, IT staff will need new skills. These range from learning the user interfaces, APIs, and scripts of cloud service providers, to managing cloud vendors to make sure service level agreement obligations are met, to monitoring and optimizing cloud solutions using vendor and third-party tools. These skills go beyond what has traditionally been expected of IT staff. There will be a learning curve, and IT staff will have to step out of their comfort zone. Some IT staff may like their new job requirements, while others may feel they are being replaced by the cloud.
IT Managers May Be Uncomfortable
IT managers and CIOs may resist the cloud for entirely different reasons. Managers may feel that they know how to solve problems in their current environment. They can replace a server, recover data from a backup, add capacity to that database, and so on. Moving to cloud services may move them out of their comfort zone. Some industry experts are predicting cloud services will greatly diminish the responsibilities and position of the CIO. Naturally, this can create resistance.
Shadow IT May Put Up Roadblocks
What is shadow IT? Shadow IT services are departments that have individuals, often called power users, department IT helpers, or someone else who provides critical IT or application support to the department. They may work closely with regular IT staff or be nearly independent. In some cases these shadow IT staff are the only real IT experts for specific systems, applications or integrations critical to a department’s operations. Departments put shadow IT staff in place to gain more control over critical systems, or to get more responsive IT support. They won’t want to give these things up for a move to the cloud without assurances that they can get similar access and service once services and applications are moved to the cloud. Again, any issues like this in an organization must be identified, and addressed thoughtfully.
Other Employees
Moving to the cloud will be a shift, not only for technologies and IT staff, but also for all employees in the organization who will access and use cloud-based solutions. While you can make the argument that employees shouldn’t care where their IT services and applications come from, they often do. They are used to, knowledgeable with, and even may like the their current applications, and how they access systems and data. Some of that will likely change when services, data, and applications are moved to the cloud. Again, those issues need to be identified, and thoughtfully examined and messaged.
Barriers can Turn Into Roadblocks
As David Linthicum said in his article, resistance can manifest itself in many different ways:
“People demand that you get their approval and then don’t show up to meetings. Budget dollars are removed that were initially earmarked for the cloud. And, my favorite, some people go to company leadership to scare them to death about the imaginary threats that cloud computing will bring. The resistance is often pretty passive aggressive.”
The result? Cloud projects may be delayed at many different points, may have scope and budget reduced, or may be stopped altogether.
Beyond that, failure to address internal IT resistance, and old IT habits can result in implementations that don’t take full advantage of the automation available in the cloud. In a Network Computing video, Joe Emison, CTO of BuildFax, points out that many organizations create problems for themselves by not appropriately identifying a move to the cloud as something that requires a full rethink of long standing IT and development processes and procedures. The desire to keep system provisioning, application deployment, and maintenance “hands on” reduces efficiency. It can hamper some of the main advantages of cloud deployments such as rapid elasticity.
The Solution? Training and Preparation
Communication is certainly key throughout the life of any cloud project, from conception, to pilot, to proof of concept, to implementation. But what do you communicate, and how do you communicate it? Cloud project leaders will have to communicate with C-Level executives, subordinate IT staff, key department heads, and employees. Each of these groups requires a different communication approach. On top of that the cloud team will have to provide updates to all employees as cloud projects move into implementation and deployment. While we’re asking questions, who should be on the cloud project team to give cloud projects the best chance for overcoming both technical and cultural challenges at an organization?
David Linthicum stated it in his article:
“A lack of understanding is at the root of the insecurity some people experience, and the (passive) aggression that results. A bit of knowledge will soften up the people who are on the fence about cloud computing. Once that happens, things will go a lot smoother because you’ll have the crowd’s momentum on your side.”
CloudMASTER cloud computing classes provide a comprehensive, in-depth, vendor neutral environment that explains critical technological, and cultural barriers to cloud implementation. It also provides hands-on migration planning, management, and automation activities throughout the three courses. Specifically, the Cloud Architect Course dives deep into designing a cloud architecture to meet organization requirements. It also explains the skills, and personalities that make up an effective cloud team. It details the causes behind these cultural barriers and provides strategies for addressing them. Beyond that, the course devotes an entire lesson to presenting a cloud project plan. This lesson covers how to structure your argument, back it up with data, and how to communicate it to executives, peers, key stakeholders, and other staff. CloudMASTER includes this training because it’s critical to successfully implementing cloud projects. Anyone who takes this course, and the other CloudMASTER classes will be well equipped to overcome the cultural barriers outlined in this article.
Download our analysis of hiring vs. training to get cloud computing expertise into your organization.
CarverTC provides CloudMASTER Cloud computing training in Portland Oregon and around the United States.
Originally published at blog.carvertc.com.
This is the Twitter account for Carver Technology Consulting LLC's cloud services. Provide strategy, implementation, and training services.
This is the Twitter account for Carver Technology Consulting LLC's cloud services. Provide strategy, implementation, and training services.
"
https://medium.com/@carvercloud/internal-resistance-to-cloud-adoption-might-be-your-biggest-barrier-video-33e3abb13f71?source=search_post---------59,"Sign in
There are currently no responses for this story.
Be the first to respond.
carvercloud
Apr 27, 2017·2 min read
In a recent article on Infoworld, David Linthicum asked the question, “Why do cloud projects fail?” The answer he came up with is often “resistance from your colleagues.” While there are many technical challenges that can become barriers such as vendor lock in, security, and compliance, cloud project stakeholders are often blindsided by another set of barriers that they don’t expect, internal resistance. In this video, we lay out some common problems, and some solutions for managing your organization’s culture to facilitate cloud adoption.
To see our full article, click here.
To see our full article, click here.
CloudMASTER cloud computing classes provide a comprehensive, in-depth, vendor neutral environment that explains critical technological, and cultural barriers to cloud implementation. It also provides hands-on migration planning, management, and automation activities throughout the three courses. Specifically, the Cloud Architect Course dives deep into designing a cloud architecture to meet organization requirements. It also explains the skills, and personalities that make up an effective cloud team. It details the causes behind these cultural barriers and provides strategies for addressing them. Beyond that, the course devotes an entire lesson to presenting a cloud project plan. This lesson covers how to structure your argument, back it up with data, and how to communicate it to executives, peers, key stakeholders, and other staff. CloudMASTER includes this training because it’s critical to successfully implementing cloud projects. Anyone who takes this course, and the other CloudMASTER classes will be well equipped to overcome the cultural barriers outlined in this article.
Download our free ebook: The Top 5 Cloud Computing Barriers and How to Overcome Them.
CarverTC provides CloudMASTER Cloud computing training in Portland Oregon and around the United States.
Originally published at blog.carvertc.com.
This is the Twitter account for Carver Technology Consulting LLC's cloud services. Provide strategy, implementation, and training services.
This is the Twitter account for Carver Technology Consulting LLC's cloud services. Provide strategy, implementation, and training services.
"
https://fullstacksecurity.com/should-you-penetration-test-third-party-saas-applications-2afcd92f9077?source=search_post---------60,NA
https://medium.com/google-cloud/calling-an-internal-gke-service-from-cloud-functions-2958f9218355?source=search_post---------61,"There are currently no responses for this story.
Be the first to respond.
Consider a Cloud Function that is exposed to the Internet. When deployed, we can call that function and the implementation logic will be executed. Now consider that the function wishes to call a REST exposed service that is implemented as a container hosted in Kubernetes. Schematically we wish to end up with:
What this shows is that the client calls the Cloud Function over the Internet. The Cloud Function then calls the target service (MyService) that is hosted in set of replicated pods. The pods are exposed as a Kubernetes service. That Kubernetes service is surfaced only inside the GCP VPC internal network. Explicitly, the service is not exposed to the Internet and hence the attack surface area of the service is decreased.
This story sounds sensible enough but we have a couple of puzzles that need addressing. The first is the notion of the MyService service definition. When we define a Kubernetes service, we have choices of how that service will be exposed. Specifically, we have the spec.type property which can be one of:
Of these, LoadBalancer seems like the obvious choice. However, if we used this, the default setting is that the service is exposed to an Internet stable IP which would look like the following:
While this would functionally work, we have exposed MyService over the Internet and increased the attack surface area. The solution is to use a GCP specific capability provided by GKE, that creates a load balancer that is exposed only to the GCP internal VPC network. This causes TCP/UDP layer load balancing by exposing the service with a VPC stable IP address. This feature is enabled by adding a metadata.annotations entry in the service description called cloud.google.com/load-balancer-type with a value of Internal.
An example service may be:
Once defined, we will now have a load balancer that, when invoked, will route traffic to our pods. The stable IP address of the load balancer will be present on the internal VPC network. There will be no exposure of our service on the Internet.
In diagram form, we have:
There is still a problem. The Cloud Function is a serverless component. What this means to us is that when the client calls the Cloud Function, the Cloud Function runs in a Google managed environment that is separate and distinct from any other resources we may have defined in our GCP project. This includes access to the VPC network. By default, the Cloud Function simply can not access that network. If it tried to reach the stable IP of the load balanced service, we would find that there is no path to it. Thankfully, there is a solution. There is a component of VPC networking called VPC access. This component allows serverless GCP products, such as Cloud Function, to be associated with a VPC network such that requests for access to IP addresses on that network will succeed.
We can now make a REST call from our Cloud Function to the target service and it will work as desired. One final tweak we will make is to register a DNS entry for our stable IP so that we can code a request to a logically named DNS entity as opposed to an IP address that is opaque. We will use a Private Zone in Cloud DNS for that effect. The end result is what we initially desired and is shown in the original diagram in this series.
What follows is a step by step walk through for creating a sample Kubernetes service and calling it from a Cloud Function as described previously.
Use the Cloud Console to create a cluster called my-cluster.
Finally we click create. This will create our cluster which will take a few minutes.
2. Create an app
We want a simple application to run within our pods.
Here is the corresponding content of Dockerfile used to build the docker image:
3. Create a docker image and put it in the registry.
4. Create a Kubernetes ReplicaSet.
Create a file called replica-set.yaml with the following content:
Apply this to our Kubernetes cluster:
Within the Console, visit Kubernetes Engine -> Workloads and wait for the status of the workload called my-replica-set to reach OK.
At this point, the pods are running.
5. Create a Service
Create a service with type of LoadBalancer with the annotation that declares we are using the Internal TCP/UDP load balancer. Create a file called service.yaml that contains:
Apply this to our Kubernetes cluster:
Within the Console, visit Kubernetes Engine -> Services & Ingress. Wait for the service called my-service to reach the OK status.
Take note of the IP address. This is the IP on the VPC network where the Load Balancer can be reached.
6. Create a DNS entry
In the Console, visit Network services -> Cloud DNS. Click Create zone.
Set the Zone type to be private.
Click Add record set.
At this point, we now have a mapping from myservice.mycompany.internal to the LoadBalancer.
7. Define Serverless VPC Access.
Visit VPC network -> Serverless VPC access. Click CREATE CONNECTOR.
8. Create a Cloud Function that will call a REST service.
This is the JavaScript code that will be executed when a call is made to the Cloud Function. The focus of our illustration is the REST request which calls our Kubernetes service:
The package.json should contain:
In the Function definition, in the advanced options under Networking, reference the VPC connector:
9. Test the function
We can now call the Cloud Function and see that the internal Kubernetes hosted service is being called.
See also:
Google Cloud community articles and blogs
183 
2
No rights reserved
 by the author.
183 claps
183 
2
Written by
IT specialist with 30+ years industry experience. I am also a Google Customer Engineer assisting users to get the most out of Google Cloud Platform.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
IT specialist with 30+ years industry experience. I am also a Google Customer Engineer assisting users to get the most out of Google Cloud Platform.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@anshoo_n/workday-presses-the-innovation-button-moves-test-dev-to-ibm-cloud-2b0819eb27ba?source=search_post---------62,"Sign in
There are currently no responses for this story.
Be the first to respond.
Anshoo Nandwaani
Aug 17, 2016·1 min read
On 15 August, 2016 IBM and Workday announced a multi-year partnership wherein Workday will use IBM’s Cloud for its internal Testing and Development environment.
The announcement adds to the existing IBM and Workday partnership which includes IBM’s global Workday Consulting Services, IBM’s acquisition of Workday services provider Meteorix (in 2015) and IBM’s own use of Workday’s Human Capital Management (HCM) for its global workforce.
Existing relationship and no fear of competition (current or potential) made IBM the likely choice for Workday. Amidst worthy competitors, IBM was the most likely choice given its existing focus on Workday and Microsoft, Google being current (and potential) competition. Per Workday’s own admission, it intends to expand the use of IBM Cloud (over time) beyond development and testing. At Greyhound Research we believe this can (in theory) result in Workday eventually moving its production environment as well to IBM Cloud.
This alliance is testimony to the growing importance of Testing and Development in the face of Digital Transformation. Per a recent Greyhound Research global independent study titled, Global CEO Priorities 2016…
To read more, please click here.
VP & Principal Analyst @Greyhound_R All things #WorkforceTech | CHRO @GreyhoundGroup | Opinions expressed here-in are mine & don't represent my employer's views
VP & Principal Analyst @Greyhound_R All things #WorkforceTech | CHRO @GreyhoundGroup | Opinions expressed here-in are mine & don't represent my employer's views
"
https://medium.com/@oneneck/internal-external-roles-for-company-cloud-security-d21130dc5451?source=search_post---------63,"Sign in
There are currently no responses for this story.
Be the first to respond.
OneNeck
Nov 27, 2018·2 min read
Outsourcing is becoming an increasingly popular business strategy. By carving off business processes and giving them to outside vendors companies save money and resources. Outsourcing enterprise computing processes using cloud services, for example, allows you to hand off the cost and responsibility of maintaining on-premise hardware and software. However, just because you outsource your enterprise infrastructure does that mean your cloud service provider assumes total responsibility for your network? What about issues such as cloud security, which is a major concern for every IT manager and CIO? Can you hold your cloud service provider accountable for providing watertight data security?
As the use of cloud continues to grow, concern for data security grows with it. When data is an important business asset, management is hesitant to surrender control. However, it has been demonstrated over and over that cloud-based services tend to be more secure than on-premises systems. Most cloud services providers maintain rigorous security protocols for disaster recovery and protection from cyber-attack. Since providing secure and stable computing services is their primary business, cloud service companies use sophisticated tools to continuously monitor systems, identify vulnerabilities, and plug holes in cloud security. They also have service-level agreements (SLAs) to assure customers that security problems are remediated quickly.
So when you engage a cloud services provider you no longer have to worry about data security, right? Wrong!
Enterprise computing is more than just hosted enterprise hardware and services. Your cloud service provider is responsible for securing the foundation of your enterprise infrastructure; the computing systems, power, data storage, database, and networking. As the customer, you are still responsible for securing applications and related services.
Your cloud provider is generally responsible for cloud security at the network layer, including network segmentation, perimeter services, DDOS spoofing, and so forth. As the cloud customer, you are responsible for threat detection, security monitoring, and incident reporting. In other words, your provider offers cloud security for hosted switches and networks, but your responsibility is to secure the network applications and data traffic. Most SLAs are structured to make it clear that the customer is responsible for host layer data traffic, such as access management, patch management, security monitoring, and log analysis, i.e. any application security elements.
Assuming that your cloud service provider will include comprehensive cybersecurity as part of their contract is a mistake. There are areas where they have control over the infrastructure and therefore can take responsibility for data security, but there are other areas that have to be the enterprise customer’s responsibility. Developing a collaborative cloud security strategy is the best approach to address risk management and deal with security threats.
Let’s consider some of the most prevalent security threats and where they tend to compromise enterprise networks. According to the 2018 Verizon Data Breach Report security issues affect both enterprise network owners and cloud service providers.
To read the full list of providers and learning how to develop collaborative cloud security strategies, visit OneNeck’s blog for the original blog post.
Get the information you need to guide your IT strategy.
Get the information you need to guide your IT strategy.
"
https://medium.com/google-cloud/increasing-cloud-sql-postgresql-max-connections-w-pgbouncer-kubernetes-engine-49b0b2894820?source=search_post---------64,"There are currently no responses for this story.
Be the first to respond.
I recently read a fantastic 3 part article from FutureTech Industries about using Helm, Kubernetes, PgBouncer, and CloudSQL to drastically increase the connections a Postgresql DB can handle. Although it was very informative, as a person who was not extremely versed in Helm and Kubernetes, I needed a more thorough tutorial for the installation of Helm, Tiller…
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@diegosucaria/phpmyadmin-on-google-kubernetes-engine-fd09b09a69dc?source=search_post---------65,"Sign in
There are currently no responses for this story.
Be the first to respond.
Diego Sucaria
May 31, 2019·2 min read
I needed to deploy an auto-scalable, cheap, phpMyAdmin cloud instance as an internal tool for developers. I wanted to keep it simple. If possible use IAP to allow people from inside the company access it.
At first I thought I will be deploying an image of phpMyAdmin using Google Cloud Run, but I found some drawbacks:
So I decided to give Kubernetes a try!
First things first.
I used Google Cloud Shell to run all the commands, it’s very useful really. If you are not using it, give it a try!
Give kubectl the credentials to work with your cluster:gcloud container clusters get-credentials yourclusternamehere — zone yourclusterzonehere
Create a file called phpmyadmin-deployment.yaml:
(change your PMA_HOSTS and PMA_VERBOSES)
Now we apply this on our cluster:kubectl apply -f phpmyadmin-deployment.yaml
And we need to create a service that will use our recently created deployment, phpmyadmin-service.yaml:
Remember to apply it:kubectl apply -f phpmyadmin-service.yaml
Now we will create an Ingress, to expose our service to the internet using a custom subdomain and a google managed cert. (this will create an HTTPs Load Balancer and you will be billed for it!)
The Ingress will need a public static IP address, so we reserve one:gcloud compute addresses create cluster1 — global
Now, let’s prepare the certificate, phpmyadmin-certificate.yaml:
The ingress itself, ingress.yaml:
Now we apply the certificate and the ingress:kubectl apply -f phpmyadmin-certificate.yamlkubectl apply -f ingress.yaml
be patient! creating the HTTPs Load Balancer, and the managed certificate takes some time!
btw, remember to point your subdomain to the recently created static IP.
After a few minutes, you will be able to access to https://phpmyadmin.yourcompany.com!!
Pilot, DJ, DevOps Cloud Engineer
2 
1
2 
2 
1
Pilot, DJ, DevOps Cloud Engineer
"
https://medium.com/@jere.krischel/vector-field-of-velocities-e6ac199284c3?source=search_post---------66,"Sign in
There are currently no responses for this story.
Be the first to respond.
Show me your measurements of the vector field of velocities inside a cloud.
Chris Crawford
Jere Krischel
Mar 8, 2019·4 min read
Cloud A of volume X at coordinates x,y has an average vector of 2 mph NW. I don’t need to model the internal fluid mechanics of the cloud in order to come up with an average measurement that is arguably more useful.
Show me the mathematical sum of ‘certain variables’ and ‘relevant quantities’.
From the WMO:
certain variables = temperature, precipitation, or wind
relevant quantities = degrees C, inches of precipitation, mph of wind
I think you’re working against yourself here — we’ve established elsewhere your appreciation for the authority of experts, but I’m not sure if this is a good place to try and disagree with them.
There’s no shame in admitting that you’ve had a difficult time explaining climate and weather. Frankly, I think the WMO specification here is more useful than “we know it when we see it”, don’t you?
I don’t give a damn about unfair or asymmetric, I care about truth, which you seem to ignore.
Oh, I care very much about truth. I believe the pejorative you were using against me was “boolean”, wasn’t it? :)
Joking aside, I think you can accept that the null hypothesis doesn’t have the same burden of proof as any other explanation.
No, I’m not, and I won’t bother explaining why.
That seems rather petulant. I know you’re intelligent, even if your reasoning seems highly motivated, but it appears that you’ve decided I’m not asking questions in good faith.
In truth, I am sincerely trying to understand your point of view better. I’ve posited that I’ve recognized a pattern to the argument you’re putting forth, not to demean you or belittle you, but to make sure I’m accurately presenting the best version of the argument you’re making. Pascal’s wager is something rational people can, and often do, engage in.
If this is not the proper analog to your assertion, please, explain why. I’m here to learn from you.
Show me the precedent or retract the claim. And no, the Younger Dryas is not a precedent.
I think you missed my point. I’m asserting that proxy data does not have the resolution required to exclude prior periods of similar rates of warming.
Do you disagree that proxies have a different resolution than daily measurements from weather stations or satellites?
It is not my opinion, nor is it the opinion of the thousands of scientists who actually know something about climate science.
Then, please, convince me. Rather than simply contradicting my assertion, tell me exactly how we can take low resolution proxies and compare them accurately to high resolution direct temperature measurements.
Your opinion, and your assertion of the opinions of others, while interesting, is not very convincing.
An honest scientist would not.
That’s unfair to scientists — the actual exercise of building hindcasts requires tuning. There’s nothing dishonest about such an endeavor, it’s just difficult to pull out false negatives and false positives from real data once you go down the statistical analysis path.
Scientists have been doing exactly that.
Citations, please. In order to recognize false negatives and false positives, don’t you need to have falsifiability? I mean, it’s literally in the word “false”, isn’t it?
If the error bars are large, you can’t draw as many reliable conclusions.
Thank you for that admission. I think we can both agree that the error bars for ECS, the most important number per your estimation, are incredibly large. Ergo, we can’t draw many reliable conclusions at this point.
That’s not how science works. If YOU have the evidence, present it. If you don’t, then you have no case.
Earlier, you talked about caring about truth. You do understand that truth exists even if I don’t present evidence for it, right?
I mean, I get it, you’re focused on a consilience of evidence — you’ve seen five different methods all converge on something, and you’re firmly convinced that you’ve hit truth. You’re not interested in any sort of skepticism of the data, the methods, or the assumptions, you’re firmly convinced that without someone presenting some other set of congruent evidence in support of some other hypothesis, there is no reason to search further for truth — it is in your hands, and you’ll keep holding onto it until something new comes along.
I hope you understand that I’m coming at it from an inverse perspective — I see the evidence, and I’m looking really hard to find some necessary and sufficient falsifiable hypothesis statement so that we can apply the scientific method and determine whether or not our initial hunch is true, or simply statistical noise. We’re both aiming for the truth, just from different angles.
I do not like to argue. I like to discuss issues with people in good faith.
Since I argue in good faith, I don’t see a conflict there, but if you feel like you’re no longer having a productive conversation, no hard feelings, thanks for hanging out as long as you have, and good luck in all your future endeavors. I may have triggered you into some uncomfortable cognitive dissonance, and I apologize for that. I’ll definitely have to work harder at becoming less triggering and more persuasive in the future.
I hope at some time in the future, you’re able to revisit this thread with perhaps a little more inner peace, and understand that despite your perception, I was genuinely trying to understand your point of view, and present the strongest version of your argument that I could. I even let slide some fairly childish behavior on your part in order to continue the conversation, but perhaps I should have seen those outbursts as hints that this was no longer something you were enjoying or engaged in.
Take care, and enjoy the interglacial!
Socially liberal, fiscally conservative, born again carnivore, musician, firearms instructor and skeptical civil rights activist.
Socially liberal, fiscally conservative, born again carnivore, musician, firearms instructor and skeptical civil rights activist.
"
https://medium.com/@edddison3D/mr-mixed-reality-vs-ar-augmented-reality-in-the-aec-industry-by-edddison-34ac7297fee6?source=search_post---------67,"Sign in
There are currently no responses for this story.
Be the first to respond.
edddison
Aug 31, 2015·4 min read
1. Online versus offline: Many AR solutions are online platforms (cloud). In some cases e.g. internal company business uploading of crucial data to the cloud is not allowed and possibility. Questions like “How safe are the files?” and “Who has access? ” have to be answered. So it often can be necessary to use offline standalone solutions and store data on your own infrastructure. edddison fulfills that requirement by plugging into the most important standalone software.
2. Interact with the 3D file: edddison enables to make a “walk through” the building out of your perspective, your point of view. It simulates exactly your view on a specific position in a room or building. It lets you move and walk smoothly through the environment, turn your head and look around. Bounding Boxes are defining allowed navigation areas and it is impossible to walk outside this areas, so you don´t get lost in space. You always know exactly where you are because you see your position on the floorplan. This makes it so satisfying to navigate with edddison. People do not get frustrated because they lose control, but instead they feel to have full control. View the model accurately and check views on relevant positions on the floorplan. AR products on the other hand let you interact with your files without boundaries. The AR software often only is capable of showing you the bird view of your building (see http://virtualplan3d.com./#!/plans or http://smartreality.co/) You can only turn the model with your hands and body (turning around the device), which quickly leads to physical constraints of the visualization. The great thing on the other hand in AR is that you can overlay your virtual model with the real physical world. For example to see what your new furniture would look like in your livingroom.
3. Import/export troubles: edddison plugs in to your existing 3D software and is not a 3D viewer. In edddison you keep your original 3D file as it is (That goes for Sketchup and Navisworks). No upload/import/export issues will appear, since users stay inside their usual environment and can easily create an interactive application. Unity is an exemption. It is a game engine which means you mostly have to import your files from your working application. Normally importing 3D files into an AR viewer or Unity in that case can be done relatively easy. If you like to have a better render quality, then you will have to do extra work like texturing, setting the lights etc. In this case with both technologies, AR viewer and game engine, you have to import 3D files and have an additional effort. Conclusion: For simple fast presentations it is better to stay in your 3D software like Sketchup or Navisworks and do an interactive presentation with edddison. For high quality presentations import your CAD file to the right tool like an AR viewer or Unity with the edddison plugin. But still with the AR viewer you will have the above mentioned problems of interacting with your model.
4. Hardware: AR is bounded mainly to tablets and phones. The device form factor is often too small to see details of big building projects. Zooming is annoying. edddison is much more open and can be integrated on every hardware, from tablet to huge power walls.
5. Preparation time: edddison lets you prepare an interactive presentation much faster without writing any code. AR solutions are different, every system works different.
6. Render quality: In AR software applications the render quality is often low and slow.
7. Tracking and wobbling: edddisons tracking technology is one of the best on the market. Fast, stable and precise. The movements and walkthroughs are beautiful and smooth. Using AR the view is often shaking. It looks ugly and unattractive.
8. Share: AR enables you to share your content via the app or the web, which mostly means using unsecure connections and your sharing partner must have the app installed. Via Web-sharing millions of people can have easy access to your file. Edddison can export projects for sharing purposes, for example with your business partner. It is like with a PDF, you can send and share files but it is not an app or online web service you are sharing with others.
Summary: edddisons strength is to make a walk through a virtual building while AR is perfect to view the building from bird view or overlay the virtual object with reality. Be sure to use the right solution for the right purpose.
Read an excellent article at AECbytes “Augmented Reality in AEC”
Benjamin Schmid, Thomas Kienzl
Originally published at edddison.com on February 23, 2015.
Create your own interactive 3D applications for fantastic presentations.
1 
1 
1 
Create your own interactive 3D applications for fantastic presentations.
"
https://blog.saaspass.com/how-to-secure-your-saas-aea19060904d?source=search_post---------68,"loud security is all about the procedures and technology that secure cloud computing environments against both internal usually being insiders and external cybersecurity threat vectors. Cloud computing is the delivery of information technology services over the internet. Using it is a necessity for governments and enterprises seeking to innovate and collaborate. Security management best practices which are designed to prevent unauthorized access are necessary to keep data and applications in the cloud secure from both current and emerging cybersecurity threats.
Cloud security is different based on the category of cloud computing that is used. The four main categories of cloud computing are the following:
Public cloud services which are operated by a public cloud provider — Software-as-a-Service (SaaS), Infrastructure-as-a-Service (IaaS) and Platform-as-a-Service (PaaS) are all part of this.
Private cloud services which are operated by a public cloud provider — These services provide a computing environment dedicated only to one customer which is operated by a 3rd party.
Private cloud services which are operated by internal staff — These services are the evolution of the traditional data center environment where internal staff operate a virtual environment that they control.
Hybrid cloud services — Private and public cloud computing configurations can all be combined, having the ability to host workloads and data based on optimization factors like cost, security, operations and access. Operations will involve internal staff and can have personnel from the public cloud provider.
Whilst using a cloud computing service provided by a public cloud provider, all data and applications are hosted with a 3rd party, marking a fundamental difference between cloud computing and your traditional IT setup, where data was held within a fully self-controlled network. Having a complete understanding of your security responsibility and architecture is the first step to building a successful cloud first security strategy.
The majority of cloud providers try to create a cloud that is secure for their customers. It is crucial that they prevent breaches in order to maintain public and customer trust. Cloud providers cannot control how their customers use their service in terms of what data they add to it or who has access to it. Customers can weaken cybersecurity in the cloud with their configuration, sensitive data, and access policies, however Zero Trust persistent authentication like SAASPASS mitigates such issues. In each public cloud service category, the cloud service provider and cloud service customer share different levels of responsibility and accountability for security. The service types are the following:
SaaS (Software-as-a-service) — Customers are responsible for securing their own data and user access.PaaS (Platform-as-a-service) — Customers are responsible for securing their own data, user access and applications.IaaS (Infrastructure-as-a-service) — Customers are responsible for securing their own data, user access, applications, operating systems and their virtual network traffic.
Within all categories of public cloud services, customers are responsible for securing their data and controlling who can access that data. The security of data within cloud computing is fundamental to have successful adoption and harness the benefits of the cloud. Organizations which consider adopting SaaS offerings such as SalesForce or Microsoft Office 365 are in need of planning on how they will fulfill their shared responsibility to protect their data in the cloud. Those considering IaaS offerings like AWS (Amazon Web Services) or Microsoft Azure need a much more detailed plan that starts with data, also covering cloud app security, operating systems and virtual network traffic which can also introduce the potential for data security issues arising.
As data in the public cloud is stored by a 3rd party and accessed over the internet, the following challenges arise in the ability to maintain a secure cloud:
Visibility into cloud data — Cloud services are accessed outside of the corporate network usually from devices not managed by IT. The IT team needs the ability to see into the cloud service itself to have full visibility over data, as opposed to the traditional means of being able to monitor network traffic.
Having control over cloud data — In a 3rd party cloud service provider’s environment, IT teams have less access to data than when they are controlling servers and applications on their own premises. Cloud customers are given limited control and thus access to underlying physical infrastructure is not possible.
Accessing cloud data and applications — Users can access cloud applications and data over the internet, making access controls based on the traditional data center network perimeter no longer effective. User access can be from any location or device, including BYOD (Bring-Your-Own-Device) technology.
Compliance — Cloud computing services implementation adds another dimension to regulatory and internal compliance. Your cloud environment may need to adhere to regulatory requirements such as HIPAA, PCI and Sarbanes-Oxley as well as requirements from internal teams, partners and global customers.
Cloud-native breaches — Data breaches in the cloud, unlike on-premises breaches, occur using native functions of the cloud. A Cloud-native breach is a series of actions by an adversarial actor in which they deploy their attack by exploiting errors or vulnerabilities in a cloud deployment without the use of malware in order to expand their access through weakly configured or protected interfaces to locate valuable data and exfiltrate that data to their own storage location.
Misconfiguration — Cloud-native breaches often fall to a cloud customer’s responsibility for security, which includes the configuration of the cloud service. The majority of companies cannot currently audit their IaaS environments for configuration errors. Misconfiguration of IaaS often acts as the front door to a Cloud-native breach, allowing the hacker to successfully deploy and then move on to expand and exfiltrate data. 99% of misconfigurations go unnoticed in IaaS by cloud customers.
Disaster recovery — Cybersecurity planning is needed to protect against the negative effects of significant data breaches. A disaster recovery plan includes policies, procedures and tools designed to enable the recovery of data allowing an organization to continue operations.
Insider threats — A rogue employee is capable of using cloud services to expose an organization to a cybersecurity breach.
In conclusion, more than a quarter of organizations worldwide have experienced a serious data breach and almost 100% of organizations worldwide use cloud services today. It is critical that everyone evaluates their cloud security and develops a strategy to protect their data. SAASPASS helps achieve this security by securing access. Zero Trust means Zero Breach!
SAASPASS Blog — 2FA,MFA,Authenticator,Password…
1 
1 clap
1 
Written by

SAASPASS Blog — 2FA,MFA,Authenticator,Password Manager,SSO,Security
Written by

SAASPASS Blog — 2FA,MFA,Authenticator,Password Manager,SSO,Security
"
https://medium.com/technology-media-telecom/how-to-achieve-business-it-alignment-12faa78dbfc4?source=search_post---------69,"There are currently no responses for this story.
Be the first to respond.
Senior executive expectations have been raised for traditional IT organizations that broker cloud computing services for their internal business stakeholders. Besides, there’s a belief that the lack of dimension maturity can slow digital service innovation outcomes.
As a result, International Data Corporation (IDC) now predicts that by 2016, 65 percent of global competitive strategies will require real-time IT-as-a-Service (ITaaS) solutions.
The ability of CIOs and their IT organizations to grasp how business wants cloud computing services that serve actual business needs — not traditional IT components — is significantly altering the ways in which service management is achieving success.
Quest for a Competitive Strategic Advantage
New business technologies are fundamentally altering how IT organizations function, how business is conducted, and how enterprises compete in the marketplace. IDC believes that this environment requires IT to deliver services that are focused on realizing the highest-level strategic objectives.
The ability of IT to immediately achieve a maturing business focus within its strategic planning and execution process is a key imperative. In this latest study, IDC has explored a planning framework that enables CIOs to attain digital business transformation goals.
“Creating a mature, strategic business alignment between IT and its customers requires a clearly defined end-state vision, an empowering IT culture capable of initiating rapid change, and an increasing focus on the financial costs and benefits — as measured from a consumption basis, versus the traditional cost-budget basis,” said Bill Keyworth, vice president of research at IDC.
IDC also believes that a method of assessing service innovation maturity is needed as a planning tool to help prioritize and evaluate the progress of an IT organization rising to meet the progressive Line of Business leader demands.
Additional findings from the IDC study include:
Without question, maturing the IT business dimension requires the active participation of top IT management. It’s equally important to ensure all stakeholders and beneficiaries of IT within the enterprise are kept abreast of the business dimension maturity process.
Moreover, IT service innovation demands a multifaceted approach that seeks to ensure that all five maturity dimensions are simultaneously addressed, continually focusing on the least mature process — or, the bottleneck that’s holding up all other maturity dimensions.
Rather than tackle IT business-facing objectives through massive project implementations, IDC recommends companies take a “theory of constraints” approach, frequently used within ITaaS to quickly identify, analyze, and resolve any impediment to IT-business alignment.
Furthermore, according to the IDC assessment, IT organizations that achieve long-term success will be characterized by a service-centric culture that tracks effectiveness through an outside-in perspective, rigorous IT competitive analysis, and business-oriented metrics.
GeoActive Group | David H. Deans
GeoActive Group | David H. Deans
Written by
Technology, Media, Telecom analyst, consultant, columnist
GeoActive Group | David H. Deans
"
https://insights.calls9.com/digital-cloud-intranet-part-1-is-your-internal-information-for-employees-ignored-d447bf78e008?source=search_post---------70,"If you want to know what’s possible with digital then our Digital Transformation Audit is for you. We’ll take you through a process that helps us quickly understand where digital can have the biggest impact for your business.
Whether you’re a start-up or an established professional services business we’re experts in getting under the skin of the problem you are trying to solve and delivering products and services that drive growth and reduce inefficiencies.
Keep up-to-date with digital transformation and how it impacts your business. Here you’ll find our expert insights, news updates, white papers and events.
﻿Benchmark your digital maturity and identify where digital can help you improve your business.
﻿Websites ﻿• Apps ﻿• Customer Portals ﻿• Personalisation ﻿• 360° Customer View
Strategic consultancy and technology solutions to improve your business. 
﻿﻿Our team embeds with your delivery team to augment your existing capabilities.
﻿﻿﻿Knowledge Sharing ﻿• Internal Communication ﻿• Intranets ﻿• Process Automation ﻿• Workplace Analytics
﻿A fully managed service where Calls9 leads on all aspects of your digital transformation.
﻿﻿﻿Launch Digital Services • E-commerce ﻿• Marketplaces ﻿• Networks ﻿• Dynamic Pricing 
Strategic consultancy and technology solutions to improve your business.
"
https://medium.com/techlog/log-making-the-google-cloud-dns-accessible-for-docker-for-internal-service-discovery-a087f70d8d13?source=search_post---------71,"There are currently no responses for this story.
Be the first to respond.
GCP [Google Cloud Platform] allows machines to be referenced by their internal DNS alias names. Sometimes we use these aliases while connecting to the host machine. For example, if you have a machine running redis, whose alias is “redis-master”, one can simply connect to it as follows:
When running the same code inside a docker container, one will not be able to access this network alias because the default DNS which docker uses is :
# /etc/default/dockerDOCKER_OPTS=""--dns 8.8.8.8 --dns 8.8.4.4""
Local VM instances on GCP use an internal DNS to figure out the network aliases.
# /etc/resolv.confnameserver x.x.x.x
Modify the docker defaults using the nameserver address from the resolve.conf. # /etc/default/dockerDOCKER_OPTS=""--dns x.x.x.x --dns 8.8.8.8 --dns 8.8.4.4""
And then, you will only need to restart the Docker demon and you will be able to access all services in GCP through their network aliases.
Tech ramblings
Written by
Philomath, Tech Guy. https://www.arriqaaq.com/
Tech ramblings
Written by
Philomath, Tech Guy. https://www.arriqaaq.com/
Tech ramblings
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/connecting-cloud-functions-with-compute-engine-using-serverless-vpc-access-79c5cd7420c7?source=search_post---------72,"There are currently no responses for this story.
Be the first to respond.
Serverless products on Google Cloud Platform (GCP) such as Cloud Functions and App Engine due to their serverless nature (hidden server infrastructure) can connect to some of the GCP products (Compute Engine, Memory Store, Cloud SQL) only through their public IP addresses which of course is adding security risks by exposing them to the whole internet as well as adding latency, since traffic goes through public internet and not through the fast Google network. In most cases, there is an option for products to have just internal IP address which is not accessible from the internet but only from within GCP network, but so far it wasn’t possible for serverless products to make connections that way due to the networking issues.
Recently GCP introduced Serverless VPC Access which is like a glue between serverless product and other products in VPC network. Basically with the creation of Serverless VPC Access Connector, under the hood f1-micro instances are created which are handling connections and transfers. Only requests from serverless instance to other servers are supported, it’s not possible to make requests to the serverless instances from the other products via the internal network, only through public internet via an HTTP request.
At the moment Serverless VPC Access can be used for Cloud Functions and App Engine (not yet for managed Cloud Run) and it needs to be in the same region where serverless service is. Pricing at the moment is “as 1 f1-micro instance per 100 Mbps of throughput automatically provisioned for the connector"" which should be about ~5$ per month, although it may increase since service is at the moment in Beta state.
To demonstrate functionality, I will use Cloud Function written in Python which connects to Redis (deployed as a container on Compute Engine) which is used as a simple cache service.
When HTTP request comes to the Cloud Function, within the code, a request is made to Redis server which goes through Serverless VPC Access Connector to the Compute Engine with an internal IP address and then back as illustrated on the diagram above. This can be used as a simple cache service, although Cloud Memorystore is a similar managed product with more capabilities.
Repository with scripts and code is located on Github. There are several steps to be done to make this work:
We need to create a firewall rule which will allow access to Redis (default port 6379).
Then create instance
As I mentioned previously I am creating an instance based on Redis Docker container image which is hosted on Docker registry and automatically deployed on Compute Engine instance, I am applying firewall tag, explicitly assigning an internal IP address (based on region) and some mandatory stuff like machine type (f1-micro is for demonstration purpose, not real life uses case), zone. Now this instance has public IP as well but with the next command, I am removing it. Reason for this is when I create an instance without public IP connection doesn’t work. When I create an instance with public IP and then remove it right away, it works ok. Maybe it’s some temporary glitch or maybe I’m doing something wrong, but that’s how it worked for me. The final state is that instance has only internal IP address.
One time command to activate Serverless VPC Access API:
A command to create Connector doesn’t have many options:
Important is to place a connector in the same region where a serverless deployment is. I didn’t figure out how important IP range is, but 10.8.0.0/28 as recommend during Web UI creation worked. After creation there are many options, i.e. there is only option to delete connector or to view basic data. In Cloud Console, Serverless VPC Access in under VPC Network section.
Sample code used in Cloud Function looks like this:
I am passing IP of Redis instance through environmental variable which is set during deployment and corresponds to the internal IP address of Compute Engine instance. Besides that I am getting, setting and deleting cache key to do interaction with the Redis database.
In order to deploy Cloud Function and that works with Connector, extra IAM roles need to be set for Cloud Functions Service Agent account, those are Project/Viewer and Compute/NetworkUser. Cloud Functions Service Agent account has usually email: service-<PROJECT-NUMBER>@gcf-admin-robot.iam.gserviceaccount.com
Commands are:
To deploy Cloud Function VPC Connector needs to be defined in the following format: projects/<PROJECT_ID>/locations/<REGION>/connectors/CONNECTOR_NAME
Now when you hit Cloud Function URL you should get a response from Redis.
Finally, Serverless VPC Access should work as well with both the first and second generation of Standard App Engine runtimes for which configuration is adding Connector “path” as in case of Cloud Function to yaml config file.
Every Monday I am publishing GCP Weekly, a newsletter about Google Cloud Platform and related technologies, if you’re interested you can subscribe on the website.
Google Cloud community articles and blogs
10 
10 claps
10 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
IT Consultant with focus on Google Cloud Platform, creator of GCP Weekly, a weekly newsletter about GCP https://www.gcpweekly.com
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/@yuri-mednikov/messaging-patterns-for-vertx-4-eventbus-273ae9582c1f?source=search_post---------73,"Sign in
There are currently no responses for this story.
Be the first to respond.
Yuri Mednikov
Jun 15, 2021·9 min read
When you work on distributed cloud platforms, the topic of an internal communication arises. By communication we understand architecture decisions that ties up various level of your systems; for instance, your service’s components need to be loosely coupled in order to ensure a higher degree of an autonomy. In my practical experience with Vertx, I defined an importance of the following principle: instead of layered components that are tied together with the dependency injection container, it is wiser to separate levels to various Vertx units (verticles) and organize a fluid communication between them. Let me be clear: I don’t propose you to forget about DI immediately; at the end of the day, this approach has its bright sides. But my goal is to encourage to try Eventbus-based messaging between components.
Before we will continue with practical concerns, it seems wise to talk about theoretical foundations of the EventBus pattern (you can also find another term — “Message Bus”). This is due to the fact, that this technique is not only connected with Vertx, yet, it goes far and this principle is used among most event-driven systems. The main idea behind this pattern is to be able deliver messages (sometimes called “events”, however it is not 100% correct; we will use “messages”) asynchronously via a common component, called “broker”. In this case, the whole system is considered loosely coupled, because different parts can be connected or disconnected as needed without the necessity to change the whole system. Components communicate with each other using various messaging protocols, such as publish-subscribe, point-to-point, request-response etc. We will observe those are available in Vertx 4 later in this post.
In its most basic way, we can define the event bus to have following parts: publishers, subscribers, messages. This implementation is presented on the graph below:
The mentioned structure can be considered similar to the Observer design pattern, yet only to a certain degree. Within that pattern, the Observable component has to hold references to Subscribers in order to notify them on new events. This fact makes the whole system strong coupled, as requires more steps in order to add or remove components. From the other side, the EventBus, that acts as a intermediate actor, solves this issue, because instead of of registering with Observables, Subscribers can register with the EventBus. This also hides Subscribers from Observables and vice versa, because now, components communicate using messages, that delivered to the address. This address can belong to a single (one-to-one messaging) or to many recipients (broadcasting).
So, that is the simple event bus architecture. In next several subsections we will have a deeper overview of its components.
The publisher is known as a component, that send messages to an event bus. The difference with an Observable in this situation is following: no need to contain references to Subscribers; an allowance of broadcasting (sending messages in both directions in the many-to-many fashion); a single component can have a functionality of both Subscriber and Observable in very simple way.
When it comes to the particular implementation in Vertx 4, we can find following methods of the EventBus class, that allows to implement a publishing functionality:
Don’t worry, as we will cover these methods and underlaying principles in more details, when we will come to practical aspects. As for now, we need to remember, that publishing techniques are not limited to a single approach; as well, it is allowed to deliver messages to a single or to many consumers. Moreover, we talk about the logical functionality, however the particular component in your system can implement both publisher and subscriber behavior. That leads to another aspect — how that is implemented in reality. Unlike some other frameworks, Vertx 4 does not force you to implement interfaces, rather the implementation is organized around the EventBus component in two ways:
Your implementation depends on your needs, as Vertx does not force developers to stick with the particular approach.
The subscriber is essentially a receiver, that listen for messages and is able to process them. The last part of the definition is important. The Subscriber in the EventBus architecture has differences with Subscriber in the Observer pattern, namely it does not rely to the Observable directly, but has a particular address, where messages are delivered by a bus. It is crucial to note here, that even in the case of point-to-point communication (single Subscriber and single Observable), components are not tied directly. You need to remember, that based on its implementation, EventBus in Vertx will deliver a message to a single destination, but it does not mean that you will have a single destination in your system.
In Vertx, subscribers are called consumers and are represented by the MessageConsumer interface. There are two types of consumers in Vertx:
In both situations, we utilize the MessageConsumer, which is from the technical point of view, a stream where messages are written to and from where it is possible to read them by receiver components.
The message component is what is delivered using the EventBus. Unlike the Observer pattern, where both sides are explicitly aware about the type of data sent/received, with the EventBus its type is not forced, so it is important to ensure about compatibility. However, it is worth to mention, that the message is not limited just to data itself. Typically it contains other useful information, such as errors, replying destinations, headers.
In Vertx, the Message class acts as a container, that contains a body, headers, origin and destination addresses, as well error information. This class is also used for replying by call of the reply method. It is also possible to explicitly break the delivery and to notify the sender using the fail method (which acts similar to Bad Request response).
Please note, that by default, the EventBus class transports data as a basic Object class. You need to register a custom codec if you would like to send your own entity classes via the Event Bus and to specify it in using the MessageConsumer class. This step is outside the scope of this post; you can refer to my Principles of Vertx ebook in order to get practical information about it.
The last component which is observed in the theoretical part of this post, is a bus itself. The event bus acts as an intermediate component between publishers and subscribers. This is similar to an event broker topology pattern — the bus acts as a broker in this case. In Vertx, the Eventbus is related with the Vertx instance, in a sense, that it is a single bus for a single Vertx instance. In order to obtain a reference for the EventBus use the following code:
An interesting fact to know about the Vert.x Event bus implementation, is that it allows you to utilize bridges, that extends communication possibilities of the EventBus. Such, the TCP bridge, which is built on top of TCP protocol, allows any application that can create TCP sockets to interact with a remote Vertx instance using the event bus. A different way to communicate with the Event bus outside the JVM is by using Sock JS library. With this approach you need to register a Sock JS handler, that enables your application to communicate with JavaScript web applications.
From a technical perspective, in order to use the event bus messaging capabilities you have to go through these steps:
This is the end of the theoretical part. Now, we can continue exploring practical implementations of messaging protocols, available with the Vertx 4 EventBus.
When we talk about message driven systems (event driven systems), we consider ones, that use different asynchronous messaging protocols in order to communicate between components. This is also refereed to message channels. From the technical point of view, the selection of the particular implementation depends on a set of factors, such as:
These aspects are important to consider from the beginning to choose the right type of messaging protocol for your system. Next, we observe each of them.
The publish-subscribe messaging pattern is the most straightforward. It is commonly used to implement a broadcasting (delivery of message from a single publisher to many consumers). The Vertx Eventbus ensures that the message will be broadcasted to all subscribers, that hold the same address. The idea is presented on the graph below:
In Vertx, the publish method is used to deliver the message to all registered consumers. Take a look on the code snippet:
On the counterpart, in order to receive the message you need to perform following steps:
Please note, that the handler registration (with the handler method) is required in order to make the consumer registered within the event bus. Also, the consumer logic is similar to all messaging protocols, so we will omit it in remained subsections.
As it was mentioned already, the broadcasting (publish-subscribe pattern) is a delivery of messages from a single publisher to many consumers. However, you face cases, when you need to perform a transportation between a single publisher and a single consumer. That is called the point-to-point messaging pattern. Please refer to the diagram below:
Being a software architect for a long time, and especially working with Vertx 3 and then 4, I would like to add here an important note, that usually is missed by developers. Point-to-point messaging, implemented with the Vertx, means that the event bus will ensure that a message will be delivered to a single consumer. Yet, it does not mean, that it ensures that there is only single consumer in the system. For sure, that is a developer’s duty to aware, that the destination has a unique recipient, otherwise it will be deliver it to the first using the round-robin algorithm.
Like I said, the receiver part is same for all patterns, so please refer to the previous subsection.
The final message channel, that we will observe in this post is the request-response pattern. This is a special case of the previous technique. The difference is that a consumer can send back an answer (response) for the message (request) and the publisher should guarantee to receive it. In Vertx it is performed by specifying a reply handler. The general idea is demonstrated below:
In the code, it can implemented as following:
The consumer part of this pipeline is similar to what we already did, but we can provide a reply in addition to it. That is implemented on the Message object, not on a consumer, like here:
The EventBus is considered as a “nervous system” of Vertx, because it allows to implement event-driven apps. Such architecture is a very good way to ensure loosely coupling of components and to organize an asynchronous communication using various message patterns. In this post we observed all three message patterns (or channels) available with the Vertx 4 EventBus — publish-subscribe (broadcasting), point-to-point and request-response. Many additional aspects although were not covered because they are out of scope, such as error handling, how to deal with invalid delivery (dead messages collection) as well some advanced topics (clustering, bridges) etc. We will cover them in their turn. I hope, that this post was helpful for you and motivated you to explore such wonderful piece of software (masterpiece — I would say) as Vertx. In case of questions, please contact me or share a comment below.
I pay my bills by telling computers what to do
See all (5)
1 
1 clap
1 
I pay my bills by telling computers what to do
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@BreezeTelecom/when-cloud-shares-power-with-the-business-34f8620c6932?source=search_post---------75,"Sign in
There are currently no responses for this story.
Be the first to respond.
Breeze Telecom
Sep 23, 2016·4 min read
In 1996, it was the day when the term “cloud computing” was mentioned in an internal Compaq document. Now, a decade later, when the Amazon’s Elastic Compute has come up with modern day cloud computing, there is no looking back. The remarkable growth rate of 50% annually for dealers supplying cloud-based software solutions is perceptible.
Speed to market, scalability, lower costs and higher productivity are the widely known advantages of the cloud-computing that are acknowledged by enterprises throughout.
So what is deciding factor for implementation of such a useful tool? As the saying goes “A technology is as strong as its users”, the strongest link in this technology is the humans who manage it or are a part of the process.
A common procedure that includes in the transition to the cloud is that the Human Resources department can now adapt to a form of HR/IT hybrid. Many responsibilities that were earlier a fell under IT, will later be a falling under HR post implementation, like managing and updating HR applications.
Basically when the enterprise collaborates with cloud computing options, it takes up the role of IT middle man, who was earlier a link between real world application and businesses.
From structuring the key analytics, to innovating new services to keep up to the customer’s requirement, cloud shifts the respective roles from IT to HR and Finance. It is the HR and Finance who makes decisions based on business value necessary for business growth. There is a startling change in the operations that were some day considered as ‘back-office’ work.
This transition of power from being the Chief Information Officer, handling all of the IT sector, to the Chief Financial Officer (CFO), it simply empowers the business to work creatively that would not have been possible without the concept of cloud computing. Earlier when the businesses were beholden by the technology, now the same technology enables them to explore and make decisions in various business processes.
Along with the benefits the cloud technology introduces, it also tends to cause some anxiety and frustration among the groups working on business development models. So, instead of being the source of innovation, it can sometimes prove to be the one obstructing the growth and productivity. Therefore, even though cloud computing proves to be the ultimate partner in evolution of an enterprise, it is important that the database or functioning of the entire transition has to be managed by the right person. The person has to be one who can use the concept for the betterment and not who totally crashes the idea of cloud computing.
There are times when the challenge can be recognized simply, and the solution seems difficult, but when the team plans carefully, solution can be found. To fully utilize the true potential and power of cloud computing, you can note down the following points that surely explains the solid foundation steps for your enterprise to build a healthy environment at work place.
Bring into being the change management on very first day.
Give some time to the group to apprehend the local perspective.
Keep in mind the requirements of all the groups, not only just HR, managers and employees.
Along with training but consistent communication and engagement are the keys
Keep in mind the human factor involved in every small change.
Recognize the strong resources-internal or third party provider who helps you to modify the business solution by keeping the information updated.
When you plan to collaborate with cloud computing concept as a growth kit for your enterprise, you need to know that not only does the technology transition happens but also there is a significant organizational shift. The various teams in the business should be supporting the concept at different phases, staring from design and implementation to modification and management. In order to reap the true benefits of cloud, the HR department comprising of Finance or education needs to be the back support in all the business processes. The introduction of new features and functions to strengthen the business leader’s ability to make data-driven decisions will be great encouragement to organization’s expansion.
Breeze-Telecom brings order with its smartly packaged IT solutions in the chaotic world of digital transformation that is affecting businesses across the world. From Mobility Data Solutions to WAN/SD WAN, AWS/Azure and Datacenter on the cloud, the company caters to all contemporary enterprise-level IT requirements.
Breeze-Telecom connects #IT solutions| #FiberIOptic #Internet| #SDWAN| #VoIP| #Cloud #tech| #MSP| #BigData| #IP| #Datacenter| #SaaS| #PaaS| #UCaaS| #IaaS
Breeze-Telecom connects #IT solutions| #FiberIOptic #Internet| #SDWAN| #VoIP| #Cloud #tech| #MSP| #BigData| #IP| #Datacenter| #SaaS| #PaaS| #UCaaS| #IaaS
"
https://medium.com/adidoescode/internal-and-external-connectivity-in-kubernetes-space-a25cba822089?source=search_post---------76,"There are currently no responses for this story.
Be the first to respond.
As you are making your way through all the stages of your app’s development and you (inevitably) get to consider using Kubernetes, it is time to understand how your app components connect to each other and to the outside world when deployed to Kubernetes.
Knowledge you will get from this article also covers “services & networking” part of CKAD exam, which currently takes 13% of the certification exam curriculum.
Kubernetes services provide networking between different components within the cluster and with the outside world (open internet, other applications, networks, etc)
There are different kinds of services, and here we’ll cover some:
NodePort service maps (exposes) port on the Pod to a port on the Node. There are actually 3 ports involved in the process:
Selectors are the way to refer (link) service to a certain set of pods.
A set of pods gets selected based on the selector (in almost all cases, pods from the same deployment), service starts sending traffic to all of them in a random manner effectively acting as the load balancer.
If mentioned pods are distributed across the nodes, service will be created across the nodes to be able to link all the pods. In case of multi node service, service exposes same port on all nodes.
In the case of an application consisting of multiple tiers deployed to different sets of pods, a way to establish communication between different tiers inside the cluster is necessary.
For example, we have:
Each of above mentioned 18 pods have their own distinct IP addresses, but making communication that way would be:
ClusterIP service provides us with unified interfaces to access each group of pods — it provides a group of pods with internal name/IP.
ClusterIP is default type of service, so if service type is not specified, k8s assumes ClusterIP.
When this service gets created, other applications within the cluster can access the service through service IP or service name.
In short, a LoadBalancer type of service is provisioning external load balancer in cloud space — depending on provider support.
The deployed load balancer will act as NodePort, but will have more advanced load balancing features and will also act as if you got additional proxy in front of NodePort in order to get new IP and some standard web port mapping (30666 > 80). As you see, it’s features position it as the main way to expose service directly to the outside world.
The main downside of this approach is that any service you expose needs it’s own load balancer, which can, after a while, have a significant impact on complexity and price.
Let’s briefly review the possibilities:
Above creates external load balancer and provisions all the networking setups needed for it to load balance traffic to nodes.
Note from k8s docs: With the new functionality, the external traffic will not be equally load balanced across pods, but rather equally balanced at the node level (because GCE/AWS and other external LB implementations do not have the ability for specifying the weight per node, they balance equally across all target nodes, disregarding the number of pods on each node).
If you want to add AWS ELB as an external load balancer, you need to add the following annotations to load balancer service metadata:
When getting into space where we are managing more than one web server with multiple different sets of pods, above mentioned services turn out to be quite complex to manage in most of the real life cases.
Let’s review the example we had before — 2 APIs, redis and frontend, and imagine that APIs have more consumers than just frontend service so they need to be exposed to open internet.
Requirements are as following:
Setup needed using the above services:
ClusterIP is necessary, we know it has to be there — it is the only one handling internal networking, so it is as simple as it can be.
External traffic however is different story, we have to set up at least one service per component plus one or multiple supplementary services (load balancers and proxies) in order to achieve requirements.
Number of configs / definitions to be maintained skyrockets, entropy rises, infrastructure setup drowns in complexity…
Kubernetes cluster has ingress as a solution to the above complexity. Ingress is essentially a layer 7 load balancer.
Layer 7 load balancer is name for type of load balancer that covers layers 5,6 and 7 of networking, which are session, presentation and application
Ingress can provide load balancing, SSL termination, and name-based virtual hosting.
It covers HTTP, HTTPS.
For anything other then HTTP and HTTPS service will have to be published differently through special ingress setup or via a NodePort or LoadBalancer, but that is now a single place, one time configuration.
In order to set up ingress, we need two components:
There are few options you can choose from, among them nginx, GCE (google cloud) and Istio. Only two are officially supported by k8s for now — nginx and GCE.
We are going to go with nginx as the ingress controller solution. For this we, of course, need new deployment.
Deploy ConfigMap in order to control ingress parameters easier:
Now, with basic deployment in place and ConfigMap to make it easier for us to control parameters of the ingress, we need to set up the service to expose ingress to open internet (or some other smaller network).
For this we setup node port service with proxy/load balancer on top (bare-metal /on-prem example) or load balancer service (Cloud example).
In both mentioned cases, there is a need for Layer 4 and Layer 7 load balancer:
Layer 4 load balancer — Directing traffic from network layer based on IP addresses or TCP ports, also referred to as transport layer load balancer.
NodePort for ingress yaml, to illustrate the above:
This NodePort service gets deployed to each node containing ingress deployment, and then load balancer distributes traffic between nodes
What separates ingress controller from regular proxy or load balancer is additional underlying functionality that monitors cluster for ingress resources and adjusts nginx accordingly. In order for ingress controller to be able to do this, service account with right permissions is needed.
Above service account needs specific permissions on cluster and namespace in order for ingress to operate correctly, for particularities of permission setup on RBAC enabled cluster look at this document in nginx ingress official docs.
When we have all the permissions set up, we are ready to start working on our application ingress setup.
Ingress resources configuration lets you fine-tune incoming traffic (or fine-route).
Let’s first take a simple API example. Assuming that we have just one set of pods deployed and exposed through service named simple-api-service on port 8080, we can create simple-api-ingress.yaml.
When we kubectl create -f simple-api-ingress.yaml we setup an ingress that routes all incoming traffic to simple-api-service.
Rules are providing configuration to route incoming data based on certain conditions. For example, routing traffic to different services within the cluster based on a subdomain or a path.
Let us now get to the initial example:
Since everything is on the same domain, we can handle it all through one rule:
There is also a default backend that is used to serve default pages (like 404s), and it can be deployed separately. In this case, we will not need it since the frontend will cover 404s.
You can read more at https://kubernetes.io/docs/concepts/services-networking/ingress/
And, what if we changed the example to:
It is also possible, with the introduction of a new structure in the rule definition:
Note (out of scope): You can notice from the last illustration that there are multiple ingress pods, which implies that ingress can scale, and it can. Ingress can be scaled like any other deployment, you can also have it auto scale based on internal or external metrics (external, like number of requests handled is probably the best choice).
Note 2 (out of scope): Ingress can, in some cases, be deployed as DaemonSet, to assure scale and distribution across the nodes.
This was a first pass through the structure and usage of k8s services and networking capabilities that we need in order to structure communication inside and outside of the cluster.
I, as always, tried to provide to the point and battle-tested guide to reality… What is written above should give you enough knowledge to deploy ingress and setup a basic set of rules to route traffic to your app and give you context for further fine tuning of your setup.
Important piece of advice: Make sure to keep all the setups as a code in files, in your repo — infrastructure as a code is essential part of making your application reliable.
Where Software meets Sports
282 
282 claps
282 
Written by
http://rastko.tech/ — Programmer, Architect, Tech Lead, and Tech Enablement and SRE guy at adidas ecom.
Where Software meets Sports
Written by
http://rastko.tech/ — Programmer, Architect, Tech Lead, and Tech Enablement and SRE guy at adidas ecom.
Where Software meets Sports
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://levelup.gitconnected.com/building-an-internal-kubernetes-platform-f714a0b5ef72?source=search_post---------77,"An internal Kubernetes platform is the next step to get the full benefits of Kubernetes for your organization.
The container orchestration technology Kubernetes has become the dominant solution for cloud infrastructure and as such, it is maturing at an unrivaled pace. Many companies have already adopted Kubernetes or are in the process of it. However simple Kubernetes adoption is not enough, but you need a broader diffusion of Kubernetes in your organization.
Some early adopters show what next steps are needed for this deeper adoption of Kubernetes: An internal Kubernetes platform.
Spotify, Datadog (page 25), and Box are just some publicly available examples of companies that have built such platform for their engineering teams.
An internal Kubernetes platform is a platform that allows engineers to get a direct access to a Kubernetes environment on-demand for company-internal use.
From an engineer’s perspective, the platform must provide self-service namespaces or another Kubernetes environment on-demand. It also needs to be easy to use even for engineers that are not experienced in working with Kubernetes. And finally, an internal Kubernetes platform must provide the engineers a real Kubernetes access, which means that it is possible for them to interact with Kubernetes and use Kubernetes-native components and resources. For this, it is not sufficient to just use a Platform-as-a-Service system that runs on top of Kubernetes or provide a Kubernetes test environment behind a CI/CD pipeline.
From an administrator perspective, an internal Kubernetes platform needs to be centrally controllable, so that the admins can oversee the whole system because with a company-wide roll-out the usage of it will be much higher than with some first adoption experiments. It is thus also necessary that the platform allows to manage and limit the users efficiently, so that the total cost of running the internal Kubernetes platform does not get out of hand.
A good internal Kubernetes platform combines all of these features in a single solution that supports all stakeholders in their further Kubernetes adoption.
With the increasing amount of applications running on top of Kubernetes, it is also necessary that the adoption of Kubernetes spreads within organizations, so that developers can directly interact with the technology underlying their applications. Only if this requirement is fulfilled, the Kubernetes advantages of faster development cycles and enhanced application stability can be realized.
As can be seen in the Stack Overflow Developer Survey 2020, developers are ready for this next step as Kubernetes is a highly “wanted” and “loved” technology, i.e. developers want to work with it and the ones already doing so like it.
The first step to enable engineers to work with Kubernetes is to provide them access to it with individual Kubernetes work environments. And this is exactly what an internal Kubernetes platform does, which is why it is so fundamental in a successful further adoption.
Theoretically, there are several options to provide engineers a direct Kubernetes access: Local Kubernetes clusters, individual clusters, or shared clusters.
1. Local Clusters: Local clusters are special Kubernetes versions made for running on the engineers’ computers, such as minikube or kind. As such, they are necessarily limited to the locally available computing resources and do not have all Kubernetes features that exist in the “real” Kubernetes running in cloud environments. The local runtime environment further makes it impossible to streamline the setup process, so that it needs to be done by the engineers themselves, which requires some k8s expertise.
For this, local Kubernetes clusters are well suited and very popular for the initial adoption and experimentation phase but they are not the right solution for further adoption steps or to build an internal platform.
2. Individual Clusters: Individual clusters are clusters that are only used by one engineer. It is possible to build an internal platform that provides your engineers full individual clusters. In principle, EKS, AKS, and GKE are already “external” Kubernetes platforms and if you gave every engineer access to these public cloud provider solutions, you would have something like an “internal” Kubernetes platform. However, such a solution would be very expensive as computing resources are used extremely inefficiently (the cluster management fees without computing resources would already be $70 per month per engineer). It is also much harder for admins to oversee a system with a huge number of clusters and to find out what is still used and what could be deleted. Finally, most companies would not want to give every engineer direct access to their cloud provider account.
So, building an internal Kubernetes platform with individual clusters for engineers is theoretically possible but it would be very expensive and inefficient, which is why it will hardly be ever done in reality.
3. Shared Clusters: Shared clusters are multi-tenant clusters that are used and shared with several engineers or teams. Shared clusters are real cloud-based Kubernetes clusters and so have basically endless computing resources and can be configured in the same way as the production system. At the same time, only one cluster is needed for a whole engineering team, which makes it easy to manage and control for admins and keeps the resource utilization high.
Shared clusters are the preferred (and only feasible) way to build an internal Kubernetes platform.
To get a more detailed comparison of the different types of Kubernetes environments, take a look at the comparison of local and remote clusters and the comparison of individual and shared clusters.
If you use a shared cluster environment for your internal platform, engineers will often work with namespaces, which are used as basis to establish multi-tenancy in Kubernetes. For many use cases, the separation of users via namespaces will be enough but there also are virtual Kubernetes clusters (vClusters) that provide a harder form of multi-tenancy and so more stability for your platform. vClusters have the additional advantage that they give the engineers the feeling of working with an individual cluster, so that the engineers can freely configure everything as they need it, for example, they can even independently choose which Kubernetes version they want to use.
Now, the question remains how to provide engineers access to the namespaces or vClusters on a shared cluster. One simple solution for this would be that the cluster admin manually creates and distributes the namespaces and vClusters for the engineers. However, this would create a highly problematic bottleneck that would be a huge productivity killer for engineers, as can be seen in a VMware survey which found that “waiting for central IT to provide access to infrastructure” is the number 1 impediment for developer productivity.
For this, you will need a self-service platform that is easily useable, so developers can start working productively with it from the start and do not have to learn much about the Kubernetes details first.
As previously described, you want to build your internal platform with shared clusters. Still, you need to decide on which Kubernetes platform your internal platform will run.
Here, you have to choose between a public cloud and a private cloud environment. The most popular public cloud environments are EKS (Amazon Web Service), AKS (Microsoft Azure), and GKE (Google Cloud).
Alternatively, it is also possible to use a multi-cloud or hybrid-cloud approach, which combines several cloud providers or even public and private clouds. Special tools such as Rancher and OpenShift can be very useful to run this type of system.
A good starting point for your internal Kubernetes platform is to use just a single environment that reflects the environment of your production system best. For example, if you use EKS for your Kubernetes applications in production, it makes sense to also start with EKS for your internal platform for development, testing, CI/CD,… If you already have a multi-cloud system in production, you need to evaluate if it makes sense to recreate this setup or if a single-cloud system might be enough.
As a next step, you need to determine how your platform should work and what the architecture should look like.
One question in this area is how the platform users’ authentication will work as they will need accounts to access it. There are several options to care for this part of the user management. The simplest is to let admins create the user accounts centrally. This is a good option if you have a relatively small team and not so much fluctuation. In larger teams, it makes sense to let users sign-on themselves, e.g. by allowing users with a specific email address to create new accounts, or by implementing a Single-Sign-on (SSO) system that uses the user management of another service you already have in place, such as GitHub. For the implementation of such a system, you might take a look at dex which is an OpenID Connect provider that connects to many different services, such as LDAP, GitHub, GitLab, Google, Microsoft, and many others.
Another question that you need to answer is how the users of the internal platform will interact with it. Here, you can decide between a graphical UI, which might be easiest to work with for beginners but is also hardest to build, a CLI, that can potentially be well integrated in engineers’ workflows, or Kubernetes Custom Resource Definitions (CRDs), which would be the implementation that is closest to fundamental Kubernetes. Of course, it is also possible to combine the different options, e.g. by providing a GUI that creates CRDs in the background.
The third step in building a Kubernetes platform is the actual setup. Here, you face a make-or-buy decision.
The main benefit to build your own platform from scratch is that such a platform is perfectly customizable as you can determine every part of it. At the same time, this means that the setup requires a lot of work and time. You should also expect that you constantly have to improve your platform as your users’ needs and Kubernetes itself evolve. For this, choosing to build an own platform is mostly suited for organizations with very special needs, e.g. as they are working in a regulated industry, or for companies that are so large that some customization benefits pay off compared to the huge effort. Spotify, as an example, decided to build an own platform just for internal use. If you decide to go this way, you may still build upon existing solutions such as previously mentioned dex or kiosk, which is a multi-tenancy extension for Kubernetes.
For most companies, however, just buying an existing solution may be the better option because the setup is much easier and faster, you do not have to dedicate permanent development resources to improve the platform and because you will automatically get some best practices. At the same time, such a solution has only limited options for customization, which is why it may not be feasible to use for very specialized companies. One advanced off-the-shelf internal Kubernetes platform solution is loft. loft works with any Kubernetes cluster, it provides a GUI, a CLI and is fully based on Kubernetes CRDs. It also takes care of the user management, allows SSO and has some additional features such as a sleep mode to save cost by shutting down unused namespaces and vClusters.
After you have set up your internal Kubernetes platform, you need to prepare the roll-out in your organization. Here, you should keep in mind that for many engineers who have not worked with Kubernetes before not only the platform but also the workflows will be new. For this, you should prepare an extensive documentation for both the platform and the new development/testing/deployment workflows. To make the transition as easy as possible for the engineers, it makes sense to pre-configure and to implement smart defaults as much as possible.
Having the right developer tooling for the engineers is very helpful at this stage, especially as most of these tools can be pre-configured for your typical use cases. Examples for tools specifically made for developers are DevSpace, which also has an optional loft integration if this is your chosen platform, Skaffold, and Tilt.
As a roll-out strategy, it is often a good idea to start with a single team and progress gradually. This allows you to educate the teams individually and to get insights about their main challenges. If you additionally measure the actual adoption and usage, you can further uncover unexpected challenges and resistances. With this information, you can then improve your system and your documentation step-by-step, so that the adoption process in the teams becomes easier over time.
With increasing diffusion of Kubernetes in your organization, cost will become a more important factor because now, every engineer is permanently using a cloud-based Kubernetes environment. Since you are using a shared environment, the usage of the cloud resources should be generally relatively efficient. Still there are some areas of improvement that can reduce the cloud cost significantly.
To find these areas and to generally get a better understanding of your cost structure, e.g. which team causes which cost, you should monitor the cost. For this, tools such as Kubecost or Replex can be very helpful.
You should also start with cost control measures very early on: The most important measure is to limit engineers in their computing resource usage. This will prevent excessive cost from mistakes by engineers. You should also activate horizontal auto-scaling for your cluster as this will adapt your computing power to the current needs.
It is also a requirement for the implementation of a “ sleep-mode “, an automatic system that shuts down unused namespaces or vClusters and so prevents idle resources. The savings from such a sleep-mode in combination with horizontal auto-scaling can be enormous (around 70%) if you imagine that engineers are not needing the computing resources at night, on week-ends, or during meetings but you still pay for them if they keep running.
If you have successfully adopted Kubernetes and you now want to take the next step, providing further engineers in your organization with a Kubernetes access is inevitable. The best solution for this is an internal Kubernetes platform that allows engineers to create Kubernetes work environments on-demand. Such a platform allows to standardize the use of Kubernetes within your organization, so even non-Kubernetes experts can work with it, while admins keep central control and oversight.
No matter if you decide to build such a platform from scratch or if you simply buy an existing platform solution, the investment is worth it because only with an internal Kubernetes platform and access for every engineer you will get the full Kubernetes benefits, e.g. in form of faster development cycles, improved stability, and ultimately better software.
Photo by Clint Adair on Unsplash
Originally published at https://loft.sh.
Coding tutorials and news.
287 
287 claps
287 
Written by

Coding tutorials and news. The developer homepage gitconnected.com && skilled.dev
Written by

Coding tutorials and news. The developer homepage gitconnected.com && skilled.dev
"
https://infosecwriteups.com/setting-up-a-wireguard-vpn-server-architecture-for-internal-network-access-2adcdc79e8e1?source=search_post---------78,"Utilizing a Cloud Command and Control (C2) server, along with various endpoint configurations, you can easily set up a full WireGuard network that allows direct access to private internal networks, or even routes all traffic through one IP for easy auditing. This blog and project was born out of a penetration testing need, but the concept can be easily applied for home and enterprise use elsewhere.
One of my recent challenges for my job was the creation of a way to allow internal penetration testers to…
"
https://medium.com/@yping88/use-ubuntu-server-20-04-cloud-image-to-create-a-kvm-virtual-machine-with-fixed-network-properties-62ecae025f6c?source=search_post---------79,"Sign in
There are currently no responses for this story.
Be the first to respond.
Yu Ping
Oct 4, 2020·5 min read
In most cases, users can perform their tasks against KVM Virtual Machines (VM) without knowing their network configuration, such as the Media Access Control (MAC) addresses and the network interface names. KVM automatically generates a MAC address and assigns a default network interface name and a dynamic IP address for the guest VM if a user does not provide them when creating the VM.
However, it will sometimes require static or fixed values to configure some network properties before setting up a guest VM, especially when it comes to VM provisioning.
This post is a step-by-step tutorial that will help you to create a KVM VM using Ubuntu server 20.04 cloud image with the following network properties: a static MAC address, a pre-defined network interface name, and a static internal IP address.
Re-synchronize the package index files from their sources and install the newest versions of all packages currently installed on the system:
You can check if the server supports Virtualization Technology (VT) in various methods. In this post, you use a tool, virt-host-validate, to validates that the server is configured in a suitable way to run libvirt hypervisor drivers:
The validation tool will report the following warning message if the server has Intel processors. It is expected because the validation tool does not check Secure Guest on Intel processors:
If the server has Intel processors with only VT-x (vmx) support but no VT-d support, the validation tool will report the following warning message that you can ignore:
You may also see the following warning message from the validation tool for Intel processors:
The solution to this issue is to enable IOMMU in your GRUB boot parameters. You can do this by setting the following in /etc/default/grub:
Then update the GRUB and reboot the server:
Run the following command to install KVM and associate VM management packages:
You can verify if the libvirt daemon is active and enabled:
Run the following command to install cloud image management utilities, cloud-image-utils:
You also need to add your local user to the kvm and libvirt groups:
Log out and log back in to make the new group membership available.
Create a directory for storing downloaded cloud images:
Download Ubuntu Server 20.04 Cloud Image:
Create another directory for your VM instance images:
Create a disk image, vm01.qcow2, with 10 GB virtual size based on the Ubuntu server 20.04 cloud image:
Use an internal Bash function, $RANDOM, to generate a MAC address and write it to an environment variable, MAC_ADDR. For KVM VMs it is required that the first 3 pairs in the MAC address be the sequence 52:54:00:
Define the ethernet interface name and the internal IP address to be used in the KVM VM:
Create a network configuration file, network-config:
Create user-data:
Create meta-data:
Create a disk image, vm01-seed.qcow2, to attach with the network and cloud-init configuration:
Create and start a new guest VM with two disks attached, vm01.qcow2 and vm01-seed.qcow2:
Check if the guest VM, vm01, is running:
Type the following command from the KVM host to login to the guest VM console:
Run the following command from the guest VM to verify the network interface name, IP address and MAC address:
Type control + shift + ] to exit the guest VM console.
If everything is in order, you can connect to the guest VM using ssh from the KVM host:
Run the following commands to remove the guest KVM VM:
The network configuration file, network-config, is parsed, written, and applied to the guest VM as a netplan file. The netplan file is very selective about indentation, spacing, and no tabs. See the following link for additional help:
The guest VM will get an IP address in the 192.168.122.0/24 address space in the default KVM network configuration. NAT is performed on traffic through a private bridge to the outside network. The guest VMs, however, will not be visible to other machines on the network. You can set up KVM to use a public bridge to make guest VMs appear as normal hosts to the rest of the network.
Thank you for reading!
Infrastructure Engineer at a Startup | Certified Kubernetes Administrator (CKA)
310 
310 
310 
Infrastructure Engineer at a Startup | Certified Kubernetes Administrator (CKA)
"
https://medium.com/data-science-at-microsoft/how-we-used-ml-and-heuristic-data-labeling-to-help-customers-with-their-cloud-migration-d3af7ff020fc?source=search_post---------80,"There are currently no responses for this story.
Be the first to respond.
As a data scientist on the Microsoft Cloud Data Sciences team, I work closely with internal stakeholders to find ways to leverage Machine Learning (ML) techniques to detect and uncover patterns from data that help our business make better decisions on behalf of our customers. Because our work is often judged by the satisfaction of our stakeholders, it’s important that we listen carefully to their requirements and feedback so that we can incorporate it into our work. In this way, we make stakeholders our main focus and closely involve them in decision-making as we develop our models.
Part of my day-to-day work involves listening to stakeholders describe their problems and challenges, which I then translate into requirements. I then frame the requirements as ML problems and decide the tasks I should be working on with respect to their potential impact and feasibility. Inherent in this approach is balancing what’s most important to do right away and what can be put in the backlog.
In this article, I pick a recent project to demonstrate how I’ve taken a real business problem, worked to understand its impact and how to measure it, and finally converted it into an ML problem while mitigating various challenges that come up along the way.
Multiple industry sources suggest that cloud migration is going to be a significant factor on the horizon. According to Gartner, by 2024, “more than 45 percent of IT spending on system infrastructure, infrastructure software, application software, and business process outsourcing is expected to shift from traditional solutions to the cloud.” According to Ed Anderson, Distinguished VP Analyst at Gartner, “[t]he proportion of IT spending that is being allocated to cloud will accelerate even further in the aftermath of the COVID-19 crisis, as companies look to improve operational efficiencies.” Gartner estimates that spending on cloud system infrastructure services is expected to grow from $44 billion in 2019 to $81 billion by 2022.
To help customers make this digital transformation, Microsoft has created a flexible, cost-effective, and high-confidence program to simplify and accelerate the cloud migration journey. In this context, migration is when a customer moves an existing workload — such as an application, service, or capability — from being done on premises or from another cloud solution to Azure. Here are some examples that demonstrate how we distinguish migrated versus cloud-native workloads:
A customer can choose to migrate using our Azure Migrate tools, third-party tools, or manually by using DevOps pipelines and ARM templates. While it’s straightforward to understand the number of virtual machines migrated using our own first-party tools, it is considerably more challenging to understand the number of virtual machines created as a result of migration (instead of being originated in the cloud for a new workload). Given that most VMs are not explicitly associated with migration, it’s not always clear which ones have been deployed as result of migration activity.
We believe it’s very important to help customers accomplish their cloud migration goals. As a result, it’s necessary for us to understand the migration execution funnel and the customers present at each stage of the migration process. To help, we developed an ML model to improve visibility, actionability, and efficiency in the customer cloud migration process. This model allows us to:
The goal of the model is to detect whether a specific VM in Azure has been created due to migration or is cloud native. After some deep analysis on the existing data, however, I realized that the model is not actually the biggest challenge here, but instead, getting the data labeled so it can be used in the model.
Supervised ML involves training the machine using data that is well labeled. In this approach, the dataset is used as the basis for predicting the classification of other unlabeled data through the use of ML algorithms. In contrast, unsupervised learning is where we have only input data (x) and no corresponding output variables. The goal of unsupervised learning is to model the underlying structure or distribution of the data to help us learn more about it. This is called unsupervised learning because, unlike supervised learning, there are no correct answers. Algorithms are left to their own devices to discover and present the interesting structure in the data.
In our case, because we expected to have the right answer for each example (migrated VM versus cloud-native VM), we knew that we wanted to go with supervised learning. We found, however, that there was not much in the way of existing labels for migration in the data — only two to three percent of migrated VMs were labeled in the telemetry. As a result, it was necessary to invest significant effort in figuring out heuristics to derive labels.
To start, our domain experts shared a number of assumptions that they believed we could make. For example, anytime someone creates a VM using an older version of an operating system, that choice is probably driven by the system requirements of an existing application, so the VM is likely related to migration. That’s not always the case, of course, but these assumptions were good enough for us to create our initial labeled data.
Another way we collected labeled data involved aligning the incentive of the stakeholders to the data labeling task. If stakeholders know how the model can help them unblock an existing customer, they are eager to help with the labeling effort.
We also relied on the knowledge of our domain experts about the approximate ratio of migrated VMs versus cloud native VMs for our labeling work, building on their insights to create the same distribution in our initial labeled dataset. Figure 2 demonstrates the overall process of collecting labeled data and how we improved the model as a result.
The first step of this process is to apply the heuristic rules to the existing dataset. Next, we train an ML model and send the output to account managers and different stakeholders for validation. Then, we fix the labeled data, add new heuristic rules, and retrain the model. Although the process illustrated above looks simple and straightforward, we spent more than 50 percent of our time just labeling the data.
We knew that with the heuristic labeling approach we could cover only small portions of actual migrated VMs, but it was still a good start for the project. One key recommendation here is to communicate these sorts of challenges to stakeholders at the beginning of the project and align expectations on model accuracy. Given that there was no other way to measure migration progress, our stakeholders agreed to start with this model and work with us to improve it over time.
After we created an initial dataset for training, we developed an ML model that combines a set of features typical of VMs. The model considers the likelihood a VM was migrated from being on premises versus being cloud native.
In building the model, we tried various approaches such as decision tree, logistic regression, random forest, and gradient boosting trees, among others. We realized the best performance on our test dataset from the LightGBM model, and Figure 3 shows its results. The area under the curve for the ROC curve is 0.9 and the area under the curve for the precision-recall curve is 0.91. (A perfect model would have an area of 1.)
With this approach, stakeholders can choose a point on the precision-recall curve (at right in Figure 3) taking into account the tradeoff they must decide on between getting high precision versus getting high recall in ways that best fit their requirements.
This article summarizes a journey from business need through to developing an ML model and challenges encountered along the way. The key takeaway is that the first version of a model doesn’t need to be perfect — it’s OK to make valid assumptions that enable starting with some sort of initial model. In other words, don’t try to build the perfect model from the beginning. It’s OK to take risks and make assumptions, start small, and then scale it up. Iterate quickly and incrementally — fail fast and learn faster.
For this project, I told the business that we would make many assumptions that might have an impact on model accuracy. Stakeholders agreed because they didn’t have tools that would yield better insights into migration. (And although I started with a lot of assumptions, after a couple of months, I was able to collect more than one million confirmed labels.) It’s important to note, however, that you can make valid assumptions only if you align on the right expectations with stakeholders from the outset. Additionally, start with field validation as soon as possible — it’s always better to have the pipes already in place for when you need them later.
I also recommend keeping momentum going and incorporating feedback from real customers all the time. Don’t stop doing this after the model is already in production — feedback is the key to improving models and making them better over time.
The author would like to thank Assaf Berenson who also contributed to this work.
Lessons learned in the practice of data science at…
18 
1
18 claps
18 
1
Lessons learned in the practice of data science at Microsoft.
Written by

Lessons learned in the practice of data science at Microsoft.
"
https://medium.com/infrastructure-adventures/self-renewing-lets-encrypt-wildcard-certificates-in-kubernetes-for-internal-domains-part-i-dns-ea788eb3dbf9?source=search_post---------81,"There are currently no responses for this story.
Be the first to respond.
I had been waiting for long for the wildcard SSL certificate support in Let’s Encrypt, and after some delay, it’s finally available since this spring. At my workplace, we managed to migrate 100% of the production sites and domains (100+) fully to domain-validated LE certificates, using the classical HTTP-based verification. They are renewing every 2 months, without any human intervention, no certificate orders, manual replacement in load balancers, etc.
Still, we came across a new problem: on our bare-metal Kubernetes cluster, the developers are running a bunch of applications and they want to reach some of these microservices on HTTPS, using the built-in K8s service discovery internal domain name, and not by creating a load balancer or any nice URL.
You can use self-signed certificates, but then every automated script will complain, your browser will complain or you might even have a company browser policy deployed which prevents you to open any non-secure site. Same with running an internal, self-hosted Docker Registry: you need a valid, real SSL certificate if you don’t want hundreds of your colleagues to set an --insecure-registry flag in their local Docker client by hand.
In order to get a wildcard certificate, you must verify your domain ownership by creating a DNS TXT record. Here you have 2 problems:
Obviously, this is impossible by hand, so you must use some kind of a dynamically updated DNS API. Unfortunately, our DNS provider doesn’t support this, so we had to come up with an alternative idea:
Let’s deploy our own DNS API with nearly minimum effort, used for Let’s Encrypt validations.
I’m going to use the following components:
Because we want to have valid, real certificates for internal domains, we are going to set up a fake DNS server in the public company domain, serving a completely empty sub-zone file on the Internet. We will create here the necessary TXT records to validate the internal domains, then delete it a few minutes later, therefore continuing to hide the internal domain structure from the public.
Let’s suppose, we have a production, company domain called megye.si. We use *.intranet.megye.si for internal services and we have a Kubernetes cluster installed under cluster-01.intranet.megye.si. Those familiar with K8s know that the internal service discovery works the following way:
For example, if I have a frontend-main service in the QA namespace, it will be reachable on the internal network through the frontend-main.qa.svc.cluster-01.intranet.megye.si DNS record.
So I would like to ensure that all QA applications are reachable on HTTPS, without provisioning a load balancer for them with a real domain name. I am going to need a wildcard certificate of *.qa.svc.cluster-01.intranet.megye.si.
The whole beauty in this is that people from the Internet will only see that I have an NS record, a custom DNS server pointing to intranet.megye.si, and it doesn’t serve any records and doesn’t reply for any queries.
I want to be able to provision any number of wildcard certificates for any number of different internal namespaces, with zero effort and manual work, all hidden from the Internet.
So, let’s get to work! For easier reproducibility, and also because I wanted to play with Google Cloud, I am going to provision the entire stack on GCP. (You can get a $300 free trial which is more than enough to play.)
The original solution was done in an on-premises cluster, with Gitlab CI pipelines. The GCP solution will be a little bit less powerful, mostly due to the very basic capabilities of the Google Container Builder (the CI tool of Google).
I am going to install a PowerDNS server with the built-in DNS API enabled. I usually prefer to use the BIND backend because I like that file format for its convenience a lot, however the dynamic API updates are not supported with it. Therefore, you must use one of the SQL backends: MySQL, Postgres or SQLite. To decrease complexity, I chose SQLite, because anyway we only want to store the temporary TXT records for a few minutes only, then we will keep an empty zone file.
I start with provisioning the source code git repository for the Docker image of the DNS server:
To be able to use the API, the key components will be the following:
I mentioned that we’re not going to use the BIND backend. Yet, we can still write the zone definition in it and then convert it directly to SQLite format by internal PDNS tools!
I chose a very short DNS TTL, to avoid caching issues. About the DNS record configurations itself, I will provide more details at the end of this article.
I will convert the zone definitions at the time of creating the Docker image. Therefore in my Dockerfile the relevant part will be:
I have downloaded the empty SQL table schemas from the PowerDNS docs (schema.sql) and after importing them, I convert the BIND zone files into SQLite format. We’re ready to roll!
My next step will be to write an automated build pipeline to create a new Docker image each time I push a new commit. I’m using the Google Container Registry (gcr.io) to store my Docker images; the build is done by the Google Container Builder tool. It has 2 options to create a new image with a pipeline: use the defaults and just build the Dockerfile in the repository, or for more complex use-cases, you can specify a custom cloudbuild.yaml file:
I chose the second option, because by default you only push the git commit hash in the image tag and I prefer always 2 tags created for each Docker build:
For example, I can setup my deployment manifests in Kubernetes, to always start the latest master branch image. Imagine someone asks you the following question:
— Hey, which version is running on prod?
— The master branch.
— Ooookay, but which master branch? The one from last week or from today?
For this, I will do the automated deployments based on the commit hash, but still keeping always the latest version named after the git branch.
This is the the one and only time when we need to touch the Google Cloud Console in the browser, because unfortunately it’s not possible yet to configure the build triggers from CLI at the moment. (It’s a beta product.)
I’m storing the git repositories at Google, but it’s possible to use GitHub or Bitbucket remote repositories mirrored here as well.
I like to use labels by the Open Containers standards, to indicate inheritance between images, build time, maintainer and similar metadata. I was surprised to see that the Google Container Builder doesn’t support the --label flags in the Docker build CLI, only if it’s defined inside the Dockerfile:
So after the pipeline is done, time to start the DNS server and configure my production domain to delegate the intranet addresses to this new service.
Just to see some extra fun, I am going to create a new Kubernetes cluster in GCP. Google just released the regional type of their managed K8s service, the Google Kubernetes Engine, it’s generally available now. (Means you get an SLA on it.)
Previously, you could only choose zone-based clusters. So let’s say, you deploy a GKE cluster to europe-west3-a only. It starts 1 API server in the background. When you do a rolling upgrade of the cluster, the API is killed and you cannot read or write in it anything until the new version is started. The existing traffic is uninterrupted, but you cannot start new pods.
Now with the regional availability, you get 3 API servers running in parallel! You can do a rolling upgrade of the Kubernetes masters, the cluster stays fully operational and the upgrade process is completely transparent.
I’m going to provision the cluster using preemptible cheap nodes, with the latest 1.10 version of Kubernetes. Even though I’m specificing --num-nodes ""1"", in reality I’m going to have 3 nodes running, because it starts 1 in each availability zone.
After the cluster is bootstrapped, it’s ready to accept new services in it. For example, a wonderful DNS API server! Let’s provision it in a new namespace, dedicated for infrastructure components:
If you remember the PDNS config, we’re going to need an API key to access the PowerDNS web service. We could hardcode this password in the Docker image, but on one hand, it’s not secure and also, we’re going to need this key on the client side as well. So let’s make it fully dynamic from day 1!
I’m using here the --dry-run --output yaml flags together. This creates the YAML version of the secret definition and I can save it in my git repository. In this way, it will be very easy to reproduce it in a different namespace / another cluster and also, I have a backup!
There are 2 manifests left: the service definitions and the deployment itself!
You can see an interesting difference between the 2 service definitions. We create
I have found here 2 interesting things with how GCP works: when you expose a service as a LoadBalancer type of service, it allocates you an external IP address and exposes your service as a NodePort on all of your running cluster nodes. It’s interesting that they don’t use BGP or any internal routing instead of NAT.
The other one is that on GCP you cannot put a UDP and TCP port together in the same service definition, if it’s publicly exposed. In bare-metal clusters you can easily do this.
I highlighted the most important parts:
We do the deployments with kubectl apply, instead of kubectl create or replace, including the very first occasion. If you always use apply, you get automated version control for the applied YAML manifests and you can do easy rollbacks to any version in the history.
We can see the DNS server is up and running. I can reach the internal API using the powerdns-letsencrypt-api.infra DNS name, and the DNS service itself on a public IP addresses.
The only remaining part is to tell my DNS provider to delegate all of the intranet.megye.si subdomains to this new service. I use personally the afraid.org service because they offer the most options at the cheapest price. The GUI is nothing fancy, but I rarely need to touch it.
I am going to define two new records in my zone file:
Let’s verify the DNS settings:
And we’re done! In the next part I’m going to explain how to provision the wildcard certificates. First we will create an auto deploy pipeline for the DNS server image, because I don’t want to deploy the new Docker image by hand each time I push a new commit.
Then we will build the Let’s Encrypt client and configure it to automatically provision the wildcard certificates, save them to a git repository and then a CI pipeline will deploy them to the cluster as a Secret object, ready to be consumed by the HTTPS-aware applications.
Chapter II available: https://medium.com/infrastructure-adventures/self-renewing-lets-encrypt-wildcard-certificates-in-k8s-for-internal-domains-part-ii-certs-66c613a2279f
Hope you enjoyed this tutorial! Let me know in the comments below if something is not clear enough or you have a better approach for some parts!
You can find the source code here: https://github.com/dmegyesi/letsencrypt-wildcard-dnsapi
Interesting challenges I faced during my IT…
80 
3
Some rights reserved

80 claps
80 
3
Interesting challenges I faced during my IT operations/DevOps journey and some other related things
Written by
Tech enthusiast, systems engineer
Interesting challenges I faced during my IT operations/DevOps journey and some other related things
"
https://medium.com/infrastructure-adventures/self-renewing-lets-encrypt-wildcard-certificates-in-k8s-for-internal-domains-part-ii-certs-66c613a2279f?source=search_post---------82,"There are currently no responses for this story.
Be the first to respond.
Let’s pick up the thread where we left off; here’s what we have done in the previous article:
So today we only have a few steps left to finish the self-renewing SSL certificate infrastructure:
This is now a fairly easy part, so let’s get to it!
In the previous article, we have created the PowerDNS Docker image and configured the Google Cloud builder to create this image and push to the registry after each commit. What’s still missing, is to actually deploy it to the Kubernetes cluster, with the latest version always.
In order to achieve this, we only need to add a few extra lines to the cloudbuild.yaml file in the git repository:
So what exactly are we doing here?
The rolling update will make sure that if the new build fails, it automatically keeps the previous, working copy online. Nothing to be configured by us to achieve this.
You can notice that we need to supply some environment variables to kubectl to find the GKE cluster’s address (compute region and cluster name).
If you haven’t done it yet, you are going to have to give permissions to the cloud builder’s built-in service account (<your project’s numeric ID>@cloudbuild.gserviceaccount.com) to connect to the Google Kubernetes Engine service, by adding the Kubernetes Engine Developer permissions. You can do this either in the Google Cloud Dashboard in the IAM menu or use the command line tool:
Let’s see it rolling! The Cloud Builder output is the following after I push my new commit:
We’re all set! Now each time you push a change to your PowerDNS repository, it will create a new build and deploy it to Kubernetes.
Here comes the most important part, the actual code which generates and creates the SSL certificates.
We create a Docker image which the following things:
It’s a fairly trivial Dockerfile, you can find the full source code at my GitHub repo.
If you noticed, we introduced a new component: an additional git repository to store the certificate files. Our Docker container will have to be able to:
The easiest solution for this is just to use the official Google Cloud SDK image, so here’s how my Dockerfile starts:
This will by default include all the gcloud commands, it can handle service account based authentication to our GCP git repositories, etc., no need to write any of this magic by ourselves.
How do we clone a git repo in Google Cloud?
We’re going to need to provision 2 things here. The new git repo:
And the service account which we include in the Docker image:
2. Give it write permissions to the repo
Create an IAM policy binding file (iam-dehydrated-git-push.yaml):
And then apply this policy to the service account:
Download a JSON access key to the account, so you can include it in the Docker image:
This part will be just a few lines of simple Bash code.
Apart from some standard boilerplate, the logic is fairly simple:
Let’s see them in details!
Then actually issue the certificates, calling the client:
And save to git:
If you check the previous section, you can see we just simply call the Dehydrated script to issue the certificates, there are no fancy parameters supplied.
We can do this because we already included the 2 necessary files in thedehydrated-data repo (where we actually store the certs):
1. <repo root>/config:
2. <repo root>/domains.txt
This file’s format is very important! If you check the user manual, basically we just said:
In case you want to provision single domain names, not wildcard certificates, you would do just:
and it would create the certs under the same file name.
When we put *. in front, we’re including this asterisk in the file name as well, and obviously this breaks the shell scripts, that’s why we use the file name alias described above (the > syntax).
…and we’re done! Ready to issue certificates. Let’s try it!
We need to create and push the client’s Docker image to the registry. It’s the exact same steps as previously (cloudbuild.yaml), so I don’t want to waste lines on it; as usual, you can find the code in GitHub.
You have now 2 options to issue/renew wildcard certificates with Let’s Encrypt:
Obviously, we’re going to do the second part, let’s go for 100% automation!
And by Crontab, I mean: Kubernetes Crontab :)
Before we create our final setup, let’s see that this actually works!
In Kubernetes, you can create so called “one shot jobs”, which you only need and want to execute 1 time only, and then never again. (If you create a regular pod or deployment, it will keep restarting after the container stops. We don’t want and need this.)
To achieve this, we define a Job object in Kubernetes:
We specified the freshly baked Docker image and, if you remember from the Dehydrated configuration’s last line, we still need to supply the PowerDNS API key. We have created it in a secret in the previous article, so now let’s use it! We pass the DNS API key as an environment variable to the container.
Amazing, it works! Let’s verify the git repository: you should have the issued certificates stored and saved there. The ACME client of Let’s Encrypt uses versioned certificates, so the actual cert.pem and similar files are symlinks to the latest versioned file (cert-1527951445.pem).
Let’s put the icing on the cake and forget about issuing certificates by hand ever again in our life. Let’s convert the previous Job object to a CronJob object.
To give you a simple analogy:
a Job object is a one-shot run of an application — just like a Pod!
a CronJob object is multiple runs of the same application, with failure detection and auto-restarts — just like a Deployment for Pods!
Deployment = a template of Pods. CronJob = a template of Jobs.
You can find the entire file in GitHub.
So what did we do here? We created a scheduled task in Kubernetes, which runs 1x every week.
We actually need to run it 1x every 3 months, but if I put a new wildcard domain in my domains.txt, I want it to be live as soon as I can!
Let’s Encrypt will check the certificate expiration time every week when you run the client. If it expires in more than 30 days, it won’t do anything. If it’s close to expiration, it will generate you a new certificate with an additional 90 days of extension period.
You can notice two good-to-know things in the definition, it caused me quite a lot of headache previously:
Now we have the wildcard certificates created and stored in git. How do we deploy it to Kubernetes?
It will be the exact same way we did with the auto-deploy of the DNS image: add a CI pipeline to the dehydrated-data repo, so when there’s a new commit (= we issued new certs), create a Kubernetes Secret object from them and send it to the cluster.
With kubectl, you can generate Secrets from files, using the following syntax: kubectl create secret generic $SECRET_NAME --from-file=$FILE_NAME_IN_SECRET=$FILE_LOCATION_ON_DISK
The now-familiar cloudbuild.yaml will have the following:
(Of course you need the Cloud Builder service account to have Kubernetes Engine Developer permissions. If you already configured it for the PowerDNS image, there’s nothing to do.)
…and we’re all set! Now if you have multiple domains, of course you have to set up some kind of a loop. Either a simple for loop for iterating on your domains.txt’s file names or multiple steps manually defined in the YAML file, up to your obsession with automation.
Phew, it’s been a long journey! Thank you for taking your time and patience to go through these articles. By following these steps, you have now:
Hope you enjoyed this tutorial! Let me know in the comments below if something is not clear enough or you have a better approach for some parts!
You can find the source code here: https://github.com/dmegyesi/letsencrypt-wildcard-dnsapi
Interesting challenges I faced during my IT…
25 
2
Some rights reserved

25 claps
25 
2
Interesting challenges I faced during my IT operations/DevOps journey and some other related things
Written by
Tech enthusiast, systems engineer
Interesting challenges I faced during my IT operations/DevOps journey and some other related things
"
https://betterprogramming.pub/automate-jira-cloud-workflow-with-golang-2de29828aad6?source=search_post---------83,"Sign in
Percy Bolmér
Aug 11, 2021·5 min read
JIRA is a very popular framework for tracking issues and project management. It allows project leads or scrum masters to set up projects and creates issues that are then assigned to developers. It’s a great framework for…
"
https://medium.com/@saxenasanket135/cog-overview-and-how-to-create-and-validate-a-cloud-optimised-geotiff-b39e671ff013?source=search_post---------84,"Sign in
There are currently no responses for this story.
Be the first to respond.
Sanket Saxena
Jul 13, 2020·3 min read
Cloud optimized GeoTIFF is a regular GeoTIFF file with an internal organization like tiling and overviews which can be leveraged by HTTP range request. This allows us to access only a portion of data in the raster file which we are interested in, rather than downloading the entire file itself.
COGs enable efficient streaming of data, opening up possibilities of fully cloud-based geospatial workflows that address the potential issue of data duplication. Since COG offers on-the-fly processing of raster data, cloud workflows can access single files online instead of needing to copy and cache the data locally.
COG being the same data format as GeoTiff, has legacy support which means all kinds of legacy GIS software which can read Geotiff, can also read the COG. So data providers need only produce one format.
For more information on the COG data format, please refer to cogeo.org.
GDAL is a go-to library for reading raster files. Using GDAL CLI we can convert GeoTIFF into COG.
It’s a two-step process:
1. Create Overviews
GDAL has a CLI command to create overviews called gdaladdo. The command must be run before the gdal_translate or the resulting GeoTIFF won’t be COG compliant.
This results in creating 4 different overviews at zoom level 2, 4, 8, 16
2. Create tiles and compress GeoTiff
This will result in images with tiles of dimension 256x256 pixel for main resolution, and 128x128 tiles for overviews.
Now, we have created an internal organization that a COG should have, we need to validate if the generated COG is valid or not.
Generated COG can be verified using an opensource script as below
Link for validate_cloud_optimized_geotiff.py.
Sample Valid COG result:
Sample Invalid COG result:
Once we get a valid COG result, we can put it up on a server for allowing its online access.
Rio-Cogeo CLI is also a wonderful tool to create and validate COGs.
Thanks for reading :)
98 
1
98 
98 
1
"
https://medium.com/@hanusiak-tomasz/using-iterm2-to-work-with-openshift-clusters-to-manage-cloud-pak-for-data-b2f3224fbca8?source=search_post---------85,"Sign in
There are currently no responses for this story.
Be the first to respond.
Tomasz Hanusiak
Feb 6, 2020·3 min read
As a team working with hundreds of Cloud Pak for Data clusters running on OpenShift each month (both internal and external) we had to come up with an easy, yet reliable method to simplify and increase the speed of cluster management & troubleshooting.
While there are several tools that can help you do that, they usually require deploying applications/plugins onto the OpenShift cluster or modifying the underlying operating systems (for example changing the shell or adding extensions to the existing ones) — neither was really a good option to us.
We then found a great Terminal replacement for macOS — iTerm2, which allowed us to create a simple, yet powerful solution.
iTerm2 allowed us to do a couple of interesting things very easily:
Let me now share a couple of screenshots, and explain how you can configure iTerm2 a similar way.
Small subset of our Profiles, with corresponding tags — please note that the profiles are dynamic — each change to the configuration file will refresh the list.
Example of a Smart Selection Rule and corresponding action. Whenever the regex is matched, the actions will show up in the context menu.
Subset of actions available on selecting a line in iTerm.
Using the Smart Selectors, both Utils and Pod actions are available. No need to type the commands and copy and past the pod & namespace names.
You can find a simple template with some most common actions we use here: https://github.com/Hanneck/iterm.
Obviously there is a lot more you can do in iTerm — key bindings, adding custom actions to MacBook’s Touch Bar, and much, much more… Plus it’s open sourced, so you could potentially change/add any feature you’d like.
Please have a look at the list of all the features — https://iterm2.com/features.html & https://github.com/gnachman/iTerm2
9 
9 
9 
"
https://medium.com/cloud-native-the-gathering/utilizing-amazon-dynamodb-to-audit-user-activity-for-internal-tools-28036eaee885?source=search_post---------86,"There are currently no responses for this story.
Be the first to respond.
The use cases for Amazon DynamoDB are plentiful — it’s tough to argue against what Amazon Web Services (AWS) describes as “a nonrelational database that delivers reliable performance at any scale.”
With any company’s internal tools, an immediate use case would be to leverage DynamoDB to have one or more…
"
https://cloudnative.ly/cloud-foundry-schematic-6aa0ad13424c?source=search_post---------87,"Cloud Foundry (CF) is our preferred multi-cloud application PaaS solution, helping us to abstract away the infrastructure and focus on application innovation instead.
CF comes with a command line interface (CLI) to help with the deployment of applications.
We’ll talk about this in a future blog post, so watch this space. 🙂
Today’s blog post is focused on helping you get an insight of the Cloud Foundry internals.
As mentioned above, CF allows you to push applications via CLI or via your platform provided GUI. Whether the request is made via the web, CLI or through an application it will be handled by the Gorouter, the HTTP server responsible for all incoming traffic. The way this works is by maintaining a dynamic routing table via a routing API. This table gets constructed from information it receives from NATS, the message bus, and also from the Diego BBS, which the Gorouter will query every now and then to identify which cells and containers the applications are running on. The Gorouter then will make its decisions based on HTTP HOST headers.
Consul, the service discovery service, acts like a DNS service so CF components can find each other using hard-coded internal hostnames.
User Account and Authentication, acts as the OAuth2 Authorization Server that issues access tokens for applications that request platform resources and as the login point to CF. Various requests (login credentials, OAuth tokens, etc.) are authenticated against the UAA.
The Gorouter forwards incoming requests to the Cloud Controller via the Cloud Controller (CC) API, which then allows you to deploy your applications and manage your system.
Is the container scheduling subsystem of CF. It schedules tasks and Long-Running Processes (LRPs); these are run within an immutable container (once it gets created it won’t get changed). The API handles communication between Diego and the Cloud Controller and the Cells run the application containers.
Task = application guaranteed to run at most once (e.g.: staging an application)
LRP = web application that can have multiple instances
Auctioneer = holds auctions for tasks and LRPs
Converger = reconciles desired LRPs vs actual LRPs through auctions
An auction = is held to bid on executing a task or a LRP
CF has a pool of cells, many VMs are of type cell and your application instances get distributed across those.
Is the logging subsystem consisting of multiple VMs such as Doppler, Adapter, Log-API and the Scheduler. Various parts of this system handle collecting logs from the applications, sending the logs off to user-configured remote logging services and responding to cf logs.
The component responsible for routing traffic to the appropriate component:
To recap, CF (Cloud Foundry) is a multi cloud application PaaS solution for building, deploying, running and scaling applications, moving the focus away from infrastructure and towards application innovation.
It’s main subsystems are the Gorouter, UAA, Cloud Controller (CC), the Diego cell and the Loggregator. The Gorouter deals with traffic. Authentication happens at UAA level. The CC deals with configuration requests. The CC translated messages get forwarded to the Diego scheduler, where the BBS Brain will send tasks or LRPs to the Rep Executor component that will deal with managing container allocation on the cells. The cells, our specified OS, hold the Garden component, which manages all the containers that eventually hold the applications. The application logs are sent to the Loggregator via Metron component. The CF storage components for the above are:
We believe that a best practice for continuous delivery is having a BOSH CF cloud PaaS solution on top of a cloud IaaS, like AWS or Azure or GCP, using Concourse as our CI/CD tool. Automating your infrastructure through IaC (Infrastructure as Code), makes your environment reproducible, allowing you to have Blue/Green deployments (aka zero downtime on upgrades), to scale out and up your applications easily, to have logging and monitoring, and recover quickly, among others.
More on how to obtain this, to follow. Keep an 👀 on this space! Happy clouding, everyone! 🌤
Armakuni Blog. Thought leadership for Cloud Natives
21 
21 claps
21 
Written by
Back End Engineer keen on learning.
Armakuni Blog. Thought leadership for Cloud Natives
Written by
Back End Engineer keen on learning.
Armakuni Blog. Thought leadership for Cloud Natives
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@ozcosta/google-cloud-networking-ilb-as-next-hop-with-tags-ab5f30a0e0c3?source=search_post---------88,"Sign in
There are currently no responses for this story.
Be the first to respond.
Osvaldo [Oz] Costa
Nov 16, 2021·4 min read
The internal TCP/UDP load balancer as next-hop feature, which allows integrating virtualized network appliances in a highly available, scale-out manner, was enhanced (currently available in Public Preview) to introduce the ability to route your traffic selectively by installing routes in the clients based on configured network tags, thus allowing regional affinity for the traffic targeting the virtualized network appliances. In other words, you can configure your global VPC to keep traffic from clients to the virtualized network appliances in the same region, providing more cost control by avoiding inter-region traffic.
These new features also allow more granular ILB as Next-Hop routing by using network tags as criteria e.g. traffic from different applications/services can be forwarded to specific virtualized network appliances. This is supported not only in the VPC directly connected to the virtualized network appliance, but also from a peered VPC.
As a result, if we consider the setup below for North-South outbound inspection:
In the example above, the network tag ilbanh-region-a was added to the Client A instance. We can now add new routes to the internal VPC — the lower the value, the higher the priority. In this example, only Region A routes are shown but similar routes can be created for Region B. Note that next-hop-ilb is required, and the ILB’s virtual IP is being used instead of the forwarding-rule name — this allows the route to be created even in a peered VPC. The NVAs are configured to SNAT all traffic towards the internet.
We can now list all ILB as Next-Hop routes:
The expected output should be similar to the screenshot below:
Connect to the Client A VM and run traffic targeting a public destination. One example to quickly identify which public IP is being used is curl ifconfig.io.
Once the primary route (ilbanh-region-a-primary) is deleted, after a few seconds the traffic should switch to the ILB front-ending the NVAs in Region B (see the command below as an example to monitor the change).
If there is a need to automatically remove the route if all backends are unhealthy, a suggestion to automate this process is using Cloud Functions.
Learn more by trying out the ILB as Next-Hop with Tags Codelab.
Network Specialist Customer Engineer @googlecloud, ex-@paloaltonetworks, @ericsson, @redback. Long time network & security professional. All views my own.
8 
8 claps
8 
Network Specialist Customer Engineer @googlecloud, ex-@paloaltonetworks, @ericsson, @redback. Long time network & security professional. All views my own.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/hackernoon/starting-small-with-kubernetes-and-kubeadm-500-containers-6c76454e6e12?source=search_post---------89,"There are currently no responses for this story.
Be the first to respond.
At Vurt we are performing some internal experiments with Kubernetes. In this test and post, we are specifically discussing kubeadm as the installer and also running k8s nodes on baremetal servers. Oh and ZFS too.
We believe Kubeadm is an important project. Vurt’s team has considerable experience operating OpenStack clouds, and we see some stark differences in how the k8s community has approached operational decisions versus what the OpenStack community has historically done.
This is not to say that one or the other is correct, rather that there are different choices that can be made.
OpenStack made a clear choice not to back one particular deployment mechanism. So there are many, and new ones all the time. For example OpenStack Helm is a recently created project that will deploy an OpenStack cloud’s control plane using Helm and Kubernetes. There are probably five or six major installation and management systems for OpenStack.
To make a long story short, this is option #1: No default deployment system. Instead we get many “installers”, and, in turn, a somewhat fractured view of how to deploy OpenStack. (Let’s not even get into Day 2 operations.) But this post isn’t about OpenStack, it’s about k8s and kubeadm.
Kubernetes, on the other hand, has decided to implement a default deployment mechanism: kubeadm. Certainly there are many other “distros” and “installers” but kubeadm is a real thing.
Over at SIG-cluster-lifecycle, we’ve been hard at work the last few months on kubeadm, a tool that makes Kubernetes dramatically easier to install. We’ve heard from users that installing Kubernetes is harder than it should be, and we want folks to be focused on writing great distributed apps not wrangling with infrastructure! — k8s blog
(An important part of that blog post is the part where they mention that kubeadm will not provision servers, ie. it will deploy k8s to some existing Linux servers. So the team has decided to limit the scope ever so slightly.)
Basically, the k8s community has decided that there will be a default installer that comes as part of the k8s core code. This is an interesting, and potentially powerful, decision.
For one of our tests, we decided to perform a quick deployment of k8s using kubeadm on top of four servers: One virtual machine, and three baremetal nodes. The VM and networking is managed by OpenStack, currently the physical nodes are deployed automatically outside of OpenStack. The physical servers have to 1GB NICs configured in a bond.
As far as computing resources, the baremetal nodes have 128GB of memory and 32 CPUs. The VM will act as the k8s master and it only has 4GB of memory and 2 CPUS. Our goal was to deploy 500 containers.
It’s important to note that the this test wasn’t really about just getting 500 containers up, it was more along the lines of exploring what kubeadm does, and what we can do with its default settings, as opposed to “pushing the limits” of the hardware and software. We did not expect to be able to run 500 containers on three nodes without making some configuration changes, but we wanted to try!
The baremetal nodes also have a single SSD (for cache) and a SATA drive configured with ZFS. Further, Docker is configured on the nodes and has ZFS as the image backing engine. We would like to use ZFS if possible as our default storage system, and thus are evaluating the use of ZFS on Linux (ZoL) for use with Docker/Kubernetes.
Due to the use of ZFS, we had to run kube init with an option to skip the pre-flight checks. Here’s the warning notice.
Also, in this test we are using the (somewhat) default Weave networking plugin. The installation of Weave is fascinatingly easy!
The kube-master node and networking were configured first, and then the baremetal nodes initilizaed after.
Once k8s was up and running (thanks kubeadm!) we used an example nginx example deployment and started with a couple pods. Then we scaled up, first 50 at a time, then 100…finally reaching 500.
These scaling actions went perfectly well until we tried to go from 250 to 500. The wall we ran into was that by default the number of pods per node is limited to 110.
Looking at the kublet docs there is an option to set the pod limit.
Overall we were very happy with this particular test. In an hour or so we deployed a k8s cluster onto four nodes, deployed a software defined networking system (Weave), and created 250 containers with almost zero effort. In future tests we will alter the pod per node limit and see what happens! Certainly the baremetal nodes can run more, especially if those containers aren’t really doing anything.
We are currently looking for a small number of beta customers to join us on our journey into advanced Kubernetes deployments on hosted, private infrastructure. Please email vurt@vurtcloud.com or use our contact form if that sounds like something your organization would like to be involved with.
Hacker Noon is how hackers start their afternoons. We’re a part of the @AMI family. We are now accepting submissions and happy to discuss advertising & sponsorship opportunities.
If you enjoyed this story, we recommend reading our latest tech stories and trending tech stories. Until next time, don’t take the realities of the world for granted!
#BlackLivesMatter
11 
11 claps
11 
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
Currently in private beta
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
"
https://medium.com/google-cloud/google-app-engine-for-internal-systems-bbf539f2d290?source=search_post---------90,"There are currently no responses for this story.
Be the first to respond.
The other day I was answering question on Quora “How your company is using Google cloud and what for?”. Of course since my business is related to consulting for other companies and it usually involves Google Cloud Platform (GCP), I though to write some concrete cases what can be done with GCP (of course a lot). As I was thinking and then writing, one point especially stood out to me: using Google App Engine for internal systems. There are literally dozen of projects I did which involved Google App Engine (GAE) and some of them have the same characteristics and functionalities:
For such cases I didn’t find better solution than GAE. Why? Because everything is in one place, like Swiss knife for web (backend) development. That means:
Like I wrote, all these properties enable quick development, easy deployment and robust stability. Number one critique for GAE Standard is that it’s vendor locking, i.e. once you write code and use those services, it’s not possible to migrate off Google without significant code rewrite. That’s true, but for use cases I mentioned, I think migrating somewhere else is not so frequently mentioned topic.
GAE was first product on Google Cloud (introduced in 2008) and although these days it’s not so popular since no. 1 theme are Kubernetes, for some cases, like those I listed above it’s without competition and it’s like a hidden jewel.
Google Cloud community articles and blogs
20 
20 claps
20 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
IT Consultant with focus on Google Cloud Platform, creator of GCP Weekly, a weekly newsletter about GCP https://www.gcpweekly.com
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/@cassiniplm/cloud-plm-and-how-cassini-is-leading-the-way-89f6e7972438?source=search_post---------91,"Sign in
CassiniPLM
May 17, 2021·3 min read
Cloud PLM, i.e., Product Lifecycle Management, is an integrated approach that assists manufacturers, supply chain partners and internal teams to develop, design, and produce products during NPI (New Product Introduction) and NPD (New Product Development).
The benefits of this approach include improved productivity, better quality, reduced financial risk, and a decreased need for additional staff to manage the business processes. It helps manage the complete product lifecycle and enables businesses to deliver high-quality products within the defined time and budget. With its help, businesses can focus on what they do best, develop new products, or improve existing ones.
The adoption of Cloud PLM has grown among manufacturers due to the ease and intuitiveness it offers. With cloud PLM and its components, a business can easily monitor its entire product lifecycle and make informed decisions about changes to its strategy and products.
Cloud Product Lifecycle Management is an application that is vital for any company that wants to remain competitive in today’s market. The overall effect on the cost is a positive impact, especially for a business experiencing tight margins. It also provides businesses with greater flexibility because the software can be hosted on any platform.
The Benefits of Bringing PLM to Cloud
Here exist some perks of opting for a cloud PLM:
· Includes zero infrastructure costs.
· Lowers down support costs.
· Triggers fast implementation.
· Offers instant scalability.
· Facilitates remote working.
· Limits overall expenditure.
· Minimizes human errors.
· Enables real-time data across the supply chain.
· Controls and manages product design.
· Helps in overpowering competitive landscape.
· Eradicates duplication of effort and much more.
Cassini offers a Unified Cloud PLM Platform developed to escalate overall product development cycles. For the supply chain industries and other businesses, Cassini simplifies the whole process of product development and manages product lifecycle by integrating product real-time data with business processes and introduces more reasoning and productivity to manufacturing, engineering, and quality aspects.
Cloud PLM by Cassini brings down the costs of IT and helps business operations become more efficient while improving the business ROI. Many industries are opting for these solutions by Cassini due to its features like accelerated new product introduction, well-managed product configurations, streamlined change management, closed-loop product development, effective supplier & vendor collaboration, a digital thread of products and processes, enhanced product quality, cross-functional collaboration, and enhanced customer satisfaction, etc.
The best part of the Cassini Platform is the scalability it offers. As a company grows over time and acquires more employees, it is easy to add more applications to the Cloud. This then allows the company to use applications on-demand without having to purchase additional software or hardware.
Now that you have understood that PLM on the cloud can make all your business operations seamless and super-fast, it is time to invest in a full-fledged cloud PLM. It will not only introduce efficacy but also lets you focus on more important and multi-faceted tasks associated with your business.
Visit our website to know more about CassiniPLM.
CassiniPLM is a Unified Cloud Platform that simplifies and accelerates your product development cycles and manages your entire product lifecycle.
8 
8 claps
8 
CassiniPLM is a Unified Cloud Platform that simplifies and accelerates your product development cycles and manages your entire product lifecycle.
"
https://medium.com/@_oleksii_/how-to-connect-to-aws-rds-running-in-the-internal-network-95dd1ecbefc9?source=search_post---------92,"Sign in
There are currently no responses for this story.
Be the first to respond.
oleksii_y
Oct 4, 2019·1 min read
Often, for security reasons, databases are created only on the internal network.At the same time, the issue of connecting to this database from other networks becomes relevant.
To solve this problem, we need a bastion host that looks at one interface into the internal network we need, and the other at the world.
Then we create an entry of the form in the host's file of the required instance:
where example-db.example.int — CNAME to the endpoint of the RDS
Build the ssh tunnel
where 5432 — open RDS port (in my case Postgres)
PS. For killing SSH session that was started with the -f option (run in the background)
55 

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
55 claps
55 
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@digitaltinker/developer-experience-internal-developers-are-customers-5a626b38b1de?source=search_post---------93,"Sign in
There are currently no responses for this story.
Be the first to respond.
Olivier Schmitt
May 10, 2018·4 min read
As the leader of a technical team dedicated to cloud transformation, I had to find how the cloud could empower developers to reach some key objectives : productivity boost, improvement of SLAs, improvement of TTU (Time to user), and so on.
One important goal was to put some facts on constant disputes between the Devs and the Ops.
In order to do that I had to understand and uncover all aspects of the devs and ops’ life.
UX has many powerful techniques to gather all those little facts and events which make an experience.
The most famous artefact in the field is the UX Map, probably because it is often visually ravishing : humans tend to remember beautiful digital artefacts.
For example, here is an emotional map:
This one is nice too:
The map is a final product of a profound and powerful process, the experience gathering.
This process is dedicated to draw an average view of the life of a targeted profile when it is using a service or a product:
In the end, the map should give insights at least on:
Everybody and everything is becoming digital.
Developers are and will be one the most important human resource to have in the next years.
The shortage of skilled developers is already a blatant fact in some areas, don’t get it wrong : it will worsen as the software continue to eat the world.
This will force HR team to cuddle them because they will have a hard time to keep or attract the best profiles.
HR could offer nice packages but what if the technical environment sucks ?
What if the developer is constantly stucked by other teams or defective services he has to deal with ?
I suppose you already know the answer : he will leave or will never work for your company.
How would he know your company “sucks” without working for it ?
There are many services, such as Glassdoor, which can inform the developers about the internal context : life at work is not private anymore.
Do you have too many legacy systems ? Managers are not Agile friendly ? A bad work/personal life balance ?
It’s already on Glassdoor …
My template used numerous concepts:
Some advices:
Here is an example of raw data for one software engineering department :
A template has to match your needs : I added a role column to the left of each story. A role could be : Product Owner, Developer, and so on.
Decision-makers are often top managers and have little time to analyze raw and dry data.
Visual thinking is probably the best way to highlight salient facts about an average experience.
The brain is also very good at spotting patterns or anomalies in pictures.
Here is a visual built with some raw data I gathered:
UX and various approaches such as spanning Kanban are key to improve the Developer eXperience.
An excellent DX could bring key benefits to your company :
Started coding in Basic at 11, never stopped since then. Solution Architect & Software Craftsman, check my portfolio, here https://goo.gl/6iRhsz.
See all (17)
27 
27 claps
27 
Started coding in Basic at 11, never stopped since then. Solution Architect & Software Craftsman, check my portfolio, here https://goo.gl/6iRhsz.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/analytics-vidhya/google-cloud-platform-user-manual-for-big-data-projects-15a86fa08fa8?source=search_post---------94,"There are currently no responses for this story.
Be the first to respond.
This blog can also be found in Linkedin. This blog was originally for our team’s internal use, introducing the architectures of our Google Cloud Platform (GCP) projects, our best practice to use GCP, and guidance to gain the access and setup GCP projects. Now the company related material is removed but I have kept the main contents.
From this blog you will see how to access and create GCP project, how to manage the production and development environments in GCP, how to create dataproc cluster, submit jobs to dataproc cluster, run big query jobs, load data from cloud storage to big query, how to create computer engine with GPU for deep learning models, and copy data from on-premise Hadoop cluster to google cloud storage.
First to create a GCP project.
According to GCP documentations.“A project organizes all your Google Cloud Platform resources. A project consists of a set of users; a set of APIs; and billing, authentication, and monitoring settings for those APIs. So, for example, all of your Cloud Storage buckets and objects, along with user permissions for accessing them, reside in a project. You can have one project, or you can create multiple projects and use them to organize your Google Cloud Platform resources, including your Cloud Storage data, into logical groups.”
We have three cloud projects in our GCP account, every team member should have access to at least the dev and stg projects.
· gcpExploration, Project ID: xxxx-stg. This project is used for exploring new features in GCP.
· gcpDevelopement, Project ID: xxxx-dev. This is our development project, which is used for the developing and test of data pipeline and machine learning models, and also running adhoc jobs.
· gcpProduction, Project ID: xxxx-prd. This is our production project, all the production pipelines and models should be running in this project.
2. Access control
We have two gcp groups in our team: gcp-user and gcp-admin. The access control of gcp projects are operated based on the gcp groups: team members in gcp -admin have all accesses to the three projects, while team members in gcp-user have no access to the production project. The production pipelines and machine learning models are set to access to the production project through the service account instead of each individual team member’s account.
Here having access to the project means that you have full access to all the enabled components in the project, such as you can create and delete dataproc clusters/computer engines, create and delete big query dataset and tables, and perform write/read operations to the cloud storage associated to this project.
3. Data storage
We have 4 databases in GCP, their cloud storage path should be in the routine as gs://gcp_project_id-warehouse/database.db/. a. project1: used for project1 related tables, its cloud storage path in production project is: gs://xxxx-prd-warehouse/project1.db/
b. project2: used for project2 related tables, its cloud storage path in production project is: gs://xxxx-prd-warehouse/project2.db/
c. cmn: used for common tables, such as uber table that is used by all project1 and project2 projects, its cloud storage path in production project is: gs://xxxx-prd-warehouse/cmn.db/
In gcp, it is recommended to store all data and hdfs files in cloud storage in the corresponding projects, i.e. production data should reside in the cloud storage in production project: gs://xxxx-prd-warehouse/. And test/adhoc data should reside in the cloud storage in the dev project. All dataproc Hadoop clusters in our projects share the same meta-store, which means that tables defined in different Hadoop clusters in different projects are shared across the projects. It is set that the dev and stg projects can have read access to the cloud storage in the prod project. As a result, test/adhoc jobs in the dev/stg projects can use the table and data defined/reside in prod project. However, test/adhoc jobs in dev/stag project are not allowed to write to the cloud storage in the prod project, the data for those test tables should reside in the stag/dev’s cloud storage.
An example to create table with data stored in cloud storage:
4. Access to other teams’ projects
Both our dev and prod projects have read access to the cloud storage buckets in our collaborate team’s gcp project. It means that we can operate the data in their cloud storage in Big Query/hadoop clusters in our projects. And the dataproc cluster in their projects also use the same meta-store as we do, so we can directly read the tables in their projects in the same way as in on-premise cluster.
5. Access from on-premise to GCP
On-premise cluster needs to be connected to GCP using service accounts, then we can run jobs in on-premise cluster and dump the results in cloud, or we can create and process tables with data actually resides in cloud storage in our on-premise cluster.
In order to connect to gcp project using command line in terminal, we need to install GCP SDK and set up the configurations. Practically, using GCP console is more convenient for users, so this step is not necessary. Before starting the following steps to install GCP SDK, we need to make sure that python2.7 is installed in our laptop.
1. Download and install Google SDK
a. click to download sdk for mac os 64-bitb. unzip the tar.gz file and run ./google-cloud-sdk/install.sh
2. Set up first GCP project configuration
a. run the command: gcloud init b. In prompt as following to select cloud project, select [1] and then enter gcpStaging
c. Select a default region/zone: region=us-central1 and zone=us-central1-a
3. Set up multiple project configurations
Run “gcloud config configurations list” you will see only stg project has been configured, we need to set up configurations for the other two projects.a. “gcloud config configurations create dev-config” to create a new empty configuration and activate itb. “gcloud init” to set up the configurations by selecting “[1] Re-initialize this configuration [dev-config] with new settings”c. After the new configurations have been set up, we can switch between different projects by using “gcloud config configurations activate stag-config”d. We can also use “gcloud config configurations delete [config name]” to delete project configurations
Cloud storage is independent of any computer engines, which means that deleting computer engine will not remove the data in cloud storage. Cloud storage is also Hadoop Compatible File System, which can be used as the storage for Hadoop clusters.
There are at least three ways to access cloud storage:
1. GCP console
we can create/detele buckets and upload/delete files
2. gsutil command line
Refer to: https://cloud.google.com/storage/docs/quickstart-gsutila. gsutil mb, create storage bucketsb. gsutil cp, copy local file to storagec. gsutil ls, list the files in storage bucketsd. gsutil cat, display the storage file
Since computer engine is billed based on the resources we request, and we will be continuously charged even we are not using the VMs. To stop being charged, we need to delete the computer engine. So it is recommended that we should keep the important input/ output data and code in cloud storage. It is recommended to read input directly from cloud storage bucket and write directly to the cloud storage in the code. If it is not possible, we should at least back up the data and codes in cloud storage/laptop daily, especially before deleting the VMs.
1. Steps to setup deep-learning computer engine with GPUs
a. first switch to the project to host the computer engine using “gcloud config configurations activate [config_name]”
b. create computer engine using gcloud command line, you may need to specify the subnet for your company’s project
c. go to GCP console -> computer engine to get the VM’s internal IP and add your ssh key to the VM.
d. set up jupyter notebook:
Hadoop clusters in GCP use computer engines as master node and worker nodes, thus Hadoop clusters are charged on the requested resources, i.e. CPUs, memories, and disks. So it is recommended to use on-demand Hadoop clusters. For some adhoc jobs, production project is now hosting a small permanent Hadoop cluster, which can be expanded if needed (it is also ready to create permanent Hadoop clusters in dev project if needed). For production pipelines/models, the practice is to create Hadoop clusters using gcloud api in airflow dags, and then run jobs in this Hadoop cluster. When those jobs are completed, the Hadoop cluster should be deleted inside airflow dags. An example using on-demand hadoop cluster in airflow dag can be found here. Using this practice, we observed a rapid decrease in the spend caused by GCP dataproc.
1. Submitting jobs to Hadoop cluster
There are at least three ways to submit adhoc jobs to a Hadoop cluster.
a. In GCP console, select Dataproc → select Jobs → click submit jobs Region: us-central1 Cluster: select our cluster name, i.e. hadoop-cluster  Job type: can be either hadoop, pyspark, spark-sql, hive and so on
b. Using gcloud SDK, make sure the right GCP project is activated, i.e. the Hadoop cluster should be in the activated project configuration
Example: gcloud dataproc jobs submit hive — cluster hadoop-cluster — execute “show databases” — region us-central1
c. Login into the VM instances in the Hadoop cluster, and then submit jobs there. This is not recommended.
2. Operating on Hadoop clusters in airflow
Airflow is using gcloud python package and service account to interact with Dataproc Hadoop clusters in GCP, the API callings are encapsulated in operators. There are mainly three Dataproc operators we use often: DataprocClusterCreateOperator, DataProcHiveOperator, DataprocClusterDeleteOperator. These operators are modified for use in our GCP projects, the connection between our GCP projects and our airflow platform need to be set up using GCP service account.
a. DataprocClusterCreateOperator
b. DataProcHiveOperator
c. DataprocClusterDeleteOperator
You may notice that I use “””hadoop-cluster-kpi-pacing-ratio-{{ ds }}””” as the cluster_name, this is because we need to make sure that the cluster_name is unique between different dags and different dag runs of this same dag, the example dag is daily scheduled, so {{ ds }} is OK. Otherwise, it may happen that airflow task instance deletes a cluster which is running jobs in other dag runs. We can also use uuid to generate unique dataproc name, however with dag information in dataproc’s name will be helpful to monitor the jobs.
3. Debugging jobs in Hadoop clusters
In Dataproc, select Jobs, you will see a full list of jobs in this project. By clicking the job, we can see its log and output.
For adhoc jobs, it is recommended to run queries using Big Query, it is blazingly fast. There are two types of tables in Big Query, one is native table, the other one is external table. The data for native tables resides in Big Query’s storage, which is different from cloud storage. The data for external tables resides in other external sources, such as cloud storage. The performance of processing on native tables is faster than that of external tables. But the advantage of external tables is that it will not require additional storage resources, and what is more important is that the query results are always up to date based on external source data. But if we load HDFS files in cloud storage in Big Query as native tables, then we need to re-load it once the data in cloud storage is updated.
Practically, we use parquet format to store our data in cloud storage which can save a lot of storage spaces. And currently Big Query does not support parquet format as the external data source, thus we have to create native tables in Big Query and load data in cloud storage to the native tables. We should keep in mind that we need to re-load the data from cloud storage once they are updated.
The partitioning concept in Big Query is different from that in Hive. Also since Big Query is optimized to process large data, the practice is that we can load multiple partitions’ data in one Big Query table. For example, dctr table now is based on the daily partition, but in Big Query, we can load one month or even more days’ data in a single big query table. Note that after loading to big query table, the partition field will not be in the big query’s table schema, since there is no partition column in the HDFS path for the hive tables.
There are at least three ways to operate on Big Query, in which it is more convenient to use GCP console. Currently, we have four datasets in the dev and prod projects. Please create big query tables in the corresponding dataset. Dataset in big query is similar to database in Hive.
1. Operate on Big Query through GCP console
a. load one-month dctr data from cloud storage to Big Query, it may take around 2 minutes
Create table from: Google Cloud StorageSelect file from GCS buckets: gs_pathFile format: ParquetProject name: wmt-customer-tech-case-sci-dev Dataset name: casesci_sem Table type: Native table
b. process data in Big Query, Big Query’s SQL is pretty much the same as in Hive
Note that “CAST(date_string AS STRING)” since string type in parquet format is stored as binary to save space. In the execution details, we can see that it takes only 1.2 seconds to process 150MB data in Big Query.
c. write to a new table, in query_settings, select “Set a destination table for query results”
2. Operate on Big Query through bq command line
a. bq commands. Eg. bq ls project_id, bq show project_id:data_set.table_name, bq rm -t mydataset.mytable
b. load one-month dctr data from cloud storage to Big Query, make sure to use the right project configuration before running the commands
c. process Big query table
d. write to a new table
3. Operate on Big Query through Google Cloud API
For now, we don’t plan to migrate large sets of historical data from On-Premise Hadoop Cluster to Google Cloud Storage, which can be done by running distributed copy jobs in BFD clusters. But connection from on-premise cluster to GCP needs to be set up using GCP’s service account, and we are migrating some final production tables from on-premise cluster to GCP every day.
1. Export daily partition to GCP
a. exporting data in on-premise cluster by dumping query results in an external table pointing to Google Cloud Storage
b. creating tables in GCP pointing to the data in Google Cloud Storage
2. Migrate large sets history data to GCP
For now, we don’t plan to migrate large sets of historical data from CDC to Google Cloud Storage, which can be done by running distributed copy jobs (distcp) in BFD clusters. I am going to build a dashboard to allow users to copy large history data from on-premise Hadoop cluster to Google Cloud Storage.
Analytics Vidhya is a community of Analytics and Data…
2 
2 claps
2 
Analytics Vidhya is a community of Analytics and Data Science professionals. We are building the next-gen data science ecosystem https://www.analyticsvidhya.com
Written by
Software Engineer @WalmartLabs, interested in big-data techniques, data visualization, machine learning, etc. https://www.linkedin.com/in/boning-zhang-937b1775
Analytics Vidhya is a community of Analytics and Data Science professionals. We are building the next-gen data science ecosystem https://www.analyticsvidhya.com
"
https://medium.com/@johnjjung/creating-an-inter-kubernetes-cluster-services-using-an-internal-loadbalancer-137f768bb3fc?source=search_post---------95,"Sign in
There are currently no responses for this story.
Be the first to respond.
John Jung
Oct 23, 2018·2 min read
So, say you’ve got two Kubernetes clusters running on GCP and you need to expose a service from Cluster A to Cluster B.
Google Cloud Platform has several ways of doing this but my favorite by far is to use their internal load balancer.
Note: You must have VPC-native (alias IP) enabled when creating your kubernetes cluster on in order for this to work!
Also, the kubernetes clusters and the internal loadbalancers need to be in the same region, so make sure it’s both us-east-1c or whatever you use.
And the critical part here is to create a service that is a LoadBalancer like any other LoadBalancer but use an annotation that essentially allows you to talk between the clusters.
After you run this, you’ll be able to see an EXTERNAL-IP under services:
And from the other cluster always check and you should be able to hit that endpoint!
Hope this helps!
Sign up for GCP using the link below for an additional $50! ($350 total for new users).
https://gcpsignup.page.link/Nv9h
Director of Engineering @Nylas <- June.ai, Academia, and NASA
See all (45)
51 
1
51 claps
51 
1
Director of Engineering @Nylas <- June.ai, Academia, and NASA
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@lunajacob/utilizing-gcps-identity-aware-proxy-to-ssh-into-internal-ip-only-vm-s-237bca0123c?source=search_post---------96,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jacob Luna
Sep 27, 2021·4 min read
I will be going over Google’s Identity Aware Proxy service(IAP), which is a fully managed solution for implementing a zero-trust model on multiple GCP resources including Compute Engine. Demo will include creating a VM, enabling the IAP API to tunneling through to a VM that doesn’t have an external IP address!
Google Defines zero trust as :
“A zero trust network is one in which no person, device, or network enjoys inherent trust. All trust, which allows access to information, must be earned, and the first step of that is demonstrating valid identity. A system needs to know who you are, confidently, before it can determine what you should have access to. Add to that the understanding of what you can access–authorization–and you’ve got the core foundation of zero trust security.”
More about Google vision on zero-trust can be found here
IAP shifts away from isolating apps and VMs at the network layer (a traditional tool would be using a VPN), to managing and centralizing authorization at the application layer using HTTPS.
I created a VM called “iap-demo” without an external IAP address using the following gcloud command.
gcloud compute instances create iap-demo — project=$projectID — zone=us-central1-a — machine-type=f1-micro — network-interface=subnet=default,no-address
By running the gcloud compute instances list command I verified the VM is created without an external IP address.
Navigating to the IAP API, I simply enabled the API from the cloud console.
Heading back to IAP service I am presented with HTTPS resources or SSH/TCP resources. Selecting the SSH/TCP option I see the VM I have created earlier in the demo with an “error” icon next to it.
Since IAP wraps the SSH connection with HTTPS, a firewall rule is still needed to be created. At the time of writing this GCP needs the following Source IP range/ Allowed protocols for IAP to work.
I created the firewall rule using the following gcloud command.
gcloud compute — project=$projectID firewall-rules create iap-fw — direction=INGRESS — priority=1000 — network=default — action=ALLOW — rules=tcp — source-ranges=35.235.240.0/20
After creating the firewall rule I navigated back the the IAP service where I no longer see an error message and see the green OK message.
Now that I have created an Internal-IP only VM, enabled the IAP API, and created firewall rule required to use IAP, I can now connect to the VM without the need of an external IP address by running the following gcloud command.
gcloud compute ssh iap-demo — zone us-central1-a — tunnel-through-iap
Notice the error, I have setup the VM to utilize the IAP and created the firewall rule for it to work. What I have not done yet is authorize the user to connect to the VM. This is achieved with GCP IAM by granting the role roles/iap.tunnelResourceAccessor . After adding the role that authorizes the user to connect I re-run the above command and connect successfully!
GCP’s IAP is a very powerful tool with many other features. I hope this quick demo on how you can utilize IAP to connect to a VM without the need of an external IP has been helpful. This can be done at scale on managed instance groups and even GKE nodes.
Bio: Jake has been in IT for 5 years and is currently a Cloud Security Engineer. He currently holds CompTIA S+, Google’s ACE , Google’s PCA, Google’s PCSE, and Terraform associate certifications!
LinkedIn: https://www.linkedin.com/in/jacob-c-luna/
2 
2 
2 
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@rdodev/we-need-a-new-cloud-narrative-ba3afcee9f6e?source=search_post---------97,"Sign in
There are currently no responses for this story.
Be the first to respond.
Ruben Orduz
Feb 14, 2016·2 min read
I’ve been directly involved with ‘cloud’ things since at least 2010. By 2011 I was doing full-blown internal ‘evangelism’ and POCs to demonstrate the value of the ‘cloud’ for my then-employer. By 2013 or so, working in and being part of the field was akin to trying to wrangle and harness a whirlwind of new technologies for orchestration, monitoring, automation, configuration, new use cases, etc. In the last couple of years cloud-related technologies have continued to improve, ecosystems have evolved, platforms have gained momentum and a non-trivial adoption by “the enterprise”, which is notoriously slow to warm up and embrace new technologies, has taken place.
Currently the conversation, depending on the audience and participants, pivots around pricing, flexibility, compliance, TCO, No Ops, Cloud Ops, Data Centers, private, public, hybrid, hypervisors, containers, etc. This type of conversation is beginning to feel somewhat anachronistic and I’d even go as far to say they are against the very promise and premise of ‘the cloud’.
Having these type of conversations feels as if we, universally speaking, set anchor on a mindset that was relevant fifteen — twenty? — years ago and that mindset is hindering progress and the evolution of the ‘cloud’ concept and promise.
While fully realizing I don’t have a prescriptive approach or proposal, what I suggest is to change the conversation to one where the benefits of the cloud are assumed, its details invisible (and maybe even irrelevant!). Conversations should be refocused from the opposite direction, namely from the perspective of the application, its consumers and users. For example, if the conversation veers into virtualization type, hard disk type, that conversation would be off track (to be clear, at some point way far, far, far down the line that conversation might still need to happen and among operations teams — certainly not at the business or application development level). Much like, in every day conversation, we don’t talk about the chemical composition of an actual cloud, the components and details of a ‘Cloud’ should be unimportant in the vast majority of cases. Put another way, I think it would be of benefit to the industry if moving forward conversations took ‘the cloud’ as a commoditized platform, not as a complex collection of DCs, VMs, networks, disks and so forth like we do now.
Software, hi-fi audio, data, financial markets observer.
2 
2 
2 
Software, hi-fi audio, data, financial markets observer.
"
https://medium.com/@opcitotechnologies/securing-kubernetes-environment-with-internal-load-balancers-and-ingress-on-aws-and-vpn-5a05d7f80e94?source=search_post---------98,"Sign in
There are currently no responses for this story.
Be the first to respond.
Opcito Technologies
Feb 1, 2019·2 min read
Security is one of the biggest concern nowadays and organizations have started investing a considerable amount of time and money in it. 2018 has shown every one of us why it is of utmost importance to secure data and application against any threat imposed. This is particularly the reason why one needs to pay more attention to securing applications, production, and data-sensitive environments like Elasticsearch, MongoDB, or databases like MySQL.
Kubernetes is turning in to a go-to solution for container orchestration. Combining Kubernetes with clouds like AWS, Azure, or GCP to deploy applications and services can really augment their ability to serve users. But when it comes to data sensitive services like Elasticsearch or databases you need to take extra care. In this blog, I will be talking about securing an AWS-based K8s environment with data-sensitive services using OpenVPN. And how to configure Ingress controller with an internal load balancer to make sure these services are not accessible outside your AWS VPC network.
When it comes to controlling your services’ exposure, Kubernetes offers a lot of flexibility. You can configure your service objects to control who accesses what within the cluster or from outside the cluster. If you are using AWS, you can even request a load balanced IP address or hostname for your service. With service object, you can easily connect services running in pods. But with Ingress, you get the flexibility to configure external access and to set up basic routing rules to different services or configure things like TLS. Different Ingress controllers can be used for different implementations and the most popular one is the NGINX Ingress Controller. This blog will help you get a clear idea as to how you can secure Kubernetes’s data sensitive services, how to create Ingress controller using an internal load balancer and what should be the basic architecture for Data sensitive Services like Elasticsearch, MySQL, and MongoDB.
For this, setup any VPN server in your Kubernetes deployed VPC and expose all services using Ingress which is configured as an internal load balancer. I have used OpenVPN in this case, you can use any VPN of your choice. Setup an OpenVPN in your Kubernetes VPC. After you set up the OpenVPN and client configurations, you should …read more
Product engineering experts specializing in DevOps, Containers, Cloud, Automation, Blockchain, Test Engineering, & Open Source Tech
See all (205)
2 
2 claps
2 
Product engineering experts specializing in DevOps, Containers, Cloud, Automation, Blockchain, Test Engineering, & Open Source Tech
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@OpenCredo/google-cloud-spanner-our-first-impressions-by-david-borsos-and-dominic-fox-5c7df47bdd6?source=search_post---------99,"Sign in
There are currently no responses for this story.
Be the first to respond.
OpenCredo
Mar 7, 2017·26 min read
Google has recently made its internal Spanner database available to the wider public, as a hosted solution on Google Cloud. This is a distributed relational/transactional database used inside for various Google projects (including F1, the advertising backend), promising high throughput, low latency and 99.999% availability. As such it is an interesting alternative to many open source or other hosted solutions. This whitepaper gives a good theoretical introduction into Spanner.
Since we are currently in the process of evaluating various distributed “NewSQL” databases, we felt it was worth adding Cloud Spanner to the list. It is important to point out that at the moment the service is still in “beta” phase and there are some restrictions (for example you cannot have a multi-region Spanner instance), but functionally it should be close enough to the final version that it’s worth taking a look.
It is somewhat difficult to describe Google Spanner with a single label — it does not really fit into existing categories. Instead, let’s discuss it from several aspects:
Yes. Each Spanner instance is running on multiple appropriately sized nodes, managed by Google’s Cloud service. This provides scalability in terms of data size and transactional throughput and adds resilience to the system.
Not fully. Although Spanner’s data model is fundamentally like any other relational database’s — there are pre-defined tuples of data that can be stored in relations (tables) and queried — it is lacking in terms of constraints.
Primary keys can (in fact, must) be defined, and uniqueness can be enforced; however there is no concept of a FOREIGN KEY to its full extent (we are going to discuss INTERLEAVE shortly — it is not an exact match).
From relational databases we expect the capability to enforce the integrity of the data, to ensure that it matches pre-defined constraints. Spanner’s capabilities are somewhat limited from this perspective.
Yes. In fact Google’s transactional guarantees are very strict, probably stricter than those of most commonly used RDBMS. Additionally, transactional timestamps match “wall clock” time, thanks to TrueTime technology.
Mostly yes. Although currently only the SELECT statement is supported — any mutating operation must be executed through Spanner’s API (a REST API with client libraries available for most major languages).
CP, effectively CA. Due to the distributed nature of Spanner, this is a valid question to ask. The relevant whitepaper states that Spanner is a CP system fundamentally:
Does this mean that Spanner is a CA system as defined by CAP? The short answer is “no” technically, but “yes” in effect and its users can and do assume CA.
The purist answer is “no” because partitions can happen and in fact have happened at Google, and during (some) partitions, Spanner chooses C and forfeits A
However in Google’s infrastructure network partitions are so rare that effectively it can be treated as CA, users can assume 99.999% availability. In fact during Google’s history of using Spanner, most problems were not network related.
Setup is trivial, there are almost no options that can be specified: you have to decide which region the Spanner instance should be placed in, and the number of nodes, and that’s it.
Access control integrates with Google Cloud’s IAM mechanism. There is actually no fine-grained permission model though; you can have read/write permissions on an entire database but not on individual tables.An example command to set up an instance and a database from Google Cloud’s CLI looks like this:
The operational tooling supplied is exclusively Google Cloud’s Web UI or CLI. These give you the standard operations you’d expect: creating and deleting instances, databases, executing schema manipulation, do ad-hoc querying, monitoring key metrics, etc… Operations are accessible through Google Cloud’s REST API and in client libraries.
Ad-hoc querying can be done via the Google Clout Web UI, but there is no officially supported standalone client of equivalent functionality. The gcloud CLI has a similar facility but it is a bit long-winded to use comfortably:
Similarly, schema changes can be done through the web or via CLI, but it’s not particularly convenient:
The best way to create the schema in an automated fashion is probably to execute the necessary DDL on application startup time through the client API (it is also possible to interrogate the current schema).
There are two types of objects in Spanner: tables and indexes. These look mostly how you would expect from a traditional RDBMS. Here is an example definition of a table:
The only change is that the PRIMARY KEY clause is outside of the brackets.
Data distribution is based on the PRIMARY KEY so care needs to be taken when choosing it in order to avoid cluster hotspots. Timestamps and monotonically increasing numbers are bad choices, while random information (e.g. user name) or UUIDs are good ones.
As we noted previously, there is no way of defining a FOREIGN KEY relation between tables. However, Spanner has a concept that is somewhat similar at glance:
If you specify a table in the following way:
As a result the customer_order table’s rows will be physically co-located – interleaved with the parent table’s respective rows. There are a few restrictions on this:The “child” table must have a PRIMARY KEY that contains the parent’s primary keys first (column names must match)
Although there is a logical relationship introduced between the parent and child rows, the primary purpose of interleaving is to speed up certain queries — especially joins. This makes intuitive sense, since it’s directly altering the way the data is laid out on disk and making sure that the whole join query can be executed without accessing every node of the cluster.
In this sense this model is actually much closer to Cassandra’s concept of a partition key with clustering columns (which is a very strong definition of a physical layout) than to a relational database’s foreign key constraint (which is fundamentally a logical relationship between data items).
The other construct available in the DDL of Spanner is a secondary index. Unsurprisingly you can create a secondary index in the following way:
Please note the UNIQUE keyword – this index will enforce uniqueness of rows inserted into the customer table. Additionally, it is worth pointing out from the Spanner documentation that:
Cloud Spanner chooses an index automatically only in rare circumstances
This effectively means that having the index is not enough — when executing any queries you will explicitly have to specify (through the FORCE_INDEX directive) that you want to use one.
Google Cloud Spanner supports SQL’s SELECT statement only. That means you cannot execute any INSERT, UPDATE or DELETE statements and rely on common transaction control patterns. SELECT is fully supported (in fact with some extensions).As a result it is necessary to talk about the programming model — you are forced to use the APIs to do any data mutation. We are going to show some example code blocks to demonstrate how to interact with Spanner.
Our example application manages customer orders in an online store. A Customer can have many Orders which in turn contain Items (order lines) of Products.
Since we are currently in the process of evaluating various distributed “NewSQL” databases, we felt it was worth adding Cloud Spanner to the list. It is important to point out that at the moment the service is still in “beta” phase and there are some restrictions (for example you cannot have a multi-region Spanner instance), but functionally it should be close enough to the final version that it’s worth taking a look.
It is somewhat difficult to describe Google Spanner with a single label — it does not really fit into existing categories. Instead, let’s discuss it from several aspects:
Yes. Each Spanner instance is running on multiple appropriately sized nodes, managed by Google’s Cloud service. This provides scalability in terms of data size and transactional throughput and adds resilience to the system.
Not fully. Although Spanner’s data model is fundamentally like any other relational database’s — there are pre-defined tuples of data that can be stored in relations (tables) and queried — it is lacking in terms of constraints.
Primary keys can (in fact, must) be defined, and uniqueness can be enforced; however there is no concept of a FOREIGN KEY to its full extent (we are going to discuss INTERLEAVE shortly — it is not an exact match).
From relational databases we expect the capability to enforce the integrity of the data, to ensure that it matches pre-defined constraints. Spanner’s capabilities are somewhat limited from this perspective.
Yes. In fact Google’s transactional guarantees are very strict, probably stricter than those of most commonly used RDBMS. Additionally, transactional timestamps match “wall clock” time, thanks to TrueTime technology.
Mostly yes. Although currently only the SELECT statement is supported — any mutating operation must be executed through Spanner’s API (a REST API with client libraries available for most major languages).
CP, effectively CA. Due to the distributed nature of Spanner, this is a valid question to ask. The relevant whitepaper states that Spanner is a CP system fundamentally:
Does this mean that Spanner is a CA system as defined by CAP? The short answer is “no” technically, but “yes” in effect and its users can and do assume CA.
The purist answer is “no” because partitions can happen and in fact have happened at Google, and during (some) partitions, Spanner chooses C and forfeits A
However in Google’s infrastructure network partitions are so rare that effectively it can be treated as CA, users can assume 99.999% availability. In fact during Google’s history of using Spanner, most problems were not network related.
Setup is trivial, there are almost no options that can be specified: you have to decide which region the Spanner instance should be placed in, and the number of nodes, and that’s it.
Access control integrates with Google Cloud’s IAM mechanism. There is actually no fine-grained permission model though; you can have read/write permissions on an entire database but not on individual tables.An example command to set up an instance and a database from Google Cloud’s CLI looks like this:
The operational tooling supplied is exclusively Google Cloud’s Web UI or CLI. These give you the standard operations you’d expect: creating and deleting instances, databases, executing schema manipulation, do ad-hoc querying, monitoring key metrics, etc… Operations are accessible through Google Cloud’s REST API and in client libraries.
Ad-hoc querying can be done via the Google Clout Web UI, but there is no officially supported standalone client of equivalent functionality. The gcloud CLI has a similar facility but it is a bit long-winded to use comfortably:
Similarly, schema changes can be done through the web or via CLI, but it’s not particularly convenient:
The best way to create the schema in an automated fashion is probably to execute the necessary DDL on application startup time through the client API (it is also possible to interrogate the current schema).
There are two types of objects in Spanner: tables and indexes. These look mostly how you would expect from a traditional RDBMS. Here is an example definition of a table:
The only change is that the PRIMARY KEY clause is outside of the brackets.
Data distribution is based on the PRIMARY KEY so care needs to be taken when choosing it in order to avoid cluster hotspots. Timestamps and monotonically increasing numbers are bad choices, while random information (e.g. user name) or UUIDs are good ones.
As we noted previously, there is no way of defining a FOREIGN KEY relation between tables. However, Spanner has a concept that is somewhat similar at glance:
If you specify a table in the following way:
As a result the customer_order table’s rows will be physically co-located – interleaved with the parent table’s respective rows. There are a few restrictions on this:The “child” table must have a PRIMARY KEY that contains the parent’s primary keys first (column names must match)
Although there is a logical relationship introduced between the parent and child rows, the primary purpose of interleaving is to speed up certain queries — especially joins. This makes intuitive sense, since it’s directly altering the way the data is laid out on disk and making sure that the whole join query can be executed without accessing every node of the cluster.
In this sense this model is actually much closer to Cassandra’s concept of a partition key with clustering columns (which is a very strong definition of a physical layout) than to a relational database’s foreign key constraint (which is fundamentally a logical relationship between data items).
The other construct available in the DDL of Spanner is a secondary index. Unsurprisingly you can create a secondary index in the following way:
Please note the UNIQUE keyword – this index will enforce uniqueness of rows inserted into the customer table. Additionally, it is worth pointing out from the Spanner documentation that:
Cloud Spanner chooses an index automatically only in rare circumstances
This effectively means that having the index is not enough — when executing any queries you will explicitly have to specify (through the FORCE_INDEX directive) that you want to use one.
Google Cloud Spanner supports SQL’s SELECT statement only. That means you cannot execute any INSERT, UPDATE or DELETE statements and rely on common transaction control patterns. SELECT is fully supported (in fact with some extensions).As a result it is necessary to talk about the programming model — you are forced to use the APIs to do any data mutation. We are going to show some example code blocks to demonstrate how to interact with Spanner.
Our example application manages customer orders in an online store. A Customer can have many Orders which in turn contain Items (order lines) of Products.
This is the code you would need to write to get a Customer record by its ID:
This is equivalent to the following:
A couple of noteworthy observations:
Modification follows a similar pattern:
You have to create a collection of Mutation objects – which can be done via the relevant builder objects – and then execute write(), which provides atomic execution of the mutations.
From the Java client there are two ways of specifying the secondary index to use; rely on the relevant API methods:
Or use the FORCE_INDEX directive in the query language:
Spanner has ACID transactions which it achieves using a combination of pessimistic locks and 2-phase commit. In the API the transactional constructs are centered around the TransactionRunner and TransactionContextclasses.Our example code shows the addition of a new order. This needs to create the Order object as well as add each line to the Order – checking that there is enough stock to fulfill the order and adjusting inventory accordingly:
The most important aspects of this:
The client library is available on Maven Central:
Note the exclusions which we had to add to make our client code work — the Spanner library pulls in a large number of dependencies.
To get your client talk to the database you need to configure access. This is relatively straightforward — the following simple Spring bean definitions result in a working client application:
On SpannerOptions there is a high number of additional properties such as timeouts and pool sizes that can be set if necessary (we have tested with the defaults).
Access credentials can also be set via SpannerOptions, but the recommended approach is different:
On Google Cloud Compute instances you get an environment pre-configured that is recognised by the Spanner client, which will pick up the appropriate Service Account to access Spanner (the Service Account needs to have sufficient IAM permissions to do this)
On any other client:
We have not done a detailed load test on Google Cloud Spanner, only some exploratory testing. We mainly were interested in the performance of joins, so here are some basic figures. Please note that these tests may have been limited by our testing infrastructure’s throughput and are not necessarily the limits of what Spanner can achieve. Additionally further tuning of the client’s settings (connection pooling) could improve observed performance.
For testing we have used a 7 node Spanner cluster in europe-west1 region and 2 n1-highcpu-4 compute instances in different zones of the same region to run our test code.
In this case we were just reading single Customer records by primary key to establish a baseline performance. We observed roughly 16,000 transaction per second throughput (suspected to be limited by the testing nodes) with mean latency around 25ms and 99 percentile around 40ms.
In this case we wanted to read all the Orders (not the individual items just the order data itself) belonging to a single Customer by customer ID. The following query was used:
The observed numbers are very close to what we measured for the simple read scenario: 16,000 transactions per second, mean latency around 25ms and 99 percentile around 40ms.
The third case we repeated the same join query as before, but using non-interleaved tables. The performance drop was quite significant, in fact the queries were so slow that we would not recommend doing this in a production system: 14 transactions per second, mean latency around 14 seconds 99 percentile around 23 seconds.
Note that join performance may be dependent on volume of data in the various tables.
The final case that we have tried is using a non-interleaved table and put an index on the column that we’re joining on. Additionally this required a change in the query itself, which looked like this:
Notice the usage of the explicit FORCE_INDEXdirective. If we omit this, Spanner ignores the index on the join and reverts back to not using the index and is exactly the same as the previous case.The FORCE_INDEX directive resulted in greatly improved performance. Note that though it’s still somewhat less efficient than using INTERLEAVE, the index can provide very good levels of performance: 14,000 transactions per second, mean latency around 30ms and 99 percentile just under 100ms.
First of all, Cloud Spanner is still declared beta, meaning you should at the very least be careful when adopting. However Google has been using it internally for years, which implies that at its core it’s fairly mature (and indeed we had the impression of a mature product when using it).
What we really liked about it was the very strong ACID guarantees on transactions — if we were to single out the most impressive feature, this certainly would be it. The “relational” model given is nice, but it’s not “pure” as you’d expect a single-server RDBMS to be. We think that the compromises that Spanner has made are sensible ones — supporting arbitrary queries efficiently in a distributed system is difficult, and every attempted solution involves tradeoffs of some kind. Instead, Spanner does not shield you completely from the physical layout of the data and allows you to make the tradeoffs that make most sense for your use-case.
The other side of this point is that you have to pay attention to data modeling and have to understand not only the logical structure of the data, but the commonly used access patterns, too — at schema design time.Interestingly some strategies from the NoSQL world (especially from Cassandra) can be really handy here — each Cassandra table with complex primary keys can easily be translated to a tree of Spanner tables. However, Spanner takes this model further, since several levels of parent-child table relationships are permitted.
And finally, some of the bits we didn’t like: Spanner is not compatible with any other database technology out there. It has a completely proprietary API that you need to learn and adapt your code to. Additionally there is no fully functional bridge to common database APIs such as JDBC. The lack of tooling (e.g. a SQL “console” to explore data and experiment) makes day-to-day usage a bit unpleasant. Also, if you adapt Spanner you are locking yourself into Google Cloud — there is no way you would be able to run this database anywhere else but on Google’s infrastructure, and your code ceases to be portable.
To summarise:
If you are already a Google Cloud user and happy with it, yes. For any greenfield project, it’s worth considering if you have requirements for a very highly available and scalable datastore and you are not happy with the compromises you have to make with a NoSQL technology.
Migrating an existing RDBMS-based application to Cloud Spanner is a significant exercise.
However, we don’t think it’s the ultimate database that solves every data storage problem. It’s not far away, it certainly covers a lot of use-cases and solves many problems, but does not give you the same level of flexibility as a traditional RDBMS would.
The bottom line is that we recommend some level of caution if you are thinking of adapting Google Cloud Spanner. From a purely technical perspective, it’s a great product, but it may not be what you need exactly. Understand your use-case and what you need from your database, play with the API a bit and decide whether you are happy or not with the lock-in; then make the decision to adopt it or not.
This is the code you would need to write to get a Customer record by its ID:
This is equivalent to the following:
A couple of noteworthy observations:
Modification follows a similar pattern:
You have to create a collection of Mutation objects – which can be done via the relevant builder objects – and then execute write(), which provides atomic execution of the mutations.
From the Java client there are two ways of specifying the secondary index to use; rely on the relevant API methods:
Or use the FORCE_INDEX directive in the query language:
Spanner has ACID transactions which it achieves using a combination of pessimistic locks and 2-phase commit. In the API the transactional constructs are centered around the TransactionRunner and TransactionContextclasses.Our example code shows the addition of a new order. This needs to create the Order object as well as add each line to the Order – checking that there is enough stock to fulfill the order and adjusting inventory accordingly:
The most important aspects of this:
The client library is available on Maven Central:
Note the exclusions which we had to add to make our client code work — the Spanner library pulls in a large number of dependencies.
To get your client talk to the database you need to configure access. This is relatively straightforward — the following simple Spring bean definitions result in a working client application:
On SpannerOptions there is a high number of additional properties such as timeouts and pool sizes that can be set if necessary (we have tested with the defaults).
Access credentials can also be set via SpannerOptions, but the recommended approach is different:
On Google Cloud Compute instances you get an environment pre-configured that is recognised by the Spanner client, which will pick up the appropriate Service Account to access Spanner (the Service Account needs to have sufficient IAM permissions to do this)
On any other client:
We have not done a detailed load test on Google Cloud Spanner, only some exploratory testing. We mainly were interested in the performance of joins, so here are some basic figures. Please note that these tests may have been limited by our testing infrastructure’s throughput and are not necessarily the limits of what Spanner can achieve. Additionally further tuning of the client’s settings (connection pooling) could improve observed performance.
For testing we have used a 7 node Spanner cluster in europe-west1 region and 2 n1-highcpu-4 compute instances in different zones of the same region to run our test code.
In this case we were just reading single Customer records by primary key to establish a baseline performance. We observed roughly 16,000 transaction per second throughput (suspected to be limited by the testing nodes) with mean latency around 25ms and 99 percentile around 40ms.
In this case we wanted to read all the Orders (not the individual items just the order data itself) belonging to a single Customer by customer ID. The following query was used:
The observed numbers are very close to what we measured for the simple read scenario: 16,000 transactions per second, mean latency around 25ms and 99 percentile around 40ms.
The third case we repeated the same join query as before, but using non-interleaved tables. The performance drop was quite significant, in fact the queries were so slow that we would not recommend doing this in a production system: 14 transactions per second, mean latency around 14 seconds 99 percentile around 23 seconds.
Note that join performance may be dependent on volume of data in the various tables.
The final case that we have tried is using a non-interleaved table and put an index on the column that we’re joining on. Additionally this required a change in the query itself, which looked like this:
Notice the usage of the explicit FORCE_INDEXdirective. If we omit this, Spanner ignores the index on the join and reverts back to not using the index and is exactly the same as the previous case.The FORCE_INDEX directive resulted in greatly improved performance. Note that though it’s still somewhat less efficient than using INTERLEAVE, the index can provide very good levels of performance: 14,000 transactions per second, mean latency around 30ms and 99 percentile just under 100ms.
First of all, Cloud Spanner is still declared beta, meaning you should at the very least be careful when adopting. However Google has been using it internally for years, which implies that at its core it’s fairly mature (and indeed we had the impression of a mature product when using it).
What we really liked about it was the very strong ACID guarantees on transactions — if we were to single out the most impressive feature, this certainly would be it. The “relational” model given is nice, but it’s not “pure” as you’d expect a single-server RDBMS to be. We think that the compromises that Spanner has made are sensible ones — supporting arbitrary queries efficiently in a distributed system is difficult, and every attempted solution involves tradeoffs of some kind. Instead, Spanner does not shield you completely from the physical layout of the data and allows you to make the tradeoffs that make most sense for your use-case.
The other side of this point is that you have to pay attention to data modeling and have to understand not only the logical structure of the data, but the commonly used access patterns, too — at schema design time.Interestingly some strategies from the NoSQL world (especially from Cassandra) can be really handy here — each Cassandra table with complex primary keys can easily be translated to a tree of Spanner tables. However, Spanner takes this model further, since several levels of parent-child table relationships are permitted.
And finally, some of the bits we didn’t like: Spanner is not compatible with any other database technology out there. It has a completely proprietary API that you need to learn and adapt your code to. Additionally there is no fully functional bridge to common database APIs such as JDBC. The lack of tooling (e.g. a SQL “console” to explore data and experiment) makes day-to-day usage a bit unpleasant. Also, if you adapt Spanner you are locking yourself into Google Cloud — there is no way you would be able to run this database anywhere else but on Google’s infrastructure, and your code ceases to be portable.
To summarise:
If you are already a Google Cloud user and happy with it, yes. For any greenfield project, it’s worth considering if you have requirements for a very highly available and scalable datastore and you are not happy with the compromises you have to make with a NoSQL technology.
Migrating an existing RDBMS-based application to Cloud Spanner is a significant exercise.
However, we don’t think it’s the ultimate database that solves every data storage problem. It’s not far away, it certainly covers a lot of use-cases and solves many problems, but does not give you the same level of flexibility as a traditional RDBMS would.
The bottom line is that we recommend some level of caution if you are thinking of adapting Google Cloud Spanner. From a purely technical perspective, it’s a great product, but it may not be what you need exactly. Understand your use-case and what you need from your database, play with the API a bit and decide whether you are happy or not with the lock-in; then make the decision to adopt it or not.
Find more blogs like this at opencredo.com/blog/
Visit www.opencredo.com/blog to see our official blog and latest content.
1 
1 
1 
Visit www.opencredo.com/blog to see our official blog and latest content.
"
https://medium.com/@torocloud/automation-are-we-looking-at-the-next-evolution-of-apis-37bc304df9b7?source=search_post---------100,"Sign in
There are currently no responses for this story.
Be the first to respond.
Toro Cloud
Sep 10, 2021·3 min read
APIs are considered the cornerstone of any digital transformation project. Through APIs, organizations can securely expose data to internal systems, customers and business partners. They can also be a path to new revenue opportunities by creating digital products out of an organization’s data or business processes.
Over the years, we’ve witnessed the evolution of APIs and how they’ve ushered a new era of communication between applications, systems, and services. However, with this evolution came a fair share of challenges around API delivery, discovery, and consumption due to the rapidly growing demand of APIs.
“We historically know that automation and autonomy is actually the solution to complexity. When a lot of people are doing something manually, people start thinking about how to automate and how to give it autonomy. The number of APIs and the way we keep them together by hands is just not going to scale.” says Zdenek Nemec, founder of Superface.AI during an episode of the Coding Over Cocktails podcast.
The goal of Superface is to transform how organizations connect digitally by empowering developers through API automation.
“So, there’s this urge for us to take it further — to start thinking about automation — so we can use our engineers to create a better world, rather than just working on the wires,” adds Nemec.
One of the fundamental changes that Superface has applied is on description formats, splitting the business layer of what an API can do and the actual implementation be it through GraphQL or REST. By separating the business layer from the functional operational programmers will be able to easily identify the components of an API.
Another thing that developers can consider implementing is giving freedom to clients when creating contracts. Clients can opt to seal the deal on the business case level and entrust all the implementation side on the programmers. This will ease the process when clients decide to switch providers who will fulfill the same business case. Programmers can now freely decide on how they will implement the API while the clients focus on the business case.
Nemec stresses however, that automation in enterprises should be seen as more of a cultural change rather than a technical one.
“This automation, CI/CD, and DevOps approach takes actually more change in culture or mindset in the company. We see that we cannot have developers manually connecting to APIs if we are going to really get to some ‘AI future’. The question is, will this particle implementation be the one we think it will?” he furthers.
Changing the way companies approach API automation will be the first step of working towards achieving digital transformation efforts while striving to drive business value.
“I think we, as mankind, will get there — probably not in Superface, probably not in the next decade, but I think we can take it very far. That’s for sure. We are learning in the process.” Nemec explains.
Learn how API automation can help streamline the management of APIs with Zdenek Nemec on Coding Over Cocktails — a podcast by Toro Cloud.
Coding Over Cocktails is a podcast created by Toro Cloud, a company that offers a low-code, API centric platform for application development & integration.
This podcast series tackles issues faced by enterprises as they manage the process of digital transformation, application integration, low-code application development, data management, and business process automation. It’s available for streaming in most major podcast platforms, including Spotify, Apple, Google Podcasts, SoundCloud, and Stitcher.
Originally published at https://www.torocloud.com.
low-code, API centric platform for application development & integration
1 
1 
1 
low-code, API centric platform for application development & integration
"
https://medium.com/@angrydead/my-dealings-with-the-cloud-have-been-far-from-fun-eaaeebb137cc?source=search_post---------101,"Sign in
There are currently no responses for this story.
Be the first to respond.
Brian Thompson
Apr 27, 2019·1 min read
Andrew Leonard
My dealings with the cloud have been far from fun. The whole idea of “leasing” out development environments, testing environments, and even production instances to a third-party vendor seems like a huge risk and crossing of boundaries when dealing with ownership of intellectual property. I’m not an intellectual property lawyer, but I’m sure there are loopholes there.
I’m all for the idea of the “cloud”, but think that companies should be reliant on internal solutions rather than looking for a one-click solution (which isn’t ever the case, anyway) just to cut costs. There’s just too much risk there IMO.
See all (5)
1 
1 clap
1 
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@CloudQBnet/protecting-your-saas-data-460eef7aefaa?source=search_post---------102,"Sign in
There are currently no responses for this story.
Be the first to respond.
Cloud Quarterback
Jan 10, 2017·6 min read
This article was originally published on the Cloud Quarterback blog. Click here to see the original and access bonus content with it.
It is remarkable how many executives, IT departments and users fail to take data protection seriously. It’s especially troubling when you consider that for most SaaS businesses, the data they create, use and manipulate is their business, yet they don’t protect it vigorously.
Unfortunately, however, even companies that make data protection a priority aren’t immune to data loss. Back in May of this year, a Salesforce instance came down for 12 unplanned hours because of a database failure. Three and half hours of customer data was completely lost.
Keep in mind that Salesforce is a massive company with strong IT processes, but even they were hobbled by unexpected events. If they weren’t already taking steps to preserve their data, the situation could have been a lot worse.
According to an IDG Research report, “95% of organizations believe their SaaS provider can/will easily restore their lost data.” They don’t personally concern themselves with data protection, even though 58% of them had experienced some type of data loss in the previous year.
As a SaaS business, your data (and the data of your customers) is your responsibility. In many cases, you don’t need in-house IT. Outsourcing your security needs can be smart, especially for a growing company. But it’s imperative that you ensure your security provider is protecting you from potential threats.
Data protection is critical for SaaS businesses. Download our free guide to data protection methods you should be using.
It’s estimated that nearly 30,000 websites are infected with malware every day. We know about the high profile cases, like Target, JP Morgan Chase, and Adobe, but they affect small businesses too.
As soon as your SaaS gains a little traction, hackers will put you in their sights. Data from Symantec’s 2016 Internet Security Threat Report tells us that small businesses are becoming common targets for hackers.
Source: smallbiztrends.com
In most cases, cyber-attacks are hunting for financial data, but they also capture general user data if they feel it can be used for financial gain. Just because you don’t have credit card numbers doesn’t mean you’re safe.
Furthermore, software services become juicier targets to hackers (outsiders with malicious intents) as they get larger. Even though most SaaS businesses implement security measures as they become data-heavy, hackers are generally interested in disrupting companies for the highest reward.
So as your company grows, the likelihood of an outside attack increases. You need a data protection plan to cover that target on your head.
Preserving your data is important, but a modern, efficient business uses tools to restore their data directly back into the application with perfect accuracy and zero manual effort.
You shouldn’t just recover from data loss. You should be able to put it back just the way it was at any time. Otherwise you’ll spend hours manually recreating labels, file structures, and inputting settings.
The interval you set for restore periods is up to you. It depends on the type of data you collect, its importance to your business, and what your customers expect.
In many cases, data destruction, breaches, or leaks can be attributed to someone inside your organization. Perhaps you have a malicious employee who sends information to a competitor. Perhaps a team member accesses privileged data for personal reasons. Or perhaps someone with your company made a simple, benign error that exposed or destroyed your data.
An Intel Security study found that 43% of all data loss can be attributed to insiders. “But those could be accidents,” you might argue. Unfortunately, half of those cases were found to be intentional. Bloomberg Law discovered that only 35% of companies admit to appropriately monitoring employee behavior. Only 30% admit to doing enough when it comes to monitoring their vendors. Whether it’s accidental or intentional, it’s still a security incident.
It doesn’t matter why it happened. It’s important to protect your data from the people who access it the most — your employees. Strong security measures and data processes will help you restore your information and provide warning signs when something is compromised so you can take action.
A data sync error can occur when you import data from one device or link a device with an application. You may inadvertently overwrite data that you didn’t expect to, or cause a corruption in the database.
While data sync errors are user-driven, their nature makes them easy to roll back, provided protection measures are put in place. You should always have (and give your users) options to restore major changes.
You’re probably taking advantage of the cloud. In fact, about half of all B2B and customer-facing apps are using cloud-based services for data storage, CRM solutions, and security.
If you’re using another SaaS service to manager your data (for instance, you might be hosting through Amazon Web Services), it can be tough for them to distinguish your actions from malicious attacks. If a request (like “delete these folders”) appears to be legitimate, your provider is obligated to carry out the task.
Here’s an example that happens every day. Many businesses use Google Apps to coordinate with their employees. Google charges by the number of employees who use the application. When an employee leaves the company, the business deletes the user so they aren’t charged an additional slot for the replacement. If the user account is deleted before the data (file, documents, emails, etc.) can be assigned to a new user, the data is gone forever. There was no malicious intent here, but the data is still inaccessible.
In some cases, your vendors have security protections to identify when a request doesn’t seem valid. But they definitely can’t determine when a mistake is made due to operator error. This is called user-driven loss. Even if your vendors want to protect you from your own errors, they fundamentally can’t.
Remember the infamous Target hack? 40 million credit cards were compromised, as was the personal information of 70 million customers. Malicious code was added to the point-of-sale terminals, sending the compromised data to Russia.
The hackers invaded Target by way of an HVAC company in Pennsylvania. The HVAC company had access to Target’s vendor network. The hackers used the HVAC business’ credentials to get themselves into the Target network and make their way to the payment system.
In this case, a few mistakes were made. First, why did the HVAC company need network access? Second, why wasn’t the payment system (with sensitive financial data) separated from the rest of the network? Third, why didn’t the HVAC company have protections in place? They may have felt safe because their data wasn’t valuable (who cares how often a refrigeration gasket gets changed?), but their security flaws were just one step in the chain to a bigger fish.
Security incidents can come from all over. They can come from access points you specifically built for other purposes (like the HVAC company’s access credentials). There are likely flaws in your network that your engineers haven’t considered.
Compliance is important
If your industry (or the industry of your customers) is subject to any regulatory compliances, you’ll need to protect your data with the right certifications.
For instance, there are pages of regulations in regards to maintaining and transporting electronic medical records. If you aren’t following the law, you (or your customers) may be accountable for each instance of mishandled data.
At any given time, industry auditors or your customers may want documentation or proof that you’re in compliance. Make sure you have something to offer.
If you know you need security but you’re unsure where to start, download our free guide on security measures you should be taking.
You need a structured security strategy — something that identifies threats and proposes solutions. Your strategy should analyze likely security flaws and identify weak points in your network (like the HVAC company I mentioned before and their unnecessary access).
You could employ an in-house security team to implement a bevy of security features, but this would be costly. You could task your team members to prioritize your security concerns, but this takes them away from growing your business.
As a growing SaaS, it make sense to outsource your security needs to another company. Cloud Quarterback’s cyber security-as-a-service is what you need.
Virtual Information Security Team
1 
1 
1 
Virtual Information Security Team
"
https://medium.com/@simplyitcloud/service-relationships-the-way-to-co-create-and-collaborate-with-your-customers-in-a-digital-era-87e07c5a6220?source=search_post---------103,"Sign in
There are currently no responses for this story.
Be the first to respond.
SimplyIT.cloud
Mar 22, 2019·3 min read
Nowadays many good internal and external service providers recognize that it is necessary not only to provide services but also to cooperate with their customers and vendors to create better value. Basically, it is about the co-creation of value between two or more units or organizations. This cooperation between service providers and their customers is called service relationship.
Service relationship is about the activities between a service provider and a service consumer to ensure continual co-creation of value.
It is necessary to recognize that also a service provider needs to act as a consumer because it is necessary to obtain and configure service components from internal and external teams enabling the service provider to create the service. So, service relationship needs to be managed on both sides of the service provider. With the consumers of the service as well with the vendors of the service components.
This topic is also briefly described within the ITIL 4 best practice. The definition of the service relationship is defined as: “A cooperation between a service provider and service consumer. Service relationships include service provision, service consumption and service relationship management.” ITIL 4 also defines service relationship model which shows interactions between organizations.
In the reality, the concept of service relationships is much more complicated than the picture illustrates. For example, a facility organization can rent office space to an IT organization as well as to the financial organization supported by the IT organization. IT organization can also provide IT services to the facility organization. Interactions between organizations are more likely a network or “ecosystem” than the simple sequence of interactions between organizations.
In the era of digital transformation, all service providers need to become digital enablers for their consumers. The responsibility of the consumer is to obtain and configure service components from internal and external service providers to be able to create, offer and provide their services. Most consumers believe that it is necessary just to obtain and configure service components from their service providers and they will automatically get the value from these service components. But co-creation of value is the continual process of coordination between the consumer and their service providers, not just a one-time activity to agree on the service contract. It is necessary to manage the service relationship during the service provisioning and consumption all the time.
From the consumer viewpoint, the necessary basis for successful management of the service relationships is to define and manage the following three topics:
This service relationship approach forces to extend the scope of traditional service management practices and processes like relationship management, supplier management, service catalogue management, service level management, continual improvement and many others. We discussed this new approach to multivendor service management in this whitepaper.
The shift from processes and governance to value requires a change of mindset and personal attitude that is not as easy and takes time. To support this shift even more, it is also necessary to use efficient (software) tools built and focused more on the service (value) aspect than on the process aspect like conventional Service Management / ITSM tools do.
Author: Miroslav Hlohovský
Consultant and Trainer who helps companies to deliver better value to their customers. Head of Supervisory Board of itSMF Slovakia.
Simplyit.cloud is a cloud-based service helping you to manage your service catalogue, vendors and govern related tasks.
Simplyit.cloud is a cloud-based service helping you to manage your service catalogue, vendors and govern related tasks.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/athletigen-engineering/aws-lambda-a-new-generation-of-cloud-compute-services-2db9b45f0ffa?source=search_post---------104,"There are currently no responses for this story.
Be the first to respond.
We have been building an internal tool to manage and troubleshoot our product logistics pipeline. We embarked on this task with a small team and a limited amount of time to become feature complete. While building it we wanted to specifically consider automating many repetitive administrative tasks/chores. Our goal was to develop a product that was not only effective, but also low-cost.
This is where AWS Lambda shines. AWS Lambda is an event-driven compute service by Amazon; it can deploy and run your code in response to events. No provisioning or deployments are required. I like the idea of an event-driven programming model as it emphasizes decoupling between the sender and receiver systems/modules, eventually leading to a microservices driven architecture. I created a processing pipeline for genotype data at Athletigen Inc. to take this service for a test drive.
Allow me to motivate the discussion with some practical use cases for AWS Lambda:
If you’re planning on using Lambda for event handling within the AWS world, your life will be easier. Although there are other services, like Webtasks, which will not constraint you to Amazon as your Cloud provider.
Our current use of Lambda involves transferring data between two services on a daily basis. Our DNA kit sales are managed by an external shop service, so we needed a way to automatically bring in the external orders to our internal system and alert the fulfillment lab (external) via email. Using AWS Lambda and SES made this solution a breeze to maintain.
Once you have a function ready, I suggest using something like Apex Serverless to deploy your Lambda function with almost no effort. Apex is a great tool written to make Lambda deployments and management more usable than what AWS provides out of the box. You can do dry runs, rollback etc. A blog post from the creator of Apex is a great read to get the background and introduction to the features of Apex.
After having deployed a couple of my own Lambda functions and there are some pros and cons of using the service which I want to highlight.
Lambda has an indefinitely free tier even after your initial free 12 months tier from Amazon which includes 1 Million free requests and 400,000 GB-seconds of compute time per month.
If we compare the cheapest options in EC2 and Lambda to run a 3 min job on a daily basis, EC2 will cost approximately 6.28$/Month whereas Lambda costs 0.02$/month. This is because Lambda pricing is per execution request and EC2 pricing based on % utilization and uptime of the instance. To read more detailed calculation refer to [1], [2], [3].
Here are some useful links related to this server less ecosystem and AWS Lambda.
PS: Thanks to Kate and Martin for reviewing and helping me with this article.
same description that will be used in footers, search…
same description that will be used in footers, search results and other places
Written by
Data @SnapTravel
same description that will be used in footers, search results and other places
"
https://medium.com/@fredriccliver/service-ts-171-post-http-localhost-5001-myendpoint-500-internal-server-error-68476e1af825?source=search_post---------105,"Sign in
There are currently no responses for this story.
Be the first to respond.
Fredric Cliver
Sep 9, 2020·2 min read
I requested my cloud functions from the front end. But the request was failed with this error.
And my emulator console
stackoverflow.com
Cloud Functions .onCall method only support POST request.
Majored in Physics, self-taught and worked in the IT industry as a Dev/Design/Planning for 11 years. And I had run my Startup for 3 years. I fancy a ☔️ 🇬🇧
See all (102)

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
Majored in Physics, self-taught and worked in the IT industry as a Dev/Design/Planning for 11 years. And I had run my Startup for 3 years. I fancy a ☔️ 🇬🇧
About
Write
Help
Legal
Get the Medium app
"
https://insights.calls9.com/digital-cloud-intranet-part-2-effective-knowledge-management-teams-are-engaged-and-happy-e96f35e7d565?source=search_post---------106,"If you want to know what’s possible with digital then our Digital Transformation Audit is for you. We’ll take you through a process that helps us quickly understand where digital can have the biggest impact for your business.
Whether you’re a start-up or an established professional services business we’re experts in getting under the skin of the problem you are trying to solve and delivering products and services that drive growth and reduce inefficiencies.
Keep up-to-date with digital transformation and how it impacts your business. Here you’ll find our expert insights, news updates, white papers and events.
﻿Benchmark your digital maturity and identify where digital can help you improve your business.
﻿Websites ﻿• Apps ﻿• Customer Portals ﻿• Personalisation ﻿• 360° Customer View
Strategic consultancy and technology solutions to improve your business. 
﻿﻿Our team embeds with your delivery team to augment your existing capabilities.
﻿﻿﻿Knowledge Sharing ﻿• Internal Communication ﻿• Intranets ﻿• Process Automation ﻿• Workplace Analytics
﻿A fully managed service where Calls9 leads on all aspects of your digital transformation.
﻿﻿﻿Launch Digital Services • E-commerce ﻿• Marketplaces ﻿• Networks ﻿• Dynamic Pricing 
Strategic consultancy and technology solutions to improve your business.
"
https://medium.com/@HRBossAsia/making-our-mark-in-the-cloud-988884a4a7dd?source=search_post---------107,"Sign in
There are currently no responses for this story.
Be the first to respond.
HRBoss
Dec 19, 2014·4 min read
HRBoss Friday Internal Newsletter
HRBoss Shortlisted For Four Awards At The 2014–15 Cloud Awards
Click to read the full press release. Let’s all share this news with the World!
On behalf of our Captain, Bernie Schiemer, & the rest of the HRBoss crew, welcome aboard! We look forward to working with you and making your HRBoss experience the very best…hold on tight and prepare to set sail!
Tran Minh | Senior Programmer Analyst | Vietnam
Connect with us on LinkedIn, follow us on Twitter or be our friends on Facebook!
Thanks Tannia & Serene for the update!~
If You Want to Succeed, Here Are 5 Things You Need to Do Differently
Gartner 2014 Magic Quadrant for Business Intelligence and Business Analytics
13 insightful HR insiders that you should be following on Twitter
Introverts: Know Your Strengths, And You Can Flourish At Work, Too
HR Technology Trends in 2015 << A must-read. Definitely got us laughing.
Friday Recruiting Brief #18
Click here if you can’t view the video above.
SGL Group | Teresa Wu | HR Director China
Great Job Pak-san & Luwei for this!
“Glocal” HR SaaS
2. Describe yourself in 2 words, the words have to begin with the first letter of your name.
Joyful, Judicious Jerry
3. If money was no object, what would you do?
Soldier
4.Take a picture of a special item on your work desk — what’s its significance?
5. Share with us a photo of yourself.
But guess what we found…
This is Handsome Jerry, 4 years ago working as a developer guy in UK.
Shoutout to Luwei & Jerry for sharing!~
#LauPaSat #SiliconSingapore
Lunch with the Devs #LauPaSat #SiliconSingapore
#Nhu #Tannia #PrettyLadies #Vietnam
#WhySoSeriousMinhLe? #VietnamOffice
Bernie-san opened his own bar and restaurant in HCM and we didn’t get the invite?!
Asia's fastest growing tech startup specialising in HR & Recruiting software based in Singapore, Japan, Vietnam, Indonesia, China, Malaysia, Hong Kong & beyond.
See all (455)
Asia's fastest growing tech startup specialising in HR & Recruiting software based in Singapore, Japan, Vietnam, Indonesia, China, Malaysia, Hong Kong & beyond.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@Unicorn_H2020/an-internal-view-on-unicorn-orchestrator-200ba9ffe254?source=search_post---------108,"Sign in
There are currently no responses for this story.
Be the first to respond.
Unicorn
Jun 6, 2018·5 min read
Unicorn is a novel framework that helps SMEs and START-UPS to adopt cloud computing for the deployment of services. Developers can focus on developing their services using the suggested development paradigm and let Unicorn Orchestrator handle elasticity, security, resilience and performance across heterogeneous clouds, such as public or private enterprise clouds, or even hybrid clouds.
DOWNLOAD this blog post HERE!
In this blog, we present how Unicorn provides multi-cloud application orchestration and though dynamic allocation of cloud resources in order to help administrators minimise operating expenses, without sacrificing the QoS of their application. Unicorn’s policy-aware, multi-cloud orchestrator combines resource management, containerized services deployment over microVMs, and constant policy enforcement for dynamically managing cloud resources.
Unicorn orchestration capabilities are based on resource abstraction and interoperability, thus allowing heterogeneous cloud management and the support for inter-cloud connectivity allow the deployment of applications even in aggregated and hybrid cloud scenarios. In other words, Unicorn policy-aware orchestrator allows the deployment and management of microservice based applications on top of multi-cloud execution environments. In specific, Unicorn’s Orchestration mechanism offers:
This orchestrator is the main outcome of Work Package 3 (WP3) and it is composed of a Multi-Cloud Resource Manager, the Unicorn Cloud Orchestrator, a Policy Engine and finally an Optimization Manager.
Figure 1: High Level view of Unicorn Orchestrator
Infrastructure in computing refers to resources which can be virtual or physical and include computing, storage and networking resources. In the scope of Unicorn, we consider the usage of virtualized resources that are offered for lease by the infrastructure provider and can be used by the service developers. Moreover, as the focus of the project is towards the deployment of scalable services towards a multi-cloud environment, we need to invest on technologies that can enable the easy, fast and secure deployment over virtualized infrastructure. To achieve this, Unicorn is suggesting a combination of containerized applications paradigm and usage of virtualized multi-cloud infrastructure, and as depicted in the figure below, we enrich this combination with a set of agents that are used by the orchestrator.
Figure 2: Representation of a Microservice VM as part of Unicorn
With Unicorn Orchestrator, multi-cloud deployment and orchestration capabilities are provided in order to support the vision of Unicorn for containerized execution environments that addresses data portability and interoperability issues. In addition, cloud applications should be highly scalable and adaptable during runtime. Therefore, it should be possible to retrieve real-time measurements of resource consumption, and part of our work done in the project is to provide the means to realize cloud application monitoring, auto-scaling and management in multi-cloud execution environments. The way we achieve this in Unicorn is depicted in the figure below.
Figure 3:Representation of detailed architecture of Policy Engine (Master Node and micro-service node)
The Cloud Orchestrator and the Multi-Cloud Resource Manager are actually used to orchestrate the deployment and management of deployment artefacts on multiple IaaS resources. The deployment artefacts are described in detail as they are needed for each service defined in the Unicorn compose file of each application. When an application is deployed, the service discovery agent of each microservice VM is registering itself to the Service Discovery Server and the Cloud Orchestrator is then using dedicated agents to talk and manage each VM.
The Policy Engine and the Optimization Manager allow the constant scaling and the optimal deployment configuration of the microservice based on predefined placement constraints and the facts provided by the monitoring mechanism of Unicorn. This monitoring information is retrieved using Prometheus monitoring system that collects the data from the monitoring agents.
DOWNLOAD this blog post HERE!
UNICORN project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant agreement No 731846.
A framework for multi-cloud software fostering cloud technologies uptake from digital SMEs and startups. It is a EU's H2020 funded Project, GA No 731846
A framework for multi-cloud software fostering cloud technologies uptake from digital SMEs and startups. It is a EU's H2020 funded Project, GA No 731846
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@dockerturtle/2019-networking-secrets-in-google-cloud-8284217c3a66?source=search_post---------109,"Sign in
There are currently no responses for this story.
Be the first to respond.
Docker Turtle
Jun 20, 2019·1 min read
Containers on Cloud
Containers on Cloud
"
https://medium.com/@runtastic/internal-s3-storage-for-media-files-on-top-of-ceph-storage-technology-135f9fda3403?source=search_post---------110,"Sign in
There are currently no responses for this story.
Be the first to respond.
adidas Runtastic
Dec 7, 2017·1 min read
As a member of the Infrastructure team, it’s always exciting to work with new technologies in our company. This year we had a request from our internal Media team to create a storage solution for them within our office environment.
Our Media team created around 30TB of video, audio and images in 2016 and the amount of data is growing faster and faster. There are a lot of 4G files because that is the maximum file size of many video cameras.
The requirements from the Media team:
The requirements from the Infrastructure team:
Based on the requirements listed above, we started to evaluate which technology would be the best fit for us.
Read full story on the adidas Runtastic blog
Interesting insights into everything tech at Runtastic and what we are currently working on in the way of IT.
Interesting insights into everything tech at Runtastic and what we are currently working on in the way of IT.
"
https://medium.com/excraftexchange/excrafts-unique-internal-architecture-4bbf245bbd56?source=search_post---------111,"There are currently no responses for this story.
Be the first to respond.
Our last article External Risk Management in ExCraft’s Architecture discussed how ExCraft analyzes and categorizes threats coming from outside of our exchange. The write-up previous to that in a section titled “What does ExCraft do differently” glossed over the internal makeup that makes ExCraft state-of-the-art. Before continuing, please review these posts on Medium if you have not read them already.
Beyond hacking, CEXs have suffered scalability issues, performance outages, and long delays in implementing new trading assets; this is because most exchanges were built hastily as a single service, meaning that there is less protection between different layers. The monolithic design of traditional CEXs result in poor performance that is difficult to scale and lacks security between operational strata. Minute exploits in a server’s code could permit an attacker entry to compromise an exchange built using this outline, which has happened repeatedly. Relying on a firewall around your exchange should not be sufficient enough to ensure user confidence. ExCraft believes each exchange service must be isolated, hardened, and fine-tuned.
Most existing exchanges are coupled to physical resources relying on specific technologies in colocation facilities and inherently lack scalability, as well as the ability to pivot into better solutions. Throughout this discourse, we will delve into four main areas of interest internally that not only allow ExCraft to manage risk better than major exchanges, but also make us the most scalable, hybrid exchange available today.
Ensuring protection of our users’ data and information is of the highest priority. ExCraft separates our internal network from the core functions of our exchange by using Google’s Compute Engine and Cloud Platform; this serves two purposes: It allows us to make use of all the additional tools that come with Google’s Cloud services and limits access to ExCraft operational staff departments. Despite many people’s opinion on Google, their services meet the highest levels of regulatory compliance across all Cloud technologies. Like ExCraft, they use independent auditing services to meet standards requirements in security and privacy including ISO 27001, ISO 27017, ISO 27018, SOC 2, and SOC 3.
ISO27001 — a security standard that outlines and provides the requirements for an information security management system (ISMS). It specifies a set of best practices and details a list of security controls concerning the management of information risks.
ISO27017 — gives guidelines for information security controls applicable to the provision and use of cloud services by providing:
Additional implementation guidance for relevant controls specified ISO27002
Additional controls with implementation guidance that specifically relate to cloud services
This standard provides controls and implementation guidance for both cloud service providers (like Google) and our cloud service customers.
ISO27018 — relates to the protection of personally identifiable information (PII), and as such, deals with one of the most critical components of the cloud — privacy. This standard is primarily focused on security controls for public-cloud service providers acting as PII processors. ISO 27018 works in two ways:
Builds off of existing ISO 27002 controls with specific items for cloud privacy
Provides completely new security controls for personal data
SOC 2 — evaluates an organization’s information systems relevant to security, availability, processing integrity, and confidentiality or privacy.
SOC 3 — a report based on the existing SysTrust and WebTrust principles.
In addition to having separated exchange and internal operation functions, ExCraft has added even more security through CloudFlare’s services; this gives us a robust firewall, equipped with packet inspection, intrusion detection, and other preventative systems. Even with separated resources and traditional site security measures in place, ExCraft still uses a designated monitoring and response team to ensure the exchange continues to run smoothly.
ExCraft is what we call a “Cloud-native” exchange. Within the ExCraft Cloud, we have separated core exchange functions into secure microservice containers that loosely couple Google’s Cloud. Docker containers with custom coded-solutions in several different computer languages (Go, Python, C++, and C- -) are connected using an Istio Service Mesh by Google Remote Procedural Call (gRPC) and orchestrated through Kubernetes. Each microservice scales with high availability, clustering to achieve low latency, and high throughput all at the lowest possible management costs
By deploying these containers as a Google Virtual Private Cloud (VPC), ExCraft attains greater scalability than existing CEXs. Google’s Compute Engine allows load balancing for resource distribution across several regions as well as smart autoscaling, which can divert spikes in traffic. Multiload balancing and smart autoscaling also tie into the preventative measures we have available against DoS attacks; this means ExCraft can handle over 10 million requests per second without any preparation (prewarming).
On top of scalability and throughput, Google’s VPC makes sure that data is stored securely with privacy safeguards, and that communication between our services, as well as users, are assured privately over the internet. All transmitted information not controlled by the Google platform requires encryption, authentication and is checked for integrity at one or more layers to guarantee that data sent reaches its destination unaltered.
Finally, Google’s Cloud services make it easier to set a Disaster Recovery plan based on recovery time objectives (RTO) and recovery point objectives (RPO), thus significantly reducing even more costs in comparison to traditional recovery plans. Non-traditional Disaster Recovery plans coupled with our DevOps team working around the clock on patches, upgrades, and new functionality (see our Roadmap) means that ExCraft has prepared for the worst case scenario.
Operations are the most crucial aspect of an exchange, which we have the most control over. If you remember the table with all of the lost funds due to exchange hacks and exploits mentioned in An Introduction to the Security Behind ExCraft, you would have noticed how many have succumbed to not having enough security measures built amongst their employees. For example, look at the number of susceptible cold wallet hacks; this is a critical operational problem. ExCraft keeps our multi-sig cold wallet with the minimum liquidity necessary (<5%) for trading while promoting highest safety. All measures have been taken to prevent these and any other funds from security breaches.
The ExCraft team has received mandatory training on security practices and handling personally identifiable information (PII) for out KYC processes. We take pride that our staff is aware of the most common type of types of phishing attempts such as fake Telegram groups, Twitter giveaways, and QR code malware. We have also taken measures to acquire as many domains similar to our official ExCraft.com website to prevent potential phishing attacks. ExCraft is very selective about the levels of access given to our team members. Mandatory 2FA, password complexity, rotation, lockouts, and audit reviews on all critical services are implemented across the board internally.
To ensure compliance, which we alluded to in our external risk management article, ExCraft will routinely go through scheduled and unscheduled audits with vulnerability testing and penetration testing. Key metrics and requirements will be automated as they are with most major exchanges. With these, we hope to achieve the highest levels of transparency with our community.
In promoting our financial security, customer funds are maintained separately from operational funds. Operational reserves are kept in full, meaning no borrowing. Rest well knowing we do not margin trade with our users’ funds.
We hope that this has been an educational and worthwhile read for our community. The ExCraft team wants to build trust with our growing community and highlight what makes us stand out from other major exchanges. Continue to follow us on our social media channels for more content like this and other updates, which will be coming out shortly!
ExCraft Exchange (Website)：https://www.excraft.com
ExCraft Telegram (English)：https://t.me/ExCraftExchangeENG
ExCraft Telegram (Chinese)：https://t.me/ExCraftExchangeCN
ExCraft Telegram (Korean)：https://t.me/ExCraftExchangeKR
Twitter|Facebook|Steemit|Reddit|Mastodon|Naver
ExCraft is a cloud-native cryptocurrency exchange based in…
ExCraft is a cloud-native cryptocurrency exchange based in Hong Kong that implements microservices to achieve a highly secure and high-performance architecture.
Written by
ExCraft is a cloud-native cryptocurrency exchange based in Hong Kong that implements microservices to achieve a highly secure and high-performance architecture.
ExCraft is a cloud-native cryptocurrency exchange based in Hong Kong that implements microservices to achieve a highly secure and high-performance architecture.
"
https://medium.com/@walletapi.cloud/smart-vending-live-data-for-optimal-returns-582bc7f9e553?source=search_post---------112,"Sign in
There are currently no responses for this story.
Be the first to respond.
Cloud Wallet
Aug 5, 2019·3 min read
It is hard to believe but coffee vending machines have been around for almost 75 years. Although modern machines are superior to their predecessors, the business model has remained practically the same: to provide quick and affordable coffee in high traffic locations 24/7.
To keep up with demand, the machines need to be continuously operational, with as little downtime as possible. Considering that supplies run out at different rates, refilling them regularly can result in some machines being unstocked and others being refilled earlier than necessary.
In order to not lose out on potential sales and keep the delivery costs down, it is important to keep track of what needs refilling and when. Integrating a vending machine with our Cloud Wallet platform allows assigning individual accounts to each inventory item, no matter what it is measured in.
Separate wallets for coffee beans, milk, water, and paper cups can monitor expenses and provide you with accurate information about the level of inventory at any point in time. Another wallet is created for collecting payments from sales. You can see how it all works using our interactive simulation of a Cloud Wallet integrated coffee vending machine.
Initially, the machine is pre-loaded with all the necessary ingredients. Each available drink type has its own price and uses up a different amount of ingredients. Every time a customer buys a cup of coffee, the cash wallet increases in value, and in exchange, the relevant ingredient wallets are expensed during the brewing process.
Furthermore, the whole transaction will be completed only if each operation in it can be performed. Should any single operation be impossible, the whole transaction will fail without affecting any wallet. This, for example, makes it impossible to brew anything except black coffee if the machine runs out of milk.
The inventory wallets can store both the physical quantity of an ingredient and its monetary value. When the machine is refilled, the supplier’s wallet will be credited for the value of the goods he restocked. If the machine used up 1000 ml of milk worth 2 USD, then, when restocked, the transfer will be recorded to the supplier’s wallet from its revenue wallet.
Planning and routing deliveries based on actual inventory levels minimizes the time vending machines spent unstocked which maximizes the number of potential sales. Restocking wise, delivering only the necessary ingredients lowers the cost of transportation and increases capacity by avoiding unneeded trips.
Monitoring all transactions with dedicated wallets increases transparency and allows us to evaluate the realistic financial performance of an individual vending machine. You can easily monitor its financial metrics and compare it to others to make a well-informed decision whether the machine should be moved to a better location.
No matter how many vending machines or IoT devices you are managing, with up to date information at your fingertips, you can easily optimize your operation with a quick and easy integration with Cloud Wallet. Level up your business today!
You can play with the Smart Coffee Machine simulation here. In case you have any questions, feel free to send them to marketing@walletapi.cloud
Cloud Wallet enables closed-loop payments between IoT devices and facilitates IoT adoption worldwide.
Cloud Wallet enables closed-loop payments between IoT devices and facilitates IoT adoption worldwide.
"
https://medium.com/@shubhamc183/hi-carlos-lets-say-that-some-pods-will-use-the-nginx-for-internal-communication-within-the-daa397ff7beb?source=search_post---------113,"Sign in
There are currently no responses for this story.
Be the first to respond.
Shubham Choudhary
May 8, 2020·1 min read
Carlos Eduardo
Hi Carlos, let’s say that some pods will use the NGINX for internal communication within the cluster and we don’t want a proxy layer for these requests which are from “ within the cluster” so please suggest how will we whitelist the traffic which is coming from the internal cluster, at the same time whatever traffic is coming from outside should be landed on proxy then Nginx which is happening fine correctly.
I think just creating a service on top NGINX and not creating INGRESS on top of it not a good solution since anyone who has access to the cluster can do port-forwarding to that service!
Software & DevOps Engineer, OSS Enthusiast! @ShareChat
See all (131)
1
1
Software & DevOps Engineer, OSS Enthusiast! @ShareChat
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@SimScale/computational-wind-engineering-with-cfd-in-the-cloud-c9d47f7ba6da?source=search_post---------114,"Sign in
There are currently no responses for this story.
Be the first to respond.
SimScale
Mar 12, 2019·6 min read
When engineers think of computational fluid dynamics (CFD), some typical use cases that spring to mind are internal flow (e.g., piping) or external flow (e.g., aircraft aerodynamics). However, an area that has gained a significant increase in CFD usage is computational wind engineering (CWE), or wind engineering, due to the complexities involved and the ever-increasing availability of computational power. From wind comfort assessment to air exchange effectiveness and structural loading due to wind effects, such studies are normally done in expensive wind tunnels and rarely online. Furthermore, these practical studies are only done for very special cases and even so, in a limited capacity. Today, due to the aforementioned increase in the availability of cheap substantial computing power, more of such studies can be worked on for additional cases.
This comprehensive article in particular deals with incompressible external flow of urban environments and is a beginner’s guide for performing this type of wind engineering simulation. Some aspects outlined in this post can be used for other types of CWE simulations as well, but the methods listed may not be optimal for all types of wind simulation.
The end goal of a wind simulation is the first thing one should consider, and layout, before embarking on the task. For incompressible external flow, is wind comfort of a particular area to be analyzed? Deduction of peak wind loads on specific structures? Cross-ventilation in order to determine the ventilation performance for the internals of a building? The establishment of what exactly needs to be analyzed is the first step towards ensuring that the wind engineering project can be done as efficiently as possible.
For example, if you were planning to determine the pedestrian wind comfort in a simple urban environment with no change in ground elevation as seen in Figure 1, there would be no need to model the internal layout of the geometry even if it was available. The inclusion of the internals would not only not affect the final result, but would unnecessarily increase the computational cost due to the extra internal mesh needs.
Of course, if ventilation needs to also be analyzed, then the internals will need to be included and meshed. Yet, now the surrounding geometry loses importance as the inclusion of which would not adversely affect the results internally. A question then arises; when should details of the geometry (internal or external) be included and how detailed do they need to be?
Spatial scales are the next important consideration to help determine what should or should not be modelled. Incorrect selection or non-inclusion of the correct spatial scale will result in simulations that may not be able to sufficiently represent the actual built environment. Shown in Figure 2 are the different types of spatial scales depicted by Blocken.
In an ideal world, one would be able to include all spatial scales for the simulation in order to replicate the real environment as closely as possible. Obviously, this is not only impossible but also extremely inefficient. As such, well-made assumptions must be used. Typically, environments in the micro environment and building scale are modelled and simulated with assumptions to the inputs into that environment. In the case of the microscale, the average wind speed from a specific direction would be assumed, while for the building scale, a simple velocity inlet at an opening with the wind velocity at that height would often be sufficient.
The usage of different spatial scales in one geometry only arises when the area of interest starts becoming increasingly complex. Say that the internal ventilation characteristics of a building need to be considered, but the building lies in the midst of large irregular structures. In this case, simply assuming an inlet velocity may not be sufficiently representative, and thus the microscale will also need to be modelled. In addition, this has the potential to increase the computational cost significantly.
Software and hardware considerations are of particular importance for wind engineering and CWE simulations. Due to the large scales and subsequently large complex meshes involved, particular attention needs to be paid towards the availability of computing power. Often, simulations are completed scaled down, therefore compensation needs to be made to ensure the effects are still properly represented at a reduced scale. Inherit software and hardware costs involved to perform computational wind engineering analyses can be costly and will inadvertently limit what cases can be simulated.
Today, the widespread availability of computing power means that full-scale simulations on complex environments can be done. SimScale exists as a prime example of this, where the availability of up to 96 computing cores and the ability to run simulations in parallel is of incredible benefit to anyone wishing to perform full-scale complex computational wind engineering simulations. The availability of such computing power in the cloud that can be accessed with a simple web browser further reduces the need to invest in costly infrastructure for fluid dynamics analysis. This benefit coupled with the available validation cases, as seen in Figure 3, for all types of environments for CWE simulations along with quick and accessible support makes the SimScale platform the ideal place for users of all skill levels to perform their simulations in.
As with all types of simulations, geometry preparation involves ensuring that the geometry to be simulated is clean, free of defects, watertight and to the correct scale. In computational wind engineering simulations, considerations of what to model are highly dependent on the end goal of the analysis. As mentioned earlier, ensuring that only the necessary geometries and details are modelled will help ensure that the computational cost is as low as possible, while reducing the workload needed to also maintain the quality of the geometry. For simulation on SimScale, it is recommended that the optimal file format is used. Typically, STL files tend to be less user-friendly compared to other formats like STEP due to the nature of STL being triangulated surfaces rather than actual faces. Regardless of format, the ability to select individual surfaces on the building or the floor is key as one wants to have absolute control over which surfaces need to be refined further in order to reduce computational cost.
The usage of computational wind engineering guidelines is key to completing accurate simulations as hassle-free as possible. However, there are a variety of guidelines that dictate different requirements and they all need to be considered carefully.
Some examples are the Architectural Institute of Japan (AIJ) guidelines for practical application [1], and the Building Construction Authority (BCA) GreenMark Technical Guide [2]. These guidelines, when followed together, should provide sufficient information regarding proper setup of CWE simulations and in effect, produce sufficiently accurate simulations.
References
Originally published at www.simscale.com on March 12, 2019.
A new approach to #CAE #FEM #CFD. Platform and community for cloud-based #engineering #simulation. Join the SimScale forum: https://forum.simscale.com/
See all (86)
A new approach to #CAE #FEM #CFD. Platform and community for cloud-based #engineering #simulation. Join the SimScale forum: https://forum.simscale.com/
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@srisys/an-overview-on-the-importance-of-cloud-application-development-cloud-application-development-88162b03a830?source=search_post---------115,"Sign in
There are currently no responses for this story.
Be the first to respond.
Raghava Kolanupaka
Feb 19, 2019·3 min read
Gone is the phase when companies spent a great deal of revenue in developing applications for purely internal operations. The contemporary phase belongs to cloud computing, a phenomenon that has significantly reshaped the landscape of application development platforms and among them, enterprises dealing with massive data sets are able to design apps in real time, in a cost-effective manner.
However, developing cloud applications seems a relatively new add-on priority for most enterprises, but those which have deployed it already are reaping its benefits fivefold.
Let’s take a peek at what makes cloud application development a rewarding exercise.
Trending Cloud Application Development Platforms
1. Amazon Web Services
2. Google Cloud Platform
3. Microsoft Azure
4. IBM Bluemix
5. Ali baba
These cloud application providers have been most popular as they are competitive and ensure quality service, in real-time. These platforms are considered as having a deep expertise in cloud application development platforms.
Importance of Cloud Applications in Developing Businesses
A recent survey by Merrill-Lynch says that cloud computing is going to emerge as a 160$ billion industry in the near future only through business applications alone. Some of the reasons are as follows:
More Productivity with Cloud Applications
Firms spent considerable amount of man hours on planning, setting-up and maintenance IT infrastructure. Dedicated teams invested time and effort on installation and management of software, backup and other IT problems arising on a continual basis.
With the evolution of cloud applications, the manual installation process is curbed and the same application can be used across geographies by stakeholders. Cloud applications are designed for the respective organization, saving a lot of time which can be used for more productive tasks.
For instance, using Microsoft Office was a time taking task previously and now with cloud storage facility, everything is saved in real-time. Almost every action in cloud computing is automated so that the workforce uses their time for more productive things.
Software Piracy Can be Terminated through Cloud Applications
Software piracy is a threat for the development and expansion of a business enterprise. This situation can be addressed when cloud computing is adopted widely across industries such as software companies, publishers and distributors.
Cloud Applications Offer Multiple Storage and Back-up Options
Once in time, a 2 GB hard disk in computers was considered the next big thing and today a computer for domestic usage consists no less than 1 Terra Byte! The future generation will not even know that floppy disks or drives existed because cloud computing will reign in soon and the time where USB drives will no longer be needed is not far in sight.
Cloud applications offer better, faster and most importantly a secure method, as one need not worry about the data stored on the multi-servers that can be integrated in your personal computer and mobiles if need be.
Even in-case of accidental damages to the device, you can re–synchronize the data and re-store the copy of your files effortlessly without any help of external physical devices. That’s why cloud computing is superior and safe contrary to the traditional forms of storage.
Cloud Applications Opens Up Innovative Career Options for the Next Gen
Information technology (IT) is an integral part of cloud computing/applications and with IT comes a varied scope of job vacancies for the next generation IT aspirants. All we need to do to avail these jobs is to learn and use cloud computing on a regular basis.
With IT, creations of new kinds of job designations in the field are going to emerge. More challenging and committed candidates who are keen to learn and lead the IT industry further will fit-in these new roles.
Make Life Much Easier With Cloud Applications
Technology has inevitably changed lives of masses; however, there are still a major proportion of people who don’t avail the benefits of technology for reasons best known to them. Very soon people will not require hard disk drives or memory cards for their computers and mobiles; they can save all the data on the cloud via internet.
In the near future, cloud computing applications will help all kinds of people from software professionals to online stores, medical facilities, government organizations, students, etc.
Students will be able to save, upload and share documents on cloud storage applications such as Google Drive. They will be able to share, download; take prints anytime and anywhere without physical devices such as USB drives.
All the IT complexities and business enterprise objectives will be addressed with the help of cloud based applications with agility, cost saving and flexibility.
Originally shared on https://www.srisys.com/blog/importance-of-cloud-application-development
Hi i'm a freelance web consultant currently working for http://pigeon.srisys.com/. This is based on Geo location technology for indoor positioning & navigation
Hi i'm a freelance web consultant currently working for http://pigeon.srisys.com/. This is based on Geo location technology for indoor positioning & navigation
"
https://medium.com/@k0a8t1o6/i-am-also-using-backblaze-online-backup-service-too-for-my-internal-ssd-and-2-external-hdd-96f33b4fe91b?source=search_post---------116,"Sign in
There are currently no responses for this story.
Be the first to respond.
Takashi Kato
May 31, 2016·1 min read
I am now using Backblaze after switching from using CrashPlan. It backs up an unlimited number of files for around $5 per month through a small application installed on your Mac or PC. It also offers encryption of your files and if you use your own encryption key even Backblaze themse…
Adam Karnacz
I am also using Backblaze online backup service too for my internal SSD and 2 external HDD. Even so, the cost is still $5 which is really amazing.
Keeping photo on cloud service is not safe enough unless joining the company pool doing data back up as their first priority business. You do not want to see for example Google is shutting down Photos service just like Google Reader again.
Get multiple copies of your data asap. But remember to manage them well otherwise they would become a tiny piece of data of the bunch.
Thank Adam for his sharing :)
The Pokémon guy who takes photography, writing entry and seeking for unseen view.
The Pokémon guy who takes photography, writing entry and seeking for unseen view.
"
https://medium.com/@liam.maskell/whilst-assets-can-be-stored-to-an-internal-server-there-can-be-a-number-of-issues-associated-with-591c7d16dd05?source=search_post---------117,"Sign in
There are currently no responses for this story.
Be the first to respond.
Liam Maskell
Sep 4, 2018·2 min read
I have an internal server — why do we need a Digital Asset Management System (DAM)?
Whilst assets can be stored to an internal server- there can be a number of issues associated with it:
Sharing: each individual or department may have their own way of storing assets, away from the internal server. This can lead to files being stored on individual machines or as email attachments, preventing company-wide access.
Accessibility: internal servers often require a network connection. With cloud-based DAM you can access your files from anywhere in the world.
Functionality: DAM systems have numerous tools that make managing your images far easier — which servers do not.
Whatever questions you have please reach me on 01242 654 000 or email liam.maskell@16i.co.uk
Sales professional with a passion for technology and business. Business Development Manager @ Quick By Design — www.quickbydesign.co.uk
Sales professional with a passion for technology and business. Business Development Manager @ Quick By Design — www.quickbydesign.co.uk
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@gonzalo.cloud/why-el-salvador-is-building-a-bitcoin-city-e4108dd61052?source=search_post---------118,"Sign in
There are currently no responses for this story.
Be the first to respond.
Gonzalo.cloud
Dec 4, 2021·4 min read
Spoiler alert: is to save their internal economy
You have heard the news: the country of “El Salvador” is going all-in with Bitcoin. His president Nayib Bukele plans to build a Bitcoin city at the base of the Conchaga Volcano.
In that location, geothermal energy can be used to power the rigs of computers that supposedly will be deployed to mine for Bitcoins.
"
https://medium.com/@brianlow/and-the-worst-of-cloud-storage-is-dropdead-guys-80feff5352c1?source=search_post---------119,"Sign in
There are currently no responses for this story.
Be the first to respond.
Brian Low
May 10, 2020·1 min read
Paul Alvarez
And the worst of cloud storage is dropdead guys…. Unusable UI and out of world pricing…. Though this is another topic another time..
The only other reason why a huge internal storage is favored is download of video on device independent of wifi of cloud accessibility.. diff countries diff telco charge data differently and can be surprisingly disparaging..
A phone is only as good as it’s battery last.. Samsung is especially bad not just with the bloatware which i hated among all android phones.. what a name for Bixsy it also has battery that sucks even phone is brand new..
enjoys read on life's musing and some tech. Not-yet-qualified to write life advice..
See all (49)
enjoys read on life's musing and some tech. Not-yet-qualified to write life advice..
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@corpulentchris/abstract-is-so-much-more-than-cloud-documents-though-f565a90b1e1c?source=search_post---------120,"Sign in
There are currently no responses for this story.
Be the first to respond.
Chris Kohanik
Jan 14, 2019·1 min read
Vlad Păloiu
Abstract is so much more than cloud documents though. It’s a mentality of working more like developers with commits and branches. Right now this is how we have multiple designers working on massive projects, with a clear indication of who is doing what and what has happened.
Our shop has moved to more of a sketch + abstract for internal and invision for client presentations/faster client alignment.
Honestly if XD got version control like abstract, it would be the thing that would have us consider moving over. Or if Abstract started supporting it.
Digital experience designer in #Yeg. My opinions are onions.
Digital experience designer in #Yeg. My opinions are onions.
"
https://medium.com/@musehick/beat-down-the-internal-curmudgeon-327bb1eae665?source=search_post---------121,"Sign in
There are currently no responses for this story.
Be the first to respond.
musehick
May 21, 2021·2 min read
You hit a certain age and part of you wonders whether you don’t like something based on its own merits, or by virtue of you having imperceptibly, at least in your own mind, having slipped into the age bracket that marks you as an old fucker.
You can be old and not have a mind that is attuned that way, but you have to be on guard against it. Is it as inescapable as physics though? As a ball will fall to…
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@codeit_llc/the-enterprise-cloud-platform-understand-security-mitigate-the-risks-58878e607516?source=search_post---------122,"Sign in
There are currently no responses for this story.
Be the first to respond.
CodeIT
May 2, 2018·8 min read
However, truth be told, cloud service providers can sometimes offer you greater security than your internal IT team. Security may be a challenge, but there are so many effective ways to address it.
The only thing required of you — partner up with the right cloud provider because that really matters a lot.
Security in the cloud environment is essential for businesses to compete and meet consumer demands. Security levels can be matched to those offered by onsite solutions if proper investments are made in terms of efforts and time. Trustful applications can be deployed which would isolate data of every company even if the environment is shared or comprises of multiple tenants. If encryption, virtual data centers, and virtual LAN technologies are leveraged, the offered security can even exceed the service levels of your in-house IT team. Your cloud provider also complies with stringent regulations, so their solutions can offer the maximum security possible.
Cloud technologies may be adopted in a private, public or hybrid environment. And the services can be delivered as SaaS, IaaS or PaaS. The security requirements are unique and must be addressed in a tailored sort of way, considering the overall cloud solution that is being deployed.
If you are thinking of moving a critical application to the cloud, security would be a concerning factor. In a traditional IT environment, security is easier to manage. Techniques such as authentication, authorization, access control and encryption can be used to ensure a highly secure environment. Whatever controls are put in place, there are laws accompanied by a comprehensive IT security policy and properly written procedures. Employees are informed and trained, and together, all of them manage security cohesively.
Even in a typical environment, some of the users, like clients and vendors, do fall outside of your control but they also access your organization network. However, techniques allow you to limit this access and gain complete visibility over their activities. Through simple procedures, you can decide which of your users can access a particular dataset or can perform a certain action.
The data that is stored outside the organizational network is exposed to some risks. Third party services may bypass security controls which your IT department has over in-house IT systems. Laws do allow the end users to specify where the data to be stored and processes. In general, if you do want to make changes, you would have to under the processes and systems of your provider. So for public, shared and multitenant environments, you should trust your provider and leave it to them to guard their data for you. But providers do limit liabilities for security lapses at times, which possess additional concerns.
Governing and monitoring applications and resources the cloud can indeed, be a challenge. To deal with the issue effectively, here is what you should address.
A secure cloud environment is not enough. Security should be robust and trustful while giving you ample control to manage and monitor your applications. Here are the most prevalent security requirements which you must address in an enterprise cloud environment.
Robust security implies a layered model instead of the typical perimeter-based approach toward security. This ensures that data is properly isolated even if the environment is shared among multiple tenants. Robust security is achievable by introducing content protection mechanisms at different layers such as the storage layer, virtual machine, and the database. The implemented mechanisms must cater to confidentiality and provide access control. Encryption, key management, log management and auditing can all be utilized in this aspect.
Trust can be achieved when you have confidence in the cloud environment integrity and can depend on the physical data centers, hardware, software, processes and the people. Your cloud provider should develop trust by offering reporting capabilities and application monitoring and control. Transparency must exist between your provider and you, especially where vulnerabilities are involved. Audit trails, automated notifications, alerts configuration, incident management and other similar should be offered so that you manage security by yourself.
Solutions that are based on these features also ascertain quality and reliability of the chosen cloud provider, allowing you to play an active role in managing and governing your cloud applications.
Cloud governance refers to utilities that let you monitor security, ensure compliance and ascertain that various KPIs (for performance or reliability) are being met. You should be able to use the tools provided exactly as if the cloud application was in your organizational environment, and should be able to perform similar tasks and activities.
Moreover, the offered tools should allow you take necessary action depending on the security information that you should automatically receive from your service provider. Common actions include shutting or disabling an application if it’s under attack. You may also have to compel the provider to tighten security procedures, especially if patches or updates aren’t being carried out in a timely manner.
Governance also takes into account risk management; this allows you to customize your costs for threat impacts that are most probable as well as for ones that have a lower chance of being met. Your providers should also address issues related to legal compliance, reimbursement, and other things.
Your cloud security provider will manage security and take steps to keep your data safe. However, there are several things that you can do on your part to protect your critical applications and virtual machines. Providers offer you a wide range of tools and services for security purposes, but you’ll probably have to implement various defensive mechanisms. Just like your cloud provider you’re also responsible for protecting your data, users, and networks.
Cloud isn’t insecure, but problems arise when the applications and networks aren’t configured for security. Amazon Web Services run in a highly secure environment, but when you are adopting these platforms, you also have to configure them correctly. And for this, you should fully understand the process. Here are some of the essential security measures that any cloud application should have.
Cloud services differ from each other and so what you may be responsible for varies with your chosen model and plan. For instance, if you opt for a SaaS provider, they would make sure that the application is completely protected and data is stored and transmitted in a secure manner. But on the other hand, if you go with something such as the Amazon EBS or the AWS Elastic Compute Cloud, you would be responsible for managing OS configuration, application management, and data protection. Should you choose something like Simple Storage Service or S2, you’d be responsible for data management, access control, and identity policies. You do get tools for data encryption, but still, you are responsible for protecting the entry points of your network. Amazon does, however, take the responsibility of application and OS management for S3.
So check with your chosen cloud provider and understand who is responsible and for what.
A research estimated that more than 30% databases in the open cloud environment can be accessed over the internet and another 93% don’t put any restrictions on outbound traffic. 9% of workloads also accept traffic from simply any IP address regardless of the port, whereas only the bastion hosts and the load balance should have internet access.
Don’t enable internet access if there isn’t a need. Also, don’t enable global permissions for any of your servers; administrators often make the mistake of doing this by using 0.0.0.0/0 in subnets that are public. Leaving SSH open is another common mistake; should you do this, you are actually allowing anyone to bypass the network firewall and gain direct access to the data if they know where the server is located that is.
Generally, cloud providers give you access to various identity management and access control tools, so make sure are utilizing them. You should know who can access your data and when, and you should also know who can access how much data. When you create identity policies and develop access control levels, provide limited privileges required for a particular task or role. Additional permissions should be granted temporarily whenever required. Set up your security groups such that the focus in the narrowest, and use a reference is for each of these groups when you can.
The next step is to protect your data using appropriate mechanisms. Control your encryption network whenever you can. Generally, you assign your cloud vendor the responsibility for providing key access, but generally, you should do what you can to protect your data. Cloud providers do a good job, but it’s your data, so you should have control over all encryption keys.
That being said, remember one thing, you’re making a whole lot of difference if you use strong encryption mechanisms. They are mostly failsafe so even if your data is accessed by an unauthorized individual, they still wouldn’t be able to read it.
Access keys can be exposed to source code repositories and public websites. However, they are one of the most sensitive pieces. Educate your team and employees to never, ever, leak these keys in any public space.
You should create unique keys for all external services while restricting access wherever and to whomever, you can. None of the keys should have broad permissions; this prevents them from being used to access sensitive data if they fall into the wrong hands.
Also, rotate the keys after 90 days at the most on a regular basis. If you delay this, you may be giving hackers sufficient time to intercept the keys and enter your environment.
In addition, make sure that you don’t use the root user account, even if you want to perform an administrative task. Instead, assign all the required privileges to a new user, and then use that user for all administrative work. As for the root account, you should lock it down, enable multi-factor authentication and limits it uses to only specific tasks.
Lastly, disable all user accounts that aren’t being used.
In-depth defense is important in a cloud environment. Even if one of the controls would fail, there would still be others that would ensure the security of your data, application, and network. Also enable multi-factor authentication because it increases security, and makes it harder for potential intruders to break into.
Enable logs and monitor them continuously for potential issues and unauthorized access attempts.
Still think securing cloud will be a challenge? Get in touch with us and we’ll put in all necessary measures for a safe and secure cloud environment.
Originally published at codeit.us on May 2, 2018.
We are CodeIT — a software development company with a crazy love for challenges. https://codeit.us
We are CodeIT — a software development company with a crazy love for challenges. https://codeit.us
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@mark.london/the-choice-between-internal-it-departments-and-msps-e7e545a600df?source=search_post---------123,"Sign in
There are currently no responses for this story.
Be the first to respond.
Mark London
Aug 5, 2019·2 min read
As businesses become more advanced, so does the technology used to run the daily operations, and the more complicated they become. This is where, as a business owner, you really need to decide, what is the right kind of IT for my business? Do you stick with your internal IT department or do you look into partnering with a managed services provider for IT Support? Let’s dive into a few of the benefits of working with a managed services provider (MSP).
When it comes to working with a managed services provider, you gain access to a full team of experts. MSPs invest heavily in training, and their relationships with hardware and software vendors allows them quickly get up to speed on the latest technology so they can deploy and support it for you. One large advantage of system management that you can depend on is continually upgraded systems at no additional cost or financial risk — your IT systems will be guaranteed to be as current as possible, and you will not have access to enterprise-level services at a more affordable price.
With the help of an IT Service Provider, your internal IT staff can get back to focusing on more strategic projects, rather than being interrupted on a daily basis and losing focus because of things like password resets or assistance with software updates. Not to mention, when it comes to managed IT services, because of the fact that they work with many different kinds of businesses — inside and outside of your specific industry — they have the ability to learn from mistakes made by other companies, bringing a wealth of outside knowledge to help your business thrive.
Just because you have an in-house IT team, doesn’t mean that it doesn’t make sense to also partner with an MSP. While managed services are not a one-size-fits-all solution, they do offer a clear-cut solution to a number of significant business challenges faced by businesses of all shapes and sizes. There are several situations where outsourcing just some of your technical needs can be a huge benefit to your overall business growth.
"
https://medium.com/@chartwestcott/how-to-maximize-it-productivity-in-a-transition-to-the-cloud-1eada2739671?source=search_post---------124,"Sign in
There are currently no responses for this story.
Be the first to respond.
Chart Westcott
Oct 24, 2018·2 min read
As more businesses move to the cloud, IT teams can be a great help in the transition. Internal IT teams have the experience and knowledge to help the cloud migration team move forward. Here are some ways IT teams can help in the transition process:
Architecture and Design of New Environment
IT staff have a deep understanding of the business and technical requirements of various departments. When a managed service provider (MSP) or system integrator (SI) is designing the architecture of the new cloud environment, input from IT can produce better results. Leveraging IT experience will help cloud architects and designers avoid mistakes that have been made before. It can speed up the whole design process.
Standardization of Templates and Policies
IT departments develop strategies to combat repetitive tasks. They create templates and scripts that automate processes. They have policies in place that avoid users from repeating bad behavior. For cloud migration teams, this is a rich source of information. MSPs and SIs can discuss the standard processes that the IT teams have implemented. They can use this knowledge to create similar automation in the cloud environment. It will help improve processes and increase productivity. The transition team wouldn’t have to invest new resources to previously resolved issues.
Exploration of Emerging Technologies
Due to the intimate knowledge of the current systems, IT teams understand how various business requirements fit together. During cloud transition, if an organization is thinking of deploying emerging technologies like the Internet of Things (IoT), artificial intelligence or big data analytics, IT teams should be the first point of contact. They can tell the cloud migration teams about the hurdles in any emerging technology implementation.
Updating Legacy Applications
Cloud transition is a great opportunity to get rid of any older software. Internal IT knows the applications that are problematic. They can help the transition team figure out which ones to retain and which ones to retire. It will speed up the evaluation process.
For MSP or SI, the internal IT department of an organization is a valuable resource for providing guidance and support. Also, post-transition the internal IT will shoulder the maintenance of the systems. So it is important to get the IT department involved from the beginning of the cloud transition process.
Originally published at chartwestcott.com on October 24, 2018.
Chart Westcott is Co-Founder and COO at Ikarian Capital, LLC a long/short equity biotech focused investment adviser. Read more at http://chartwestcott.net.
Chart Westcott is Co-Founder and COO at Ikarian Capital, LLC a long/short equity biotech focused investment adviser. Read more at http://chartwestcott.net.
"
https://medium.com/@htshosting/private-cloud-10726881c684?source=search_post---------125,"Sign in
There are currently no responses for this story.
Be the first to respond.
Rajendra Mishra
Sep 25, 2021·1 min read
A private cloud is a computer service that is available both on the internet and within the private internal network. Private clouds are overburdened and used by a restricted group of users rather than the general public. Private clouds are sometimes referred to as internal or business clouds.
A private cloud can provide two services. The first is IaaS, which enables businesses to make use of infrastructure resources such as networks, computers, and storage devices. The second is a platform as a service, which allows the firm to provide everything from basic cloud-based apps to large corporate systems.
It has additional control and customization, as well as flexibility, scalability, and self-service. Furthermore, private clouds provide a high level of privacy and security. Virtualization technology is used in private clouds to link resources from physical hardware into common pools.
To learn more about our fully functioning hosting plans, call
1800–123–8156 or visit www.htshosting.org
See all (9)
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@dpaunin/kubernetes-production-to-be-or-not-to-be-3f79516016a6?source=search_post---------126,"Sign in
There are currently no responses for this story.
Be the first to respond.
Dmitriy Paunin
Sep 22, 2017·9 min read
Hundreds of containers. Millions of external requests. Myriads of internal transactions. Monitoring and incident notifications. Simple scalability. 99.9% uptime! Deployment and rollback.
Kubernetes as a magic pill! To be, or not to be: that is the question!
About
Write
Help
Legal
Get the Medium app
"
https://medium.datadriveninvestor.com/use-google-sheets-as-your-database-using-python-77d40009860f?source=search_post---------127,"There are currently no responses for this story.
Be the first to respond.
Today, we will see how we can use google spreadsheets as our database, like if you are creating some internal app and you think you might need a database then you are wrong.
All you need is Google Spread Sheet and Python.
This is our spreadsheet which we are going to use as our database.
2. Add google drive API to the project which will allow us to access spreadsheets inside google sheets account.
3. Once that's added then we need to create some credential.
Since we are doing this from a web server then we will add the “web server” option and give it access to application data.
After filling the options click “What credential do I need”.
4. Next, we will create a service account and allow the role “project editor” which we allow to access and edit data within the API.
5. Click continue and it will generate a JSON file that I will rename and add to the project as “statup_funding.json”.
6. Open that file in a text editor then we will find an email address, in a property called “client_email”.
7. Copy that and take it over the spreadsheet, we can share the spreadsheet to that email address to give us the access to the spreadsheet from the API.
8. Use pip to install gspread and oauth2client packages.
9. Then we will create a file “tutorial.py” and write our code inside that and use google sheets as our database.
We can do a lot more like access data in a particular row/column/cell.
Update a particular cell and lot more
empowerment through data, knowledge, and expertise.
236 
6
236 claps
236 
6
empowerment through data, knowledge, and expertise. subscribe to DDIntel at https://ddintel.datadriveninvestor.com
Written by
just another coder.
empowerment through data, knowledge, and expertise. subscribe to DDIntel at https://ddintel.datadriveninvestor.com
"
https://wheniwork.engineering/how-to-setup-google-sso-and-aws-4496f054a707?source=search_post---------128,"Internal authentication and authorization is a common challenge for modern organizations. Ensuring only the right users have access to the right things can be a complicated task, especially as user numbers and third-party service consumption expands. At some point, managing unique credentials for every service, across every user, is untenable. Enter single sign-on.
In this article, we will step through the process of leveraging single sign-on to control user access to Amazon Web Services (AWS) resources via Google’s G Suite accounts.
Before we get started, you will need the following tools:
First, we will need to set up a custom schema element to hold role information for our users. By default, when you map attributes for SAML applications and pass the roles to AWS, you’ll only be able to select from existing attributes of your users. Examples include:
I’ve seen other articles that mention putting a single role ARN in one of these, but it’s really not suitable for that information (especially if you use those fields already).
The Solution is to setup a Custom Attribute for your users.
Setup the Google G Suite SAML Applications for AWS
You’ll need to configure your Google G Suite account as an identity provider (or IdP) for AWS to use.
Google has written some pretty good instructions for this here. Go check them out and run through them on your own. Otherwise, follow the brief instructions below:
1. https://aws.amazon.com/SAML/RoleSessionName: Basic Information: Primary Email
2 .https://aws.amazon.com/SAML/Role : SSO : Role
12. Click Finish
13. Turn the application on, by clicking on the “settings” button, then Turn ON for everyone.Confirm the dialog when asked.
You’ll need to tell AWS that you want to use the Google G Suite application you just set up as an IdP. You can do that with the command below:
Make sure you substitute GoogleIDPMetadata-yourdomain.xml with the path to the IdP metadata file you downloaded earlier.
This will spit out a response with the ARN of the identity provider you created, so make sure you note this down for later.
Create Some Roles
Make sure you replace <Replace Me with your IdP ARN> with the ARN of the identity provider you created earlier.
2. Run the following command to create the role. Note down the ARN that is returned as we’ll need it later
3. At this stage, I’ve not attached any permissions to the role — you can read how to do that here
Add some roles to your Google G Suite Users:
An example my look something like this (with two roles):
4. Click Authorize and Execute
Open your Google G Suite account and then select the Amazon Web Servicesapplication. It should redirect you on to a page that lets you select role to login with (only if you created multiple Roles) or just to the AWS Console Homepage (if you created a single role).
Notes:
Some things to note… and these are kinda important.
Stories and ideas from the team making work, less work.
46 
9
46 claps
46 
9
Written by
DevOps Engineer in Maine. I have worked remotely since 2003 for companies of all size. Currently working for MakerBot. These are my own opinions not my employer
Stories and ideas from the team making work, less work.
Written by
DevOps Engineer in Maine. I have worked remotely since 2003 for companies of all size. Currently working for MakerBot. These are my own opinions not my employer
Stories and ideas from the team making work, less work.
"
https://medium.com/google-cloud/full-relational-diagram-for-ethereum-public-data-on-google-bigquery-2825fdf0fb0b?source=search_post---------129,"There are currently no responses for this story.
Be the first to respond.
Find a full relational diagram for Ethereum data coming from an amazing opensource project Blockchain-ETL.
The transactions table has the normal Ethereum transactions, and if you need to find the internal transactions, you need to query the trace table.
Internal transactions are not actually considered transactions, as they are not included directly in the blockchain. Instead can only be seen as a byproduct of having tracing on.
Note — Don’t have any unique key on contracts table
Contracts & Transactions
contracts.address can be fond transactions.to_address or transactions.receipt_contract_address.
Joining Contracts & Transactions
You must use a composite foreign key when joining contracts & transactions.
Contracts & Trace
Trace.from_address and Trace.to_address can both have contract.address
Learn more about tracing in Ethereum https://openethereum.github.io/JSONRPC-trace-module, the JSONRPC-trace-module is used to populates table crypto_ethereum.traces.
Internal transactions are not actually considered transactions, as they are not included directly in the blockchain. Instead can only be seen as a byproduct of having tracing on
Etherscan.io Internal Transactions Simplified and Advanced
If you use Etherscan to look up internal transactions, be aware of a Toggle between Simple and Advanced view
Internal Transactions Simple View
How to query the trace table for internal transactions (default/simple view from Etherscan.io ). The total rows returned will match the Etherscan information screen regarding blocks.
Internal Transactions Advanced View
How to query the trace table for internal transactions (advanced view from Etherscan.io)
Token amended with data from CSV.
Deduplicate first since the tokens table might have duplicate entries due to CREATE2 https://medium.com/@jason.carver/defend-against-wild-magic-in-the-next-ethereum-upgrade-b008247839d2
List of tokens
The most popular type of transaction on the Ethereum blockchain invokes a contract of type ERC20 to perform a transfer operation, moving some number of tokens from one 20-byte address to another 20-byte address. This table contains the subset of those transactions and has further processed and denormalized the data to make it easier to consume for analysis of token transfer events.
Similar to the token_transfers table, the logs table contains data for smart contract events. However, it contains all log data, not only ERC20 token transfers. This table is generally useful for reporting on any logged event type on the Ethereum blockchain.
This table contains Ether balances of all addresses, updated daily.
You can see “Partitioned Field” if the table does have a partition setup in the diagram.
When working with the following tables
Try to use a partitioned field. This will speed up your queries and reduce your costs.
Find an interactive diagram at https://dbdiagram.io/d/602e721380d742080a3b1265 with more column-level details.
I have a few posts that will get you learning and using SQL very quick with blockchain data from different cryptocurrencies. Try “The fastest way to learn SQL with Bitcoin data on a live database from Google”.
Google Cloud community articles and blogs
99 
1
99 claps
99 
1
Written by
Happy to answer questions on SQL, ERP, Blockchain & Google Cloud Platform.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Happy to answer questions on SQL, ERP, Blockchain & Google Cloud Platform.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://shipt.tech/infraspec-declarative-microservices-557639222c7c?source=search_post---------130,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
The way we build and run services has changed dramatically in recent years. Companies have moved away from monolithic, multi-tiered applications in favor of microservice architectures. Loosely coupled independent services allow teams to develop capabilities quickly, using diverse technologies appropriate for the job while limiting the need for coordination. As long as the microservice adheres to a negotiated contract, teams are free to move as fast as they can.
Containerization has become the standard way to run these microservices. Technologies like Docker, Terraform, and Kubernetes have entered the common vernacular of today’s engineering teams — and for good reason: these technologies make it easier to abstract and manage the different parts of a diverse microservice architecture. Containers have increased productivity, made it easier to move fast, and let the teams focus more on building great products.
These technologies have something special in common: they are all declarative — they use some sort of specification that declares what the user wants. Dockerfiles step through the process of building a Docker image and leave the user with the final result. HashiCorp Configuration Language (HCL) files describe what a user wants in their cloud, and Terraform makes the appropriate changes to reach that state. All sorts of YAML files get applied to a Kubernetes cluster, and the controllers produce the desired result.
This declarative approach is great because it requires very little action on the user’s part. The user simply declares what they want and the technology fulfills their request. It also leaves a paper trail behind, letting others clearly know what has been created. Declarative technologies have moved us away from performing actions to declaring state and behavior. The technology is now responsible for performing the actions that will achieve the desired state.
At Shipt, we have moved to embrace these technologies as the backbone of our cloud infrastructure. We started with a Rails monolith when the company was young and have since moved to a microservice architecture backed by Docker, Terraform, and Kubernetes. This has offered organizational benefits for engineering teams, as they get to move independently of one another and work on smaller services with a singular purpose.
However, the move to microservices has also produced a significant amount of cloud infrastructure that needs to be well managed. Microservice architectures produce highly decoupled collections of business logic, which makes it easier for individual teams to get their work done but also harder to manage as a whole.
Declarative configuration has made it easier to produce the infrastructure that we want in the cloud, but we still needed to come up with structures and common patterns that fit our business. We needed one more level of abstraction.
We decided that the right way to do this is with an internal spec. We call it Infraspec, and it describes all the components a microservice needs to run, from web servers to workers, cron jobs to data sources, deployment strategies to routing.
The Infraspec file is the source of truth for a microservice. It is used to provision infrastructure and deploy to our production and staging environments. It is also used to spin up short-lived provisions for testing purposes.
This internal spec has improved engineering at Shipt in multiple ways. It abstracts away the underlying implementation of our microservice architecture and eliminates the need for engineers to know how services are provisioned. It also allows the Platform team to manage the complexity of the cloud by creating consistent infrastructural and architectural patterns, which enables us to provide great tooling out of the box. Most importantly, it makes it easy for an engineer to go from “ready for deployment” to “deployed” in a short amount of time.
Infraspec is an abstract representation of the infrastructure supporting everything within the domain of a service. It describes the moving parts of the service at a high level and is designed to be implementation agnostic.
The Infraspec file describes a few key roles:
It also supports a number of other options:
Below is an example Infraspec for an order service:
The roles section describes the bulk of our order service. In this service, a web server receives order requests which are placed on an SQS queue. This queue is consumed by the process_order worker, which handles the various functions needed to fulfill an order. We also have a scheduled task called backup_orders that periodically makes a backup of orders.
Our webserver runs a specified command and has some other important fields: the healthcheck endpoint for the service, the port that the web server listens on, and the replicaCount or number of web server instances to run behind the service’s load balancer.
The workers are a bit simpler: they just have a name and a command and are expected to continue running until the service is terminated.
The schedule section is similar to our workers with a name and a command, but it also has a cron field that contains a CRON expression specifying when the task should run. Unlike workers, scheduled tasks are expected to stop running after some time.
Under datasources we declare the SQS queue where requests will be sent and PostgreSQL database where order records will be stored. We also specify the configVar, or environment variable, that will hold the connection URL for each data source.
We use the routes section to specify where our service can be reached. In this case it can be reached at api.shipt.com/orders (a combination of subdomain and paths) and the API gateway will handle JWT authentication as specified by jwtAuth. We can add more routes if we’d like, but in this case our service only has one.
Lastly, we specify when our service should be deployed to different environments in the deploy section. Our service will be deployed to staging whenever code is pushed to the master branch, and to production when a new semantic versioning tagFormat shows up on the master branch.
With relatively little information, we’ve described all the essential parts of our microservice. We’ve described the components it needs to run (the roles and data sources), how to access it (the routes), and when it should deploy to different environments (the deployment strategies). From just this, we can provision infrastructure, set up deployment pipelines, and start running a service that everyone can access.
Compare this to a collection of Kubernetes and Terraform files managed by hand. Services that run in Kubernetes consist of multiple object types — Pod, Service, Deployment, Ingress, just to name a few — and all these objects must be managed individually for each service with their own YAML spec. Terraform is no different: in the case of AWS, a user needs to declare the Route 53 record, SNS/SQS, ElastiCache, RDS, and any other infrastructure that a service needs to run. Then they must manage secrets, create CI/CD pipelines, set up monitoring, and so forth.
When you think about the amount of declarative files a team must write to provision their service by hand, it becomes clear how simple the Infraspec format is in comparison.
So what does it take to actually get a microservice deployed? There are only two major requirements for engineers to fulfill, two files that must live at the root of a repository: an infraspec.yaml file (described above) and a Dockerfile (used to build a Docker image).
Once these are ready, engineers add a webhook to their GitHub repo, push their code, and the application gets deployed. The webhook listens for push events on every branch of the repo. With each push, we pick up the latest code, which includes an infraspec.yaml file, and perform actions based on what’s in that file.
We’ve designed our system to be stateless and deferred the responsibility of state onto other technologies. Jobs are kicked off whenever events occur (e.g. a git push) and the infraspec.yaml file indicates what actions should be taken and what things should be created. Terraform manages the state of our AWS infrastructure, namely data sources. Kubernetes manages the state of our Docker containers. We translate our Infraspec format into other specs using Helm charts and other templating solutions and pass these off to other technologies.
One of the best features about the Infraspec file is that it allows engineers to spin up development versions of their code before deploying to production or staging. This allows multiple engineers to test code changes in the cloud and gives them an extra degree of confidence in their changes. We call these development provisions — short-lived provisions that are used for development. They are created whenever a pull request is opened, but engineers can also spin up development provisions of a service from any branch or tag.
There are some things to note about development provisions that make them different from long-lived provisions in production and staging:
Development provisions promote a culture of testing and confidence in code. They allow individual engineers and whole teams to test changes before going live. They are an essential part of the development pipeline at Shipt and offer significant value to our engineering teams.
For the Platform team at Shipt, the customer is the Engineering team. Our goal is to build tools and pipelines that make it easy for engineers to do what they want. This means tackling a number of complex problems that engineers face on a daily basis and coming up with intuitive and functional solutions to these problems.
As we moved from a monolith to a microservice architecture, engineers had to figure out how to provision these new services in the cloud. We also faced another challenge: with a growing number of microservices, we had to manage the complexities of cloud infrastructure. Infraspec solves both of these problems.
Infraspec is a high-level specification. It is an abstract representation of a service and abandons any sense of implementation details. This benefits both Engineering and Platform in different ways:
We have used the Infraspec format to create services in both ECS and Kubernetes. Functionally these services behave the same way, but the underlying implementation has varied significantly. In both cases, the Platform team created consistent architectural patterns to manage the various microservices being deployed to the cloud.
Consistency in the cloud has enabled us to provide value to engineers in other ways too. With the same patterns being used by every microservice, we’re able to provide logging, metrics, secrets, and other monitoring and tools out of the box for new services. What may have once been considered a “nice to have” is now an “easy to have” feature.
Even though the Infraspec format has solved a number of problems in our microservice architecture, it is not a perfect solution.
One of the major drawbacks of Infraspec is that it can be a blocker for teams that want to adopt different architectural patterns or use technologies that we don’t support. One great example of this is Lambda: to date, the Infraspec format does not offer support for Lambda functions. For now teams still have to come up with their own patterns for serverless code.
One possible solution to this is allowing teams to provide their own Kubernetes and Terraform files that are consumed alongside those generated from the Infraspec file. Doing so allows teams to create infrastructure at a lower level while the Platform team comes up with a managed pattern for the future.
Engineering teams are always exploring new ways to build services. We want Infraspec to be the one-stop solution for creating new services in the cloud, but this means we must keep up with trends in the technology ecosystem and support the needs of our engineers — both of which are ever changing. Unfortunately we can’t support every new technology or paradigm in the industry. We have to pick and choose the technologies and patterns that we’re going to support, lest we spread ourselves too thin. Striking a good balance between consistency and flexibility in our cloud infrastructure is one of the biggest challenges we face with Infraspec.
Today’s Infraspec file will not be the same as tomorrow’s. Infraspec is a living format that we are constantly iterating on to provide more features and value to our engineers. We work closely with engineers and receive feedback about what features they need for their microservices. This feedback drives our iterative process and the direction that we take the Infraspec format in.
There’s a lot that goes into provisioning a single microservice. You need to run the service, provide data sources, store and read secrets, and have tooling in place for logging and monitoring. That’s a lot for each service team to manage, and it becomes even harder to manage for the organization if every team uses a different approach to their provisioning.
Infraspec solves both of these problems. It provides Engineering with a simple way to describe and deploy their services and lets the Platform team create consistent architectural patterns that help us manage those services. These consistent patterns also enable us to provide more value out of the box with logging, monitoring, and other forms of tooling. The ability to spin up short-lived development provisions has created a culture of testing and confidence in our code before going live, and the ability to easily provision services to production and staging has made it faster and easier for engineers to go from idea to microservice.
Infraspec is not a perfect solution, nor is it a final solution. It is a format that the Platform team iterates on to meet the developing needs of our engineering teams. However, even in its current form, it has solved a crucial problem in a microservice environment — the provisioning and managing of microservices — in an elegant way. It’s one of the many ways that the Platform team supports Engineering at Shipt.
Shipt is a membership-based online grocery marketplace delivering fresh foods and household essentials through a community of shoppers and a convenient app. Shipt offers quality, personalized grocery delivery to members for $99 per year, and is available to nearly 70 million households in more than 200 markets across the country. For more information (including how to become a Shipt Shopper), visit Shipt.com!
Interested in joining a Shipt team? As part of our “people first” mentality, we value our open, collaborative culture and work environment where everyone has the opportunity to grow and succeed. Learn more about careers at Shipt.
Delivering the Goods
103 
103 claps
103 
Written by
DevOps Engineer @ Shipt. Go, Docker, Kubernetes. Building cool things and learning all the time.
Where Shipt’s technology teams share their ideas, their interests, what they’re building, and who they are.
Written by
DevOps Engineer @ Shipt. Go, Docker, Kubernetes. Building cool things and learning all the time.
Where Shipt’s technology teams share their ideas, their interests, what they’re building, and who they are.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/powtoon/security-first-compliance-as-a-feature-69daf45b10e5?source=search_post---------131,"There are currently no responses for this story.
Be the first to respond.
Another day, another headline about data hacks, misuse of confidential information, or strident new regulations meant to keep private information safe. And every day, another large organization takes steps to ensure the protection of their data — up to, and including, requiring compliance from software vendors like Powtoon.
There are as many responses to this growing trend as there are companies. Here is the story of how and why Powtoon put security at the center of our new Enterprise Cloud.
Learn more about Powtoon for Enterprise
Powtoon is a platform that lets anybody create their own professional-looking videos. From the beginning, we built Powtoon in a very, very agile way — primarily driven by customer requests. We said, “Ok, what’s important to our customers now, let’s give them that, yeah?”
That meant the first version of Powtoon was very limited. You could animate some characters on screen. We had only one style. We had only one character in all kinds of poses. And that was pretty much it: no video, no image backgrounds, no nothing. You could make stickman animations with some text.
But it also meant that, as we gained users, their requests guided how we developed Powtoon. People asked for more of this, and more of that, until the platform evolved into what it is now, with live-action video capabilities, thousands of animated assets, a raft of integrations, and more.
The thing is, Powtoon began as a tool that was used mostly by individuals for their small businesses or for their freelance activity. Compliance is not exactly something you think about in that situation. You care that your Powtoons are your Powtoons, and nobody’s accessing them without your permission, or without you sharing the Powtoon on a page.
Individual users don’t think much of, “How is Powtoon managing their infrastructure?” “How does their organization deal with backend access?” “How does their customer support team access, or not access data?” These aren’t the kind of things SMBs or a typical user making an invitation for their kid’s graduation tend to be concerned with.
But when it came to corporations, to bigger companies, to people who seek out Powtoon because they’re looking for something to spice up their internal communications, their presentations, their HR announcements, or whatever it may be — suddenly we began to get many questions about security and compliance.
We started to receive questionnaires from IT departments and from security officers, and from all kinds of functions within corporations. They were asking, “How do you backup your data? Where is your data stored? Who has access to it? Where is it hosted, in the U.S.? U.K.? Europe?” And we found ourselves investing more and more time into reviewing these questions. Answering them, first to ourselves, and then to our clients.
It was actually heartbreaking to discover that I would have to answer many of these questions in the negative. Finding out that many standard practices (that were just fine six years ago) didn’t answer our customers’ growing needs — that was a hurtful surprise. I knew that we could do better, and deliver a higher standard.
“Compliance, in its widest sense, is a feature.”
There’s a demand for a new set of features today. These features are not meant to please the actual end-user, but they’re meant to satisfy the requirements of the organization employing the user. Compliance, in its widest sense, is a feature that we have to deliver to these customers and their employers in order to do business with them.
No compliance? No business!
That’s really the situation out of which the idea for the Enterprise Cloud was born. Enterprise customers care even more about the security of their data. They may in fact care more about their confidentiality than they do about features in the library. And they needed a solution that was built from the ground up with security first in mind. Which is exactly what we did.
So the software is deployed differently, and managed differently. A much smaller team has access to the Enterprise Cloud — something of a team within a team. We have very strict regulations with regards to who can access the back office of the Enterprise Cloud. Customer support is not allowed to view any customer data without the prior consent of the customer. This wouldn’t be workable for our Public Cloud, but with Enterprise clients who demand and can sustain this level of support and security, we defined policy, and implemented systems that actually enforce these practices.
There are four main areas of concern for large organizations when it comes to their data:
Companies want to ensure that their data’s integrity is not compromised. They are concerned that their data is what it is actually supposed to be, that only people they want to access that data may access it. They want to know that something that “belongs to us,” as an organization, stays “with us, and us only.”
Imagine you create a Powtoon with sensitive information that your competition might like to know. You would want to be sure that your competition cannot get access to that information from the outside. Large companies want to rest assured that all measures have been taken so their sensitive data is accessible only to whomever they have defined authorized to do so.
Companies need to know they can access their data whenever they need to. They have employees working, putting in their time and expertise to create compelling video content, and saving it on Powtoon’s cloud. Companies need to know that their data will not be lost due to some technical hiccup. This touches on questions around how we replicate data, how we back up the data, what measures we take to ensure our service is constantly available, and what kind of service-level do we agree to.
Finally, not only do companies want to ensure their data’s integrity, confidentiality, and accessibility, but they also need to know that their partners and software vendors (like Powtoon), can make the same commitment from inside their own organization. Ultimately, strong organizational controls bolster the other three pillars of information security.
Check out Powtoon for Enterprise
As I mentioned earlier, we have a very small “team within a team” that has direct access to the Enterprise Cloud. This helps us manage our organizational controls and limit access to any data related to Enterprise users. In fact, we built the Enterprise Cloud to be hermetically sealed, and access to it is granted only on a need-to-have basis. But that isn’t the only measure we take. We still need to ensure the integrity, security, and accessibility of the data. That means that even viewing a Powtoon goes through a different process.
For example, in the Enterprise Cloud, we have an additional layer of security that checks every request going into the application against a continuously updated database of attack patterns. So we make sure on a request-by-request basis — every time somebody sends any request for a webpage or command for an API call to the Enterprise Cloud — we secure the account from any known malicious activity.
Additionally, we ensure that only the people you want to see your content can see it. Normally, when you share a Powtoon with somebody, it is private if you keep the link private. Much like sharing a link to a Google doc, you can send a link to someone who is not signed into Google to review a document, and they’ll be able to see it. As long as they don’t post this link on Twitter, it’s private. Nobody can find it, you won’t find it on Google or anything. But if you have the link, you can access it. That’s how it worked for Powtoon for the last six years.Your link is private as long as you don’t announce it. But if somebody hacks into your email and sees the link, he can see your content.
In the Enterprise Cloud, that’s not possible. Nobody can view a Powtoon if they’re not part of your organization. We put a virtual perimeter around your organization, and we say, “Hey. If you don’t prove to me that you belong to Accenture, let’s say, you won’t be able to see anything that belongs to Accenture — even if you’ve got the url of the player page.
Video is growing faster than ever, and the power to communicate with deep impact is available for almost anyone today. Powtoon is a big part of that revolution, and I couldn’t be prouder to have had a hand in building it. But let’s face it, it doesn’t matter how fancy or effective a software product is, if a company can’t work with it for lack of security compliance — that company might as well not exist in the first place.
Powtoon is committed to seeing everyone, including people working for large organizations, add a touch of awesomeness to their work and their life with video. Companies that care about pushing the boundaries of working communication, but can’t afford to sacrifice compliance finally have a solution that puts security at the center, and treats compliance like a feature.
Powtoon is a video creation platform that enables anyone to…
274 
274 claps
274 
Powtoon is a video creation platform that enables anyone to create professional-looking, engaging videos and presentations in minutes, without any design or technical skills. Over 26 million people, including 96% of the Fortune 500 companies, use Powtoon to engage their audiences
Written by

Powtoon is a video creation platform that enables anyone to create professional-looking, engaging videos and presentations in minutes, without any design or technical skills. Over 26 million people, including 96% of the Fortune 500 companies, use Powtoon to engage their audiences
"
https://medium.com/google-cloud/how-to-ssh-into-your-gce-machine-without-a-public-ip-4d78bd23309e?source=search_post---------132,"There are currently no responses for this story.
Be the first to respond.
In this article, I will show you how you can ssh into your Compute Engine machine without public IP, only using the internal IP.
Before we start I will add a few important concepts of how we can achieve that.
Today more and more companies are using extra layers of VPNs, MFA, security process, firewalls, routers, etc… in order to authenticate who needs access to a server or to an application.
Google created the BeyondCorp implementation within GCP that contains several security products, and the one we will use here is Identity-Aware Proxy (IAP).
BeyondCorp is Google’s implementation of the zero-trust security model that builds upon eight years of building zero trust networks at Google, combined with ideas and best practices from the community. By shifting access controls from the network perimeter to individual users and devices, BeyondCorp allows employees, contractors, and other users to work more securely from virtually any location without the need for a traditional VPN.
BeyondCorp began as an internal Google initiative to enable every employee to work from untrusted networks without the use of a VPN. BeyondCorp is used by most Googlers every day, to provide user- and device-based authentication and authorization for Google’s core infrastructure.
IAP lets you establish a central authorization layer for applications accessed by HTTPS, so you can use an application-level access control model instead of relying on network-level firewalls.
IAP policies scale across your organization. You can define access policies centrally and apply them to all of your applications and resources. When you assign a dedicated team to create and enforce policies, you protect your project from incorrect policy definition or implementation in any application.
Max Saltonstall created a really good post explaining more details about IAP and BeyondCorp that you can check it out here.
You will need a GCP project, so if you haven’t created yet you can follow these steps:
You can try to ssh using the button below, but you will face Not Authorized even if you are the owner of the project
This happens because the Owner role doesn’t have the iap.tunnelInstances.accessViaIAP permission, so let’s add this permission to our user.
First, make sure you have the IAP API enable
→ You can check all IAP roles on this page as well.
You can try clicking the button ssh and use the terminal in the browser …
…or you can use gcloud from your terminal with the command
gcloud compute ssh — zone “<region>” “ssh-iap” — tunnel-through-iap — project “<project_ID>”
You can also see this command in the UI if you prefer just copy and paste
IAP gives us the ability to keep our instance close to the internet and still ssh into it in a safe manner.
With the gcloud command, we don’t even need to create our own public/private key as the tool do all the work creating a new
You can check in your machine on ~/.ssh folder usually called google_compute_engine and also on GCE -> metadata -> ssh keys.
Let me know if you have any questions!
Google Cloud community articles and blogs
28 
1
28 claps
28 
1
Written by
Brazilian Software Engineer living in London. Java addicted. Clean Code and Software craftsmanship Enthusiast. Cloud. https://www.linkedin.com/in/femrtnz/
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Brazilian Software Engineer living in London. Java addicted. Clean Code and Software craftsmanship Enthusiast. Cloud. https://www.linkedin.com/in/femrtnz/
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://blog.joinblink.com/whos-going-to-keep-an-eye-on-ai-6559dca631f0?source=search_post---------133,"Sorry, the page you were trying to view does not exist.
By using this website, you agree to our cookie policy
"
https://blog.joinblink.com/the-danger-lurking-in-your-shadow-data-75a331bfda29?source=search_post---------134,"Sorry, the page you were trying to view does not exist.
By using this website, you agree to our cookie policy
"
https://faun.pub/setup-email-notification-using-external-smtp-server-from-ec2-basic-aws-networking-revisited-344246231858?source=search_post---------135,"There are currently no responses for this story.
Be the first to respond.
Last discussion with internal team that there is this usecase that we are in need to create email notification from the apps inside EC2 instance which are using external SMTP. As simple as this is, yet as I mostly work on development infra and apps, I am not familiar with setting-up SMTP especially from this AWS environment to external SMTP Server (commonly I do configure on-premise which is local setup and I could manage the entire stacks).
So with that in mind I try to create a simple lab which may reflect a simple production environment with following criteria:
Following is the concept of my Lab to actually deploy the environment
It is quite refreshing to start over the basic networking in AWS as I revisit all basic network settings.
Here is the Terraform that I prepared (its only a draft, it may not be 100% useable). I love this Infrastructure-as-Code approach, as I could bring up environment fast (well documented and I could do some copy-paste)
After the environment are prepared, the last thing that need to be done is to install the SWAKS and send the email. As the architecture, I need to actually SSH from my local laptop to the so-called-Bastion-Host and then I do another jump using SSH to the Instance that have SWAKS.
And after all installation is done I need to test to send email
Which later-on prompt the username/password interaction. One note that the Gmail may have a hard time to authenticate this too-simple-auth so I think for temporarily I need to turn on the Google allow less secure apps permission.
Then it is done, Love to explore more and if there are more proper way to explore this approach please do not hesitate to send some ideas or inputs.
Follow us on Twitter 🐦 and Facebook 👥 and join our Facebook Group 💬.
To join our community Slack 🗣️ and read our weekly Faun topics 🗞️, click here⬇
The Must-Read Publication for Creative Developers & DevOps Enthusiasts
57 
57 claps
57 
The Must-Read Publication for Creative Developers & DevOps Enthusiasts. Medium’s largest DevOps publication.
Written by
Cloud Customer Engineer — Infrastructure Modernization @GoogleCloud. Stories are my own opinion. https://linktr.ee/alevz
The Must-Read Publication for Creative Developers & DevOps Enthusiasts. Medium’s largest DevOps publication.
"
https://aws.plainenglish.io/custom-dns-for-private-api-gateway-5940cb4889a8?source=search_post---------136,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
AWS API Gateway Private is the ideal way to front our internal applications and services, but there’s one frustrating limitation — you cannot assign a custom domain name to a private API like you do with one that is public facing.
This leads to internal APIs with less than friendly names like:
Or if you need the name to resolve for remote users (VPN or on-prem over Direct Connect), then it would look like this:
Either way, it’s a pain to remember! What’s worse is if it’s redeployed for any reason the name will change — ok for services part of your CI/CD pipeline that know about the update, but not so good for users or applications out of the loop.
The solution for now is to put an Application Load Balancer (ALB) in front of the API and access it using the hostname of the ALB instead. It adds a little cost to the scenario, but if you have users and applications out of your control accessing the API’s it’s probably essential. You can use host and path-based routing on the ALB to access many APIs, and even use the same ALB to access web apps on EC2/ECS/Fargate in smaller environments or test accounts.
There are a few tricks to it though, so here’s what you need to do (with a CDK example at the end)…
I’m not going into detail about how to create and configure APIs here, just how to access them using an internal ALB and custom domain name.
You will need a working API or two, and the internal API VPC Endpoints to access it.
The key trick with API Gateway itself is adding a Custom Domain Name and mapping it to your internal APIs.
This is not intuitive, because Custom domains can not be created with a Private endpoint type:
That’s ok — using the Regional endpoint type will work. You will also need a matching certificate from ACM.
Next, map the Custom domain to your internal API stages as you would an external API:
Note the path(s) you use here will be needed later in the ALB Rules.
And that’s it for the API configuration. You can still access the APIs using the unfriendly endpoint URLs, but accessing it directly using the custom name won’t work yet.
Next, we create and configure a load balancer. Again I’m not going into all the detail here, just the parts relevant to the API and custom domain.
First, create the ALB and attach it to the same subnets as your API VPC Endpoints.
Add a Listener for HTTPS and attach your certificate. The default action should be a Fixed Response with a 400 or 500 error.
Next, we need a Target Group
First, get the IP Addresses of the API VPC Endpoint:
Then create a new Target Group:
We need to configure the Health Check Settings. API Gateway will send a 403 response to health checks so we have to include that:
Add the IP Addresses of the VPC Endpoint and add to pending:
Go back to the Load Balancer, and to Listeners. We need to edit the HTTPS Listener to add the new targets:
Edit the Listener and Add a New Rule with Path-based routing:
Note I have added the paths mapped above for both of my test API’s here — we can forward to multiple API’s using the same ALB and Rule. The leading slash is required.
Save the Rule and we are done.
You can now access the API’s using the hostname of the ALB, for example:
will be forwarded to the API with a domain path mapping of:
In my environments, I’m deploying the ALB’s and configuration using AWS CDK. I’ve created a demo to show how to create an Application Load Balancer and internal private API Gateways with a custom domain name.
The demo is self-contained and will deploy into the AWS default VPC by default. Check out the Readme for more detail. You will need VPN/DX or a bastion/jumpbox to access the internal load balancer for testing.
Aside from the tricks above, the other issue automating this with CloudFormation and CDK is that there is no export for the IP addresses attached to the VPC Endpoints. I use a CloudFormation custom resource to work around it, which is pretty easy using CDK. If you are interested in the generic CloudFormation version let me know.
About the Author
Mark is a 20+yr IT industry veteran, with technical expertise and entrepreneurial experience in a range of fields. For many years now his passion has been building on AWS, and he holds certifications including Solution Architect Professional and the Advanced Networking Specialty. He wants you to know that he is an infrastructure and architecture expert who writes code, not a professional software developer — just in case you spot something odd.
Mark has spent the last few years in the banking industry, were he is currently building and leading a team focused on using AWS serverless technologies to reduce costs and speed up the time to market for new services and products.
You can also find Mark on:
GitHub — https://github.com/markilott
LinkedIn — https://www.linkedin.com/in/markilott/
Twitter — https://twitter.com/mark_ilott
More content at plainenglish.io
New AWS and Cloud content every day.
60 
3
60 claps
60 
3
Written by
Solution Architect specialising in AWS, sharing IaC tips and tricks
New AWS and Cloud content every day.
Written by
Solution Architect specialising in AWS, sharing IaC tips and tricks
New AWS and Cloud content every day.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/opstalk/whats-new-in-openshift-container-platform-4-1-168a6d12b90d?source=search_post---------137,"There are currently no responses for this story.
Be the first to respond.
When Kubernetes was developed by Google as a successor to their own internal container management tool, Google Borg, and then open sourced in 2014, the world that developers and administrators lived in was turned on its head. However, challenges soon presented themselves: getting a container orchestration tool like Kubernetes to play well with containerization tools like Docker could be tricky.
A solution needed to be found that would allow complex open source projects designed for on-premise or in-cloud use to be integrated with familiar, easy-to-use platforms designed for streamlined application packaging and delivery.
When Red Hat OpenShift Enterprise 3 (later renamed OpenShift Container Platform or OCP) released in 2015 based on Docker and Kubernetes, it revolutionized PaaS (Platform as a Service) management. OCP integrated the features of the Docker platform with the management of Kubernetes, and then went further by integrating the networking and security layers that make pure Kubernetes clusters so complex to manage and secure.
OCP 3 gave developers a platform that allowed them to focus on code and administrators a platform that didn’t keep them up at night worried about resiliency and security. Over its 11 iterations, OCP 3 continued to take the innovations of the upstream Kubernetes ecosystem and make them ready for the enterprise.
Now, we are in 2019 and OpenShift Container Platform (OCP) 4.1 is here. What has Red Hat brought to the table in their new flagship PaaS offering?
OCP 4.1 represents a revolutionary leap forward that leverages the new developments in the upstream projects of the Kubernetes ecosystem and the acquisition of the CoreOS container platform.
OCP 4.1 has an installer-provisioned infrastructure where the installer controls all areas of the installation process on AWS. For administrators installing in AWS, this is an excellent feature that allows you to provision an OCP 4.1 cluster from scratch in minutes. Expect to see other cloud providers available in this space soon.
For user-provisioned environments, administrators can deploy simply on any platform by filling out an inventory file and giving the installer connection credentials for all of the hosts in the environment.
Upgrading from 3.x to 4.x is currently not available. You must install your 4.x cluster in a fresh environment.
The OCP update service offers a simple interface that shows administrators available updates and analyses whether an update is safe for your cluster as well as verifying all downloaded updates before implementation.
Upgrades can now be performed using the web console by administrators. By logging into the console administrators can see new updates available and with a few clicks begin the upgrade of their cluster. This simplifies the process of upgrading clusters from being a major outage event to a simple task that can be done whenever it fits into your schedule.
At the core of OCP is certified, conformant Kubernetes as validated by the Cloud Native Computing Foundation. This means migration to OCP from another Kubernetes platform like Google’s GKE is a fairly straightforward operation. The difference with OCP is Red Hat takes has hardened the codebase for security and stability.
The changes start at the operating system level with Red Hat Enterprise Linux CoreOS (RHCOS). Red Hat has leveraged its acquisition of the container platform CoreOS for this change. This will replace Red Hat Linux Atomic as the underlying thin host operating system of choice. What does that mean for administrators? It means hosts in a cluster can finally be treated as livestock instead of pets.
In previous platform models, individual hosts needed regular care. Hosts required patching and other regular maintenance tasks. When a host was misbehaving, administrators put in the effort to repair it, much like one would a pet. With RHEL CoreOS, this mentality switches to treating the hosts more like livestock, favoring simply replacing one host with another when one is failing. As it is immutable, each host is guaranteed to have an identical operating system.
Configuration for the cluster is stored in a central, distributed service that allows new hosts to immediately just start working the moment they join the cluster. Instead of patching hosts they receive a new image. During the upgrade process each host has its container workload migrated to another host, it receives the new image, and it is brought back into the cluster in a rolling update that requires no downtime.
At the host level, OCP takes advantage of the added security benefits of SELinux categories to secure containers. SELinux labels each container with a unique kernel level context on the host. These contexts isolate containers preventing direct container to container access. Even if a container is compromised and root level rights are attained, the attack vector is walled off to only that container. Put differently, with SELinux categories, even someone with root credentials cannot break out of a container because they lack the correct allowed context.
OCP 4.1 is designed for diverse, hybrid cloud environments across traditional on-prem and cloud platforms. OCP has automation providers for on-prem bare metal and virtualization platforms such as VMware, Red Hat Virtualization, and OpenStack. OCP also has providers that will be available in the coming months to integrate with Alibaba, Amazon Web Services (AWS), Google Cloud, IBM Cloud, and Microsoft Azure.
These integrations will give a unified platform experience for developers allowing companies to take advantage of shifting price points in the various clouds while giving end users and developers a seamless experience. This means companies can host non-critical development environments on cheaper spot instances and also maintain a multi-cloud production workload giving the highest possible availability and redundancy.
As defined by the CoreOS team, “An Operator is a method of packaging, deploying and managing a Kubernetes application. A Kubernetes application is an application that is both deployed on Kubernetes and managed using the Kubernetes APIs and tooling.” That’s a lot of jargon. To simplify that definition, Operators are the runtime that manages your application on Kubernetes, allowing your code to directly interface with the Kubernetes system to get work done more efficiently and dynamically.
Red Hat now offers the Red Hat OperatorHub for Red Hat OpenShift giving customers a curated and tested repository of trusted operators, taking the guesswork out of Operator implementation. This allows companies to implement automation capabilities, such as self-service provisioning, self-tuning, data-replication, automated backups, and automated updates, for their respective services.
Using the OpenShift Certified Operators mentioned above, OCP can directly integrate with Red Hat Middleware for an unprecedented level of integration between platform and middleware. By unifying development environments around Operator capabilities, developers can focus on delivering next-gen services instead of worrying about underlying tooling.
According to OpenShift documentation, “OpenShift Service Mesh combines Istio, Jaeger, and Kiali projects as a single capability that encodes communication logic for microservices-based application architectures, freeing developer teams to focus on business-add logic.” Let’s break down that jargon.
Istio — Istio controls the complexity of microservice network connectivity allowing secure communication between containers in a controlled and monitored manner.
Jaeger — Jaeger provides distributed tracing, transaction monitoring, service analysis, and root cause analysis giving you unprecedented visibility into your platform.
Kiali — Kiali provides visibility into the microservices integrated into your service mesh, giving topology, health status, metrics, configuration validation, and distributed tracing.
OpenShift Service Mesh is one of the most exciting changes to come to the OpenShift ecosystem. It solves for some of the greatest weaknesses in previous OCP platforms.
OCP 4.1 has a slew of new innovations either in tech preview or soon-to-be released.
Knative is in developer preview for OCP 4.1 and is ideal for building Function-as-a-Service (FaaS) workloads. It is designed for building, deploying, and managing a serverless workload that can scale down to zero when not in use and on-demand scale to meet your needs. This allows you to create on demand functions in a similar fashion to AWS Lambda but internal to your OCP cluster.
Developed in a collaboration between Microsoft and Red Hat, KEDA supports deployment of serverless event-driven containers on Kubernetes, enabling Azure Functions in OpenShift, in Developer Preview. This allows for accelerated development of event-driven, serverless functions across hybrid cloud and on-premises with Red Hat OpenShift.
Currently under development, Red Hat OpenShift Container Storage 4 will offer a highly scalable persistent storage for cloud-native applications with encryption, replication, and availability designed into the solution allowing application teams to dynamically provision secure, fast, and stable storage for workloads such as SQL/NoSQL databases, CI/CD pipelines, and AI/Machine Learning.
The release of OCP 4.1 represents a quantum leap in PaaS technologies. OCP offers a platform with unprecedented levels of control and visibility for administrators and tighter integration between services that allow developers to simply focus on innovating with code. The current feature set is excellent and the coming features represent new heights in enterprise container management.
Next Steps
Stone Door Group is an early adopter of all things Red Hat OpenShift. Our OpenShift Container Platform Accelerator℠ solution takes all the guesswork out of transitioning brownfield applications to OpenShift by executing an industry best practice services migration methodology that delivers tangible and valuable outcomes.
Michael Clarkson is a Red Hat Certified Architect Level 4, two time Red Hat North American Instructor of the Year, and a Senior Cloud Consultant for Stone Door Group, a Cloud and DevOps consulting company. To learn more about hiring consultants like Michael or becoming a part of the Stone Door Group team, drop us a line at letsdothis@stonedoorgroup.com.
OpsTalk explores the latest technological advancements…
13 
13 claps
13 
OpsTalk explores the latest technological advancements facing IT today and the power of the gig economy.
Written by
OpsTalk is a blog brought to you by Stone Door Group that explores the latest technological advancements facing IT today and the power of the gig economy.
OpsTalk explores the latest technological advancements facing IT today and the power of the gig economy.
"
https://labs.acmi.net.au/building-mobile-apps-without-code-9ae75343d929?source=search_post---------138,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
If you need a mobile app to collect and then display some relatively straightforward data to a small group of internal users, do you really need an app? Or more to the point: what sort of app is it that you need, and how much time are you going to have to spend developing it?
Here I’ll be going through the process we took to go from idea to an installable app in just a couple of days using cloud services- Google Drive and Appsheet- and zero lines of code.
The team that came to us needed a way to collect reports about our collection items, both at ACMI and in other venues around the world when our exhibitions tour. These reports are mostly text, but they also need to collect signatures, photos and sketches. The app needed to be mobile-first, because the team works with iPads most of the time, and because of frequent flaky WiFi encounters, it needed to be something that easily worked offline.
We briefly considered all the fun of building an offline-enabled progressive web app from scratch before realising we definitely didn’t have the time for a project of that scale right now. Since this was a pretty straightforward data collection app, maybe there was a cloud solution that could work?
The user need was basically to collect data about a variety of object types and save it somewhere (that sounds like what Google Forms does well). Was there a way to use Google Forms, but have it work offline? I poked around a bit, and found that yes, some interesting tools have been built on top of Google Drive apps to get us exactly what we were looking for.
Because Appsheet offers offline functionality out of the box, it’s the one we decided to build our prototype in. Here are the steps for creating an installable data-collecting app in Appsheet:
You will probably want to do further configuration of your app inside Appsheet, but at this point you can already deploy the app, which means it can be installed on a mobile device inside the Appsheet wrapper, and start collecting data. If you collect data while offline, the app will show that it has unsynced data, and you can manually sync once you’re back online.
The further configuration we’ve done has mostly been around setting up specific field types, such as timestamps, signature fields, and ‘drawings’, which allow users to take a photo, draw on it, and upload the result. In the Appsheet interface for your app you can set up advanced field types by going to Data > Columns and adding settings per column.
Appsheet’s terms of use may be overly onerous for some organisations, and it does charge a monthly fee per user, which means it’s probably not going to work for apps with a goal of rolling out to huge user bases. With our finite pool of users working on a handful of devices, and the high annual costs of competing solutions, a monthly per-user subscription is something that works for us.
Having the data in a spreadsheet we can easily back up, restore from and export is handy, and means we have easy access to our data in case we need to change our solution in future. Most importantly, we’ve gotten much further much faster than we would have if we had started out writing code.
Our results so far are very promising and we see a high likelihood that this app will be able to meet the data-collecting needs of our internal team. We’re now in an active testing phase, pushing the performance of the sync functionality and making sure we’ll be able to support this solution. I can recommend looking into Appsheet or similar services to any organisation faced with a need for relatively straightforward mobile, offline-friendly data collection by a finite group of users. I think this platform also has a lot of potential as a rapid prototyping method for building interactive wireframes that function as real apps and give you real data to play with.
Experiments in media, technology & user experience from…
8 
Some rights reserved

8 claps
8 
Written by
Backend developer and creative technologist at ACMI, gardener & crafter elsewhere. She/her.
Experiments in media, technology & user experience from inside @ACMI, Melbourne.
Written by
Backend developer and creative technologist at ACMI, gardener & crafter elsewhere. She/her.
Experiments in media, technology & user experience from inside @ACMI, Melbourne.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@techgigdotcom/top-3-no-code-and-low-code-tool-builders-for-developers-932ac3b0aaea?source=search_post---------139,"Sign in
There are currently no responses for this story.
Be the first to respond.
TechGig
Aug 24, 2021·3 min read
We developed a list of the most feature-rich internal tool builders you can use now based on these factors that toolmakers prefer to select a low code or no code tool builder.
There are two ways to construct internal tools: with code and without code, depending on your tech skills and capabilities. The use of an internal tool builder is required in the second option.
Although coding offers a lot more flexibility, using internal tools allows you to create the software you need faster, with less effort, and without blowing your budget.
What is an internal tool? An internal tool is usually internal-facing software that you create to automate or simplify some of your company’s internal activities. Internal tools are designed to automate micro-processes, increase team productivity, improve customer assistance and ticket management, and so on.
A CRM, a database GUI, an admin dashboard, and other internal tools are examples. Custom internal tools outperform ready-made general out-of-the-box solutions since they are always designed with your business operations in mind.
Preferences for an internal tool builder
We developed a list of the most feature-rich internal tool builders you can use now based on these factors:
1. UI Bakery There is no free plan available. The cost of a paid subscription per end-user begins at $7 per month. Based on the criteria, the following are the key features:
2. Retool There is a free plan available. The cost of a paid subscription per end-user begins at $10 per month. Based on the criteria, the following are the key features:
3. JetAdmin There is a free plan available. Paid subscriptions begin at $24 per month per user. Based on the criteria, the following are the key features:
ConclusionEach of the internal tool builders listed has its own set of advantages and disadvantages. They differ in terms of customization options, price points, the level of data protection they can provide, third-party integration capabilities, and customer support, among other things.
You should see and attempt all of them inside before engaging with a specific internal tool / internal database app builder. The majority of the internal tool builders we’ve listed offer a free trial (UI Bakery, DronaHQ) or a forever-free plan (UI Bakery, DronaHQ) (Retool, Internal, Jet Admin). In general, such plans limit the number of features you can try.
India's Largest Tech Community | 4 Million+ Developers | Guinness World Record Winner | Limca Book of Records
33 
33 
33 
India's Largest Tech Community | 4 Million+ Developers | Guinness World Record Winner | Limca Book of Records
"
https://medium.com/@saxenasanket135/drawing-a-polygon-on-the-base-map-and-rendering-cog-file-based-data-for-that-region-in-an-optimal-ee21fdc8448c?source=search_post---------140,"Sign in
There are currently no responses for this story.
Be the first to respond.
Sanket Saxena
Jul 13, 2020·3 min read
A cloud optimized GeoTIFF is a regular GeoTIFF file, aimed at being hosted on an HTTP file server, whose internal organization is friendly for consumption by clients issuing ​HTTP GET range request.
If you are very new to the COG data format, please refer to my article on the COG overview and how we can create and validate them.
The project is live here at render-cog-data-for-polygon. It allows users to draw a polygon on the map and render COG data for that region in the most optimal way, which is an enhancement to work done by Chris Holmes in COG map.
The map is powered by tiles.rdnt.io, a tiling server hosted on AWS lambda.
We’ll be following six steps hereby to achieve this, which are:
Let’s go!
1. Add Base map layer and debug layer:
We have added two layers here. First is the base map layer provided by Open Street Map and on top of that, a debug layer is added which renders a grid outline for the tile grid/projection along with the coordinates for each tile.
2. Adding buttons for different COGs:
Different buttons hold URLs for different COGs stored on AWS S3 storage service. The text box allows you to enter the URL for any online COG. After submitting it’ll zoom to its geolocation and render tiles.
3. Adding draw interaction to map:
Button on the top right enables the drawing of a polygon on the map. With the polygon drawing control active, click on the map to add the points that make up your polygon. Double-click to finish drawing. It is achieved through ‘addInteraction’ method provided by OpenLayers.
The ‘addFeature’ event is triggered when a feature is added to the source. Its callback function captures the feature and its geometry passing it to ‘loadTiles’ function.
When any of the COG is selected, it’s URL is saved in ‘cogUrl’ global variable. If ‘cogUrl’ is not an empty string, we do the followings:
4. Adding COG layer to map:
‘createTilesUrl’ function creates the tiles URL taking URL of COG as an input parameter. This map is using tiles.rdnt.io tiling server, which is an instance of marblecutter hosted on AWS Lambda. The function can be changed to use another lambda tiling server.
5. Conditional loading of tiles:
‘setTileUrlFunction’ sets the tile URL function of the source. This function is called only when the condition is true. ‘intersectsExtent’ tests if the geometry of that drawn polygon and the passed extent intersect, returning boolean true or false. In other words, the tile URL function will be set only for those tiles which are intersecting with the polygon.
6. Cropping loaded TileLayer in polygon shape:
Now when we’ve loaded desired TileLayer we need to crop it in that shape of the polygon. ‘addFilter’ function provided by ol-ext allows to add crop filter to tile layer.
The results of these aforementioned steps can be visualized in this attached screenshot.
We should note that, Even if we skip step B, the result will be the same but that won’t be an optimal approach to achieve it because in that scenario we’ll be loading all tiles in that view, including even unnecessary ones.
I have pushed my code here. Your feedback on this is much appreciated.
Special thanks to Richa Hukumchand for introducing me to this promising cloud-native technology and for all your guidance :)
Thanks for reading :)
See all (156)
66 
66 claps
66 
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/yugabyte/spanning-the-globe-without-google-spanner-c7c8683dac65?source=search_post---------141,"There are currently no responses for this story.
Be the first to respond.
Google Spanner, conceived in 2007 for internal use in Google AdWords, has been rightly considered a marvel of modern software engineering. This is because it is the world’s first horizontally-scalable relational database that can be stretched not only across multiple nodes in a single data center but also across multiple geo-distributed data centers, without compromising ACID transactional guarantees. In 2012, Google introduced Spanner to the outside world by publishing the details of its core architecture in an academic research paper. A follow-on paper in 2017 provided a deep dive into the SQL query processing layer. These two papers combined highlight how Spanner solves some of the thorniest issues in building a geo-replicated globally-consistent RDBMS. At the top of the list is the use of TrueTime, Google’s proprietary high-precision global atomic clock, for executing distributed transactions and atomic schema changes across multiple shards where each shard could be located in a distant data center. Such transactions adhere to `external consistency`, the strictest isolation level possible. Next comes the use of per-shard distributed consensus (based on Paxos) for ensuring single-row linearizability.
Google made Spanner available for public use in 2017 as a managed database service on Google Cloud. However, users interested in adopting Spanner have to weigh the benefits of a fully-managed service with the proprietary and expensive nature of the service. This brings open source geo-distributed SQL databases like YugabyteDB into consideration. YugabyteDB’s sharding, replication, and distributed ACID transactions design principles are inspired by the Google Spanner architecture. However, it does not require any specialized infrastructure components like Spanner’s TrueTime atomic clock. Distributed transactions instead rely on an intelligent tracking of max clock skew across different nodes of a cluster. This allows YugabyteDB to run natively on any commodity infrastructure on any public or private cloud. Additionally, YugabyteDB relies on the easier-to-understand Raft protocol (instead of Paxos) for its per-shard distributed consensus implementation.
Kubernetes, an open source technology that also originated at Google, has rapidly grown over the last few years to become the de facto infrastructure standard for application deployment, using containers as the foundation. Containerized deployments with Kubernetes allow developers and operations engineers to pick the best-of-breed cloud platform for their application as opposed to the one-cloud-fits-all value proposition of proprietary managed services like Google Cloud Spanner. So the natural question is — what if we could have a Spanner-like globally-consistent multi-region deployment of YugabyteDB on Google Kubernetes Engine (GKE)? We can then simply replace GKE with other Kubernetes distributions and managed services (including Amazon EKS and Azure Kubernetes Service) to realize the benefits of geo-distributed SQL on the cloud of our choice. We can even deploy a single database cluster across multiple cloud platforms if needed. As highlighted in 9 Techniques to Build Cloud-Native, Geo-Distributed SQL Apps with Low Latency, the deployment combinations are seemingly endless, thus allowing users to pick and choose the combinations that best fit their business needs. For the purpose of this post, we will focus on the Single Cloud, Multi-Region deployment as shown in the figure below.
Note that Kubernetes is not a mandatory requirement here since open source geo-distributed SQL databases can be deployed equally well even on regular cloud VM instances. However, the significant DevOps tooling necessary for VMs for each cloud platform can become a barrier to the promise of picking the best-of-breed cloud platform for a given application. With managed Kubernetes, this DevOps challenge is mitigated to a great extent. While administering Kubernetes clusters itself is cloud platform specific, managing application deployments on Kubernetes is cloud agnostic given Kubernetes’ standard deployment constructs and tooling.
Deploying multi-region applications on Kubernetes is a non-trivial endeavor given some of the core design principles behind Kubernetes. The first question is — why can’t we deploy a single Kubernetes cluster across multiple regions? While the Kubernetes worker nodes can be theoretically placed across multiple regions, the issue lies in how the single Kubernetes master node is deployed. Since network partitions across regions will be common in a single multi-region cluster, the Kubernetes master node will often get cut off from Kubernetes worker nodes in other regions. These worker nodes can no longer receive commands from the master, and hence are no longer available for scheduling/orchestration. Additionally, the master will treat these worker nodes as lost and reschedule the missing pods on the available worker nodes. This results in lost capacity in all regions disconnected from the master and insufficient capacity in the region connected to the master.
The next obvious question is — what if we run one replica of the master node in each region (alongside a replica of the etcd metadata store)? We have to ensure there is always a single `leader` master node so that there is a single source of truth for the cluster. Note that the number of regions has to be odd in this case for etcd’s Raft distributed consensus to work properly. This is the recommended approach for making Kubernetes masters highly available in a single cluster deployed across multiple zones of a single region. However, this approach is not recommended for a multi-region deployment. Even though the metadata changes can be persisted fast enough using a quorum of etcd replicas, the cross-region latency problem can get magnified when Kubernetes master initiates cluster-wide administration operations based on this persisted data.
Given these cross-region networking challenges, the consensus in the Kubernetes community today is that a single cluster should be limited to deployment in a single region only. Multi-region deployments have to be solved using multiple clusters, where each cluster is scoped to a single region only. SIG-Multicluster is a dedicated Special Interest Group in the Kubernetes community focused on solving common challenges related to the management of many Kubernetes clusters, across multiple cloud providers and applications deployed across many clusters. Kubefed is a sub-project in SIG-Multicluster currently in alpha that has the most promise in this regard. It is a successor to the original Federation v1 specification introduced in Kubernetes 1.3.
While there isn’t a single standard to connect multiple heterogeneous Kubernetes clusters, connecting multiple homogeneous Kubernetes clusters on a single cloud platform is certainly possible. For example, we can connect 3 Google Kubernetes Engine clusters by updating the kube-dns service on each of them to access the kube-dns service on other clusters for namespace-scoped requests. We can even make such a configuration secure from unauthorized access by using the global access option of Google Cloud internal load balancers instead of publicly-accessible external load balancers.
The instructions in this section describe a multi-cluster GKE deployment, leveraging the global DNS approach noted above, for running a multi-region YugabyteDB distributed SQL database cluster. This configuration is shown in the figure below. Detailed instructions including prerequisites for the scenario described here are available in YugabyteDB Documentation. The end result is a Spanner-like globally-consistent multi-region distributed SQL cluster on Google Cloud.
Following commands create 3 Kubernetes clusters in 3 different regions ( us-west1, us-central1, us-east1), with 1 node in each cluster. Note that global access on load balancers is currently a beta feature and is available only on GKE clusters created using the rapid release channel.
Create clusters
Create a storage class per zone
We need to ensure that the storage classes used by the pods in a given zone are always pinned to that zone only. Detailed instructions on how to do so are available here.
Create load balancer configuration for kube-dns
The yaml file shown below adds an internal LoadBalancer, which by definition is not exposed outside the Google Cloud region it was created in, to Kubernetes's built-in kube‑dns deployment. By default, the kube-dns deployment is accessed only by a ClusterIP and not a load balancer. Additionally, we allow this load balancer to be globally accessible so that each such load balancer is now visible to the 2 other load balancers in the other 2 regions. Note that using external load balancers for this purpose is possible but is not recommended from a security best practices standpoint. This is because the DNS information for all the clusters would now be available for access on the public Internet.
Apply the configuration to every cluster
Download the yb-multiregion-k8s-setup.py script from the YugabyteDB GitHub repo that will help you automate the setting up of the load balancers.
The script starts out by creating a new namespace in each of the 3 clusters. Thereafter, it creates 3 internal load balancers for kube-dns in the 3 clusters. After the load balancers are created, it configures them using Kubernetes ConfigMap in such a way that they forward DNS requests for zone-scoped namespaces to the relevant Kubernetes cluster's DNS server. Finally it deletes the kube-dns pods so that Kubernetes can bring them back up automatically with the new configuration.
Open the script and edit the `contexts` and `regions` sections to reflect your own configuration. And then run it as shown below.
We now have 3 GKE clusters that essentially have a global DNS service as long as services use zone-scoped namespaces to access each other. Now we are ready to install a single YugabyteDB cluster that spans these 3 GKE clusters.
Add YugabyteDB Helm charts repository
Create override files
We have to create an override file for each GKE cluster so that the YugabyteDB Helm chart can correctly bring up one third of the nodes in each cluster. The example file for the us‑west1 GKE cluster is shown below. You can find the files for the other two clusters here.
Note that we also set the leader_failure_max_missed_heartbeat_periods option to 10. This option specifies the maximum heartbeat periods that the leader can fail to heartbeat before the leader is considered to be failed. Since the data is geo-replicated across data centers, RPC latencies are expected to be higher. We use this flag to increase the failure detection interval in such a higher RPC latency deployment. Note that the total failure timeout is now 5 seconds since it is computed by multiplying raft_heartbeat_interval_ms (default of 500ms) with leader_failure_max_missed_heartbeat_periods (current value of 10).
Install YugabyteDB
Now create the overall YugabyteDB cluster in such a way that one third of the nodes are hosted in each Kubernetes cluster.
Default replica placement policy treats every yb-tserver as equal irrespective of its placement_cloud, placement_region, and placement_zone settings. Run the following command to make the replica placement region/cluster aware so that one replica is placed on each region/cluster.
We can even make one region the default leader for all the shards in the YugabyteDB cluster. This can act as an important performance optimization where cross-leader communication happens internally to a single region, thus keeping transaction latency low. Multi-region deployments of Google Cloud Spanner also start out with a default leader region for exactly the same reason.
Now we can connect clients such as ysqlsh, the Yugabyte SQL shell, to the database. Let us connect to the nodes connected in the us-west1 region.
You can run YSQL queries using the sample e-commerce schema that ships with YugabyteDB.
It’s time to test the resilience of the DB cluster when subjected to the complete failure of one region. We will simulate such a failure by setting the replica count of the YugabyteDB StatefulSets to 0 for the us-central1 region.
If we re-run the queries from Step 5 after connecting to the nodes in the us-west1 region, we will see that there is absolutely no impact to the availability of the cluster and the data stored therein. However, there is higher latency for some of the transactions since the farthest us-east1 region now has to be involved in the write path. In other words, the database cluster is fully protected against region failures but may temporarily experience higher latency. This is a much better place to be than a complete outage of the business-critical database service. The post Understanding How YugabyteDB Runs on Kubernetes details how YugabyteDB self-heals the replicas when subjected to the failure of a fault domain (the cloud region in this case) by auto-electing a new leader for each of the impacted shards in the remaining fault domains. The cluster goes back to its original configuration as soon as the nodes in the lost region become available again.
Google Spanner has been the gold standard for powering globally-distributed and/or horizontally-scalable relational apps. A cost-effective, multi-cloud alternative to Spanner helps users derive the same benefits as that of Spanner but on the cloud and infrastructure of their choice. YugabyteDB is such an alternative. It is a Spanner-inspired, geo-distributed, PostgreSQL-compatible SQL database that is not only fully open source but is also easy to operate via Kubernetes. Deploying YugabyteDB on a managed Kubernetes service like GKE strikes that perfect balance of a managed infrastructure service (that keeps operator burden low) coupled with an open cloud native SQL database (that enables picking the best cloud platform for a given application).
While YugabyteDB can be deployed in single zone and multi-zone Kubernetes clusters with extreme ease, multi-region Kubernetes deployments require additional considerations. This has less to do with YugabyteDB and more to do with the fact that multi-region Kubernetes is essentially a multi-cluster Kubernetes problem because of Kubernetes’ underlying architecture. One solution to this problem is highlighted in this tutorial where we connect multiple GKE clusters using a global DNS that can route namespace-scoped pod/service requests in one GKE cluster correctly to other GKE clusters. Running a globally-consistent YugabyteDB cluster becomes simple thereafter.
Originally published at https://blog.yugabyte.com on April 16, 2020.
YugabyteDB is the open source, high-performance…
10 
10 claps
10 
Written by
Product @harnessio, ex-Yugabyte, AppDynamics & Salesforce
YugabyteDB is the open source, high-performance, distributed SQL database for global, internet-scale apps.
Written by
Product @harnessio, ex-Yugabyte, AppDynamics & Salesforce
YugabyteDB is the open source, high-performance, distributed SQL database for global, internet-scale apps.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@plazagonzalo/snowflake-snowpro-core-certification-stages-24be111e65ea?source=search_post---------142,"Sign in
There are currently no responses for this story.
Be the first to respond.
Gonzalo Fernandez Plaza
Sep 15, 2021·5 min read
Stages in Snowflake specify where data files are stored (staged) so that the data in the files can be loaded into a table. It is the location of where the files are before moving them to SnowFlake tables. There are two types of stages, internal and external…
"
https://medium.com/@vallard/cisco-pipeline-the-last-survivor-of-the-innovate-everywhere-challenge-a5029d3f2eed?source=search_post---------143,"Sign in
There are currently no responses for this story.
Be the first to respond.
Vallard Benincosa
Mar 22, 2017·4 min read
Last year there was an internal competition in Cisco called the “Innovate Everywhere Challenge”. Cisco had a public blog about it here. There were 1,100 ideas submitted and my idea to build a serverless cloud service reached the semi-finals along with 15 other lucky people.
Following the semi-finals each team was asked to videos and further refine the pitch. This was then blasted out to the Cisco-verse again. From here, 6 teams were selected to the finals round. Our team again made the cut. This time we were to refine the pitch and go before a live internal broadcast and convince a panel of judges in 5 minutes that our project was worth investing in.
There were 3 winners selected of the six finalists. We were not one of them. In hindsight I can see why we didn’t win. Our idea was still vague and while we presented with passion it didn’t quite captivate nor convince.
In the opening scene of “Indiana Jones and the Last Crusade” young Indiana Jones tries to steal back the cross of Coronado. He fails and has to return it to a man who tells him: “You lost this time kid, but that doesn’t mean you have to like it”
Not liking to lose can create fuels for motivation. In the months that followed our loss, most of the other teams disbanded. Even some of the 3 finalist teams fell apart with some team members leaving Cisco. Our team suffered losses too as some people stopped working on the project all together. Through it all, however, I kept on plowing through. I didn’t like losing and I didn’t like a panel of judges telling me they thought something else was more worthwhile. I kept developing and working at it on and off company time learning skills I didn’t think I’d have to know before starting this including Kubernetes and Functional programming.
In January of this year, right before Cisco Live in Berlin, I finally shipped an alpha, with the help of some of the people on my team. We released https://ciscopipeline.io to the public and narrowed the idea all the way down to the ability to create Cisco Sparkbots.
While there are other platforms out there for creating bots in chat services, for doing it in Cisco Spark, nothing is easier. And why wouldn’t Cisco want to offer a service to help users of its own product? Pipeline was built on Kubernetes, Golang, and Clojurescript with React (reagent and re-frame). Perhaps the most shocking thing about our solution was that it was built on top of all Cisco products. It uses Cisco Metacloud under the covers (No AWS, no Azure, no GCE) and that Metacloud runs on Cisco UCS with Cisco switches.
Now the sparkbot market isn’t necessarily the hugest market opportunity in tech right now. Cisco has a lot of competition in this market from Slack, HipChat, and other messaging platforms as well as other companies that offer bot services across different platforms. Focusing just on Spark limits our user base quite a bit, but still gives us experience in running a real service.
Since we’ve released we’ve added support for code editing within the system using Python in the backend with code samples and better documentation. The Cisco Pipeline Business Unit today is an unsanctioned Business Unit comprised of one person. So we are agile, but slow.
Pipeline is the only project from the Innovate Everywhere Challenge that has shipped. The other projects have either been absorbed by the Cisco universe in other projects, or are still being evaluated.
I would argue that if a project doesn’t ship in 6 months after 6 months of refinement and planning then it should be considered dead. Pipeline was late and could be considered dead as well. The contest ended in April 19th 2016 and Pipeline didn’t ship until January 2017. The reason for this is that the world moves too fast and something designed and then released a year later has probably already been disrupted.
I will say however, that in the end: It did ship.
What I’ve learned is that I can innovate even if no one is watching. While Pipeline may not be the hugest hit, it is still something that I created from beginning to end. Something I am proud of and will show off at the drop of a hat. And if I have to shut it down, or no one uses it, or someone else takes credit, I have no problem. It’s not the golden egg that matters. Its the goose. Be the goose, and keep laying golden eggs.
Dad, code
4 
2
4 
4 
2
Dad, code
"
https://medium.com/microsoft-student-champs-sri-lanka/how-to-create-your-first-customized-app-with-face-api-using-net-c94df8651240?source=search_post---------144,"There are currently no responses for this story.
Be the first to respond.
In the previous part of I discussed what is the internal structural behavior of face API. Then in this part let’s move on to some practical experiments. Simply I just want to do here, how to implement face API into your customized Projects. For simplicity, I choose Microsoft .NET SDK for this purpose.
Basic Understanding HTTP web request handling
Fundamental knowledge on Microsoft Visual Studio and POSTMAN
In the previous article I covered up what is the internal interaction of Face API and how it works. I had done a video tutorial also on that article.Please find it in here. From the last point that we have stopped let’s start up-to the .NET customized application.Then let me divide the article into 03 main section as follows. Because it will make easy for you.
1. Get a Face API subscription from Microsoft Azure
2. Simulate Face API using POSTMAN
3. Simulate Face API using .NET SDK
Before moving on to the Development, you have to get a Face API subscription from Microsoft Azure. let’s take a tour to the Azure portal.
Once you logged in to your Azure Portal it will look like this. If you already have a Face API subscription key. You don’t need to have the following parts. If you having it, it will display like above. Otherwise please follow the following steps.
According to the above figure, search type Cognitive Services and search it. When you are typing that it will be auto-suggested.Then move on to the Cognitive Services section. Then you could be able to see a window as bellows.
If you are already having subscriptions on Cognitive Services, those will be listed on here. However, If you are freshens one to this. Click on Add Button above. Then there will be displayed a window like below.
On this window (Marketplace), search the Face. (In previously this one came as Face API. But from 2019 it will be renamed as “Face”)
Then you will be able to see a figure like above. Then click the create button on there and create a Face API Service. In this procedure, you have to follow a form.
Please do not give numerical values to your service name. Because it was restricted from Microsoft Azure. Then select the subscription method, location, and Pricing Tier. If you are new to Azure you will receive a 200 USD free subscription at the initial level. Then your subscription will be displayed as “Free-Trail”. Select the resource group or otherwise, you have to create a new resource group if you have not yet. Then Finish the process and move on to your Azure Portal’s dashboard. As of 1st figure, you will be able to see your subscription on there from the given name.
elect the subscription on your dashboard. After the portal will automatically redirect to the inside of its configurations. From that window, you can see your current selected subscription on the Menu panel on the left side. Go to resource management and click on “Keys and Endpoint”.
Then you will be displayed a window like a bellow.
You will be getting two API keys here. In addition to API keys, your API endpoint is already here. When using Face API in your customized application you have to use this endpoint. You can use one of those API keys as an authentication token to your endpoints. At any time that you want, you can renew those API keys by clicking the refreshing button.
So this is the story so far too for getting the Face API subscription key from Azure.
Cool. Now we have an endpoint and its authentication keys. What we have to look up next? :D. No doubts its POSTMAN. :D:D. This is the super easy platform to verify your API endpoints.
If you currently did not have the POSTMAN, you can download it as either chrome to either or desktop application.(Recommended- Desktop App)
Download Chrome Extensions
Download Desktop App
In this scenario, you are sending an input throughout your customized application via Face API. It accepts the data from your request’s body. Therefore you have to use POST as your HTTP web request. Here you are interacting with JSON formatted data as I explained in theoretical session. Therefore, make sure the content type of your data should be JSON data. That’s why you have to mention it in your header as above. Then the other key value is “Ocp-Apim-Subscription-Key”. You have to mention this one on your header and assign the API authentication key which you had got from the Azure subscription.
Then what you have to call here? Simply the API endpoint which you have got from the subscription. You can attach any amount of parameters to this endpoint. Because in the hidden implementation, inside the Face API it will capable to handle “request.params” access points in your API endpoint.
You can learn more about how to handle the above API endpoint with necessary parameters on the following link.
The Detect API
The above image shows you what the current available regional vice API endpoints based on Microsoft Azure Data Centers.
The above figure shows you the valid parameters and useful request headers. This is Microsoft’s documentation about the Detect API.
In this tutorial, I will pass my parameters as follows.
In the above example, I will be returning The face Attributes. So what are face attributes? I had done an in-depth analysis of face attributes on the previous blog. Then there is a video tutorial also. If you forget those theoretical parts, please follow those first. I pass here age, gender, glasses, and emotion as selected face attributes. If you want to learn more about how to pass this endpoint, you could refer to the Detect API documentation above.
The following figure shows you how to pass your image URL which can analyze using the Face API. In the body of POSTMAN, you are passing this one as a JSON object. That is why previously I mentioned you should have to configure your content-type as “application/JSON”.If you are familiar with REST API and any other Full-stack Development, you might have prior experience with this one. Make sure when u attaching a URL of the image, the particular image should be published on the Internet. Face API does not access to your local file structures inside of Azure Infrastructure.
Then, your output would be display as bellows. You can see here, by default it included the Face ID. Because it is the Primary key to identifying each face in a given image.
faceRectange attributes show the location of a particular face throughput of its orientation. There is a parent element called faceAtrributes and it hooked those age, gender, emotion, and glasses, etc. Please check the previous article or video on what are the possible outcomes of the above-defined attributes.
In the theoretical section, I mention there are emotion attributes listed inside the Face API. You can see here all listed emotion attributes are shown the values as probabilities on each image. The summation of all emotion attributes is equal to 1.
Then if u get the output as follows, your API configuration and part are OK. Then now we are moving on to the .NET SDK to simulate the functionalities that we had done using POSTMAN.
Start your Visual Studio and open a .NET Console Application. Here you have to use a dependency. Simply a .dll file. If you are not much familiar with, NET, you herd about plugin dependencies in JAVA. Which are nothing but JAR files in JAVA contain the pre defied classes, interfaces, and some other 3rd party dependencies. Even in package managers like NPM and Yarn do the same as this. By itself, it provides some other components, which are developed by other developers. Then you know while importing the JAR file into your JAVA project, you can directly access to either classes or interfaces inside the particular JARA file.
Therefore here we are using dependency called “Microsoft.ProjectOxford.Face”. It is an open-source repository. You will be able to find it here on GitHub.
Then the next question is popped up into your mind is, how to install this dependency on your .NET Project. Like npm. In .NET there is allowed to have a package manager console. We call it “Nuget Package Manager Console in .NET”. In the Visual Studio IDE, it is enabled on the tools menu in Visual Studio’s top menu bar.
Tools → NuGet → Package Manager → Package Manager Console
After you selecting the Package Manager Console at the bottom of your visual studio a console panel may open.
If you want to go to the GUI of this Nuget Package Manager you have to select PackageManager Console. Then you will be able to see a GUI on NuGet packages.
If you are using the package manager console, there is a separate site called NuGet org, which included all available NuGet packages with its versions.
At that site, you have to search “Microsoft.ProjectOxford.Face” and copy the command given there and paste it in previously opened package manager console and press enter.
Be careful on your C# version, because here we are using the async functions in C#. So the async functions are only compatible with the C# 7.2 and upper. Otherwise, you have to upgrade your C# version.
We apply “Microsoft.ProjectOxford.Face” previously into the project. Then you can see here there are some red color rectangular frames. Keywords that are surrounded by red cor rectangular frames come from above “Microsoft.ProjectOxford.Face” dependency. You can clone this project from GitHub on here.
In this article, I discussed how to configure Face API with the .NET SDK. In the upcoming parts let see how to configure Face API into web frameworks like React JS.
Microsoft Learn Student Ambassadors of Sri Lanka
9 
9 claps
9 
Written by
Software Engineer at WSO2 | Gold Level Microsoft Learn Student Ambassador
The official medium publication of Microsoft Learn Student Ambassadors of Sri Lanka
Written by
Software Engineer at WSO2 | Gold Level Microsoft Learn Student Ambassador
The official medium publication of Microsoft Learn Student Ambassadors of Sri Lanka
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@hobiehenning/microsoft-intune-mdm-thinking-about-the-future-of-it-support-work-life-balance-6481220a3262?source=search_post---------145,"Sign in
There are currently no responses for this story.
Be the first to respond.
Hobie Henning
Jul 22, 2019·5 min read
I have been playing with Windows Intune at work the last two weeks just kicking the tires as part of an internal test and its really stunning how much faster and easier to use it is than something like System Center Configuration Manager (SCCM). I ran a command in the Intune console to run a PC refresh and the test laptop rebooted within 30 seconds and started rebuilding it. After that, I joined it to Azure Activity Directory and after applying all of my policy I had previously configured without it prompting me, installing the security configuration that I had assigned it and the applications I had required to the laptop, including Chrome, Spirion, Microsoft Office, and others. This definitely feels like the future and put a big smile on my face. I can see in about 5 or 10 years the market share of Windows Server diminishing as more IT departments more to the cloud.
Yeah, I know there has been a lot of heartburn in the IT community about Microsoft coming for people’s jobs, but maybe because I’m a hybrid role that deals with end-users as much as I do servers, I see this a boon to my time at work, giving me back valuable time to introduce better security measures and educate my end-user on what technology we already have. If you subscribe to the Office365 blog or YouTube page(I highly recommend you do to just keep up…links posted below), there are always new features across Office365’s services. Microsoft is continuously updating the desktop clients, web apps, mobile apps, etc across iOS, Windows, Android, Mac, and web browsers. I struggle every month just to introduce a fracture of the features we get through Office365 to my end-users.
For example, I have just started pushing Microsoft To-Do to my coworkers now that they have a Mac app and excellent iOS apps. As a college we have about 50% share in Mac versus PC and Microsoft To-Do integration with Outlook flagged items and the ability to share lists with coworkers no matter the device they happen to use has really caught the attention of my coworkers, especially people who manage a lot of projects who I have introduced to Microsoft Planner. By not spending so much of my time configuring servers and patching them, it’s freed me up to teach them about those tools. Security and compliance are of vital importance as one of my primary job functions, but why not delegate that to Microsoft’s teams of engineers who work on that full time when I could be providing top-notch security, compliance, and enhancing people’s day to day lives by exposing them to technology they did not even know we had access to?
I’ve always worked in IT departments with just two, maybe three IT staff. I see the move to the cloud as a great thing for smaller departments, especially those people who yearn for a better work-life balance. Oftentimes, bosses who do not work in IT do not understand all the extra (often not compensated for), time we dedicate in the after hours or wee hours of the morning to maintaining, preparing, and securing the backend gears of an organization. One of the reasons I left my previous job was because of burnout. I didn’t realize it at the time, but working as a junior system administrator + backfilling desktop support tickets + working maintenance windows + always being on call 24/7/365 was just exhausting. We were a telecommunication company stretched across 5+ states so there was ALWAYS something going on. I used to get 200–300 emails a day that I didn’t necessarily have to reply to, but out of habit I kept an eye on them to get a pulse on the coming and goings of my coworkers because I could always anticipate trouble tickets or phone calls based off of what Operations or Inside/Outside plant were up to. Its well-known that IT staff are often overworked to the point of exhaustion and its also known that a person who is exhausted does not perform as well as somebody who is well-rested.
Maybe I’m an optimist, but I just see moving our IT operations over the cloud as a win-win in the long term. I don’t see things getting easier or less complicated to be honest. if anything, it's the opposite. I’m doing more and more for my end users from data management to cloud applications to SharePoint forms for HR and other operations to hosting video services in the cloud. I’m going to be going to focusing on Windows Virtual Desktop, Microsoft Intune, Windows Admin Center, PowerShell, SharePoint Online, Cyber-Security, Office365 , Surface devices for Creatives, and more. If you’re in Orlando feel free to reach out, I’m always welcome to making new friends in the industry. I look forward to seeing what other technology that I can embrace going forward to may and my end-user’s lives better.
-Hobie 🤖
Microsoft Intune
www.microsoft.com
Microsoft Ignite
www.microsoft.com
Office365 Blog
techcommunity.microsoft.com
Office 365 YouTube
www.youtube.com
IT Support Specialist V and Spring Hill College graduate who loves all things tech. If it has a flashing LED it has my immediate attention.
See all (466)
3 
3 claps
3 
IT Support Specialist V and Spring Hill College graduate who loves all things tech. If it has a flashing LED it has my immediate attention.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/zoosk-engineering/refactoring-dinosaur-java-code-to-the-latest-and-greatest-b91a360c6b58?source=search_post---------146,"There are currently no responses for this story.
Be the first to respond.
At Zoosk, the backend contains internal Java microservices that handle up to 400,000 requests per minute. New features and debugging issues in production became troublesome because the codebase was polluted with anti-patterns. It did not help that the creators of the services left the company without having much documentation on why certain code behaved the way it did. The services were abstract to handle every possible scenario, even ones that would never happen in production. “Simplicity over complexity, complexity over complicatedness” otherwise one ends up like this:
I never said “WTF” more in my life then when I first looked at the Java code. As part of migrating our Java microservices at Zoosk to Amazon Web Services we wanted to improve the quality of the code and reduce the number of “WTF”s developers would have to face. It got to the point where people had a sour taste for the Java services. It is a challenge for companies to update their old technology. The whole process is a pain and a huge investment, especially if the application is a giant monolith. Microservices following the Single Responsibility Principle (SRP) provides a simpler refactor because they are not complexly intertwined with other code paths of your application. They are responsible for one thing and testing is simple. The refactor started with investigating frameworks and best practices other companies were adopting. The research helped craft an informed decision on what aspects of the services needed change. The issues solved included updating the old stack, easing/automating the development process, improving API documentation, improving monitoring and alerting, simplifying, and evangelizing the new processes.
How: Java is backwards compatible the upgrade and updating dependencies was easy. Make sure unit tests are in place for regression testing of the upgrade.
Why: Falling behind on updates lead to missing out on features and bug fixes of libraries that have moved to Java 8. By updating the Java version, we removed the annoying UnsupportedClassVersionError caused by JVM and the compiled artifact having different Java versions.
How: Spring Framework to follow RESTful methodology in the routing tier. It was an easy adoption because the clients were internal calls from our PHP monolith.
Why: Following no standards in the code base led to a lot of confusion in understanding how the service worked. Services contained one POST endpoint where the actual endpoints that needed to be called were found inside the body of the request. This is great for a batch call endpoint, too bad none of the services used batch calling. The endpoint just became a dumping ground for all calls and debugging what was called when an issue occurred was a nightmare. Instead of following HTTP standards of returning a 401 if a user is unauthorized , all services returned HTTP 200 even if there were errors in processing a request. There was a lot of home brewed code to handle request processing which could have been replaced by Spring with a couple of annotations.
How: Created a Swagger Spec of each service. Ran Swagger CodeGen against the Swagger Spec to create boilerplate code for the Spring Boot App with Swagger UI annotations.
Why: Contract changes for service endpoints involved updating FogBugz pages containing the API documentation that clients used. Developers were not keeping the docs updated whenever a change to the contract of the endpoint occurred. Clients interacting or adopting the endpoint would use stale documentation leading to confusion and developer time spent debugging. With Swagger there is one source of truth for the API documentation. The docs are generated from the annotations in the codebase. Every developer gets code reviewed therefore developers who modify the contract of the service without updating the annotations to reflect the new contract would be found.
How: Java VisualVM to profile the Spring boot app and run load tests against the service. We discovered from the results that services had way more heap allocated than it needed.
Why: Charges are made by how much is used when in the cloud. If services are underutilize, money goes flying into the trash. By optimizing the JVM to be more performant and use less resources one can maximize the dollars spent. We wanted to instill the practice of not arbitrarily setting JVM values. Instead we wanted to load test with predicted traffic and analyze how much CPU and memory the service actually needs.
Before our Auth Service used an average size 989 MB of memory
After JVM changes the service reduce the average amount of heap memory used to 85.9 MB with no noticable performance impact.
How: Converted from Java 6 JUL to SL4J, removed log guards in the code base, created a standard logback.xml file for all services to consume, and sent logs to ElasticSearch.
Why: All logs used to be dumped into catalina.out. The worse part was it was in a format that made it hard to do root cause analysis. Unlike other tiers at Zoosk that had their logs standardized and shipped to Splunk for querying and alerting. The changes to ship logs to Elasticsearchusing Fluentd in a standardized format allowed us to query the application logs in Kibana. SL4J parameterized logs allowed us to remove the log guards from our code.
How: Replace the custom JMX metric logging framework with Spring Boot Actuator Metrics framework. We used Telegraf to funnel the data into InfluxDB to be visualized in Grafana. New Relic APM instrumentation was added to each service.
Why: By removing the custom JMX framework created at Zoosk we were able to reduce the amount of code we needed to support in favor using Spring for managing our application metrics. Visualization load time decreased in Grafana compared to when we used Ganglia. New Relic has provided us application performance monitoring for all transactions that are served. New Relic has made my life a whole lot easier in debugging issues in production with their APM product.
How: Integrated Cloudwatch, Grafana, Elastalert, Slack, and PagerDuty to our alerting framework for notification of issues with services.
Why: The only time there was an alert raised was when the service was down and a feature on the site stopped working. We have the data now to create rules to recognize these patterns and catch these failures before they happen. For example, if the CPU or memory is over 80% or if we see N number of error logs. Instead of being reactive with our services we became proactive with alerting in place.
How: Evangelized using Sonar Linter, Code Coverage, and JAutoDoc as part of the development phase.
Why: The quality of the Java services were sporadic. Services had complex code and formatting errors that a Linter would have prevent. We had unit test suites that only covered 10% of the code base discovered by using a code coverage tool. Services sometimes contained no Java docs, which JAutoDoc can generate for you. Using these tools added standardized code quality across all services with minimal effort.
How: Enforced test pass before deploys. Used RestAssured framework for integration tests, Junit for unit tests, code coverage reports, and JMeter for load testing.
Why: With code coverage we are able to find branchs of code not tested. One could have hundreds of unit tests, but if the tests only cover 15% of the code base that is not as good as five unit tests that cover 90% of the code. It’s not about the quantity but the quality. In our case it was zero percent code coverage. The existing unit tests were being skipped or broken. No insights could be provided on whether a feature developed broke existing behaviors of a service. In this model, our developer cost for implementing a feature because bugs catchable from a unit test are not caught until they hit production.
How: Created Docker files for each service to integrated with Zoosk Docker framework.
Why: The majority of tiers at Zoosk are in Docker containers and internal tools used to ease the development process involved containerized applications. We decided to follow the standard to allow for easier deployment and testing for Java services. To QA test a feature on Zoosk it requires a QA VM with all containerized apps tagged with the feature name. All the work for setup requires one crane command. Because Java services were not containerized we had to create the artifact, set up the service in the QA VM’s. Now all the QA person has to do is run the same crane command and not do any special setup with the Java services. By moving to dockerized services we were able to leverage the orchestration service Amazon Elastic Container Service and reduce the amount of dev-ops support. For autoscaling it is generally faster to spin up a container than it is to spin up an EC2 instance off an AMI.
How: Converted Java WAR file deployed to Tomcat into a Spring Boot standalone Jar
Why: Testing changes for our Java service required us to SCP the artifact to our development VM, put it in a specific location and name, and restart tomcat in order to get the service in a runnable state to test. Following the steps to run a service was confusing and a hassle, but with Spring Boot I can now run it on my local machine, development VM, or wherever I want all with one command.
How: Adopt open source frameworks such as Spring and Dropwizard to replace custom code.
Why: By shifting to open source one can reduce the amount of code to manage and get new features from these frameworks without having to develop them. Developers around the world use these frameworks. Bringing someone up to speed for developing these services is easier than getting them to learn a custom homebrew framework that is not actively maintained because the guy who made it left.
How: I took the first stab of the refactoring process and documented the process for one service. Members of the team each took a service, followed the guide, and updated with their experience refactoring for migrating the service to the cloud.
Why: Documented steps of development for developers new and old to getting a standardized service to the cloud. This was team developed document where feedback from everyone who will be working on Java services was integrated. The guide improved the speed and estimation accuracy of developing new services in the cloud.
Making the changes to our Java microservices was a tech debt that needed to be address now instead of later. Otherwise every day put off added more dollars to the cost of tackling the task. We were able to simplify and update the code base. We implemented best practices to improve the entire development process of our services. We now have better visibility into the health of our Java microservices and a standardized quality for each service we ship. Developers can now develop, test, and ship a Java microservice to AWS with ease.
Anything and everything the Zoosk engineering team is up to.
3 
3 claps
3 
Anything and everything the Zoosk engineering team is up to.
Written by
Backend Software Engineer
Anything and everything the Zoosk engineering team is up to.
"
https://medium.com/@debipmishra/snowflake-performance-tuning-checks-towards-query-optimization-part-1-c7b8bde48010?source=search_post---------147,"Sign in
There are currently no responses for this story.
Be the first to respond.
Debi Prasad Mishra
Sep 15, 2021·6 min read
Introduction: In general, Snowflake executes the queries quickly and very efficiently by it’s internal mechanism which makes completely different from other vendors in the analytical platform. Doesn’t require any intervention, so often a slowness in the query is a symptom of a mistake in the way query has been written and few external factors. As we all know Snowflake provides the faster query performance without even indexes. Should design the system in such a way that in the extraction step, should aim to minimize the latency of each query i.e. ability to maximize the throughput. And rapidly should able to transform the raw data into business ready format. Finally should able to load the massive volumes of data into the target without any bottlenecks. In this blog I tried my best to put some light on those areas which needs to be taken care of while designing the ETL data pipelines in Snowflake. Let’s move on.
1- Bulk data load: The common method of bulk data load, is transferring data from on-premise to cloud storage. And using the COPY command to load into Snowflake. Before copying the data, Snowflake checks if the file has been loaded already or not, and this leads the very first step to maximize the load performance by partitioning the staged data files to avoid the extra scanning terabytes of files that have already been loaded. The code snippet listed below shows a COPY using a range of options:
2- Split file into multiple chunks: To make use of all the nodes in the cluster as Snowflakes provided multi cluster and multi threading architecture better to split your data into multiple small files rather one large file, to make use of all the nodes in Cluster. Loading a large single file will make only one node at action and the other nodes are ignored. Need to follow the same practice for data unloading as well.
3- Virtual warehouse allocation: Selecting a large warehouse (scaling up) to speed the loading process is not a good idea when loading a large data files. In reality, the scaling up has no performance benefit in this case. Copy statement will open10Gb data file and start loading sequentially using a single thread on one node, then leaving the other servers idle. Unless you have parallel loads using the same virtual warehouse, the above solution is inefficient. As you will pay for the four servers, while using only one. A better approach is to break up the 10Gb file into 100 x 100Mb files to make the use of Snowflake’s automatic parallel execution, runs much faster as compared to past.
4- Latency vs Throughput: To reduce the latency which is maximizing the performance of the queries, is equally important to maximize the throughput is to achieve the greatest amount of work done in shortest possible of time. By increasing the virtual warehouse size, it is possible to reduce the elapsed time from maximum to minimum hours. But this is not the solution for each and every use-case. Here one solution to improve throughput is to scale up to a bigger virtual warehouse to complete the work faster. While it might improve the query performance, but there’s also a greater chance of inefficient use of resources on a larger warehouse.
One other approach is to execute multiple parallel tasks each with a different connection, and each task uses the same virtual warehouse. As the workload increases, the jobs begin to queue as there are insufficient resources available. The Snowflake multi-cluster feature can be configured to automatically create another same-size virtual warehouse, and this continues to take up the load. As the tasks are completed, the above solution automatically scales back down to a single cluster, and once the last task finishes, the last running cluster will be suspended. This is by far the most efficient and optimal way of completing the batch of parallel tasks, and we still have the option of scaling up. Below mentioned SQL code demonstrates to create a multi-cluster warehouse, which will automatically suspend after 60 seconds idle time, having economy scaling policy to favor throughput and saving credits over individual query latency.
5: Select only the required columns- Snowflake follows the columnar data store architecture. Columnar store databases achieve significant gains over by the physically organizing the rows. Storing the data in columns makes it much more efficient to retrieve a small sub-set of columns from the entire table.
6: Dedicated warehouse for workloads- Snowflake automatically caches the data in the warehouse in local disk cache, so place the users querying the same data on the same virtual warehouse. This maximizes the chances that data retrieved to the cache by one user will also be used by others. Will erase this cache once warehouse will suspend. Result Cache is maintained by global services layer, any query executed by any user on the account will be served from the result cache, provided the SQL text is the same. Results are retained for 24 hours only. However, it’s a good practice to separate the workloads by the type of workload. This means running BI queries from marketing users on one warehouse, while running a separate warehouse to support finance users.
7: Maximize cache usage- Per Snowflake internal architecture that it caches data both in the warehouse and the cloud services layer. Should take the steps to maximize the cache usage is a simple method to improve overall the query performance. Dashboard same queries frequently execute on a daily basis or on schedule. Snowflake automatically optimizes these queries by returning results from the results cache with results available for 24 hours after the each query execution. You should be aware, when resumed, the virtual warehouse cache may be clean, which you may lose the performance benefits of caching.
8: Scale Up for large workloads- Snowflake allows for a scale-up option for the warehouse to handle the large workloads in a better way. Very sensible to resize the warehouse to improve the overall query performance. As scaling up adds additional servers, it distributes the workloads among the nodes and it increases the overall warehouse cache size effectively by gaining the power.
9: Scale Out for concurrency- Unlike the scale up option described above, this technique is used to deploy additional clusters of same-sized nodes for concurrency i.e. increasing the numbers of users rather than task size or the complexity. The SQL Snippet above shows the statement needed to deploy a Multi-Cluster Scale-out Architecture. Using this method, instead of deploying a cluster of larger machines, this instructs Snowflake to add additional same size clusters, as needed, up to a fixed limit.
Conclusion: Thank you for reading, I hope this blog will help you getting the basic understanding of Snowflake performance tuning activities, and it’s measures either using Snowflake components or your design approach. You can reach out to me in case of more questions you have, on my twitter handle or my LinkedIn or leave a comment below. Good luck!
References: Link to Snowflake online documentation
COPY INTO <table> — Snowflake Documentation
Managing Regular Data Loads — Snowflake Documentation
Preparing Your Data Files — Snowflake Documentation
Multi-cluster Warehouses — Snowflake Documentation
https://community.snowflake.com/s/article/Tuning-Snowflake
Business Intelligence & Data Analytics — A Data Guy
See all (7)
5 

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
5 claps
5 
Business Intelligence & Data Analytics — A Data Guy
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@debipmishra/snowflake-performance-tuning-checks-towards-query-optimization-part-2-333f6002dffc?source=search_post---------148,"Sign in
There are currently no responses for this story.
Be the first to respond.
Debi Prasad Mishra
Sep 15, 2021·4 min read
This is a continuation post of Query Optimization- Part 1. I guess you will get some idea on Snowflake internal memory architecture, the caching concepts and etc. Would like to request, please do check out Part 1 before moving with this post. Below mentioned things are the Few Design Recommendations while implementing the data pipelines.
1- Storing Semi-structured Data in a VARIANT Column: For structural data we are using only native data types such as strings or integers and etc., as per the storage requirement. Data in a VARIANT column is very similar. Load the data set into a VARIANT column in a table. Use the FLATTEN function to extract the objects and keys you plan to query into a separate table.
2- Date/Time Data Types for Columns: When defining columns to contain dates or timestamps, choose a date or timestamp data type rather character or varchar data type. Snowflake stores date & timestamp data more efficiently than VARCHAR, resulting in better query performance in general.
3- Set a Clustering Key for larger datasets: Specifying a clustering key is not necessary for most tables. In general, Snowflake produces well-clustered data in tables. Snowflake performs automatic tuning via the optimization engine and micro-partitioning. Set Cluster keys for larger data sets greater than 1 TB and if the query profile in the web UI indicates that a significant percentage of the total duration time is spent scanning. A clustering key can contain one or more columns. As Snowflake suggested maximum of four columns. Analysis on the queries those having problem, to find the correct clustering key column is the most important. The SQL snippet mentioned below can help to identify the potential performance issues on queries that run for more than 2 minutes.
4- Use Transient tables as needed: Snowflake supports the creation of the transient tables. Snowflake does not preserve a history for these tables, which can result in a measurable reduction of your Snowflake storage costs.
5- Maintain a Dashboard for Snowflake Usage: Snowflake provides the usage dashboard such as Snowsight dashboard to visualize your worksheet data results using charts and dashboards using web UI. Try to design some features like to Audit Cost incurred for the “Account” and other usage metrics as good visualizations under “Account” section. This feature is available to Account Admin role only as per Snowflake and can be used for audit trails.
6- Leverage Materialized View: In every RDBMS system, materialized views are used to improve the query performance, where some sets of queries are being used as repeated manner. Materialized views are helpful when
7- Joining Order Criteria: Snowflake does not expose the functionality of optimizer hints that is found in other databases to control the order in which joins are performed. In general, the SQL query optimizer chooses the correct order for the joining tables. In some rare situations, it is not possible for the optimizer to identify the join ordering that would result in the fastest execution. Hence joins can create very slow query performance under the following conditions, such as:
Proper modeling of data in the warehouse will minimize the occurrence of exploding the joins. There are techniques to solve the performance problem such as convert the query into sub-query avoiding the joins criteria. For any calculation or expression, good habit is to create a temporary table and solve the problem. Columns with Numeric or Number data type should always a good choice of participating in the join conditions. Let’s take any example —
Hence in reality, the best approach it depends on the problem area itself and moreover it is situation specific, which may often in the data ingestion side or in the transformation steps or in end-user queries. And there are few effective solutions are based on a design approach also rather the query tuning. Scaling up to the large warehouse is not only a good solution always, neither a good strategy too. That being said, thinking all majors from Snowflake rather, you should first identify & focus on the actual problem statement.
Conclusion: Thank you for reading, I hope this blog will help you getting the basic understanding of Snowflake performance tuning activities, and it’s measures either using Snowflake components or your design approach. You can reach out to me in case of more questions you have, on my twitter handle or my LinkedIn or leave a comment below. Good luck!
References: Link to Snowflake online documentation
Clustering Keys & Clustered Tables — Snowflake Documentation
Multi-cluster Warehouses — Snowflake Documentation
https://community.snowflake.com/s/article/Tuning-Snowflake
Business Intelligence & Data Analytics — A Data Guy
See all (7)
3 

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
3 claps
3 
Business Intelligence & Data Analytics — A Data Guy
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/lightrail/activity-aware-ids-for-aws-the-simple-way-to-stay-aware-of-activity-in-your-aws-account-2c1c6f2335fc?source=search_post---------149,"There are currently no responses for this story.
Be the first to respond.
We’re open sourcing the internal tool we use to notify our team of suspicious activity in our AWS Account. Given our least privilege approach to IAM policies, Activity Aware IDS for AWS provides an integral part of our configuration debugging and intrusion detection (IDS).
If you’re already sold, feel free to check out the Repository, or read on to find out more about the problems it solves, how it works, and how it can help you maintain the security of your AWS account.
Activity Aware IDS helps you be more aware of activity in your AWS account, including those that might suggest potential account compromises. In this article, we will discuss the common use cases for Activity Aware IDS for AWS, an overview of its architecture and how you can start using it today. Before we get to that, it’s important to understand the security threats you face as an AWS customer, your responsibility in protecting against them, and overview of the principle of least privilege as a best practice in thinking about security and access control.
A decent understanding of the principle of least privilege is vital when talking about security and access controls. In this section, we will briefly describe the principle of least privilege and work through an example of working towards this principle.
Activity Aware IDS is most effective when following the principle of least privilege, that is, that in any system, any identity (users, programs, systems, etc.) granted privileges to access resources or information, should be granted only the minimum privileges necessary to perform their tasks. In the Activity Aware IDS default configuration, it will inform your team when users and roles are attempting to use actions or access resources beyond their privileges. It is possible to configure Activity Aware IDS to notify you of AWS API actions, even when these actions are permitted, but this requires knowledge of the specific actions for which your team wants to be informed.
As an example of following the principle of least privilege, let us look at a recent scenario we faced. We wanted to back up the logs from our instances into an S3 bucket so that if anything happened to one of our instances, we could always get access to its recent logs for investigating the root cause.
To allow instances to upload their logs, we could have used the Identity and Access Management (IAM) system to attach the AmazonS3FullAccess Managed policy to the roles associated with the EC2 instances. With those permissions, they could easily have uploaded their logs, but they would also be able to do other things, like Reading or Deleting the log files. In the case that this instance had these extra privileges, and for some reason, it attempted to delete all the log file, it would be successful. Under the default configuration, we would not be informed of this because the instance only performed actions it was granted. Instead, we could give them an inline policy like the following:
This would restrict the instances to only be able to upload to an S3 bucket, but not allow them to Read or Delete from the buckets. This is still not least privilege because, although an instance can only upload files, it can upload them to any bucket in your account. Instead, we could restrict the privilege to a specific bucket:
So now we’ve restricted our instances to being able to upload only to a specific bucket, we must be good right? Well, we wanted to find the logs for a specific instance, so we wanted them to put their logs into a unique prefix for each instance. For this, we used a prefix that was a combination of the unique id for the role associated with the primary role of the instance, combined with the instance id, like role-id:instance-id. This gives us something special when it comes IAM policies. We can make the above policy even more restrictive:
In the policy above, ${aws:user_id} is what is known as an IAM policy variable. In this case, this specific variable is equivalent to the unique identifier for which ever role, user, or other identity, is making a particular request. So in this example, it would be coming from an EC2 instance. If you consult the table from the IAM policy variables page, you will find that for an EC2 instance, this resolves to role-id:ec2-instance-id, which is what we used for the prefix above. Thus, our instance is only able to upload its log files into a specific bucket, with the specific prefix for its instance. Under the default configuration of Activity Aware IDS, if this instance attempted to perform any other action on this or any other bucket, or upload its logs to a different prefix, a notification would be sent to the company Slack channel, thus informing the team of the aberration.
So now we must be good right? Well, that’s the hard thing about the principle of least privilege. If we happened to know that the instances only upload their logs at a specific time, we could reduce it’s privilege even further. The purpose of the principle of least privilege is to reduce the attack surface, that is, the different contexts that a malicious user could attempt to cause damage to your systems. This damage could be caused by creating bad data, getting a hold of your good data, using your resources like your EC2 instances, or creating EC2 instances that get charged to your account. But since we want to backup our logs frequently throughout the day, for all reasonable intents and purposes, we will call this example done.
Setting up proper IAM permissions like above isn’t just good practices, it’s one part of what AWS customers are responsible in the Shared Responsibility Model. Let us explore this a little further.
As a customer in any cloud provider, it’s important to understand your responsibilities. In this section, we’ll cover AWS’ Shared Responsibility Model, and overview your responsibilities as an AWS Customer.
When securing your resources, the cornerstone of security in the AWS Cloud is the Shared Responsibility Model. At a high level, this is a delineation between what AWS takes responsibility to secure, and what you as an AWS Customer are responsible for securing. Most simply, AWS takes care of security OF the cloud, and you are responsible for security IN the cloud.
For the AWS Side of this, they take responsibility for securing the building blocks used to compose your systems. These include the Compute (EC2 hosts), Storage (S3 infrastructure), Databases (RDS Hosts), Networking infrastructure, and so on. That’s not to say that you can just throw all of your customers’ credit card data into an S3 bucket, and your security responsibility is complete.
As an AWS Customer, you are responsible for maintaining the security of the operating systems of your EC2 instances, your application, network and firewall configurations, identity and access management, encryption, and customer data. A big part here is that AWS provides the building blocks, like the Storage (S3), and Encryption (KMS) to protect the security of this data, but it’s your responsibility to use those services properly, and rotate your access keys regularly. If we stored our data with too broad of permissions, and some attacker gained access to our customer data and leaked it, we, as the AWS customer, are responsible for that.
If this doesn’t sound obvious, or you just need to review, I recommend you check out The AWS Shared Security Responsibility Model in Practice video as a starting point.
Now that we have a better understanding of what we as AWS Customers are responsible for security-wise, let us go on to an overview of the potential security threats you need to consider when operating in AWS.
When trying to secure your resources in the AWS Cloud, it’s valuable to understand what kind of security threats you’re likely to face. In this section, we will briefly cover the different types of security incidents you need to consider.
Although there are a number of different potential security threats you need to be aware of in the AWS Cloud, they have been nicely broken down into the 3 major types of security incidents in AWS by Adobe. These are Infrastructure Impact, Host Compromise, and Account Compromise.
Infrastructure Impact includes external attacks on the underlying infrastructure of your application. This type of attack largely consists of Distributed Denial of Service (DDoS) attacks, where an attacker sends a large volume of traffic at your site. The main objective of this type of attack is to occupy your resources with garbage traffic, so that legitimate traffic can not get through, thus taking your site offline. DDoS Protection tools, like the new AWS Shield, can help to alleviate these types of threats, by identifying common patterns in the DDoS traffic, and blocking it before it gets to your servers.
Host Compromise involves using a technique like command injection to gain access to your existing resources, such as your EC2 instances. When an attacker gains access to one of your instances, for example, they are usually targeting one of two things: Using the instance for its compute resources, such as BitCoin Mining; or Using the instance to gain access to its data or approach another instance that is likely to have valuable data, called an Advanced Persistent Threat (APT). In the case that the attacker can’t find valuable data on the instance they gain access to, they will sometimes use that instance to look for other potentially vulnerable instances on the same network (or VPC). Host-based intrusion detection and intrusion prevention are the most common methods of alleviating this type of threat. These protections regularly involve placing some software or agent on your host, which scans incoming and outgoing traffic to look for common attack patterns, and informing you and/or stopping this traffic automatically.
Account Compromise involves an attacker gaining access to some identity (either users or roles on an instance), and then using that identity for the data it has access to or the resources it can create. As in the Host Compromise case above, if the attacker is interested mining bitcoins, they might spin up a number of large (potentially 4xLarge or 8xLarge) instances, using them to mine bitcoins, then leave you with the bill for the instances. Alternatively, if they’re interested in the data, existing safeguards might not allow them to export a snapshot of your database directly, for example, but if they can create an instance that has access to the database, they can export it from there.
When it comes to protecting against Infrastructure Impact and Host Compromise, there seems to be no shortage of tools to help with these threats. When it comes to Account Compromise, the solution space seems to be a lot more open. This is where Activity Aware IDS for AWS fits in. When an attacker is attempting to determine the permissions of a compromised identity, they will likely be denied access to a number of the actions and resources they attempt to use while probing the identity. Activity Aware IDS notifies you of these denials where your team will notice them most, like Slack.
Now that we have a better idea of the common types of security incidents that might occur in the AWS Cloud let us look at the use cases for Activity Aware IDS for AWS.
As we covered briefly in the last section, Activity Aware IDS for AWS can help you discover Account Compromises. This works by using CloudTrail, and the principle of least privilege. CloudTrail is essentially a log of all of the activity in your account, and the principle of least privilege, as we covered above, means giving any identity (users and roles) only access to the permissions it needs to fulfill its tasks.
Now let us imagine that a member of your team, Alice, had her credentials compromised by a malicious attacker Mallory. Mallory might want to see what Groups and Roles they now have access to through Alice’s account. For this example, let’s assume that Alice doesn’t have access to view the Groups she is associated with, or at the very least, doesn’t have access to view the policies attached to those groups. If Mallory attempts to view the policies of those groups, she will get denied access, which gets logged to CloudTrail. At this point, Activity Aware IDS receives the denial log, converts it into a friendly format, and sends it to a Slack Channel monitored by your team. Once the message arrives at Slack, your team will see that there are strange “Access Denied” messages associated with Alice, speak with her about the denial, to find out that she is not performing the actions, and replace her credentials.
In the example above, the attacker Mallory was blocked from performing actions due to the principle of least privilege. Although the principle of least privilege is a common recommendation regarding security, it can be difficult in finding the exact set of permissions that a User or Role should have. Activity Aware IDS can also assist with this.
Imagine that you are deploying a new service, and you want to give it permissions following the Principle of Least Privilege. The easiest place to start with the principle of least privilege is to create a role with no permissions. Now lets assume that the system needs to send logs to CloudWatch Logs (this is a common requirement for AWS Lambda). When the system attempts to create a new Log Group, for example, it will get denied, because the role has no permissions. This denial will get logged to CloudTrail, then Activity Aware IDS for AWS will send the denial to your Slack, where you can see the specific action being attempted, the Role attempting to perform the action, and even the arn of the specific resource it’s trying to perform the action on. At this point, you can add a statement to the policy for the role allowing it access to “CreateLogGroup” and even provide a fairly restrictive resource description.
There is usually value in understanding, at least at a high level how systems you use work. In this section, we’ll briefly overview the architecture of Activity Aware IDS, its major components, and how they work together.
The architecture of Activity Aware IDS for AWS is fairly simple. It’s composed mainly of 3 pieces: Source Lambdas, Destination Lambdas, and an SNS Topic for message passing.
Above is an architectural overview of Activity Aware IDS for AWS.
On the left side, we have Sources. These are the services that generate the events of which you want to be aware. Each of these services is coupled with a specific lambda function that knows the structure of events for that service, converts them into a common message format, and sends them to the SNS Topic. In the initial release, we have a CloudTrail source, but we’ve designed the system to be extensible with additional sources, such as VPC Flow logs.
On the right side, we have Destinations. These are the services or methods of communication by which you want to be informed. Each of these destination services is coupled with a specific lambda function that is subscribed to the SNS Topic, knows how to interpret the common message format, and send a message to the given destination service. In the initial release, we have a Slack destination, but like with Sources, the system is designed to be extensible, and support additional sources, such as SES for email, PagerDuty, and more.
The SNS Topic is a simple, but central messaging hub in Activity Aware IDS. It receives messages from the source lambdas and sends them out to the subscribed destination lambdas.
So there you have it, it’s fairly simple, but with its extensibility, it can also be quite powerful.
If you’ve made it this far, you’re probably still interested, so why don’t you give it a shot. Check out our Getting Started guide for the requirements you’ll need, and how to install Activity Aware IDS for AWS in your AWS account.
Do you have a source you’d like to get notified about in your Slack? Maybe you’d love to have your CloudTrail events go to a different destination, like Email? Or maybe you found a bug in the system? We love getting all of this feedback. Be sure to add an issue for your Source and Destination requests, as well as any bugs you’ve located in our Issues tracker.
Feel like getting your hands dirty? If you want to contribute to the project, we’d be happy to have it. Create an issue for it first, so that we might give you pointers, and so that others know if a feature they’re excited about is being worked on.
The branded currency platform for customer growth.
3 
3 claps
3 
The branded currency platform for customer growth.
Written by
DevOps Engineer at Giftbit/Lightrail. Writes personally at www.jamiestarke.com
The branded currency platform for customer growth.
"
https://medium.com/@tabu-craig/how-we-cut-10-minutes-off-our-deploy-time-by-using-storage-buckets-2c2faa9f090?source=search_post---------150,"Sign in
There are currently no responses for this story.
Be the first to respond.
Craig Taub
Sep 11, 2019·4 min read
At Nested one of our applications is an internal tool. We decided long ago to follow the single-page app 100% client-side route, and functionally that has served us very well.
We make use of the Google Cloud Platform. It offers quality and variety of services for sensible money.
The plan with the SPA deployment was to use GAE (Google App Engine) as its the service intended for deploying applications.
We also use CircleCI to run our PR and “merge to master” workflows. The flow for our merge to master looked like the below:
For those unfamiliar GCR is Google Cloud Registry and is a place for storing and pulling docker images. We actually build our JavaScript bundle on the image and push that to GCR.
Most of the workflow was pretty efficient (running in parallel) except for deploying to GAE. This was taking 11 minutes in itself. The result of which was:
As all the processing is done on the browser we had the idea to run the application from a bucket on GCS (Google Cloud Storage). The idea being to remove the time-heavy GAE deployment step.
The change itself was an update to our CircleCI config file and it resulted in the below flow:
As you can see the entire dependency on GAE and GCR has now been removed. Rather than using GCR we use the CircleCI workspace. By persisting and re-attaching it we are able to build the bundle in parallel to running tests and linter, and then upload it to our bucket on success.
The overall saving is quite staggering:
We had a number of concerns which we had to settle before we could make the change with confidence, here are those concerns and our answer to them.
1. Will there be a difference in NodeJS’s dependency cache?
The image was always installing dependencies fresh whereas CircleCI has a cache. However we noted that CircleCI cache is busted with the lockfile hash (and CircleCI itself runs on Docker), so basically if anything has changed in your lock everything is downloaded fresh. Problem solved.
2. Will environment variables all work the same?
Ensuring that all the variables found in the Dockerfile were moved into the Circle config was crucial. Without this we would not be able to serve an optimised bundle and would have wound up serving a 8mb JS file rather than 300kb.
3. Can we deploy without an outage?
Having an outage would be a disaster for our internal operations team so it was crucial we made these architecture changes without incurring an outage cost. Thankfully the index file and the JS asset it points to are built together, so if someone is using the buckets index file, they are therefore using the latest JS asset.
4. What about Asset caching?
As we were constantly pushing new assets to the bucket we need to ensure the index file is not cached on the users browser, otherwise the user would pull an old asset. Thankfully GCS allows you to set meta headers when you upload. Allowing us to give the JS asset a long cache life (as it has a hash in its name) and set the index as ""no-cache"", so the latest will always be fetched.
5. What is the difference in rolling back?
This one is slightly controversial. With GAE it keeps a record of stopped instances so you can easily start up an old instance relatively quickly. However with our new setup we have 2 choices
We were happy to take the slight hit as we do not experience outages often.
6. How did you get all url paths to use the bucket index file?
We have the domain point to an nginx app (it serves all our applications). The nginx box returns the buckets index file for all paths as GCS/CDN can’t do this itself.Our config looks like this:
Now all paths get our index file.
We are really pleased we made the decision to go ahead and use a storage bucket over GAE. As you can see it is a simpler architecture (with less moving parts) which makes it easier to manage, as well as more time efficient.
It has been running successfully for a month now so we recommend anyone with a similar application move it to using a bucket.
JS ""under-the-hood of"" series https://bit.ly/36GLhlo. dev @ fiit.tv. Formerly BBC. https://craigtaub.dev/
See all (35)
63 
1
63 claps
63 
1
JS ""under-the-hood of"" series https://bit.ly/36GLhlo. dev @ fiit.tv. Formerly BBC. https://craigtaub.dev/
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@mktucker12/ready-your-data-for-ai-c0d18bf74f9d?source=search_post---------151,"Sign in
There are currently no responses for this story.
Be the first to respond.
Michael Tucker
May 22, 2018·1 min read
We’re seeing more businesses engaging in some level of AI exploration and use, but as any of them would say, the readiness of internal data and having a full understanding of data and analytic assets is key to adopting a successful AI strategy.
Many executives believe they lack the ability to act quickly on data, due to obstacles such as the time-consuming process of discovering actionable data in their organizations.
Join IBM Distinguished Engineer Jay Limburn for an upcoming webinar in partnership with Dataversity to learn how to ready your data for AI and lower your organization’s barrier of entry to AI and all its benefits.
Jay will cover:
- The evolution of metadata from collection to consumption
- How to turbo charge your path to AI
- Optimizing your data science practice
Jay is an expert in the fields of Data Governance, Data Management and Master Data Management and has worked with some of IBM’s largest clients to define and develop industry-leading and innovative solutions. As an IBM Master Inventor, Jay holds 17 patents in areas such as machine learning, mobile device interaction and application generation.
Register here: http://content.dataversity.net/052318-IBM-Webinar_Sponsor-Registration.html
Director of Product Management at UserZoom
5 
5 
5 
Director of Product Management at UserZoom
"
https://medium.com/@OPTASY.com/10-cases-when-you-should-use-apigee-edge-microgateway-7b519c9eb390?source=search_post---------152,"Sign in
There are currently no responses for this story.
Be the first to respond.
OPTASY
Mar 4, 2020·4 min read
So you’re evaluating and comparing all the available solutions for centralizing and standardizing your APIs. And you can’t help wondering: “When is it recommended that I use Apigee Edge Microgateway?”
Why would you use it in the first place, over other platforms for managing internal APIs? Over… the Apigee Edge gateway, for instance?
What “irresistible” capabilities does it provide?
And when precisely should you use it? What are its typical use cases?
Let’s get you some answers:
If I was to compress its definition in a short line, it would go something like this:
Apigee Edge Microgateway is a hybrid cloud solution for managing your APIs.
Whereas if I was to opt for a detailed one:
It’s an HTTP-based message processor for APIs, built on Nodejs, that enables you to centralize and manage all your APIs in a hybrid cloud deployment setup.
From traffic management to enterprise-grade security, to analytics, spike arrest, and quota (and the list goes on), it provides you with most of the standard Apigee Edge capabilities.
Moreover, you get to install it either on the same machine or in the same data center.
“And how exactly does it work?” you’ll then ask yourself.
It processes the messages — requests, and responses — going to and from your backend services. Meaning that it’ll asynchronously push API execution data to Apigee Edge once consumed by the Edge Analytics.
“What about the Edge Microgateway deployment scenarios? What are my options?”
Well, there are quite a few, since you get to deploy it:
Why would you go with Apigee Edge Microgateway over another setup for centralizing and administering your APIs?
Here are some of the strongest API management capabilities that it provides you with:
When is it best to go with a hybrid deployment model for managing your APIs?
Here are some of the most common use cases of Apigee Edge Microgateway:
Apigee Edge vs Microgateway… why should you consider the later?
How are they different more exactly?
Especially since it gets even more confusing when considering that you even get Apigee Edge Microgateway plugins, which are so similar to Apigee Edge’s policies…
What capabilities, that you can benefit from using Apigee Edge, does Microgateway lack, for instance?
Now, in terms of differences, here’s a general “rule of thumb”:
Apigee Edge Microgateway does not come to replace the Edge gateway.
It has its own specific “lighter” use cases, like various mediations, key verification, quota that you can do with your backend services in close proximity, all while tapping into its robust above-mentioned capabilities.
The END!
Are you using Apigee Edge Microgateway? If so, what’s your specific use case?
And why have you decided to go with Microgateway instead of Edge Gateway?
Article originally published on OPTASY.com.
Drupal, Wordpress, AR/VR, AI Development in Toronto, Canada
1 
1 
1 
Drupal, Wordpress, AR/VR, AI Development in Toronto, Canada
"
https://medium.com/google-cloud/enforce-tls-policy-on-gke-cluster-with-istio-or-anthos-ab46ee1fe038?source=search_post---------153,"There are currently no responses for this story.
Be the first to respond.
Photo by Jon Moore on Unsplash
Google Cloud provides an easy way to enforce TLS policies such as allowed version(s) of SSL and TLS protocols and cipher suites allowed to be used with these versions. It can be configured at HTTP(S) External Load Balancer. If your workload runs on GKE clusters that expose services in a way that does not use Google Load Balancers or the workloads are exposed internally (e.g. using HTTP(S) Internal Load Balancer) then you need another way to enforce TLS policies. The following tutorial shows how to configure it using Anthos Service Mesh (ASM) service. It is also possible to implement the tutorial using Istio OSS. However, ASM allows to apply TLS policies both to ingress and pod-to-pod traffic while Istio supports TLS policies for ingress traffic only.
The tutorial runs on GCP and requires a project with a valid billing account. The commands in this tutorial imply that there is an environment variable PROJECT_ID that stores the project id of that project.
NOTE: If you run this tutorial you will be billed for using GCP resources. Enabling some of the APIs may incur additional cost.
Use the following command to enable the required APIs:
If you use Istio OSS you only need compute.googleapis.com and container.googleapis.com APIs.
GKE clusters with ASM need at least e2-standard-4 machine types. The tutorial was not tested on the clusters composed of the smaller machines. Create a GKE cluster:
NOTE: No need to provision the cluster with --workload-pool parameter if you mean to use Istio OSS.
Enforcement of pod-to-pod TLS policy is supported starting with ASM 1.9. The following commands install the latest revision of the ASM 1.9:
Need to capture the ASM revision for later:
The tutorial uses an Online Boutique shop application that can be found on github. The following steps deploy the application into the dedicated namespace, configure ASM ingress gateway with proper certificates and then apply the TLS policies for ingress and pod-to-pod traffic.
Configure a Kubernetes namespace to run the demo application in it.
Enforce mTLS for all workloads deployed into the demo namespace:
The Online Boutique shop is a microservice application running on GKE and used for different demonstration purposes. This tutorial uses the latest version of the microservice container images.
To test the work of TLS you will need to have a valid domain name for the demo application. There are several ways to configure it. You can use Cloud DNS or another DNS service like one that your domain registrar provides. You can use a local /etc/hostname just to do tests. Either way, achieve the ASM gateway IP address:
And associate it with a hostname of your choice. For the following steps will use the hostname demo.acme.clouddemo.acme.cloud. Please, replace it with your selected hostname.
To enforce TLS policy for ingress traffic you have to configure ASM ingress gateway. To do this the ingress gateway has to be configured for the demo application hostname (see secure gateways for more details).
Edit a virtual service:
And replace all the '*'in the list of hosts: with the 'demo.acme.cloud'.
Edit the frontend gateway:
And replace all data under servers: with the following:
NOTE: Keep the indentation in-check to preserve YAML format of the manifest.
The tls section of the gateway manifest defines TLS policy as a range of TLS protocol versions and a set of cipher suites allowed for these versions. Note that according to the TLS 1.3 protocol specifications the cipher suites of the TLS 1.3 cannot be changed. So the set of cipher suites is applied to protocol versions ≤ TLS 1.2.
All steps until now can run with Istio OSS. The TLS policy enforcement for pod-to-pod communication is possible with ASM when mTLS is enabled. In contrast to TLS policy for the ingress traffic which can be enabled and configured per host basis, the TLS policy for pod-to-pod communication is enabled cluster-wise. For this, edit the ASM controller:
And add the following environment variables to the single container spec of the deployment:
ASM by default limits TLS protocol versions to 1.2 and 1.3 only. So, there is no need to limit the range of the TLS protocol versions.
It is possible to constraint which TLS protocol versions and which cipher suites will be used for ingress and pod-to-pod communication within GKE. The values can be configured using Istio OSS or ASM gateway for ingress and ASM controller for pod-to-pod communication.
Google Cloud community articles and blogs
3 
3 claps
3 
Written by
DevRel Engineer in Google Cloud, specializing in Observability and Reliability. I try to whisper to horses in free time.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
DevRel Engineer in Google Cloud, specializing in Observability and Reliability. I try to whisper to horses in free time.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://blog.crosscloud.me/sync-windows-shares-with-crosscloud-even-offline-a72eaf7c8205?source=search_post---------154,NA
https://medium.com/innoventes/upgrading-from-ubuntu-bionic-beaver-18-04-lts-to-focal-fossa-20-04-lts-23d35d9fd07f?source=search_post---------155,"There are currently no responses for this story.
Be the first to respond.
We use an E2E public cloud system to run our website, some of our internal applications as well staging instances of the applications that we develop.
The applications use various technologies — html, php, ruby, python, nodejs, java as well as docker container. We use nginx and run each application on dedicated subdomains. We also use MySQL and postgresql databases for many of the apps.
Our server has been running 18.04 LTS with most of the programming tools/technologies locked to the LTS version. In some cases, we have done manual upgrades.
For python, we use pyenv and pyenv-virtualenv thereby making us independent of OS python version. Likewise rbenv for ruby and nvm for nodejs. However, our php apps have been using the version provided by the OS (7.2.x). Ditto for our databases — mysql and postgresql.
We keep regularly updating the system to the latest version. This is often an issue since the driver for the backup tool provided by E2E is linked to the kernel version, and the driver sometimes lags behind the kernel update.
Ubuntu 20.04 came out in May first week, but the option to upgrade from 18.04 to 20.04, usually happens only after the first point update is done, which was on August 6. But the update prompt didn’t come for a while, courtesy some upgrade-blocking bugs.
Finally, we got the prompt that a new LTS version was available for our system. The questions was should we update or not? One of the determining factors was that most of our new applications would run on the newer OS. So, it was imperative that our environment where it was staged was kept updated. Yes, we had the option to dockerize some of the apps, but we decided that it was best to keep our server updated.
The first barrier to upgrade is any pending upgrades on the existing version. Unless there are no updates left, you cannot upgrade the OS. In our case, we had a couple of applications which refused to upgrade.
Searching for an explanation, we found a few ways to overcome it. We chose
In one case, we discovered one of the repo sources (in /etc/apt/sources.list) was incorrectly pointing to Trusty (14.04!) instead of Bionic and correcting it, fixed the issue.
We were now ready to upgrade. Since our system was backed up, we weren’t too worried about anything going wrong. (Just the hazzle of needing to re-install/restore).
However, as we ran sudo do-release-upgrade we came up with the following
We had no other option. So, having noted down the relevant details (in case of failure) and having added the port number where the additional daemon was running to ufw, we continued. One typical failure scenario is power failure that can sever the ssh connection to the server. Luckily for me, this didn’t happen and the upgrade went through successfully.
Now, we needed to check if all our apps were running fine on the updated OS. It appeared that most were not running!
To start with, php was not present! Looks like our upgrade uninstalled the older version, but didn’t install the newer one.
So, we manually installed php, the version being 7.4.x on 20.04. It was not enough just to install php, but also a few extensions required by the various web apps (like php7.4-fpm for instance). We also needed to update nginx configuration file
(Previously, it was /var/run/php/php7.2-fpm.sock)
We also needed to install php7.4-msyql.
With that, our php apps were now running.
Shouldn’t our python apps be fine, since we weren’t relying on the OS version python? No!
The upgrade had messed up the native library required by python. The correct fix for this was to reinstall the python in our pyenv.
We also needed to recreate the virtualenvs for each of our apps.
Since the upgrade had also upgraded mysql server from 5.x to 8.x, the python-mysql package also needed to be upgraded to support the newer python version.
After doing these changes, we got our python apps running again.
Now for the ruby (on rails) apps. Here we only had issue with the mysql package, which was fixed by reinstalling the package.
Our postgresql configuration had gone missing, again, due to the postgresql update from 10.x to 12.x. We had to add relevant entries for our docker container to connect to the postgresql running on our host.
Our applications were now running on Focal Fossa. Our upgrade from Bionic Beaver to Focal Fossa was complete and successful.
Learnings:
Innoventes Technologies
1 
1 clap
1 
Innoventes Technologies
Written by
Software developer, blogger, co-founder of Innoventes Technologies
Innoventes Technologies
"
https://medium.com/@jeremy-dessalinesdorbigny/create-a-game-web-app-fast-with-google-app-maker-challenge-accepted-b21130284994?source=search_post---------156,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jérémy Dessalines
Dec 16, 2019·3 min read
The context
In my company we have express our purpose, it describes both why our group exists, what it does and how, for the benefit of all its stakeholders.
In my department, IS&T we are committed to preserving, using and recycling natural resources efficiently. We want to be useful, exemplary, contribute to the general interest and reduce the environmental footprint of digital uses . This digital sobriety approach promotes the controlled use of tools and increases our environmental and economic performance.
Cloud-only pay-as-you-go solutions and the adoption of Chromebook devices are lightening the weight of our infrastructure and maintenance. Their appeals are fully in line with our “Ressourcing the World” mission.
That is why we want to engage our end-users. So, we have created a web-app to push good practices in the office around a collective and solidarity challenge we have launch this autumn.
Our challenge
It is based on Google App Maker as we were able to create this app following an agile methodology fast.
With this app we have identified 35 good eco-responsible practices at the office spread around four families. This calculates a score for each IS&T employee. The score is then sum by team at the manager level.
The result
The app benefits from Google App Maker to authenticate the user, create a responsive application that allows the visualization of good practices in the form of cards.
The application works with a few lines of code. Access is secure and restricted to a small group of users via a Google Group.
In addition users can see the evolution of the score live via a DataStudio report connected to the database of the application (Google Cloud SQL).
All this was done in a few weeks with a UX designer to prepare the UX of the game, a developer and a project manager.
Good luck to our IS & T players and at the end let the planet wins!
PS : You can get the reusable source app (.zip) here. If you want to do the same within your company. You have to modify and store the app images. I have included reusable samples.
PS2: Might be english in french in the client/server side scripts comments. I’m a bit lazy.
See all (32)
1 
1
1 clap
1 
1
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@lyledodge/azure-in-the-enterprise-devsecops-spend-36feac9e1969?source=search_post---------157,"Sign in
There are currently no responses for this story.
Be the first to respond.
Lyle Dodge
Aug 26, 2018·2 min read
When talking to customers about how our internal operations engineering teams work, they frequently ask how DevOps works for us. I’ve noticed the conversation often has a bias towards developers — people want to know what developers do when building our line of business applications that run the business of Microsoft, and it’s often slanted towards greenfield applications.
For our Line of Business software developers (LoB devs) that build the applications that run the business of Microsoft, DevOps should really be renamed DevSecOps+Spend, or something to that effect. Our LoB developers themselves are also responsible for all of the things that used to be dealt with by service operations teams.
I spend part of my day talking to the software engineers that build our internal applications across finance, hr, sales, SAP, and on and on. What I’ve noticed is that the daily stand-up of the mature engineering team looks something like this:
Other characteristics of the mature engineering teams that I’ve seen so far:
I’m a director in Microsoft’s sales organization focused on landing Linux Workloads on Azure.
2 
2 
2 
I’m a director in Microsoft’s sales organization focused on landing Linux Workloads on Azure.
"
https://medium.com/@itsabox/encryptotel-the-telecom-breakthrough-d8177f43fc23?source=search_post---------158,"Sign in
There are currently no responses for this story.
Be the first to respond.
Its a Box, Creative Solutions
May 31, 2017·1 min read
Encrypto Telecom will be the first ever cloud secured, private branch exchange(PBX) — an internal telephone exchange VoIP system based on blockchain technology. The project plans to deliver its new solutions for telecommunications that address privacy and civil liberties. Funds that were raised in the recently successful ICO, which just hit $3m USD, will allow EncryptoTel to use encrypted communication and a cloud-based PBX to ensure privacy in offering a range of services for its customers.
On a daily basis, our personal data is being monitored constantly. It is collected by a range of organizations or individuals. ranging from state agencies, compiling data for the purposes of ensuring national security, to business corporations, wishing to learn more about customer’s habits.
Ecommerce Director for Multiple Brands & Project Manager (SaaS)
1 
1 
1 
Ecommerce Director for Multiple Brands & Project Manager (SaaS)
"
https://medium.com/@onserve8/using-an-it-audit-to-select-managed-it-services-39c3652fa7e6?source=search_post---------159,"Sign in
There are currently no responses for this story.
Be the first to respond.
Onserve IT
Nov 10, 2016·3 min read
The managed IT services Ottawa businesses use depends on their size, industry, internal IT staff, and technical capabilities. Many businesses are taking advantage of cloud services to solve some of their IT needs.
Benefits of an IT Audit
When deciding to use managed IT services Ottawa businesses benefit from conducting an IT audit as the first step in deciding how to implement new core systems. An IT audit is a comprehensive review of the IT solutions used by a firm to determine what needs to be improved and what are the most effective ways to make improvements. It also determines the maturity model level that the organization currently achieves for its IT functions and sets a path for future improvements.
Maturity Models
Maturity models are five basic levels of IT operations that include 1) beginning; 2) developed; 3) defined; 4) managed, and; 5) optimized.
Beginning — This is the startup phase, which has the basic IT functions needed to operate; however, they are not coordinated. There are few business controls. Document creation happens on an as-needed basis, without any standardization.
Development — Things begin to improve at this level. The internal IT system has some more functional ability. There is more focus on cost efficiency. Customer service is more standardized; however, there is no integration with email delivery. At this phase, there are numerous IT projects yet to accomplish.
Defined — At this level, more processes become powered by automation. Communication with customers or clients is organized and standardized documents create a clear organizational style. Operational interlinked networks are in place to eliminate stand-alone systems making IT processes are more efficient.
Managed — At this level, the organization takes full advantage of cloud services offerings that include public, private, and hybrid cloud services. Depending on the needs of the organization, it may use cloud services, such as Infrastructure as a Service (IaaS), Platform as a Service (PaaS), and Software as a Service (SaaS).
IaaS examples are Voice Over Internet Protocol (VOIP), database management, data backup, security, and having virtual private servers. SaaS examples include Microsoft’s Azure platform and the Google App Engine. SaaS has thousands of choices that include Salesforce, Microsoft Office Online, SAP, Google docs, and many others.
Optimized — This is the most mature level. Mobile technology devices integrate with the system. Customers and employees are able to access information anytime, anywhere, and on any type of device. Data management is more effective by using the best mixture of on-site processing with online services. Data mining of Big Data is ongoing and used to discover market opportunities for new product development and sales opportunities. Automation of customer service is enhanced using artificial intelligence programming.
Managed IT Services Ottawa
Many businesses in Ottawa benefit from managed IT services. Smaller enterprises benefit because of the cost savings and the lower requirement of capital investment for in-house IT workers and equipment. Larger organizations that use robust cloud services are able to ramp IT solutions up or down in response to market demand and can redistribute processing loads in real-time.
Security issues are always a big concern. Smaller firms benefit from having outsourced IT security to firms that have staff, which concentrates on security issues much more effectively. Using any IaaS, PaaS, or SaaS services in the cloud means that the software updates automatically, especially for security issues.
Using Expert IT Consultants
To make the best use of managed IT services, organizations take the information learned from an IT audit and then make an IT upgrade plan with the help of expert IT consultants. For the best results, the IT upgrade plan should have a design that clearly supports the long-term strategic business plans of the organization.
Summary
Managed IT services are increasingly in use by organizations of all sizes in Ottawa because of the efficiency possible and cost effectiveness. Companies no longer have to go at this on their own and can reach out to find the IT managed services and the help they need for the best IT solutions.
1 
1 
1 
"
https://medium.com/calytera/how-the-amanda-case-management-system-gives-public-sector-cios-peace-of-mind-when-it-comes-to-security-4b01070dba56?source=search_post---------160,"There are currently no responses for this story.
Be the first to respond.
Local and city governments embrace cloud technology to enable faster decisions, improve internal workflows, reduce resources, slash administration budgets, and provide 24/7 citizen services.
But with more and more of the technology moving outside local government-owned data centers and into the public or private cloud, the security threat is getting very real.
Well-reported breaches in data in 2016, in a variety of industries, just add to the anxiety and diffidence of CIOs in the Public Sector regarding he shift to the Cloud.
According to a KPMG survey in 2014, on data-centric audit and protection [DCAP], the exponential growth in data generation and its use makes current methods of data security obsolete. It recommends significant changes in both architecture and solution approaches.
The report cites 2 problems related to security.
It is against this background that we explore how AMANDA 7, declared the best municipal case management system in an independent research, tackles security.
The nice part is that AMANDA offers an integrated system for effectively managing workflows in permits, licenses, compliance, freedom of information, and court and jury management all within the same system.
This means that traditional silos of information are handled beautifully within AMANDA. The case management system works with multiple departments and agencies and provides them centralized data operations based on granular permissions configured into the system.
Next, we need to understand the security protocols as well as the security vulnerabilities in application software development and what AMANDA does to secure itself.
The OWASP Top Ten Proactive Controls 2016, lists out the security concepts that should be included in every project.
They are ordered by order of importance, with control number 1 being the most important.
This protocol for security has arisen from the common security issues and the top 10 vulnerabilities in application software development surveyed by OWASP.
These are:
With the reported data breaches, known problems in application software development and the defined protocols for testing, AMANDA users need to know how this affects the software they use and how rigorous the testing standards for AMANDA are.
It will reassure them that the software they buy measures up well to vulnerabilities and the highest testing standards in the business.
So how does AMANDA tackle security?
It would be heartening for government CIOs to note that AMANDA adheres to OWASP top 10 Vulnerabilities. Compliance verification is encoded as part of its Product Development Lifecycle.
What’s more, OWASP tools such as IBM AppScan, Arachni, Burpsuite, SOAP UI are used at different stages of the testing lifecycle to ensure that all vulnerabilities are tested.
There’s more to the security practices followed by AMANDA.
3 ways in which AMANDA 7 gives you peace of mind.
Let’s look at how each of these is implemented in AMANDA 7.
It follows secure coding guidelines
AMANDA Secure coding guidelines ensure developers adhere to the security standards to avoid any leaks at the code level during runtime.
Vulnerabilities due to application error messages, runtime stack trace, web services fault strings are not thrown to the users whenever a fault request is being sent to the Application.
Injections such as uploading vulnerable files, queries injected as form inputs, etc, are not processed and blocked from being sent to the server.
The AMANDA environment has tools like SonarQube, and FindBugs to validate the code for security at build stage.
AMANDA architecture enforces security at different layers of the application and developers ensure that secure design and coding is not broken during functional implementation.
AMANDA has a cryptographic module which enforces application security rules and methods such as FIPS-approved algorithms: AES, HMAC, SHA-1 with a combination of other algorithms for data encryption.
Security awareness sessions are conducted, each quarter, with the CSDC Systems’ development and testing teams.
The latest vulnerabilities and security standards are shared and discussed threadbare with action points for the team.
Reports and graphs can be generated to view the vulnerabilities at application, server, browser, etc. Organizations can create their own trust and severity level for each vulnerability based on the infrastructure setup, nature of the business they are into, map the vulnerabilities to their day to day operations with their AMANDA solutions.
It employs multiple tools to determine security issues
AMANDA security test cases are derived from OWASP, FIPS, and programming language specific potential vulnerabilities.
Security profiles with different threshold limits for browser cluster, audit, URL checks, request concurrency, sub-domain inclusions, etc, are created for each module in AMANDA and during scanning, these profiles are applied.
We partner with 3rd party labs for certification of any major release of the product.
You test the solution once more before you take it live
CSDC customers test the AMANDA solutions for vulnerabilities during the implementation stage and work with CSDC client services team to ensure AMANDA is secure in customer environments.
Customers use tools like IBM AppScan, HP Fortify, etc.
AMANDA 7 gives you peace of mind
Security is built into the architecture of AMANDA. Testing is factored into each level of design and coding. Multiple tools are used to detect vulnerabilities.
AMANDA complies with the stringent OWASP Security rules. What’s more, the quarterly security training refreshes for development teams ensure that the latest vulnerabilities are factored into the system build. And finally, our customers are encouraged to test and report as well.
All this ensures that the integrated system you buy for Permits, Licenses, Code Enforcement, Compliance, FOIA Management and more, are secured well to bring peace of mind to local government CIOs.
Corporate blog for Calytera
Written by
Calytera helps governments create safe and prosperous communities.
Corporate blog for Calytera
Written by
Calytera helps governments create safe and prosperous communities.
Corporate blog for Calytera
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@Jimmy.Cai/firebase-limit-access-to-certain-domains-954795702b91?source=search_post---------161,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jimmy Cai
Jul 31, 2018·3 min read
While I was developing Iridescent, due to the fact that it’s a internal project, I had to limit the access only to user with email of certain domains.
Firebase Auth has provided a parameter hd to limit accounts shown in Google OAuth UI:
But in my case, employees and students belong to two different GSuite domains, and hd parameter only supports one domain.
Also, Firebase’s API Key is exposed to the client-side, validate domain only on front-end is obviously not an wise option, as the user can download and modify the script. There must be a validation also on the back-end.
A small description: My project uses Firestore and Firebase Authentication. Login is required to visit the page. When a user logs in, a Firestore document named with user’s ID is created (or updated if it exists) under users collection to store information like name, email address or avatar URL.
So I wrote the following function for Firestore:
Considering the case that I want to allow access only for people who has email s.example.com and example.com
This is a small example of Firestore rule:
Now that we have denied requests from non-whitelisted domain, to improve the experience, an alert should appear when validation fails.
As I said, each time user logs in, a function is called to update personal information received from Google OAuth. Instead of making another request to validate domain, we can reuse this one to reduce number of API call.
I created a function called validAccountCheck to do so:
If the attempt to set new values to user/${userID} fails, it means current user doesn’t have a email with white-listed domain.
validAccountCheck() returns a Promise with a Boolean, which indicates if domain is white-listed or not.
In my project, this function is called after login. If domain is not valid, user will be logged out and alerted.
Originally published at blog.jimmycai.com on July 31, 2018.
Just talking.
See all (42)
Just talking.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@cloudaware/3-requirements-for-customer-friendly-devops-6f1e4b64fd17?source=search_post---------162,"Sign in
There are currently no responses for this story.
Be the first to respond.
Cloudaware
Oct 3, 2017·2 min read
Enterprises rushed into AWS because in part business was tired of poor customer service from internal IT department. Of course, this is an oversimplification but for what I am going to share with you, it is the most important detail.
DevOps must realize this and avoid making the same mistakes. So what does it take to build a sustainable customer friendly DevOps?
Treat your customers… well like customers. They are not captive audience. They have choices and they will leave you. You cannot hide behind corporate standards and security or architecture review boards to force them to use your services. Focus on attracting your customers rather than forcing them to work with you or using your stack only.
Create service maps that not only map services to infrastructure components but also map down to specific people. You should be able to look at any piece of infrastructure and map it to a service, to business unit, to specific contacts within seconds. Btw, check out Cloudaware CMDB Slack integration it is awesome. Basically, your service maps should be people-aware if that makes sense.
Visibility leads to accountability. If you’re delivering services like monitoring, security or backup, those must be visible to your customer. If your customer is asking you what you are monitoring, that is kind of a failure already. If your customer is asking you whether “we are secure,” that’s also a failure. Create tools for your customers to see and understand what services you’re delivering for them.
Customer will do two things. Firstly, they will love you for the transparency and actually delivery of the services since they can actually now see them and secondly they will know raise hell when they see something off which will keep your staff in top shape.
Find out more about Cloudaware here.
The Most Complete Cloud Management Platform
The Most Complete Cloud Management Platform
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@tom.5610/aws-private-serverless-api-part-1-cfe7e81e5987?source=search_post---------163,"Sign in
There are currently no responses for this story.
Be the first to respond.
Tom Liu
Sep 9, 2018·3 min read
Recently, my mate asked for a favour to review a cloud architecture on AWS, for which is an internal simple API service to render staff roster detail with HTML format. API consumer will be TV displayers within private network and the staff roster detail is provided by third part RESTful APIs.
Here are the questions:
As a matter of fact, who doesn’t like Serverless if you can run applications on cloud without looking after virtual infrastructure resources?! Surely, the simple answer to first one is a big ‘YES’, but… what about the second one? Let’s look at the detail of security requirement:
And this is the deployment environment Diagram:
Now, we shall focus on where to place ‘StaffRosterDisplay API’ and what can be a suitable solution architecture.
Solution Architecture Design
Assumptions:
With that, we can easily conclude that the service should be hosted on Corporate AWS Account and the next question is: What & How-To.
What can be a suitable solution architecture? Surely, the obvious answer is to do an Application Load Balancer with Auto Scaling Group managing a fleet of EC2 instance providing the API service. But wait… does it sound like overfit the need? We know that this definitely is a solid-proved option but it is not convincing to tackle a simple API service with a fleet of AWS EC2 resources, which also require people to look after.
With that, AWS PrivateLink service does sound in this scenario:
How to implement ‘StaffRosterDisplay API’ with private API Gateway? I’ve created Github repo — staffrosterdisplay project for demo. Detailed instructions have been captured in project README.md file.
To accomplish the demo api service, I use a template engine on api service to wrap up html page content instead of calling third-party API services to get staff roster data.
Last but not least, this private API Gateway service is like a small but non-trivia piece of API Gateway puzzle. It make private Serverless API service feasible and easy to achieve.
Next Step, I shall utilise AWS Endpoint Service to create an API for staff roster data. With AWS PrivateLink service, we can provide the private API service from different AWS account, which builds up private API service provider and consumer pattern in multiple AWS accounts or VPCs.
Until then, stay tuned.
Curiosity, Love and Life — find me in LinkedIn https://www.linkedin.com/in/tom-liu-64843313/
Curiosity, Love and Life — find me in LinkedIn https://www.linkedin.com/in/tom-liu-64843313/
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@spoljaric/strategic-decision-for-developing-our-own-international-dc-net-hw-infrastructure-2bea7b96fe5b?source=search_post---------164,"Sign in
There are currently no responses for this story.
Be the first to respond.
Damir Špoljarič
Oct 7, 2018·4 min read
This article gives you a sneak peek into our internal strategic decision-making process and explains why we decided to do things the hard way and create our own infrastructure including building our own datacentres and developing our own hardware.
Looking around, we can see that various foreign companies specialising in providing cloud-based services choose different approaches. Some choose to rent the infrastructure as a service (e.g. Heroku in AWS), others opted for creating their own infrastructure — such as OVH and other companies. Some companies have changed their approach over the years– RackSpace is leaving their own infrastructure and moves to Amazon, Dropbox, on the other hand, left Amazon in favour of creating their own solution. For these companies, the decision is mainly based on the analysis of costs; simply put, some companies are large enough to benefit from developing their own infrastructure solutions. On the other hand, companies can choose to outsource the “commodity part” and focus on the development of their own services with added value, which is a perfectly valid reasoning.
We faced a similar decision and after long deliberation and planning, we came to a conclusion to build our own infrastructure. In many ways, this approach includes more risks and is definitely more demanding in respect of investments and given the current size of the company the operating costs might seem unreasonably high. For the Central European market, we are using the services of VSHosting and, starting next year, Zerops.io with locations based in Czech Republic. We built our first data centre here and are currently planning to build a second datacentre. Besides the data centre, we use our own means to create our own solution of the entire backbone network infrastructure including strategic parts of optical paths.
What brought us to this decision?
A, the risk factor
The first impetus for leaving rented third party infrastructure is based on the risk factor. We take pride in the quality of our services and believe that the highest quality can be achieved only by being able to manage the operation risks, ideally down to the high voltage power supply. From the point of view of the SLA, the concept of provision of cloud-based, colocation and similar services is a mixture of limited guarantees; in reality, nobody actually provides a guarantee for the full scale of problems caused. For example, I recommend you read through the standard SLA of Amazon (https://aws.amazon.com/compute/sla/), particularly the part saying that in the case of outages with the downtime of 7 and more hours per month, you are eligible for a 30% discount. As a client, you never know whether the provider of datacentre or cloud-based services is doing everything to eliminate all possible operation risks. You do not know whether the providers carry out inspections of all equipment of the datacentre as they should and you have no real way to see if they do. When maintaining the infrastructure yourselves, you can afford to invest into an “unnecessary” redundancy of those items that pose increased operational risk and threaten your business. We have invested in the N+2 redundancy of cooling system; although the risk of an outage of two cooling units within several hours is really small, it has happened to us before. Same reasons apply for the operation of our own backbone network; we are still forced to rely on other operators we are connected to on the Internet as that is the fundamental principle of the Internet, but we can limit the risks by connecting using multiple networks within a dynamic routing.
B, Costs
As mentioned above, for a company of our size to build its own infrastructure is very expensive in respect of the investment as well as operating costs. But as the company grows, building our own infrastructure makes more sense, particularly as we are able to plan the infrastructure and look for optimal solutions. That is why we came up with the idea of building the datacentre ServerPark DC2 which will be constructed on the basis of DC power supply, combination of direct and indirect freecooling with adiabatic (cooling by evaporation of water mist on the heat exchanger) and utilising the concept of our own servers, which means that we can increase the capacity of the datacentre (up to tens of thousands of server units) whit significant savings on the operation due to reduced power consumption. In numerical terms, the cost savings can amount to 40–45% in comparison with using third party infrastructure.
Locations outside Central Europe
As we are planning to expand our business to other countries, e.g. to the United Kingdom, we are going to develop solutions for local infrastructure. Our ambitions do not include a construction of a datacentre outside of Czech Republic (yet), therefore, we need to use third party datacentres in the Central European countries; however, we are going to solve the remaining parts of the infrastructure ourselves. The above mentioned disadvantages (inability to control the datacentre operation) apply in this case, however, in the case of Zerops.io-type services, we will compensate for the disadvantages by running operations in several independent locations. Moreover, Zerops.io is technically prepared for operating on completely flat infrastructure on unified hardware, which further facilitates things.
What are the disadvantages of using our own infrastructure?
The operation of our own infrastructure is very demanding in terms of personnel and knowledge requirements. It is very easy to make a mistake and cause some technical issue simply due to insufficient experience, which is something that does not happen with rented infrastructure of an experienced provider. Beside the development of our own services with added value, we are also creating our own solutions of the “lower” infrastructure (datacentre, network) which increases the costs and causes additional stress. But we know why we are doing it. We want to be a premium provider.
Co-founder of VSHosting.eu, Zerops.io, SnackHost.com cloud
Co-founder of VSHosting.eu, Zerops.io, SnackHost.com cloud
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@reneweurope/green-light-to-the-free-flow-of-non-personal-data-in-the-european-union-70728651eadc?source=search_post---------165,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renew Europe
Oct 4, 2018·2 min read
Today the plenary votes in favour for an Internal Market report on free flow on non-personal data which will help to unlock the growth potential of EU data economy.
Currently, data localisation restrictions by Member States’ public authorities and obstacles to the movement of data across IT systems prevent business and organisations in the EU from capturing economic, social and business opportunities. Legal uncertainty and lack of trust cause additional barriers to the free flow of non-personal data. In practice, this means a business may not be or feel free to make full use of cloud services, choose the most cost-effective locations for IT resources, switch between service providers or port its data back to their own IT systems.
The general policy objective of the initiative is to achieve a more competitive and integrated internal market for data storage services and activities.
MEP Dita Charanzová, shadow rapporteur on this file said:
“Today, we confirm the new fifth freedom — the free movement of data. Goods, services, capital and persons can go anywhere in Europe and after today, now too can data.
The Free Flow of Data is vital to the digital economy and the Single Market as a whole. Today, cloud computing services are backbone of everything from accountancy software to road traffic management. It is clear that, at the European level, we needed to prevent actions which would have put our digital future at risk. This includes ill-advised national measures restricting this free flow.
This is why the ALDE group and I were the first supporters of this Regulation. I hope that this will be an important step towards more access to affordable cloud services for businesses and taxpayers alike.”
The creation of this 5th freedom of movement in the EU will enable innovation in numerous sectors such as Artificial Intelligence, e-health, bid data or cloud computing. Positive environmental impacts are also expected, as locating hosting in greener and cooler countries will be possible, leading to less pollution.
More info: david.vidal@europarl.europa.eu
We are the pro-European political group in the EP fighting for your freedom, civil rights while securing economic growth and jobs. www.reneweuropegroup.eu
We are the pro-European political group in the EP fighting for your freedom, civil rights while securing economic growth and jobs. www.reneweuropegroup.eu
"
https://medium.com/@philwicklund/devops-needs-a-product-mindset-f4d221cef990?source=search_post---------166,"Sign in
There are currently no responses for this story.
Be the first to respond.
Phil Wicklund
Mar 27, 2021·9 min read
Ten months ago I stepped into a new and exciting opportunity at Medtronic. My basic goal is to help my internal “customers” modernize their apps and the ways in which they work, with objectives around increasing our release frequency, quality/reliability of our apps, and to enable a more innovative, autonomous, and accountable development culture.
The journey thus far has been nothing short of invigorating! We’re standing up cutting-edge self-service AWS cloud infrastructure, templatizing pipelines to enable faster adoption, and facilitating the capture and dissemination of DevOps best practices. I’m learning A TON and working with extremely brilliant teammates!
However, my background is in Product Management, so what am I doing gushing over an internal IT adoption project? Why is this opportunity at Medtronic something to be so excited about?
Well, that’s my goal today, to share how applying a Product Management methodology to internal IT initiatives can enable deeper customer success and create deeper meaning for myself and my teams.
Before joining Medtronic I brought to market B2B products in the digital communications industry. Today, I’m still building products… but I like to say I’ve moved from B2B to B2E to fully embracing “E2E products”.
E2what???
E2E, or “Employee to Employee” is my adaptation of B2E, or “Business to Employee”. Workday and other HR platforms broke ground in B2E, and E2E is my extension of that concept, except where teams within the company are productizing their value to other teams.
My team exists to help other Medtronic teams deliver their apps faster and at higher quality through the internal productization of DevOps and Cloud services.
E2E moves beyond the simplistic “producer/consumer” model of team dependencies common in operations. E2E product teams disdain the notion of a “project” and the iron triangle, so commonly leading to failure.
Fully embracing an agile Product development mindset, E2E product teams see the enterprise as a vast “marketplace” with “customers” who have “underserved needs” and “pain” that is uniquely felt by them but yet is generalizable to many.
These “opportunities” beg a product solution with a “value proposition” that can be brought to “the market”.
Customer success, net promoter scores, objective-key-result indicators, key performance indicators, and even internal revenue/expenses P&Ls (chargeback) can be set up to measure success.
This is what I’m trying to do at Medtronic around DevOps and cloud adoption.
In many spaces, I leave my product-y nomenclature at the door, but the basic thrust of how I’m approaching my mission is unabashedly product-centric, even within the depths of a 100k global employee enterprise.
Well, we learned like 30 years ago (Hammer & Champy, 1993) that 70% of tech projects fail, so I don’t feel like I need to rehash my “agile vs waterfall” diatribe. That should be obvious by now.
But what is agile anyways?
Damn, I just can’t help myself.
I’ll keep it short, I promise. My basic premise here is “Agile” and “SCRUM” are inherently customer-centric ways of working. With mechanisms such as “customer feedback loops”, sprint reviews, “product owners”, and so forth, one can argue that there’s a built-in customer bias, vs. say “business objectives”, “scope” or other measures of success found in more traditional waterfall settings.
DevOps in particular is particularly difficult to define project scope for, making an agile methodology all the more critical.
But what is DevOps? What does it mean to be “the DevOp team”? We know the DORA report and all the objective measures of DevOps performance (lead time, frequency, etc), but what do we do, build, and/or buy to help us “do DevOps” better? The problem is of course these questions themselves.
You can’t “do DevOps” and there’s no such thing as “the DevOps team”. One can only help others in THEIR DevOps transformation, and a Product Management methodology is the best methodology I know of to help drive this program of culture change.
DevOps is what I call an “infinity term”… there are infinite ways to define the meaning and deploy the word to suit one’s purposes. “The Cloud” is also equally infinite. How do we deal with this?
Every Product Manager knows that “if you build it, they will come” is not a successful product launch strategy. The same is true for internal adoption initiatives and DevOps in particular.
DevOps is NOT a tool. DevOps is a way in which we work, a combination of people, processes, and technology to continuously deliver value. Its goals are to enable us to go faster, achieve higher quality, and be more innovative. Too often folks scope a “DevOps Initiative” to deploying a tool, but that’s only one of the three dimensions.
Product Managers know their biggest blind spot is their own bias to their product. Likewise, with DevOps, we know we need to look past the tools. Product Managers know that the value needs to be bought into and the customer experience with the product needs to be great. Just dropping in a new tool doesn’t ensure either is true.
Don’t start a DevOps conversation with tools. Instead, start where Product Managers start… with your own internal market research.
Tech startups similarly begin their journey with limitless possibilities. The marketplace is infinite with many industries, customers, and potential competitors. Key point: before you get yourself sold on one DevOps tool or another, be sure you know what problem you’re trying to solve.
Tabulate these problems as “opportunities” to add value. The DevOps space is huge, so your list may be long. Some examples might be:
And so on.
I state the obvious here just to say that knowing what problem you’re trying to solve, and what business impact you’re trying to make, is so foundational to a successful DevOps program.
A “tool” doesn’t solve any of the above, per se. None of that gets better by punching in your credit card number. It takes concerted effort supporting the improvement of people, process, and technology to impact the organization change in mindset that is “DevOps”. More important, you need to know what problem you’re trying to solve, and you need to measure your progress towards solving that problem.
Don’t buy a DevOps tool unless you have a measurement strategy to gauge the desired “opportunity” impact!
As you do this market research, you’ll find that some problems are bigger problems than others. Some problems are more important to your business than others. Also, some problems come with solutions more readily than others. These solutions can in fact be “competitors” too… such as other tooling or legacy processes that limit your penetration.
The key: which problem space will you focus on? Your value proposition cannot be “We’ll do DevOps” or “We’ll deploy XYZ tool”… it has to be more specific and a value statement.
The minimum viable product is your friend when you have infinite opportunities such as in the DevOps and Cloud spaces. Just deploying a tool does not mean you derive the intended value. Even if the tool does 50 things great, it’s still crucial to dial in on a few key results and value you are focusing on delivering.
The key with any MVP is:
Some of these DevOps tools are SOOOOO full-featured that all you get is dear-in-the-headlights looks, stakeholders are overwhelmed and as a result, old ways of working (culture!) don’t change.
In DevOps, culture is king, and a DevOps initiative is more about changing that culture than anything else. FOCUS is the key, knowing what aspects of culture you are seeking to change, and how you’ll change it (tools, support, engagement, etc) is key too or culture won’t change.
Viability is key too.
If you have a 20-year-old monolith application and you drop in support of Kubernetes, you’ll soon find out that just because you have a cluster running doesn’t mean folks will start using it.
Viability MUST be defined by your customer!
Many DevOps tools and practices far exceed their practicality in legacy, monolith application settings, and as a result “viability” may look different from their perspective than yours.
And isn’t that the entire point of an MVP? To deliver just enough product to get feedback? To then take those learnings into the next “release” and thereby continuously adjust your actions to best achieve success for your customers?
In the DevOps space, a “feature” could be:
The other day I met with a buddy and we were discussing my new role at Medtronic. He was surprised I took the job because of how much I enjoy sales, marketing, and sales engineering, as he knew well. He worried I couldn’t be happy in my new role.
The reality is a “Product Mindset” comes with me no matter where I go or what job title I have. Sales and marketing are important adjacent support roles of any Product Manager, and it’s no different with me in my new role.
DevOps is all about culture. Culture does not change without deep relationships and engagement in those relationships to influence new possibilities, new ways of working. I may not be “dialing for dollars”, per se., but driving adoption of new ways of working indeed takes a lot of salesmanship.
Marketing too is a key enabler of any DevOps initiative. It is invaluable to have a clear and concise “story” for why you do what you do, what value it provides, and how you provide that value. Otherwise old ways of working will just manifest themselves in the new shiny tool you just bought. Culture means folks are bought into a common vision, and good marketing can help us get there together.
There’s a lot more I can share, such as what a DevOps E2E roadmap could look like, what “features” you might scope into your DevOps MVP, and some lessons learned as I’ve transitioned into the E2E space. But, this post is already way longer than I thought it would be. Stay tuned for more!
However, I hope at the very least this gives a sense of how a Product mindset can go with you no matter your setting, role, title.
Even at the most basic level, our individual raw skills are our “product” and our manager could be seen as “customer”.
Whether a custodian at your local high school, Product Manager at a swanky start-up, or a leader in an enterprise setting working to raise all boats, product-centricity is a great approach in each circumstance.
Let me know if you agree/disagree! Any examples you have of taking a Product Mindset into the enterprise? Lessons you’ve learned?
Cloud hobbyist, engineer & Product Manager @ Toppan Merrill. Lover of the arts, cooking, and photography. Welcome to my buffet :)
Cloud hobbyist, engineer & Product Manager @ Toppan Merrill. Lover of the arts, cooking, and photography. Welcome to my buffet :)
"
https://medium.com/hashmart-blog/bitmain-announces-antminer-t19-what-impact-it-will-cause-in-bitcoin-mining-market-e8511e22578a?source=search_post---------167,"There are currently no responses for this story.
Be the first to respond.
The Antminer T19 by Bitmain may not have a big impact on the Bitcoin network, and it comes out amid the firm’s internal and post-halving uncertainty.
Description: Bitcoin miners now have a brand new machine, Antminer T19, delivered by Bitmain.
Earlier this week, Chinese mining-hardware firm Bitmain started Antminer T19 sales. This mining unit belongs to the latest generation of Bitcoin mining equipment. It is noteworthy that the announcement of Bitmain came at a bad time for Bitcoin miners: after Bitcoin halving their income dropped by half. How can Bitmain win a bigger market share with its new product?
According to the official, the Antminer T19 features a mining speed of 84 TH/s and a power efficiency of 37.5 joules per TH. Bitmain has designated the T-series as the most effective in terms of cost/income ratio. According to some data, T19 can generate $3.97 of profit every day. The price of T19 is set at $1749.
Marc Fresa, the founder of mining firmware company Asic.to:
“When chips are designed they are meant to achieve specific performance levels. Chips that fail to hit their target numbers, such as not achieving the power standards or their thermal output, are often ‘Binned.’ Instead of throwing these chips in the garbage bin, these chips are resold into another unit with a lower performance level. In the case of Bitmain S19 chips that don’t make the cutoff are then sold in the T19 for cheaper since they do not perform as well as the counterpart.”
It seems like Bitmain rolled out its new piece of equipment for mining not because other ASICs get bad attention from customers. Officially, Bitmain machines selling well. It is just simple product diversification, which is common strategy for mining hardware producers.
Kristy-Leigh Minehan, a consultant and the former chief technology officer of Genesis Mining:
“ASICs don’t really allow for one model as consumers expect a certain performance level from a machine, and unfortunately silicon is not a perfect process — many times you’ll get a batch that performs better or worse than projected due to the nature of the materials. Thus, you end up with 5–10 different model numbers.”
Bitcoin’s hash rate dropped 30% soon after the third halving occurred as much of the older generation equipment became unprofitable due to the increased mining difficulty. But what will happen when new generation ASICs will be plugged in in the next few weeks? Minehan said she doesn’t expect the T19 model “to have a huge impact that’s an immediate cause of concern”. So T19 probably will not boost up the mining difficulty forcing many miners to leave this market:
“There isn’t a strong reason to expect the new model to significantly affect the hashrate. It might be a slightly more compelling option to a miner with extraordinarily inexpensive electricity, but otherwise they likely would have just purchased an S19 instead.”
As you can see, mining business with ASICs is pretty complicated. This is the constantly changing market, depending on Bitcoin price, mining difficulty, electricity bills and much more stuff. But there is a better solution — join our Bitcoin cloud mining platform Hashmart.io to earn BTC on a daily basis just with an investment plan!
Cloud Bitcoin Mining with Hashmart.io
Cloud Bitcoin Mining with Hashmart.io — it is a convenient, easy and reliable way to earn Bitcoin and multiply your investments regularly. Here you can find everything about cloud mining and cryptocurrencies. Join the Hashmart.io Miners Club — we know how to get Bitcoin!
Written by
Bitcoin-maximalist. Optimistic family man and miner with six years of age. I write about complicated things from the future for people of our days.
Cloud Bitcoin Mining with Hashmart.io — it is a convenient, easy and reliable way to earn Bitcoin and multiply your investments regularly. Here you can find everything about cloud mining and cryptocurrencies. Join the Hashmart.io Miners Club — we know how to get Bitcoin!
"
https://medium.com/rekerrsive/should-you-use-a-backend-as-a-service-for-your-enterprise-mobile-app-13f1b9f1eb99?source=search_post---------168,"There are currently no responses for this story.
Be the first to respond.
A few types of apps don’t need a “backend” — for example, a calculator app is self-contained and needs no back-end at all. But most apps depend on backend services of some kind. Backend services provide essential functionality to many mobile apps:
Most users don’t think about web service layers underneath their favorite apps, but users do expect applications to be responsive. Backend services need to be scalable, reliable and able to deliver sub-second responses to many concurrent users.
In the mid-1990s, when the Internet was young, it was common for companies to build their web site using internal web servers racked in their own data centers. Hosting services weren’t yet ubiquitous, and web sites were built and managed much like any other business application.
Today it’s less common for a company build its website from scratch and host it on-site. Content management systems are now the standard building block for a web site, and external hosting providers generally provide higher levels of scalability and availability than most in-house data centers could at a fraction of the cost.
Backend services for Internet-connected mobile devices is beginning to follow the same pattern — even for enterprise applications. Backend services scratch-built as general-purpose web sites are giving way to specialized mobile backend service architectures. Much like web services, mobile backend services can be purchased and used on-premises, or built entirely on public cloud infrastructures in a platform-as-a-service (PaaS) model, or architected using a hybrid approach that blends both internal and cloud-hosted components.
As mobile services become more in demand, many providers are rushing into this crowded space. MBaaS offerings are available in an array of architectures — each one suitable for a different kind of application. Would your project be better served by a platform provider’s MBaaS offering, or by an MBaaS provider with a tightly integrated application development tool? Or something in between?
Understanding the different MBaaS segments is the first step in finding the best match for your requirements. Let’s consider the general categories.
If you’re an enterprise with a strong IT-led software design competency, the most comfortable fit may well be your cloud platform provider of choice. These providers include (but are not limited to):
These types of MBaaS providers offer mobile services as a part of a larger suite of offerings. They can provide not only mobile backend services, but other types of application services that can be leveraged as well to build large, comprehensive enterprise architectures.
If you have broader IT architectures to build along with your mobile project, a general-purpose provider with a strong MBaaS on offer may be an obvious choice. But even if you don’t need a broad platform today, the stability and future-proofing you get by using an experienced platform vendor are appealing.
On the downside, platform providers provide broad platforms that can be more complex to understand and learn to use. This shouldn’t be too surprising — platforms that have more knobs to turn, give more control to the application developer, and have more ways to meet the same requirements inherently have a steeper learning curve.
If your primary goal is to support your mobile app — and particularly if it’s a narrowly-targeted consumer focused app — you might first consider a cloud provider that’s narrowly focused on its MBaaS offering. Examples of these include:
There are advantages to using a provider like these. These providers are more focused on providing MBaaS services as their core product, whereas platform providers typically offer MBaaS as a feature of their overall portfolio offering. Developers without enterprise application experience may find that the more narrow product focus of these MBaaS providers leads to a lower learning curve for them.
On the other hand, mobile-centric MBaaS providers may not provide as much flexibility as a larger platform provider. For example, a mobile-centric provider may not provide a SQL-based relational database as part of the platform. Large, general-purpose platform providers like the ones typically would provide such an offering within their overall platform — as well as Internet of Things (IoT), Big Data/Hadoop, and cloud-based virtual machines that could be provisioned to support application requirements.
When looking at specialized MBaaS providers, bear in mind that they can be quite narrowly focused on specific mobile app segments. For example, Parse is more focused on delivering consumer apps, while FeedHenry is much more focused on the needs of internal enterprise applications.
As a final note of caution: the narrowly-focused MBaaS space has been recently characterized by frequent acquisition announcements. A few of these acquisitions have led to services being completely discontinued (perhaps the acquisition was to acquire engineers — not customers?).
Since your iOS or Android code will include APIs specific to your MBaaS provider, the last thing you want to do is migrate to a new provider on a forced deadline after an unexpected acquisition. In this regard, a general-purpose platform provider may provide more stability and predictability of long-term support.
Each of the previous two providers types generally offer MBaaS services provisioned as web service layers to your application — but you still build your own application using whatever development tools you choose. This might be XCode for iOS, Java for Android, C# for Windows or PhoneGap for an HTML5 solution. You then use the MBaaS provider’s APIs to connect your application to their backend service layer.
App Building Platforms go much further, and provide not only the web service and data layers, but also a complete application development environment. These providers usually promise tools that let you code an application once (typically in JavaScript) and deploy it to all devices (either by cross-compiling to native applications or by running an HTML 5 app in a proprietary container). This approach promises tighter end-to-end integration between the front-end development and back-end development as well as higher levels of developer productivity.
On the other hand, using an App Building Platform to build and host your application would seem to provide the highest degree of vendor lock-in. Moving your application away from a proprietary development environment — should you ever decide to — would likely be a far more significant re-engineering effort than switching from one platform provider’s MBaaS layer to a new one’s.
Providers that provide this type of full application development and application hosting services include:
All of the MBaaS providers in the previous three categories focus on providing public cloud services to back-end customer applications. Some have no built-in facilities to integrate with on-premises services (e.g. Parse), while others have a fully-developed tooling to build hybrid cloud/on-premises architectures (e.g. IBM). What if you want a mobile backend, but for whatever reason none of it should be deployed in a public cloud?
Predictably, there are alternatives for enterprises that want or need to build and deploy their mobile infrastructures entirely on-premises. Some examples include:
I previously listed two of these three providers above in the public cloud segments. In fact all three of these providers offer both on-premises and cloud offerings. The converse is not true, however. Most cloud-based MBaaS providers do not offer on-premises deployment solutions. So, if you know you must deploy on-premises, your options will be fewer (though still excellent!). Always know your constraints before selecting your technology!
Why use a mobile backend platform for on-premises deployment? Why not just crack open your favorite development IDE and build it yourself? The answer revolves around the trade-off between building every nut and bolt in your application stack — or using pre-built components as a way to avoid building and supporting so much complex code yourself.
Using a pre-built stack like IBM MobileFirst provides application deployment and monitoring, as well as pre-built web service integrations for all major platforms — right out of the box. Buying a backend service platform provides a lever to get a solution deployed faster and with higher quality than a completely build-from-scratch development strategy.
As always, deploying on-premises software involves higher up-front capital investment, requires more sophisticated infrastructure support resources in-house (or contracted from an external provider), and more hardware and network investment than cloud provisioning. Yet depending on how application partitioning, information security requirements and service dependencies fall, on-premises may be the best (or only) alternative when considering a mobile application deployment strategy.
Originally published at rekerrsive.ghost.io on February 8, 2015.
Tech blog about iOS development, cloud deployment, mobile…
Tech blog about iOS development, cloud deployment, mobile DevOps and building great software products
Written by
Founder at Cuvenx, Inc. I provide architectural/advisory consulting, UI design and hands-on software engineering for native iOS and Android platforms.
Tech blog about iOS development, cloud deployment, mobile DevOps and building great software products
"
https://medium.com/@UDig/docker-brings-scale-at-cost-dfc5aaeab5a9?source=search_post---------169,"Sign in
There are currently no responses for this story.
Be the first to respond.
UDig
Sep 10, 2019·4 min read
Our UDig Software Engineering team has always been a cloud–first engineering group, but while inventorying our internal application and cloud usage we were disappointed at our spend and over-usage of EC2 instances with AWS. Through research and experience we knew if we could containerize our applications with Docker, we’d then be able to consolidate and maximize our servers‘ output. Before we go any deeper let’s look at why this is the case.
First off, you’ve likely heard before from teams leveraging Docker that it is much faster than virtual machines (VM) or other setups. While this may be true for some applications, what I think users are anecdotally recognizing is the efficiencies of Docker and how it can allow applications to use more of the horsepower their servers carry. One of the key reasons here is unlike virtual machines running on a piece of hardware with fixed CPU and memory limits, Docker doesn’t need to know these things per se. Docker is smart enough to automatically allow the containers who need the most horsepower to leverage more when they need it and containers not in use or between runs to release and idle using little to no resources on the server. This fundamental paradigm shift in virtualization is one of the many benefits of Docker and containerized applications.
So back to our UDig story and our journey to use Docker and containerize all our internal applications, services and batch jobs. The first part of our process was to catalog all our applications / capabilities and ensure that Docker was a viable home for them to live and run. Upon completion of this exercise we had a list of applications mapped with any call outs that were of concern or required further investigation. Fortunately for us all our applications could make the transition. Only a couple needed some adjustments to optimize them for running within containers. With this information in hand we were able to quickly map out an approach and begin the process of setting up proper container repos, CI/CD pipeline builds and ultimately deploying out applications to their new home. With this effort, we took a series of applications (10 in total) and consolidated them from across 6 instances to a single instance. The new single instance was setup with more horsepower, but the cost of this single instance was less than half our spend for running the various multiple servers. All in all, our total spend on AWS was decreased by half.
Understanding how Docker can leverage your hardware more efficiently than VMs is easier to understand once you review the diagram below. By simply removing the Guest Operating System from the equation we allow the Docker framework to handle all sub-process (applications) with maximum efficiency. Containers share the entire hardware’s resources with the administrator’s ability to prioritize containers over one another to ensure the most critical services are available and responsive when you need them most. This brings up another point, resiliency. By building containers with a health check, Docker can also automatically respawn hung or fail processes allowing for developers to handle auto recovery scenario and more.
Stack comparison between Virtual Machines (left) and Docker (right)
Docker simplifies your deployment strategies through the ease of:
Docker allows for the use of many administrative tools. Pick a flavor but we’ll recommend Kubernetes. The power of Kubernetes allows for remote administration of Docker clusters, nodes, networks, volumes, containers and more. Basically, everything within the Docker eco-system can be managed by a Kubernetes admin. Another huge benefit is Kubernetes Control Platforms such as Kublr, Portainer and GCP Kubernetes Engine. These platforms allow for quick creation of clusters within your favorite cloud provider to enable almost instant access to a production deployable environment.
Docker has tremendous scaling abilities both at the container resource level, in the number of container instances per node and even across clusters. This means your administrators can fine tune environments to provide maximum benefit to users and clients.
As mentioned above in scalability, being able to run containers across clusters (read geographic regions) we can enable fail safe measures if any one node fails in a cluster.
If it hasn’t become clear yet, Docker’s ability to automate all aspects of an environment allows for rapid recovery strategies when multi region outages occur. Moreover, the ability to leverage health checks and event default restart parameters for each container / service ensures total control over how your environments handle anything from hung processes to regional power outages.
If we haven’t sold you on Docker yet then we may never, but for those that are wondering how to get started, it starts with a proper application assessment / readiness exercise. With this effort we’re looking to identify the environment requirements for each application, whether it is Java, Python, PHP or another application, we want to ensure Docker can handle the framework. The good news here, apart from .NET standard, which requires a Windows only image, almost every major web technology can run within Docker. This is great news and another reason we’re seeing such high adoption / usage rates with Docker.
Choosing to leverage Docker is a cultural technology change. One we think can benefit any sized organization through its simplicity, ease of management and superior enterprise enablements. Our Software Engineering teams build and deploy critical business applications leveraging Docker all the time. If you’re struggling to embrace this new world of hosting and DevOps, we have professionals capable of demystifying the process and helping you modernize applications with minimal ramp.
We develop innovative and modern technical solutions to solve the toughest business challenges.
We develop innovative and modern technical solutions to solve the toughest business challenges.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@vernieman/google-is-winding-down-in-house-stadia-game-development-division-74129dfb4a9b?source=search_post---------170,"Sign in
There are currently no responses for this story.
Be the first to respond.
Vernon Chan
Feb 3, 2021·2 min read
Google has announced that it is shutting down its internal game development division- Stadia Games and Entertainment (SG&E) to refocus on building Stadia as a technology platform.
In a blog post, Phil Harrison, VP and GM of Google Stadia said, “Creating best-in-class games from the ground up takes many years and significant investment, and the cost is…
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/cartonvault/introduction-to-carton-vault-6366813663ce?source=search_post---------171,"There are currently no responses for this story.
Be the first to respond.
Carton Vault (https://cartonvault.com) started as a internal tool which we built for our team. Our team have a number of email accounts and desperate need some tools to back them up instead of using the most famous backup tool — PST.
Thus, we developed Carton Vault which take copies of the email, files we need backup from the email accounts (from IMAP, to Exchange to Office 365) and store them in the cloud.
As of now, the Carton Vault is able to perform the following:
You can also upload the annoying PST to Carton Vault and forget about it.
Come to https://www.kickstarter.com/projects/1359110186/carton-vault-ultimate-backup-solution to support our Kickstarter campaign.
Ultimate Online Backup Tools for your data, like Email…
Written by

Ultimate Online Backup Tools for your data, like Email, Files
Written by

Ultimate Online Backup Tools for your data, like Email, Files
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@cloudopsgeek/hi-max-saltonstall-f824246ee675?source=search_post---------172,"Sign in
There are currently no responses for this story.
Be the first to respond.
Azhagiri
May 9, 2020·1 min read
Max Saltonstall
Hi Max Saltonstall,
I am trying to access our on-premise internal application which is running on port 8080 via Google Cloud IAP Connector. I have deployed the IAP Connector using following link https://github.com/GoogleCloudPlatform/iap-connector
https://cloud.google.com/iap/docs/enabling-on-prem-howto?authuser=1#iap-access.
after deploying the IAP Connector, While accessing the Application I am getting the following error.
I understand after GKE 1.14 the masquerade rule was removed and we need to white list our POD IP to the on-premise side.
Are there any configuration changes required since my application is running on the port 8080?
Your input will be helpful for fixing my issue.
Thanks in Advance. Waiting for your response.
AWS | DevOps | GCP | Cloud Architect | 3 * AWS Certified| Blogger
See all (78)
AWS | DevOps | GCP | Cloud Architect | 3 * AWS Certified| Blogger
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@stephen_22404/great-fun-accessing-your-infrastructure-how-secure-are-you-19c3a83fd1c0?source=search_post---------173,"Sign in
There are currently no responses for this story.
Be the first to respond.
Stephen Spector
Feb 6, 2018·2 min read
How secure is your infrastructure? Not just your internal data centers, but what about your networks connecting to public clouds or hosting providers? How about your corporate data which could be anywhere in the world as you certainly have Shadow IT somewhere?
RackN believes that IT security begins with a secure foundation for provisioning not only within your data center but into your cloud environments as well. Having a single tool architected with security as a key feature allows SecOps to spend more time worrying about protecting attacks at the application and data storage layer instead of allowing attacks at the metal.
Issue — Secure the Enterprise
Impact — Business is Digital
RackN Solution — Secure Foundation
Get 1-Pager from SlideShare: Infrastructure Vulnerability
The RackN team is ready to start you on the path to operations excellence:
Open Source Junkie and Recovering Cloud Computing Marketer working to promote innovative and next generation solutions in bare metal and automated provisioning
Open Source Junkie and Recovering Cloud Computing Marketer working to promote innovative and next generation solutions in bare metal and automated provisioning
"
https://medium.com/@NordicBackup/data-security-best-practices-for-smbs-keeping-data-safe-from-loss-54f40fe1f624?source=search_post---------174,"Sign in
There are currently no responses for this story.
Be the first to respond.
Nordic Backup
Apr 9, 2016·5 min read
With the wide array of internal and external threats to data within a business environment, keeping records safe poses more challenges today than ever before. Luckily, for all of the new technological threats that are created, there are a number of data loss prevention tools and solutions that businesses can rely on to keep their data protected and safe from permanent loss.
If your small business is concerned about keeping your data safe from loss, follow these data backup and data security best practices.
Before we get into the best practices your business should follow to keep its data protected and retrievable, it’s important to understand the need for them.
More and more businesses are transitioning away from physical storage and into digital storage spaces, and with that movement comes an increased risk for errors and losses. Businesses can come face to face with data loss due to natural disasters, technological failures, physical device damage, malicious intent, human error and more every day.
While many businesses recognize the reality of these threats, not all of them are prepared to handle them when they arise. “Cyberattacks” and “Data Fraud or Theft” are listed as the two highest areas of concern for businesses in the U.S., according to The Global Risks Report 2016, ranking above terrorist attacks, fiscal crises, asset bubble and a number of other risks.
There are however, ways to keep your data safe from loss and recoverable to you should your data become inaccessible or deleted.
By far, the best way to keep data safe in any business is through preventative measures. With the right security and backup measures in place, you can keep your data safe from digital threats and have the data recovery tools you need on hand, should your data ever get deleted.
To prevent data breaches and leaks, make sure your employees are informed and up to date on how to handle data securely. They should be aware of which data in your organization is classified, as well as which members within, and outside of, your organization are considered ‘authorized’ to view data. There may be different levels of access control you need to consider, as is the case with medical offices handling PHI.
Be sure to outline these considerations so that everyone in your office is on the same page about whom can receive information, and what information they are privy to. Also be sure to distinguish how data should be shared, destroyed, archived, and saved for later use.
Without clear instructions for this, it’s possible for an employee to improperly dispose of a document, leaking private information. Additionally, they could accidentally delete a file that was intended to be saved.
Have a plan for how you manage data on all fronts, and you’ll be much less likely to have to deal with the repercussions of mishandling.
Viruses are a common cause of data loss. Cryptoviruses are notorious for encrypting a user’s data, and only giving them the unlock key after a high dollar ransom amount has been paid. And small businesses are frequent targets, as they have more capital than an individual and have more holes in their network security than larger companies. To avoid them:
Taking steps to prevent the occurrence of risks like employee error and viruses is a good step toward eliminating data loss, however you also need a plan for when threats inevitably make their way past your security measures.
Backing up your data is the only way to recover it in the event of a data loss disaster — whether that disaster is by human error, physical device failure, theft, viruses, or some other threat.
Many businesses make the mistake of backing up their data to physical storage devices alone — especially when these devices are all housed within the same location, keeping them open to the same threats. Physical storage devices are vulnerable and should never be your business’s only backup plan. It’s important to back data up to at least 3 locations: your primary storage, secondary storage device, and most importantly a secure, off-site cloud backup.
Cloud backup is the most reliable form of data backup, keeping data protected and available at your fingertips, regardless of the loss scenario. It can be used to retrieve and restore healthy data after a malicious attack, restore correct versions of files after an employee error, or even reinstate all data to a repaired or new computer if the primary device crashes.
There are a variety of other features that make cloud backup the preferred storage method by businesses. Look for a cloud backup provider, like Nordic Backup, who offers:
Ultimately, cloud backup is the data security best practice that will keep your data safe from data loss arising from nearly any situation. If you want a guaranteed way to retrieve and restore your data, no matter what, cloud backup is the answer.
Secure your business data free, for 90 days with cloud backup, provided by Nordic Backup, and experience total peace of mind.
Originally published at pages.nordic-backup.com.
Leading the industry in secure online backup. The most comprehensive backup and recovery solution for business and personal users. http://hubs.ly/H01kyJQ0
Leading the industry in secure online backup. The most comprehensive backup and recovery solution for business and personal users. http://hubs.ly/H01kyJQ0
"
https://medium.com/the-cloud-builders-guild/emr-cluster-stuck-at-provisioning-de40fafb9ed6?source=search_post---------175,"There are currently no responses for this story.
Be the first to respond.
Recently i have been facing EMR Cluster suck at provisioning issue since we start using custom encrypted AMI Image.
EMR service 3 times tried to launch a new instance to run as the master node and each time the instance was terminated after timeout (~16 minutes). eventually it throws Terminated with errors Failed to start the job flow due to an internal error.
I raised a support ticket and main response from AWS as below:
This causing EMR to not be able to install the required EMR daemon and the instance being terminated because of the timeout Due to our shared responsibility model, issues related to the OS of the instance are primarily the responsibility of the customer to sort out. This is because, for security reasons, AWS doesn’t have access to your OS or know exactly what was installed on the OS. Although, as premium support I can provide general assistance based on what I see from my side regarding your OS. I can see that you’re using Red Hat and based off the fields: “type=1305”, “auid=xxxxxx”, “ses=xxxx”; I suspect the issue to either be with: — the AuditD daemon configurations being misconfigured as type “1305” syscall is related to “CONFIG_CHANGE” causing the daemon to hang for a very long time — SELinux being misconfigured as I can see at some point it’s initialized then subsequently disabled “selinux=0” . Additionally, the AuditD logs which are causing the errors share the same session ID/user ID as the one used when disabling SELinux. For your convenience I’ve attached to the case the EC2_console.log for your perusal. From your end you can only get them while an instance is still running or shortly after it’s been terminated using the either the console or the “get-console-output” AWS CLI command. In terms of further troubleshooting, One of the very good recommendations provided, it states that you should be using Amazon Linux as the base AMI for custom AMIs in EMR. Should you instead want to still try with Red Hat you can maybe try totally disabling SELinux from your AMI if you’re not going to use it. Instead of enabling and then disabling SELinux.
According to the response from AWS, I changed the base AMI from RedHat to Amazon Linux, problem solved!
For more details, please check out AWS official document
docs.aws.amazon.com
I hope this could help you met the same problem.
Cloud enthusiasts building things in the cloud.
17 
17 claps
17 
We are Cloud enthusiasts writing about coding and building things in the Cloud.
Written by
AWS Community Builder | AWS AZURE GCP Certified Engineer | A Cloud Technology Enthusiast | AWS Certified Security/Machine Learning/Database Analytics Specialty
We are Cloud enthusiasts writing about coding and building things in the Cloud.
"
https://medium.com/interact-software/building-a-successful-g-suite-intranet-interact-software-5331362fdc50?source=search_post---------176,"There are currently no responses for this story.
Be the first to respond.
Initially launched back in 2006 as ‘Google Apps for Your Domain’, the G Suite has seen increasing popularity in the enterprise market in recent years: entering 2019 with over five million paying G Suite customers, an increase of 25% since 2017.
With a global brand, reputation for innovation, a cloud-first approach to development and a range of solutions designed to answer top business challenges including communication, productivity, and collaboration, it’s no surprise Google is taking the business market by storm.
Ensuring both the organization and its employees are able to maximize the value and potential of the Google toolbox is a growing concern and priority for today’s CIO. An effective G Suite intranet will unite the digital workplace, futureproof and maximize your Google investment, and support users in getting the most from their G Suite experience.
But while Google answers many of today’s leading business productivity challenges, it has its limitations. Creating an effective G Suite intranet demands us to look at the bigger picture of today’s digital ecosystem, where it fits, and where there are gaps.
In this blog, we explore:
Maximize the value of your G Suite investment. Interact offers seamless out-the-box integration with leading applications from the G Suite, alongside a host of additional features and functionality designed to support your employees.
Download the free Guide
There’s no denying the digital ‘big bang’ we’ve seen take place in our workplaces in recent decades. Technology has fundamentally changed how we work and live, and research suggests the pace of innovation shows no sign of slowing down.
Keeping pace with that change is a top-ranking priority for today’s C-Suite, and for good reason. Ensuring the investments we make today aren’t obsolete in 15, 10, even 5 years’ time is a huge concern: how do we know that the tools and technologies we’re committing to aren’t going to disappear off the radar?
Will they continue to deliver value? Keep pace with the rapidly evolving technology landscape? Work with other future tools? Satisfy the growing demands of future generations entering the workplace?
Alongside these concerns, we’re seeing an increasingly mobilized, diverse, and dispersed workforce. Greater demands for flexibility, work-life balance, and globalization are challenging the traditional bricks-and-mortar workplace: and enterprises are beginning to answer the call.
It’s predicted that 83% of enterprise workloads will be in the cloud by 2020.
(Forbes)
The result is a growth in cloud-based or SaaS (software-as-a-service) technologies, enterprise mobile apps, and shifts in organizational structures and working practices: such as remote or flexible working, a sharp rise in the gig economy, and a move away from silo-ed departmental ways of working.
Work needs to be user-focused and wherever we are: delivering what we need, when, where, and how we need it. With a tool now available to answer every niche need, perhaps it’s no surprise the average enterprise now boasts over 500 applications as part of its technology stack.
For the organization, this means we now have to consider:
Each organization’s digital workplace is unique: no two toolboxes are the same. However, the trends and challenges threatening it are universal.
With over ten applications designed to meet a diverse range of business productivity needs, the G Suite has a long-standing reputation with particular experience in the education and nonprofit sectors.
The cloud-native approach to development makes it ideal for this new agile model of digital working. In fact, the G Suite — with its low-cost, subscription-based pricing model — is cited as one of the driving forces behind Microsoft’s launch of Office 365, its biggest competitor.
As student users of the G Suite — who enjoyed free access as part of Google’s education suite — enter the workplace, it’s an interface and system that is both familiar and easy to use, one of the motivators for Google’s rising popularity in the enterprise market.
In addition, the G Suite Marketplace — through which users can install third-party cloud applications to use as part of their technology stack — makes the G Suite a frontrunner solution for those looking to create a centered, connected digital workplace.
Under the Google umbrella, already a recognized pioneer in the tech sector, the G Suite comes with a promise to stay at the cutting edge of innovation.
In short, Google’s cloud-first approach, brand stability, reputation for forward-thinking, experience among the younger workforce generation and willingness to collaborate and integrate with other applications makes it a logical and low-risk option for today’s enterprise.
Maximize the value of your G Suite investment. Interact offers seamless out-the-box integration with leading applications from the G Suite, alongside a host of additional features and functionality designed to support your employees.
Download the free Guide
In today’s knowledge-led economy, we deal in information.
Connecting our employees to the information they need to get their jobs done, to serve our customers/clients, and to innovate, problem-solve, develop or simply stay safe and compliant, is critical.
Information and content are, therefore, at the foundation of a successful intranet: providing centralized access to the information your employees need. For those organizations hosting knowledge assets, information, and content within Google Drive, utilizing the G Suite as a foundation for their corporate intranet is the logical solution.
Instant access and the ability to search, surface and collaborate on that information is key. This may also include calendar events from Google Calendar, or perhaps upcoming project tasks through Google Tasks.
For common workflows or business processes — whether that’s the submission of expenses, paid leave request or a staff survey — again, the centralized hosting of your Google Forms is important for efficiency.
These are all forms of information management, communication, and collaboration that should be facilitated through a Google intranet, which ultimately unites the different applications together in a single location.
One of the big benefits of today’s cloud-focused digital workplace is the growing ability to get different tools talking and working intuitively together. Integration has been a game-changer in the enterprise market: and it’s a trend set to continue.
This is where the value and potential of the G Suite can really come into play.
Though a market leader for business productivity solutions, Google’s decision to launch its Marketplace and public API shows an understanding that the core, specialized offering of the G Suite can be built and improved upon to create a better user experience. Other, purpose-built applications may offer a better solution for specific needs.
The purpose of the modern intranet is to support organizations in connecting and informing each and every employee.
To meet that core purpose, an intranet needs to connect employees to one another, to their organization, and to the diverse range of tools, applications, and information they require, all from a centralized location.
So, while the information and collaboration-led tools of the G Suite — such as Google Drive, Documents, Sheets and Forms — make for a solid foundation for an intranet, it’s important to remember that the G Suite isn’t designed as a purpose-built intranet solution.
It will pose limitations in areas such as internal comms and connecting staff to their organizations’ purpose, mission, and values. Connecting individuals from across an organization who may never meet also requires rich profiles and digital social tools, rather than information alone.
G Suite apps are also designed for collaboration but have limited functionality for two-way communication or tools to support employee engagement, such as forums, pulse surveys, or reward and recognition tools. Content and information are stored in Google Drive, but aren’t tailored or personalized to the end-user.
Building a G Suite intranet, then, requires us to see Google’s enterprise apps as just one piece of a much bigger puzzle.
Integration of the G Suite into a purpose-developed intranet solution, such as Interact, can answer those gaps: as well as bringing additional integration opportunities to other core business applications, such as your HR or payroll systems, IT systems, project management tools, and more.
With the G Suite at its foundation for content or information storage and collaboration, email and instant messaging, forms, and calendar events, an intranet then goes above and beyond to answer the demands and challenges posed by the new digital workplace, including:
…and many more.
Global non-profit War Child works with children in war zones across to world to protect, educate and stand up for the rights of young people.
Due to the specialized work its volunteers and workers do — often in very dangerous places — the organization relies heavily on effective communication, alongside easy access to its policies, procedures, safeguarding guidance and more, to keep both staff and children safe. This often comes with additional challenges including language and cultural barriers, or differing compliance or international laws.
As an organization committed to its cause, War Child is also under continuous pressure to operate with limited resources and restricted budgets. For their productivity needs, the answer came in the form of the G Suite, which offers its Basic suite free-of-charge to nonprofits.
However, when looking to establish its G Suite as the hub of its digital workplace, War Child faced a number of challenges as they grew as an organization.
“We had a Google Site, but it was really cluttered, nobody knew which file was the right version, what documents were up-to-date and people had different versions saved in different places,” explains Katherine Tedham, Planning and Internal Communications Officer.
“Our Digital Manager Dave called it ‘the place where forms go to die’. There wasn’t any way of connecting staff between country offices and no way for us to check they were all using the same resources.
“After a program retreat to Uganda in 2014, we realized staff were using different tools and approaches to do the same jobs in different offices. We knew then that we needed a solution to make us more efficient.”
As the organization’s long-established productivity suite and document management system, it was important to War Child that the G Suite remained at the foundation of how they operated day-to-day. However, recognizing they were gaps in the G Suite offering that meant it was failing to meet their needs, the non-profit sought to find a vendor and solution that wouldn’t replace it, but work with it. The answer came in the form of Interact.
Seamless integration with its Google Drive storage and Google Forms is complemented by a broad range of Interact’s social, collaboration and user-focused features to create a successful G Suite intranet that ticks all the boxes for War Child and its users.
This includes a single source of truth policy library, blogging and news to share stories from across the organization, an out-the-box rewards feature for peer recognition, forums for ideation and discussion, and many more.
“Phileas [War Child’s G Suite intranet] is now the place where all staff can go to not only find the resources they need for their day-to-day jobs, but to connect with colleagues in different offices and find out more about the people they work with,” says Katherine.
“If we’re learning good techniques in one country, Phileas empowers those staff to share what they’re doing with other countries and help us improve the quality of our work quicker. As an example, we create child-friendly spaces for children who are very young and not yet verbal. Staff can share ideas between countries about how to make those work for the children.
“It’s been a real game-changer for us.”
Google’s productivity apps are set to play a crucial, even central role in the developing digital workplaces of many enterprises in the future.
Collectively, they answer a broad range of business needs and if the Google legacy can tell us anything, it’s that the G Suite is here to stay.
Building a G Suite intranet requires us to see Google’s enterprise apps as just one piece of a much bigger puzzle.
Organizations already working with Google technologies at the heart of their digital workplaces, and those considering the move in the future, need to be looking for ways to maximize their investment. A G Suite intranet that serves as the gateway to the digital workplace and works with, complements, and enhances the use of these core technologies is the perfect solution.
By selecting vendor partners who work closely with the G Suite — and demonstrate stability, innovation, and agility of their own — organizations can ensure they deliver a digital workplace that delivers the very best experience for its users, now and in the future.
Maximize the value of your G Suite investment. Interact offers seamless out-the-box integration with leading applications from the G Suite, alongside a host of additional features and functionality designed to support your employees.
Download the free Guide
Originally published at www.interact-intranet.com on September 18, 2019.
Interact is a complete intranet solution that unites…
1 
1 clap
1 
Written by
Interact is a complete intranet solution that connects enterprises all over the world. We power internal communications.
Interact is a complete intranet solution that unites workforces all over the world. We power internal communications.
Written by
Interact is a complete intranet solution that connects enterprises all over the world. We power internal communications.
Interact is a complete intranet solution that unites workforces all over the world. We power internal communications.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://blog.searce.com/internals-of-google-cloud-spanner-5927e4b83b36?source=search_post---------177,"Sign in
What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Bhuvanesh
Feb 5, 2020·9 min read
We have learned a lot more internal things about Google Cloud Spanner from past two days. We read some of the portions of the Spanner white paper and the deep internal things from the Google Cloud Next event videos from Youtube. We shared the video links here, but we want to summarize all the learnings in one place. A special thanks to Deepti Srivastava(Product Manager for Spanner) who presented the Spanner Deep Dive sessions in the Google Cloud Next Event.
In 2005, 2006 Google was using the MySQL at massive scale. Google Adwords is one the biggest platform where 90+ MySQL Shards are used to store the data. Due to some maintenance, they re-sharded the MySQL Clusters. This process took 2 years to complete. Google understood that they are growing very fast and these kinds of databases will be a pain in future. That is how Spanner was born.
Once they decided to build something new with distributed, the Big Table team was the one who started working for the Spanner process. Because BigTable uses distributed process, storage and highly available(or maybe some other reasons as well).
Colossus is the distributed file system which is derived from the GFS. A high performance file system is needed for a super database. This project started by BigTable team and the BigTable is powered by Colossus. So Spanner also got the colossus as a filesystem.
The Google Adwords is MySQL based stack and the new system required to have essentials of a relational database like ACID compliance without the limitations of scale. The pain with MySQL is resharding. So they wanted the sharding features like the traditional NoSQL sharding that will take care of resharding and rebalancing. Plus more availability, Horizontal Scale and globally distributed.
Spanner is a global database system, per region we’ll get a minimum of 3 shards. Each shard will be in each zone. In Spanner terms, a shard is called Split. If your provision 1 Node Spanner cluster, you’ll get 2 more Nodes on the different zone which are invisible to you. And the Compute and Storage layers are de-coupled. Paxos algorithm is used to maintain one leader at a time and the rest of the nodes will be the followers.
Based on the partitions, we’ll have more Splits(shards) in the storage layer. Each shard will be replicated to the other Zones. For eg: if you have a shard called S1 on Zone A, it’ll be replicated to Zone B and C. The replication works based on Leader follower method. So the Paxos will help to maintain the quorum and will help to select a new Leader during the failure. If you are writing something on this Split, the Spanner APIs are aware of the Leaders. So the write directly goes to the Zone where it has the Leader Split. Each Split has its own leader zone.
when I was watching the deep dive video of Spanner, they were discussing the strong consistency. Spanner supports strong consistency across all the nodes(Globally). If you write something in US region, you can read that same data from the Asia region or any other region. How they implemented this logic? It’s called TrueTime.
Spanner synchronizes and maintains the same time across all the nodes globally spread across multiple data centers. The hardware is built with Atomic Clocks to maintain the time. If you take a look at the Server Hardware Rack, the Server is having 4 time servers. 2 Servers are connected with GPS and the remaining 2 are connect with Atomic Oscillators. There are 2 different brands of Oscillators for better failover processing. The GPS time servers will sync with Oscillators to synchronize the time across the global data centers with every 30sec interval.
Let’s now try to understand how this TrueTime helps Spanner stay consistent.
To understand the relationship between consistency and TrueTime, we have to understand how a write operation works in Spanner. During every write operation Spanner picks up the current TrueTime value and this TrueTime timestamp will create an order for the write operations. So every commit has been shipped with a timestamp.
For Eg: If you are writing a data on Node 1, it’ll commit the data with the TrueTime timestamp and replicate the data and timestamp to the other nodes. This timestamp is the same on all the nodes. Lets say we committed this data on Node 1, if you are reading the same data from the Node B, then the Spanner API will ask the leader of the Split for last committed data’s timestamp, if the timestamp is matching from the Node A’s timestamp then the data will be returned from Node B, else it’ll wait until the Node A sync the data to Node B and then it’ll return the data.
Here is the lifecycle of a single write operation. We are writing a row that will go to Split 2. Now the Spanner API will understand who is the leader node for Split 2, then the request will go to Zone B node(Blue indication refers to the leader). Then it’ll acquire the lock write it on the split. Once this write has been done, it’ll send the requests to Zone A and C Nodes to write the same. It’ll wait for the acknowledgement from the majority of the nodes. Once the leader split got the majority of the acknowledgement, then it’ll send the success response to the client.
If you are writing the data in a single transaction, but the data resides on different splits, then the spanner will handle it in a different way. For eg: we have to update 2 rows.
When we initiate the transaction, the Spanner API will understand that the rows are in the different split. And they will randomly pick a Co-ordinator zone. In our example, the API has chosen Zone C is the coordinator zone. The following steps will be performed for the multiple row operations.
While reading the data from Spanner, data will be fetched from the nearest follower split. Let’s explain this with an example. Refer the below image.
We want to read the data from MyTable, for the value 123. This value is stored in Split 2. Now once the request reached the Spanner Frontend server, then it’ll understand who is the nearest follower split and forward the request to that split. In our case, Zone A is the nearest split. Once the request reached the split, then that split will ask to the Leader split to get the last committed TrueTime. And then it’ll compare the Timestamp with its own timestamp. If both match then it’ll serve the data to the application. If the timestamps are not matched then the leader split will ask the follower to wait until it sync the data to that Zone. And then the split will serve the data.
Spanner supports MVCC. So it’ll keep the old data for some period of time. If our applications are fine to get the old data (older than X seconds) then we don’t need to wait for data sync from the leader split. For example, We have to tell the Split that we are fine with 15sec old data, then it’ll check the committed timestamp and that is less than 15 seconds, then the old data will be served to the application.
All scenarios explained above apply to clusters within a single region — zone level. But Spanner is built for global scale and multi-region deployments. The architecture and write/read operations will have a slight difference in the multi region setup. In the single region concept, we need a minimum of 3 zones to create the cluster. And the zones support both read and write. But in Multi region concept, One Continent will be act as a Leader and the rest of the Continent will be the followers. In Spanner terms, the Continent where we have more region will be the quorum. All the writes will go to any region in this continent. In the quorum continent, 2 regions will be hosting the data nodes, and 1 region will host the witness for failover. Other continents will have read only replica nodes.
In a multi region cluster, the writes are always performed on the Quorum continent. Let’s say, US region is the R/W continent, then if you are sending a write request from the US region, then the Spanner API will send it the nearest region, once the data has been committed then the success response will go to the client. If you are sending a write request from Asia region, then the Asia region’s API servers will put the request into Google’s internal network and send the request to the US region’s API server. Then that US region API server will commit the data and the success response will be send it to Asia region client.
For Reads, the process is same as single region concept, if the TrueTime matches, then the data will be served from the local region, else it’ll wait until the data sync to the local region and then served to the clients.
We covered most of the internal concepts of Spanner. But still there are a lot more things to learn in Cloud Spanner. Please refer the Google Cloud Next event videos links below.
Less Talk, More Data | https://thedataguy.in
See all (503)
38 
1
38 claps
38 
1
Searce is a niche’ cloud-native technology consulting company, specializing in modernizing infra, app, process & work leveraging Cloud, Data & AI. We empower our clients to accelerate towards the future of their business.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@BeNitinAgarwal/understanding-the-docker-internals-7ccb052ce9fe?source=search_post---------178,"Sign in
There are currently no responses for this story.
Be the first to respond.
Nitin AGARWAL
Jan 5, 2017·3 min read
Docker takes advantage of several features of the Linux kernel to deliver its functionality.
Docker makes use of kernel namespaces to provide the isolated workspace called the container. When you run a container, Docker creates a set of namespaces for that container. These namespaces provide a layer of isolation. Each aspect of a container runs in a separate namespace and its access is limited to that namespace.
Docker Engine uses the following namespaces on Linux:
Docker also makes use of kernel control groups for resource allocation and isolation. A cgroup limits an application to a specific set of resources. Control groups allow Docker Engine to share available hardware resources to containers and optionally enforce limits and constraints.
Docker Engine uses the following cgroups:
Union file systems operate by creating layers, making them very lightweight and fast. Docker Engine uses UnionFS to provide the building blocks for containers. Docker Engine can use multiple UnionFS variants, including AUFS, btrfs, vfs, and devicemapper.
Docker Engine combines the namespaces, control groups and UnionFS into a wrapper called a container format. The default container format is libcontainer.
Docker Engine makes use of AppArmor, Seccomp, Capabilities kernel features for security purposes.
Source: ~ http://docker-saigon.github.io/post/Docker-Internals/
Disclaimer: Content and Image source has been mentioned. Special credit to concerned folks.
Senior Software Engineer — Cloud Native and Distributed Systems
794 
3
794 
794 
3
Senior Software Engineer — Cloud Native and Distributed Systems
"
https://medium.com/martinomburajr/maglev-the-load-balancer-behind-googles-infrastructure-architecture-internals-part-2-3-a426c011f673?source=search_post---------179,"There are currently no responses for this story.
Be the first to respond.
In part 1, we gave an overview of what Maglev was about and why it can withstand significant scaling requirements given Google’s gargantuan size. In this article, we shall get more technical and look at the 3 main areas behind what makes Maglev a success. The technical aspects mentioned in the paper are quite dense, so I will use a…
"
https://medium.com/@rlogan9/cloud-edge-fog-mesh-cd9a9bcd6c56?source=search_post---------180,"Sign in
There are currently no responses for this story.
Be the first to respond.
Ramesh Loganathan
Apr 2, 2018·2 min read
Cloud Edge Fog Mesh.. . Any more? ☺ While all Rae buzz words in cloud computing models.. They are all just meaningful variations of a core. From single server mainframe models we moved to the client-server then on to multi tier and then to cloud and now onto rich client models and now edge and fog computing. The driver in all the shifts is the balance between central single source of truth to computing closer to the users/edge devices. And when data exchange between the users end and the server end gets chatty and affects responsiveness/performance, we move some of the computing to the server. And when user experience is affected because of interactions with server needing bunching of operations then we move some of this to the client side. Like we did with ajax/web 2. 0. Now with proliferation of sensors and devices, we started with all decisions in server. And soon realizing new sensors like cameras are generating tones of data and that is affecting real-time responsiveness in use cases like computer vision AI. So now we are pushing some of the computing closer to the sensors.. To the edges. And IOT use cases coming up where real-time responses are critical and cannot go down with any network issues. This is bringing intermediate servers that are closer to the edge. And thus Fog computing.
All of these are the scope of my Internals OF APP server course this semester at IIIT Hyderabad. A project based course where I pick some topical app platforms technology each year for the class to build. @
<Startup, Creativity, Hyderabad and.. joy> enthusiast. https://www.linkedin.com/in/rameshl
1 
1 
1 
<Startup, Creativity, Hyderabad and.. joy> enthusiast. https://www.linkedin.com/in/rameshl
"
https://stackbar.io/migrate-a-small-project-from-ubuntu-14-10-to-16-04-14427305e5f7?source=search_post---------181,NA
https://medium.com/thrive-global/inversion-invasion-c34906ed6393?source=search_post---------182,"There are currently no responses for this story.
Be the first to respond.
Every winter in the beautiful state of Utah, a natural weather event occurs in the Salt Lake valley. Referred to by the locals as “the inversion,” this chilly layer of fog and pollution covers the valley near ground level, polluting precious fresh air. In addition to creating a brownish-grey environment, it clouds the minds and attitudes of everyone living, working, and visiting in the area.
Those affected by the inversion adopt strategies to cope, most often without conscious awareness. We all do that from time to time. As we practice those strategies, we often alter our perspectives, change the story and leave out details that remind us of a reality that is different, desirable, and happier. We forget that on the other side of the temporary inversion is a bright blue, sun-filled, warmer, open sky. It’s there relief waits patiently to be recognized and embraced.
We all have the power within us to move through and around our own custom temporary obstructions. We have the ability to ask ourselves the questions that will open us to new insight and even remind us of that which we know but may have temporarily forgotten. We can clear our personal inversion invasions. For example, my mother mentioned how good she feels lately due to adjustments in her eating habits. I was surprised to hear her voice this concept, as her lifetime relationship with food has been based on a simple “count the calories” perspective. Until recently, concepts like quality, processed versus not processed, organic, slow food movement, etc., had not been on her radar screen. Her personal inversion invasion in this regard lasted for decades and muddied her waters with numerous persistent health and weight issues.
My own experiences with recognizing and clearing personal inversion invasions have been powerful and enlightening. For example, in recent years, I found myself feeling lonely and socially isolated after eight years of single life. As I acknowledged the situation, I realized the process had been slow and subtle, my feelings spawning one relationship disappointment and several dashed hopes. All of which eventually resulted in measurable lethargy and depression. I had built a wall of sorts around my physical self and heart.
For an inanely long time, I accepted my situation. I was completely immersed in and suffering from feelings of loneliness and isolation lobbed onto me by myself and others. I was trapped in my personal muck and I was miserable.
Then as the good graces of fate would have it, one significant and pivotal day, a friend mentioned he was driving for Uber and absolutely enjoying the interactions and conversations with his riders. For the first time in a long time, something clicked in me. It was as if he casually blew a bit of the fog from my brain. Something so simple was my first glimpse into light. Driving riders from place to place would allow me to interact with new people everyday, get a quick glimpse into their interesting and busy lives, and make a lot of connections! It was as though the sun suddenly burst through a crack in my muddied mental resistance and shined a light on an idea for a way to shift my situation.
Within a few weeks, I was on the road accepting rides and thoroughly enjoying delightful and engaging interactions and conversations with a variety of interesting riders. My personal inversion invasion cleared. I was upbeat. I was once again basking in the sunshine of clarity and positive forward motion.
What precipitated the change in awareness?
For mom, it was persistent, unbearable physical pain that produced a very strong desire to find relief. For me, it was the intense emotional pain and lethargy that became intolerable. It is in those moments that we crack open. The clouds part just enough for the sun to peak through and shine a light on the other side of our agony. We see for the first time the options available to us, embracing the expanding perspective like a long lost friend. As information coupled with powerful desire fills our minds and bodies with fresh energy, a wind ushers in a storm that clears the inversion. We find new strength, wisdom, and power to create improved ways of thinking and living.
We all have tremendous power to create internal and external habits and conditions that nurture and support a better life. American businessman, author, and keynote speaker Stephen Covey wrote, “Power is the faculty or capacity to act, the strength and potency to accomplish something. It is the vital energy to make choices and decisions. It also includes the capacity to overcome deeply embedded habits and to cultivate higher, more effective ones.”
Wisdom dictates that we did not come here to simply survive. We came to thrive. Let’s harness our power and do it!
Opinions expressed by Community contributors do not reflect opinions of Thrive Global.
Opinions expressed by Community contributors do not reflect the opinions of Thrive Global or its employees.
Written by
Lover of nature, the arts, and New York City. Change agent. Traveling light with a little dog. Free Spirit.
Opinions expressed by Community contributors do not reflect the opinions of Thrive Global or its employees.
"
https://anecdotes.dev/from-cloud-run-to-kubernetes-how-learning-the-internals-of-your-stack-empowers-you-as-a-developer-3d0cdf7cb7ec?source=search_post---------183,"It’s common for those of us in the tech space to be drawn in by cool buzzwords and shiny new logos. In this post you’ll learn why internals matter more than those buzzwords.
You’ll also pick up a few important “gotchya’s” that can save you tons of time if you ever need to migrate your serverless apps from Google Cloud Run to Kubernetes.
I’ll be using a specific problem in this example — but I want you to really soak in the process that we used to solve the issue. You can learn a lot from the “divide and conquer” methodology that we used in debugging; it can be applied to countless other problems.
One more thing before we dive in, this process emphasizes why understanding the internals of the technology stack you choose is important and how you sometimes need to unfold the abstraction layers and dig in to solve a problem.
For some background, at anecdotes, we used Google’s Cloud Run as our serverless platform in our first year. However, our infrastructure rapidly grew to a point where this service was no longer feasible, so we started planning a move to Kubernetes. To do so, we first needed to come up with a proof of concept (PoC).
As part of the PoC, we wanted to spin up a fully working anecdotes environment on Kubernetes. Sounds legit.
Theoretically, this should have been easy since Cloud Run, like Kubernetes, works with containers. Therefore, making our containers run over Kubernetes should have been the same, right?
No, it wasn’t. -_-
When we spun up the environment for the first time, the pods kept restarting. We couldn’t understand why.
It became even more frustrating when we saw a Segmentation fault (core dumped) message in the pod logs, stating that the service process was killed by OS. This was an absolute shocker. Aren’t Segmentation faultssomething that only low-level engineers know?
But we needed to solve it, so we started to dig in.
When in doubt — check Stack Overflow (SO).
To summarize this really great SO explanation — here are some reasons why a segmentation fault may occur in Python:
After eliminating the easiest suspect (lack of RAM/CPU), we understood that we needed to go deeper to understand what was happening.
Before zooming in, I’ll quickly go over our service structure at anecdotes. From there, we will have a better understanding of how we could tackle this problem.
Let’s review the typical anecdotes service structure:
We have 4 layers in the stack — let’s look at how each can be potentially related to the segmentation fault problem:
Going back to the pod, we saw that the segmentation fault was printed from the Gunicorn process. This didn’t help much, because it didn’t eliminate any of the layers above. Gunicorn can collapse whether a broken library is installed on the image or if it was installed by the app itself.
So we knew that Gunicorn crashes with Segmantation fault, what was next?
To get a working debugging environment, we had two problems we needed to solve.
As the base image for anecdotes services is python-slim , we didn’t have any available tool to start our investigation (the python-slim image even doesn’t include ps ). In addition, we hardened our image so it wouldn’t work with the root user, so we couldn’t even install anything.
Therefore, we needed a debugging image. An image that would contain more tools and could run with root.
Moving to another base image would add the risk that the crash wouldn’t be reproducible. However, at this point, we didn’t have any other option. So, for the purpose of investigating this segmentation fault, we built a new Ubuntu-based container with the same configuration.
2. Problem #2 — Entrypoint
The second problem was that we needed a stable pod to debug. Remember that Kubernetes kept killing the pods because the Gunicorn crashed and the live checks were failing. To overcome this, we removed the livenessProbes and changed the entrypoint to a long sleep (sleep 36000). From there, it was easy to execute bash into the container and start exploring.
So now we had a debugging environment, yay!
But how the heck do you start investigating a Segmentation fault?
Like every engineer, we asked Google for an answer and got this SO thread introducing Python’s faulthandler module. For the first time, we felt we could finally get into the root cause. Excitedly, we ran the gunicorn process with the faulthandler turned on:
And… voila! We now a stacktrace of the segmentation fault!
But wait, what were we even seeing here?
We could see the stacktrace of 5 different threads (3 x elasticapm agent threads,snowflake connector thread, and the maingunicorn thread), all of them potentially having a part in the crash.
Before jumping to conclusions, we reproduced this step over and over to verify that this was reproducible. It was. So was this the problem?
Looking closer at the stacktrace, we could see that all threads were waiting/sleeping. Weird. We would expect the suspicious thread to do something while the Gunicorn process crashed, not while it was sleeping.
Okay, but we needed to work with what we had, right?
To eliminate the first suspect, the Elastic APM agent, I disabled it and restarted the process. gunicorn kept dying. Same goes for snowflake.
Dead end again.
So now it was time to take the gloves off and start playing dirty. I knew that every time a process crashes with a segmentation fault, a core dump file is created, so that was my new direction.
So what do we see here? We can see that a core dump file was generated. But notice that interesting file name pattern. Maybe it could tell us something.
Apparently, the pattern of the core dump files is kept under the following file:
If we look at man core , we see that:
So we now had a new interesting lead — the process that crashed (or at least what the OS told us) was grpc_local_tim .
Surprisingly, Googling it didn't tell us a lot —
So it was now time to open the core file. Reading another great SO thread, I usedgdb , and specifically, thebtcommand to print the stack trace. We got the following output:
Voila again! What a nice error!
Remember that earlier we said that the most common reason for Segmentation fault in Python is “An extension, written in C, crashes the app”? So here it is! We can see that CyGRPC is part of grpcio, which is The C based GRPC — practically the grpcio extension for Python. We had finally found what we were looking for.
After finding the library that had been crashing our app, it was pretty easy to get a solution. Googling “cygrpc segmentation fault” points to an open bug in grpcio — https://github.com/grpc/grpc/issues/23796.
From there, it was a matter of reading the whole thread to get a workaround
The bug, that lies in the grpcio library, is that they move from one polling strategy (epoll1) to a new one (epollex) before the new one supports fork . If you want to read more, you can read the mentioned thread and GRPC documentation about polling https://grpc.github.io/grpc/core/md_doc_core_grpc-polling-engines.html
Amazingly, adding GRPC_POLL_STRATEGY=epoll1 actually solved the problem and Segmentation fault disappeared. Thank you Jrmlhermitte.
In this blog post, I tried to emphasize the importance of understanding the internals of your stack. Nowadays, it's easy to be attracted to cool buzzwords and logos—Kubernetes, Containers, Serverless, and thousands more. But in the end, remember that they’re all abstraction layers for the same thing — running code on some Linux look-alike environment.
Keeping this in mind gives you, as an engineer, enormous power and the ability to build whatever you want on what tech stack you want.
Developing the first Compliance OS is complex, writing about it shouldn’t be
153 
153 claps
153 
Written by
Chief architect at anecdotes.ai. Passionate about building stuff. Less is more. Long-distance runner.
anecdotes engineering builds the anecdotes compliance platform which automates the InfoSec compliance process, lifting the burden off your organization
Written by
Chief architect at anecdotes.ai. Passionate about building stuff. Less is more. Long-distance runner.
anecdotes engineering builds the anecdotes compliance platform which automates the InfoSec compliance process, lifting the burden off your organization
"
https://medium.com/@frankwessels_nl/golang-internals-part-2-nice-benefits-of-named-return-values-1e95305c8687?source=search_post---------184,"Sign in
There are currently no responses for this story.
Be the first to respond.
Frank Wessels
Jun 29, 2017·5 min read
You may know that Golang offers the ability to name return values. Thus far at minio we have not been using this feature much, but that’ll change since there are some nice hidden benefits as we will explain in this blog post.
If you are like us, you may have considerable amounts of code as shown down below whereby for every return statement you are instantiating a new object in order to return a ‘default’ value:
If you look at the actual code that the Golang compiler generates, you’ll end up with something like this:
All fine and dandy, but if that looks a bit repetitive to you, you are quite right. Essentially for each of the return statements the object to be returned is more or less allocated/initialized (or more precisely copied via the DUFFCOPY macro).
After all that is what we asked for by returning via return objectInfo{} in every case.
Now look at what happens if we make a very simple change, essentially just giving the return value a name (oi) and using the ‘naked’ return feature of Golang (dropping the argument for the return statement, although this is not strictly required, more on that later):
Again looking at the code generated by the compiler, we get the following:
That is a pretty massive difference with all four occurrences of the object initialization and DUFFCOPY stuff gone (even for this trivial case). It reduces the size of the function down from 243 to 67 bytes. Also as an additional benefit you will save some CPU cycles upon exiting out because there is no need anymore to do anything in order to setup the return value.
Note that if you don’t like or prefer the naked return that Golang offers, you can use return oi while still getting the same benefit, like so:
Examining a bit further in minio server we took the following case:
Looking at the assembly we get the following function header (we’ll spare you the full listing…):
If we modify the code to use a named return parameter (second source code block below), check out what happens to the size of the function:
It is shaving off some 300 bytes out of a total of 1150 bytes which is not bad for a such a minimal change to the source code. And depending where you are coming from, you may prefer the ‘cleaner’ look of the source code too:
Note that actually the ch variable is a normal local variable just like any other local variable that is defined within the function. And as such you can change its value from the default ‘zero’ state (but of course then the modified version will be returned upon exiting out).
As pointed out by several persons, another benefit of named return values is the use in closures (i.e. defer statements). Thus one may access the named return value in a function that is called as the result of a defer statement and act accordingly.
In case you missed the first part of this series, here is a link to it:
So we will be gradually adopting named return values more and more, both for new code as well as for existing code.
In fact we are also investigating if we can develop a little utility to help or automate this process. Think along the lines of gofmt but then modifying the source automatically to make the changes outlined above. Especially in the case where the return value is not yet named (and so the utility would have to give it a name), it necessarily cannot be the case that this return variable is changed in any way in the existing source, and thus using return ch (in case of the listing above) will not result in any functional changes of the program whatsoever. So stay tuned for that.
We hope that this post was useful to you and provides some new insights into how Go operates internally and on how to improve your Golang code.
An issue has been filed for Golang to optimize the compiler to generate identical code for the cases described above which would be a good thing.
164 
8
164 
164 
8
"
https://medium.com/swlh/lambda-internals-exploration-ae6c21d9521e?source=search_post---------185,"There are currently no responses for this story.
Be the first to respond.
Cloud provides an abstraction to a lot of complex stuff. And it is good, but I like the details, the internals. With an aim to see how lambda service works I watched an awesome video from AWS on the Lambda service. This post tries to summarize things.
"
https://medium.com/@frankwessels_nl/golang-internals-part-1-autogenerated-functions-and-how-to-get-rid-of-them-6ca4749cc236?source=search_post---------186,"Sign in
There are currently no responses for this story.
Be the first to respond.
Frank Wessels
Jun 14, 2017·5 min read
Maybe if you are like us at Minio, you have every now and then come across ‘autogenerated’ functions in your Golang call stacks and wondered what they are all about?
We had a case the other day where the call stack showed something like the following:
As you can see the second function looks similar to the first function (value receiver, defined in our minio code at minio/cmd/retry-storage.go:183), however it takes a pointer receiver instead. Also suspiciously there is just a line number shown for the second function but the file name for the source code is absent.
The code that created this call stack originates from the following function where a (recursive) function is declared that in turn calls storage.ListDir whereby storage is passed into cleanupDir.
Looking at the code above, one would assume that storage.ListDir (value receiver) is called immediately without any need for a pointer receiver version (cmd.(*retryStorage).ListDir) to be called in between. So what is going on here?
Let’s first examine what this pointer receiver function is all about. By running go tool objdump -s ListDir minio we can get the definition of the function and see that it is apparently <autogenerated>:
So we can see that, about half way in at offset 0x1f2eaa, it calls retryStorage.ListDir (value receiver) which we have defined in our Golang code at minio/cmd/retry-storage.go:183. Before this it sets up all arguments on the stack which includes dereferencing *retryStorage and creating a copy of it on the stack (as retryStorage.ListDir needs).
After the CALL the return arguments are loaded (offsets 0x1f2eaf through to 0x1f2ec3) and copied into the appropriate slots (offsets 0x1f2ec8 through to 0x1f2ee8) for the return arguments of (*retryStorage).ListDir after which the autogenerated function returns.
So all that (*retryStorage).ListDir essentially does is wrapping the call to retryStorage.ListDir and making sure that the object pointed to by the pointer receiver is not modified.
Now that we know what (*retryStorage).ListDir does, the next question is why does Golang do this? This has to do with the fact that StorageAPI is an interface type, and therefore the storage argument in cleanupDir is an interface value which effectively is a two-pointer structure: Interface values are represented as a two-word pair giving a pointer to information about the type stored in the interface and a pointer to the associated data.
So when we make the call to storage.ListDir we are caught in the situation that we have access to a pointer to retryStorage but only a value receiver method for ListDir. This is then where the Golang compiler is nice enough to (auto)generate it for us, although it does come at a considerable execution cost since we need to dereference our object and copy the return arguments etc. Thanks to @thatcks for providing a clarification on this point.
Well, first of all you don’t necessarily need to ‘fix’ anything because there is nothing inherently wrong and your code will run just fine.
However if you do want to fix it, it is deceptively simple: simply change func (f retryStorage) ListDir(...) to func (f *retryStorage) ListDir(...) at minio/cmd/retry-storage.go:183 and you are all set as per:
Obviously you will want to make sure that your implementation of the function does not by accident change the value of f because you will then start noticing this (which would most likely be a little weird anyway because presumably the intent of changing the contents of f would be of the caller to take notice of this which does not happen with value receiver functions).
And, as a free bonus, you will shave a couple of hundreds of bytes of the size of your executable (for just this single function, not a bad result for just adding a single * in your source code file…).
Hopefully this blog post will have given you some insights into how Golang functions internally and clarifies what these autogenerated functions are about and what you may do about them.
In a later post in this series we will also elaborate more on the performance of pointer receiver versus value receiver functions.
22 
1
22 
22 
1
"
https://medium.com/@thelaith/learn-kubernetes-like-you-would-teach-it-4-10-8a83898c0b7?source=search_post---------187,"Sign in
There are currently no responses for this story.
Be the first to respond.
Laith
Jul 3, 2018·6 min read
As mentioned in previous articles , Kubernetes is the basic scheduling platform for cloud native applications. It is equivalent to the cloud’s native operating system. In order to facilitate system expansion, the following interfaces open in Kubernetes can be connected to different backends to implement their own business logic:
container runtime interface, providing computing resources .
Each container runtime has it own strengths, and many users have asked for Kubernetes to support more runtimes.
Kubelet communicates with the container runtime (or a CRI shim for the runtime) over Unix sockets using the gRPC framework, where kubelet acts as a client and the CRI shim as the server.
The protocol buffers API includes two gRPC services, ImageService, and RuntimeService. The ImageService provides RPCs to pull an image from a repository, inspect, and remove an image. The RuntimeService contains RPCs to manage the lifecycle of the pods and containers, as well as calls to interact with containers (exec/attach/port-forward). A monolithic container runtime that manages both images and containers (e.g., Docker and rkt) can provide both services simultaneously with a single socket. The sockets can be set in Kubelet by –container-runtime-endpoint and –image-service-endpoint flags.
Although CRI is still in its early stages, there are already several projects under development to integrate container runtimes using CRI. Below are a few examples:
container network interface, providing network resources to container runtime engines
The CNI project is a Cloud Native Computing Foundation project, consists of a specification and libraries for writing plugins to configure network interfaces in Linux containers (lately , windows too), along with a number of supported plugins.
CNI concerns itself only with network connectivity of containers and removing allocated resources when the container is deleted. Because of this focus, CNI has a wide range of support and the specification is simple to implement.
It has been created by multiple companies and projects; including CoreOS, Red Hat OpenShift, Apache Mesos, Cloud Foundry, Kubernetes, Kurma and rkt.
First proposed by CoreOS to define a common interface between the network plugins and container execution, CNI is designed to be a minimal specification concerned only with the network connectivity of containers and removing allocated resources when the container is deleted.
container storage interface, providing storage resources, One of the key differentiators for Kubernetes has been a powerful volume plugin system that enables many different types of storage systems to:
Kubernetes volume plugins are currently “in-tree”, meaning they’re linked, compiled, built, and shipped with the core Kubernetes binaries.
Adding support for a new storage system to Kubernetes (a volume plugin) requires checking code into the core Kubernetes repository, But aligning with the Kubernetes release process is painful for many plugin developers.
The existing Flex Volume plugin attempted to address this pain by exposing an exec based API for external volume plugins. Although it enables third party storage vendors to write drivers out-of-tree, in order to deploy the third party driver files it requires access to the root filesystem of node and master machines.
CSI addresses all of these issues by enabling storage plugins to be developed out-of-tree, containerized, deployed via standard Kubernetes primitives, and consumed through the Kubernetes storage primitives users know and love (PersistentVolumeClaims, PersistentVolumes, StorageClasses).
The above three resources are equivalent to the most basic resource types of a distributed operating system, and Kubernetes is the bond that binds them together.
Following are the key components of Mastes\s servers which are necessary to communicate with Kubernetes node\s .
It stores the configuration information which can be used by each of the nodes in the cluster. It is a high availability key value store that can be distributed among multiple nodes. It is accessible only by Kubernetes API server as it may have some sensitive information. It is a distributed key value Store which is accessible to all.
Kubernetes is an API server which provides all the operation on cluster using the API. API server implements an interface, which means different tools and libraries can readily communicate with it. Kubeconfig is a package along with the server side tools that can be used for communication. It exposes Kubernetes API.
This component is responsible for most of the collectors that regulates the state of cluster and performs a task. In general, it can be considered as a daemon which runs in a loop and is responsible for collecting and sending information to API server.
It works toward getting the shared state of a cluster and then make changes to bring the current status of the server to the desired state. The key controllers are replication controller, endpoint controller, namespace controller, and service account controller.
The controller manager runs different kind of controllers to handle nodes, endpoints, etc.
This is one of the key components of Kubernetes master. It is a service in master responsible for distributing the workload. It is responsible for tracking utilization of working load on cluster nodes and then placing the workload on which resources are available and accept the workload. In other words, this is the mechanism responsible for allocating pods to available nodes. The scheduler is responsible for workload utilization and allocating pod to new node.
Following are the key components of Node\s servers which are necessary to communicate with Kubernetes master.
The first requirement of each node is Docker which helps in running the encapsulated application containers in a relatively isolated but lightweight operating environment.
This is a small service in each node responsible for relaying information to and from control plane service. It interacts with etcd store to read configuration details and write values.
This communicates with the master component to receive commands and work. The kubelet process then assumes responsibility for maintaining the state of work and the node server. It manages network rules, port forwarding, etc.
This is a proxy service which runs on each node and helps in making services available to the external host. It helps in forwarding the request to correct containers and is capable of performing primitive load balancing.
It makes sure that the networking environment is predictable and accessible and at the same time it is isolated as well. It manages pods on node, volumes, secrets, creating new containers’ health checkup, etc.
2 
2 claps
2 
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@gabrieledecapoa/git-internals-f84d71f9281d?source=search_post---------188,"Sign in
There are currently no responses for this story.
Be the first to respond.
Gabriele de Capoa
May 5, 2021·2 min read
Git, after its first release in 2005 thanks to Linus Torvalds, became rapidly the most source code management (SCM) tool used in the world, especially for open source projects.This was due to its design. Torvalds, in fact, develop this system to be very stupid, fast and simple, because it has to support Linux kernel development. So, other features added were the strong support for non-linear development and for large projects.
Git, as we know, is a fully distributed SCM, and each developer that works on that project has in his/her machine a full copy of the repository into the hidden directory .git\.
But how data are stored internally?
Git thinks of its data like a set of snapshots of a miniature filesystems. If a file has not changed, Git does not store it again but create a link to the previous version. Everything is check-summed before it is stored and it is referred to by that checksum.Each snapshot (commit) refers to one or more other snapshot, creating a directed acyclic graph (DAG).
This DAG is immutable and append-only, and could be view like an object store, in which we have:
Originally published at https://gabriele-decapoa.github.io.
Cloud software engineer, wanna-be data scientist, former Scrum Master. Agile, DevOps, Kubernetes and SQL are my top topics.
See all (5)
Some rights reserved

Cloud software engineer, wanna-be data scientist, former Scrum Master. Agile, DevOps, Kubernetes and SQL are my top topics.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/operators/operator-pattern-kubernetes-openshift-380ddc6a147c?source=search_post---------189,"There are currently no responses for this story.
Be the first to respond.
In this post, we will understand the Operator Pattern, Kubernetes internals, and essentials concepts for building an operator.
In the real world, we can define Operators: as a person who operates on something (machine, equipment, process). Let’s say it works on a given task “X”. Kubernetes Operator also works to follow the same principles.
We can classify any operator qualities into three categories:
They observe the X work, analyze the state of X, and finally act based on the current state of X to reach the desired state. Which will make the operators task completed, once we attain the desired state.
Similar work/tasks are performed by Kubernetes Operators. It helps us to extend the Kubernetes API to allow custom functionality or behavior based on the needs of the consumer.
We can categories building cloud-native applications in two categories:
In the above Figure 1, the Operator acts like having an understanding of Kubernetes internals like Events and API. It allows us to execute custom logic and query Kubernetes API for modifying the state of resources. Before going deep into Operators we can build our understanding of the Kubernetes internals:
It implements multiple controllers in a single process that runs in the control plane as a replicated set of processes. It consists of controllers like Node Controller, Route Controller, Service Controller, Authorization Controller, etc. We will see in detail how the controller works internally in the Operator pattern.
As shown in Figure 3, we can see each resource defined within the Kubernetes Control Plane is defined as an API end-point. When you are using the kubectl CLI tool. You are interacting with the Kube API like
kubectl <verb> <resource>
For extending the Schema, we can define a Custom Resource Definition type and create instances (CR) to perform the desired task using a custom controller.
In the above figure, we are creating an instance(“example-job”) of type: Job. This is managed by Job controller runtime which handles the creation of container which executes the defined arguments. We got an understanding of the core internals. Now we can explore the pattern
Operator = Customer Resource Definition + Custom Controller
In the case of Kubernetes Operator, the same qualities are followed
We can understand each in-depth:
Classification based on deployment type
For understanding read: namespace vs cluster scope operator
We have a customer resource for Kind: Memcached (Custom Resource). With a specification field size (defined as 3). In the case of any modification to the customer resource. For eg: size 3 to 4.
An event will be triggered which can be watched by configuring a Watcher(Observe) for the resource object: Memcached within the Custom Controller. The reconciliation loop consists of custom logic defined by us. which can be used to compares the current (size: 3) vs desired (size: 4) & make the desired action.
Example Operator Workflow
Example: https://github.com/operator-framework/getting-started
In conclusion, the above workflow is applicable to any operator. Except for the last point where creating a pod is a custom logic defined in the reconciler loop. Operator-Framework can be used to generate an operator and which provides high-level APIs, useful abstractions, and project scaffolding. Currently only support: Helm, Ansible & Go lang based operators. You can design operators in any preferred language using the Kubernetes client libraries with a combination of the helm charts. The following links will help to get started with operators.
github.com
github.com
medium.com
Thank you for reading this article. If you like this post, please give a Cheer!!! Follow the Collection: operators for learning more…
Happy Coding ❤
Kubernetes/Openshift Operators
54 
54 claps
54 
Focus on Kubernetes/Openshift:Operators & Operator framework
Written by
#redhatter #opensource #developer #kubernetes #keycloak #golang #openshift #quarkus #spring https://mentorcruise.com/mentor/abhishekkoserwal/
Focus on Kubernetes/Openshift:Operators & Operator framework
"
https://medium.com/@julierbutler/the-google-memo-james-damores-viewpoint-diversity-unpacked-6b5e2c7783b8?source=search_post---------190,"Sign in
There are currently no responses for this story.
Be the first to respond.
Julie R Butler
Sep 25, 2017·31 min read
The reaction was swift. James Damore’s internal memo, Google’s Ideological Echo Chamber: How bias clouds our thinking about diversity and inclusion, went viral; he was fired; and the responses piled in.
Those appalled by the memo and in favor of Damore’s firing see the document as a sexist attempt to explain the disparity in the number of males and females in tech and leadership using inaccurate stereotypes and warped science (see So, about this Googler’s manifesto., What do scientists think about the biological claims made in the document about diversity written by a Google employee in August 2017?, and Female employee on the Google memo: ‘I don’t know how we could feel anything but attacked by that’),
while those favorable to Damore and his memo claim that the ideas expressed are not sexist, the scientific evidence supports them, and the author is the victim of an unjust overreaction by the authoritarian PC police (see The Most Common Error in Media Coverage of the Google Memo, Sundar Pichai Should Resign as Google’s C.E.O., and The Google Memo: Race and Gender Gaps and their Solutions).
As a progressive who is very alarmed at the way Trump’s election has emboldened extremists on the right and furthered the narrative of the “dangerously totalitarian left,” I’m in the first group, and everything about this memo makes me cringe.
Because it’s obvious that a person’s biases have a lot to do with whether they reject or support the memo, it is essential to understand what’s going on with the science that Damore is so certain backs up his assertions, which involves considering the context of the references cited and taking into account the larger sociopolitical atmosphere that surrounds the discussion of these issues of sexual differences, equality, and diversity.
My intent is therefore to take a look at the language Damore uses and the material he links to in the document to shed some light on the ideas underpinning the claims he makes.
(For an excellent survey of the psychological research about sex and gender, see Does biology explain why men outnumber women in tech? — Short answer: There’s no clear-cut evidence supporting this.)
The original document opens with a brief summary of the arguments, followed by a short paragraph introducing the central idea that we all have unconscious biases. This paragraph includes a footnote acknowledging the author’s own political bias as a “classical liberal,” i.e., a libertarian, according to Wikipedia and the linked-to article The Largest Study Ever of Libertarian Psychology. This article is by Jonathan Haidt, the social psychologist who popularized moral foundations theory in his book The Righteous Mind: Why Good People Are Divided by Politics and Religion (2012). And as we shall see, Damore’s ideas about political biases are closely aligned with those of Haidt and his colleagues, who have been raising alarms in recent years about what they see as the dangers of a liberal bias in academia and the social sciences.
Damore’s argument begins with a statement about the need to discuss the moral biases at the root of our political orientations, drawing inspiration from Haidt’s work.
“Considering that the overwhelming majority of the social sciences, media, and Google lean left, we should critically examine these prejudices,” Damore writes in introducing a table of left/right biases that appears without any sourcing whatsoever. Evolutionary biologist Suzanne Sadedin characterizes the table as “a dangerous misrepresentation” of Haidt’s moral foundations theory.
What the statement does reference is a study titled A Measure of Media Bias *from fourteen years ago* (I’ve linked to the study originally published back in 2003) to immediately paint himself as a victim of leftwing biases coming from all sides. Of course, there have been many more examinations of media bias in the intervening years, including this takedown of that study on liberal bias in the media.
So what about the social sciences? Damore’s mention of this here is meant to cast doubt on the legitimacy of Google’s diversity programs, which are part of Google’s “politically correct monoculture” that’s devoid of checks against “encroaching extremist and authoritarian policies” and maintained by “shaming dissenters into silence,” since they were developed within a field that, like Google, skews heavily to the left.
There’s no denying that liberal bias in the social sciences is real. But the causes, solutions, and degree to which it is a problem are a matter of fierce debate.
As with the media, evidence of a majority of social scientists having a left-leaning political viewpoint is not evidence of a leftwing conspiracy. For starters, in both these fields, the importance of managing bias (or minimizing bias) and taking measures to avoid research bias in order to strengthen the integrity of one’s work are strongly emphasized. Furthermore, the debate about the significance of leftwing bias in the social sciences itself shows that it’s a known issue that social scientists are working to deal with in order to avoid promoting agendas that only take them farther from the truth.
Ironically, after concluding that this lack of conservative viewpoints is harmful to the field, Haidt’s solution is . . . wait for it . . . affirmative action for conservatives! Others believe the solution is to become ever more diligent about reducing ideological bias in the social sciences rather than simply adding more bias in the name of balance.
Now, I find Damore’s final statement in this section to be quite telling:
“For the rest of this document, I’ll concentrate on the extreme stance that all differences in outcome are due to differential treatment and the authoritarian element that’s required to actually discriminate to create equal representation.”
He’s admitting that the stance that differential treatment is responsible for *all* differences in outcome is extreme. And indeed, this is not at all what most advocates of diversity programs believe. But it fits into the narrative that leftists harbor totalitarian intentions to enslave free people with their authoritarian government regulations and oppressive social engineering pogroms. This overwrought concern about “encroaching extremism” is the same slimy slippery-slope argument that white supremacists, including, of course, President Trump, have used to warn that removal of Confederate statues in public spaces will lead to removal of statues of our Founding Fathers.
In the introduction for this section, Damore sets up his argument in contrast to Google’s determination that women are being held back by implicit and explicit biases by insisting that this isn’t the whole story. His purpose is to show that there’s another factor involved in holding women back: biological differences. These differences, he proclaims, “aren’t just socially constructed” because, well, universality, heritability, evolutionary psychology, prenatal testosterone, and the fact that “biological males that were castrated at birth and raised as females often still identify and act like males.”
OK then!
He next offers a statement clarifying that the differences between men and women that he will discuss are due *in part* to biological factors, and he includes the further caveats that “many of these differences are small,” “there’s significant overlap between men and women,” and “reducing people to their group identity and assuming the average is representative . . . is bad” (and he doesn’t endorse that). What he does endorse, by virtue of this memo, is cherry-picking bits of information that seem to cast doubt on what he refers to as “social constructionism” and “arbitrary social engineering” — ideas that are associated with feminism and communism.
In his list of personality differences, Damore links to Wikipedia entries on sex differences in psychology / personality traits, neuroticism, and empathizing-systemizing theory, along with a 2010 study in support of the concept that women are generally more people-oriented while men tend to be more thing-oriented that I will get to in a moment.
Now, anyone who has done any serious fact-checking knows that citing Wikipedia articles comes with its own caveats, the most important being that you need to check out the references to determine the accuracy, credibility, and larger context of the information found there. Rather than trying to analyze these three Wikipedia articles, I would just like to point out one example of cherry-picking to emphasize a certain point of view. Consider the most-cited reference in that entry on neuroticism, which (at the time of publication) includes the text, “Research in large samples has shown that levels of N [neuroticism] are higher in women than men. This is a robust finding that is consistent across cultures.” This appears to be very compelling evidence, indeed. Yet, if you click either of the external links there, you’ll see that the 2013 study of the biological and psychological basis of neuroticism actually concludes that “strong claims on N’s biological basis, however, are not yet justified due to inconsistencies and lack of replication which are in part due to methodological limitations and N’s heterogeneity.” Perhaps more recent studies have strengthened the claims of a biological basis for women being more neurotic than men; perhaps not.
Regarding that 2010 study, Gender Differences in Personality and Interests: When, Where, and Why? by Richard A. Lippa. As the title suggests, it addresses questions about psychological gender differences to get to the “ultimate” question, “Why do men and women (sometimes) differ in personality and interests?” and it opens with an informative introduction to the two camps of gender difference studies that are central to how we interpret and deal with the gender gap in tech and leadership. Lippa explains that biologic theories focus on how biologic evolution produces gender differences (see Damore’s list of reasons why certain differences “aren’t just socially constructed”), while social-environmental approaches focus on cultural and social influences (such as social roles and gender self-socialization). If the biological theories are correct, the argument goes, then we must accept that men and women are innately different in certain respects and stop trying to force women into employment environments that they are simply less suited for. Or, as Demore is insisting, at least stop blaming sexism for women’s inability to get ahead in the workplace.
Lippa goes on to discuss the study’s methodologies and evidence. According to the abstract, it concludes that while gender differences in personality traits are “small” to “moderate,” differences in interest with respect to people versus things are “very large,” and “gender differences in interests appear to be consistent across cultures and over time, a finding that suggests possible biologic influences.”
However, in the full text of the study, Lippa qualifies this by noting the need for more explicit models and predicts that these “may very well yield ‘messy’ results — e.g., results that show both biologic and social-environmental factors.”
Those claiming that diversity programs are misguided like to hold this study up as evidence for their case because it specifically identifies the “very large” gender difference in the people-thing dimension right in the abstract, which makes it seem fairly certain that women are less inclined to be good at coding because of biology — even though the author of the study actually concludes that biological and social factors will be increasingly recognized as being intricately intertwined, not more clearly defined.
Following his list of possible biological personality traits, Damore offers this:
Note that contrary to what a social constructionist would argue, research suggests that “greater nation-level gender equality leads to psychological dissimilarity in men’s and women’s personality traits.” Because as “society becomes more prosperous and more egalitarian, innate dispositional differences between men and women have more space to develop and the gap that exists between men and women in their personality traits becomes wider.” We need to stop assuming that gender gaps imply sexism.
The unsourced quotes here are from the 2008 article Why Can’t a Man Be More Like a Woman? Sex Differences in Big Five Personality Traits Across 55 Cultures (Don’t you just love a humorous My Fair Lady reference in scientific literature?). This is also the source of much of the information about personality traits that appears in that Wikipedia entry on “sex differences in psychology.”
It’s important to note that this study divides the theories explaining gender differences into three groups: social roles, evolutionary psychology, and measurement artifacts. Unlike the Lippa study, this framing characterizes environmental factors as evolutionary factors, making changes in gender traits and interests a matter of biology — by virtue of natural selection — rather than being due to socialization. But then, like Lippa, the authors concede that “ultimately, these competing approaches . . . are not mutually exclusive, and each may explain part of the observed variability in personality sex differences across cultures.”
In using those particular quotes in his chiding about sexism, Damore conveniently ignores the study’s other conclusion that “it is mainly men, not women, who became less neurotic but also less agreeable and conscientious in their self-descriptions. In less fortunate conditions, the innate personality differences between men and women are attenuated.” Couldn’t that imply that as women gain rights and power in society, men become more belligerently defensive of their higher status?
Case in point: The article Damore links to in the pronouncement, “We need to stop assuming that gender gaps imply sexism.” Why It’s Time To Stop Worrying About First World ‘Gender Gaps’, by Aaron Neil, commits the same omission that Damore does, drawing on the same quotes from the same study to decry “faulty conclusions from gender gaps” and even going so far as to make the preposterous claim that “robust differences in vocational choices are a sign of prosperous and egalitarian societies.” Um, no. The study says nothing of the sort, while there is plenty of research showing that women are universally trapped in low-paying jobs and low-productivity businesses by a plethora of forces beyond their control, rather than by choice. Neil’s attempt to use the World Economic Forum’s 2015 Global Gender Gap Report to support his nonsense is even more laughable.
So, to recap, when Damore insists that the implications of the research go against the “constructionist” premise that differences in the sexes are driven more by social forces than by biology, he’s putting himself in the evolutionary psychology camp, which theorizes that personality differences between men and women are vulnerable to environmental pressures such as availability of resources, yet not to the social pressure of sustained systemic sexism. And by failing to acknowledge that the widening gaps occur due to changes in the personalities of the very demographic that is seeing its hold on power and influence wane as society becomes more egalitarian — while the personalities of women remain stable — he leaves the impression that diversity and inclusion efforts only heighten the female traits that are supposedly holding them back.
Which looks a lot like blaming the victim.
Moving on, the subsection on “men’s higher drive for status” introduces the premise that leadership positions require long, stressful hours and women simply prefer to leave such high-stress, low-satisfaction jobs to men.
According to Damore, it’s all about men’s testosterone-driven desire for status, “the primary metric that men are judged on.” A footnote clarifies that this status metric occurs specifically in the context of how women judge men for romantic relationships while emphasizing that this drive for status has biological origins, pointing to the article Testosterone causes both prosocial and antisocial status-enhancing behaviors in human males to imply that women are being held back by a biological lack of testosterone, thus, drive. However, I can play this “find a scientific study that supports my views” game too. And I did find a study, Testosterone and the struggle for higher social status, revealing that “testosterone can promote threat vigilance, which enables an individual not only to detect potential status challenges, but also, as a consequence of, and facilitated through the mechanisms detailed above, to act accordingly to defend his high status and favourable economic position.” So there it is. Studies show that testosterone causes men to not only seek status but also defend it in the face of challenges — a part of the equation that those who blame women for their lack of equal status are loath to acknowledge.
But then again, the whole concept of “testosterone did it” is rubbish, according to Cordelia Fine. Her book Testosterone Rex: Unmaking the Myths of Our Gendered Minds was just awarded the Royal Society prize for science book of the year.
As for Damore’s claim that it’s their drive for status that pushes men to take “undesirable and dangerous jobs like coal mining, garbage collection, and firefighting, and suffer 93% of work-related deaths”: it’s not only disingenuous to pretend that this has little to do with factors such as decent pay and benefits, family tradition, or thrill-seeking behaviors but also painfully ignorant of the dangers women face in low-paying jobs and non-paying tasks ranging from illicit sex work to carrying extremely heavy loads as those responsible for the family’s firewood and water in the most poverty-stricken areas of the world to childbirth in the wealthiest country in the world.
Here, Damore revisits some of the personality traits he has identified and suggests ways to address each one without discriminating (against white males, that is), beginning with the suggestion of “making software engineering more people-oriented.”
To address this, I turn to point number 2 of the response to the Google Memo by Yonatan Zunger, an engineer who had just left a position as “a fairly senior Googler” when the memo first surfaced. His second point is that “the author does not appear to understand engineering.”
Zunger writes:
Engineering is not the art of building devices; it’s the art of fixing problems. Devices are a means, not an end. Fixing problems means first of all understanding them — and since the whole purpose of the things we do is to fix problems in the outside world, problems involving people, that means that understanding people, and the ways in which they will interact with your system, is fundamental to every step of building a system.
And this is why, Zunger says, the memo’s conclusions are “precisely backwards.” If women are, indeed, better at paying attention to people’s emotional needs (which he attributes it to socialization, not biology), then “this is something that makes them better engineers, not worse ones.”
I’m not going to spend time discussing the merits of Damore’s other proposed solutions (which boil down to more collaboration and cooperation, more part-time work, and allowing men to be more “feminine”), reserving my commentary for his suggestions at the end of the memo. I’d just like to draw attention to the two publically available sources he links to in this section and provide alternative perspectives to those.
The first one, The War Against Boys, is an article in the Atlantic from 2000 that’s based on a book by the same title. According to Wikipedia, author Christina Hoff Sommers, a resident scholar at the American Enterprise Institute, a conservative think tank, is known for accusing feminist thinkers of harboring an “irrational hostility to men” along with the “inability to take seriously the possibility that the sexes are equal but different.”
Here is the response of David Sadker, one of the researchers whose work Sommers grossly misrepresents in the article. His most salient point, beyond highlighting the author’s “long history of inaccuracies and inflammatory rhetoric in support of far right causes”:
It is bias we oppose, not boys.
For a much more recent — and much less polemical — analysis of the topic, see Why Girls Tend to Get Better Grades Than Boys Do (2014), by clinical psychologist Enrico Gnaulati. He focuses on girls’ higher predisposition for “self-discipline,” or “conscientiousness” (one of the traits that declines in males in more egalitarian societies, you may recall) to show how classrooms tend to play more to girls’ strengths than to boys’ while tests continue to advantage boys’ abilities.
The second source, Women, careers, and work-life preferences, was written in 2006 by Catherine Hakim, who is known for her “criticism of many feminist assumptions about women’s employment” as well as for her controversial book Honey Money: The Power of Erotic Capital (2011), which led the London School of Economics to distance itself from her, in part because she was claiming to be a “senior research Fellow of sociology” at the school despite not being employed there since 2003.
Again, for a more relevant take on the issue at hand, a Guardian piece from January 2017 points out that as men become more engaged with their families, they are coming up against the same barriers to work-life balance that women have been dealing with forever.
And psychologist Andrei Cimpian puts Damore’s assumptions about women’s preferences to the test in the article “Work-Life Balance” and “Empathizing” Do Not Explain Women’s Career Choices.
This is the part of the memo where Damore accuses Google of engaging in illegal discriminatory practices — in the tiny print of footnote number 6 — and of being tools of leftist ideologues.
Here’s the full text of Damore’s list of allegedly discriminatory practices:
· Programs, mentoring, and classes only for people with a certain gender or race
· A high priority queue and special treatment for “diversity” candidates
· Hiring practices which can effectively lower the bar for “diversity” candidates by decreasing the false negative rate
· Reconsidering any set of people if it’s not “diverse” enough, but not showing that same scrutiny in the reverse direction (clear confirmation bias)
· Setting org level OKRs (Objectives and Key Results) for increased representation which can incentivize illegal discrimination
And here’s the full text of footnote number 6, which refers to the last bullet point:
Instead set Googlegeist OKRs, potentially for certain demographics. We can increase representation at an org level by either making it a better environment for certain groups (which would be seen in survey scores) or discriminating based on a protected status (which is illegal and I’ve seen it done). Increased representation OKRs can incentivize the latter and create zero-sum struggles between orgs.
Although I don’t quite know what Damore is talking about or exactly what the corporate human relations jargon means, what I do know is this, from the explainer What is Discrimination?:
1. State and federal discrimination laws apply when the discrimination occurs in specific settings against members of certain, protected groups.
2. Some kinds of unequal treatment are legal.
Furthermore, as explained by the American Association for Access, Equity and Diversity, affirmative action is all about “taking positive steps to end discrimination, to prevent its recurrence, and to create new opportunities that were previously denied minorities and women.”
Is it just me, or does it appear that Damore is way off base with his accusation of illegality here?
Contradicting previous assurances that he values gender and racial diversity, Damore blunders even farther into the alt-right weeds with his claims that Google’s diversity and inclusion programs are “based on false assumptions” that could actually heighten race and gender tensions; that this is “veiled left ideology,” i.e., Marxist gender and race politics, according to another footnote; and that there is no evidence that these programs are “both the morally and economically correct thing to do.”
Where to even start!
I guess with the evidence.
So, is there evidence that implementing corporate diversity and inclusion programs is “both the morally and economically correct thing to do”?
The Geneva-based World Economic Forum thinks so. From the 2016 Global Gender Gap Report: The case for gender parity:
There is a clear values-based case for promoting gender parity: women are one-half of the world’s population and evidently deserve equal access to health, education, economic participation and earning potential, and political decision-making power. However, it is pertinent to note that gender parity is equally fundamental to whether and how societies thrive. Ensuring the healthy development and appropriate use of half of the world’s total talent pool has a vast bearing on the growth, competitiveness and future-readiness of economies and businesses worldwide.
Sociologists, recruiters, and other business researchers think so. From the 2014 article The evidence is growing — there really is a business case for diversity in the Financial Times:
Research last year . . . found that publicly traded companies with two-dimensional diversity were 45 per cent more likely than those without to have expanded market share in the past year and 70 per cent more likely to have captured a new market. When teams had one or more members who represented a target end-user, the entire team was as much as 158 per cent more likely to understand that target end-user and innovate accordingly.
And here’s more from the 2016 Harvard Business Review report Why Diverse Teams Are Smarter:
Striving to increase workplace diversity is not an empty slogan — it is a good business decision. A 2015 McKinsey report on 366 public companies found that those in the top quartile for ethnic and racial diversity in management were 35% more likely to have financial returns above their industry mean, and those in the top quartile for gender diversity were 15% more likely to have returns above the industry mean.
Even the US intelligence community thinks so. From Diversity and Inclusion: Examining Workforce Concerns Within the Intelligence Community:
The case for diversity and inclusion is clear. Along with an inclusive work environment, diversity in both leadership and the workforce are critical to maximizing mission effectiveness and impact.
Thus, there is plenty of evidence showing why companies and institutions should invest in diversity and inclusion — and it isn’t only coming from leftist ideologues.
That said, the next question is, what is the evidence that Google’s programs are actually working?
Well, according to the eye-opening 2017 report Are Diversity Programs Merely Ceremonial? Evidence-Free Institutionalization, they aren’t. Authors Frank Dobbin and Alexandra Kalev even call out Google by name, stating, “Based on the evidence, if there are two companies that you should not emulate to promote diversity they are Google and Facebook.”
In fact, the authors couldn’t ask for more convincing evidence of how anti-bias training can backfire by raising resistance and activating bias than the Google Memo.
Finally, what about Damore’s assertion that Google’s practices “can actually increase race and gender tensions”?
This statement contains a link to a 2016 essay in the Wall Street Journal titled Hard Truths About Race on Campus by Jonathan Haidt and Lee Jussim — yes, the same Jonathan Haidt who proposes affirmative action for conservatives, along with a colleague who is convinced that empirical research upholds the accuracy of stereotypes. In this WSJ essay, they argue that programs to create more diversity on college campuses, employed under pressure from student groups driven by the Black Lives Matter movement, are “likely to damage race relations and to make campus life more uncomfortable for everyone, particularly black students” — especially when they involve “microaggression training and other measures that magnify racial consciousness and conflict.” They decry the lack of evidence for diversity measures, hold up the US Army as “a model of integration and near equality,” and encourage more generosity of spirit, humility, and, quoting Martin Luther King Jr., forgiveness.
At first blush, it may appear that Haidt and Jussim are saying the very same thing that Dobbin and Kalev are saying: that many diversity measures are evidence-free and likely to backfire, only making things worse. But closer examination reveals crucial differences. Beyond the obvious dissimilarity that the former are talking about learning institutions while the latter are talking about businesses, the authors of the WSJ piece trot out half-truths about race on campus and contend that diversity measures are counterproductive because they bring into focus people’s differences, creating tension on all sides, whereas Dobbin and Kalev rely on their analysis of three decades worth of data along with their own interviews to discover why diversity programs fail and offer “a theory of organizational change wherein innovations that turn managers into change agents are effective while those that seek to constrain managers and control their discretion will lead to resistance to program goals.” I cannot emphasize enough the importance of their finding that diversity training, particularly when it is mandatory, tends to cause people to resent its implications and resist its message — just as Damore obviously resents the implication that he, along with the entire Silicon Valley corporate environment, is sexist.
Finally, with a healthy dose of forgiveness for the male-centered language, a note about tension, from Letter from a Birmingham Jail, by Martin Luther King Jr.:
I must confess that I am not afraid of the word “tension.” I have earnestly opposed violent tension, but there is a type of constructive, nonviolent tension which is necessary for growth. Just as Socrates felt that it was necessary to create a tension in the mind so that individuals could rise from the bondage of myths and half-truths to the unfettered realm of creative analysis and objective appraisal, so must we see the need for nonviolent gadflies to create the kind of tension in society that will help men rise from the dark depths of prejudice and racism to the majestic heights of understanding and brotherhood. . . . Too long has our beloved Southland been bogged down in a tragic effort to live in monologue rather than dialogue.
“We all have biases and use motivated reasoning to dismiss ideas that run counter to our internal values,” begins Damore in this next section, before going on to use motivated reasoning to confirm his internal values.
First, he sets up a false equivalency between science denial on the right and on the left, only to cite the November 2016 article The Real War on Science by contrarian John Tierney that alleges “two huge threats to science” that are “peculiar to the left”: confirmation bias and a “long tradition of mixing science and politics.” Unsurprisingly, Tierney cites Haidt in making his case that the left has done more to harm scientific progress by, among many other things, overtaking social psychology and “creating what Jonathan Haidt calls a ‘tribal-moral community’ with its own ‘sacred values’ about what’s worth studying and what’s taboo.” The rise of progressivism, he writes, “saw scientists as the new high priests, offering them prestige, money, and power. The power too often corrupted. Over and over, scientists yielded to the temptation to exaggerate their expertise and moral authority, sometimes for horrendous purposes.” Never mind any evidence that scientific malpractice is a bipartisan problem (see The rise of the evolutionary psychology douchebag). While Tierney mischaracterizes the science he disagrees with throughout the article, one of his worst offenses is against Planned Parenthood founder Margaret Sanger and her controversial views on eugenics.
Of course, contrary to the title of this section, Damore isn’t really trying to explain why we’re all blinded by our biases here at all.
The next links in the document go to HeterodoxAcademy.org, the website founded by Haidt, Jussim, and others “to advocate for a more intellectually diverse and heterodox academy,” and then to Is Social Psychology Biased Against Republicans?, a thorough exploration of Haidt’s mission and its implications by Maria Konnikova in the New Yorker.
Then there’s the footnote attached to Damore’s statement about the dangers of confirmation bias in the social sciences, such as “myths like social constructionism and the gender wage gap.” If you’re wondering where the idea that women’s lower salaries are a myth comes from, well, here’s your answer: the book Why Men Earn More: The Starling Truth behind the Pay Gap — And What Women Can Do About It, by Warren Farrell, “the father of the men’s movement.”
Here’s an alternative take on this “mythical” wage gap, from the Economic Policy Institute: ‘Women’s work’ and the gender pay gap: How discrimination, societal norms, and other forces affect women’s occupational choices — and their pay.
Next comes a paragraph that begins, “In addition to the left’s affinity for those it sees as weak, humans are generally biased towards protecting females,” in which Damore complains that expressing any male problems will get him labeled “a misogynist and a whiner” while displaying a total lack of awareness of what systemic sexism entails. Take a look at these non-feminist FAQ he links to, if you want to understand more about where he is coming from here.
The final paragraph of this section also opens with lefty compassion for the weak, which supposedly leads to political correctness. Whereas much of the content of this memo left me scratching my head, until being enlightened as to what he is really talking about by his links, this passage remains baffling, as it appears to have misconstrued the information in the Scientific American article The Personality of Political Correctness — only to end up in utter nonsense about the greenness of the grass on the “other side.”
First, there’s the matter of the “PC-authoritarians.”
The article describes a study that distinguishes two “shades” of political correctness: PC-egalitarians “tended to attribute a cultural basis for group differences, believed that differences in group power springs from societal injustices, and tended to support policies to prop up historically disadvantages groups,” while PC-authoritarians “tended to attribute a biological basis for group differences, supported censorship of material that offends, and supported policies of harsher punitive justice for transgressors.”
Throughout the memo, Damore resists the kind of thinking attached to the egalitarian shade while agreeing with the authoritarians in insisting that biology is, at least in part, responsible for group differences. But that distinction doesn’t fit his narrative of the tyrannical left, so he misuses the term “PC-authoritarian” to vilify his ideological opponents, whereas his source material describes those he opposes as “PC-egalitarians.”
Importantly, the study also found that “PC is not a purely leftwing phenomenon, but is better understood as the manifestation of a general offense sensitivity, which is then employed for either liberal or conservative ends,” and that PC-authoritarianism has many similarities to rightwing authoritarianism.
Then there’s the matter of compassion.
What the article says about compassion is that, regardless of which shade, respondents who agreed that considering political correctness was important to them “tended to be female, non-White, and report higher levels of compassion. It is likely that it’s the high levels of compassion that produces the offense sensitivity seen in high-PC individuals.”
At the end of the article, Author Scott Barry Kaufman also includes a section on “the dark side of compassion,” where he walks us down a path of social psychology that leads to the often contrarian world of evolutionary biology by talking about empathy vs. sympathy, then“in-group” vs. “out-group,” ending up at “pathological altruism.”
While Kaufman does not mention left/right politics in this final section on “extreme compassion,” Damore’s laser focus on this being a leftwing characteristic is blinding him to the finding that it’s people on the right who are more concerned about their own group and more aggressive toward others, not those on the left.
A quick glance back at his table of left/right biases at the beginning of the memo makes clear that Damore believes the left’s “compassion for the weak” is countered on the right by “respect for the strong/authority,” whereas Haidt’s moral matrices (see Figures 12.2–12.4) set up no such dichotomy. Rather, he specifies that the right is invested in all six foundational values — Care-Harm, Liberty-Oppression, Fairness-Cheating, Loyalty-Betrayal, Authority-Subversion, and Sanctity-Degradation — while the left is most concerned with the first three and libertarians focus almost entirely on individual liberty. Haidt characterizes the left’s “most sacred value” as “care for victims of oppression” — which has nothing to do with weakness! — and the right’s as “preserve the institutions and traditions that sustain a moral community” — which describes tribalism on the right, not the left.
In introducing his list of suggestions, Damore restates his contention that Google has “an intolerance for ideas and evidence that don’t fit a certain ideology” and insists that, rather than restricting people to their gender roles (which, he implies, would be tribalism), he’s advocating for treating people as individuals.
I would just reiterate that if the intolerance is against inappropriately treating women differently than men in the workplace — or writing up a memo that defiantly rationalizes inappropriately treating women differently from men in the workplace — then Damore has done a stellar job of showing us what digging in and resisting anti-sexism messaging looks like.
Now, on to Damore’s suggestions.
De-moralize diversity.
Seriously?! Isn’t moralizing diversity exactly what Damore is doing by drawing on Haidt and his moral foundations theory; by inciting moral disgust over the concept of being censured for stereotyping half the world’s population; by even suggesting that we de-moralize diversity when the world’s institutions have already moved beyond the moral imperative to recognize the multiple financial and strategic benefits of diversity and inclusion?
The link here goes to the abstract for the article The Process of Moralization by Paul Rozin, which references Haidt’s work in discussing moralization of health concerns such as smoking and vegetarianism.
Stop alienating conservatives.
Again — seriously?!
The irony of demanding “viewpoint diversity” with a viewpoint that pits the inclusion of underrepresented groups against the individual liberty of members of the group that would prefer maintain their homogeneity is just staggering.
Links go to Liberal Privilege in Psychology by Lee “Rabble Rouser” Jussim, Conservative professors must fake being liberal or be punished on campus by Kyle Smith, and Liberals, conservatives, and personality traits by Steve Bogira.
A note on that third one, which Damore includes in support of his contention that alienating conservatives is “bad business because conservatives tend to be higher in conscientiousness”: The article touts the 2011 study Personality correlates of party preference: The Big Five in five big European countries, which concludes, “The Openness trait [associated with the left] has been shown to be the most generalizable predictor of party preference across the examined cultures. Conscientiousness [associated with the right] was also a valid predictor, although its effect was less robust and replicable” (emphasis mine).
It’s another major blind spot of Damore’s that, over and over again, the left is found in these studies he cites to be the ones who are open-minded, creative, and curious.
But then again, there’s this: Politics and Personality: Most of What You Read Is Malarkey by Maria Konnikova.
Confront Google’s biases.
Damore thinks it’s a good idea for the company to track employees’ political orientations and personalities. Because moral values.
Stop restricting programs and classes to certain genders or races.
No, Mr. Damore, empowering women and people of color is neither discriminatory nor divisive. Insisting that the status quo is as nature designed it is.
Have an open and honest discussion about the costs and benefits of our diversity programs.
The jury is already out on this. The better question is, are these programs working to achieve their goals?
I have to say, the points Damore makes in this section are stunningly ridiculous. For example:
Discriminating just to increase the representation of women in tech is as misguided and biased as mandating increases for women’s representation in the homeless, work-related and violent deaths, prisons, and school dropouts.
That’s not honest discussion; it’s polemical claptrap.
As for his allegation of a lack of transparency, see Google Diversity.
Focus on psychological safety, not just race/gender diversity.
Now this seems like a decent idea.
But how does the statement, “Having representative viewpoints is important for those designing and testing our products, but the benefits are less clear for those more removed from UX [user experience],” have anything to do with that?
Maybe Damore meant to place this under the previous suggestion, which would not bode well for his claim to greater conscientiousness.
De-emphasize empathy.
In the linked-to article Against Empathy, author Paul Bloom does not say that empathy causes us to harbor “irrational and dangerous biases,” as Damore implies. Rather, he says, in part:
Empathy is biased; we are more prone to feel empathy for attractive people and for those who look like us or share our ethnic or national background. And empathy is narrow . . . In light of these features, our public decisions will be fairer and more moral once we put empathy aside. Our policies are improved when we appreciate that a hundred deaths are worse than one . . .
He then moves from public policy making to personal relationships, pointing out that because emotional empathy can be draining and ineffective in helping the person whose pain you are mirroring, relying on compassion is a better choice. And finally, he likens empathy to anger, an emotion that we shouldn’t avoid altogether but should modify, shape, and direct with rationality and compassion.
That’s a far cry from the emotional disengagement Damore is calling for here.
Prioritize intention.
I think this is an excellent idea — but not for the same reasons that Damore does.
He is talking about how focusing on microaggressions “and other unintended transgressions” increases sensitivity and leads to “authoritarian policies.” “Microaggression training,” he writes, “incorrectly and dangerously equates speech with violence and isn’t backed by evidence,” linking to the Atlantic article Why It’s a Bad Idea to Tell Students Words Are Violence by Jonathan Haidt and Greg Lukianoff and to the article Microaggressions: Strong Claims, Inadequate Evidence in Perspectives on Psychological Science by Scott O. Lilienfeld.
Contrary to the Atlantic article, which declares, “Free speech, properly understood, is not violence. It is a cure for violence,” Katherine Cross states, in her powerful essay in response to the death of Heather Heyer in Charlottesville titled When ‘Free Speech’ Kills:
The road that James Alex Fields Jr. sped down was paved with countless editorials in major newspapers and magazines that positioned student movements or black women on Twitter as existential threats to “free speech” . . . It was paved by countless people saying, “they’re just words” or “it’s just the internet, it’s not real life” in defense of extremists’ vitriol, never realizing that such statements are not mere words on the wind: they are promises.
Later, in speaking about the phrase “free speech rally,” she hits on the key point that “these people are adopting this term for a reason. When we use ‘free speech’ as moral spackle to cover up the true content of these people’s beliefs and deeds, they will take that as a cue and use it accordingly.”
Cross is expressing the same principle as judicial rulings that uphold our Constitutional right to equal treatment under the law, which often look beyond “the letter of the law” to recognize the intent of legislation such as voter ID laws and regulations like Trump’s “travel ban” (wink, wink) because the very nature of systemic racism, sexism, and fascism is to find rationalizations for them, to whitewash them and normalize them, to hide them right there, in plain sight, using careful language that masks their true character.
So yes, we should prioritize intention — beginning with this memo.
And what about those “unintended transgressions,” those accidental slights that microaggression training focuses on?
First of all, the idea of a microaggression isn’t to “equate speech with violence”; it’s about aggression and vulnerability. The concept of words as violence applies when Nazis, white supremacists, and neo-Confederates are given permission by institutions to ignite hatred with their dog whistles and coded language — not when a person unknowingly says something insensitive or inappropriate to a person of their out-group.
And second, there’s the African proverb, “The true tale of the lion hunt will never be told as long as the hunter tells the story,” which appears at the opening of Microaggressions and “Evidence”: Empirical or Experiential Reality? by Derald Wing Sue. This is a critique of the findings on microaggressions by Lilienfeld, who, as a member of the Heterodox Academy executive committee, is a part of the group trying to inject more ideology into science rather than working harder to avoid it. The proverb encapsulates the issue of the victims of microaggressions not being well represented in Lilienfeld’s “selective review of the literature.” The takeaway is that people who have never been in a position where they feel vulnerable have no right to tell those who have how to deal with the problem without acknowledging that they are part of the problem (or that there even is a problem).
Be open about the science of human nature.
Yes, let’s!
The problem here is — and this goes to the very heart of why Damore is so wrong (and so fired) — no one in the sciences is saying that “all differences are socially constructed or due to discrimination.” That is a mischaracterization by conservative and libertarian ideologues of social role theory, which acknowledges that biological differences between men and women are part of the story but is more interested in the cultural and societal parts of the story, since those are the parts we can actually take action to change.
If Damore had actually read the two studies he cites to support his denial that sexism has anything to do with the gender gap, then he would know that even the scientists who have found evidence that points to possible biological differences note that the different approaches to gender studies are not mutually exclusive.
And if he did read them and is just ignoring that part, well . . .
This whole memo reeks of the same kind of muddying-the-waters tactics that climate science deniers, creationists, and conspiracy theorists engage in.
Reconsider making Unconscious Bias training mandatory for promo committees.
The devolution of a good idea, in three steps:
Going back to the research conducted by Dobbin and Kalev, it’s obvious that Damore’s first point is true, that mandatory bias training just doesn’t work, although it’s not because of any political bias or factual inaccuracies, as Damore alleges in his second point. Rather, it’s all about control. Negative messaging, compulsory courses, and targeting a specific group are control tactics that tend to backfire. Per Dobbin and Kalev:
Most companies with training have special programs for managers. To be sure, they’re a high-risk group because they make the hiring, promotion, and pay decisions. But singling them out implies that they’re the worst culprits. Managers tend to resent that implication and resist the message.
These authors have solutions, which involve “three basic principles: engage managers in solving the problem, expose them to people from different groups, and encourage social accountability for change.”
Damore, on the other hand, takes a cue from the Heterodox Academy folks and suggests, in point three, that we fight bias with more bias:
Spend more time on the many other types of biases besides stereotypes.
Anti-bias training, he claims, is factually incorrect in stressing that stereotypes are inaccurate, referencing Lee Jussim’s report Stereotype Accuracy is One of the Largest and Most Replicable Effects in All of Social Psychology. However, Andrei Cimpian’s research puts the accuracy of stereotypes in question and highlights the inappropriate surety with which Damore believes he is in the right.
The aim of disparaging the focus of anti-bias training on the harm of stereotyping is to expose “other types of biases,” i.e., ideological and political biases; in particular, the ideologies of the left that are conspiring to silence his view that women and people of color are not underrepresented but rather, they simply don’t belong in high-paying jobs at Google and other tech companies.
In short, what is Damore’s final suggestion? Why, declaring a conspiracy and painting the oppressor as the victim, of course!
Note: This article was edited for style and clarity on 1 Sept. 2017
I’m a freelance writer and editor pining to return to Mexico. Complexity turns me on, and connectivity tunes me in. A hopeful ponderer, some say I’m a dreamer…
3 
3 
3 
I’m a freelance writer and editor pining to return to Mexico. Complexity turns me on, and connectivity tunes me in. A hopeful ponderer, some say I’m a dreamer…
"
https://blog.gcodetec.com/migrando-para-nuvem-com-tsuru-204448ad836f?source=search_post---------191,"Após algum tempo desenvolvendo aplicações internas para um cliente começamos a enfrentar uma série de problemas. Usávamos uma hospedagem compartilhada e por conta disso tínhamos uma série de restrições como:
Procurando uma alternativa para o nosso problema, encontramos o Tsuru.
O Tsuru é um PaaS criado pelo pessoal da globo.com e foi a alternativa mais adequada para nossa necessidade. Gostaríamos de trabalhar com Docker e não gerenciar containers na mão, fazer balanceamento de carga e muitas outras coisas.
O Tsuru já vem com muita coisa embutida, listo abaixo os principais componentes:
Existem muitos outros componentes, clique aqui.
Adotando o Tsuru tivemos grandes vantagens, segue abaixo algumas delas:
Com essa mudança, saímos de um cenário totalmente preso e restrito para um cenário cheio de possibilidades e liberdade. Podemos prototipar, fazer o deploy desse protótipo em minutos e caso exista algum bug na aplicação fazemos um rollback rápido, sem downtime. Temos um dashboard bem bacana para monitorar, ver logs e métricas das aplicações.
Já estamos utilizando o Tsuru a mais ou menos 8 meses em produção e estamos curtindo muito. Temos um suporte bem bacana no gitter, pessoas sempre dispostas a ajudar e tirar dúvidas.
Nosso cliente ainda possui aplicações na hospedagem compartilhada, mas em breve migraremos tudo para o nosso PaaS :D
A gCode é uma empresa de TI focada em desenvolvimento de…
8 
8 claps
8 
Written by

A gCode é uma empresa de TI focada em desenvolvimento de soluções empresariais. Trabalhamos com computação em nuvem, desenvolvimento de sistemas e aplicativos móveis
Written by

A gCode é uma empresa de TI focada em desenvolvimento de soluções empresariais. Trabalhamos com computação em nuvem, desenvolvimento de sistemas e aplicativos móveis
"
https://medium.com/sorceresss-data-stories/adls-101-the-story-of-an-archivist-i-2e342c070f54?source=search_post---------192,"There are currently no responses for this story.
Be the first to respond.
Imagine you are an archivist in big Bureau. Bureaus are famous for generating various documents which are crucial for their being. Almost every process has its form, and you have to store them securely and reliably.
That is your task, as Bureau’s Principal Lead Archivist.
You sit in a big room with a lot of racks just ready to be filled with reports and documents. Your colleagues are coming to you with piles of papers and want you to store them, and — of course — to obtain them when needed.
For a few days, you receive the papers from your colleagues, toss them on one big pillar of other sheets and say goodbye. There is no time overhead when storing those documents this way and the Bureau’s employees are happy because they do not wait so long for you to save their possessions. What a relief in an office where everything has its time.
But there’s a catch!
One day, a colleague, who is not so sure if she correctly filled some HR form the day before yesterday, came to you and asked you to give her the desired document so she can check it. You start frantically go through the piles of papers on your desk, under your desk, and all around you, trying to remember where the hell you put those papers from the day before yesterday and if this woman was here in the morning or after lunch, because you always toss the documents under the table after meal. Your colleague is not satisfied. She waits for a few minutes, then turns back and goes directly to her manager to make a complaint. Now your bonuses are endangered!
This scenario is in a sense similar to store all your files on desktop — you can create files ad hoc without spending much time creating folders and folder hierarchies, but the more documents you create, the messier your desktop will be — and one day, you will run out of space — or, probably earlier — out of patience.
You realise it is not enough to save those papers; there has to be at least some convenient way to look for them after they were archived. Because you are a lazy archivist, you want to spend the least time possible both while retrieving old documents and also while saving new materials in the future.
You remember that people from Finance Dept. are coming at least thrice per day with piles of invoices, but the guy from IT Dept. appears only once a week and never comes back for any of his files.
Similarly to our archivist, Azure Data Lake Storage (referred to also as ADLS) also analyse the traffic around the files. It stores various information, such as how often they are read from or written to, if there are more processes wanting to access them parallelly, how much of data is added (appended) with every write to the file — is it only a few rows every minute, or a big chunk with million of rows once per day?
Write/Read scenarios could also change over time. In the beginning, you can receive only a handful of information each hour (f.e. a few clicks per hour on your unknown website). Once your web goes viral, amount of data you receive skyrocket to hundreds of thousands of rows per hour (once you have a favourite video, not only more new people are coming every second, but the same people are returning, pushing replay, etc.).
Every scenario needs a different approach, and the file-storage platform has to be agile to switch the strategy if the situation changes significantly.
You decide to store documents from the Finance Dept. in the first, second and third shelf next to you, but you can toss those papers from IT guy in the back of your office. With this enhancement you kill two birds with one stone: Since the IT guy always comes with the documents later in the evening, you go to the back of the office only once with all his papers. Also, there is no way any inconvenient log file could appear between your KPI reports for a Finance manager since they are handled and stored separately.
For the sake of simplicity, we refer to “orders”, “documents” and “logs” as pieces of papers. In Databases and Data Lakes reality, orders and invoices are usually stored as rows, either inside a database table or as row inside text file (CSV, TSV, etc.).
An order — a row in a text file — holds an unbreakable piece of information. Give one only half of the row and the data is useless. Here comes the concept of atomicity, which defines that the content of any file is broken into the smallest possible amount of data which still holds some meaningful information. This piece of information is called block and can be up to 4MB large.
Blocks are the smallest piece of a file handled by ADLS — every file consist of one or more blocks. Thanks to atomicity concept, the block will always be processed (written to or read from a file) with one thread. Therefore, blocks are also referred to as the smallest unit of parallelism.
If you manage to break your file into many blocks, your data could be read, and even be written to, parallelly, and therefore quickly. CSV file is convenient — each row (line) is one block, and each block is roughly the same size. You can both read from and write to a big CSV file parallelly since thanks to end-lines you know where the previous row ends and where the new begins.
Hands-on data analytics with open data and Microsoft data…
Written by
BI & Stats & Data analysis. Microsoft Data Platform enthusiast, believing that data can help us defining and reaching better future.
Hands-on data analytics with open data and Microsoft data platform.
Written by
BI & Stats & Data analysis. Microsoft Data Platform enthusiast, believing that data can help us defining and reaching better future.
Hands-on data analytics with open data and Microsoft data platform.
"
https://medium.com/sorceresss-data-stories/adls-101-leverage-your-laziness-into-efficiency-431bf0448370?source=search_post---------193,"There are currently no responses for this story.
Be the first to respond.
For a time, everyone is happy. Sure, people coming in your office for archiving their papers have to wait a minute before you assure them that their documents are stored, but they do not need to wait an hour in case they need to check some item on an Invoice archived a day before.
Unfortunately, this approach requires contact walking, climbing over the shelves and checking the head of the document. So even though you’re receiving your monthly bonus, you come back home exhausted. For the sake of laziness and your mental health, you decide to conduct an analysis and figure out how to minimise both the amount of time you spent walking while storing the data and time spent while you try to find the right paper on the department shelf.
You soon find out that employees from the HR Dept. tend to come with big piles of documents once a week, all almost exclusively about one topic (i.e. monthly payroll or employee satisfaction survey) and when needed, they are usually taken all together grouped by their type (payroll, satisfaction survey) and Year-Month (January 2018 or December 2016).
On the other hand, people from the Finance Dept. are coming daily with a bunch of various documents (like invoices and orders) together, and they almost never correspond to each other. Also, they are never asked to be obtained together as they came. On the contrary, your accountant wants either all orders from given year month or all invoices from given year month.
For the beginning you decide to sort incoming documents first by the department they come from and then by document type. You buy some folders to store similar documents together. Each paper you receive from a colleague will be first sorted by department and type and put into right folder accordingly. Each file will have a unique code consisting of department name, document type and date range of documents inside.
There will always be some files from departments/document types on your desk which won’t be full, so you can quickly add new documents as they come without even leaving your chair. Only once the folder is full, you will put a Date Range stamp on it and take it back into the archive on shelves.
With this improvement, the time spent receiving documents will go down (because you do not need to go back to shelves each time new paper arrive), but you also don’t lose track where any particular document is, thanks to your basic indexing.
In ADLS, any file usually consists of one type of information, like rows of invoices. Files in ADLS can be up to 5TBs in size. This is a massive portion of data, and you do not want to process this file as one piece. Therefore, you need to split the file into manageable parts.
The reason behind splitting the file from one big chunk to few separate parts is you want to leverage the force of parallelism — big 1GB file split into four parts can be sent to four cores in your cluster and processed parallelly — quicker than processing one file on one core.
File splitting is done after enough appends (writing into the file) occurs. The file is split into units called extents. An extent consists of blocks (unit of atomicity) and is usually 250 MB large (in Gen 1 ADLS). Every file consists of 1 or more extents which inside store all data. Extents are sequential, that means, newly created extents usually (but not necessarily, see later) consist of newer data than the previous one. But this does not mean that data inside are ordered!
Extents by themselves do not enforce the order of the data, and this is because of parallelism. Blocks of data are stored inside the extent as they come, and if more threads are writing to the same extent, blocks will be saved randomly, if not enforced by import job itself.
Think about it as about a situation when two of your colleagues from Finance Dept. come with their documents and you store them both in the same file right after you put them together in one pile and archive them — not necessarily in the same order you received those documents.
Extents are used to store the content in the form of convenient chunks of data, ready to be processed by Azure Data Analytics jobs and to be easily replicated to achieve high availability, as we see later in the story.
A file can consist of many extents, but there is always only one extent, referred to also as a tail of the file, in which you can append new data. It is crucial regarding reliability and consistency — if there will be many concurrent processes wanting to append to the file, it could lead in an enormous number of almost empty extents and inconsistency between data (which was first, is this one empty or full, etc.). Every other extent beside the tail is sealed, that means, it cannot be changed.
Now, imagine a few write scenarios we mentioned earlier (those documents from HR versus documents from Finance):
First, let’s have a file which is appended to with bulk of rows once per day (the HR scenario), each time the process appends circa 0,5 GB of data. Therefore with each append, at least two new extents have to be created and safely stored inside the Data Lake. Remember, writing to a disk an saving something is not cheap, so you want to save as much data as possible during one append.
The first scenario is very convenient for ADLS: as soon as new extent is created during append because the previous one is full and sealed, a new extent is also immediately filled with new blocks (a bunch of payrolls in this case) and sealed. Now ADLS can take all those extents at once, catalogize them and store them securely together in one batch.
For the second case, let’s have a file which is appended to regularly, once every few seconds (the Finance scenario), but each time some process want to add a data, it adds only one or two rows (few bytes only). Therefore, with each append, you would have to receive the row (an invoice), go back to the office to find the file on the shelves, store the record, and go back to the reception desk only to receive another invoice. This is very resource exhaustive and clearly not efficient.
For this particular scenario, ADLS has something called Small Append Service. Remember those half-empty files laying on your reception desk? Small Append Service is handy “program”, which can, according to your traffic around your file, create in-memory, but durable (!) buffer for specific files which are regularly appended to in small batches. Only once the buffer is big enough, Small Append Service sends those appends to ADLS storage in one, big batch, which will be stored as an extent! So, as in our story, you archive the file with documents/extent with blocks only after you receive enough data! This service enormously helps ADLS with I/O traffic.
Hands-on data analytics with open data and Microsoft data…
Hands-on data analytics with open data and Microsoft data platform.
Written by
BI & Stats & Data analysis. Microsoft Data Platform enthusiast, believing that data can help us defining and reaching better future.
Hands-on data analytics with open data and Microsoft data platform.
"
https://medium.com/journal-of-lost-souls/trance-murder-express-29a65a69955?source=search_post---------194,"There are currently no responses for this story.
Be the first to respond.
Murder clouds lie low today,
I watch them from my cabin,
As they pass I feel the urge,
Besotted by those charms
To actually kill,
Take a life, rip a form apart
Legs, face, ribcage torn open; and a still-beating heart
"
https://medium.com/new-york-magazine/how-amazon-web-services-reinvented-the-internet-and-became-a-cash-cow-45c17bcfb19f?source=search_post---------195,"There are currently no responses for this story.
Be the first to respond.
Amazon Web Services (AWS) is a web platform that powers everything from Netflix to Slack to NASA. Launched in 2006, AWS is a suite of…
"
https://news.voyage.auto/classifying-vehicles-pedestrians-in-point-cloud-83db7cd6192e?source=search_post---------196,"Sign in
Top highlight
Michael Gump
Aug 24, 2017·6 min read
In school it is very easy to know exactly what something like Agile is and yet have absolutely no idea what it is in the real world. At Voyage I’ve not only uncovered deep insufficiencies in my education, but I’ve gotten to build a very, very real feature into the coolest product out there, a self-driving taxi. This summer has made the months of scouring AngelList well worth it.
I study computer science at MIT and I was thrilled to be working at Voyage this summer. I’m excited by machine learning, especially reinforcement learning, and I like thinking about OpenAI’s requests for research. Over the winter I took a class on self-driving cars, which hosted a competition called DeepTraffic and from there I’ve been excitedly moving to get into the growing autonomous vehicle space.
This summer I’ve been working on several problems in computer vision, which means reading a lot of papers and training a lot of models. Most generally, I’ve been working with public datasets to classify objects in LIDAR data. When we as humans see the world, we automatically perceive depth and produce information about objects like “that’s a car”, or “oh no, that’s a person in the middle of the highway, hit the brakes!” Most of my work this summer has been on bringing these types of insights to Voyage’s self-driving taxi, using a 3D view of the world (LIDAR point cloud) and deep learning.
To properly process the world, a self-driving car needs to take raw sensor information (like point cloud) and figure out what it’s seeing.
Arguably, two of the most important pieces of information are depth: “how long until I hit this object?” and category: “what kind of object is this?”
We can either try to design sensors that directly observe this information, or develop methods to extract that information from existing sensors.
In the case of depth we already have a sensor fit for the purpose, and unless we were extremely brave we wouldn’t try to develop around it. It is mostly accepted that, at least for now, LIDAR is a crucial sensor for a self-driving car. However, information like category (car, pedestrian etc.) isn’t information that can be directly sensed, and we need to develop methods to deduce it. This is the area I’ve worked on most this summer.
For humans, the problem of understanding the above scene is simple: we can recognize the car on the right and the van in the back on the left and the pedestrians milling around in the center. But we have millions of years of evolution behind us: how can we give an autonomous vehicle that same ability? If you are familiar enough with the space, the automatic answer is convolutional neural networks (CNNs). CNNs have produced incredible results on a number of difficult problems, and with some small tweaks they’re very applicable to our LIDAR data.
However, a deep convolutional neural network might not be our best bet. A crucial constraint on our system is that it must operate at a high frame rate. The final system has to be able to run in real time and because of this we need to pick a solution that can produce good results very quickly. While a CNN can recognize complex patterns in images, they are often slow, so a lot of my work this summer has been in exploring alternatives. One alternative was hand-picking informative physical features like height that were highly correlated with an object’s category. Because we have a strong understanding of the physical world (often called a prior), we can save our models a lot of work by engineering these features ourselves.
If my mentor, Tarin Ziyaee, has taught me one thing this summer it is to experiment, experiment, experiment. As I developed and tested new models having detailed breakdowns of their performance helped out tons.
One of the first things I shipped this summer was a good, quick visualizer. Using Vispy let me visualize large point clouds in sequence and debug my models in conditions as similar to the real world as possible. Another lesson from this summer is that a good visualizer should always be the first step for problems like this. It can be very difficult to figure out what the problem is from just the model’s loss curve.
To build a set of tools for training and validating my models I used PyTorch. I wasn’t familiar with PyTorch before but it’s become my new favorite framework. It loses some of the specificity of TensorFlow but it is much easier to get started.
One of the models I built was an Encoder-Decoder network that transforms several channels of input data into categorical predictions. From these noisy predictions we can infer the real categories of all the objects in front of us. This kind of model is powerful because it can become independent of certain sensor and processing errors. For example, a model relying on the size and shape of objects to predict their class can be prone to detection errors. An encoder-decoder model can sidestep issues like this by recognizing patterns in the scene and turning them into predictions directly.
In the near future Voyage will be releasing more technical information on my internship and the work I’ve done, but the good news is a lot of the resources I’ve used are available right now! Almost everything I’ve done can be recreated using open source data and accessible papers. Voyage started life as an open source self-driving car, and it still capitalizes on open source research. I’m excited to be able to help share this work!
From his very first day, Michael has embodied all of the values we hold dear at Voyage, but especially “We Win By Shipping”. Even though we hear from a lot of engineers who say they can ship, it’s easier said than done. We were super lucky to have Michael work side-by-side with us this summer, and couldn’t be happier to have his work running on our self-driving car.
I’m a summer intern at @voyage working on perception and specifically classification.
772 
5
Thanks to Luke Beard and Oliver Cameron. 
772 claps
772 
5
Voyage is delivering on the promise of self-driving cars. Voyage has built the technology and services to bring robotaxis to those who need it most, beginning in retirement communities.
About
Write
Help
Legal
Get the Medium app
"
https://onezero.medium.com/cloudflare-has-a-plan-to-change-everything-about-cloud-security-e831b38608c5?source=search_post---------197,"Sign in
There are currently no responses for this story.
Be the first to respond.
Will Oremus
Jan 7, 2020·5 min read
Cloudflare is best known for protecting websites and keeping them online — and for the occasional deplatforming. As one of the world’s leading…
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@i.gorton/six-rules-of-thumb-for-scaling-software-architectures-a831960414f9?source=search_post---------198,"Sign in
There are currently no responses for this story.
Be the first to respond.
Ian Gorton
Apr 4, 2020·12 min read
We live in an age where massive scale, Internet-facing systems like Google, Amazon, Facebook and the like are engineering icons. They handle vast volumes of requests every second and manage data repositories of unprecedented size. Getting precise traffic numbers on these systems is not easy for commercial-in-confidence reasons. Still, the ability of these sites to rapidly scale as usage grows is the defining quality attribute behind their ongoing success.
We can luckily get some deep insights into the request and data volumes handled at Internet scale through the annual usage report from one tech company. You can browse their incredibly detailed usage statistics here from 2019. It’s a fascinating glimpse into the capabilities of massive scale systems. Beware though, this is Pornhub.com. The report is not for the squeamish. Here’s one PG-13 illustrative data point — they had 42 billion visits in 2019!
For the vast majority of business and Government systems, scalability is not a primary quality requirement in the early stages of development and deployment. New features to enhance usability and utility are drivers of our development cycles. As long as performance is adequate under normal loads, we keep adding user-facing features to enhance the system’s business value.
Still, it’s not uncommon for systems to evolve into a state where enhanced performance and scalability become a matter of urgency or even survival. Attractive features and high utility breed success, which brings more requests to handle and more data to manage. This often heralds a tipping point, where design decisions that made sense under light loads are now suddenly technical debt. External trigger events often cause these tipping points — look in the March 2020 media at the many reports of Government Unemployment and supermarket online ordering sites crashing under demand caused by the coronavirus pandemic.
When this tipping point is reached, it’s the responsibility of architects to lead the system evolution towards a highly responsive, scalable system. Core system architectural mechanisms and patterns will need re-engineering to make it possible to handle the increased demand. For many architects, this is unknown or unfamiliar territory, as scalability leads us down paths that are sometimes heretical to widely understood software architecture principles.
The following six rules of thumb represent knowledge that every software architect should have to aid them in building a scalable system. These general rules can act as guidance for architects on their journey towards reliably handling ever-growing request loads and data volumes.
A core principle of scaling a system is being able to easily add new processing resources to handle the increased load. For many systems, a simple and effective approach is deploying multiple instances of stateless server resources and using a load balancer to distribute the requests across these instances (see Figure 1). Assuming these resources are deployed on a cloud platform such as Amazon Web Services, your basic costs are:
In this scenario, as your request load grows, you will need to have more processing capacity in terms of deployed virtual machines running your server code. This incurs higher costs. Your load balancer costs will also grow proportionally to your request load and data size.
Hence cost and scale go together. Your design decisions for scalability inevitably affect your deployment costs. Forget this and you might find yourself, along with many prominent companies, receiving unexpectedly large deployment bills at the end of a month!
In this scenario, how can your design reduce costs? There are two main ways:
Scaling a system essentially means increasing its capacity. In the above example, we increase request processing capacity by deploying more server instances on load-balanced compute resources. However, software systems comprise multiple dependent processing elements, or microservices. Inevitably, as you grow capacity in some microservices, you swamp capacity in others.
In our load-balanced example, let’s assume our server instances all have connections to the same shared database. As we grow the number of deployed servers, we increase the request load on our database (see Figure 2). At some stage, this database will reach saturation, and database accesses will start to incur more latency. Now your database is a bottleneck — it’s doesn’t matter if you add more server processing capacity. To scale further, the solution is to somehow increase your database capacity. You can try to optimize your queries, or add more CPUs and/or memory. Maybe replicate and/or shard your database. There are many possibilities.
Any shared resource in your system is potentially a bottleneck that will limit capacity. As you increase capacity in parts of your architecture, you will need to look carefully at downstream capacity to make sure you don’t suddenly and unexpectedly overwhelm your system with requests. This can rapidly cause a cascading failure (see next rule) and bring your whole system crashing down.
Databases, message queues, long latency network connections, thread and connection pools and shared microservices are all prime candidates for bottlenecks. You can be assured that high traffic loads will quickly expose these elements. The key is to guard against sudden crashes when bottlenecks are exposed and be able to quickly deploy more capacity.
Under normal operations, systems should be designed to provide stable, low latencies for communications between the microservices and databases that comprise a system. While your system load stays within the operational profile you have designed for, performance remains predictable and consistent and fast, as shown in Figure 3.
As client load increases beyond operational profiles, request latencies between microservices will start to increase. At first, this is often a slow but steady increase that may not seriously affect overall system operations, especially if load surges are shortlived. However, if the incoming request load continues to exceed capacity (Service B), outstanding requests will begin to back up in the requesting microservice (Service A), which is now seeing more incoming requests than completed ones due to slow downstream latencies. This is shown in Figure 4.
In such situations, things can go south really quickly. When one service is overwhelmed and essentially stalls due to thrashing or resource exhaustion, requesting services become unresponsive to their clients, who also become stalled. The resulting effect is known as a cascading failure — a slow microservice causes requests to build up along the request processing path until suddenly the whole system fails.
This is why slow services are evil, much more so than an unavailable service. If you make a call to a service that has failed or one which is partitioned due to a transient network problem, you receive an exception immediately and can decide what to sensibly do (e.g. backoff and retry, report an error). Gradually overwhelmed services behave correctly, just with longer latencies. This exposes potential bottlenecks in all dependent services, and eventually, something goes horribly wrong.
Architecture patterns like Circuit Breakers and Bulkheads are safeguards against cascading failures. Circuit breakers enable request loads to be throttled or shed if latencies to a service exceed a specified value. Bulkheads protect a complete microservice from failing if only one of its downstream dependencies fails. Use these to build resilient as well as highly scalable architectures.
Databases lie at the heart of virtually every system. These typically comprise ‘source of truth’ transactional databases, which hold the core data your business needs to function correctly, and operational data sources that contain transient items to feed data warehouses.
Core business data examples are customer profiles, transactions and account balances. These need to be correct, consistent and available.
Operational data examples are user session lengths, visitors per hour and page view counts. This data typically has a shelf life and can be aggregated and summarized over time. It’s also not the end of the world if it’s not 100% complete. For this reason, we can more easily capture and store operational data out-of-band, for example writing it to a log file or message queue. Consumers then periodically retrieve the data and write it to a data store.
As your system’s request processing layer scales, more load is placed on shared transactional databases. These can rapidly become a bottleneck as the query load grows. Query optimization is a useful first step, as is adding more memory to enable the database engine to cache indexes and table data. Eventually though, your database engine will run out of steam and more radical changes are needed.
The first thing to note is that any data organization changes are potentially painful in the data tier. If you change a schema in a relational database, you probably have to run scripts to reload your data to match the new schema. For the period the script runs, which may be a long time for a large database, your system is not available for writes. This might not make your customers happy.
NoSQL, schemaless databases alleviate the need for reloading the databases, but you still have to change your query-level code to be cognizant of the modified data organization. You may also need to manage data object versioning if you have business data collections in which some items have modified formats, and some the original.
Scaling further will probably require the database to be distributed. Perhaps a leader-follower model with read-only replicas is sufficient. With most databases, this is straightforward to set up, but requires close ongoing monitoring. Failover, when the leader dies, is rarely instantaneous and sometimes needs manual intervention. These issues are all very database-engine dependent.
If you adopt a leaderless approach, you have to decide how best to distribute and partition your data across multiple nodes. In most databases, once you have chosen the partition key, it is not possible to change without rebuilding the database. Needless to say, the partition key needs to be chosen wisely! The partition key also controls how data is distributed across your nodes. As nodes are added to give more capacity, rebalancing needs to take place to spread data and requests across the new nodes. Again, how this works is described in the guts of your database documentation. Sometimes it is not as smooth as it could be.
Due to the potential administrative headaches of distributed databases, managed cloud-based alternatives (e.g. AWS Dynamodb, Google Firestore) are often a favored choice. Of course, there are trade-offs involved — that is the subject of another story!
The message here is simple. Changing your logical and physical data model to scale your query processing capability is rarely a smooth and simple process. It’s one to want to confront as infrequently as possible.
One way to reduce the load on your database is to avoid accessing it whenever possible. This is where caching comes in. Your friendly database engine should be able to utilize as much on node cache as you care to give it. This is a simple and useful, if potentially expensive, solution.
Better, why query the database if you don’t need to? For data that is frequently read and changes rarely, your processing logic can be modified to first check a distributed cache, such as a memcached server. This requires a remote call, but if the data you need is in cache, on a fast network this is far less expensive than querying the database instance.
Introducing a caching layer requires your processing logic to be modified to check for cached data. If what you want is not in the cache, your code must still query the database and then load the results in the cache as well as return it to the caller. You also need to decide when to remove or invalidate cached results — this depends on your application’s tolerance to serving stale results to clients.
A well designed caching scheme can be absolutely invaluable in scaling a system. If you can handle a large percentage of read requests from your cache, then you buy extra capacity at your databases as they are not involved in handling most requests. This means you avoid complex and painful data tier modifications (see previous rule) while creating capacity for more and more requests. This is a recipe to make everybody happy! Even accountants.
One of the issues all teams face as they experience greater workloads is testing at scale. Realistic load testing is simply hard to do. Imagine you want to test an existing deployment to see if it can still provide fast response times if the database size grows by 10x. You first need to generate a lot of data, which ideally echoes the characteristics of your data set and relationships. You need to also generate a realistic workload. For reads, or both reads and writes? You then need to load and deploy your data set and run load tests, probably using a load testing tool.
This is a lot of work. And it is hard to get everything representative of reality so you get meaningful results. Not surprisingly, it is rarely done.
The alternative is monitoring. Simple monitoring of your system involves making sure your infrastructure is operational. If a resource is running low, such as memory or disk space, or remote calls are failing, you should be alerted so that remedial actions can be taken before really bad things happen.
The above monitoring insights are necessary but insufficient. As your system scales, you need to understand the relationships between application behaviors. A simple example is how your database writes perform as the volume of concurrent write requests grows. You also want to know when a circuit breaker trips in a microservice due to increasing downstream latencies, when a load balancer starts spawning new instances, or messages are remaining in a queue for longer than a specified threshold.
There is a myriad of solutions available for monitoring. Splunk is a comprehensive and powerful log aggregation framework. Any cloud has its own monitoring framework, such as AWS Cloudwatch. These allow you to capture metrics about your system’s behavior and present these in a unified dashboard to support both monitoring and analysis of your performance. The term observability is often used to encompass the whole process of monitoring and performance analysis.
There are two things (at least!) to consider.
First, to gain deep insights into performance, you will need to generate custom metrics that relate to the specifics of your application’s behavior. Carefully design these metrics and add code in your microservices to inject them into your monitoring framework so they can be observed and analyzed in your system dashboard.
Second, monitoring is a necessary function (and cost) in your system. It is turned on. Always. When you need to tune performance and scale your system, the data you capture guides your experiments and efforts. Being data-driven in your system evolution helps ensure you invest your time modifying and improving the parts of your system that are fundamental to supporting your performance and scaling requirements.
High performance and scalability are often not the priority quality requirements for many of the systems we build. Understanding, implementing and evolving the functional requirements is usually sufficiently problematic to consume all available time and budget. But sometimes, driven by external events or unexpected success, scalability becomes necessary, otherwise your system is unavailable as it has crashed under the load. And unavailable systems (or effectively unavailable due to slow performance) are of no use to anyone.
Like any complex software architecture, there’s no cookbook or recipes to follow. Trade-offs and compromises are essential to achieve scalability, guided by your precise system requirements. Bear the above rules in mind and the road to scalability should be littered with less unexpected bumps and potholes!
Software architect, author, professor, teacher. Fascinated by the principles and technologies for building scalable distributed systems.
1.7K 
4
1.7K claps
1.7K 
4
Software architect, author, professor, teacher. Fascinated by the principles and technologies for building scalable distributed systems.
"
https://medium.com/@Whipcord/part-1-the-business-case-for-cloud-computing-34734f383b4b?source=search_post---------199,"Sign in
There are currently no responses for this story.
Be the first to respond.
Whipcord Ltd
Feb 1, 2018·3 min read
It’s difficult to talk about computing and IT infrastructure without the conversation quickly turning to the Cloud. For those who approach new technology with some skepticism or reservation, this can appear as another overly-hyped marketing gimmick — especially for those that have recently made an investment in their premise-based infrastructure. However, for others it’s a powerful technology, enabling businesses to do more with less and access technology previously out of reach or reserved for very large enterprises. In this blog series we’ll present the business case for cloud computing to empower more organizations to properly understand the business case and financial advantages that augment the technological benefits of migrating to Cloud computing.
There are many ways to explain the Cloud, but the easiest way is by understanding the structure and limitations of traditional infrastructure. Traditionally, computing and storage infrastructure are on company premises and accessed by users via a local area network (LAN), usually within the same building. Or, alternatively it can be accessed via a wide area network (WAN), typically connecting various offices in the same community, or more broadly between communities. This access can be through the internet or any dedicated communications infrastructure.
The infrastructure, compute and storage, are shared in this way by all users with access, with each user having fractional use of the resource, which is usually dependent and granted on the basis of application and job type. This architecture most often appears like a hub and spoke or ring and was traditionally a very effective way of sharing resources, representing an exponential leap from the traditional mainframe computing, which was the hallmark of enterprise computing in the 70’s and 80’s.
In the description above, the infrastructure is restricted to the size and power of the servers deployed and located in an office data centre that has a finite amount of space and power. The company’s computing power is therefore constrained to this resource as it’s deployed.
However, in the event of change like a divestiture, acquisition or a project requiring high performance or storage from the computing resources temporarily, an incremental new deployment is required. This will consume budget, IT staffing resources and physical space, which are often not available and certainly not on an “on-demand” basis and so the business takes a back seat to their IT limitations. Conversely, consider a company that has overestimated and over spent deploying their data centre; they are now hand-cuffed to the care and maintenance of a resource that has a scale inappropriate for their business.
So what about Cloud? Simply, the Cloud can be defined as computing infrastructure, residing in a remote data centre, accessible via the internet or a WAN. The platform is owned and managed by a third-party organization and sold fractionally on a short-term basis. That’s it. All of the components described above, now centralized.
In this blog series we will present the business case for cloud computing to empower more organizations to see the financial advantages that augment the technological benefits of migrating to Cloud computing. In the next installment of this series, we will delve further into the different types of Cloud and the associated costs.
Originally published on Whipcord’s Blog.
When passion gets powered with new insights, the possibilities are quite extraordinary
When passion gets powered with new insights, the possibilities are quite extraordinary
"
https://medium.datadriveninvestor.com/writing-your-own-file-system-is-not-as-hard-as-you-may-think-aa4af09329cd?source=search_post---------200,"There are currently no responses for this story.
Be the first to respond.
In the cloud computing era, distributed storage systems are pervasive. But have you ever thought of how they store data internally? The answer is obviously non-uniform, but let me tell you the adventure we had in designing how the fastest and most scalable deduplicating backup appliance in the world was going to do that.
When we started building the NEC HydraStor backup appliance as 9LivesData back in 2003, the obvious choice for storing data on disks was to use ext3 — the most popular Linux file system at that time. However, our subsequent struggles in ensuring best performance and storage utilization proved this approach suboptimal. It turned out that a standard POSIX file system might not be the best choice, as it provided a lot of unneeded functionality while not giving us some features which we really wanted. A few examples:
Other problems included bugs in asynchronous I/O (turning it silently into synchronous), lack of metadata integrity checks, spurious security features in a controlled single-user system and unnecessary directory structures, messing up with performance and requiring additional fsync calls. We also wanted to have the file system supply additional features like on-demand shreddingof user data, which is quite hard to achieve otherwise (at least without dirty hacks).
But essentially the most painful problem was the lack of control. We could only guess what was exactly going on beneath the file system interface and we couldn’t directly influence it. How could we implement a good resource manager of block device I/Os in this case? How could we keep track of fragmentation without incurring significant overheads?
Finally we decided to face our fate and push the limits of HydraStor’s performance and manageability by pursuing the most promising approach: replacing ext3 with our own file system layer, IMULA (derived from being optimized for immutable large files). Stay tuned to hear how this happened.
File systems tend to be large beasts. So, dropping unnecessary items from your backpack (in our case: directories, file names, timestamps, permissions and even precise file sizes) is crucial to succeed with limited budget and timespan. But it’s not enough. You need a good plan.
Our plan started by ensuring that we can successfully defeat the fragmentation issue and squeeze out big bandwidth increases from the underlying hardware. All in all the primary goal was to remove the performance bottleneck from disks, wasn’t it? So, we started by designing the block allocation algorithm.
How would you approach this? Note that a typical workload of HydraStor consisted mostly of:
First, you want to minimize fragmentation. So let’s allocate whole “extents” instead of single blocks and let’s strive for finding as long extents as possible. Great, but you can do more. You can try to keep subsequent extents close to each other — it will pay off during sequential reads. Interestingly, this is true also between file boundaries, because files written together are typically read together as well (at least in HydraStor). So, let’s leverage the trick of log-structured file systems and try to allocate each new extent right after the last allocated one.
So, with this approach we have two conflicting goals: large extents and extents close to their predecessors. How can both be reached? Let’s just use a few size thresholds to resolve this. Namely, we can first seek the closest extent of size at least 1024 blocks. If this fails, let’s try 256, then 32 and a single block at the end.
But wait! We forgot about an important optimization: keeping data as close to the beginning as possible. So let’s add an additional, first step to this algorithm: seeking the closest extent of 1024 blocks within some initial part of the drive. But what should “initial part” mean?
Imagine that we’d rewritten all data to the very beginning of the partition. Sounds feasible, right? So, why not pick the total occupied space as the “initial part” size? Well, this would bring fragmentation problems of nearly full partitions to that initial part. So, to avoid this, let’s just relax this amount to 120% of total occupied blocks.
Following this reasoning we prepared an allocation algorithm, which looked roughly as follows:
Then time for testing came. We tracked bandwidth of a few write patterns with this allocator and a prototypical implementation of user operations in hand. Here’s the chart showing relative comparison between IMULA (blue) and ext3 (red), each running on four HDDs. The graphs are scaled horizontally to the same timespan and have linear vertical scale:
As you can see, our first approach was from 2x through 4x times better than ext3, which is a quite good result, isn’t it?
But did it fragment space over time? To check this, we simulated filling the drive to 99% of capacity, then deleting random 5% of data chunks, refilling it back to 99% with new data, deleting 5% again and so on. How was our new shiny idea going to work under such stress? Let’s see:
This chart shows how the distribution of the number of extents per file evolved over these 20 rounds of deletion and filling. After 20 cycles our allocation algorithm kept usually around 4 extents per file. This amounted to the mean extent size of 3 MB. Considering the fact that this size was enough to amortize seek times in data access, our first idea proved to work quite well.
However, usually when you finish prototyping something and start productizing, many additional efforts pop out on the stage. See what this meant for us.
OK, so we had a block allocator, what else did we have to implement to have our “file system” up and running?
First, file metadata had to be kept somewhere permanently, so that they could be restored after a power outage. Second, the allocator algorithm needed to keep some map of free extents (which we called a free map). Also, both these structures needed some summary to be kept in a superblock, or rather two superblocks (for robustness against bad sectors and against power outages during updates). So we ended up with the following on-disk layout:
Ideally, free map could work in memory only (we were hoping for small fragmentation, weren’t we?), but the requirement to support a fast in-place upgrade from ext3 forced us to keep base block size at 4 KiB and be prepared for high initial fragmentation. Luckily, experiments showed that our new shiny allocator decreased high initial fragmentation over time, when fed with HydraStor’s typical workload. That was great news, but still we had to be prepared for an aged file layout and we had to write code evicting metadata to the disk, at least temporarily. So, even despite being able to rebuild the whole free map from sequentially read inode table, we still needed some “Free map file” on disk, at least for swap.
Knowing that we needed “pages” of free map, we divided the disk into “groups”. In order to keep free map file of constant size we decided to save group descriptions as bitmaps of free blocks (just like ext3 and a few other file systems do). This led to a natural group size of 128 MiB — as this is the size corresponding to the number of blocks covered by a single block of bits.
Fine, but note also that direct implementation of our allocator wouldn’t be too fast. We needed a quick operation for finding the closest free extent of some minimal size (within some initial region, falling back to the beginning if none found). Right. Running this on bitmaps could force us to iterate through the whole drive space.
We solved this problem by representing each group by an array of extents starting within it and by building an in-array tree upon these groups, with each node keeping the maximum free extent size within its subtree. Here’s what the typical operation on this tree looked like:
Each circle at the bottom represents the maximum free extent size within one group. Thanks to this approach, each free map traversal required no more than O(log G + g) CPU time (where g was the group size and G — the number of groups) and at most one I/O operation (to fetch a group from disk if needed). Note that we also made our structures quite memory-compact, which helped in keeping as much information in RAM as possible.
Here’s a picture summarizing the design of Free map:
Of course, a diligent reader would instantly spot a few other, minor problems to be solved, including: how extents spanning multiple groups should be handled, what cache eviction strategies should we use or how to optimize traversal of groups containing many free extents. We’ll not tell you in details how we solved each of them, so that you can have more fun tackling it yourself.
Now, let’s come to the next cornerstone of every file system: file metadata.
Well, as I said before, we didn’t need most of the metadata which file systems typically associate with files, like precise sizes (as they were tracked in a different place anyway), paths, directories, names or extended attributes. All we needed was a mapping from a 32-bit file ID to the inode, being a plain list of file’s extents. That was it.
The trick was that the mapping had to be persistent: not only after a clean reboot, but also after a power failure. This was the place where the assumed external journal came to help us.
For resistance against power outages we decided to issue each update in the “ID to extents” mapping immediately to the journal (as a whole new extents list, for simplicity). But in order not to abuse the hospitality of that journal, we avoided keeping enormous amounts of data in it eternally by dumping the updated inodes periodically to the Inode Table file.
The Inode Table file typically had multiple gigabytes on multi-terabyte drives. That was quite a lot to read on startup, so we decided to keep its actual contents in a compact way. We did it by dividing this file into fixed-size buckets and keeping the number of active buckets (that is: the ones we had to read on startup) minimal. Note that this division was not static like in case of Free map — every inode could be kept in any bucket. In fact, it could be kept in a few of them in different versions. By reading buckets in the right sequence we ensured that the most recent inode version was read as the last one. This was the sequence of populating buckets with data and it was stored in Inode Table’s header. Note also that stale versions of inodes, despite being kept in some potentially active buckets, didn’t contribute to their fill.
How could we keep the number of active buckets small? Well.. We had to reuse them whenever possible. So, in common case, when flushing updates from journal we just picked the bucket with the least fill and rewrote its contents together with the new data to a new bucket. To make it possible we only needed to ensure that some bucket was filled to at most half and that one active bucket was empty. This, in turn, could be done by simply setting the desired number of active buckets to a number capable of storing twice the amount of active data plus one. The set of active buckets was adjusted on the fly to the calculated desired number by simply choosing between 0, 1 or 2 buckets to be rewritten together with updates dumped from journal.
Here’s a chart showing the design of Inode Table. Note the duplicated header (for resiliency against power failure during updating one of them):
Of course, we didn’t forget about caching inodes in memory. We did it on two LRU lists (for clean and dirty inodes), so that access to them was quick in common case.
An interesting feature of this design was the ability of recovering some consistent (although not necessarily perfectly up to date) state of metadata of all files in case the journal, on which we relied, got corrupt or rewound.
We could continue this story with details of a few other nice features, like in-place upgrade from ext3, metadata integrity checksums, dumping all metadata to a tarball for easier support or built-in shredding support by tracking the region of disk to be shredded. Implementing all of this wastremendous fun and has not only increased the performance of our product significantly, but also allowed for easier behavior customization and further performance optimizations due to more control.
Admittedly, a few years after we implemented IMULA, mainstream filesystems like ext4 improved their performance as well and XFS even caught up with our results. However, they involve a lot more complexity and it’d still be quite tricky to tweak them to our workload in comparison to a fully controlled setting of our making. Not to mention implementing custom filesystem-layer features…
This approach has also been used by other systems. One of the most well-known is Ceph, which recently introduced its new storage back end “BlueStore” as a replacement for legacy “FileStore”.
If you, dear reader, also happen to build storage solutions, you might consider implementing your own file system layer as well. As you have seen, it is not as difficult as it may seem and in our case it brought a lot of performance and control improvements. We’d really like to see how it improves your product.
IMULA wouldn’t have been implemented so well without Marek Dopiera, Michał Wełnicki, Rafał Wijata, Krzysztof Lichota, Marcin Dobrzycki and Witold Kręcicki, who all contributed to this project, not only me. Great thanks to all of them!
Originally published in 9livesdata.com on October 2, 2018
empowerment through data, knowledge, and expertise.
132 
132 claps
132 
Written by
System Software Engineer, former leader of the NEC HydraStor Backend team @ 9livesdata
empowerment through data, knowledge, and expertise. subscribe to DDIntel at https://ddintel.datadriveninvestor.com
Written by
System Software Engineer, former leader of the NEC HydraStor Backend team @ 9livesdata
empowerment through data, knowledge, and expertise. subscribe to DDIntel at https://ddintel.datadriveninvestor.com
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
