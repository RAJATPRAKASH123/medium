story_url,bodyText
https://towardsdatascience.com/running-jupyter-notebook-in-google-cloud-platform-in-15-min-61e16da34d52?source=search_post---------0,"Sign in
What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Top highlight
Amulya Aankul
Sep 8, 2017·5 min read
Recently, while I was doing my research project on Computer Vision using Convolutional Neural Network, I found out that my 8GB RAM laptop is useless. It took me an hour to learn from just 1 epoch. Therefore, rather than spending 1500$ on a new GPU based laptop, I did it for free on Google Cloud. (Google Cloud gives 300$ credit, and I have 3 gmail accounts and 3 credit cards :D)
So lets not waste anymore time and move straight to running jupyter notebook in GCP.
For this step, you will have to put your payment information and verify your account. It’s the most simple step. If you fail this step, close your laptop and think where you are going in life.
Click on the three dots shown in the image below and then click on the + sign to create a new project.
Click on the three lines on the upper left corner, then on the compute option, click on ‘Compute Engine’
Now click on ‘Create new instance’. Name your instance, select zone as ‘ us-west1-b’. Choose your ‘machine type’. (I chose 8v CPUs).
Select your boot disk as ‘Ubuntu 16.04 LTS’. Under the firewall options tick both ‘http’ and ‘https’ (very important). Then, choose the disk tab and untick ‘ Delete boot disk when instance is deleted’.
If you click on ‘customize’, you will be able to find options for using GPUs. You can choose between 2 NVIDIA GPUs.
Some firewall settings:-
Now click on ‘Create’ and your instance is ready!
Your new VM instance should look something like this. Note down the External IP.
IMPORTANT : DON’T FORGET TO STOP YOUR GPU INSTANCE AFTER YOU ARE DONE BY CLICKING ON THE THREE DOTS ON THE IMAGE ABOVE AND SELECTING STOP. OTHERWISE GCP WILL KEEP CHARGING YOU ON AN HOURLY BASIS.
By default, the external IP address is dynamic and we need to make it static to make our life easier. Click on the three horizontal lines on top left and then under networking, click on VPC network and then External IP addresses.
Change the type from Ephemeral to Static.
Now, click on the ‘Firewall rules’ setting under Networking.
Click on ‘Create Firewall Rules’ and refer the below image:
Under protocols and ports you can choose any port. I have chosen tcp:5000 as my port number. Now click on the save button.
Now start your VM instance. When you see the green tick click on SSH. This will open a command window and now you are inside the VM.
In your SSH terminal, enter:
and follow the on-screen instructions. The defaults usually work fine, but answer yes to the last question about prepending the install location to PATH:
To make use of Anaconda right away, source your bashrc:
Now, install other softwares :
Open up a SSH session to your VM. Check if you have a Jupyter configuration file:
If it doesn’t exist, create one:
We’re going to add a few lines to your Jupyter configuration file; the file is plain text so, you can do this via your favorite editor (e.g., vim, emacs). Make sure you replace the port number with the one you allowed firewall access to in step 5.
It should look something like this :
To run the jupyter notebook, just type the following command in the ssh window you are in :
Once you run the command, it should show something like this:
Now to launch your jupyter notebook, just type the following in your browser:
where, external ip address is the ip address which we made static and port number is the one which we allowed firewall access to.
Congratulations! You successfully installed jupyter notebook on GCP!
Lets connect : https://www.linkedin.com/in/aankul
Follow me on medium: https://medium.com/@aankul.a
Edit :All those facing ‘ERR_CONNECTION_TIMED_OUT’ error. Please try Bastardized Eloquence’s solution in the comments.
Data Engineer at Amazon
See all (170)
5.2K 
80
Some rights reserved

Every Thursday, the Variable delivers the very best of Towards Data Science: from hands-on tutorials and cutting-edge research to original features you don't want to miss. Take a look.
5.2K claps
5.2K 
80
Your home for data science. A Medium publication sharing concepts, ideas and codes.
About
Write
Help
Legal
Get the Medium app
"
https://lugassy.net/why-we-moved-from-amazon-web-services-to-google-cloud-platform-726c412fd667?source=search_post---------1,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Top highlight
TL;DR: AWS’s awesome, but Google is Googol awesome.
Participate in any AWS re:Invent conference ($1,600 admission) or follow head evangelist Jeff Barr and you’ll fall in love instantly with Amazon Web Services.
100s of new features every year and an all-you-can-eat, elastic, no-ops bouffe of on-demand services. Well, until you get to actually taste the food…
Amazon’s awesome, but Google Cloud is built by developers, for developers, and you see it right away.
GAE just works, has auto-scaling, load-balancers and free memcache all built in. Want to connect to Cloud SQL? use the virtual linux socket /cloudsql. Want to write custom logs and see them instantly? Append /var/log/app_engine/custom_logs or simply console.log() and console.error(). Want to profile and debug your application IN PRODUCTION? put breaking points in StackDriver or SSH to the managed instance.
Under the hood you’ll see GAE is 100% docker. Use it to run your 20 microservices with *.appspot.com service discovery or run one mamooth application at scale.
Update: I’ve received horror stories about what it was like with GAE at the early days, especially with the built-in datastore. I had no experience with GAE or Google Cloud back then so can’t really comment. As before any blind date, be prepared to leave anytime. Code a wrapper around your cache, data and message stores so you can switch technologies/providers.
Google lets you create custom machine types with any cpu/memory configuration. They let you opt for cheaper, preemptible (a-la spot) instances with a single click and no bidding/auction code whatsoever.
Google connects each and every VM to its super-fast, low-latency networking. Amazon requires you to buy expensive 10G-capable instances and/or enable enhanced networking.
Google lets you set up simple Firewall rules. Amazon gives you VPC, security groups, network access control lists and a big, fat headache.
Update: Some have commented AWS VPC is great and lets you tightly secure instances, create sandboxes and control internal networking. I still found it confusing since I rarely need anything beside basic ports/home IP rules. However if you’d like to investigate every connection issue like a murder mystery go ahead.
Google bills by the minute (not hour) and apply AUTOMATIC DISCOUNTS for long-running workloads, with absolutely no reserved pricing nonsense (warning: AWS EC2 pricing page might crash your browser).
Want to run a message bus? AWS will make your head spin with SNS, SQS, Kinesis, Kinesis Streams and Kinesis Firehose. GCP has only Pub/Sub which just works and is insanely scalable.
Update: I realize SQS (~Hosted RabbitMQ?) and Kinesis (~Hosted Kafka?) are two different buses, but getting GCP to work with one messaging product regardless of volume/velocity sounds better for me.
Google BigQuery is nicely priced by the GB stored and TB queried, has day partitioner built-in, 50% reduction in price for unmodified partitions (so you can keep data for longer) and full SQL support.
Google DataFlow is an amazing framework for consuming and processing data in batch or streams, with windowing, automatic triggering/speculative data and easy to use transformations.
Update: AWS started adding “Streams” to each service and spun them as new products, further increasing confusion. Dataflow is easier to comprehend as it treats all I/O targets as either sources and/or sinks.
Amazon has one of the most confusing IAM. While it is nice to set up a role to only allows usage for a particular resource from a specific device and times of day, you end up spending most of your time debugging policies.
Google security is more leaned back, assumes all resources are allowed within each trusted “project”.
Moreover, people you invite to projects must have a Google Account which are secure by default and usually already set up.
Update: Apparently you can have a Google Account with any email address, but if you are like me and using Google for Work (Google Apps) for SSO, you are already set. Amazon also supports MFA but you have to create a new set of users.
We moved to GCP because we wanted to work on infrastructure that runs YouTube, Gmail and Google Analytics. We moved because Google is fair, much more tech-savvy and launch products that just works.
Update: It is unclear if Google (the search engine, youtube, gmail) is using GCP. It has certainly created it and happily released it as they are working internally on the next big thing.
AWS is still fantastic. I just hope they would close issues before releasing new, half-baked features in time for re:invent.
Someone wrote: “I guess nobody ever got fired for using AWS. It’s the IBM of the cloud” and I couldn’t agree more, but if you are your own boss and like to look further, go with GCP.
Your mileage may vary.
Follow me here or on Twitter as I plan to write deeper posts battling specific products (i.e Pub/Sub vs. Kinesis, Google CDN vs. CloudFront).
Updated based on some love and hate from HackerNews.
Thoughts about Startups, Development & Ops by Michael…
636 
20
636 claps
636 
20
Written by
Co-Founder, HashUnited.com. Built 7 other companies, sold 3, worked with dozens more. Writing about startups, dev & ops.
Thoughts about Startups, Development & Ops by Michael Lugassy
Written by
Co-Founder, HashUnited.com. Built 7 other companies, sold 3, worked with dozens more. Writing about startups, dev & ops.
Thoughts about Startups, Development & Ops by Michael Lugassy
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/what-are-the-google-cloud-platform-gcp-services-285f1988957a?source=search_post---------2,"There are currently no responses for this story.
Be the first to respond.
See Also: What is Google’s Cloud Platform (described using on the 1,000 most commonly used words)?
Google Cloud Platform (GCP) offers dozens of IaaS, PaaS, and SaaS services. Sadly, the Wikipedia entry for GCP is garbage, and while the official docs are pretty good, the marketing-dust sprinkled on them gives me a toothache. For my own reference, I pulled together an objective description for each of the services available on GCP.
Big hat-tip to Greg Wilson, the Google Cloud Developer Relations team, and the Google Cloud Developer Experts for the 4-word descriptions.
Something missing or wrong? Add a comment!
Google Cloud community articles and blogs
774 
6
774 claps
774 
6
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Developer Advocate @ Google, software engineer, and author of “Professional Android” series from Wrox. All opinions are my own.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://towardsdatascience.com/machine-learning-with-tensorflow-on-google-cloud-platform-code-samples-7c1bc07cd265?source=search_post---------3,"Sign in
There are currently no responses for this story.
Be the first to respond.
Lak Lakshmanan
Feb 24, 2018·4 min read
Over the past few months, my team has been working on creating two 5-course specializations on Coursera called “Machine Learning on Google Cloud Platform” and “Advanced Machine Learning on GCP”. The full 10-course journey will take you from a strategic overview of why ML matters all the way to building custom sequence models and recommendation engines.
These courses provide an interactive, practical, pragmatic way to get started doing ML quickly and effectively. While there are many theoretical machine learning courses, my goal with this specialization is to provide practical training, so that you can hit-the-ground running. In order for you to get that jump start, the courses come with lots of open-source, example TensorFlow applications that you can take and train/deploy immediately.
The first course in the Machine Learning on GCP series. How Google Does ML, is now live on Coursera. Please go take the course, and take the remaining courses as they show up once every few weeks.
But even as you are waiting for the awesome team of presenters to teach the courses, the source code for the labs in the specialization is already available. In this blog post, I’ll list what is available in each folder of the GitHub repo:
data_analysis.ipynb shows you how to do data analysis on huge datasets:
mlapis.ipynb shows you how to invoke pre-trained ML models:
repeatable_splitting.ipynb illustrates the importance of repeatable splitting of data
create_datasets.ipynb shows you how to explore and create datasets using Pandas and BigQuery
a_tfstart.ipynb shows you how to use TensorFlow as a numeric software package
b_estimator.ipynb shows you how to write simple ML models in TensorFlow
c_batched.ipynb shows you how to work with large datasets in TensorFlow
d_traineval.ipynb shows you how to do distributed training with TensorFlow
debug_demo.ipynb shows you how to debug TensorFlow programs
e_cloudmle.ipynb shows you how to deploy TensorFlow models and predict with them in a serverless way, with Cloud ML Engine
a_features.ipynb illustrates the importance of representing features correctly
dataflow shows you how to use Apache Beam on Cloud Dataflow for preprocessing
taxifeateng shows you how to implement feature engineering in TensorFlow models
a_handtuning.ipynb shows you how to change a variety of parameters associated with TensorFlow models to gain greater accuracy
b_hyperparam.ipynb shows you how to autotune your TensorFlow models on Cloud ML Engine so that you don’t have do hand-tuning.
c_neuralnetwork.ipynb shows you how to train and predict with distributed neural network models in TensorFlow the easy way.
d_customestimator.ipynb shows you how to take a model that you find in a paper and implement in a way that is distributed and scaleable.
This set of notebooks:
1_explore.ipynb shows you how to explore data using Pandas and BigQuery
2_sample.ipynb shows you how to repeatably split the data
3_tensorflow.ipynb shows you how to build an Estimator model on the data
4_preproc.ipynb shows you how to preprocess data at scale using Dataflow
4_preproc_tft.ipynb shows you how to preprocess data at scale using TF.Transform
5_train.ipynb shows you how to train a TensorFlow model on Cloud ML Engine
6_deploy.ipynb shows you how to deploy a trained model to Cloud ML Engine
serving shows you how to access the ML predictions from web applications and from data pipelines.
Together the above 7 labs summarize the lessons of the previous six courses on a realistic problem, taking you from data exploration to deployment and prediction.
No labs with this one — this is about design and architecture considerations around ML models.
mnist_estimator.ipynb shows you how to build an image classifier using the Estimator API
mnist_models.ipynb shows you how to build a custom estimator with all the tricks (convolutional layers, augmentation, batch normalization, etc.) that go into a good image classification model
flowers_fromscratch.ipynb shows you how to apply the image model in the previous notebook to “real” images.
Labs on Transfer learning and AutoML are not in GitHub since they don’t involve any coding — just point & click!
sinewaves.ipynb show you how to build a time-series forecasting model in TensorFlow using a variety of techniques including CNNs and LSTMs.
temperature.ipynb illustrates how hard LSTMs can be to get right
txtcls1.ipynb shows you how to build a from-scratch text classification model using a variety of techniques including CNNs and LSTMs
txtcls2.ipynb shows you how to use pretrained word embeddings in a text classification model.
word2vec.ipynb shows you how to create a word embedding from your own dataset.
poetry.ipynb shows you how to use Tensor2Tensor to solve your own text problems, whether it is text summarization or text generation
content.ipynb shows an example of a content-based recommendation system
wals.ipynb shows you how to build a collaborative filtering recommendation system in TensorFlow
wals_tft.ipynb makes the collaborative filtering model production-ready by adding in a tf.transform pipeline to map unique user-ids and item-ids automatically.
As ML matures, there will be more labs, of course, and some of the labs above may prove unnecessary. The GitHub repo is a living repository and we plan to keep it up-to-date and reflective of recommended TensorFlow practice. So, if you are building your own ML models, the notebooks (and their corresponding model directories) are a great starting point.
Happy exploring!
Data Analytics & AI @ Google Cloud
826 
7
826 
826 
7
Your home for data science. A Medium publication sharing concepts, ideas and codes.
"
https://medium.com/google-cloud/serverless-on-google-cloud-platform-an-introduction-with-serverless-store-demo-41992dec085?source=search_post---------4,"There are currently no responses for this story.
Be the first to respond.
This document is the opening piece of the Serverless on Google Cloud Platform: an Introduction with Serverless Store Demo How-to Guide. It discusses briefly serverless computing and its patterns in the context of Serverless Store, a web e-commerce demo app for showcasing serverless products and services on Google Cloud Platform. Serverless Store is not an official Google product.
Serverless Store is featured in Google Cloud Global Digital Conference 2019. View the keynote and breakout recordings to learn more.
Deploying a service used to be a tremendous commitment: to get everything up and running, developers and operators have to build a server, setup OS and network, install a variety of dependencies and prepare for years (if not longer) of upgrade and maintenance ahead. Nowadays many developers choose to use virtual machines (VMs) provided by Google, Amazon, and other cloud service providers instead, leveraging their experience in hardware and networking for better, more secure and more reliable service deployments. Nonetheless, VMs are still servers; the fact that they are running on the cloud in a virtualized form does not eliminate heavy server management workload. Additionally, VM deployments require careful capacity planning and they are charged per-second; in other words, every idling core and unit of RAM are a waste of budget.
Serverless computing promises a pay-as-you-go future with (almost) no server management at all. Serverless platforms take the code from developers and perform all the deployment tasks (networking, dependencies, maintenance, etc.) automatically behind the scenes. The greatest advantage serverless computing provides is that the deployment scales itself without additional configuration; your apps will always have exactly what they need computationally (within an understandable margin of error, of course) when running.
This is not saying that serverless computing is an ideal solution for every use case. Going serverless implies that you trust Google Cloud Platform, AWS, or other cloud service providers to manage a large portion of your computing stack for you and expect them to perform as you hope, which may not be the case in some scenarios. More importantly, the architecture of your app has a great impact on how well serverless computing keeps its no-server-management easily-scalable promise: for example, if you use a VM-based database backend solution, such as a Cloud SQL instance, with your serverless function deployment, its scalability will be heavily limited by the performance of the SQL instance, and unexpected errors may occur if you are not careful enough.
The ever growing popularity of AWS Lambda and Google Cloud Functions leads many to believe that FaaS (Function as a Service) is a synonym for serverless computing. FaaS platforms take a function from developers, build it into an app, and deploy it in the cloud; it advocates a quite special pattern for app development, where an app is broken down into a collection of functions, grouped by a (managed) gateway, and accessed by users via static HTML files served online:
Effectively transforming the backend into an API service, this signature architecture enjoys all the benefits of serverless computing (little to none server management, scalability, low cost), and enables team collaboration on a different level (UI design, for instance, is now decoupled; team members may each work on their own functions as opposed to a full app). Many developers have successfully adopted the pattern and created compelling experiences with minimal amount of effort.
But there is a catch. This distinct design is, as with serverless computing in general, not perfect for all use cases, and bears the risk of gruesome fragmentation: without careful coordination developers may end up with a large number of functions difficult to assess, monitor, maintain, orchestrate, and keep a complete picture of, in many ways akin to a 100,000-piece jigsaw puzzle. The architecture is also fairly new; it takes some time for developers and teams to adapt to, and they may need to overhaul their workflows to accommodate.
Serverless and FaaS are not necessarily the same, and it would be dangerous to force a function mindset. It is recommended that developers experiment with serverless solutions and integrate them into their apps to the extent they are most comfortable with. For teams with a comprehensive knowledge of serverless computing, it might be OK to build a service from ground up using nothing but functions; those who are interested in but not yet very comfortable with serverless computing should, however, consider taking a hybrid approach and migrating some select compatible workload to serverless platforms first. Serverless is, after all, a flexible misnomer.
Generally speaking, code in an app is executed sequentially. The sequence (or execution path) is essentially a contract crystalized in the deployment, triggered by an input (request) and finishes with an output (response). The input, output, and the sequence are bundled inseparably together in the contract; once deployed, it cannot be modified. Developers will have to modify the code and re-deploy.
Event-driven computing plans to break the contract and free all the parts involved in the sequence. Parts now emit an event (a piece of message with execution contexts) at the time of completion; they cares little about what happens next, and leaves the following step at the discretion of whatever delivers the event, which, for cloud-native applications, is usually a data/event streaming solution, such as Google Cloud Pub/Sub and Apache Kafka. Parts that send events are event publishers, and those that receive events become event subscribers.
This publisher/subscriber pattern grants greater development freedom and easier team collaboration. It is now possible to hot plug/swap blocks of code without redeployment, and developers can easily add multiple subscribers to the same event stream, creating a fan-out structure. One of the most prominent use cases of such structures is real-time data analytics:
Traditionally, data analysts process data in batch, usually via auto-generated log files. The application executes a sequence, writes the details to logging agents (as a step of the sequence), and data analytics team extracts insight from them, with an understandable delay. In event-driven systems, however, once connected to the event stream as one of the subscribers, data analytics team can get the data they need in real time without interrupting normal operations. If connected to real-time data processing and warehousing solutions such as Google Cloud Dataflow and Google BigQuery, developers can have analysis done in real-time as well.
The data/event streaming solutions themselves also offer great help in application development. You can set up Google Cloud Pub/Sub, for example, to reattempt delivery automatically when one of the subscribers is experiencing temporary technical difficulties. Cloud Pub/Sub also supports snapshot, enabling developers to go back in time and replay events, which can be particularly helpful when testing new pieces of code.
Event-driven computing works particularly well with serverless computing. Events are natural triggers for serverless deployments; more importantly, the (almost) infinite scalability of serverless computing platforms allows message queues to pass events around as fast as possible, saving the trouble of capacity planning commonly witnessed in VM-based deployments.
Breaking the contract of sequential code execution has some serious complications, with the most important being the dissociation of inputs (requests) and outputs (responses). In the world of event-driven computing, requests and responses arrive in two separate dimensions; whoever sends the request are not naturally guaranteed a response. For asynchronous operations it is usually fine: when people post a new photo in their social feeds they usually do not expect it to show up in the feeds of their friends immediately; quick feedback is appreciated but not required. Synchronous operations, however, are a different story. When people open a website, they expect the page to load as quickly as possible; no other actions can be performed until the contents show up. Event-driven computing, unfortunately, does not work well in this scenario, as the request is considered served when the event is published; developers have to manually retrieve the response.
There are many solutions that address this issue, though none of them is perfect. Common strategies include using statically generated pages throughout the app, polling the system for results right after event publishing, and setting up dedicated asynchronous routines for synchronous operations when required (for example, when people request a webpage, prepare it synchronously first, then publish an event to the message queue so that event subscribers can track the action). Once again, it is up to developers themselves to decide how far they would like to go with event-driven computing; for some apps it may be a heavenly solution.
As introduced earlier, Serverless Store is an e-commerce app designed to showcase serverless products and services on Google Cloud Platform. Note that it is for demonstration purposes only and does not necessarily reflect best practices in the app development.
Serverless Store features a fully serverless architecture. It runs on two Google provided serverless computing platforms, Google App Engine and Google Cloud Functions, with all of its components, storage, data analytics, machine learning/AI, etc., supported by managed services. There are no VMs involved at all. The app is scalable, and you pay only what the app uses.
Serverless Store consists of both app and functions. It is a hybrid, featuring a standard (traditional, if you may) Python flask web app as the centerpiece, with many critical and supplementary features served by Cloud Functions. For people who prefer the FaaS pattern, with a relatively small amount of effort it is possible to make Serverless Store purely function-based; you can also easily revert it back to a web app with no Cloud Function workers at all, ready for VM deployment.
Additionally, Serverless Store adopts event-driven computing for all asynchronous operations in the app. Synchronous operations, such as listing products, are still performed in a sequential manner. It is hoped that this approach can help interested developers ease into event-driven computing, a fairly new pattern, and better discover its benefits and limitations.
You can view live demos of Serverless Store in these keynotes.
Download the project here.
Guides below introduces briefly each product and service used in the Serverless Store demo app and explains how they are integrated in the app. Follow them to set up Serverless Store:
See Further Discussions for some tips and notes about Serverless Store.
Watch Google Global Digital Conference 2019.
Google Cloud community articles and blogs
734 
3
734 claps
734 
3
Written by
Developer Relations @ Google Cloud Platform
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Developer Relations @ Google Cloud Platform
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/linux-gui-on-the-google-cloud-platform-800719ab27c5?source=search_post---------5,"There are currently no responses for this story.
Be the first to respond.
Top highlight
Having servers on the cloud is great: you can access them from anywhere and at anytime! You can easily ssh into your server and do whatever you want, right?
Well, what if you want to browse the web? I want to use Chrome! Or you could use Lynx … but I’ve heard it’s not CSS3 compatible. This is a little tutorial that will take you through all the steps to have GUI access to a Google Compute Engine instance.
Important: if you start an instance you’ll be charged per minute. Go to the end of this post to see how to stop it and get $300 in credits!
Update: My next post will discuss how to make this connection secure by using VNC over VPN.
Visit the developers console, log in, and create a project if needed clicking on the Create Project button. Navigate the left menu to list the VM instances running on your project: Compute > Compute Engine > VM instances. If it’s the first time you do this for this project it might take a bit longer, since it’s setting some stuff up, don’t worry this happens only once.
Click on the Create Instance button to access the instance creation form. Choose a name for your instance. Any instance type and linux distribution would work, but if you want to go with something safe choose n1-standard-1 with backports-debian-7-wheezy-v20150423.
Choose a zone close to you to have the best latency in your connection.
If you’d like to use Windows the instances already come with support for RDP (Remote Desktop Protocol) so you don’t need any extra steps.
Once your instance is running you can SSH directly into it by simply clicking the SSH button. This also handles the authentication for you, really handy!
Once connected let’s update our sources list and install some extra packages:
Before we continue configuring the VNC server that will allow us to access our instance through a desktop environment we should install one. You can install your favorite one:
a) If you like Gnome and are not in a hurry:
Gnome is beautiful … and heavy! So be patient while everything gets installed, two to five minutes is completely normal.
b) If you prefer something faster to install you might like Xfce:
Now that our instance has a desktop environment let’s make it accessible via VNC. Start the vncserver, and follow the directions to create a password
Note: this password will grant access to your instance, so make it strong.
If everything went fine your VNC server is now running and listening on port 5901. You can verify this with netcat from the Google Compute Engine instance:
There’s many options available, my favorite one is RealVNC Viewer. Install one but don’t try to connect to your server just yet: it will fail as the firewall rules don’t allow it.
In order to communicate with our instance we need its external IP. You can find it on the Developers Console.
Let’s try to connect to it using netcat again:
Regardless of the tool you use the connection will fail, this is expected as the firewall rules block all communications by default for security reasons.Let’s fix that.
Navigate to the configuration for the default network “Compute > Compute Engine > Network” and then click on default. Or you could also click here and choose your project.
We’re going to add a new firewall rule, pressing the corresponding button.
Choose a descriptive name for the rule.
We will allow traffic coming from any source, which is why we use 0.0.0.0/0, the IP mask equivalent to a wildcard.
The traffic will be on the port 5901 for protocol TCP and going to instances tagged as vnc-server.
The last step is to tag our instance as a vnc-server, for that go back to the VM description page and click on “add tags”
Let’s first of all make sure that the connection is now allowed by the firewall:
Great! Everything seems ready for our VNC client to connect. Open your VNC viewer and connect to the IP of your Compute Engine instance on port 5901.
To connect you’ll need to provide the password you gave at the beginning of this tutorial.
And voilà! You can now use your favorite Desktop environment on your Google Compute Engine instance.
If you still cannot connect to VNC after you have created a firewall rule you should make sure that your IP has not been banned by sshguard.
To see if this is the case you can run:
If your output differs from this one flush the table and retry:
An instance running on the cloud has a cost but the good news is that you can simply stop it and restart whenever you need it again. Click on the Stop button and you’ll be charged only for the associated disk which at the moment of writing of this article is 40¢ per month. I dare you finding a cheaper cup of coffee in San Francisco!
Finally, if you’re new to the Google Cloud Platform make sure to get into the Free Trial to access $300 in credit so you can try it out and have some fun!
I hope this was useful. Feel free to add any comments for feedback or questions on twitter.
Google Cloud community articles and blogs
1K 
41
Some rights reserved

1K claps
1K 
41
Written by
VP of Product at Dgraph 🏳️‍🌈 Catalan
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
VP of Product at Dgraph 🏳️‍🌈 Catalan
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-developers/5-aha-moments-with-the-google-cloud-platform-14b44b7ecdc3?source=search_post---------6,"There are currently no responses for this story.
Be the first to respond.
Posted by: Jerome Poudevigne, Startup Architect, Google Cloud
Throughout the past couple of years, I have helped a good number of companies, big and small, migrate their systems to the Google Cloud Platform (aka GCP). During the course of these migrations, there are always a few of those moments where people look at a specific Google Cloud feature and say, “now, that’s cool!”.
More often than not, it is because, coming from other platforms, they have gotten used to some features requiring multiple steps, or some operations being complicated, etc. And often they find out that in GCP you can do this specific operation in a couple of clicks, or by setting up a simple text-based configuration. Then you see that light bulb turning on in their head, and there you go… happy customer.
A few of these happen so often that I compiled them in a list to share with others who might also benefit from these “aha!” moments. You could say these are the five things I wish they told me when I started using Google Cloud.
A project is a namespace where resources live. Every resource you instantiate in GCP, from load balancers to Kubernetes clusters to virtual machines, belongs to a single project, and has no access (by default) to resources in other projects. User roles and authorisations can be defined per-project and trickle down to everything in it. This has two immediate benefits: you can group things that belong together in neat logical units, and things that don’t belong together are isolated from each other (and isolation is a Good Thing)
This is powerful and quite simple, but it often takes new users off-guard. I’ve had many clients call me and ask me “How can I make sure my developers cannot access the production machines? What’s the best way to create access policies? ”
The answer to this is actually super-simple:
Every machine/other resource in the production project won’t be accessible to developers.
Of course there is a lot more to it, and you can refine roles and permissions to a much greater degree using Organizations, Folders, etc. Not to mention all the crazy things you can do with per-project billing. But at least you can say “hey, if it’s a machine in the staging environment then it can be found in the “staging” project”.
Imagine you are using a Cloud provider and that you have servers in the US, and servers in Singapore, and that they need to communicate.
So you create a VPC (Virtual Private Cloud) network in the US data center, another one in the Singapore data center, and then you will connect them by setting up inter-region VPC peering or a VPN (Virtual Private Network) or a transit VPC or other routing magic.
Lots of work, right? And many moving parts, so lots of opportunities for things to break.
With GCP, however, what makes my clients go “aha!” is when they realize that in GCP a single VPC network covers the entire planet. Only subnets are attached to a geographic location, and virtual machines communicate between subnets on private IPs (good old RFC1918 addresses) — no extra routing needed.
So, to make your server communicate across continents on GCP, here are the steps:
That’s all there is to it. Your VPC network spanning 2 continents is ready to use. Below is a screenshot of how it looks on my account, for a VPC network called ‘my-global-network’ with 2 subnets. The first column (“us-central1” and “asia-southeast1”) contains the name of the GCP regions (read: data centers). The second column is the subnet name that I picked when I created them.
A machine in the US (on the “us-central” subnet) with IP 10.0.0.5 can communicate directly with a machine in Singapore (on the “singapore”) subnet with IP 10.10.0.8.
Nothing else to set up.
And thanks to the way these networks work, the Google Cloud Load Balancer can present a single IP to the world, and forward traffic to the instances that are the closest to you geographically without having to setup a tedious DNS-based load balancing. But that’s worth an entire blog. I’ll save it for another day.
There is no network security without a firewall so unsurprisingly GCP comes with one built-in.
Now, I don’t know about you, but nothing makes my brain hurt like a list of firewall rules displaying IP ranges and addresses and ‘Allow/Deny’ directives. It looks a bit like this:
If you imagine a normal network with a few dozen (hundred?) servers, you can quickly see how this can get out of control. You’d better have a solid printout of your network layout to refer to when you start adding and changing rules. And good luck debugging things!
Wouldn’t it be nice if, instead, you could just tell the firewall: “the HTTP traffic from outside can only reach the HTTP servers and the MySQL database is only reachable by the HTTP server(s) on the same network?”
Turns out it’s pretty simple on GCP by using a little thing called network tags. As the documentation says:
“Network tags are text attributes you can add to Compute Engine virtual machine (VM) instances. Tags allow you to make firewall rules and routes applicable to specific VM instances.”
So let’s see how it works. Firewall rules in GCP are defined in terms of source and target (the traffic flows from the source to the target). You can define filtering rules that apply to the source or the target, and in both cases you can use tags.
This is simpler shown with an example. The rule below states that on the default network, the traffic to the VMs with the tag mysql-server can come from the VMs with the tag http-appserver. Any other traffic is “Deny”-ed by default.
All you have to do is to tag your machines properly, and they will automatically be covered by the rule. You don’t need to enter their IP range.
That’s neat if you ask me. It makes it a lot simpler to grasp what’s happening.
Of course, there’s a TON more to firewalls in GCP. Tags also apply to routes and you can mix and match IP-based rules with tag-based rules. Not to mention that thing called service accounts, but I’ll leave those for another day.
The bottom line is that you can create most rules by just expressing a business need and not having to remember complicated network layouts. I have no hard stats, but I’m pretty sure this has saved me hours of work.
Easily access virtual machines (VMs) from the Google Cloud console was one of my first “aha!” moments when I started using GCP.
This is a screen capture of my Google Cloud console, with a virtual machine and its internal IP.
The last column has a header that says “Connect” and when you click on the word “SSH” a separate windows pops up. You wait for a few seconds, and… this is what you get. Your personal shell access — in a browser popup no less.
You are connected through ssh to the virtual machine of your choice. You did not have to download ssh keys and put them in the ~/.ssh directory, do the correct chmod command and run a long-winded ssh -i ~/.ssh/somekey me@<it-took-me-forever-to-copy-paste-the-address-here>
In addition, you have access to a few nifty features such as uploading and downloading files, changing the user etc. Just use the menu behind the cog icon at the top right.
In truth, you should not need to connect directly that often, but when you have to, this is a godsend.
The Google Cloud console has a cool trick: you can actually connect to a virtual environment that is managed by the Google Cloud console itself. It serves a bit as a jump host. You can access most resources from the projects from it, and you can activate it directly from the top menu with, no particular setup on your side. It’s called the Cloud Shell.
This is how it looks at the top right of the console:
When you activate the Cloud Shell, the session opens at the bottom of the console. You get a command line prompt and it’s fully configured with the gcloud command line tool (the jack-of-all-trades of Google Cloud scripting).
You can do a great many things from there, and this even includes uploading and downloading files, editing code or deploying it, a web preview for your AppEngine application, and more.
So you can get access to a fully configured shell environment in your project from any laptop where you can connect with your credentials. On top of this, it persists between connections so you can fine-tune it to your needs and have these changes available the next time you re-connect.
This has saved me many times during my previous life as a traveling consultant!
Did I say 5 “Aha!” moments ? Well, you’ve been patient reading all the way to here, so here’s one more for free.
Google Cloud has an amazing way to literally “teleport” a running virtual machine between physical hosts without stopping it. It’s called Live Migration. It allows Google to move your virtual machine away from a defective host, or a host that needs a patch or an upgrade, or for any other infrastructure related reason.
It’s all done in the background, and is totally transparent, so you never really see it happening. Unless you look VERY closely. I once did a demo to a client, where a machine was live migrated while he was simulating a solid network load — and we did not lose a single packet, with no noticeable degradation in latency.
So there you go. These are 5+1 things that made me go “Aha!” when I became more familiar with the Google Cloud Platform, and that still make my clients do the same.
There is a lot of depth to the platform, and my examples above only scratch the surface of our features. I encourage you to try it yourself. There is a generous free tier, and when you are ready to take the plunge and create that new company, please contact us at Google Cloud for Startups. We’ll get you up and running in no time.
Jerome is a Startup Architect at Google Cloud. Based in Singapore, he helps startups make the most of the Google Cloud Platform.
Engineering and technology articles for developers, written…
668 
3
668 claps
668 
3
Engineering and technology articles for developers, written and curated by Googlers. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
News about and from Google developers.
Engineering and technology articles for developers, written and curated by Googlers. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/uploading-resizing-and-serving-images-with-google-cloud-platform-ca9631a2c556?source=search_post---------7,"There are currently no responses for this story.
Be the first to respond.
Top highlight
More and more applications today require their users to upload photos. From simple things like updating a profile photo, to more complicated services like Snapchat where you share a bunch of them. These applications are running on all kind of devices with different resolutions and at different network conditions. In order to do it right we have to deliver the images as fast as possible, with the best available quality, taking into account the targeted device and screen resolution.
One option is to create an image service yourself, where you can upload the images, store them and possibly resize them in few different sizes. Unfortunately, doing so is usually very costly in terms of CPU, storage, bandwidth and can end up very pricy. It is also quite a complicated task and many things can go wrong.
By using Google App Engine and Google Cloud Storage though, you can easily achieve this seemingly difficult task by using their API. Start by completing a simple tutorial on how to upload files into the cloud and read the rest if you want to see it in action to understand why it’s one of the coolest things ever.
App Engine API has a very useful function to extract a magic URL for serving the images when uploaded into the Cloud Storage:
get_serving_url()
Returns a URL that serves the image in a format that allows dynamic resizing and cropping, so you don’t need to store different image sizes on the server. Images are served with low latency from a highly optimized, cookieless infrastructure.
In practice it’s the same infrastructure that Google is using for their own services like Google Photos. The magic URLs usually have the following form: http://lh3.googleusercontent.com/93u...DQg
If all that wasn’t enough for what’s coming out of the box, there are also no charges for resizing the images and caching them when using that Google’s magic URL. Yes, you read it correctly, this is a free of charge service and you pay only for the actual storage of the original image.
You don’t have to worry about anything when it comes to serving images for your next big idea today. All you need to do is to upload your images once, extract the magic URL and then use it directly on the client-side by updating the arguments depending on the environment. Prototyping applications similar to Snapchat or even more complicated ones could be implemented over a weekend.
Besides the already mentioned tutorial from the documentation, you can play with a live example on gae-init-upload (which is based on the open source project gae-init).
The photo in the example is taken by Aleksandra Kiebdoj.
Google Cloud community articles and blogs
713 
20
713 claps
713 
20
Written by
The world chico.. and everything in it...
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
The world chico.. and everything in it...
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@imrenagi/serverless-lambda-architecture-with-google-cloud-platform-35cb3123206b?source=search_post---------8,"Sign in
There are currently no responses for this story.
Be the first to respond.
Imre Nagi
May 29, 2018·7 min read
I’ve been thinking about writing a technical blog post since I join Traveloka about 4 months ago. However, because of so many things went around: work’s stuff, commencement stuff (YES! I finally graduate and become #CMUGrad),etc. , I barely had enough time to work on small POC and write several things about it.
This work actually was inspired by my Big Data AWS training couple months ago with AWS Solution Architects in Traveloka Jakarta office. On that training, they proudly told us that they now have full set of serverless solution for Big Data. It includes AWS Kinesis Firehose (AWS Messaging Service solution), AWS Glue (AWS Serverless Batch Processing Solution), AWS S3 (Cloud Storage), AWS Firehose Analytics (AWS Realtime Streaming SQL Solution) and their new product named AWS Athena used for running Adhoc query to data stored in AWS S3. Go to this link to read the complete explanation about those things. Then, I’m thinking, why not doing the same thing in GCP?
First thing first. If you are not familiar with Lambda Architecture, you might need to read some articles on internet about it. Here is the some key concepts cited from http://lambda-architecture.net/ about lambda architecture:
I started this project by using public streaming data available on the internet and by picking one simple user story. The decision comes to Meetup.com RSVP streaming API. Fortunately, meetup.com provides free public streaming API that we can use to get all RSVPs that have been made world-wide. This should be enough for us because we just want to create speed layer and batch layer consuming those data. Then the user story chosen is:
As an user, given the range of date, I should be able to get the number of RSVP created for every 15 minutes.
You must be asking why it is for every 15 minutes while we can actually put the data somewhere, run the query and get the data for every minutes. Don’t think too fast. This is just for simplifying my further explanation.
Techinically in lambda architecture, the speed layer and batch layer are made so that they can provide (near) realtime analytics to the business decision maker or business analysts. In the real world, running batch job is expensive in terms of money and time consumed by the application. On the other hand, business stakeholder simply can’t wait to get the current data until the next batch job runs on the cluster. However, it is worth to note that batch processing should be the source of the most accurate data a company or organization can have. What we can do to solve this dilema?
Streaming or speed layer comes to the resque. Speed layer provides us with the estimated data in (near) realtime manner. Yes! It is estimated! It’s really hard to get the accurate data by using the speed layer. On the other hand, speed layer can provide the user with the current data easily. So, what we can do to get the realtime data? The answer is simply by combining the data from the batch job and realtime streaming job. Yes we need to take the trade off.
Let me give you example. Assume that now is 8 AM in the morning and CEO meetup.com wants to know the number of RSVP have been made up until now since yesterday midnight. However, the last batch job run 12 AM on last night so we clearly dont have accurate data from 12 AM until 8 AM. In this case, we need to combine the accurate data from the last batch job with the estimated data from the straming job runs from 12 AM until 8AM. Once we run the next batch job and get the accurate data for today, we can simply rewrite the result written by the streaming layer in the serving layer. The idea is simple, right? Don’t forget that we sacrifice the accuracy of the data a bit in this case, but to save the costs and for faster data driven decision, it is worth investment, IMHO.
I know that you have been waiting for the buzz words. Here we go.
As visually described on the diagram above, we can breakdown the component into several parts:
This is a simple java application I wrote to pull the data from RSVP Streaming API of meetup.com and push the data to GCP Pubsub. This application runs in Kubernetes Pod and deployed in Google Container Engine. For more detail, check in on my github in event-delivery/MeetupRSVPPublisher.java.
Google Cloud Pubsub is a centralized messaging system like Apache Kafka, RabitMQ, etc. If you are familiar with Topic and Consumer Group concept in Apache Kafka, it will be easier for you to understand the concept owned by Cloud Pubsub. It has topics (equivalent to Kafka’s Topic) and Subscription (equivalent to Kafka’s Consumer Group).
In Cloud Pubsub, all consumer should pull the message from the subscription instead of directly to the topic partition like what Kafka does. Once the subscription is created, the message will start flowing from the Pubsub and are ready to be consumed by the consumer subscribing the pubsub subscriptions.
In this case, Pubsub layer is not part of Lambda Architecture. However, to help getting clearer picture and creating scalable architecture, Cloud Pubsub has a very important role to achieve it.
From the data above, it is easy to see that the rate of published message and consumed message is not that huge. Pubsub only received about 3 messages per seconds and get about 3 pull operations per seconds.
Cloud Dataflow is used as the streaming engine in our implementation of speed layer. There are two responsibilities of the speed layer in our use case. First is to write the data pulled from the Pubsub to Google Cloud Storage by using TextIO so that the Batch layer can consume these data later and run batch processing on top of it. The second is to aggregate the number of RSVP comes to the system for every 15 minutes window. Once it gets the number, it will store the result to Google NoSQL technology named Cloud Bigtable by using BigtableIO. We can also dump the data to Google BigQuery by using BigqueryIO, however we don’t really need it in this use case.
You can go to the streamprocessor/RSVPStreamPipeline.java to see what is happening. :D
The DAG is pretty simple. First, rsvpParser used to serialize String given by Pubsub to Java Object. Then, every object parsed will be grouped by using 15 minutes fixed window in rsvpGroupWindow . In order to group the RSVP, I use rsvpKeyify and rsvpGroupByKey to give every RSVP a key representing the time window of its arrival timestamp. Then to aggregate the number of RSVP within the same fixed window, I used rsvpReducer to simply accumulate the count. Then transformation to a Hbase Put object is done and then the result is stored in our CloudBigtable using BigtableIO plugin.
Dataflow jon running for batch processing is not that different with the one for batch processing. The different is only the data source used, which is the Google Cloud Storage. Other than that, the batch processing as we know are not the long running process. It only runs once in a particular range of time. For instance once a day, a week or even a month.
To get the full picture of the code, you can take a look in streamprocessor/RSVPBatchPipeline.java.
We simply decided to use NoSQL technology for the serving layer provided by GCP called Bigtable.
In designing NoSQL schema, we need to think about how we gonna query the data. Since we want to query the data based on its date, we can simply use date yyyyMMdd as the partition key of the table. To get the data for every 15 minutes, we can create a column family called count containing many columns for storing the count for every 15 minutes. The way I do it is by using string 0000, 0015, 0030, 0100 and so on as column name to represents the time window of 15 minutes. By using this schema design, we can get additional benefits if:
Both of batch and speed layer will write to the same partition and to the column name and family. The speed layer will write the estimation count and the batch layer will write the corrected count of the data.
Once the data are stored in the bigtable, the other application such as backend RESTful API will be able to read the data and exposed it to the outside world.
All works mentioned in this blog post are made available in my github repository. Feel free to take a look, submit issues or even submit Pull Request for any kind of advancement.
Google Developer Expert, Cloud Platform Engineer @gojek
513 
513 
513 
Google Developer Expert, Cloud Platform Engineer @gojek
"
https://medium.com/google-cloud/using-a-gpu-tensorflow-on-google-cloud-platform-1a2458f42b0?source=search_post---------9,"There are currently no responses for this story.
Be the first to respond.
Warning before reading this: I am very happy this blog has been useful to lots of people. Since its now over a year old some of the commands are based on older versions of software. For an easier setup you may now want to use the Google Cloud optimized compute images:
I recommend reading this blog by Viacheslav Kovalevskyi instead of continuing with this one.
This blog was written for:
Google has some nice documentation here but there were a few additional steps I needed to take. So, lets start from the beginning:
There are two ways to set the instance up: 1) you use the command-line interface that Google Cloud offers or 2) you use their incredibly friendly web-ui to help you along. Since I am a big fan of the Google Cloud web interface this is what I’ll do. Setting up a server is very simple.
When in the Google Cloud platform make sure you have created a project and then navigate to the Compute Engine. There you will be asked if you want to create a new instance and once you get the popup dialog shown here on the left you can configure the number of cores, the memory (RAM) and a little option saying “GPU”. Click on this and the additional options show up that will allow you to indicate if and how many GPU’s you want to use. I then selected Ubuntu 16.04 as a boot disk, left all the other options the same and then click Create to start the instance.
Once the instance is ready you can connect to it by either using the web-shell Google Cloud offers or by copying the gcloud command to connect from your own command line.
Now that we are in we need to install some drivers for the GPU. The GPUs on Google Cloud are all NVIDIA cards and those need to have CUDA installed. To install CUDA 8.0 I used the following commands for Ubuntu 16.04 (taken from the Google Cloud documentation):
To verify its all working properly run the command below which will show you that the GPU is recognized and setup properly.
Yay! It exists and is being recognized. We’ll also need to set some environment variables for CUDA:
NVIDIA provides the cuDNN library to optimize neural network calculations on their cards. They describe it as:
The NVIDIA CUDA® Deep Neural Network library (cuDNN) is a GPU-accelerated library of primitives for deep neural networks. cuDNN provides highly tuned implementations for standard routines such as forward and backward convolution, pooling, normalization, and activation layers. cuDNN is part of the NVIDIA Deep Learning SDK.
Deep learning researchers and framework developers worldwide rely on cuDNN for high-performance GPU acceleration. It allows them to focus on training neural networks and developing software applications rather than spending time on low-level GPU performance tuning.
Summary: they have done a lot of work to make your life easier… You will need to register for the NVIDIA Developer Program and then you can download the latest version of the software. In this case I downloaded version 5.1 for CUDA 8.0 (I just noticed a newer version 6.0 is available as well). Once downloaded move it over to the instance using SCP or via Google Cloud Storage.
Once its on the instance install it using:
So the GPU instance is running and the drivers are in place, all that is left is to get TensorFlow installed to function for the GPU. You can see Google is trying to make it super simple to get all this working because you literally need to lines to get this last step done:
Installing tensorflow-gpu ensures that it defaults to the GPU for the operations where its needed. You can still manually move certain things to the CPU whenever you want to. Lets test if it all works…
Now to test if it was all successful you can use the python code below. It assigns two variables and one operation to the cpu and another two variables and an operation to the GPU. When starting the session we are telling it via the ConfigProto to log the placement of the variables/operations and you should see it printing out on the command line where they are placed.
Thats all…
based on feedback from ChrisAMancuso I have replaced the line
with
to ensure that it installs CUDA 8.0 (CUDA 9.0 is/was not supported by TensorFlow).
Google Cloud community articles and blogs
794 
12
794 claps
794 
12
Written by
Co-founder / CTO of @orbiit_ai. Data (Scientist) junky. All views my own.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Co-founder / CTO of @orbiit_ai. Data (Scientist) junky. All views my own.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/free-code-camp/continuous-deployment-for-node-js-on-google-cloud-platform-751a035a28d5?source=search_post---------10,"There are currently no responses for this story.
Be the first to respond.
Google Cloud Platform (GCP) provides a host of options for Node developers to easily deploy our apps. Want a managed hosting solution like Heroku? App Engine, check! Want to host a containerized app? Kubernetes Engine, check! Want to deploy serverless app? Cloud Functions, check!
Recently at work, I’ve been enjoying using our in-house continuous deployment service that quickly builds, tests, and deploys new commits pushed to GitHub. So when I read about Google’s new Cloud Build service, I wanted to take it for a spin and see if I could recreate a similar seamless deployment experience for myself. Further, in a conversation with Fransizka from the Google Cloud team, she identified this as an area where a tutorial would be helpful. So here we go…
Cloud Build is a managed build service in GCP that can pull code from a variety of sources, run a series of build steps to create a build image for the application, and then deploy this image to a fleet of servers.
Cloud Build works well with Google’s own source code repository, Bit Bucket or GitHub. It can create a build image using a Docker configuration file (Dockerfile) or Cloud Build’s own configuration file (cloudconfig.yaml). It can deploy applications (and APIs) to App Engine, Kubernetes Engine, and Cloud Functions. A really cool feature is Build Triggers. These can be setup to watch for a new commit in the code repository and trigger a new build and deploy.
This post shares the detailed steps and code to setup the continuous deployment for Node apps on GCP. It assumes that you’re familiar with developing simple Node applications, working with the command line, and have some high level understanding of deploying apps to cloud services like Heroku, AWS, Azure or GCP.
For each of the sections, a companion GitHub code repository is provided for you to follow along. Don’t sweat it though — feel free to skim over the article to learn about the high level ideas, and you can bookmark it and come to it later if you plan to set this up. The real fun of having a setup like this is that you get to deploy applications quickly.
Deploying a Node app to App Engine is quite simple. Create a new project in Google Cloud Console, add an app.yaml configuration file in our code directory (which describes the node runtime we want to use — I’ve used Node 8), and run gcloud app deploy on our terminal — and done!
If you want to try this out for yourself, here are a couple of resources:
So, what we’ve done so far by following the quickstart guide above:
….now how can we automate setup such that code changes get deployed automatically on push to GitHub?
Here is what we need to do:
2. Enable Cloud Build
3. Create a Cloud Build configuration file
This configuration has three build steps (each line starting with a hyphen is a build step) that will run npm install, then npm test and if all looks good then deploy our code to App Engine.
Each build step is just like a command we run on our machine. But in this case, since this file is in yaml and each step is split over 2 lines of name and args, it can look like a bit of a mind-bender.
Let’s try this: for the line starting with “name”, read its last word and then read the values in the “args” line. I hope this file makes more sense now!
4. Run a Build manually (optional, just for verification)
5. Create a Build Trigger
6. Run a Build automatically by pushing a commit to GitHub
Here is a screenshot for builds being triggered through a GitHub push for our app:
Too good to be true?? Run this last step a few times times to test it out a few more times. Our first application now gets deployed to App Engine on every commit to master 👏
Great, so we’ve setup our application to deploy to App Engine on GitHub push, but what if we wanted the same setup for our containerized applications? Let’s give it a spin!
At a high level, deploying a Node app to Kubernetes engine has two main tasks. First, get our app ready: Containerize the application with Docker, build it, and push the Docker image to Google Container Registry. Then, setup things on the GCP end: create a Kubernetes Cluster, create a Deployment with your application image, and then create a Service to allow access to your running application.
If you want to try this out for yourself, here are a few resources:
So, what we’ve done so far by using the guides above:
…but what we want is an continuous deployment setup such that a new commit kicks off a build and deployment.
Here is what we need to do:
2. Enable Cloud Build
3. Create a Cloud Build Configuration file
This configuration has five build steps that will run npm install and then npm test to make sure our application works, then it will create a Docker image and push to GCR and then deploy our application to our Kubernetes cluster. The values my-cluster, my-deployment and my-container in this file refer to resources in the Kubernetes cluster we have created (as per the guide we followed above). $REVISION_ID is a variable value that Cloud Build injects into the configuration based on GitHub commit that triggers this build.
4. Run a Build manually (optional, for verification)
We’re also passing the revision id in this command, since we are manually running this build vs it being triggered by GitHub.
5. Create a Build Trigger
Here is a screenshot for a build being triggered through a GitHub push for our app:
The steps in this section were pretty much the same as the App Engine section. The main differences were that we had to containerize our application with Docker, spin up our Kubernetes cluster, and then have a Cloud Build configuration with just a few more steps.
But at its core, Cloud Build and its Build Triggers work pretty much the same and give us a seamless deployment experience. Our second application now gets deployed to Kubernetes Engine on every commit to master 👏👏
Sure, App Engine and Kubernetes Engine are great, but how about automated deployments for our Serverless app? I mean, having no servers to manage at all is really the best, right? Let’s do this!
Deploying a Node app to Cloud functions will require us to create a new project. No configuration files are needed, and once GCloud functions deploy on our terminal, our functions are deployed!
If you want to try this out for yourself, here are the resources you will need:
If you’ve been following along, you can probably already picture what steps we need to do:
2. Enable Cloud Build
3. Create a Cloud Build Configuration file
Similar to the App Engine configuration, this configuration has 3 steps to install. Then test the build, and if all is good, then deploy it to Cloud Functions.
4. Run the Build manually (optional, for verification)
5. Create a Build Trigger
6. Run a Build automatically by pushing to GitHub
Here is a screenshot for build being triggered through a GitHub push for our sample app:
Cloud Functions were super easy to setup with automated builds and makes the “code → build → test → push → deploy” workflow really really fast! Our third application now gets deployed to Cloud functions on every commit to master 👏👏👏
Phew! We covered a lot of ground in this post. If this was your first time trying out GCP for Node, hopefully you got to see how easy and straightforward it is to try out the various options. If you were most eager to understand how to setup continuous deployment for apps on GCP, I hope you weren’t disappointed either!
Before you go, I just wanted to make sure that you didn’t miss the fact that all the sections had a sample app: Hello World for App Engine, Hello World for Kubernetes Engine and Hello World for Cloud Functions.
That’s it for now! Let’s go ship some code! 🚢
Thanks for following along. If you have any questions or want to report any mistakes in this post, do leave a comment.
If you found this article helpful, don’t be shy to 👏
And you can follow me on Twitter here.
We’ve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
1K 
7
1K claps
1K 
7
We’ve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
Written by
Engineering Manager at Facebook. Posts about software engineering, mobile app reliability and performance.
We’ve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
"
https://medium.com/google-cloud/continuous-delivery-in-google-cloud-platform-cloud-build-with-app-engine-8355d3a11ff5?source=search_post---------11,"There are currently no responses for this story.
Be the first to respond.
Agile and DevOps continue spreading among IT projects — and I would say: at a pace that we have never seen before! As the interest in these matters increases, the set of processes and tools which make it possible to deliver better software also increases.
At the end of the software development lifecycle is the delivery “phase”. And if the delivery is not fast, the whole Agile development process may be broken. Modern software should be designed to be fast delivered, using appropriate automation tools. In this context, I’ll write about Continuous Delivery in the Google Cloud Platform. It's a series of 3 articles, covering deployment to App Engine, Compute Engine, and Kubernetes Engine.
Starting with App Engine, GCP’s fully managed serverless application platform, I will show the steps to set up a development + automated build +continuous delivery pipeline. To keep it practical, we’ll create a very simple front-end application and deploy it to App Engine Standard Environment. Please remember we will focus on delivery and not on app development itself.
Before proceeding, make sure you have created a GCP Project and installed Google Cloud SDK in your machine if you wish to run the examples. Don’t forget to run gcloud auth login and gcloud config set project <your-project-id> for proper gcloud CLI usage.
A simple way to create a front-end web application is by using Angular CLI. The steps to install this tool are out of the scope of this article and can be found here. Once installed, cd to your preferred folder and type ng new <app-name>. Wait a few seconds. After the app is created, type cd <app-name> and ng serve. Point your browser to http://localhost:4200 and make sure the app is running. As we have a fully running app, we stop the development here ;). It’s time to think about delivery!
An Angular app is a set of HTML, CSS, JS, images, and other web-related static files. So, what we need to do now is to build the app and deploy it to an HTTP server in order to make it available to the users. To build the Angular app, we run npm install and npm run build --prod in the <app-name> folder. Angular CLI then creates a new dist folder, where it places the ready-to-deploy content. We could figure lots of ways to copy these files to the webserver, but following this path we are adding complexity to our build process, don’t you agree? As we desire simplicity and automation instead of complexity and manual tasks, we’ll learn how to use Cloud Build.
Cloud Build is a Google Cloud Platform fully-managed service that lets you build software quickly across all languages, counting on containers to get the job done. Having that said, let’s prepare our project to use Cloud Build. Add a file named cloudbuild.yaml to your project’s root folder, with the following content:
This simple file instructs Cloud Build on how to build and deploy the app, similar to a Docker multi-stage build. When we invoke Cloud Build using the command gcloud builds submit --config cloudbuild.yaml ., it will compress the project’s source files (a .gitignore file, if present, will be considered in order to determine which files shall be skipped), and copy the tarball to a managed Cloud Storage Bucket. It happens in your development machine.
Then, running on GCP servers, Cloud Build will uncompress the sources and execute npm install in a container of Cloud Build’s built-in image gcr.io/cloud-builders/npm. The second step will use another npm container to run npm run build --prod. The third one will run a container of another Cloud Build’s built-in image, named gcr.io/cloud-builders/gcloud, to execute a gcloud app deploy command.
I've already talked about the 1st and 2nd steps, but not about the 3rd one, gcloud app deploy, so let me do it. This command is responsible for deploying the built content to App Engine and finish the whole delivery process. In order to succeed it requires an app.yaml file in the project’s root folder (see sample content below), the <your-project-number>@cloudbuild.gserviceaccount.com Service Account must have the App Engine Admin role, and the App Engine Admin API must be enabled for the GCP Project in which the app will be deployed. This step replaces copying the built files to an HTTP server — mentioned before — with copying them to GAE Python 2.7 Standard Environment, where they can be served as static content.
Point your browser to https://<your-project-id>.appspot.com or click a service's name in Cloud Console to see the app running on App Engine! Also, visit https://console.cloud.google.com/cloud-build/builds?project=<your-project-id> to check your project’s build history.
Ok, now we have an almost fully automated deployment process.
— Hey, Ricardo, what do you mean by “almost fully automated”?
— I mean GCP can help us doing better than this :).
We have seen how Cloud Build interoperates with Cloud Storage and App Engine, but there’s a missing piece: Source Repositories.
Git is widely used as a source control management tool nowadays. Source Repositories allow you to use GCP as a Git remote repository or to automatically mirror repositories from Github or Bitbucket. Visit https://console.cloud.google.com/code/develop/repo?project=<your-project-id>, click CREATE REPOSITORY, and follow the steps — it’s pretty straightforward.
Once the repository is set, navigate to Cloud Build’s triggers page: https://console.cloud.google.com/cloud-build/triggers?project=<your-project-id>. Click ADD TRIGGER, select a source, a repository, and proceed to trigger settings. Give a name to the trigger, let's say Push to master branch, select Branch as the Trigger type, type master as the Branch (regex), and select cloudbuild.yaml as the Build configuration. Type /cloudbuild.yaml for the cloudbuild.yaml location and save. And here we go: every time a new commit is pushed to master, the build process is automatically triggered. We don’t need to manually execute gcloud builds submit … to trigger the build anymore. The deployment process is now fully automated!
The steps described in this article can be used or customized to deploy any kind of application supported by Google Cloud Platform’s App Engine Standard Environment. Deploying to Flexible Environment requires changes in folders' structure.
The sample code is available on Github: https://github.com/ricardolsmendes/gcp-cloudbuild-gae-angular. Feel free to fork it and play.
Happy deploying :)
2019–08–04: I created a GitHub repository to demonstrate how a similar deployment might be done to App Engine Flexible Environment. Please refer to gcp-cloudbuild-gae-flex-angular for details.
This is the 1st of a 3-article series on Continuous Delivery in Google Cloud Platform:
App Engine | Compute Engine | Kubernetes Engine
Google Cloud community articles and blogs
515 
4
515 claps
515 
4
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
head of data @ ciandt.com • hobbyist tech writer • dad • birder
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/how-to-run-deep-learning-models-on-google-cloud-platform-in-6-steps-4950a57acfa5?source=search_post---------12,"There are currently no responses for this story.
Be the first to respond.
Top highlight
Using Deep Learning Virtual Machine
Google Cloud Platform is a tool provided by Google which one can leverage to build large scale solutions. This platform has recently gained a lot of popularity because of their easy access to GPUs. Additionally, they also give you $300 worth of credits for free with a year of validity which depending on the kind of processing you need to do can last up to a year.
However, some of the instances can get really challenging since there isn’t a proper GUI or many packages won’t get installed. In this blog I am going to talk of an easy way to deploy a marketplace solution for running Deep Learning model. Moreover, this would also create a Jupyter Notebook GUI that can be used to view quick results.
The first thing you need to go is setup a google cloud account.
Go to https://cloud.google.com/ and sign in using your Gmail account. If you have a school or organization account it may lead to some collaboration issues in the future so I would strongly recommend to create an account using your personal Gmail account. If you don’t have a Gmail account, now maybe a good time to create one.
While signing in, Google will ask you to share your credit card details. You can put them in but your credit card wont be charged unless you have run out of all your $300 worth of credits, so you need not worry.
Once you have signed in, it will take you to a console screen which should look something like this.
If you don’t get an automatic assignment of a project, you should go ahead and create one. Click on the “Create New Project” icon and the banner and create a new project. The project ID is assigned automatically, however you can change it if you like. I decided to stay with the default project ID assigned to me.
Now that you have an account and project, you can deploy a marketplace solution.
Your billing will only start once you deploy the solution.
To set up a deep learning marketplace solution, search for “Deep Learning VM” in the search bar. This should take you to the landing page for “Deep Learning VM”.
The advantage of using Deep Learning VM is that we don’t have to install python or tensorflow since it is a part of a pre-packaged image developed by Google. Once you’re on this page, you just hit the “Launch Compute Engine” button. This page also shows the number of past deployments you have for this engine. It is 3 in my case.
Once you launch the compute engine, you will be taken to the configuration page, where you can set a name for the environment, select the zone for the machines and select the number of CPUs and GPUs you would want.
It is important to note the Zone you select for your deployment since the machine configuration you choose will depend on it. For example, there maybe restrictions in some zones on the number of CPUs and GPUs you can access.
Depending on the kind of machine that you choose, you can see the billing amount on the right hand side change accordingly.
For example, if you choose 16 CPUs and 0 GPUs, you can see that you will be charged $392.36 per month if you use 730 hours per month. If you hit “Details” it will give you the break up for the billing. Generally, GPUs are more expensive than CPUs, so if you don’t need GPUs, it is better to skip them altogether. You also need to request for GPU quota in the zone of your deployment (which I will talk about in detail in Step 6).
For now, choose Zone: us-west1-b, 16 CPUs and GPUs as “None”. The next thing to choose is the size of your hard-drive. “Standard Persistent Disk” should be good for any project, but if you want you can expand the memory if you have a lot of data. Keep in mind larger disk sizes will lead to bigger bills, so best to be parsimonious about the requirements. They can always be modified later on if required (covered in Step 5).
Once you have selected the config hit “Deploy”. Based on your selection, it may take 5 to 10 mins for the deployment to set up. If you get an error after deploying, check to see if you selected GPUs by mistake. If you select GPUs without having an assigned quota, it may lead to an error. Just create another deployment without any GPUs and you should be good to go.
Now there are 3 different ways of running code on this VM. The easiest is using a GUI of Jupyter Notebook which runs on localhost:8080 on your machine. To access this, you need to install Google SDK to SSH this VM.
You can install Google SDK here. Initialize Google SDK and connect to your google account and project that you created. Initialization options should show up automatically after installation, if it doesn’t you can run the command : gcloud init and make sure that you connect with the same email and project ID as before.
Once you have Google SDK installed and configured, you just copy the SSH link that shows up on your deployment page and paste it on to the Google SDK. The SSH link will be under the header “ Create an SSH connection to your machine” (see image below)
If you have successfully created a SSH connection, a PuTTY screen will pop-up (image below)
Once you have your SSH setup, you are just once click away from your Jupyter GUI. Go back to the deployment manager and hit the localhost:8080 button
Voilà!! That will take you to the Jupyter Notebook instance that is deployed on 16 CPUs. You can use this like any machine.
Additionally you can also run python batch jobs on the PuTTY terminal or by hitting the SSH terminal in the Compute Engine VM. More on that in the next step.
Before we add GPUs we need to request GPU quota in the same zone our instance is deployed on.
Just search for “Quotas” in the search bar and that should take you to the Quotas page under “IAM & admin”.
Here under “Metrics” first, select “None” and then search for GPU. Based on the GPU present in your zone you can select the name of the GPU and we also need to select “GPUs (all regions)”. For example, since our zone is us-west1-b, you can select the “NVIDIA P100 GPUs” and “GPUs (all regions)”
Once you select both the GPUs, you hit the “Edit Quotas” button on top.
Make sure that the GPU you select is in the same zone as your instance, or else your deployment will not be able to access it.
This will generate a form where you need to share your personal phone number and reason for this request. As soon as you submit this form, you will get an email from Google saying that your request is under process and it will take 2–3 business days.
Although their email says 2–3 business days, the request is approved within a couple of hours. Once your quota request is approved, you can edit your virtual machine to include more GPUs.
To add the requested GPUs, you need to edit the instance that is created on the Compute Engine page. Go to the menu on your google console and then hit “Compute Engine”.
The VM instances pages gives a list of the VMs that we have installed across various solutions on google cloud platform. An important thing to note here is that we should stop all instances if we do not want to be billed for the machines. Even if we don’t run any code, google charges us for the instances.
Hence it is important to stop all instances when we aren’t running anything.
Once you have stopped the instance, you can edit it. If your quota request is approved, you should be able to add more GPUs and deploy the solution again in no time.
Another neat trick is to “Enable connection to serial ports” and “Allow full access to cloud APIs” (under Access scopes) to enable your instance to talk to buckets and vice versa.
Once your config has been modified by adding another GPU, you can either run a deep learning model on the Jupyter Lab UI or the PuTTY terminal. You will notice that it will be much faster as we have added GPUs to our system. This also means our bill is higher so make sure to keep checking the “Billing” page to ensure that you don’t run out of credits.
Below is a short video on how I accessed the Jupyter Notebook GUI on the cloud to run models.
You can copy data from your bucket to the instance that you just created using “gsutil” feature of GCP.
You can either use the PuTTY terminal or the SSH on the Compute Engine to write this command.
More details on gsutil here.
Sometimes we end up storing large files or objects on the virtual jupyter notebook. This might lead to memory constraints. However just deleting them doesn’t free up memory. Instead we need to manually go and clear the trash from the jupyter location in the VM to free up memory.
To check the available memory run, you can use du -sh * in the PuTTY terminal. Once you navigate to the jupyter folder in PuTTY, you can force delete it to free up memory. In my case the command:
rm -rf /home/jupyter/.local/share/Trash
worked. By running du -sh* again, I could observe that the available memory in dev/sda1 environment went up after running this command.
I am sure there are many other ways to run Deep learning models on Google Cloud Platform without investing too much time in setting up an environment.
What has been your experience with GCP? Please share your comments and let me know if I can help solve your queries in any way.
Google Cloud community articles and blogs
509 
7
509 claps
509 
7
Written by
End to end problem solving. Data nerd. Facebook, UT Austin
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
End to end problem solving. Data nerd. Facebook, UT Austin
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/hackernoon/aws-vs-google-cloud-platform-which-cloud-service-provider-to-choose-94a65e4ef0c5?source=search_post---------13,"There are currently no responses for this story.
Be the first to respond.
While AWS is undoubtedly the benchmark of cloud service quality, it has some drawbacks.Today we compare Amazon Web Services (AWS) with Google Cloud Platform (GCP).
AWS definitely is the leader of the cloud computing services, due to being the pioneer in the IaaS industry since 2006 and being 5 years ahead of other popular cloud service providers. However, this leads to certain inconveniences and drawbacks that can be exploited by the competition. Essentially, the sheer amount of AWS services is overwhelming.
While Google Cloud Platform does not boast such an ample list of services, it rapidly adds new products to the table. The important thing to note is that while AWS does offer a plethora of services, many of them are niche-oriented and only a few are essential for any project. And for these core features, we think Google cloud is a worthy competitor, even a hands-down winner sometimes, though many of essential features, like PostgreSQL support are still in beta in GCP.
Look for yourselves. Google Cloud can compete with AWS in the following areas:
Customer loyalty policies are essential as they help the customers get the most of each dollar, thus improving commitment. However, there is an important difference here: AWS provides discounts only after signing for a 1-year term and paying in advance, without the right to change the plan. This, obviously, is not the perfect choice, as many businesses adjust their requirements dynamically, not to mention paying for a year in advance is quite a significant spending.
GCP provides the same flexibility, namely the sustained-use discounts, after merely a month of usage, and the discount can be applied to any other package, should the need for configuration adjustment arise. This makes long-term discount policy from GCP a viable and feasible alternative to what AWS offers, and rather an investment, not an item of expenditure. Besides, you avoid vendor lock-in and are free to change the provider if need be, without losing all the money paid in advance.
AWS is definitely the leader for building Big Data systems, due to in-depth integration with many popular DevOps tools like Docker and Kubernetes, as well as providing a great solution for serverless computing, AWS Lambda, which is a perfect match for short-time Big Data analysis tasks.
At the same time, GCP is in possession of the world’s biggest trove of Big Data from Google Chrome, which supposedly deals with more than 2 trillion searches annually. Having access to such a goldmine of data is sure to lead to developing a great kit of products, and Bigquery is definitely such a solution. It is capable of processing huge volumes of data rapidly, and it has a really gentle learning curve for such a feature-packed tool (it even produces real-time insights on your data). The best thing about it is that Bigquery is really user-friendly and can be used with little to none technical background, not to mention $300 credit for trying out the service.
As we explained in our article on demystification of 5 popular Big Data myths, cloud computing can be more cost-efficient as compared to maintaining on-prem hardware. Essentially, this really goes down to using the resources optimally and under the best billing scheme. AWS, for example, uses prepaid hourly billing scheme, which means running a 1 hour and 5 minute-long task would cost 2 full hours.
In addition, while AWS offers a plethora of various EC2 virtual machines under several billing approaches, these configurations are not customizable. This means if your task demands 1.4GB RAM, you have to go with the 2GB package, meaning you are overpaying. Of course, there are several ways to save money with Amazon, from bidding for Spot instances to lending Reserved instances and opting for per-second billing. Unfortunately, the latter option is currently available only for Linux VM’s.
GCP, on the contrary, offers per-second billing as an option for ALL their virtual machines, regardless of the OS’s they run on, starting 26th of September 2017. What’s even more important, their instances are fully configurable, so the customers can order 1 CPU and 3.25GB RAM, or 4.5GB, or 2.5GB — you get the meaning.
As The Washington Post told us, NSA has infiltrated the data center connections and eavesdropped on Google once (many more times, supposedly). This breach has lead to Google opting for full-scale encryption of all their data and communication channels. Even the stored data is encrypted, not to mention the traffic between data centers.
AWS is still lagging in this regard. Their Relational Database Service (RDS) does provide data encryption as an option, yet it is not enabled by default and requires intense configurations if multiple availability zones are involved in the equation. The inter-data center traffic is also not encrypted by AWS as of now, which poses yet another potential security threat.
All things considered, GCP is actually a serious contestant for both AWS and MS Azure. Yes, AWS leads in terms of the numbers of customers and products, due to 5 years of head start. At the same time, GCP already provides all the needed functionality and offers competitive pricing and configuration models, backed up by serious traffic privacy and security measures. With time, as more and more businesses accept the AI-first approach to doing business, GCP’s immense power in Big Data analytics and Google Chrome’s leading position amongst the browsers will allow Google Cloud Platform become an even more serious counterpart for AWS.
Just keep in mind many of GCP features are in alpha or beta stage, so their behavior and API might change. It means that using GCP in conjunction with long-term projects may require GCP connectors upgrade during the project lifetime.
We highly recommend evaluating the actual requirements of your project and think GCP is the best choice for development and staging environments. It might turn out you will suffice to go for GCP to meet all your needs and avoid opting for multiple AWS services required to use the platform efficiently but not actually needed to run your project. Feel free to contact us with any questions regarding the project requirements, we are always ready to assist!
#BlackLivesMatter
746 
3
how hackers start their afternoons. the real shit is on hackernoon.com. Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
746 claps
746 
3
Written by
DevOps & Big Data lover
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
DevOps & Big Data lover
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://servian.dev/exploring-beam-sql-on-google-cloud-platform-b6c77f9b4af4?source=search_post---------14,"Sign in
Graham Polley
Jan 24, 2019·10 min read
Did you know that a new feature was recently rolled out for Apache Beam that allows you to execute SQL directly in your pipeline? Well, don’t worry folks because I missed it too. It’s called Beam SQL, and it looks pretty darn interesting.
In this article, I’ll dive into this new feature of Beam, and see how it works by using a pipeline to read a data file from GCS, transform it, and then perform a basic calculation on the values contained in the file. Far from a complex pipeline I agree, but you’ve got to start somewhere, right!
<tl;dr> You can now inject SQL directly into your Apache Beam/Dataflow pipeline (using the Java SDK only), which means having to write a lot less code. It’s still very new and probably not ready for production workloads just yet. But, it’s certainly something to keep an eye on as it quickly matures. Full source code for below example here.
After seeing Beam SQL pop up in the docs a few months ago, I’d been meaning to test drive it for a while. But, being summer here in Australia, I was finding it rather hard to pull myself away from lounging around in my hammock all day. I was also really busy baby proofing the house (again) for the arrival of our latest ankle-biter in April.
But this week I found some time between commuting to work and meetings to finally have a proper poke around with Beam SQL, and put it through its paces. So, what’s it all about it then? Well, a good place to start is the official documentation blurb:
Beam SQL allows a Beam user (currently only available in Beam Java) to query bounded and unbounded PCollections with SQL statements. Your SQL query is translated to a PTransform, an encapsulated segment of a Beam pipeline. You can freely mix SQL PTransforms and other PTransforms in your pipeline.
Let’s break that statement down y’all:
In addition to that blurb, here’s another one that tells us Beam SQL is essentially based on Apache Calcite:
Calcite provides the basic dialect underlying Beam SQL. We have added additional extensions to make it easy to leverage Beam’s unified batch/streaming model and support for complex data types.
So, you’ll need to use Calcite’s dialect of SQL when using it in Beam. Check out the Apache Calcite SQL docs here. However, for all intent and purposes, you can simply write SQL in your pipelines in order to work on your data coming in it.
Got it? Great. Let’s move on.
Now that you know what it’s all about, let’s have a look at the code needed to get a simple pipeline that uses Beam SQL up and running. To test it, I devised a cunning data pipeline:
The first thing we need is some sample data to push through the pipeline. I usually turn to the public datasets in BigQuery whenever I need some dummy data, or if I want to test something. One of the entries in the public datasets is the Wikipedia benchmark page view tables.
These tables are great tables for doing quick tests and demos on GCP. They are simple to understand and easily relatable to the audience. Also, the size of the tables range from a 1K rows, all the way up to 100B rows (6TB). Wowsers. So, you can easily scale and test your workloads with absolutely no changes needed.
To start with, we simply export the smallest of the tables (1K rows) to GCS as a CSV file from the BigQuery console. Of course, we could just read the table directly from BigQuery in the pipeline (using BigQueryIO), but I can’t think of a use case that would require using Beam/Dataflow to pull data from BigQuery in order to run SQL over it. You’d be much better off just querying it directly in BigQuery! 😀
The first part of the code constructs the pipeline and creates the read from GCS into a PCollection<String>. Simples:
To work with Beam SQL, you must convert the type of your PCollection to a Row. Don’t confuse this with a BigQuery TableRow object though. They are different things altogether. So, the next thing we nee do is to run a ParDo over the PCollection<String>, which we obtained from the initial file read from GCS, and turn it each element into a Row object and then finally set the values for each element:
Now that we have a PCollection<Row> object, we can finally run some sweet, sweet SQL over the contents of that collection. Who doesn’t love a bit of SQL, huh?
Remember, that the elements will be distributed across all the workers in the Dataflow cluster. But, we don’t need to care about that because it’s all abstracted away. Using SQL also means we don’t need write boilerplate’y GroupByKeys, Combines, etc. in Java. Using SQL is much easier to express what we’d like to do with the data:
In this example, I’m simply aggregating by language and summing the views for each one. As simple as this sounds, under the hood a lot of work needs to be done to make this happen on a distributed cluster. And it’s something I’m not smart enough to understand. But once again, the complexity of this is hidden from users like us. You don’t need to worry about it — until you have a bug and you need to debug that is! Mwaaaah!
We’ve now got the bulk of the pipeline written. The last thing we need to do is collect the results of the SQL statement, and store them somewhere. This could be anywhere really (e.g. BigQuery, Datastore, Memorystore etc.), but for brevity’s sake I simply writes the results back as a CSV file to GCS.
To do this we need to convert the SQL Row object back to String so we can write it out to GCS. Just hitting it with a simple ParDo makes this a walk in the park. Then one more apply to write the result file out (without sharding so we get just one file) to GCS.
And you’re done folks!
Putting it all together, here’s the pipeline in all its Java glory:
The final piece of the puzzle is actually running the pipeline on GCP. Now, you could run it locally using the DirectRunner, but where’s the fun in that? Actually, snark aside, executing it locally is a great way to develop and debug your Beam/Dataflow pipelines. Don’t be like me and run your pipelines on GCP just for fun — your boss won’t like you for very much longer when they get the bill.
Anyway, after kicking off the pipeline with the DataflowRunner this is what happens:
Drilling down into the transform_sql step, it’s clear to see the steps that the SQL generated for us in the pipeline. Remember, I didn’t need to code these steps. I just wrote some simple SQL to express my intent:
With the pipeline run, and the output written to GCS, it’s time to validate the results of the magical SQL code. Once I downloaded the output.csv file from the GCS bucket, I dutifully hit it with some wonderfully dodgy Bash. Yes, yes, I know I could have federated out to the file in GCS from BigQuery and, wrote some SQL to check the results. But, doing it with Bash makes me look like I know how this computering thing works.
Firstly, let’s check the 10 most popular languages line up between what the pipeline produced, and what BigQuery spits out. The left terminal is the Beam SQL results, and the right terminal in hitting the original table in BigQuery:
That looks good. Now, let’s make sure Beam SQL calculated the corrected number of aggregations for the languages field i.e. distinct values:
Finally, let’s double check the total sum of views to make sure it’s 100% correct:
Everything lines up with the original data that I exported from BigQuery into GCS.
I like it when things work.
As awesome as all that is however, 1K rows isn’t really that impressive, now is it. Instead, let’s run the exact same pipeline on 1 billion rows (~100GB) and let the Dataflow cluster scale up to 10 workers. That’s much more fun!
The pipeline took about 25 minutes to process 1B rows. That’s not bad at all. Now, we need to perform some checks on the results just like before:
Whoopsie! The calculation for the en value was much higher in BigQuery than it was in the result of my pipeline, whereas the other values were just fine. Luckily, it didn’t take me that long to figure out the problem (thanks again Stack Overflow). Looking at the number — this time with commas — it’s much easier to see why it all went pear-shaped:
4,990,236,853
Remember our pipeline was written in Java. Yes, JAVA! Got it yet? That’s right. I had bootstrapped my pipeline by copying the example from the Beam documentation, and that sample code calls addInt32Field(). Then, I had unwittingly used Integer.valueOf() when setting the value.
An Integer in Java is 2³¹-1, so the max value is 2,147,483,647. As such, because the sum for the en value was so large, the JVM was wrapping back around to -2,147,483,648 when it topped out at the max value. Ahh, gotta love Java!
Alas, the fix was simply to use addInt64Field() and instantiate aLong (2⁶³-1) object to store the values in Java instead of an Integer. This gives you just a tad more head room to play with as a Long has a max value of 9,223,372,036,854,775,807.
So, a quick fix to the code and it was time to run the pipeline again.
This time the results looked much better, and the value for en is appropriately sized number. Wonderful.
We just took a look at Beam SQL and how it works. It was relatively easy to get going, and put it through its paces. That said however, this was a very simple example using basic aggregation and summing. It would be interesting to see how it does joins and more complex operations. Finally, I only tested it using a batch workflow. I’d be curious to see how it works with a streaming source e.g. PubSub or Kafka.
It’s also import to note that it’s a very new feature and still maturing. For that reason, I ‘d be wary of using it in production workloads just yet — unless you thoroughly test it for your use case(s). Also, being so new, there’s very little material on it aside from the Beam documentation itself.
However, this is certainly something to keep on eye on if you’re using Beam/Dataflow as part of your data analytics workloads. It may help solve a lot of problems and prove useful to many developers and data engineers. It’s got a lot of potential, but most importantly, the KSQL folks can’t say Beam is missing this functionality anymore. 😀
Cause trouble on that cloud thing that everyone is talking about. I like BigQuery. Work @weareservian and tweet nonsense @polleyg. Moving blog to polleyg.dev
364 
7
364 claps
364 
7
At Servian, we design, deliver and manage innovative data & analytics, digital, customer engagement and cloud solutions that help you sustain competitive advantage.
"
https://blog.percy.io/tuning-nginx-behind-google-cloud-platform-http-s-load-balancer-305982ddb340?source=search_post---------15,"Percy is a visual testing and review platform that helps you deploy every UI change with confidence. Learn more at https://percy.io ✨
This is a consolidation of learnings from weeks of tuning and debugging NGINX behind the Google Cloud Platform HTTP(S) Load Balancer.
There is an unfortunate lack of documentation around the web for some of this, so I hope it helps you! But remember, tuning is always specific to different environments and conditions—your mileage may vary.
By default, NGINX does not compress responses to proxied requests (requests that come from the proxy server). The fact that a request comes from a proxy server is determined by the presence of the Via header field in the request.- NGINX Admin Guide: Compression and Decompression
Google’s load balancer adds the “Via: 1.1 google” header, so nginx will not gzip responses by default behind the GCP HTTP(s) Load Balancer. This happens because nginx does not think the proxy can handle the gzipped response.
To re-enable gzipped responses, configure gzip_proxied in nginx.conf (in http, server, or location blocks):
Traffic from the load balancer to your instances has an IP address in the range of 130.211.0.0/22. When viewing logs on your load balanced instances, you will not see the source address of the original client. Instead, you will see source addresses from this range.- https://cloud.google.com/compute/docs/load-balancing/http/
For security reasons, you can force all HTTP(S) traffic to flow through the load balancer and block direct access to your instances (from port scanners, for example). You just need to know this specific CIDR range:
When making a GCE firewall rule, just set Source IP ranges to this range.
Update: Google has added more ranges that load balance traffic might come from. As of Jan. 31, 2018, you’ll also need to have this range allowed:
These ranges only apply for the HTTP(S) Load Balancer and SSL Proxy. If you are using Network Load Balancing, see the docs for the applicable ranges.
This was a hard one. Many hours tuning sysctl settings, running tcpdump, re-architecting flows, and trying to recreate a rare and intrusive error before figuring out what was happening.
Summary: the default nginx keepalive_timeout is incompatible with the Google Cloud Platform HTTP(S) Load Balancer!
You must increase nginx’s keepalive_timeout, or risk intermittent and sporadic 502 Bad Gateway responses to POST requests.
The “650 seconds” here is not arbitrary, see below for justification of why we picked this specific timeout. Notably, this is opposite of the advice that most articles will give you, but most of them are configuring nginx for direct connections and not for sitting behind a global load balancer.
Several times a day, POST requests to our API would return a 502 Bad Gateway response, with no backend log of the error. Long ago we added client-side retries to our API libraries to handle these cases, but I decided to finally root-cause this bug once and for all so we didn’t have to keep patching it across libraries.
In Google Cloud Logging, you can see if you are experiencing these particular 502 responses using an advanced filter:
Now it gets tricky. There were no other logs that correlated with this rare error—no logs from nginx itself, nothing from the API app, nothing related to any syslog or kernel message—nothing, except rare 502s in load balancer logs.
That looks very much like a problem with the GCP HTTP(S) Load Balancer itself. But, as an ex-Googler who has configured services behind Google load balancers in the past, I knew it was very unlikely that my site was a special snowflake that was hitting some new bug uncaught by the billions of requests that flow through those LBs every day. It was much more likely I had a bad config somewhere.
Digging further, we see more info:
Ah ha. So, this was not a problem finding a backend machine (which would have been “failed_to_pick_backend”). The docs have a tiny description of this error:
backend_connection_closed_before_data_sent_to_client: The backend unexpectedly closed its connection to the load balancer before the response was proxied to the client.- Google Cloud Platform: Setting Up HTTP(S) Load Balancing
Meaning, a TCP connection is being established from the load balancer to the GCE instance, but the instance is terminating the connection prematurely.
Some things were common to the error:- It only happened on POST requests, never on GET requests.- It only happened on our nginx + API services, not on our nginx + static.- It only happened under moderate traffic load (but the server was not overloaded, still 2–5% CPU usage and plenty of memory). This made it feel like a race condition or timeout problem of some sort.
I finally found the right knob to turn. It turns out that there is a race condition between the Google Cloud HTTP(S) Load Balancer and NGINX’s default keep-alive timeout of 65 seconds. The NGINX timeout might be reached at the same time the load balancer tries to re-use the connection for another HTTP request, which breaks the connection and results in a 502 Bad Gateway response from the load balancer.
Let’s dig deeper.
I was actually able to reliably reproduce this error using a simple curl POST load test script, combined with an aggressive nginx timeout:
And here’s a tcpdump of what happened:
So, why doesn’t the load balancer just retry the request?
Well, that’s where the POST part of this whole thing becomes important—the Google Cloud HTTP(S) Load Balancer will retry failed GET requests, but will never retry failed POST requests. These retry behaviors are completely undocumented as far as I can tell, but relatively standard practice—GET requests are assumed to be idempotent, and POST requests are not.
So what timeout should we use to fix this? Unfortunately, the load balancer timeout is also undocumented on the actual Load Balancer docs. After some tcpdump research, I found that it matches the same ten minute TCP timeout noted elsewhere in GCE.
If you’re familiar with GCE, you might be thinking that we missed the tcp_keepalive_time tuning mentioned in Compute Engine: Tips, Troubleshooting, & Known Issues. We didn’t. :) This error happens regardless of the fact that we had tuned sysctl net.ipv4.tcp_keepalive_time long ago based on the docs. I haven’t completely figured out why, but I assume this is because the connection is inbound not outbound, and NGINX probably sets custom socket options that override the system defaults and then enforces it’s own keepalive_timeout regardless.
To fix this race condition, set “keepalive_timeout 650;” in nginx so that your timeout is longer than the 600 second timeout in the GCP HTTP(S) Load Balancer. This causes the load balancer to be the side that closes idle connections, rather than nginx, which fixes the race condition! (This is not a 100% accurate description for how closing TCP connections works, but it’s fair enough for here).
After setting this, we have not seen a single 502 Bad Gateway response from our APIs in weeks.
There are more nobs we turned and tested to support higher concurrency and number of connections, but I’ve gone on long enough. :)
Check out these additional resources:
Ultimately, Google Cloud Platform has provided us with a fantastically scalable and configurable platform, and these and other system tweaks helped us dramatically reduce our app machine footprint while making our APIs more reliable.
Percy is a visual testing platform that helps you deploy your UI with confidence. Learn more at https://percy.io ✨
Words, news, and stories from the people building Percy
801 
24
801 claps
801 
24
Written by

Words, news, and stories from the people building Percy
Written by

Words, news, and stories from the people building Percy
"
https://medium.com/google-cloud/gcp-the-google-cloud-platform-compute-stack-explained-c4ebdccd299b?source=search_post---------16,"There are currently no responses for this story.
Be the first to respond.
Google Cloud Platform (GCP) offers a myriad of services, one particular set of services is its compute stack that contains, Google Compute Engine (GCE), Google Kubernetes Engine (formerly Container Engine) (GKE), Google App Engine (GAE)and Google Cloud Functions (GCF). These services all have pretty cool names, but can get somewhat confusing with regards to their function and what makes them…
"
https://medium.com/google-cloud/continuous-delivery-in-google-cloud-platform-cloud-build-with-compute-engine-a95bf4fd1821?source=search_post---------17,"There are currently no responses for this story.
Be the first to respond.
This article is the 2nd part of a series written to tackle Continuous Delivery in Google Cloud Platform. In the 1st article, Google App Engine was the main actor. In this one, Google Compute Engine — or simply GCE — will come into the scene. GCE is the Infrastructure as a Service component of GCP, which is built on Google’s global infrastructure and allows its users to launch virtual machines on demand.
As we had already seen, GAE fits perfectly for continuously delivering projects. However, there are some limitations to use the platform: the code must be written in specific languages/versions, mainly if your team aims to use the Standard Environment, which is less expensive (more info at https://cloud.google.com/appengine/docs/the-appengine-environments); and surely will be fully compatible if the project was designed to run on GAE, but may have troubles if trying to migrate legacy code to the platform, even if choosing the Flexible Environment.
After a pros and cons analysis, your team may conclude that GAE is not the best option for a project. Maybe because the project uses some language or tool that is not supported by the platform; maybe because they want more control or customizability over the execution environment; maybe because they want just to migrate their workload from existing servers to GCP, to take immediate advantage of scaling up the app without changing so much the codebase. For all of these cases, consider Google Compute Engine.
What we will see in the next lines is how to use a set of tools available in GCP that allows your team to set up a development + automated build + continuous delivery pipeline using GCE. Also, using an example Angular app, it will be possible to compare the solution with the one proposed to use with GAE in the 1st article. The key point here is that the application must be shipped as a Docker image. If you are able to do it in your project, you have great chances to automate the whole deployment process as described in this article.
Before proceeding, make sure you have created a GCP Project and installed Google Cloud SDK in your machine if you wish to run the examples. Don’t forget to run gcloud auth login and gcloud config set project <your-project-id> for proper gcloud CLI usage.
For the sake of simplicity, the Angular app will be served by Nginx. So, create your new Angular app (follow the steps described here) and cd <app-name>. In order to pack the app plus the Nginx server as a Docker image, let’s create a file named Dockerfile in its root folder, with the following content:
Basically, it’s a multi-stage container build. The lines from 1 to 5 (stage 1) use a NodeJS 8 image just to build the app. The lines from 7 to 11(stage 2) copy the result of the build process — a set of HTML, CSS, and JS files — to an Nginx image and replace the default server’s home page with app’s content. Finally, line 12 starts the server.
Let’s also create a second file,.dockerignore, in the same folder, with the below content. It will prevent Docker from copying unnecessary files to the built images, decreasing their sizes.
If you trigger a docker build -t <app-name> . command followed by docker run -d --name <app-name>-container -p 80:80 <app-name>, and point your browser to http://localhost:4200, you’ll see the application running with Docker.
What we have seen so far is just Angular and Docker setup stuff. From now on GCP will be in action, making things more interesting!
In typical non-automated or semi-automated deployment scenario, an Engineer could (1) set up a CI tool such as Jenkins to monitor a Git repository for new pushes, (2) trigger the build process when new content is pushed and (3) find a mystic way to update all VMs that are running the Docker containers to update the new version. Steps 1 and 2 are simple. Step 3 maybe not... At this point GCP offers a much more sophisticated approach: instead of creating VMs, installing Docker on them, and manually managing the containers, what about creating VMs that exclusively run a specific container and automatically update themselves when new versions of the images are published? Sounds good, right? So, let’s see how to do it.
First of all, we need to add one more small file to the project root folder, named cloudbuild.yaml, as follows:
This file will be used by Cloud Build to build the Docker image and create a repository to store it in the Container Registry, using the specified name — at build time, Cloud Build automatically replaces $PROJECT_ID with your project ID.
Ops… a new GCP component was mentioned in the previous paragraph: Container Registry. According to the official documentation, it’s more than a private container repository: is a single place for your team to manage container images, perform vulnerability analysis, and decide who can access what with fine-grained access control.
Coming back to the code: after adding the file, run gcloud builds submit --config cloudbuild.yaml . in the project’s root folder, wait a little bit, and access https://console.cloud.google.com/gcr/images/<your-project-id> after the building has finished. You’ll see the new repository there (similar to the picture below). This manual step is required only once. And you may copy the full repository name, we will need it later ;).
We're almost done with Docker image building. Just need to set up a trigger that will start the build process automatically every time new code is pushed to a monitored Git repository, and this is a Source Repositories job, as we saw in the first article of this series. It will be set up exactly as it was for delivery with GAE. Bear in mind the main difference will be the result of Cloud Build processing: for GAE, it publishes a new version of the app; for the current process, it only pushes an image in the Container Registry.
Now, let’s create some VMs to run the containers. In GCP Navigation Menu, click Compute Engine > Instance Templates. Click on CREATE INSTANCE TEMPLATE. In the next screen, select Deploy a container image to this VM Instance, type (or paste) the full Container image — aka Repository — name, select Allow HTTP traffic, and click on Create.
In the Navigation Menu, select Compute Engine > Instance Groups. Click on Create instance group. In the next screen, select the Instance template you have just created and set the Minimum number of instances to 3. Click on Create.
After the Instance Group has been created, select Compute Engine > VM Instances in the Navigation menu. Click on instances’ External IP links to make sure the application is running in each of them. It may take a while to return the home page for the first time…
Finally, grant the Compute Instance Admin (v1) and Service Account User roles to the <your-project-number>@cloudbuild.gserviceaccount.com Service Account (Navigation menu > IAM & admin > IAM).
So, what if we push a new version of the application to the Git repository? Which steps do we need to repeat in order to have it running in production? Just restart the VMs! There are many ways to do it in GCP, including publishing a message to some PubSub topic to invoke a Cloud Function that restarts the VMs, or use advanced options for update managed instance groups (https://cloud.google.com/compute/docs/instance-groups/updating-managed-instance-groups). But, to show a simple example, let’s just add the following lines to cloudbuild.yaml:
They will include a new step in our build process that consists of running the gcloud compute instance-groups managed rolling-action restart command. The instances that belong to the group we created a few steps before will be restarted after the build, one by one, in a process that ensures there will always be available machines at any given time. It may take some minutes depending on how many instances the group has, but works perfectly. To see it in action, change something in your code, the title in app.component.ts for example, and push the new code to a Git repository that is monitored by Cloud Build. Wait a few minutes and refresh the HTML pages served by each instance (external IPs may change after restart).
Well, that’s pretty much it for this topic. As demonstrated here, setting up a CI environment to GCE is usually more complex than to GAE, but is a valid option if your project requirements don’t fit GAE. And sure, this is one solution, but there are others.
Below picture shows the main GCP components mentioned in the article:
The sample code is available on Github: https://github.com/ricardolsmendes/gcp-cloudbuild-gce-angular. Feel free to fork it and play.
Hope it helps!
This is the 2nd of a 3-article series on Continuous Delivery in Google Cloud Platform:
App Engine | Compute Engine | Kubernetes Engine
Google Cloud community articles and blogs
586 
9
586 claps
586 
9
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
head of data @ ciandt.com • hobbyist tech writer • dad • birder
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/how-to-deploy-a-static-react-site-to-google-cloud-platform-55ff0bd0f509?source=search_post---------18,"There are currently no responses for this story.
Be the first to respond.
Top highlight
Looking at the few guides out there, I was pretty confused when trying to find out how to deploy my app.
I looked at a few guides from Google’s site and some Stack Overflow posts but didn’t find them too helpful.
I found a great resource which got me 70% there on MDN:
developer.mozilla.org
Filling in a few gaps I was able to deploy my static React site. So here’s my guide, in a few easy steps.
Note: we’ll be deploying a static site which uses React to render the Front End. No backend/server is hooked up in this example.
3. Create a bucket in GCP. You can keep the default values, just click create.
We’ll be using this bucket to easily upload our build folder files into GCP. Afterwards we’ll transfer the files to our GCP project. Take note of the bucket name as we’ll be using this later. This bucket’s name is straight-veld-8658
4. Once the bucket has been created, click into it and select upload files. Browse to your project directory and upload the entirebuild folder.
5. Another thing we need is an app.yaml file. This file is a config file that tells the App Engine how to map the URLs to static files. I’ve used the same app.yaml file as provided in the sample-app of MDN’s tutorial (but have changed the website directory to build. It looks like this:
Upload this file to the bucket as well.
Your bucket should now be populated with the following files:
6. On the same page find the icon that lets you open a Google Cloud Shell to your app instance. Click on it and open the shell.
Now we’ll upload the build directory and app.yaml file that we uploaded into the straight-veld-8658 bucket into our instance so we can launch the app. Use the following commands:
Note: the format of the gsutil rsync command is as follows : gsutil rsync -r [source] [destination] (-r means sync files recursively)
You can make sure that the files are there. When you cd test-app and ls you should see both app.yaml and build
7. Deploy the app by running gcloud app deploy in the shell. You should see some sort of success message indicating the app is served. It should also provide you with the url you can visit to see the app.
Generally this url is in the following format unless you’ve linked a custom domain name to it: https://[app_name].appspot.com
Google Cloud community articles and blogs
694 
25
694 claps
694 
25
Written by
Learn more about me here: https://dddotcom.github.io/ !
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Learn more about me here: https://dddotcom.github.io/ !
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@hbmy289/how-to-set-up-a-free-micro-vps-on-google-cloud-platform-bddee893ac09?source=search_post---------19,"Sign in
There are currently no responses for this story.
Be the first to respond.
HBMY 289
Nov 21, 2018·5 min read
While there are a lot of providers that offer smaller VPS you can test for free for a certain time range, there are not many that are permanently free. This article will focus on setting up an f1-micro VPS on Google Cloud Platform that is advertised as “always free”. Google offers a very broad range of possible VPS configurations and in contrast to other providers, the free tier is rather hidden in all the available options. This article will guide you through the setup process step by step.
When testing Google cloud computing for the first time you will currently also get a voucher that allows renting a wide range of non-free VPS configurations up to a total amount of 300$ in the first 12 months.
If you already have a valid account at Google Cloud Platform (either in test phase or upgraded) you can directly go to Set up an f1-micro instance. Otherwise, you first need to get a new test account.
Visit the Google cloud console (https://console.cloud.google.com/getting-started) and press Try For Free.
Fill out all necessary information on the displayed dialog. You will be required to enter valid credit card information. Google will not charge your credit card even if you use up all of the 300$ that is part of the test phase.
After entering all the required information, the free-trial phase is started.
The f1-micro VPS is part of Google’s “always free” products. If the 12 month test period has ended or if you used up all the credit of your voucher, you will not be able to use these free products anymore. In this case, you will have to upgrade your account, enabling Google to charge your credit card. However, the f1-micro VPS will still be free. Be aware that you might be credited in this case if you use more than the free network egress traffic of 1 GB per month (excluding Australia and China).
Although setting up an f1-micro VPS as shown in this article will not generate any cost for you, it could happen that due to some misconfiguration you are actually using a non-free product.
I cannot take responsibility for any cost that might be generated on your account, especially when exceeding the allowed traffic volume of 1 GB per month. Be sure to use the correct configuration and check your balance repeatedly to avoid any unwanted charges!
Visit the compute engine section of the Google Cloud Platform (https://console.cloud.google.com/compute/) and create a new instance.
The next dialog will allow you to define the configuration of your VPS. You can also choose a much more powerful setup, but only the correct F1-micro settings will result in a free VPS.
Choose a meaningful name for your new VPS and select micro (1 shared CPU) as the machine type for the N1 series. Only instances located in certain US regions will be part of the “always free” deal. At the time of writing this included the regions us-central1 (Iowa), us-east1 (South Carolina) and us-west1 (Oregon).
IMPORTANT!
Check the text next to your settings. It has to look like below and show a free usage time that is equivalent to the number of days in the current month times 24 hours (720 hours for November with 30 days).
Only if this text is displayed your VPS instance will be free of charge!
Click on Change in the Boot disk section. Optionally you can change the pre-installed operating system you want to use. More importantly, you can increase the amount of persistent disk space in this dialog. Increase the value to 30 (included in the “always free” package) and leave the dialog using the Select button.
The firewall section allows to automatically add exceptions for port 80 and 443 if you plan to use your VPS for a web server. Firewall rules can also be added and edited later.
Finally, finish the process by hitting Create.
After a few seconds, you will have a running f1-micro instance. You can access it via the SSH drop-down menu.
Use the first option in the menu to open an ssh-shell in a new browser window to access the VPS.
I hope you liked this short article and now can start using your micro-VPS instance for a lot of cool stuff.
HBMY289
607 
4
607 
607 
4
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/running-a-mean-stack-on-google-cloud-platform-with-kubernetes-149ca81c2b5d?source=search_post---------20,"There are currently no responses for this story.
Be the first to respond.
In my most recent post, I talked about running a MEAN stack with Docker Containers.
Manually deploying Containers is all fine and dandy, but is rather fragile and clumsy. What happens if the app crashes? How can the app be updated? Rolled back?
This post is going to be a longer deep dive into the world of containers. I am assuming you have read / completed part one. There is a lot of content, but if you just skim through, it should take only around 15 minutes for all the steps!
Thankfully, there is a system we can use to manage our containers in a cluster environment called Kubernetes. Even better, Google has a managed version of Kubernetes called Google Container Engine so you can get up and running in minutes.
At Google, it is rare that someone actually logs into a production machine to perform updates. With millions of servers, you can see how this would become impossible. Manually logging into servers also leaves room for human error.
Instead, we use a system called Borg to do the management for us.
In a nutshell, you tell Borg to run X copies of your job, and Borg will do it. Job dies? Borg will start a new one. Health check failing? Borg will shut that job down and start a new one.
We took the lessons learned from Borg, and made an open source project called Kubernetes. Kubernetes lets you manage a cluster of machines and run containers on top of it. Pretty cool stuff, and it just reached version 1.0!
Before we jump in and start kube’ing it up, it’s important to understand some of the fundamentals of Kubernetes.
Why would you want to have a group of containers instead of just a single container? Let’s say you had a log processor, a web server, and a database. If you couldn't use Pods, you would have to bundle the log processor in the web server and database containers, and each time you updated one you would have to update the other. With Pods, you can just reuse the same log processor for both the web server and database.
In my previous post, I used off-the-shelf containers to keep things simple.
I had a stock MongoDB container and a stock Node.js container. The Mongo container ran fine without any modification. However, I had to manually enter the Node container to pull and run the code. Obviously this isn't ideal in Kubernetes land, as you aren't supposed to log into your servers!
Instead, you have to build a custom container that has the code already inside it and runs automatically.
To do this, you need to use more Docker. Make sure you have the latest version installed for the rest of this tutorial.
Getting the code:
Before starting, let’s get some code to run. You can follow along on your personal machine or a Linux VM in the cloud. I recommend using Linux or a Linux VM; running Docker on Mac and Windows is outside the scope of this tutorial.
This is the same sample app we ran before. The second line just moves everything from the EmployeeDB subfolder up into the app folder so it’s easier to access. The third line, once again, replaces the hardcoded localhost with the mongo proxy.
Building the Docker image:
First, you need a Dockerfile. This is basically the list of instructions Docker uses to build a container image.
Here is the Dockerfile for the web server:
Dockerfiles are pretty self explanatory, and this one is dead simple.
First, it uses the official Node.js image as the base image.
Then, it creates a folder to store the code, cds into that directory, copies the code in, and installs the dependencies with npm.
Finally, it specifies the command Docker should run when the container starts, which is to start the app.
Right now, the directory should look like this:
Let’s build.
This will build a new Docker image for your app. This might take a few minutes as it is downloading and building everything.
Side Note: It’s good practice to put a user id in front of the image name, for example:
But I'm going to ignore this practice in this tutorial.
After that is done, test it out:
At this point, you should have a server running on http://localhost:3000 (If you are on Mac or Windows, this won't be so simple). The website will error out as there is no database running, but we know it works!
Now you have a custom Docker image, you have to actually access it from the cloud.
As we are going to be using the image with Google Container Engine, the best place to push the image is the Google Container Registry. The Container Registry is built on top of Google Cloud Storage, so you get the advantage of scalable storage and very fast access from Container Engine.
First, make sure you have the latest version of the Google Cloud SDK installed.
Windows users click here.
For Linux/Mac:
Then, make sure you log in and update.
Now you can push the container. We need our Google Cloud Project ID (we made one in part one).
After some time, it will finish. You can check the console to see the container has been pushed up.
So now you have the custom container, let’s create a cluster to run it.
Currently, a cluster can be as small as one machine to as big as 100 machines. You can pick any machine type you want, so you can have a cluster of a single f1-micro instance, 100 n1-standard-32 instances (3,200 cores!), and anything in between.
For this tutorial I'm going to use the following:
There are two ways to create this cluster. Take your pick.
After a few minutes, you should see this in the console.
Three things need to be created:
To create the disk, run this:
Pretty simple, just pick the same zone as your cluster and an appropriate disk size for your application.
Now, we need to create a Replication Controller that will run the database. I’m using a Replication Controller and not a Pod, because if a standalone Pod dies, it won't restart automatically.
Pretty straightforward stuff. We call the controller mongo-controller, specify one replica, and open the appropriate ports. The image is mongo, which is the off the shelf MongoDB image.
The volumes section creates the volume for Kubernetes to use. There is a Google Container Engine specific gcePersistentDisk section that maps the disk we made into a Kubernetes volume, and we mount the volume into the /data/db directory (as described in the MongoDB Docker documentation)
Now we have the Controller, let’s create the Service
Again, pretty simple stuff. We “select” the mongo Controller to be served, open up the ports, and call the service mongo.
This is just like the “link” command line option we used with Docker in my previous post. Instead of connecting to localhost, we connect to mongo, and Kubernetes redirects traffic to the mongo service!
At this point, the local directory looks like this
First, let’s “log in” to the cluster
Now create the controller.
And the Service.
kubectl is the Kubernetes command line tool (automatically installed with the Google Cloud SDK). We are just creating the resources specified in the files.
At this point, the database is spinning up! You can check progress with the following command:
Once you see the mongo pod in running status, we are good to go!
Now the database is running, let’s start the web server.
We need two things:
Let’s look at the Replication Controller configuration
Here, we create a controller called web-controller, and we tell it to create two replicas. Replicas of what you ask? You may notice the template section looks just like a Pod configuration, and that's because it is. We are creating a Pod with our custom Node.js container and exposing port 3000.
Now for the Service
Notice two things here:
At this point, the local directory looks like this
Create the Controller.
And the Service.
And check the status.
Once you see the web pods in running status, we are good to go!
At this point, everything is up and running. The architecture looks something like this:
By default, port 80 should be open on the load balancer. In order to find the IP address of our app, run this command:
If you go to the IP address listed, you should see the app up and running!
And the Database works!
By using Container Engine and Kubernetes, we have a very robust, container based MEAN stack running in production.
In my next post, I'll cover how to setup a MongoDB replica set. This is very important for running in production.
Hopefully I can do some more posts about advanced Kubernetes topics such as changing the cluster size and number of Node.js web server replicas, using different environments (dev, staging, prod) on the same cluster, and doing rolling updates.
Google Cloud community articles and blogs
246 
14
Some rights reserved

246 claps
246 
14
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/going-serverless-on-google-cloud-platform-b770803c5ee7?source=search_post---------21,"There are currently no responses for this story.
Be the first to respond.
Calling something serverless is so hot right now. Every developer tools company found out and suddenly everything is #serverless.
In fact, in the Google Cloud Next 17 Keynotes, so many things were called serverless. These products were amazing in their own right, but the word serverless was thrown around so loosely that it was cringeworthy at times.
As expected, people are upset about this.
So what the hell is serverless? Again, everyone has their own opinions on what serverless is.
So let me define my definition.
I’m going to break down “cloud” products into four different categories. The names are basically made up, but this is what makes sense to me.
SaaS products are ready to use out of the box by end users. There is nothing to provision. You either pay for what you use, or purchase bulk credits of some sort.
Serverless products are used by developers to build higher order products. There is nothing to provision, but there may be some knobs to tune the infrastructure. You pay for exactly what you use, nothing more.
Managed products are used by developers to build higher order products. Developers need to provision capacity and tune things, but don’t need to maintain the infrastructure. You pay for excess infrastructure when it is not being used.
IaaS products mirror on-prem solutions, except you can provision resources in seconds/minutes instead of days/months. They require explicit provisioning and maintenance. You pay for excess infrastructure when it is not being used.
Do you agree with this categories? Am I TOTALLY WRONG IN EVERY WAY!!?Let me know in the comments!
I am going to map most of Google Cloud’s products to one of these four categories. Of course, arguments can be made to fit products in multiple categories, but I’m going to make the tough calls and put them in only one. I think this will make it very clear how I think about these categories.
G SuiteThe obvious entry into this category. G Suite is ready to be used by the end user, no need for developers to do anything. Of course, Apps Script can be considered serverless, but as it depends so much on the rest of G Suite, it feels right in this category.
Data StudioData studio lets you build dashboards for your Google Analytics, Cloud SQL, BigQuery, etc data. There is a lot of drag and drop involved, and end users can be productive without developer support.
Cloud FunctionsThe obvious entry into this category. AWS Lambda started the FaaS revolution, and Cloud Functions follows a similar pattern. You write your functions, specify when they trigger, tune how much memory/cpu is allocated per call, and deploy. Everything else is handled for you. You pay per function invocation, so it is exactly matched to your traffic.
Cloud RunGoogle’s serverless container offering, Cloud Run is similar to Cloud Functions or App Engine, except you deploy a container instead of code. Everything else is handled for you. Cloud Run charges per incoming request, so you pay for exactly what you use.
You can also use Cloud Run on GKE, and in that case it becomes more managed than serverless. See the GKE section for more details.
Cloud StorageCloud Storage is similar to Google Drive, but isn’t usable by end users. It requires a developer to build higher order systems on top. You don’t provision anything, and pay for exactly what you use. You can tune the latency and price by choosing the bucket type (Nearline, Multi-Regional, etc).
App Engine StandardThe first Google Cloud product. Write your code, tune how much memory/cpu you want, and deploy. Everything else is handled for you. App Engine Standard scales to exactly match your traffic and scales to zero when you have none, so you pay for exactly what you use.
Cloud DatastoreThe first NoSQL database at Google Cloud. You don’t provision anything, just read and write data. You pay per read/write and for the exact storage you use, and it scales automatically.
Firebase Realtime Database / HostingSimilar to Cloud Datastore, but used directly on the frontend. You don’t provision anything, just read and write data. You pay for data transfer and for the exact storage you use, and it scales automatically.
Cloud FirestoreThis next generation database brings the best from Datastore and Firebase Realtime Database together. You can use it directly from the frontend like the Realtime Database and from the server like Datastore, and get advanced query support and strong consistency. You don’t provision anything, just read and write data. You pay per read/write and for the exact storage you use, and it scales automatically.
BigQueryBigQuery is Google’s data warehouse service, letting users query TB of data with SQL. There is nothing to manage, just dump/stream your data in and query. You pay to store data and query data, no need to provision machines or storage or set up indexes.
Pub/SubPub/Sub is Google’s many-to-many async messaging bus (think Apache Kafka or RabbitMQ). No need to provision anything, and it scales to millions of messages a second instantly. Pub/Sub charges per message, so you pay for exactly what you use.
Stackdriver (Logs, Monitoring, Debug, Tracing)The Stackdriver tools let you access powerful tools without needs to set anything up. They are all have a free tier, but you can pay to monitor more resources and other clouds like AWS. There is nothing to provision or tune and you pay per resource monitored so there is no extra spending.
Cloud DataflowCloud Dataflow uses Apache Beam to create fully managed ETL, batch processing, and streaming analytics pipelines. It autoscales to process your pipeline, and scales back to zero when there is no more work left.
Kubernetes EngineGoogle Kubernetes Engine creates a Kubernetes Cluster for you in one click. Google manages the Master and Nodes and auto-updates them for you. You have to provision a cluster ahead of time(though there are some auto-scaling capabilities) so you are paying for unused resources.
App Engine FlexibleApp Engine Flexible is similar to Standard, but runs on VMs instead of a sandbox. While this gives it more “flexibility” over what it can run, you lose the “serverless” magic of standard. Auto-scaling and deploying is not as fast, but the biggest difference is you have to always have one VM instance running, meaning you are paying for unused resources.
Cloud SQLCloud SQL gives you managed MySQL and PostgreSQL instances. Google worries about backups, failover replication, etc, which reduces the operational overhead of running a database. While there are some auto-scaling capabilities, you need to provision a machine and disk ahead of time, which means paying for unused resources.
Cloud BigtableCloud Bigtable is Google’s HBase compatible NoSQL database. While storage scales automatically (you pick SSD or HDD), you need to choose how many nodes you want to tune performance. More nodes = more performance, but of course you can over/under provision.
Cloud SpannerCloud Spanner is similar to BigTable except it is globally consistent and relational instead of NoSQL. This is pretty magical! You need to choose how many nodes you want to tune performance. More nodes = more performance, but of course you can over/under provision.
Cloud Load Balancing (L4/L7)Both the L4 and L7 load balancers offered by Google Cloud are fully managed services that require no provisioning, pre-warming, or tuning. The only reason this is not in the Serverless category is you have to pay a flat rate to launch the load balancer regardless of how much traffic it handles.
Cloud DataprocCloud Dataproc launches a managed Spark/Hadoop cluster. You need to specify how many VMs you want and tune them, but after that you can use the cluster without any additional setup. You pay for the VMs even when they are not being used.
Cloud Machine Learning EngineCloud Machine Learning Engine creates and manages a distributed TensorFlow cluster for you to train and serve your models on. You have to provision a cluster, but the cluster is fully managed and you can just submit jobs to it.
Compute EngineThese are Linux and Windows VMs. While you can autoscale them using Managed Instance Groups, and you can do cool things like have the disks automatically grow, at the end of the day you are 100% responsible.
Cloud LauncherCloud Launcher lets you deploy pre-configured apps on Compute Engine. Though the initial setup is automatic, you are responsible for maintaining the servers and pay for unused resources.
What do you think? Does this make sense? Is this whole debate worth it? Is there something I missed? Please let me know!
Google Cloud community articles and blogs
206 
8
206 claps
206 
8
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/rar-design/%E5%8D%81%E5%88%86%E9%90%98%E7%94%A8-google-cloud-platform-gcp-%E5%85%8D%E8%B2%BB%E9%A1%8D%E5%BA%A6%E6%9E%B6%E8%A8%AD-wordpress-%E7%B6%B2%E7%AB%99-ae5413d4805a?source=search_post---------22,"There are currently no responses for this story.
Be the first to respond.
可以架設 WordPress 網站的主機有很多家，但 Google 是我體驗過效能不錯且機制友善的方案，若讀者是第一次摸 WordPress，相當推薦！
我們可以透過目前 Google 所提供的 $300 美金額度，在一年內可以體驗使用，這篇文章主要是透過圖文方式，快速地示範如何用這免費額度，來架設我們的第一個 WordPress 網站。
若對於這系列文章有興趣，可以發摟 林育正 Riven 或是透過 Facebook 臉書 聯繫我 😃
事前只要預先準備 Google 帳號，以及一張信用卡就可以囉。並點擊以下網頁開始申請：
個人使用練習的帳戶類型，選擇「未登記稅藉的個人」也會比較單純。
如果已經是公司等商業專案用途，記得確實填寫噢～
透過 Google 的快速安裝服務，立刻取得 WordPress 網站。
找到「WordPress Certified by Bitnami」然後按「在 COMPUTE ENGINE 上啟動」。
接著呢我們來開始建立這項部署作業⋯
這邊只要輸入部署名稱之後呢，按照上圖的設定即可。
其中asia-east-a、asia-east-b、asia-east-c 是位在台灣彰化的主機，較推薦。
Machine type 選擇 小型(1個共用)
Boot disk type 選 SSD ，並輸入 30GB 的容量。
完成設定之後按下「部署」就可以囉！
如此一來就完成了無痛在 GCP 上架設 WordPress 網頁啦，如果還算受用的話請可以拍手鼓勵最多 50 下！
改天有空再分享後台操作～​若對於這系列文章有興趣，可以發摟 林育正 Riven 或是透過 Facebook 臉書 聯繫我 🙂
設計攻略
959 
959 claps
959 
Written by
是數位遊牧型態的設計師💻 喜歡邊旅行邊工作的生活，逐網路、插座與咖啡而居。期待能夠將艱難的設計與開發技術，用麻瓜都能夠聽得懂的話，說給每一個人聽。更多關於我：riven.design
關於設計的技巧 × 知識 × 方法 × 教學 × 攻略
Written by
是數位遊牧型態的設計師💻 喜歡邊旅行邊工作的生活，逐網路、插座與咖啡而居。期待能夠將艱難的設計與開發技術，用麻瓜都能夠聽得懂的話，說給每一個人聽。更多關於我：riven.design
關於設計的技巧 × 知識 × 方法 × 教學 × 攻略
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/docker-swarm-on-google-cloud-platform-c9925bd7863c?source=search_post---------23,"There are currently no responses for this story.
Be the first to respond.
There are some interesting things going on with the new Docker 1.12 release. Docker is bundling Swarm into Docker itself, as well as upgrading Swarm with more mature container orchestration abilities.
Swarm now joins Kubernetes, Mesos, and Nomad as a fully-fledged orchestration engine. With these new orchestration abilities, I wanted to take another look at Swarm. I also like the name “Swarm” ;)
Swarm does not yet have an out-of-the-box option on Google Cloud. Hopefully this is added soon, but until then let’s look at how you can manually set up a Swarm cluster on Google Cloud Platform.
All in all, you should have a fully functional Swarm cluster in about 15 minutes!
In fact, Google Compute Engine is so fast at provisioning VMs that if you run the node creation and setup in parallel you can create a cluster in around five minutes! Crazy fast!
Use this script for a fully automated install!
Note: Swarm 1.12 is still very young. Documentation is sparse and still evolving. I expect things to get better very soon!
You need Docker 1.12 to get the upgraded Swarm features. Here is what I have:
You also need the Google Cloud SDK installed, as well as a Google Cloud project. Once you do that, make sure you log in:
The first step is to create the Swarm nodes.
Ideally you would use a Managed Instance Group, but I’ll save that for another tutorial.
Let us create a manager node and a worker node.
Use docker-machine to create the manager node:
Replace <YOUR_PROJECT_ID> with your project ID. Also feel free to change the zone, machine type, and disk size. The important thing to do is tag this instance with the “swarm-cluster” tag, which will let us open firewall ports later on.
In about five minutes the manager will be created.
Now we need to set up this machine as a Swarm manager
Your manager is now created!
Create a worker node the same way as the manager:
Now we need to get the IP address of the manager so we can join the Swarm.
Use the internal IP for the Swarm Manager to connect your worker to the Swarm. The default networking setting opens all ports on the internal subnet so you don’t have to mess with firewall rules.
Repeat these steps to add more workers to the Swarm.
Log back into the Swarm Manager to start executing commands.
For example, you can see all the nodes in the Swarm:
You are done with the cluster setup!
Creating a service is straightforward. It’s basically the same commands you would use in normal Docker.
For example, to start a single nginx server on port 80, run:
And we can see the process running.
Swarm will make sure that the replica is always running. We can also scale up and down the number of replicas with a single command:
Pretty cool stuff! There are more complicated things you can do, but documentation is sparse. The launch blog post has the best information so far.
Now you have nginx running in your Swarm, you have to open it up to the outside world. By default, Swarm will expose the service on the specified port on every node in the Swarm. This is very similar to creating a Service in Kubernetes with NodePort. We need to expose this port!
Side Note: I really hope Docker adds in native support for Google Cloud Platform so that these things are automatic, similar to how they function in Kubernetes.
The easiest thing to do is open the port on a node, and use the node IP address for your website or service.
I would use this option for small clusters that have a single manager and a few workers.
Open the port on the Swarm instances
Now get the external IP address of the nodes.
Use the external IP of one of the nodes to reach your service. I recommend using the manager’s IP address.
Using the same steps as option one, you can use all the external IP addresses with Round Robin DNS. This basically gives you a form of load balancing for free! The only problem with this is if you start removing or adding nodes in your cluster, you need to update your DNS settings every time. DNS clients also cache heavily, so if you scale down it is possible your users will hit nodes that no longer exist.
If you have multiple managers, I would use this method to provide a simple form of load balancing and fault tolerance.
This is the most robust, but also the most complicated, method for exposing your service. When you create a Network Load Balancer, you get a single IP address, but traffic is sent to all the nodes in the Swarm. Additionally, you can set up a health check so if a node goes down, traffic is not sent to it.
If you want the best reliability, or have larger cluster that may be spread over multiple zones for high availability, I recommend this option.
I will set up a TCP Load Balancer. You can also set up a more powerful HTTPS Load Balancer if that is appropriate for your service.
While you can do this on the command line, I find the UI to be more intuitive.
Open the Load Balancer page.
Click “Create Load Balancer”
Now click “Start configuration” for the TCP Load Balancer
Give your Load Balancer a name
Now click “Backend configuration”
Select the region your Swarm is in, then click “Select existing instances.” Add all the Swarm nodes.
Now create a health check.
Give your health check a name, and configure the numbers how you see fit. I used the defaults. We are going to ping port 80 (where the nginx service lives) every 5 seconds to make sure the node is healthy. Save and continue.
Now go to the Frontend Configuration, and specify the port for your Swarm service.
Finally, click “Create,” and the Load Balancer will spin up in a few minutes.
You can get the IP address for your Load Balancer with this command:
With the new 1.12 release of Docker, Swarm was very easy to setup and use. Once it is officially released, it will be even easier. Great job by the Docker team!
I hope Docker adds more documentation and examples, and I really hope they add support for native Google Cloud features so people don’t need to mess with Firewalls and Load Balancers!
I also plan on doing a comparison between Swarm and Kubernetes. I found there to be a lot of differences and similarities. Stay tuned for that!
Google Cloud community articles and blogs
189 
14
189 claps
189 
14
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/building-token-recommender-in-google-cloud-platform-1be5a54698eb?source=search_post---------24,"There are currently no responses for this story.
Be the first to respond.
In this article I will guide you through the process of creating an ERC20 token recommendation system built with TensorFlow, Cloud Machine Learning Engine, Cloud Endpoints, and App Engine. The solution is based on the tutorial article by Google. The data used for training the recommendation system is taken from our public Ethereum dataset in BigQuery.
The article is broken down into the following parts:
The collaborative filtering technique is a powerful method for generating user recommendations. Collaborative filtering relies only on observed user behavior to make recommendations — no profile data or content access is necessary.
The technique is based on the following observations:
The collaborative filtering problem can be solved using matrix factorization. Suppose you have a matrix consisting of user IDs and their interactions with your products. Each row corresponds to a unique user, and each column corresponds to an item. The item could be a product in a catalog, an article, or a token. Each entry in the matrix captures a user’s rating or preference for a single item. The rating could be explicit, directly generated by user feedback, or it could be implicit, based on user purchases or the number of interactions with an article or a token.
The matrix factorization method assumes that there is a set of attributes common to all items, with items differing in the degree to which they express these attributes. Furthermore, the matrix factorization method assumes that each user has their own expression for each of these attributes, independent of the items. In this way, a user’s item rating can be approximated by summing the user’s strength for each attribute weighted by the degree to which the item expresses this attribute. These attributes are sometimes called hidden or latent factors.
To translate the existence of latent factors into the matrix of ratings, you do this: for a set of users U of size u and items I of size i, you pick an arbitrary number k of latent factors and factorize the large matrix R into two much smaller matrices X (the “row factor”) and Y (the “column factor”). Matrix X has dimension u × k, and Y has dimension k × i.
To calculate the rating of user u for item i, you take the dot product of the two vectors. The loss function can be defined as Root-mean-square error (RMSE) between the actual rating and the rating calculated from the latent factors.
For our token recommender we will use the percentage of supply a user is holding for a particular token, as the implicit rating in the user rating matrix.
Check out the code and install the dependencies:
Query token ratings from BigQuery:
Run the following query in BigQuery and export the results to a GCS bucket e.g.gs://your_bucket/data/token_balances.csv
The above SQL queries top 1000 tokens by transfers count, calculates the balances for each token, and outputs (token_address, user_address, rating) triples. Rating there is calculated as the percentage of supply held by the user. This filter — where balance/supply * 100 > 0.001 — prevents airdrops appearing in the result.
Understand the code structure
The model code is contained in the wals_ml_engine directory. The code's high-level functionality is implemented by the following files:
The csv file is loaded in the model.py file:
Then the following arrays are created:
These triplets are then randomly split into test and train datasets and converted to sparse matrices:
The WALS model is created in wals_model method in wals.py, and the factorization is done in simple_train method in the same file. The result are the row and column factors in numpy format.
Train the model locally and in Google ML Engine
To train the model locally run the following command, specifying the path to the CSV file exported on the previous step:
The output should look like the following:
The RMSE corresponds to the average error in the predicted ratings compared to the test set. On average, each rating produced by the algorithm is within ± 0.95 percentage points of the actual user rating in the test set. The WALS algorithm performs much better with tuned hyperparameters, as shown in the following section.
To run it in Cloud ML Engine:
You can monitor the status and output of the job on the Jobs page of the ML Engine section of the GCP Console. Click Logs to view the job output.
After factorization, the factor matrices are saved in four separate files in numpy format so they can be used to perform recommendations:
When training locally, you can find those files under wals_ml_engine/jobs directory.
To test out the recommendations use the following code:
You can find the configuration file for hyperparameters tuning here.
To tune the hyperparameters, first change the BUCKET variable in mltrain.sh to your bucket. Then run the following command:
You can see the progress of tuning in Cloud ML Engine console. The results of hyperparameter tuning are stored in the Cloud ML Engine job data, which you can access in the Jobs page. The job results include the best RMSE score across all trials of the summary metric.
Below are the best parameters from my tuning, which you can also find in the repository:
The error though is just slightly lower comparing to the default parameters:
You can find REST API definition in Swagger format for serving token recommendations in the repository: openapi.yaml. The implementation of the API for App Engine is in the main.py file.
First, prepare to deploy the API endpoint service:
The output of this command should look like the following:
Run the provided command:
Create the bucket where the app will read the model from:
Upload the token_balances.csv file to the bucket:
Train the model and upload the model files to the bucket:
Create an App Engine application:
Prepare to deploy the App Engine application:
The following output appears:
Run the provided command:
After the app is deployed you will be able to query the api: https://${project_id}.appspot.com/recommendation?user_address=0x8c373ed467f3eabefd8633b52f4e1b2df00c9fe8&num_recs=5 (replace ${project_id} with your value)
You can try out the recommendations at similarcoins.com. Read our article where we describe how we evaluated and improved the recommendation system:
towardsdatascience.com
Google Cloud community articles and blogs
275 
1
275 claps
275 
1
Written by
Creator of https://github.com/blockchain-etl, Co-founder of https://d5.ai and https://nansen.ai, Google Cloud GDE, AWS Certified Solutions Architect
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Creator of https://github.com/blockchain-etl, Co-founder of https://d5.ai and https://nansen.ai, Google Cloud GDE, AWS Certified Solutions Architect
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/developing-a-cryptocurrency-price-monitor-using-firebase-and-google-cloud-platform-34d5538f73f6?source=search_post---------25,"There are currently no responses for this story.
Be the first to respond.
In this article I demonstrate how to build a cryptocurrency price monitoring app using Firebase and Google Cloud Platform (GCP). The app receives notifications whenever the price of Bitcoin or Ethereum change in the market. Users can configure minimum and maximum price thresholds for each cryptocurrency, and we use these settings to determine which users to notify for any given price change.
The inspiration for this app came from an article published by the Pusher community (read part 1 and part 2 here). Their cryptocurrency price alerts app uses Pusher for delivering notifications, which seems to be using Firebase Cloud Messaging (FCM) under the hood. Furthermore they employ a SQLite database and a standalone back-end server developed in Go to implement the necessary functionality. As I was reading that article I couldn’t help but think that the use case in question would make a great demo for Firebase and GCP as well. This article and the associated app are results of that notion.
The version of the app we develop does not require deploying a standalone database or a back-end server. We implement all the server-side functionality using various Platform-as-a-Service (PaaS) and serverless products readily available in the Firebase and GCP ecosystems. Specifically, our example app demonstrates the following features.
In the client-side we use the Firebase Android SDK to directly interact with Firestore and FCM. This precludes the need for implementing a separate REST API, or having to program any HTTP interactions in the Android app.
Our app uses two Firestore collections.
There’s no need to manually create these collections. They get created automatically as our app goes into action. But if you wish to test the app by adding some sample data, figure 1 shows what your Firestore database should look like.
As part of setting up Firestore, we deploy the following security rules. These essentially make the prices collection read-only, and the prefs collection read-write. Any other collections you happen to have in the same Firestore database will be inaccessible to the app.
We don’t implement user authentication in this demo, but it is fairly straightforward to do so using Firebase Auth. If you decide to explore that option, you can use the unique user IDs provided by Firebase Auth as the document IDs in the prefs collection. That would also allow you to further tighten up your security rules, allowing each user to write only to their designated documents.
Also note that Firebase security rules only apply to the client apps (Android users in this case). In the back-end we use Firebase Admin SDKs, which bypass security rules, and access the database as a privileged user.
The full source of the app can be found in my firecloud GitHub repo. The whole thing amounts to about 170 lines of Kotlin code, plus the usual Android resources and manifest files. We start a Firestore realtime listener on the prices collection when the app launches. This way the app always displays the latest cryptocurrency prices stored in Firestore. Listing 1 shows the relevant code fragment from the MainActivity.
The MainActivity layout contains two text views with the IDs btc and eth. Note that we use the same IDs for Firestore documents in the prices collection. Therefore we can employ the trick in lines 16–18 to map each Firestore document to a text view in the UI.
Tapping on a text view launches the settings dialog for the corresponding cryptocurrency. The user can specify a minimum and a maximum threshold and save the settings. The idea is that the app will notify the user whenever the price drops below the min threshold or exceeds the max threshold. Listing 2 shows the method that saves the settings to Firestore.
In addition to the threshold values, we also save app instance’s registration token to Firestore (line 6). This is used later when we want to notify the device via FCM. We use the stable Firebase instance ID as the document identifier. In an app that implements user authentication, we can use the unique user ID here instead.
The await() method used in listing 3 (lines 2 and 9) is an extension method we have added to the Android GMS Core’s Task class. This makes it easier to use the Task API with Kotlin coroutines. Listing 3 shows the implementation of the await() method.
Finally, we add the FCM Android SDK to the app, and extend the FirebaseMessagingService as shown in listing 4 in order to receive push notifications.
This service handles incoming push notifications when the app is in foreground. It displays a simple pop-up with the notification payload. When the app is in the background the notification will be delivered to the Android’s system notification tray.
That’s pretty much all the exciting bits in the client application. Now lets look at the back-end components of the app.
We implement an App Engine service in Go (v1.11) to periodically check the cryptocurrency market prices, and save the results to Firestore. Our implementation is comprised of following files:
Full source code of the service can be found in the firecloud repo. You can run the following command to directly import the code into your GOPATH:
The price checker service uses the Firebase Admin SDK for Go to access Firestore. Since our code is going to be deployed in App Engine, we can let the SDK auto discover Google Application Default Credentials (ADC) to authorize Firestore API calls. This means we can initialize the Admin SDK with a minimal configuration as shown in listing 5.
Note that we are only passing a Context to the firebase.NewApp() function. The SDK is able to auto discover authorization credentials and any other settings required to access Firestore (e.g. GCP project ID). The firestore.Client that we subsequently obtain at line 19 provides access to the same Firestore database used by the Android client apps.
We use the CryptoCompare REST API to fetch the latest prices of Bitcoin and Ethereum. Listing 6 shows how the discovered prices are saved to Firestore.
We execute a batched write on Firestore to update the prices of both Bitcoin and Ethereum in a single operation. Since our Android app is already listening to updates in the prices collection, these changes become immediately visible to the users.
We expose this service as an HTTP endpoint at the URL path /fetch. Next, in order to keep our app data up-to-date, we need to instruct Google App Engine to invoke our service periodically. This is done by writing a cron.yaml file as shown in listing 7.
To test this service locally, set the GOOGLE_APPLICATION_CREDENTIALS environment variable to point to a service account JSON file downloaded from your Firebase project. Then execute the main.go file of the service:
This starts the service on port 8080, and you can try it out by sending a request to http://localhost:8080/fetch. You will see the prices collection getting updated in the Firebase console as a result. The updates will also appear on the Android client app, if it happens to be running at the time. Note that when testing locally, you must manually invoke the /fetch endpoint of the service. The cron.yaml file only takes effect once deployed to the cloud.
The rate at which cryptocurrency prices change in the real world may not be enough to trigger many updates while testing the service. If this becomes a problem you may forego the CryptoCompare API, and get the service to produce random pricing data at each invocation. Set the following environment variable prior to launching the service to enable this feature.
To deploy the service to App Engine, install and set up the Google Cloud SDK. Make sure the gcloud command-line utility is configured to manage your GCP/Firebase project.
Then run the following commands from the same directory as the app.yaml file.
The first command deploys the service implementation. The latter starts the scheduled task that periodically invokes the service. Shortly afterwards, you will see entries similar to the following in your App Engine log.
We have also programmed our service to accept requests only from the App Engine cron scheduler. Trying to manually invoke the HTTP endpoint in the cloud will yield a 404 Not Found response. You can change this behavior by removing the CRON_ONLY environment variable from the app.yaml file.
The last piece we need to complete our app is the service that notifies interested users when the cryptocurrency prices change. We already have a service that updates the prices in Firestore. Therefore we can use Cloud Functions for Firebase to implement a serverless function that sends out notifications whenever a new price is written to the prices collection of Firestore. Listing 8 illustrates what this implementation looks like.
We define an onUpdate Firestore trigger for the documents in the prices collection. The ID of the document that is being updated (i.e btc or eth), and its price value can be obtained from the arguments passed into the trigger. We pass these values to the findTargetDevices() helper method, which queries the prefs collection in Firestore to determine the users that should be notified of the price change.
We wish to notify users whose min threshold is higher than the current price, or whose max threshold is lower than it. Since Firestore does not support disjunctive queries (i.e. OR queries), we run two separate queries (lines 33–34), and aggregate the results. This is also where we reference the device registration tokens that the Android clients have saved in Firestore. Finally we use the FCM API in the Admin SDK to send push notifications to the selected users.
Again, notice that we are initializing the Firebase Admin SDK with a minimal configuration (lines 4–5). The SDK auto discovers valid authorization credentials, and connects to the same Firestore database used by Android clients and the App Engine service.
The full implementation of the cloud function is available on GitHub. Use the Firebase functions emulator to test the code locally. Run the following commands from the functions/ directory of the project.
This launches the Firebase emulator shell. You can now directly invoke the function with some sample data as shown below.
Having verified our function works as expected, we can deploy it to the cloud using the Firebase CLI.
You can now either wait for the App Engine service to update the cryptocurrency prices, or enter some sample prices into Firestore manually. It is also possible to manually run the App Engine cron job using the GCP console. Either way, the cloud function will get triggered, and you will be able to see the corresponding logs in the Firebase console.
Figure 2 is a screenshot from the GCP console, showing both App Engine and Cloud Functions logs in the same window. All the entries shown in this figure were produced by a single run of the App Engine service.
Figure 3 shows various screens of the Android client app, including how a notification is being delivered while the app is in the background.
In this post we looked at how to develop a cryptocurrency price monitoring app using several Firebase and GCP products. We used the Firebase Android SDK to directly interact with Google Cloud Firestore and FCM. We implemented a service in Google App Engine to periodically check the market prices of Bitcoin and Ethereum. Finally, we implemented a Google Cloud Function that notifies users of price changes based on the price thresholds configured by individual users. The whole exercise took me a couple of hours, and you can find the full implementation on GitHub.
The back-end functionality of this app is split between Google App Engine and Google Cloud Functions. However, it is also possible to implement all the back-end functionality using App Engine alone. We can program our Go service to send push notifications every time it updates the cryptocurrency prices. In fact, the service I have implemented already supports this, but it is disabled by default. See if you can figure out how to enable push notifications in the App Engine service by going through the code.
Personally, I very much prefer the idea of having distinct services for checking the cryptocurrency prices, and sending push notifications. This makes separation of concerns more explicit while resulting in a more loosely coupled implementation. It also makes each service independently testable and deployable, thus bringing us closer to a microservices architecture. For instance, imagine trying to change how our app checks cryptocurrency prices. With two distinct services in place, we can simply update the price checking service, and never touch the notification sender.
I hope you find this article and the associated demo app useful. Please feel encouraged to reach out with any questions or feedback. If you have any demo app ideas that you would like me to try and implement, I would like to hear them too. As always, you are also welcome to engage with the Firebase community via various opensource Firebase repositories.
Google Cloud community articles and blogs
239 
239 claps
239 
Written by
Software engineer at Google. Enjoys working at the intersection of cloud, mobile and programming languages. Fan of all things tech and open source.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Software engineer at Google. Enjoys working at the intersection of cloud, mobile and programming languages. Fan of all things tech and open source.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/hackernoon/google-cloud-platform-hacker-noon-2ba28a29512f?source=search_post---------26,"There are currently no responses for this story.
Be the first to respond.
This is an email from Get Better Tech Emails via HackerNoon.com, a newsletter by HackerNoon.com.
“Google Cloud Platform and Firebase give Hacker Noon the flexibility to craft a custom publishing platform optimized for technologists,” said Hacker Noon Interim CTO Dane Lyons. “Our serverless infrastructure will generate static content and pipe it into the low latency, low-cost Google Cloud CDN. We’re excited to focus on important product details and worry less about devops and cost optimizations.”
As a startup working to free ourselves from platform dependency, it’s humbling to have Google support our own infrastructure. These resources make our future more secure, our monthly burn rate more manageable, and ultimately provide a stronger partner for serving high volumes of traffic in the long-term. We’re very excited about launching the next iteration of Hacker Noon with Google Cloud Platform and Firebase!
Deploy A Backend App As An Android Engineer (& Part 2) by Adam Hurwitz
Android App Architectures: Example of MVP with Kotlin by Rohit Surwase
Can Google’s AI Make Better AI Than the Googlers? by The Next Web
Google’s AI Based AutoDraw Turns Your Rough Scribbles Into Beautiful Icons For Free by Vinoth George
Chrome Extension Development: Lessons Learned by Sam Jarman
Optical Character Recognition With Google Cloud Vision API by Evelyn Chan
Launch a GPU-backed Google Compute Engine instance and setup Tensorflow, Keras and Jupyter by Steve Domin
Google Search Analysis: Rich Search Results and Structured Data by Garrett Vorce
The Art of Searching something on the Internet. by Vikas Yadav
Building Google’s Art and Culture Portrait Matcher by Grant Holtes
70% of People Worry About Fake News — And How Google Combats It by Chhavi Shrivastava
Getting Started with Firebase ML for iOS by Mohammad Azam
Firebase to the Rescue: Dynamic Routing via Hosting + Functions Integration by Peter LoBue
How to Build a Product Loved by Millions and Get Acquired by Google: The Firebase Story by Founder Collective
Infinite Scrolling In Firebase by Linas M
Introduction to Firebase by GeekyAnts
Prototyping with Firebase by David Kerr
Flutter — 5 reasons why you may love it by Paulina Szklarska
I love you Flutter by Shalom Yerushalmy
What are the Google Cloud Platform (GCP) Services? by Reto Meier
An actionable checklist to being a Google Summer of Code student by Chhavi Shrivastava
My (slightly unconventional) path to a Google Internship by Abhimanyu
This Student Talked His Way Into The Googleplex Through Courageous Networking by John Greathouse
Deep Reinforcement Learning in Robotics using Vision by SAGAR SHARMA
Acing Your Product Manager Interview by David E. Weekly
Interview Questions Deconstructed: The Knight’s Dialer by Alex Golec
Google Search — How A Master’s Thesis Became An Idea Worth $70 Billion by Soundarya Balasubramani
Revisiting How Serge and Larry Saw Advertising in 1998 by Gennaro Cuofano
Build a Google Home App with PubNub Functions by Kaushik Ravikumar
How I set up room-cleaning automation with Google Home, Home-Assistant and Xiaomi vacuum cleaner by Muh Hon Cheng
“This Is Going to Be Huge”: Google Founders Collection Comes to Computer History Museum by Computer History Museum
Google + Machine Learning + Getting Started = Awesomeness, and Good Cucumbers by BeeHyve
Instance Segmentation in Google Colab with Custom Dataset by RomRoc
Train Your Machine Learning Models on Google’s GPUs for Free — Forever by Nick Bourdakos
Implement Google Maps in ReactJs by Mohammed Chisti
Let’s make Reusable Web Components: Google Maps by Just Chris
Contributing to Golang using Google’s Pixelbook by Daniela Petruzalek
Getting Started with Go Development in the new Google Pixelbook by Daniela Petruzalek
Google Cirq and the New World of Quantum Programming by Jesus Rodriguez
Building an automated dashboard with Google Sheets (with example) by Nick Boyce
Google Sheets As Your Database by Just Chris
Web Scraping With Google Sheets by SeattleDataGuy
A Recommendation Engine Framework in TensorFlow by James Kirk
Building a Facial Recognition Pipeline with Deep Learning in Tensorflow by Cole Murray
Creating insanely fast image classifiers with MobileNet in TensorFlow by Matt Harvey
9 Things You Should Know About TensorFlow by Cassie Kozyrkov
Query a custom AutoML model with Cloud Functions and Firebase by Sara Robinson
Meeting The YouTube CEO by Felix Josemon
Thanks Google! For giving startups a chance** at independence and prosperity.
Onwards!
David Smooke
*We decided to leave some of our more critical articles of Google, like How Google Will Collapse, off this particular list. But rest assured, this grant won’t impact our editorial line. Whatever tech or company you are in favor or not in favor of, there is always a place for well written stories by tech professionals on Hacker Noon. Contribute today.
**If you are a rapidly growing startup thinking about building on Google Cloud Platform, you can apply for credits yourself! Apply here.
#BlackLivesMatter
739 
3
739 claps
739 
3
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
https://www.davidsmooke.net/ https://hackernoon.com/ Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant,
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
"
https://medium.com/google-cloud/service-discovery-and-configuration-on-google-cloud-platform-spoiler-it-s-built-in-c741eef6fec2?source=search_post---------27,"There are currently no responses for this story.
Be the first to respond.
Service discovery and configuration are required for almost all distributed cloud architectures. Instead of hardcoding the IP address of a service or pushing your client secret into git (which are both terrible things to do) you rely on a secure, consistent, and decentralized system for managing these configuration and environment variables.
Many different services provide this functionality; the top three that come to mind are ZooKeeper, etcd, and Consul. All of these are really awesome, and some even provide additional services such as DNS based discovery, but they require you to maintain and run them in your project, not to mention they cost money in the form of CPU time and disk space!
So let me blow your mind. Did you know that Google Cloud basically gives you this service for free? Yes you heard that right, it’s built right into your Google Cloud project! The best part is you can use it across all of our computing platforms: App Engine, Compute Engine, and Container Engine.
It’s called the metadata server, and it’s pretty awesome.
There are two types of metadata, instance metadata and project metadata.
Instance metadata is used primarily for Compute Engine. It gives you information about your instance such as IP address, zone, machine type, networks, and more. You can also set some custom metadata when you create the instance. This environmental data can be really useful for your app.
Project metadata can be used by ANY app; it doesn’t even need to be running on Google Cloud Platform! It gives your app (or apps) a shared set of environment variables and configs that they can access in a secure fashion. It uses the built-in OAuth and service accounts you already use on App Engine, Compute Engine, and Container Engine, so it’s super easy to use! This is what I will be focusing on for the remainder of this post.
The easiest way to set metadata is to use the gcloud command line tool or the Developers Console on the web. You could also use the Compute Engine API if you wanted to automate some of this stuff.
The Developers Console is self-explanatory:
Here is how to do it with the command line:
With Compute Engine, Container Engine, and Managed VMs, there is a magic URL you can CURL to get metadata. Authentication is taken care of transparently! You just need to remember to set the “Metadata-Flavor” header.
This means you can use any programing language that can make REST API calls (i.e. all of them) to query the metadata server!
The magic URL only applies to Compute Engine, Container Engine, and Managed VMs. If you are running in App Engine or doing local development, you need to use OAuth to securely connect to the metadata server.
The easiest way to do this is use a Compute Engine API library. For everything running on Google Cloud, you should be able to use the Application Default Credentials to connect to the service. Otherwise, you can download a JSON key file (called a service account) and use that to authenticate.
Here is a great resource on connecting and authenticating to Google APIs that you can follow.
I also wrote some sample code in Go demonstrating how an App Engine app can access the metadata server. Just use the getMetadata function to get all the metadata!
Before I sign off, there is one super cool feature I want to talk about: “wait for change”. This enables you to get updates when metadata values change! Check out the details here. Due to the nature of App Engine, this probably won’t work well there, but it will work great on Compute Engine, Container Engine, and Managed VMs.
I hope you find this nugget of information useful. It’s definitely a hidden gem of Google Cloud Platform and something we don’t talk about much!
Google Cloud community articles and blogs
146 
5
146 claps
146 
5
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/a-day-in-the-life-of-a-developer-advocate-for-google-cloud-platform-fe681c8645cf?source=search_post---------28,"There are currently no responses for this story.
Be the first to respond.
There’s no routine when you’re a Developer Advocate for a very rich environment like Google Cloud Platform. It’s not gonna be a long post praising GCP or anything like that, but a very quick one to give you an example of a day in my life, since I joined Google.
Sometimes, I focus on a particular topic for a little while, like preparing a talk on Google Cloud Endpoints, creating a demo or two of Cloud Natural Language API, etc. And there are days, like Monday, where I do the big leg-splits, touching many areas, in the span of a single day! So what did I play with?
I started my day being a guinea pig for my colleagues who were preparing a code-lab on Cloud Dataflow and Apache Beam, launching interesting flows to analyze billions of NYC taxi rides.
I continued my day answering an interview for an upcoming white paper on Web API documentation, where I had the chance to talk a bit about Web APIs at Google and about the new Cloud Endpoints supporting Open API specifications.
I answered an internal question about our Cloud Natural Language API, with which I recently played quite a bit, for analyzing sentiments of tweets and White House speeches.
Then, to answer a customer question, I blogged about & played with the firewall configuration of the GCP cloud networking, in order to see how one can filter access to a Compute Engine VM based on the IP address of the requester.
And to finish the day, to cool down a bit from this busy schedule, I read a bit about the paper on Spanner, Google’s “internal scalable, multi-version, globally-distributed and synchronously-replicated database”. That’s a bit of a mouthful, but really awesome tech!
There are days which are way less busy than this, but sometimes, you know, that rocks to be able to touch so many topics: Machine Learning, networking, databases, Web APIs, and more. All in a single day! That’s crazy cool!
Google Cloud community articles and blogs
169 
3
169 claps
169 
3
Written by
Developer Advocate for Google Cloud Platform, Apache Groovy programming language project VP/Chair
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Developer Advocate for Google Cloud Platform, Apache Groovy programming language project VP/Chair
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@iamgique/google-cloud-platform-vs-amazon-web-services-gce-and-ec2-c7ddb6b93e7a?source=search_post---------29,"Sign in
There are currently no responses for this story.
Be the first to respond.
Sakul Montha
Aug 26, 2017·4 min read
ปัจจุบัน ระบบ infrastructure ที่ได้รับความนิยมสุด ๆ ในไทย และ สากล คงปฏิเสธไม่ได้ว่าเป็น AWS ของ Amazon หรือ Google Cloud จาก Google หรือ AZURE ของฝั่ง microsoft เรามาคุยกันก่อน ว่า ทั้ง 3 เจ้าที่ผมกล่าวมาคืออะไร มันคือระบบ “IaaS” ทำไมผมถึงได้กล่าวว่ามันเป็นระบบ infrastructure as a Services ก็เพราะว่ามันมีให้มากกว่าคำว่า “เครื่อง”
แล้วทำไมมันถึงเป็นที่นิยมหละหลัก ๆ เลยก็เพราะว่า เราไม่จำเป็นที่จะต้องมีเครื่องที่เป็น Physical อยู่จริง ไม่ต้องเสียเงินค่าเครื่อง ไม่ต้อง Maintenance ไม่ต้องไปนั่งดูระบบ infrastructure ไม่ว่าจะ Hardware Physical หรือ Software พวก Security ก็จะมีคนคอยจัดการให้เรา เรื่อง Avalability ก็มีคนดูแลให้เรา (ไม่ได้หมายความว่าท่านมาใช้ Cloud แล้วระบบของท่านจะไม่ล่มนะครับ) เรื่องสุดท้ายสำหรับระบบที่มีผู้ใช้จำนวนมาก Scalability มัน set ได้ด้วยว่าจะให้มัน Scale ขึ้นเมื่อไหร่ เอาลงเมื่อไหร่ (ทำให้เราประหยัดไปอีก) เราทำเพียงแค่ คลิ๊ก คลิ๊ก คลิ๊ก แล้วก็ คลิ๊ก โดยเราจ่ายเงินตามจริงเท่าที่เราใช้เท่านั้น
เกริ่นมาซะเยอะ เอาเป็นว่า ของมันดีครับ ทีนี้เราจะมาเข้าสู่บทความกัน เราจะมาเปรียบเทียบมวยคู่ระหว่าง Google Cloud Platform กับ Amazon Web Service ว่าอะไรดีอะไรเด็ดกว่ากันเป็นส่วน ๆ
เริ่มจาก Service ของทางฝั่ง AWS กันก่อน เรียกได้ว่า มีให้เลือกใช้กันไม่หมดเลยทีเดียว
ดูรายการทางฝั่ง AWS กันไปแล้ว ทีนี้มาดูฝั่ง Google Cloud Platform กันบ้าง
GCE เป็นชื่อย่อของ Google Compute Engine ส่วน EC2 มาจาก Amazon Elastic Compute Cloudก่อนจะไปเปรียบเทียบกัน ต้องบอกก่อนว่า เจ้าสองตัวเนี่ย อยากให้มองว่ามัน คือ เครื่อง server เครื่องนึงของเราครับ ข้างในไม่มีอะไรเลย จนกว่าเราจะไป คลิ๊กให้มันเป็นอย่างนู้นอย่างนี้ ซึ่ง เราไม่รู้หลอกว่ามันอยู่ตรงไหน เพราะเรามองไม่เห็น
ของทางฝั่ง GCE จะมีอยู่ 3 types ใหญ่ คือ Standard machine types กับ High-memory machine types และ High-CPU machine types ส่วนของ AWS EC2 จะมีอยู่ด้วยกัน 3 types คือ T2, M4 และ M3 ทั้งนี้ทั้งนั้นขึ้นอยู่กับการนำไปใช้งานว่าท่านต้องการ เครื่อง ขนาดเท่าใด
เราเอาแบบเล็กที่สุด มาให้ดู ซึ่งท่านสามารถเข้าไปดูรายละเอียดต่อเองได้ที่ https://cloud.google.com/compute/docs/machine-types และ https://aws.amazon.com/ec2/instance-types/
มาถึงเรื่องของราคา AWS จะคิดราคาเราเป็นราย ชั่วโมง ตามการใช้งานของเครื่องนั้น ๆ ซึ่งท่านสามารถเพิ่ม หรือ ลดจำนวนเครื่องได้ตามต้องการ (ช่วงแรกท่านไม่ต้องกังวลว่าจะเสียเงินเยอะ เขาให้ลองเล่นฟรีก่อนได้ มีวิดีโอสอนด้วย จะสร้าง จะบึ้มเครื่องอะไรทำช่วงนั้นแปปเดียวก็เป็นครับ)
มาดูฝั่ง Google Compute Engine machine types กันบ้าง Google เหมือนจะจงใจทำตลาดมาหักกับ AWS เลยรึเปล่าผมไม่แน่ใจ AWS คิดรายชั่วโมงใช่มะ Google คิดหลังจากใช้งานไปแล้ว 10 นาที แล้วก็คิดค่าบริการเป็นนาที เศษนาที คิดเป็น 1 นาที “Pay per use” แล้วก็ยังมีส่วนลดเพิ่มอีกถ้าหาก instances นั้นถูกเปิดขึ้นคิดเป็นเวลา 25% ของเดือนนั้น ๆ ทั้งสองเจ้าคิดราคาแยกเป็นไปตาม Regions ต่าง ๆ ทั้งคู่
https://aws.amazon.com/ec2/pricing/https://aws.amazon.com/ec2/pricing/on-demand/https://cloud.google.com/compute/pricing
ฝั่ง AWS เครื่องเขามี Amazon VPC (Amazon Virtual Private Cloud) ติดมาให้เลยในด้านของ Security Amazon มี ACL (Access Control List)ในในการจัดการระบบ network มาให้ สรุปง่าย ๆ คือ Amazon มีระบบกำหนดได้ว่า จะให้ใครเข้ามาที่ instances ของเราได้บ้าง โดยแบ่งได้ระดับ IP หรือ ระดับ เครือข่ายเลย(Amazon VPC กับ ACL คืออะไรขอไม่กล่าวถึงนะครับเดี๋ยวยาว)
มาที่ฝั่ง GCE กันบ้าง GCE ก็สามารถกำหนดการเข้าถึง instances ของเราได้เช่นกัน ผ่าน iptables firewall ของ Google
AWS มี Amazon Elastic Load Balancer (ELB) ที่จะช่วยปรับการรับส่งข้อมูลระหว่าง instances ของคุณภายในโซนให้เป็นไปตามต้องการ (ได้แค่ใน regions เดียวกันเท่านั้น) โดยทำการกระจายการเข้าชมไปยัง instances อื่นของคุณโดยใช้ algorithm rounded weighted round robin นอกจากนั้น ELB ยังเทพต่อไปอีกคือ ความต่อเนื่องของ Session ที่เข้ามา ปัจจุบัน ELB รองรับ IPv4 และ IPv6 HTTP และ TCP load balancing แล้วก็มีการเก็บ log ให้ด้วย
GCP มี Compute Engine load balancer มันมีที่ ELB มาหมดเลยแต่มากกว่าในเรื่อง Scaling pattern ELB: Linear ส่วน GCP: Real-time แล้วที่เทพมากคือ Deployment locality เครื่องอยู่ regions ไหน ก็ใช้ Compute Engine load balancer ได้ ฝั่ง ELB ทำได้แค่ใน regions
จากการเปรียบเทียบต่าง ๆ ที่กล่าวมาแล้วข้างต้น Service ของทาง AWS แลดูจะเยอะกว่า GCP อยู่พอสมควร แต่ว่าทาง GCP ที่มีมาให้นั้นก็น่าเพียงต่อต่อความต้องการอยู่แล้ว
- จำนวน Regions ของ AWS นำอยู่เล็กน้อย 14 ต่อ 11 แต่คิดว่าก็ไม่ได้มีผลอะไรกับเรา- Compare EC2 and Compute Engine Capacity ถ้าดูจากหน่วยเล็กที่สุด n1-standard-1 ให้ vCpu เท่ากันคือ 1 cpu แต่ AWS เสียเปรียบมากที่ Memory AWS มีมาให้ 500mb ส่วน GCE จัดเต็มมาก 3.75 แล้วถ้าเกิดเพิ่มขนาดเครื่องไปเรื่อย ๆ Memory Google จะยิ่งนำขาดไปอีก ขอนี้ Google ชนะใส ๆ- Compare EC2 and Compute Engine Pricingในด้านของราคา AWS เก็บเงินรายชั่วโมง GCP เก็บเงินตามการใช้งานจริงเป็นนาที ด้านนี้ผมให้ GCP ชนะเล็ก ๆ- Compare Compute Engine Security Groups, Network ACLs, and Firewalls ในส่วนนี้ผมให้เสมอกันไปครับ- Compare Amazon ELB vs Compute Engine Load Balancing ในส่วนนี้ จะสังเกตุได้ว่าของ Google เหนือกว่า AWS จริง ๆ ครับ load balance อยู่คนละ regions ได้ ยอมใจ
ที่เปรียบเทียบมาทั้งหมดเป็นเพียงแค่ GCE กับ EC2 นะครับ ยังมี Service อื่น ๆ ที่ผู้ให้บริการทั้งสองเจ้านี้ มีอีกมากมายที่ยังไม่ได้ถูกกล่าวถึง
หวังว่าจะเป็นประโยชน์กับทุกท่าน หากผิดพลาดตรงไหน สามารถแจ้งได้เลยนะครับ
ที่มา: https://cloudacademy.com/blog/ec2-vs-google-compute-engine/ที่มา2: https://cloudacademy.com/blog/google-cloud-vs-aws-a-comparison/ที่มา3: https://cloud.google.comที่มา4: https://aws.amazon.com
Technology Advocacy Manager, a man who’s falling in love with the galaxy.
See all (63)
196 
196 claps
196 
Technology Advocacy Manager, a man who’s falling in love with the galaxy.
About
Write
Help
Legal
Get the Medium app
"
https://rominirani.com/google-cloud-platform-factors-to-control-your-costs-5a256ed207f1?source=search_post---------30,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
This blog post provides general guidelines to help understand various factors that you should take into consideration while trying to understand your monthly cloud bill.
This is a guide for beginners to start thinking in terms of costs and making informed decisions to keep costs in control, especially during development. The goal is to make developers think from a financial point of view too and not just assume that they have nothing to do with it.
cloud.google.com
This is by no means a complete guide but a general direction to make you aware of these recommendations/suggestions. I might be wrong on a few points too since cloud billing can get fairly complex and would appreciate you pointing it out in the comments, it will help me better inform readers.
Best practice is still to regularly analyze your monthly cloud bills and take actions based on that.
In no order of preference, here are some points to consider:
Google Cloud Platform comes along with a generous Free Trial when you sign up. This has been repeated often enough so I want to go into the details here. In summary, when you sign up with your credit card, you get $300 and 12 months to try it, whichever is consumed earlier. This is good enough to try out the cloud and if you are looking to understand what you could do with $300 of the credit, I have written an article earlier on the same : You have $700 GCP Credit — Now What? . Most of it should it apply.
rominirani.com
One thing to note here — Your credit card will not be charged if you say exhaust your $300 credit. You will be notified of the same first. This is the #1 question that I get asked from folks who want to try out Google Cloud Platform. See the 3rd point in the right infobar below, when you go to the Free Trial page.
This is a top feature on Google Cloud Platform and available on other cloud platforms too. A free tier actually means “an always free tier” and what that means in practical terms is that you are given a little bit of most services and if you are within those limits, you will not be charged anything.
The above screenshot is just a few of the services and the free tier capabilities. For e.g. you get a Google Compute Engine instance for free (f1-micro instance) and so on.
The best part of the free tier is that they provide you as an individual developer, just about enough services that will help you try out things and even run things for the while. The key thing to understand here is that the free tier is baked into the monthly costs and hence all charges will be for resources that you utilized above the free tier limits.
Do note that some services might not be available at all in the Free tier
Study these in detail and chances are you are good for a long time:
Remember that the Free tier do not come with any SLAs and that is to be expected. Also this offer could be withdrawn at any time but given the dynamics and competition between the Cloud providers to bring new customers to the platform, this is likely to stay in some form or the other.
Get familiar with the Cost Calculator provided by GCP. This should be your first step to understand what it is going to cost you. Again, the cloud is all about elasticity and the fact that you might end up using more than one service or two, while you are trying out things, which is fine.
The whole idea of the cost calculator is to give you a sense of what it will be cost you. Try your best to fill in some values. One of the things that you should immediately notice is which services end up costing more, which Virtual Machine configurations end up being costly and so on.
If you are migrating your workloads to the cloud, it is recommended that you use the Total Cost of Ownership, which is a much more accurate measure of the savings that you might incur, if you were to manage the workloads on your own.
If you would like a JSON file of the current prices, take a look here.
It is recommended that you have setup/shutdown/destroy scripts for all the resources that you create. You can script this out using the gcloud, which is a command-line interface to Google Cloud Platform. Via gcloud commands, you can manage most resources and it is recommended that you have scripts to start and stop these services.
The reason for this is simple and it applies especially to compute resources. One of the costs is how much time you keep these resources in an Active State. For e.g. if you provision a Compute Engine instance and you leave it in the RUNNING state, then you are being charged for it. So the general guideline is that during development or in general, you should optimize and keep the resources in STOPPED state if you are not using them. This is not just from a cost point of view but also you will prevent them for potentially being misused too.
The best way is to be a bit disciplined from the start and have the scripts ready to execute and get into the habit of running the startup / shutdown scripts as needed.
Google Cloud Platform have introduced pricing innovations like Sustained Use Discounts, where an automatic discount is applied if you commit to using the resources for longer period. For e.g. you would get a 30% discount if you use a Compute Engine instance for a period of 30 days. You could possibly look at these options too as you move along and be aware that the more you use , you will be given the discount automatically.
Do take a look at the Google Cloud Console Dashboard too. Click on the Activity tab to see what is going on in the project too. A sample snapshot from my console is shown below:
You are charged for the storage space that you allocate and not for the space that you have used. So if you have allocated a 100GB drive, you will be charged for that and not for the 10GB space that you are using out of it.
Storage is cheap and prices continue to fall but multiply that by hundreds of storage drives and the cost could end up quickly. Do keep in mind that at times, you might to allocate much more storage space than you actually need due to the fact that certain compute configurations and IOPS requirements might demand a bigger storage drive for better performance.
As a thumb rule, SSDs are much more expensive and you should keep that factor in mind, unless you are clear on the performance requirements of your applications.
For e.g. here is the pricing on Persistent disks (Standard and SSD) in the Northern Virginia region:
One of the things to pay attention to is to identify resources that are expensive.
One way to learn is to play with the cost calculator and see the itemized cost that is provided to you. Identify resources that you will be charged for even if you do not use them. For e.g. consider a Load Balancer. A typical charge for the Load Balancer depends on the number of rules that you set and also the traffic moving in and out across to the different VMs that you have put behind the load balancer. This could easily go into several dollars a month, even if you do not put it to use. So be careful while assigning such resources, review your rules, where the VMs are and more.
For e.g. take a look at the pricing example given here: https://cloud.google.com/compute/pricing#lb
This is a very important feature and can go a long way in ensuring that you remain friends with your manager and finance team. Google Cloud Platform allows you to set up Budgets and Alerts for your project.
You can set a monthly spending limit, which will help keep the costs under control and also protect you from any unexpected surge in traffic and even a possible misuse/external attack. Check out the documentation that details how you can set up a Budget. It is easily accessible from the Google Cloud Console via Billing → Budgets and Alerts.
But before you set some arbitrary monthly limit, get a sense of a few days of usage and what it is costing and then set the right limits. If you run out of these limits, one should expect the services to fail.
Along with a budget, you should also setup Billing Alerts. Billing Alerts will notify you if you reach a certain percentage of your spending limit. This can help alert you before things could get out of control. So definitely setup Billing Alerts.
A sample screenshot is shown below. Notice that you can set the Budget amount to be either a specific amount or limiting it to last month’s spend.
This is a difficult metric to track but the point here is that you need to understand the traffic flowing into your resources and out of them.
Google Cloud Platform does not charge for Ingress but puts a charge for Egress i.e. traffic flowing out of their infrastructure. Again it depends if you are talking to a service with the same region, different region, outside of Google infrastructure, same zone, different zone and so on.
For e.g. if you make an API call from your instances hosted in Cloud A to another Cloud Providers infrastructure, you are going to be charged for the Egress. Also be cognizant of the fact that you have different services of GCP that talk to each other but are provisioned in different zones/regions.
Here is the Network pricing table for Compute Engine at the time of writing this article and it should give you a good idea of different combinations. It is not the easiest of things to predict and a lot depends on how well you understand the different services and the interactions between them in your application.
Try to come up with a rough number for the Egress but this is best judged once you have a few billing cycles in place and can see what the data is looking at. Always provision for a bit more when estimating the cost.
One of the best features of the cloud is elasticity. Cloud providers have gone to great extents to help your application scale without you taking the trouble of managing the compute that needs to be provisioned on the fly to meet your application demand.
If you see a checkbox or two, that allows you to auto-scale, keep in mind that it means that Google is provisioning additional resources for you and you will be charged for that. As the demand goes down, it will scale things down too. But you need to understand and consider this factor.
You will have to make a decision if you would like to manually reserve a fixed number of instances or let the cloud provider auto-scale. There is a cost opportunity to everything.
Google Cloud Platform services are available across several regions. Pricing is not the same across regions. You might be able to bring down your costs by considering a region that is cheaper for that particular service. But do keep in mind that you ideally want your services to be as close as possible to your customers and latency does kick in, which could hamper the user experience. So consider cost v/s latency when choosing which regions to host your services.
Additionally, there are some services that are global, multi-region or just zonal. Take some time to understand that and the costs that could kick in.
One of the most popular services on Google Cloud Platform is Google Cloud Storage, a globally available object store. Google Cloud Platform has a class for a Storage bucket, the container in which you store your objects.
The classes at a high level, differentiate across each other via things like how frequently you may want to access the data stored in the buckets and also the latency to access the data. If you are ok with the latency and your needs for frequency of accessing that data is low, you could opt for different storage class, which can definitely reduce the cost by a large extent.
Take a look at this table with different classes for the buckets and how it can affect costs.
For e.g. if you are looking at storing data for archival purpose, log files that you might access later, etc — your best options could be Nearline Storage and you could cut your cost by half. Just look at the price column in the table above and understand the various Use Cases that are best served by the respective Storage classes.
I would not like to label this section as hidden costs but it usually surprises anyone the first time. It typically applies to services where you think you are paying for just querying the data but you often overlook that to save your data, the cloud provider has to end up using cloud storage and hence will have to charge you for the storage costs too.
For e.g. consider a service that allows you to do streaming inserts of data, stores the data for you and allows you to query that data too. What does that mean in terms of cost. Well, there could be multiple things and which at a high level could be:
Let us consider BigQuery pricing as shown below:
Hope you get the gist of what this means and why you are being charged for the different operations that helps Google deliver the BigQuery service to you. Google Cloud Platform is transparent by explaining to you the different costs and states that explicitly.
Paying attention to these factors will help you retain your sanity when you are faced with an itemized bill that does not make too much sense in terms of additional costs for using a particular service.
Managed Services are great for everyone but they come with an additional cost, which is well worth it but it is something that you need to look at. For e.g. consider the Managed Cloud SQL service that is provided. Google Cloud Platform takes care of the maintenance, patches and a lot more while all you need to do is specify the configuration, click a button and you have a Database Service abstracted and running for you.
However, if you have the operations chops, you could provision a Compute Engine instance, install MySQL on it, expose a few ports, setup incoming network restrictions and get going too. The latter will be cheaper compared to the fully managed service that is provided.
During development phase, you might not have a need for Automatic Backup, Failover and Replica features that are provided as part of the Cloud SQL offering .
On the flip side, certain Managed Services will definitely help you save costs especially those in the Big Data services provided on Google Cloud Platform. For e.g. setting up and running your own Hadoop / Spark cluster is going to be both time consuming and difficult to manage and limit costs. On the other hand with a service like Dataproc, you could focus on running your jobs and spin up and tear down the Hadoop cluster with ease, thereby just charging you for the resources used for that period of time that you kept things running.
Google Cloud Platform provides an API for accessing your Billing data and optionally exporting this information into other tools for Analysis. It would definitely help to consider this if you are having a system in production and need to continuously monitor costs.
Check out the Getting Started guide with the Billing API. The Billing API provides multiple operations that include getting information on your billing accounts, projects under that, billing data for your project and more.
It is also advisable to setup a Dashboard where you can visualize your running costs. Check out the guide to Visualize Spend over time with Data Studio, an interesting approach to exporting your Billing Data over to BigQuery and then visualizing that data via Data Studio.
We have just touched upon a few high level points that you should look at to better understand and control costs on Google Cloud Platform. This is just a start and for each service, you should ideally be spending time in the Pricing section to understand it better.
Technical Tutorials, APIs, Cloud, Books and more.
179 
2
179 claps
179 
2
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
Technical Tutorials, APIs, Cloud, Books and more.
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
Technical Tutorials, APIs, Cloud, Books and more.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/running-a-mean-stack-on-google-cloud-platform-with-app-engine-and-mongolab-4bbd2040ea75?source=search_post---------31,"There are currently no responses for this story.
Be the first to respond.
In two of my previous posts, I talk about running a MEAN stack with Docker and then making it more robust by using Kubernetes.
In this post, I’m going to go on a tangent and take a different approach. Let’s say you don’t care about all that complicated stuff, you just want your MEAN application to run and scale without worrying about VMs and Pods and Docker and blah blah blah…
To do that, it makes sense to use a Platform as a Service (PaaS), which is a fancy way of saying you write the code and we run the code. There is no server or database management needed, which lets you focus on building a great product!
Google’s PaaS is called App Engine. App Engine can run Node.js and handle all the scaling and maintenance so you don’t have to.
For the database, I’m going to use MongoLab. It’s the MongoDB you know and love, but you don’t need to worry about hosting it.
Sign up for MongoLab.
Once your account is created, you should see a dashboard.
Click “Create New”.
I’m using the free plan with Google Cloud Platform.
Also, make sure the Database Name is mean-demo, because that’s what our sample app expects.
Your config should look like this:
Once you click “Create new MongoDB deployment”, you should see your new Database up and running:
Click the database, and note the connection URI.
Now, create a user so the app can connect to the database:
Done.
Go to console.developers.google.com and select (or create) your project. For the rest of this tutorial, I’m going to be using Cloud Shell, which is basically a command line in your browser that has all the tools you need already installed. It is really cool!
Now, pull in the sample code:
And fix the configuration so you can connect to your MongoLab instance:
Replace <dbuser> and <dbpassword> with what you specified in Step 1.
Replace XXXXX with your Database ID (in my case, ds049624).
Replace #### with your Database port (in my case, 49624).
All you are doing here is changing the MongoDB client to use your MongoLab instance.
With Cloud Shell, you can easily test your code just like it was running on a local machine.
You should see this:
Now, launch the Web Preview (Click the little button on your Cloud Shell):
And ta-da, it works!
If you add some data, it will add it into your MongoLab database. Try it!
While Cloud Shell is great for testing, it can’t host your app! It’s time to deploy to production!
In the code, there is a file called app.yaml, which contains all the deployment details. Let’s take a look:
Pretty self-explanatory. The vm: true option lets App Engine know it should use the Managed VM runtime (i.e. Docker).
To deploy, run:
Or you can use the shorthand (thanks to the package.json file):
And the deployment will begin.
gcloud will pull the project configuration from the Cloud Shell environment and your app.yaml file.
App Engine will copy your code, create a Docker container, spin up VMs and load balancers, and launch your code. You just sit back and relax.
Once you see this:
You are done. Go to the URL to see your MEAN app running!
That’s it! Now you don’t need to worry about servers crashing or database management. Of course, you lose some flexibility because you are not nitpicking every little detail, but for most applications App Engine works great!
For more info, check out the Node.js getting started guide and the MongoLab docs.
Google Cloud community articles and blogs
151 
9
151 claps
151 
9
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/data-engineering-on-google-cloud-platform-coursera-courses-ca24840d2a3a?source=search_post---------32,"There are currently no responses for this story.
Be the first to respond.
I’ve been heads down the past few weeks recording a bunch of Coursera courses that together form the Data Engineering on Google Cloud Platform specialization. Please enroll in these courses to learn how to ingest, transform, and learn from data on Google Cloud Platform:
Fortunately, all I had to do was to talk into a microphone — my colleagues who are video production wizards did the rest. And of course, the product & engineering teams helped create the products that make GCP such a pleasure to use.
p.s. So, why are the courses paid? Three reasons: (1) Coursera needs to cover their costs — support their great platform by paying. (2) Think of it as a subtle nudge — we have noticed that completion rates for our paid courses are 5x more than completion rates for our free courses (75% vs. 15%). (3) The course fee will, once we iron out a few details, cover the cloud computing costs so you don’t have to use your own GCP project to try out the labs.
Google Cloud community articles and blogs
99 
2
99 claps
99 
2
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Data Analytics & AI @ Google Cloud
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://itnext.io/getting-started-with-red-hat-ansible-for-google-cloud-platform-fa666c42a00c?source=search_post---------33,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
You have 1 free member-only story left this month. Sign up for Medium and get an extra one
In this post, we will explore the use of Ansible, the open source community project sponsored by Red Hat, for automating the provisioning, configuration, deployment to, and testing of resources on the Google Cloud Platform (GCP). We will start by using Ansible to configure and deploy applications to existing GCP compute resources. We will then expand our use of Ansible to provision and configure GCP compute resources using the Ansible/GCP native integration with GCP modules.
Ansible, purchased by Red Hat in October 2015, seamlessly provides workflow orchestration with configuration management, provisioning, and application deployment in a single platform. Unlike similar tools, Ansible’s workflow automation is agentless, relying on Secure Shell(SSH) and Windows Remote Management (WinRM). Ansible has published a whitepaper on The Benefits of Agentless Architecture.
According to G2 Crowd, Ansible is a clear leader in the Configuration Management Software category, ranked right behind GitLab. Some of Ansible’s main competitors in the category includes GitLab, AWS Config, Puppet, Chef, Codenvy, HashiCorp Terraform, Octopus Deploy, and TeamCity. There are dozens of published articles, comparing Ansible to Puppet, Chef, SaltStack, and more recently, Terraform.
According to Google, Google Compute Engine (GCE) delivers virtual machines (VMs) running in Google’s data centers and on their worldwide fiber network. Compute Engine’s tooling and workflow support enables scaling from single instances to global, load-balanced cloud computing.
Comparable products to GCE in the IaaS category include Amazon Elastic Compute Cloud(EC2), Azure Virtual Machines, IBM Cloud Virtual Servers, and Oracle Compute Cloud Service.
According to Apache, the Apache HTTP Server (“httpd”) is an open-source HTTP server for modern operating systems including Linux and Windows. The Apache HTTP Server provides a secure, efficient, and extensible server that provides HTTP services in sync with the current HTTP standards. The Apache HTTP Server was launched in 1995 and it has been the most popular web server on the Internet since 1996. We will deploy Apache HTTP Server to GCE VMs, using Ansible.
In this post, we will demonstrate two different workflows with Ansible on GCP. First, we will use Ansible to configure and deploy the Apache HTTP Server to an existing GCE instance.
In the second workflow, we will use Ansible to provision and configure the GCP resources, as well as deploy the Apache HTTP Server the new GCE VM.
The source code for this post may be found on the master branch of the ansible-gcp-demo GitHub repository.
The project has the following file structure.
Source code samples in this post are displayed as GitHub Gists which may not display correctly on all mobile and social media browsers, such as LinkedIn.
For this demonstration, I have created a new GCP Project containing a new service account and public SSH key. The project’s service account will be used the gcloud CLI tool and Ansible to access and provision compute resources within the project. The SSH key will be used by both tools to SSH into GCE VM within the project. Start by creating a new GCP Project.
Add a new service account to the project on the IAM & admin ⇒ Service accounts tab.
Grant the new service account permission to the ‘Compute Admin’ Role, within the project, using the Role drop-down menu. The principle of least privilege (PoLP) suggests we should limit the service account’s permissions to only the role(s) necessary to provision the required compute resources.
Create a private key for the service account, on the IAM & admin ⇒ Service accounts tab. This private key is different than the SSH key will add to the project, next. This private key contains the credentials for the service account.
Choose the JSON key type.
Download the private key JSON file and place it in a safe location, accessible to Ansible. Be careful not to check this file into source control. Again, this file contains the service account’s credentials used to programmatically access GCP and administer compute resources.
We should now have a service account, associated with the new GCP project, with permissions to the ‘Compute Admin’ role, and a private key which has been downloaded and accessible to Ansible. Note the Email address of the service account, in my case, ansible@ansible-gce-demo.iam.gserviceaccount.com; you will need to reference this later in your configuration.
Next, create an SSH public/private key pair. The SSH key will be used to programmatically access the GCE VM. Creating a separate key pair allows you to limit its use to just the new GCP project. If compromised, the key pair is easily deleted and replaced in the GCP project and in the Ansible configuration. On a Mac, you can use the following commands to create a new key pair and copy the public key to the clipboard.
Add your new public key clipboard contents to the project, on the Compute Engine ⇒ Metadata ⇒ SSH Keys tab. Adding the key here means it is usable by any VM in the project unless you explicitly block this option when provisioning a new VM and configure a key specifically for that VM.
Note the name, ansible, associated with the key, you will need to reference this later in your configuration.
Although this post is not a primer on Ansible, I will cover a few setup steps I have done to prepare for this demo. On my Mac, I am running Python 3.7, pip 18.1, and Ansible 2.7.6. With Python and pip installed, the easiest way to install Ansible in Mac or Linux is using pip.
You will also need to install two additional packages in order to gather information about GCP-based hosts using GCE Dynamic Inventory, explained later in the post.
I created a simple Ansible ansible.cfg file for this project, located in the /ansible/inventories/ sub-directory. The Ansible configuration file contains the location of the project’s roles and inventory, which is explained later. The file also contains two configuration items associated with an SSH key pair, which we just created. If your key is named differently or in a different location, update the file (gist).
Ansible has a complete example of a configuration file parameters on GitHub.
To decouple our specific GCP project’s credentials from the Ansible playbooks and roles, Ansible recommends setting those required module parameters as environment variables, as opposed to including them in the playbooks. Additionally, I have set the GCP project name as an environment variable, in order to also decouple it from the playbooks. To set those environment variables, source the part0_source_creds.sh script in the project’s root directory, using the source command (gist).
Oftentimes, enterprises employ a mix of DevOps tooling to provision, configure, and deploy to compute resources. In this first workflow, we will use Ansible to configure and deploy a web server to an existing GCE VM, created in advance with the gcloud CLI tool.
First, use the gcloud CLI tool to create a GCE VM and associated resources, including an external IP address and firewall rule for port 80 (HTTP). For simplicity, we will use the existing GCP default Virtual Private Cloud (VPC) network and the default us-east1 subnetwork. Execute the part1_create_vm.sh script in the project’s root directory. The default network should already have port 22 (SSH) open on the firewall. Note the SERVICE_ACCOUNT variable, in the script, is the service account email found on the IAM & admin ⇒ Service accounts tab, shown in the previous section (gist).
The output from the script should look similar to the following. Note the external IP address associated with the VM, you will need to reference this later in the post.
Using the gcloud CLI tool or Google Cloud Console, we should be able to view our newly provisioned resources on GCP. First, our new GCE VM, using the Compute Engine ⇒ VM instances ⇒ Details tab.
Next, examine the Network interface details tab. Here we see details about the network and subnetwork our VM is running within. We see the internal and external IP addresses of the VM. We also see the firewall rules, including our new rule, allowing TCP ingress traffic on port 80.
Lastly, examine the new firewall rule, which will allow TCP traffic on port 80 from any IP address to our VM, located in the default network. Note the other, pre-existing rules controlling access to the default network.
The final GCP architecture looks as follows.
Two core concepts in Ansible are hosts and inventory. We need an inventory of the hosts on which to run our Ansible playbooks. If we had long-lived hosts, often referred to as ‘pets’, who had long-lived static IP addresses or DNS entries, then we could manually add the hosts to a static hosts file, similar to the example below.
However, given the ephemeral nature of the cloud, where hosts (often referred to as ‘cattle’), IP addresses, and even DNS entries are often short-lived, we will use the Ansible concept of Dynamic Inventory.
If you recall we pip installed two packages, requests and google-auth, during our Ansible setup for use with GCE Dynamic Inventory. According to Ansible, the best way to interact with your GCE VM hosts is to use the gcp_compute inventory plugin. The plugin allows Ansible to dynamically query GCE for the nodes that can be managed. With the gcp_compute inventory plugin, we can also selectively classify the hosts we find into Groups. We will then run playbooks, containing roles, on a group or groups of hosts.
To demonstrate how to dynamically find the new GCE host, and add it to a group, execute the following command, using the Ansible Inventory CLI.
The command calls the webservers_gcp.yml file, which contains logic necessary to associate the GCE hosts with the webservers host group. Ansible’s current documentation is pretty sparse on this subject. Thanks to Matthieu Remy for his great post, How to Use Ansible GCP Compute Inventory Plugin. For this demo, we are only looking for hosts in us-east1-b, which have ‘web-’ in their name. (gist).
The output from the command should look similar to the following. We should observe our new VM, as indicated by its external IP address, is assigned to the part of the webservers group. We will use the power of Dynamic Inventory to apply a playlist to all the hosts within the webservers group.
We can also view details about hosts by modifying the inventory command.
The output from the command should look similar to the following. This particular example was run against an earlier host, with a different external IP address.
For our first taste of Ansible on GCP, we will run an Ansible Playbook to install and configure the Apache HTTP Server on the new CentOS-based VM. According to Ansible, Playbooks, which are YAML-based, can declare configurations, they can also orchestrate steps of any manual ordered process, even as different steps must bounce back and forth between sets of machines in particular orders. They can launch tasks synchronously or asynchronously. Playbooks are used to orchestrate tasks, as opposed to using Ansible’s ad-hoc task execution mode.
A playbook can be ‘monolithic’ in nature, containing all the required Variables, Tasks, and Handlers, to achieve the desired outcome. If we wrote a single playbook to deploy and configure our Apache HTTP Server, it might look like the httpd_playbook.yml, playbook, below (gist).
We could run this playbook with the following command to deploy the Apache HTTP Server, but we won’t. Instead, next, we will run a playbook that applies the httpd role.
According to Ansible, Roles are ways of automatically loading certain vars_files, tasks, and handlers based on a known file structure. Grouping content by roles also allows easy sharing of roles with other users. The usage of roles is preferred as it provides a nice organizational system.
The httpd role is identical in functionality to the httpd_playbook.yml, used in the first workflow. However, the primary parts of the playbook have been decomposed into individual resource files, as described by Ansible. This structure is created using the Ansible Galaxy CLI. Ansible Galaxy is Ansible’s official hub for sharing Ansible content.
This ansible-galaxy command creates the following structure. I added the files and Jinja2 template, afterward.
Within the httpd role:
To apply the httpd role, we will run the 20_webserver_config.yml playbook. Compare this playbook, below, with the previous, monolithic httpd_playbook.yml playbook. All of the logic has now been decomposed across the httpd role’s separate backing files (gist).
We can start by running our playbook using Ansible’s Check Mode (“Dry Run”). When ansible-playbook is run with --check, Ansible will not make any actual changes to the remote systems. According to Ansible, Check mode is just a simulation, and if you have steps that use conditionals that depend on the results of prior commands, it may be less useful for you. However, it is great for one-node-at-time basic configuration management use cases. Execute the following command using Check mode.
The output from the command should look similar to the following. It shows that if we execute the actual command, we should expect seven changes to occur.
If everything looks good, then run the same command without using Check mode.
The output from the command should look similar to the following. Note the number of items changed, seven, is identical to the results of using Check mode, above.
If we were to execute the command using Check mode for a second time, we should observe zero changed items. This means the last command successfully applied all changes and no new changes are present in the playbook.
There are a number of methods and tools we could use to test the deployments of the Apache HTTP Server and server tools. First, we can use an ad-hoc ansible CLI command to confirm the httpd process is running on the VM, by calling systemctl. The systemctl application is used to introspect and control the state of the systemd system and service manager, running on the CentOS-based VM.
The output from the command should look similar to the following. We see the Apache HTTP Server service details. We also see it being stopped and started as required by the tasks and handler in the role.
We can also check that the home page and PHP info documents, we deployed as part of the playbook, are in the correct location on the VM.
The output from the command should look similar to the following. We see the two documents we deployed are in the root of the website directory.
Next, view our website’s home page by pointing your web browser to the external IP address we created earlier and associated with the VM, on port 80 (HTTP). We should observe the variable value in the playbook, ‘Hello Ansible on GCP!’, was injected into the Jinja2 template file, index.html.j2, and the page deployed correctly to the VM.
If you recall from the httpd role, we had a task to deploy the server status configuration file. This configuration file exposes the /server-status endpoint, as shown below. The status page shows the internal and the external IP addresses assigned to the VM. It also shows the current version of Apache HTTP Server and PHP, server uptime, traffic, load, CPU usage, number of requests, number of running processes, and so forth.
Apache Bench (ab) is the Apache HTTP server benchmarking tool. We can use Apache Bench locally, to generate CPU, memory, file, and network I/O loads on the VM. For example, using the following command, we can generate 100K requests to the server-status page, simulating 100 concurrent users.
The output from the command should look similar to the following. Observe this command successfully resulted in a sustained load on the web server for approximately 17.5 minutes.
Using the Compute Engine ⇒ VM instances ⇒ Monitoring tab, we see the corresponding Apache Bench CPU, memory, file, and network load on the VM, starting at about 10:03 AM, soon after running the playbook to install Apache HTTP Server.
After exploring the results of our workflow, tear down the existing GCE resources before we continue to the next workflow. To delete resources, execute the part2_clean_up.sh script in the project’s root directory (gist).
The output from the script should look similar to the following.
In the second workflow, we will provision and configure the GCP resources, and deploy Apache HTTP Server to the new GCE VM using Ansible. We will be using the same Project, Region, and Zone as the previous example. However this time, we will create a new global VPC network instead of using the default network as before, a new subnetwork instead of using the default subnetwork as before, and a new firewall with ingress rules to open ports 22 and 80. Lastly, will create an external IP address and assign it to the VM.
Instead of using the gcloud CLI tool, we will use Ansible to provision the GCP resources. To accomplish this, I have created one playbook, 10_webserver_infra.yml, with one role, gcpweb, but two sets of tasks, one to create the GCE resources, create.yml, and one to delete the GCP resources, delete.yml. This is a typical Ansible playbook pattern. The standard file directory structure of the role looks as follows, similar to the httpd role.
To provision the GCE resources, we run the 10_webserver_infra.yml playbook (gist).
This playbook runs the gcpweb role. The role’s default main.yml task file imports two other sets of tasks, one for create and one for delete. Each set of tasks have a corresponding tag associated with them (gist).
By calling the playbook and passing the ‘create’ tag, the role will run apply the associated set of create tasks. Tags are a powerful construct in Ansible. Execute the following command, passing the create tag.
In the case of this playbook, the Check mode, used earlier, would fail here. If you recall, this feature is not designed to work with playbooks that have steps that use conditionals that depend on the results of prior commands, such as with this playbook.
The create.yml file contains six tasks, which leverage Ansible GCP Modules. The tasks create a global VPC network, subnetwork in the us-east1 Region, firewall and rules, external IP address, disk, and VM instance (gist).
If your interested in what is actually happening during the execution of the playbook, add the verbose option (-v or -vv) to the above command. This can be very helpful in learning Ansible.
The output from the command should look similar to the following. Note the changes applied to localhost. Since no GCE VM host(s) exist on GCP until the resources are provisioned, we reference localhost. The entire process took less than two minutes to create a global VPC network, subnetwork, firewall rules, VM, attached disk, and assign a public IP address.
All GCP resources are now provisioned and configured. Below, we see the new GCE VM created by Ansible.
Below, we see the new GCE VM’s network interface details console page, showing details about the VM, NIC, internal and external IP addresses, network, subnetwork, and ingress firewall rules.
Below, we see the VPC details showing each of the automatically-created regional subnets, and our new ‘ansible-subnet’, in the us-east1 region, and spanning 14 IP addresses in the 172.16.0.0/28 CIDR (Classless Inter-Domain Routing) block.
To deploy and configure Apache HTTP Server, run the httpd role exactly the same way we did in the first workflow.
In the first workflow, we manually tested our results using a number of ad-hoc commands and by viewing web pages in our browser. These methods of testing do not lend themselves to DevOps automation. A more effective strategy is writing tests, which are part of the role, and maybe run each time the role is applied, as part of a CI/CD pipeline. Each role in this project contains a few simple tests to confirm the success of the tasks in the role. First, run the gcpweb role’s tests with the following command.
The playbook gathers facts about the GCE hosts in the host group and runs a total of five test tasks against those hosts. The tasks confirm the host’s timezone, vCPU count, OS type, OS major version, and hostname, using the facts gathered (gist).
The output from the command should look similar to the following. Observe that all five tasks ran successfully.
Next, run the the httpd role’s tests.
Similarly, the output from the command should look similar to the following. The playbook runs four test tasks this time. The tasks confirm both files are present, the home page is accessible, and that the server-status page displays properly. Below, we all four ran successfully.
To observe what happens if we apply a change to a playbook, let’s change the greeting variable value in the /roles/httpd/defaults/main.yml file in the httpd role. Recall, the original home page looked as follows.
Change the greeting variable value and re-run the playbook, using the same command.
The output from the command should look similar to the following. As expected, we should observe that only one task, deploying the home page, was changed.
Viewing the home page again, or by modifying the associated test task, we should observe the new value is injected into the Jinja2 template file, index.html.j2, and the new page deployed correctly.
Once you are finished, you can destroy all the GCP resources by calling the 10_webserver_infra.yml playbook and passing the delete tag, the role will run apply the associated set of delete tasks.
With Ansible, we delete GCP resources by changing the state from present to absent. The playbook will delete the resources in a particular order, to avoid dependency conflicts, such as trying to delete the network before the VM. Note we do not have to explicitly delete the disk since, if you recall, we provisioned the VM instance with the disks.auto_delete=true option (gist).
The output from the command should look similar to the following. We see the VM instance, attached disk, firewall, rules, external IP address, subnetwork, and finally, the network, each being deleted.
In this post, we saw how easy it is to get started with Ansible on the Google Cloud Platform. Using Ansible’s 300+ cloud modules, provisioning, configuring, deploying to, and testing a wide range of GCP, Azure, and AWS resources are easy, repeatable, and completely automatable.
All opinions expressed in this post are my own and not necessarily the views of my current or past employers or their clients.
Originally published at programmaticponderings.com on January 30, 2019.
ITNEXT is a platform for IT developers & software engineers…
197 
197 claps
197 
Written by
AWS Senior Solutions Architect | AWS Certified Pro | Polyglot Developer | Data Analytics | DataOps | DevOps
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
Written by
AWS Senior Solutions Architect | AWS Certified Pro | Polyglot Developer | Data Analytics | DataOps | DevOps
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@sathishvj/google-cloud-platform-authorized-trainer-1b202f3eef61?source=search_post---------34,"Sign in
There are currently no responses for this story.
Be the first to respond.
sathish vj
Apr 2, 2019·15 min read
What it is, who can apply, how to apply, and the interview process.
Subscribe to my YouTube channel that teaches you to apply Google Cloud to your projects and also prepare for the certifications: youtube.com/AwesomeGCP. Check out the playlists I currently have for Associate Cloud Engineer, Professional Architect, Professional Data Engineer, Professional Cloud Developer, Professional Cloud DevOps Engineer, Professional Cloud Network Engineer, and Professional Cloud Security Engineer.
Edit May 1st 2020: There is now an official website for the Google Authorized Trainer: https://sites.google.com/view/gcptrainer/home
Edit Nov 2019: I see that there is a new logo/badge for the authorized trainer.
Edit Dec 2019: There is now an official certificate issued for authorized trainers. Mine is here: https://googlecloudcertified.credential.net/profile/40a1ddc5b98dbac0952e85b616a19c5d74c6e762
I’d been conducting workshops and trainings for many years now. When a training coordinator learnt that I had picked up a few Google Cloud Certifications, he asked me whether I would like to try out to be a Google Cloud Authorized Trainer. I replied, “Huh? What’s that?”
Unfortunately, there’s hardly any information about this online. So here are my notes on the whole process. This process involves people only and therefore potentially subjective decisions and actions. It isn’t as clear cut and objective as the multiple choice certification tests. My intention is not to pass judgement on the process or any people. I’m only putting down notes from my perspective so that you can be better prepared when you attempt to become a GCP trainer.
Process: https://sites.google.com/view/gcptrainer/process
Trainer Evaluation: https://sites.google.com/view/gcptrainer/evaluation
Trainer Onboard: https://sites.google.com/view/gcptrainer/onboard
You have to be Google Cloud Certified — so far, the Professional Architect or the Professional Data Engineer,o̵r̵ ̵t̵h̵e̵ ̵P̵r̵o̵f̵e̵s̵s̵i̵o̵n̵a̵l̵ ̵C̵l̵o̵u̵d̵ ̵D̵e̵v̵e̵l̵o̵p̵e̵r̵. (Edit: to become a trainer even for the Application Development track, apparently you require the Professional Architect certification.)
I have a question here that I do not have an answer to yet. Assume you became an Authorized Trainer based on the Professional Architect. Later you also got the Data Engineer certification. Are you now automatically qualified to also teach Data Engineering? Or do you have to go through the process again for this? I’ll try to find that out later. (I got the answer, and it is this: the authorized trainer selection process is only once. If you become an Authorized Trainer, say, for the Architect track once, and later got certified as a Data Engineer, then you are automatically qualified to teach that also. There is no second evaluation.)
As of now, there is no restriction on what course an Authorized Trainer can take — they can conduct any of the courses irrespective of the certification that they have. So, let us say you have a Professional Architect certification and no others. You can still conduct a course of Networking, Security, DevOps, etc. Of course, it would be definitely preferable that you have the corresponding certification, but it is not mandated.
Note that, this could change in the future.
Interestingly, you can’t apply to be an Authorized Trainer on your own as an individual. Google ties up with Authorized Training Partners (ATP). (Edit: This doesn’t mean that you have to be employed by that training institute. You can remain independent while you are affiliated with them, like most of us are.) I don’t know the process well, but here is the little I learnt. Google selects ATPs based on their business history in coordinating trainings. So if your firm was just starting out, the chances that you will become an ATP are slimmer than a well established training firm. The ATP itself must have a minimum of two Authorized Trainers associated with them before they are allowed to coordinate trainings.
So, the ATP fills up a form with your name and details on it and submits it to Google. You fill up additional details on the link provided. Google then contacts you via email later for the next steps of the process.
The very first email I got gave me some topics that I had to prepare to present during the evaluation. All the material was on QwikLabs. Three subjects were chosen. You get slide decks (pdf) corresponding to them.
During the evaluation, we could be asked to present any of those topics for about an hour. For me, in one of the evaluations, I primarily presented one and the evaluator wanted to do the other subject also but there wasn’t enough time. However, he did still ask questions from the other module. The questions weren’t necessarily restricted to points in the slides; so you have to know the subject way more.
Where can I find Authorized Training Partners in my region and in my domain?
You can find the full list of training partners here: https://cloud.withgoogle.com/partners/?products=TRAINING_PRODUCT
Then you can filter by your region and also your specialization.
There is no money to be paid for applying or for getting the authorization.
Once the ATP and I had submitted our details, it was a period of silence lasting about a month. I thought things might be stuck, but from what I understand now, it is a process that stretches over time.
That first email asked me to pick a few time slots in the weeks ahead to see if an evaluator was available. These evaluations are typically done on a Friday, which was mentioned in the email. So I replied with the next three Fridays as options. I suppose those dates didn’t work out. I got another mail asking for new dates. The evaluation was scheduled within the new dates. From application to first evaluation, it had already been 7–8 weeks.
There wasn’t much information on the internet about the process. So I contacted an ATP who knew somebody who’d gone through the evaluation successfully. This person's experience was seemingly very different from mine. So I’m putting this down based on what I remember from the conversation sometime around January, 2019. (Typically, I would have left out somebody else’s experience as it is not my place to say it. However, this other story seemed to contrast significantly with mine and I think it will help people to have a feel for the amount of variance in the process.)
He explained that he had done some GCP training on his own before and had prepared his own material. Instead of using the QwikLabs material, he used his own. He was asked to present the same topic as I was. If I remember correctly, he told me that he hadn’t done much work in the area in the recent past. So for example, when he was asked to show how to do something on the console, he could not figure out where it was because the UI had changed. Additionally, the way he narrated it to me, the whole process sounded super easy. He apparently went through the material, the questions were light, and even though his experience wasn’t very deep in the topic, he sailed through the entire process. (Note that this is what I interpreted it as over the phone. For all you know, he might just have been super modest about it.)
Of the modules that I was assigned, one was related to Networking and the other to Kubernetes. I’d recently written the Networking exam, thought I did badly, and had started preparing with a vengeance going deep into the subject assuming that I was going to flunk the first exam and will have to retake it. But when the results came, I’d passed. Not only that, I thought the Networking Certification was the toughest and people would need help practising, so I’d started preparing a mock test. Suffices to say, I knew GCP Networking fairly well.
On Kubernetes, I had worked on it before and had even made contributions to some projects using Kubernetes. I had also started preparing a little for the Certified Kubernetes Administrator. And of course, I’d passed the GCP certifications. Hence again, safe to say that I knew Kubernetes concepts fairly well, even though I definitely wasn’t an expert yet.
The evaluation was scheduled over Hangouts. Since time was short, I setup two GCP projects beforehand based on the topics that I had been given so that I could do a demo if and when required. As part of my regular teaching/training, I typically do a lot of illustrating on boards, which I wouldn’t have here. Instead, I screen casted my iPad on to the Mac screen and sketched on the iPad Pro with the Apple Pencil. This is definitely not a requirement, but I was more comfortable with that setup than merely droning on and speaking off the slides.
After a couple of minutes of initial pleasantries, I was asked to start on one of the topics – Kubernetes. Now, a quick flash back, I have conducted a Google Cloud Jam for Docker and GKE once. I’d also conducted a Kubernetes 101 session for the Kubernetes community in Bangalore. This was towards the last quarter of 2018 probably. Make note, therefore, that this was not new to me. Yet, I did not get through in this evaluation.
This particular evaluation process itself left me a bit flustered. Now, I want to state this again, this is not a judgement on the interviewer or the process. I’m merely stating my experience from my point of view. Therefore, some bias is to be expected.
The topic itself was straightforward. I’d run through it a few times and even timed it, so that I could complete it in time. However, right off the bat, I was asked questions. The answers were either in other slides later on or were not in these particular set of slides. Usually in trainings that I’ve conducted, this wouldn’t matter as I’d either answer them then and there if it made sense or note down the questions and inform the attendees that I’d answer that question later after having covered some other topics necessary to fully understand it. That approach didn’t work here for me. Initially, I was told something to the tune of, “Assume we are a new team and we know nothing about containers or kubernetes”. But the very next question would be an in-depth question. I definitely stuttered in answering some of those as I couldn’t square that circle on as to at what level I answer the question.
There was also the question of time. These Q&As were definitely taking time. No issues answering the questions and I was glad to do that. Except that we had a hard cut off at about an hour. There were about 50 slides and it was at the back of my mind whether it was a requirement to complete it on time, and if not, would that be held against me?
Let me answer that right now: in the end I realized that wasn’t an issue. I did not complete all the slides. In the 2nd evaluation, the evaluator asked me to skip slides as they weren’t important enough for a discussion. Neither did we finish all the slides.
Back to the interview again and the timing. At that point though, it was a concern in my mind about completion. And I did consciously and, possibly, subconsciously speed up.
Of the questions that were asked, I was able to clearly answer some and I did not know the answer to some. One of them was straightforward, 101 question but I just blanked out on a convincing answer, which honestly was embarrassing. It’s the kind of thing one knows but then struggles for a clear definition and then ends up bumbling for words.
At the end of the allotted time, the interviewer was kind enough to give me feedback, which was very helpful. The interviewer in the next round did not give me any feedback. So, there is no fixed approach or defined process to this. These are the main points in the feedback that I got and you’d do well to learn from these:
The evaluator was also kind enough to inform me directly that I did not make it in this round but he did encourage me to try again.
One thing that surprised me about the evaluation was that my facilitation skills were rated high, but I did not pass. When I read the web page for this program, I interpreted the content somewhat like this: the primary purpose of the evaluation is checking facilitation skills and not your tech skills as those were already validated via the certification. My discussion with the other person who qualified gave me the same impression. My personal experience, however, was that this is not true at all. In-depth technical knowledge on the topic is necessary.
At this point, I wasn’t very sure if and when I would try for the authorized trainer again. The upcoming months were looking very busy for me with a bunch of travel and other assignments. I thought maybe I’ll apply again after a few months. A couple of days later, however, I got a mail from the Google coordinator asking for dates for the 2nd evaluation. I don’t think this is really a process that they reach back to you for a 2nd round. I’d not heard about this before and I’d suggest you don’t expect it. In this mail she mentioned that there might be available dates within the next few weeks, and I thought, sure, why not? I was preparing for CKA anyways and also had done additional work on networking. The second evaluation was scheduled in the last week of March.
The evaluator asked me to start on either of two topics. I thought I was comfortable with both, but we picked one to start with. This time, I did apply the feedback from the last time and made it a point to speak at a slower pace. I did not worry about finishing the slides either. The evaluator told me at the beginning that he wanted to cover both modules. His approach was to make me skip the slides with less relevant content and to spend time on slides with more in-depth content. Personally, I liked this approach. The discussions were always at a consistent level and hence, from my point of view, it flowed more smoothly than the first evaluation. The questions and discussions were also deeper – think all chapters in the book Kubernetes in Action. Obviously, in the given time we couldn’t have covered all topics in detail but it did touch upon a few. Did I get all questions right? I definitely doubt it but I don’t think I did bad either.
Around the 50 minute mark, I was asked to stop on Kubernetes. There wasn’t enough time to also cover the full Networking module. Instead, he asked me a couple of questions about Networking on GCP.
I asked a couple of questions and clarifications on the Authorized Trainer process in the few extra minutes we hung on. Unlike the first evaluation, I did not receive any feedback. I thought that was a loss though, because I did benefit from feedback on the first one.
I do not know the result of the second evaluation. If I compared it to the other person’s experience, I should have probably got through the very first time itself. But very honestly, I appreciate tough processes and tight filtering in such contexts. I lose some, I win some. But the system is better for allowing in only the best. If I failed this one and had to try again later to ensure high quality, so be it.
The only anxious part of this whole process is figuring out whether anything is happening or when things are expected to happen. There are no fixed timelines, and you are definitely going to witness a different schedule. Yet, seeing my timeline might give you some calm.
12th Jan: An ATP applies for me. I get a mail with the next steps.
14th Jan: Step 1: I fill in my details and submit.
14th Jan: Step 2 is completing the certification, which I’m already done with. I receive a mail confirming this.
14th Jan: Step 3a. I receive a mail with the modules to prepare on. In step 3b, I will be assigned an interview/er.
11th Feb: I receive a mail asking for available dates on which I can attend the evaluation. I give three dates — Feb 15th, Feb 22nd, March 1st.
26th Feb: I receive another mail asking me to choose a date the following week. An interview is scheduled for March 5th.
5th March: Step 3c. Interview happens. I don’t pass.
8th March: I get a mail requesting dates for a second interview. I give a range of dates. An interview is scheduled for 29th March.
29th March: Step 3c again. 2nd interview happens. Results awaited.
Maybe in the future sometime: Step 4. Pass the evaluation and you are now a Google Cloud Platform authorized trainer.
Edit 4th April: I̶’̶m̶ ̶t̶o̶l̶d̶ ̶b̶y̶ ̶t̶h̶e̶ ̶A̶T̶P̶ ̶c̶o̶o̶r̶d̶i̶n̶a̶t̶o̶r̶ ̶t̶h̶a̶t̶ ̶h̶e̶ ̶n̶e̶e̶d̶s̶ ̶t̶o̶ ̶f̶i̶l̶l̶ ̶i̶n̶ ̶a̶ ̶f̶o̶r̶m̶ ̶a̶n̶d̶ ̶s̶e̶n̶d̶ ̶i̶t̶ ̶t̶o̶ ̶G̶o̶o̶g̶l̶e̶ ̶t̶o̶ ̶g̶e̶t̶ ̶t̶h̶e̶ ̶r̶e̶s̶u̶l̶t̶ ̶o̶f̶ ̶t̶h̶e̶ ̶e̶v̶a̶l̶u̶a̶t̶i̶o̶n̶.̶ ̶A̶n̶d̶ ̶h̶e̶ ̶n̶e̶e̶d̶e̶d̶ ̶t̶h̶e̶ ̶d̶a̶t̶e̶s̶ ̶o̶f̶ ̶t̶h̶e̶ ̶c̶o̶m̶m̶u̶n̶i̶c̶a̶t̶i̶o̶n̶.̶ ̶S̶o̶,̶ ̶y̶o̶u̶ ̶s̶h̶o̶u̶l̶d̶ ̶p̶r̶o̶b̶a̶b̶l̶y̶ ̶i̶n̶f̶o̶r̶m̶ ̶y̶o̶u̶r̶ ̶A̶T̶P̶ ̶a̶f̶t̶e̶r̶ ̶t̶h̶e̶ ̶i̶n̶t̶e̶r̶v̶i̶e̶w̶ ̶i̶s̶ ̶o̶v̶e̶r̶.̶ (So this did not prove to be true. The approval mail is sent directly to you. The ATP sending a mail has no effect on the process but they have work to do after you pass.)
Edit 22nd April: I’m a GCP Authorized Trainer! 🎉
̶I̶ ̶o̶b̶v̶i̶o̶u̶s̶l̶y̶ ̶d̶o̶n̶’̶t̶ ̶h̶a̶v̶e̶ ̶f̶i̶r̶s̶t̶-̶h̶a̶n̶d̶ ̶k̶n̶o̶w̶l̶e̶d̶g̶e̶ ̶o̶f̶ ̶t̶h̶i̶s̶ ̶s̶i̶n̶c̶e̶ ̶I̶ ̶h̶a̶v̶e̶n̶’̶t̶ ̶q̶u̶a̶l̶i̶f̶i̶e̶d̶ ̶y̶e̶t̶.̶ ̶I̶ ̶s̶h̶a̶l̶l̶,̶ ̶h̶o̶w̶e̶v̶e̶r̶,̶ ̶p̶u̶t̶ ̶d̶o̶w̶n̶ ̶a̶ ̶f̶e̶w̶ ̶p̶o̶i̶n̶t̶s̶ ̶t̶h̶a̶t̶ ̶I̶ ̶w̶a̶s̶ ̶c̶u̶r̶i̶o̶u̶s̶ ̶a̶b̶o̶u̶t̶ ̶a̶n̶d̶ ̶a̶b̶o̶u̶t̶ ̶w̶h̶i̶c̶h̶ ̶I̶ ̶e̶n̶q̶u̶i̶r̶e̶d̶.̶ I know a little better about some of this now.
I asked a few questions to the team regarding certification expiry, re-certification, and its linkage to the Authorized Trainer status. Below is that Q&A. I’ve made edits for clarity.
Is there a connection between the certification continuity and the authorized trainer status?
Is the Trainer status automatically removed if the certification expires?
Is the certification connected to the Series ID of the certificate?
If the person re-certifies after it expires, will they need to go through the interview process again?
What happens if the re-certification is done before it expires?
My notes on all my certifications — https://medium.com/@sathishvj/on-passing-all-google-cloud-certifications-54b2cc1e428c
Authorized Trainer Website: https://sites.google.com/view/gcptrainer/home
Training Partners: https://cloud.withgoogle.com/partners/?products=TRAINING_PRODUCT
Wish you the very best with your GCP certifications and qualifying for the Authorized Trainer. You can reach me at LinkedIn and Twitter. If you can support my work creating videos on my YouTube channel AwesomeGCP, you can do so on Patreon or BuyMeACoffee.
tech architect, tutor, investor | GCP 12x certified | youtube/AwesomeGCP | Google Developer Expert | Go
144 
5
144 
144 
5
tech architect, tutor, investor | GCP 12x certified | youtube/AwesomeGCP | Google Developer Expert | Go
"
https://medium.com/@retomeier/what-is-googles-cloud-platform-d92a9c9e5e89?source=search_post---------35,"Sign in
There are currently no responses for this story.
Be the first to respond.
Reto Meier
Feb 11, 2017·3 min read
Sadly, the Wikipedia entry for GCP is garbage, and while the official docs are pretty good, the marketing-dust sprinkled on them gives me a toothache.
As part of my re-self-introduction to Google’s Cloud Platform, I wrote an objective summary of GCP for my own reference. I have a conflict of interest, so I can’t fix the Wikipedia entry, but I can share what I wrote, so here it is.
Google Cloud Platform (GCP) is a collection of Google’s computing resources, made available via services to the general public as a public cloud offering.
The GCP resources consist of physical hardware infrastructure — computers, hard disk drives, solid state drives, and networking — contained within Google’s globally distributed data centers, where any of the components are custom designed using patterns similar to those available in the Open Compute Project.
This hardware is made available to customers in the form of virtualized resources, such as virtual machines (VMs), as an alternative to customers building and maintaining their own physical infrastructure.
As a public cloud offering, software and hardware products are provided as integrated services that provide access to the underlying resources. GCP offers over 50 services including Infrastructure as a Service (IaaS), Platform as a Service (PaaS), and Software as a Service (SaaS) offerings in the categories of Compute, Storage & Databases, Networking, Big Data, Machine Learning, Identity & Security, and Management & Developer tools.
These services can be used independently or in combination for developers and IT professionals to construct their own, custom cloud-based infrastructure.
GCP is hosted on the same underlying infrastructure that Google uses internally for end-user products including Google Search and YouTube.
Each of Google Cloud Platform’s services and resources can be zonal, regional, or managed by Google across multiple regions.
A zone is a deployment area for resources within a region. Zones are isolated from each other to prevent outages from spreading between them, so each zones is considered a single failure domain within a region.
Zonal resources operate within a single zone; if a zone becomes unavailable all of its resources are unavailable until service is restored. Regional resources are deployed with redundancy across zones within a region. Multi-regional services (Google App Engine, Google Datastore, Google Cloud Storage, Google BigQuery) are managed by Google to be redundant and distributed within and across regions.
All GCP service instances are configured such that maintenance events are transparent to applications and workloads via live migration. Live migration moves running virtual machine instances out of the way of maintenance that is being performed.
GCP services are available in 18 zones in 6 regions: Oregon, Iowa, South Carolina, Belgium, Taiwan, and Tokyo¹.
In 2016, Google announced plans to make 22 zones and 8 new regions available in 2017: Sydney, Sao Paulo, Frankfurt, Mumbai, Singapore, London, Finland, and Northern Virginia.
Within each region, traffic tends to have round-trip network latencies of under 5ms on the 95th percentile.
Note that like everything I publish on Medium, these are my views on GCP, and may not represent the opinions of my employer.
Something missing or wrong? Add a comment!
Developer Advocate @ Google, software engineer, and author of “Professional Android” series from Wrox. All opinions are my own.
See all (353)
214 
2
214 claps
214 
2
Developer Advocate @ Google, software engineer, and author of “Professional Android” series from Wrox. All opinions are my own.
About
Write
Help
Legal
Get the Medium app
"
https://blog.machinebox.io/deploy-docker-containers-in-google-cloud-platform-4b921c77476b?source=search_post---------36,
https://towardsdatascience.com/a-gentle-introduction-to-apache-druid-in-google-cloud-platform-c1e087c87bf1?source=search_post---------37,"Sign in
There are currently no responses for this story.
Be the first to respond.
Antonio Cachuan
Oct 21, 2019·9 min read
Making easy to analyze billions of rows
In order to have a clear understanding of Apache Druid, I’m going to refer what the official documentation says:
Apache Druid (incubating) is a real-time analytics database designed for fast slice-and-dice analytics (“OLAP” queries) on…
"
https://blog.doit-intl.com/gslack-9391be7c191a?source=search_post---------38,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
One of the perks I really like as part of my work as CTO at DoiT International, is my day-to-day conversations with our customers. On many occasions, they are enlightening and I always learn something new.
Last week, I’ve noticed a ticket from one of our clients with a question on what would be the best way to integrate Google Cloud Platform with Slack.
Specifically, they wanted to get notified to their Slack channel when some activity took place in one of their Google Cloud projects, for example instance being started or terminated, new bucket being created or deleted and so on. Isn’t it a nice idea? Quick search with google.com didn’t reveal any immediate results so I had to figure out what would be the quickest and simplest way of achieving Slack and Google Cloud Platform integration.
Fortunately, there is an elegant, completely serverless solution for this which I will try to explain in this post. Even better, today we are open-sourcing the gSlack which you can deploy in your own GCP project in just few minutes and get instant and flexible notifications to your Slack channel.
gSlack uses Stackdriver Logging — Google’s centralized logging platform which allows you to store, search, analyze, monitor, and alert on log data and events from Google Cloud Platform (and Amazon Web Services). Most of Google’s cloud services send its logs to Stackdriver Logging. I was specifically interested in “Activity Logs” which include changes in Google Cloud Platform environment.
Here is how it looks in the Google Cloud Console:
One of the most cute features of Stackdriver Logging is its ability to automatically export new log entries to Google Cloud Storage, Google BigQuery and Google Pub/Sub. You just configure an ‘export’ and everything else just magically happens for you.
I needed a transport which could relay log entries from Stackdriver Logging & Slack so Pub/Sub looked like the best alternative for this use-case. If you are not familiar with Pub/Sub, it is Google’s fully-managed real-time messaging service that allows you to send and receive messages between independent applications.
Setting up Pub/Sub export is as easy as configuring log filter (I only need ‘activity’ based logs, hence the logName=”projects/doit-playground/logs/cloudaudit.googleapis.com%2Factivity”) and the name of Pub/Sub topic to push the messages to:
So now, every time there is a new entry in Stackdriver’s Logging, it will be automatically pushed to my Pub/Sub topic. Pretty nice, right?
Next, I needed to put some ‘glue’ between the Pub/Sub and Slack so new messages being published to Pub/Sub will be posted as Slack notifications. Luckily, Google now has (in beta) Cloud Functions — a serverless environment to build and connect cloud services. Basically, you can code a ‘function’ written in NodeJS which will be triggered by one of the supported triggers such as new file in the bucket, http request or (I bet you’ve already guessed!) — new message in a Pub/Sub topic!
Our complete flow would look like this — StackDriver Logging logs certain activity in our project, automatically sends it to Pub/Sub topic which in its turn invokes Cloud Function sending a message to Slack channel using official Slack’s NodeJS SDK.
To setup Cloud Functions, you’ll need to upload a zip file containing the code and your package.json file:
To avoid sending every activity log entry coming to Slack channel (some of them might be not that interesting) and better format messages being published to Slack, I’ve decided to use one more Google’s managed service — Google Cloud Datastore.
Google Cloud Datastore is a managed NoSQL document database built for automatic scaling, high performance, and ease of application development. It has built-in integration with Cloud Functions and also a nice UI which you can use to quickly edit entries (‘kinds’ and ‘properties’ in Datastore’s terminology).
We need something like Google Cloud Datastore to persistently store runtime configuration of gSlack, specifically the definition of which messages we want to publish and how messages published to Slack will look like.
For each set of log entries, you’ll need to configure test, message and the slackChannel where you want to publish the notification.
The test must be a valid JS expression that returns a boolean. If it returns true the test passes and the message will be sent to Slack. For example, if we want to only include messages from Google Compute Engine and track ‘start’ and ‘stop’ instance events, we can use the following test:
Similarly, the message must be a valid JS string template. It will be evaluated to produce the message. e.g.
As a result, the following notification will be sent to Slack. In my tests, it only takes about 5 seconds between the actual event and the time the message gets to the Slack channel.
You can add as many tests & messages as you’d like, parsing various Google Cloud Platform services such as Compute Engine, App Engine, Cloud Storage, BigQuery and even Billing. Actually, we include some of these examples in the gSlack repository.
The full code as well as deployment instructions are available at GitHub’s gSlack repository. Feel free to star or pull request and improve the gSlack ;-)
As always, you can reach me at vadim@doit-intl.com with your suggestions and ideas.
P.S. I’d like to thank Shahar Frank, Cloud Architect at DoiT International who actually coded the whole example in just few hours as well as helping me to prepare this post.
Software & Operation Engineering. Written By Engineers.
185 
5
Your monthly dose of Google Cloud and Amazon Web Services articles  Take a look.
185 claps
185 
5
Written by
CTO of world’s best cloud consultancy
Here, our SRE and SWE teams share the solutions to some of the most interesting challenges we encounter during our daily work at DoiT International. If you like what we do, check out our https://careers.doit-intl.com page and join the team!
Written by
CTO of world’s best cloud consultancy
Here, our SRE and SWE teams share the solutions to some of the most interesting challenges we encounter during our daily work at DoiT International. If you like what we do, check out our https://careers.doit-intl.com page and join the team!
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/ethereum-on-google-cloud-platform-8f10c82493ca?source=search_post---------39,"There are currently no responses for this story.
Be the first to respond.
Google’s super-fast network, powerful VMs and SSDs coupled with its state-of-the-art container technologies make Google Cloud Platform (GCP) an unparalleled destination for Blockchain platforms.
Google could do better in ensuring that developers are aware of this.
Late last year, I wrote a story describing how to deploy Ethereum to GCP. I’d not used Ethereum between then and today. This story updates that previous post with easier ways to run Ethereum on GCP using: (a) Container-Optimized OS; (b) using Kubernetes Engine.
You’ll need a GCP Project and there are free options. You will need Cloud SDK (aka gcloud) installed. Let’s get started.
Container-Optimized OS permits you to deploy a Docker image to a Google Compute Engine VM. This is the simplest way to run the ethereum/client-go image:
NB To avoid exhausting disk, the container uses 500GB (the entire default Quota) of SSD Persistent Storage. Reduce the boot-disk-size or change the boot-disk-type to pd-standard as you prefer.
NB If you would prefer to use the test network add — container-arg=""— testnet” before the existing — container-arg flags. If you would prefer to use Rinkeby add — container-arg="" — rinkeby"" before the existing--container-arg flags.
The VM should be running in under 30 seconds, you can peek into its state with:
NB konlet-startup is the systemd service that corresponds to the Docker image deployment in the Container-Optimized OS.
Success looks like:
And you may confirm the container is running:
Then, you may use gcloud to start the ssh port-forward to the instance:
And — from a different shell — you can connect to the Ethereum node running on the instance:
Which should yield:
Running Ethereum on GCP leverages Google Cloud Logging. You will automatically be able to view and query logs.
https://console.cloud.google.com/logs/viewers
NB Slightly confusingly (!), you must select “Global” as the resource and you should then see “gcplogs-docker-driver” as an option. This will — as above — provide the container’s logs.
Everything you see via Cloud Console is accessible from the CLI too. In this case, you may run the following command to view the container’s logs:
You should log entries like this:
NB What you *don’t* want are log entries of the form “no space left on device” as these terminate the container. If you receive these you’ll need to increase the size of the boot-disk-size flag or grab less data.
You also get basic monitoring *but* of the VM not the container(s) specifically:
You may exit the JavaScript console then delete the Ethereum VM with:
If you would prefer to use a Kubernetes (Engine) cluster, the commands are as straightforward.
Assuming you are authenticated against a cluster, the following steps create a namespace for Ethereum, create a disk (actually a Kubernetes PersistentVolumeClaim), deploy a single Go Ethereum (aka “geth”) node and port-forward to your local workstation so that you may attach to it:
NB If you would prefer to use the test network insert ""--testnet"" (including the quotation marks) between lines 33–34. If you’d prefer to use Rinkeby network insert ""--rinkeby"" instead.
This should result in:
And you should be able to see the Persistent Volume Claim created:
https://console.cloud.google.com/kubernetes/storage
And the Deployment:
https://console.cloud.google.com/kubernetes/workload
Running Ethereum on GCP yields some useful, automatic benefits including, Console monitoring:
Logging:
https://console.cloud.google.com/logs/viewer
Some Stackdriver goodness (although this is even better with the recently-announced Stackdriver Kubernetes Monitoring — not enabled on this cluster). If you’ve not configured Stackdriver for the project, you’ll need to do that first and you may use the free tier:
https://console.cloud.google.com/monitoring
We’ll use ssh port-forwarding (through gcloud) to access the Go Ethereum node from our workstation without opening a firewall.
NB These nebulous looking commands grab one of the Kubernetes cluster’s node names; determine which NodePort the Ethereum Service is being served on; and then uses gcloud to port-forward that node’s port to the localhost’s (same) port. If you prefer you could use — ssh-flag=“-L 8545:localhost:${PORT}” and then in the subsequent command replace ${PORT} with 8545 too. Your choice!
NB In my case echo ${PORT} (this time) returned 30873.
Then, from your local workstation, run the geth JavaScript client and attach to the node running on the Kubernetes cluster:
NB In my case, the value of ${PORT} is 30873.
And you should then be put in the Ethereum JavaScript console:
When you’re done, it’s simplest to whack the Kubernetes namespace which will delete the Deployment, the Service and the PeristentVolumeClaim:
I uncovered an issue using Wallets passed to Kubernetes (Engine) as Secrets. Providing data to Pods using Secrets (and ConfigMaps) is considered a good practice with Kubernetes. However, the implementation of both Secrets and ConfigMaps when (volume) mounted into Pods is to present file content as symbolic links. This appears to be a problem with Wallets and Ethereum:
https://github.com/ethereum/go-ethereum/issues/16793
If you’d like to use existing wallet file(s) when you deploy to Kubernetes, this should be as simple as creating a Secret (you may use ConfigMaps similarly but Secrets provide opacity for… secrets and other confidential info) and then mounting the Secret as a volume mount for the Ethereum node to access. However, as above, this does not work correctly with Ethereum. So, you will need the additional step in the ‘hacky workaround’ below.
First, create the Secret:
NB Your path should end in a directory called keystore and this directory and its wallet(s) will be encoded in the Secret.
My hacky workaround is to use init containers to copy the wallet(s) from the Secret (called /keystore) into an emptyDir (called /cache) volume *before* the Ethereum node starts and configure Ethereum to look for the wallet in /cache rather than /keystore (because this doesn’t work). This works because copying the symbolic link duplicates the underlying file.
NB See the next section ‘SSD’ to configure the datadir as SSD rather than HDD.
NB Line #31–39 create an init container called init-service. init-service uses Alpine to copy files of the form UTC* from /keystore to /cache. These directories are volume mounted. The volumes are defined (for the Pod not specific containers) in lines #66–75. keystore is the Secret containing wallet file(s) mounted as symbolic link(s) that do not appear to work with Ethereum if used directly. cache is an emptyDir volume and, if Ethereum uses the wallet(s) when copied here, it does work!
NB In line #46, Ethereum is configured to use the /cache directory for wallets with --keystore=/cache.
The Kubernetes Deployment used above includes a PersistentVolumeClaim spec and line #7 defines storageClassName of standard. This corresponds to regular Persistent Disk.
If you’d prefer to use faster SSD, we must first register the new Storage class in Kubernetes Engine:
and:
Now, when you query the Kubernetes Engine storage classes, a new type pd-ssd called ssd should be added:
Then, we can revise the Deployment file’s line #7 and change storageClassName: standard to storageClassName: ssd.
NB Default “Persistent Disk SSD (GB)” Quotas for GCP Projects are 500 GB (gigabytes) per zone. If you’ve not increased these quotas, you will be unable to provision a 500 GiB (!) disk when you Apply the Deployment. For this reason, please also *reduce* the value of line #12 in the Deployment to e.g. 400Gi (~429GB) which should be sufficient *unless* you have other SSD PD in your Project in the zone. Or, request a Quota increase:
https://console.cloud.google.com/iam-admin/quotas?project=${PROJECT}&service=compute.googleapis.com&metric=Persistent%20Disk%20SSD%20(GB)
To effect this change we must delete and recreate the Deployment:
NB There are 4 Compute Engine Disks: 3 are “Standard Persistent disk” one for each of the 3 Nodes in the Kubernetes cluster. The 4th disk is 430GB (==400GiB) and is of type “SSD persistent disk”. It was created by the PersistentVolumeClaim when we applied the Deployment.
In this post, I’ve provided two straightforward ways to run a single Ethereum node on Google Cloud Platform.
Feedback is always welcome.
That’s all!
Google Cloud community articles and blogs
237 
2
No rights reserved
 by the author.
237 claps
237 
2
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/swlh/how-to-ci-cd-on-google-cloud-platform-1e631cded335?source=search_post---------40,"There are currently no responses for this story.
Be the first to respond.
Google Cloud Platform is one of the leading cloud providers in the public cloud market. It provides a host of managed services, and if you are running exclusively on Google Cloud, it makes sense to use…
"
https://medium.com/machines-school/data-science-on-the-google-cloud-platform-%E0%B8%97%E0%B8%B3%E0%B8%84%E0%B8%A7%E0%B8%B2%E0%B8%A1%E0%B8%A3%E0%B8%B9%E0%B9%89%E0%B8%88%E0%B8%B1%E0%B8%81%E0%B8%81%E0%B8%B1%E0%B8%9A-google-cloud-platform-gcp-7605b4560fb8?source=search_post---------41,"There are currently no responses for this story.
Be the first to respond.
“Cloud Computing” หรือแปลเป็นภาษาไทยว่า “การประมวลผลแบบกลุ่มเมฆ” เป็นลักษณะของการทำงานของผู้ใช้งานคอมพิวเตอร์ผ่านอินเทอร์เน็ต ที่ให้บริการใดบริการหนึ่งกับผู้ใช้ โดยผู้ให้บริการจะแบ่งปันทรัพยากรให้กับผู้ต้องการใช้งานนั้น
โดยสถาบันมาตรฐานและเทคโนโลยีแห่งชาติของสหรัฐอเมริกา (NIST) ได้ให้จำกัดความไว้ว่า “เป็นรูปแบบที่ให้บริการเพื่อใช้ทรัพยากรคอมพิวเตอร์ร่วมกับผู้อื่นผ่านทางเครือข่ายเน็ตเวิร์ค เช่น โครงข่ายเน็ตเวิร์ค เซิร์ฟเวอร์ หน่วยเก็บสำรองข้อมูล แอพพลิเคชั่นและเซอร์วิสต่างๆ เป็นต้น โดยขึ้นอยู่กับความต้องการของผู้ใช้ ”
Google Cloud Platform หรือเรียกย่อๆว่า GCP เป็นกลุ่มทรัพยากรในการประมวลผล (computing resource) ของ Google ที่เปิดให้ทุกคนสามารถเข้าใช้งานได้ในรูปแบบของ Public Cloud
Google Cloud Platform สามารถใช้งานได้ทั่วโลก โดยเราสามารถเลือกใช้งานได้ตาม “regions” และ “zones” ทั้งนี้ขึ้นอยู่กับ latency, availability, durability
regions จะขึ้นอยู่กับลักษณะทางภูมิศาสตร์ เช่น US East, US West, US Central, Europe, East Asia, Southeast Asia, South Asia หรือ Australia เป็นต้น และในแต่ละ regions จะประกอบด้วย zone เช่นใน regions ของ Southeast Asia ที่ตั้งอยู่ในสิงค์โปร์ก็จะมี zone A และ zone B เป็นต้น
Google Cloud Platform จะแบ่งตามลักษณะการใช้งานดังนี้ (ข้อมูลเมื่อวันที่ 7 มิถุนายน 2561) แต่บาง region ก็ไม่สามารถใช้งานได้ในบาง serivice
*หมายเหตุ: ข้อมูลเมื่อวันที่ 7 มิถุนายน 2561
บทความเรื่อง “Data Science on the Google Cloud Platform”ตอนที่ 1: ทำความรู้จักกับ Google Cloud Platform (GCP)ตอนที่ 2: วัฎจักรข้อมูล (Data Lifecycle) บน Google Cloud Platform
Machines School
106 
106 claps
106 
Written by
Founder of Humaan.ai—The AI as a tools for unleash human capabilities. 🧠 🚀
Machines School
Written by
Founder of Humaan.ai—The AI as a tools for unleash human capabilities. 🧠 🚀
Machines School
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/javarevisited/5-best-courses-to-learn-google-cloud-platform-gcp-in-2021-169093a3771a?source=search_post---------42,"There are currently no responses for this story.
Be the first to respond.
Hello guys, if you want to learn Google Cloud Platform and looking for the best online courses then you have come to the right place. Earlier, I have shared the best AWS courses and best courses to learn Azure, and today, I am going to share the best Google Cloud courses for beginners.
Disclosure — Btw, some of these links are affiliate links and I may get paid if you join these courses using my links.
The GCP or Google Cloud Platform is a slightly late entrant in the world of public cloud computing but it has completely changed the public cloud landscape in the last few years, particularly the monopoly of AWS on Cloud has been challenged.
It’s still neither the most popular public cloud platform like Amazon Web Services, nor the darling of the corporate world like Microsoft Azure but its Performance, Tools, and innovation Google is attracting more and more companies to Google Cloud Platform, particularly in the areas of Big Data and Machine Learning.
With tools like BigTable, BigQuery, and libraries like TensorFlow, Google Cloud Platform is quickly becoming the go-to platform for Machine learning innovations.
Because of all these, there is an increased demand for Cloud experts who are familiar with GCP and Google Cloud Platform concepts and tools. Unfortunately or fortunately, currently, there is a lot of shortage with developer knowing Google Cloud Products like Bigtable, BigQuery, Google Compute Engine, etc. Similarly, If you look for a certified Google Cloud Architects, you will find one or two in your network, compared to a handful of AWS architects. This popularity of Google Cloud Platform and shortage of skilled GCP developers means it's the perfect time to learn Cloud Computing and Google Cloud Platform, especially if you are looking to make a career in Cloud Computing. On top of that currently, Google is currently offering $300 free credit which means there won’t be a better time to learn Google Cloud Platform than now. So, how do we start?
Well, there is no better way to learn Google Cloud than joining some of the best Google Cloud online courses on Coursera and Udemy which is offered by Google Cloud itself like Developing Applications with Google Cloud Platform Specialization on Coursera.
coursera.com
You can also use these courses to prepare for Google Cloud Associates and Professional Certifications like Google Cloud Associate Cloud Engineer and Google Cloud Professional Cloud Architect, GCP Professional Data Engineer, and Google Cloud Developer.
Here are some of the best online courses to learn about the Google Cloud Platform or GCP. In these courses, you will not only learn about concepts and technologies that make up the Google cloud world, but also understand what Google’s cloud has to offer for DevOps, Developers, and Machine Learning enthusiasts. Some of the courses will also help you to prepare for the Google Cloud Associate Cloud Engineer certification exam like the second course on the list is by Ryan Kroonenburg, who is also the author of best selling AWS Certifications course like this AWS Solution Architect course. I have also included some of the best Google Cloud courses from Coursera, particularly a Specialization which will not only teach you how to design, develop, and deploy Apps on GCP. but also provides lots of hands-on labs to practice with Google Cloud components and services, which will eventually help you to build secure, scalable, and intelligent cloud-native applications. Without any further ado, here is my list of best courses to learn Google Cloud Platform online:
This is one of the best online courses to learn the fundamentals of Google Cloud and its Big Data technologies as well as to pass the Google Data Engineer and Google Cloud Experts Certification exam. In this course, you will not only learn about GCP fundamentals like Compute Engine and App Engine but also about their Big Data and Machine learning tools like BigQuery, Bigtable, DataProc, Datalab, TensorFlow, and Hadoop clusters. I highly recommend this course for beginners who wants to learn about Google Cloud Platforms as well as people who want to prepare for Google Data Engineer and Google Cloud experts platform.
Here is the link to join this GCP course — GCP: Complete Google Data Engineer and Cloud Architect Guide
Both instructors are very knowledgeable and have strong experience in Google big Data technologies which reflect in this course. They are also from Google itself and had first-hand experience of these technologies, which makes this course even more interesting. Talking about social proof this course has already taught Google Cloud Fundemantsl to well over 20,000 students and have on average 4.2 ratings from close to 3,110 rating participants, which speaks volumes about its quality.
This is one of the most complete online resources to learn the Google Cloud Platform. This Coursera specialization is a collection of 4 online courses that will teach you how to design, develop, and deploy applications that seamlessly integrate components from the Google Cloud Platform. The course is very hands-on and thorough on explanation. You will learn key GPC concepts like compute engine, cloud storage, dataflow, etc to develop a scalable and intelligent cloud-native application. The best part of this specialization is that the courses are offered by Google Cloud Training itself. I have taken this course and vouch for the quality of them. They are just too good on both content and delivery. It would take approx 1 month to complete if you can spend 14 hours/week, but the course is completely online and you can go on your own schedule. The course is also available in French, Portuguese, German, Spanish, and Japanese apart from English.
Here is the link to join this GCP course — Developing Applications with Google Cloud Platform
Ok, let me tell you that this is probably the best online training course to pass the prestigious Google Certified Associate Cloud Engineer and Architect exam, not just for content but also for presentation and delivery. The cloud guru Ryan Kroonenburg is one of the authorities when it comes to cloud and having gone through his AWS Architect certification course, I have become a big fan of him. Even though this course is not taught by him but Mattias Andersson, the quality remains the same. The course is actually a bundle of three-course — an Introduction to Google Cloud Platform, Google Certified Cloud Data Engineer course, and Kubernetes deep dive by Nigel Poulton, which is an important topic for Google Cloud certifications.
This means it's also an excellent course to learn about Google’s cloud offering for beginners or anyone who want to learn GCP. The course also includes 2 practice tests to prepare you will well for Google’s Associate Cloud Engineer certification exam.
Here is the link to join this Google Cloud course — Google Certified Associate Cloud Engineer Certification
This is one of the best but old introductory course on Google Cloud Platform or GCP for programmers and architects who want to move into the Google cloud. In this course, Google Developer Expert Lynn Langit introduces you to Google’s Cloud technologies and provides an overview of what is possible with Google Cloud. By the end of the course, you’ll know and understand essential Google cloud services like Google App Engine, Google Compute Engine, and more into your organization. No Google cloud knowledge is required, but a lot will be imparted. This course is aimed at developers and business decision-makers and is actionable for executives as well.
It also includes a ‘Hello World’ GAE demo using Eclipse (Java) which makes it ideal for Java developer who wants to learn about the Google Cloud Platform.
Here is the link to join this course — Introduction to Google Cloud By Lynn Langit
Btw, this course would require Pluralsight membership. If you already have a Pluralsight membership then this is a great introductory course about GCP. If you don’t have a membership then you can either subscribe, it cost around $29 per month or $299 per annum (currently just $179, 40% discount), or take this course for free by signing their 10-day free trial.
pluralsight.pxf.io
While this course is good for beginners, it’s a bit outdated given how fast technology is changing and Lynn Langit has also created GCP Essentials, GCP Enterprise Essentials, and GCP Machine Learning Essentials courses on LinkedIn Learning, you can also check them out. LinkedInLearning offers 1 month free so that’s good enough time to check these courses for FREE.
This is another excellent specialization from Coursera for Data Scientists who want to deploy their machine learning models on Google Cloud to take advantage of TensorFlow and the performance offered by GCP.  Like the first specialization, this one is also offered by Google Cloud which makes it a kind of official resource to learn Machine learning for Google Cloud Platform.  The Specialization is a collection of the following 5 courses — How Google does Machine Learning — Launching into Machine Learning — Intro to TensorFlow — Feature Engineering, and — Art and Science of Machine Learning All courses are 100% online which means you can learn on your own schedule. In general, it takes 1 month to complete this specialization given you spent around 15 hours/week but you can go at your convenience. The course is also available in English, French, Portuguese, Brazilian, German, Spanish, and Japanese. The online labs are provided by Qwiklabs which makes working with GCP really pleasant. You can start with whichever course you want but you need to finish all lectures and assignments a certification will be awarded to you which you can put on your LinkedIn profile or your CV.
Here is the link to join this GCP ML course — Machine Learning with TensorFlow on Google Cloud Platform
By the way, if you find Coursera courses and specialization useful then you should also join the Coursera Plus, a subscription plan from Coursera which provides you unlimited access to their most popular courses, specialization, professional certificate, and guided projects.
coursera.com
Another great course on Udemy to learn Google Cloud Platform for Beginners. It’s great to learn Google Cloud Platform from scratch. You will learn key GCP services with their pros and cons and learn when to use them in the real world.
Here is the link to join this course — Google Cloud Platform (GCP) Fundamentals for Beginners
udemy.com
If you have some knowledge about Cloud computing and you have used AWS before then learning Google Cloud Platform is not difficult for you and this course make it even easier.
Enterprises are going multi-cloud. It is NOT sufficient to know JUST one cloud anymore. You would need to understand multiple clouds.
In this course, you will learn Google Cloud by building on your AWS knowledge.
Instead of learning Google Cloud Platform the same way you learned AWS, you will learn GCP comparing GCP services with AWS services. You will learn Google Cloud Platform using AWS as the stepping stone.
By the end of the course, you will see that learning Google Cloud Platform — GCP is very easy when already know AWS!
Here is the link to join this course — Google Cloud Platform for AWS Professionals
udemy.com
That’s all about some of the best courses to learn Google Cloud Platform or GCP. I have also included some courses to prepare for Google Certified Associate Cloud Engineer Certification which is another great way to learn Google Cloud Platform and get a certificate for your skill. Something which you can put in your resume and LinkedIn profile.  This is not really a big list as I am also learning Google Cloud, so if you have a course that should be on this list or something I should take a look at, feel free to suggest in the comments.  Other Certification Resources for IT Professionals and Java Programmers
Thanks for reading this article so far. If you like this article then please share it with your friends and colleagues. If you have any questions or feedback, please drop a note. P. S. — If you are new to the world of Cloud and AWS and looking for some free courses to learn Amazon Web Service then you can also, check this list of Free AWS Courses for Beginners to start with.
medium.com
Medium’s largest Java publication, followed by 14630+ programmers. Follow to join our community.
268 
3
Collection of best Java articles, tutorials, courses, books, and resources from Javarevisite and its authors, Java Experts and many more.  Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
268 claps
268 
3
Written by
I am Java programmer, blogger, working on Java, J2EE, UNIX, FIX Protocol. I share Java tips on http://javarevisited.blogspot.com and http://java67.com
A humble place to learn Java and Programming better.
Written by
I am Java programmer, blogger, working on Java, J2EE, UNIX, FIX Protocol. I share Java tips on http://javarevisited.blogspot.com and http://java67.com
A humble place to learn Java and Programming better.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@retomeier/an-annotated-history-of-googles-cloud-platform-90b90f948920?source=search_post---------43,"Sign in
There are currently no responses for this story.
Be the first to respond.
Reto Meier
Feb 11, 2017·8 min read
See Also: What are the Google Cloud Platform services?
Google Cloud Platform (GCP) has been growing rapidly as Google invests heavily in it development. Sadly, the Wikipedia entry for GCP is garbage, and it’s difficult to skim the blog as a timeline. For my own sake, I wrote this attempt at an objective timeline of significant events and launches for GCP.
Something missing? Add a comment and I’ll update the timeline.
Developer Advocate @ Google, software engineer, and author of “Professional Android” series from Wrox. All opinions are my own.
120 
4
120 
120 
4
Developer Advocate @ Google, software engineer, and author of “Professional Android” series from Wrox. All opinions are my own.
"
https://medium.com/zrealm-ios-dev/%E4%BD%BF%E7%94%A8-python-google-cloud-platform-line-bot-%E8%87%AA%E5%8B%95%E5%9F%B7%E8%A1%8C%E4%BE%8B%E8%A1%8C%E7%91%A3%E4%BA%8B-70a1409b149a?source=search_post---------44,"There are currently no responses for this story.
Be the first to respond.
以簽到獎勵 APP 為例，打造每日自動簽到腳本
一直以來都有使用 Python 做小工具的習慣；有做正經的，工作上自動爬數據、產報表，也有不正經的，排程自動查想要的資訊或是交給腳本完成本來要手動執行的動作。
一直以來「自動」這件事，我都很粗暴直接開一台電腦掛著 Python 腳本讓他掛著跑；優點是簡單方便，缺點是要有台設備接著網路接著電；就算是樹莓派也是要消耗著微量的電費網路錢，還有也不能遠端控制啟動或關閉（其實可以，但很麻煩）；這次趁著工作空擋，研究了一下免費&上雲端的方法。
將 Python 腳本搬到雲端執行、定時自動執行、可透過網路開啟/關閉。
本篇以我耍的小聰明，針對簽到獎勵型 APP 撰寫的自動完成簽到的腳本為例，能每日自動幫我簽到，我不用在特別打開 APP 使用；並在執行完成後發通知給我。
之前有發過一篇「APP有用HTTPS傳輸，但資料還是被偷了。」的文章，道理類似，不過這次改用 Proxyman 取代 mitmproxy；同樣免費，但更好用。
「Certificate 」->「 Install Certificate On this Mac」->「Installed & Trusted」
電腦的 Root 憑證裝好後換手機的：
「Certificate 」->「 Install Certificate On iOS」->「Physical Devices…」
依照指示在手機上掛好 Proxy 並完成憑證安裝及啟用。
這時候 Mac 上的 Proxyman 就會出現嗅探到的流量，點擊裝置 IP 下想要查看的 APP API 網域；第一次查看需要先點「Enable only this domain」之後的流量才能被解包出來。
「Enable only this domain」後就能看到新攔截的流量就會出現原始的 Request、Response 資訊：
我們使用此方法嗅探 APP 上操作簽到時打了哪隻 API EndPoint 及帶了哪些資料，將這些資訊記錄下來，等下使用 Python 直接模擬請求。
⚠️要注意有的 APP token 資訊可能會換，導致日後 Python 模擬請求失效，還要多了解 APP token 交換的方式。
⚠️如果確定 Proxyman 有正常運作，但在掛 Proxyman 的情況下 APP 無法發出請求，代表 APP 可能有做 SSL Pining；目前無解，只能放棄。
⚠️APP 開發者想知道怎麼防範嗅探可參考之前的文章。
在撰寫 Python 腳本之前，我們可先使用 Postman 調試一下參數，觀察看看哪個參數是必要的或是有時效會改變；但要直接照搬也可以。
⚠️main(args) 這邊的 args 用途後面會講，如果要在本地測試直接帶 main(True) 就好。
使用 Requests 套件幫我們執行 HTTP Request，如果出現：
請先使用 pip install requests 安裝套件。
這部分我做的很簡單，僅共參考，僅通知自己。
下一步填好基本訊息後按「Create」送出建立。
有了 User Id 跟 Token 之後我們就能發訊息給自己了。因沒有要做其他功能所以連 python line sdk 都不用裝，直接打 http 發。
串上之前的 Python 腳本後…
測看看通知有沒有發成功：
Success!
小插曲，通知部分我本來是想用 Gmail SMTP 用信件來發，結果上到 Google Cloud 後發現無法使用…
前面基本的講完了，正式進入本篇重頭戲；將 Python 腳本搬上雲端。
這部分我一開始向中的是 Google Cloud Run 但用了下覺得太複雜，我實際懶得研究，因為我的需求太小用不到這麼多功能；所以我用的是 Google Cloud Function serverless 方案；實際上比較常用來做的是構建 serverless web 服務。
⚠️記下「觸發網址」
區域可選：
⚠️建立 Cloud Functions 時會需要 Cloud Storage 寄存程式碼。⚠️詳細計價方式請參考文末。
觸發條件選：HTTP
驗證：依需求，我希望我能從外部點連結執行腳本，所以選擇「允許未經驗證的叫用」；如果選擇需要驗證，後續 Scheduler 服務也要做相應設定。
變數、網路及進階設定可在變數中設定變數給 Python 使用（這樣參數有變動就不用改到 Python 程式碼）：
在 Python 中調用的方式：
其他設定都不需要動，直接「儲存」->「下一步」。
補充 main(args)，同前述，此項服務比較是用來做 serverless web；所以 args 實際是 Request 物件，你能從其中拿到 http get query 及 http post body 資料，具體方式如下：
example: ?name=zhgchgli => request_args = [“name”:”zhgchgli”]
example: name=zhgchgli => request_json = [“name”:”zhgchgli”]
如果使用 Postman 測試 POST 記得使用「Raw+JSON」POST 資料，否則不會有東西：
我們使用「request」這個套件幫我們打 API，此套件不在原生 Python 庫裡面；所以我們要在這裡加上去：
這邊指定版本 ≥ 2.25.1，也可不指定只輸入 requests 安裝最新版。
需要花約 1~3 分鐘的時間等他部署完成。
如果出現 500 Internal Server Error 則代表程式有錯，可點擊名稱進入查看「紀錄」，在其中找到原因：
測試沒問題就完成了！我們已經順利將 Python 腳本搬上雲端。
依照我們的需求，我們需要能有個地方存放、讀取簽到 APP 的 token；因為 token 可能會失效；需要重新要求並寫入共下次執行時使用。
想要從外部動態傳入變數到腳本中有以下方法：
在程式中使用相對路徑 ./ 就能讀取到，僅限讀取無法動態修改；要修改只能在控制台這修改＆重新部署。
想要可以讀取、動態修改就需要串接其他 GCP 服務，例如：Cloud SQL、Google Storage、Firebase Cloud Firestore…
按照入門步驟，建立好 Firebase 專案後；進入 Firebase 後台：
在左方選單列找到「Cloud Firestore」->「新增集合」
輸入集合 ID。
輸入資料內容。
一個集合可以有多個文件，每個文件可以有各自的欄位內容；使用上非常彈性。
在 Python 中使用：
請先到 GCP控制台 -> IAM與管理 -> 服務帳戶，按照以下步驟下載身份驗證私鑰文件：
首先選擇帳號：
下方「新增金鑰」->「建立新的金鑰」
選擇「JSON」下載檔案。
將此 JSON 檔案放到同 Python 的專案目錄下。
本地開發環境下：
安裝 firebase-admin 套件。
在 Cloud Functions 上要在 requirements.txt 中多加入 firebase-admin。
環境弄好後，可以來讀取我們剛剛新增的數據了：
如果是在 Cloud Functions 上除了可以把 身份驗證 JSON 檔一起上傳上去，也可以在使用時將連接語法改成以下使用：
如果出現 Failed to initialize a certificate credential. ，請檢查身份驗證 JSON 是否正確。
新增、刪除更多操作請參考官方文件。
有了腳本之後再來是要讓他自動執行才能達到我們的最終目標。
執行頻率：同 crontab 輸入方式，如果你對 crontab 語法不熟，可以直接使用crontab.guru 這個神器網站：
他能很直白的翻譯給你所設定的語法實際意思。（點 next 可查看下次執行時間）
這邊我設定 15 1 * * * ，因為簽到每天只需要執行一次，設在每日凌晨 1:15 執行。
網址部分：輸入前面記下的「觸發網址」
時區：輸入「台灣」，選擇台北標準時間
HTTP 方法：照前面 Python 程式碼我們用 Get 就好
如果前面有設「驗證」記得展開「SHOW MORE」進行驗證設定。
都填好後，按下「建立」。
⚠️請注意，執行結果「失敗」僅針對 web status code 是 400~500 或 python 程式有錯誤。
我們已達成將例行任務 Python 腳本上傳到雲端＆設定自動排成自動執行的目標。
還有一部分很重要，就是計價方式；Google Cloud、Linebot 都不是全免費服務，所以了解收費方式很重要；不然為了一個小小的腳本，付出太多的金錢那不如電腦開著掛著跑哩。
參考官方定價資訊，一個月 500 則內免費。
參考官方定價資訊，每月有 200 萬次叫用、400,000 GB/秒和 200,000 GHz/秒的運算時間、 5 GB 的網際網路輸出流量。
參考官方定價資訊，有 1 GB 大小容量、每月 10 GB 流量、每天 50,000 次讀取、20,000 次寫入/刪除；輕量使用很夠用了！
參考官方定價資訊，每個帳號有 3 項免費工作可設定。
對腳本來說以上免費用量就綽綽有餘啦！
東躲西躲，還是躲不掉可能被收費的服務。
Cloud Functions 建立好之後會自動建立兩個 Cloud Storage 實體：
如果剛剛 Cloud Functions 選擇的是 US-WEST1、US-CENTRAL1 或 US-EAST1 這三個地區則可享有免費使用額度：
我是選擇 US-CENTRAL1 沒錯，可以看到第一個 Cloud Storage 實體的地區是 US-CENTRAL1 沒錯，但第二個是寫 美國多個地區 ；我自已估計這項是會被收費的。
參考官方定價資訊，依照主機地區不同有不同的價格。
程式碼沒多大，估計應該就是每個月最低收費 0.0X0 元（？
⚠️以上資訊均為 2021/02/21 時撰寫時紀錄，實際以當前價格為主，僅共參考。
just in case…假設真的有狀況超出免費用量開始計價，我希望能收到通知；避免可能程式錯誤暴衝造成帳單金額報表卻渾然不知。。。
點擊「查看詳細扣款紀錄」進入。
下一步。
下一步。
動作這邊可以設定當預算達到多少百分比時會觸發通知。
勾選「透過電子郵件將快訊傳送給帳單管理員和使用者」，這樣當條件處發時就能第一時間收到通知。
點擊「完成」送出儲存。
當預算超過時我們就能馬上就能知道，避免產生更多費用。
人的精力是有限的，現今科技資訊洪流，每個平台每個服務都想要榨取我們有限的精力；如果能透過一些自動化腳本分擔我們的日常生活，聚沙成塔，讓我們省下更多精力專心在重要的事情之上！
有任何問題及指教歡迎與我聯絡。
有自動化相關優化需求也歡迎發案給我，謝謝。
「解決問題的道路上你並不孤單」整理開發上遇到問題的解決方法跟一些新鮮事。
283 
283 claps
283 
Written by
專職IT狗，寫了多年網頁，因緣際會踏入開發 iOS APP，求知若渴、教學相長；更愛電影、美劇、西音、運動、生活． www.zhgchg.li
「解決問題的道路上你並不孤單」整理開發上遇到問題的解決方法跟一些新鮮事。
Written by
專職IT狗，寫了多年網頁，因緣際會踏入開發 iOS APP，求知若渴、教學相長；更愛電影、美劇、西音、運動、生活． www.zhgchg.li
「解決問題的道路上你並不孤單」整理開發上遇到問題的解決方法跟一些新鮮事。
"
https://itnext.io/google-cloud-platform-and-flutter-mini-course-released-9193efebb8b0?source=search_post---------45,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Becoming a full-stack developer requires a hand in all layers of the application development stack. This includes Frontend, Backend, Database and Infrastructure.
Google Cloud Platform is a suite of cloud computing services used by Google and external business for running various kinds of software. This platform contains relevant services allowing us to be effective in building full-stack applications.
In this mini course we will build an Image Upload Mobile App using a couple of Dart packages primarily gcloud and googleapis_auth. These packages provide a set of helper classes for connecting to Google Cloud Platform services.
We will be using the Flutter SDK to build our mobile app since it provides a great set of features and tools for building cross-platform mobile applications. It also makes sense to be using Flutter as it’s also written in Dart 😉
We need to set up a Google Cloud Platform account. It’s free to set up and they’re currently offering $300 credit for 90 days. This will allow you enough time to use the Cloud Storage service for uploading images to.
The contents of the video are as follows:
Once you’ve set up a Google Cloud Platform account, create a Flutter project by running the following command:
Add the following packages under the dependencies key of your pubspec.yaml file:
Then run pub get to update your packages. Once you are setup, see the course for the full demonstration.
→ Watch full mini-course
If you enjoyed reading this post, please share this through your social media accounts. Subscribe to the YouTube channel for tutorials demonstrating how to build full-stack applications with Dart and Flutter.
Subscribe to the newsletter for my free 35-page Get started with Dart eBook and to be notified when new content is released.
Like, share and follow me 😍 for more content on Dart.
Originally published at creativebracket.com
ITNEXT is a platform for IT developers & software engineers…
197 
1
197 claps
197 
1
Written by
Christian | Web Developer | Egghead.io instructor | Sharing exclusive Dart content on my Blog → https://creativebracket.com | @creativ_bracket
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
Written by
Christian | Web Developer | Egghead.io instructor | Sharing exclusive Dart content on my Blog → https://creativebracket.com | @creativ_bracket
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://blog.doit-intl.com/predicting-gcp-monthly-bills-with-cloudml-f5b2f2cbff9d?source=search_post---------46,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
As some of you might already know, DoiT International is the engineering power behind reOptimize — Cost Discovery and Optimization SaaS for Google Cloud Platform.
With reOptimize you can get instant insights on your Google Cloud Platform billing, manage budgets, set up cost allocations and explore different cost optimization strategies.
One of the features we had from the day one is an estimation on how your monthly bill will look like at the end of the month. Here is how it looks like in reOptimize’s dashboard:
Initially, our estimation model used a very trivial linear regression — taking the current spending and the day of the month and extrapolating the value assuming spending will increase linearly. We did make some adjustments such as taking into consideration Google Compute Engine Sustained Discounts and few other things as well but still it was very naive and simplistic.
Unfortunately cloud services are cumulative, and in general, the cloud spending by itself doesn’t act linearly at all. There are seasonality patterns such as longer or shorter months, holidays seasons, marketing campaigns and so much more! The estimation given by this model usually is pretty far off. We calculated it to have a root-mean-square deviation (RMSE) of ~900.
Since many of our customers rely on reOptimize to forecast their Google Cloud Platform spendings, we started to work on Machine Learning based model to increase the accuracy of our estimations.
Naturally, we decided to use TensorFlow —an open-source software library for Machine Intelligence and specifically the Google CloudML which is Google’s managed service for TensorFlow so we would not need to take care of scaling, deployment, monitoring and other not too fascinating operational burden ;-)
There are three major parts to a our project:
All of these need to be automatically performed iteratively as new billing data arrives daily.
Google Cloud Billing API can export billing records into a variety of destinations and one of these is Google BigQuery. We are using BigQuery table as a source of data to train our prediction model.
The information in the table has the following columns:
To optimize the training process, we want to transform the raw data into a format better suitable for TensorFlow. This includes both the format and the semantics of the data.
TensorFlow supports multiple formats but the standard format is called TFRecords format. There are many built-in utilities to read and write data in this format.
The data semantics itself should be transformed into values best fitted for a DNN. The DNN or a Deep Neural Network — is a type of mathematical model that is used in machine learning and is built on the basis of the functionality of a human brain.
A DNN doesn’t work well with inputs that are not numbers and preferably numbers with real meaning. For example, product names and project names do not really mean anything on their own as well as year values and month values. On the other hand, the day numeric value has such meaning as well as the cost. Also feeding the raw cost rows would not be productive as we need to supply a target value per sample.
We are transforming the data from BiqQuery into something better suited for our DNN model.
The way we thought would be best to define our samples is to create a daily aggregation for a product and project and the sum of the costs from the beginning of the month up to that day. We also need to add the total cost for that product and project at this month and use that as the target value to train on. Finally, the product and project names must be first mapped into integers.
In our case, each sample would include the project, product, day, partial cost (from beginning of month) and total cost. In order for the DNN to be able to understand the data we want to augment the sample with additional data that mind prove useful but hard for the DNN to calculate on it’s own.
The number of days in the current month is an arbitrary constant that may affect the total cost. We can “calculate” this for the DNN, thus saving time and complexity. We can also calculate the ratio of cost per day by dividing the partial cost from the current day.
Another useful value would be the mean ratio and mean total cost for the product and project — these values would probably be used by a humans when trying to forecast the monthly bill. We can easily get BigQuery to build us a mapping table that provides these mean values per project and product and we used this table to create a static lookup table to enrich samples with.
Some other ratios were also calculated to provide more information for the DNN to use such as the estimated linear total cost (i.e. ratio * days in month)
Now that we have the data, we would feed the samples, one by one, along with the label value for the sample. We will train the model to predict the total cost from the sample input values. This way we could later use the model to feed the current spending and date per project and product and predict the total cost for the end of the month.
Now that we have our data ready, we wanted to have a way to evaluate our model against what we have today. We used the transformed data to calculate the simple linear regression performed by our code today. Each sample was calculated and then we subtracted the expected total from the prediction, squared the value and then calculated an average for this value for all samples. The square root of this value is the RMSE. We got an RMSE of ~900.
Looking at the data at this stage it was obvious that it didn’t behave in any linear fashion. Trying to create a DNN that would predict such data would be very hard — but we can help build a simpler DNN by feeding the network with some calculated values which we already know are relevant. To do that we would like to enrich the data as well as transform it.
One thing that looks helpful is to feed a value with the ratio between the current cost from start of month and the days since start of month. This value can also be averaged over each month and even globally to provide some more relevant info about spending behaviour.
And of course, it might help the DNN to know the history of total cost for previous month so we add the average total cost over each month and also a global average of the total cost.
This is the preprocessing function we built:
We then want to package it all together in TFRecord format ready to be trained. Luckily a package already exists that can help us to it. The python tensorflow-transform package uses a combination of Tensorflow and Apache Beam to process large amounts of data, transform them and write them into TFRecord files.
Another advantage of tensorflow-transform is that it creates a TensorFlow graph to process the data and runs it through a Beam pipeline — and that means you can use this graph later for the prediction — thus doing all these transformations on the data for you so you don’t need to transform the data prior to feeding it for prediction.
At the heart of the preprocessing script we run the Apache Beam pipeline:
The get_data_from_bq function reads the BigQuery data by using the SQL query shown above.
It then uses AnalyzeAndTransformDataset to perform the processing itself. The AnalyzeAndTransformDataset needs a pre-processing function to define the transformation graph. The transformed data is written using TFRecord formats into two file sets for training and testing.
In addition to the data, the pipeline also saves the metadata for the model, including the raw input metadata, the transformed data metadata and the transformation function graph. These are later used for training the model.
After the data is ready, the next step is to train a model with the data we have just prepared.
We decided to use a simple DNN and train it using the TensorFlow DNNRegressor class. The metadata created by tensorflow-transform in the pre-processing step is now used to create input functions for training, testing and serving.
Finally we use TensorFlow contrib.learn.Experiment to coordinate the training and evaluation.
To do this we define a function which generates the experiment:
And then use it to run the experiment:
The result of this process is a folder with the saved model which we can later deploy to Google CloudML to do actual predictions.
To allow reOptimize use the predictions, we obviously need to create some infrastructure that serves API requests coming from the application returning the output from the model.
Google CloudML does exactly that with TensorFlow Serving. TensorFlow Serving uses a trained model saved into a local disk or a GCS bucket to run the model and retrieve output. It exposes a RESTful API that can be used by any web or mobile client. Google CloudML takes it a step further by providing a serverless, fully managed serving infrastructure for your model so you don’t need to set up machines, autoscale them and so on..
After our model has been trained and saved, we are deploying it using the gcloud command line.
This will create a URL that can be used to predict using our model. The URL points to a fully managed backend API service created by Google CloudML.
It can both handle REST calls as well as be used by the gcloud command line tool like this:
The data/predict.json file contains information such as this:
And the results would be something like this:
By implementing machine learning to predict Google Cloud Platform bills, our forecast are now much more accurate. They are taking (indirectly) into account huge amount of signals such as seasonality, changes in pricing models, marketing campaigns and so on.
Our RMSE went down from ~900 to just ~100 so our predictions are much more credible and can be relied on by our customers.
By using Google CloudML, Dataflow and BigQuery, we could implement the machine learning infrastructure in reOptimize in just couple of weeks, while investing more time into the engineering rather than operations or maintenance.
Software & Operation Engineering. Written By Engineers.
253 
Your monthly dose of Google Cloud and Amazon Web Services articles  Take a look.
253 claps
253 
Written by
CTO of world’s best cloud consultancy
Here, our SRE and SWE teams share the solutions to some of the most interesting challenges we encounter during our daily work at DoiT International. If you like what we do, check out our https://careers.doit-intl.com page and join the team!
Written by
CTO of world’s best cloud consultancy
Here, our SRE and SWE teams share the solutions to some of the most interesting challenges we encounter during our daily work at DoiT International. If you like what we do, check out our https://careers.doit-intl.com page and join the team!
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://blog.appsecco.com/using-google-cloud-platform-to-store-and-query-1-4-billion-usernames-and-passwords-6cac572f5a29?source=search_post---------47,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Recently it came to our attention that there was a combined password dump which contained passwords cracked to plaintext.
The dump, said to be one of the largest, was 42 GB in size. That is a lot of usernames and passwords! Woah!
The username and password dump came conveniently sorted alphabetically and with simple scripts to query for email addresses. It also had scripts to count the total number of entries etc. On any decent laptop/virtual machine with an SSD, the query time is mere 4–5 seconds. But we wanted to do dig a bit deeper. We wanted to count things like:
Since the dump files were formatted in the format of username and password separated with a colon it seemed like a great opportunity to try and use Google BigQuery to query for these questions and more.
Google BigQuery enables super-fast SQL queries using the processing power of Google’s infrastructure.
It was a fairly compelling offer, whilst none of us at Appsecco are data scientists, we are definitely familiar with using SQL for querying databases.
We started at /r/netsec sub-reddit which gave us the magnet link for the torrent to this dump.
(Internally we now refer to this password dump as treasure trove!)
The post linked to a gist on Github with a magnet link to download the sorted list as a torrent (magnet link), the torrent didn’t start since it didn’t have any tracker information. But a redditor (Thank you CiNXNppjlK) posted the magnet link with tracker information.
Permalink to the comment for magnet link comment.
Since we planned to use Google BigQuery and wanted the data to be in Google Storage, we decided to download the torrent directly in a virtual machine on GCP. We are using a Debian Stretch available from the console.
We used aria2 for downloading magnet links on the command line.
aria2 is a complete bittorrent downloading solution. More information about aria2 https://aria2.github.io/
Since we planned to download at least 42 GB of data, it made sense to add an additional SSD to the machine.
We created a folder called torrents on the SSD
Starting a new bittorrent download using aria2 client with a magnet link is as simple as
The double quotes in BASH ensure that anything that needs to be escaped which is part of the link is escaped.
Since this was a torrent being downloaded (and we assumed it would take some time) we went out for a cup of coffee. By the time we came back 30 minutes later, the download was done!
A nice touch to every compute engine instance is that some of the command line tools required for working with other Google cloud services are already installed.
Before we could copy files to a Google Storage Bucket we needed to run the config command
Apart from some random output, we got a message saying
We copied the link given in message and got an authorization code
We knew it worked when we saw the following line of output
We created a bucket for our treasure trove
If you plan to do this as well you would ideally want to add some random hexadecimal string suffixed with a hyphen as well.
Optional, but looks cool kind of command
This is just a hangover of how we create AWS S3 buckets. You can chose to ignore this if you prefer your bucket names to be simple words
For any reason if you are unhappy with the bucket created, here is the command to remove it
Now we were ready to copy the data.
The gsutil utility had an option to parallelise copying of data by using the -m flag. Since we have to copy a lot of data, we used it. The entire copying got done in about 5 minutes or so.
If you plan to use this flag, if something does go wrong while copying you will not be able to use the retry feature. Therefore it makes sense to run this command inside tmux.
Now we just had to be inside the data directory
and execute the command
All this typing made us thirsty. That and the fact that we are copying about 42 GB of data, so time for another cup of coffee.
Now that we had the data in storage, we could if required, trash the compute engine instance so as to not incur any more charges.
So far all that we have done was to get ready for the main event. This is roughly what our plan was:
We started by creating a new dataset
Clicking on Create new dataset opened up a new dialog box
The new dataset name was visible and we clicked on the really small plus sign which opened up this
These were the options we decided to go with
Since we used a CSV file format, we needed to let BigQuery know about the following
With some trial and error we figured out that:
At this point we were ready to create the table. As soon as we clicked on create table, data started loading in a job.
Once the loading of data was successful we could see table details and that is a lot of data!
Compared to this the rockyou dump (an all time favourite for most security practitioners and penetration testers was about 32 million records in a 140 MB file
All this hard work would only be useful if we could get some interesting answers from the database which would be difficult scripting in BASH on the virtual machine where we downloaded the database in the beginning.
So it was time to start asking the query editor our questions in SQL
If you recall at the beginning of this post we listed down the kind of questions we wanted to answer
The answer is 15,511.
This doesn’t mean that these long passwords are sufficiently random but we can assume that not everyone of these 15000+ users would be typing these passwords by hand. So they may be using a password manager (clearly a subjective opinion at this point).
The answer is 119,340,157.
There are a whopping 119 million+ email addresses from Gmail in this dump!
The two queries required to get this to work
Result of this query need to be stored in a table named temptable
The main corporate domain that we were interested in was ours. For the query
The answer thankfully was zero.
The easy way was to allow different users access to the project where this particular BigQuery dataset is residing. But we saw that BigQuery had a nodejs client API.
We quickly wrote up a script to query for email addresses and uploaded it as a Google Cloud function. Since we are not developers, at this point we aren’t very keen to release the entire source code but snippets at this point.
This was a great learning experience for us. With breaches becoming so common and widespread, OSINT is already an integral part of our security assessment services.
While we were already searching password dumps for client email addresses to keep them secure, using services like BigQuery we can do this faster and reduce errors than do creep in with ad-hoc scripts.
If there are any gotchas in our approach or if you have a better one, please do let us know in the comments.
At Appsecco we provide advice, testing, training and insight around software and website security, especially anything that’s online, and its associated hosting infrastructure — Websites, e-commerce sites, online platforms, mobile technology, web-based services etc.
Making sense of application security for everyone.
171 
1
171 claps
171 
1
Written by
Author Burp Suite Essentials;Co-Founder+Director — http://appsecco.com , Community Manager @ null0x00; Ex-Chapter Lead OWASP BLR
Blog posts from the Security Testing Teams and DevSecOps Teams at Appsecco. Covering security around applications, Cloud environments like AWS, Azure, GCP, Kubernetes, Docker. Covering DevSecOps topics such as Secrets Management, Secure CI/CD Pipelines and more
Written by
Author Burp Suite Essentials;Co-Founder+Director — http://appsecco.com , Community Manager @ null0x00; Ex-Chapter Lead OWASP BLR
Blog posts from the Security Testing Teams and DevSecOps Teams at Appsecco. Covering security around applications, Cloud environments like AWS, Azure, GCP, Kubernetes, Docker. Covering DevSecOps topics such as Secrets Management, Secure CI/CD Pipelines and more
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://netflixtechblog.com/netflix-security-monkey-on-google-cloud-platform-gcp-f221604c0cc7?source=search_post---------48,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Today we are happy to announce that Netflix Security Monkey has BETA support for tracking Google Cloud Platform (GCP) services. Initially we are providing support for the following GCP services:
This work was performed by a few incredible Googlers with the mission to take open source projects and add support for Google’s cloud offerings. Thank you for the commits!
GCP support is available in the develop branch and will be included in release 0.9.0. This work helps to fulfill Security Monkey’s mission as the single place to go to monitor your entire deployment.
To get started with Security Monkey on GCP, check out the documentation.
See Rae Wang, Product Manager on GCP, highlight Security Monkey in her talk, “Gaining full control over your organization’s cloud resources (Google Cloud Next ‘17)”:
We released Security Monkey in June 2014 as an open source tool to monitor Amazon Web Services (AWS) changes and alert on potential security problems. In 2014 it was monitoring 11 AWS services and shipped with about two dozen security checks. Now the tool monitors 45 AWS services, 4 GCP services, and ships with about 130 security checks.
We plan to continue decomposing Security Monkey into smaller, more maintainable, and reusable modules. We also plan to use new event driven triggers so that Security Monkey will recognize updates much more quickly. With Custom Alerters, Security Monkey will transform from a purely monitoring tool to one that will allow for active response.
More Modular:
Event Driven:
Custom Alerters:
We’ll be following up with a future blog post to discuss these changes in more detail. In the meantime, check out Security Monkey on GitHub, join the community of users, and jump into conversation in our Gitter room if you have questions or comments.
We appreciate the great community support and contributions for Security Monkey and want to specially thank:
By: Patrick Kelley and Mike Grima
Learn about Netflix’s world class engineering efforts…
154 
154 claps
154 
Written by
Learn more about how Netflix designs, builds, and operates our systems and engineering organizations
Learn about Netflix’s world class engineering efforts, company culture, product developments and more.
Written by
Learn more about how Netflix designs, builds, and operates our systems and engineering organizations
Learn about Netflix’s world class engineering efforts, company culture, product developments and more.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://towardsdatascience.com/hyperparameter-tuning-on-google-cloud-platform-with-scikit-learn-7d6155195efb?source=search_post---------49,"Sign in
There are currently no responses for this story.
Be the first to respond.
Rob Salgado
May 20, 2019·10 min read
Google Cloud Platform’s AI Platform (formerly ML Engine) offers a hyperparameter tuning service for your models. Why should you take the extra time and effort to learn how to use it instead of just running the code you already have on a virtual machine? Are the benefits worth the extra time and effort?
One reason to use it is that AI Platform offers Bayesian Optimization out of the box. It is more efficient than a grid search and will be more accurate than a random search in the long run. They also offer early stopping…
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/hackernoon/snaps-2-billion-gcp-purchase-is-strategic-e6ce3ef5f5e1?source=search_post---------50,"There are currently no responses for this story.
Be the first to respond.
Top highlight
Snap’s announcement that it will be using the Google Cloud Platform (GCP) for its infrastructure, to the tune of $2 billion over the next five years, drew much attention from the tech community.
It even made it to the front page of Hacker News TWICE with two different stories in the same day!
A day later, still sits in the Top 10.
Most of the attention has been paid to the buy vs. build argument. Always a question for companies of any size. Should Snap build out its own infrastructure or outsource it to a big provider? And $2B? Seems like a lot.
But it’s also about alliances, even more than the tech.
Given Snap’s computing and bandwidth needs and potential for future growth, there would really only be three options, if you take the view that you HAVE to go with one of the big boys: Amazon, Google, or Microsoft.
But, Google is building platforms. In a way not seen since Microsoft gained domination of PCs in the 1980s and 90s. Yes, Apple owns a huge client footprint, but G is poised to move on the corporate world much better than Apple.
Snap’s decision to go with Google makes much more sense as a strategic business decision than just deciding on an infrastructure provider.
Take into consideration:
Google is having tremendous success on the client side. Not just Android (1.5–2 billion users), but Chrome (> 1 billion users). Google is integrating its client and server side technologies.
Look at JavaScript technology Web push. This is a push notification technology, and in Chrome, MUST be used with Google Messaging service. How much longer until Chrome natively supports saves to Firebase?
Google is linking their client and server offerings. In short, AWS is a powerhouse, but among AWS and Microsoft, Google is the real force in modern computing. They set standards and own consumer platforms. Amazon does not.
So, go from the infra-arch perspective to the business management perspective. Who would you rather align with? Who would you want to give a *stake* in your success? Snap has the mound of cash right now.
In one move, Snap becomes one of GCP’s biggest customers. G has a stake in Snap’s success. Alliances.
GCP will be massive in the years to come. If you work in IT, look at how much infra is still maintained in-house.
As far as the buy vs build — I would argue —
IT staffs like running in-house servers, network, and managementCIOs HATE running in-house servers, network, and management
Snap’s decision makes more sense from the suits’ perspective than the techs’.
Hacker Noon is how hackers start their afternoons. We’re a part of the @AMI family. We are now accepting submissions and happy to discuss advertising & sponsorship opportunities.
If you enjoyed this story, we recommend reading our latest tech stories and trending tech stories. Until next time, don’t take the realities of the world for granted!
#BlackLivesMatter
32 
how hackers start their afternoons. the real shit is on hackernoon.com. Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
32 claps
32 
Written by
Opinions are my own and not those of Google, Inc.
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
Opinions are my own and not those of Google, Inc.
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/hackernoon/deploy-react-application-using-docker-and-google-cloud-platform-4bc03f9ee1f?source=search_post---------51,"There are currently no responses for this story.
Be the first to respond.
In this article, you will learn how to deploy applications on GCP. We will deploy a create-react-app.
Link to the Repo — https://github.com/Harshmakadia/react-docker
Before we get started with the actual steps of deploying the React App using GCP and Docker. First, let’s understand what docker actual is?
Docker is a tool which is designed to make the creating, deploying and running of applications easier with the help of containers. Containers are something which allows the developer to bundle the application with all the necessary ingredients like different libraries, dependencies and ship is as only a single package.
We will go step by step
Create react app is a lot easier using the create-react-app (CRA)
We will use create-react-app package to install and configure simple react application from NPM, Open your terminal and install react app.
For more info on creating a react app
github.com
once you run the application using command$ npm start
After that, it’s time to create a build the app, run$ npm run build
creating a docker file is a cup of your tea. The is no rocket science in creating a docker file.
Just Create a new file with name DockerfileNow once the file is created we will add some command to that which will help us to create, run, deploy the application.
Here the content of Dockerfile for react app. Note I’m using Nginx to as a server.
Once the docker file is created. I’m creating a new folder named deployment within the app directory which has a nginx.conf file
The content of nginx file, note that this is default configuration you may not need to alter this file unless you have some special requirements.
Head to this link below and download it for your respective operating system
www.docker.com
Once it is installed run open your terminal and run below command to check it is installed successfully
docker — — version
Now that we have the docker setup on our machine it’s time to create the first image using the following command
docker build -t first-docker .
more info about different command can be found here.
Once you run this command it will execute all the command listed down in the Dockerfile.
we have successfully created the image. Let’s proceed to next step
Download SDK from the below link and setup on your machine
cloud.google.com
Now that we have the gcloud SDK setup on our machine
Next step is to create a new project in the GCP where we will be pushing our docker images to the containers.
Configure Docker to use gcloud as a credential helper or are using another authentication method. To use gcloud as the credential helper, run the command:
It’s time to push the image to the registry
Push the tagged image to Container Registry by using the command:
This command pushes the image that has the tag latest. If you want to push an image that has a different tag, use the command:
When you push an image to a registry with a new hostname, Container Registry creates a storage bucket in the specifiedmulti-regional location. After pushing your image, you can:
Navigate the GCP console and search of Container Registry, you will be able to see the image which we push.
It’s time to create cluster now inside Kubernetes Engine in GCP
Create deployment under workloads
Select the image which you want to deploy and finally click on Expose to expose the deployment.
Set port to 80 in Target Port since we had EXPOSED our application to 80 in the Dockerfile
Once you have exposed the port you will get IP Adress where your react application will be running live.
And that it’s you are all set with docker. Whenever you want to push new image to the container first build the image with above-specified commands and then push the image to container registry and finally make that image live by going to rolling update option.
Please note down your questions in the comment section below if you have any doubts & I’ll be happy to address them.
That’s the end 🔚 I hope you have learned something new. Happy Learning! 💻 😃
#BlackLivesMatter
226 
3
how hackers start their afternoons. the real shit is on hackernoon.com. Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
226 claps
226 
3
Written by
I talk about JavaScript, Web Development, No-code to help you stay ultra-modern. See you on Twitter — https://twitter.com/MakadiaHarsh
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
I talk about JavaScript, Web Development, No-code to help you stay ultra-modern. See you on Twitter — https://twitter.com/MakadiaHarsh
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/aws-to-google-cloud-platform-service-mapping-5a1689d41f01?source=search_post---------52,"There are currently no responses for this story.
Be the first to respond.
UPDATE 9/1/2016: We now have a very comprehensive mapping that goes far beyond this post. You can read it here.
As a Developer Advocate on the Google Cloud Platform team, I am frequently asked what services we provide. If the person I’m talking to is familiar with Amazon Web Services (AWS), the quickest way to jump start an explanation of Google Cloud Platform is to start with a comparison to AWS’s similar services, then cover the differences.
Below is a simple map between some of the major services in AWS and Google Cloud Platform. This is not intended to be a complete mapping. It would be unfair to both platforms to list every service because Google and Amazon are taking different approaches in many areas, making direct comparisons practically impossible. I’m only listing the services where the comparison is helpful.
EC2→ Compute EngineEC2 Container Service → Container EngineElastic Beanstalk → App Engine **
S3 → Cloud StorageGlacier → Cloud Storage NearlineCloudFront → Cloud Storage (edge caching is provided for public buckets)
RDS → Cloud SQLDynamoDB → Cloud Datastore and Cloud Bigtable
Redshift → BigQuerySQS/Kinesis → Cloud Pub/SubEMR → Cloud Dataflow
CloudWatch → Cloud Monitoring and Cloud Logging
Route53 → Cloud DNS and Google DomainsDirect Connect → Cloud Interconnect
CloudFormation → Cloud Deployment ManagerSES → SendGrid (partner)WorkMail → Gmail (also see Google for Work)WorkDocs → Google Docs (also see Google for Work)
** AWS Elastic Beanstalk and Google App Engine are often described as similar offerings, but there are significant differences in their approaches. Both offer auto-scaling, load balancing, monitoring, etc., but unlike App Engine, Elastic Beanstalk requires the typical system administration that raw VMs require (OS updates, etc.). Google App Engine is a PaaS, meaning that it’s fully managed, so all of these administrative tasks are handled by Google. The basic App Engine setup includes built-in services such as Task Queues, Memcache, Users API, and more.
If you require unmanaged VMs, Google also has auto-scaling, load balancing, and monitoring of unmanaged VMs as features of Google Compute Engine. There is also an alternative hosting model now available as part of Google App Engine called Managed VMs.
My advice is to do your homework and understand these models thoroughly before diving in on either platform. Each has unique advantages.
I’ll have more posts in the near future with more specifics on several of the offerings. Stay tuned!
Originally published at gregsramblings.com on May 12, 2015.
Google Cloud community articles and blogs
65 
1
65 claps
65 
1
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Google Developer Relations — Living in San Francisco https://gregsramblings.com | https://cloud.google.com | http://instagram.com/gregsimages
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://betterprogramming.pub/how-to-save-money-on-google-cloud-platform-22bf4c302d32?source=search_post---------53,"Sign in
What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Gaurav Agarwal
Jul 3, 2020·5 min read
Google Cloud Platform is one of the fastest-growing cloud platforms, maintaining a steady position at a number three, behind AWS and Azure. It prides itself for its network quality and the edge on data science and engineering.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/free-code-camp/decentralize-your-application-with-google-cloud-platform-7149ec6d0255?source=search_post---------54,"There are currently no responses for this story.
Be the first to respond.
When first starting a new software project, you normally choose a certain programming language, a specific framework and libraries. Then you begin coding. After 2 - 3 months you end up with a nicely working single application.
But, as the project grows and more functionalities are added, you quickly realize the disadvantages of a centralized system. Difficult to maintain and unscalable are some of the reasons which will make you search for a better solution. Here is where Microservices come in help.
Microservices are independently built systems, each running in their own process and often communicating with REST API. Representing different parts of your application, they are separately deployable and each part can be written in any language.
You can easily see how, by dealing with the problems of a monolithic system, Microservices have become a requirement for any state-of-the-art software.
I strongly recommend reading Microservices (by James Lewis) and On Monoliths and Microservices if you want to understand more in depth what are the key concepts in this architectural style.
This article will walk you through the process of implementing a Microservice using Google Cloud Platform.
Imagine you’re developing an application that accepts a text input from a user and determine the category of the key words within the input.
We’ll use an example to illustrate the functionality of the App. Consider the sample text below from the GCP Cloud Natural Language API website:
“Google, headquartered in Mountain View, unveiled the new Android phone at the Consumer Electronic Show. Sundar Pichai said in his keynote that users love their new Android phones.”
Our web App would accept the text above as input, and return the category that the key words belong to, as in the figure below:
This feature is quite likeable and people use it hundreds of times each day. Now, if you’re going to offer this functionality as a service that receives a high amount of daily traffic, you want to respond with a stable and reliable system.
That’s why we’ll build a lightweight Flask App, hosted on Google App Engine. Integrating it with Google Cloud Pub/Sub will help us handle all the asynchronous requests we receive and help us assure that users don’t wait too long for a response.
Let’s first start with the Flask app (you can also choose Django, Node.js, Go or anything used to build server-side applications). If you’re not very familiar with developing a Flask App, this Flask Series can show you step-by-step how to set up an application.
For the purpose of this tutorial we will use this simple example:
First you need to install the dependencies pip install Flask gunicorn. You will be using gunicorn to run the application on Google App Engine. For local access you can run python text.py in the console and find the app on port 8080.
To deploy the app to Google App Engine, you need to take these steps:
The app.yaml file looks like this:
Line 3 is important, where you use gunicorn to tell Google App Engine to run the application app from a file called text.py (the Flask app). You can learn more about the .yaml file structure here. After deployment you should be able to access your project from https://[YOUR_PROJECT_ID].appspot.com.
When building production ready applications, you often want to test your code before pushing it live. One way to do this is to run your App within a server locally. A better approach is to have a development version of the app which can be tested not only from your local machine but also from a hosted environment. You can use Google App Engine versions for this.
Just deploy your App with gcloud app deploy -v textdev (for development) or gcloud app deploy -v textprod (for production).
Then navigate to https://textdev.[YOUR_PROJECT_ID].appspot.com or https://textprod.[YOUR_PROJECT_ID].appspot.com to access the specific version.
So far so good. You have a working application, hosted on the Google Cloud Platform. Now you need to add Google Cloud Pub/Sub and Google Natural Language API.
But first, let’s explain the architecture.
Once a request is received, the Flask app will publish a message with the text to a topic (created below). Then a subscriber (Python script) will pull this message and apply the Google Natural Language API to each text. Finally, the result will be saved to a database.
For multiple requests, the app asynchronously publishes them to the topic and the subscriber starts executing the first one. When ready, it picks the second one and so on.
Now you need to modify text.py file:
The code on line 15 and 16 creates the publisher. On line 18 it publishes a message containing the user email and text input.
You only need to fill in the project_id and topic_id (line 6 and 7).
Since the project_id was used earlier, just add it here.
For the topic_id you need to do the following:
Wonderful! Now you have a working publisher.
Let’s jump into setting up the subscriber. There are two files that need to be created: worker.py and startup-script.sh.
The worker.py looks like this:
The file is slightly larger but we will examine it step-by-step, starting from the bottom.
When the file is executed, the code on line 44 runs main(). This function sets the subscriber with your project_id and subscription_id and assigns a callback to it.
The callback (initialized on line 7) is going to receive all messages and perform the required task (to determine the category of a text). If you follow the code from the callback, you can easily see how the Google Natural Language API is being used.
The interesting line is 11 where message.ack() acknowledges the current message. You can see this is as if the worker is saying: “I am done with this message and ready to handle the next one”.
Now, you need to implement startup-script.sh.
This is a shell script with several commands:
Before explaining the code above, I need to clarify the process.
Basically, Google Cloud Compute Engine gives you the ability to scale an application by providing as many virtual machines (VM) as needed to run several workers simultaneously.
You just need to add the code for the worker, which you already have, and set the configurations of the VM. Together with the worker.py, you also need to add a startup-script.sh which will run every time a new VM boots up.
New VM instances are booted up to prevent delay in responses when a high number of messages is received.
For a deeper and more technical explanation of this process check out the documentation.
Now, let me walk you through the script:
Now you need to upload worker.py and startup-script.sh to your storage and set up the VM. To upload the files just go here and create a new bucket with the same name as your project id. Create a folder called workers and upload the scripts inside. Make sure to change the worker.py to a ‘Public link’ and edit the permissions of the startup-script.sh to have your service account as an owner.
The final step is to set up the configurations of the VM and test the system. Just follow the ‘Create an instance template’ instructions from the documentation and you are good to go!
Once the VM boots up, you can try sending requests to your application and examine how it reacts by checking the logs.
Going through Google’s documentation may help you a lot. Also check out this tutorial - you may find it useful while implementing some of the steps above.
I want to express my gratefulness to Logan Allen for helping me better understand this process. I hope you find it useful.
Leave any questions or suggestions in the comment section.
We’ve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
286 
3
286 claps
286 
3
We’ve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
Written by
Obsessed with creating a positive impact. Love blogging about AI and reading books. For more content, follow me 👉 https://www.linkedin.com/in/simeonkostadinov/
We’ve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
"
https://medium.com/google-cloud/simple-google-api-auth-samples-for-service-accounts-installed-application-and-appengine-da30ee4648?source=search_post---------55,"There are currently no responses for this story.
Be the first to respond.
I’ll be adding updates to my new blog here: https://salrashid123.github.io/
This article describes the various mechanisms to access GCP Services using our APIs. I find it pretty confusing to keep track of all the various ways to access a service and that coupled with the changes in the library set accross languages, i often lose trac….so I’ve kept this repo as a running reference. Hope you find some of the samples useful.
This article will only describe the libraries in general terms but point back to a gitRepo for all the code samples. What i’ll describe is the following:
Revisions:
github.com
As an introduction, here is a quick summary of the various libraries:
The samples use Application Default Credentials which uses credentials in the following order as described in the link. Set the environment variable to override.
You can always specify the target source to acquire credentials by using intent specific targets such as: ComputeCredentials, UserCredentials or ServiceAccountCredential.
There are two types of client libraries you can use to connect to Google APIs:
The basic differences is the Cloud Client libraries are idomatic, has gcloud-based emulators and much eaiser to use.
It is recommended to use Cloud Client Libraries whereever possible. Although this article primarily describes the API Client libraries, the python code section describes uses of Cloud Client libraries with Google Cloud Storage.
For more information, see
This article also describes how to use IAM’s serviceAccountActor role to issue access_tokens, id_tokens and JWT. For more information on that, see Using serviceAccountActor for impersonation.
The following examples use the Oauth2 service to demonstrate the initialized client using Google API Client Libraries. The first section is about the different client libraries you can use.
Cloud Client Libraries and API Client Libraries
→ Cloud Client Libraries
→ API Client Libraries
→ serviceAccountActor role for impersonation
For more information, see:
This code samples contained in this article can be found on my github repo.
As described in the introduction, this section details the two types of libraries you can use to access Google Services:
These libraries are idiomatic, easy to use and even support the gcloud-based emulator framework. This is the recommended library set to use to access Google Cloud APIs.
For more information, see:
The following example describes various ways to initialize a service account to list the Google Cloud Storage buckets the account has access to. It also shows listing the buckets using the default account currently initialized by gcloud.
To use the mechanisms here, you need to initialize gcloud’s application defaults:
See:
medium.com
Conclusion
Navigating GCP libraries is pretty confusing at the moment. I hope the git repos cited here help atleast bootstrap. If you find this useful, +1 this article…or better yet, if you find something i’ve missed in the git repo or should add, please let me know or file a PR!. thanks
Google Cloud community articles and blogs
81 
2
81 claps
81 
2
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@wkrzywiec/how-to-publish-a-spring-boot-app-with-a-database-on-the-google-cloud-platform-614b88613ce3?source=search_post---------56,"Sign in
There are currently no responses for this story.
Be the first to respond.
Wojciech Krzywiec
Oct 15, 2018·9 min read
I’ve found myself in such situation many times. I was done with my web service project and I wanted to share it with my friend, but my only options were either send a link to my GitHub repository (to deploy it on a local machine), or I bring my laptop to her/him. Not so convenient, right? In this blog post I’ll try to fix it, so everyone could check my awesome apps over the Internet. And for that I’ll use the Google Cloud Platform.
As already mentioned I’ve recently worked on my first Spring Boot RESTful web service Library API. It’s a simple app that process few HTTP requests to access/modify resources that are in MySQL database. It works fine on a local machine, but I want to move into next level to publish it on the Google Cloud Platform.
If you already has a Google account you might be familiar with Google Drive service which offers “free” storage space for your files and it can be accessible from any part of the planet. It works, as we call it Software-as-a-Service (SaaS), which means that Google hosts Google Drive application on their servers and allows users to access it from the Internet.
Other delivery model, in cloud computing world, is called Platform-as-a-Service (PaaS), and on the contrary to SaaS the provider gives the infrastructure to the customers, so they can deploy and run their application in the Cloud. Google Cloud Platform (GCP), along with Amazon Web Services (AWS) and Microsoft Azure, provides couple services that could be assigned to this category.
One of them is Cloud SQL which is used for MySQL and PostgreSQL database management. Which provides not only the storage, but also other features like backup.
Another one is Google App Engine (GAE). It provides an easy way to deploy an app that is written in languages like Java, Python, C#, PHP, Node.js, Ruby and Go. It also supports Docker images deployment.
With Google App Engine we don’t need to worry about:
If you want to read more about Cloud SQL and App Engine capabilities check their official documentation here and here, respectively. Except explaining how they works they also guide how to make a use of each feature with step-by-step approach.
Before I jump into next section, make sure that you have go thru all these steps:
Once we are done with basic set up we can add MySQL instance to the project. Therefore go to you project dashboard and select SQL from Navigation menu (top left corner).
You should get a page with a single window. Click on Create instance button and then choose MySQL as database engine. On a next page select MySQL Second Generation (my database is MySQL 5.7, which is supported only by this generation).
Finally we need to configure the database. My application won’t be handling big traffic on run, so I decided to pick the lowest set up I can get (pricing for such are much lower).
Below there are screenshots of my config. To see the whole configuration options click on Show configuration options. I’ve kept almost all properties as defaults, except for Machine type and storage and Backup feature.
After couple of minutes the Cloud SQL will be up and running.
Next, enter SQL instance dashboard, which can be picked from the list of instances. Then, move to Users tab and click Create user account button, where you can provide username and password.
Next go to Databases and click Create database button. In the pop up provide database name.
My project has Flyway implemented which will create all necessary tables during app deployment, so I don’t need to run any script at this point. But if you would like to run them manually, here are the instructions.
The database is now set up. The only thing that we’ll need from this point is the Instance Connection Name, which can be read from the SQL Instance Dashboard, and is required to establish connection between app and database.
The base project is a Spring Boot app which cannot be simply packed into WAR file and copy-paste into Google App Engine. It requires some modifications, but luckily not so much.
First of all, GAE service uses Jetty webserver/servlet container, but Spring Boot per default uses Tomcat. Therefore we need to update build.gradle file.
With the first line we tell Gradle that we want to exclude the embedded Tomcat dependencies (they will conflict with Jetty), in second we explicitly add Java Servlet dependencies.
Last thing to do is to remove JUL to SLF4J bridge that interferes with App Engine’s log handler that’s provided through Jetty server.
The app is now running on Jetty server, so we can move on to add Google Cloud dependencies. Below code snippet presents all required plugins and dependencies that needs to be added to Gradle build file. They enable Gradle tasks for GAE deployment and add dependencies that allows internal connection between the GAE app and Cloud SQL. More information about App Engine Gradle tasks can be found here.
Next we need to add app.yaml configuration file into the project (into src/main/appengine directory). It contains information about the URL, destination GAE service where it will be deployed or general settings for scaling. In my app it looks like as follows:
Note. Most of the tutorials over the Internet doesn’t include lines with resource settings. I’m doing it because I’ve faced some difficulties during start up when they have not been provided. But if you work on your own project you probably won’t need that.
Last thing is to create a class that extends SpringBootServletInitializer class. It is required for traditional WAR file deployment and which is not generated automatically by Spring Boot Initializr. I’ve decided to not create a new class but to extend the main.
Project is set up, so the only thing to do is to deploy it on GAE. To do that we need to first authenticate on ourselves with the Google Cloud Platform using Google Cloud SDK Shell (installed on a locally) and typing following command:
Above command will trigger web-based authentication process, after which you will be able to access GCP from the command line and be able to deploy an app.
As a GCP user you can have multiple projects within it so there is last thing to — we need to explicitly decide into which project we want to deploy a software. Try this:
Second command prints the default project to operate on.
Finally we can run the appengineDeploy Gradle task (from your IDE). You’ll need to wait several minutes but after that your app will be successfully deployed 😄.
So you think that all of these came up really quick for me? No at all ;)
It was first time when I was playing around Google Cloud and I must say that for a first project it was quite challenging.
At the beginning I thought that the whole transition, from local Spring Boot app to Google Cloud deployed one will go really smooth and will be done in a day or two. But nothing goes as it was planned as it should be (but when it does?😏).
The problems were that even though appengineDeploy Gradle task says that build has been successful but it doesn’t run. There could be several reasons for that, but here are the problems I’ve stomp
As usual here is the link to the entire project on GitHub.
github.com
cloud.google.com
cloud.google.com
github.com
github.com
github.com
cloud.google.com
codelabs.developers.google.com
github.com
cloud.google.com
cloud.google.com
dzone.com
docs.spring.io
stackoverflow.com
Java Software Developer, DevOps newbie, constant learner, podcast enthusiast.
See all (109)
151 
4
Subscribe to my feed to get notification for new publications

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
151 claps
151 
4
Java Software Developer, DevOps newbie, constant learner, podcast enthusiast.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@gmusumeci/how-to-create-a-service-account-for-terraform-in-gcp-google-cloud-platform-f75a0cf918d1?source=search_post---------57,"Sign in
There are currently no responses for this story.
Be the first to respond.
Guillermo Musumeci
May 24, 2020·7 min read
Before we start deploying our Terraform code for GCP (Google Cloud Platform), we will need to create and configure a Service Account in the Google Console.
In this example, we will create a master Service Account with permissions at Organization-level and Project-level.
"
https://towardsdatascience.com/how-to-train-and-deploy-a-vaex-model-pipeline-on-google-cloud-platform-d5023ef46322?source=search_post---------58,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jovan Veljanoski
Mar 23, 2021·17 min read
Training a machine learning (ML) model is often a rather lengthy and computationally intensive task, especially when using larger quantities of data.
Regardless of whether you are using your favorite deep learning framework, trusted gradient boosting machine or a custom ensemble, the model training phase can easily consume most if not all available resources on your laptop or local server, effectively “freezing” your machine and preventing you to do other tasks. Or you may not even have enough resources at hand to train a sophisticated model on all of the data you have so painstakingly gathered, or use all of the features you have so thoughtfully engineered.
On top of that, curating ML models is a continuous process: one will likely need to re-train the model relatively often to account for new data, concept drift, changes in the domain, or simply to improve the model by adjusting the input features, architecture or hyper-parameters.
Therefore, it can be rather convenient, if not necessary, to outsource the heavy lifting stage of ML model creation to a service managed by a recognized cloud provider. With this approach computational resources will not be a problem. Nowadays, one can “rent” computing instances having few hundreds of vCPUs and several Terabytes of RAM attached, or provision custom cluster configurations. In addition, one can submit multiple training jobs which will run independently and not compete with each other for resources. All this means that you can spend more of your time on creating the best model possible, and virtually no time managing, maintaining and configuring your computational resources. An important note is that such services are becoming cheaper are more accessible with time.
So how does Vaex fit in all of this? There are several substantial benefits to using Vaex as the core technology for building ML solutions even in cloud environments. To start with, you can host your data on a Google Cloud Storage (GCS) or an Amazon Web Services (AWS) S3 bucket, and stream it lazily to your compute instance, on a “need-to-have” basis. This means that only the specific columns that your model needs will be downloaded, and not all of the files in their entirety. One can even choose to download only a fraction of the data, which is especially useful for testing and continuous integration.
All Vaex transformations are done by fully parallelized, efficient out-of-core algorithms. That means that you always take full advantage of the compute instance you rent out, without any additional set up. The no-memory-copy policy makes it easier to choose the type of machine you require while minimizing cost without performance trade-off.
There are a couple of more benefits that will be highlighted throughout the article. So, without further ado, let us see how one can use Vaex to build a ML solution, and how to then use GCP to make it come to life.
This article assumes some basic knowledge of GCP and how to interact with it through the Google Cloud Console, and via the gcloud command-line tool. If you would like to follow along with this tutorial, you will need an authenticated Google account, a GCP project, and a GCS bucket already set up. There are many useful guides in case you are not sure how to do this. If in doubt, the official GCP documentation is always a good place to start.
All the materials from this article are fully available here, along with a variety of other Vaex examples.
This example uses the public Heterogeneity Activity Recognition (HAR) dataset. It contains several sets of measurements captured from a group of volunteers performing one of six activities: walking, going up & down stairs, sitting, standing and biking. The measurements are captured via popular smart-phone and smart-watch devices, and comprise the triaxial acceleration and angular velocity sampled from the on-board accelerometer and gyroscope respectively. The dataset contains also the “Creation_Time” and “Arrival_Time” columns, which are the time-stamps attached to each measurement sample by the OS and mobile application respectively. The goal is to detect the particular activity performed by the wearer using just a single measurement sample. The activity itself is specified in the “gt” column which stands for “ground truth”.
The following example uses the accelerometer data obtained via the smart-phone devices. It comprises just over 13 million samples. For brevity, we will not present any exploratory analysis of the data, but will jump straight into building a production ready solution.
Let us start by creating a Python script that will fetch the data, engineer relevant features, and train and validate a model. Since we are using Vaex, fetching the data is trivial. Provided the data is in the HDF5 file format and hosted on GCS (or Amazon’s S3), Vaex will lazily stream the portions that are needed for analysis. Since we already know which columns of the data are going to be needed, they can be pre-fetched right away:
The next step is to randomly partition the data into 3 sets: training, validation and testing. The validation set will be used as a quality control during the training phase, while the test set will be the final, independent performance indicator on the already trained model.
At this point we can start to create some useful features. Let us begin by making a couple of coordinate transformations. We will convert the triaxial acceleration measurements from Cartesian to spherical coordinates, as well as to their “natural” coordinate system by the means of a PCA transformation:
Even though some of the above transformations are not overly complex, we can still choose to accelerate them by using just-in-time compilation via numba. Note that we are also using the new API available in vaex-ml version 0.11, instead of the more traditional scikit-learn “fit & transform” approach.
To capture some non-linearity in the data, we can create some feature interactions between the PCA components:
Now, we will get a bit more creative. First, let us calculate the mean and standard deviation for each Principal component per activity class. Then, we will calculate the difference between the value of each Principal component and the mean of each group, scaled by the standard deviation of that group:
Notice how we are combining the usage of both Vaex and Pandas in creating these features. While the df_summary DataFrame will not be stored, its values are “remembered” as part of the expressions defined in the for loop that follows the groupby aggregation. The above code-block is an example of how one can very quickly and clearly create new features, without the otherwise necessary exercise of creating a custom Transformer class.
Another interesting approach to feature engineering is to apply a clustering algorithm on subsets of already defined features, and use the resulting cluster labels as additional features. The vaex-ml package directly implements the KMeans clustering algorithm, so it is guaranteed to very fast and memory efficient. Using the KMeans algorithm, we create 3 sets of cluster labels: one by clustering the PCA components, two by clustering PCA interaction components:
In Vaex any model is treated as a transformer, so it’s outputs are readily available to be used as any other features in the downstream computational graph.
Finally, we can also make use of the time-stamp features, and calculate the difference between the “Arrival_Time” and “Creation_Time” columns, to which we apply standard scaling:
Once we are done defining all the features, we can gather them into a single list for convenience:
The last part of the data preparation is to encode the target column “gt” into a numeric format. Following the encoding, we will also define a inverse mapping dictionary, which we will later use to translate the predicted classes to their true labels.
At this point, we are finally ready to start training the model. You may have noticed that we have not bothered to explicitly create a pipeline to allow for all the data transformations to be propagated to the validation and test sets. This is because a Vaex DataFrame implicitly keeps record of all the transformations and modifications done to the data. Filters, categorical encodings, scaling, even the outputs of an ML model are considered to be data transformations and are part of the state of a DataFrame. Thus, in order to get the validation set up to speed so we can use it as a reference point during the model training, we only need to get the state of df_train and apply it to df_val:
Now we are ready to instantiate and train the model, which we have chosen to be a LightGBM classifier:
When used in conjunction with Vaex, ML models are also transformers. That means that the predictions can be added to a DataFrame just as if you were applying another transformation. This is incredibly useful when building ensembles, but also for performing model diagnostics. In our case, the outputs of the LightGBM model are arrays of probabilities. To make the outputs more meaningful to an end user of the model, we are going to find the most likely class, and to it apply the inverse transformation, so we can get the name of the most likely activity — yet another in the series of transformations!
Once the model is trained, we can get a sense of its performance by computing a couple of metrics on both the validation and test sets, the latter of which was completely unused in the process so far. Again, all we need to do to get the predictions, is to get the state from df_train, which now includes the model predictions, and apply it to the df_val and df_test DataFrames:
Notice usage of the log function in the above and in previous code blocks, which is an instance of the standard Python logging system. When this code is run on the AI Platform, the logs will be automatically captured and made available in the centralized Cloud Logging section of GCP. Neat!
And we are done. The final step is to save the final state file in Google Cloud Storage (GCS) bucket, so it can be deployed later. Vaex can save the state file directly to a GCS or a S3 bucket:
Now that our training script is ready, it needs to be made into a Python package so it can be installed and executed on the AI Platform. Let us call our training module “har_model”. Its constituent files should be organized in the following tree structure:
Note that we are also including an empty “__init__.py” so that Python treats the “har_model” directory as a package. The “setup.py” script installs the package along with the required dependencies:
The nice thing about the AI Platform is that we can run our package locally before submitting a job to the GCP. This is quite useful for debugging and testing purposes. The following shell command will execute the training script locally, in the same manner as in the cloud:
Since the core technology in the current solution is Vaex, one can quite easily limit it to use a small fraction of the data to make the tests run faster. Once we are sure that the training module works as expected, we can submit the training job to GCP via the following command:
Given the large number of parameters, it can be rather convenient to execute the above command as a part of a shell script. That way, it can be version controlled, or part of a CI/CD pipeline for example.
Once the above command is executed, the AI Platform training job will start and you can monitor its progress in the Logging section of GCP. With the machine type we choose in the above example (n1-highcpu-32, 32vCPUs, 28GB RAM), the entire training job takes ~20 minutes. Once the job is finished, we can examine the logs to see how model has done on the test set:
That is it - with a minimal set up we successfully trained a fully custom Vaex pipeline on GCP! The pipeline itself, which contains the full feature engineering & data processing steps, the classification algorithm and the post processing manipulations, has the form of a Vaex state file and is saved to the designated GCS bucket, ready to be deployed.
The AI Platform is a rather convenient way to deploy ML models. It ensures high availability of the prediction service, and makes it easy to deploy and query multiple model versions, which is useful for doing A/B testing for example.
Deploying a Vaex pipeline is quite simple — all a prediction server needs to do, is to convert the incoming batches or data samples to a Vaex DataFrame, and apply the state file to it.
In order to deploy a custom Vaex pipeline, we must instruct the AI Platform on how to handle the requests that are specific to our problem. We can do this by writing a small class which implements the Predictor interface:
The above VaexPredictor class has two key methods: the from_path method simply reads the state file from a GCS bucket, while the predict method converts the data to a Vaex DataFrame format, applies the state file to it, and returns the predictions. Notice that the predict method conveniently intercepts data that has been passed either as a Python list or dict type.
The next step is to package the VaexPredictor class as a .tar.gz source distribution Python package. The package needs to include all the dependencies that are needed for obtaining the predictions. Creating such a package needs a “setup.py” file:
The package is created by running the following shell command:
Finally, we need to move the prediction package to GCS, so the AI Platform can pick it up and deploy it:
It may come in handy to bundle the above two commands in a bash script for convenience, especially if one needs to iterate a few times while creating the prediction package.
For reference, the directory tree of deployment part of this example project should look something like this:
We are now ready to deploy the prediction package. It can be rather convenient to define some environmental variables in the shell first:
The model deployment is done with the following two commands. First we need to create a “model” resource on the AI Platform like so:
Then we create a “version” resource of the model, which points to the model artefact, i.e. the state file, and the predictor class:
It may take a minute or two before the above command executes. That’s it! Our Vaex model is now deployed and is ready to respond to incoming prediction requests!
We are now ready to query our model. The batches of data sent to the AI Platform need to be in a JSON format. If the input has the form of lists, each row of the file should be a list containing the features of a single sample. Care should be taken that the order of features are consistent with what is expected by the Predictor class. And example of such a file is shown below:
Prediction requests are then sent with the following command:
The input data can also be formatted as JSON objects. One can be more flexible here — one line can be a single or multiple samples:
The following is a short screen cast of querying our “har_model” with the file above:
It is that simple!
The lives of ML models are not infinite. When the time comes to undeploy a model, one needs to first delete the version resource, and then delete the model resource:
Finally, it may be important to note that even though we trained the model on GCP in this example, this is not at all a deployment requirement. All that is required is for the state file to reside in a GCS bucket, so that the prediction module can pick it up. One can train the model and create the state file locally, or using any other service that is available out there.
I hope this article demonstrated that Vaex is an excellent tool for building ML solutions. It’s expression system and automatic pipelines are especially useful for this task, while its efficient, out-of-core algorithms ensure speed and keeps computational costs low.
Using Vaex in combination with GCP brings considerable value. Vaex is able to stream data directly from GCS, and only those portions that are absolutely necessary to the model. Training ML models on Google Clouds’ AI Platform is also rather convenient, especially for the more demanding, longer running models. Since the entire transformation pipeline of a Vaex model is contained within a single state file, deploying it with the AI Platform is straightforward.
Happy data sciencing!
In mid-November 2020 Google launched the next iteration of the AI Platform, dubbed AI Platform Unified. As the name suggest, this version unifies all ML related services that GCP offers: autoML, ready to use APIs as well as options to train and deploy custom models can all be found in the same place.
One significant improvement that the new AI Platform brings is the option to train and deploy models using custom Docker containers. This brings additional flexibility compared to the “classical” AI Platform in which only specific environments are available with a limited options to install or modify their contents.
Lets see how we can use the Unified AI Platform to train and deploy the Vaex solution we built earlier in this article, now using custom Docker containers.
Training the model is rather straightforward: all we need to do is create a Docker image that when started will execute the training script we prepared earlier. We begin by creating a Dockerfile:
In the above Dockerfile, “env.yml” all the dependencies we need that can be install via either conda, mamba, or pip. The “setup.py” and “har_model” make up the model training package we defined earlier. We then install the required dependencies and the model training package, and finally set up an entrypoint so that the training process starts when the container is run. Pro-tip: if you want to build very small Docker containers check out this guide by Uwe Korn.
Now, we can build the Docker image locally and push it to Google’s Container Registry, but it is much more convenient to simply use Cloud Build and build the image right in GCP:
Then we can launch the container and start the training job simply by executing:
The training progress can be monitored via Cloud Logging, which captures any logs that come out of the custom Docker container. Some 20 minutes later the job should finish, and we can inspect it via the Google Cloud Console:
Now let us deploy the model we just trained using a custom Docker container on the Unified AI Platform. When started, the container should run a web application that will respond to requests with the predictions. The web application should implement at least two method: one that the AI Platform will use to make “health checks” i.e. make sure the web application is running as expected, and another method which will accept incoming prediction requests and respond with the answers. For more information on the requirements of the container and all available options for customization you can check out the official documentation.
We will not go into detail in how to build such a web application, as there are plenty of resources on the web focusing on this. For reference, you can see the web application we have prepared for this example here.
After building and testing the web application, we need to create a Docker container that will run it when started. This is easily done following the same steps for creating the model training container.
Once the Docker image is available in Container Registry, we need to make a model resource out of it via the following gcloud command:
The next step is to create a model endpoint used to access the model:
We can now deploy the model resource to the endpoint like this:
This step may take a few minutes to complete. Note that you can deploy several model resources to a single endpoint, as well as have a single model deployed to multiple endpoints.
The model now is finally deployed and ready to accept requests. The requests should be in JSON format and have the following structure:
Here you see how such a file would look like for this particular example. The Google Cloud Console will also give you an example of how to query the model endpoint. It will look something like this:
That’s it! When your model outlives its lifetime, do not forget to undeploy it and delete the endpoint and model resource, as to avoid unwanted costs. That can be done like this:
As a final note, one can think of training and deployment as two separate, fully independent processes. This means that one can use the “classical” AI Platform to train, and the Unified AI Platform to deploy the model, and vice-versa. Of course, one can always create the model “in house” or using any resources available out there, and just use GCP for serving.
Just another data scientist | PhD Astrophysics | co-founder of vaex.io | https://www.linkedin.com/in/jovanvel/
253 
1
Thanks to Maarten Breddels, Jonathan Alexander, and Bulat Yaminov. 
Every Thursday, the Variable delivers the very best of Towards Data Science: from hands-on tutorials and cutting-edge research to original features you don't want to miss. Take a look.
253 claps
253 
1
Your home for data science. A Medium publication sharing concepts, ideas and codes.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/this-week-in-google-cloud-platform-how-google-does-ml-by-coursera-iot-core-goes-ga-multi-cloud-13bfc8a5a42a?source=search_post---------59,"There are currently no responses for this story.
Be the first to respond.
A “How Google does ML” course is now available on-demand via Coursera. This is the first course in a two 5-course specializations. Already lots of open-source example ML applications are available here.
[GA] The thing is . . . Cloud IoT Core is now generally available (with partners and customers) (Google blog). You can now publish data streams from the IoT Core protocol bridge to multiple Cloud Pub/Sub topics.
Enhanced Compute Engine instance templates allow you to create instances from existing instance templates and create instance templates based on existing VM instances. See “Managing your Compute Engine instances just got easier” (Google blog) for details. Added bonus: you can now protect your virtual machines from accidental deletion.
From the “all things Cloud Spanner” department :
From the “multi-cloud is real” department :
From the “how-to {Kubernetes|TensorFlow|Istio}” department :
From the “In case you’ve missed it (ICYMI)” department :
This week‘s picture is taken from the Cloudflare multi-cloud Kubernetes deployment blog post :
That’s it for this week!-Alexis
Google Cloud community articles and blogs
56 
56 claps
56 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Google Cloud Developer Relations
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://betterprogramming.pub/how-to-terraform-with-jenkins-and-slack-on-googles-cloud-platform-56c5e8b3aeeb?source=search_post---------60,"Sign in
There are currently no responses for this story.
Be the first to respond.
Gaurav Agarwal
Apr 7, 2020·10 min read
Terraform is the most popular Infrastructure as Code (IAC) tool provided by Hashicorp. With the advent of cloud and self-serviced infrastructure, a groundbreaking concept called Infrastructure as Code (IaC) emerged.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/javarevisited/7-free-courses-to-learn-google-cloud-platform-for-beginners-cbb260fbd8e4?source=search_post---------61,"There are currently no responses for this story.
Be the first to respond.
Hello guys, If you want to learn Google Cloud Platform in 2021 and looking for some free online Google Cloud training courses, tutorials, and learning materials to start your GCP journey then you have come to the right place.
Earlier, I have shared the best Google Cloud Platform courses, and today, I am going to share free online courses from Udemy Youtube, and Coursera which you can join to learn Google Cloud platform.
If you don’t know, Google Cloud Platform is the massive Cloud platform for Google which is one of the three biggest public cloud platforms along with AWS and Microsoft Azure.
There is a huge demand for Google Cloud Professionals but there are not many skilled professionals in the market, hence more and more people are learning about Google Cloud and getting certified.
This article contains free Google cloud courses from sites like Udemy, Coursera, and Pluralsight which you can use to learn Google cloud from scratch. These Cloud services are providing great help to the technical world easing up the tasks, remote access, and security features. People are taking their business online, and the market is growing at a steady rate.
We have to keep up with the trend if you’re a business enthusiast, you know how much importance our words hold. To provide you with knowledge of the cloud and how you can work with them we have picked up a few courses that you can access for free and make your way in the market with new technology available. We have tried to provide you all the free Google cloud courses that can satisfy your curiosity and help you attain your goal whether you’re a developer or just looking to gain some Cloud computing skills.
We specifically focused on the part to provide you with the best kind of teaching that doesn’t waste much of your time and can make learning easy for you.
By the way, if you don’t mind spending few bucks on learning a valuable skill like Google Cloud and looking for more comprehensive online training courses then I highly recommend you check out the Google Cloud Platform (GCP) Fundamentals for Beginners course on Udemy. One of the best online courses to learn GCP concepts from scratch.
udemy.com
Here is the list of the best free courses that you can access online to learn the Google Cloud Platform or GCP. These courses not only cover the platform details but also get you familiar with essential GCP services and features.
This is one of the best available courses on Udemy. To begin this course you need to have little knowledge of IT. Since the Google cloud platform is the fastest growing cloud service in the world, people are taking their business online and enhancing them with services the cloud has to provide. If you are looking to be in the developer section of the industry this course will help you a lot. Also, big data concepts and machine learning concepts are introduced recently. Here are things you will learn in this course:
This course focuses on providing you with a deep understanding of google cloud concepts and working. You’ll be learning a lot about its applications in different segments of this course.
Here is the link to join this free GCP course — Google Cloud Platform Concepts
This is one of the best free courses to learn Google Cloud Platform and it’s offered by none other than Google Cloud on Coursera. You can access the course to learn it all from their official website as well. They have sincerely categorized and provided concept-wise learning, distributed among topics. All the new technology and its functioning can be accessed on the official website if you have a learner id. This course is also part of multiple Coursera specializations like Developing Application with Google Cloud and completing this course counts towards getting the certification. Here are things you will learn in this course:
All the topics are covered with examples. It is a theoretical way to teach so you have to be patient while you learn and apply concepts. Doubts can be easily addressed in the query section.
Also, real-time practice is going to provide you a detailed understanding of cloud services and functions. You can interact with other users following the same course as well. There are other premium courses suggested by Google as well.
Here is the link to join this Coursera course — Google Cloud Platform Fundamentals: Core Infrastructure
Overall a fantastic free Coursera course to learn key GCP concepts and services like Google App Engine, Google Compute Engine, Google Cloud Storage, Google Kubernetes Engine, Google Cloud SQL, and BigQuery, etc.
This is another awesome free Udemy course to learn about Google Cloud Platform from scratch. To start this course you should have a little experience with coding or software development.
This is an introductory course to get started on Google Cloud Platform (GCP). During this course, you will learn about what is Cloud Computing, what are different Cloud computing models, Important GCP services, and hands o
You can also use this free course to prepare for highly valued Google Certifications like “Google Associate Cloud Engineer”, “Professional Cloud Architect” etc. Here are things you will learn in this free Google Cloud course:
For all those people who are looking to gain application development skills in this course focus on that too.
Also, the basic knowledge of the cloud would provide you a better understanding. You will be getting a lot of theory and hands-on to prepare you for a bigger platform.
Here is the link to join this free course — Google Cloud Fundamentals 101
Google Cloud has many courses on Pluralsight and this is one of them. PluralSight uses effective tutorial learning to provide critical cloud skills. Even they’ll teach you how to operate on hybrid as well as in multi-cloud computing.
You’ll be able to arrange and understand the way digital information works in the cloud. Pluralsight has teamed up with Google to provide you with the essential skillset to make your career in the cloud.  This course designed for AWS professionals like AWS developers, solution architects, and sysops administrators who are already familiar with essential cloud computing concepts and have prior experience with AWS. Here are things covered in this course:
They start from basic and the course takes you through all the important topics that need to be learned.
Here is the link to join this course — Google Cloud Fundamentals for AWS Professionals
Google has mentioned the Pluralsight courses on its official website for cloud computing as well. This course is personally recommended by Google and since the website doesn’t support free access for a longer time, make sure you get to the learning as soon as possible. Alternatively, you can also use Pluralsight 10-day-free-trail to access this course for FREE.
pluralsight.pxf.io
This is a list of small online tutorials from a Youtube Playlist to learn about Google Cloud Platform from Google Tech itself. Learning Google Cloud Platform! GCP can be overwhelming at first, but these small tutorials will help you get started.
You will watch a short video to understand the difference between Google Cloud Platform’s (GCP) free tier and free trial and understand how they can help you test GCP or use it for development purposes at no or little cost.
There are many short videos to learn about essential services and Google Cloud Platform concepts like compute, storage, networking, security, pricing, and many more.
If you like to learn from short videos then I highly recommend this free Youtube playlist from Google Tech. You can watch them right here or on Youtube.
This is another awesome course to learn about Google Cloud Platform and it's available on Pluralsight. This course will provide an overview of the platform and a framework for diving deeper.
While the Google Cloud Platform is a more recent offering than some of its competitors, it draws on years of experience running Google’s massive internal infrastructure and exposes a streamlined set of solution-focused capabilities to help you build great systems.
Created by Howard Dierking this course will first teach the core building blocks of the platform. Next, you’ll explore the characteristics that differentiate Google’s offering from other cloud platforms.
Finally, you’ll learn the common application architectural patterns. By the end of this course, you will be able to understand how the areas fit together and provide starting points for deeper exploration.
Here is the link to join this course — Google Cloud Platform Fundamentals
By the way, this course is not exactly free as you would need a Pluralsight membership which costs around $29 per month or $299 per year. Alternatively, you can use their 10-day-free-trial to watch this course for FREE.
pluralsight.pxf.io
This is one of the best free Udemy courses available online to learn Google Cloud Platform concepts, plus learners have provided it near-perfect ratings to learn. One thing that would be beneficial is that you don’t need to learn anything to begin this course, you can even start right now. If you are willing to learn the basics of google cloud and cloud computing this is the course for you. Here are things covered in this course:
This course has a goal to provide you with a simple conceptual introduction to cloud services and computing. There is not much technical information to remember, this course will help you to have the perspective if you are starting from scratch. Plus if you choose to learn from this course you’re going to get Linux Academy’s Hands-On lab and maybe you’ll get flashcards.
Here is the link to join this course — Google Cloud Concepts
That’s all about the best free courses to learn Google Cloud Platform or GCP in 2021. Google cloud is the fastest growing cloud service in the world. Hope you found these courses useful to help you learn the new technology and implementation. We would recommend you to visit each course personally to have a better insight.
Other Cloud Computing Articles you may like
Thanks for reading this article so far. If you like these free Google Cloud Platform online courses for beginners then please share them with your friends and colleagues. If you have any questions or feedback then please drop a note.
P. S. — If you don’t mind spending few bucks for learning a valuable skill like Google Cloud and looking for more comprehensive online training courses then I highly recommend you check out the Google Cloud Platform (GCP) Fundamentals for Beginners course on Udemy. One of the better courses to learn GCP concepts from scratch.
udemy.com
Medium’s largest Java publication, followed by 14630+ programmers. Follow to join our community.
164 
164 claps
164 
Written by
I am Java programmer, blogger, working on Java, J2EE, UNIX, FIX Protocol. I share Java tips on http://javarevisited.blogspot.com and http://java67.com
A humble place to learn Java and Programming better.
Written by
I am Java programmer, blogger, working on Java, J2EE, UNIX, FIX Protocol. I share Java tips on http://javarevisited.blogspot.com and http://java67.com
A humble place to learn Java and Programming better.
"
https://medium.com/google-cloud/this-week-in-google-cloud-platform-cloud-tpus-gpus-on-gke-96-vcpu-and-a-serverless-landscape-2ec95d726e01?source=search_post---------62,"There are currently no responses for this story.
Be the first to respond.
This past week, GCP has unleashed key performance enablers, some will say game-changers :
Staying with Kubernetes, Pivotal Container Service has recently reached GA (pivotal.io). What make it special for GCP is its constant compatibility with GKE.
CNCF is thinking about serverless computing (via its Working Group) and has released a whitepaper and a landscape chart.
“In our genes: How Google Cloud helps the Broad Institute slash the cost of research” (Google blog). You can now run the germline GATK Best Practices Pipeline for $5 per genome (down from $45). 76k genomes processed to date and 36PB of data stored on GCP.
From our “In Case You Missed It (ICYMI)” department :
This week’s GCP Podcast (#114) is about “Machine Learning Bias and Fairness” with Timnit Gebru and Margaret Mitchell (gcppodcast.com)
This week‘s picture is CNCF’s Serverless Landscape :
That’s it for this week!-Alexis
Google Cloud community articles and blogs
115 
115 claps
115 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Google Cloud Developer Relations
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/fabric-on-google-cloud-platform-97525323457c?source=search_post---------63,"There are currently no responses for this story.
Be the first to respond.
With thanks to the IBM engineers that wrote a Helm Chart to deploy Fabric as part of the IBM Blockchain Platform. It was relatively straightforward to port this to Kubernetes Engine although — at present — this is just a working Fabric network and I need to now get the tools (Composer) configured.
Create yourself a Kubernetes. I’m trying to pioneer and so am using a Regional Cluster and I’ve enabled the new Kubernetes Monitoring functionality.
Update: So, I’m not so pioneering, Regional Clusters are now GA :-) Yay!
This all appears to work just fine, something like:
You should be able to:
The Helm Chart requires a Kubernetes PersistentVolume that’s ReadWriteMany. This is not currently as easy as it ought be on Google Cloud Platform … cough… Google… cough…
So, we’re going to use NFS to provide the read-many capability. Ironically, the NFS solution we’re going to use is itself backed by Google Persistent Disk. Let’s create the NFS server because the Helm Chart depends upon it.
With thanks to mappedinn, I used their repo kubernetes-nfs-volume-on-gke to get this setup. This uses Google’s volume-nfs image and works great.
Create the underlying Persistent Disk:
Then apply the following Deployment and Service your Kubernetes cluster. The Deployment creates the NFS service using Google’s hosted volume-nfs image and binds the service to the Persistent Disk:
NB If you’d prefer to use SSD Persistent Disk instead of Standard (HDD) Persistent Disk, replace line #7 “default” with “ssd” in the Deployment script and before applying the Deployment, apply the following file to your cluster to register SSD as a storage class:
You should:
This will yield an NFS service that’s accessible through the Kubernetes’ Service’s DNS name: nfs.default.svc.cluster.local.
OK. You should not create a PersistentVolume or PersistentVolumeClaim for NFS because these will be created using the Helm Chart. Here’s a sneak-peek of the PersistentVolumeClaims *after* the Helm Chart has been deployed. You’ll do that in the next step:
Download and unzip the latest Helm binary (releases), add it to your $PATH and install Helm’s Tiller into the Kubernetes cluster. Assuming we’re in ${WORKDIR} and that you unzipped Helm into ${WORKDIR}/linux-amd64:
NB Helm has binaries for OSX and Windows, I’ll leave it to you to work out the specific instructions for non-Linux.
If — as is likely — you’re using an RBAC-based Kubernetes (Engine) cluster, I recommend the following steps to install Helm’s Tiller to the cluster:
This should return:
Then, clone my GitHub repo (with all credit to the folks at IBM to doing 98% of all this work for us):
But do not change into the ibm-blockchain-network directory created by the clone; remain in the parent (${WORKDIR}).
Optional: It’s good practice to run the Helm Linter over the Chart before deploying helm lint ibm-blockchain-network
Optional: It’s good practice to ‘dry run’ the deployment before applying it to your cluster. This is a useful feature of Helm and provides you with a way to see the Kubernetes specs that would be applied to your cluster: helm install --dry-run — debug ibm-blockchain-network.
When you’re confident in the results:
Helm will apply the Chart to your Kubernetes and provide you with an enumeration of its work:
You may check the Cloud Console for Kubernetes Engine Workloads:
https://console.cloud.google.com/kubernetes/workload
And Services:
There’s a bunch of logs created by the various containers that are deployed. Here’s the bootstrap container created by the ibm-blockchain-network-utils Pod:
For configtxgen:
For cryptogen:
For ca:
For orderer:
And org1peer1 which is similar to org2peer1:
You may list and delete deployed Charts with:
NB: There’s not currently an easy way to grab dynamically generated Chart names (such as solitary-possum) in this example. So, to delete a Chart, you’ll need to do the list in order to identify its name.
Don’t forget to delete the NFS Deployment when you’re done with it too:
You can whack your cluster:
Since we covered logging, I’d mentioned that I deployed the cluster enabling support for the new Stackdriver Kubernetes Monitoring.
Apart from needing to create the NFS service, the only other change that was needed for the Chart to deploy to Kubernetes Engine was a tweak to the Composer configuration. The ibm-blockchain-network-utils Workload creates 3 pods including bootstrap. bootstrap has a volume called composer-credentials that was mounted onto the Node’s (host’s) root (/) directory.
Container-Optimized OS is built to be secure and the OS includes the minimal amount of extraneous tooling. You can see here that the root (/) directory is mounted as read-only “to maintain integrity”. For this reason, the Chart was revised to use /tmp/composer instead of /composer. You can see this change in blockchain-utils.yaml lines 20–22:
and also in blockchain-debug-nfs.yaml. Wait what?
I provided some examples recently of ways you may debug Kubernetes Deployments. In this case, I wanted to ensure that the NFS service was working correctly. If it were working correctly, containers would be able to access its volume mounts.
To confirm this, I added a template to the Helm Chart. This template is called blockchain-nfs.debug.yaml. There are two advantages for including this in the Chart. The first is that it gets to use Helm’s variable replacement. The second is that it keeps everything together and ensures the debugging Deployments is created|deleted with the Chart.
blockchain-debug.nfs.yaml:
The NFS volume is referenced by the PersistentVolumeClaim defined in lines 34–36. You’ll note that there’s a second (and non-NFS) volume called composer-credentials that I added when debugging why this was failing when deployed to Kubernetes Engine (because of COS and not permitting “/” explained previously).
What’s neat is that, once deployed, we can grab the resulting Pod’s (!) name and exec into its (Alpine) shell and enumerate the contents of the directories:
This returns:
Helm is a good tool and it’s easier to use than I’d expected.
I’m working on a Helm Chart for Trillian too.
IBM’s Helm Chart is designed to deploy Fabric to IBM’s Kubernetes service but, because Kubernetes is Kubernetes is Kubernetes, as you can see, it’s trivial to convert this Chart to work on Kubernetes Engine too.
I’m hoping to learn more from the IBM folks about how to integrate Hyperledger Compose into this deployment and will update this post then.
Google Cloud community articles and blogs
41 
3
No rights reserved
 by the author.
41 claps
41 
3
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/georgian-impact-blog/comparing-google-cloud-platform-aws-and-azure-d4a52a3adbd2?source=search_post---------64,"There are currently no responses for this story.
Be the first to respond.
A starting point to help you choose the right platform for your ML project.
By Jing Zhang
If you’re looking for an end-to-end machine learning (ML) platform, you’re spoiled for choice. There are three main choices for cloud providers: Google Cloud Platform (GCP), Amazon Web Service (AWS) and Microsoft Azure Platform (Azure). The question is: how do you choose between the three? What functionality do they provide to build ML pipelines? We set out to answer these questions in a recent hackathon.
The R&D team at Georgian, where I work as an ML Engineer, decided to organize a hackathon to explore how each provider can help with the workflow. Our team builds machine learning software components ourselves, but we’re typically working with the growth-stage software companies in the Georgian family to deploy, so we wanted to familiarize ourselves with the different platforms and to be able to adapt to their workflows faster.
In this blog post, we’ll give a high-level overview of the ML platform solutions provided by GCP, AWS and Azure based on our experience during the hackathon. This list is not exhaustive but shows the results of our hackathon research.
Specifically we’ll cover:
We hope this can offer help as a starting point to help you choose the right platform for your ML project.
To help compare the different platforms, we chose a project that we could run on all three so we could compare apples to apples. Our project was to build a binary classification model with a given dataset and an end-to-end pipeline on a cloud provider. We used a dataset of companies that we use for our own machine learning platform, Spring. Spring helps us identify companies that fit our investment profile.The dataset contained general information about companies including their management team, financial information and office locations. The goal was to identify whether a company fit well into our investment profile (labeled 1) or not (labeled 0).
Since we used our own data for this project, we can’t share the specifics here, but to give you a sense what we were working with, here’s an overview of the dataset:
We also intentionally injected noise and error in the dataset so that we could run some tests on the data cleaning functionality. Specifically, we introduced:
One of the goals of this hackathon was to explore and assess each aspect of the end-to-end ML pipeline shown in the figure below. We decided to approach the challenge this way so that we would be able to assess the needs of a given project against the providers. For example, if explainability is important to a project, which is the best choice? Or, which provider performs the best if you want to run heavy testing?
Specifically, we wanted to assess each platform’s functionality in these areas:
Preprocess
Discover
Develop
Train
Test & Analyze
Deploy
Since we were building a model, we needed to think about model performance. The metric we tracked was Area Under the Receiver Operating Characteristic Curve (ROC_AUC). We weren’t, however, using model performance to make a judgment on which platform was better, so long as the results were roughly comparable.
Fairness in ML has emerged as an enormous issue area for policy makers, industry and the public. It’s important to have tools at your disposal to verify that your model is not treating any group and individual unfairly. To be able to address fairness, explainability tools are important too. These tools allow you to query why a model reached a certain decision. We were looking for the availability of the tools rather than assessing their relative performance.
Unlimited budgets are nice to dream about, but the reality is that cost is a constraint we should always keep in mind, so we tracked the costs for each provider.
As you would expect with these three platforms, there is a product or service for each step in the ML pipeline. The differences between them are in how well the services integrate with each other to build an end-to-end ML pipeline experience.
We have summarized the available services in the areas we’re interested in the table below. Going into detail is beyond the scope of this blog post. You can use this summary table as a starting point to see what’s available at each step for each provider.
For our team, the most important features are the model development and model deployment sections and, to a lesser extent, data visualization and QA. We were glad to see that all three providers have hosted notebook services, experiment tracking and version control, and easy deployment methods.
As for AutoML, all three providers have developed their own offerings. We used it to build our initial models and check whether there were valuable signals in the dataset. It would be great if the explainability component could be integrated so we could understand how the models are built for further model analysis and development.
Speaking of model explainability, while all three platforms all provide some tools, the functionality varies. If you have specific requirements, make sure you check if the current functionality satisfies your needs.
GCP uses a package called “the what-if tool”. You can integrate it with your notebook and play with the model by changing threshold or feature value for a given example. This allows you to check how certain changes affect the prediction result.
If you are using the Sagemaker debugger, it allows you to analyze how feature engineering and model tuning are done.
Azure provides a built-in module in their SDK, which seems to have the best integration.
So how did the models perform? As measured by ROC_AUC, performance was comparable across all three platforms. Azure and GCP scored slightly higher than AWS. This doesn’t necessarily mean one platform is better than another. It matched our expectation that the scores vary but should be close.
Cost, on the other hand, was more interesting. The cost on AWS was considerably lower than GCP and Azure. Please note this isn’t strictly an apples-to-apples comparison. The Azure team on our hackathon explored more services since Azure was a completely new platform to our team and we wanted to use the opportunity to learn more about it, so that may account for some of the cost difference.
One question we asked was: “Is it worth spending 4 to 6 times of money to get a 5% performance improvement?” The answer depends on the problem you’re addressing. For example, we’re looking for companies that could potentially generate large returns for our fund, so it may be worth spending the extra few hundred dollars. If you have budget concerns and the spending doesn’t justify the return, then it’s a different story.
Based on our observations, all three cloud providers cover the aspects of the ML workflow we care about.
Two hot topics in the industry right now are the rise of AutoML and the need for an end-to-end machine learning workflow in one place to provide a frictionless experience. As we mentioned earlier, AutoML products are already available on all the platforms but the end-to-end pipeline experience doesn’t seem to be mature yet.
On GCP and AWS, you’ll need to assemble multiple products together to get to the desired outcome. Azure, on the other hand, provides a machine learning designer service with drag and drop UI. This might imply that they are targeting different customer bases.
With the drag and drop UI interface, Azure’s machine learning designer may be more friendly to those new to data science, with little coding and technical background, to try out machine learning projects and evaluate whether it brings value to the business. Many corporations already use Microsoft products, so it might be an easy choice for teams at larger companies who are looking to try machine learning for the first time.
AWS and GCP seem to be more developer-focused. Though it’s a little more work to assemble a pipeline, they’re more customizable with the different components available. These components and the connection of the pipeline are often developed through code and configurations rather than an user interface. Companies who are more familiar with the options and know what they want to achieve may prefer this option.
We certainly learned a lot from this Georgian hackathon and it puts us in a better position to undertake projects on any of these three platforms in future. We hope that this is useful to you and helps you pick the right platform to start your next project.
A blog focused on machine learning and artificial…
139 
139 claps
139 
A blog focused on machine learning and artificial intelligence from the Georgian R&D team
Written by
Investors in high-growth business software companies across North America. Applied artificial intelligence, security and privacy, and conversational AI.
A blog focused on machine learning and artificial intelligence from the Georgian R&D team
"
https://medium.com/google-cloud/this-week-in-google-cloud-platform-datastore-backups-configurable-ssl-a-new-billing-api-and-52cd3aeba9a2?source=search_post---------65,"There are currently no responses for this story.
Be the first to respond.
You can now set SSL policies, such as a minimum required TLS version, on GCP load balancers. Both predefined and custom profiles are available: “Announcing SSL policies for HTTPS and SSL proxy load balancers” (Google Blog)
A long-awaited feature for our highly-scalable NoSQL managed database service is now Generally Available: “Fully managed export and import with Cloud Datastore” (Google Blog)
Cost transparency has always been important to Google Cloud so this enhancement to the Google Cloud Billing API should come as no surprise: “Introducing Cloud Billing Catalog API: GCP pricing in real time” (Google Blog)
This was already shared last week, but here’s now the formal post: “Announcing Google Cloud Spanner as a Vault storage backend” (Google Blog). Developed as open source and supported by Google.
From the “TWINR (This Week In New Releases)” department :
From the “it takes great research to make great AI” department :
From the “In case you’ve missed it (ICYMI)” department :
From the “Customers talk best about Google Cloud” department :
A couple of networking solutions have been released by the Google Cloud team this past week (check out the latest GCP podcast about Solutions authors) :
From the “how-to” department :
From the “listen and watch” department :
This week’s picture is taken from the Google Research “Mobile Real-time Video Segmentation” blog post :
That’s it for this week!-Alexis
Google Cloud community articles and blogs
46 
46 claps
46 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Google Cloud Developer Relations
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/@hoffa/connecting-africa-nomanini-gcp-and-bigquery-c613aedb6d3c?source=search_post---------66,"Sign in
There are currently no responses for this story.
Be the first to respond.
Felipe Hoffa
Aug 21, 2018·4 min read
Nomanini is a startup powered by Google Cloud that enables micro-transactions in various countries around Africa. I flew to Johannesburg to meet their CTO and learn how they use App Engine, Kubernetes, BigQuery and other Google Cloud Platform tools.
Nomanini builds point-of-sale systems to help remote villagers get access to prepaid airtime, prepaid electricity, and basic banking services like being able to withdraw or deposit cash.
Back in 2011, they started by focusing on selling prepaid mobile air time. Distributing boxes of scratchcards out into rural villages isn’t easy — so they created a wireless alternative.
Now people in the villages don’t have to travel far to go and get essential services like air time or electricity, and even banking services.
In this video, Dale Humby — Nomanini’s CTO — shares more about the custom devices they created and why they chose Google Cloud to power their backbone. Originally on App Engine, they now have most of their infrastructure running on Kubernetes.
And for their analytics they use… BigQuery!
Each of their devices reports back its own stats — thus Nomanini is able to track their status and what kind of connectivity they have. This information allows them to debug the status of the different cell networks — and even inform the mobile networks of problems before they notice them.
OpenCellID is the world’s largest open database of cell towers.
As of October 2017, the database contained almost 36 million unique GSM Cell IDs. More than 75,000 contributors have already registered with OpenCellID, contributing millions of new measurements every day to the OpenCellID database.
Thanks to open data, Nomanini can look beyond their devices and check the daily reports of thousands of contributors around the world.
In the video, we use a combination of BigQuery, Data Studio, and re:dash to visualize the status of cell networks around the world, and in the cities where Nomanini is present.
For example, let’s see what are the most popular mobile radio technologies around the world, according to the OpenCellID tables:
UMTS is the most popular radio tech according to OpenCellID, while CDMA is the least. But they’re not distributed evenly around the planet. A quick check with Data Studio can show us the actual patterns:
We also visualized how many cell towers each country in Africa has, relative to its population:
Nomanini is able to join this data with the reports from their own devices.
For example, to visualize the state of each of their devices around Mozambique (with Redash):
For the full story, watch our video!
OpenCellID is an open data project — let us know if you’d like to see it permanently updated in BigQuery through our public datasets program.
Want more stories? Check my Medium, follow me on twitter, and subscribe to reddit.com/r/bigquery.
And try BigQuery — every month you get a full terabyte of analysis for free.
Data Cloud Advocate at Snowflake ❄️. Originally from Chile, now in San Francisco and around the world. Previously at Google. Let’s talk data.
See all (1,656)
74 

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
74 claps
74 
Data Cloud Advocate at Snowflake ❄️. Originally from Chile, now in San Francisco and around the world. Previously at Google. Let’s talk data.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/hands-on-with-google-cloud-platform-tensorflow-bigquery-early-thoughts-4d379cd693fc?source=search_post---------67,"There are currently no responses for this story.
Be the first to respond.
I spent two days at Google’s Sydney office this week, using their new Google Cloud Platform (GCP) and getting some hands on experience with the GCP tool set.
I was given the opportunity to use GCP tools for querying billion-row data sets, set up virtual servers and create machine learning models.
We were encouraged to try … test … trial GCP to our hearts content.
At the end of the two days, I walked away impressed and excited. I think Google really has something here.
It’s much cleaner and easier to use than Google’s predecessor called App Engine. App Engine didn’t work that well if we’re being honest and Google will be the first to tell you that.
The Google Cloud Platform has clearly been rethought and redeveloped — complete with data analysis, compute power, machine learning, storage, data processing and more.
Overall, Google has created a great platform in GCP and its ahead of the field when compared to the market.
I spent my time using the GCP tool set, thinking about how I might perform the same tasks with the IBM BlueMix/Watson suite as well as Amazon’s platform.
The difference came down to ease-of-use and elasticity. GCP felt miles ahead of both IBM and Amazon in terms of easily creating machine, conducting data analysis and running machine learning scripts. In terms of elasticity, GCP allows you to run virtual machines for seconds at a time which is much more elastic than Amazon where you pay for machines by the hour.
Another attendee at the workshop, also comparing tools, summed it up perfectly when he said “The other tools have a high barrier to use, with this I can just make it work.”
His words echoed the same sentiment I took away from my time in Sydney. GCP was built to ‘just work’. And while there were improvements needed for various processes, Google has put out a great product and platform.
I’m excited to start throwing real data into GCP and trialling the platform in the wild. Look out for more from me as I try BigQuery and TensorFlow with real-world projects in upcoming weeks.
Google Cloud community articles and blogs
21 
2
21 claps
21 
2
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Head of Decision Science + Analytics @ Spaceship. Ex Instagram + Intuit. PhD. Social Scientist. Conservation, paddleboards & smoothie fan. Views are mine only.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/@maxxsh/setup-free-ssl-certificate-for-website-on-google-cloud-platform-90a249a6bce3?source=search_post---------68,"Sign in
There are currently no responses for this story.
Be the first to respond.
Top highlight
Max Shestov
Jan 25, 2021·3 min read
This guide provides step by step instructions that will help you to install the auto-renewing Let’s Encrypt SSL Certificate using automated client Certbot for your site hosted on GCP.
Make sure that you have: ∘ A running VM instance with the Apache server on Ubuntu 20.04 installed.  ∘ Your domain name set up with your Google Cloud instance. ∘ A website that works over an HTTP connection. (URL of the website begins with “http://”. If you change it to “https://” it will not load the page).
Log into your Google Cloud Console and selectCompute Engine > VM InstancesOn your VM instance click SSH to open the terminal.
First, the Certbot needs snapd installed. But If you’re running Ubuntu 16.04 or later, you don’t need to do anything. Snap is already installed and ready to go. You can check it is installed by attempting to run snap version on the command line:
Ensure that the latest essential release of a snap is installed and tracked for updates.
Now, install Certbot, executing the command
Then create the symbolic link to the Certbot directory
Next, launch the Certbot to generate an SSL Certificate for your website
Provide an email address (optional), that Let’s Encrypt automatically send you expiry notices when your certificate is coming up for renewal. If the certificate is already renewed, they won’t send an expiry notice.
Agree with the Terms of Service by entering Y.
Enter Y if you are willing to share your email in order to get news from Let’s Encrypt project. Otherwise, enter N.
Type your domain with both naked domain name and www sub. If you have a subdomain enter it also.
Let Certbot edit your Apache configuration automatically.
Choose a vhost with enabled HTTPS typing its number
All done!Now you have enabled the SSL certificate provided by Let’s Encrypt with automatic renewal by cron job systemd timer.
By default, the crontab attempts to renew the certificate twice a day, but renewal will only occur if expiration is within 30 days.You may change the periodicity in the tasks file running by Cron, a time-based scheduling service.
For example, the code * 4 1.15 * * will run on every first and 15th day every month at 4:00 am. More about scheduling tasks in a crontab file see the documentation here.
Simulate automatic renewal
Verify that Certbot works by typing in your browser’s address bar “https://” at the beginning of the website’s address.
Lead Developer at Reverence Global. Husband of a wonderful wife, Entrepreneur, Dad
See all (53)
440 
1
440 claps
440 
1
Lead Developer at Reverence Global. Husband of a wonderful wife, Entrepreneur, Dad
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/using-serviceaccountactor-iam-role-for-account-impersonation-on-google-cloud-platform-a9e7118480ed?source=search_post---------69,"There are currently no responses for this story.
Be the first to respond.
I’ll be adding updates to my new blog here: https://blog.salrashid.me/
The serviceAccountActor IAM role on Google Cloud has some very useful and powerful capabilities. It is an IAM role that allows you to grant another user or serviceAccount the ability to impersonate a service Account. In this way, you can have service account A impersonate B and acquire the access_tokens or id_tokens for B.
This article covers how you can acquire id and access tokens for service account B by service account A. You can also have a end-user assigned the serviceAccountActor role too. For clarity, I’ve separated out the id and access token steps below.
Note:
One final mechanism described here is creating an arbitrary JWT using Service Account’s private key. These JWTs are signed by one of the keys associated with a given service account and can convey additional verifiable information within the claim such as the intended target (audience) and custom scopes.
For more information see
You can find the script referenced here on my github page
Update 7/1/18: GCP IAM now allows for IAMCredentials.generateAccessToken(). This feature allows you to directly genreate an ID or AccessToken and place conditionals on delegation too. While the samples below will work, I’ll rework the sample in this repo to use that API as shown in this example. In several ways, this article is obsolete given the direct API is now available. I’ll keep it posted as a reference. The full code sample for iamcredentials API is shows at the end of the article
The first step is to assign the serviceAccountActor role on B to A. That is, you are allowing A the permission to act as B. This configuration is done on the IAM Permissions page by selecting B as shown below.
In the example screenshot,
Note: serviceAccountB_ID does not even have any valid certificate keys. This means there are no valid physical, distributed certificate key files.
As mentioned, you do NOT have to use a service account to impersonate another one; you can easily assign the serviceAccountActor role to an end user (e.g. user@domain.com).
Since we are doing operations as service Account A, we need to create a client for access to IAM using Application Default Credentials:
After we assigned the role, download the certificate for A (the service you will use to impersonate B). You may also impersonate B as a user and to do that, you will need to initialize a client as an end user. For more information on, see Google Cloud Authentication Samples.
Now that we have an initialized, authenticated client for A, we need to generate a JWT with some specific claims. Since we are creating an access_token request, the JWT needs to be in the form:
Set the scopes to limit the capabilities of a given token. A list of scopes can be found here.
Now that we have an JWT claim set, we need to sign it using A’s credentials but instruct .signJWT() for B:
Now that we have the signature for the JWT, we need to append the signature to the unsigned part
Now you have an access token. With this, you can access any GoogleAPI that is scoped correctly and to which B has access.
One way to initialize a Google API client given a raw access_token is to use oauth2client.client.AccessTokenCredentials. Other languages have similar bindings.
You can also verify the access_token by interrogating the tokeninfo endpoint as shown here:
Though id_tokens and access_tokens have similar flows, they are used for very different things: as the name implies, access_tokens are used to grant access to google resources (eg. GCS, PubSub, etc) while id_tokens simply assert the bearers identity.
These id_tokens are digitally signed by google so given an id_token, a system can verify its authenticity by verifying its signature against Google’s public certificate. The id_tokens are simply convey who the caller is (not what it can do).
If you need two system to communicate securely, you can pass (via SSL), an id_token. The receiving system can locally validate the token using a cached copy of Google’s certificate. However, Google-issued ID tokens simply identify the caller and cannot assert if the receipient of the token is the intended target. For that, you can mint a Service Account id_token as described below.
See:
The first step is to create an unsigned JWT but have the scope to the service Account B’s ID:
Note the audience is different than for an access_token.
Now that we have an JWT claim set, we need to get a jwt using A’s credentials but instruct .signJwt():
Now that we have the id_token, we can transmit it to another system.
The recieving system that gets the id_token must validate its authenticity.
You can validate it using some standard libraries too:
The public certificate used by Google to sign can be found at:
Users can also generate a JWT that is signed by a given Service Account. The steps here are very simple: create a JWT and use .signBlob() to sign the header and claims. The signedJWTs can be used to verify caller identity as well as see if the recipient is the intended target. Finally, you can customize the scopes fields to convey capability for the token.
You can specify the intended target for the JWT using the aud field. In this case, the audience is ‘System C’. The scopes field is arbitrary and user-defined.
The JWT issued may look like the following:
A GCP service account has multiple keys that are rotated. Unless the current keyID is specified in the JWT header, you need to iterate the keys in the keystore URL (shown below) to verify the correct one.
If the key used to sign is: ‘cc1080d1a4c61e8cb821331a5a2652dee2c901a1’, you may add on the ‘kid’ header value to the JWT.
Once you have the JWT, you must verify the authenticity of the JWT by verifying against the public certificate. The public certs for any Google Service account is visible at URLs similar to: For serviceAccount B:
The following node sample verfies a self-signed JWT:
You can find the script referenced here on my github page. To use it, you need to download a certificate for service Account A, assign the serviceAccountActor role to it for B, then install the libraries and invoke it with service account B’s ID value
Using IAMCredentials API to issue Access and ID Tokens
Finally, I’ll rework the samples above to use iamcredentials.generateAccessToken() and iamcredentials.generateIDToken() shortly. For now, here is the sample set for that:
Google Cloud community articles and blogs
24 
24 claps
24 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/this-week-in-google-cloud-platform-1-billion-per-quarter-two-new-regions-and-wordpress-63b34a6fc50a?source=search_post---------70,"There are currently no responses for this story.
Be the first to respond.
This past week Google has started disclosing its Cloud revenue and “(Google) says its cloud now brings in $1 billion per quarter” (cnbc.com)
At the same time the investments are not slowing down with two new datacenters :
On the security front, here are the weekly updates :
Assuming security is top of mind for just about everyone, cost is probably a close second which means this detailed post by Romin Irani will come handy for many — “Factors to control and understand your costs” (rominirani.com)
#marketplace: Orbitera and MobileIron team up to make it easier to buy and sell apps in the cloud (Google blog)
From the “BigQuery remains awesome” department :
If you like to take your .NET App migration to the cloud as an opportunity to embrace cloud-native principles, here’s a post about a white paper and github repo to do just that (Google blog)
If you’re curious about Kubernetes history, its release cadence, SIGs, and a lot more, I’d encourage you to read the well-documented “The full-time job of keeping up with Kubernetes”, it covers both the software and the project (gravitational.com)
This week’s GCP Podcast (Episode #112) is a conversation with percy.io creator Mike Fotinakis.
From the “Istio in the trenches” department :
From the “In case you’ve missed it (ICYMI)” department :
This week’s pictures are celebrating our two new datacenters in Montréal and in the Netherlands :
That’s it for this week!-Alexis
Google Cloud community articles and blogs
21 
21 claps
21 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Google Cloud Developer Relations
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/google-cloud-platform-technology-nuggets-august-16-31-2021-edition-730ea8df7dfe?source=search_post---------71,"There are currently no responses for this story.
Be the first to respond.
Welcome to the August 16–31, 2021 edition of Google Cloud Platform- Technology Nuggets.
A friendly reminder that Registration for Google Cloud Next, happening on 12th-14th October, is open.
For anyone working with the cloud, I am sure that you have had requests to look at current costs and try to optimize it. Our latest guide titled Cost optimization through automated VM management, takes you to multiple methods ranging from time-based scheduling to idle recommendations to help start/stop your fleet of VMs and control costs. The interesting part of this article is that it will introduce you to multiple other managed services like Cloud Scheduler, Cloud Functions, etc.
If you are looking for Cost Optimization across all our services, check out our Cost Optimization guide for developers and operators, available in the Architecture Center. It has guides to Cost Optimization across our multiple compute offerings, storage services, data analytics and more.
Google Datasets is an ever-increasing set of databases that have been available to the public for their analytics needs. At last count, there are more than 200+ public datasets that are available ranging from Google Trends, Climate Data and more.
An interesting development is that Google Cloud Platform release notes are also available now as a dataset that you can query across releases, security bulletins and across product lines.
Check out this blog post that highlights some of the recent datasets that have been released.
Our Conversational AI platform, Dialogflow CX, has got a bunch of new features. A couple of features that are particularly interesting and which several customers have requested are private network access to Webhook targets and an ability to stream partial responses. The partial responses feature addresses the situation of getting a timeout from your Webhook after 5 seconds. It presents an elegant way to inform the user about the ongoing fulfillment process. Take a look at the sequence diagram below that shows how instead of waiting for 5 seconds, you present a message “One moment while I look that up” after 2 seconds of the process.
In a big win for Security, Dialogflow now integrates with Service Directory private network access so it can connect to webhook targets inside our customers’ VPC network.
If you have deployed any Google Cloud Project in Production, you would have experienced the challenges of setting up a strong security foundation from the grounds up. This blog post is a detailed guide on various security controls to consider while doing a GCP deployment. It is a go to guide and an absolute must if you are deploying applications in production. It is an exhaustive guide to setting up the organization hierarchy, resource controls, logging, authentication and authorization, billing and more.
As an application developer, you are spoilt for choice when it comes to various managed services available to you like App Engine, Cloud Run, Cloud Functions, etc. A key indicator of application performance is often latency and it can become challenging when there are potentially multiple services invoked to fulfill a customer request. The recommended way to determine this indicator is via an application trace. For our serverless products mentioned above, we have Traces built by default across your request as it goes through every layer of a distributed system, including the load balancer, compute, database, etc. Check out more information on Cloud Trace and how you can use it today to debug your application performance.
In keeping with the tradition to highlight a GCP Sketchnote, this edition presents All you need to learn about Google Cloud Storage (GCS), our object store for data such as images, videos, text files and other file formats. GCS has deep integration and is central to connecting multiple GCP services together and hence a good understanding of this is key to utilizing the range of services across GCP.
How about learning more about storage options on GCP other than Google Cloud Storage? We have several services ranging from SQL, NoSQL and specialized databases that can help with your specific use cases. “Take a look at which database shall I use?”.
Have questions, comments, or other feedback. Do send it across.
Looking to keep a tab on new Google Cloud product announcements? We have a handy page that you should bookmark → What’s new with Google Cloud.
Google Cloud community articles and blogs
49 
49 claps
49 
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/this-week-in-google-cloud-platform-k-is-for-kubernetes-dialogflow-goes-enterprise-and-multi-563a6a6bd67e?source=search_post---------72,"There are currently no responses for this story.
Be the first to respond.
Google’s Dialogflow (né API.AI) now offers an Enterprise Edition for a new way to build voice and text conversational apps. A Dialogflow V2 beta API is also available for testing.
Cloud Spanner now delivers five 9s with Multi-Region support. The database transactions and synchronous replication now span across regions and continents! Straight from the post — “At Google, Spanner powers apps that process billions of transactions per day across many Google services. In fact, it has become the default database internally for apps of all sizes.”
From the “where will you take Cloud Spanner today” department:
From the “Kontainer all the things” department :
From the “data gravity and cost” department :
From the “stream of portability“ department :
From the “Tensorflow, from tiny to humongous” department :
Following up on the recent Cloud DataPrep updates, users now have access to a preloaded sample dataset, in-product step-by-step walkthroughs, and guidance videos (Google blog)
From the “Customers and partners talk best about GCP” department :
What does Istio with GCP look like? The answer lies in this new page (cloud.google.com), a recent presentation by Ray Tsang (youtube.com), and this free codelab (g.co/codelabs).
You may want to add Cloud OnAir to your agenda, an online conference in a couple of weeks (December 6th) on the journey from big data to AI.
This week’s GCP Podcast #103 covered Performance Atlas with Colt McAnlis. If you’ve only watched the videos, you’ve only seen the trailers, make sure to read Colt’s detailed posts on Medium (medium.com)
This week’s picture is from the Dialogflow Enterprise announcement :
That’s it for this week!-Alexis
Google Cloud community articles and blogs
52 
Thanks to Jack Wilber. 
52 claps
52 
Written by
Google Cloud Developer Relations
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Google Cloud Developer Relations
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@SandeepDinesh/moving-ezschool-com-to-google-cloud-platform-bbb845ccbc97?source=search_post---------73,"Sign in
There are currently no responses for this story.
Be the first to respond.
Sandeep Dinesh
Jul 16, 2019·15 min read
A lot of the time, people like myself blog about greenfield apps, best practices, and the latest and greatest cutting edge technology. However, the reality is most real apps are never that easy or that clean.
Many people are not ready to go full-in on cloud-native hype technology, or they just don’t know where to start. The truth is, you can adopt a little at a time where it makes sense for you to do so.
In this blog post, I’ll share the story of how I moved EZSchool.com from a traditional LAMP stack running on a VPS to a containerized system running on GCE and Cloud SQL. I’ll share my thought process and the tradeoffs I had to make in a real world scenario.
TL;DR: EZSchool is a monolithic PHP app with a MySQL database. Ops were challenging due to lack of best practices. Moved self hosted MySQL to Google Cloud SQL to benefit from a managed solution. Dockerized the PHP and Nginx layers to simplify operations. Considered Kubernetes, but found the cost of entry too high. Considered serverless, but found the change in developer workflow too high. Settled on running containers on a single GCE VM controlled with Docker Compose and startup scripts.
Lesson Learned: You don’t have to use cutting edge tools and best practices to start your modernization journey. You can modernize slowly and only the pieces which make sense at the time.
Skip to the Modernization section if you don’t want the long history lesson.
In 1998, my mom launched EZSchool.com. EZSchool was one of the first “cloud-based” educational platforms, where teachers could manage all aspects of a classroom online. From assigning homework, give tests, and ensuring students were on the right track, the vision for EZSchool was ambitious.
Of course, being a bootstrapped startup in the early 2000s meant that the whole company ran out of two servers in the closet. One server ran Oracle DB, and the other ran Java Web Server 1.1 and has two (2!) physical CPU cores. Dual core baby. Of course, we were in the middle of Californina’s rolling blackouts, so we also had a UPS that could power both servers for about 30 minutes before falling over. I vividly remember the noise they would make when activated; a power cut to the database server could potentially corrupt everything so when the UPS activated it was a mad scramble to cleanly shut everything down before it lost all power, even at 2am.
When the dot-com crash happened and funding dried up, all hopes of raising money vanished overnight. It looked like the end, but my mom was determined to push forward even if it would be a one-woman operation.
At this point, it was clear EZSchool wouldn’t have the money to get space and servers in a colo facility, and having servers at home that went down all the time was putting massive strain on everyone. So what was the solution?
Around this time, the concept of shared PHP Hosting was becoming more and more popular. For a few dollars a month, you basically got access to a folder on a server that had Apache and CPanel installed. From here, you could FTP HTML and PHP files over to the folder, and bam you have a running website. Of course, there were tons of limitations, the biggest of which was no database support. This meant all the “cloud based” ideas like an online classroom were cut and EZSchool pivoted to a free content platform (games, worksheets, tutorials, etc) that made money from display ads. These were the days that the online advertising market was taking off, and you could make good money from modest amounts of traffic.
So let’s recap:
This also meant throwing away the existing Java codebase and starting from scratch in PHP.
Eventually, EZSchool got enough traffic that even the biggest shared hosting plan couldn’t support it. Remember, there was no way to isolate customers from each other in this model, it was all based on good faith. Our provider gave two choices, move to a “Virtual Private Server” or leave.
So once again, we had to migrate.
A VPS was a fully isolated Virtual Machine that was provisioned just for you. You got full root access to the machine and could do anything you wanted with it. Unlike shared PHP hosting, the resources for your VPS were pre-provisioned and dedicated for you.
Moving to a VPS had it’s pros and cons. More control means more flexibility, but it also means managing the OS updates and running Apache ourselves. Nothing too hard, especially since my mom was already familiar with Linux and bare metal servers.
Eventually, the little VPS we were using started to fall over as well. The biggest problem was Apache’s habit of spinning up individual processes for each request, which caused huge amounts of overhead. I started reading about a new web server called “nginx” that claimed to be a lot more performant with a lot less overhead. So along with upgrading to a bigger VPS, we moved from Apache to Nginx.
At the end of the day, we now had more computing power than we needed, and could start to do more. At this point, display ad revenue was getting worse and worse, so we decided to let people pay for memberships that would remove advertisements from the website. Because we had a VPS with full control, my mom spun up MySQL, created a PayPal integration, and implemented a membership program.
So let’s recap:
Now we had memberships and a fully functional database, it was time to come full circle and go back to the original vision of a “cloud based” learning platform. Of course by this time, this wasn’t a unique proposition anymore, but it still seemed like the right direction to go. I had just finished college, and I had a summer free before starting a full time job.
Over those three months, we designed a database driven platform, where all content was stored in MySQL or dynamically generated and then rendered by PHP and JavaScript. It was a big success, and over the next few years we moved everything to this model.
This let us remove hundreds of folders and thousands of static HTML and PHP files that were mostly all copies of each other and removed a bunch of manual, repetitive, and error prone work.
However, this “centralization” had some negative side effects. I remember a nasty hours-long outage where we overwrote a critical routing component, forcing us to literally rewrite it from scratch using ‘vim’ on the server, because we had no backups. So we finally adopted Git, yay! Still just pushed everything to master, but it was better than nothing! Another negative was increased resource usage, with more and more functions going through the database and PHP routers.
A huge issue was MySQL would randomly die, probably due to a spike in traffic causing memory issues. I say probably because monitoring consisted of looking at the default CPU/Memory graphs and our nginx logs and squinting really hard.
We also started running into ops issues. The “server setup” guide was a poorly updated Google Doc with commands copied from random blog posts I had found. While this initially worked, the production server had experienced config drift, and was getting more and more out of sync every day.
So let’s recap:
You still with me? Here is where things get interesting.
At this point, I had been working at Google Cloud for some time, and whenever I helped my mom with EZSchool it was like a blast from the ugly past. I knew we could be using a lot of new technology to simplify our lives, but my mom just didn’t have the cycles to learn and implement it.
Lucky for me, our contract with our VPS provider was coming to a close, and we were having some reliability issues with them. So, this was a good opportunity to both modernize and migrate EZSchool to GCP.
There were multiple goals for this project:
I won’t focus much on upgrading to PHP 7, except for saying that the massive improvement in performance over PHP 5 was amazing, and it was mostly backwards compatible with our PHP 5 code.
One decision I made from the start was to move everything into containers. Using containers brings a lot of benefits. Upgrading the version of Nginx or PHP is much easier and safer than using the OS package manager. They also make it easy to add the nginx config into source control, as they can be mounted into the right place in the Nginx container easily. They also make running near identical stacks in prod and dev possible. Restarting everything is just one command, and support for auto-restart makes it even easier.
The first challenge was moving the MySQL server over to Google Cloud SQL. Creating the instance was easy enough, but how would weto move the data over?
One way would be to follow the migration guide in the documentation. This involves setting up our current server as an external master for Cloud SQL, waiting for all data to be replicated over, turning off the app, promoting Cloud SQL to master, and finally restarting the app to point to Cloud SQL.
So it seems pretty complicated, and you still have to experience downtime. The downtime would be super minimal, but it’s a lot of moving pieces. Can we trade off a little more downtime for a simpler migration?
Because we only had a few GB of data, I came up with a much simpler solution.
→ Run mysql-dump to dump the data into a .sql file
→ Compress .sql file with gzip
→ Upload compressed file to Google Cloud Storage with gsutil
→ run ‘ gcloud sql import sql ‘ command to load the data into the database
All in all, these steps took about 5 minutes to finish. Not bad! I then put these four commands into a bash script, and we were good to go.
There are many ways to connect to Cloud SQL from your application. I would highly recommend using the Private IP for better performance and security. On top of that, I would also use the Cloud SQL Proxy to set up the connection.
I like the proxy for a few reasons:
To come up with the new architecture, it was important to understand the current architecture and the issues it had.
Pretty simple design, with some pretty big flaws.
So what were the goals for the new architecture:
Because my day job was working with Kubernetes, the first thought I had was to move everything to Kubernetes. It would then look something like this:
At this point I took a step back and weighed the pros and cons of this new architecture:
Good:
Bad:
Ugly:
It really seemed that Kubernetes was out of the picture for this project. The killer feature of Kubernetes, namely managing multiple microservices at scale, was basically unnecessary. We had a small monolith that really only needed one replica. We would be paying the cost of entry that Kubernetes brings of managing a complex system with none of the benefits.
My next thought was to use something serverless. These days, I would recommend Cloud Run, but that wasn’t an option when I was doing the migration. Google Cloud Functions doesn’t support PHP, so that was out. That left App Engine Standard and App Engine Flex.
App Engine Standard seemed to be an ideal choice. It should dramatically reduce ops work and give us new features like traffic splitting and rolling updates. However, there was a fundamental problem with using App Engine, and that was Nginx. We had thousands of lines of routing rules in the form of HTTP 301 and 302 redirects. built up over years of operation, Unfortunately, App Engine requires rewriting them in a proprietary format which would be very time consuming. We also do some other interesting things in Nginx to get all the routing working the way we want, have a seperate non-public directory for dynamic question generation, and use CIOPFS to make the website case insensitive due to legacy reasons.
App Engine Flexible was out due to the high price, which you can read more about here.
If we were doing the migration these days, Cloud Run would be a great choice. Because it allows us to run arbitrary Docker containers, we have the option to run Nginx and keep all the routing rules and customizations we need. Let’s look at the pros and cons for Cloud Run
Good:
Bad:
Ugly:
Cloud Run seems like a great solution with limited downsides. However, it does require a new dev workflow, which is the biggest hurdle to adoption. It would require a new container to be built and deployed on each change and would remove the ability to SSH into the machine to make hotfixes. Now, both of these things are actually features and good things to have, but it is different and thus harder for my mom to adopt. The biggest thing is it didn’t really exist when I was doing the migration! So I’m going to ignore it for now, but might take another look in a later post.
So is it possible to take the positives from the Kubernetes world and use it without the downsides? We wanted to reduce the ops overhead, but didn’t need scalability. Running on a single VM was fine.
The solution I came up with was to use Docker Compose. Docker Compose gives some of the advantages of using Kubernetes when it comes to using multiple Docker Containers together on a single machine, is super simple to set up, and just requires a simple “docker compose up” to start.
Good:
Bad:
Ugly:
The biggest issue with this setup is that it doesn’t follow best practices. Containers are best when they are “immutable”. This would mean adding all the files into the container image when building it, and every time there is a change you should rebuild and retag the container and do a redeploy. Instead, we are just using a Docker Volume Mount to add in the appropriate Nginx config and the source code. However, I’m not trying to win any awards for the cleanest setup, my goal is to decrease ops toil and this hits the sweet spot perfectly.
Coming up with this exact architecture took a little bit of time. After looking for a combination Nginx/PHP Docker container for quite a while and only finding a bunch of half baked images, I decided to run separate containers for Nginx and PHP and then link them together with Compose.
The PHP container’s Dockerfile is as follows:
These were the options we needed, but obviously your application will be different. Check out the official documentation for more configuration options.
For Nginx and MySQL, the official containers worked without any modification!
Now came the interesting part, using Docker Compose to maintain Dev/Prod parity. I wanted an easy way for the prod service to use Cloud SQL, but dev to use a local MySQL instance.
To do this, I used the extension feature of Compose to have two different MySQL configurations, but a base nginx and php configuration.
The base configuration looked like this:
As you can see, Compose runs two containers with a shared bridge network. This is very similar to a Pod in Kubernetes, which is exactly what I want!
We mount in the config files using volume mounts, and make sure logs are saved outside the container. In an ideal world, we would not be doing these things. Containers work best when they are immutable, so ideally we should be running a build step that adds in all relevant files to the container and tags it with a unique ID. However, this means we need a CI/CD pipeline and more which we don’t have. This is a great topic for a future project. Similarly, logs shouldn’t be written to disk, but rather a logging service like Stackdriver. However, because we only have one host, it was easy to set up fluentd to send logs to stackdriver.
Now for the MySQL configuration. First, we have the local config stored as a file called “docker-compose.override.yaml”
This file is a pretty standard MySQL setup. The only difference is the volume mount, which let’s us pre-populate the database with a snapshot of the data from prod in the form of a .sql file.
For production, we instead use the CloudSQL Proxy. In a file called “docker-compose.prod.yaml”:
In this case, we are using the CloudSQL Proxy docker container instead of the standard MySQL one, and use the command section to configure it to connect to the correct database. You might also notice that we are not exposing the MySQL port anymore, which makes sense. In development, you might want to connect directly to the database, but in production you can use the gcloud cli to connect to Cloud SQL.
Finally, I created a few scripts to automate basic tasks. I created the following Makefile:
Most of the commands are self-explanatory. The sync-db command is neat as it creates a new sql backup using gcloud, then copies it into the mysql_init folder using gsutil. This means you can get a fresh snapshot for local use by running “make sync-db start-local-server”
We pulled the code onto the new GCE server, and everything worked! Of course, I am glossing over the changes we had to make here and there due to slight differences in configuration and moving to PHP7, but all in all it was very minimal.
The last part was creating a startup script that would automatically start the containers on boot.
Now that everything was running, it was time to migrate. Like I said before, we were willing to deal with some downtime. So we SSH’d into the old server and stopped Nginx, ran the db migration script I had previously created, and then switched DNS to point to the new server. All in all, it took just 5 minutes!
At the end of the day, this was a great lesson in finding low hanging fruit. Did we follow all the best practices? Did we create the most optimized infrastructure? Do we have amazing automation and autoscaling? No, no and no. What we did do is cut our costs, increase our uptime, simplify ops, and establish dev/prod parity. I call that a job done well enough :P
Future Posts:
Originally published at https://blog.sandeepdinesh.com on July 15, 2019.
See all (155)
45 
45 claps
45 
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/lemmings/lemmings-partners-with-amazon-web-services-and-google-cloud-platform-ed6477e6a73d?source=search_post---------74,"There are currently no responses for this story.
Be the first to respond.
A few days ago Google’s AI project AlphaGo Zero beat the world’s leading chess engine (Stockfish) after just four hours of training on Google’s Tensor Processing Units (TPUs). Just one of the many examples of how a generic machine learning approach can within hours yield better performance than specialized software that got fine-tuned over many years.
Yet most fascinating to me is that the technology behind extraordinary achievements like these is not behind a walled garden. It is readily available to all of us.
Most of the leading machine learning tools are open source and well documented. The data needed for machine learning is more accessible than ever before and even the hardware used is becoming ever more cost effective. We all can play around with the same ingredients that big players like Google, Amazon, Facebook and Apple do.
For early stage teams this is amazing. If you have an idea, playing around with it and getting some results is fast and there are few barriers. Machine learning is an open field and there are countless problem domains to apply it to in addition to chess.
While the recent technological shift was largely about making services more accessible by making sure they are also available on mobile (“mobile first”) we are now in a shift focused on making more sense of existing data by using machine learning (“artificial intelligence first”).
“In an AI-first world we are rethinking all our productsand apply machine learning and AI to solve user problems.”
— Sundar Pichai
Having access to machine learning tools as well as to data and hardware is great but for very early stage teams (pre-product, pre-market, pre-funding …) there is still a huge barrier: infrastructure cost.
While it only takes a few hours to run an experiment similar to AlphaGo Zero’s chess training it can be prohibitively expensive for an early stage team to do so unless you have a few thousand dollars to burn.
This puts early stage teams at a disadvantage.
Imagine an ambitious painter. She is pouring all her heart into filling one canvas after the other. Yet every canvas, every brush and every pot of paint is incredibly expensive. It puts a lid on what she can express. Every mistake is costly. Every idea gets scrutinized. Every stroke is restrained.
What could the painter do if canvases, brushes, paint and storage would not only be readily available but free? What if her studio would automagically scale with her efforts? What could she do?
We were asking ourselves what could Lemmings do if their imagination would not be artificially constrained by infrastructure cost. If they would not have to restrain their thoughts to the GPUs in their laptops? What if they would not have to worry about purchasing hardware that becomes obsolete within weeks?
I’m glad that we are not the only ones who are curious to see what ambitious teams can do with machine learning on modern infrastructure.
We are partnering with Amazon and Google to provide every Lemmings team with USD 100.000 for Amazon Web Services as well as with USD 100.000 for the Google Cloud Platform.
On top of this all of our teams get access to first class 24 / 7 supportas well as access to training directly provided by each platform partner.
This not only means massive infrastructure and tech supportcost reductions for our early stage teams.
It means unleashing the mind.
https://aws.amazon.com/blogs/aws/new-amazon-ec2-instances-with-up-to-8-nvidia-tesla-v100-gpus-p3/
https://aws.amazon.com/blogs/ai/new-aws-deep-learning-amis-for-machine-learning-practitioners/
https://chess.stackexchange.com/questions/19366/hardware-used-in-alphazero-vs-stockfish-match
https://news.ycombinator.com/item?id=15556789
https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/alphago-zero-goes-from-blank-slate-to-grandmaster-in-three-dayswithout-any-help-at-all
https://cloud.google.com/blog/big-data/2017/05/an-in-depth-look-at-googles-first-tensor-processing-unit-tpu
https://news.ycombinator.com/item?id=15869083
https://cloudplatform.googleblog.com/2017/09/introducing-faster-GPUs-for-Google-Compute-Engine.html
https://cloudplatform.googleblog.com/2017/11/new-lower-prices-for-GPUs-and-preemptible-Local-SSDs.html
https://www.nvidia.com/en-us/data-center/dgx-1/
http://images.nvidia.com/content/technologies/volta/NVIDIA-Volta-GPU-Architecture_The-Future-of-AI.pdf
https://en.wikipedia.org/wiki/Tensor_processing_unit
Incubator focused on Art and Artificial Intelligence
287 
287 claps
287 
Written by
Founder at Magic, Lemmings, Blossom …
Incubator focused on Art and Artificial Intelligence
Written by
Founder at Magic, Lemmings, Blossom …
Incubator focused on Art and Artificial Intelligence
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/rd-shipit/como-a-migra%C3%A7%C3%A3o-para-o-google-cloud-platform-em-2018-potencializou-o-crescimento-da-rd-station-8e2fdf744427?source=search_post---------75,"There are currently no responses for this story.
Be the first to respond.
A RD Station nasceu em 2011 como um produto SaaS (Software as a Service), 100% em cloud que tinha como base uma PaaS (Platform as a Service) chamada Heroku. O Heroku ajudou a ganhar muita velocidade e a otimizar boa parte do pipeline de desenvolvimento. Essa solução oferecia desde deploy com apenas 1 comando e gestão simplificada em produção, até um marketplace que conectava múltiplos serviços (por exemplo monitoramento e bancos-como-serviço), eliminando a necessidade de termos times dedicados de SRE e DevTools, como temos hoje. Sim, acredite, essas eram “inovações” na época :)
O crescimento do nosso produto e do número de clientes foi exponencial, porém ali por volta de 2015 percebemos que a solução que tínhamos não atendia mais as nossas necessidades. Queríamos customizar nosso uso para ter mais controle da operação e estruturar melhor nossos serviços, entretanto, isso não era transparente e nem muito configurável. Decidimos então migrar para uma IaaS (Infrastructure as a Service), que ao mesmo tempo que traria mais controle, também exigiria mais desenvolvimento e governança do nosso lado.
Em 2016 testamos pelo menos dois provedores conhecidos do mercado usando uma parcela dos seus serviços, entretanto nunca ficamos completamente satisfeitos (nossa régua é bem alta, confesso). Acontece que ou a solução era boa no aspecto técnico (ex: diversificação de serviços, escalabilidade, disponibilidade), ou era boa apenas no atendimento (suporte ativo, capacidade de resolução de problemas). Depois de muitos e muitos testes, decidimos usar o Google Cloud Platform como nosso principal provedor de serviço na nuvem (cloud), e de lá pra cá essa foi a melhor decisão que tomamos.
A seguir vou os critérios que utilizamos nessa tomada de decisão e também os aprendizados que tivemos até aqui. Espero que esse conteúdo te ajude a tomar uma boa decisão quando for necessário fazer uma escolha parecida.
Antes de tudo é preciso entender que uma decisão sobre um fornecedor cloud não é meramente por requisitos técnicos. Toda parte técnica, serviços disponíveis, investimento de P&D, capacidade e histórico de crescimento, performance, etc, é sim super importante (na verdade ela é fundamental, pois sem isso pouco importa olhar para o resto). Porém, os fornecedores atuais já entregam de forma parecida boa parte dessas coisas, quase como um celular, que você não escolhe por “fazer ligação ou não” pois isso já é esperado. Ele precisa entregar mais do que o esperado.
No entanto, o restante também é importante. No que tange isso, gosto de pensar em ao menos mais 3 itens: custo, suporte e valor ao negócio.
O fato é que o custo que sua solução tem para rodar, impacta diretamente na sua margem. Quanto menor seu custo, melhor sua margem, simples. Se você consegue crescer sua base de clientes descolada do custo de cloud, excelente, você tem uma solução eficiente. É esperado que seu COGS (custo de infra para servir seu produto por cliente), diminua proporcionalmente com o tempo.
Para isso, é importante contar com um provedor que tenha boa base de preço, ofereça descontos fixos em máquinas, possua cobrança local com menor impostos (ou hedge de dólar), e que tenha cupons para migração ou atenuação dos custos. Dica bônus: vale revisar a infra-estrutura para consumir menos e também negociar bons contratos. Você precisará negociar isso, porém só conseguirá quando atingir um tamanho mais relevante.
Por fim, tome cuidado com fornecedores menores ou menos estruturados, já vi empresas mudarem seu empacotamento de serviços e preços sem critério, afetando diretamente seus clientes e exigindo migrações a toque de caixa.
Com relação ao suporte, ao escalar nosso negócio aprendemos que é importante ter ao lado um parceiro estruturado, que possa não só ajudar a evitar problemas (por exemplo, através de serviços para evolução da arquitetura, revisão de custos etc), como também dar um suporte extra durante incidentes. Dependendo da configuração do seu time, a língua pode ser uma barreira também.
Em nossa experiência vimos que alguns fornecedores cloud não estavam preparados para oferecer esse suporte mais próximo pois atuavam em um modelo mais “faça você mesmo”, em que o cliente adotava no início da jornada empreendedora, mas depois não tinham estrutura para acompanhar. Crescer e escalar muito sem isso pode ser um risco.
Custo e valor são conceitos diferentes, porém nem sempre um provedor de serviço na nuvem consegue adicionar valor ao seu negócio, exceto quando ele ajuda seu negócio a crescer mais ou melhor. “Crescer mais” pode acontecer através da construção de novos canais ou bizdev de produtos, já “crescer melhor” é, por exemplo, quando esse fornecedor consegue potencializar o desenvolvimento das pessoas que fazem parte da sua equipe técnica. Em ambos os cenários, essa parceria agrega valor no serviço que você vende, seja em resultados institucionais, seja em capital intelectual.
Quando adotamos o GCP, há 3 anos, entendi que a proposta de valor deles era resolver de forma mais estruturada todos os pontos listados anteriormente.
Na pandemia, em 2020, pude também confirmar toda parceria que estávamos construindo e o respeito que tratam seus clientes. Diante do cenário nebuloso dos primeiros meses, onde estávamos entendendo diariamente tudo que estava acontecendo e ajudando nossos clientes com fluxo de caixa (em especial setores impactados e empresas com pouca visibilidade), o Google se mostrou um grande parceiro ao congelar o dólar para evitar ainda mais problemas. Isso reforçou bastante sua cultura e abertura pra conversar.
Recentemente fizemos uma nova negociação com GCP na ordem de dezenas de milhões, que nos reforça como um dos principais clientes do Brasil. Seguimos com nosso uso intenso, tanto no aspecto de quantidade de serviços, como no volume de dados, e suporte muito próximo. Em paralelo também estamos desenhando programas de desenvolvimento em conjunto, que irão auxiliar nossa equipe internamente a contar com toda a expertise do time deles.
Estou super animado com nosso momento e tudo que está por vir! O fortalecimento dessa parceria com o Google é uma possibilidade a mais de desenvolvimento para nosso time aprender com uma empresa referência mundial. E se você quiser conhecer mais sobre nosso modelo de trabalho e cultura, confira nossas oportunidades de carreira. Temos vagas para diferentes produtos e estágio de carreira.
Conteúdo, opinião, vivência e compartilhamento de ideias da…
42 
2
42 claps
42 
2
Written by
Co-founder / CTO @ResDigitais. Endeavor Entrepreneur. Angel. Dad.
Conteúdo, opinião, vivência e compartilhamento de ideias da equipe de Produto e Engenharia da RD Station @RD
Written by
Co-founder / CTO @ResDigitais. Endeavor Entrepreneur. Angel. Dad.
Conteúdo, opinião, vivência e compartilhamento de ideias da equipe de Produto e Engenharia da RD Station @RD
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@maxxsh/website-on-google-cloud-platform-221d79d69976?source=search_post---------76,"Sign in
There are currently no responses for this story.
Be the first to respond.
Max Shestov
Jan 25, 2021·3 min read
This guide shows you how to set up Apache and PHP on a Compute Engine virtual machine instance on GCP. The instructions are for Ubuntu 20.04 Linux distribution (goes with PHP 7.4 in its default repositories).
Go to your Google Cloud Console and clickCompute Engine > VM Instances > CreateCreate a name for your instanceRegion — us-central1(Iowa)Zone — us-central1-c
[Update] Google introduced E2-micro Free Tier, which is a part of a second-generation VM family on August 1, 2021 Series - E2Machine type - e2-micro (2 vCPU, 1 GB memory)
Boot disc - Standard persistent disk, Image Ubuntu, 20.04 LTS (LTS — Long Term Support)
Firewall - checkmarkAllow HTTP trafficAllow HTTPS traffic
Click Create.It takes a few seconds to create the instance.
On your instance click SSH to open the SSH windowIn the opened window update packages that need upgrading typing
Now, install Apache
and install PHP and the extensions with the apt package manager
Verify PHP Version
In Google Cloud console on your instance copy external IP and type it in a browser to verify that Apache is running:
Alternatively, you can install a LAMP (Linux, Apache, MySQL, PHP) stack by Google Click to Deploy automatically.
If you don’t have a connection to your instance check this guide on how to connect to Google Cloud VM Instance.
In the Cloud Console clickVPC Network > External IP addressesIn the column Type of your instance select Static
Create an A record in your DNS Management console and copy/paste your external IP address.
Create a CNAME record for WWW subdomain and point it to your naked domain.
It usually takes effect within a few next hours.
Read this article to install auto-renewing Let’s Encrypt SSL Certificate using automated client Certbot.
Lead Developer at Reverence Global. Husband of a wonderful wife, Entrepreneur, Dad
See all (53)
369 
369 claps
369 
Lead Developer at Reverence Global. Husband of a wonderful wife, Entrepreneur, Dad
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/velotio-perspectives/a-primer-on-http-load-balancing-in-kubernetes-using-ingress-on-google-cloud-platform-d45108f90ff1?source=search_post---------77,"There are currently no responses for this story.
Be the first to respond.
Containerized applications and Kubernetes adoption in cloud environments is on the rise. One of the challenges while deploying applications in Kubernetes is exposing these containerized applications to the outside world. This blog explores different options via which applications can be externally accessed with focus on Ingress — a new feature in Kubernetes that provides an external load balancer. This blog also provides a simple hand-on tutorial on Google Cloud Platform (GCP).
Ingress is the new feature (currently in beta) from Kubernetes which aspires to be an Application Load Balancer intending to simplify the ability to expose your applications and services to the outside world. It can be configured to give services externally-reachable URLs, load balance traffic, terminate SSL, offer name based virtual hosting etc. Before we dive into Ingress, let’s look at some of the alternatives currently available that help expose your applications, their complexities/limitations and then try to understand Ingress and how it addresses these problems.
There are certain ways using which you can expose your applications externally. Lets look at each of them:
You can expose your application directly from your pod by using a port from the node which is running your pod, mapping that port to a port exposed by your container and using the combination of your HOST-IP:HOST-PORT to access your application externally. This is similar to what you would have done when running docker containers directly without using Kubernetes. Using Kubernetes you can use hostPort setting in service configuration which will do the same thing. Another approach is to set hostNetwork: true in service configuration to use the host’s network interface from your pod.
Limitations:
Kubernetes services primarily work to interconnect different pods which constitute an application. You can scale the pods of your application very easily using services. Services are not primarily intended for external access, but there are some accepted ways to expose services to the external world.
Basically, services provide a routing, balancing and discovery mechanism for the pod’s endpoints. Services target pods using selectors, and can map container ports to service ports. A service exposes one or more ports, although usually, you will find that only one is defined.
A service can be exposed using 3 ServiceType choices:
Limitations:
Endpoints are usually automatically created by services, unless you are using headless services and adding the endpoints manually. An endpoint is a host:port tuple registered at Kubernetes, and in the service context it is used to route traffic. The service tracks the endpoints as pods, that match the selector are created, deleted and modified. Individually, endpoints are not useful to expose services, since they are to some extent ephemeral objects.
If you can rely on your cloud provider to correctly implement the LoadBalancer for their API, to keep up-to-date with Kubernetes releases, and you are happy with their management interfaces for DNS and certificates, then setting up your services as type LoadBalancer is quite acceptable.
On the other hand, if you want to manage load balancing systems manually and set up port mappings yourself, NodePort is a low-complexity solution. If you are directly using Endpoints to expose external traffic, perhaps you already know what you are doing (but consider that you might have made a mistake, there could be another option).
Given that none of these elements has been originally designed to expose services to the internet, their functionality may seem limited for this purpose.
Traditionally, you would create a LoadBalancer service for each public application you want to expose. Ingress gives you a way to route requests to services based on the request-host or path, centralizing a number of services into a single entry-point.
Ingress is split up into two main pieces. The first is an Ingress resource, which defines how you want requests routed to the backing services and second is the Ingress Controller which does the routing and also keeps track of the changes on a service level.
The Ingress resource is a set of rules that map to Kubernetes services. Ingress resources are defined purely within Kubernetes as an object that other entities can watch and respond to.
Ingress Supports defining following rules in beta stage:
When no host header rules are included at an Ingress, requests without a match will use that Ingress and be mapped to the backend service. You will usually do this to send a 404 page to requests for sites/paths which are not sent to the other services. Ingress tries to match requests to rules, and forwards them to backends, which are composed of a service and a port.
Ingress controller is the entity which grants (or remove) access, based on the changes in the services, pods and Ingress resources. Ingress controller gets the state change data by directly calling the Kubernetes API.
Ingress controllers are applications that watch Ingresses in the cluster and configure a balancer to apply those rules. You can configure any of the third-party balancers like HAProxy, NGINX, Vulcand or Traefik to create your version of the Ingress controller. Ingress controller should track the changes in ingress resources, services, and pods and accordingly update the configuration of the balancer.
Ingress controllers will usually track and communicate with endpoints behind services instead of using services directly. This way some network plumbing is avoided, and we can also manage the balancing strategy from the balancer. Some of the open-source implementations of Ingress Controllers can be found here.
Now, let’s do an exercise of setting up an HTTP Load Balancer using Ingress on Google Cloud Platform (GCP), which has already integrated the ingress feature in its Container Engine (GKE) service.
Ingress-based HTTP Load Balancer in Google Cloud Platform
The tutorial assumes that you have your GCP account setup done and a default project created. We will first create a Container cluster, followed by deployment of a nginx server service and an echoserver service. Then we will set up an ingress resource for both the services, which will configure the HTTP Load Balancer provided by GCP
Get your project ID by going to the “Project info” section in your GCP dashboard. Start the Cloud Shell terminal, set your project id and the compute/zone in which you want to create your cluster.
Fetch the cluster credentials for the kubectl tool:
Create a Service resource to make the nginx and echoserver deployment reachable within your container cluster:
When you create a Service of type NodePort with this command, Container Engine makes your Service available on a randomly-selected high port number (e.g. 30746) on all the nodes in your cluster. Verify the Service was created and a node port was allocated:
In the output above, the node port for the nginx Service is 30746 and for echoserver service is 32301. Also, note that there is no external IP allocated for this Services. Since the Container Engine nodes are not externally accessible by default, creating this Service does not make your application accessible from the Internet. To make your HTTP(S) web server application publicly accessible, you need to create an Ingress resource.
On Container Engine, Ingress is implemented using Cloud Load Balancing. When you create an Ingress in your cluster, Container Engine creates an HTTP(S) load balancer and configures it to route traffic to your application. Container Engine has internally defined an Ingress Controller, which takes the Ingress resource as input for setting up proxy rules and talk to Kubernetes API to get the service related information.
The following config file defines an Ingress resource that directs traffic to your nginx and echoserver server:
To deploy this Ingress resource run in the cloud shell:
Find out the external IP address of the load balancer serving your application by running:
Use http://<external-ip-address> and http://<external-ip-address>/echo to access nginx and the echo-server.
Ingresses are simple and very easy to deploy, and really fun to play with. However, it’s currently in beta phase and misses some of the features that may restrict it from production use. Stay tuned to get updates in Ingress on Kubernetes page and their Github repo.
*****************************************************************
This post was originally published on Velotio Blog.
Velotio Technologies is an outsourced software product development partner for technology startups and enterprises. We specialize in enterprise B2B and SaaS product development with a focus on artificial intelligence and machine learning, DevOps, and test engineering.
Interested in learning more about us? We would love to connect with you on ourWebsite, LinkedIn or Twitter.
*****************************************************************
Thoughts and ideas on startups, enterprise software &…
29 
1
29 claps
29 
1
Written by
Velotio Technologies is an outsourced software and product development partner for technology startups & enterprises. #Cloud #DevOps #ML #UI #DataEngineering
Thoughts and ideas on startups, enterprise software & technology by the Velotio team. Learn more at www.velotio.com.
Written by
Velotio Technologies is an outsourced software and product development partner for technology startups & enterprises. #Cloud #DevOps #ML #UI #DataEngineering
Thoughts and ideas on startups, enterprise software & technology by the Velotio team. Learn more at www.velotio.com.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/google-cloud-platform-tech-nuggets-june-2021-eb54079b31fe?source=search_post---------78,"There are currently no responses for this story.
Be the first to respond.
Welcome to Google Cloud Platform Technology Nuggets. We look at all the announcements in the previous month and highlight a few articles to read along with our own commentary.
Updates from Google I/O
Our annual developer event Google I/O was held on May 18–20 and saw a great set of announcements ranging from Android, WearOS, Flutter, Tensorflow and of course Google Cloud. Specifically speaking of Google Cloud, there were announcements in areas of AI, Serverless and Google Workspace. You can read the summary but more specifically, check out the Cloud Developers guide to I/O that contains links to the sessions and workshops, which are now available on-demand. If you’d like to try a hands-on lab, feel free to go for any of the Cloud codelabs from the event.
Customer Stories
Our Customer stories this month focuses on how 6 customers chose to run their SAP Workloads on Google Cloud Platform with the transformation and benefits that they are seeing today.
Check out our SAP solution site for more information on SAP on GCP and our customers.
Anthos Updates
We have published a number of articles on Anthos and we have aggregated them for you in an easy to reference blog post, titled Anthos in depth, that covers our articles published in our hybrid and multicloud development series. An interesting article that we want to highlight here tackles a common situation once you’ve bought Anthos. What do you do next to build that momentum? We cover six initiatives to start your Anthos Day-1 journey.
If you are looking to get started with learning Anthos, check out our Anthos 101 series.
Security Updates
Cloud Armor, our Distributed Denial of Service (DDoS) protection and Web-Application Firewall (WAF) service on Google Cloud, saw an update with the introduction of Google Cloud Armor Managed Protection Plus. The updates include: A machine learning powered service to detect Layer 7 attacks, Curated rules to simplify the deployment of effective access controls for your applications , a 24/7 DDoS Response support and DDoS Bill Protection, where customers will be able to open a claim to receive a credit in the amount of the bill spike due to a DDos attack.
Do check out our post that lists down best practices to protect your organization against ransomware attacks. In this article, we list down 5 pillars of protection (Identify, Protect, Detect, Respond and Recover) as identified by NIST in their Cybersecurity Framework and give you information on how various Google Cloud Services help you to achieve that.
Google Cloud has also published CISO Perspectives: May 2021, that provides a good roundup of cloud security and industry highlights.
Big Announcements from Data Cloud Summit
The Data Cloud Summit, held on May 26, brought together leading companies to share how they are using Google Cloud to build their data clouds. The summit saw key new product announcements and updates to multiple services. You can check out the list of announcements here. We’d like to highlight a few here:
We also announced Vertex AI, an AI Platform that simplifies the process of building, training, and deploying ML models at scale.
The sessions from Data Cloud Summit are available on-demand.
Bonus : A handy guide to product offerings across Cloud Service Providers and Compute Engine #Sketchnotes
We have a handy new guide to product offerings across Google Cloud, AWS and Azure. This is a revamped version of our earlier document but this time with the ability to filter in a specific area that you are looking at, e.g. Compute. Check out the blog post, which lists other useful resources like 2-minute guide to various GCP services and describing each Google Cloud Product in 4 words or less.
Our core compute offering, Compute Engine, gets its own sketchnotes. Learn about this service, key use cases, pricing and how it works.
Stay in Touch!
Have questions, comments, or other feedback. Do send it across.
Looking to keep a tab on new Google Cloud product announcements? We have a handy page that you should bookmark → What’s new with Google Cloud.
Google Cloud community articles and blogs
26 
26 claps
26 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/google-cloud-platform-technology-nuggest-august-1-15-2021-edition-9d053196e4ad?source=search_post---------79,"There are currently no responses for this story.
Be the first to respond.
Welcome to the August 1–15, 2021 edition of Google Cloud Platform- Technology Nuggets.
A friendly reminder that Registration for Google Cloud Next, happening on 12th-14th October, is open.
What does Application Modernization look like inside of Google? Check out this blog post, on how Google teams moved vendor managed applications, Confluence and Acronix from Compute Engine Virtual Machines to Anthos. The post highlights the pain points of running these applications on VMs and the benefit of moving them to Anthos.
We have published a number of self-service guides that help to migrate to Google Cloud. These guides are categorized into different workloads to migrate (Microsoft, VMware, SAP) to general migrations around storage and databases.
The recent GA release of our Database Management Service made it easier to migrate your SQL databases to Cloud SQL. But did you know that you could potentially use this service to even do a major version upgrade of your database? Check out this blog post.
If you are looking to deep dive into Big Query, there is a fantastic series titled “Big Query Admin Series” that has published multiple posts already starting with Resource Hierarchy to the latest post on Data Governance. Check out the deep dive articles published so far and bookmark this series for ready reference:
Speaking of Analytics, every organization wants to tap into data insights. While that is the goal that organizations want, there are still fundamental questions that arise when you build out an entire strategy for building out an analytics pipeline. Is it a Data Lake or a Data Warehouse? What tools should we be using? What if we want to model and run all stages of a typical data pipeline from Ingestion right to visualization and inference. We have written a paper that shows what it makes to build out a Unified Platform for Analytics. Check it out.
OWASP Top 10 is a list by the Open Web Application Security (OWASP) Foundation of the top 10 security risks that every application owner should be aware of. Check out this detailed guide from our Architecture section, that covers various Google Cloud Security Products and which specific OWASP Top 10 risk that it helps to mitigate.
This is a great way to understand not just the risk but to map it to the specific GCP security product. For example, Google Cloud Armor, our DDoS and WAF service, is shown below vis-a-vis the Top 10 risks and you can click on each of the risks (tick mark) to see what Google Cloud Armor does.
In another win for User Experience, we have integrated several in-context observability metrics right within the Compute Engine VM details page. This helps to troubleshoot and monitor key metrics while investigating the VM itself rather than switching context across multiple UIs and tools.
You can now click into any VM and get a rich set of visualizations acrossCPU, Disk, Memory, Networking, and live processes.
Are you looking to learn more about the GCP story around IOT and what services it provides to help you launch your next IoT product powered by GCP? Take a look at this technical note that helps you understand the core components behind IoT Core.
Finally, the first-ever Google Cloud Startup Summit will be taking place on September 9, 2021 and we encourage you to register for the same. Check out the published agenda for the event.
Have questions, comments, or other feedback. Do send it across.
Looking to keep a tab on new Google Cloud product announcements? We have a handy page that you should bookmark → What’s new with Google Cloud.
Google Cloud community articles and blogs
86 
86 claps
86 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/javarevisited/5-best-gcp-associate-cloud-engineer-certification-courses-in-2021-c93d7e35228a?source=search_post---------80,"There are currently no responses for this story.
Be the first to respond.
Hello guys, If you are looking to pursue a career as a Google Cloud Engineer in 2021 or merely want to acquire a cloud certificate to add to your colorful resume, then taking the Google Cloud Engineer exam might be just the right thing for you.
Google Cloud Platform or GCP is one of the top 3 public cloud providers along with AWS and Microsoft Azure and the demand for certified Google cloud professionals is growing exponentaitonal because of increased adoption of the Google Cloud Platform.
If you already have a fair amount of experience, then you may want to go for the Professional Cloud exam. But if you are a newbie, or looking to add more certifications to add your CV then the Associate Cloud Engineer exam is the way to go.
This exam tests your ability to set up a cloud solution, plan, configure, deploy, and implement it. And it also checks the ability to configure access and security, ensuring the successful operation of a cloud solution with Google Cloud Platform services and technologies. The Google Cloud Engineer exam lasts exactly two hours and is available in Japanese, Spanish, Indonesian, and English. The registration fee is approximately 125 US dollars and can be taken remotely or in person at a test center. Apart from checking the syllabus and exam guide about Google Cloud Engineering, you should also join online training courses to prepare well before you take the exam. It is better to have more than six months of background knowledge and hands-on experience but that’s optional and not really a requirement for an exam. If you don’t have practical experience working with Google cloud you can still join these courses and practice in their hands-on lab. The Coursera GCP training course (second course in this list) has a partnership with Quiklabs for practicing on Google cloud and it just likes working in the real world. Google even has the Google Cloud Free Tier of few products for monthly use.
coursera.com
Without wasting any more of your time, here is the best course you can take to prepare well for the Google Cloud Professional Associate Cloud Engineer certification exam. These courses will help you to better prepare for different topics and keeping the exam pattern in mind.
This is one of the best courses to prepare for Google Cloud Associate engineer certification on Udemy. This course has almost 60,000 students enrolled and a rating of 4.2 out of 5.0 with an impressive success rate. It prepares students for the Cloud Engineer Certification by taking the basics of the Cloud and Google cloud platform. Because this course only covers the Associate Cloud Engineer exam, most of the sessions are focused on more theory and less practical application. Although most users are satisfied with the course, some students commented that most of the sections were repeated, making it bulkier than it would initially have been. However, most people have more positive than negative reviews, so we feel the course is worth checking out.
Here is the link to join this online course — Ultimate Google Certified Associate Cloud Engineer 2021
This is another awesome Udemy course for Google Cloud Assocaiten Engineer certification. The best thing about this course is that it's prepared by none other than Dan Sullivan, the guy who wrote the Official Certification Guide for Google.
This course is designed to help prepare you for the Google Associate Cloud Engineer exam by introducing all of the major GCP services covered on the test, including Compute Engine, App Engine, Kubernetes Engine, Cloud Functions, Cloud SQL, BigQuery, Bigtable, VPCs, VPNs, Stackdriver, and more.
In this course, you will learn how to:
The Google Associate Cloud Engineer exam is two hours in length and contains 50 multiple-choice and multiple-select questions. This course uses demonstrations as well as lectures to ensure you know how to work with GCP and understand its key design and operational concepts.
Here is the link to join this awesome course — Google Associate Cloud Engineer: Get Certified 2021
And, if you like to read the books and study guides then you can also check out the Official Google Cloud Certified Associate Cloud Engineer Study Guide from Amazon by Dan Sullivan.
www.amazon.com
This course is completely free and has more than 22,580 students already enrolled. It offers lessons on skills needed to perform a cloud engineering role and prepare for the Cloud Engineer certification.
It also teaches about the infrastructure and platform services provided by Google Cloud Platform with a basic and advanced understanding of the purpose and intent of the Associate Cloud Engineer certification and its relationship to other Google Cloud certifications.
Here is the link to join Course — Cloud Engineering with Google Cloud Professional Certificate
By the way, If you are planning to join multiple Coursera courses or specializations then consider taking a Coursera Plus subscription which provides you unlimited access to their most popular courses, specialization, professional certificate, and guided projects. It cost around $399/year but its completely worth your money as you get unlimited certificates.
coursera.com
The most amazing thing about this course that makes it stand out from other courses is that it’s offered by Google Cloud. This means that students will get a little bit of everything with enough material to pass the exam. This course takes approximately 7 days to take and guides students in familiarizing themselves by covering the structure and format of the examination, as well as its relationship to other Google Cloud certifications.
This course will make you a better Google Cloud Engineer, especially if it is your first time and you have no prior experience. However, students still need additional materials and study plans to ace the exams. Fortunately, this course offers these materials and platforms.
Here is the link to join this GCP course — Preparing for the Google Cloud Associate Cloud Engineer Exam
Btw, you would need a Pluralsight membership to access this course which costs around $29 per month or $299 per year(14% discount) but gives access to their 5000+ online courses on various cloud certifications, software development, coding, and other technical topics. I highly suggest programmers have Pluralsight membership because as a programmer and IT professional you need to constantly learn and update yourself. Even if you don’t have Pluralsight membership, you can check their 10-day free trial to access this course for free.
pluralsight.pxf.io
Apart from being one of the best sellers on Udemy with a 30-day money-back guarantee, this course also offers a hands-on experience with Google Cloud Platform (GCP) and aims at helping students pass the ACE exam and become a Google Certified Associate Cloud Engineer. It has about 25,289 students enrolled and does not require any background knowledge to take it. It also will give you all the basics you need to study other skills based on the Google Cloud Platform. So if you want to expand your skills, then this course is for you.
The course is also interactive and active so you must be willing to commit and learn. If you are very busy and cannot commit fully then you might want to skip this one.
Here is the link to join this Google cloud course — Google Certified Associate Cloud Engineer Certification
This course is a combination of four Google cloud certifications, namely Associate Cloud Engineer, Cloud Architect, Cloud Developer, and Network Engineer. It has a rating of 4.2 and about 66,000 students enrolled.
It covers beginner to advanced levels, but you don’t need any prior programming or coding experience to keep up. However, the most important thing to remember is that this course is intense. You can decide to take all four aspects of Google clouds or you can just go straight up to Google cloud engineer. It starts the course from scratch and will help you learn about Google Cloud Platform and Certification.
Here is the link to join this GCP course — Ultimate Google Cloud Certifications: All in one Bundle
This is another awesome interactive course to prepare and learn about Google Cloud Platform from Educative, a text-based, interactive online learning portal.
This course will not only help you to hone your cloud computing skills but also to ace your certification exam to help you stand apart.
Even if you’re an AWS user, you will find this course valuable and easy to pick up considering their similarities. All the topics and resources required to pass the Associate Cloud Developer certification are provided in the course, so you can think of it as a cheat sheet where all the information is in one place.
You’ll learn about the many services GCP offers and at the end, you will take a timed practice exam with 50 questions. This will give you the confidence you need to ace your exam.
Here is the link to join this course — Cracking the Google Cloud Associate Cloud Engineer Certification
And, if you find the Educative platform and their interactive courses useful in learning new tech or preparing for coding interviews then consider getting Educative Subscription which provides access to their 250+ courses for just $14.9 per month. It’s very cost-effective and great for preparing for coding interviews.
www.educative.io
This is another awesome resource for people preparing for Google Cloud Associate Cloud Engineer Exam. This practice test is prepared by Dan Sullivan the guy who wrote the official certification guide and author of the second course in this list.
Dan Sullivan is the author of the Official Study Guides for the Cloud Engineer, Architect, and Data Engineer exams and this practice test contains two tests with 50 questions to math the real exam format where you need to solve 50 questions in 2 hours.
You should take this practice test as part of your final preparation and build your speed and accuracy. You can also use this to gauge your preparation level and find your strong and week areas.
Here is the link to join this practice test — Google Cloud Associate Cloud Engineer Practice Exams
That’s all about the best courses to pass the Google Cloud Platform Associate Cloud Engineer certification exam. These courses cover most of the exam topics as given in the exam guide and good to learn both the GCP platform as well as to pass the certification. Whichever option you choose, committing to a course can give help you learn a new program or keep your Google Cloud skills sharp. Google even has the Google Cloud Free Tier of few products for monthly use.
Cloud computing technologies (like GCP) have become a fundamental requirement for most organizations as these technologies are relatively easy to use and maintain, they scale with your business, and they are affordable.
Cloud engineering is also an extremely hot market and is pretty much foundational to every industry. In order to be a cloud engineer, you have to distinguish yourself in some way. That’s where a cloud certification comes in and by cracking the Google Cloud Platform Associate Cloud Engineer exam, you will be one step ahead of your competition.   Other IT and Cloud Certification Articles you may like:
Thanks for reading this article so far. If you find these Google Cloud Associate Cloud Engineer certification courses useful then please share with your friends and colleagues. If you have any questions or feedback then please drop a note. P.S. — Apart from going through these GCP Associate Cloud engineer courses you can also check out the exam guide or take the practice exam that will help you in identifying and focusing on the main areas during your preparation.
udemy.com
Medium’s largest Java publication, followed by 14630+ programmers. Follow to join our community.
160 
Collection of best Java articles, tutorials, courses, books, and resources from Javarevisite and its authors, Java Experts and many more.  Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
160 claps
160 
Written by
I am Java programmer, blogger, working on Java, J2EE, UNIX, FIX Protocol. I share Java tips on http://javarevisited.blogspot.com and http://java67.com
A humble place to learn Java and Programming better.
Written by
I am Java programmer, blogger, working on Java, J2EE, UNIX, FIX Protocol. I share Java tips on http://javarevisited.blogspot.com and http://java67.com
A humble place to learn Java and Programming better.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/serverlessguru/companies-moving-from-aws-and-azure-to-google-cloud-platform-55fe74f54bd2?source=search_post---------81,"There are currently no responses for this story.
Be the first to respond.
Amazon Web Services (AWS) is the oldest and most mature provider of cloud computing, offering the most services. However, Google Cloud Platform (GCP) and Microsoft Azure are innovating at a quick pace, and all three of these industry leaders have their pros and cons.
"
https://medium.com/google-cloud/running-node-red-on-google-cloud-platform-under-docker-3d4185e97f28?source=search_post---------82,"There are currently no responses for this story.
Be the first to respond.
Node-RED is very interesting. The Node-RED documentation does not include instructions for running Node-RED on Google Cloud Platform (GCP). If you haven’t used GCP, there are free options.
There have been (many) changes to GCP since I wrote this post almost 2 years ago. In response to Shiv Kodam, here’s an update that will help you get Node-RED running on (a) Container-Optimized OS image (container-vm is being deprecated); (b) Kubernetes Engine.
Container-Optimized OS uses cloud-init for configuration. Here’s a tweaked cloud-init file that runs a Node-RED container:
Create the Container-Optimized OS VM using the above cloud-init file:
NB This is a cheap-cheap VM f1-micro and it’s preemptible. If you want more horse-power, choose a more powerful VM.
Give the image a short amount of time to stabilize, download Node-RED and run the container. You can either ssh into the VM and then run the — command, or — as here — run them together:
You want to see something similar to the following for the successful start of the node-red-docker container:
Once the container is running, you can curl the Node-RED endpoint remotely:
And you should see output of the form:
If you’d prefer not to create a firewall rule, you may use gcloud to create an ssh port-forward from the VM’s port 1880 to your local machine’s 1880:
And then, from your browser:
When you’re done, don’t forget to blat the VM:
Deploying Node-RED as a single Pod|Service to Kubernetes (Engine) is straightforward. Assuming you have a cluster that you’re authenticated against:
Then you can use this shortcut to grab one the of Kubernetes’ Nodes, determine the Node-Red Service’s NodePort (in the Console screenshot above you can see this is31156 in my case) and then using gcloud to port-forward to this NodePort on this Node:
Then you can assess Node-RED as before *but* using the NodePort (${PORT} not 1880):
I was using a Chromebook to revise this post yesterday. It continues to be possible to run the port-forward when using a Chromebook thanks to the excellent Cloud Shell.
Using Cloud Shell, you may run the commands as above. The one difference is that Cloud Shell has a constrained set of ports (8080–8084). So, when it comes to the gcloud port-forward, please use one of these values instead of 1880, let’s use 8083 for argument’s sake:
Then — if not already changed — click “Change port”:
And set it to 8083:
And, you should see the Node-Red console as before.
That’s all!
It’s very easy to run Node-RED on a GCP.
If you’ve not used Google Cloud Platform before, start here. Otherwise, assuming you have a project [[PROJECT-ID]] and have the Cloud SDK installed, let’s start by defining some environment variables:
To make things simplest, we’re going to run Node-RED in Docker on the VM. We provide a configuration file to GCP with the command to create the VM.
Create a file called whatever you’ve decided for [[CONTAINER_FILE]] and use the following text for its content. This tells GCP where to find the Docker image for Node-RED and to run it on port 1880:
Now, to create a VM running Docker, with Node-RED installed and configured using the [[CONTAINER_FILE]], use the following command:
When the command completes and the VM is created, it will summarize the VM details including the EXTERNAL_IP address. Keep a record of this EXTERNAL_IP address as it is the IP address you will use to browse to Node-RED. You may also find the EXTERNAL_IP for your NODE with this command:
In order to access the Node-RED VM from other machines, you must open the firewall for port 1880. Use this command:
That’s it!
You should now be able to visit Node-RED using the following URL. Replace EXTERNAL_IP with the IP address that you determined previously:
And, then with the “first flow” deployed:
To delete the VM and delete the firewall, use the following commands:
It’s very easy to run Node-RED in a container on a VM in Google Cloud Platform. Have fun!
Google Cloud community articles and blogs
66 
4
No rights reserved
 by the author.
66 claps
66 
4
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@jaychapel/google-cloud-platform-vs-aws-is-the-answer-obvious-maybe-not-c85623f7d86e?source=search_post---------83,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Dec 12, 2019·7 min read
Google Cloud Platform vs AWS: what’s the deal? A while back, we also asked the same question about Azure vs AWS. After the release of the latest earnings reports a few weeks ago from AWS, Azure, and GCP, it’s clear that Microsoft is continuing to see growth, Amazon is maintaining a steady lead, and Google is stepping in. Now that Google Cloud Platform has solidly secured a spot among the “big three” cloud providers, we think it’s time to take a closer look and see how the underdog matches up to the rest of the competition.
As they’ve been known to do, Amazon, Google, and Microsoft all released their recent quarterly earnings around the same time the same day. At first glance, the headlines tell it all:
The obvious conclusion is that AWS continues to dominate in the cloud war. With all major cloud providers reporting earnings around the same time, we have an ideal opportunity to examine the numbers and determine if there’s more to the story. Here’s what the quarterly earning reports tell us:
You can see here that while Google is the smallest out of the “big three” providers, they have shown the most growth — from Q1 2018 to Q1 2019, Google Cloud has seen growth of 83%. While they still have a ways to go before surpassing AWS and Microsoft, they are moving quickly in the right direction as Canalys reported they were the fasted growing cloud-infrastructure vendor in the last year.
It’s also important to note that Google is just getting started. Also making headlines was an increase in new hires, adding 6,450 in the last quarter, and most of them going to positions in their cloud sector. Google’s headcount now stands at over 114,000 employees in total.
The Obvious: Google is not surpassing AWS
When it comes to Google Cloud Platform vs AWS, we have a clear winner. Amazon continues to have the advantage as the biggest and most successful cloud provider in the market. While AWS is growing at a smaller rate now than both Google Cloud and Azure, Amazon still holds the largest market share of all three. AWS is the clear competitor to beat as they are the first and most successful cloud provider to date, with the widest range of services, and a strong familiarity among developers.
The Less Obvious: Google is actually gaining more ground
While it’s easy to write off Google Cloud Platform, AWS is not untouchable. AWS has already solidified itself in the cloud market, but with the new features and partnerships, Google Cloud is proving to be a force to be reckoned with.
We know that AWS is at the forefront of cloud providers today, but that doesn’t mean Google Cloud is very far behind. AWS is now just one of the three major cloud providers — with two more (IBM and Alibaba) gaining more popularity as well. Google Cloud Platform has more in store for its cloud business in 2020.
A big step for google was announced earlier this year at Google Cloud’s conference — Google Cloud Next — the CEO of Google Cloud announced that they would be coming out with a retail platform to directly compete with Amazon, called Google Cloud for Retail. What ‘s different about their product? For starters, they are partnering with companies such as Kohl’s, Target, Bed Bath & Beyond, Shopify, etc. — these retailers are known for being direct competition with Amazon. In addition to that, this will be the first time that Google Cloud has had an AI product that is designed to address a business process for a specific vertical. Google doesn’t appear to be stopping at just retail — Thomas Kurian said they are planning to build capabilities to assist companies in specialized industries, ex: healthcare, manufacturing, media, and more.
Google’s stock continues to rise. With nearly 6,450 new hires added to the headcount, a vast majority of them being cloud-related jobs, it’s clear that Google is serious about expanding its role in the cloud market. In April of this year, Google reported that 103,459 now work there. Google CFO Ruth Porat said, “Cloud has continued to be the primary driver of headcount.”
Google Cloud’s new CEO, Thomas Kurian, understands that Google is lagging behind the other two cloud giants, and plans to close that gap in the next two years by growing sales headcount.
Deals have been made with major retailer Kohl’s department store, and payments processor giant, PayPal. Google CEO Sundar Pichai lists the cloud platform as one of the top three priorities for the company, confirming that they will continue expanding their cloud sales headcount.
In the past few months, Pichai added his thoughts on why he believes the Google Cloud Platform is on a set path for strong growth. He credits their success to customer confidence in Google’s impressive technology and a leader in machine learning, naming the company’s open-source software TensorFlow as a prime example. Another key component to growth is strategic partnerships, such as the deal with Cisco that is driving co-innovation in the cloud with both products benefiting from each other’s features, as well as teaming up with VMware and Pivotal.
Driving Google’s growth is also the fact that the cloud market itself is growing so rapidly. The move to the cloud has prompted large enterprises to use multiple cloud providers in building their applications. Companies such as Home Depot Inc. and Target Corp. rely on different cloud vendors to manage their multi-cloud environments.
Home Depot, in particular, uses both Azure and Google Cloud Platform, and a spokesman for the home improvement retailer explains why that was intentional: “Our philosophy here is to be cloud-agnostic, as much as we can.” this philosophy goes to show that as long as there is more than one major cloud provider in the mix, enterprises will continue trying, comparing, and adopting more than one cloud at a time — making way for Google Cloud to gain more ground.
Multi-cloud environments have become increasingly popular due because companies enjoy the advantage of the cloud’s global reach, scalability, and flexibility. Google Cloud has been the most avid supporter of multi-cloud out of the three major providers. Earlier this year at Google Cloud Next, they announced the launch of Anthos, a new managed service offering for hybrid and multi-cloud environments to give enterprises operational consistency. They do this by running quickly on any existing hardware, leverage open APIs and give developers the freedom to modernize. There’s also Google Cloud Composer, which is a fully managed workflow orchestration service built on Apache Airflow that allows users to monitor, schedule and manage workflows across hybrid and multi-cloud environments.
Google Cloud Platform vs AWS is only one of the battles to consider in the ongoing cloud war. The truth is, market performance is only one factor in choosing the best cloud provider. As we always say, the specific needs of your business are what will ultimately drive your decision.
What we do know: the public cloud market is not just growing — it’s booming. Referring back to our Azure vs AWS comparison — the basic questions still remain the same when it comes to choosing the best cloud provider:
Right now AWS is certainly in the lead among major cloud providers, but for how long? We will continue to track and compare cloud providers as earnings are reported, offers are increased, and price options grow and change. To be continued in 2020…
Originally published at www.parkmycloud.com on December 5, 2019.
CEO of ParkMyCloud
21 
1
21 
21 
1
CEO of ParkMyCloud
"
https://medium.com/google-cloud/faster-serviceaccount-authentication-for-google-cloud-platform-apis-f1355abc14b2?source=search_post---------84,"There are currently no responses for this story.
Be the first to respond.
A couple weeks ago I wanted to understand the AccessTokenCredentials flow that certain google cloud APIs supported.
It’s described here in the addendum of our developers oauth documentation as a specific optimization over the “normal” oauth flows providers maintain. The optimization is pretty dramatic so I thought I’d write an article describing it and how its used…and finally a quick bakeoff to demonstrate its advantage.
First, a quick background on serviceAccount oauth flows
A serviceAccount on GCP can take many shapes but it normally represents a non-user accessing a system. Think of it as machine accounts that require access to a service. When a system needs to access a GCP service (eg Pub/Sub), it needs to acquire an Oauth2 token described here
Essentially, a cryptographic private key is issued for a ServiceAccount and that key is used to sign a JSONWebToken (JWT) which includes some claims and information about the token capabilitites requested. At that point, the JWT is trasmitted to Google which verifies the claims and identity of the service account. Once google verifies the identity, it issues an access_token for the ServiceAccount with the scopes the original JWT requested and returns that token back to the client. The client application at that point has the bearer access_token to make the request to the service (Pub/Sub, in this example).
Note that this flow involves a roundtrip exchange for the JWT for an access_token ... so what if we could bypass that one roundtrip?
Allright, so how can we optimize the flow above if we already have a crypto key we can sign with? How about we create a JWT with a specific audience that is the service we intend it for? That is the optimized flow we're dealing with in this article and that is the flow that will save us this roundtrip.
The golang sample here basically reads the private key and uses it to sign a JWT with a specific aud: field that denotes the serivce its intended for.
This flow saves a round trip call but only applies to specific services within Google Cloud. These specific services utilize a different backend system which allows for this abridged flow. For example, the services listed below are the only ones that allows for this:
If you’re interested, the JWT that is signed by the service account uses the aud: field that describes the target service itself:
If you want to try this sample out, you would need to first create a service account and download its JSON private key. Once you do that, enable IAM access for that service account to Pub/Sub Viewer role as shown below:
At that point, download the JSON certificate and initialize the client:
And then run the sample:
In which response is the latency in milliseconds.
I must note: this whole procedure ONLY applies to the inital acquisiton of the access_token. In normal usecases, you can reuse an access_token or even the id_token until it expires (normally 3600s). What that means is the latency described below is only to get the first token for most usecases.
The following sample runs through the abridged flow against the standard ServiceAccount oauth flow where the full cert is loaded already and the measure is the Percentile Latency
I ran each mechanism 100 times on the same computer separately (and yah, trust me, the workstation where Iran it had lots of compute and very high network bandwith to GCP endpoints!).
As you can see, in any bracket, the lack of the additional roundtrip makes a difference in getting and making the same API call!
The following describes various other language bindings for the same abridged flow:
Additional references for oauth and service accounts:
You can find the source below or under the git repo i maintain here:
github.com
AccessTokenCredentials:
ServiceAccountCredentials:
Google Cloud community articles and blogs
96 
2
96 claps
96 
2
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/playing-with-concourseci-via-a-google-cloud-platform-free-trial-65acfbdd02d2?source=search_post---------85,"There are currently no responses for this story.
Be the first to respond.
I’ve started a practice of spending a couple of hours each weekend honing and growing my skills as a software developer. This weekend’s lesson was deploying my own ConcourseCI cluster to Google Compute Platform via the Bosh Google CPI.
I’m guessing that everyone knows about Amazon’s cloud computing platform called Amazon Web Services, AWS for short. Until recently, I hadn’t realized that Google had a competing platform called Google Compute Platform, GCP for short. One thing I like about GCP over AWS is the pricing model. Where AWS charges you for full hours of usage, GCP charges by the minute after a minimum of 10 minutes. Not to mention $300 free trial for 60 days.
Lucky for me, the folks at Google have done a great job documenting the procedure for installing Concourse to GCP.
Here are some of the tweaks I had to make to the above instructions to make it fit into the free plan (latest version numbers may be different by the time you read this post):
And that’s it. Following the directions with these minor tweaks should get you up and running quickly with your own GCP based Concourse installation.
If you are looking for tutorials to help you learn Concourse, check out the Concourse tutorials page. The flight school example is a great way to get started. You can check out my running version here: http://107.178.255.195/
Google Cloud community articles and blogs
15 
15 claps
15 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Home of my latest thoughts on software development. If you are looking for code, check out https://github.com/mikegehard.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/this-week-in-google-cloud-platform-quantum-bristlecone-interactive-clis-and-a-cloud-healthcare-e9ca40d3be01?source=search_post---------86,"There are currently no responses for this story.
Be the first to respond.
Announcing Bristlecone (Google Blog), a new quantum processor from the Google Quantum AI Lab built for researching and testing Google’s qubit technology
[Alpha] gcloud and other CLIs are getting even smarter — “Introducing GCP’s new interactive CLI” (Google Blog).
Google Cloud for Healthcare: new APIs, customers, partners and security updates (Google Blog). This includes a new API to ingest and manage HL7, FHIR, and DICOM formats for further analytics and ML processing with GCP. Read also the post for customer and partner momentum announced at HIMSS 2018.
From the “customers talk best about GCP” department :
From the “some insights about how foundations operate” department :
From the “in case you’ve missed it (ICYMI)” department :
From the “How to” department :
From the “Watch, listen, & learn” department :
From the “taking a step back” department :
The lovely terminal colors from this week’s picture are here to celebrate the release of interactive CLI tools :
That’s it for this week!-Alexis
Google Cloud community articles and blogs
35 
35 claps
35 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Google Cloud Developer Relations
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/google-cloud-platform-technology-nuggets-december-1-15-2021-edition-1f11dd72668a?source=search_post---------87,"There are currently no responses for this story.
Be the first to respond.
Welcome to the December 1–15, 2021 edition of Google Cloud Platform Technology Nuggets.
We have just two weeks left in 2021 and if you are looking to boost your GCP Skills, check out the offer to register before January 10, 2022 and get access to free 30 day access to Cloud Skills Boost, where specific courses ranging from Core Infrastructure, Kubernetes and Machine Learning are available.
This edition of the newsletter contains several Top 10 Blog posts in the respective categories that have been published on the official Google Cloud Blog site.
Google Cloud launched its 29th region in Santiago, Chile and has plans for multiple other regions in 2022. Check out this blog post that gives an overview of the infrastructure investments and roadmap for the near future.
In more infrastructure news, Google Cloud has been placed as a Leader in the Forrester Wave, AI Infrastructure, Q1 2021 Report.
Since we are at the end of the year, what better way to cap off the Infrastructure segment, with top 10 Infrastructure blogs on Google Cloud Blog in 2021.
I find reading case studies a bit dry and prefer listening to customers telling us about their journey, what worked for them, what did not and more. Did you know that we have an Architecting with Google Cloud series where customers share their stories with us of how they have built innovative solutions on top of Google Cloud Platform.
Check out this video playlist where customers share their journey.
Google Fellow Eric Brewer has been in the driver’s seat to building ground breaking services at Google and helping to externalize them. Eric currently oversees multiple services ranging from Kubernetes, Istio, Serverless and more. In a 4-part video series, Eric discusses four key Kubernetes and Open Source insights that have defined the future of cloud computing.
GKE Autopilot has taken the managed GKE offering to a new level with its mode that reduces the burden of cluster configuration with optimizing and secure default configurations. It has gone a step further now by partnering with multiple Tech Partner solutions like Gitlab, Dynatrace, Splunk and more, which allows you to run these products on Autopilot without modification. Check out the post which references a full list of integrations.
You know that Anthos Config Management helps to manage an ever expanding Kubernetes footprint enabling you to set and enforce consistent configurations and policies for your Kubernetes resources. If you would like to see this in action via the “Config Sync” GitOps methodology, take a look at this blog post.
We conclude this section with the Top 10 posts from the Managed Compute offerings on Google Cloud.
Cloud Pub/Sub now allows you to retain messages sent to topics for a period of 31 days instead of the early maximum of 7 days. This gives you a bit more breathing space in case you want to use these messages to debug subscribers, transform existing messages into other event information, replay the messages if you want to and more.
Understanding your BigQuery environment has got better with the availability of Slot Estimator and Resource Charts. The Slot Estimator is an interactive capacity management tool that helps administrators estimate and optimize their BigQuery capacity based on performance, while Resource Charts allow you to monitor their slot usage, manage capacity based on historical consumption, troubleshoot job performance, and self diagnose queries, and take corrective action as needed. Check out the blog post that gives a detailed explanation of how you can use them today via examples.
It has been a hectic week for operations teams across the world with the discovery of the log4j vulnerability. The Google Cybersecurity Action team has published recommendations around these vulnerabilities and how to mitigate them. Additionally, there have been updates to Cloud Armor, Security Command Center (Premium) to help manage the risk. It is also recommended to check the security advisory page for updates.
While talking about Google Cloud Security, a discussion around Service Accounts comes up regularly. It is recommended that Service Accounts be used minimally and with the right permissions to prevent attacks. Any organization would like to know its current inventory of Service Accounts and audit them for their activity, last usage and more. This blog post goes into those specifics and helps you understand how you can look at 3 specific GCP services has help you understand:
Looking to sharpen your skills with Google Cloud Serverless services like Cloud Run, Cloud Functions and more? How about taking part in the Serverless Hackathon that has just been launched and runs till February 2022. Be one of the first to try out the latest (2nd generation) versions of Cloud Functions and additional features to pick and demonstrate how you will address the serverless challenges thrown at you in the Hackathon. Check out the blog post or directly visit the website for more information.
If you are targeting a GCP certification in the coming year, the question you might have is where to start and how to build up your readiness not just in terms of the exam content but also the tools to ensure that you are ready to use them in your role. This blog post gives a good starting path to first picking up critical skills via GCP Skill badges and then moving on to a specific exams and helping you understand what those exams cover:
​Next up, we have a SketchNote and blog post on Google Cloud DevOps Overview, where you are given an overview of the CI/CD processes and what various tools are available in GCP to make that happen.
Since we are on the CI/CD topic, what about taking it one step further and understanding that once your application is in production, how do you manage, debug and ensure that your application is performing reliably. You do that via the Cloud Operations Suite, which was formerly called Stackdriver. Check out the notes to learn more about Cloud Operations.
Have questions, comments, or other feedback. Do send it across.
Looking to keep a tab on new Google Cloud product announcements? We have a handy page that you should bookmark → What’s new with Google Cloud.
Google Cloud community articles and blogs
42 
42 claps
42 
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://read.acloud.guru/getting-to-know-the-google-cloud-platform-714a8f033828?source=search_post---------88,"Most-read and must-read. Dig into news, insights, and assorted awesomeness around all things cloud, certifications, and the pursuit of modern tech skills.
Ahh, that new blog smell. Read up on the most recent updates for individuals and businesses, plus what’s new from ACG.
Say hello to the ACG blog crew — a mix of instructors, industry pros, and cloud enthusiasts. We’re write up your alley.
ACG Co-founder + O.G. AWS Guru
AI/ML Maestro
Cloud Adoption Whiz
 We like cloud. You like cloud. Let’s keep in touch.
Cybersecurity Junkie
Azure Geek Extraordinaire
Enterprise Talent Transformation Sage
"
https://medium.com/google-cloud/google-cloud-platform-technology-nuggets-october-1-15-2021-edition-2fcd0a6b2d05?source=search_post---------89,"There are currently no responses for this story.
Be the first to respond.
Welcome to the October 1–15, 2021 edition of Google Cloud Technology Nuggets.
Our biggest conference of the year, Google Cloud Next, happened this week from October 12–14, 2021 and this edition is going to have several announcements from the event spread across various GCP services. In case you are looking for the entire catalog of keynotes and sessions that you can watch at your own convenience, simply visit the on-demand catalog or just track the new announcements with “What’s New at Next”
We are proud of our commitment to be running on carbon-free energy 24/7 by 2030. This is in addition to having matched 100% of our electricity use with renewable energy purchases.
We are taking this a step further to providing you with tools to understand your digital application footprint on Google Cloud. Towards this, we have released Carbon Footprint, now available to every GCP user within the Cloud Console, as a way to understand the gross carbon emissions associated with your GCP Projects.
Check out the blog post with the announcement that also gives you recommendations on how you can reduce your carbon footprint.
IaaS is the workhorse of all services and we have a range of services in this portfolio, starting from Compute Engine, our Networking Stack and more. We have compiled a list of 10 reasons why Google IaaS stands out and try to compare how much of these points you are aware about?
At Next ’21, we announced Google Distributed Cloud, a portfolio of solutions consisting of hardware and software that extend our infrastructure to the edge and into your data centers. Google Distributed Cloud is powered by Anthos.
We announced our first ever Google Cloud Customer awards and selecting these customers would not have been an easy exercise looking at the innovation. A few of these customers were highlighted in the Opening Keynote at Google Cloud Next ’21 by Sundar and Thomas. Take a look here.
Google Cloud is thankful for GCP Communities, Google Developer Experts, Student Leads who have been championing on behalf of GCP and helping developers across the world learn more about GCP. At Google Cloud Next ’21, we announced the Google Cloud Innovators program, which is designed for developers and technical practitioners using Google Cloud and everyone is welcome. Watch the Cloud Innovators video.
To begin with, we have recognized some of our top experts across the world via the initial list of Champion Innovators and via this program, there will be exclusive invitations to hear from Google Cloud executives and Developer Advocates, exclusive roadmap presentations, and a chance for an invitation to join our new series of semi-annual Innovator events
Get started on your journey to share and learn together.
There were significant announcements at Cloud Next ’21 in this area. First up, we have a new homepage for “Data Science on Google Cloud” at https://cloud.google.com/data-science, which is a one-step place for training, reference architectures and codelabs along with an explanation of the list of products in this area.
At Next, some key announcements around our Data Cloud included:
If you are a Data & ML enthusiast, check out the Data Analytics Next ’21 playlist.
This blog post, rounds up the recent announcements around what we have been doing to make it easy for both operators and developers on Google Cloud. Key points are highlighted below:
For what’s new in Serverless, go no further than this session from Next ’21, where you get to hear the latest features announced in Google Cloud Functions and Google Cloud Run.
Check out the Application Development playlist from Next ’21 too.
In this edition, learn about Service Orchestration on Google Cloud. The Sketchnote starts off first with identifying the key differences between Service Orchestration and Choreography.
It then proceeds to highlight key services that we have that make it possible ranging from Cloud Workflows, Event Arc, Cloud Scheduler and Cloud Tasks.
Have questions, comments, or other feedback. Do send it across.
Looking to keep a tab on new Google Cloud product announcements? We have a handy page that you should bookmark → What’s new with Google Cloud.
Google Cloud community articles and blogs
33 
1
33 claps
33 
1
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/google-cloud-platform-tech-nuggets-june-1-15-2021-f5866326a5fa?source=search_post---------90,"There are currently no responses for this story.
Be the first to respond.
Welcome to the June 1–15, 2021 edition of Google Cloud Platform- Technology Nuggets.
We heard your feedback about increasing the frequency of Technology Nuggets and this is our first iteration to bring you technology news on Google Cloud Platform every two weeks. For those, who are reading this for the first time, we look at the recent announcements and highlight a few articles to read along with our own commentary.
Build on GCP Architecture Series
Priyanka Vergadia, Developer Advocate at Google Cloud, has launched a new mini-series titled “Build on GCP”. The series will be exclusive on Twitter and LinkedIn and you can follow @pvergadia on Twitter. Two episodes are already published and it leads to architecture blueprints for the solution, which are published in the official Google Cloud documentation:
The series will continue for 13 days with a new architecture being published every day.
Customer Stories
In our customer stories for this edition, check out how Arab Bank accelerated their application development and testing using Google Cloud and enabled true digital transformation.
Migrate for Anthos: The Toolset to Modernization
We constantly hear about customers wanting to take the next step in not just adopting the cloud but also ensuring that they are well on their way to application modernization. Our customers that are running workloads in VMs (Windows and Linux applications) and are looking to move to containerization, ask us about a tool that would help to discover, assess the modernization fit of their applications and help them to containerize. We do have Migrate for Anthos, that helps you do exactly that. Mike Coleman, Developer Advocate at Google Cloud, describes in the blog post, how you can use Migrate for Anthos today. Mike has published a series of videos on this topic, that start from the planning phase, migrating Windows and Linux VM based workloads, Day 2 Operations and more.
Modernize your Microsoft and Windows workloads
Google Cloud is a great platform for you to run your Windows workloads. These workloads could range from on-premise applications that you are running on Windows virtual machines, to migrating your SQL Server database to our fully-managed Cloud SQL or even looking at running Windows containers on GKE.
Sounds interesting? What if you could try out one of the above scenarios, see the experience for yourself via our simulated demos. You can do that today via the demo center that we have set up.
Google Applied ML Summit
Google Applied ML Summit was held recently on 9th June. The digital event covered how you can apply our Machine Learning services in your applications. The sessions provided great coverage of our Machine learning services ranging from training your own custom models, monitoring models, building an end-to-end ML pipeline and more.
The sessions from Data Cloud Summit are available on-demand and if you are looking at some cool demos from the event, check out the demo videos section in the site, that includes a Contact Center AI Virtual Agent, a Pizza Authenticator and more .
Bonus : For those who have never used Firestore before
Firestore is our fully-managed NoSQL database. If you have never heard of Firestore before or want to learn more about it, this is a great post to get familiar with not just Firestore but get knowledge on basic concepts about the type of databases out there.
We also suggest that you can use his handy quickstart guide to give Firestore a spin.
Stay in Touch!
Have questions, comments, or other feedback. Do send it across.
Looking to keep a tab on new Google Cloud product announcements? We have a handy page that you should bookmark → What’s new with Google Cloud.
Google Cloud community articles and blogs
20 
20 claps
20 
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/one-lap-around-google-cloud-platform-with-photo-scavenger-hunt-ca0487b620d3?source=search_post---------91,"There are currently no responses for this story.
Be the first to respond.
At a recent hackathon, a small group of us wrote Photo Scavenger Hunt — A simple mobile game that gives you four clues that you have to take pictures to match. The harder the clue, the more points you get.
At GCP NEXT today we got a chance to demo how to build, deploy, scale and diagnose the server side of this app hosted on App Engine.
The server is very simple. App Engine frontend written in nodejs, storing the clues in a mongoDB and of course using the Vision API to automatically check images for matching clues.
The code is very simple. We use the gcloud-node library to store the images uploaded into Cloud Storage (line 6).
Next, we use the vision API to get the list of what things are in the uploaded image. While we are there, we verify there are no naughty images.
Now let’s run that from the command line with a cute image
Here we see that the Vision API found raccoon correctly (with 94.3% confidence). BTW, for the curious a procyonidae is the family that raccoon, coatis, kinkajous, and olingos are members of.
Now we just need to deploy this app to the server. That could be a complex process of spinning up machines, figuring out OS, database, load balancers, security groups, etc.
But App Engine lets me just code, so all I have to do is gcloud app deploy
What’s happening behind the scenes is we’re pushing the source code into GCP. That’s standard App Engine.
But what’s really powerful and new here is that for our newest languages — NodeJS, Ruby, Java 8 and others — it’s also being packaged as a docker container and stored in a private Google Container Registry before being deployed.
I was even able to do that without having docker locally installed on my machine!
Not only that, the containerized app we built can be deployed anywhere within the container ecosystem: in our cloud or on-prem. And of course it can be orchestrated by Kubernetes!
This makes App Engine developers first class participants in the world of containers.
Let’s go to the load test VM already running in our cloud project. It is simple to find the VM with search built right into the cloud console.
We can now just click right in the browser to SSH right into the machine. No passwords that can easily be guessed, no keys to leave around your machine, just quick seamless access right from the browser.
While that loadgen script is running, let’s go check that image we uploaded. As you recall, we stored it in Cloud Storage. We can just navigate to the storage section in the cloud console and we see… oh no, permission denied.
Of course I don’t have permission. I am logged in here as the developer. As the developer, I don’t need to access the production data — users pictures that are uploaded. This is a feature of Cloud IAM — a powerful feature that gives administrators fine grained control of who can access which resources in GCP.
Luckily, I am logged in as the admin in this window, so I can grant Greg the developer access to read images.
Now, I am paranoid when it comes to security of my projects, so I want to verify I have done this correctly. As the admin, I can use Activity Stream to see all the changes in the project — handy for tracking down production issues or compliance concerns. You can fully audit activity on your project with Activity Stream.
Now, back as the developer, I can see the images in cloud storage. Here is a cute baby hippo we photographed earlier.
Now let’s see how the load script is going. In the App Engine section, we can see a spike in the number of requests per second.
But I wonder how many VMs we are up to now.. How is the load balancer doing? I could look at some graphs, but as a developer, I like to have more control, directly from the command line.
Clicking the cloud shell icon gives me access to a free, persistent machine. pre configured with all the latest tools, already authenticated and up to date, and of course, accessible from anywhere right in the browser.
All you need is a browser.
I can use it to list all instances being spun up. Given this is a full power linux box, I can grep and format this data in cool ways. Here you can see the number of instances spinning up.
Looks like I have gotten an error from Stackdriver Error Reporting. Let’s check out what is causing that error.
You can see the error just started happening on the alpha2 version of the application I just deployed, but it is happening a lot. “Error:twitter rate limit exceed”. Seems like something I should take care of.
Error Reporting is a new service we are launching today that gathers all the errors from all services in your project and does advanced grouping and frequency analysis to show you only the most relevant server errors and some diagnostics information (stacktrace, logs) to help track down the root cause.
Looks like that is the only one that is new, but we should really look at the others later.
The right thing to do in this case is to rollback first and ask questions later. We know the version I pushed had issues, so let’s roll back to a known good state. App Engine makes this simple to do with route all traffic.
But now I want to know what really happened here. We have Alpha1 that seems to be working but errors coming to Alpha2. Let’s use Stackdriver Trace to compare the RPCs from each to see what the differences are.
Stackdriver Trace is a distributed tracing utility that helps you track down exactly where your server time is being spent. You can see in this Trace report page that our new “buggy” version Alpha2 labeled as B in orange also has longer latencies.
If we check out one of these sample traces from the 99 percentile, we see call to the vision API and before that a call to GCS — that makes sense, we wrote those calls, but what are all these calls to twitter… Oh, they are part of this doQuestionableInitialization().
method! I knew I should have been suspicious of that. You will recall our initial code looked like:
So that doQuestionableInitialization() code is make a few spurious calls to twitter. While you never want errors to happen, Goole Cloud Platform made it easy for us to handle this error. We were able to:
— Spot this issue and get an alert with Error Reporting — Rollback our change instantly with App Engine Versions — Track down the root cause with Trace
I hope this quick lap around Google Cloud Platform has given you the desire to try it out on your own app! Get a $300 Free Trial at https://cloud.google.com.
Please let me know if you have questions or comments.
Google Cloud community articles and blogs
16 
1
16 claps
16 
1
Written by
Champion of the Obvious
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Champion of the Obvious
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@nikantvohra/aws-vs-google-cloud-platform-4dfdff7d1df8?source=search_post---------92,"Sign in
There are currently no responses for this story.
Be the first to respond.
Nikant Vohra
Apr 6, 2016·5 min read
Google Cloud Platform and AWS may seem similar in many ways but there are lot of fundamental differences in the two platforms. Engineers need to evaluate both platforms and choose the best ones that matches their computing needs.
Products and Services
Both Google Cloud Platform and AWS offer a plethora of services in the area of storage, computing, networking, big data and operations. AWS is a clear winner in terms of number of services it offers to its customers. This offers the customer a lot of flexibility to implement every thing they would need in their own data center. The services offered by AWS are well tested over a period of time and integrate incredibly well with each other.
On the other hand GC is focussed on the most used IAAS and PAAS products. For most cases, the services offered by GC would suffice but if you have some special needs AWS may be the way to go. GC is extending its services at a very fast pace and recently has launched variety of Machine Learning and Computing services.
EC2 is the most widely used cloud computing offering by Amazon. AWS offers a variety of EC2 instances for different workloads divided into different families like General Purpose, Compute Optimized, Memory and GPU.
Google Computing Engine is not much far behind from EC2 in terms of variety of instances and offers different types of machines like Standard Machines, High Memory Machines and High CPU Machines.
Geographical Presence
AWS is geographically more widespread than GCE and has about its presence in 10 regions spanning US, Europe, South America and Asia. Compared to this, GCE is present in only 4 regions. The closer your customer is to a particular region, the faster he will be served with your content.
Storage And Databases
Both the platforms allow temporary storage and persistent disks. Both offer services like object storage, archiving and support for relational and NO SQL databases.
As Google is a pioneer of technologies like Big Data, Big Query and Hadoop, their services in these areas are much better than AWS.
The maximum size supported by EBS is 1TB. GCE Persistent Disks can be up to 10TB. GCE’s persistent disk can be attached to multiple instances in read-only mode, an opportunity that is not available in AWS and allows to distribute data to a large workforce effectively.
The storage performance of the platforms may differ depending upon your choices but GCP seems to have a slight edge over AWS in this case.
Networking
Services like Load Balancing, Cloud CDN and Domain Name System are provided by both the platforms. GCE load balancers scale instantly on a traffic spike and can handle a lot of incoming requests automatically. AWS Elastic Load Balancer needs to be configured prior to scaling for additional requests. GCE is focussed a lot more on networking and offers a lot of features like creating firewalls, subnets and routes for your instances. It can be done in AWS as well but GCE does the heavy lifting for you in this case.
Flexibility
Although both platforms are converging in the features they offer with time, AWS still offers a lot more flexibility to the customers to configure their instances according to their computing requirements. When launching an AWS instance you have full control over networking, storage and other services that you would require in your own data center. As pointed out by Sean Fitzgerald AWS has a way better handle on access control as compared to GC. This can be taxing at times as you need to go through a lot of documentation to get all the things in place.
GCE on the other hand does heavy lifting for you in many cases and is operationally simpler to configure which comes at a loss of flexibility.
Pricing
The pricing wars between AWS and GC are not new. There has been a reduction in prices by both platforms several times over the last few years. Google has tested and compiled data showing that comparable Google Cloud Platform (GCP) instances are still between 15% and 41% less expensive than AWS.
Google Compute Engine bills in minute-level increments so you always pay exactly for what you use. Compared to this AWS has an hourly pricing model for EC2 instances which is something you may not need.
These prices reflect that Google is currently winning the war against AWS, but the prices may differ depending upon the services you use or the computing power you require.
Performance
A lot of benchmarking tests have been done to compare the performance of various instances of different cloud providers. A recent test of real-world Java benchmarks shows Google is fastest when it comes to computing performance.
Additional services
A lot of additional services are provided by these platforms. AWS offers services like lambda, Mobile Testing and Analytics, Code Deployment and Continuous Delivery while GC offers a lot of great Machine Learning API’s like their Vision and Speech API. If you require any of these additional services the choice of the platform can become easy.
Conclusion
Google seems to winning war in the pricing and performance section while AWS leads in the variety of services and the flexibility it offers to the customers. That being said your choice of AWS over GCE or vice-versa depends entirely on your requirements. For example you might need your application to serve in a region where GCE is not available which may turn the tide in favor of AWS, or your requirements may include heavy machine learning capabilities which are stronger in case of GC.
Both AWS and GC are competing very hard to capture the huge cloud computing market. Every month we hear announcements of price cuts and new services from both players. AWS seems to be leading the war right now due to the early mover advantage and mature services. This might change in the future as Google is constantly trying hard to close this gap.
References:
Engineering manager on WhatsApp Payments. Creator of Swift Programming Course https://www.udemy.com/swift-development/?couponCode=SWIFTMEDIUM.COM
See all (486)
14 
1
14 claps
14 
1
Engineering manager on WhatsApp Payments. Creator of Swift Programming Course https://www.udemy.com/swift-development/?couponCode=SWIFTMEDIUM.COM
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/microk8s-on-google-cloud-platform-d8b7a71a3ef?source=search_post---------93,"There are currently no responses for this story.
Be the first to respond.
https://microk8s.io/
How cool is this!?
NB Use whichever Ubuntu version you prefer. I really wanted Ubuntu Core but I think -minimial- is likely as good as. Ubuntu comes with snap preinstalled.
If you want to save time prefixing microk8s.kubectl commands with sudo, you may add your user to the instance’s microk8s group with:
Then:
Then:
And, of course:
Returns:
Or, if you’d prefer:
NB Because you must run the gcloud command from your local workstation, you won’t have ${PORT} set. So you must replace ${PORT} with the value (32592 in my case) or set the variable’s value locally.
NB There’s no requirement that the remote ${PORT} value be mapped to same port on your workstation; you could also use 80:localhost:${PORT} to access the Nginx service on :80.
And then you can access the service from your local workstation:
As a Googler, I’m spoiled by Kubernetes Engine *but* sometimes you just want the simplest possible cluster *or* you want to develop|deploy locally. MicroK8s is excellent.
Securely accessing the remote cluster locally.
The API server is available on :16443. If we can create a local kubernetes config file *and* we can port-forward to :16443, then we can access the cluster remotely:
Then, from another terminal (while the above ssh port-forward is running), we can copy the microk8s config file (
And then, either use KUBECONFIG:
Or, explicitly referencing the config on each command:
For completeness, Google’s Cloud SDK (gcloud) andkubectl are both available as Snaps if your local workstation also uses Snap.
Google Cloud community articles and blogs
132 
1
No rights reserved
 by the author.
132 claps
132 
1
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/this-week-in-google-cloud-platform-grafeas-istio-0-2-1ac9a68e3e12?source=search_post---------94,"There are currently no responses for this story.
Be the first to respond.
As is now often the case, this past week’s announcements are numerous and wide-ranging :
From the “how it’s built and why it matters” department :
From the “community GCP coverage” department :
From the “In Case You Missed It (ICYMI)” department :
The latest episode of the GCP Podcast (#0098) is “Sydney Region with Andrew Walker and Graham Polley”. Mark and Francesc are excited to be hosting Vint Cerf for the upcoming 100th episode and are collecting questions with the #askvint tag.
The picture of the week highlights the 100% renewable energy for Google’s operations as discussed in the latest Environmental report.
That’s it for this week!-Alexis
Google Cloud community articles and blogs
27 
27 claps
27 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Google Cloud Developer Relations
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/google-cloud-platform-tech-nuggets-july-1-15-2021-cb8f9a703158?source=search_post---------95,"There are currently no responses for this story.
Be the first to respond.
Welcome to the July 1–15, 2021 edition of Google Cloud Platform- Technology Nuggets.
Certification Update
We have launched a new Certification Cloud Digital Leader certification, that tests an individual’s general knowledge and understanding of cloud, independent of a specific technical role. Check out the certification page for more details.
Infrastructure
Our 2nd Cloud Region in India, Delhi NCR has gone live. This will act as a great boost for Indian customers who need a second region in the country for highly available workloads and for compliance and data sovereignty reasons.
Our GCP Network now spans 26 Regions across the world. Check out our Google Cloud locations page.
In other news, our Migrate for Compute Engine, gets a significant upgrade with the goal of making the migration process easier to manage. Migrate for Compute Engine (MCE) V5 is now available as a Managed Service and it does away with the need to install any migration specific resources in the Cloud. Simply enable the Migrate for Compute Engine API in your GCP project and deploy the Migrate Connector in the on-premises environment. Read more here.
Have you struggled setting up Cloud Monitoring Dashboards and deciding which Services to monitor, what metrics to capture, how to place the charts, etc. Well help is on your way via our library of Cloud Monitoring Dashboards. As the blog post says “These sample dashboards are now available directly from your Cloud Console under the Cloud Monitoring section. You don’t have to programmatically create them anymore — all of the samples are available with just one click!”
Finally, as part of our Intelligent Cloud, we introduced Predictive Autoscaling, which uses your instance group’s CPU history to forecast future load and calculate how many VMs are needed to meet your target CPU utilization.
Customer Story
Our customer Wunderkind is a performance Marketing channel that caters to online retailers and publishers. The need for flexibility and high scale demand put a stress on their legacy database system. They moved to Google Cloud and utilized BigTable to help them with these requirements. Check out the story of how they made it happen.
Analytics & AI/ML
First up, if you are looking for a summary of announcements in our Analytics offering, look no further than this post, which captures new features across BigQuery, Dataflow, Data Fusion, and more.
There are a couple of interesting posts in this cycle, that we would like you to take a look at:
Choosing the right technology stack for your AI/ML initiatives is not a simple task. In this article, Lak Lakshmanan, takes you through the choices and focuses on key use cases like Predictive Analytics, Unstructured Data, Automation and Personalization.
Dialogflow CX Competition and Native App Development Skill Challenge
Google Cloud Dialogflow is our Conversational AI platform for building virtual agents (chatbots) that run in web sites or multiple other messaging platforms. Dialogflow CX is the new version of Dialogflow and provides more intuitive and powerful ways to design conversational experiences. If you are looking to learn Dialogflow CX, we have just launched a competition that helps to not just learn but earn some prizes along the way.
Google Cloud provides developers with powerful serverless application development services ranging from Firebase, Cloud Run and more. We invite you to join us for a 30-day Skills Challenge on Native App Development to help to learn about these services. A Skills Challenge is a series of guided Hands-on labs that you complete and then do a final challenge lab that tests your skills.
Bonus
Firestore is a flexible, scalable NoSQL cloud database to store and sync data for client- and server-side development. Check out this developer sheet to learn more about Firestore.
Stay in Touch!
Have questions, comments, or other feedback. Do send it across.
Looking to keep a tab on new Google Cloud product announcements? We have a handy page that you should bookmark → What’s new with Google Cloud.
Google Cloud community articles and blogs
37 
37 claps
37 
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/google-cloud-platform-technology-nuggets-september-16-30-2021-edition-613e6b6ea5b2?source=search_post---------96,"There are currently no responses for this story.
Be the first to respond.
Welcome to the September 16–30, 2021 edition of Google Cloud Platform- Technology Nuggets.
We are less than 2 weeks away from our biggest conference of the year. Google Cloud Next is happening on October 12–14, 2021. Hope you have registered for it? The full event catalog is now live and you can build your own customized playlist of sessions. Stephanie Wong has published a guide to the NEXT sessions too. Check it out.
N2D Compute Engine machine types now support the latest 3rd generation AMD EPYC™ processor. Available at the same price as the 2nd generation, they deliver over 30% price-performance improvements. N2D VMs are suitable for a wide variety of general-purpose workloads such as web serving, app serving, databases, and enterprise applications. These VMs are currently in preview in several Google Cloud regions: us-central (Iowa), us-east1 (S. Carolina), europe-west4 (Netherlands), and asia-southeast1 (Singapore) and will be available in other Google Cloud regions globally in the coming months.
Our N2 VMs with Intel Xeon Processors got a refresh too.
If you would like to get an overview of the new services and enhancements we have done in our Networking Portfolio, check out this post that also highlights Google Cloud NEXT sessions around Networking that you can choose to tune into.
Quillbot provides a platform for Natural Language Processing that assists users with paraphrasing, summarization, grammar checking and more. It has over 10 million monthly active users and its complex and large AI models have been a great test of our infrastructure services.
Check out this blog post, which highlights the range of compute offerings that Qullbot has utilized to build models and scale in a cost-effective manner.
We have several announcements around Cloud Storage and Backup.
First up is the introduction of two key features to our Cloud Storage dual-region buckets.
Looking for a backup solution for your Stateful workloads on GKE? We have you covered now with Backup for GKE. This is a new native GKE service that makes it easier to protect your critical container-based data. Check out this blog post for more information.
Customers have the option of securing their data on Cloud Storage with the aid of Customer Managed Encryption Keys (CMEK). We recently announced a series of improvements to deliver better performance, lower costs and more capabilities vis-a-vis Cloud Storage and CMEK.
Eventarc, our service that enables events between GCP Cloud Services and your own applications, now provides a native Cloud Storage trigger to receive events from Cloud Storage buckets. Check out the blog post.
A Pricing Optimization Analytics Reference Pattern has been published, that helps you to build out a data pipeline on GCP, right from ingestion of data, to data preparation, analysis and then analytics driven outcomes. Key services highlighted in this pattern include BigQuery, BigQuery ML, Looker and Trifacta DataPrep. It also covers the key concept of having a Common Data Model (CDM).
Do check out our post on building a data engineering driven with first principles. Additionally, we are offering the following on-demand training at no cost, to help you learn more about BigQuery.
First up, we released our 2021 DORA State of DevOps Report. DORA, as you know, is the longest running research program that studies how organizations excel at software delivery. Our 2021 Report has a very interesting observation where we have added a fifth key “Reliability” to the existing four keys. I suggest previewing this blog post that gives a good summary of the findings and also download the full report.
We have a brand new service, Google Cloud Deploy, a managed, opinionated continuous delivery service that makes continuous delivery to GKE easier, faster, and more reliable.
Check out the blog post to learn more about Google Cloud Deploy.
Site Reliability Engineering (SRE) is a prescription set of practices that Google has documented to help organizations deliver and run software that is reliable. Reliability is a key pillar and is the fundamental premise to ensure that customers are happy with your software while at the same time, both your development and operations team have an objective model to work collaboratively to ensure that reliability. We published an interesting blog post titled “What’s your organizations’ reliability mindset?”, where we emphasize that “…reliability of a product is a property of the architecture of its system, processes, culture, as well as the mindset of the product team or organization that built it. In other words, reliability should be woven into the fabric of an organization, not just the result of a strong design ethos.”
We have several GCP Sketchnotes that you can use to learn about our services. We start off with Cloud Load Balancing:
Cloud DNS comes next:
And we cap off our learning section with Cloud Composer:
Have questions, comments, or other feedback. Do send it across.
Looking to keep a tab on new Google Cloud product announcements? We have a handy page that you should bookmark → What’s new with Google Cloud.
Google Cloud community articles and blogs
57 
57 claps
57 
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/tech-digest/google-cloud-platform-gcp-basics-in-its-own-words-tutorials-or-documentations-9f3acd4b68cd?source=search_post---------97,"There are currently no responses for this story.
Be the first to respond.
Google Cloud Platform GCP in Google’s Own Words
“The Google Cloud Platform (GCP) is a suite of cloud services hosted on Google’s infrastructure. From computing and storage, to data analytics, machine learning, and networking, GCP offers a wide variety of services and APIs that can be integrated with any cloud-computing application or project — be it personal or enterprise-grade.”
If we write a value proposition for Google Cloud Platform:
For customers big and small, from Fortune 500 to startups, who wants to host servers, infrastructure, software and data in the cloud, Google provides the Google Cloud Platform(GCP) that offers end-to-end hosted, scalable cloud platform, unlike competitors like Amazon Web Service and Microsoft Azure, Google offers integrated, easy-to-use, expert-supported with state-of-the-art documentation, GPU-enabled services at the operation scale of Google, can easily support the launch of a game like Pokemon GO, a global viral and massively compute intensive AR game.
Advanced Options on GCP Dashboard
Great selection of courses, but expensive though.
Free GCP podcast by Googlers — https://www.gcppodcast.com/
GCP offers Machine Learning engines, including Tensorflow (its branded machine learning, deep learning framework), ML APIs and even TPU like GPU in the cloud optimized for Tensorflow. “Google Cloud Machine Learning (ML) Engine is a managed service that enables developers and data scientists to build and bring superior machine learning models to production. Cloud ML Engine offers training and prediction services, which can be used together or individually. “- https://cloud.google.com/ml-engine/- https://cloud.google.com/ml-engine/docs/tensorflow/using-gpus- https://cloud.google.com/compute/docs/gpus/
First Google Cloud’s official certification specialization.
Google Cloud Cost Management
For example, the google natural language API, you can set the following limits.
New technology explained in easy-to-digest format for the…
85 
85 claps
85 
New technology explained in easy-to-digest format for the busy ones. Beginner friendly. Non-technical founders friendly.
Written by
Learn data, machine learning skills w/ us. hi@uniqtech.co We’d like to hear from you! Like what you read? Tip us https://www.buymeacoffee.com/uniqtech
New technology explained in easy-to-digest format for the busy ones. Beginner friendly. Non-technical founders friendly.
"
https://medium.com/@gmusumeci/getting-started-with-terraform-and-google-cloud-platform-gcp-e718017376d1?source=search_post---------98,"Sign in
There are currently no responses for this story.
Be the first to respond.
Guillermo Musumeci
Nov 8, 2019·6 min read
Terraform is the most popular Infrastructure as Code (IaC) tool in the market, so popular and good that it is included in Google Cloud Shell.
This post is part 1 of this tutorial and it will introduce to the process of writing Terraform scripts to automate your cloud…
"
https://open.nytimes.com/continuous-deployment-to-google-cloud-platform-with-drone-7078fe0c2eaf?source=search_post---------99,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
By TONY LI and JP ROBINSON
Over the course of the last year, the software development teams at The New York Times have been evaluating Google Cloud Platform for use in some of our future projects. To do this, we’ve surveyed a wide variety of software management techniques and tools, and we’ve explored how we might standardize building, testing and deploying our systems on GCP.
Our newly formed Delivery and Site Reliability Engineering team came up with two methods of deployment with Google Container Engine and Google App Engine for computing environments and the open source implementation of Drone as a continuous integration tool. As a result of this work, we are open sourcing two plugins for Drone: drone-gke and drone-gae respectively.
Container Engine is Google’s managed Kubernetes container orchestration platform. Kubernetes is an open source project that provides a declarative approach to managing containerized applications, enabling automatic scaling and healing properties. It encourages a common standard of how our applications are designed, deployed, and maintained across many independent teams. And because Kubernetes pools compute resources, developers can run many isolated applications in the same cluster, maximizing its resource usage density.
App Engine is a mature serverless platform Google has offered since 2008. It is capable of quickly scaling up and down as traffic changes, which is ideal for many scenarios at The New York Times when you consider possible sudden spikes from breaking news alerts or the publication of the new daily crossword every weekday at 10 p.m.
Drone is an open source continuous integration and delivery platform based on container technology, encapsulating the environment and functionalities for each step of a build pipeline inside an ephemeral container. Its flexible yet standardized nature enables our teams to unify on a plugin-extensible, ready-to-use CI/CD pipeline that supports any custom build environment with isolation, all with a declarative configuration similar to commercial CI/CD services. The result is the ability for developers to confidently ship their features and bug fixes into production within minutes, versus daily or weekly scheduled deployments. As a containerized Go application, it is easy to run and manage, and we hope to contribute to the core open source project.
Google provides an excellent set of command line utilities that allow developers to easily interact with Google Container Engine and Google App Engine, but we needed a way to encapsulate those tools inside of Drone to simplify the workflow for developers. Luckily, plugins for Drone are simple to create as they can be written in Go and are easily encapsulated and shared in the form of a Docker container. With that in mind, the task of creating a couple reusable plugins was not that daunting.
drone-gke is our new plugin to wrap the gcloud and kubectl commands and allow users to orchestrate deployments to Google Container Engine and apply changes to existing clusters. The Kubernetes yaml configuration file can be templated before being applied to the Kubernetes master, allowing integration with Drone’s ability to encrypt secrets and injecting build-specific variables.
Here is an example Drone configuration to launch a Go application in a container via a Kubernetes Deployment resource into Google Container Engine:
And the corresponding Kubernetes configuration:
And the corresponding Kubernetes secrets configuration:
drone-gae is our new plugin to wrap the gcloud and appcfg commands and allow users to make deployments to Google App Engine standard environment with Go, PHP or Python or to the flexible environments with any language.
Here’s a very basic example of all the configuration required to launch a new version of a Go service to Google App Engine’s standard environment with a second step to migrate traffic to that version:
Deploying new versions to the flexible environment requires a little more work, but it’s straightforward when using the plugin. We first use a build step to test and compile the code, then a publish step to build and publish a Docker container to Google Container Registry (via the drone-gcr plugin) and finally, we kick off the deployment via our new plugin.
We hope open sourcing these tools helps other engineers who want to leverage Drone as a continuous delivery solution. We are also looking to the community to take a look and help us harden our systems. Please raise an issue if you find any problems and follow the contributing guidelines if you make a pull request. For further reading and more documentation, you can visit the code repositories on Github:
github.com/NYTimes/drone-gaegithub.com/NYTimes/drone-gke
How we design and build digital products at The New York Times
14 
14 claps
14 
Written by
We’re New York Times employees writing about workplace culture, and how we design and build digital products for journalism.
How we design and build digital products at The New York Times.
Written by
We’re New York Times employees writing about workplace culture, and how we design and build digital products for journalism.
How we design and build digital products at The New York Times.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/javarevisited/top-10-cloud-certification-to-aim-in-2022-aws-azure-and-google-cloud-platform-bd054fff0538?source=search_post---------100,"There are currently no responses for this story.
Be the first to respond.
Hello guys, if you are aiming for cloud certifications in 2022 but are not sure which cloud certification should you go for then you have come to the right place. Earlier, I have shared a list of the best IT certifications for Java developers, and today, I am going to talk about the best cloud certification to aim for in 2022.
You can go through this list of cloud certifications and choose the best one depending upon your skills and experience. I have shared the best cloud certifications for beginners, developers, system admins, and solution architects from Amazon AWS, Microsoft Azure, and Google Cloud Platform.
Cloud computing services is growing exponentially in nowadays technology and become a priority among big-name organization such as Netflix which uses Amazon AWS to run their business from hosting to database and analytics.
For this reason, many cloud computing has come to the real world with different infrastructure and services such as Google Cloud and Microsoft Azure, and more.
All of those cloud services are complicated in their infrastructure and it requires the person who wants to deal with them to get some sort of certificate to deal with a specific service and companies nowadays are requiring people to have these certifications in order to validate their skills.
This article will discuss with you some of the best cloud certifications from companies like Microsoft Azure, Amazon AWS and Google cloud and having those certifications will make your resume stand out among the other competitor during the hiring process and you can get a higher salary than the others without those certifications.
Without wasting any more of your time, here is a list of the best Cloud certification of 2022. You can aim for this certification to boost your career and also start your career in Cloud Computing as Cloud Professional, Developer, and Solution architect.
This AWS certificate is designed for people who can do solution architecture such as deploying the web applications and securing them and also it targets individuals who have one year working with AWS services.
In other words, this is the best cloud certification for experienced developers who wants to become software architects or solution architects. If you have been working on the AWS cloud platform, personally or for our company then you should aim to pass this exam to get certified for your skills.
The exam is very vast and you need to know a lot of things about AWS services. If you are aiming for this prestigious certification then this AWS Certified Solutions Architect course on udemy can assist you to acquire those skills.
This is the best Cloud certification for beginners or anyone who wants to start with Cloud computing and the AWS cloud platform.
If you want to jump to Associate-level or specialty certification then make sure to get this certification that gives you an overall understanding of AWS cloud services such as security and account management and more.
This is a relatively easier exam and you can easily pass this with a couple of weeks of preparation.
If you need a recommendation, I highly recommend you check out Stephane Maarek’s AWS Certified Cloud Practitioner [NEW] course on Udemy. Stephane is an AWS Guru and this will help you learn and prepare for the exam.
This is the best Cloud certification for programmers and software developer who wants to create cloud-native applications. If you have more than one year of managing AWS services then this associate certification is right for you and it will teach you how to use the AWS core services as well as its architecture, develop, and deploy the application on AWS.
This is also one of the toughest AWS certifications, compared to the previous two certifications like cloud practitioner and solution architect. It’s not enough to just be familiar with different AWS services, you need to know them in-depth so that you can use correct configurations in a given scenario.
If you are a developer and software engineer then I highly recommend this cloud certification to you as it will significantly boost your profile and make you eligible for many more opportunities.
As I said, the exam is tough to crack and you need multiple resources to prepare well but to start with I highly recommend you to go through this course AWS Certified Developer from CloudGuru, which will teach you the skills needed to pass the exam.
This associate certification is for people having at least one year in deployment, management, and operations on AWS and teaches you to choose the appropriate service for your needs as well as control the data flow from AWS and more.
In other words, this is the best Cloud certification for system admins and IT professionals who work on the Infra side.
If you are working in IT support or working as a System admin, you can aim for this certification to further boost your career.
If you need a course to prepare for this certification, I recommend you to this course named AWS Certified SysOps Administrator — Associate is a good resource to learn those skills.
If you notice, all top four cloud certification is from Amazon AWS, and it's because AWS is the most popular public cloud platform for both startups and big companies but Microsoft Azure is catching up quickly which has, in turn, boost the demand for certified Azure Cloud professionals.
The Azure fundamentals certification is better for individuals to know some foundation of the cloud services and this certificate will teach you the cloud concepts as well as how to use Microsoft Azure services, security, privacy, and pricing.
In short, the best cloud certification for beginners who wants to learn Azure. This is very similar to AWS cloud practitioner and you can pass this certification with a couple of weeks’ preparation.
If you need recommendations then this course on udemy AZ-900 Azure Exam Prep will explain all of this in one course and prepare you for the exam.
This is the best cloud certification for experienced programmers, developers, and DevOps engineers who want to become Azure experts.
When you pass this certification exam you will have the skills to design and implement solutions in Microsoft Azure and that includes security, network computing, and storage.
If your companies are migrating into Microsoft Azure cloud then aiming for this Azure certification and boost your profile and also help you to get promoted.
When it comes to preparation, this is a vast certification and you need to cover a lot of topics but thankfully there are many courses to learn these skills but this course on udemy but the AZ-300/AZ-303 Azure Architecture Technologies is the best of them all and it will help you a lot on this journey.
This is another Azure certificate that made the list of top 10 Cloud certifications. This is the best Azure certification for system admins and people who are working in IT support.
This certificate will get you the experiences to implement, manage, and monitor cloud services such as storage, security, and virtual environment, and many more responsibilities.
For preparation, you need to know all the essential Azure services and how to use them, configure them, and troubleshoot in case of any issue.
If you need an online course, I recommend you to check out this Udemy course AZ-104 Microsoft Azure Administrator is a good resource to learn all of those skills.
A list of best cloud certifications cannot be completed without Google cloud certification, another big player in the public cloud market. Google cloud has some of the best capabilities when it comes to dealing with Big Data and Machine learning and that’s why many startups who are working in those fields are opting for Google cloud.
This is the best Google cloud certification for programmers, developers, and software engineers.
The holder of this certificate will have the responsibility of deploying web applications in the cloud as well as monitoring the operations and managing the whole enterprise solutions and configuring access and security.
Regarding preparing for this exam, this Ultimate Google Certified Associate Cloud Engineer course can help you learn and pass the exam.
This is another popular and in-demand Google cloud certification. This is similar to AWS solution architect and Azure Technology expert but on Google cloud.
This is the best Google Cloud certification for experienced IT professionals who wants to become solution architect on Google Cloud technologies like Big Table, Big Query, and other GCP platforms services.
The holder of a professional cloud architect certificate plays an important role inside the organization since he could design develop, deploy, and manage your web application as well as secure them and more responsibilities.
Regarding preparation for this prestigious cloud certification, this course on udemy Ultimate Google Certified Professional Cloud Architect is a good resource for this certificate.
This is the ultimate cloud certification you can aim for in 2022. It’s also regarded as the toughest AWS cloud certification and requires extensive experience and knowledge of the AWS cloud platform.
There is a huge demand for this certification as there is always a shortage of AWS experts and I highly recommend this to expert cloud professionals.
This advanced certification teaches you how to design and deploy scalable web applications on Amazon AWS servers as well as select the appropriate service and power to use for your application and more skills you will have.
Regarding preparation, you may need to consult a lot of resources and AWS papers, documentation, and courses but to start with this course on udemy Ultimate AWS Certified Solutions Architect will help you in this journey.
That’s all about the best cloud Certification you can acquire in 2022. Those certifications are almost the most useful in the field of cloud computing and are issued by the cloud provider itself such as Amazon, Google, and Microsoft, and receiving one of these certifications will open the door to a successful career in this growing industry.
Other Certification Resources for IT Professionals and Programmers
Thanks for reading this article so far. If you like these best Cloud Certifications then please share them with your friends and colleagues. If these courses have helped you to pass the exam, then please spread the word so that other people can also benefit.
P. S. — If you are a complete beginner to Cloud Computing and looking for some free courses to learn Cloud Computing in general then you can also check out this Introduction to Cloud Computing (FREE Course) on Udemy. More than 210,000 people have joined this course and it’s completely free, you just need a Udemy account to join this course.
Medium’s largest Java publication, followed by 14630+ programmers. Follow to join our community.
194 
194 claps
194 
A humble place to learn Java and Programming better.
Written by
I am Java programmer, blogger, working on Java, J2EE, UNIX, FIX Protocol. I share Java tips on http://javarevisited.blogspot.com and http://java67.com
A humble place to learn Java and Programming better.
"
https://levelup.gitconnected.com/how-to-create-a-highly-available-nginx-load-balancer-on-google-cloud-platform-9ebc8b6abb09?source=search_post---------101,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
With the advent of virtualised cloud-based infrastructure, more and more organisations are migrating to the cloud. There are a lot of advantages in doing so…
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/this-week-in-google-cloud-platform-dataproc-ha-tensorflow-1-5-c1fafefcb1f8?source=search_post---------102,"There are currently no responses for this story.
Be the first to respond.
Cloud Dataproc now offers fully supported HA clusters (no SPOF for both YARN and HDFS) as well as SSD persistent disks for those workloads with plenty of reads and writes. Full details in “Updating Cloud Dataproc for faster speeds and more resiliency” (Google Blog)
TensorFlow 1.5 is here (Google blog). It includes Eager Execution as a preview, ships with the developer preview of TensorFlow Lite, and has enhanced GPU acceleration.
Kaggle Learn (kaggle.com) is a new service offering interactive tutorial notebooks to teach new users machine learning, deep learning, and data visualization. It uses Kaggle Kernels and offers GCP-hosted Jupyter notebook with a single click.
Following up on the recently added tutorial capabilities, see how Cloud Shell can now deliver tutorials right into the GCP console: “Cloud Shell Tutorials: Learning experiences integrated into the Cloud Console” (Google blog)
This next piece is a bit long but it covers a lot of ground about giving AI a purpose through UX : “The UX of AI” (design.google)
From the “Best practices all around” department :
From the “In case you missed it (ICYMI)” department :
From my favorite “Customers talk best about GCP” department :
This week’s GCP Podcast, episode #000111, is a conversation with Sam Ramji, VP Product Management for GCP (gcppodcast.com)
This week‘s picture is Felipe Hoffa and Graham Polley’s “Cloud Dataflow and the Tram Challenge” video :
That’s it for this week!-Alexis
Google Cloud community articles and blogs
20 
20 claps
20 
Written by
Google Cloud Developer Relations
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Google Cloud Developer Relations
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/qwiklabs/pizza-hut-makes-them-delicious-google-cloud-platform-delivers-them-to-you-hot-eda799705ef3?source=search_post---------103,"There are currently no responses for this story.
Be the first to respond.
A subsidiary of United States-headquartered corporation Yum! Brands, Pizza Hut prides itself on serving more pizzas than any other pizza business. Founded in 1958, Pizza Hut operates 18,000 restaurants in over 100 countries. In the Indian subcontinent, Pizza Hut and franchise partners Devyani International and Sapphire Foods India operate more than 500 pizza restaurants, including 430 in India itself.
With Google Cloud Platform and Google Maps, Pizza Hut India is meeting demand from tech savvy consumers for fast, traceable delivery and interaction over multiple customer channels, including web, mobile application, voice and in-store — and gaining the insights to expand quickly and effectively and operate efficiently.
Pizza Hut India uses Distance Matrix API through Google Maps Platform to provide travel time and distance based on recommended routes between origins and destinations. This service helps provide delivery riders’ estimated time of arrival to customers. With Google Maps Platform and Google Cloud Platform, Pizza Hut India and its franchise partners are realizing the goals of the transformation program. Rather than take up to seven days to manually map trade zones for stores, Pizza Hut India uses Google Maps Platform to create and update them as required.
Further, Pizza Hut India is running advanced analysis of data in the BigQuery data warehouse, enabling the business to determine which restaurants are doing well, which deliveries may be delayed, and what locations are most promising to open a new store. These insights enable the business to boost productivity, expand effectively, and operate more efficiently.
Find out more about how Google Cloud Platform is satisfying your pizza cravings in the Creating with Google Maps Quest. You’ll use several tools available in Google Cloud to manipulate data and create a Google Map! Use Code 1q-pizzagcp-400 (Valid through, Sunday 20th June 2021)
Future-proof your career with Google Cloud Skills
186 
186 claps
186 
Get temporary lab credentials to Google Cloud Platform so you can learn cloud skills using the real thing — no simulations. From 30–90 minutes, from intro to expert level, with topics like machine learning, security, infrastructure, app dev, and more, we’ve got you covered.
Written by

Get temporary lab credentials to Google Cloud Platform so you can learn cloud skills using the real thing — no simulations. From 30–90 minutes, from intro to expert level, with topics like machine learning, security, infrastructure, app dev, and more, we’ve got you covered.
"
https://medium.com/google-cloud/this-week-in-google-cloud-platform-next-18-a-k8s-comic-and-jeffdean-98dec4302062?source=search_post---------104,"There are currently no responses for this story.
Be the first to respond.
20k attendees expected in San Francisco next July — “Announcing Google Cloud Next 2018”. Save the date!
Some GKE (Google Kubernetes Engine) announcements to go along with KubeCon :
… and a somewhat less expected comic :
More GCP announcements :
More Cloud, Big Data, and ML news :
From the “badass women company” department :
From the “Customers talk best about GCP” department :
This past week’s GCP Podcast (#000106) is a discussion with New York Times’ Deep Kapadia and JP Robinson.
From the FOMO department :
From the “How-to” department :
From the “In case you missed it (ICYMI)” department :
@JeffDean (’nuff said)
This week‘s picture has to be the Kubernetes Comic :
That’s it for this week!-Alexis
Google Cloud community articles and blogs
15 
15 claps
15 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Google Cloud Developer Relations
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.datadriveninvestor.com/big-query-sql-on-google-cloud-platform-part-1-c714f69b6831?source=search_post---------105,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
I’m creating this post not just to let the world (alright, atleast my readers! ;) )know how much I love Google Cloud Platform but also to provide a quick-starter i.e how to get started with Big query on GCP. Big query is a highly scalable, server less, cost-effective multi-cloud data warehouse designed for business agility and is…
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/this-week-in-google-cloud-platform-automl-all-the-things-opencensus-and-more-subsea-cables-975bcae32ad3?source=search_post---------106,"There are currently no responses for this story.
Be the first to respond.
If you were paying careful attention you’ve heard of AutoML already. Google is now introducing “Cloud AutoML: Making AI accessible to every business”. GCP is now on its way to offering a gradient of machine learning approaches. Check out also this GCP Podcast interview.
The Google Open Source blog introduces “OpenCensus: A Stats Collection and Distributed Tracing Framework”. OpenCensus currently supports Prometheus, SignalFx, Stackdriver, and Zipkin.
How many subsea cables has Google invested in and what difference does it make for GCP users? Find the answers in “Expanding our global infrastructure with new regions and subsea cables”
From the “In case you missed it (ICYMI)” department :
From the “BigQuery DDL and scheduling” department :
From the “Amazing GCP customers adding open source value” department :
From the “how-to and hands-on” department :
From the “the more you know about those vulnerabilities“ department :
This week‘s picture is taken from the BigQuery DDL beta documentation page :
That’s it for this week!-Alexis
Google Cloud community articles and blogs
70 
70 claps
70 
Written by
Google Cloud Developer Relations
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Google Cloud Developer Relations
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/a-to-z-of-google-cloud-platform-gcp-a-personal-selection-a-is-for-app-engine-and-api-s-311d408249f8?source=search_post---------107,"There are currently no responses for this story.
Be the first to respond.
App Engine is GCP’s PaaS .
There are lots of real cool things about App Engine and I’m only going to talk briefly about one of them here Features
I feel “features” reflects the philosophy of being the “simplest” cloud in that it just provides functionality you’d otherwise have to code or feed and water yourself.
These are mostly API’s to services that are used frequently for a lot of use cases. Some are in GA others in beta or alpha .
Consider basic Image manipulation such as resizing and reformatting . This is a common requirement so App engine provides an images API that makes it super easy. The Images service can accept image data directly from the app, or it can use data retrieved from Google Cloud Storage.
Caching that’s a super common requirement so App Engine provides memcache Those are examples of just two of the really useful features where you as the developer don’t have to spend time writing the functionality or looking after the service yourself. Basically you just consume it.
At the time of writing App Engine provided an impressive list of features .
Using features helps you deliver basic functionality quickly and integration with App Engine makes it super easy to start using the features .
It was when I first started playing around with App Engine it became obvious to me that features had a lot in common with API management systems only without the trendy label. ( A bit like Devops being nothing really new but once it got a label well..)
To understand why I’d be bold enough to stick my neck out and say that well you need to understand what I mean by API Management :
Well I reckon App Engine features meets that criteria.
A mechanism to allow the discovery of API — Via the console. You can start at the features table and drill down into the feature . If client’s consuming then this is a contract between the application and the clients ( see Endpoints )
A documented API so easily consumed — API is documented for supported languages: App Engine Python API’s , Google App Engine Java API , PHP App Engine API’s
Each feature also has a dedicated docs page for the supported language which shows how to consume the service and lists the quotas e.g the Images Python API Overview
A Gateway that secures access to the traffic between the APIs and its consumers and provides metrics on usage — App Engine API’s have quotas . Authorisation and authentication requires having a GCP account to access . Cloud endpoints generate APIs and client libraries from an App Engine application.
Lifecycle Management to manage the process of designing, developing, deploying, versioning and retiring API’s — Google develops & provides the API’s . Note: For third party supplied features you need to check with those suppliers
I don’t believe I’m being disingenuous with making a direct analogy !
Before I leave talking about App Engine’s features I just wanted to touch lightly ( okay one lightly the other not so) on two features I found particularly cool and happen to fit in with my API management viewpoint. URL fetch and Cloud endpoints . So yes I ‘m going to sneak in the letter E here as Endpoints are actually called out as a GCP product!
URL Fetch
Well the name of the feature( service) here is pretty succinct. It’s a service that allows you to issue a http or https request and receive a response ! These can be other app engine applications or other applications on the web.
So not trivialising it at all it’s a way for your application to easily deal with http and https requests and responses.
The service has some rules around it to prevent recursive requests i.e no calling yourself and time outs so it’s not waiting for a response for ever. It also supports both synchronous and asynchronous calls.There are also quotas assigned.
Cloud Endpoints
This feature is super cool. As a way to improve developer productivity as it provides a way to generate APIs and client libraries from an App Engine application. You will probably come across this being referred to as an API backend which may have you scratching your head as to what exactly that means. A far better description of it I feel is as an API server for your application and indeed it is referred to as such in some of the docs . In a nutshell Cloud endpoints performs business logic and other functions for Android, iOS clients and JavaScript web clients. It provides access to App engine’s features ( I am pretty sure I am not a fan of the word features though way of describing the services available via App Engine) .
So if you’re still not clear as to what they are I have regurgitated the official docs for a succinct description of what Endpoints are and how they remove a level of lower level coding for basic functionality and thus allow you to focus on your usp:
“Cloud endpoints Create RESTful services and make them accessible to iOS, Android and Javascript clients. Automatically generate client libraries to make wiring up the frontend easy. Built-in features include denial-of-service protection, OAuth 2.0 support and client key management”
So what exactly do you need to do to create your own API server and application that uses it :
It’s all a little inception like as in my analogy App Engine features and Endpoints (which is a feature itself) can technically both be considered as API gateways/ servers.
Google Cloud community articles and blogs
20 
20 claps
20 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Chocolate addict - I have it under control really I do. I do stuff involving cloudy tech. Tweets my own so only me to blame, except for retweets.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/google-cloud-platform-tech-nuggets-july-16-31-2021-e3521375dfab?source=search_post---------108,"There are currently no responses for this story.
Be the first to respond.
Welcome to the July 16–31, 2021 edition of Google Cloud Platform- Technology Nuggets.
First up, do note that Registration for Google Cloud Next, happening on 12th-14th October, is open
In our Infrastructure section, we have a Q2 Edition, which covers multiple announcements that have been made in this space ranging from . This is a great blog post to review all Infrastructure (Compute, Networking, Storage, Containers and more) related news around Google Cloud Platform.
A new Cloud region is open for business in Melbourne. This takes the number of cloud regions to 27 and we continue to invest in this area. Do read about Blue and Raman, subsea cables that link the Middle East with Southern Europe and Asia.
If you have worked with Google Cloud Storage (GCS), chances are high that you have used gsutil to transfer your files to GCP. While, gsutil does have a few tricks up its sleeve to break up file transfers in parts and upload them in parallel, we now have gcloud storage˘ command in our gcloud SDK. This command uses a new parallelization strategy to speed up transfers to the cloud. Our tests have shown improvement in higher double-digit percentages compared to doing the same thing with gsutil. Worth a look.
Learn how Home Depot, not just migrated its SAP applications to GCP but also tapped into various Analytics offerings, including BigQuery ML and AutoML. If you are an existing SAP on GCP customer and looking to utilize the power of our Analytics and AI, especially BigQuery, then take a look at this blog post that highlights the various ways in which you can model the SAP Data in BigQuery. Moving your SAP data into an Analytics Warehouse like Big Query allows you to not just consolidate your SAP data along with our data sources but can help you tap into a key strength of Google Cloud i.e. AI and ML services.
Additionally, do check out Sky Media Group’s journey to embark on a FinOps strategy and save $$$ in expenses.
Vertex AI, our end-to-end ML Platform can be a bit overwhelming to understand in terms of different services that it offers.
To help you navigate the various services, we have a blog post that highlights via an interesting real world use case, how to go about both learning and understanding which services to use. The article presents various options ranging from pre-trained models, custom models via AutoML and also developing your own custom models. The article moves on to how you can deploy and monitor your model along with an entire MLOps workflow around it. This is an essential read to help you understand Vertex AI.
Speaking of MLOps, one of the key requirements is to monitor the deployed models in production for training-serving skew. Check out this article on how to enable Model Monitoring in Vertex AI and a short video to understand it.
We recently announced Datastream, a serverless change data capture and replication service. Check out this post on how it enables data changes to be streamed over from a source to target systems.
If you have been using our legacy metrics and agents for advanced visibility into your infrastructure, we have our unified Ops Agent, that has gone GA. Ops Agent, an OpenTelemetry based agent, is now our primary agent to collect telemetry from Compute Engine instances. Read more about it.
We have multiple infrastructure services ranging from Compute Engine, App Engine, Google Kubernetes Engine, Cloud Run and Cloud Functions, where you can run your applications. But the question is which service is ideally suited for your workloads which could range from legacy monolithic applications to containerized workloads and even to loosely coupled event-driven architectures.
Check out our post titled “Where should I run my stuff? Choosing a Google Cloud Compute option”, that helps you navigate this.
Stay in Touch!
Have questions, comments, or other feedback. Do send it across.
Looking to keep a tab on new Google Cloud product announcements? We have a handy page that you should bookmark → What’s new with Google Cloud.
Google Cloud community articles and blogs
25 
1
25 claps
25 
1
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/@gmusumeci/getting-started-with-terraform-and-google-cloud-platform-gcp-deploying-vms-in-a-private-only-f9ab61fa7d15?source=search_post---------109,"Sign in
There are currently no responses for this story.
Be the first to respond.
Guillermo Musumeci
Nov 12, 2019·8 min read
This is part 2 of the tutorial Getting Started with Terraform and Google Cloud Platform (GCP).
Part 1, deploying VMs in a public-only single region of the tutorial →…
"
https://medium.com/google-cloud/using-google-cloud-platform-s-blobstore-553a01b6f464?source=search_post---------110,"There are currently no responses for this story.
Be the first to respond.
By writing a simple Go web app for Geotagged photos
A friend recently saw a picture posted on Facebook by a friend and wanted to find out where that picture was taken. The picture was of someone in a park and he could not pinpoint exactly which park it was, but had a general idea. He asked me if there was a way for him to see the location of the photo on a map.
Before you can determine the location of a photo, the photo has to be geotagged. Geotagged basically means that the longitude and latitude of the photo need to be stored in the photo metadata. The metadata is the invisible part of the photo called EXIF data. Depending on the camera, EXIF data will store the current state of the camera when the photo was taken including date and time, shutter speeds, focal lengths, flash, lens type, location data, etc.
Of course, the only way you’ll see where a picture was taken is if the camera is GPS enabled. If you have a camera that doesn’t have any type of GPS option, then there won’t be any location data in the EXIF data. This is true of most SLR cameras. However, if the photo was taken with a smartphone and location services are enabled, then the GPS coordinates of the phone will be captured when you snap a picture.
Let’s write a Go program to extract longitude and latitude information from a photo’s EXIF data and throw a map of the area where the photo was snapped. Our app will be deployed on the Google App Engine.
I thought of writing this Go program while studying the Blobstore Go API.
Blobstore Go API
Needless to say, I have made extensive references to the official documentation of the Blobstore Go API which states:
The Blobstore API allows your application to serve data objects, called blobs, that are much larger than the size allowed for objects in the Datastore service. Blobs are useful for serving large files, such as video or image files, and for allowing users to upload large data files. Blobs are created by uploading a file through an HTTP request. Typically, your applications will do this by presenting a form with a file upload field to the user. When the form is submitted, the Blobstore creates a blob from the file’s contents and returns an opaque reference to the blob, called a blob key, which you can later use to serve the blob.
Blobs can’t be modified after they’re created though they can be deleted. We shall use the Blobstore API to upload a photo that could have geotagged information.
Uploading a blob
The package appengine provides basic functionality for the Google App Engine.
Function “NewContext” returns a context for an in-flight HTTP request. Repeated calls will return the same value.
Package blobstore provides a client for App Engine’s persistent blob storage service.
Call “blobstore.UploadUrl” to create an upload URL for the form that the user will fill out, passing the application path to load when the POST of the form is completed. These URLs expire and should not be reused. The opts parameter may be nil.
A ResponseWriter interface is used by an HTTP handler to construct an HTTP response.
“WriteHeader(int)” — sends an HTTP response header with status code. If WriteHeader is not called explicitly, the first call to “Write” will trigger an implicit “WriteHeader(http.StatusOK)”. Thus explicit calls to “WriteHeader” are mainly used to send error codes. We use this in the function “serveError”.
“func (h Header) Set(key, value string)” — sets the header entries associated with “key” to the single element value. It replaces any existing values associated with “key”. In our program, we have set the header to:
2. Create an upload form
The form must include a file upload field, and the form’s enctype must be set to multipart/form-data. When the user submits the form, the POST is handled by the Blobstore API, which creates the blob. The API also creates an info record for the blob and stores the record in the datastore and passes the rewritten request to your application on the given path as a blob key.
Note: You must serve the form’s page with a Content-Type of “text/html; charset=utf-8”, or any filenames with non-ASCII characters will be misinterpreted. This is the default content type in Go, but you must remember to do this if you set your own Content-Type.
3. Implement upload handler
In this handler, you can store the blob key with the rest of your application’s data model. The blob key itself remains accessible from the blob info entity in the datastore. Note that after the user submits the form and your handler is called, the blob has already been saved and the blob info added to the datastore. If your application doesn’t want to keep the blob, you should delete the blob immediately to prevent it from becoming orphaned.
4. Using goexif
We first use the function:
“NewReader” returns a reader for a blob. It always succeeds; if the blob does not exist then an error will be reported upon the first read. The blob key is passed as the URL argument “r.FormValue(“blobKey”)”.
We are going to use the goexif package to retrieve EXIF metadata from image files.
To install, in a command window, type:
Our function “getLatLng” uses:
“Decode” parses EXIF-encoded data from “Reader” r and returns a queryable Exif object.
Function “LatLong” returns the latitude and longitude (as “float64”) of the photo and whether it was present. Our function converts the “float64” value to a string.
Next, we use the “Delete” function to delete the blob.
5. Use the Google Static Maps API
The Google Static Maps API lets you embed a Google Map image on your web page without requiring JavaScript or any dynamic page loading. The Google Static Map service creates your map based on URL parameters sent through a standard HTTP request and returns the map as an image you can display on your web page.
We create a string based on the above API, as follows. The base URL is:
The additional parameters used in the string are:
The “markers” parameter takes set of value assignments (marker descriptors) of the following format:
The set of “markerStyles” is declared at the beginning of the markers declaration and consists of zero or more style descriptors separated by the pipe character (|), followed by a set of one or more locations also separated by the pipe character (|).
The marker style descriptors contain the following key/value assignments:
Do read the article “Go Code Organization” to understand how your files should be organized.
Our complete program is stored in the folder “$GOPATH/src/github.com/SatishTalim/blobstrex”
6. File app.yaml
In case you are new to the Google App Engine, please do read the article “Download and Install the App Engine SDK for Go.”
The above file is stored in the folder “$GOPATH/src/github.com/SatishTalim/blobstrex”.
7. File upper.css
Create a folder named “css” under the folder “$GOPATH/src/github.com/SatishTalim/blobstrex”. This “css” folder contains the file “upper.css”.
You can test the app locally and then deploy to the Google App Engine.
Finally, our app displays the map with the location where the photo was snapped or if it does not have EXIF data, the error “Sorry but your photo has no GeoTag information…”.
Note: I would love your feedback on these study notes. If I can make it better, I’d love it!
Google Cloud community articles and blogs
8 
Public domain.

8 claps
8 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Senior Technical Evangelist at GoProducts Engineering India LLP, Co-Organizer GopherConIndia. Director JoshSoftware and Maybole Technologies. #golang hobbyist
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/data-folks-indonesia/setup-apache-airflow-in-multiple-nodes-in-google-cloud-platform-cdc33591d002?source=search_post---------111,"There are currently no responses for this story.
Be the first to respond.
Apache Airflow is one of the most used platform in data engineering world to schedule and monitor data workflows. All using python to create a job make it easier for data analyst, data scientist and data engineers to create and maintain data pipelines.
I use Apache Airflow at work. I heard some companies also use Airflow for their data pipelines. Scalability, extensibility, complexity, and user friendly are the reasons that make Airflow popular in data community.
Existing articles mainly focus in a single node, make me wonder how to make Airflow horizontally scale. This article explains how to install and setup airflow multi nodes in ubuntu using google cloud platform.
In this article we need 2 instance:
Now, create airflow-main instance in Google Compute Engine.
Now, ssh to your instance by ssh username@<ip_address> There is a different name in the name instance. I have already created an instance before writing this tutorial. But, that shouldn’t be a problem. My instance is called indo-large-text
Don’t forget to add ssh key :)
For not being repetitive, I follow the guidelines from this article to setup main node and some modifications. But, these are the ubuntu commands that are similar with the cited article to setup PostgreSQL and Airflow.
taufiq-ibrahim.medium.com
If you haven’t install pip
in case it is not the updated version
Install Airflow Database
In this article, PostgreSQL is used for Airflow metadata.
Create User and Database
Add new user to sudo group
Add user airflow in psql role
Now, GRANT user airflow in psql
Verify
Output:
Change PostgreSQL Configurations
Change IPV4 address to 0.0.0.0/0 and the IPV4 method to trust.
Next, we configure postgresql.conf.
RabbitMQ is a message broker that handles jobs in queue and mange how to distribute jobs into several nodes.
I follow this article to install
To install RabbitMQ and setup the configuration:
Enable RabbitMQ management dashboard
To open the dashboard, open a browser and go to localhost:15672 or <ip_addr>:15672
The default user is guest and password guest. Note that this only apply in localhost. If you access via ip_address you need to create a user.
I encountered errors by following the article above. So, I visited celery documentation to setup RabbitMQ.
create user and password
Install Apache Airflow in Main Node
Initialize Airlfow DB Config and Generate airflow.cfg
Re-config airflow.cfg
Change executor to CeleryExecutor
Change DB connection to PostgreSQL
Change result_backend to PostgreSQL
Disable examples, let True if you want to see the examples of DAGs
Save and run airflow initdb
Create Airflow UI user
Run Airflow and Friends in the background. remove -D to debug
Apache Airflow should run by now and the DAGs may loaded from the examples.
Congratulations!!!
Now you can try try your DAGs and explore Airflow UI.
Next, we need to setup airflow workers in the other instances.
Create Google Compute Engine
Open Google Cloud Console and Point to Compute Engine
Create worker instance:
You will look in the compute engine dashboard
The steps are basically the same as the above. We only need install and change the airflow.cfg in some lines.
ssh to you instance and let’s start to config airflow celery worker. Install Apache Airflow and Its dependencies.
Verify installation
Edit airflow.cfg
Change 3 variables
Save and close.
Now, to verify if the worker is connected to our main airflow. Open airflow celery flower in a browser localhost:5555 or <ip_addr>:5000. You supposedly see like this.
Create DAG:
save this file as get_instance.py and locate to ~/airflow/dags/ in the main and worker instance.
You will see in the Airflow DAGs
Detail DAGs
To check if each task run in the different node.
Congratulations! you should be able to run DAGs in multiple nodes.
I learn this airflow stuff about two weeks. As a person who just migrate from Machine Learning Engineer to SQL Query Maker that often to deal with this thing and often encounter DAG error and wonder how the hell this thing works. I learn by myself via googling, read articles, asking my colleagues who work as data engineer, and from the books.
If you are in the data role and wants to connect with me. I am on Discord andreaschandra#4851.
If you are ✨ RECRUITER ✨and have an open position that mainly dealing with machine learning especially NLP and Computer Vision. Drop a message via Linkedin.
www.linkedin.com
Artikel tentang seluruh yang berkaitan dengan data mulai…
119 
2
A Newsletter to keep you up to date with the data world Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
119 claps
119 
2
Written by
Data Scientist | NLP and Speech Recognition Researcher | Indonesian
Artikel tentang seluruh yang berkaitan dengan data mulai dari machine learning, data analisis, data engineering, data science, business intelligence
Written by
Data Scientist | NLP and Speech Recognition Researcher | Indonesian
Artikel tentang seluruh yang berkaitan dengan data mulai dari machine learning, data analisis, data engineering, data science, business intelligence
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://levelup.gitconnected.com/encrypt-database-backup-and-save-on-google-cloud-platform-2f0421987569?source=search_post---------112,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
After I set up my database backup using a cronjob I was thinking about a good solution to have also backups in case of a server crash.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@timtech4u/connecting-to-cloud-sql-from-vm-instances-on-google-cloud-platform-f43166716346?source=search_post---------113,"Sign in
There are currently no responses for this story.
Be the first to respond.
Timothy
Feb 10, 2019·4 min read
I recently just completed a Coursera course on GCP Fundamentals and this gave me more opportunity to explore additional tools provided by Google Cloud Platform.
Shall we begin..
Google Compute Engine delivers virtual machines running in Google’s innovative data centers. We’ll be deploying an instance in Google’s infrastructure and assigning a Static IP address to it.
A static external IP address is an external IP address that is reserved for your project until you decide to release it. If you have an IP address that your customers or users rely on to access your service, you can reserve that IP address so that only your project can use it.
Login to your GCP Console, click on the Navigation Menu and Scroll down to Compute Engine section, hover your mouse over it and Select VM Instances.
Click on Create Instance and enter desired values / use defaults as below:
Please do not click on Create yet, let’s go ahead and setup a Static IP for our instance. We’ll proceed by clicking Management, security, disks, networking, sole tenancy.
After collapse of the additional options, Select the Networking Tab and click on the edit icon on Network interfaces (we’ll be editing the default).
On the new collapse form we get, click External IP drop-down and change from Ephemeral to Create IP Address, on the dialog box that gets popped up, enter a name for your static IP (leaving other fields as default) and click on Reserve.
Good job, you should now have a collapsed form similar to what I have below, go ahead and select Create (our VM instance would get created with a Static External IP)
Cloud SQL is a fully-managed database service that makes it easy to set up, maintain, manage, and administer your relational databases on Google Cloud Platform. You can use Cloud SQL with either MySQL or PostgreSQL.
On your GCP Console, click on the Navigation Menu and Scroll down to Storage section and finally click SQL.
You’ll get a dialog box, go ahead and select Create Instance.
Select MySQL and Choose Second Generation
Go ahead and input your desired Instance ID, Root Password and Zone.
Click on Create and sit back as your Database Instance gets created with all necessary configurations done by Google Cloud Platform.
Click on the name of the newly created Cloud SQL Instance then Click the Connections tab.
Click Add network and enter the static IP address of your VM instance created earlier, then Click Done and Save.
On the detail page for our Cloud SQL Instance, take note of Public IP
Let’s return to our VM Instance and test our connection.
Click on the Navigation Menu > Compute Engine > VM Instances.
On our created VM, Click on SSH, this would open a new window with SSH connection to our VM instance.
On the SSH window, execute the following commands, remember to change 35.202.255.49 to your own Cloud SQL Instance’s Public IP.
You’ll get a password prompt, enter the password used when creating the Cloud SQL Instance, if correct, you’ll get connected to your SQL Instance.
Awesome! Thanks for reading. 😀 For more guides, checkout the Google Cloud Documentation.
Also, Google Cloud provides $300 for first timers, so feel free to get started using GCP. 🕺
Shout out to Developers Student Club. ✋
Software / DevOps Engineer | Google Developer Expert for Cloud | https://timtech4u.dev
89 
1
89 
89 
1
Software / DevOps Engineer | Google Developer Expert for Cloud | https://timtech4u.dev
"
https://faun.pub/machine-learning-on-google-cloud-platform-first-impressions-4781524924eb?source=search_post---------114,"In the last two years I have used Watson Studio and Paperspace extensively as machine learning platforms. I’ve also explored Azure Machine Learning. In preparation for an upcoming book, I have taken the plunge and migrated the deep learning project I’m currently working on from Paperspace to Google Cloud Platform (GCP). This article summarizes my first impressions of GCP and concludes with a high-level comparison of pros and cons of the four ML development environments that I’ve used.
The good news about setting up GCP is that there is a very clear article that takes you step-by-step through setting up a Google Cloud instance for deep learning. The article had exactly the right tone — fast path to getting a model to work rather than a meandering trip through everything that GCP has to offer. I got almost everything I needed from this article with one exception: the instructions on modifying the Jupyter Notebook config file didn’t work for me out of the box. For the lines to add to the config file:
I needed to make the following modification:
With this change I was able to successfully get the Jupyter environment running and upload a test notebook and data (in the form of a pickled Pandas dataframe).
Once I tried to run the notebook I found a number of imports that didn’t work (including objects in matplotlib and sklearn). I had to do about five iterations to find and install all the missing imports. To get train_test_split I needed to import it from sklearn.model_selection rather than sklearn.cross_validation. I am sure that the mismatches were down the age of the Paperspace environment that was the source of the notebook, and it only took about half an hour to detect and install the missing libraries.
From a standing start it took me about 4 hours to follow the steps in Setting up a Google Cloud instance for Deep Learning and get a DL model running on GCP:
For the sake of comparison, making the same move from Paperspace to an existing Watson Studio setup took me less than 30 minutes. All I had to do was:
All the imports I needed in the DL notebook, including the entire DL stack, were already available to me “out of the box” in Watson Studio.
For a comprehensive comparison of cloud platforms for deep learning, check out this article. It includes a great car metaphor for chips and some very solid technical background. One caveat — take that article’s harsh assessment of Watson Studio with a grain of salt. I’ve used Watson Studio for two years and I am extraordinarily impatient, so if it works for me it can’t be as challenging as the author of the article implies.
To conclude, here are my overall impressions of the four cloud environments that I have investigated as deep learning development platforms:
Google Cloud Platform:
PRO:
CON:
Watson Studio:
PRO:
CON:
Paperspace:
PRO:
CON:
Azure Machine Learning:
PRO:
CON:
Join our community Slack and read our weekly Faun topics ⬇
The Must-Read Publication for Creative Developers & DevOps Enthusiasts
62 
62 claps
62 
Written by
Technical writing manager at Google. Opinions expressed are my own.
The Must-Read Publication for Creative Developers & DevOps Enthusiasts. Medium’s largest DevOps publication.
Written by
Technical writing manager at Google. Opinions expressed are my own.
The Must-Read Publication for Creative Developers & DevOps Enthusiasts. Medium’s largest DevOps publication.
"
https://medium.com/rootusercc/deploy-vm-based-application-%E0%B8%87%E0%B9%88%E0%B8%B2%E0%B8%A2%E0%B9%86-%E0%B9%81%E0%B8%96%E0%B8%A1-rollback-%E0%B8%94%E0%B9%89%E0%B8%A7%E0%B8%A2-jenkins-%E0%B8%9A%E0%B8%99-google-cloud-platform-fb4b3c0475f6?source=search_post---------115,"There are currently no responses for this story.
Be the first to respond.
This article also available in english version. Please click here!
ต้องบอกก่อนว่า Infrastructure หลักๆ ของบริษัท HotelQuickly ตอนนี้ รันอยู่บน Google Container Engine (Kubernetes Cluster) เกือบหมดแล้ว แอพพลิเคชั่นส่วนใหญ่เราสร้างละจัดวางบน Concept ของ Micro-Services ซึ่งจะมีขนาดเล็ก, เบา, รู้น้อย… ขอเรียกแอพพลิเคชั่นที่วางอยู่บนระบบนี้ว่า Modern-System ครับ
แต่ก็เหมือนบริษัทอื่นๆ นั่นแหละ… เรามีแอพพลิเคชั่นอีกกลุ่มนึง ที่มันใหญ่โตมโหฬาร (Monolithic application) แอพพลิเคชั่นอเนกประสงค์ ที่ผ่านการปู้ยี้ปู้ยำมาร่วมสิบถึงยี่สิบ มือด้วยกัน… ซึ่งแอพเหล่านี้ใช้ Nette PHP Framework ในการพัฒนา และรันอยู่บน Google Compute Engine ที่ตั้ง Autoscale Group ไว้ หลัง Load Balance ขอเรียก แอพเหล่านี้ว่า Legacy-System
ในบทความนี้ ผมจะพุ่งเป้าไปที่เรื่องของขั้นตอนการ Develop และ Deploy ลง Production ของเจ้า Legacy-System ที่กล่าวถึงไปนะครับ
ก่อนหน้านี้ หลังจากที่เราเสร็จสิ้นส่วนการ Coding แล้ว สิ่งที่เราทำต่อไปเป็นประจำก็คือ Commit และ Push ขึ้น Git Remote Repository ซึ่งของที่ HotelQuickly เราจะใช้ GitHub ครับ ซึ่งตัว Commit ที่ Developer Push เข้ามานี้เอง จะเป็นตัวจุดชนวนเริ่มต้นเส้นทาง CI (Continuous Integration)​ ซึ่งในทีมผมใช้ Jenkins เป็นตัวจัดการ CI Jobs ต่างๆ ครับ
วิธีการกำหนดขั้นตอนต่างๆ ว่า จะให้ Jenkins ทำอะไรบ้าง เราใช้ Jenkins Pipeline (Jenkinsfile) ในการควบคุมครับ ซึ่งหน้าที่หลักๆ ของขั้นตอนก่อนการเปลี่ยนแปลง จะเน้นเรื่องของการ Run Test อย่างเดียว หลังจากนั้น Jenkins ก็จะส่ง Response ตอบกลับไปให้ GitHub ว่า Test ผ่าน หรือไม่ผ่าน เป็นอันจบขั้นตอนที่ผมเรียกว่า Development Phase
เอาล่ะ! เมื่อทุกอย่างไปได้สวย, Run Test ก็ผ่านฉลุย… เราก็พร้อมเข้าสู่ขั้นตอนการ Deploy… ซึ่งก่อนหน้าการเปลี่ยนแปลง, Developer จะต้องเข้าห้อง Slack #deployment ซึ่งจะมีเจ้า hubot นามว่า @wall-e คอยเฝ้าห้องนี้อยู่ เมื่อ Developer พร้อม Deploy ก็เพียงพิมพ์ข้อความสั่ง @wall-e หลังจากนั้น มันก็จะไปรัน Script ansible-playbook เพื่อ Update working directory บนเครื่อง Production ทุกเครื่องหลัง Load Balance ให้ตรงกับ Commit ล่าสุดบน GitHub Repository เป็นอันจบส่วนของ Deployment Phase
แน่นอนครับ ก่อนที่เราจะเริ่มต้นสร้างอะไร เราควรตอบคำถามให้ได้ก่อนว่า มันเข้ามาช่วยแก้ไขปัญหาอะไร… ซึ่งผมลิสต์รายการปัญหาของระบบเดิมไว้ดังนี้
เมื่อเรารับทราบปัญหาเหล่านี้แล้ว ก็ถึงเวลาที่ต้องจัดการมัน! ซึ่งสิ่งที่ผมทำก็คือ ควบรวมส่วนของ Develop และ Deploy เข้าด้วยกัน โดยใช้ Jenkins CI จะได้รูปตามด้านล่างครับ
จากไดอะแกรมด้านบนนี้ ผมจะใช้ Jenkins ในการ Deploy แทนที่จะให้ Developer เป็นคนจุดฉนวน (แต่ยังต้องอาศัยการโต้ตอบ ในช่วงก่อน Deploy จริงอยู่ดี)
ในรูปแบบใหม่นี้ ตัว Jenkins จะเป็นคนตรวจสอบก่อนว่า Test Case ผ่านหรือใหม่ ซึ่งเจ้า Jenkins ก็จะอนุญาตให้เฉพาะ Test Case ที่ผ่านเท่านั้น ได้เดินทางต่อไปยังขั้นตอนถัดๆ ไป นั่นคือ การทำงานของ Packer ที่จะ Build VM-Image โดยอาศัย ansible-playbook ครับ
เมื่อเจ้า Packer Build Image สำเร็จแล้ว มันก็จะรีบวิ่งมาบอก Developer ผ่าน Slack Channel ว่า “พร้อม Deploy ละนะจ้า” … หาก Developer พร้อมรับชะตากรรมแล้ว ก็กดปุ่ม Proceed บน Jenkins เพื่อเริ่มทำการ Rolling-update ของเหล่า Servers ใน Instance Groups โดยอาศัยคำสั่งของ gcloud SDK command-line. (อันนี้เป็น Alpha Feature อยู่ครับ สนใจเพิ่มเติมคลิกที่นี่เลย)
รูปแบบการ Deploy ใหม่นี้ ช่วยผมแก้ปัญหาต่างๆ ที่ได้กล่าวไปข้างต้น… เราใช้ Jenkins CI เพื่อให้เป็นมาตรฐานเดียวกันกับบริษัทอื่นๆ Developer ก็ไม่สามารถ Push Test Failed Code ได้อีกต่อไป เวลา Rollback ก็ทำได้ง่ายๆ เพราะเรามี Image Version ก่อนล่าสุดเก็บไว้ แถมเวลาจะ Spin-up เครื่องก็ทำได้รวดเร็ว เพราะไม่ต้องเสียเวลา Install นู่น นั่น นี่ จาก Based-image (ไม่ต้องเสียเวลา Provisioning) แค่ Pull image แล้วก็พร้อมรับ Traffic เลยจ้า
หลังจากที่ผมเขียนบทความเสร็จ ก็แชร์ไปตามที่ต่างๆ แล้วก็มีคนแนะนำว่า ให้ลองพิจารณา Spinnaker.io ด้วย เพราะเป็น Google Product เองเลย แถมยัง Opensource อีกต่างหาก… เท่าที่ผมดูคร่าวๆ คือ จะเป็นที่โฟกัสในช่วงของ CD (Continuous Deployment) และมีฟีเจอร์เด็ดๆ อย่าง Blue/Green Deployment หรือ Rollback มาพร้อมการติดตั้งเลย… ลองฟัง Podcast ของผู้พัฒนาแบบเต็มๆ กันได้ที่นี่เลย
ส่วนตัวผมยังไม่ได้ลองเล่น แต่คิดว่าเด็ด ใครได้ลองสัมผัสแล้ว ก็มาแชร์กันได้นะครับ… และในส่วนของ CI อาจจะลองหยิบ Container Registry มาเล่นดูได้ครับ เท่าที่ฟังๆ บางบริษัท ใช้ Tools ตัวนี้แทน Jenkins ไปเลยทีเดียว…
ถ้าถูกใจบทความ ฝากกดไลค์ Facebook Page เตาะแตะต๊อกแต๊ก ทีนะคร๊าบบ https://www.facebook.com/rootusercc/ เป็นกำลังใจให้ผู้เขียนสุดพลัง :-)
ทีมของพวกเรามีขนาดไม่ใหญ่มาก… ทำให้เรามีความ Agile สูงสุดเต็มพิกัด… รับฟังความคิดเห็นของทุกคน พยายามเปลี่ยนโน่น ทำนี่ ตลอดเวลา เพื่อพัฒนาทีมให้เลเวลอัพตลอดเวลา ทีมของพวกเราไม่เคยกลัวความผิดพลาด เพราะเรามีมุมมองว่า นั่นเป็นสิ่งที่ทำให้เราเติบโตขึ้น… ถ้าคุณกำลังมองหาบริษัทที่เป็นเหมือนสนามเด็กเล่น ให้คุณได้ทดลอง ค้นคว้าเทคโนโลยี และเครื่องมือใหม่ๆ สำหรับพัฒนาโครงสร้างต่างๆ ของระบบแล้วละก็ HotelQuickly คือคำตอบที่คุณตามหาอยู่… มาร่วมพัฒนาทีม และโปรดักส์ไปด้วยกัน… ดูตำแหน่งงานว่างได้ที่นี่เลยครับ!
This article also available in english version. Please click here!
The great power come with great responsibility…
15 
15 claps
15 
Written by
Instagram: @chaintng
The great power come with great responsibility…
Written by
Instagram: @chaintng
The great power come with great responsibility…
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/this-week-in-google-cloud-platform-gke-updates-all-around-g-suite-calling-ml-apis-containers-d6f66b0be2d5?source=search_post---------116,"There are currently no responses for this story.
Be the first to respond.
The week started off with some pricing news — “Cutting cluster management fees on Google Kubernetes Engine”, and followed with security best practices for your Kubernetes cluster — “Securing containers with Kubernetes Engine 1.8” #RBAC #LeastPrivilege #NetworkPolicy.
Staying with GKE, we have brand new Google Kubernetes Engine documentation with new overviews, how-to guides, and a task-oriented TOC. Tell us what you think!
And finally, a new GKE Course is now also available on Coursera !
On the other hand, not everyone uses Kubernetes, so here’s a new VM-level container deployment approach — “Introducing an easy way to deploy containers on Google Compute Engine virtual machines” #ContainerOptimizedOS #ManagedInstanceGroup. Check out these related docs : Deploying Containers on VMs and Managed Instance Groups, and Configuring Options to Run Your Container.
Bridging G Suite and Cloud ML APIs is a common use-case and a common request, so here is “Analyzing text in a Google Sheet using Cloud Natural Language API and Apps Script”.
From the “TensorFlow is fun but funner with a cloud” department :
From the “keep moving Machine Learning forward” department :
From the “GCP roundup in a roundup, we must go deeper” department :
From the “In case you missed it (ICYMI)” department :
From the “I commute therefore I listen to podcasts” department :
Finally, some news for one of Google’s most important 2017 projects : “Google Just Bought Enough Wind Power to Offset 100% of Its Energy Use” (fortune.com)
This week’s picture illustrates the above article about Google’s 100% renewable energy :
That’s it for this week!-Alexis
Google Cloud community articles and blogs
13 
13 claps
13 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Google Cloud Developer Relations
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/google-cloud-platform-technology-nuggets-november-16-30-2021-edition-b764c2168a49?source=search_post---------117,"There are currently no responses for this story.
Be the first to respond.
Welcome to the November 16–30, 2021 edition of Google Cloud Platform Technology Nuggets.
The availability of powerful computing options available today along with the needs to localize processing has made architects think beyond the confines of traditional data centers. Edge Computing is often considered as a way to address these architectures but what exactly is Edge Computing, what are some of the considerations to take into account and what are the options available today across Google Cloud solutions to make this happen. In a 3-part series of Edge Computing, these points are explored in detail:
Black Friday sales are a great indicator of how leading brands get their technical infrastructure ready to handle the huge load that is typically seen during this period. In this episode, the kind of scale that Shopify, a leading provider for internet infrastructure for commerce achieved during the Black Friday Cyber Monday (BFCM) sales period is simply mind-boggling.
I quote from the blog post, “peak sales of $3.1 million per minute”, “averaged about 30TB/min of egress traffic across our infrastructure” and more. The Twitter thread here has some more interesting stats and it is interesting to see the scale that MySQL servers handled.
Read more in this blog post.
Security Command Center (SCC) is Google Cloud’s security and risk management platform. It provides visibility into cloud assets, discovering misconfigurations and vulnerabilities, detecting threats, and helping to maintain compliance with industry standards and benchmarks. If you have used SCC to scan your environments, you would have noticed that the findings are reported into hundreds of items. This is likely to result in either some critical findings not surfacing right to the top and most likely will result in the tool not meeting its prime objective.
Just like we have alert fatigue as one of the areas to address in operations support, it is important to mute some of the findings of SCC and instead focus on the ones that matter. Some examples of findings that you might want to mute would include assets in non-production environments, recommendations for using a customer managed key in projects that don’t contain critical data, etc. With mute findings in SCC, you can now setup rules to mute certain findings while still retaining control to manage those settings as needed.
Our next topic to discuss is about interesting work being done to bring more Site Reliability Engineering (SRE) principles to your day to day Security Operations work. The research particularly looks at toil, which is repeatable manual work that you do to address the task. In other words, if you left the system in the same state that it was after the work, it is likely to be toil. What you instead need to look at is to bring one of the guiding principles of SRE i.e. reduce toil. In this blog post, the authors highlight how you can bring SRE principles to your Security Operations Center (SOC) team in a gradual phased manner that is likely to see multiple benefits.
Continuing on Security, sign up for a live online event on December 15th titled Google Cloud Security Talks of 2021. Come and learn about the Zero Trust approach to employees, users and user data.
Google Cloud has made available more than 30 Data Analytics Design Patterns, a set of technical solutions that help customers to use referenceable architecture, models and deployable source code, to jump start their journey. The need for this has come up due to the fact that organizations still struggle to tap into various data sources to gain insights and a set of solutions to address typical domain problems can definitely help here.
The blog post highlights 3 such patterns: Anomaly Detection, Price Optimization and Unified App Analytics but you can definitely reference the entire set here.
If you are looking to understand Machine Learning and have found it difficult to navigate the sheer amount of material available on the Internet, here is a course that I can recommend. The 4-part course titled Making Friends with Machine Learning by Cassie Kozyrkov, which was originally an internal-only Google course, is available to everyone.
The course is designed to give you the tools you need for effective participation in machine learning for solving business problems. It aims to give you a correct understanding of core machine learning concepts, methods, avoid common errors in machine learning, steps to take in leading machine learning projects from conception to launch and improve your ability to communicate with ML experts and non-experts alike.
Here is the entire tutorial (several hours but worth your time):
When it comes to AI, the move to ethical AI outcomes is becoming a priority to ensure that the models are fair in their outcomes. What does it entail to create a framework to ensure that organizations are on their way to delivering fair outcomes. Lopez Research gives a 4-step process framework SEED (Security, Ethics, Explainability and Data) to ensure that:
Read more in the blog post.
First up is the blog post that looks at the next big evolution in Serverless Computing. It is a given that Serverless Computing is here to stay and Functions as a Service (FaaS) has completely changed how developers look at architecting modern event-driven applications. However, developers are pushing the limits in terms of the type of workloads that they would like to fit within a Serverless architecture, are looking at pricing innovations, support for open standards, securing the software supply chain and more. The post takes a look at this evolution and how Cloud Run is well positioned to address this next evolution in Serverless Computing. Cloud Run is one of the most successful products in the last 2 years and has seen significant adoption. If you are a Cloud Run user or planning to learn more about it, this is an essential read.
Moving on, the Serverless series on addressing anti-patterns in Google Cloud continues with new episodes. Two additional posts have been added:
Google Cloud Certification is one of the most sought after certifications out there in the market. There are several Professional level certifications in addition to an associate level certification. The official certification page is available here and it could get confusing to navigate the path in terms of which certification you should do.
Check out this blog post that gives an overview of each of the certifications in a minute.
We have a couple of sketch notes for you to take a look at in this episode of the newsletter.
The first note provides an overview of what Microservices architecture is all about. Take a look.
The second note covers Cloud CDN and what it is all about?
Have questions, comments, or other feedback. Do send it across.
Looking to keep a tab on new Google Cloud product announcements? We have a handy page that you should bookmark → What’s new with Google Cloud.
Google Cloud community articles and blogs
10 
10 claps
10 
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/google-cloud-platform-tech-nuggets-june-16-30-2021-6958d635b705?source=search_post---------118,"There are currently no responses for this story.
Be the first to respond.
Welcome to the June 16–30, 2021 edition of Google Cloud Platform- Technology Nuggets.
Tau VM Family
Compute Engine, a core offering on Google Cloud that provides Virtual Machines, is known for its security, reliability and flexibility vis-a-vis the choice of VMs for running your workloads. While we provide you a range of Compute Engine VM families ranging from standard, high compute, high memory and even custom configurations, we have just introduced our latest offering : Tau VM family that provides cost-effective performance of scale-out workloads.
To help you position the new VM family correctly, we have also put together a post on 5 things that you should know about Tau VMs. As bonus, we have a brand new chart on how to select the appropriate VM family for your workload.
Developer and Customer communities
We have announced investments in two communities that are core to us : developers and customers, with the goal of supporting them to adopt new technologies and use them to solve business problems.
BigQuery Updates
We have announced General Availability of Row-level Security (RLS) in BigQuery. This allows for organizations to restrict users to subsets of data in the same table. Do keep in mind that in addition RLS, BigQuery also supports access controls at the project-, dataset-, table- and column-level.
In one of the key feature releases that is crucial for enterprises to bring transactional data to analytical warehouses, we have announced the public preview of multi-statement transactions in BigQuery. As the official blog post states “Multi-statement transactions support multiple SQL statements, including DML, spanning multiple tables in a single transaction. This means that the changes to data across multiple tables associated with all statements in a given transaction are committed atomically (all at once) if successful or all rolled back atomically in the event of a failure. “
Google Cloud VMware Engine Updates
Google Cloud VMware Engine delivers an enterprise-grade VMware stack running natively in Google Cloud. We have recently released multiple updates to the service, a few are listed below:
For customers in India, Google Cloud VMware Engine is now available in the Mumbai region.
No-Code Development
AppSheet, Google Cloud’s no-code development platform, lets anyone with minimal or no programming experience, build custom applications that can run on desktop or mobile. With the recent announcement by companies to have a hybrid model, when we all return to work, everyone needs an application that helps to streamline the reservation of office meeting rooms, desks and more.
To help you understand how easy it is to create an application using Appsheet, this blog post, in addition to the source project that you can customize, shows how you can take an existing Google sheet and design, develop and automate a fully-functional application.
Google for Games Developer Summit and #BuildonGCP Architecture Series
The Google for Games Developer Summit 2021 is being held on July 12–13. The online event will cover products and solutions, best practices and more that will help developers take the gaming experiences that they deliver to the next level.
In our last edition, we covered the #BuildOnGCP Architecture Series. Here is a blog post that covers a summary of the series, with 13 GCP Architectures for your reference.
// Stay in Touch!
Have questions, comments, or other feedback. Do send it across.
Looking to keep a tab on new Google Cloud product announcements? We have a handy page that you should bookmark → What’s new with Google Cloud.
Google Cloud community articles and blogs
15 
15 claps
15 
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@timtech4u/simplified-continuous-deployment-on-google-cloud-platform-bc5b0a025c4e?source=search_post---------119,"Sign in
There are currently no responses for this story.
Be the first to respond.
Timothy
May 26, 2019·3 min read
If you’re new to Google Cloud Run, feel free to catch up by reading Deploying Containerized Application with Google Cloud Run.
Developers love to focus on writing codes while automating deployment processes. Google Cloud Platform provides such flexibility by providing tools to make deployment seem effortless.
In this article, our focus would be to integrate Cloud Build Trigger with Cloud Source Repositories, GitHub or Bitbucket and also setup continuous deployment to Google Cloud Run.
Our source codes can be found on GitHub here:
github.com
Cloud Build Triggers automatically builds our image whenever there are changes pushed to the build source. We could build our images either by using a Dockerfile or cloudbuild.yaml ( our build config can also be a JSON file).In addition, Cloud Build offers 120 free build-minutes per day.
We’ll be using a cloudbuild.yaml to perform the following operations:
Replace Project ID [projectz-239507] and Service Name [app] with your own values.
We need to add the Cloud Run Admin and Service Account User roles to our Cloud Build Service Account [@cloudbuild.gserviceaccount.com].
Visit IAM and Admin Page and modify the Service Account as follows:
We’ll proceed to creating a trigger that listens to changes on a particular branch on our source codes and performs the operation in our cloudbuild.yaml file.
Visit the Cloud Build Triggers Page
Select your Source Code option and Continue.
Define your trigger, enter it’s Name, Trigger Type and Branch, Build Configuration ( as mentioned, we’ll be making use of the cloudbuild file).
Click on Create Trigger. You can monitor the Trigger on the History tab.
From this point on, anytime you push to your repository, you automatically trigger a build and a deployment to your Cloud Run service. You can learn more about Cloud Build from here.
Thanks for reading through! Let me know if I missed any step, if something didn’t work out quite right for you or if this guide was helpful.
Originally Posted on Mercurie Blog
Software / DevOps Engineer | Google Developer Expert for Cloud | https://timtech4u.dev
126 
126 
126 
Software / DevOps Engineer | Google Developer Expert for Cloud | https://timtech4u.dev
"
https://medium.com/google-developer-experts/journey-to-serverless-on-google-cloud-platform-493fa5434ede?source=search_post---------120,"There are currently no responses for this story.
Be the first to respond.
Google Cloud’s serverless platform lets you write code your way without worrying about the underlying infrastructure. Deploy functions or apps as source code or as containers. Build full-stack serverless applications with Google Cloud’s storage, databases, machine learning, and more while using your favorite language, runtimes, frameworks, and libraries.
Serverless doesn’t mean there are no servers, instead, it takes the workload of managing a server which removes the needs for handling the configuring, provisioning, load balancing, sharding, scaling, and infrastructure management, so you can focus on building great applications.
Serverless computing (also known as No-Ops) distributes majorly across operational and developer experiences, thus ensuring:
Function as a Service (FaaS) and serverless are often used interchangeably, however serverless has more to offer than FaaS. FaaS platforms take a function from developers, build it into an app, and deploy it in the cloud. Serverless is much more than just FaaS.
Google Cloud provides serverless compute options that can fit into your kind of application. These options span across Event-based applications, HTTP applications and also Containerized applications.
Google Cloud Functions is a lightweight compute solution for developers to create single-purpose, stand-alone functions that respond to cloud events without the need to manage a server or runtime environment. Common Cloud Functions use cases could be Data processing, Webhooks, Lightweight APIs, Mobile backend, IoT
Cloud Functions allows you to trigger your code from Google Cloud Platform, Firebase, and Google Assistant, or call it directly from any web, mobile, or backend application via HTTP.
Cloud Functions supports code written in JavaScript (Node.js), Python, and Go. There are no new languages, tools, or frameworks to learn. All you need to do is bring code — including native libraries you bring to the platform.
Cloud Functions provides a connective layer of logic that lets you write code to connect and extend cloud services. Listen and respond to a file upload to Cloud Storage, a log change, or an incoming message on a Cloud Pub/Sub topic.
Learn more about Cloud Functions.
App Engine is a fully managed, serverless platform for developing and hosting web applications at scale. You can choose from several popular languages, libraries, and frameworks to develop your apps, then let App Engine take care of provisioning servers and scaling your app instances based on demand.
Quickly build and deploy applications using many of the popular languages like Java, PHP, Node.js, Python, C#, .Net, Ruby and Go or bring your own language runtimes and frameworks if you choose.
Application Versioning in App Engine allows you easily host different versions of your app, easily create development, test, staging, and production environments.
Traffic Splitting in App Engine enables incoming requests to different app versions, A/B test and do incremental feature rollouts.
App Engine also provides security to your applications by defining access rules with App Engine firewall and leverage managed SSL/TLS certificates by default on your custom domain at no additional cost.
Learn more about App Engine
Cloud Run is a managed compute platform that automatically scales your stateless containers. Cloud Run is serverless: it abstracts away all infrastructure management, so you can focus on what matters most — building great applications.
Cloud Run is built on the Knative open-source project, enabling the portability of your workloads across platforms. Knative offers features like scale-to-zero, autoscaling, in-cluster builds, and eventing framework for cloud-native applications on Kubernetes.
Cloud Run (fully managed) allows you to deploy stateless containers without having to worry about the underlying infrastructure. Your workloads are automatically scaled up or down to zero depending on the traffic to your app. You only pay when your app is running, billed to the nearest 100th millisecond.
Cloud Run for Anthos (on-premises) abstracts away complex Kubernetes concepts, allowing developers to easily leverage the benefits of Kubernetes and serverless together. It provides access to custom machine types, additional networking support, and Cloud Accelerators.
Cloud Run automatically scales up or down from zero to N depending on traffic. It also allows for custom domain mappings and provides SSL for free.
Learn more about Cloud Run
Google Cloud integrate other products into deploying a serverless application and also in monitoring logs.
Deployment to serverless environments such as Cloud Function and App Engine uses Cloud Build in the background and monitoring logs for serverless applications also leverages on Stackdriver.
Cloud Build is a service that lets you easily create continuous integration and delivery (CI/CD) pipelines for your serverless applications.
Stackdriver is a monitoring service that aggregates metrics, logs, and events for your serverless applications running on Google Cloud or on-premises.
Experts on various Google products talking tech.
24 
24 claps
24 
Written by
Software / DevOps Engineer | Google Developer Expert for Cloud | https://timtech4u.dev
Experts on various Google products talking tech.
Written by
Software / DevOps Engineer | Google Developer Expert for Cloud | https://timtech4u.dev
Experts on various Google products talking tech.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/a-to-z-of-google-cloud-platform-a-personal-selection-j-is-for-jenkins-7a718d1f458?source=search_post---------121,"There are currently no responses for this story.
Be the first to respond.
Jenkins is hugely popular and there’s no surprise that you can easily use Jenkins ( and Git) as part of a CI/CD pipeline targeting deployments on the GCP compute services namely compute engine, app engine and container engine.
And as I like to point out GCS ( cloud storage) is a valid target too and I will cover that in detail in the second half of this post !
There are a number of plugins that specifically target GCP and in the first part of this post all I’m doing is rounding them up for you together with a few that are just really useful anyway like the Docker ones.
To integrate Jenkins with your CI/CD deployment pipeline you will need to use a combination of a number of plugins specifically targeted for use with Google Cloud Platform .
If you know of any plugins I’ve missed out please let me know so I can keep the list updated.
Google Container Registry Auth Plugin — This plugin provides the credential provider to use Google Cloud Platform OAuth Credentials (provided by the Google OAuth Credentials plugin) to access access Docker images from Google Container Registry (GCR)
Google Cloud Storage Plugin — This plugin provides the “Google Cloud Storage Uploader” post-build step for publishing build artifacts to Google Cloud Storage
Google OAuth Plugin — This plugin implements the OAuth Credentials interfaces for surfacing Google Service Accounts to Jenkins
Google Source Plugin — provides the credential provider to use Google Cloud Platform OAuth Credentials (provided by the Google OAuth Credentials plugin) to access source code from https://source.developer.google.com as well as https://*.googlesource.com. It supports both kinds of credentials provided by Google OAuth Credentials plugin
Google Deployment Manager Plugin . — Provides the ability to use Google Cloud Deployment Manager to create and delete GCP resources from within Jenkins jobs
Google Metadata Plugin — provides a basic framework for steps in a build’s lifecycle to attach JSON-serializable metadata to a build
The docker image here can be used to create a jenkins docker instance with the plugins described above automatically installed
Jclouds Plugin This plugin provides the capability to launch jenkins slaves on any Cloud provider supported by JClouds. You can use it to spin up jenkins slaves running on Google’s Compute Engine
There are also a series of Docker and kubernetes focused plugins that are also worth configuring :
Kubernetes Plugin — Allows you to use multiple docker hosts managed by Kubernetes to dynamically provision a slave (using Kubernetes scheduling mechanisms to optimize the loads), run a single build, then tear-down that slave.
Docker plugins:
Docker build step plugin — This plugin allows to add various Docker commands into your job as a build step
Docker plugin — use a docker host to dynamically provision a slave, run a single build, then tear-down that slave.
CloudBees Docker Custom Build Environment Plugin — lets you configure your build to run inside a Docker container
Jenkins workflow plugin — A Groovy based domain-specific language (DSL) to allow the creation of scripted workflow steps
or the
CloudBees Docker workflow plugin — This provides a convenient domain-specific language (DSL) for performing some of the most commonly needed Docker operations in a continuous-deployment pipeline from a Workflow script.
Okay if you’ve been following this series you’ll know I said I’d conclude the topic of hosting static sites here. So although the section above about plugins can be read as a stand alone entry like the previous entries in this series the rest of this post assumes that you have read or are at least familiar with the entry for H. This is a hands on walkthrough so you may want to actually save reading the rest of this post till you’re sat in front of a machine if you do want to follow through .
Note from here on in I’m making a number of assumptions if you decide to follow along
If you’re going to follow along I’d suggest a creating a new project first .
Ensure you are using the correct project by typing the following to check the project you are working in:
If it’s not the right project then type
Configuring Git with your content and Cloud repositories
The steps assume you are using Hugo as that is what I used but you can use any static site generator you want . You just need to adjust the instructions below to reflect the location of your content
If you do not already have some web pages set up then take time up to use a static site generator to create some local pages . In the entry for H I discuss using Hugo to accomplish this.
Note: using the hugo command sans server will create content in a public folder in your working directory
The next thing to do is to configure git ( It’s beyond the scope of this already rule busting post to detail installing git follow the link for that)
Change to your Hugo working directory and initialise git in the public folder by typing the following command
then
Log into the Cloud platform development console
When you create a project you automatically get access to a cloud repository. This repository needs to be initialised so from the side menu under tools select development then source code .
Click the get started button
Select Push code from a local Git repository to your Cloud repository
From the public folder follow the instructions you see as shown above.
note: doing it this way round will re initialise git so it will synchronise with your cloud repository.
follow through with the various questions responding appropriately
Make sure you choose the following for the two questions below (This is because we already have content in our local public folder)
Pick configuration to use:
[1] Re-initialize this configuration [default] with new settings
[2] Create a new configuration
Please enter your numeric choice: 1
This project has one or more associated git repositories.
Pick git repository to clone to your local machine:
[1] [default]
[2] Do not clone
Please enter your numeric choice: 2
Then use git to add and commit the files
Then Use the following command to push the changes to your remote repository( in this case all the files you added with the git add command)
Now if you look at the repository you will see the files you pushed.
Create a bucket to use for hosting a static website
Create a target bucket to host your static web site . See the entry for H in this series for a discussion .
For the purposes of this walkthrough we do not need to setup a CNAME as it’s the mechanics of the pipeline we are looking at.
Create a bucket
Leave it empty at this stage
Setting up Jenkins to use GCP resources
Firstly for this stage I hope you are familiar with jenkins as it’s UI is all over the place and finding the right place to do stuff can be frustrating so there are plenty of screenshots in this section!
A BIG WARNING WHEN YOU SET UP YOUR JENKINS INSTANCE CHANGE THE PASSWORD IMMEDIATELY ( yes I shouted that on purpose) try not to lock yourself out as I managed to do as I’d not touched Jenkins for a while .. just saying :-)
GCP have published an article and detailed walkthrough on github on setting up a highly available Jenkins configuration . As I am familiar with that I used that to fire up a jenkins server . You may find it simpler to use the bitnami jenkins image.
The jenkins configuration steps I outline below should work regardless of how you decide to deploy jenkins but I have not tested the bitnami image though so don’t quote me :-)
Once you have jenkins started
Make a note of your cloud repository url or keep a tab open on the repository settings page in the console
Launch the jenkins image.
Once it is ready for use
log in
Note: Both approaches have the required plugins already installed so all you need to do is configure it.
First create a credential by clicking on the Credentials link in the left nav, then clicking the Global credentials link, then clicking the “How about adding some credentials?” link
Click ok
Select Google Service Account from metatdata
Click the create new jobs if you see that or click new item from the left nav. Select freestyle project, enter a job name in the item box
Under source code management select Git and paste in your repository URL from the repository settings page
Configure the post build steps so that it looks like the following:
So at this stage we have your web pages in your repository ( and the build logs . For a production environment you would push the logs to a different bucket ) and your bucket is empty.
Now select build now from your newly created jobs page .
Assuming you have followed the steps above the build artifacts in this case the files in the public folder of your hugo working folder has now been copied to the bucket you created earlier.
You can see what has been copied to the bucket from within jenkins by looking at the Google cloud storage upload report and also by looking at the bucket itself
Now you have some content you can configure the bucket to serve static pages as discussed in the post for H in this series
Jenkins can be set to poll for any changes and automatically push changes to the target bucket or you can invoke a manual build and push .
You may well need to tweak things a little but you should see that creating additional files in your public folder , doing a git commit & push then triggering a build either manually or automatically the updated files get pushed to your target bucket.
Adding jenkins and a cloud repository to your pipeline allows you to:
Google Cloud community articles and blogs
11 
11 claps
11 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Chocolate addict - I have it under control really I do. I do stuff involving cloudy tech. Tweets my own so only me to blame, except for retweets.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/swlh/running-bert-on-google-cloud-platform-with-tpu-555537fd387a?source=search_post---------122,"There are currently no responses for this story.
Be the first to respond.
By 2020, various deep learning models such as Transformer, BERT, and GPT-series make AI become smarter and smarter, using them can achieve many incredible tasks. On the hardware, Google launched Cloud TPU allows programmers to train these “Big” models at a reasonable cost.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/trillian-on-google-cloud-platform-621a37f2431c?source=search_post---------123,"There are currently no responses for this story.
Be the first to respond.
As my better-educated colleague — Annie — pointed out yesterday, “Trillian” is named after a character in “The Hitchhiker’s Guide to the Galaxy” this is (a) excellent; (b) not Greek which is a refreshing change having been submerged in Kubernetes. The eponymous project is a Google OSS contribution: Trillian.
This post is a quick-hit walking through all the hard-work that’s been done by this team in developing Trillian and in making it straightforward to deploy to Kubernetes.
Please see the README on the Trillian GitHub here for the source of these instructions.
For this project (${PROJECT}), you will need ≥12 IP addresses as 12 machines (4+2+4+2) are created by the Kubernetes cluster. The default Quota is 8 and so you may need to request more Quota. You may do so through the Cloud Console:
https://console.cloud.corp.google.com/iam-admin/quotas?project=${PROJECT}&service=compute.googleapis.com&metric=In-use%20IP%20addresses&location=${ZONE}
Select the Quota that represents the “In-use IP Address” for the zone you wish to use, click the “Edit Quotas” and request ≥12 IPs. Await the email confirming the Quota has been approved.
NB Do not proceed until you receive confirmation of the Quota increase because the scripts will fail without sufficient Quota.
Here’s a screenshot *after* the Quota was approved and *after* I’d run ./create.sh as shown in the next section:
You will need a local copy of Trillian in order for the Deployment scripts to build the containers that will be used in the Kubernetes Deployment:
Please edit the config.sh file to correctly reflect your preferences:
We are ready to begin the 2 phase deployment.
First, run:
This provisions the Google Cloud Platform resources: Kubernetes cluster and a Cloud Spanner instance.
Here’s the Kubernetes cluster:
If the cluster does not provision with 4 Node Pools as shown above, you should revisit the instructions. The deployment will not succeed without this configuration.
A soupcon of Cloud Spanner:
https://console.cloud.google.com/spanner/instances/trillian-spanner/databases?project=${PROJECT}
And, a couple of container images:
https://console.cloud.google.com/gcr/images/${PROJECT}/
If everything appears to have completed successfully, you may proceed to the deployment phase with:
This script builds the container images and pushes them to Google Container Registry. Then it creates Kubernetes Deployments that leverage these images.
If this script succeeds, you will be presented with an enumeration of all the Kubernetes resources on the command-line (not reproduced here) and, you may confirm the Deployments (is proceeding | have completed) using Cloud Console:
And:
And:
This brings you to the step in the GitHub README “Next steps”. We’ll want to do something useful with our Trillian deployment to Kubernetes (backed by Cloud Spanner).
You may not yet be using Trillian’s minimal container images but — hopefully — you will soon be using them. When you are, you can’t ssh into a trillian-log-server Pod and run curl commands because the Pods have no shell and don’t include curl. Instead, you can port-forward to test the Trillian API
Assuming you have Trillian deployed, you should have 4 trillian-log-server and 2 trillian-log-signer Pods running. The follow command will grab the 0th trillian-log-server Pod name for us and plonk it in ${LOG_POD}:
Knowing that these Pods all expose port 8091 as an HTTP endpoint, we can then setup port-forwarding from your local machine to the Pod. For convenience, we’ll use the same port on either end:
From another shell, you should then be able to issue the command cited in the Trillian tutorial:
And, all being well, you should receive something of the form:
And, then you may:
And you should receive something of the form:
I’ll update this post next week with something for us to use it for. One of the members of the Trillian team has a nifty example :-)
I was focused elsewhere this week and didn’t get to build a app on Trillian. However, I did gloop around with the API and discovered the next useful method.
Following on from above… with ${TREE_ID} and having called ${TREE_ID}:init:
Or for some bashity to get a random strings of 256 characters...
More soon!
Trillian is an interesting solution from Google. This post has (thus far) shown how you may easily deploy Trillian to a Kubernetes cluster. Now we just need an example application!
Google Cloud community articles and blogs
13 
13 claps
13 
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/experimenting-with-google-cloud-platform-441da66154ba?source=search_post---------124,"There are currently no responses for this story.
Be the first to respond.
Google offers a $300 credit to get started with GCP for free. It’s time to experiment.
What’s needed to sign up for the trial:
Once set up,
Deploying a serverless env in minutes with GCP
Google Cloud community articles and blogs
8 
8 claps
8 
Written by
Tech lover, passionate about software, hardware, science and anything shaping the future • ⛅ explorer at Google • Opinions my own • You can DM me @PicardParis
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Tech lover, passionate about software, hardware, science and anything shaping the future • ⛅ explorer at Google • Opinions my own • You can DM me @PicardParis
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@DazWilkin/starting-w-google-cloud-platform-apis-41221830ba76?source=search_post---------125,"Sign in
There are currently no responses for this story.
Be the first to respond.
Daz Wilkin
Aug 29, 2017·5 min read
Planning a small series of ‘getting started’ posts for those of you, like me, who may get to the point of wanting to write code against a Google service, having a language chosen but then, having not written code for a week or two, I’m stalled by “How exactly do I get started?”
As a Googler in Google Cloud, I use many of these Google Cloud Platform services and occasionally — though less frequently — other Google services. When I have my druthers, I’ll write as often as possible in Golang, Python, JavaScript (Node.js) and C# (Microsoft .NET). Some of my customers prefer me to write samples in Java. Truthfully, I’m (not yet) competent in PHP and Ruby but these round out the languages in which Google provides libraries and sample code. If I’m really good, I’ll use those last 2 in this series too :-)
Google does a wonderful job in providing so-called API Client Libraries for *all* Google’s services for *all* the languages listed above. When Google exposes REST-based services through its public API infrastructure, the service is published to Google’s API Discovery Service and is described by a Discovery document. Client libraries for these services may be machine-generated from the Discovery documents.
This is powerful.
It means that, all being well, libraries in each of the languages is available concurrently with every version of every Google API. But wait, there’s more… there’s a second set of libraries and the rules are different.
For Google Cloud APIs (not *all* Google APIs), Google provides Cloud Client Libraries. For the Google Cloud APIs, you may use either (or both) types of library. For non-Cloud APIs, you may only use API Client Libraries. So, why two flavors?
I won’t duplicate this Google document’s content “Client Libraries Explained”. Suffice it to say that, if the Cloud Client Library is available for the language for the service(s) that you need, I recommend you use the Google Cloud Client Library.
There are 2 caveats to this advice:
In the worrying case that I’ve merely exacerbated confusion, in my next posts, I’ll provide tangible examples for using both of the Libraries in each language. I’m not going to provide a singular example across all of them because that’s unlikely to benefit many of you. I will provide what I think are “missing tutorials” for anyone — like me — who would benefit from such getting started guides.
Before we begin, I’d like to introduce 2 tools that I encourage you to consider using.
Is very useful. It enables you to construct REST-based API calls against any version of any Google service. For example, let’s use it to create (insert) a Compute Engine instance.
The search bar provides autocomplete. Typing “compute” returns all the APIs prefixed “compute”. You can hit return to navigate results including “compute” or you can continue to filter the results by typing “.” and then, because in this case I know what I’m seeking, “instances.insert”:
There are 3 API versions that match this method. The current is “v1”. Once the version is selected, API Explorer provides a form, built using the service’s Discovery document, that can be used to invoke the service. Don’t forget to click the “Authorize requests using OAuth 2.0” slider in the upper right hand corner of Explorer.
Although API Explorer makes it easy to invoke service methods, it’s not always obvious what property and property values should be used. This is where the Google API documentation helps.
For all the Google services, API documentation is provided. From the previous example, a Google search for “Google Compute Engine API” will provide a link to the relevant documentation:
For each API version, the resources (e.g. “instances”) are listed and then the methods that are supported for each resource (e.g. “insert”):
From this we can see how to complete the “Request body” for example in the API Explorer form:
Because of its utility, you should also see that API Explorer is embedded in a right hand pane of the API documentation for many Cloud services.
Unless you have a strong reason not to do so, I recommend you write your code assuming Application Default Credentials (ADCs). As Google’s authorization mechanisms have evolved and Google’s Libraries have evolved, many solutions have been created to reduce the complexity securing your applications. ADCs represent the pinnacle of this process. It is trivial (really!) using ADCs to write code that correctly acquires auth credentials. As importantly, using ADCs means that your code will work (while you debug) on your local workstation, on App Engine, Compute Engine and Container Engine. Another case of, unless you have a compelling reasons *not* to, please default to using ADCs.
If Java makes it this easy… !
Unless you’re developing using Google’s services every day or if you’re new to developing against Google services, the intention of this blog series is to provide some basic guidance for each of the languages for both the primary Library types to help you get started. Google, its services and the Libraries are all changing rapidly.
Hopefully the guidelines in this series will help you remain productive.
Let’s begin…
7 
No rights reserved
 by the author.
7 
7 
"
https://medium.com/google-cloud/this-week-in-google-cloud-platform-gke-1-9-5593f7bb5ceb?source=search_post---------126,"There are currently no responses for this story.
Be the first to respond.
See how Cloud Spanner manages Strong Consistency, Linearizability, Consistency, Serializability, and Eventual Consistency and “Why you should pick strong consistency, whenever possible”.
The Kubernetes 1.9 release from mid-December is coming to a GKE near you via the early access program. “Stateful and ML workloads now run better on Google Kubernetes Engine with the latest version 1.9” (contains the link to join the early access). A few more GKE features are listed in the post. My favorite — Hardware accelerator enhancements (GPUs).
The Google Brain team, led by Jeff Dean has a 2-part “Looking Back on 2017” series: part 1, and part 2. What a year!!
A week after the Spectre and Meltdown disclosure, Google has this follow-up post: “Protecting our Google Cloud customers from new vulnerabilities without impacting performance”
From our friends offering Atlas, a MongoDB fully-managed service on GCP : “New to MongoDB Atlas: Availability across all Google Cloud Platform regions” (mongodb.com)
From the “podcasts for commuters and for everyone else” department :
From the “In case you missed it (ICYMI)” department :
This week‘s picture is from Jeff Dean’s Google Brain recap post #2, which shows how deep learning is assisting pathologists in detecting cancer:
That’s it for this week!-Alexis
Google Cloud community articles and blogs
37 
37 claps
37 
Written by
Google Cloud Developer Relations
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Google Cloud Developer Relations
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/this-week-in-google-cloud-platform-a-north-pole-report-who-did-what-to-my-gke-and-tons-of-d82f84d11aa9?source=search_post---------127,"There are currently no responses for this story.
Be the first to respond.
On behalf of everyone here at Google Cloud, let me wish you a very happy, healthy, and successful 2018! Whether you took an extended break or just enough time to celebrate the New Year, here’s your chance to catch up with two weeks worth of GCP news starting with a report from the North Pole — “Greetings from North Pole Operations! All systems go!” (Google Blog).
From the “yes, we had a few announcements and news while you were out” department :
From the “looking back at 2017 and out to 2018” department :
From the “You might learn a thing or two, and share it with your friends and colleagues” department :
From the “carefully crafted solutions straight from Google” department:
From the “In case you missed it (ICYMI)” department :
From the “call it ML, AI, or deep learning, it’s made immense progress in 2017” department:
Recent addition to the GCP Developer Advocate team and former Docker engineer David Gageot has a couple of posts out :
From the “customers and partners keep on amazing me” department :
From the “a few articles you might find useful” department :
From the “a few deep-dives from GCP articles on medium.com” department :
Finally, GCP Tips (@gcptips) kept going strong during the holidays, check them out!
This week‘s picture comes from the North Pole report :
Buckle up and onwards to 2018 !
Google Cloud community articles and blogs
13 
13 claps
13 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Google Cloud Developer Relations
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/a-to-z-of-google-cloud-platform-a-personal-selection-the-index-39a8fc1a6c99?source=search_post---------128,"There are currently no responses for this story.
Be the first to respond.
This is a list of the Posts in this series
A -App Engine and API’s
B -BigTable
C- compute
D- Deployment Manager
E — I have this entry under A ( cloud Endpoints as it seemed the right place )
F- Firewalls
G- Global Load Balancing
H- Hosting static sites
I-Interconnect
J-Jenkins
K- Keys
L- logging
M- Monitoring
N- Naming stuff
O-object store
P-Pub/Sub
Q- Queues
R-Rolling Updates
S-Streaming ( Transfers mostly)
T-Timeouts
U-Uniqueness
V-VPN’s
W-Wrappers
X-Xternal factors
Y-Yarn
Z-Zero Ops
Google Cloud community articles and blogs
54 
54 claps
54 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Chocolate addict - I have it under control really I do. I do stuff involving cloudy tech. Tweets my own so only me to blame, except for retweets.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/journey-to-serverless-on-google-cloud-platform-67b8d392ffa2?source=search_post---------129,"There are currently no responses for this story.
Be the first to respond.
Google Cloud’s serverless platform lets you write code your way without worrying about the underlying infrastructure. Deploy functions or apps as source code or as containers. Build full-stack serverless applications with Google Cloud’s storage, databases, machine learning, and more while using your favorite language, runtimes, frameworks, and libraries.
Serverless doesn’t mean there are no servers, instead, it takes the workload of managing a server which removes the needs for handling the configuring, provisioning, load balancing, sharding, scaling, and infrastructure management, so you can focus on building great applications.
Serverless computing (also known as No-Ops) distributes majorly across operational and developer experiences, thus ensuring:
Function as a Service (FaaS) and serverless are often used interchangeably, however serverless has more to offer than FaaS. FaaS platforms take a function from developers, build it into an app, and deploy it in the cloud. Serverless is much more than just FaaS.
Google Cloud provides serverless compute options that can fit into your kind of application. These options span across Event-based applications, HTTP applications and also Containerized applications.
Google Cloud Functions is a lightweight compute solution for developers to create single-purpose, stand-alone functions that respond to cloud events without the need to manage a server or runtime environment. Common Cloud Functions use cases could be Data processing, Webhooks, Lightweight APIs, Mobile backend, IoT
Cloud Functions allows you to trigger your code from Google Cloud Platform, Firebase, and Google Assistant, or call it directly from any web, mobile, or backend application via HTTP.
Cloud Functions supports code written in JavaScript (Node.js), Python, and Go. There are no new languages, tools, or frameworks to learn. All you need to do is bring code — including native libraries you bring to the platform.
Cloud Functions provides a connective layer of logic that lets you write code to connect and extend cloud services. Listen and respond to a file upload to Cloud Storage, a log change, or an incoming message on a Cloud Pub/Sub topic.
Learn more about Cloud Functions.
App Engine is a fully managed, serverless platform for developing and hosting web applications at scale. You can choose from several popular languages, libraries, and frameworks to develop your apps, then let App Engine take care of provisioning servers and scaling your app instances based on demand.
Quickly build and deploy applications using many of the popular languages like Java, PHP, Node.js, Python, C#, .Net, Ruby and Go or bring your own language runtimes and frameworks if you choose.
Application Versioning in App Engine allows you easily host different versions of your app, easily create development, test, staging, and production environments.
Traffic Splitting in App Engine enables incoming requests to different app versions, A/B test and do incremental feature rollouts.
App Engine also provides security to your applications by defining access rules with App Engine firewall and leverage managed SSL/TLS certificates by default on your custom domain at no additional cost.
Learn more about App Engine
Cloud Run is a managed compute platform that automatically scales your stateless containers. Cloud Run is serverless: it abstracts away all infrastructure management, so you can focus on what matters most — building great applications.
Cloud Run is built on the Knative open-source project, enabling the portability of your workloads across platforms. Knative offers features like scale-to-zero, autoscaling, in-cluster builds, and eventing framework for cloud-native applications on Kubernetes.
Cloud Run (fully managed) allows you to deploy stateless containers without having to worry about the underlying infrastructure. Your workloads are automatically scaled up or down to zero depending on the traffic to your app. You only pay when your app is running, billed to the nearest 100th millisecond.
Cloud Run for Anthos (on-premises) abstracts away complex Kubernetes concepts, allowing developers to easily leverage the benefits of Kubernetes and serverless together. It provides access to custom machine types, additional networking support, and Cloud Accelerators.
Cloud Run automatically scales up or down from zero to N depending on traffic. It also allows for custom domain mappings and provides SSL for free.
Learn more about Cloud Run
Google Cloud integrate other products into deploying a serverless application and also in monitoring logs.
Deployment to serverless environments such as Cloud Function and App Engine uses Cloud Build in the background and monitoring logs for serverless applications also leverages on Stackdriver.
Cloud Build is a service that lets you easily create continuous integration and delivery (CI/CD) pipelines for your serverless applications.
Stackdriver is a monitoring service that aggregates metrics, logs, and events for your serverless applications running on Google Cloud or on-premises.
Originally posted on Mercurie Blog
Google Cloud community articles and blogs
15 
15 claps
15 
Written by
Software / DevOps Engineer | Google Developer Expert for Cloud | https://timtech4u.dev
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Software / DevOps Engineer | Google Developer Expert for Cloud | https://timtech4u.dev
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/google-cloud-platform-for-sql-practitioners-2b2e4507535e?source=search_post---------130,"There are currently no responses for this story.
Be the first to respond.
The purpose of this article is to give an overview of the SQL features in Google Cloud Platform (GCP). This article is intended for anyone who currently uses SQL and is looking to understand the current options for using their SQL skills on Google Cloud Platform.
If you want to run MS SQL Server on GCP you have a few options, IaaS BYOL, IaaS, and fully managed. A few benefits of running MSSQL Server on Windows Server on GCP:
If your organization has license requirements that limit physical hardware usage or a service provider license agreement (SPLA), you can use Google Compute Engine (GCE) sole-tenant nodes. These instances allow you to import a custom image into GCE, use it to start an instance, and enable in-place restarts so the VM restarts on the same physical server. By using sole-tenant nodes you ensure your VMs run on hardware fully dedicated to your use while limiting physical core usage. To prove your server usage for reporting Stackdriver monitoring allows determine server usage for reporting and export server IDs. More on using existing Microsoft Application Licenses.
If you do not have a physical hardware license requirement you can pay for SQL Server on Google Cloud Platform with your instance cost. This is the easiest way to get started with SQL Server on GCP with no upfront costs and per second billing. This is your flexible license option that allows you to pay for licensing the same way you pay for cloud infrastructure, pay as you go, pay only for what you use.
If you are looking to run SQL server in a fully managed service where the provider takes care of managing the instance for you Cloud SQL for SQL Server is being released and is in alpha as of writing this article. Cloud SQL takes care of backups, replication, patches and updates, and has features such as read replicas for scaling out. Cloud SQL has limits of 10TB for storage, so if you need beyond 10TB you’ll need to consider other options (or clean up your DB :)).
More on SQL server on Google Cloud Platform here.
We previously covered MSSQL server relational transaction processing use cases for SQL. Now we will cover analytics processing with SQL on GCP.
Bigquery is a very powerful serverless, highly scalable, data warehouse with in-memory BI Engine and machine learning built in on Google Cloud Platform. The idea with BigQuery is to focus on the analytics, not your infrastructure.
Enterprises use BigQuery to unlock insights and run blazing-fast SQL queries on gigabytes to petabytes of data. To learn more about BigQuery check out the product page here. We’ll go over some of the more specific features of BigQuery that may be interesting to SQL practitioners in the remainder of this article.
If you are an experienced SQL administrator you can use your SQL experience to operate BigQuery. BigQuery has a mode called Standard SQL which is compliant with the SQL 2011 standard, supports nested and repeated data, allows for user defined functions, and DML.
If you are a SQL administrator you can use SQL queries for data transformation, analytics, and even machine learning in BigQuery.
The syntax and features of both MS SQL Server and Standard SQL are very similar and in most cases you will be changing a function or data type such as DATEADD vs DATE_ADD in your query.
Sqllines.com has a good resource of SQL Server to MySQL migration reference here.
SQL server:
SQL Standard:
More on Standard SQL Reference here.
BigQuery ML (BQML) enables users to create and execute machine learning (ML) models in BigQuery using Standard SQL queries. BQML makes ML accessible to data analysis teams and enables SQL practitioners to build models using existing skills.
ML on large datasets typically requires python experience and knowledge of ML frameworks. These requirements have been restrictive in organizations to a small group of individuals and in many cases organizations just do not have these skill sets yet in existing data teams. Many of these organizations have data analysts who understand their companies data but have limited or aspiring machine learning and programming skillsets.
Analysts can use BigQuery ML to build and evaluate ML models in BigQuery. Analysts no longer need to export small amounts of data into spreadsheets or other applications or wait for limited resources from a data science team.
So what type of insights and machine learning can be done within BigQuery by SQL practitioners? To start understand in BigQuery ML, a model can be used with data from multiple BigQuery datasets for training and prediction.
Types of machine learning models that are supported by BigQuery ML:
After you have a model (dataset) and you have ran the ML.EVALUATE function query, you can use your model to predict outcomes using the ML.PREDICT function.
Example query:
Result:
This is giving you total predicted purchases based on Google analytics data.
More on BigQuery ML here.
Cloud Dataflow SQL lets you use SQL queries to develop and run Cloud Dataflow Jobs in the BigQuery UI. Dataflow SQL uses Beam SQL and this is big for SQL practitioners because previously you could only write Apache Beam or Dataflow pipelines with Java, Python, or Go languages. This functionality opens up a whole new area of data processing for SQL practitioners to explore.
What you can do with this:
Currently you can only read from a PubSub topic or BigQuery table and write to a BigQuery table however I am sure more data sources and destinations will be enabled in the future.
Some possible use cases for Cloud Dataflow SQL
If you are experienced with SSIS and looking for a UI based ETL service, also consider Cloud Data Fusion. While it does not include SQL capabilities at this time it is a code free GCP native ETL pipeline service.
This article was intended to give an overview of some of the SQL options in GCP to show how SQL is supported in many ways on Google Cloud. Lastly, this week Google Cloud databases named a Leader in The Forrester Wave Database-as-a-Service, Q2 2019.
I hope this post was helpful to show some of the options that SQL practitioners can try on Google Cloud Platform!
Google Cloud community articles and blogs
11 
11 claps
11 
Written by
Customer Engineer, Google Cloud. All views and opinions are my own. @mkahn5
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Customer Engineer, Google Cloud. All views and opinions are my own. @mkahn5
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/ha-lamp-and-other-nice-features-on-google-cloud-platform-ec2a9ad44ab2?source=search_post---------131,"There are currently no responses for this story.
Be the first to respond.
I lied. I admit. I know very little PHP for HA but I do know enough to point you to the right direction. This article covers a LAMP deployment but goes on to demonstrate several key features you can use to make your own HA system. It also shows some pretty cool capabilities of GCP live that I’ve described below.
My intent with this post is to show how these components fit together and how you can mix and match whatever you need for a quick deployment. Speaking of deployment, the full code sample is basically a Deployment Manager Script that you can find at the end of this article.
Global HTTPS LoadBalancer:
→ Yes, one IP worldwide. anycast, google’s edge, like none other!
2. SSL termination at LB:
→ We will terminate and manage the SSL session for you at the edge
3. Regional Managed Instance Group:
→ You VMs are spread across a region!
4. Autoscale Instance Group based on CPU usage:
→ Yep, autoscale the cluster
5. Apache+PHP:
→ As mentioned, i don’t know PHP but its just a sample
6. Cloud SQL Master + N replicas:
→ The CloudSQL runs one master and N read-replicas
7. Cloud SQL proxy:
→ Connect from the VM to CloudSQL via the secure Tunnel!
8. Stackdriver Logging with apache plugin:
→ Read Apache specific logs via StackDriver
9. Stackdriver Monitoring with apache plugin
→ Visualize Apache specific metrics via Stackdriver
10. X-Forwarded-For IP address in Cloud Logging
→ Find the user’s origin IP even through our load balancer
11. Stackdriver Trace with custom spans
→ Trace user’s request and emit custom spans
12. GCS Backend Bucket as backend service for static content
→ Serve static content from GCS (w/ CDN!)
All of this may seem like a lot, but there is a handy deployment manager script which demonstrates all the above in several ‘easy’ steps.
This is NOT intended to run in production but to give you a sample to demonstrate various HA capabilities of Google Compute Engine. It also features StackDriver monitoring and Logging as well as Cloud Trace.
Ok, lets get started…i’ll assume you’re new to GCP so..
You can use an existing project or create a new one with a $300 free trial. You are welcome to delete the project or deployment entirely later on to clean up.
Ok, the first step is to install and configure Google Cloud SDK
Once you’re setup, confirm you’re using the correct project and login:
The sample here uses various GCP APIs so lets enable them:
Lets setup some environment variables for the current project:
Now create a ‘docroot’ on our GCS bucket where we will upload our .php files and static content
During runtime, the startup script will download the php files from this bucket.
first download the git repo:
github.com
git clone https://github.com/salrashid123/lamp_ha.gith
then
There are many, many better variations here…you do not have to use GCS…you can use Google Cloud Source Repository, github, and so on. I ended up using this to simplify.
This bucket hosts the static content (images, css, .js). The
/static/* path will be automatically get fetched from GCS so the request never hits the webserver.
First confirm the bucket names to setup for the config files
then Edit cluster.yaml and change the following variables:
(ofcourse, use your project number as shown above )
The following may take 10mins
After which you should see
You can find the progress here with Deployment Manager section of GCP
Once complete, you can confirm on the console too:
Goto “Network Services >> Load Balancing” and note down the IP address.
This is the Global LB IP!
In the case above, its 35.190.80.170
The export an environment variable for later use:
export GlobalIP=your_global_ip
Also note that
/static/* is served by a backend bucket
VMs are in us-central1 (regional)
SSL certificate was assigned to the LB
You can verify that the requisite VM count is up and running and spread across zones for HA:
Note that the VMs are across availability zones
Now we can check each endpoint to see them working.
(ofcourse change the IP to your GLOBALIP)
Open a browser and goto https://GlobalIP/db.php
You should see the following text which denotes a successful connection from PHP to both the master and replica count you setup
Open a browser and goto https://GlobalIP/trace.php
You should see the text similar to the bit below
Doesn’t look like much but on the Cloud Console, goto “Cloud Trace” to see:
What that shows is cloud trace loaded and two spans emitted.
You can see the Loadbalancer logs by going to Cloud Logging → LoadBalancer
Then to go see the apache logs, see:
Note, the apache configuration accounts for X-Forwarded-For Header in the apache configuration file:
Which means the actual origin IP address of the browser is sent through to apache.
The Apache FluentD plugin is disabled at the moment because it cannot process structured exports. That is, the following is commented out for now:
(for reference, see issue#10)
The sample DM script installs and configures not only Stackdriver, but the Apache Plugin as well for Stackdriver.
What that means is you will see both the default Host Metrics:
but also the Apache specific (!) metrics
Goto
https://GlobalIP/static/google-cloud.php
You should see a simple logo. This being served directly from a GCS Bucket:
Ok, enough steps and enough LAMP…but I hope you found some of this useful..you can mix and match what you need…or none of it via DM…but my intent is to give you some ideas on what you can try out on GCP with or without LAMP or Deployment Manager!
Google Cloud community articles and blogs
28 
28 claps
28 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/@timakin/gke-go%E3%81%8B%E3%82%89datastore%E3%81%AF%E3%81%98%E3%82%81google-cloud-platform-api%E3%81%B8%E6%8E%A5%E7%B6%9A%E3%81%99%E3%82%8B-e30de718da98?source=search_post---------132,"Sign in
There are currently no responses for this story.
Be the first to respond.
timakin
Sep 21, 2017·4 min read
最近GKEを触る機会がありまして、それまでGAEしか触ったことのなかったため、Datastoreへの接続方法について戸惑ったため、備忘録として作業ログを残します。
GAEを使った方はわかると思うのですが、GAEは内部で持ってくれているクレデンシャルを元に、よしなにDataStoreへ接続することができます。
しかし、GKEだとそうはいかず、Google Cloud PlatformのAPI経由でアクセスする必要がありまして、そのためには権限設定やクライアントパッケージが標準のものしかなかったりして、手順や使い勝手が微妙に複雑です。
今回はその手順と作成したクライアントライブラリについてのメモです。
まず、そもそもDataStoreをインスタンスから叩くことができなかったら何もできません。
GCPのコンソールに入って、左メニューの「データストア」から、DataStoreのAPIを有効にします。
その後、「APIとサービス」の管理画面にいって、k8sのpodからDatastoreを叩けるように、サービスアカウントの発行を行います。
JSON形式でサービスアカウントのkeyを発行したら、任意の場所に保存します。
GKEのプロジェクトのコンテナに、keyを保存してあげる必要があります。
以下のコマンドでk8sのインスタンスに権限を付与することができるので、実行します。
GCP_KEY_JSON_PATH は必要な値に置き替えてください。 先ほどダウンロードしたサービスアカウントのキーが置いてあるpathです。
保存されたシークレットを、コンテナで読み出すために、例として以下のようにkeyをマウントしてあげる必要があります。
重要なのは volumes, volumeMounts ,envあたりです。
GOOGLE_APPLICATION_CREDENTIALSに、サービスアカウントを設定してGoogle Cloud Platform APIをコンテナが叩けるようにします。
記入したら、忘れずにdeploymentをapplyして、k8sの設定を更新します。
GAEからGKEに移ると、主に上記のような権限周りで戸惑うことが多いです。
kubectl経由でキーを保存したり、コンテナで読み込むフローに一度慣れれば楽ですが、上記手順を記憶してないと詰まるので、同じように課題を感じた方はご参照ください。
サービス アカウントを使用した Cloud Platform への認証
18 
18 claps
18 
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/google-cloud-platform-technology-nuggets-october-16-31-2021-edition-9cb980f9fcd7?source=search_post---------133,"There are currently no responses for this story.
Be the first to respond.
Welcome to the October 16–31, 2021 edition of Google Cloud Technology Nuggets.
Missed Google Cloud Next 2021 ? Don’t worry, we have you covered with our post titled “Google Cloud Next 2021: What You Missed”.
At Google Cloud Next, ’21 , we announced Spot VMs, a highly affordable Compute Instance that is suitable for batch jobs and fault-tolerant workloads. If you are familiar with Pre-emptible VMs on Google Cloud, think of Spot VMs as having a new dynamic pricing model that provides a discount between 60–91% of the list prices. Spot VMs can be used in your GKE clusters too and provisioning Spot VMs via the gcloud command line tool is as simple as the flag : “-provisioning_model=Spot”.
Spot VMs upto 91% discount of the list prices is available in the regions given below:
Check out the blog post that provides more details on Spot VMs, the pricing model and availability.
Google Cloud Architecture Framework , version 2.0 is out ! The Architecture Framework is a set of best practices across 6 pillars given below, to design and build workloads on Google Cloud.
These best practices have been collected over years of learning to build solutions on Google Cloud with our customers. Each of the pillars is further broken down into specific areas to help you zoom into a service/solution and best practices to build, deploy and operate it securely and reliably.
In our customer story for this edition, Developer Advocate Priyanka Vergadia caught up with James Prompanya, Senior Engineering Manager at Niantic Labs who leads the server infrastructure team for the popular game, Pokémon GO. A couple of key services that are used by the game are Google Kubernetes Engine (GKE) and Cloud Spanner. Learn more about the solution architecture given below and other interesting facts about running Pokémon Go on Google Cloud.
Identity and Access Management (IAM) is one of the most crucial aspects of configuring and running your workloads in the cloud securely. IAM services along with their policies, constraints and large number of roles, can make it daunting for anyone to understand and get it right. Forest Brazeal, Head of Content at Google Cloud, gives his perspective on how Google Cloud IAM gets it right in the article “9 things I freakin’ love about Google Cloud identity & environments”.
In our October roundup for CISO perspectives, we having collected a rich collection of articles that address 3 key areas:
If you are into securely running workloads on Google Cloud, this is an essential read.
The first step in using any of the Data Services that Google Cloud Platform provides is often how we ingest data into GCP. In this guide, check out the planning that needs to go into data ingestion. If you are an organization looking to understand the best practices around planning, taking your first steps and controlling the costs to get data into the cloud, this is your guide.
Cloud Data Loss Prevention (DLP) is now automatic. Currently, it is available as a preview for BigQuery only and the automatic stands for: automatic discovery, automatic inspection, automatic classification and automatic data profiling. This is part of Google Cloud’s Invisible Security vision, where the capability to understand and protect your data is engineered into the platform.
A new series on addressing anti-patterns in Google Cloud Functions has been announced. In the first article, you get a look at how to write idempotent Cloud Functions, both foreground and background Cloud Functions and address this important aspect on ensuring that in case your Google Cloud Functions get called multiple times for the same unique operation, you can handle that and not create an incorrect or additional transaction.
If you are looking to understand Event Driven Architecture (EDA) and see how various Google Services come together to achieve that, check out this introductory article.
Looking to deploy an application to Cloud Run? Your workflow typically consisted of a 3-step process: Write the Dockerfile, build the Docker image and then use the deploy command. With support for Google Cloud Buildpacks, you can now eliminate the first two steps completely by simply using the deploy command. Buildpacks will detect the language, build out the container image and deploy the same for you. Try out Source based deployment in Cloud Run today.
In this edition, we have an overview of Google Cloud Networking. Our Networking Services are a key differentiator for GCP, ranging from a private global network, multiple connectivity options, Security Perimeters and much more. Check out the blog post for more details.
Looking for session recordings from our recent Google Cloud Next 2021, simply visit the on-demand catalog and check out the sessions at your convenience.
Have questions, comments, or other feedback. Do send it across.
Looking to keep a tab on new Google Cloud product announcements? We have a handy page that you should bookmark → What’s new with Google Cloud.
Google Cloud community articles and blogs
13 
13 claps
13 
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/google-cloud-platform-technology-nuggets-september-1-15-2021-edition-7ad3bdb68598?source=search_post---------134,"There are currently no responses for this story.
Be the first to respond.
Welcome to the September 1–15, 2021 edition of Google Cloud Platform- Technology Nuggets.
Google Cloud Next is happening on October 12–14, 2021. Hope you registered for it? The full event catalog is now live and you can build your own customized playlist of sessions.
Google Cloud is now available in Toronto and this makes it our 2nd region in Canada. With Toronto going live, we now have 28 regions across the globe. You can check for more details on our services across locations over here.
Disaster Recovery (DR) is an important facet for our customers and while we have several solutions including Marketplace solutions from our partners, it probably can be confusing in terms of how you can bring it together and yet meet the customers requirements around DR. In this blog post, you can take a look at basic concepts of DR and see a fictional example of how an organization with stringent RTO/RPO can utilize an array of solutions ranging from Actifio, Zerto and Powerscale.
Site Reliability Engineering (SRE) is gaining mindshare across customers, who want to ensure that they balance software development agility and reliability of their services. Lowe’s, a leading retailer has met with solid success in their adoption. In a two-part series, we cover how they went from one release every two weeks to 20+ releases daily and decreased their mean-time-to-recovery (MTTR) by over 80 percent.
Don’t forget to check out SRE Google Cloud website to learn more about implementing SRE best practices in the cloud.
It has been a busy time in the Data Analytics space and one of the ways to catch up on all the announcements in this space is via the monthly recap. The announcements during the last month include Google Datasets, Pub/Sub message retention, workload management in Big Query and more.
We also published a detailed guide on how to set up an efficient and incremental data ingestion pipeline into Big Query. This is specifically useful when the source data is large in size and the only way is to look at designing an incremental way to do that.
Google Translation is a well known service and API, that supports over one hundred languages, with built-in language detection. In this blog post, we share 4 best practices for using Google Translation for translating your website. It includes securing access to the API, architecture optimization for best performance, setting and monitoring the cost budget and utilizing the custom terminology feature for a specific domain.
Machine Learning Engineers are responsible for training the ML models. Often the training does not complete successfully. Check out 7 tips that offer a hint into what could be going wrong and best practices to avoid that from happening.
Google Cloud Shell, your development machine in the cloud, is an excellent tool available to anyone with a Google Account. Cloud Shell comes with command-line tools and programming languages SDK all pre-installed for you. It is often used as a companion tool to try out commands, codelabs and more. In what is possibly a great usability addition to it, you can now run your code samples directly from the Google Cloud documentation pages. No need to switch anymore. Check out the screencast below.
Remember App Engine, our Platform As a Service (PaaS) offering? If you are running legacy App Engine applications with older runtimes of the supported languages, we are committed to supporting them but you should note that we have started a new series titled Serverless Migration Station, that helps you to modernize your applications to our newer serverless services. The series is currently focused on Python and App Engine.
The Serverless wave often makes us think if Virtual Machines are relevant in our discussions anymore. Here is an interesting conversation between a VM proponent and a VM skeptic, that helps to understand the relevance of VMs even today.
The bonus section usually provides you a service or two that you learn about in an easy to digest manner. This edition is no different and we start off with Traffic Director, our fully-managed traffic control plane for service mesh. Check out the blog post for more details.
We concluded our series on Big Query Administration, where there were deep dives on topics ranging from Tables, Storage Internals, Query Optimization and more. Check out this recap blog post that lists all the sessions in a single page. A very handy page to bookmark, if you are responsible for Big Query Administration in your organization.
You can check out a detailed post on understanding Cloud SQL too. Bonus: It has a sketch note too!
Have questions, comments, or other feedback. Do send it across.
Looking to keep a tab on new Google Cloud product announcements? We have a handy page that you should bookmark → What’s new with Google Cloud.
Google Cloud community articles and blogs
14 
2
14 claps
14 
2
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/my-2018-countdown-with-a-tip-a-day-about-google-cloud-platform-2261161cb37?source=search_post---------135,"There are currently no responses for this story.
Be the first to respond.
A few weeks ago, I’ve started a new blog dedicated to Google Cloud Platform, to share tips’n tricks I come across while exploring the platform, getting to know new products, or gathered through experience with a particular service I’ve been using:
With the holidays season, I went with a “2018 countdown” approach (like an “advent calendar” without the religious connotation), where I publish a tip every day of the month of December.
As of today, December 18th, we already have 18 tips available!
Those tips span about a dozen technologies! (as you can also explore the tips via “tags”, which represent a technology / service / API / product each)
Initially, I thought it would be a challenge to author tips every day (without even thinking of the fact you have to publish tips during weekends, holidays or vacations), but there are actually plenty of tricks to share. Furthermore, I opened the blog to contributions: anyone who wants to contribute a tip or two is welcome, and should just share with me a quick gist describing the tip. I’ve already received a bunch of contributions, from 8 distinct authors. Thanks a lot to Alexandre, Bastien, Fabien, Graham, Jim, Mark, Victor, or Wassim!
Although it all started as a tip a day for the 2018 countdown, it won’t stop there. Perhaps the frequency will be a bit lower (once a week? more?), but I definitely intend on continuing sharing tips on a regular basis next year and beyond!
If you want to help, please spread the word! Tell your friends and colleagues about the site: https://googlecloud.tips/.
Also please follow the @gcptips Twitter account where new tips’n tricks are announced.
And if you’ve got some time, don’t hesitate to share your own tips! All help is welcome :-)
Google Cloud community articles and blogs
27 
27 claps
27 
Written by
Developer Advocate for Google Cloud Platform, Apache Groovy programming language project VP/Chair
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Developer Advocate for Google Cloud Platform, Apache Groovy programming language project VP/Chair
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/a-to-z-of-google-cloud-platform-a-personal-selection-c-is-for-compute-2e5f0293e463?source=search_post---------136,"There are currently no responses for this story.
Be the first to respond.
Compute is a product any Cloud provider provides and they tend to provide a range of instance types based on CPU or memory. It’s the important but not particularly differentiating service on the surface anyway they all provide. Under the surface though there are differences and you need to understand those to make a considered choice. What I like about Google Cloud Platform (GCP’s) compute is that it differentiates in a number of ways the following are the ones that always leap to the front of my mind:
Custom machine types
Preemptible VMs
Transparent maintenance
Sustained use discounts
I won’t talk about performance though as last year GCP put out a couple of blog posts here and here and they speak for themselves I really have nothing to add here it’s just blindingly the most preformant imho. Plus I am trying to promise not to regurgitate official docs & posts but give you my take on things . If you want to dive into the performance aspects of GCP’s Instances I suggest starting with perfkit benchmarker
Custom machine types are something I’ve been hearing as a a please do this for years no matter what Cloud I happen to be working with, so when Google delivered I was doing that fist pumping thing and I’m a Brit so afterwards was appropriately embarrassed! That aside it really is cool as many people who are moving from an on-premises hosted DC’s are used to dealing with a fixed pizza box and never actually being able to match their application usage in terms of RAM & CPU exactly and usually in my experience wasting CPU and RAM. Don’t get me wrong in a lot of cases those standard instance types will be absolutely fine and have been chosen to meet the majority of use cases . But those cases where you need to squeeze the absolute maximum out of your chosen instance type then custom machine types ride to the rescue. Custom machine types let you create an instance with a custom number of vCPUs and amount of memory you can do this by logging into the console and literally use a slider to configure, use gcloud to pass the config in from the CLI or use the API. All funky but it pays to understand how your application works and understand what it really uses in terms of RAM & CPU by collecting metrics if you don’t know . It’s probably better to use a standard instance type and maybe move to custom types later when you fully understand your application in terms of resources it actually uses . I see a future that having a stupidly long list of instance types just being not a thing as custom machine types becomes the norm make it so GCP :-)
Preemptible VMs well I love these no really I do ! They are VMs that have a definitive end of life that can last up to as long as 24 hours but they are definitely gone by 24 hours. This means you can get them at a reduced cost around 70% cheaper than the standard instance cost . No you don’t need to bid for them and watch market prices the cost is fixed and you know that it has a limited life span that is no longer than 24 hours! Just an easy way to get hold of cheap instances that can be used for workloads that need supplemental compute and can cope with the fact they will be terminated at short notice. Use cases for these types of instances are great if you have designed your application to be be decoupled and/or are able to not blink at one of these instances disappearing. You can keep the disks by making sure you set the Persistent Disk (PD) options to not auto delete they are using so another instance can continue from where it left off . Great use cases :
Running Worker tasks in batch processing jobs e.g for Dataproc / hadoop clusters
Spinning up extras instances to assist with processing messages in pub/sub queues
Providing extra processing power to say HPC clusters, rendering tasks etc
Transparent maintenance — when I think of this I just think of how awesome live migration is . There’s always patching & maintenance required for the servers that host the instances and Google is fanatical about maintaining patch levels and rolling out updates …
Live migration automatically moves your running instance to another host in the same zone.
For those applications that depend on that same instance being up and running such that a restart means you actually incur downtime live migration capabilities has got to be a key decision point . As an example think of games where you run on a dedicated single server and shutting down basically kicks off all players using that server. You can immediately see the advantage of using Live migration.
There is also the ability to set your instances to terminate & reboot if you prefer though!
Sustained use discounts — This is a headache free way to automatically receive discounts . It really is what it says . If you keep an instance running for more than 25% of a month then you automatically get a discount for every incremental minute you use for that instance. The longer your instance is up and running for that month the greater the discount applied and you can get up to a 30% net discount for instances that run the entire month. You don’t have to make any commitments or worry about what instance qualifies if you’re running an instance that qualifies it just kicks in.
Google Cloud community articles and blogs
8 
8 claps
8 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Chocolate addict - I have it under control really I do. I do stuff involving cloudy tech. Tweets my own so only me to blame, except for retweets.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/@salesforce/how-salesforce-and-google-cloud-platform-are-simplifying-big-data-3646f74bee89?source=search_post---------138,"Sign in
There are currently no responses for this story.
Be the first to respond.
Salesforce
Aug 12, 2015·4 min read
To check out a demo of Google Cloud Dataflow and Wave Analytics in action, check out this video.
Earlier in May we announced Salesforce Wave Analytics for Big Data. Leading innovators joined our Analytics Cloud ecosystem to extend the use cases for big data to the business user. Among those companies was Google, who is announcing the general availability of Google Cloud Dataflow, a fully-managed data processing service on Google Cloud Platform. We are excited to be part of this major announcement.
But what does that mean exactly from a technology point of view? And why does it matter to the business user?
You’ve heard it many times — Big Data is awesome and can drive amazing results for many business functions, from streamlining operations to influencing product development. Yet traditional data architecture for processing big data can still be very complex, and require multiple technology layers:
Simply put, it can be a very fragmented and complex architecture that’s hard to set-up and hard to maintain.
If there’s one thing to remember about the combination of Google Cloud Dataflow and the Wave Analytics Platform, it’s that we remove all that complexity.
Google Cloud Dataflow is a simple, flexible, and powerful tool that can perform data processing tasks of any size. It overcomes complexity at both the architecture and programming levels. And it automates data processing in both batch and stream mode.
First, the architecture is simplified. Big Data from various sources can be combined with customer data from Salesforce. There’s no need for separate ETL or data preparation tools, Cloud Dataflow takes care of it. There’s no need to manage distributed systems.
Secondly, there’s no need to use old-school MapReduce, a complex programming framework for the processing of large datasets that’s also well-known for latency limitations and occasional lack of performance. Cloud Dataflow is really designed to handle very large datasets and complex workflows — and do it simply. Once data is processed and enriched, it can be pushed to Wave with a native connector (this connector was built by another of our great partners, SpringML, see their blog here).
Now that our big data is in Wave it’s ready for the business user! Any user can freely explore and drill through that rich data, run various visualizations and consume dashboards to get insights — on any device. Faceted and multi-dimensional dashboards can be built in minutes. Any business user can drill down into their dimension or measure of choice.
And because Wave Analytics is native to Salesforce, users can use Chatter directly from inside Wave so that their teams can collaborate on insights from anywhere. Just like Google Cloud Dataflow, Wave Analytics removes the complexity that is inherent to traditional, legacy BI and analytics solutions.
Need an example? Let’s imagine a media company, called Acme Media, that uses Salesforce as their global CRM solution. When a sales rep closes an opportunity with an advertiser, the status of that opportunity object will be updated to “Closed” in Salesforce. It will then be entered in DoubleClick as an order. DoubleClick is Google’s ad technology that manages digital advertising. DoubleClick then proceeds to serve the ads.
But that sales rep needs visibility into ad performance. With Google Cloud Dataflow, Acme Media can process very large datasets in near real-time, so they can be aggregated, transformed and enriched on the fly. Then that data will be sent natively to Wave Analytics, where that sales rep can analyze, explore, and consume it as a set of drillable dashboards.
So now our sales rep constantly has access to the latest information, on any device. For example, he might notice that the ad is performing poorly in a particular demographic segment or geographic region, and he may want to modify the ad immediately from his mobile phone. By accessing fresh data and analyzing it at the point of decision, every sales rep at Acme can now take appropriate steps at any time to better target their audience.
With Google Cloud Dataflow and Wave Analytics, this is now simple and business users can easily analyze massive amounts of customer data and turn Big Data into actionable insights.
Connect to Your Customers in a Whole New Way
See all (4,030)
11 
1
11 claps
11 
1
Connect to Your Customers in a Whole New Way
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@DazWilkin/deploy-docker-engine-in-swarm-mode-to-google-cloud-platform-8b88a5a43be8?source=search_post---------139,"Sign in
There are currently no responses for this story.
Be the first to respond.
Daz Wilkin
Jul 9, 2016·10 min read
At DockerCon, Docker announced the addition of a swarm mode to Docker Engine. Swarm mode requires Docker Engine v1.12.0-rc1 and, is best experienced when run on a cluster of nodes in a so-called Swarm.
I thought it’d be useful to deploy a Swarm to Google Compute Engine (GCE) in order to follow along with the tutorial provided by Docker. So I created some simple Bash scripts to do this before realizing this was a good opportunity for me to teach myself the basics of Google Cloud Deployment Manager (CDM). CDM is Google Cloud Platform’s native deployment tool. Like similar tools ( Chef, Puppet etc.), it’s syntax is intentional rather than declarative and therefore facilitates composability (reuse).
The result of my adventure is a basic CDM Python script and YAML configuration that should permit you to deploy a Docker Swarm comprising one manager and your choice of number of workers to GCE. You may use the Swarm to work through Docker’s Swarm tutorial in order to become familiar with Swarm or as the runtime for your own Swarm deployments.
If you simply want to get a Swarm deployed, please jump ahead to “Deploying the Swarm using CDM”.
If you’re interested in understanding how I built it, please continue reading here.
Docker provides a Bash script to install Docker releases. To help with my understanding and debugging and because I chose to deploy only to Ubuntu 16.04 LTS, I can simplify the script:
To create a Swarm, one of the nodes must:
Other nodes then join this swarm with:
For simplicity, I create a single manager node with a hostname of “swarm-master”
You can run these commands for yourself on multiple nodes to spin up your own Swarm but, I wanted to run my Swam on Compute Engine (GCE)
The simplest way to create a manager node and worker node(s) on compute engine is to create compute engine instances that use a version of the Bash script outlined in How to install Docker v1.12.x on Ubuntu 16.04 LTS.
If you’ve not used Google Cloud Platform before, start here. Otherwise, assuming you have a project [PROJECT] and have the gcloud command-line tool installed, a script to create the manager node is:
Replace [PROJECT] with your project’s ID. You may find your current project’s ID here. You may choose to use a different zone. If you wish to use a different name for the master, you must not only edit the value of the MASTER environment variable but also change the scripts where you see ‘swarm-master’.
The only difference between the startup script used by the manager and the workers is that the manager initializes the swarm.
It takes GCE under 2 minutes to provision the VM, install Docker and start it.
You may check progress by SSH’ing into the MASTER and tailing the startupscript.sh log:
When everything has completed satisfactorily, you will see:
The command to create the worker nodes is:
As before, please replace [PROJECT] with your project’s ID. The only difference between the startup script used for the workers and that used for the manager is that the workers join the swarm that was created by the manager. The script has no dependency checking for the existence of the manager (‘swarm-master’) so you should run the workers script after the manager script.
There are two differences between the manager and the workers script. The workers script creates a Compute Engine Instance Template. This template is passed to the command that creates a Managed Instance Group (MIG). A MIG is Compute Engine’s construct for creating an arbitrary number of VM clones. In this script, we create ‘3’ VMs but you may change this as you wish.
It is common to apply an Autoscaler to a MIG in order to scale the number of clones on demand by CPU, some monitoring metric or by the incoming traffic load from a load-balancer. For our purposes, a static number of VMs is sufficient.
Optional: If you would like to add an autoscaler, the following command will create one that will scale the MIG to a maximum of 5 instances and attempt to maintain an average CPU utilization of 80%:
You should not need to do so but you could SSH into any of the worker VMs and tail the startupscript.sh log to see that the worker has started correctly. Each worker VM is ready with Docker installed within approximately 2 minutes.
To test that the Swarm is ready, you can SSH into the master (!) and enumerate its nodes:
When only the manager is running, the command will return something similar to:
As the other nodes come online, the command will return:
You may then proceed with the Docker tutorial for Swarm from the step Deploy a Service. To avoid having to ‘sudo’ every Docker command, you can add your $USER to the Docker group with:
When you are done, you may delete everything using the following commands:
To use Deployment Manager is mostly a process of converting the gcloud commands that you used previously into their equivalent REST API calls and scripting them, in this case using Python. Deployment Manager also supports the use of JINJA templates but I found Python to be more…. effective.
CDM uses YAML for its configuration files. Where I used environment variables with the Bash scripts, I converted these into CDM properties. I took advantage of CDM configuration and pulled a few more variables out from the Python script too including the names of the swarm master and workers etc.
CDM expect the Python script to have a method called GenerateConfig that takes a context (that includes the YAML configuration properties) and returns a Python Dictionary containing the resources to be created on GCP. As with the Bash scripts, we need a master VM, an arbitrary number of workers VMs managed by a MIG and built from a Template. So, here’s the skeleton of the Python script:
It’s clear that we need a consistent way to define resources to CDM so that it may build these resources on Compute Engine. The Compute Engine API is the definitive way to interact with Compute Engine to build resources. gcloud commands are converted into REST calls. And so, it should come as no surprise, that CDM effectively represents this API’s methods and request objects as the way to define resources. How does the developer know that the Template has a type of ‘compute.v1.instanceTemplate’? That it needs a name, properties etc. etc.? There are two ways to determine this. The easiest is to use the Cloud Console to do the hard work for you and then have it show the REST equivalent:
An alternative and more precise approach is to review the Compute Engine API documentation. This approach shows you the definitive required parameters and helps you drill down as necessary. For Instance Templates, the link is:
https://cloud.google.com/compute/docs/reference/latest/instanceTemplates/insert
Once the Python script and the YAML configuration are ready, we can deploy the template to Google Cloud Platform (GCP) and explore our Docker Swarm.
At this point we have a Python script (dockerswarm.py) that receives contextual information (e.g. project and zones details) from the CDM configuration file (dockerswarm.yaml) when deployed to the CDM service. The result is a series of orchestrated REST API calls to Google Cloud Platform (GCP) that create:
Assume both files have not been renamed and exist in the same directory, the command to deploy the result to Google Cloud Platform is:
‘docker-swarm’ is the name we want to give the deployment and dockerswarm.yaml refers to our configuration template. This should result in:
Your operation name will be different. You may view the result using Cloud Console:
And, if you click on ‘docker-swarm’:
And, if you click (for example) on swarm-worker-mig, you will see:
And, if you then click on the right hand side “MANAGE RESOURCE”, you will see:
As before, you can then access the Swarm by SSH’ing into the manager and running commands against it:
And, to tear everything down, simply delete the deployment:
My original goal was to investigate the new ‘swarm mode’ in Docker Engine. I detoured through teaching myself Cloud Deployment Manager and hope that this recount of my experience is of interest to you.
Next steps include creating a MIG to autoscale the Swarm managers and learning how to use CDM Runtime Configurator in order to more elegantly spin-up the Swarm nodes.
dockerswarm.yamldockerswarm.py
https://docs.docker.com/engine/swarmhttps://github.com/docker/docker/releaseshttps://experimental.docker.comhttps://cloud.google.com/https://cloud.google.com/compute/https://cloud.google.com/deployment-manager/https://cloud.google.com/free-trial/https://cloud.google.com/compute/docs/autoscaler/https://cloud.google.com/compute/docs/instance-groups/https://cloud.google.com/compute/docs/instance-templateshttps://cloud.google.com/sdk/gcloud/
27 
27 
27 
"
https://medium.com/neo4j/how-to-automate-neo4j-deploys-on-google-cloud-platform-gcp-6e123eccfd5e?source=search_post---------140,"There are currently no responses for this story.
Be the first to respond.
EDIT MAY 2020: Neo4j official documentation has published updated versions of this documentation.
— — — — — — — —
Neo4j already provides some documentation on its site for how to do deployments of Neo4j to common clouds, including Google Cloud. But in this article, I’ll provide sample shell scripts that can do this automatically for you.
These are useful when you want to integrate Neo4j into your CI/CD pipeline and be able to create/destroy instances temporarily, and also just to spin up a sample instance. But really, if you can automate Neo4j deployment, then any other piece of software can make a Neo4j instance whenever it needs, which is extremely handy.
If you have any questions, feedback, or want to discuss drop by this thread on the Neo4j Community site!
Before we begin, you’ll need the gcloud command line interface program, which you can download and install with directions here. The gcloud CLI is the main way you can automate all things with GCP.
It will also be necessary to authenticate your gcloud CLI, to make sure it can interact with your GCP projects.
Neo4j provides Deployment Manager templates for Neo4j Causal Cluster (highly available clusters), and VM images for Neo4j Enterprise stand-alone. So first thing’s first, pick which one you would like to deploy. We’ll cover both in this article.
Deployment Manager is really just a recipe for GCP that tells it how to deploy a whole set of interrelated resources. By deploying all of this as a stack we can keep all of our resources together, and delete just one thing when we’re done.
How to deploy a cluster is very simple: we just submit a new Deployment Manager job, pointing to the right template URL to tell GCP what to deploy. We then will provide various parameters to control how much hardware we’re using and so on.
We’ll need to specify several common parameters, which you’ll see in the scripts below. Here’s an explanation of what they are.
Let’s get started. Simply run any of these scripts, and it will result in a Deployment Manger stack being deployed.
In addition to the parameters listed above, because this is a clustered deploy, take note that we’re also using a parameter for “Cores” and “Read Replicas” to control how many nodes are in our cluster.
After all of the setup where we declare parameters of what we’re deploying, the heart of the entire script is just one simple call to gcloud deployment-manager deployments create which does all of the work.
We capture the result in the variable OUTPUT, which contains a lot of text telling us about our deployment. We then process that with a little bit of perl to pull out the password and IP address of our new deployment, because it will have a strong randomly assigned password.
What is nice about the Google deployment process is that this command will block and not succeed until the entire stack has been deployed and is ready. This means by the time you get that IP address back, you’re ready to go, and don’t need to check if Neo4j is up, because it is!
If you lose these stack outputs (IP, password and so on) they will also appear in your Deployment Manager window within the GCP console, so you can refer back to them.
To delete a deployment created in this way, you just need to take note of the STACK_NAME that we deployed. I use a short script to delete deployments like this:
When you delete Neo4j stacks on GCP, they intentionally leave their GCP disks behind, to make it hard for you to accidentally destroy your valuable data. But because these disks are left behind, you may wish to uncomment that last line, which will clean up those disks if the deploy is truly temporary and the disks aren’t wanted.
This will create a single instance of Neo4j without high-availability failover capabilities, but it’s a very fast way to get started. For this deploy, we don’t use Deployment Manager but just create a simple VM and configure its firewall/security rules.
Because we’re not using Deployment Manager for this one, this also provides an example of polling and waiting until the VM service comes up, and then changing the Neo4j default password when it does. You’ll notice at the top of the script we choose a random password by running some random bytes through a hash.
The launcher-public project on GCP hosts Neo4j’s VM images for GCP. In this example we’re using neo4j-enterprise-1–3–5–3-apoc, but other versions are available too. By substituting a different image name here, you can use this same technique to run Neo4j Community.
To delete an instance created like this, again we take note of the STACK_NAME and just use another utility script:
Developer Content around Graph Databases, Neo4j, Cypher…
6 
6 claps
6 
Written by
Architect at Neo4j
Developer Content around Graph Databases, Neo4j, Cypher, Data Science, Graph Analytics, GraphQL and more.
Written by
Architect at Neo4j
Developer Content around Graph Databases, Neo4j, Cypher, Data Science, Graph Analytics, GraphQL and more.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/this-week-in-google-cloud-platform-introducing-cloud-firestore-humongous-vms-custom-roles-and-a5381e39365a?source=search_post---------141,"There are currently no responses for this story.
Be the first to respond.
The big news this past week has been the release of Cloud Firestore — “Introducing Cloud Firestore: Our New Document Database for Apps”.
The release comes with a video (youtube.com), a comparison with the Firebase realtime database (Firebase blog), a podcast interview with the product managers (GCPPodcast.com), and three codelabs (g.co/codelabs).
Another announcement made this past week : “Introducing custom roles, a powerful way to make Cloud IAM policies more precise”. This is a pretty big deal for some enterprise customers. The documentation is here.
If you think bigger is better, we’re “Now shipping: Compute Engine machine types with up to 96 vCPUs and 624GB of memory” (currently in beta in four regions). Also, the team is looking for early testers for up to 4TB machines (follow the link to find out more).
We’ve also released five Chef cookbooks for GCP — “Partnering on open source: Managing Google Cloud Platform with Chef”. These cookbooks are Chef-certified and just the beginning.
Not an announcement per say but this latest “teachable machine” A.I. Experiment is really worth looking at.
This week from the “How-to” department :
From the “bag of everything GCP” department :
The image of the week is actually a video — “Introducing Cloud Firestore” :
That’s it for this week!
-Alexis
Google Cloud community articles and blogs
15 
15 claps
15 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Google Cloud Developer Relations
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/a-to-z-of-google-cloud-platform-a-personal-selection-f-is-for-firewalls-3e45852630c9?source=search_post---------142,"There are currently no responses for this story.
Be the first to respond.
I am a firm believer in defence in depth and in this post just wanted to talk about the compute engine firewalls which is a crucial part of that approach when using Compute Engine on GCP.
I”m going to assume you know how a firewall is typically implemented on premise by managing what traffic can and cannot flow between subnets and the internet by implementing a set of rules that allow or disallow traffic passing through it based on ports, protocol and source & target subnets or IP addresses.
The GCP firewall is actually pretty similar in application in that firewalls are associated with a network so each network in a GCP project has its own firewall controlling connectivity .
All GCP projects have a default network with an associated default firewall rule . This default firewall has the following rules:
default-allow-internal
Allows network connections of any protocol and port between instances on the network.
default-allow-ssh
Allows SSH connections from any source to any instance on the network over TCP port 22.
default-allow-rdp
Allows RDP connections from any source to any instance on the network over TCP port 3389.
default-allow-icmp
Allows ICMP traffic from any source to any instance on the network
When you create a new network however there are no firewall rules permitting connections of any type. so after creating a new network, you need to create firewall rules for it to allow connectivity of any sort with your instances.
An instance must be associated with a network and thus by default the firewall rules associated with that network apply to the instances in that network. If you do not explicitly state the network that the instances should be deployed into using the — network flag the instance will be deployed ( or associated depending on how pedantic you want to be — deployed just sounds more natural to me when discussing this) into the default network and connectivity subjected to the firewalls rules associated with that.
Note :You can add additional firewall rules to the default network
A GCP project by default has up to 5 networks ( you can get this upped by making a quota request) and each network can have multiple firewall rules associated with them.
GCP also has the concept of sub networks. This allows you to partition your network into chunks ( a subnetwork ) that have non-overlapping RFC1918 ranges i.e private IPv4 address ranges that you can define and manage within a single network. A project can have up to 100 subnetworks that can be arranged as you see fit i.e all in one network or spread across a number of networks ( up to 5 networks in total )
As I don’t want to get sidetracked talking about subnets ( I know too late!) hopefully the image below from the docs gives you an idea of how you can configure subnetworks within and across zone boundaries .
I’d also suggest taking time out to read through the docs on subnetworks before planning out how you intend to configure any application using Compute engine in case you feel you want to make use of this feature .
Subnetworks do not change what I have already described about how firewall rules work but you may be asking how you can apply firewall rules that are specific to subnets then if the rules apply to the network. Tags are the answer! Using tags means you can create additional isolation between subnetworks by selectively allowing only certain instances to communicate. If you arrange for all instances in a subnetwork to share the same tag, you can specify that tag in firewall rules to simulate a per-subnetwork firewall. For example if you have a subnet called subnet-a , you can tag all instances in subnet-a with the tag my-subnet-a and use that tag in firewall rule target-tags or as source ranges. The docs walk through a nice example of using tags and subnets with firewall rules
Google Cloud community articles and blogs
32 
32 claps
32 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Chocolate addict - I have it under control really I do. I do stuff involving cloudy tech. Tweets my own so only me to blame, except for retweets.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/this-week-in-google-cloud-platform-better-dlp-ruby-love-bigquery-stackdriver-and-security-2ebd56b0de0a?source=search_post---------143,"There are currently no responses for this story.
Be the first to respond.
Hello Google Cloud Insiders,
Week 42! Hoping that you find all the answers you were looking for…
“Accelerate BigQuery solution development with intelligent log analysis”. Stackdriver + BigQuery = 💕
“New ways to manage sensitive data with the Data Loss Prevention API”. More data de-identification capabilities with bucketing, K-anonymity, and L-Diversity techniques.
“Now, you can monitor, debug and log your Ruby apps with Stackdriver”. So much progress made for Rubyists in the past 18 months or so, by Aja and others.
“Turns out, security drives cloud adoption — not the other way around”. A Cloud adoption report sharing the current mindset of business and IT leads. Check out also these short and recent GCP security-related videos.
“21 new open-source solutions available from Google Cloud Launcher”. Kafka, jenkins, drupal and many more.
From the “with a little help from my friends getting around” department :
From the “it’s been a while since we had a Kubernetes roundup” department:
From the “Customers talk best about GCP” department :
From the “ICYMI in the docs” department :
This week’s GCP Podcast (#00099) is a conversation on Cloud Functions and Firebase Hosting with David East (gcppodcast.com)
From the “not strictly Cloud-related but seriously cool nonetheless” department :
The photo of the week is announcing the GCP Podcast’s 100th episode with Vint Cerf. Make sure to tune in on Wednesday (gcppodcast.com) !
That’s it for this week!-Alexis
Google Cloud community articles and blogs
12 
1
12 claps
12 
1
Written by
Google Cloud Developer Relations
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Google Cloud Developer Relations
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/hostspaceng/short-tips-ive-learnt-from-using-google-cloud-platform-gcp-8cd9235147b9?source=search_post---------144,"There are currently no responses for this story.
Be the first to respond.
I started my Cloud adventure with Amazon Web Services (AWS) and ported to Google Cloud Platform (GCP) , there’s still alot of tools I haven’t really dug into in GCP but this article shares some tips I’ve learnt.
Majorly, I’ve used the Compute Engine more than any other product of GCP, I’d say I prefer GCP because of it’s free tier :-D , pricing, simplicity, auto-scaling and other few reasons. Let’s get down to business…
🔥 The cheapest prices are mostly in this regions: us-west1, us-central1, us-east1, read more here
🔥 Set Static IPs for your Virtual Machines (VM) Instance, read more here
🔥 Backup your Virtual Machine Instances with Snapshots (Can also be used in creating a Clone of Instance), read more here
🔥 You don’t need a .pem key to connect, use the provided Browser SSH, read more here
🔥 Deploy your Servers/Applications e.g. LAMP, CouchDB etc.. faster with Marketplace (previously Cloud Launcher), read more here
🔥 Use the gcloud shell in your browser to create and manage cloud services, read more here
🔥 You can share a GCP Billing Account across Multiple Projects and Multiple Google Account, read more here
That’s all for now 😉. Feel free to share and make comments.
Providing Affordable and Reliable Hosting Services
9 
9 claps
9 
Written by
Software / DevOps Engineer | Google Developer Expert for Cloud | https://timtech4u.dev
Providing Affordable and Reliable Hosting Services
Written by
Software / DevOps Engineer | Google Developer Expert for Cloud | https://timtech4u.dev
Providing Affordable and Reliable Hosting Services
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/windows-and-net-on-google-cloud-platform-4f942c695f2d?source=search_post---------145,"There are currently no responses for this story.
Be the first to respond.
Originally published in SDN Magazine 131 in February 2017.
Until recently, there were two distinct camps in the software world: the Windows (A.K.A. closed) world and the Linux (A.K.A. open) world. In the Linux world, we had tools like the bash shell, Java programming language, Eclipse IDE, MySQL database, and many other open-source projects by Apache. In the Windows world, we had similar, yet distinct tools mainly developed by Microsoft, such as the C# programming language, Visual Studio IDE, SQL Server and PowerShell.
These two worlds existed side-by-side for many years with minimal interaction. You had to pick your side and stick with it. If you had to switch sides, you had to go through a slow process of readjusting your existing tools with similar-yet-quite-different counterparts and it was painful.
In the last few years, the tech world has gone through a gradual revolution. In 2014, Microsoft open-sourced the .NET framework to everyone’s surprise. This was followed by OpenSSH running on Windows in 2015. 2016 was probably the most exciting year with SQL Server and PowerShell running on Linux, Bash running on Windows, and most imporantly ASP.NET Core, the new cross-platform version of ASP.NET, running on Linux, Mac and Windows.
As Microsoft opened up its technology to the world, we were very busy at Google ensuring that .NET has first-class support on Google Cloud Platform (GCP). In 2016, we added support for deploying traditional ASP.NET apps to Windows Servers on Compute Engine. We introduced a Visual Studio plugin and PowerShell cmdlets to manage GCP resources. We made Microsoft SQL Server available on Compute Engine. Last but not least, we started supporting containerised ASP.NET Core apps on App Engine and on Kubernetes running on Container Engine. I cover both in detail later in the article. As a result of our work, Google joined the .NET foundation in November 2016. It was a busy year!
It is very exciting that the Windows and Linux worlds are coming together, and opening up many opportunities for .NET developers. In the rest of the article, I want to talk specifically about what GCP is doing for .NET.
GCP provides a number of services and tools for developers to build on top of Google’s infrastructure. Java, Python, Go, Node.js, Ruby, PHP and of course C# are some of the supported languages. Let’s take a look at the options you have when it comes to application development.
At top of the chain is Cloud Functions. This is the serverless platform for event-driven microservices. It currently supports Node.js functions. The beauty of Cloud Functions is that you only need to worry about writing and deploying your function and Google takes care of running that function at scale. This is perfect for simple apps with a limited number of specialised microservices.
Sometimes, you need more than a function. You need an application with frontend and backend talking to different services. For those kind of apps, GCP offers App Engine. The idea behind App Engine is similar to Cloud Functions, in that you write your app and let Google manage and auto-scale it as required. The underlying infrastructure is abstracted away from you which means you don’t have to deal with DevOps.
If you already made the switch to containerised apps using Docker and need more control in how your app is structured and run, there’s Kubernetes and Container Engine (GKE). You can very easily get a Kubernetes cluster running on GKE with a single command and deploy your containers in any configuration you like.
Finally, if you want full control, GCP has Linux and Windows Server virtual machines (VM) running on Compute Engine. Since they are VMs, you have full control on what gets installed however, you also have full responsibility which means that you need to manually configure auto-scaling, patch software, and so on.
GCP provides a number of ways to support your app development. Let’s take a look at how GCP specifically supports .NET apps.
If you have a traditional ASP.NET app running on Windows, you can easily take that app and migrate it to Compute Engine on GCP.
First, you need a Windows Server with the ASP.NET framework installed. Thankfully, GCP has Cloud Launcher which makes it really easy to explore, launch, and manage production-grade solutions. It is literally a couple of clicks to get a Windows Server with ASP.NET framework installed in a Compute Engine VM.
If your app uses SQL Server, there are pre-configured SQL Server images that you can install on Compute Engine VMs and you can use Visual Studio to publish your ASP.NET app to your Compute Engine VMs.
ASP.NET Core is the next generation, multi-platform version of ASP.NET. It is the leaner version of traditional ASP.NET framework and runs on Linux, Mac and Windows.
App Engine has been around as a PaaS offering from Google for a while but it wasn’t available to .NET developers until ASP.NET Core came along. It is now possible to wrap an ASP.NET Core app into a Docker container and deploy that container to App Engine to run. The main advantage of App Engine is that it abstracts away the infrastructure, so developers simply deploy their app and the day-to-day running and scaling of that app is done by Google.
If you want more fine-grained control on how your containers are structured and deployed, you can always create a Kubernetes cluster on Container Engine (GKE). GKE makes it trivial to create a cluster and Kubernetes makes running containers easier by providing a high level API to automate deployment, scaling and running of containers in production.
Once you have your app running in Google Cloud, many services automatically become available to your app through native .NET client libraries.
You can integrate with services like Cloud Storage for binary storage, Pub/Sub for messaging, BigQuery for incredibly fast queries, Vision API to detect images, and many other machine learning APIs such as the Natural Language Processing API, Speech API, and Translate API.
By running on Google Cloud, you will automatically gain access to these new capabilities as new services are added, and that’s the beauty of the cloud.
GCP has a Visual Studio plugin to manage cloud resources directly from Visual Studio. It is available from the Visual Studio Gallery and can be installed directly within Visual Studio. It provides some ASP.NET MVC and Web API templates to work with GCP projects. It also has a Google Cloud Explorer where you manage see and manage Compute Engine and Cloud SQL instances, as well as Cloud Storage resources.
PowerShell is a command-line shell and associated scripting language built on the .NET Framework. It’s the default task automation and configuration management tool used in the Windows world.
Cloud Tools for PowerShell is a collection of cmdlets for accessing and manipulating Google Cloud resources such as Google Compute Engine, Google Cloud Storage, Google Cloud SQL and Google Cloud DNS — with more to come!
We’re going through some exciting times. With Windows ecosystem opening up and ASP.NET Core’s multi-platform story, there are a lot of new opportunities for .NET world. At Google, we’re serious about supporting Windows and .NET workloads on Google Cloud Platform. It’s a great time to be a .NET developer for sure!
https://cloud.google.com
https://cloud.google.com/dotnet
https://cloud.google.com/windows
https://codelabs.developers.google.com/windows
Originally published at meteatamel.wordpress.com on March 20, 2017.
Google Cloud community articles and blogs
6 
6 claps
6 
Written by
Developer Advocate at Google
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Developer Advocate at Google
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/analytics-vidhya/hints-and-tips-for-coursera-google-cloud-platform-fundamentals-core-infrastructure-42b781bbc311?source=search_post---------146,"There are currently no responses for this story.
Be the first to respond.
I recommend Architecting with Google Cloud Platform (GCP) or Developing applications with GCP Courseraspecializations. Going through these specializations will significantly help you obtain the Google Cloud Architect certification or Google Cloud Developer certification. More importantly, the hands-on labs enable you to build GCPapplications.
"
https://medium.com/@jaychapel/google-cloud-platform-vs-aws-is-the-answer-obvious-maybe-not-1a11482beca9?source=search_post---------147,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Oct 31, 2017·5 min read
Google Cloud Platform vs AWS: what’s the deal? A few months ago, we asked the same question about Azure vs AWS. While Microsoft continues to see growth, and Amazon maintains a steady lead among cloud providers, Google is stepping in. Now that Google Cloud Platform has solidly secured its spot to round out the “big three” cloud providers, we think it’s time to take a closer look and see how the underdog matches up to the 800-pound gorilla.
As they’ve been known to do, Amazon, Google, and Microsoft all released their recent quarterly earnings on the same day. At first glance, the headlines tell it all:
The natural conclusion is that AWS continues to dominate in the cloud war. With all major cloud providers reporting earnings at the same time, we have an ideal opportunity to examine the numbers and determine if there’s more to the story. Here’s what the quarterly earning reports tell us:
The Obvious: Google is not surpassing AWS
When it comes to Google Cloud Platform vs AWS, presently we have a clear winner. Amazon continues to have the advantage as the biggest and most successful cloud provider on the market. While AWS is growing at a smaller rate now than both Google Cloud and Azure, Amazon’s growth is still more impressive given that it has the largest market share of all three. AWS is the clear competitor to beat as the first successful cloud provider, with the widest range of services, and a strong familiarity among developers.
The Less Obvious: Google is gaining ground
While it’s easy to write off Google Cloud Platform, AWS is not untouchable. Let’s not forget that 76% year-over-year growth is nothing to scoff at. AWS has already solidified itself in the cloud market, but Google Cloud is just beginning to take off.
We know that AWS is at the forefront of cloud providers today. At the same time, AWS is now only one among three major cloud providers. Google Cloud Platform has more in store for its cloud business in 2018.
Google’s stock continues to rise. With nearly 2,495 new hires added to the headcount, a vast majority of them being cloud-related jobs, it’s clear that Google is serious about expanding its role in the cloud market. Deals have been made with major retailer Kohl’s department store, and payments processor giant Paypal. Google CEO Sundar Pichai lists the cloud platform as one of the top three priorities for the company, confirming that they will continue expanding their cloud sales headcount.
In discussing Google’s recent quarterly earnings, Pichai added his thoughts on why he believes the Google Cloud Platform is on a set path for strong growth. He credits their success to customer confidence in Google’s impressive technology and a lead in machine learning, naming the company’s open-source software TensorFlow as a prime example. Another key component to growth is strategic partnerships, such as the recent announcement of a deal with Cisco, in addition to teaming up with VMware and Pivotal.
Driving Google’s growth is also the fact that the cloud market itself is growing fast. The move to the cloud has prompted large enterprises to use multiple cloud providers in building their applications, such as Home Depot Inc. and Target Corp., who rely on a combination of cloud vendors. Home Depot in particular uses both Azure and Google Cloud Platform, and a spokesman for the home improvement retailer explains why that was that intentional: “Our philosophy here is to be cloud agnostic, as much as we can.” This philosophy goes to show that as long as there is more than one major cloud provider in the mix, enterprises will continue trying, comparing, and adopting more than one at a time, making way for Google Cloud to gain further ground.
Andy Jassy, CEO of AWS, put it best:
“There won’t be just one successful player. There won’t be 30 because scale really matters here in regards to cost structure, as well as the breadth of services, but there are going to be multiple successful players, and who those are I think is still to be written. But I would expect several of the older guard players to have businesses here as they have large installed enterprise customer bases and a large sales force and things of that sort.”
Google Cloud Platform vs AWS is only one battle to consider in the ongoing cloud war. The truth is, market performance is only one factor in choosing the best cloud provider, and as we always say, the specific needs of your business are what will drive your decision.
What we do know: the public cloud is not just growing, it’s booming.
Referring back to our Azure vs AWS comparison, the basic questions still remain the same when it comes to choosing the best cloud provider:
Right now AWS is certainly in the lead among major cloud providers, but for how long? We will continue to track and compare cloud providers as earnings are reported, offers are increased, and price options grow and change. To be continued in 2018…
Originally published at www.parkmycloud.com on October 31, 2017.
CEO of ParkMyCloud
24 
24 
24 
CEO of ParkMyCloud
"
https://medium.com/the-artificial-impostor/prepare-deep-learning-ready-vms-on-google-cloud-platform-309e34c0fea?source=search_post---------148,"There are currently no responses for this story.
Be the first to respond.
The 2nd YouTube-8M Video Understanding Challenge has just finished. Google generously handed out $300 Google Cloud Platform(GCP) credits to the first 200 eligible people, and I was lucky enough to be one of them. I wouldn’t be able to participate in this challenge at a higher level otherwise. My local hardware can barely handle the size of the dataset and is not strong enough to handle the size of the model. The least I can do to return the favor is to write a short tutorial on how to set up deep-learning-ready VMs on GCP and about some tips that I’ve learned.
By using ML Engine you can get rid of the problem of setting up the VM. It also comes with other tools like HyperTune. However, if your budget is tight, creating VMs on Cloud Compute will save you a lot of money. Especially after Cloud Compute started to offer “preemptiblity” option for GPUs earlier this year, and recently further dropped their prices. It’s so cheap that for me the costs incurred from storing the full 1.6 TB datasets were almost the same as from the K80 GPU core. (I used n1-standard-2/n1-standard-4 + 1 K80 core for training and n1-standard-4 for evaluating and predicting.)
GCP actually provides images that work with GPUs now. I did not know that when I started working on this competition, so I started from scratch(to be precise, from one of my CPU VM snapshot). You can skip the next step which installs NVIDIA driver if you use these images.
Find the corresponding driver on the NVIDIA website. For me, it’s Tesla Driver for Ubuntu 16.04 (Go to “SUPPORTED PRODUCTS” tab to double-check). Download and install it:
Run nvidia-smi command to check if the installation was successful.
I chose to use nvidia-docker and used docker images to mange my environments. As introduced in one of my previous post (link below), nvidia-docker only depends on the NVIDIA driver, so we get to use different versions of the CUDA toolkit in different images/containers. This can be extremely helpful when the pre-built binaries of deep learning frameworks supports different versions of CUDA toolkits.
medium.com
Just follow the installation instruction from the nvidia-docker :
Run docker run --runtime=nvidia --rm nvidia/cuda nvidia-smi to check if the installation was successful.
You’ll probably want to take a snapshot of the boot disk at this point.
For Tensorflow there are plenty of official images on Dokcer Hub, I used this image: tensorflow/tensorflow:1.8.0-gpu. Simply run this command to pull the image to local:
For PyTorch you have to build the image yourself locally. Clone the PyTorch Git repo and run this command:
Or you can write your own Dockerfile and includes all the packages you’d like to use in your project. Here are the ones I’ve been using: Tensorflow and PyTorch.
You are good to go! Start a container by docker run --runtime=nvidia -ti <other options> image_name bash and do training/prediction inside that container. The deep learning framework should be able to utilize the GPU core(s) now. It’s really this simple.
Pro tip #1: It’s generally a good idea to separately store the dataset on a different disk from the boot/system disk. This also leads to independent snapshots of the dataset and system data. It adds a lot of flexibilities.
You’ll have to create the disk, and attach it to the VM either via Web UI or the command line tool. Then you’ll have to format the new disk and mount it:
To make the system auto-mount the new disk, edit this file: /etc/fstab.
Pro tip #2: You can share a data disk among different VMs as long as it’s attached in read-only mode. This is what you need to do:
Now you can run two VMs and two models simultaneously using the same data disk. Note that a disk have I/O limits, so don’t attach it to too many VMs:
If you plan to use Jupyter Notebook, Jupyter Lab, visdom, or Tensorboard on the VMs, you’ll need to open the corresponding ports in the firewall settings:
I recommend using a config file (e.g. jupyter_notebook_config.json) to set the password for Jupyter.
This tutorial is not really GCP-specific except for the last two optional steps. It should be applicable to other cloud platforms and even to local machines with no or very little modification.
Having said that, I still had to spend quite some time searching and experimenting things I found on the Internet before finally reached a solution that I’m satisfied with. Hopefully this tutorial can save you folks some time. And please feel free to leave any kinds of feedback.
Towards human-centered AI. https://veritable.pw
5 
1
5 claps
5 
1
Written by
Data Geek. Maker. Researcher. Twitter: @ceshine_en
Towards human-centered AI. https://veritable.pw
Written by
Data Geek. Maker. Researcher. Twitter: @ceshine_en
Towards human-centered AI. https://veritable.pw
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/this-week-in-google-cloud-platform-scheduling-dataprep-postgresql-ha-perf-tips-and-all-about-e0c9c1c4832f?source=search_post---------150,"There are currently no responses for this story.
Be the first to respond.
On the product front this past week we had important new features added to Cloud Dataprep, Cloud SQL for PostgreSQL, and Cloud DNS :
We’ve also announced a new partnership with Commvault for lifting and shifting existing workloads to GCP. This includes backups to Cloud Storage Coldline, protection of Compute Engine workloads with Commvault and G Suite backups.
Building on CloudShell’s popularity we’re “Introducing Open in Cloud Shell, a new way to create frictionless tutorials”. Expect to see a few of these buttons flourish in documentation, tutorials and codelabs.
Speaking of which… did you know about the 104+ GCP Community Tutorials ? Request a tutorial, fix an existing one, or create a net new one, all based on a github workflow.
Staying with learning GCP material, Google Cloud Codelabs has had 90 new or updated codelabs in the last 60 days, including Istio, gRPC Java, and quite a bit of Spring Boot material (w/ App Engine standard, Pub/Sub, Cloud Storage, Cloud SQL, Sleuth/Stackdriver, …).
Colt MacAnlis’ focus on Cloud performance takes us through “5 steps to better GCP network performance” (Google blog).
Colt also posted this piece on Medium — Google Cloud Storage : What bucket class for the best performance? (medium.com)
From the “ML is hype, fast-evolving, and also about people” department :
From the “you should really check out the GCP on Medium.com” department :
This week’s podcast has lots of figures for IoT Core, Pub/Sub, Datastore, BigQuery (both data and $$) and is #102 “Smart Parking and IoT Core with Brian Granatir”.
This week‘s picture is from the “AutoML applied to large datasets” article :
That’s it for this week!-Alexis
Google Cloud community articles and blogs
13 
13 claps
13 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Google Cloud Developer Relations
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/a-to-z-of-google-cloud-platform-a-personal-selection-p-pub-sub-130538dab6e5?source=search_post---------151,"There are currently no responses for this story.
Be the first to respond.
Last entry I spoke about GCS which I believe is the first fundamental building block for any cloud native application being built on GCP . In this entry I’m going to talk about what I believe is the second foundational block Cloud pub/sub
Pub/Sub is a fully managed service that provides a way to implement many-to-many, asynchronous messaging between applications. Publisher applications/services can send messages to a “topic” and other applications/services can subscribe to that topic to receive the messages. By decoupling senders and receivers, Pub/Sub allows developers to communicate between independently written applications/services. It allows the creation of more resilient applications , that are truly scalable . It also allows a way to assist in allowing you to independently upgrade parts of your application without losing data passing through the interconnected whole.
I like this picture from the docs which I feel illustrates this fairly succinctly.
It’s fully integrated with many GCP products and can act as a source or sink.
To use Pub/Sub you need to authorise against the Pub/Sub API using oauth 2.0 and set the scope.
Below are a few lines of code from a python example illustrating how to set this up :
You then set up a publish or subscribe function to push or take messages off of Pub/Sub below are a few lines of example python code to illustrate this.
With Cloud IAM you can implement granular level controls. This table from the docs shows you the controls you can set using IAM with Pub/Sub :
Pub/Sub is not a storage system though and if you do need to replay or keep the messages after they’ve gone through your initial processing persist them to somewhere like Cloud storage, Bigtable or BigQuery.
Think what other productive differentiating stuff the folks managing your own messaging application could be doing instead if you use a fully managed service like Pub/Sub. ( see why Spotify chose to migrate to Pub/Sub from kafka if you want another view on why this is good advice)
Lastly I just want to discuss FIFO ( first in first out) . Pub/Sub does not guarantee the order in which messages are received by subscribers, so at this point you may be thinking huh really? or maybe you aren’t. FIFO at scale is an anti -pattern for truly scalable distributed systems hence why FIFO isn’t a thing with Pub/Sub but wait a minute before you turn away if you need to do it then rather than resorting to feeding & watering your own systems like RabbitMQ or kafka pause for thought and read this which provides strategies for achieving this with Pub/Sub but it also gives you food for thought with regards whether you actually need FIFO at all.
Before I go I guess I owe Frances Perry @francesjperry of the Dataflow team an apology as even after being accosted at GCP Next & it being pointed out to me that D is for Dataflow (I wrote about Deployment Manager) and B for Apache Beam ( I wrote about the history of BigTable) :-) The next excuse I had to talk about Dataflow I guess would have been here ( Pipelines, PCollections, ParDo , python support ) . It would have worked I admit but I still haven’t !
In my defence this Pub/Sub post has been waiting to get out there! But why not watch this excellent talk from Frances she gave at Scale from last sept if you too wanted to see something about Dataflow in this series . I’ve got some real awkward letters to get through yet and not sure I can fit Dataflow in though so this is my get out clause :-)
Google Cloud community articles and blogs
5 
5 claps
5 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Chocolate addict - I have it under control really I do. I do stuff involving cloudy tech. Tweets my own so only me to blame, except for retweets.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/this-week-in-google-cloud-platform-a-taylor-swift-detector-preemptible-gpus-and-a-training-not-571f13f37f67?source=search_post---------152,"There are currently no responses for this story.
Be the first to respond.
By now you’ve probably heard of the industry-wide Meltdown and Spectre CPU vulnerabilities. Here’s a Google-focused roundup of links, starting with what you need to know and do and on to details of Google’s Project Zero team discovering and documenting consequences of CPU data cache timing attacks :
On to better news, preemptible CPUs has been one of Compute Engine’s unique and most liked features. Preemptible GPUs make just as much sense and are now a reality : “Introducing Preemptible GPUs: 50% Off” (Google blog)
Simplify Cloud VPC firewall management with service accounts (Google blog). Because you have better things to do than configuring and maintaining IP-based firewall rules.
We’ve previously covered here various courses offered by Google and partners, and this latest one is probably NOT for you: Build a Business Transformation Vision with Google Cloud (cloud.google.com). Instead, this new course is targeted at business decision makers who can influence cloud adoption, so pretty important still IMO.
From the “how-to” department :
Kubernetes developers interested in an improved developer experience may find this GitHub project interesting: “Freshpod — Restart Pods on Minikube automatically when their images are updated” (github.com)
From the “maybe I should take time to look at this Bazel thing” department :
From the “Customers talk best about Google Cloud Platform” department :
Crack open Google Cloud Shell for infrastructure management (searchcloudcomputing.techtarget.com)
Not strictly GCP-related but well-written and a very complete guide worth reading and sharing : “Introduction to modern network load balancing and proxying” (blog.envoyproxy.io)
This week‘s picture is taken from Sara Robinson’s detailed Taylor Swift detector article :
That’s it for this week!-Alexis
Google Cloud community articles and blogs
14 
14 claps
14 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Google Cloud Developer Relations
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/a-to-z-of-google-cloud-platform-a-personal-selection-o-object-store-2f7cf508780d?source=search_post---------153,"There are currently no responses for this story.
Be the first to respond.
There are two GCP products that I feel are fundamental foundation blocks and as such they tend to get very little attention as they’re just there doing what they do at scale .
So in the next two posts I’m going to give them both a little love and focus on them. Starting here with Cloud Storage.
Cloud Storage (GCS) is GCP’s object store. It’s used for so many use cases I really can’t give even a brief list justice . Use cases range from storing pictures, The source of large amounts of data to be crunched by GCPs awesome data products, Acting as a backup target through to hosting static websites ( which I covered in H and J in this series) Well anything you can think of really but please it’s not designed to act as a database so please don’t do that!
There’s a ton of information in the docs on using GCS so I’m not going to regurgitate that as promised! I am however going to pull out stuff that I feel you should know to give you in a few minutes read a head start in understanding some basic concepts and how to use GCS.
I did cover this in my previous entry on naming stuff but for completeness in case you’re only interested in Cloud Storage and found yourself here I’ve repeated it here.
In addition to the standard methods of transferring data to GCS ( gsutil and api calls) I’d just like to mention two methods that may not be so commonly known
Cloud storage transfer — Transfers data from a data source to a data sink. Your data source can be an S3 bucket, an HTTP/HTTPS location, or a GCS bucket. Your data sink (the destination) is always a GCS bucket. It makes data transfers and synchronization between data sources and data sinks easier by allowing you to easily
Streaming transfers — GCS supports streaming transfers using the gsutil tool or boto library, based on HTTP chunked transfer encoding.
Cloud storage has three storage classes . The docs have this nice table that outlines the differences at a high level :
Nearline is designed to be used as “nearline” storage for archiving and backup use cases so ageing objects out from standard storage as part of a tiered storage solution is a typical configuration.
You can use the Storage transfer service to create a job that transfers data from a standard bucket to a nearline bucket at a time interval you specify after the object has been uploaded to the standard bucket. You can set it up using the console or programmatically
Hope this gives you a good idea of the flexibility and power of GCS and why I see it as foundational building block.
Before I leave this post I’d just like to say thanks to my colleague Karan @sdksb who kind of helped me make up my mind that “Object store” would be my entry for O .
Google Cloud community articles and blogs
7 
7 claps
7 
Written by
Chocolate addict - I have it under control really I do. I do stuff involving cloudy tech. Tweets my own so only me to blame, except for retweets.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Chocolate addict - I have it under control really I do. I do stuff involving cloudy tech. Tweets my own so only me to blame, except for retweets.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/discovering-google-cloud-platform-with-next-18-a41b049529f4?source=search_post---------154,"There are currently no responses for this story.
Be the first to respond.
Before 2017, little did I know about Google Cloud Platform. By chance, a few videos from Cloud Next ’17 led me to dive in and discover GCP. I was impressed by the developer friendliness and technological performance of Google’s public cloud, and — long story short — I’m super happy to have joined the team.
One year later, I still find it most interesting to have a look at Cloud Next from a developer point of view. This Google Cloud annual milestone gives an extensive snapshot of where GCP is and also a good idea of where it’s heading.
TL;DR
If you’d just like to discover GCP with one video, Terry’s overview (40-minute) is a must-see:
What’s new in 2018?
techcrunch.com
blog.google
www.cnbc.com
Keynotes
If you’re into keynotes, the presentations were structured around 3 major keynotes:
More sessions than we can humanly watch!
Overall, you can cherry pick among 430+ session recordings. Here is my selection, based on quotes from users and Googlers:
“When I say large-scale, I’m serious… We’re migrating over 300 PB of data… The network performance was so good that we were able to decouple storage and compute.”Parag Agrawal — CTO, Twitter
→ Bringing the Cloud to You
“At the end of the day, it wasn’t me who chose Google. It was my engineers.”“Google Cloud underpins our speed and innovation. It abstracts all the complexities of the underlying infrastructure…”Mike McNamara — CIO, Target
→ Building a Cloud for Everyone→ Google Cloud Customer Innovation Series
“Most of us just want to talk to a human being.”Dan Leiva — VP Customer Service, Ebay
→ Building a Cloud for Everyone
“We process around 400 PB of data monthly.”Nicole Bouchard — TPM, Spotify
→ Building A Petabyte Scale Warehouse
“You can literally hear MySQL scream in pain when you’re trying to query a 3.2 billion row table.”Pawan Valluri — Twitter
→ Building A Petabyte Scale Warehouse
“We shut down out datacenters this year… The New York Times had a website in 1996… It’s been in existence online longer than Google has been.”Deep Kapadia — Engineering Director, New York Times
→ A Transformation that Enabled Developer Autonomy
“We actually have more than 500 MySQL clusters and… they grow like mushrooms.”Kerry Munz — Engineering Director, HubSpot
→ Infrastructure as Code: Going Multi-Cloud in 30 days
“He hit 580,000 cores in a single job… about 300 core-years of compute…changes your whole outlook on research… get an answer in hours rather than months.”Wyatt Gorman — HPC & ML Specialist, Google
→ Introduction to High Performance Computing
“This graph is… transactions per second at the ‘Pokemon Go’ launch in 2016… reached 50 times of actual traffic.”Niniane Wang — Engineering Director, Niantic
→ Lessons Learned at Niantic: How to Move Live Systems to a New Database Platform
“…a billion downloads a day supporting our 11 million users…”C J Silverio — CTO, NPM
→ Made Here Together
“…after 4 years at Google and hearing terms for the first time like petabytes…”Greg Wilson — Director, Cloud Developer Relations, Google
→ Day 2 Keynote Recap
“There will be 163 zettabytes of data by 2025.”Sudhir Hasbe — Director, Cloud Data Analytics, Google
→ Rethinking Big Data Analytics
“I’ve been using Kubernetes so long I finally ended up in a container.”“Good programmers copy… Great programmers paste.”Kelsey Hightower — Cloud Developer Advocate, Google
→ Made Here Together
“There’s a couple of solutions here… get a planet-scale wallet and pony up.”Bret McGowen — Cloud Developer Advocate, Google
→ Serverless All the Way Down: Build Serverless Systems with Compute, Data, and ML
3 takeaways
Obviously, scalability is still what impressed me most, but here are my takeaways for 2018:
Time to conclude. I jumped on board one year ago and this just keeps being more exciting: there are new possibilities every week and it seems like an ever-expanding universe. I hope you could feel part of the passion and maybe got inspired by some videos.
If there’s a Google Cloud event coming nearby, don’t miss it. It may change the way you architect your solutions or simply the way you develop:
Last but not least, to stay tuned, check out our blog or follow @GCPcloud (and me ;). See you soon…
Google Cloud community articles and blogs
9 
9 claps
9 
Written by
Tech lover, passionate about software, hardware, science and anything shaping the future • ⛅ explorer at Google • Opinions my own • You can DM me @PicardParis
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Tech lover, passionate about software, hardware, science and anything shaping the future • ⛅ explorer at Google • Opinions my own • You can DM me @PicardParis
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/this-week-in-google-cloud-platform-cisco-partnership-go-1-8-2bf1adcbdd1f?source=search_post---------155,"There are currently no responses for this story.
Be the first to respond.
Here are the product and partnership announcements for this past week :
First, following the recent Java 8 GA release here’s another App Engine Standard announcement this week, this time with Go 1.8 on App Engine Standard going GA. Check out the Release Notes.
Cisco and Google announced an important partnership on a new open hybrid cloud solution with GCP, Kubernetes, Istio, and Apigee all playing a central role. Plenty of details on the Google blog.
Data Studio now offers more visualizations and analytical functions. Pivot tables, coordinated coloring, sampling indicator, and more. Check out the post to find out where to submit and vote for new Data Studio features.
From the “how-to” department:
From my always favorite “customers talk best about GCP” department :
From the “it’s been a while since we had a BigQuery roundup” department :
More announcements :
From the “in case you weren’t paying close attention” department :
As announced last week, the GCP Podcast celebrated its 100th episode with an interview with Vint Cerf (gcppodcast.com) and a blog post on its top episodes (Google Blog).
GCP products described in 4 words or less is back, now updated with Firebase and Free tier information (medium.com)
The picture of the week is Greg Wilson’s “GCP products described in 4 words or less”
That’s it for this week!-Alexis
Google Cloud community articles and blogs
5 
5 claps
5 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Google Cloud Developer Relations
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/evenbit/deploying-firebase-task-worker-to-google-cloud-platform-with-gitlab-ci-51e2c63d3bbf?source=search_post---------156,"There are currently no responses for this story.
Be the first to respond.
Make sure that you have read the article below to get an understanding of why I am making a case for putting in Google Cloud Platform into the mix.
oddbit.se
Before reading any further, you should be fairly comfortable with the concept of continuous integration to fully appreciate the article. I will go through some tips and tricks that I think are good practices and how to set up a flow that allows you to jump directly from pushing the code to Gitlab and going straight to see the updated application in the Cloud Console, without executing a single command apart from git commit and push.
This is what your Gitlab continuous integration configuration file will look like when we’re done.
If you think that parts of the code look familiar, it might be that you have read any of my previous articles about Gitlab continuous integration for Google App Engine or Firebase. This project is utilizing parts of both those articles in one and the same configuration file.
There is no “testing stage” in the example above . That step should go first of all and if that step fails, the subsequent deployment steps will not be executed. This is how your file actually should look like.
In my case, the NodeJS worker does not provide any external access at all apart from the data that it can read from the Firebase database. The server worker does not offer any API request endpoints or similar. It’s a confined worker that only reacts to the data that is passing through its event listeners.
The NodeJS worker is in fact only using the database part of the Firebase suite. So I have chosen to maintain the database definition rules in this project. The firebase.json configuration file is as simple and short as this:
Deploy the Firebase database rules first to make sure that the application instance is not running any new code that is depending on any potential new rules. If there are non-backward compatible changes, then you should of course account for that and take appropriate actions to make sure that your old app code isn’t doing something strange with your new data structures or rules.
This is the final step: to deploy the NodeJS application to Google App Engine Flexible Environment.
N.B. At the time of writing, the GAE Flexible environment is still in Beta and should be used with care, i.e. not in production.
But since I know what I’m doing (famous last words), I am having “production configuration” in this example anyway 😎
Starting from the bottom, we’ll first use the Gitlab project variables and pipe them into files that we’ll be needing. In this case you can see that I am having a file called “service-account.json” which is used for service account access to my Firebase application.
Pay attention to that the PROJECT_ID variable is being set according to the different project names in the different build types for master and develop branch.
After that, let’s jump to the job template to see the most interesting part of the script.
Everything under “before_script” should be self-explanatory if you have read my previous article about GAE deployment with Gitlab. So let’s focus on the “after_script” parts.
Make sure that you keep your app versions and services cleaned up in the dashboard. You could automate the deletion process with some scripting magic, but it might or might not be a good idea, depending on the flow and level of control that you want when maintaining your application. I would say that you probably know which way to go if you’re reading this. Just know that you can run into a “max versions” error if you’re not keeping it clean.
I find it convenient to also automate the distribution of the code to the Google Cloud Repository at the same time as deploying the application. This will allow you to debug your application with the same code base as you deployed.
The last line is simply just deploying the application and setting a version number that is linked back to the Gitlab CI pipeline in which it was deployed from.
The pipeline numbers are ever increasing and unique (at least for your project) which makes them good version numbers that can be ordered chronologically.
Now, scrolling up to the very beginning, it should be possible to just copy/paste the .gitlab-ci.yml script and make it work with very little modifications.
The odd bit of technology
54 
Some rights reserved

54 claps
54 
Written by
Google Developer Expert for Firebase, nerd and passionate problem solver | Founder of Kumpul coworking space in Bali
The odd bit of technology
Written by
Google Developer Expert for Firebase, nerd and passionate problem solver | Founder of Kumpul coworking space in Bali
The odd bit of technology
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/this-week-in-google-cloud-platform-osaka-region-spring-on-gcp-and-a-bitcoin-public-dataset-b90ea5203b6a?source=search_post---------157,"There are currently no responses for this story.
Be the first to respond.
GCP will soon reach 19 regions as it is “building its second Japanese region in Osaka” (Google blog)
Spring Framework 5 and Spring Boot 2 users rejoice, you can now use your favorite technologies and access the full power of GCP (Google blog). See also Pivotal’s take on this — “Spring Cloud for Google Cloud Platform 1.0 Milestone 2 Available” (spring.io)
Qwiklabs has released two new “baseline” quests, one for Kubernetes Engine, and one for Data, ML, and AI. Start here (qwiklabs.com)
From the “GCP users talk best about the tech” department :
From the “a week without BigQuery is a wasted week” department :
From the “putting TensorFlow versatility to the test” department :
From the “it didn’t really fit into any other category” department :
From the “In Case You Missed It (ICYMI)” department :
If you’re on twitter, here are some updates to Google Cloud’s presence there :
The GCP Podcast episode gets into how Google works with the community on TensorFlow — Episode #113 Open Source TensorFlow with Yifei Feng.
This week’s screenshot is taken from the Spring on GCP blog post
That’s it for this week!-Alexis
Google Cloud community articles and blogs
7 
Thanks to Jack Wilber. 
7 claps
7 
Written by
Google Cloud Developer Relations
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Google Cloud Developer Relations
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/a-to-z-of-google-cloud-platform-a-personal-selection-h-hosting-static-sites-1d0db2eb5d89?source=search_post---------158,"There are currently no responses for this story.
Be the first to respond.
Many many websites have no need of a web server and thus the associated compute services as there are essentially no moving parts on the server side. So an ideal use case to use Google Cloud Storage ( GCS) for hosting .
The benefits of a static site hosted on GCS imho are:
To host a static site on Google Cloud Storage (GCS) distils down to 5 steps:
For a full walk through have a look at the docs . For a look at how the economics stack up this post about the journey to hosting BaindAid 30 on GCS is worth a read.
Easy huh but what I really wanted to talk about is how to continually update your site hosted on GCS i.e treat the creation of your static site in the same way the rest of your code is built and deployed .
I then realised that I’d break every single one of my self imposed rules if I tried to cram it all into one single post so I am going to have to do this as a two parter and come back to part 2 later in the series. Okay onwards…
Using a static generator is the way to be able to do continuous updates.
When I think of static site generators I think of a system that automatically creates a complete static website by rendering markdown files to html. The system usually has a local server so you can see in real time the changes you make . There are lots of static site generators and when I did a search for static site generators on github it found 1744 repositories.
The most popular generators I’ve come across tend to be one of jekyll , Hugo or Ghost
which all work in the way I described earlier to generate pages. The reasons why you choose one over the other well depends on what “unique” feature is important to you . There are plenty of posts dotted around the internet though to help you decide.
I’m going to use Hugo to help me generate a static site . I’ve already used Jekyll and ghost in the past but I like the fact Hugo was written in GO so less dependencies needed (jekyll needs ruby and ghost node.js ) . With Hugo I can just download the binary with no need for anything else so imho it seems that is the cleanest approach ( This is just my personal preference though make up your own mind which generator you want to use)
Okay now I have my bucket set up I now need to install Hugo on my local machine, create a folder where I will store the files for my site ,Use Hugo to generate the structure of my site , create some content then upload the files to GCS.
This last section literally takes a few minutes depending on how much content you have. so expanding on those 5 steps we now have 8 steps
Then you literally rinse and repeat steps 5 to 7 whenever you do updates and running the local Hugo server means you can view real time updates locally
and the image below shows the content pushed to my bucket .
For a nice easy to follow end to end walk through of using HUGO with GCS fleshing out the 8 steps read this nice post by moxie.io
That’s great I hear you say but why a two parter?
Firstly I felt that it was a natural break to stop here as it shows how quickly you can get started hosting your own static site and as such is a stand alone entry in the series . The next part in this two parter is all about building on this foundation .
Secondly collaboration . The above is good enough for a single creator but when you have multiple content creators it starts to get unworkable.
Thirdly I also admit it gave me the idea for J !
In the next part of this ( J so not too long a wait! ) I’ll walk through using Jenkins to show how you can set up a publishing pipeline that can be used by multiple contributors. I’ll explain how you can hook up the local folder where your site content is generated to push to a cloud repository hosted in GCP .
Google Cloud community articles and blogs
5 
1
5 claps
5 
1
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Chocolate addict - I have it under control really I do. I do stuff involving cloudy tech. Tweets my own so only me to blame, except for retweets.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/this-week-in-google-cloud-platform-tensorflow-shooting-for-the-stars-etsy-banking-on-gcp-and-d13b51591632?source=search_post---------159,"There are currently no responses for this story.
Be the first to respond.
NASA and Google teamed up and took TensorFlow on a planet hunt. Here’s what happened. “Earth to exoplanet: Hunting for planets with machine learning” (Google Blog)
Cloud ML Engine price drops and new features (TF 1.4, Python 3, …), what’s not to like? “Bringing Cloud ML Engine to more developers with online prediction features and reduced prices” (Google Blog)
Tagging is often a flexible yet powerful way to group entities together and GCP now fully supports labels to group resources. “Use labels to gain visibility into GCP resource usage and spending” (Google Blog)
Google Cloud does *a lot* to secure your data both at rest and in transit, but it’s best to read “How Google protects your data in transit” (Google blog) and how ATLS is applied to all internal RPC calls and has been in use at Google for a decade now.
What is a Google Cloud MSP partner, how can they help, and who are they? Find out ! “Expanding our partner ecosystem with managed services providers” (Google Blog)
A couple of releases :
From the “How-to” department :
From the “customers talk best about GCP” department :
From the “videos for the holidays” department :
From the “Rejoice, year in review posts have started” department :
From the “ICYMI (In case you missed it)” department :
This week‘s picture is taken from the NASA and Google blog post :
That’s it for this week!-Alexis
Google Cloud community articles and blogs
20 
20 claps
20 
Written by
Google Cloud Developer Relations
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Google Cloud Developer Relations
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/a-to-z-of-google-cloud-platform-a-personal-selection-b-is-for-bigtable-87fbe9630f0d?source=search_post---------160,"There are currently no responses for this story.
Be the first to respond.
Now I’m not going to dwell on BigTable itself but more about it’s relationship with HBase . To make up for my rule busting entry for A this is a really short post . Hbase is based upon Google’s BigTable and GCP provides Bigtable as a managed product . Yeah I can hear a thousand cries of “but what about lock in ?”, “I have invested time and effort into Hbase” …. But the quirky thing is that you can in the most part forget about managing your own Hadoop clusters (which is what Hbase is built upon) but can let Google manage all that infra stuff as you can use the Cloud Bigtable HBase client . Yes I can still hear the thousand cries and yes there are some differences . But the list is quite small and you need to decide on whether the feeding & watering of your own hadoop cluster is actually worth the time and effort invested in administering it ! By the way if you really want a hadoop cluster then Google have a managed hadoop and spark service for that for you in Dataproc
As an aside: M in this series is spoilt for topics so I won’t be talking about how the Hadoop ecosystem basically derives from Google’s mapreduce and Google File System papers. You can read the research papers yourself they can be found here and here .
I find it mind blowing how much of the technology Google needed to invent itself to meet the needs of an internet scale business have made it out as Open sourced projects in some form that have basically turned into foundational lego bricks.
Google Cloud community articles and blogs
5 
5 claps
5 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Chocolate addict - I have it under control really I do. I do stuff involving cloudy tech. Tweets my own so only me to blame, except for retweets.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/cloudflare-blog/using-google-cloud-platform-to-analyze-cloudflare-logs-d5abf48157ba?source=search_post---------161,"There are currently no responses for this story.
Be the first to respond.
by Kamilla Amirova
We’re excited to announce that we now offer deep insights into your domain’s web traffic, working with Google Cloud Platform (GCP). While Cloudflare Enterprise customers always have had access to their logs, they previously had to rely on their own tools to process them, adding extra complexity and cost.
Cloudflare logs provide real time insight into traffic, malicious activity, attack incidents, and infrastructure health checks. The output is used to help customers adjust their settings, manage costs and resources, and plan for expansion.
Working with Google, we created an end-to-end solution that allows customers to retrieve Cloudflare access logs, store and process data in a simple way. GCP components such as Google Storage, Cloud Function, BigQuery and Data Studio come together to make this possible.
One of the biggest challenges of data analysis is to store and process large volume of data within a short time period while avoiding high costs. GCP Storage and BigQuery easily address these challenges.
Cloudflare customers can decide if they wish to obtain and process data from Cloudflare access logs on demand or on a regular basis. The full solution is described in this Knowledge Base article. Initial setup takes no more than 30 minutes to an hour. Moreover, customers can still replace any part of the process with their own tool or solution.
Below is a simple visualization of the data flow:
Cloudflare logs are obtained via a REST API. Usually this service can be run on your local workstation or Virtual Machine. The illustrated solution uses GCP Compute micro-instance.
For storing and managing log files we used GCP Storage bucket. All logs are stored in JSON format. Google Cloud Storage allows you to adjust the storage capacity when needed and set the retention policy.
Analyzing large data sets can be challenging. Google BigQuery makes it straightforward. When there is a new log file uploaded to the GCP Storage bucket, GCP Cloud Function triggers the process to import data from the new log file into BigQuery. BigQuery allows you to access your data almost immediately by running a simple query. As illustrated below you can, for example, pull top requested URIs with status code 404.
Based on feedback from our customers about which data they are interested in, we used GCP Data Studio to create visual reports. The following reports can be created in Data Studio using BigQuery as an input: top client IP address requests, requests by URL, error types, cached or uncached URLs, top triggered WAF rules, traffic types by device or location and many more.
Google Cloud is offering a $500 credit towards a new Google Cloud account to help you get started. In order to receive a credit, please follow these instructions.
Costs depend on several factors including the number of requests, storage, retention policy and number of queries in BigQuery, among others. For more pricing details, please use the GCP Pricing Calculator.
Please reach out to your Cloudflare Enterprise Solution Engineer or Customer Success Manager for more information.
Originally published at blog.cloudflare.com on October 26, 2017.
Select highlights from blog.cloudflare.com
5 
5 claps
5 
Written by
The Official Account of Cloudflare
Select highlights from blog.cloudflare.com
Written by
The Official Account of Cloudflare
Select highlights from blog.cloudflare.com
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://lab.wallarm.com/how-to-protect-web-applications-on-google-compute-cloud-with-waf-bc10924a849b?source=search_post---------162,"
					We're sorry, but we can't find the page you were looking for. It's probably some thing we've done wrong but now we know about it and we'll try to fix it. In the meantime, try one of these options:					
© 2021 Wallarm				

			Type above and press Enter to search. Press Esc to cancel.		
"
https://medium.com/@hiranya911/logging-in-java-libraries-for-firebase-and-google-cloud-platform-f8742493b73f?source=search_post---------163,"Sign in
There are currently no responses for this story.
Be the first to respond.
Hiranya Jayathilaka
Feb 5, 2018·7 min read
When developing server-side Firebase applications using the Firebase Admin SDK for Java, it is sometimes useful to look at the debug logs produced by the SDK. The FirebaseDatabase class provides a setLogLevel() method that facilitates this to a certain extent. However, setLogLevel() method was deprecated in the version 5.3.0 release of the Admin Java SDK. The new recommended approach for obtaining debug logs from the SDK is via SLF4J. This change comes with several advantages that enterprise Java developers would find useful.
Logging in enterprise applications is a complicated matter. There are many aspects to consider including persistence, log rotation, formatting and performance. Fortunately, there are many powerful logging frameworks such as Logback, Apache Log4J, and Java’s built-in java.util.logging (JUL) that take care of most of these concerns. However, each of these frameworks have a slightly different API and a configuration model. Therefore if you write an application using any one framework, you will be tied to that framework. This is particularly problematic for library developers, since the users of the library may want to use different logging frameworks.
This is where the logging facades come in. A logging facade is a high-level logging API that acts as a mediator between application/library code, and the logging frameworks. We write our code using a logging facade, instead of a specific logging framework. Then at the deployment time we can decide what concrete logging framework to use (i.e. late binding the logging framework). By decoupling code from concrete logging frameworks, facades yield greater levels of code portability and maintainability.
SLF4J is one such logging facade. It supports all the common logging frameworks, and is favored by many Java developers due to its simple API and the performant implementation. Starting from version 5.3.0, Firebase Admin SDK for Java uses SLF4J as its logging facade. This enables developers using the Admin Java SDK to use any concrete logging framework of their choice. SLF4J can be easily wired up to a logging framework by dropping a binding library to the classpath. It has been tested in many environments, and also works well in Google App Engine.
This is mainly useful when testing or debugging an application locally. Simply add the slf4j-simple binding to the application classpath, and set the -Dorg.slf4j.simpleLogger.defaultLogLevel=debug system property. The Admin Java SDK will start printing debug log statements to your console:
Take a look at the API docs of the SimpleLogger for additional options you can set to further configure the log output. Instead of setting them as individual system properties, you can also save them to a file named simplelogger.properties, and put it in your application classpath to be auto discovered.
The deprecated setLogLevel() method also prints logs to the console. Therefore, the approach described here can be used as an alternative to calling setLogLevel(Level.DEBUG) in your application code.
Most Google Cloud Platform (GCP) libraries for Java do not use a logging facade yet, and instead directly log to the JUL framework. Personally I’m not a fan of JUL, but one cannot ignore the appeal of an API that is built into the language. You can configure how JUL operates by creating a logging.properties file for your app. Just remember to set the -Djava.util.logging.config.file system property to point to that file when executing the application. If you are not familiar with logging.properties files, listing 1 shows an example.
Configuration in listing 1 instructs JUL to log to the console. It enables debug logging for com.google.* packages, which includes all GCP libraries. Then it sets a pattern for the log statements. Some sample JUL logs captured using listing 1 are shown below. These logs were produced by the Google API client library — a dependency used by the Admin SDK to make HTTP calls. This specific interaction was triggered by a call to the FirebaseAuth.getUser() API in the Admin SDK:
If you wish to also direct the Firebase Admin SDK logs to JUL, you just need to drop the slf4j-jdk14 binding to the application classpath. Here are some Realtime Database logs captured in this manner:
Since listing 1 enables debug logging for all com.google.* packages, it activates the debug mode for Firebase Admin SDK as well, which uses com.google.firebase as its package name.
Google App Engine (GAE) expects applications and libraries to log to the JUL framework. You can configure logging by creating a logging.properties file in the WEB-INF/ directory of your app. Listing 2 shows a configuration that enables debug logs for the Firebase Admin SDK.
You also need to reference the configuration file in your appengine-web.xml, as illustrated in listing 3.
Finally, to actually direct Admin SDK logs to JUL, bundle the slf4j-jdk14 binding with your app (i.e. make sure it goes in the WEB-INF/lib directory of the app). Now when you deploy the app in the cloud, GAE will capture the debug logs produced by the Admin SDK, and make them accessible through the GCP web console.
Logging is an important aspect in enterprise Java applications that often comes in handy when testing or debugging code. GCP libraries log to Java’s built-in JUL framework, which can be configured via a logging.properties file. Firebase Admin SDK for Java goes one step further by using SLF4J as a logging facade, which enables developers to use any logging framework of their choice (including JUL). Logs produced by these libraries can be directed to the console, the file system or any other persistent storage suitable for logs. When deployed in the App Engine environment, the logs are automatically collected, parsed and sent to the GCP console for presentation.
I hope this answers most of the questions Java developers may have on obtaining logs from server-side Firebase and GCP libraries. If you have come across any interesting logging-related use cases, feel free to share them. Happy coding with Firebase and Google Cloud Platform!
Software engineer at Google. Enjoys working at the intersection of cloud, mobile and programming languages. Fan of all things tech and open source.
10 
10 
10 
Software engineer at Google. Enjoys working at the intersection of cloud, mobile and programming languages. Fan of all things tech and open source.
"
https://medium.com/@gmusumeci/getting-started-with-terraform-and-google-cloud-platform-gcp-deploying-a-postgresql-database-ce1664fb038c?source=search_post---------164,"Sign in
There are currently no responses for this story.
Be the first to respond.
Guillermo Musumeci
Nov 15, 2019·6 min read
This is part 5 of the tutorial Getting Started with Terraform and Google Cloud Platform (GCP).
Part 1, deploying VMs in a public-only single region of the tutorial →…
"
https://rominirani.com/benchmarking-redis-on-google-cloud-platform-3eb43465cb2d?source=search_post---------165,"Redis, the high-performance data structure server comes along with its own benchmarking tool. The tool is similar to Apache Bench (ab). I decided to benchmark Redis on Google Compute Engine VM instance and see what it looks like.
I decided to set up 2 Compute Engine instances. One instance will run the Redis Server and the other instance will run the Redis benchmarking tool.
You can use the gcloud tools to create instances or use the Web console (ofcourse there is an API option too, but we will leave that for now) to create the Compute Engine instances.
I used the Web console to create the 2 Compute Engine instances (redis-instance-1) and (redis-client-1) as given below:
This will get 2 Compute Engine instances for you (redis-instance-1) and (redis-client-1). All we need to do now is to get Redis setup on both of them and that is the next step.
The next step was to setup Redis on both redis-instance-1 and redis-client-1 instances. The steps below need to be repeated for each of the instances:
3. Once we are done with that, we can get the latest 3.0 Release of Redis and expand that as shown below:
4. Now, let us build and install the software:
The above steps will get your instance setup with Redis and the all the applications i.e. redis-server, redis-cli, redis-benchmark, etc will be available.
All we need to do now is to SSH into redis-instance-1 and start the Redis server (default) on that instance. The steps are straightforward:
3. The server starts on port 6379 (default)
In case you do not want to run the Redis benchmarking tool on another Compute Engine instance and instead want to do it from your local network, then you need to allow traffic on port 6379 i.e. the default Redis port.
To do that with the following steps:
4. Click on Save.
The last step was to SSH into the other Compute Engine instance (redis-client-1) and fire the following command:
This is a simple Redis benchmark test that simulates 100000 operations. Note that I need to provide the IP Address of the redis-instance-1 VM. The -q parameter is the quiet mode.
Note: From your Developer console, you can get the External IP Addresses from the Compute Engine → VM Instances list. To get the internal IP Address, click on any of the Instances and the detail page should show you the Internal IP Address.
Refer to the Redis benchmarking tool page for more information.
We know that to talk from one Compute Engine instance to another, we can use either the Internal IP Addresses or External IP Addresses.
The results shown below is from a sample run when I use the Internal IP Address for the redis-instance-1.
The next set of results are when I use the External IP Address for the redis-instance-1.
Of course, I should do a few runs of the above test and then average things out, but you get the point. It should be straightforward now for you to probably extend these to across compute zones/regions and do a bench mark test from your local machine.
Technical Tutorials, APIs, Cloud, Books and more.
21 
21 claps
21 
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
Technical Tutorials, APIs, Cloud, Books and more.
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
Technical Tutorials, APIs, Cloud, Books and more.
"
https://medium.com/google-cloud/this-week-in-google-cloud-platform-gcp-lands-in-india-firebase-summit-imperative-tensorflow-b4898233fdf9?source=search_post---------166,"There are currently no responses for this story.
Be the first to respond.
Most of this week’s announcements are about bringing GCP closer to you :
In other news, the Firebase Dev Summit came to Amsterdam last week with a series of announcements all detailed in this post (Crashlytics, A/B testing, and Predictions) : What’s new at Firebase Dev Summit 2017 (Firebase blog). All session videos are also now available (youtube.com)
From the “Machine Learning from announcements to implementations for everyone” department :
From the “Customers and partners talk best about GCP” department :
From the “Container orchestration and beyond” department :
From the “BigQuery dataset, automation, and data prepping” department :
GCP Podcast episode 101 covers Cloud IoT Core and has a new permanent host. Check it out ! (gcppodcast.com)
The picture of the week is taken from the AI-based upsampling article (dpreview.com) :
That’s it for this week!-Alexis
Google Cloud community articles and blogs
8 
8 claps
8 
Written by
Google Cloud Developer Relations
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Google Cloud Developer Relations
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/a-to-z-of-google-cloud-platform-a-personal-selection-r-rolling-updates-fb6634c5f400?source=search_post---------167,"There are currently no responses for this story.
Be the first to respond.
I’m going to talk about how you can achieve a rolling update to applications on GCP compute products.
Before I do that I just want to ensure we are on the same page as regards to what I mean when I say rolling update .
A rolling update when I talk about it is the ability to release a new version of software with pretty much zero downtime . While the new version is being rolled out the previous version of the software is being rolled out of the way until by the end of the single step update process only the new version of the software is running . As well as being to roll the update out you must be able to pause the rollout and roll it back if need be. If you don’t have that ability to recover from a rubbish deployment then it’s a one way street!
An alternative way to carry out an update to your application is a canary release. With canary releases you release the new version alongside the existing version but whatever you decide after you have the new version and old version running side by side you don’t just carry on rolling out the update you stop the update process for a period of time . You will eventually switch over to the new release fully or not if the new version turns out to be a bit sucky. Have a look here for a nice walkthrough of canary releases.
I’m not going to debate the pros and cons of each approach though as that would derail this post somewhat I feel. So onwards :-)
What I will be discussing here assumes that you “bake” new custom images for each new version.
GCP have a great solution & tutorial ( Thanks @evandbrown) on creating “baked” images using Jenkins , packer and kubernetes. So if you’re interested in understanding how to set up a CI/CD system for baking/creating your own custom images it’s definitely worth a read.
I know you could use a configuration management tool to roll out application level updates across your fleet but by treating the instances as immutable you can easily take advantage of using managed instance groups to achieve rolling updates.
Managed instance groups use instance templates to define the properties for every instance in the group.
Any of the settings you can define in a regular request to create an instance can be described in the instance template, including any instance metadata, startup scripts, persistent disks, service accounts etc.
You can easily update all of the instances in the group by specifying a new template in a rolling update.
To start a rolling update You need to follow the steps outlined below ( see the docs for a detailed walk through).This high level walkthrough assumes you already have a fleet of instances that are defined and managed via managed instance groups.
As we’re using GCE you don’t need to create any other credentials and you’re able to start using the API
3. Create a new template with the updated properties. So assuming you have created a new image you can add the name of the image to the template
4. Use the gcloud command or the API to start the update. You can Change the defaults by passing in the values you want.
The optional flags you can change are listed below. They give you a good indication of how you can control the update and meet my requirement of being able to abandon the rollout.
— max-num-concurrent-instances , which defines how many instances are updated at the same time.
— instance-startup-timeout , which defines the maximum number of seconds that the update waits for an instance to start after the updates have been applied. If the instance does not start before the time limit, the updater records the update as a failure.
— min-instance-update-time, which defines the minimum number of seconds that the updater spends to update each instance. The updater starts the next update only when the current update is complete and the minimum update time is spent.
— max-num-failed-instances, which defines the maximum number of instance updates that can fail before the updater records the entire group update as a failure.
— auto-pause-after-instances, which tells the update to automatically pause after updating a specific number of instances. After the pause, you can decide whether to cancel or continue the update.
The default gcloud command to kick off a rolling update looks similar to this:
You can pass the optional flags listed above to override the default values to the gcloud command.
A rolling update can be applied to all instance groups, whether or not they have autoscaling enabled. Have a look at the docs for detail on how autoscaling managed instance groups and rolling updates interact.
GKE (Google Container Engine) is GCP’s fully managed kubernetes service and as such when we talk about how to implement a rolling update we are talking about how to update your application or components that you have running on a kubernetes cluster. As I’m talking about GCP I’ll be referring to GKE but the majority of what I describe about rolling updates with GKE actually refers to k8s ( as kubernetes tends to be abbreviated )
The table below describes Kubernetes concepts
Your application container is deployed to a pod.
Containers should absolutely be treated as immutable . Yep I’m not entering into a debate , not leaving you to make up your mind just stating that and leaving it here! With that out the way it implies that you will be creating a new container image when you update your application or micro service component .
The rolling update process basically replaces your running containers with an updated container.
There are two ways that you can manage your application via replication controllers or via deployments so depending on which configuration you have selected the rolling update process is slightly different. Using deployments should be the method used so this is what I’ll talk about.
So what is a deployment ? you may well ask. Deployments provide declarative updates for Pods and Replica Sets (the next-generation Replication Controller). You only need to describe the desired state in a Deployment object, and the Deployment controller will change the actual state to the desired state at a controlled rate for you. ( supported in the version of k8s managed by GKE)
The kubernetes docs talks you through what a deployment is and how to use in some depth so I won’t repeat that.
Here’s an example deployment yaml file called webapp-deployment.yaml
It is used to deploy an image called mywebapp whose version is 1.0.1 and will bring up 3 pods of mywebapp
So let’s assume You’ve already got a GKE cluster up and running and have deployed this version ( see the docs for how to do the deployment to a running cluster)
To implement a rolling update to your application here’s a summary of the steps needed:
Alternatively you can directly edit the deployment directly using kubectl edit ( Note unless you want to find yourself in vi then ensure you set your KUBE_EDITOR, or EDITOR environment variables) . Edit the Deployment and change .spec.template.spec.containers[0].image from mywebapp:1.0.1 to mywebapp:2.0.0
This carries out a rolling update . If you follow through with the hello world example in the GKE/K8s docs you can see this in action . Specifically noting that it will not destroy all the pods running the old version but keep some running until there are some new pods with the updated application running. What’s really neat though is that using .spec.strategy.type==RollingUpdate You can specify maxUnavailable and maxSurge to control the rolling update process. The docs have an excellent explanation of how this works
To meet my criteria of a rolling update you must be able to roll back to a previous version and GKE / k8s allows this by implementing the methods .spec.rollbackTo and .spec.rollbackTo.revision or using the kubectl command: kubectl rollout undo deployment So in our example to roll back to the previous version of my-app using the kubectl rollout command we would use this:
You can also roll back to a specific version .
GAE being the compute target that requires the least customer configuration and admin you’d expect it to be the most straightforward to undertake a rolling deployment and you’d be right to think that.
GAE allows you to upload a different version of of your application and cut over to that version. You can also traffic split but that would not meet my definition of a rolling update.
App engine serves traffic by default from the default version of your application ( I know talk about a clumsy sentence!) . You can upload a new version and this will not serve any traffic until you switch over to the new version you have just upload
Using python and App Engine standard to elaborate on that admittedly short description.
GCP provides a tool call appcfg.py. This is used to upload new versions of your python application ( GO uses this tool as well but Java uses maven so check the language specific docs )
Your first deployment automatically becomes the default and serves 100% of the traffic .
Uisng appcfg.py to upload your new version you will then see in the console that you now have two versions of the application
By checking the box beside version 2 you are then able to route all or some of the traffic or migrate all traffic across to this version. To enact a rolling deployment select migrate traffic. Migration takes a short amount of time (possibly a few minutes), the exact interval depends on how much traffic your application is receiving and how many instances are running. Once the migration is complete, the new version receives 100% of the traffic.
When using traffic migration you should use warmup requests to load application code into a new instance before any live requests reach that instance thus reducing thus helping to reduce latency request for your users as you switch over to a new version
You can load 10 different versions and swap over to anyone of them easily.
Note The traffic migration functionality I have described is only available in the App Engine standard environment
The behaviour with the App engine flexible environment is slightly different as there you are creating a new Docker container when you create a new version of your application and there you use the gcloud app deploy command to automatically build the Docker container and switch traffic over to this new version.
You can override this default behaviour by using the — no-promote flag
Google Cloud community articles and blogs
6 
1
6 claps
6 
1
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Chocolate addict - I have it under control really I do. I do stuff involving cloudy tech. Tweets my own so only me to blame, except for retweets.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/google-cloud-platform-at-ubiquity-dev-summit-2016-ffe0943812b9?source=search_post---------168,"There are currently no responses for this story.
Be the first to respond.
Developer Advocates were busy building awesome demos to show at the Ubiquity Dev Summit! In case you missed it, here are four great talks to show you how to build IoT devices to leverage the power of cloud.
Julia Ferraioli sat down with Dan Shapiro, founder and CEO of Glowforge, to discuss his company, crowd-funding, start ups, and more.
Amy Unruh led a code lab “Processing And Analyzing Real-time Event Streams in the Cloud”. The setup instruction and the lab itself are both online.
Google Cloud community articles and blogs
6 
6 claps
6 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Loves #Java. Lives in NYC. Creator of @JDeferred. Ex @Google @JBoss, @RedHatSoftware, @Accenture. Opinions stated here are my own.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/google-cloud-platform-http-load-balancers-explained-via-the-cli-4f4d61805297?source=search_post---------169,"There are currently no responses for this story.
Be the first to respond.
This is a post cross posted from my blog.
The Google Cloud Platform Load Balancers are based off of technology that Google developed for our applications. There are two types of load balancers, the Network (L3) Load Balancer and the HTTP (L7) Load Balancer. The HTTP Load Balancer is global so the same IP can be used everywhere in the world, but still supports very high scalability with no warmup.
Setting up the HTTP Load Balancer is fairly straightforward in the Developers Console. You can create one in the Networks section of the console and create a load balancer. Once you’ve started creating an HTTP Load Balancer, you get a page something like this:
Each of the sections is nicely laid out and allow you to create the load balancer all at once. But there are many objects being created under the covers here, many of which only vaguely map to the UI. It can be a bit daunting to set up via the the Google Cloud CLI.
The HTTP Load Balancer documentation has some good info and diagrams that help understand how it works. But I found the diagram there to be a bit too simplistic when I wanted to set up the load balancer via the CLI. I needed to know a bit more about all the parts so I came up with this diagram.
Let’s go step by step through how to create the load balancer via the CLI. As we do that, I’ll try to point out what each of the objects we are creating correspond to in the Cloud Console so you have an idea where to look for them later.
Since most of the objects depend on one another, we will need to go from “back” to “front” starting with health checks and backend services and ending with forwarding rules. The health check object doesn’t depend on anything else so we can create it first. Even though we create the object here, it only really becomes active after we attach it to a backend service.
Here we create a health check that will connect to our app via port 80 at the /healthz URL. Note that creating the health check only tells further configuration the port and path to check but doesn’t actually send the health checks. The host parameter isn’t actually used as the host to connect to but only to set the Host header. Some apps check this header so we want them to be able to return a successful status. The instances to health check are specified later by the backend service.
Health checks are used in more than one place so they live under the Compute Engine part of the Cloud Console UI. The health checks listed here are the what correspond to http-health-checks and https-health-checks in the CLI.
Next we’ll create a Backend Service. A backend-service object defines the backend VMs that actually serve the requests. The Backend Service contains a number of Backends. Each Backend is essentially a link to an instance group but has some other options attached like which port numbers to use and load balancing mode. The prefered way to set the port is via named ports on the instance group. You can set a named port for port 80 called http-port using the following command. I’m assuming you already have an instance group called my-instance-group set up. You can find out more about creating managed instance groups here.
Next you can create the backend service:
Now that we have the health check attached to the backend service, it will connect to the instances specified by the backend service to do the health checks.
The UI shows backend-services in the “backend configuration” part of the UI.
Next we have to create a Backend. A Backend specifies the instance group you want to send traffic to, and how the load should be balanced among the available instances. You can create more than one backend and a backend is generally created one per instance group. You can use this to do cross-region load balancing for instance.
This sets up a backend that sends traffic to my instance group and uses the request rate as a way to load balance. I am setting it so that each instance will get 10 requests per second maximum but you can also set this up to use CPU utilization based on your needs.
In the UI, backends are part of the backend service form. You can add any number of backends to the backend service just like can in the CLI.
Url maps are used to map hosts and urls to backend services. The url maps hold two types of objects, host rules and path matchers. Each host rule can have multiple path matchers. Each request is matched against the host rule and then the path matchers for the host rule that matches. When creating a url-maps object you specify the default backend service that is used when no host rules match.
If you have only one backend service then this one command is usually enough since all traffic can be sent via the default service. However if you have multiple backends you can set the up based on host or url. A host rule can have multiple path matchers but the host rule must have at least one path matcher so we create the path matcher and host rule at the same time.
You can specify that requests with a different host go to a separate backend service as well.
Like the url-maps itself you specify a default service if the host rule matches but no path rules match. You can also specify different backend services be used based on the path.
In the UI the URL maps, host rules, and path matchers are specified in the “Host and path rules” section.
The first row contains the default service which is used when a request doesn’t match a host rule/path matcher combination. The other rows contain the host rule/path matchers.
Target HTTP Proxies and Target HTTPS Proxies are objects that connect one or more forwarding rule to a URL map.
Target Proxies terminate the connection to the user so you specify the SSL certificate to use when you are using HTTPS. SSL certificates are created like so:
You can then use the certificate to create the HTTPS proxy.
You still need to create one but at this point I felt that Target Proxies made things more complicated than it needs to be since you almost always use one forwarding rule per target proxy for HTTP load balancers.
Forwarding rules are final object we need to create. These are the objects that your are actually billed against. The Forwarding Rules map the IP address for your load balancer to the Target Proxy that will handle the requests. First we will need to create our IP address though. We will need a global, rather than regional, IP address for our HTTP load balancer.
Then we can create our forwarding rule. Notice that we will need to put in the actual IP address that we just created rather than the IP address name. Not also that you can only put a single port as the — port-range option and that we need to add the — global option.
Many applications will want to redirect users that access http://www.example.com/ to https://www.example.com/. This is a pretty common use case that is not supported by the load balancer. You need to create a totally separate Target HTTP Proxy and Forwarding Rule for HTTP. You essentially need to have two load balancers to handle the traffic, and then actually redirect users in your application.
Notice that we put the same IP address in for the HTTP Forwarding Rule. This makes is so that we can listen on port 80 and on port 443 at our IP address.
Now that you’ve created a forwarding rule, it will show up in the “Load balancing” section of the developers console.
There is also an “Advanced View” that allows you to view the objects in a format that is much closer to the CLI counterparts. There are tabs for each of the major objects as well as a couple for the network load balancers.
The objects that make up the HTTP(S) Load Balancer and the commands that you need to run to set it up on GCP are not totally obvious given how you create a in the UI. But hopefully this post has shed some light on how they map together. Be sure to also check out the HTTP Load Balancer documentation it has lots more info and guides like how to do some more complex setups like cross-region load balancing and content based load balancing.
Google Cloud community articles and blogs
4 
1
4 claps
4 
1
Written by
Developer Advocate @Google for @GoogleCloud. Vice Chair for @PyConJ. Sometimes tweets in Japanese. #python #golang #kubernetes #atheist
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Developer Advocate @Google for @GoogleCloud. Vice Chair for @PyConJ. Sometimes tweets in Japanese. #python #golang #kubernetes #atheist
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.datadriveninvestor.com/how-to-start-a-tech-startup-for-0-with-google-cloud-platform-and-other-services-stitched-together-b7ff08f34e2b?source=search_post---------170,"There are currently no responses for this story.
Be the first to respond.
Learn how to start a tech startup with no money. You might think I am joking but I am not. In this no-fuss step-by-step semi-technical guide, I will walk you through the process to start a tech startup.
This guide will not dive deep into the technical details. If you know technical things like how…
"
https://medium.com/google-cloud/a-to-z-of-google-cloud-platform-a-personal-selection-n-naming-stuff-ff5618dcb247?source=search_post---------171,"There are currently no responses for this story.
Be the first to respond.
Okay I admit I wanted to talk about networking here but realised that I’d already covered a lot of that subject under F ( firewalls & subnets briefly) , G ( Global load balancing) and I ( interconnect) so I’d essentially be mopping up which didn’t feel right for this series .
So after mulling it over and despite the urge to delve into neural networks I’m sticking to “naming stuff”
I’m not going to be talking about naming conventions though in this post I’m just picking out a few things that should help you manage your resources in GCP.
Firstly Just don’t give nice names to your instances. They should be treated as though they are ephemeral and you shouldn’t think of cuddling them. There’s enough posts out there discussing this ( pets v cattles etc) so I’m not going to dwell on that point.
But naming certain categories of stuff with sensible names is always a good idea for usability and manageability plus when setting up firewall rules they can be very handy too ( I know I can’t stay away from networking!). Databases, tables, columns all obvious so I’ll move on.
Groups of stuff like instances absolutely you should .. Just not the individual instances!
Okay so how do you actually apply names to groups of things or resources in GCP so you know what those instances or service keys are for …
Let’s look at tags first.
To identify what your instance’s function is, the environment it’s running in etc you may want to add one or more tags to it when creating the instance using the -tags flag as part of the gcloud command . For example the following gcloud command will add the tags test and web to the instance :
You can also add tags when creating instances from the console or when using Deployment manager. From the console after the instance is created you can see the tags associated with the instance
After an instance is up and running you can use the add-tags flag to add additional tags. This is easy to do via the console as well for example if I want to allocate the instance I created earlier to Joe I can add an additional tag using the following command to let me know that Joe is using it:
You can add the tags to your instance templates so any instance launched as part of an auto scaling action will be tagged.
If you’ve been following this series you may recall in my entry for f that I discussed how you can use tags with subnets and firewall rules
Okay onto labels .
Huh you say what’s the difference between tags and labels? You may well ask! Here’s what the docs say about what a label is:
A label entity is a key:value pair that you can attach to a project or a virtual machine (VM). VM tags that you defined in the past will also show up as value-less labels.
Labels are to help you organise your Google Cloud Platform resources by allowing you to filter and group resources using the labels for example to identify all those resources that are in use for testing as opposed to those in production .
You create labels at the project level by using the resource manager api or console to create the labels.
The key thing to note is if you are using labels with Instances (VM’s ) Networks and firewalls do not use labels, but will recognise the label keys that are added as instance tags.
But saying that the key of a label is added as a tag to your instance so technically you can use a value-less label as your tags. I played around with this though and to cut a long story short use tags if you will need to manage firewall rules to narrow down rules to specific subnets and labels to help you filter projects based on how you organise your projects.
Naming service accounts sensibly . If you’re implementing the principle of least privilege and you know you should be really. Service accounts can be scoped down in many cases to only access the GCP resources with the permissions they need no more no less. Thus a naming convention to indicate what permissions and what resources a service account can access will help enormously for example when creating a service account that will have read access to your Cloud storage buckets give it a name that reflects it’s scope.
Lastly I just wanted to cover naming Cloud Storage objects.
Google Cloud community articles and blogs
23 
23 claps
23 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Chocolate addict - I have it under control really I do. I do stuff involving cloudy tech. Tweets my own so only me to blame, except for retweets.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/setup-guide-for-fastai-part1-v3-fastai-1-0-on-google-cloud-platform-with-0-2-hour-fe92c7ec855d?source=search_post---------172,"There are currently no responses for this story.
Be the first to respond.
Note that this post is mainly an update previous post on fastai v2 setup, if you are interested how to use Jupyter Notebook effectively with extension. Please go to here.
You will need to fill in your Credit Card information, they will charge you 1 USD for verification. After that you will get $300 USD free credit and get access to GPU.
In the home page of GCP console, there are some info that you would like to copy down to a text file. Copy your Project ID, as you will need it later when you SSH to your instance. (For me, the project ID is endless-fire-xxxxxxx)
In your Google Cloud console home page, you should see your Project ID, copy it somewhere now, you will need this later for setting up your environment.
We can start creating instance now. Click “Compute Engine” which appears at the left side.
This is the only option I found to create a VM that install almost everythin included Nvidia Driver, anaconda, git and everything.
Please check your IP and project ID on your GCP console and put change the parameters in below script. You also need to pick a Zone, for me I am using asia-east1-a, pick the one close to you.
Things like numbers of CPU, RAM can be changed later if you want, so don’t worry about it for now.
PROJECT_ID is the Project ID that shown in your console’s home page, which I have asked you to copy down earlier.
STEP 1 Install Google Cloud SDK or use the Cloud Shell on the console page
I will not spend time to explain how to install it. Please follow the instruction here, the documentation is straight forward.
STEP 2Create a firewall rule for accessing port 8888 from your local machine(you will be using port 8888 for accessing your Notebook), using the console or the command line:
STEP 3 SSH into the instance. Open the Google Cloud SDK Shell or use the Cloud Shell directly
Port Forwarding
What these command does it setting up your configuration on your local computer, and ssh to your instance with port-forwarding. This allows you to connect to your remote VM jupyter notebook with localhost:8888 with port-forwarding
STEP 4 Run jupyter notebook:jupyter notebook --ip=0.0.0.0 --port=8888
Note the token displayed in the terminal. Go to the notebook using the external IP of your instance. You can find your external IP of your instance in the console.
xxx.xxx.xxx.xxx:8888 is the link of your notebook. If you are using Google Cloud SDK Shell with port forwarding, you can also use localhost:8888 to connect to your notebook.
You will need a token/password to login to your Notebook. Your token is here:
Alternatively, in your cloud terminal, you can do this to setup a password for access.
STEP 5 Install everything on your Cloud Terminal
I have create a bash script to install everything, it install conda, setup the fastai conda env, install fastai library and clone their repository.
STEP 6 Done
Remember to shut down your instance when you don’t need it, otherwise you will burn your credit pretty soon.
Google Cloud community articles and blogs
5 
3
5 claps
5 
3
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Learning | data science | HK @ GitHub: https://noklam.github.io/blog | Linkedin: https://Linkedin.com/in/noklamchan
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://rominirani.com/learning-docker-on-google-cloud-platform-coreos-is-your-friend-ccd763d75664?source=search_post---------173,"December 2017 : Its been a while since this post was published. Google Cloud Platform now has a fantastic tool that you can use as a development machine in the cloud. Its called Google Cloud Shell and is free to use. It comes with most developer tools (including Docker) all setup and ready for your use. Check it out: https://cloud.google.com/shell/
In an earlier blog post, I had presented the view that one of the best ways to learn Docker in my opinion is to move to the cloud and not do this stuff on your local machine. Chief reasons among them being bandwidth issues that could completely spoil your experience around Docker.
I usually recommend Google Cloud Platform to almost anyone that I meet due to its simplicity and behind the scenes power that is provided to you via Google’s amazing infrastructure. However, this is not the post for that.
So, assuming that you want to get started with Google Cloud Platform as a vehicle to learn Docker, I suggest that you do the following:
There is a free trial period of 2 months with a credit for $300 and this should be sufficient for your needs to learn Docker. I am not a marketing vehicle for anyone but you will love the platform and the power that it gives you.
Compute Engine is the Infrastructure as a Service (IaaS) component in Google Cloud Platform. While it is a vast topic, our goal here is to simply get ourselves a Virtual Machine (Compute Engine instance).
To do that, go to the Compute Engine → VM Instances option in Developer Console for your project. Click on Create Instance.
This should lead you to a form where you can define your resource (VM) capabilities in terms of Processor, RAM , which Operating System, which Region it should be hosted in and so on.
Just focus on the following attributes while filling up the form for Create a new instance:
This should create the Compute Engine instance for you. For e.g. if you named your instance as yourname-01, you should see the instance listed when you go to the Compute Engine → VM Instances page.
You will see a button titled “SSH” next to each Compute Engine instance that you create. This will directly enable you to SSH into the running VM instance via the browser itself. This is a very useful feature to avoid any other tools to be setup on your machine.
Once you successfully SSH into the instance, you should see the familiar Linux Screen. A sample screenshot is given below:
Try out a few commands as given below:
Obviously we do not have any images right now and if we run docker images, there will be none listed but that is just the start.
If you are itching to get started, check out the Getting Started Guides at:
Google Cloud Platform provides 3 ways to interact with it, as a client. They are:
So everything that I mentioned via the Web console can be done by (2) and (3) — but since the focus was on learning Docker and using Cloud resources, I did not want to create any local setup for you.
Google Cloud Platform also provides Container optimized Linux Images for your to use while creating the instance. If you are familiar with the Google Cloud SDK from the command line / terminal and want to use the gcloud tool to create instances, do read up on Container VMs.
Technical Tutorials, APIs, Cloud, Books and more.
3 
3 claps
3 
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
Technical Tutorials, APIs, Cloud, Books and more.
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
Technical Tutorials, APIs, Cloud, Books and more.
"
https://medium.com/google-cloud/a-to-z-of-google-cloud-platform-a-personal-selection-g-global-load-balancing-82e1b8550298?source=search_post---------174,"There are currently no responses for this story.
Be the first to respond.
When designing a globally accessible application two of the most common pain points I come across are figuring out how to ensure that there is no weirdness occurring related to DNS propagation as front end load balancing services scale out and the ability to direct end users to services in the region closest to them . The aim being to solve both without having to expend a disproportionate amount of effort on the infra services to solve them.
What I love is that using GCP load balancing these problems are mostly addressed out of the box. ( There are edge cases as with every situation I know ! )
Specifically the following load balancing components are the bits that solve this elegantly ( although at first looking at the diagrams in the docs it may seem more complicated than it really is !)
To understand how these services elegantly solve the problem we first need to understand how the load balancing components work together so I’ve just regurgitated from the docs a description of how GCP load balancing works :
So now you understand the flow let’s just touch briefly on the components I called out at the beginning and see how they contribute to solving the pain points I described.
A global forwarding rule provides a single global IP address which means there are no changes to IP addresses as the load balancer scales thus avoiding DNS propagation problems. and as the docs explain pretty clearly the flow I’ve just regurgitated:
Backend services is a centralised service that manages a group of instances that handle user requests . (The docs can come across slightly clunky imho at trying to describe this ). The service knows which instances are available and healthy , the amount of traffic that they can handle, and how much traffic they are currently handling.
Okay but why have I called them out ? Well as the docs say you can configure a backend in specific zones but the key thing is that those zones can be in different regions . Yup it’s built into the load balancing service out of the box to allow you to set up backend services in specific regions and zones and then using the magic of url maps you can now direct traffic to the appropriate back end be that directing users in Europe to backends services in the Europe region and those in the US to the US region
In setting up a cross regional set up and all you are concerned about is directing users to the closest region then in summary the steps you need to take are:
That’s it ! Really that’s it for this scenario . If your users when hitting the Load balancer are from Europe then they will be directed to instances in Europe and users from the US will be directed to instances in the US
For a step by step walkthrough see here
But what about URL Maps? They’re there doing the magic but in this case the default URL map takes care of the funkiness for you so you don’t actually have to do anything special apart from setting up the backend service and that isn’t complicated ( have a look at the example )
You can see for yourself that there is indeed a default URL map by using the following gcloud command:
For a graphical representation this image from the official docs shows how it all fits together :
So you see URL maps really are quite funky and it’s where all the cool magic really happens ( well I think so anyway). In the scenario I’m talking about in this post the detail is so abstracted that you didn’t even really know that URL maps were there doing it’s thing .
In many scenarios though you want to do more complicated routing then you need to configure a URL map using URL map resources which are the properties you need to use when creating a URL map
But rather than scratch your head about that I feel that the graphic below from the docs succinctly imho shows how a URL map works when you need to set up content based rules for example.
Hopefully this post has helped in understanding how GCP’s load balancing product can solve a difficult problem in a quite elegant way.
Google Cloud community articles and blogs
4 
4 claps
4 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Chocolate addict - I have it under control really I do. I do stuff involving cloudy tech. Tweets my own so only me to blame, except for retweets.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/google-cloud-platform-technology-nuggets-november-16-31-2021-edition-4d5cd245a538?source=search_post---------175,"There are currently no responses for this story.
Be the first to respond.
Welcome to the November 1–15, 2021 edition of Google Cloud Platform Technology Nuggets.
There have been key updates available in GKE and the push to make it the best managed platform for Kubernetes continues with this set of features.
First up is the concept of Container Image Streaming. When scaling up your application, the entire container image needs to be fully downloaded and available on the node before the application can boot up . Container Image Streaming takes the path that not all of the container image data is required for the application to boot up and this means that for large images, it could boost the startup times. Check out the blog post for more details. Keep in mind that this feature is available for Artifact Registry and not Container Registry.
Next up is the announcement of Spot Pods for GKE Autopilot. GKE Autopilot supports a Pod-level SLA, but you may not necessarily need that for workloads that are fault-tolerant and can survive some disruption. Think of Test and Development Environments too. If that is indeed the case, Spot Pods takes this to the next limit, where Google could preempt (with a 25 seconds grace period) the Pod and take it away. In return, you get a discount ranging from 60–90% on the regularly priced pods. This feature definitely makes running cost-effective workloads on GKE Autopilot a reality. Spot Pods are in Preview, and available starting with GKE version 1.21.4.
Language Translation is one of the critical and challenging areas in Machine Learning. Google Cloud is constantly on the move to provide quality translation services via various advances in machine learning and to match it to feature requests. Document Translation API is now in GA and it allows translation of documents in multiple formats. You can now avoid the step of separating out the text to translate from the formatting of the text in the document. It is available for 100+ languages and supports document types such as Docx, PPTx, XLSx, and PDF while preserving document formatting. There are multiple other features like real-time translation, regional endpoints for our Translation APIs and a great customer usage of our Translation services from Eli Lilly. Check out the blog post for more details.
The TechStop team at Google provides IT support to entire Google and given the work from home environment for the last 1.5+ years, it has had to deal with its own challenges of addressing the multiple IT support requests that come its way. One of the key things to help address the flood of support requests is to organize/categorize the requests coming in and given the various ways that users would report issues, they had a problem on their hands to create the right taxonomy and classification for the requests. In an interesting approach, they looked at how SPAM detection at Google has over the years protected users from variations that are subtle but mean the same thing. For example, think of users reported IT support tickets which mean the same thing but differ only in words and the way they put it across.
Check out the blog post that highlights the various algorithms around density clustering and a set of GCP tools that they are using to build a pipeline to achieve this. Keep a lookout for the followup posts to this one, since the entire solution will be demonstrated that you can apply with your own customizations.
Google Managed Service for Prometheus is now available in Public Preview. This managed service maintains compatibility with Open Source Prometheus offering and can be a drop in replacement for your existing Prometheus services, which you are managing. It lets you reuse your existing Prometheus configs, keep your existing dashboards in Grafana, retain your PromQL-based rules & alertsm deploy managed collectors and with its hybrid + multi-cloud capability, you can monitor any environment where Prometheus can run.
Check out the blog post for details.
Billing should be boring? That is exactly what the article states given the fact that you don’t wnat to end up with any surprises vis-a-vis your monthly cloud bill. Check out the post to get a wealth of information around:
Our Serverless series on addressing anti-patterns in Google Cloud continues with new episodes. Each blog covers a topic that we have seen appear multiple times in our support forums and we want to highlight best ways to address them or in other words, avoid an incorrect way to approach them.
The following blog posts have been published so far:
1. Designing your Functions for Idempotency
2. Reusing Cloud Functions instances for future invocations
3. Establish Outbound Connections from your functions correctly
4. How to handle Promises correctly in your Node.js Cloud Function
We have a follow up sketch note to our previous edition sketchnote, where we had an Introduction to Cloud Networking. In this edition, the sketchnote helps you understand several networking options that we have of connecting your infrastructure to GCP. When do you choose one over the other? What factors do you need to consider? Check the details in the blog post titled Choosing a network connectivity option in Google Cloud and the sketchnote below.
Speaking of Google Network, learn about our different Network Service Tiers (Premium v/s Standard) and when you would chose one over the other?
Have questions, comments, or other feedback. Do send it across.
Looking to keep a tab on new Google Cloud product announcements? We have a handy page that you should bookmark → What’s new with Google Cloud.
Google Cloud community articles and blogs
3 
3 claps
3 
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://blog.devgenius.io/amazon-web-services-vs-microsoft-azure-vs-google-cloud-platform-fb36595f1df4?source=search_post---------176,"1) — Currently, three prominent cloud service providers dominate the industry: Microsoft Azure (AWS), Google Cloud Platform (AWS), and Amazon Web Services (AWS). While the cloud computing market is flooded with cloud…
"
https://medium.com/google-cloud/a-to-z-of-google-cloud-platform-a-personal-selection-v-vpns-a8ba8471f37b?source=search_post---------177,"There are currently no responses for this story.
Be the first to respond.
I know I’m back to networking before anyone says anything :-) !
Being able to access your GCP resources securely from your own network is a common ask To do this you typically set up a VPN between your premises ( I’ll refer to this end as local from here on in) and GCP.
GCP provides Cloud VPN which allow you to set up an IPsec VPN connection. Traffic traveling between the two networks is encrypted by one VPN gateway, then decrypted by the other VPN gateway
As well as connecting your network to a GCP network. You can also connect GCP networks in different regions together using cloud VPN.
I’m not going to detail the specifics on configuring VPN’s and the permutations you can have as the docs are clearly laid out, see here for detailed setup instructions but at a high level the steps are:
Below are two examples diagrams straight from the docs illustrating what a cloud VPN configuration can look like . A simple 1:1 set up and one which has redundant tunnels .
Okay all pretty straight forward ( if you’re into networks that is !) but before I leave this topic I just wanted to mention Cloud router.
Cloud Router enables dynamic Border Gateway Protocol (BGP) route updates between your Google Cloud Platform network and your on-premise network. Why is this cool? Well paraphrasing the benefits directly from the docs:
The diagram below taken from the docs shows what a VPN configuration with cloud router typically looks like:
Google Cloud community articles and blogs
3 
3 claps
3 
Written by
Chocolate addict - I have it under control really I do. I do stuff involving cloudy tech. Tweets my own so only me to blame, except for retweets.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Chocolate addict - I have it under control really I do. I do stuff involving cloudy tech. Tweets my own so only me to blame, except for retweets.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/this-week-in-google-cloud-platform-gcp-coming-to-hong-kong-more-skylake-with-perf-libs-and-67c7026a9c82?source=search_post---------178,"There are currently no responses for this story.
Be the first to respond.
Last week’s announcements were essentially focused on rolling out new infrastructure and performance :
The Firebase team also announced multi-database support (in a single project)
Here’s what happens when your conference starts trending on Twitter (and how Cloud Vision API can help): “Twitter spam bots: with 🧡 from Russia” (linkedin.com)
Kelsey Hightower’s latest guide is an end-to-end deployment pipeline using Cloud Container Builder, GitHub, and multiple Kubernetes clusters (github.com)
From the “if you are not following the GCP publication on medium.com you probably should” department:
From the “you should check out the recent progress made supporting Spring Boot in App Engine Standard” department:
From the “GCP users talk best about GCP” department :
From the “Apache Beam and Cloud Dataflow picking up steam” department :
From the “In case you missed it (ICYMI)” department :
This past week’s GCP Podcast (#000104) covers one of the most recent GCP products, currently in beta — Cloud Dataprep, with product manager Eric Anderson.
Staying with podcasts, Jordan Tigani, engineering lead for BigQuery was the guest for this Software Engineering Daily episode (softwareengineeringdaily.com)
This week‘s picture is from the Hong Kong region announcement :
That’s it for this week!-Alexis
Google Cloud community articles and blogs
3 
3 claps
3 
Written by
Google Cloud Developer Relations
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Google Cloud Developer Relations
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@Titan_Hunters/titan-hunters-ama-recap-with-google-cloud-platform-gcp-representatives-91d773a4c837?source=search_post---------179,"Sign in
There are currently no responses for this story.
Be the first to respond.
Titan Hunters
Nov 28, 2021·5 min read
Titan Hunters is a rising new star in the NFT Gaming industry that has recently gained popularity with the successful IGO. With the current momentum from the IGO result and high expectations from the audience, the Titan Hunters team strives to deliver the official game sooner than planned and decides to host the game on a major cloud service provider — Google Cloud Platform.
Today we are bringing you to our Ask-me-anything (AMA) session, with Titan Hunters CEO Tiep Vu, Thetan Arena Solution Architect Advisor as speakers, and Google Cloud Customer Engineer Duc Le, Google Vietnam Territory Account Manager Chau Tran as moderators.
This AMA session will question Tiep Vu’s vision about blockchain video games in general, the current NFT video game industry, and the future of NFT gaming.
1. What makes a commercially successful blockchain video game?
To create a globally successful video game, it is critical for the game itself to be creative and engaging so that everyone can grow to love and stick with the game for a long time.
Currently, most NFT video games seem to focus on cryptocurrency users only. Their primary interest is investing and return-on-investment (ROI), not caring about whether the gameplay is unique or decent at all.
Due to this reason, most NFT gaming studios are rushing to provide the best ROI. Unfortunately, this ROI race is not sustainable in the long run. The token value decreases when the crypto market dips, and invested people will quickly abandon your game.
In our opinion, a successful NFT video game with longevity prioritizes top-quality and a much more diverse demographic, not consisting only of crypto investors. At the same time, NFT features will complement incredibly well with an already fun video game, as it can attract hundreds of millions of gamers and directly compete with current top-grossing game titles.
2. Why do businesses choose the Google Cloud Platform to develop NFT games?
I expect many businesses to make the same decision as us by choosing Google Cloud Platform. It delivers many advantages: highly scalable, intuitive user interface, easy and quick to reach the global audience, improved security, and stability.
3. What makes Titan Hunters different from other projects in the NFT gaming market?
The first difference is that we take inspiration from the best: dungeon-crawling similar to Diablo, simple and easy-to-learn gameplay similar to Archero (a shoot-em-up rogue-lite action mobile game), 3D blocky world from Minecraft and Lego.
These are the kinds of games that people have played for years. And by combining and distilling all of the above factors, we are introducing a breath of fresh air to the current video gaming space.
The second difference is the business model: Titan Hunters has both the Free-to-play and Invest-to-earn models. In contrast, most NFT video games are promoting the Play-to-earn model, which focuses on crypto investors.
We prioritize the largest audience first: millions of traditional gamers playing video games like Minecraft and Roblox. With Free-to-play, everyone should be able to play and enjoy the whole game at completely no cost. At the same time, the Invest-to-earn can give players opportunities to permanently own unique items or trade in-game items at any time (they must be converted to NFTs first, by using a little TITA).
4. The NFT gaming trend may not last due to the recent surge of low-quality NFT games, and many people are flocking to these games. What do you think about this and the future of NFT games?
I don’t think NFT gaming is a short-term trend. And it is genuinely revolutionizing the gaming industry. However, it will take some time for the market to adjust and the low-quality games to fall behind.
Honestly speaking, it is quite a sensitive issue and may provoke many people, as we are somewhat disappointed with the standard of the current NFT video game titles. Most are unfinished clones of popular titles that attach the “blockchain” tag to become an NFT game. Unfortunately, these actions result in terrible NFT games — the empty husks without the actual gameplay and core value.
Over time, more big players will come into the scene with possibly triple As NFT titles, and the clones will also gradually disappear.
We are completely serious about making Titan Hunters: We do not follow trends or make video games due to FOMO. Instead, we want to play the long war and give people an NFT video game that lives up to its name.
We could also say and many users believe the current NFT titles following the Play-to-earn business model bear striking similarities to the Ponzi schemes. However, such dodgy systems should not last for long since people are only there to earn money and not play the actual game.
5. Cloud Kinetics is honored to be selected by Topebox as a strategic technological partner regarding the Google Cloud Platform services. May I ask what criteria made you decide to choose Cloud Kinetics?
Cloud Kinetics, together with GCP, had prompt technical support on GCP, meeting the rapid scale needs of another viral NFT project, My Defi Pet. In addition, Cloud Kinetics also has additional support in terms of billing procedures and costs.
Titan Hunters is an MMORPG game with an incredible voxel graphic style and crypto-friendly, simple but addictive gameplay. In Titan Hunters, players can become hunters to discover Titans’ exciting and mysterious world and participate in heroic battles against epic bosses for rewards. Hunters can play solo or team up with other buddies to collect materials from fallen foes and craft new gears to subdue even deadlier Titans. As an easily accessible but high-quality NFT game, we bring our players the best and safest environment via a free-to-play and invest-to-earn model for mass adoption.
Website | Telegram Official Community | Telegram Announcement | Twitter | Facebook | Youtube| Discord
Titan Hunters is a creative and innovative game project with the combination of NFT technology (Earning nature) and the best game genre MMORPG (Fun nature).
3 
3 claps
3 
Titan Hunters is a creative and innovative game project with the combination of NFT technology (Earning nature) and the best game genre MMORPG (Fun nature).
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@devpicon/un-vistazo-a-google-cloud-platform-72bf5e84de66?source=search_post---------180,"Sign in
There are currently no responses for this story.
Be the first to respond.
Armando Picón
Jan 24, 2016·3 min read
Uno de los temas de los que he venido conversando con los chicos últimamente es la plataforma en la nube que se conoce como Google Cloud Platform. Esta plataforma te permite desplegar aplicaciones web, almacenar data y analizarla, construir el backend de aplicaciones mobiles y varias cosas. Lo bueno es que se cuenta con una plataforma escalable y robusta.
Esta plataforma se compone de varios servicios como se muestra en la siguiente gráfica:
Como se puede apreciar contamos con cuatro grandes categorías Compute, Storage, Big Data y Services, de todos estos me gustaría hacer mención de algunos servicios:
Si quieres saber quiénes emplean Google Cloud Platform puedes visitar el siguiente enlace.
Hace poco la gente de Google hizo un esfuerzo y preparó este site en Github lleno de ejemplos sobre cómo emplear las diversas APIs y tecnologías.
Finalmente, les comparto una presentación que empleé en el 2014 para dar un vistazo a lo que la plataforma ofrece:
Otros enlaces útiles:
Publicado originalmente en apiconz.blogspot.pe el 4 de Junio, 2014. Editado el 23 de Enero, 2016.
👨🏽‍💻 Android Tech Lead @CornershopChile | 💬 @GDGOpen organizer | 🎙 @CodalotDev Podcast host | 👨🏽‍ Lifelong learner Hola! Olá! Hi! 🇵🇪 🇧🇷 🇺🇸
3 
3 
3 
👨🏽‍💻 Android Tech Lead @CornershopChile | 💬 @GDGOpen organizer | 🎙 @CodalotDev Podcast host | 👨🏽‍ Lifelong learner Hola! Olá! Hi! 🇵🇪 🇧🇷 🇺🇸
"
https://medium.com/google-cloud/a-to-z-of-google-cloud-platform-a-personal-selection-q-queues-4f2220e5c80d?source=search_post---------181,"There are currently no responses for this story.
Be the first to respond.
It’s been a while since I’ve talked about App Engine and I just wanted to touch briefly on its Task queue feature .
Task queues are designed to allow your App Engine application to carry out small jobs or tasks as they are known in the background. Using Task Queues your application defines tasks, adds them to a queue, and the queue is then used to process them in aggregate.
App Engine’s standard environment provides two types of task queues Push & Pull Queues. Below are descriptions of these taken straight from the docs:
A task as defined in the docs is a unit of work to be performed by the application. Each task is an object of the Task class. Each Task object contains an endpoint (with a request handler for the task and an optional data payload that parameterises the task).
An example application where we can use a push queue could be where your application accepts sign ups for various newsletters. The data payload for this task consists of the name, email address and newsletters to be subscribed to . The webhook might live at /app_worker/register_subscriber and contain a function that adds the subscription details to Datastore . The app can create a separate task for each subscriber it receives.
The table below provides a comparison of the queue types hopefully in an easily digestible format
I’d say the size of your payload and whether the queue needs to be accessible outside of App Engine would be the first two decision points to consider when trying to decide wether a pull or push queue is the one you need.
Google Cloud community articles and blogs
3 
3 claps
3 
Written by
Chocolate addict - I have it under control really I do. I do stuff involving cloudy tech. Tweets my own so only me to blame, except for retweets.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Chocolate addict - I have it under control really I do. I do stuff involving cloudy tech. Tweets my own so only me to blame, except for retweets.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/a-to-z-of-google-cloud-platform-a-personal-selection-x-xternal-factors-3a1f7c0dded8?source=search_post---------182,"There are currently no responses for this story.
Be the first to respond.
Okay I admit I’m stretching the rules slightly but it is pronounced like this See “xternal” Okay with that excuse out of the way.
Using GCP allows you to scale and be accessible to millions of users from around the world but GCP isn’t responsible for factors outside of it’s infrastructure.
Simply put to communicate via the internet you need a source and destination address and a way to get from the source to destination.
The communication involves end user devices, the applications you build , TCP/IP (I have posted about that too here I admit I like networking 😃 ) , DNS servers, routers, firewalls, cables of various types , ethernet, wifi , 3G/4G and more it’s just fascinating ( well it is to me!).
I know I’m going on about networks again but to understand the external factors that a cloud provider isn’t responsible for a little bit about how the internet works is important to understand . I’ll keep it brief and not dense I promise.
This journey from your end user to your application is complicated. DNS is important and acts as a giant address book. So when you type into chrome for example bbc.com a DNS lookup occurs which provides the IP address of bbc.com ( think of it as a combination of house number and a postcode/ zipcode) ) . From the IP address a path from source to destination can be navigated
Users can be located anywhere in the world that has access to the internet and now it has the address of your target your requesting packet goes off on its journey to ask for a page. This means you’ve got data packets finding its way to bbc.com.
Users can literally be anywhere so that means your packets could literally be moving under water through submarine cables and when the packet hits land changing physical transport until they finally get to their target and then the return trip happens reversing the journey and so on.
Just look at this screenshot I took from the Submarine cable map
To get packets moving across these cables and indeed then moving across land there are routers think of them as signposts which have route tables. These routers use BGP to dynamically advertise routes. In very very simplistic terms the routers because they know what the next hops on the way to the destination is can tell the packet where to go next for further directions . ( I know I haven’t talked about switches but hey this is high level to get a point across!)
[an aside ]I couldn’t resist looking at RIPE’s bgplay widget. This visualises the routes that are advertised . Enter the IP address of your favourite site . Below are the routes advertised for bbc.com. Nuts right!
The internet is pretty amazing and we use it to fill the web with cute dog & cat pictures! Just saying! I don’t mind either but the fact we can send data packets in milliseconds back and forth across the world and we just take it for granted!
Anyway that diversion into a high level look at how a packet physically moves is important as once outside of GCP’s network there are a multitude of things that can affect the transport of packets I’ve barely touched the surface and don’t tempt me to lift the lid else this post will really derail :-)
The main point of my diversion about cables and bgp was to show that data is moving from a to b , it’s physically doing that and where your end user starts their journey from will have an effect on the length of time data travels around the internet. If there’s a problem with one route the packet may get routed via a less direct route . The packet will in many cases still get there just take a little longer . This time lag between the request and response is latency.
GCP has a fast high performance global network and has numerous features to help address the latency issue with more than 70 global network points of presence close to your users.
I’m just going to touch upon some of these features that you can take advantage of to help address latency and some of the things you can do to keep an eye on those external factors.
Use edge caching services to keep static data close to your users so they’re not all pulling data back from the origin You can use GCP’s Cloud CDN .
Take advantage of GCS as that is integrated into the Google front end, a powerful distributed edge routing and caching system that provides incredible performance for serving static content .
Use in memory caching systems like redis or memcached to cache things like lookup tables etc.
Think carefully about your architecture keeping services close together in the same zone and in the same region i.e keep as much traffic within GCP’s network ( It’s pretty awesome after all!)
What about patchy connectivity from your users devices ? Persist data to your end users device and then synch when connectivity is restored. Firebase Google’s mobile platform which is integrated with GCP has a realtime database that syncs data across all clients in realtime, and remains available when your app goes offline.
Make sure you’re monitoring from outside as well as inside. Inside monitoring may be giving the thumbs up that your application is working fine but if none of your users can get to it because of say a dodgy DNS entry or somewhere along the route to you something is timing out then your users are not seeing your application as working as expected even if you are!
You should also undertake due diligence anyway and use an independent third party such as cloudharmony ( Not an endorsement just the first one that popped into my head when writing this)
Google Cloud community articles and blogs
4 
4 claps
4 
Written by
Chocolate addict - I have it under control really I do. I do stuff involving cloudy tech. Tweets my own so only me to blame, except for retweets.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Chocolate addict - I have it under control really I do. I do stuff involving cloudy tech. Tweets my own so only me to blame, except for retweets.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@mluggy/yes-there-are-some-downsides-to-google-cloud-platform-in-no-particular-order-c4a6a4aeb05?source=search_post---------183,"Sign in
There are currently no responses for this story.
Be the first to respond.
Michael Lugassy
Aug 6, 2016·1 min read
Mike Carter
yes, there are some downsides to google cloud platform. in no particular order:
Co-Founder, HashUnited.com. Built 7 other companies, sold 3, worked with dozens more. Writing about startups, dev & ops.
15 
15 
15 
Co-Founder, HashUnited.com. Built 7 other companies, sold 3, worked with dozens more. Writing about startups, dev & ops.
"
https://medium.com/@blockapps/blockapps-partners-with-google-cloud-platform-to-provide-rapid-deployment-blockchain-solutions-5093e2d4b4f3?source=search_post---------184,"Sign in
There are currently no responses for this story.
Be the first to respond.
BlockApps
Jul 23, 2018·3 min read
Enterprises on the GCP can easily build full-scale blockchain networks with BlockApps STRATO.
BROOKLYN, NEW YORK — July 23, 2018: BlockApps is excited to announce its partnership with Google Cloud Platform (GCP) — offering BlockApps STRATO, a rapid-deployment Blockchain-as-a-Service solution platform for enterprises.
Google’s entrance into the blockchain space is a landmark event for the growing blockchain ecosystem and cements the continued investment in blockchain solutions for Enterprises. As GCP adoption grows, the developer-friendly BlockApps STRATO platform enables more enterprises the ability to test and implement blockchain application solutions across any business sector.
“BlockApps is delighted to integrate our platform with GCP and support GCP customers, both existing and new, on their journey to build blockchain technology solutions.” says BlockApps CEO, Kieren James-Lubin.
Blockchain technology is quickly moving from production pilots to full scale industry-wide deployments, providing wide-ranging business solutions like tracking supply chain inventory with greater accuracy, to managing and transferring assets, as well as enabling personal data control and privacy.
Until now, implementing enterprise-grade blockchain applications required a substantial investment in time and resources. Engineering teams had to spend countless hours establishing different software tooling layers before they could even begin building or scaling applications — let alone seeing their ROI.
But thanks to BlockApps STRATO enterprises can launch a blockchain node that provides a complete development and deployment experience out of the box. With the BlockApps STRATO RESTful API developers can begin building and testing their blockchain applications without manually assembling IDEs, compilers, wallets, APIs.
BlockApps STRATO is the first Blockchain-as-a-Service platform, enabling the creation of blockchain solutions for all industry verticals. BlockApps launched the category of Blockchain-as-a-Service over two years ago and continues to set the standards for enterprise blockchains.
Based on the Ethereum protocol, BlockApps STRATO provides enterprise grade API integration capabilities, configurable consensus algorithms, and the capability to query and report on blockchain data using a traditional SQL database.
Application developers will find STRATO familiar, easy to use, and easy to integrate with existing enterprise systems. This enables enterprise teams to quickly begin building blockchain applications.
Benefits of BlockApps STRATO include:
BlockApps additionally provides a robust standard for role based access control. Enterprises can grant granular permissions to specific members within consortium networks and can also easily assign application level entitlements.
BlockApps is the world’s first Blockchain-as-a-Service company, enabling flexible blockchain solutions for both startups and Fortune 500 organizations. Customers have used the BlockApps STRATO platform to build solutions in finance, insurance, supply chain, energy, healthcare, and other industries.
BlockApps is a founding board member of the Enterprise Ethereum Alliance, which is the world’s largest open source enterprise blockchain project with over 500 members including Microsoft, Intel, BNY Mellon, JPMorgan, CME Group, and Santander.
BlockApps continues to support the Enterprise Ethereum Alliance in its efforts to strengthen enterprise-grade extensions of Ethereum-based blockchain networks, which include setting the standards for scalability, security, and privacy.
Learn more about BlockApps STRATO by contacting our sales team
Originally published at blockapps.net on July 23, 2018.
Blockchain API for Ethereum & Bitcoin 2.0
See all (131)
3 
3 claps
3 
Blockchain API for Ethereum & Bitcoin 2.0
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@gmusumeci/getting-started-with-terraform-and-google-cloud-platform-gcp-deploying-a-mysql-database-f55f3bf57ad3?source=search_post---------185,"Sign in
There are currently no responses for this story.
Be the first to respond.
Guillermo Musumeci
Nov 14, 2019·6 min read
This is part 4 of the tutorial Getting Started with Terraform and Google Cloud Platform (GCP).
Part 1, deploying VMs in a public-only single region of the tutorial →…
"
https://medium.com/@gmusumeci/getting-started-with-terraform-and-google-cloud-platform-gcp-deploying-vms-in-a-private-only-f8b5ce7858d8?source=search_post---------186,"Sign in
There are currently no responses for this story.
Be the first to respond.
Guillermo Musumeci
Nov 12, 2019·9 min read
This is part 3 of the tutorial Getting Started with Terraform and Google Cloud Platform (GCP).
Part 1, deploying VMs in a public-only single region of the tutorial →…
"
https://medium.com/@timtech4u/get-started-with-google-cloud-platform-for-free-90c4059081ca?source=search_post---------187,"Sign in
There are currently no responses for this story.
Be the first to respond.
Timothy
Oct 4, 2019·3 min read
Whether your business is early in its journey or well on its way to digital transformation, Google Cloud’s robust set of solutions and technologies helps chart a path to success.
Secure your data, gain real-time insights, boost productivity, and more.
Innovative machine learning products and services on a trusted platform.
Design, secure, analyze, and scale APIs anywhere with visibility and control.
From app platform to containers to VMs, cloud compute tailored to your needs
Efficiently capture, process, and analyze data with Google Cloud data analytics products.
Fully managed, scalable database services to support all your applications today and tomorrow.
Tools and libraries to enhance developer productivity on Google Cloud Platform
Transform your IT and build apps for the future
A fully managed service to easily and securely connect, manage, and ingest data from globally dispersed devices.
Tools to help you develop, deploy and manage your cloud apps
Deliver seamless multiplayer gaming experiences to a global player base.
Migrate your IT landscape to Google Cloud on your terms
A global fiber network, connecting you to the world.
Google Cloud’s security model, world-scale infrastructure, and a unique capability to innovate will help keep your organization secure and compliant.
Simple, reliable, and secure solutions for your media, analytics, and application data.
The Google Cloud Platform Free Tier gives you free resources to learn about Google Cloud Platform (GCP) services by trying them on your own.
The Always Free program provides limited access to many common GCP resources free of charge.
Get $ 350 credit to use across the platform. This credit is only used when you exceed the free usage limits. Credit expires in 12 months.
To get $350 of GCP credit by signing up for a trial using my referral link: https://gcpsignup.page.link/k7eQ . Terms apply.
Software / DevOps Engineer | Google Developer Expert for Cloud | https://timtech4u.dev
See all (664)
12 
12 claps
12 
Software / DevOps Engineer | Google Developer Expert for Cloud | https://timtech4u.dev
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/a-to-z-of-google-cloud-platform-a-personal-selection-i-interconnect-9e2f37a77a8c?source=search_post---------188,"There are currently no responses for this story.
Be the first to respond.
Yes I know I left you hanging with my previous post but rest assured I have written up J but “i” does come next so here you have it . ( I’ve kept it a nice short one ) and not being able to leave networks alone here’s another entry for you!
Interconnect is a way to connect GCP directly with your on premises DC or potentially another cloud provider. There are three products that fit under this heading
Carrier Interconnect — This is a way to connect your network to Google’s network edge by using an interconnect service provider who will work with you to connect your network be that either on premises or already in the Data centres that some interconnect service providers operate to their (for want of a better description) exchange point which is in turn connected directly to Google’s network.
Cloud VPN -This allows you to set up an IPsec VPN between two separate networks . The networks could be two separate GCE networks in different regions ; an on premises network to GCE networks or even between GCE networks and another cloud. In all scenarios it should be noted that Compute Engine VPN only supports gateway-to-gateway scenarios. You must have a dedicated physical or virtual VPN gateway on the client side.
Direct Peering - A direct peering connection between your business network and Google’s. It allows the exchange of Internet traffic between your network and one of Google’s broad-reaching Edge network locations. It exchanges BGP routes between Google and the peering entity
The network bods amongst you will immediately understand why you have a choice and in fact you can combine Cloud VPN with carrier interconnect or Direct Peering. The reason being that they serve to address different tasks
Carrier interconnect & direct peering is all about overcoming the variances of traversing the internet with the many hops that are out of Google’s or your control by providing a greater degree of availability and better latency .
VPN is about providing secure connectivity to GCP
Okay next week it will be back to the second part of the static hosting post I promised .
Google Cloud community articles and blogs
2 
2 claps
2 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Chocolate addict - I have it under control really I do. I do stuff involving cloudy tech. Tweets my own so only me to blame, except for retweets.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/a-to-z-of-google-cloud-platform-a-personal-selection-d-is-for-deployment-manager-f40b00b53276?source=search_post---------189,"There are currently no responses for this story.
Be the first to respond.
Deployment manager allows you to create a set of declarative templates that allow you to consistently deploy Cloud platform resources.
This means you can set up identical configurations in different projects and environments . For example Test environments that accurately mirror production. You don’t need to keep the replica enviroment up all the time either as you can have confidence that you can easily reproduce an environment by using Deployment manager.
There are typically two types of files that are used with Deployment manager
A Configuration file which is a YAML file
Template files which are Jinja 2.7.3 or Python 2.7
There’s also a schema file but I’m not going to talk about that in this post you can hop over to the docs and have a look at that.
In this post I’m just going to focus on configuration and template files and hopefully leave you with an understanding as to why there are typically these two types of files when using Deployment manager .
A Configuration file is required as a minimum to be passed to deployment manager . This file defines what your deployment will look like . It defines the resources you wish to deploy and settings such as zones, machine type etc .
For a list of supported resources and associated properties you can use the table here or retrieve a list from the command line by using the gcloud command :
To deploy a configuration you pass the configuration file to deployment manager via the gcloud command or the API.
Now a big huge configuration file can get unwieldy and difficult to manage so here’s where templates come into play. Templates are a way to break down configurations into logical composable units that you can individually update and reuse.
To include these templates as part of your configuration you import them by using the imports property in the configuration file that calls the template (I’m going to refer to the configuration file that has the templates imported as the master configuration file from here on in ).
Templates are incredibly flexible in a number of ways:
Composability — means easier to manage and maintain the definitions . It also makes re-using the definitions in different deployments easy. You may not want to recreate everything included in a configuration file but may just want to maintain consistency in how you define instances to be used.
Template variables- are an easy way to reuse templates as it allows you to abstract properties by allowing you to declare the value to be passed to the template in the configuration file. This means that you can change its value for each configuration without having to update the template. For example you may wish to deploy your test instances in a different zone to your production instances so you declare within the template a variable that inherits the zone value from the master configuration file.
Environment variables — are the variables you declare that allow you to easily reuse the templates in different projects and deployments. So these are things like the Project ID or deployment name i.e not the properties of the resources you want to deploy that’s what Template variables are for.
To get the distinction between the two types of variables let’s say you have two projects that you wish to deploy instances to but you know that you will want to deploy those instances into different zones in each project. You would declare environment variables for the Project ID and Deployment name and use Template variables for the zone resource property of the instances you want to deploy .
Template modules — are pretty cool as they can be used as modules by other templates . When would you want to do this. Let’s say you want to deploy stage , test and production versions of the same deployment and ideally they are identical configurations but you want a way to identify each instance deployed by the name assigned to the instance. The easiest way to do this would be to name each resource as application-name — environment e.g stage, test or production. Thus rather than declaring lots of template variables use a helper template that will auto generate the name of the instances based on the environment ( Now this is becoming an overloaded term — sorry !) . the docs has a good example of using template modules for this type of scenario.
If you’ve made it this far you’re probably wondering why both Jinja and python. It’s a choice thing really. Jinja maps easily to YAML so if you just want composability and all that gives you it’s not a huge leap to start using Jinja with configuration files which are written in YAML anyway.
Using python gives you the ability to programmatically create template files. You can mix and match jinja and python templates so if you start off with jinja files and then decide you need to programmatically define some templates and want to use the same master configuration file you can.
And yes I know anyone who follows me on twitter probably knows my intransigence about anything that makes you care about indentation and whitespace so by inference I am really not a fan of YAML but that aside Deployment manager does address the infrastructure as code thing I am a huge believer in so that overrode my objections. I still dislike YAML though but I do like Deployment manager!
Google Cloud community articles and blogs
2 
2 claps
2 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Chocolate addict - I have it under control really I do. I do stuff involving cloudy tech. Tweets my own so only me to blame, except for retweets.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/a-to-z-of-google-cloud-platform-a-personal-selection-s-streaming-740428b77f9d?source=search_post---------190,"There are currently no responses for this story.
Be the first to respond.
I’m going to be talking mostly about streaming transfers and by that I mean I’m not referring to the processing of unbounded datasets but the process of feeding data as a continuous flow of data from a source to a sink .
Streaming transfers are useful if you have a process that generates data and you do not want to buffer it locally before uploading it or if you want to send the result from a computational pipeline directly into Google Cloud Storage.
GCP has a number of products that allow you to stream data directly to them i.e to accept a continual flow of data
Pub/Sub — is the work horse of GCP and is flexible enough that it can receive data from a variety of sources such as telemetrics, gaming data etc and then the data can be removed from pub/sub and passed to a sink such as BigQuery, cloud storage , BigTable etc . It’s the plumbing that is a critical part . I spoke about Pub/Sub here so I won’t spend any more time on it in this post.
Cloud Storage — supports streaming transfers with the gsutil tool or boto library, based on HTTP chunked transfer encoding. Streaming data lets you stream data to and from your Cloud Storage account as soon as it becomes available without requiring that the data be first saved to a separate file. Many scenarios require data be eventually stored on Cloud Storage for example if you want to send the result from a computational pipeline directly into Google Cloud Storage then streaming transfers are the way to do this.
The docs examples are great on this so using the examples from there to illustrate using gsutil or boto.
Using gsutil to stream data to your pipeline use the gsutil cp command and replace the file to be copied with a dash. The following example shows a process called collect_measurements whose output is being transferred to a Google Cloud Storage object named data_measurements:
Using boto to stream data you use the following command:
So an example usage could look like :
This performs a streaming upload of a file named data_file to an object with the same name:
Although this example demonstrates opening a file to get an input stream, you could use any stream object in place of my_data above.
You can also do streaming downloads where Cloud storage is the source
Big Query — You can Stream your data into BigQuery one record at a time by using the tabledata().insertAll() method. This approach enables querying data without the delay of running a load job.
Using Python here’s an example snippet of code taken from the docs that illustrate using this:
Note the following limits apply for streaming data into BigQuery. ( Taken straight from the docs)
I know I said I was mostly going to talk about streaming transfers but those of you who have been reading this series know I’ve felt a bit guilty about not really spending any time on Dataflow ( I gave it a quick hello at the end of my Pub/Sub post ) so I’m just going to add a piece here on streaming processing . What do we mean by that ? we are discussing a data processing engine that is designed with infinite data sets ( unbound) in mind. Tyler from the Dataflow/ Apache Beam team discusses this concept nicely in this great 101 .
Dataflow needs to be used in conjunction with pub/sub to accept streams of data from devices .
In your dataflow code you specify a pub/sub subscription and read from the topic using the PubsubIO.Read transform
The PubsubIO.Read transform continuously reads from a Pub/Sub stream and returns an unbounded PCollection of Strings that represent the data from the stream
It’s only a few lines of code to set that up here’s an example from the docs :
I was impressed at how easy it is to set up .
Google Cloud community articles and blogs
2 
2 claps
2 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Chocolate addict - I have it under control really I do. I do stuff involving cloudy tech. Tweets my own so only me to blame, except for retweets.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/a-to-z-of-google-cloud-platform-a-personal-selection-k-is-for-keys-10bc0cdf7e47?source=search_post---------191,"There are currently no responses for this story.
Be the first to respond.
For K I wanted to touch upon some key related topics .
When working with GCP you are inevitably going to be using API keys , SSH keys for logging into stuff and maybe encryption keys for encrypting data at rest. So I’ll chat a little about these here ( I’m only scratching the surface though! )
By default Compute Engine encrypts all data at rest and manages this encryption for you without any additional actions on your part. There are various reasons that I won’t go into here where you may want to use your own encryption keys.
Key management though is hard and you need to ensure that you have processes in place to manage things like:
I’m not going to discuss any of those points though as that really is out of scope and there are tons of articles out there discussing these points and the solutions to address them .
With regards Key management systems I just wanted to let you know that assuming you have a fit for purpose key management system in place you can actually bring your own keys (BYOK) and use them with GCP. Read here for details on how to use your own keys with GCE or here for details on doing the same with GCS.
Now seems a natural place to touch upon SSH and how you can add and remove additional SSH keys to GCE instances looking at two different methods ( I know I know!) .
SSH
By default project owners and editors can SSH into GCE instances but You may want to use additional SSH keys on the instance for other users or applications . You can do this with GCP and it’s documented here .
A couple of points I want to draw attention to when doing this:.
You can also add individual user accounts to your projects and the process of creating and managing keys is abstracted if you use
To create the user account then use the following command
This will automatically generate a public/private key pair for the account and allow you to login.
( Note you can also accomplish what the above gcloud commands do by using API calls or the console)
Note this is different from the earlier scenario I talked about as in this scenario it uses user accounts to log into the instance, while the previous scenario uses the gcloud compute ssh command which uses metadata SSH keys, ignoring any user accounts.
You can easily remove the accounts and revoke SSH access by using the commands:
To remove a user
and
To revoke all public keys for an account
So when would you use one over the other well I would say the metadata approach is great for scenarios where you want to use the SSH keys for every instance in that project whereas the second is definitely a great approach for user access to a smaller set of instances in the project.
It made sense to me anyway at this point to mention the GCP command line tool gcloud . This allows you to manage your GCP resources from the command line .
It uses oauth 2.0 ( There are some nice diagrams in that link that do a far better job than I ever could of explaining how oauth 2.0 works ) . The actual flow is described nicely here . If you wondered where the credentials created by the oauth flow can be found they’re in ~/.config/gcloud
API Credentials
Service accounts are special accounts that can be scoped down to restrict access down to the GCP resources you need. Crucially the key management for these accounts can be managed by GCP where GCP manages key rotation and is the approach you should take when your application is running on GCE and GAE ( It works seamlessly )
If however you need to access the GCP resources from on premises or another cloud for example then you need to create and download the keys .
The private key can be downloaded in JSON and PKCS12 format . Associated with the key is some additional information namely an email address and a client ID. If you look in the console you will see the identifiers associated with the generated key:
The usual caveats re key management apply
I don’t want to do an injustice to service accounts but I would say take a little time out to read and understand about using service accounts for authorising requests to GCP API’s.
For scenarios where using your own keys to encrypt & decrypt data is required a combination of service accounts and bring your own keys are probably the way to do this. Service accounts to communicate with GCP resources and using you own keys to encrypt and decrypt data.
I’d like to say thanks to my colleague Vic @vicnastea who kindly reviewed this for me . Mightily impressed he didn’t complain about my avoidance of the letter “z” either 😃
Google Cloud community articles and blogs
2 
2 claps
2 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Chocolate addict - I have it under control really I do. I do stuff involving cloudy tech. Tweets my own so only me to blame, except for retweets.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/@yanwei-liu/start-your-first-ml-dl-ds-project-with-google-cloud-platform-8a7a58965b08?source=search_post---------192,"Sign in
There are currently no responses for this story.
Be the first to respond.
Yanwei Liu
Feb 9, 2020·1 min read
I read a awesome article today. The article includes 6+1 steps on how to create a Jupyter Notebook environment on GCE.
towardsdatascience.com
Machine Learning | Deep Learning | https://linktr.ee/yanwei
25 

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
25 claps
25 
Machine Learning | Deep Learning | https://linktr.ee/yanwei
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/a-to-z-of-google-cloud-platform-a-personal-selection-l-is-for-logging-230d459976f4?source=search_post---------193,"There are currently no responses for this story.
Be the first to respond.
Collecting and analysing Logs is a thing done for numerous reasons typically for things like
Although monitoring is closely related to logging and often go hand in hand I’ll talk about monitoring in the next post so you can think of this as part one of a two parter.
Hopefully you collect logs because you’re going to do something with them and if so then you’ll be happy to know that GCP has ways to help you with both collecting logs and analysing them for whatever reason you’re collecting them for . If you’re not sure why you’re collecting them then just use nearline to store them until you do.
Collecting & storing application logs
Log collection requires a way of getting the logs from where they are generated and moving them somewhere else
If you are not running your applications on GCP then you can easily enough stream your logs to GCS or Big Query ,or pub/sub so most of this post is still applicable particularly the last bit on analysing & processing logs . I’ll be focusing mostly on assuming your application is running on GCP though so make the mental adjustments you need to . This is a series on GCP so I don’t want to stray too far outside the lines as some of the posts here have been very long as it is ( I’m looking at you J!)
GCP has a cloud logging service that collects and stores logs from applications and services.
It supports out of the box when running applications on GCE a set of common applications . If you are using Compute engine then install the logging agent . This agent does the heavy lifting of streaming the logs from common third-party applications to Cloud Logging.
For writing log entries to the Cloud logging service from your own applications GCP provides a logging API . You can also use the API to export your logs to other services.
As well as collecting logs from applications GCP also collects activity logs. These track certain events that affect your project, such as as API calls and system events.
The cloud logging service stores data for 30 days so for longer term storage you can use the Log export service to export to GCS, Big Query and via pubsub you can stream to other services like Dataflow or Dataproc.
Logs & auditing
An important reason for collecting log files is to provide an audit trail . GCP provides Audit logs that are actually two log streams that capture Admin Activity and Data Access, which are generated by GCP services . As with the other log types these can be exported for longer term storage.
You will probably want to restrict who has access rights to those logs . This can be achieved by using IAM roles to apply access controls to logs
Analysing & processing logs
So you have your logs which you’ve exported. If you’ve exported directly to BigQuery you’re all set to get started asking questions of your logs by starting off with a few queries . This is so simple and yet so powerful!
You can export to Cloud storage using the Log export service but really you’re collecting logs to do something with them so unless you need to do some ETL type processing on them that maybe it makes sense to keep the logs on Cloud storage exporting directly to BigQuery probably makes sense .
But for a more complicated process using Dataflow or Dataproc may be what’s needed. If your app runs on GCP then you can export the logs to GCS and from there process using Dataflow or Dataproc. GCP have a great walkthrough for using Dataflow in this scenario and I have nothing of value to add to that as it’s a pretty comprehensive walkthrough .
Maybe you want to stream logs directly to Dataflow in that case you can export logs to pub/sub using the export service or just stream to pub/sub from your source applications if running on premises or on another cloud for example. You can then transfer the data from pub/sub to your target such as Dataflow .
Logs and monitoring
As I mentioned earlier applications typically have associated logs files . These logs will record various application events . You can keep track of messages of the same type/ error by counting them Then by using these logs based metrics you can take advantage of GCP’s fully managed monitoring solution Cloud monitoring (which I will talk about in the next post in this series ) to include the log based metrics collated from the various logs your applications emit to create charts and alerting policies
Visualising Log data
You may also want to visualise some of this and guess what GCP makes it easy to do this too. You can use Datalab to analyse data stored in BigQuery and Cloud storage. If you have exported your logs to either of those then you can just use Datalab to enhance your queries with some visualisation. Datalab is built on Jupyter notebooks and there is a large ecosystem to help get you started
In next weeks entry in this series I’ll be building upon this to discuss monitoring .
Google Cloud community articles and blogs
2 
2 claps
2 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Chocolate addict - I have it under control really I do. I do stuff involving cloudy tech. Tweets my own so only me to blame, except for retweets.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/techmagic/5-advantages-of-google-cloud-platform-solutions-for-startups-62957d75a2c7?source=search_post---------194,"There are currently no responses for this story.
Be the first to respond.
Startups are incredibly active in adopting Cloud technology. The reason is apparent: small businesses can’t or don’t want to invest in their in-house servers — they are too expensive and difficult to maintain. Instead, startups prefer using a ready-to-go Cloud infrastructure, owned by technological giants. We prepared a guide to Google Cloud for startups, its features, and benefits.
The most popular Cloud computing providers are Amazon, Microsoft, and Google. All three have special programs for startups, growth initiatives, and discounts. However, Google Cloud stands out among the others — the company even launched a separate offer for startups and offered one of the best pricing tiers in the game. Let’s take a look at what Google Cloud for startups has to offer.
Google Cloud platform offers more than 90 products, all of which can be broken down in 7 main categories. So, what is Google Cloud for startups?
In a nutshell, the Google Cloud for startups program offers entrepreneurs to get services worth $100,000 for free, along with 24/7 support. After the trial is done, the Google Cloud price for startups will be lower than in the general tier. Google Cloud Platform bills users for the number of services that they used, so the startup is the one to decide how long this trial version will last. All 90 instruments available in the full version are accessible by the rules of the program.
Mito.aiA Norwegian startup used Google Cloud platform to host its data processing software. The startup offers a series of products for social listening, which is why their platforms deal with terabytes of unstructured data content on a daily basis.
The company used Google Cloud to deploy their social listening software and manage data. As a result, the company saved its costs, increased throughput by three times, and now is predicted to reach the market worth of more than $8 million in 2022.
WeVideoThe team behind a Cloud-based platform for video creation decided to switch to Google Cloud to organize media data and run big data algorithms, enabling a higher level of personalization. The company used BigQuery to collect user data, G Suite for internal communication and management, and Firebase for fast software deployment.
As a result, the team achieved a 30% reduction of data processing costs and used Google Cloud to power marketing activities which led to a 2% increase in the conversion rate. Now the company has a 50-person team that collaborates via Google Suite and uses Google Cloud tools for their development needs.
TorchAn AR platform for developers that build immersive 3D applications for desktop, smartphones, tablets, and headsets. Currently, the platform supports more than one billion devices, which posed challenges in data processing and storage.
The team used Google Cloud to build and deploy new apps, set up seamless dataflow, and host a new UI technology. With Firebase, the team’s developers can quickly write and deploy code, while WebGL is responsible for 3D rendering. The company was able to release a new product and Ui elements with Google Cloud tools and continue using these instruments for further development and internal operations.
Google Cloud is quickly getting startups’ interest. Even though AWS still is the most popular Cloud infrastructure, the attention to Google’s services is growing fast. For startups, Google Cloud is often considered to be the best Cloud infrastructure out there, even in comparison with a more popular AWS — and here’s why.
Let’s take a look at the key characteristics of both platforms to determine which one is best for startups. This hugely depends on the structure and purpose of your software, but we’ll strive to create an objective bigger picture.
Computing power
Google Compute Engine based its virtual machines on KVM to create and support VMs. The engine supports Linux and Windows. Users can choose a different number and type of VMs depending on the project needs. Google Engine can handle up to 160 CPUs and 3,844 GB of RAM.
AWS uses Xen to run and support its virtual machines. Similarly to Google Cloud, their VMs can be based on Linux or Windows. Technical characteristics, however, are slightly different. AWS allows working with 128 CPUs and 3,904 GB of memory.
Overall, Google Cloud is more powerful but also, it offers more customization options. You can change the type and number of your VMs anytime, whereas AWS doesn’t allow as many editing opportunities for existing instances.
Price
The cost of a Cloud platform depends on the price of individual services and the amount and conditions of discounts. In both parameters, Google is a clear winner. Its storage is cheaper than Amazon’s S3, while it’s still faster in data retrieval. Flexible settings for changing a CPU’s size and number allows using exactly as much computing power as you need.
For startups, Google’s program is a good head-start, but even after your trial is over, you can apply for long-term discounts, provided that you use a platform for more than a year.
Scalability
Google is more flexible when it comes to adding and editing VMs, which allows it to fit a dynamic project needs better. With more than 90 tools, it covers all the technological needs of a software development project, from fundamental development tools to advanced instruments for AI, ML, and big data.
AWS doesn’t offer as many customization opportunities, but it’s at the forefront of enabling AI and IoT computing. In terms of innovation, it actually might be further than Google right now.
So, in terms of scalability, Google and AWS are equal: while Google wins with flexibility, AWS offers a better-tuned environment for innovative services.
Storage capacity
Google Cloud platform offers more virtual machines — the company upped its amount of RAM from 1.4 TB to 3.75 TB. Google Cloud storage for startups is the same as in the paid version. AWS offers more RAM but fewer CPUs. Technically, it’s a tie, but in reality, Google is slightly more powerful. Then again, it depends on your project’s needs and functionality requirements.
Now that we’ve taken a look at Google’s functions and even compared it to the leading competitor, Amazon Web Services, let’s see what benefits the Google Cloud startup program offers.
If you are thinking about creating your startup in Cloud from scratch or switching ready tech solutions, Google Cloud, with its cost-efficient Startup program and more than 90 tools, can be the right choice.
TechMagic experts help clients to migrate to Google Cloud and get started with Google Startup Programs. We will show you our previous work and walk you through the process, step by step. Contact our team to get started with Google Cloud development.
All about JavaScript, AWS, and Serverless in one place.
2 
2 claps
2 
Written by
TechMagic is an AWS Consulting Partner with a narrow technological focus on JavaScript, Serverless, Salesforce, and Native Mobile. https://www.techmagic.co/
TechMagic is a tech consulting company focused on JavaScript, AWS, and Serverless.
Written by
TechMagic is an AWS Consulting Partner with a narrow technological focus on JavaScript, Serverless, Salesforce, and Native Mobile. https://www.techmagic.co/
TechMagic is a tech consulting company focused on JavaScript, AWS, and Serverless.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@john-tucker/google-cloud-platform-and-api-call-attribution-b0ccdf7a0305?source=search_post---------195,"Sign in
There are currently no responses for this story.
Be the first to respond.
John Tucker
Jun 28, 2021·8 min read
When using multiple Projects, which Project an API call is attributed to is more complicated than one might think.
When using Google Cloud Platform (GCP), one often thinks about resources, e.g, storage buckets, instances, etc.; less apparent are the API calls made to access them. Even less apparent is that the frequency of these API calls is limited using Rate Quotas.
Rate quotas are typically used for limiting the number of requests you can make to an API or service. Rate quotas reset after a time interval that is specific to the service — for example, the number of API requests per day.
— Google Cloud — Working with Quotas
One might next think, how are these Rate Quotas enforced?
Quotas are enforced on a per-project basis, with the exception of the number of projects that you can create, which is enforced per user account and billing account.
— Google Cloud — Working with Quotas
When using multiple Projects, which Project an API call is attributed to is more complicated than one might think.
TL;DR:
In each of scenarios, we made API calls using both the gcloud command-line tool (CLI) and the Client Libraries. In each case, we performed two sample operations:
To allow the member (User or Service Account) to perform the operations, we granted it two Roles in each Project used.
The specific gcloud commands used are:
The Client Libraries are used in a sample Go application.
We then validated which Project an API call is attributed using Cloud Monitoring Metrics Explorer; using the following query:
Resource Type: consumed_apiMetric: serviceruntime.googleapis.com/api/request_count
The Project with data is the Project the API calls were attributed to.
In this simplest scenario, we started by authenticating the User and selecting the Project using the gcloud command-line tool:
Using the gcloud commands, we observed that the API calls were successful and attributed to the Project. At this point, it is not clear if the API calls are attributed to the Project the User is logged into or the Project owning the resource (because there is only one Project).
Authenticating the User to use the Client Libraries requires uses a separate gcloud command that does not involve selecting a Project:
When we ran the command before running gcloud init, we observed the following warning; running the command after results in a different but similar warning.
Ignoring the warning (will address later), we ran the sample Go application and observed that the API calls were successful and attributed to the Project. Here it is clear that the API calls are attributed to the Project owning the resource.
As before, we authenticated the User and selected the Project. This time, however, we added the following option with a different Project to the commands:
Using the gcloud commands, we observed that the API calls were successful and attributed to the Project owning the resource.
The conclusion we can draw from this and the two previous scenarios is that when a User is accessing resources, the Project owning the resource is attributed the API calls.
Building off the previous example where we added the project option, we next added the following option to control the billing (aka. quota) Project; the Project that API calls are attributed to:
A critical prerequisite in making this work is to grant the User the Service Usage Consumer Role in the billing Project.
Ability to inspect service states and operations, and consume quota and billing for a consumer project.
— Google Cloud — Understanding Roles
Using the gcloud commands, we then observed that the API calls were successful but their attribution varied between the two operations.
Before drawing conclusions, we repeated this scenario but with the Client Libraries.
After authenticating to use the Client Libraries, we also set the billing (quota) Project using the following command; essentially what the earlier warning directed us to do:
We ran the sample Go application and observed that the API calls were successful but their attribution varied between the two operations (same as the previous scenario).
The conclusion we can draw from this and the previous scenario is that when a User is accessing resources, for some API calls the Project owning the resource is always attributed the API calls; for others, the API calls can be configured to be attributed to a billing (quota) Project.
Having explored all the permutations of a User accessing resources, we switched to exploring scenarios involving a Service Account. In this scenario, we started by authenticating the Service Account using the gcloud command-line tool (requires creating and downloading a Service Account Key file).
As this command does not automatically select a Project, we added the following option with a same Project to the commands:
Using the gcloud commands, we observed that the API calls were successful and attributed to the Project. As when using a User, it is not clear here if the API calls are attributed to the Project owning the Service Account or the Project owning the resource (because there is only one Project).
Authenticating the User to use the Client Libraries required setting an environment variable to the downloaded Service Account Key file.
We ran the sample Go application and observed that the API calls were successful and attributed to the Project. As in the previous scenario, it is not clear here if the API calls are attributed to the Project owning the Service Account or the Project owning the resource (because there is only one Project).
As before, we added the following option with a different Project to the commands:
Using the gcloud commands, we then observed that the API calls were successful but their attribution varied between the two operations.
Before drawing conclusions, we repeated this scenario but with the Client Libraries.
We ran the sample Go application and observed that the API calls were successful but their attribution varied between the two operations (same as the previous scenario).
The conclusion we can draw from this and the previous scenario is that when a Service Account is accessing resources, for some API calls the Project owning the resource is attributed the API calls; for others, the API calls are attributed to the Project owning the Service Account.
As in an earlier scenario, we next added the following option to control the billing (aka. quota) Project; the Project that API calls are attributed to:
Also, a critical prerequisite in making this work is to grant the Service Account the Service Usage Consumer Role in the billing Project.
Using the gcloud commands, we then observed that the API calls were successful but their attribution varied between the two operations.
Before drawing conclusions, we repeated this scenario but with the Client Libraries.
Previously, we used a gcloud command to authenticate the User to the Client Libraries and used another gcloud command to set the billing (quota) Project. In the case of a Service Account, we instead set the GOOGLE_APPLICATION_CREDENTIALS environment variable to authenticate it; as such we needed another mechanism to set the billing (quota) Project.
In this scenario, we updated (uncommented the code) the Go application to set the billing (quota) Project; the key function is WithQuotaProject:
func WithQuotaProject(quotaProject string) ClientOptionWithQuotaProject returns a ClientOption that specifies the project used for quota and billing purposes.
— Google Cloud — option
We ran the sample Go application and observed that the API calls were successful but their attribution varied between the two operations (same as the previous scenario).
The conclusion we can draw from this and the previous scenario is that when a Service Account is accessing resources, for some API calls the Project owning the resource is always attributed the API calls; for others, the API calls can be configured to be attributed to a billing (quota) Project.
Weaving all these conclusions together, we end up with:
Broad infrastructure, development, and soft-skill background
See all (26)
6 
6 claps
6 
Broad infrastructure, development, and soft-skill background
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/containers-101/deploying-your-applications-using-codefresh-google-cloud-platform-and-google-kubernetes-engine-5c1576a245fc?source=search_post---------196,"There are currently no responses for this story.
Be the first to respond.
Kubernetes offers scalability and reliability for your container-based applications. Combine this with GKE, and you now eliminate the need to install or operate clusters on your own. A huge plus to using GKE is that you now will be running Kubernetes in a GCP environment, therefore you can utilize all the handy integrations Google has to offer.
Codefresh simplifies the process even more, by automating the process of getting your code built, tested, and deployed.
Let’s take a look at how easy it is to deploy your application to Kubernetes using Codefresh, GCP and GKE.
Note that if you sign-up using this tutorial, you will receive $500 in GCP credits!
In the GCP Console, navigate to the Compute section in the left-hand panel, and select Kubernetes Engine. Click on Create Cluster.
You can also use the Google Cloud CLI to create a cluster, if you are not using GCP’s UI.
Login to Codefresh. On the left-hand panel, select Account Settings > Integrations. Under Kubernetes, select Configure.
From the Add Provider dropdown, select Google Cloud Platform.
Click on Authenticate and sign in using the Gmail account linked to GCP.
Expand the GCP row and click the Add Cluster button. Select your Project ID and Cluster from the dropdown menus.
After you have added your cluster, you will be able to see all of your clusters, their names, IPs, and statuses once you expand the Show Cluster Nodes link.
Integrating your GKE cluster with Codefresh was as simple as that! Now, we need to deploy. Codefresh offers several ways of deploying your Docker image to your Kubernetes cluster:
In addition, you can view your deployment environments using Codefresh’s Environment Dashboard, by navigating to the DevOps Insights section in the left-hand panel, and selecting Environments. From here, you can see an overview of your Helm and Kubernetes deployments, monitoring your clusters and pipelines.
And there you have it! Now, you can see how easy it is to integrate GCP/GKE with Codefresh and you are ready to utilize the three within your pipelines. Check out this quick start guide on more information on how to deploy to Kubernetes.
New to Codefresh? Create Your Free Account Today!
Anna Baker is a Software Engineer/Technical Writer. She previously worked at Red Hat and is passionate about the open source community. In her free time, she enjoys drawing and cooking dishes from all over the world.
Anything and everything container related.
2 
2 claps
2 
Written by
Codefresh is a next-generation enterprise software delivery platform for cloud-native applications.
Anything and everything container related.
Written by
Codefresh is a next-generation enterprise software delivery platform for cloud-native applications.
Anything and everything container related.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@pingcap/how-to-deploy-tidb-on-google-cloud-platform-part-1-2e1ee2c1d0db?source=search_post---------197,"Sign in
There are currently no responses for this story.
Be the first to respond.
PingCAP
Sep 17, 2021·8 min read
Author: Mike Barlow (Solution Architect at PingCAP)
Editors: Tina Yang, Tom Dewan, Caitin Chen
This two-part blog series will show you how to get a simple TiDB cluster up and running on Google Cloud Platform (GCP). The goal is to let you quickly set up TiDB and learn about its benefits. The entire process should take you about 30 minutes. In this first part, we’ll prepare the GCP environment. In the second part, we’ll set up and run TiDB on GCP.
By the end of this series your architecture will look similar to the following:
Before you try the steps in this article, make sure you have:
Preparing your environment
The following diagram shows prerequisite operations. Each row indicates where you perform the corresponding task: either on your Local computer (or laptop) or on a GCP instance, which is a virtual machine instance that we will create in GCP.
You should have Google gcloud installed and configured on your local computer. The gcloud configuration should look like this:
Notice that I’m using a user account; not a service account. Usually a service account ends with gserviceaccount.com.
In the following example, I have the compute configuration set for region and zone. Setting this information is optional. In this tutorial, when we use gcloud commands we will specify where the resource is located.
Let’s create a GCP instance where we will run TiDB. Enter the commands below. This may take a few minutes to complete.
As a sanity check, you can run the following command to validate that the instance is up and running:
If you get an error and cannot create an instance, most likely you do not have valid permissions with the GCP account that gcloud is associated with.
Here we will use the gcloud ssh feature to access our newly-created GCP instance:
The server we connect to via SSH has a prompt that references the tidb-vm instance. (See the red arrow.)
gcloud account
We should now be logged into the GCP instance tidb-vm.
To validate our level of access, we can display a list of our GCP instances:
Sometimes you may get an error (as I did below). This was because my account associated with gcloud is a service account that does not have permissions to get a list of instances. A GCP service account is automatically added to your project and used by an application or virtual machine (VM) instance.
Even though you may not get an error, I recommend following the instructions below to change from a service account to your user account.
To check what account we are running, run the following command:
Take note of the account’s email address and domain. If it ends in compute@developer.gserviceaccount.com, you are running as a service account. For this exercise, we want to execute gcloud commands with our user account.
Changing from a service account to a user account
To change from a service account to a user account:
To start the process, run the following command:
You will see a long URL (see the image below). Copy this URL and paste it in your browser address bar.
In the browser, select the Google Account that has the permissions you need. In some cases, this may be the same account you used when you created the GCP instance.
Login with your GCP user account
To log in to your GCP user account, do the following:
2. A screen displayed asking you if the Google Cloud SDK can access your account. Click Allow.
3. Click the copy button (see arrow below).
4. Paste the verification code from the browser to the gcloud command line.
Our user account should now be associated with gcloud.
5. As a sanity check, let’s confirm the account we are running with the following command:
If you see that your account is active, then your gcloud is now running under a user account. Let’s try again to get a list of machines:
While this doesn’t prove that we have all the access we will need, it is a good baseline.
Think of TiUP as a package manager that makes it easier to manage different cluster components in the TiDB ecosystem. We will install TiUP in Part 2, but here we will go ahead and configure SSH for TiUP.
TiUP uses SSH, so first we will need to configure SSH.
Let’s see what’s in the .ssh directory:
There are no actual keys in this directory. We can use gcloud to create the SSH keys that TiUP will need.
When you run the following command, you will be prompted to enter a passphrase. For simplicity, do not enter a passphrase. Just hit Return to provide an empty passphrase.
As a sanity check, let’s see what files were created:
Note that we now have three new files. The primary file we are interested in is the google_compute_engine, which is the private key that TiUP will use for SSH.
You will want to access TiDB resources (TiDB Dashboard, Grafana, and Prometheus) from a browser on your local computer. To do this, you will need to open ports on the GCP network by creating a GCP firewall rule.
In the steps below, we’ll create a firewall rule that allows only our local computer to access TiDB resources.
Let’s get our local computer public IP Address. Here’s a link that provides different ways of getting your local IP.
We want our local computer IP address, NOT the GCP instance IP address. Therefore, run the following command on your local computer. For me, I’m running the following command on my MacBook®. Note in the image below that the command prompt does not include tidb-vm.
Notice the blurred-out IP address above. This is the IP address used in the following steps. Your IP address will be different and unique.
Let’s go back to our GCP instance and look at the current firewall rules:
By default, GCP already has some firewall rules. We will create a new firewall rule because all the ports on a cloud instance are closed by default, and we need to tell the firewall which ports should be open. This way, we only open the necessary ports to retain security and prevent cyber-attacks.
We’ll make the following TiDB ports available to our local computer:
You can see a list of all the TiDB ports here.
Let’s create a firewall rule with our local IP Address.
In the code example below, for the parameter --source-ranges, replace <Your Local IP> with your own local IP Address; however, keep the /32. This will limit access from your specific IP address.
I added port 2379 to the above command. This port is different from the one shown below.
In Part 2 of our series, we will use these ports to access TiDB from our local computer.
TiDB is fully compatible with the MySQL 5.7 protocol and the common features and syntax of MySQL 5.7.
In Part 2, we will install TiDB and access it with a MySQL client. Here, we will first install a MySQL client using apt-get:
You should now have a GCP instance set up where you’ll be able to install and run a simple TiDB environment. Check out Part 2 where we install and run TiDB!
Originally published at www.pingcap.com on Aug 2, 2021
PingCAP is the team behind TiDB, an open-source MySQL compatible NewSQL database. Official website: https://pingcap.com/ GitHub: https://github.com/pingcap
See all (22)
37 
37 claps
37 
PingCAP is the team behind TiDB, an open-source MySQL compatible NewSQL database. Official website: https://pingcap.com/ GitHub: https://github.com/pingcap
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@timtech4u/writing-google-cloud-platform-articles-on-https-fullstackgcp-com-29fe504e82f8?source=search_post---------198,"Sign in
There are currently no responses for this story.
Be the first to respond.
Timothy
Jun 26, 2019·1 min read
…
Here’s the list of articles I’ve published so far on https://fullstackgcp.com
Software / DevOps Engineer | Google Developer Expert for Cloud | https://timtech4u.dev
9 
9 
9 
Software / DevOps Engineer | Google Developer Expert for Cloud | https://timtech4u.dev
"
https://medium.com/@synabreu/how-to-get-all-labs-and-demos-for-google-cloud-platform-on-coursera-c3460e2077b7?source=search_post---------199,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jinho Seo
Jun 25, 2019·5 min read
In this year, Google is marching on Enterprise industry bravely by delivering powerful AI’s platform, TensorFlow, to their familiar customers and developer communities.
Google Cloud acquired Qwiklabs in order to train their cloud platform and framework, as well as to certify outstanding skills passing through exams like Professional Cloud Architect, Professional Data Engineer, and Professional Cloud Developer.
Therefore, they produced numerous Google Cloud platform developers courses with Coursera. But, one day, I wondered, “how can I get all labs and demos for Google Cloud Platform on Coursera in one place?”
Prerequisite
Check up Coursera Courses
All right! You can show the development codes of the following Coursera’s courses. These repositories include in github. Please click on the step sentences at the below links!
Step by step
To collect them in one place so, you have to log in the google cloud console first. Then, you create this project to use codes of labs and demos. In my case, I created a project name as “Training-ML-TensorFlow” and click on the create button.
Second, you must click API & Services on Navigation menu and then select Cloud Source Repositories API to manage all codes of demos and labs in your project.
Third, you have to activate cloud shell on right side of menu. After a few minute, you will see cloud shell on the bottom of window.
To make your datalab virtual machine, you check up your zones. I write it up and then enter it as below.
gcloud compute zone list
Forth, you will see the all list of regions and zones on google cloud. So you have to select one of them. In my case, I selected us-east1-c like [Feature 4].
To do so, I typed below and then enter it. Internally, Google Cloud makes mydatalabvm on the compute Engine by locating on us-east1-c.
datalab create mydatalabvm –zone us-east1-c
Fifth, you have to change preview port from 8080 to 8081 in the cloud shell menu by clicking change port menu of web preview icon like [Feature 5].
After then, Sixth, you will see folders on Google Cloud Datalab below[Feature 6]. In cloud Datalab home page on web browser, navigate into “notebooks” and add a new notebook using the icon notebook on the top left.
Seventh, rename this notebook as ‘repocheckout’. It’s just temporary.
Eighth, enter the following commands in the cell in the new notebook, and click on Run (on the top navigation bar) to run the commands below:
%bash
git clone https://github.com/GoogleCloudPlatform/training-data-analyst
rm -rf training-data-analyst/.git
After cloning the source into your datalabvm successfully, you have to confirm that you have cloned the repo back to Datalab browser. So you ensure that you see the training-data-analyst directory [Feature 9].
Finally, click on the Home icon, and then navigate to notebooks/training-data-analyst/courses/machine_learning/deepdive/08_image/labs.
Now you are ready to practice a hands-on lab on datalab. Ah, wait a minute! I forgot one thing. If the cloud shell used for running the datalab command is closed or interrupted, the connection to your Cloud Datalab VM will terminate.
If that happens, you may be able to reconnected using the command below.
datalab connect mydatalabvm
Once connected, try above step again. So far so good? I hope this guide will help you to study Google Cloud on Coursera by hands-on coding.
Technology Enthusiast, Evangelist, Cloud Architect and Advisor
See all (641)
3 
3 claps
3 
Technology Enthusiast, Evangelist, Cloud Architect and Advisor
About
Write
Help
Legal
Get the Medium app
"
https://kobkrit.com/shrink-disk-in-google-cloud-platform-on-ubuntu-with-the-smallest-effort-possible-e25efe31f68d?source=search_post---------200,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Like everyone else, when you creating a disk for an instance, we usually allocate the size of disk much much higher than we actually need. We have a very pessimistic view on a disk space we need, and finally, we end up wasting money on unnecessary matters.
I created an instance on GCP, aimed for running several docker containers. I create an extra 500GB drive located in /dev/sdb (to be mounted on /var/lib/docker) attached to my instance, but actually, an only 60GB drive is needed. The following steps are for shrinking a disk for the unmountable partition.
2. Down your docker in Ubuntu, ``$ sudo service docker stop``
3. Umount the old disk, ``$ sudo umount /dev/sdb``
4. Resize it, ``$ sudo resize2fs /dev/sdb 60G`` You need to wait a while.
5. Edit its partition table, ``$ sudo cfdisk /dev/sdb`` will give you a text-based gui to inspect your partition table. I would recommend you to print the partition table to a file or screen at that point, and take note of the current configuration as backup. You can then select /dev/sdb and delete the partition. In its place, free space will be displayed. Use new to create a new partition with 60 GB in its place, and set the type to ext4. Then, move to the trailing free space and create the 440GB swap partition with type swap.
6. Create a new disk in GCP with the size of 60GB attached with the instance. It will be on /dev/sdc, you can view it by ``$ lsblk`` (The image is taken after the process is done.)
7. Finally clone the disk, ``dd if=/dev/sdb of=/dev/sdc`` (It will take a while)
8. Try to mount /dev/sdc on /var/lib/docker instead of the old disk ``mount /dev/sdc /var/lib/docker``
9. Start the docker service ``$ sudo service docker start``
10. Hooray!, Now everything works with the smaller disk need.
11. Get rid of the old disk on GCP. We do not need to pay from them anymore.
In summary, we unmount a disk, shrink the disk, edit the partition table, and then using dd to clone disk from the old to the new. Finally, mount the new on the old’s mount point and finally, we can get rid of the old disk.
Hope this guide saves your time.
Thank you.
For those whoever want to develop and get consult on creating your own AI model, please getting more information at our company website “iApp Technology” (https://iapp.co.th) and testing our AI demoes (https://ai.iapp.co.th). You can contact me directly at kobkrit@iapp.co.th. Thank you very much. #iApp #Ai
See more at https://iapp.co.th and https://ai.iapp.co.th
Crazy about AI, Deep Learning, Coding, Web App & Mobile App
1 
1
1 clap
1 
1
Written by
Success is easy, if you do it.
Crazy about AI, Deep Learning, Coding, Web App & Mobile App
Written by
Success is easy, if you do it.
Crazy about AI, Deep Learning, Coding, Web App & Mobile App
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/a-to-z-of-google-cloud-platform-a-personal-selection-m-for-monitoring-385497b56e26?source=search_post---------201,"There are currently no responses for this story.
Be the first to respond.
After talking about logging in my previous entry in this series Monitoring seemed the natural choice for M so here ya go a little about monitoring for your delectation.
A typical monitoring solution consists of some way to collect metrics, dashboards to view the status of your systems and applications and a way to send alerts.
Firstly let’s briefly run through what a self managed monitoring system may typically look like ( and I’m keeping it high level here as this isn’t a post on diving deep on self managed monitoring systems ).
Metric collection/ monitoring framework : Something like Nagios, sensu, logstash or zabbix to actually collate all the events ( typically as log data) . There are tons to choose from and those were just the first ones that came to my mind when I was writing this post .
Typically you install an agent or plugin on your target systems that allows the events from your applications and O/S to be pushed/pulled to a central server.
Alerts: So when you get those events you need to do something with them to allow you to react to problems detected or even to bring up extra resources etc. The point is you’re passing the events to the collector / framework to act upon the data received. You need to have a way for your systems to react to events that fall outside of tolerance levels you’ve defined be that stuff like I need more resources or oh crap this is bad news and call for help. Calling for help you’ll need a notification system of some sort to reach an actual human . Email, pagerduty , hipchat, slack or you know good old fashioned IRC are all examples of ways of getting messages out to your staff that there’s a problem and could they please have a look.
Graphs , graphs lots of graphs to visually see how things are doing typically displayed via
a dashboard: This needs to integrate with the monitoring framework for example solutions like kibana which via elasticsearch integrates with Logstash ( the elk stack) . Zabbix has it’s global dashboard and sensu has it’s enterprise dashboard . As with the monitoring frameworks there’s a variety of dashboarding solutions you can layer on top .
That’s a number of moving parts that you have to knit together, keep available and manage. This may not be a problem if you have the operational staff to manage that .
GCP has a fully managed monitoring solution in Stackdriver that provides metrics, dashboards, and alerting . It’s very easy to use & fully integrated with GCP .
Metric collection/monitoring framework: It supports a number of GCP resources including : GAE, GCE, GKE, Pub/Sub, and Cloud SQL. In addition You can view your security rules and your GCS resources . I particularly like the fact that the GCS view provides a single view of the number of buckets, the number of the objects in each bucket and the total size of the objects in the bucket ( very nice).
By default you get core metrics from the supported services .
However If you install the optional plugin then you can get additional metrics from GCE and you can also collect metrics from a list of popular applications look at the docs for the list of supported applications . Just to mention a few of the applications supported cassandra, kafka and of course the ever popular Mongodb are amongst them. The list of available metrics are listed here. But if your particular package isn’t supported or you want to monitor specifics from your application then you can create custom metrics there’s an API that you need to use . It’s a comprehensive fully managed solution.
Some other stuff that caught my eye in this part of the Stack Driver Monitoring stack:
Alerts: You can create a set of rules that determine whether your resources or groups are operating normally . These are alerting policies. The rules are logical conditions involving metric thresholds and uptime checks. An alert is triggered if the alert policy conditions are met this will cause an incident to appear in the incidents section of the Stackdriver monitoring dashboard.
As with the self managed solutions you can easily integrate Stackdriver monitoring with a range of notification solutions such as slack, hipchat , pagerduty etc . I love that you can configure it to send alerts directly to the Cloud Monitoring mobile app so you can view incidents directly from the app and you can also ssh directly from there if you need to.
Dashboard: You can create a dashboard that shows different types of charts. For example you can create a chart that displays the time series data for one or more metrics, or you can aggregate several metrics. As well as the charts you can also view event logs and incident lists
This image from the docs gives you an idea of what a typical dashboard can look like :
Hopefully that whistle stop tour of Stackdriver together with the discussion on the logging capabilities I discussed in the previous entry in this series gives you a flavour of the capabilities available in GCP that means you don’t have to install and maintain your own logging & monitoring system .
Google Cloud community articles and blogs
1 
1 clap
1 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Chocolate addict - I have it under control really I do. I do stuff involving cloudy tech. Tweets my own so only me to blame, except for retweets.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/a-to-z-of-google-cloud-platform-a-personal-selection-t-timeouts-9d531f764dd7?source=search_post---------202,"There are currently no responses for this story.
Be the first to respond.
Timeouts occur all the time and at different layers of your application architecture . I am going to discuss mostly about how they pertain to GCE & GAE rather than the code you write to handle timeouts in your application.
So you have an instance and it’s timing out where do you start ? here is a good starting point if it’s an instance you expect to have direct access to the internet . This talks about idle TCP connections, TCP keep alives, firewall configurations all in a few paragraphs so I won’t regurgitate here.
I talked about the Cloud load balancing service back in G where the concept of Instance groups was introduced. You can check that services are running properly on the instances in that group by setting a http health check against the group . Be careful though as checks set up directly against the instance group are different from those set up against the load balancing service and the actions they invoke to respond to a failed health check are distinct . Health checks set up directly against the instance group will cause the instance returning a failed health check to delete that instance and start up a new instance. Whereas Health checks set up against the load balancing service basically stop directing traffic to the instance failing the healthcheck but leaves the instance running. You should not set the same check via the load balancing service as you do directly against the instance group ( here be dragons).
In both the above cases you set a timeout flag value . ( I know I’ve kind of diverted onto health checks but I hadn’t forgotten what the core of this post is about honest !)
The gcloud command for setting up a health check which you can then associate with the load balancing service or directly against the managed instance group looks similar to this example:
This runs a simple http check every 10 seconds. The healthy-threshold indicates how many subsequent health checks must return healthy before an instance is marked as permanently unhealthy. The unhealthy-threshold indicates how many consecutive failures causes an instance to be marked unhealthy.
The timeout is how long to wait for request to respond before returning a failure ( in our example set to 5 seconds). The timeout value should not be greater than the check-interval ( it can have the same value).
You can then associate the health check resource you’ve created with a target pool or backend service . If you want it associated with the load balancing service. To associate the check with the group directly you use the set-autohealing flag
The initial-delay flag allows the instances to start and finish running their startup scripts so that the managed instance group does not prematurely recreate that instance i.e to prevent an infinite loop of timeouts.
Using App Engine standard to access Cloud Storage you use the App Engine UrlFetch feature. The Cloud Storage client library handles timeout errors on both the App Engine side and the Cloud Storage side performing retries automatically, so your application does not need to add logic to handle this. The configuration of the timeout and retry mechanism is exposed through the RetryParams class, which you can use to change any or all of the default settings either on an application-wide basis, or for a specific invocation of a Google Cloud Storage client library function (copy2, delete, listbucket, open, stat) Check the docs out
App Engine restricts resource usage to certain limits to prevent resource overuse and to provide better isolation between apps running in the same cluster. The App Engine request timer (Java/Python/Go) ensures that requests have a finite lifespan and do not get caught in an infinite loop. Currently, the deadline for requests to frontend instances is 60 seconds. (Backend instances have no corresponding limit.) . If a request fails to return within 60 seconds a DeadlineExceededError is thrown. This needs to be caught and there are plenty of ways to mitigate the effects of throwing a DeadlineExceededError or timeouts. This article explains the various options eloquently so if you’re using App Engine I recommend taking time out ( see what I did there!) to have a read.
Google Cloud community articles and blogs
1 
1 clap
1 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Chocolate addict - I have it under control really I do. I do stuff involving cloudy tech. Tweets my own so only me to blame, except for retweets.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/a-to-z-of-google-cloud-platform-a-personal-selection-u-uniqueness-974cd289f305?source=search_post---------203,"There are currently no responses for this story.
Be the first to respond.
This is another of those slightly odd posts in the series ( normal service resumes at least for a bit next post but some of these remaining letters are challenging to say the least! )
Being able to “really” have applications that operate at scale identities need to be abstracted such that you as the consumer of Cloud scale services don’t actually care. Just because you don’t care about those identities though doesn’t mean that the services you use don’t!
So stating the obvious everything has some sort of identity yes even those instances I keep harping on you shouldn’t care about ( Google cattle & pets I’m not going to directly add to the hundreds of posts on that concept)
I’m going to focus initially on GCE in this post and explore Uniquness and what it means to you ( yes I’m gonna say it!) in a cloud native world. I’ll then discuss uniqueness in a microservice orientated world on GCP.
So an instance is launched and it gets given a number of identifiers that together allow you or the service that is fronting it to identify it (e.g name, IP addresses). So let’s have a quick look at an instance I’ve started. If you’re new to GCP then check out the docs to see how to start instances.
If you look at the console you’ll see some basic details that allow you to identify the instance as an individual so to speak!
You can also use the gcloud instances describe command:
So in my example using the instance example-instance the command looks like this
And you can see a lot more detail
If the instances are part of a managed instance group you personally don’t care but the load balancer that uses those managed instances does , after all it needs to know what instances are healthy and where to direct traffic .
Can you imagine being in a meeting where everyone is called “Joe Brown” and no one has any nicknames to make identification easy? ( sorry Joe you know who you are! )
Disks also have an identity and are also given a name. In my example I only have the boot disk which has defaulted to the name of the instance that it’s used by. It looks like this in the console
That is great but the O/S needs identifiers that it can use .
If I ssh onto the instance and do a fdsik -l you can see the O/S info on the disks . Note the name you gave the disk using the gcloud command or console is not used by the O/S apart from when you format & mount the disk.
Read the docs here on creating persistent disks for detail on how to create additional disks and how to get the O/S to see it.
Snapshots are unique too and need naming too. See the docs
Okay I can hear you so we get that but why should I care. If you are running a truly immutable configuration using managed instance groups and your instances are ephemeral then you don’t have to really worry about the individual instances ( yes yes I know trouble shooting etc but in the context of this post this is a fair statement) but you do need to know the identity/ name of the instance group so that you can auto scale and allow the instances in that group to be fronted by the Load balancer.
Where you or your applications do need to care directly about the instance Identity then using a Service registry or central index where the instances and applications can self register and also be removed if they no longer exist is the way to go. Your applications should use the registry to look up the instances and/or services they need to use .
With microservices being a thing this suddenly increases the number of unique services and instances you need to keep track of and inevitably you need to think about the use of a service registry.
A common example of a service registry is a DNS server . For example if you use windows then you’ll be using DNS to allow discovery of domain controllers .
Before I move on I’m just going to make a call out for one of my fav features in GCP’s compute solution in live migration
In cases like this where the domain controller and the DNS identity need to be set and be known by other machines having ephemeral instances isn’t really going to work so you can take advantage of GCP’s Live migration to help you maintain that identity. This blog post from March 2015 is a great overview of how Live migration works.
DNS is arguably the most popular registry service and if you’re reading this you made use of it to get here.
I’d also like to mention three other service registries in no particular order of preference. I’m not going to take any time to compare & contrast each of them (again the internet has done a good job of that already) . Ultimately they provide a dynamic way to register and unregister service end-points that can be looked up and then used by other applications.
Zookeeper — ZooKeeper allows distributed processes to coordinate with each other through a shared hierarchal namespace which is organised similarly to a standard file system. The name space consists of data registers — called znodes, in ZooKeeper parlance — and these are similar to files and directories. There’s a zookeeper plugin for Stackdriver . @vicnastea discusses using Zookeeper with exhibitor to integrate with GKE
Consul — Consul clients run on every node in the cluster. These clients are part of a gossip pool which serves several functions, including distributed health checking. .Can easily be run on GCP and this article on Autoscaled Internal Load Balancing using HAProxy and Consul on Compute Engine has a great walkthrough of configuring and using Consul on GCP
etcd a key/value store accessible through HTTP. It is distributed and features hierarchical configuration system that can be used to build service discovery . It is used by Kubernetes
They all have at their heart a consensus protocol to provide Consistency
For those of you interested in the theory of the consensus models used by these registries here’s a few starting points:
Paxos — you can’t start delving into consensus without understanding paxos so I would suggest starting here which has a brilliant visualisation to help explain how it works. Both zab & Raft can be traced back to paxos . This paper from Google describing some of the challenges they met when implementing paxos ( kinda indicates why Raft at least became a thing) is a good read
Raft is the model used by etcd and consul
Zab is the model used by Zookeeper
Google Cloud community articles and blogs
1 
1 clap
1 
Written by
Chocolate addict - I have it under control really I do. I do stuff involving cloudy tech. Tweets my own so only me to blame, except for retweets.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Chocolate addict - I have it under control really I do. I do stuff involving cloudy tech. Tweets my own so only me to blame, except for retweets.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@garystafford/recent-posts-about-developing-on-the-google-cloud-platform-90860da5c20b?source=search_post---------204,"Sign in
There are currently no responses for this story.
Be the first to respond.
Gary A. Stafford
Jan 4, 2019·2 min read
Looking for information about developing on the Google Cloud Platform? Enjoy some of my most recent articles on the subject.
"
https://medium.com/@danielrothmann/i-was-referring-to-running-notebooks-on-virtual-machines-on-google-cloud-platform-where-you-can-926da8171e64?source=search_post---------205,"Sign in
There are currently no responses for this story.
Be the first to respond.
Daniel Rothmann
Aug 26, 2018·1 min read
Luciano Strika
I was referring to running notebooks on virtual machines on Google Cloud Platform where you can scale memory and computational resources. Loading big CSV files there is still slow when the not parallelized.
When preparing big datasets for a machine learning project, for some tasks, like converting string columns to numerical categories, normalizing column values or checking the percentage of missing values, it’s necessary to iterate through the whole dataset. Loading the CSV as chunks with Pandas adds a lot of unnecessary work to complete those tasks. With Dask, I’m hoping to speed up that process!
Senior data engineer @ Kanda.
3 
2
3 
3 
2
Senior data engineer @ Kanda.
"
https://medium.com/@john-tucker/planning-google-cloud-platform-gcp-compute-resources-a478bde04d7c?source=search_post---------206,"Sign in
There are currently no responses for this story.
Be the first to respond.
John Tucker
Dec 25, 2021·7 min read
Categorizing GCP compute resources to understand them.
GCP offers a range of compute resources and thus it is challenging to determine which to use. To help differentiate them, here we categorize them based on the following factors:
Those resources that do not have direct VPC connectivity still can use Serverless VPC Access to connect to VPC Networks.
vCPU Pricing is in contrast to pricing based on compute time; compute time is measured from the time your workload receives a request to the time it completes.
While one does not typically think of GCE Instances running containerized workloads, one certainly could install a container engine, e.g., Docker, on a GCE Instance and use it to run a containerized workload. GCE also more directly supports running containerized workloads.
You can configure a virtual machine (VM) instance or an instance template to deploy and launch a Docker container. Compute Engine supplies an up-to-date Container-Optimized OS (COS) image with Docker installed and launches your container when your VM starts.
— Deploying containers on VMs and MIGs
Here we do not have an underlying virtual instance per-se; we rather have the GCE Instance itself. We have SSH access to it.
Compute Engine uses key-based SSH authentication to establish connections to Linux virtual machine (VM) instances
— About SSH connections
GCE Instances are associated with a Subnet of a VPC.
Each network interface of a Compute Engine instance is associated with a subnet of a unique VPC network.
— Virtual machine instances
As one might expect GCE pricing is, principally, driven by the allocation of vCPU.
Kubernetes is designed to run containerized workloads and GKE is Google’s managed Kubernetes offering.
Kubernetes, also known as K8s, is an open-source system for automating deployment, scaling, and management of containerized applications.
— Kubernetes
GKE Standard Nodes run on GCE Instances; thus the Access and VPC factors are the same as for GCE.
Google Kubernetes Engine (GKE) provides a managed environment for deploying, managing, and scaling your containerized applications using Google infrastructure. The GKE environment consists of multiple machines (specifically, Compute Engine instances) grouped together to form a cluster.
— GKE overview
Other than the relatively low cluster management fee, GKE Standard’s pricing is based on the GCE Instances underlying the Nodes; thus the vCPU Pricing is the same as for GCE.
In Standard mode, GKE uses Compute Engine instances worker nodes in the cluster. You are billed for each of those instances according to Compute Engine’s pricing, until the nodes are deleted. Compute Engine resources are billed on a per-second basis with a one-minute minimum usage cost.
— Google Kubernetes Engine pricing
GKE Autopilot is much like GKE Standard except that its Nodes run on virtual instances that one does not have access to; i.e., one does not have SSH access.
Autopilot is a new mode of operation in Google Kubernetes Engine (GKE) that is designed to reduce the operational cost of managing clusters, optimize your clusters for production, and yield higher workload availability. The mode of operation refers to the level of flexibility, responsibility, and control that you have over your cluster. In addition to the benefits of a fully managed control plane and node automations, GKE offers two modes of operation:
Autopilot: GKE provisions and manages the cluster’s underlying infrastructure, including nodes and node pools, giving you an optimized cluster with a hands-off experience.Standard: You manage the cluster’s underlying infrastructure, giving you node configuration flexibility.
— Autopilot overview
The virtual instances underlying the Nodes are associated with a Subnet of a VPC Network.
Networking Pre-configured: VPC-native (alias IP)
— Autopilot overview
Instead of GKE Standard’s pricing based on Nodes, GKE Autopilot’s pricing is based on Pods provisioned resources; thus still vCPU Pricing.
Autopilot clusters accrue a flat fee of $0.10/h per cluster for each cluster after the free tier, plus the CPU, memory and ephemeral storage compute resources that are provisioned for your Pods. The amount of pod resources provisioned is based on the resource requests of the Kubernetes PodSpec. You are not charged for system Pods, the operating system overhead, or unallocated space. All Autopilot resources are charged in 1 second increments, there is no minimum charge.
— Google Kubernetes Engine pricing
In addition to the language-specific runtimes, App Engine Flexible Environment supports running Docker containers. Also, as it runs on GCE Instances; thus the Access and VPC factors are the same as for GCE.
App Engine flexible environment instances are Compute Engine virtual machines, which means that you can take advantage of custom libraries, use SSH for debugging, and deploy your own Docker containers.
— App Engine flexible environment
App Engine Flexible Environment pricing is based on GCE Instances; thus it is vCPU Pricing.
Apps running in the flexible environment are deployed to virtual machine types that you specify. These virtual machine resources are billed on a per-second basis with a 1 minute minimum usage cost.
— App Engine pricing
App Engine Standard Environment is limited to one of several runtimes; thus does not support running arbitrary containerized workloads. It also runs on virtual instances that one does not have access to; i.e., one does not have SSH access.
The App Engine standard environment is based on container instances running on Google’s infrastructure. Containers are preconfigured with one of several available runtimes.
— App Engine standard environment
The App Engine Standard Environment workloads do not have direct VPC connectivity.
Connecting to a Shared VPC network for App Engine Standard environment
— Configuring Serverless VPC Access
App Engine Flexible Environment pricing is based on the instance class; thus it is vCPU Pricing.
The billing rate depends on the instance class you specify for your app.
— App Engine pricing
Cloud Run runs containerized workloads.
Develop and deploy highly scalable containerized applications on a fully managed serverless platform.
— Cloud Run
The Cloud Run workloads run on Google managed infrastructure; thus one does not have SSH access.
Cloud Run abstracts away all infrastructure management by automatically scaling up and down from zero almost instantaneously — depending on traffic. Cloud Run only charges you for the exact resources you use.
The Cloud Run workloads do not have direct VPC connectivity.
Connecting to a Shared VPC network for Cloud Run
— Configuring Serverless VPC Access
Cloud Run pricing is based on compute time.
Cloud Run charges you only for the resources you use, rounded up to the nearest 100 millisecond. Note that each of these resources have a free tier. Your total Cloud Run bill will be the sum of the resources in the pricing table.
— Cloud Run pricing
Cloud Functions do not run containerized workloads. They also run on Google managed infrastructure; thus one does not have SSH access.
Google Cloud Functions is a serverless execution environment for building and connecting cloud services. With Cloud Functions you write simple, single-purpose functions that are attached to events emitted from your cloud infrastructure and services. Your function is triggered when an event being watched is fired. Your code executes in a fully managed environment. There is no need to provision any infrastructure or worry about managing any servers.
— Cloud Functions Overview
The Cloud Function workloads do not have direct VPC connectivity.
Connecting to a Shared VPC network for Cloud Functions
Cloud Functions pricing is based on compute time.
Cloud Functions are priced according to how long your function runs, how many times it’s invoked and how many resources you provision for the function.
— Cloud Functions pricing
Broad infrastructure, development, and soft-skill background
See all (26)
11 
11 claps
11 
Broad infrastructure, development, and soft-skill background
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@jaychapel/announcing-google-cloud-platform-cost-control-with-parkmycloud-eb5ec83a2970?source=search_post---------207,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Jun 6, 2017·2 min read
Today, we’re excited to announce that ParkMyCloud now supports Google Cloud Platform!
Amazon Web Services (AWS) customers have been using ParkMyCloud for automated cost control since the product launch in 2015, and Azure customers have enjoyed the same capabilities since earlier this year. With ParkMyCloud, you can automate on/off scheduling to ensure your resources are only running when you actually need them. Customers such as McDonald’s, Fox, Capital One, Sage Software, and Wolters Kluwer have already saved millions.
If you use multiple public cloud providers, you can manage them together on a single dashboard.
With the addition of Google Cloud Platform, ParkMyCloud now provides continuous cost control for the three largest cloud providers in the $23 billion public cloud market. This means ParkMyCloud enables enterprises to eliminate wasted cloud spend — a $6 billion problem in 2017. See more in our official press release.
It’s simple to get started using ParkMyCloud to manage your Google compute resources:
If you’re new to ParkMyCloud, please see these additional resources:
We’re happy to schedule a demo for you to see ParkMyCloud in action — if you’re interested, please contact us.
You can get started now with a free 14-day trial of ParkMyCloud, with full access to premium features.
After your trial expires, you can choose to continue using the core parking functionality for free (forever!), or upgrade to use premium features such as the API, advanced reporting and SSO. Happy parking!
Originally published at www.parkmycloud.com on June 6, 2017.
CEO of ParkMyCloud
See all (317)
CEO of ParkMyCloud
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@Apiumhub/google-cloud-platform-solutions-for-online-systems-f88755ecf1c3?source=search_post---------208,"Sign in
There are currently no responses for this story.
Be the first to respond.
Apiumhub
Apr 4, 2017·3 min read
This month Apiumhub was proud to be invited to assist the conference that was organized by Google and Ackstorm in Tecnocampus Mataró where Google Cloud Platform solutions for online systems were presented.
We were invited there by Ackstorm, while we were discussing the project of Pulpolab. We always try to work hand by hand with the clients and go to events with them, support each other.
As you may know, Google introduced their Cloud Services 7 years ago. Since then, it is becoming more and more popular due to its quality-price relation and performance efficiency. In this event, members of Google explained the Google Cloud Platform solutions and how their Cloud Infrastructure can be properly designed and managed with the support of Ackstorm-ECmanaged.
We were happy to be part of this conference that was mainly about cloud platform and the main functionalities and used cases. This event was extremely successful; you could breath the tech geek atmosphere! The place was superb, with great views to the sea and the facilities of the Tecnocampus.
The first speech was given by Juan Lorenzo, the Head of Google Cloud Platform Sales in Iberia. He gave an introduction to the GCP, explained the problems it can solve, the way it helps startups and companies and he showed some case studies. He had a short speech, but he was so precise, that from the start you could feel the greatness and power of what was about to come.
Later speeches were given by Ackstorm people. Actually, they are a modern cloud company with great expertise in the area.
We attended the speech of Javier Domingo, who is a senior Engineer at Ackstorm — ECmanaged. His goal was to show current needs of Cloud Architecture and Design, different practices used of GCP tools, google app, google computer storage, google cloud datastore, google big query and Docker orchestration with Kubernetes.
The final speech was given by Gabriel Rivera, business development at Ackstorm — ECmanaged. He actually gave recommendations about cloud solutions for companies, elastic computing, cloud architecture, cloud performance, management and monitoring.
GCP seems to be one of the best solutions for online systems. It has its own network that connects its data centres and does not rely on the common infrastructure.
Furthermore, it uses software-defined networking technology to route packets across the globe and enable fast edge-caching. But this is one of the infinite services that google offers. You can find them all on their own web page.
It was a good example of how a conference should look like and be held.All of us really enjoyed it and networking while having a beer towards the end of the event was the icing on the cake.
Don’t forget to subscribe to our monthly newsletter to receive latest news in the software world!
Originally is published on https://apiumhub.com/tech-blog-barcelona/.
Software architecture, web & mobile app development www.apiumhub.com
See all (7,844)
1 
1 clap
1 
Software architecture, web & mobile app development www.apiumhub.com
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/a-to-z-of-google-cloud-platform-a-personal-selection-z-zero-ops-f921b3d2a24?source=search_post---------209,"There are currently no responses for this story.
Be the first to respond.
This is the last entry in my series and I had a number of topics I could have chosen to write about only I ended up talking about them elsewhere in the series or I felt I couldn’t add anything to the topic so I’m going out opinionated.
Firstly I’ll just state that I do not believe how much of a managed platform you have there is no such thing as Zero Ops as you will always be looking at logs (these include audit logs!) , responding to issues, managing security controls etc . Using the Cloud does not absolve you of that sort of responsibility so someone in your org needs to do that whatever label you deicide to use for those activities!
Okay now that is out the way what do I mean when I use the term “Zero Ops” ( as if I ever would use it anyway , it’s in the same bucket I keep the term DevOps these days ) :
Not having to worry about cuddling tin ;not investing in tasks that provide no benefit for your business focus; not worrying about compute sizes, sharding strategies; disk perf etc because all that stuff is taken care of .
[ An aside ] I believe the latest buzz term in the industry I work in has moved on somewhat from No Ops/ Zero Ops to Serverless and that seems to be irritating folks as much as the abuse of the term DevOps and No Ops has been !
GCP has plenty of services that fall into the Zero Ops box. It’s nothing new to them it just is . They may have been a bit ahead of the curve I feel.
So here’s my personal list of GCP services that fit my definition of Zero Ops today with accompanying Zero Ops strapline :
BigQuery — Fully managed, petabyte scale, low cost analytics data warehouse. BigQuery is serverless, there is no infrastructure for you to manage and you don’t need a database administrator to tweak performance parameters.
Dataflow — Transparently handles resource lifetime and can dynamically provision resources to minimise latency while maintaining high utilisation efficiency. Dataflow resources are allocated on-demand providing you with nearly limitless resource.
Datastore — Automatically handles sharding and replication, providing you with a highly available and durable database that scales automatically to handle your applications’ load.
App Engine — Scales your application automatically in response to the amount of traffic it receives so you only pay for the resources you use. Just upload your code and Google will manage your app’s availability. There are no servers for you to provision or maintain.
Datalab — Built on Jupyter (formerly IPython) , runs on Google App Engine and orchestrates multiple services automatically so you can focus on exploring your data.
Pub/Sub — Fully-managed real-time messaging service that allows you to send and receive messages between independent applications
GCS — Durable and highly available object storage
Cloud Functions — Lightweight, event-based, asynchronous compute solution that allows you to create small, single-purpose functions that respond to cloud events without the need to manage a server or a runtime environment
With those services alone you can build some pretty funky solutions without ever having to worry about the nuts & bolts of lower level infra services . Someone else worries about keeping those services up and running. If you’re interested in how that is managed as you know someone has to do that stuff just not you as the consumer of the “Zero ops” (sorry !) service then have a read of the SRE book .
So I’ve done it I managed to have an entry for every letter of the alphabet despite Y challenging me somewhat. I posted one a week for 24 weeks ( E was put under A as it was a logical place for that entry and I posted 2 in one week early on hence why 24 and not 26 weeks ). I’ve enjoyed writing the series and I hope that you have found something useful, or at least some food for thought amongst the selection.
For a little bit anyway I’m dropping the digital pen here for a bit ( Yeah I know I did end the series like that !) .
Google Cloud community articles and blogs
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Chocolate addict - I have it under control really I do. I do stuff involving cloudy tech. Tweets my own so only me to blame, except for retweets.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/@alexismp/google-cloud-platform-gcp-en-fran%C3%A7ais-dans-le-texte-69d5be447eea?source=search_post---------210,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alexis MP
Nov 21, 2017·2 min read
Google Cloud Summit s’est tenu à Paris en octobre 2017 au Palais des Congrès.
L’occasion d’entendre parler de nombreux sujets GCP en français.
Parmi les sessions publiées sur YouTube, en voici deux qui ne sont pas particulièrement techniques et que je vous recommande vivement.
Tout d’abord il y a la session d’Antoine Leblanc sur le métier de SRE (Site Reliability Engineer), sujet souvent couvert en anglais, mais dont voici un très bonne description dans la langue de Molière. Antoine vous explique entre autre pourquoi même Google n’essaye pas d’offrir 100% de disponibilité (!) :
Ensuite il y a cette présentation de Maya Kaczorowski et Fenitra Ravelomanantsoa sur les leçons apprises de la sécurisation de Google (puce Titan, encryption par défaut, DDoS, 2FA, …) ainsi qu’une deuxième partie (importante) sur la conformité, la confidentialité des données et la GDPR :
Voici également six autres sessions Google Cloud Platform (GCP) sur Big Data, Kubernetes, Machine Learning, et Serverless (toujours en français) qui méritent votre attention :
Enfin, la playlist complète des vidéos Google Cloud Summit 2017 est ici :
Google Cloud Developer Relations
Google Cloud Developer Relations
"
https://medium.com/@lakshmanok/interview-about-book-about-data-science-on-google-cloud-platform-fef4feaad2af?source=search_post---------211,"Sign in
There are currently no responses for this story.
Be the first to respond.
Lak Lakshmanan
Mar 3, 2017·1 min read
The Google Big Data & ML blog has an interview with me about my upcoming O’Reilly book. Read the blog post here:
cloud.google.com
This was a less stressful interview than the last time I was interviewed by Google (for a job). Plus:
Data Analytics & AI @ Google Cloud
See all (63)
1 
1 clap
1 
Data Analytics & AI @ Google Cloud
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/a-to-z-of-google-cloud-platform-a-personal-selection-y-for-yarn-b9b0312d66b9?source=search_post---------212,"There are currently no responses for this story.
Be the first to respond.
I was absolutely defeated by this letter. I’ve been asking for a couple of weeks for suggestions and the same subjects kept being thrown my way . So I just wanted to thank @complex and @sveesible for both suggesting YARN and keeping me on track to meet my said goal of finishing this series in June and keeping mostly everything consecutive letter wise .
YARN — This stands for yet another resource negotiator . It was introduced as part of the Hadoop v2 architecture.
YARN actually decouples MapReduce’s resource management and scheduling capabilities from the data processing component. Why is this a thing? well before YARN the Hadoop JobTracker limited scalability and was also a single point of failure .
So from a tightly coupled configuration found in Hadoop v1 the introduction of YARN changed the model as illustrated in the diagram below (As I’m talking about GCP in these series of posts GCS is accurate in the storage part of Hadoop v2 in the diagram):
With the introduction of YARN this allowed Hadoop to support more varied processing approaches and a broader array of applications such that you can now run interactive querying and streaming data applications simultaneously with MapReduce batch jobs. Some of these other tools are illustrated in the diagram above.
The diagram below from the apache YARN pages illustrates the architecture
I’m an old skool Hadoop/ MapReduce user so YARN isn’t something that was around when I was initially getting my hands dirty with Hadoop but I wish it had been !
I won’t spend time talking about the history of Hadoop v1 and Hadoop v2 there are some excellent posts out on the interwebs that goes into that. This is a series about GCP so I’ll focus on where YARN fits into the GCP ecosystem now.
GCP has a fully managed service that lets you run the Apache Spark and Apache Hadoop ecosystem called Cloud Dataproc . It provisions big or small clusters rapidly, supports many popular job types. YARN is integrated with this service . Starting a Dataproc cluster from the Cloud console this integration is obvious.
You can start or stop a Dataproc cluster in a number of ways including using the YARN web interface . The interface can be found on your Dataproc cluster master node on port 8088.
To access this interface it is highly recommend to use a SSH tunnel to create a secure connection to the master node. The SSH tunnel supports traffic proxying using the SOCKS protocol. This means that you can send network requests through the SSH tunnel in any browser that supports the SOCKS protocol allowing you to transfer all of your browser data over SSH, eliminating the need to open firewall ports to access the web interfaces. See the docs for how to do this.
Through this interface you can also monitor your cluster in addition to using the Cloud console or command line. This integration means you can continue to use the tools you are already used to
You can adjust the YARN parameters in the yarn-site.xml
As well as system logs application logs like the YARN logs are also forwarded to Cloud Logging ( see my entry for L in this series for a bit about Logging)
Google Cloud community articles and blogs
Written by
Chocolate addict - I have it under control really I do. I do stuff involving cloudy tech. Tweets my own so only me to blame, except for retweets.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Chocolate addict - I have it under control really I do. I do stuff involving cloudy tech. Tweets my own so only me to blame, except for retweets.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/google-cloud-platform-news-roundup-for-august-2015-13032ccc0f3d?source=search_post---------213,"There are currently no responses for this story.
Be the first to respond.
I hope everyone had a great weekend! Here is a quick snapshot of what you may have missed during the past month. There were five product General Availability announcements, as well as a bunch of new useful articles from Cloud Developer Advocates!
That was a lot of GA announcements for August!
Julia Ferraioli (@juliaferraioli) posted a series of articles on running a Minecraft server on Google Compute Engine with Docker:
Even if you don’t play Minecraft, you can learn a lot about Google Compute Engine and Docker! Follow her to get the series finale.
Felipe Hoffa (@felipehoffa) published a video titled Open data unleashed: The NYC taxi dataset. I live in New York City so it was especially interesting for me to watch! It features hackers dissecting taxi data using BigQuery. One team figured out the location of the highest grossing fire hydrant. I just hope someone figures out how to solve the shift change problem. Felipe loves BigQuery — check out his top posts on Reddit.
New to Google Cloud Platform? Check out Sandeep Dinesh’s (@SandeepDinesh) webinar on Getting Started with Google Cloud Platform. This video is pretty popular — I even saw it playing on the big screens at our Google Cloud Platform community events in Indonesia and Malaysia!
Got a minute? Learn something useful about Google Cloud Platform from Aja Hammerly (@thagomizer_rb), Paul Newson (@newsons_nybbles), and Sandeep Dinesh. They published several one-minute tutorial videos known as the Cloud Minute series. Check it out and save the playlist.
www.compoundtheory.com
If you work with Go — check out how easy it is to write server-side software with Go and Google App Engine. Learn about tips and tricks and how to set up your environment from Mark Mandel (@Neurotic). It’s an epic post.
Ever wonder what Kubernetes namespaces are and what you can use them for? Ian Lewis (@IanMLewis) explains it all in his article Using Kubernetes to Manage Environments.
Take a look inside Google’s Data Center Network and how we achieved 1 petabit/second of total bisection bandwidth. There is another post with more photos. It’s mind blowing. High Scalability also followed up with even more detail.
You may have seen some of these articles on our Medium publication already, but ICYMI:
medium.com
medium.com
medium.com
medium.com
medium.com
Like these articles? Follow us on Twitter (@googlecloud), Medium, and Flipboard Magazine!
Google Cloud community articles and blogs
Thanks to Jack Wilber. 
Written by
Loves #Java. Lives in NYC. Creator of @JDeferred. Ex @Google @JBoss, @RedHatSoftware, @Accenture. Opinions stated here are my own.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Loves #Java. Lives in NYC. Creator of @JDeferred. Ex @Google @JBoss, @RedHatSoftware, @Accenture. Opinions stated here are my own.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@PicardParis/discovering-google-cloud-platform-e621ea3200d7?source=search_post---------214,"Sign in
There are currently no responses for this story.
Be the first to respond.
Laurent Picard
Mar 17, 2017·2 min read
Google Cloud Next 2017 event was huge. Over 200 sessions (about 45 min each) are available, covering many subjects: IaaS, PaaS, Containers, Web Apps, APIs, Machine Learning, Databases, G Suite, Chrome, Android… The momentum around Google Cloud Platform (GCP) is just impressive.
Over the 3 days, they had 100 announcements.
blog.google
Last but not least, Google is currently offering a $300 credit to get started with GCP for free.
cloud.google.com
Experimenting with Google Cloud Platform
Tech lover, passionate about software, hardware, science and anything shaping the future • ⛅ explorer at Google • Opinions my own • You can DM me @PicardParis
Tech lover, passionate about software, hardware, science and anything shaping the future • ⛅ explorer at Google • Opinions my own • You can DM me @PicardParis
"
https://medium.com/@devops4solutions/gcp-tutorial-for-beginners-learn-how-to-start-with-google-cloud-platform-a9aaa7a6c9fc?source=search_post---------215,"Sign in
There are currently no responses for this story.
Be the first to respond.
Nidhi
Nov 23, 2020·6 min read
In this blog, we will explore how to create an account in GCP and learn creating resources like VPC, subnets, firewalls.
"
https://medium.com/google-cloud/a-to-z-of-google-cloud-platform-a-personal-selection-w-wrappers-f5618cbce672?source=search_post---------217,"There are currently no responses for this story.
Be the first to respond.
Another odd one but come on it’s “w” . I plucked this one for the same place I got the idea for “u” !
Okay I hear of lock in all the time and many times it’s usually not a rational discussion as somewhere along the line they’re already using something that “locks” them in they’re just subconsciously not acknowledging that fact. Chosen a specific database system ? You have haven’t you? Made use of that nice stored procedure oh want to use another data store now the fun begins? What was that about lock in?… You see where I’m going here!
You should absolutely not be afraid to use the best tool for the job and yes if you can get a managed service embrace it as while your provider is looking after the nuts and bolts you’re focused on that awesome application you want to get to market!
Keeping to the lowest common denominator really is a poor choice .
Even If I haven’t persuaded you that “lock-in” is something you are already doing anyway and it’s not really a thing but you are still absolutely convinced that “lock in” is something you need to really worry about read on.
You are still able to take advantage of the higher level managed services using a cloud provider like GCP has to allow you to get to market faster while still giving you a get out clause .
Let’s address data and data focused products firstly. Just stop worrying about not using the best your provider has to offer. On GCP this could be BigQuery where the advantages outweigh anything you can do elsewhere . I’m not going to spend any more time here on BigQuery as there are plenty of independent assessments on how great it is and I don’t need to add to the platitudes ( needless to say I am a big fan) .
GCP is very open anyway though so for example if you use Bigtable you can use the Bigtable hbase api’s I wrote about that way back in b of this series. Google have open sourced Apache beam the GCP managed service of that is Dataflow. So go ahead use Dataflow it’s pretty awesome too and if you really need to run the same data processing pipelines elsewhere you have the option of using Apache beam.
The thing about Data is how you move it around and where you can store it. A simple way to address the perceived lock-in is to take regular exports of your data into a highly-portable flat format such as CSV, XML, or JSON, and store it in Cloud Storage Nearline. This approach will provide maximum flexibility, allowing you to import the data into any database system you choose to use.
You don’t even have to do the export though as there are plenty of tools around to help transfer data from one data format to another when you need to.
So I’d go as far to say that data storage and processing in whatever format isn’t the lock in factor you should be wary of ( assuming you are still worrying about this being a thing ).
How you interface with The Providers API’s is sometimes something I hear concerns about for example rather than use Pub/Sub say ( and if you read p you know I love this service too ) avoidance tactics means they are still doing heavy lifting managing a queuing/messaging service on GCE rather than use the managed service. Data is just passing through a queuing/messaging system so is it really worth all the effort to manage your own service? If you still want to abstract then put a wrapper around the calls ( Bet you were wondering when I’d get round to wrappers :-)) .
Another technique you could use are feature flags so you can make native calls to the api’s . Feature flags are typically used in the following situations:
You can also easily use feature flags to direct native calls to a provider’s specific API’s so say you have a multi cloud strategy or are undertaking a migration this is neat way of pointing to the right environment.
Feature flags are basically just boolean values and can be as simple as a switch statement or hold the actual values in config files or in key value stores like redis.
There are frameworks available for various languages
For example togglz java, rollout ruby,
And a simple switch statement example:
Open source is in Google’s DNA and I have already touched upon some services that GCP has already opensourced: Apache Beam/ Dataflow and Bigtable with a Hbase client and it would be remiss of me not to mention a couple of other services that have opensourced cousins namely Kubernetes/GKE and Tensorflow/Cloud ML( which is currently in alpha)
So to sum up:
Google Cloud community articles and blogs
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Chocolate addict - I have it under control really I do. I do stuff involving cloudy tech. Tweets my own so only me to blame, except for retweets.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/cloudflare-blog/a-fast-secure-migration-to-google-cloud-platform-using-cloudflare-276c9097e397?source=search_post---------218,"There are currently no responses for this story.
Be the first to respond.
by Brady Gentile
Looking to host your website, application, or API in the cloud, or migrate to a new cloud provider while keeping your data secure? In this webinar, Trey Guinn, Head of Solutions Engineering at Cloudflare, will discuss how companies should approach security, during and after migration. We’ll highlight the migration story of LUSH, one of the largest global e-Commerce cosmetic retailers, and how they took the right steps to migrate from their previous cloud provider to Google Cloud Platform, in less than 3 weeks. Trey will be performing a live demo on setting up Cloudflare load balancing across cloud providers, as well as optimizing security through web application firewall (WAF), SSL / TLS Encryption, and Rate Limiting.
Asad BaheriSecurity & Networking Partner Manager Google Cloud Platform
Trey Guinn Head of Solutions EngineeringCloudflare
Asad Baheri
Today we’re going to talk about LUSH’s migration to Google Cloud and how Cloudflare, one of our top security and performance partners, can help you with your own cloud migration. Throughout our presentation, we’ll be talking about security best practices, how CDNs and the CDN Interconnect program works, and we’re also going to also give you a demo of Cloudflare’s load balancing to start your migration.
One of the main things that many people don’t realize is the amount of effort that Google has put into security. You may be familiar with safe browsing, which protects over three billion devices every day. That’s something Google has done not for profit, but just out of our own commitment to security.
If you actually look at what Google does, they have over 600 engineers dedicated to security and privacy with Google, which is more than a lot of other pure-play security companies. And the result of that is we’re trying to make the Internet a more secure place for our users. We believe that raising the security awareness level makes it easier for our customers. So we’ll talk about how that philosophy actually translates into Google cloud. If you look at some of the open source projects out there, even if you look at iOS, you’ll see Google was one of the top reporters of bugs and security vulnerabilities. Again, we are trying to make the Internet more secure for everyone using our products.
When we talk about Google Cloud, it really starts from the bottom up, at the hardware level. From the second device boots up, the OS, the application, the network, the storage, etc. So if you look more in depth, you get a pretty good idea of all the different areas that we have security.
As you probably are aware, Google has many different data centers which are maintained by us, but we’re also in a lot of third party data centers. When a hardware device boots up, we want to make sure that it is one of our devices. We know what it is, what it’s supposed to do, and then we have actual Google written security codes to monitor that. And that works hand-in-hand with the Titan chip to actually make sure we have that secure level of trust from the second the device is powered-on, to when users can use it when network traffic is going across.
When we look at that, network plays a big part. As you can see here, there are a lot of data center locations; you’ll see all of the lease and owned fiber that Google has. What that really translates to for our users is: No matter where you are in the world, you’re going to be close to a Google data center or Google location that can handle your traffic.
Trey Guinn Cloudflare also has one of the world’s largest networks, although we are focused less on compute and infrastructure as a service, we’re focused on providing a smart network which sits in-between the web visitor or API consumer and the origin infrastructure — which could be GCP. Cloudflare is something you may not have heard of, but you’ve definitely used us if you use the Internet. Cloudflare proxies around 10% of all HTTP requests today. With over six million domains on Cloudflare, we’re a fairly large presence. Our job is to make sure that we stop threats, and try to improve performance as as traffic is flowing through our network.
Asad Baheri When you say 6 million domains, you mean you’re actually managing the DNS records? Whether it’s A records, CNAMEs, AAA zones, all of that?
Trey Guinn Correct. One of the services at Cloudflare is to be an authoritative DNS provider; we’re the world’s largest and fastest authoritative DNS provider. Customers can also CNAME specific subdomains over to us, and we’re handling not only DNS for a lot of these customers, but we’re also handling proxying of HTTP and HTTPS traffic.
Through the Cloudflare CDN Interconnect program, you can see we sit in-between the visitor and the Google Cloud Platform. We try to remove all of the online threats, while at the same time speeding up the communication which goes from the visitor to GCP. The best thing is the direct interconnections between our services; it’s essentially wire between the Cloudflare datacenter and the Google Cloud platform data centers at 53 locations (which is nearly half of the locations). This interconnection has all kinds of advantages because it’s higher performance and you’re not having to worry about congestion or fighting the public Internet. But on top of that, as GCP customer, you pay significantly discounted egress pricing when you’re using CDN interconnect.
One of our many joint customers is our customer LUSH. They had a big migration over to GCP, and they had to do it really quickly. Interestingly enough, they were already a Cloudflare customer before they moved to GCP. Tell me a little bit about their migration to GCP.
Asad Baheri This is something LUSH decided on a Friday, and they had to get started on Monday. They had 22 days to move everything. They had a very short period of time. Part of the reason for their migration was around general availability and scale during the holiday season traffic spikes. The other part of it was that they wanted to have their online presence match their corporate philosophy. A very large percentage of our network electrical costs come from renewable sources, and that was something that was important to them. From technology to philosophy, it was just very well aligned and made sense to make that move quickly. LUSH here had a great amount of savings, correct?
Trey Guinn LUSH as a customer sees about a 75% bandwidth savings, but also a 95% reduction in the number of requests to their origin server. That reduces the amount of infrastructure that they need on the origin, because we’re either filtering out bad traffic or caching content. Part of that, of course, is that we stop about 60,000 threats a month for LUSH. And that’s something to be expected with Cloudflare’s infrastructure and network. We run one of the largest security networks in the world, and are versed in stopping these threats.
Asad Baheri The hit rate seems dependent on the website or API using Cloudflare’s services, correct? If you have a lot of cat pictures versus a super dynamic API, it’s going to be a difference experience.
Trey Guinn Exactly. If you’re serving a bunch of GIFs, you can get your cache hit rate up into the high 90s, maybe over 99%. If you are a weather application, and someone’s checking the weather, your cache hit rate might be a little bit lower because if you’re delivering whether by zip code there’s not that many people per zip code or checking for weather.
Asad Baheri Can I actually set my cache rates by different regions and say “Hey for this geography, I want this cached versus another geography I want to have something else cached?”
Trey Guinn It’s possible to go into deeper levels of customization, but generally the caching is going to be around per URL; you can customize it based on certain headers, cookies etc. And the flip-side of that is you can get fine-grained control around rate limiting, in addition our web application firewall (WAF) and IP reputation database allow you to add security and performance at the same time.
Asad Baheri One of the most important things is actually making sure you have a SSL certificate, that’s following best practices; not all SSL certificates are created equally.
Trey Guinn Independent of whatever origin network or security network that you run, we want to share some security best practices and, as you mentioned, everything should be on SSL. If you didn’t know that, this is your last warning, and everything should be over SSL. Chrome as of next month is going to start positively identifying websites which are not encrypted with an “insecure” flag. If you want to be able to retain the trust of your customers, it’s required. And a key thing is that not all SSL is created equal: some SSL is more secure than others, some SSL is faster than others, etc. We shared a link on on these slides for you to go look at the SSL labs best practices; they’re a third party, but we want to do things like support session resumption and HTTP/2, etc.
Asad Baheri Google also has a series of SSL best practices for anyone who’s using SSL; it’ll walk you through how to create the right type of key requests, where you want to load it, etc. And when using Cloudflare, where does the SSL session terminate?
Trey Guinn Cloudflare will terminate the SSL session at the Cloudflare network edge, and we’ll make sure that you have the best SSL. If you check your SSL grade, you’ll get an A or an A+ with our SSL. We decrypt because we need to be able to protect against layer 7 attacks and look within the application layer. Then we re-encrypt we go back to the origin, so it sits and it’s encrypted as it goes across the network.
Asad Baheri And then also, you can provide protection for DNS services, because that’s another vector of attack where someone can just knock out your DNS or try to inject a bad DNS record.
Trey Guinn Exactly. A lot of things people look at DDoS prevention and security but they forget about the fact that DNS infrastructure is one of the key things that you need to protect. It’s the Internet’s phonebook and if you can find someone’s phone number, you can’t call them and you’re knocked offline. We all number the Dyn attack in October of last year… it took around a third of the internet offline.
Beyond DNS and SSL, other things to be prepared for are large layer 3 and layer 4 floods, this is sort of like UDP floods from DNS amplification — that’s sort of a “caveman with a club”, not very sophisticated, but it fills up your pipes. Luckily, if you’re on GCP, you have very very big pipes. Beyond that though, you also have to worry about layer 7 attacks. Less sophisticated is: What if someone goes in and searches or scrapes the product pages on your ecommerce site, but they just decide to search your product pages 10,000 times a second? Can your application infrastructure handle that? And even if it can scale up to that, do you want to pay to scale up to that? And then beyond that, while those attacks are occurring, you should also be aware of application vulnerabilities. This is where attacks are going to try to extract data using SQL injection, cross-site scripting, etc. So these are the layers that we want to make sure that you’re taking care of.
Asad Baheri So those are the basics of it, but today we’re going to actually show people a demo on how they can set up some of these service and protections on Cloudflare.
Trey Guinn We’re going to jump into the Cloudflare demo, and see how it is that Cloudflare can be configured to sort of meet some of those requirements that we just talked about. This is going to be a live demo, so feel free to play along from home if you’d like. We have a web application; let’s assume we grew from a US-only audience. We had lots of folks in North America, but my business has taken off and it’s doing great, and it’s now distributed out to Australia and the UK.
Trey Guinn Previously, let’s say I had a single origin on AWS. In order to support a geographically distributed origin, I needed the google Spanner database and I’m moving over to GCP. Now I can have an origin within GCP in Australia, U.S. West, and Europe, all at the same time.
Asad Baheri And for those not familiar with BigQuery or Google Spanner, this is really where some of Google’s technologies shine, where you have these globally distributed databases.
Trey Guinn But you know once I’ve added a globally distributed database, I need to be able to access it from everywhere.
So we’re going to set up a geographic load balancing that’ll migrate our traffic over to GCP. We’re also going to look at those layer 7 protections; we’ll set up a rate limiting rule and see that come into effect. We’ll also make sure that Cloudflare’s SSL is working. We’ll turn on our WAF and block SQL injections. And then, a little special thing, like any migration project there’s always that extra person who wasn’t reading their email or didn’t check in at the meetings, and we’re going see how to handle that.
So I’ve already added a domain multicloud.tech to Cloudflare. The signup process takes about five minutes, and most of that is part of the DNS change. And really what you’re doing is making Cloudflare your authoritative DNS provider or you can CNAME specific subdomains. In our DNS infrastructure, we’ve four records but we can see the WWW record is going to the domain apex, the domain apex is going to this IP address, and that’s our AWS origin to start with. In our DNS servers listed, we’ll see the orange cloud and grey cloud. And what is the difference there?
If we do a dig on www.multicloud.tech, we will see that these are a bunch of Cloudflare IPs. And so we see those are the orange clouded record, the WWW. But what I also had created here, just for usability, is this origin. Warning here: You shouldn’t really have things pointing to your origin that are gray clouded, because it allows people to hit your origin directly. Just for demonstration purposes, I’ve this record is grey clouded. If I do a DNS lookup on origin.multicloud.tech, then we’ll see that it’s returning back the origin IP address.
What this is doing is when you orange cloud a record in Cloudflare, it’s routing all of your customers through the Cloudflare network. And if you gray cloud the record, then all traffic is just going to go straight to your origin server. So it’s just a simple sort of “on / off” switch for the Cloudflare network.
So we’re now sending traffic through Cloudflare. So if you went to www.multicloud.tech, wherever you are, you’d be going through the Cloudflare network to hit this website. So if I want to look at multicloud.tech, I’m going through the website and we’re hitting our AWS infrastructure.
So let’s go ahead and set up a load balancer. We’ve got folks in Australia and UK, and we want to make sure it’s fast for everybody. I’m going to create a load balancer here called lb1.multicloud.tech. So when you create a load balancer, it just creates another DNS record that can be used to accept traffic. Now I’m going to create a few origin pools because the idea is that we could have 10 origins in Australia, 30 in North America, and 15 in Europe.
Asad Baheri Am I going to that one DNS record, and then that’s getting sent out to different locations, or am I going to come to multiple DNS records that are just going to do round robin?.
Trey Guinn That’s a great question; so what’s happening is you’re just seeing the Cloudflare IP on the outside. It’s an Anycast network, so it’s the same IP all over the world; you connect to Cloudflare and then this is all happening behind the scenes.
Asad Baheri So, I don’t need to worry about maintaining the DNS records, what I want to do for load balancing, etc… you’ve taken care of all that?
Trey Guinn Exactly. Now we’re going to set up our first origin pool. This origin pool only to have one origin in it, because this is a demo. But we’ll start with the first one in US West; we’re going to go with our GCP origin in US West. And we’re going to do an active health check of just the root page.
Asad Baheri And I can customize that obviously, if I want something more specific?
Trey Guinn Exactly. In that health check, that’s where you can set the number of times it has to fail before it’s unhealthy, and does it check specifically URL, does a check for certain status codes, etc.
So we’ve got our U.S. West set up, so let’s also set up Australia. We’re going to go ahead and set the same monitor, and we’re going to save that.
And then we still need to do our our European origin. Same health check monitor and we’ll hit save. We’ve created three origin pools and, as I was saying, those origin pools could hold more than one origin each.
And if you add multiple origins in a pool, it would round robin between those. Now we have three pools and we can do a clever migration between them. If you had an active / passive data center setup, you could use two pools and put them in the right order so you’d say origin pool #1 / origin pool #2 and then active / passive. In this instance, we’re going to say if, say Europe fails, we want to fail back over the U.S., we’ll make US our primary, Europe secondary globally, and then Australia third.
But we also want to do some geo-routing, so let’s choose which regions we’re going to do some geo-routing.
So we’re going to get Europe, Oceania, The Middle East, Africa, Southern Africa, India, and Asia. So we’re going to take Europe, and we’re going to send that to the European origin. Eastern Europe to European origin. Oceana will go to Australia. Middle East we’ll send to Europe. North Africa we’ll send that to Europe. South Africa let’s send that to Europe. India let’s send to Australia. Northeast Asia we’ll also send that to Australia.
Now we’re going to hit next; and we’re ready to save and deploy.
Asad Baheri That was less than 10 minutes; we actually set up an infrastructure, set up our pools, and set up our geo-routing.
Trey Guinn So we’re doing active health checks and probes to each of these data centers. Now we’re going to say that WWW, which was going to the apex record and went to AWS, will be sent to the load balancer. The other thing I want to do is take the zone apex, multicloud.tech, and I want to send that to the load balancer, as well. This is something else that is special with Cloudflare; if you’ve ever had to CNAME your apex record, it’s a real bugger; we allow you to do that on our infrastructure, because we do a thing called CNAME flattening. So now all the traffic now is going to the load balancer. We’re also going to set up a WAF and setup Rate Limiting rules and we can make all that work together.
Asad Baheri So now we’re just getting into protection; we’ve setup the infrastructure, the routing, and now we’re going to protect it.
Trey Guinn Exactly. I’m running out of time already on my demo, so how hard is it to set up a WAF?
Asad Baheri It’s pretty much an all day process setting up rules, I think?
Trey Guinn So now our WAF is setup; it’s pretty easy. We made this product easy to use. We’ll also turn on the OWASP top 10 ruleset and put that into block mode.
Now that our WAF is fully engaged, let’s set up a rate limiting rule, because you want to be able to stop someone from hammering away at your website. So we’ll just call this rate limiting rule “Global” on http & https * it’ll just say any URL that you’re checking.
Asad Baheri It’s great to have different rulesets on different parts of my website; if I’m managing multiple customers or there’s different regions, I can say “hey I want this part of my site or this subdomain to have one type or protection vs. another part.”
Trey Guinn One of the common use cases is to protect your login page, and we can look at the response code and say if you’re getting 200’s that’s fine, but if you’re getting 401s or 403s, then clearly you’re logging in with the wrong password, so we’re going to really restrict you. So if you make more than 10 requests in 30 seconds, we’re going to block you for 30 seconds.
Asad Baheri And that’s really going to stop those automated bot attacks, where people are trying credential stuffing and they’re just trying to see how fast can I hammer on a login page, through these credential dumps that I’ve gotten from somewhere, and see which ones work.
Trey Guinn Exactly. Before we reload this webpage, it came from AWS. Now, if I do a refresh, it says “Google Cloud Platform”. And I’m coming from US West; if you happen to be watching this from Australia, you’ll see that you’re coming from the Australian data center, if you’re watching this from Europe and hit this website you’d see Europe.
Now let’s check our web application firewall (WAF); I snuck a little trick in here in one of my notes, because I wanted to remember a SQL injection command. So maybe with this command I was trying to dump the customer recordbase, and we’ll see I’ve been blocked by Cloudflare. The WAF is in place and working; that command never even made it to GCP, it never hit your infrastructure.
And in our Cloudflare dashboard is the bad request that came through, and it was just blocked.
And the last thing we need to do test out rate limiting. I’ve setup rate limiting already, so I’m going to do 200 requests against our website. I’ve just curled multicloud.tech and I’m grepping for the HTTP status code that comes back. And it’s blocked.
Like all migrations, there’s always something that comes up; Jerry came to us, and said: “Hey my website stopped working, and I can’t find it anymore; what’s going on?” And if we look, he’s got this awesome old marketing website.
And you know what: We’re not going to migrate it to GCP, because it’s getting killed off in about six months from now. But how can you create a path and route it over to AWS?
I have the legacy AWS origin defined here as “legacyorigin”, and what I’ll do in Cloudflare is create a thing called a “Page Rule”.
And this way, we can take the website www.multicloud.tech/legacy/*, so it takes anything under the legacy path and override the resolution of the origin and resolve it to legacyorigin.multicloud.tech. Now this has actually replicate out globally to a bunch of data centers after hitting it here.
Asad Baheri So in this 10 minutes: We’ve set up pools, we’ve set up rules, we’ve actually kept some of our legacy stuff back where it was, we’ve shown protection against script kiddies and credential stuffing. And all in 15 minutes, which is pretty amazing. Especially for something we can easily take a month.
Originally published at blog.cloudflare.com on October 6, 2017.
Select highlights from blog.cloudflare.com
Written by
The Official Account of Cloudflare
Select highlights from blog.cloudflare.com
Written by
The Official Account of Cloudflare
Select highlights from blog.cloudflare.com
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@hasurahq/authorization-rules-for-a-multi-tenant-system-google-cloud-platform-d9010aca7b51?source=search_post---------219,"Sign in
There are currently no responses for this story.
Be the first to respond.
Hasura
Apr 21, 2020·5 min read
We had earlier covered writing permission rules for some popular use cases. In this post we will look at writing permission rules for a multi-tenant system.
Authorization in a multi-tenant system usually means two things:
If your use case does not require per tenant roles, then have a look at this gist for a simpler implementation. We will loosely base this post on predefined roles in Google cloud platform’s (GCP) role-based access control system. GCP’s role-based access control system is an interesting use case because it is not only multi-tenant, the roles within each tenant are also hierarchical.
In GCP, every resource type has roles associated with it. For example: access to Google cloud storage buckets is controlled using storage_admin, storage_editor, storage_viewer roles. Similarly roles compute_admin, compute_editor, compute_viewer, etc control access to the compute engines.
Roles can be per resource as well. For eg: Each bucket has a storage_admin, storage_editor, storage_viewer role associated with it. So a storage_viewer for a given bucket can access only that bucket but a someone with a global storage_viewer role can view any bucket.
Both global roles and per resource roles are hierarchical i.e a Storage Admin can do anything a Storage Editor can do, etc.
Finally, every role is per project so a user can be storage_admin in one project but be storage_editor in another.
We will assume the following schema:
We have a projects table and a users table to keep track of projects and users in the system. roles table has the list of all roles along with hierarchy information. user_project_roles maps users to their roles in a project and user_bucket_roles maps users to their roles for a given bucket.
We will also assume that we’ve created a one-to-one relationship user_bucket_roles from storage_buckets to user_bucket_roles. Note that keeping with the common convention, the relationship and the target table have the same name.
Note: While GCP has many resources, in the above schema we have only included the storage_buckets table. We will be looking at how to write permission rules for this table. Permission rules for other tables would be similar.
Creating a Storage Bucket in practice will probably involve coordinating between multiple services, allocating space, etc. The goal for this post is to only explain how to implement a system similar to GCP's role-based access control system. As such, we will conveniently ignore these complexities.
Similar to Example 3 in the previous post we will flatten user_project_roles and user_bucket_roles into flattened_user_project_roles and flattened_user_bucket_roles using the gist below. This allows us to not worry about the hierarchy in the roles.
For example if a user is assigned the storage_admin role in the user_project_rules table, flattened_user_project_roles will have 3 rows assigning them: storage_admin, storage_editor and storage_viewer.
Since we have modeled the roles in the database, we will use a single role called user in Hasura to define permissions rules. We will assume that the session variable X-Hasura-User-Id contains the user_id and X-Hasura-Project-Id contains the project that the user is trying to access.
Select permissions:
A user can view a storage_bucket if they have the storage_viewer role on the given bucket or they have the global storage_viewer role.
We can implement the first rule with the following permission rule:
The above JSON uses the _exists operator to check if there is a row in the flattened_user_project_roles with the given role, project_id and user_id.
The second rule can be implemented as:
In the above rule user_bucket_roles is a many-to-many relationship between storage_buckets and user_bucket_roles. Hasura evaluates the rule by fetching user_bucket_roles for the current row and validating that user_id and role_id are equal to the given values.
We can now put the two in an _or clause for the final rule:
Update permissions:
Update permissions would look the same as the select permissions except we would be using the role_id storage_editor instead of storage_viewer
We also we need to prevent the user from updating the project_id column of a bucket:
Delete permissions:
Delete permissions would again look the same as the select permissions except we would be using the role_id storage_admin instead of storage_viewer
Insert permissions:
Only a user with a storage_admin role should be able to create a bucket:
We also want the project_id of the created bucket to be X-Hasura-Project-Id. We can do this using column presets:
We also need rules that allow certain users to assign or remove the roles of other users. We can have roles role_admin & role_viewer for being able to edit and view user roles respectively. Permission rules on user_project_roles and user_bucket_roles would then look similar to the insert permission rule on storage_buckets with the role name changed.
In this post we’ve seen how to implement permission rules for a full-fledged hierarchical multi-tenant system. Postgres Views and Hasura’s Permission DSL make a rather powerful combination!
If you are using Hasura and need help with authorization, or want to share some interesting use cases you have implemented, ping us on Discord or tweet to us at @HasuraHQ!
Enjoyed this article? Join us on Discord for more discussions on Hasura & GraphQL!
Sign up for our newsletter to know when we publish new articles.
Originally published at https://hasura.io on April 21, 2020.
⚡️ Instant realtime GraphQL APIs! Connect Hasura to your database & data sources (GraphQL, REST & 3rd party API) and get a unified data access layer instantly.
⚡️ Instant realtime GraphQL APIs! Connect Hasura to your database & data sources (GraphQL, REST & 3rd party API) and get a unified data access layer instantly.
"
https://medium.com/terri-hanson-mead/google-cloud-platform-gcp-in-life-sciences-qualifying-the-cloud-iaas-paas-6059824bed29?source=search_post---------220,"There are currently no responses for this story.
Be the first to respond.
As life sciences companies, especially biotech companies, become more and more virtual, they are shifting (or have shifted) to the cloud for applications and infrastructure. It’s the right thing to do, especially since most don’t appreciate the value of technology in optimizing business performance and getting to market faster (separate rant).
Simply throwing the responsibility over the fence to the vendors does not relieve a life sciences company of its obligations around ensuring data quality and integrity.
Savvy life sciences companies know this and qualify their underlying cloud infrastructure, whether on AWS, Google Cloud Platform (GCP), or Microsoft Azure.
The qualification process is more than a compliance activity; it can provide value to the business in demonstrating system and process controls, and ensure data integrity. It’s all about the data, after all.
In this blog post, I share my recommendations for qualifying Google Cloud Platform. I covered the same for Microsoft Azure in a separate post.
For the purposes of this post, I am focusing on IaaS and PaaS, and not SaaS. For validating a SaaS solution, check out this post from 2018 and this post from 2019.
Qualification versus Validation
In this post I will use the terms synonymously but generally speaking, you qualify infrastructure and you validate applications for a company’s intended use.
ISPE GAMP5
I am a GAMP girl so my qualification approach is based on the International Society of Professional Engineer’s guidance documents as detailed in GAMP5 and the associated guidance documents.
CSV versus CSA
FDA released its draft guidance on Computer Software Assurance for Manufacturing, Operations, and Quality System Software and while it was expected to be finalized in 2020, it is now on the docket for fiscal year 2021 per FDA’s Center for Devices and Radiological Health (CDRH).
The guidance document is intended to shift validation approaches from CSV (computer system validation) to CSA (computer software assurance). If you’ve been following a risk-based approach (as defined in GAMP5) and have been using your noggin to focus on intended use and the value add to the business, there’s not much of a shift.
Risky Business
Two years ago I qualified GCP for a client and recently followed a similar approach for Azure for another client this past year. The approaches to both are nearly identical.
It may appear to be largely a documentation effort but it’s more than that. It’s all about defining what is needed for the business, executing against those requirements, documenting the baseline, and maintaining the platform in a controlled fashion.
With IaaS and PaaS (and SaaS), there is a lot of reliance on the vendor, and therefore the customer (the life sciences company) needs to oversee the vendor and the platform to ensure an acceptable level of control based on the company’s risk assessment and risk tolerance. To not do so is a risky business and compliance proposition.
Risk-Based Approach
A risk-based approach was the best thing to happen to computer systems and software when GAMP5 was released. It meant that we could prioritize what we focused on, company by company, system by system, process by process.
This also meant that those folks who liked to apply a cookie-cutter approach to computer validation had to start applying critical thinking to their validation / qualification efforts. It’s not easy if you just want to check boxes and follow checklists.
A risk-based approach offers up flexibility for companies leveraging software and technology. It is not a one-size fits all approach.
This isn’t rocket science but if you have not been through a qualification or validation effort, this will be challenging as there’s a steep learning curve. I have yet to figure out how to get my clients to understand this process until after they’ve gone through it. Expect some discomfort as you go through it for the very first time.
What is Google Cloud Platform (GCP)?
The Google Cloud Platform (GCP) is a suite of clud computing services providing infrastructure as a service (IaaS), platform as a service (PaaS) and serverless computing environments in Google managed data centers. This includes computing, data storage, data analytics, application deployment, and machine learning.
The companies I work with rely on it as the virtual backbone for virtual machines, storage, and other GCP assets to support business applications, databases, and web deployment.
Above and Below the Line
I draw the line at the compute instances, containers, management tools, security, and storage. Anything above that line is at the application level and should be covered under those application validation plans and activities. Below the line is in scope for the GCP platform.
If we think in terms of a bare metal data center, IT historically commissioned the servers and prepared for application and database installation.
This typically included installing things like the operating system, anti-malware software/tools, monitoring tools, backup software or agents, and other baseline tools used by IT. This is all below the line and what I consider in-scope for the qualification of the GCP platform.
Then it’s handed over to the folks managing the application project. What they do, even with the help of IT, is above the line.
Deliverables
Remember, if it’s not documented, it didn’t happen. I highly recommend that all of the validation deliverables be approved and stored in the company’s validated document management system (eDMS).
— GxP assessment for the GCP platform based on its intended use and what is expected to reside on the platform
— Risk assessment for the GCP platform and determination as to whether to audit Google for GCP
— Vendor qualification asessment and possible (paper) audit
— Validation plan for the GCP platform
— Functional requirement specification (combined FRS/FRA/TMX document)
— Functional risk assessment (combined FRS/FRA/TMX document) for the GCP platform
— Technical specification documents for the platform and the virtual machines and other GCP assets on the platform; these provide baseline documentation for operation and control and are created after the migration or the platform / virtual machines / other assets are configured or installed.
— Migration plan or protocol including verification activities
— Executed migration and supporting documentation
— Migration plan summary report
— Testing summary to document any verification activities performed whether automated or manual
— Traceability matrix (combined FRS/FRA/TMX document)
— Procedures (SOPs) and work instructions (WIs) to operate and maintain the GCP platform in a controlled fashion
— Trained administrators on the platform and SOPs/WIs
— Trained personnel on the SOPs/WIs
— Validation plan final report
What is not on the list?
— Installation Qualfication (IQ): there’s nothing to install and therefore nothing to verify
— Operational Qualification (OQ): if you are focusing on qualifying for intended use, performing an OQ provides no added value
— Performance Qualification (PQ): we can debate whether this is necessary or not. My clients have decided not to do any verification testing on the platforms (Azure / GCP) and have left the verification testing to the application layer. Or they have performed automated and manual testing and have summarized in a document that they approve and store with the validation deliverables.
Procedures and Work Instructions
Assuming the SOPs are in place to support validated systems (computer validation, SDLC, change management, backup/restoration, deviation management, training, security/passwords, monitoring, etc.), the following should be considered at a minimum:
— GCP Platform operation and maintenance procedure including monitoring
— Work instructions to support the platform administration including administration of virtual machines and other GCP assets, backup and restoration, user and security administration, etc.
The timeline of the activities will depend on where/what you are migrating from (assuming you are), what is being migrated, the impact to the business, and the available resources to work on the configuration, migration, and qualification project.
So, it depends.
Some words of wisdom based on my experience with these qualification and migration projects:
— Work with an experienced and well-referenced GCP vendor (assuming you are working with a vendor) who understands CSV/CSA
— Work with a GCP vendor who will detail their activities and assumptions in their statement of work before you start the project. Trust me on this one.
— Create the timeline of activities and include the qualification tasks in a single project / project plan. The vendor will only care about their portion but there’s a lot more to it than the technical activities.
— Train the team on what to expect with the qualification activities and the post-migration activities. It won’t make 100% sense at first but it will at the end.
— Have the SOPs/WIs in place prior to the migration and the production cutover.
— Be flexibile. Things will not go 100% to plan. Apply critical thinking to address issues as they arise and continue to focus on the goal(s) of the qualification effort.
Qualification and validation do not end after the project is over. It starts at the beginning with a kernel of an idea and goes through to retirement.
Don’t forget to operate and maintain in a controlled fashion your beautifully qualified GCP platform.
Still Have Questions or Need Some Help?
Feel free to reach out to me with any questions you might have via email at terri.mead@solutions2projects.com or through my website SolutionsProjects, LLC. I’d be happy to have an initial call, free of charge, to discuss your qualification project.
Related article: Solutions2Projects, LLC
Living my best life…one day at a time
Written by
IT consultant, expert witness, YouTuber, helicopter pilot. Making the world a better place, especially for women. Award winning author of Piloting Your Life.
I want to live in a world where everyone has the opportunity to live freely, equally and have an extraordinary life. #PilotingYourLife #Angel Investing #Digital Health #Sol2Proj #Womanism #Tipsy
Written by
IT consultant, expert witness, YouTuber, helicopter pilot. Making the world a better place, especially for women. Award winning author of Piloting Your Life.
I want to live in a world where everyone has the opportunity to live freely, equally and have an extraordinary life. #PilotingYourLife #Angel Investing #Digital Health #Sol2Proj #Womanism #Tipsy
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@adamhurwitz/following-up-regarding-running-with-google-cloud-platform-gcp-ive-generated-a-jar-file-that-93f70ba60612?source=search_post---------221,"Sign in
There are currently no responses for this story.
Be the first to respond.
Adam Hurwitz
May 16, 2018·1 min read
Adam Hurwitz
Following up regarding running with Google Cloud Platform (GCP), I’ve generated a JAR file that runs successfully with IntelliJ, however I am running into issues with the deploy to my AppEngine instance.
I’m debugging with the GCP team here: https://github.com/GoogleCloudPlatform/google-cloud-java/issues/3275.
In the meantime, I’ll see if it is feasible to simply host my working JAR on Digital Ocean.
DeFi Punk
DeFi Punk
"
https://medium.com/@DazWilkin/google-cloud-platform-evolves-continuously-1257099f3a8a?source=search_post---------223,"Sign in
There are currently no responses for this story.
Be the first to respond.
Daz Wilkin
Jul 22, 2018·1 min read
Myles
Google Cloud Platform evolves continuously. Please always refer to Google’s documentation for the most current information:
https://cloud.google.com/functions/
Cloud Storage is synonymous with Cloud Storage Bucket for the trigger. That’s the correct option. You’ll need to select Finalize/Create for the Event Type too.
It appears the staging process has (appropriately) been made transparent to the developer so you needn’t worry about that.
Any zipped file you then gsutil cp to the Bucket you specify (during deployment and in the index.js), should be unzipped to the bucket you specified in the index.js.
2
2
"
https://medium.com/@pingcap/how-to-deploy-tidb-on-google-cloud-platform-part-2-bccdf020f109?source=search_post---------224,"Sign in
There are currently no responses for this story.
Be the first to respond.
PingCAP
Sep 17, 2021·10 min read
Author: Mike Barlow (Solution Architect at PingCAP)
Editors: Tina Yang, Tom Dewan, Caitin Chen
Welcome to the second of a two-part blog series on getting a simple TiDB cluster up and running on Google Cloud Platform. The goal is to let you quickly set up TiDB and become familiar with its benefits. In Part 1, we created a GCP instance. Here in Part 2, we will install, configure, and run TiDB on that instance.
By the end of this article, your architecture may look similar to the following:
In Part 1, we focused on the Prerequisite Operations of setting up and configuring GCP in order to run TiDB. (See the image below.) Now, we will actually set up and run TiDB.
In Part 1 we had set up the following:
If you have not yet set these up, or are having any issues, please refer to Part 1. Also, your local computer will need to be a MacOS or Linux operating system. First, let’s check that things are set up correctly.
If you haven’t securely logged into your GCP instance, do so now:
Notice that the server we SSH into has a prompt that references the tidb-vm instance.
Let’s check the account that gcloud is associated with:
Once you see your registered email address, that confirms that you are running under the correct user account.
Let’s check which SSH keys we have available:
The primary file we are interested in is the google_compute_engine, which is the private key that TiUP will use for SSH.
We should have a firewall rule that grants our local computer access to TiDB components (TiDB Dashboard, Grafana, Prometheus, and the SQL interface). Let’s check the rules with the following command:
In Part 1, we had created the firewall rule access-from-home. GCP automatically created the other firewall rules. This should provide us a comfort level that the GCP instance is set up correctly to run TiDB.
TiUP is a package manager that makes it easier to manage different cluster components in the TiDB ecosystem.
TiUP references:
We don’t want to accidentally install TiUP and TiDB on our local computer, so let’s confirm that we are on the GCP instance. In the image below, notice that the command prompt prefix includes tidb-vm. This lets us know that we are on the GCP instance.
Next, let’s install TiUP:
Notice that the .bashrc file was updated to include the path to the TiUP working directory that was created in our home directory under .tiup/.
Since the path has been updated in the .bashrc, we need to reload it by running the following command, which will not display any output:
As a sanity check, let’s make sure that TiUP is installed:
Notice the output for the tiup cluster list command. Here we can see that TiUP has been installed, and there are currently no TiDB clusters running. Your version of TiUP will probably be different.
Create a TiDB topology configuration file
We use TiUP to deploy and scale the TiDB ecosystem. A topology file is a configuration file that identifies the components that will be deployed and on which server they will be deployed.
Let’s create the topology.yaml file. This topology file is fairly simple and tells TiUP to do the following:
· Placement Driver (PD)
· TiDB
· TiKV
· Monitoring Server
· Grafana Server
Copy and paste the following into the topology.yaml file:
Note that all the IP addresses are 127.0.0.1 (localhost). We are installing one instance of each component on the same computer (localhost). We do this on purpose to create an environment to test the different components.
To deploy a TiDB cluster, we use the TiUP command tiup cluster. Below is a check argument that does dry run to validate the topology.yaml file and determine whether ssh access is sufficient.
In Part 1, we created the Google private key for SSH. We will use this private key in the command below with the parameter --identity_file.
We include a reference the topology.yaml file:
Since we are doing a quick test, don’t worry about the output of Pass, Fail, and Warn status.
You want to see output similar to the following:
If things didn’t work correctly, you may see certain error messages, as in the results below. Notice that on the command line I misspelled the private key name. TiUP couldn’t find it and raised an error. If you do see this error, please reference creating SSH Keys in Part 1.
In the previous section we did a dry run. Now, let’s use TiUP to deploy TiDB.
For a deployment, we need to add a few additional parameters. The main parameters are identified below:
Here’s a breakdown of the command:
You’ll see a prompt “Do you want to continue? [y/N]”. Type y to continue.
We successfully deployed a TiDB cluster, but have not yet started the cluster.
Let’s do a sanity check and confirm which cluster is being managed by TiUP:
The output should include the cluster name, path, and private key.
We should see only the one cluster that we deployed.
Let’s see the details of the tidb-test cluster:
We haven’t started TiDB and its components, so it is offline with the statuses of inactive, down, and N/A. Before we start TiDB, I like to see which service ports are open on the GCP instance. These ports are assigned to processes that run on the GCP instance, and are different from the GCP firewall rules that we had created. After we start TiDB, we will run this command again and see which ports are associated with the different TiDB processes.
By default, Google GCP instances have assigned these ports to these processes:
Let’s start the TiDB ecosystem using tiup cluster:
There’s a lot going on here, but the key is that there are no errors being shown.
As a sanity check, let’s see the details of the tidb-test cluster using the display parameter:
As we can see, all the components are up and running.
Let’s see which ports and services are available:
There are many processes running that have TCP ports associated with them. In the image above, I’ve highlighted the ports that we opened with the GCP firewall rule.
In this section we will access the following components from a browser on our local computer:
Also, we will use a local SQL client to access TiDB and run a few SQL commands. To access the TiDB components that are running on our GCP instance from our local computer, we will need the external IP address of our GCP instance:
Here we can see that the GCP instance’s external IP address is 34.83.139.90. (Your IP address will be different.) We will use this IP address from our browser on our local computer.
In a browser on your local computer, provide the URL that starts with your GCP instance IP address, port number 2379, and, at the end, add /dashboard. My URL is http://34.83.139.90:2379/dashboard. The IP address for your GCP instance will be different.
You shouldn’t need to login, so go ahead and click the “Sign In” button:
Here’s the TiDB Dashboard. To learn more about the dashboard, see our online documentation.
To access Grafana, create a URL and use your GCP instance’s public IP address with port 3000. My URL is http://34.83.139.90:3000/. Your IP address will be different.
Login to Grafana using admin for both the username and password.
You may be prompted to change your password. I selected “Skip” since this is a temporary system that only I have access to with my local computer.
The initial Grafana Dashboard should look something like this:
To access Prometheus, create a URL and use your GCP instance’s public IP address with port 9090. For me, the URL will be http://34.83.139.90:9090. Your IP address will be different.
You should not need a user ID or password.
In Part 1, we installed a MySQL Client on the GCP Instance. Now, let’s use the MySQL Client to connect to TiDB.
TiDB SQL interface default port is 4000.
From our GCP instance, run the following command. This command will start a MySQL Client and connect to 127.0.0.1 (localhost) on port 4000 with the username root:
Now we should have a mysql> prompt. Let's run a few commands:
If you already have a MySQL Client or other tool, you can use it to access TiDB over port 4000. Just remember to use the GCP instance public IP address.
Congratulations! You have gotten a simple TiDB cluster up and running. Now, you can access monitoring tools that include TiDB Dashboard, Grafana, and Prometheus.
You now have a foundation to learn more about TiDB and try things out.
If you are done with the GCP instance, you can tear it down by following the steps below.
The easiest way to tear down this environment is by deleting the GCP instance.
From our local computer, let’s get a list of GCP instances. We will then destroy (delete) the tidb-vm instance and then confirm the instance has been deleted.
From your local computer, run the following gcloud commands.
The delete command may take a few minutes to complete.
Let’s remove the firewall rule that we created. In the commands below, we first get a list of firewall rules. Then, we delete the firewall rule access-from-home, and then validate that the rule was deleted.
Your GCP environment should be cleaned up and back to its original state.
Originally published at www.pingcap.com on Aug 4, 2021
PingCAP is the team behind TiDB, an open-source MySQL compatible NewSQL database. Official website: https://pingcap.com/ GitHub: https://github.com/pingcap
See all (22)
PingCAP is the team behind TiDB, an open-source MySQL compatible NewSQL database. Official website: https://pingcap.com/ GitHub: https://github.com/pingcap
About
Write
Help
Legal
Get the Medium app
"
https://rominirani.com/google-cloud-platform-roadshow-india-talks-experience-a127783e6ec4?source=search_post---------225,"March 25, 2014 is a day to remember. A day where Google broke through the ranks with a series of announcements that clearly demonstrated that there was no need to play catch up in the Cloud Platform wars. Instead, how about leapfrogging the competition on multiple fronts? It did exactly that. Check out the Cloud Platform Live Event.
Google India announced the Cloud Platform Roadshow across 5 Indian metros through April — May 2014. The series kicked off in Hyderabad on April 12 and since then has moved across to Chennai, Bengaluru, Mumbai and is set for Delhi this week.
The day-long event had talks on:
While I did present at these events, my personal favorites were the Compute Engine and BigQuery sessions, where I learnt a lot. I could not help but hear the wow’s of the audience when they saw what BigQuery can do. Janakiram clearly rocked the audience with his Compute Engine sessions. Googler’s Rohit and Anudeep are all set to make their future talks “must attend” with their ‘jugalbandi’ (fusion or beautiful mix) in presenting technical topics.
If its been a while you have taken a look at the Google Cloud Platform, you owe it to yourself to learn about the various services that are now under a single Cloud Platform umbrella and present a more unified and complete solution.
I was delighted to present the App Engine session during these roadshows and I focused on Google Cloud Endpoints, which provides a service to create RESTful APIs powered by the App Engine platform. If you are looking to scale up your APIs and power it with Google’s infrastructure, Endpoints is the way to go. You can check out my slides.
The number of queries and feedback from the participants was excellent. Some of the questions are solid enough for me to go back and think more on how it can be addressed. It was very encouraging to learn from some of them that they were excited enough to try things out soon and learn more about the platform.
As of writing, the roadshow makes its last stop at Delhi on May 31. If you live in and around Delhi and want to learn about the Google Cloud Platform, come and spend May 31st to get vowed with multiple product demonstrations. Click that Registration link.
Technical Tutorials, APIs, Cloud, Books and more.
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
Technical Tutorials, APIs, Cloud, Books and more.
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
Technical Tutorials, APIs, Cloud, Books and more.
"
https://rominirani.com/google-cloud-platform-updates-articles-3302aa318329?source=search_post---------226,"Over the last couple of weeks I have covered the news around several features that have been released on Google Cloud Platform (GCP). I write these articles on ProgrammableWeb. Here are the list of articles:
The articles demonstrate the recent spate of feature releases on GCP. They are targeted squarely towards the developers/organizations and helping them get more productive, write secure apps and finally get a shot at a messaging service in GCP. Not to speak of Google Cloud Storage Nearline that changes the game when it comes to cold data storage.
Technical Tutorials, APIs, Cloud, Books and more.
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
Technical Tutorials, APIs, Cloud, Books and more.
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
Technical Tutorials, APIs, Cloud, Books and more.
"
https://medium.com/@knoldus/overview-of-google-cloud-platform-gcp-with-cloud-run-fd18a533e00d?source=search_post---------227,"Sign in
Knoldus Inc.
Jun 22, 2020·4 min read
In this blog, we are going to discuss deploying stateless containers on Google Cloud Platform(GCP) with Cloud Run service. At very first we see What is Google Cloud Platform(GCP)? Then we see what is cloud run and how to deploy containers using Cloud Run.
There are some prerequisites, you need to know for deploying containers on Google Cloud Platform with Cloud Run Service.
So, now let’s start with the introduction to Google Cloud Platform
Google Cloud Platform is a set of servers that are accessible all over the internet along with the set of databases that run on those servers.
Cloud servers are located in the data center all over the world.
Google cloud platform enables developers to build, test, and deploy the application on highly scalable and reliable infrastructure.
It is a collection of Google’s computing resources, are available via services to the general public as a public cloud offering.
Google cloud platform provides some physical assets like computers, hard disk drives, and virtual resources, such as virtual machines (VMs), that are contained in Google’s data centers.
The different resources of GCP are categorizing as follows
Global Resources are the resources that are accessible across any region and any Zone. Under the Global resources of Google Cloud Platform Networks comes
Regional Resources are the resources that are accessible across any zone within the same region. Under the regional resources, the static external IP address comes
Zonal Resources are the resources that are accessible by resources within the same zone. These zonal resources include VM instances, their types, and disks.
There are the different services provided by a google cloud platform, But in this tutorial, we see about the “Cloud Run” Service.
In this blog, we have gone build our own containers and deploy it to the cloud run. Follow the following steps
First, go to the directory where your project is and then run a command on the terminal. At the place of PROJECT-ID specify the id of your project created previously. Or by running the below command you can also get project-id.
And the place of Image-Name you have gives image-name that you want.
When you use 1'st approach to build container you don’t need to push the container to GCR but if you are using the 2'nd approach you need to push content that is built locally to GCR. And for that, you just need to configure docker in the gcloud as follows
Through the cloud console.
For more information, if you are creating a container using the jar artifact, then you can refer to creating executable jar artifact.
Group of smart Engineers with a Product mindset who partner with your business to drive competitive advantage | www.knoldus.com
Group of smart Engineers with a Product mindset who partner with your business to drive competitive advantage | www.knoldus.com
"
https://rominirani.com/google-cloud-platform-updates-articles-march-april-2015-b96271b08336?source=search_post---------228,"I have covered several feature/service announcements on Google Cloud Platform, Android, Web via short blog posts. These posts are published on ProgrammableWeb. These posts covers news on March — April 2015 timeline.
Technical Tutorials, APIs, Cloud, Books and more.
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
Technical Tutorials, APIs, Cloud, Books and more.
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
Technical Tutorials, APIs, Cloud, Books and more.
"
https://medium.com/@jaychapel/parkmycloud-announces-smartparking-for-google-cloud-platform-1bd8fdd317fe?source=search_post---------229,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Apr 3, 2018·2 min read
Example of an automatically-generated SmartParking recommendation based on instance usage patterns.
April 3, 2018 (Dulles, VA) — ParkMyCloud, the leading enterprise platform for continuous cost control in public cloud, announced today that it has released SmartParkingTM for Google Cloud Platform (GCP), allowing GCP users to automate cloud cost optimization.
The ParkMyCloud platform helps customers of the “big three” cloud providers, Amazon Web Services (AWS), Microsoft Azure, and GCP, save money on cloud resources by automatically integrating cost control into their DevOps processes. ParkMyCloud does this by scheduling cloud resources to turn off when they are not needed — which they call “parking”.
This expansion of the SmartParking functionality to Google Cloud Platform comes on the heels of the release of SmartParking for Microsoft Azure in March. It uses Google Cloud Platform metric data to find patterns in instance utilization. ParkMyCloud then uses that data to automatically recommend specific parking schedules for each instance, designed to turn them off when they are typically idle. This maximizes savings on cloud resources by shutting the instances down when they are not being used. Using ParkMyCloud’s automated cost-saving technology, individual AWS and Azure customers have saved over $1 million on their cloud bills. Now, Google Cloud users, too, can eliminate wasted spend.
“We are consistently saving $15,000–25,000 a month using ParkMyCloud,” said Bill Gullicksen, Director of IT, QCentive. “It’s the perfect tool for what we needed, and in addition to the automation it also lets us provide access to developers, QA testers, and even data analysts and business folks to turn instances on and off without having to write a bunch of complex policies ”
“We’ve gotten great feedback on SmartParking from our AWS and Azure customers,” said ParkMyCloud CTO Bill Supernor. “Our Google customers have been looking forward to the same optimization opportunities, and we’re happy to deliver.”
Now that SmartParking has been achieved for all three major cloud providers, ParkMyCloud plans to develop cost-savings measures for other services within each cloud provider, and to expand to additional cloud providers.
Originally published at www.parkmycloud.com on April 3, 2018.
CEO of ParkMyCloud
See all (317)
CEO of ParkMyCloud
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@jaychapel/announcing-smartparking-for-google-cloud-platform-automated-custom-on-off-schedules-based-on-gcp-15a5dc801417?source=search_post---------230,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Apr 3, 2018·3 min read
Today we’re excited to announce the latest cloud provider compatible with ParkMyCloud’s SmartParkingTM — Google Cloud Platform! In addition to AWS and Azure, Google users will now benefit from the use of SmartParking to get automatic, custom on/off schedules for cloud resources based actual usage metrics.
The method is simple: ParkMyCloud will import GCP metric data to look for usage patterns for your GCP virtual machine instances. With your utilization data, ParkMyCloud creates recommended schedules for each instance to turn off when they are typically idle, eliminating potential cloud waste and saving you money on your Google Cloud bill every month. You will no longer have to go through the process of creating your own schedule or manually shutting your VMs off — unless you want to. SmartParking automates the scheduling for you, minimizing idle time and cutting costs in the process.
SmartParking’s benefits are not “one-size-fits-all.” The recommended schedules can be customized like an investment portfolio — choose between “conservative”, “balanced”, or “aggressive” based on your preferences.
And like an investment, a bigger risk comes with a bigger reward. When receiving recommendations based on your GCP metric data, you’ll have the power to decide which of the custom schedules is best for you. If you’re going for maximum savings, aggressive SmartParking is your best bet since you’ll be parked most of the time, but with a small “risk” of occasionally finding an instance parked when needed. But in the event that this does happen — no fear! You can still use ParkMyCloud’s “snooze button” to override the schedule and get the instance turned back on — and you can give your team governed access to do the same.
If you’d rather completely avoid having your instances shut off when needed, you can opt for a conservative schedule. Conservative SmartParking only recommends a parking schedule during times that instances are never used, ensuring that you won’t miss a beat when it comes to having instances off during any given time that you’ve ever used them.
If you’re worried about the risk of aggressive parking for maximum savings, but want more opportunities to save than conservative schedules will give you, then a “balanced” SmartParking schedule is a happy medium.
Since ParkMyCloud debuted SmartParking in January for AWS, adding Azure in March, customers have given positive feedback to the new functionality:
“ParkMyCloud has helped my team save so much on our AWS bill already, and SmartParking will make it even easier,” said Tosin Ojediran, DevOps Engineer at a FinTech company. “The automatic schedules will save us time and make sure our instances are never running when they don’t need to be.”
ParkMyCloud customer Sysco Foods has more than 500 users across 50 teams using ParkMyCloud to manage their AWS environments. “When I’m asked by a team how they should use the tool, they’re exceedingly happy that they can go in and see when systems are idle,” Kurt Brochu, Sysco Foods’ Senior Manager of the Cloud Enablement Team, said of SmartParking. “To me, the magic is that the platform empowers the end user to make decisions for the betterment of the business.”
Not yet a ParkMyCloud user? Start a free trial here.
Originally published at www.parkmycloud.com on April 3, 2018.
CEO of ParkMyCloud
CEO of ParkMyCloud
"
https://medium.com/@viveknaskar/i-have-been-working-and-deploying-my-applications-in-google-cloud-platform-6dc3e07742dd?source=search_post---------231,"Sign in
There are currently no responses for this story.
Be the first to respond.
Vivek Naskar
·Jun 11, 2021
Weihe Wang
I have been working and deploying my applications in Google Cloud Platform for a while now. I am guilty of forgetting his contributions on BigTable, Spanner, MapReduce, Tensorflow and many others (Yes, I have researched these services when I was working on them). I have used these services at my workplace. His contributions have been immense over the years.
Thank you for highlighting and suggesting him (and I have added him). 🙂
A software developer by the day and a writer by the night! | Explore more articles by being a member here → https://viveknaskar.medium.com/membership
"
https://medium.com/@knoldus/google-cloud-platform-overview-d40754841d9e?source=search_post---------232,"Sign in
There are currently no responses for this story.
Be the first to respond.
Knoldus Inc.
Dec 31, 2021·7 min read
As all of you might know, Cloud services can be broadly classified into three categories:
IaaS(Infrastructure as a Service), PaaS(Platform as a Service), and SaaS(Software as a Service).
GCP (Google Cloud Platform) is an offering by Google that provides IaaS and PaaS services. SaaS services are also provided by Google in the form of G Suite(Google Workspace). Both G Suite and GCP as well as all other products from Google like Google Search, YouTube, etc. use the same Google infrastructure.
GCP provides several IaaS and PaaS services. We will go through some of the widely used services and understand them briefly. But before starting with the services let’s take a look at Regions and Zones.
Google Cloud resources are hosted in multiple locations worldwide. These locations are composed of zones and regions.
A region is a specific geographical location where you can host your resources. For eg. us-west1, europe-north, asia-south1 etc.
Zones represent a specific area within a region. Each region consists of three or more zones. For example, the us-west1 region denotes a region on the west coast of the United States that has three zones: us-west1-a, us-west1-b, and us-west1-c.
Now each Zone has one or more discrete clusters. Here, a cluster is a distinct physical infrastructure that is housed in a data center.
Google provides a lot of services under GCP and we will take a look at some of them in brief. We can broadly classify the services into 4 categories:
Google Compute Engine-
Google Compute Engine is a secure and customizable compute service that lets you create and run virtual machine instances on Google’s infrastructure. It offers highly customizable and secure virtual machines which can handle any type of workload. Choose from the wide varieties of VMs optimized for Scale-out workloads (T2D), General purpose workloads (E2, N2, N2D, N1), Ultra-high memory (M2, M1), Compute-intensive workloads (C2), Most demanding applications and workloads (A2).
Google Kubernetes Engine-
Run containerized applications on a secured and managed Kubernetes service. It is a simple way to automatically deploy, scale, and manage Kubernetes. Now, it also comes with Autopilot mode(a hands-off, fully managed solution that manages your entire cluster’s infrastructure without worrying about configuring and monitoring).
Google App Engine-
Build highly scalable applications on a fully managed serverless platform. Custom runtimes allow you to bring any library and framework to App Engine by supplying a Docker container. Easily host different versions of your app, and easily create development, test, staging, and production environments. It lets you focus on your code while the fully managed environment of App Engine manages the infrastructure concerns.
Cloud Functions-
Scalable pay-as-you-go functions as a service (FaaS) to run your code with zero server management. Run lightweight single-purpose functions on Google Cloud Functions. It is basically the lite version of App Engine.
Cloud Run-
Develop and deploy highly scalable containerized applications on a fully managed serverless platform. This is the lite version of Google Kubernetes Engine. It is a fully managed compute environment for deploying and scaling serverless containerized microservices without worrying about provisioning machines, configuring clusters, or autoscaling.
Cloud Bigtable-
A fully managed, scalable NoSQL database service for large analytical and operational workloads with up to 99.999% availability. It handles millions of requests per second and provides consistent sub 10ms latency. It can be used for ad tech, fintech, digital media, IoT, etc. It offers seamless zero downtime scaling. It is designed with a storage engine for machine learning applications leading to better predictions.
Cloud Storage-
Object storage for companies of all sizes. Store any amount of data. Retrieve it as often as you’d like. It offers seamless data transfer into Cloud Storage using Storage Transfer Service and Transfer Service for on-premises data. Save costs without sacrificing performance by storing data across different storage classes. Transition to lower-cost classes automatically using Object Lifecycle Management (OLM).
Cloud SQL-
Cloud SQL is a service that delivers fully managed SQL databases in the cloud. Cloud SQL provides PostgreSQL, SQL Server, and MySQL databases. Cloud SQL does the time-consuming yet necessary tasks like applying patches and updates, managing backups, and configuring replications for you so that you can focus on building your applications. Cloud SQL uses standard wire protocols, hence you can connect from just about any application, anywhere.
Cloud Spanner-
Fully managed relational database with unlimited scale, strong consistency, and up to 99.999% availability. It is like the big brother of Cloud SQL. Cloud Spanner features all the benefits of relational semantics and SQL along with unlimited scaling capabilities. It provides high availability with zero scheduled downtime and online schema changes.
FireStore-
Easily develop rich applications using a fully managed, scalable, and serverless document database. It can scale effortlessly to meet any demand, with no maintenance. Using FireStore you can accelerate the development of mobile, web, and IoT apps with direct connectivity to the database. It also features built-in live synchronization and offline mode which makes it easy to develop real-time applications, fully customizable security and data validation rules to ensure the data is always protected.
It can integrate with Firebase and Google Cloud services like Cloud Functions and BigQuery
Big Query-
BigQuery is Google Cloud’s fully managed, petabyte-scale, and cost-effective analytics data warehouse that lets you run analytics over vast amounts of data in near real-time. With BigQuery, there’s no infrastructure to set up or manage, letting you focus on finding meaningful insights using standard SQL and taking advantage of flexible pricing models across on-demand and flat-rate options.
Pub/Sub-
A fast, reliable way to land small records at any volume, an entry point for real-time and batch pipelines feeding BigQuery, data lakes and operational databases. Use it with ETL/ELT pipelines in Dataflow. It is a secure, scalable messaging or queue system. It features In-order and any-order at-least-once message delivery with pull and push modes. Secure data with fine-grained access controls and always-on encryption.
Data Flow-
Dataflow is a fully managed, serverless, reliable service for running Apache Beam pipelines at scale on Google Cloud. Dataflow is used to scale processing of the input text and the extractions of the embeddings to store them in BigQuery. It provides unified stream and batch data processing that’s serverless, fast, and cost-effective. It is a fully managed data processing service. Supports automated provisioning and management of processing resources, horizontal autoscaling of worker resources to maximize resource utilization.
It is an OSS community-driven innovation with Apache Beam SDK.
Data Catalog-
A fully managed and highly scalable data discovery and metadata management service. Pinpoint your data with a simple yet powerful search interface. Using Data Catalog you can sync technical metadata automatically and create schematized tags for business metadata.
Tag sensitive data automatically, through Cloud Data Loss Prevention (DLP) integration.
Natural language API-
Derive insights from unstructured text using Google machine learning.The powerful pre-trained models of the Natural Language API empower developers to easily apply natural language understanding (NLU) to their applications with features including sentiment analysis, entity analysis, entity sentiment analysis, content classification, and syntax analysis.
Vision API-
Derive insights from your images in the cloud or at the edge with AutoML Vision or use pre-trained Vision API models to detect emotion, understand text, and more.Use machine learning to understand your images with industry-leading prediction and accuracy.
Train machine learning models that classify images by your custom labels using AutoML Vision.
Detect objects and faces, read handwriting, and build valuable image metadata with Vision API.
This was a brief look into some of the popular services in GCP. To find out more about each of these services and many other technical blogs check out: Knoldus Blogs
Group of smart Engineers with a Product mindset who partner with your business to drive competitive advantage | www.knoldus.com
See all (15)
Group of smart Engineers with a Product mindset who partner with your business to drive competitive advantage | www.knoldus.com
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/google-cloud-platform-technology-nuggets-january-1-15-2022-edition-109f12e498e5?source=search_post---------233,"There are currently no responses for this story.
Be the first to respond.
Happy New Year ! Welcome to the January 1–15 2022 edition of Google Cloud Technology Nuggets.
If one of your resolutions in 2022 is to learn GCP, you can get off the blocks immediately with 3 learning programs tailor made to help you achieve that:
For more details, check out the blog post.
If there is one article that I would recommend to read in this edition is the one titled “Are you a multicloud engineer yet? The case for building skills on more than one cloud”, which builds the case for each one of us to consider building skills on more than one cloud provider. The article brilliantly makes the case by establishing the premise that multi-cloud is here to stay and suggests to look at one cloud provider as your primary skill (go deep) and choose a second cloud provider platform to learn (go broad). Unlike most articles, it gives you actionable guidance on how to go about it, by suggesting core topics and concepts that are common to all, possibly looking at a professional certification and more. Do take a look.
A new episode in the series titled ‘Architecting Cloud Solutions’ covers Independent Health Group, where they share their insights into a Data Warehouse Migration Strategy and things to consider in a Data Migration.
The key thing to note in the Data Warehouse Migration strategy is the choice of platform that will not only integrate various sources of data but can scale with increasing data volume in the future and act as a provider of data for multiple downstream applications.
If you would like to check out the entire Architecting Cloud Solutions series, check out this video playlist.
In our story on Containers and Kubernetes, we take a look at the journey of Geotab, a provider of fleet management hardware and software solutions. Geotab needed to serve its customers at scale, in various geographies with strict data security and jurisdictional requirements. The post describes their journey to running all their services on GCP, specifically GKE and then multi-cluster GKE services.
There is a brand new 2-part series on Data Governance, that dives into what Data Governance is about, the processes that are part of it and most importantly, the technologies that help to achieve that. The first part of the series is published and it covers the role of data governance, why it’s important, and processes that need to be implemented to run an effective data governance program.
Are you applications more secure when they are running on-premise or in the Cloud? The Cloud definitely has a lot going for it in this argument but you always run the risk of not configuring your applications securely in the cloud. It is a shared responsibility after all. Google Cloud puts security at the top of the list with Zero-Trust principles, Defense at Depth, a top security team and more. Additionally, this post also identifies 8 industry mega trends that Google Cloud continues to adopt and which strengthens its security posture beyond on-premise. The megatrends range from Economies of Scale, Shared Fate, Software defined infrastructure, Sovereignity and more.
Continuing on the topic of security, how does a board address questions across security? Here is a list of 10 questions to help Boards safely maximize their cloud opportunities.
Eventarc is GCP’s managed platform that helps you build event-driven architectures, by letting you asynchronously deliver events from Google services, SaaS, and your own apps using loosely coupled services that react to state changes. The goal is to eventually link multiple events from various services and deliver them to destinations.
Eventarc has seen a lot of development and the latest blog post goes into some updates that have been released. Top of the list is an Eventarc UI, right in the main menu, this helps you to look at the various triggers that you have configured in the system and the source and destination platform/configuration.
Prior to the recent update, there was just one destination available for Eventarc i.e. Cloud Run. Cloud Run for Anthos is the new addition in the list of destinations. Check out the post for more details.
If you were to build your next application on GCP, would you use serverless or fully managed services available on the platform? If you read that carefully, you might be debating if there is any difference between them? I for one have at times, used these words to describe the same capability at times. As Priyanka Vergadia, writes in the article, there are some key differences between Serverless and fully managed services and she explains that via some of the GCP Services. Take a look.
There were several Top 10 blog posts of 2021 published on Google Cloud Blog and these were published across various categories like Infrastructure, Data Analytics, Security and more. I am listing down a few posts, should you want to do a recap of some of the top stories of 2021 on Google Cloud Blog:
Have questions, comments, or other feedback. Do send it across.
Looking to keep a tab on new Google Cloud product announcements? We have a handy page that you should bookmark → What’s new with Google Cloud.
Google Cloud community articles and blogs
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/gcp-products-described-in-4-words-or-less-f3056550e595?source=search_post---------236,"There are currently no responses for this story.
Be the first to respond.
Google Sheet | PDF | High-res image | GCP Products Page | Tweet
Google Cloud community articles and blogs
1.6K 
7
1.6K claps
1.6K 
7
Written by
Google Developer Relations — Living in San Francisco https://gregsramblings.com | https://cloud.google.com | http://instagram.com/gregsimages
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Google Developer Relations — Living in San Francisco https://gregsramblings.com | https://cloud.google.com | http://instagram.com/gregsimages
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@davidmytton/how-much-is-spotify-paying-google-cloud-ebb3bf180f15?source=search_post---------237,"Sign in
There are currently no responses for this story.
Be the first to respond.
David Mytton
Mar 7, 2016·4 min read
Two weeks ago, Spotify announced it was migrating from its own datacentres to Google Cloud Platform.
This is a huge win from Google because Spotify is the first major service running at huge scale that is deploying across many of its cloud products (and talking about it). We all know that Snapchat has been running on Google for a while, but since it is primarily on App Engine, Google needed a credible use case for its other services. Now it has one. This is similar to AWS’s Netflix.
"
https://medium.com/17live-tech/%E5%BE%9E-gcp-%E6%95%85%E9%9A%9C%E7%9C%8B-17-media-%E5%B7%A5%E7%A8%8B%E5%9C%98%E9%9A%8A%E7%9A%84%E7%81%BD%E9%9B%A3%E6%87%89%E8%AE%8A-51aa5701ba77?source=search_post---------238,"There are currently no responses for this story.
Be the first to respond.
Google Cloud Platform (GCP) 在萬聖節隔天發生了三年來最大規模的故障，在故障的 12 小時內，因為網路問題造成無法新增機器。這次故障也是個好機會來檢視 17 Media 的 Site Reliability Engineering (SRE) 團隊平日的防護及練習的成果。筆者有幸參與故障排除全程，本文將以一位 17 Media SRE 觀點來介紹當天發生經過，探討為何這次 17 Media 受影響特別大，以及日後的改進事項，與大家一同交流。
目前主要負擔線上流量的有十組伺服器，全都以容器透過 Google Kubernetes Engine (GKE) 運行，利用 CPU 用量自動擴容。每日尖峰及離峰流量差距約為六倍，尖峰約在晚上 11 點鐘左右，QPS 超過 10,000。這對開發團隊是相當刺激的一件事，因為每天晚上都是個小小的雙十一挑戰，如果不小心寫出了 bug，當天晚上就會收到很真實的用戶反饋。
17 Media 的主要用戶分布於亞洲及美國，主力開發由台灣團隊負責，整個後端程式由 Golang 所撰寫。為了要讓開發團隊更為敏捷，SRE 每個上班日都會佈署新版本：透過 Jenkins 在凌晨四點自動將前一日 master branch 的代碼佈署到 staging 環境，SRE 則是在上班後將 staging 環境測試穩定後的代碼佈署到生產（production）環境。
事故發生於 11/1 中午，當時一位 SRE 發現 staging 環境的伺服器無法正常部署新版本，一段時間後在 GCP 狀態頁面確定是 GCP 的故障，造成無法開啟新機器。這對我們意味著幾件事：
其中 3. 的影響最大。在故障剛發生時（12:00）系統處於離峰狀態，依照過往的流量波形，預估稍晚流量上升後系統會無法支撐。但由於以往 GCP 故障多在一小時內修復，當下我們不太擔心。
數個小時過後從 GCP 狀態頁面仍看不出修復跡象，這時團隊開始緊張了，因為晚上就是高峰期，現有的系統資源肯定支撐不住流量。SRE 團隊很快討論後決定了以下應變措施：
由於無法佈署新版本，需要改動後端代碼的 hotfix 就不在考慮範圍內，所以這次的應變措施大多以運維的角度出發，跟以往與後端團隊合作的模式不同。以下分別介紹各項應變措施：
這是一項限制用戶數的功能。系統會持續追蹤目前線上人數，當發現無法支撐過多的用戶時，會將超出上限的新上線用戶擋在系統外。新上線的用戶會看到超載訊息，而已經在線上的用戶可以繼續保有良好的直播體驗。
我們預估了當前系統可以支撐的用戶數並開啟限制功能。從事後看來這個措施效果最好，因為這是一個開發成熟的功能，當下只要定義人數上限就會產生效果。
目前 17 Media 線上服務大多為 CPU bounded。當 CPU 成為限制資源時，我們只好透過重新分配 CPU 的運用來讓重要的工作可以順利完成。對於直播平台來說，最重要的體驗就是直播本身，任何佔用 CPU 的非直播功能都可以關閉。但由於這些改動不能牽涉代碼改變（因為無法佈署新版本）所以可以關閉的功能有限。最後僅關閉了一些可以動態開關的功能，例如排行榜以及發送紅包。
當可以使用的運算資源無法擴充時，除了關閉次要功能外，另一個方式是重新分配各個服務所運行的容器數量。對於某些延遲要求比較低的服務（例如 job queue worker），可以進一步降低機器數，把運算資源分配給延遲要求高的服務（例如 API service）。
這個方式說起來容易做起來難。由於更換容器不屬於 SRE 團隊日常工作之一，實際執行上只能土法煉鋼進入一台一台機器手動更換，而且更換到第三台後就因為不明原因失敗了。相信大家都有聽過寵物跟牲畜的故事，一時要將牲畜當作一隻隻的寵物看待並不是件容易的事。
17 Media 的伺服器位於美西，剛好是各大雲服務供應商都有機房的位置。如果當下沒辦法在 GCP 開機器，那就在 AWS 開機器吧！
因為總故障時間長達 12 小時，前面幾項工作也很快的嘗試完了，我們大部分的時間花在跨雲佈署的準備。團隊已經好一陣子沒接觸 AWS 的容器服務，我們不確定哪個解決方案能在短時間解決問題，所以決定分頭嘗試不同的服務，包括 EKS / ECS / EC2 以及 Elastic Beanstalk，能夠嘗試的我們都試了。後來每個人分別遭遇了不同的問題，由於團隊對於 GCP 黏著度較深，導致這項方案完全失敗。現在反思當時應該要所有人專注在同一項 AWS 服務，或許能在時限內找出一項解決方案。
這是全球性故障，我們原先預期全球各大 app 都會發生問題。但在網路上搜尋一輪的結果超出意料：各大 app 都運作正常。相較於其他公司，17 Media 受到的影響相當大。事後研究發現可能原因有二：
SRE 團隊的風格是快速嘗試且不責怪（blamelessness），在故障後的隔天上班日，團隊就開會檢討了當天的應變以及日後的改進。從事後反思當下的措施，真正奏效的反而是平日已經準備好的災難預備方案。這也讓我們認真檢討改進方案，若日後發生一樣的故障時服務能繼續運行。以下兩項是團隊討論之後得出的改進方案：多雲佈署及微服務，分別屬於 SRE 及 backend 的工作範疇：
多雲佈署是個解決雲服務故障的好方法，畢竟各大雲服務商從來沒有同時故障過；當其中一個雲服務故障時，可以將流量導至另一個雲來排除問題。但由於其運維複雜度以及可能帶來的延遲上升，之前並不是 SRE 團隊的工作重心。這次故障讓我們認真思考多雲佈署的可能性。我們計畫從無狀態（stateless）服務開始，原因如下：
一般來說無狀態服務前面有一層 load balancer 分散流量，服務之下會有另一層服務或資料庫（database）來儲存或提供狀態。同時因為資料庫的存取延遲較長，前面多半有一層快取（cache）來降低延遲，同時降低資料庫負擔。也有人稱這為 3-tier architecture。
這一類運維層面的改動有個重點，必須對開發人員無感，也就是不能造成開發人員額外負擔。
基於以上假設，我們計畫做以下架構改動：在 AWS 建立另一組無狀態服務，前面由 weighted DNS 分散流量；在兩個雲服務商各建立一組快取以加速服務讀取速度，但寫入一律透過 GCP；資料庫讀寫依舊透過 GCP 。下圖描述了計畫中的改動。
在開發人員眼中，這個策略是非常合適的，因為開發人員對於資料庫及快取的假設不因這個策略而改變，也就是說工程師可以在不知道底層複雜度的前提下進行開發。原因如下：
在 SRE 眼中，這個策略略有風險：17 Media 的 DNS 服務直接面向用戶，切換 DNS weighting 的時候會有 DNS cache poisoning 的問題；但這可以透過後續架構微調解決（例如前面再搭一層 load balancer），所以目前較不擔心。
在之前提到，這次 17 Media 會成為重災戶，其中一個原因是主要功能集中在少數服務，解決方法就是將單體服務（monolith）分拆成微服務（micro services）。但要怎麼拆就是個大學問，拆分方式需要視團隊組成而決定，後端團隊正在討論不同拆分的可行性。
在這次故障的處理過程中，讓我感到產品團隊的責任越來越重大。目前團隊支撐的已經是個多元的直播產品，平台上包括早期用戶大多來自台灣，系統故障的內部 Slack 訊息只需準備中文；但現在已經跨足許多地區，這次故障時內部更新訊息包含中英日等語言。
以上改進事項正在進行中，有興趣歡迎一起討論。也歡迎你參與進化的過程，17 Media 工程團隊持續徵才中，包括 backend 以及 SRE。在 17 你可以遇到頂尖好手，這是一個多元化 app 的壯大旅程，歡迎加入我們！
職缺列表：http://bit.ly/2qnPLwm
Thoughts and ideas from 17LIVE’s product, engineering and…
2.4K 
6
2.4K claps
2.4K 
6
Written by
Software and math enthusiast. Leverage technology to help people and business.
Thoughts and ideas from 17LIVE’s product, engineering and data team.
Written by
Software and math enthusiast. Leverage technology to help people and business.
Thoughts and ideas from 17LIVE’s product, engineering and data team.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/hackernoon/launch-a-gpu-backed-google-compute-engine-instance-and-setup-tensorflow-keras-and-jupyter-902369ed5272?source=search_post---------239,"There are currently no responses for this story.
Be the first to respond.
Bringing the Udacity Self-Driving Car Nanodegree to Google Cloud Platform. The step-by-step guide. 🚗
Back in June I became a student of the Udacity Self-Driving Car Nanodegree program. It’s a nine months long curriculum that teaches you everything you need to know to work on self-driving cars. Topics include:
Our first project was to detect lane in a video feed and most of the students from my batch are now very deep into the deep learning classes.
These classes rely on Jupyter notebook running Tensorflow programs and I learned the hard way that the GeForce GT 750M on my Macbook Pro and its 384 CUDA cores were not going to cut it.
Time to move on to something beefier.
All students get AWS credits as part of the Udacity program. It’s the de facto platform for the course and all our guides and documentation are based on it.
However I’m a long fan of GCP, I’m already using it for some of my personal projects and I’m constantly looking for new excuses to use it.
This time I wanted to take advantage of these nifty features of the platform:
I thought I might end up with a pretty cheap deep learning box and also (mostly?) it seemed like I would be in for a good learning experience and some fun.
After some research I quickly discovered that Pre-emptible VMs can’t have GPUs attached and that my predicted monthly usage was definitely not going to trigger sustained-use discount.
Oh well. Looks like it’s only going to be for the learn & fun then. 😅
Before we dive in, a quick note about pricing. I settled on the setup below for the needs of this course:
Total per hour: $1.20/hourTotal for average CarND student usage (15h/week/month): $77Total for a full month (sustained use discount applied): $784.40
I haven’t done any benchmarking nor did I try to optimise this setup in any way. It might be overly powerful for the purpose of this Nanodegree, or on the contrary not powerful enough, so feel free to tweak if you have a better idea of what you are doing. 😉
Let’s get started.
Your first step will be to launch an instance with GPU on Google Compute Engine. There are two versions of this guide, one for Web UI lovers and another one for people who don’t like to leave their terminal.
In both cases this guide assumes you already have a Google account, if you don’t please create one here first.
Head to https://console.cloud.google.com.
If it’s your first time using Google Cloud you will be greeted with the following page, that invites you to create your first project:
Good news: if it’s your first time using Google Cloud you are also eligible for $300 in credits! In order to get this credit, click on the big blue button “Sign up for free trial” in the top bar.
Click on “Create” and you will get to the page shown in the image below.
If you are already a GCP user, click on the project switcher in the top menu bar and click on the “+” button in the top right of the modal that shows up.
Choose a name for your project, agree to the terms of service (if you do!) and click on “Create” again.
If you are already using Google Cloud Platform you most likely already have a billing account and so you can select it on the project creation form.
If you are a new user, you will have to create a billing account once you project is created (this can sometime take a little while, be patient!). This as simple as giving your credit card details and address when you are prompted for it.
Use the project switcher in the menu to go to your project’s dashboard.
Before you create your first instance you need to request a quota increase in order to attach a GPU to your machine. Click on the side menu (a.k.a the infamous hamburger menu) and head to “IAM & Admin” > “Quotas”.
Display all the quotas using the “Quota type” filter above the table. Select “All quotas”. Then use the “Region” filter to select your region.
Look for “Google Compute Engine API — NVIDIA Tesla K80 GPUs” in the list, select it and click on “Edit quotas” at the top.
In the menu that opens on the right, enter your personal details if they are not already pre-filled and click on “Next”. Fill the form asking you how many GPUs you want and the reason for requesting a quota increase with these details:
After a while (should be quite short) Google will approve your request. You are now ready to move to the next step!
Click on the side menu again and go to “Compute Engine” > “VM Instances”.
You’ll be greeted with a notice inviting you to create your first instance. Click on “Create”.
You have already chosen the specs for your server so this form will be pretty straightforward:
Next you need to select an OS and add a SSD Persistent Disk to your instance. Click on “Change” in the boot disk section.
Use these settings:
You are almost done.
Click on “Management, disks, networking, SSH keys” at the bottom of the form.
In the “Networking” section add the following network tag: jupyter. This will be useful later on when you configure the firewall.
The last step is to attach your SSH key to the instance. Go to the “SSH Keys” section.
Copy and paste your SSH public key. It needs to have this format:
This is very important as <username> will end up being the username you use to access your server.
Click on “Create” at the bottom of the page.
Note the instance’s external IP for later.
In order to access your Jupyter notebooks you need to add a firewall rule to allow incoming traffic on port 8888.
Using the side menu go to “VPC network” > “Firewall rules”.
Click on “Create firewall rule”.
Put these values in the form:
You can skip this section if you already started your server using the web-based console.
Follow the guide available on the Google Cloud Platform documentation site to install the latest version of the Cloud SDK for your OS.
If your project ID is already taken feel free to append some random number to it.
Go to the following page to enable billing for your newly created project:
Note: I haven’t a way to do this from the CLI but if someone reading this does, please let me know in comment.
Before you can start an instance with an attached GPU you need to ensure that you have enough GPUs available in your selected region. Failing that, you will need to request a quota increase here:
Then you can follow the web version of this guide to request a quota increase.
In order to create an instance with one or more attached GPUs you need to have at least version 144.0.0 of the gcloud command-line tool and the beta components installed.
If you don’t meet these requirements, run the following command:
Create a file with the format below anywhere on your local machine:
For instance (they key was shortened for practical reasons):
Time to create your instance:
The output should be similar to this if the command runs successfully:
Write down the external IP.
Here is what the output should be:
Now that your instance is launched and that the firewall is correctly setup it’s time to configure it.
Start by SSHing into your server using this command:
[EXTERNAL_IP_ADDRESS] is the address you wrote down a couple of steps earlier.[USERNAME] is the username that was in your SSH key.
Since it’s you first time connecting to that server you will get the following prompt. Type “yes”.
Let’s first update the system software and install some much needed dependencies:
Luckily Ubuntu ships with both Python 2 and Python 3 pre-installed so you can move directly to the next step: installing Miniconda.
The output from the last command should be something like:
This is the MD5 sum of your downloaded file. You can compare it against the MD5 hashes found here to verify the integrity of your installer.
You can now run the install script:
This will give you the following output:
Press [ENTER] to continue, and then press [ENTER] again to read through the licence. Once you have reached the end you will be prompted to accept it:
Type “yes” if you agree.
The installer will now ask you to choose the location of your installation. Press [ENTER] to use the default location, or type a different location if you want to customise it:
The installation process will begin. Once it’s finished the installer will ask you whether you want it to prepend the install location to your PATH. This is needed to use the conda command in your shell so type “yes” to agree.
Here is the final output from the installation process:
Source your .bashrc file in order to complete the installation:
You can verify that Miniconda was successfully installed by typing the following command in your shell:
In order to reap all the benefits of the Tesla K80 mounted in your server you need to install the NVIDIA CUDA Toolkit.
First, check that your GPU is properly installed:
At the time of writing the most recent version of CUDA Toolkit supported by Tensorflow is 8.0.61–1 so I’ll use this version throughout this guide. If you read this guide in the future, feel free to swap it with the newest version found on https://developer.nvidia.com/cuda-downloads.
Compare the MD5 sum of this file against the one published here.
If they match, you can proceed to the next step:
Run this command to add some environment variables to your .bashrc file:
Source your .bashrc file again:
NVIDIA provides some sample program that will allow us to test the installation. Copy them into your home directory and build one:
If it goes well you can now run the deviceQuery utility to verify your CUDA installation.
You can also use the nvidia-smi utility to verify that the driver is running properly:
cuDNN is a GPU-accelerated library of primitives for deep neural networks provided by NVIDIA. If it is required by Tensorflow when you install the version with GPU support.
Tensorflow 1.2.1 (the version we are using in this guide) only supports cuDNN 5.1.
In order to download the library you will have to register for a NVIDIA developer account here.
Once your account is created your can download cuDNN here (you will have to login).
Agree to the terms and download the “cuDNN v5.1 for Linux” archive on your local computer. Be sure to use the version for CUDA 8.0.
Run these commands to go into the directory where the archive was downloaded (in my case it’s ~/Downloads) and upload it to your server:
Once it’s successfully uploaded, uncompress and copy the cuDNN library to the CUDA toolkit directory:
Being able to push and pull to your repos on Github directly from the server is way more convenient than having to synchronise files on your laptop first. In order to enable that workflow you will add a SSH key specific to this server to your Github account.
First, generate a SSH key pair on your server:
Enter a passphrase and choose a location for your key when you are prompted.
Read the content of the public key and copy it to your clibboard:
Head to your Github SSH and GPG keys settings, click on “New SSH key”, give a name to your key in the “Title” section and paste the key itself in the “Key” section.
Once the key is added, you can go back to your server.
Create a new folder named carnd in your home directory and clone the CarND-Term1-Starter-Kit repository:
There is one minor change we need to do in the environment-gpu.yml file provided by Udacity, before we create the conda environment.
Using vim, nano or whichever text editor you are most familiar with, change this:
into this:
This change will ensure you grab the latest available version of Tensorflow with GPU support.
You are now ready to create the conda environment:
This command will pull all the specified depencies. It may take a little while.
If it successfully creates the environment you should see this output in your console:
Time to try your new install!
Activate the environment:
Run this short TensorFlow program in a python shell:
If you get this output then congratulations, your server is ready for the Udacity Self-Driving Car Nanodegree projects.
If you get an error message, see this section on the Tensorflow website for common installation problems or leave a comment here.
Now that your server is ready, it’s time to put it to good use.
I’ll run through how to use your server using the LeNet lab as an example but these steps apply to any other Jupyter-based lab in the course.
In order to access your Jupyter notebook you need to edit the Jupyter config so that the server binds on all interfaces rather than localhost.
This command will generate a config file at ~/.jupyter/jupyter_notebook_config.py.
Using a text editor, replace this line:
with this:
Next time you launch a Jupyter notebook the internal server will bind on all interfaces instead of localhost, allowing you to access the notebook in your browser.
First, let’s retrieve the content of the lab by cloning the repository from Github.
Run these commands on your server:
If your conda environment is not active already, activate it now:
Launch the notebook:
On your local machine, open your browser and head to:
If it’s not open already, click on “LeNet-Lab-Solution.ipynb” to launch the LeNet lab solution notebook.
Run each cell in the notebook. The network with the hyper parameters given in the solution notebook should be trained in a minute or so on that machine.
Tada! You have a fully functioning GPU instance.
Important: You are billed for each minute your server is on so don’t forget to stop it once you are done using it. Note that you will still be paying a small amount for storage (~$8/month for 50GB) until you terminate the instance. Check this guide to learn how to stop or delete an instance.
Hopefully this guide convinced you that setting up a GPU-backed instance on Google Cloud Platform is as easy as on AWS.
In order to keep this guide as short and digestable as possible (I really tried!) I glossed over some very important topics:
I hope you enjoyed it, if you have any question or comment please free to get in touch using the comment section below or by emailing me directly.
#BlackLivesMatter
1.2K 
24
how hackers start their afternoons. the real shit is on hackernoon.com. Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
1.2K claps
1.2K 
24
Written by
Building @duffelhq
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
Building @duffelhq
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/automation-generation/build-a-day-trading-algorithm-and-run-it-in-the-cloud-for-free-805450150668?source=search_post---------240,"There are currently no responses for this story.
Be the first to respond.
Top highlight
Commission-free stock trading on a free Google Cloud Platform instance, step-by-step.
When I first started learning about how easy it’s become to take control of my money in the market, day trading jumped out at me as the thing to do. Of course, if it were really as easy as it sounds, everyone would be doing it. Finding a profitable strategy takes time and work, and sticking to it in turbulent market conditions takes discipline.
It’d be a lot easier to take the room for human error — and a whole lot of stress — out of the equation if we could have a computer execute the strategy for us. Just a few years ago, there were a lot of hurdles to doing that, even if you had the programming know-how. Most brokerages that the average person could access charged commission fees, which would eat away the profits of a day-trading strategy. Finding any sort of API access among those that offered commission-free solutions was a challenge to me.
In this article, I am using Alpaca’s commission-free trading API with their premium data service Polygon. (Please note that, according to their docs, you’ll need to sign up for a brokerage account in order to access the premium data feed used here.) I’ll provide a day-trading script that leverages this premium data for a little technical analysis, and I’ll go over what it does and how to run it yourself.
First off, go ahead and get the script from GitHub with this command:
Now, you can open it up in your favorite text editor and follow along. Note that near the top of the file, there are placeholders for your API information — your key ID, your secret key, and the URL you want to connect to. You can get all that information from the Alpaca dashboard.
Replace the placeholder strings with your own information, and the script is ready to run. But before we let it touch even your simulated account’s (entirely make-believe) money, let’s go over what it does. (If you’re more interested in how to get it running on GCP than what it’s doing, skip ahead to the next section.)
Broadly, this is a momentum-based algorithm. We’ll not trade for the first fifteen minutes after the market opens, because those are always pretty hectic. Between the fifteenth minute and the first hour, though, we’ll look for stocks that have increased at least 4% from their close on the previous day. If they’ve done that and they meet some other criteria, we’ll buy them, and we’ll hold them until they either rise high enough (meeting our price target) or fall too low (meeting our ‘stop’ level.)
You’ll notice that below the connection information in the code, there are some additional variables that can be configured. These can be tweaked easily to best suit your needs for the algorithm. There are thousands of stocks available to trade, but not all of them are suitable for a strategy like this.
We filter down the list by looking for a few things — we want a relatively low share price, but not one that’s so low that it behaves more like a penny stock. We also want to be sure that the stock is liquid enough that we’ll get our orders filled. We make sure that the dollar volume of the stock was at least min_last_dv on the previous trading day.
The default_stop and risk parameters are important to making sure that our algorithm stays within acceptable limits. Risk is what percent of our portfolio we’ll allocate to any given position. Since we sell when we hit the stop loss, the amount of cash from our portfolio at risk on a trade is default_stop * risk * account_balance .
I won’t go over how we get our initialization data here — if you want, you can take a look at the code and check out Polygon’s documentation on their ‘ticker’ data. What’s a little more interesting is the fact that we can also stream data in real time from Polygon. (This is also done in a recently-published “HFT-ish” example, another Alpaca day trading algorithm that trades much more frequently than this one and tries to profit from tiny order book imbalances.)
Using Alpaca’s Python SDK, we connect to three types of streaming channels. The first is trade_updates, which is simply a connection to Alpaca on which we can hear updates on our orders as they happen. We’ll use this to make sure we’re not submitting multiple open orders at once for a stock and to see whether or not our orders get filled.
The other two channels are A.<symbol> and AM.<symbol> . For each stock that we’re going to watch, we subscribe to those channels to receive updates from Polygon about the price and volume of the stock. The A channel updates every second, whereas the AM channel updates every minute. We aggregate the information from the A channel ourselves so we can do up-to-the-second calculations, but we consider AM to be the source of truth, replacing whatever we’ve aggregated with what comes through that channel. While we might get away with only watching A and relying on our own aggregation, trusting AM gives us a little extra resilience to hiccups in the connection and such.
Once we’ve added the incoming data to our aggregate, if we haven’t already ordered shares of a stock, we check to see if it looks like a good buy. We define a “good buy” as something with a positive, growing MACD that’s been trading at a decent volume and is up over 4% from yesterday’s close so far today. We also want to make sure that it’s maintained its momentum after the open, so we look to see that the price is higher than its highest point during the first fifteen minutes after the market to open. We hope that these stocks will continue to rise in value as the day goes on.
If we have a position in a stock, we also check with each bar that comes in for that stock if it’s time to sell. We sell when the stock has reached either our target price or our stop loss, or if the MACD suggests that the security is losing its momentum and it’s fallen back to our cost basis. Ideally, enough stocks hit the target price we set that we can recover the losses from those that hit the stop loss, with some extra profits on top.
At the end of the trading day, we liquidate any remaining positions we’ve opened at market price. The use of market orders is generally not ideal, but they are used in this case because the potential cost of holding overnight is greater than we were willing to risk on the position. Ideally, we have already liquidated our shares based on our defined stop losses and target prices, but this allows us to catch anything that sneaks by those by trading flat.
If you scroll down past the bottom of the long run() method, you’ll see how we check to see when the market will be opening and closing using the Alpaca Calendar API endpoint. Using this means that, if you like, you can set up a Cron job to run the script at the same time every day without having to worry about market holidays or late opens causing issues. Many people prefer to run their scripts manually, but it’s nice to have the option to just let it run on its own.
At this point, if you’ve plugged in your own API keys, you could just run python algo.py and be off to the races, watching it buy and sell stocks as its signals are triggered through the Alpaca dashboard. But you might not want to leave your own machine running all day — or maybe you’re just worried about a cat bumping the power button during market hours.
Fortunately, it’s easy to get a machine in the cloud — away from your cat — set up with Google Cloud Platform’s free tier. At the time of writing, they’re even offering several hundred dollars in free credit for trial accounts, so you don’t need to worry too much about running into minor incidental costs. Once you’ve signed up for a GCP account, head to the GCP console and create a project.
Once you’ve created a project, go to the sidebar and navigate to Compute Engine > VM Instances. In this panel, you can see any instances you’ve created. That might be blank for now, but let’s go ahead and make one. Hit Create and fill out your configuration like this.
Hit Create at the bottom, and after a short bit of watching a circle spin, you’ll have a machine up and running, visible in the VM Instances panel. It’ll look something like this:
You can click Open in browser window to get a shell to the machine. Go ahead and do that now — we’ll need it in a minute. But before we type anything into the terminal, we need to get our script onto the machine. (You might know of some ways to do that through the terminal — if so, go for it! Or, you can follow along to do it through the GCP console interface.)
In the sidebar, scroll to Storage > Browser. On the Storage page, go ahead and hit Create Bucket. You’ll get a prompt like this:
We’re not too concerned about the storage cost, since we’ll be storing much less than 1 GB and only sharing it with our one instance. (And we have a few hundred dollars in credit anyhow, if you’ve signed up for the trial.) Once your bucket is created, it’s easy to put your files into it.
Upload the requirements.txt and algo.py files you checked out from the GitHub repository and plugged your Alpaca API credentials into. We’re almost done! All that’s left is to get the files onto our instance. In the browser terminal you’ve opened, you can type gsutil cp gs://<your-bucket-name>/* . to download your files onto the VM.
Now that the script is on the cloud instance, we just need to run it. Python is already installed, but to make requirement management easy, go ahead and download and install pip. In the VM terminal, do this:
And then install the algorithm’s dependencies:
And that’s it! Now, when you’re ready to get the algorithm running, you can just type python3 algo.py into the VM’s terminal and watch it get to work. (At this point, you can also delete the bucket you created, if you’re worried about a few pennies in storage fees.) Now that you’re on your way to being a GCP pro; feel free to play around with the Python code in the script and see what works best for you! I hope this guide has been helpful, and good luck!
Technology and services are offered by AlpacaDB, Inc. Brokerage services are provided by Alpaca Securities LLC (alpaca.markets), member FINRA/SIPC. Alpaca Securities LLC is a wholly-owned subsidiary of AlpacaDB, Inc.
You can find us @AlpacaHQ, if you use twitter.
News and thought leadership on the changing landscape of…
1K 
18
1K claps
1K 
18
Written by

News and thought leadership on the changing landscape of automated investing. Changing the market one algorithm at a time.
Written by

News and thought leadership on the changing landscape of automated investing. Changing the market one algorithm at a time.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/cloud-iot-step-by-step-connecting-raspberry-pi-python-2f27a2893ab5?source=search_post---------241,"There are currently no responses for this story.
Be the first to respond.
Hi friends!
So you’ve got an awesome idea to connect a Raspberry Pi to a weather station in your backyard so you can automate opening and closing your windows when it’s going to rain. Oh and while you’re at it, you’re going to hook up a microcontroller like an ESP32 to turn your sprinklers on and off when it’s a bit too hot out. How about never forgetting to turn off your oven again!
So now what? There are plenty of tutorials out there on building a weather station. Nice list of components, some of them are pretty good about physically connecting it, others kind of gloss over it a bit and you have to search around to piece everything together. Some go a step further and tell you how to connect it up to the Cloud! Eureka! Now we’re talking. But a lot of the ones that go that far are really long, no table of contents so you have to kind of search through all the pieces to find what you need. And rarely do they fill in all the gaps for a specific use case.
The goal of this blog, is to start small, and focused (even though it still feels kinda long). I want you, by the end of this, to have a step-by-step guide to connecting your Raspberry Pi to Google’s Cloud Platform. There’s a TON of stuff to do beyond that, but like I said, start small. I’m not going to go into even close to the full features that our IoT platform has to offer. Even with regards to communication with devices. I’m ONLY covering one way, device to Cloud communication. But I want to enable you to bring your project to the Cloud. Taking that first step is all about being able to see your device talking to the Cloud.
After I get this covered, I’ll start building up similar tutorials. More things you can do with devices like two-way communication. How to move data around inside the Cloud Platform, how and where to store the data for different use-cases. How to run basic analysis. How to train a machine learning model in the cloud with your data.
In this tutorial, note that for some folks, the level of detail I go into will be way more than you need. For example, in the Pi setup basics, I go into a lot of detail around things like, changing the font of the terminal to make it easier to read. I have set up logical headings so it’s easy to bounce to the parts of the tutorials that you need to get up and running.
I used a Pi 3+ for the tutorial, but from the library dependency setup portion down, this will also work with a Raspberry Pi Zero W. If you want to setup your Pi Zero for this, but haven’t set one up before, I highly recommend this guide. And then skip down to the dependency section in this post.
Some of these steps might be slightly different for you, I have some things I like to do to my Pi just for quality of life. If you already know how to setup WiFi, and adjust your Pi to how you like it, then you can safely skip this bit and go straight to the library dependency setup.
Direct connect to your Pi, and hook the HDMI.
Depending on the monitor you’re on, or how old you are, I change the font in my terminal to Monospace 14. Makes it much easier to read. I also change my keyboard layout in the preferences to US->English. Default is UK->English which has symbols in unexpected places for me.
One thing you definitely need to do if you’re using WiFi, is be sure that in the settings on the Pi, your WiFi country is set to wherever you are. If it’s set to Germany, and you’re in the United States, it’s likely not going to work. Sometimes different combinations of countries work, but it’s pretty random, and better to just set it.
Next step is to be sure your Pi can connect to the internet. Either plug in an ethernet cable, or if you’re using WiFi, scan for networks your Pi can see. Run this from a terminal on the Pi:
Pick the one you want to use, remember the SSID.
Note that this will override any existing WiFi settings you had, so if you want to preserve the current settings, move the file to be a backup. This can be done with:
Then run:
You’ll replace the contents of that file if there’s anything in there with one of two things. If your WiFi has a password, then use this:
If there’s no password:
Now that you have a configuration in there for your WiFi, you need to restart that service to engage the WiFi:
Now you should be able to verify that you have WiFi by running:
You should have an IP address. Depending on your WiFi’s configuration it’s probably something that starts with 168.1 or 127.1 (as long as it’s not 127.1.1.1 which isn’t right for this).
If it fails, and you don’t have an IP address. You can try:
This hard resets the WiFi. If it STILL doesn’t work, try rebooting the Pi completely with:
Now that you’re Pi can talk to the internet, let’s get it setup to have all the Python dependencies we need.
If you’re new to Linux, Pi, and all things apt-get, each of these steps will do some variation of spew out a bunch of text about what you’re asking for, and what dependencies it relies on, to tell you what it’s going to install on your Pi. If you want to know what it all means, the internet pages for each library are pretty good about explaining what the library is all about. I’ll TL;DR each one just to blurb on what the particular library is used for.
First up, make sure we’re current:
This makes sure that the list of places your Pi will be getting its libraries from is current. Sometimes these repositories move around, get deprecated, etc, so it’s always good to run this when starting a project. Note that if you’re using Raspbian, that they’re constantly updating it as well, so some of these dependencies may just say they’re already there. That’s totally fine, this is just, for the sake of completeness, all the things you need.
Next up is to get the pieces we need to handle the secure connection with IoT Core. Authentication is done via JWT instead of user/password as it’s way more secure. To handle that, the library is pyjwt, which depends on a Python library called cryptography. Why am I telling you this? Because to install those pieces, you need certain base libraries. To get them, run each of these commands:
Note here, that depending on your version of Python, being able to install one of the libraries, cryptography, might require 3.x. Go ahead and try to go through everything with whatever version you have of Python, but if you get compile errors on the step when you get to sudo pip install cryptography then you’ll need to run all these pip installs with pip3 instead of just pip so you’re using Python 3.x. I’ve run it recently with Python 2.7.13 and it appears they’ve cleaned up a bunch, so you’ll get messages for a few of these like Requirement already satisfied: <library>. I’m just including all the manual library additions for completeness.
For this demo, we’re using MQTT, so we need the library that allows us to use that protocol, paho-mqtt. Run:
For our encryption we’re using pyjwt. Run:
For crypto, the cryptography library is the dependency for our JWT library. Run:
And finally, I’m using a Sense HAT for my telemetry data. Just makes it nice and easy since it provides telemetry sensors, has a nice API, and the library is pre-installed on the Pi model 3. If you’re not using a Pi that has it installed, just run:
Now your device should be all set (minus code, coming later).
If you’ve never done this before, then go to Google’s Cloud Platform landing page and click through the “Try now” button. It’s the most seamless if you have a gmail address, but you can still do this if you don’t have one. You can go here and create a Google account using any email address.
Once the account is all setup, head to the console. First step will be to set up billing. Keep in mind, setting up the Cloud project if it’s the first time you’re doing it, gives you $300 in Cloud credits, and there’s a generous free tier so while you’re trying things out, you’re unlikely to hit any pay walls.
First step is to enable Cloud IoT. Number of ways to get there:
1) Put IoT Core into the search bar in the console2) Select it from the hamburger menu in the upper left. IoT Core is down near the bottom. 3)Click this link.
Clicking the enable button will grant the appropriate permissions for the default security rules to allow your user to use IoT Core. If you’re wanting to do it by hand, be sure the user you’re going to use for your IoT work has permission to publish to Pub/Sub. IoT Core bridges the device messages to Pub/Sub for you, which means you need to have permission to write. These permissions are handled in the IAM part of the console. There be dragons. I would only suggest doing that if you’re really comfortable with Google Cloud Platform.
Next we’ll create your registries where you want your IoT devices to live. Registries are logical groupings of devices. So if you’re doing like, a smart building infrastructure, each registry might represent a building’s worth of devices.
Easiest path is to create a device registry from the IoT Core console page, and as part of that process, in the drop-down where it asks for a Default telemetry topic, you can elect to create a Pub/Sub topic inline. Select the region closest to you, and by default it enables MQTT and HTTP protocols. Easiest to leave that alone unless you know you’ll only be using one vs. the other. The example code in this blog uses MQTT.
Now we need to create out device representation in IoT Core. In the page that opened when you created your registry, click on the Create Device button.
Give it an id, and leave all the options alone. For the SSL certificate, we’ll create an RSA with x509 wrapper here, but if you want to look at the other options, you can see how to create them here. Be sure that you select the appropriate radio button for the type of cert you create. By default, the RS256 radio button is selected so if you’re just creating the cert using the code snippets below, select the RS256_X509 radio button for Public key format.
The default install of Raspbian has openssl installed. If you’re running a custom OS on the Pi and it doesn’t have openssl installed, you should be able to just run this off the Pi, and put the private key on it later. To create the key, run the following:
You can either upload the key directly, or copy/paste the contents of the key. There’s two radio buttons on the device page to pick which way you want to do it.
If you want to upload, click the radio button on the create device page for Upload. Browse to and select the demo.pub key. Scroll down and click the Create button. You should then see your newly created device in the list on the registry details page.
If you’ve run the openssl commands on a device that can’t run the webpage, leave the radio button on manual, and on your device run:
and copy everything (including the tags) between:
and paste it into the text box for the Public key value.
This handles the auth from the Google side confirming your device is okay to talk to Google. Last security piece is to grab the Google roots.pem file so your device knows it’s talking to Google. Again, on the Pi if possible, off the Pi and transfer it over if not, run:
One last thing to setup. Pub/Sub is efficient in that, if nothing is listening to the Pub/Sub topic, any messages sent to that topic won’t be stored. Since subscriptions don’t go into the past to pick up messages, there’s no reason to store them if no one’s listening. So enter Pub/Sub into the search field on the Cloud Platform console, or pick Pub/Sub from the hamburger menu to open up the Pub/Sub page. You should see at least the topic you created above listed here.
Click on the 3 dot menu on the right, and pick New subscription. Give it an id.
Now you have all the pieces setup to get your device talking to Google Cloud Platform. A few last bits with the code itself. We’re almost there!
The code can be found on my GitHub. 01_guide.html is an abbreviated version of this blog post. If you need to find things later, it may well be faster without the talking through things here to just access that guide and pick up what you need. 01_basics.py is the basic code you want to get and run. As I mentioned above, it uses the Sense HAT to gather the telemetry data. So if you have one, you should be able to modify the variable block in the code to point to the various pieces you’ve setup and just run it.
The ssl_private_key_filepath is the full path to the private half of the key we created: demo_private.pem. The root_cert_filepath is the full path to the roots.pem we grabbed using wget above. project_id is the Google Cloud Platform project id that your device was registered to. gcp_location is the region you picked when creating the registry. registry_id and device_id are the ids you gave above when creating those pieces.
If you want to use something other than the Sense HAT then comment out any reference to sense and replace with whatever you want to use.
Last piece, is I have commented out the actual publish line of code so you can test to be sure everything ELSE is working before you start spamming messages into the Cloud. That’s down around line 100 (as of the time of publishing this blog). I’d suggest running first, then uncommenting that line.
Now you should be able to run:
If it’s working, you should see messages on the console like:
And then:
If all that’s working right, then go ahead and uncomment the publish line in the code, re-run it, and then after a bit (there IS a delay before Pub/Sub will return values using the command line SDK, but the actual publish time is faster than this) you can verify by running this command anywhere you have the gcloud SDK installed (if you don’t, go here for instructions on installing it):
If all has gone well, you should see your messages showing up there!
Thanks for reading, now go connect your device to the Cloud! Think of a project, tell me your ideas.
If you have any questions, about a project idea, the Cloud or IoT, or anything, please don’t hesitate to ask in the comments below, or reach out to me on Twitter!
Next step: Sending communication back to your device from the Cloud.
Google Cloud community articles and blogs
818 
14
818 claps
818 
14
Written by
Husband, father, actor, sword fighter, musician, gamer, developer advocate at Google. Making things that talk to the Cloud. Pronouns: He/Him
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Husband, father, actor, sword fighter, musician, gamer, developer advocate at Google. Making things that talk to the Cloud. Pronouns: He/Him
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://towardsdatascience.com/how-to-train-and-predict-regression-and-classification-ml-models-using-only-sql-using-bigquery-ml-f219b180b947?source=search_post---------242,"Sign in
There are currently no responses for this story.
Be the first to respond.
Lak Lakshmanan
Jul 25, 2018·3 min read
In my book (Data Science on the Google Cloud Platform), I walk through a flight-delay prediction problem and show how to address it using a variety of tools including Spark Mlib and TensorFlow. Now that BigQuery ML has been announced, I thought I’d show how to predict flight delays using BQ ML.
Make no mistake — you still have to collect the data, explore it, clean it up, and enrich it. Essentially all the stuff I do in Chapter 1–9. In Chapter 10, I used TensorFlow. In this article, I will use BQML.
Here’s a BigQuery query to create the model:
Note that:
About 10 minutes later, the model is trained and evaluation results have been populated for each iteration:
The loss here is mean squared error, so the model converges on iteration #6 with a RMSE of about sqrt(97) = 10 minutes.
The purpose of training a model is to predict with it. You can do model predictions with a SQL statement:
This results in:
As you can see, because we trained the model to predict a variable called “arr_delay”, ML.PREDICT creates a result column named predicted_arr_delay. In this case, I’m pulling 10 rows from the original table and predicting the arrival delay for those flights.
In the book, I don’t actually try to predict the arrival delay as such. Instead, I predict the probability that a flight will be more than 15 minutes late. This is a classification problem, and you can do that by changing the training query slightly:
Here’s the evaluation results:
and an example of the predictions:
It is possible to evaluate the model on an independent dataset. I don’t have one handy, so I’ll just show you how to run the evaluation on the same dataset the model was trained on:
The result:
BQML is really easy and really powerful. Enjoy!
Data Analytics & AI @ Google Cloud
603 
4
Every Thursday, the Variable delivers the very best of Towards Data Science: from hands-on tutorials and cutting-edge research to original features you don't want to miss. Take a look.
603 claps
603 
4
Your home for data science. A Medium publication sharing concepts, ideas and codes.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/enrique-dans/finally-could-googles-chickens-be-coming-home-to-roost-7cfcce963710?source=search_post---------243,"There are currently no responses for this story.
Be the first to respond.
Google is telling anybody who will listen that the APIs and functionalities built into its Google Cloud platform will remain stable over time and will not fall victim to arbitrary decisions by the company, a fiction designed to avoid discussion of the company’s longstanding disregard for its users, which has led it, over the years, to ruthlessly eliminate countless services that had large…
"
https://towardsdatascience.com/choosing-between-tensorflow-keras-bigquery-ml-and-automl-natural-language-for-text-classification-6b1c9fc21013?source=search_post---------244,"Sign in
There are currently no responses for this story.
Be the first to respond.
Lak Lakshmanan
Jan 5, 2019·8 min read
Google Cloud Platform offers you three¹ ways to carry out machine learning:
Choose between them based on your skill set, how important additional accuracy is, and how much time/effort you are willing to devote to the problem. Use BigQuery ML for quick problem formulation, experimentation, and easy, low-cost machine learning. Once you identify a viable ML problem using BQML, use Auto ML for code-free, state-of-the-art models. Hand-roll your own custom models only for problems where you have lots of data and enough time/effort to devote.
In this article, I will compare the three approaches on a text classification problem so that you can see why I’m recommending what I am recommending.
I explain the problem and the deep learning solution in detail elsewhere, so this section will be very brief.
The task is that given the title of an article, I want to be able to identify where it was published. The training dataset comes from articles posted on Hacker News (there’s a public dataset of these in BigQuery). For example, here are some of the titles whose source is GitHub:
The model code to create a Keras model that uses a word embedding layer, convolutional layers, and dropout:
This is then trained on Cloud ML Engine as shown in this Jupyter notebook:
It took me a couple of days to develop the original TensorFlow model, my colleague vijaykr a day to modify it to use Keras, and maybe a day to train it and troubleshoot it.
We got about 80% accuracy. To do better, we’d probably need a lot more data (92k examples is insufficient to gain the benefits of using a custom deep learning model) and perhaps incorporate more preprocessing (such as removing stop words, stemming words, using a reusable embedding, etc.).
When using BigQuery ML, convolutional neural networks, embeddings, etc. are (not yet anyway) an option, so I dropped down to using a linear model on a bag-of-words. The point of BigQuery ML is to provide a quick, convenient way to build ML models on structured and semi-structured data.
Splitting the titles word-by-word and training a logistic regression model (i.e., a linear classifier) on the first 5 words of the title (using more words doesn’t help all that much):
This was fast. The SQL query above is the full enchilada. There is nothing more to it. The model training itself took only a few minutes. I got 78% accuracy which compares quite favorably to the 80% I got with the custom Keras CNN model.
Once trained, batch predictions using BigQuery are easy:
Online predictions using BigQuery can be accomplished by exporting the weights into a web application.
The third option I tried is the code-free option that, nevertheless, uses state-of-the-art models and techniques underneath. Because this is a text classification problem, the Auto ML approach to use is Auto ML Natural Language.
The first step is to launch Auto ML Natural Language from the GCP web console:
Follow the prompts and a bucket will be created to hold the dataset that you will use to train the model.
Where BigQuery ML requires you to know SQL, AutoML just requires that you create a dataset in one of the formats the tool understands. The tool understands CSV files arranged as follows:
text, label
The text itself can either be a URL to a file containing the actual text (this is useful if you have multi-line text, such as reviews or entire documents) or it can be the plain text item itself. If you are providing the text item string directly, you need to put it in quotes.
So, our first step is export a CSV file from BigQuery in the right format. This was my query²:
Which yields the following dataset:
Note that I have stripped out punctuation and special characters. Whitespace has been trimmed, and SELECT distinct is used to used to discard duplicates and articles that appear in multiple classes (AutoML will warn you about duplicates, and can deal with multi-class labels, but removing them is cleaner).
I saved the result of the query as a table using the BigQuery UI:
and then exported the table to a CSV file:
Next step is to use the Auto ML UI to create a dataset from the CSV file on Cloud Storage:
The dataset takes about 20 minutes to ingest. At the end, we get a screen full of text items:
The current Auto ML limit is 100k rows, so our 92k dataset is definitely pushing some boundaries. A smaller dataset will get ingested faster.
Why do we have a label called “source” with only example? The CSV file had a header line (source, title) and that too has been ingested! Fortunately, AutoML allows us to edit the text items in the GUI itself. So, I deleted the extra label and its corresponding text.
Training is as easy as clicking on a button.
Auto ML then proceeds to try various embeddings, and various architectures and does hyperparameter tuning to come up with a good solution to the problem.
It takes 5 hours.
Once the model is trained, we get a bunch of evaluation statistics: precision, recall, AUC curve, etc. But we also get the actual confusion matrix from which we can compute anything else we want:
The overall accuracy is about 86% — higher even than our custom Keras CNN model. Why? Because Auto ML is able to take advantage of transfer learning from models built on Google datasets on language use, i.e. includes data that we did not have available to our Keras model. Also, because of the availability of all that data to transfer learn from, the model architecture can be more complex (read: more deep).
The trained AutoML model is already deployed and available for prediction. We can send it a request and get back the predicted source of the article:
Notice that the model is much more confident than the BQML one (although both gave the same correct answer), a confidence driven by the fact that this Auto ML model was trained on more data and is built specifically for text classification problems.
I tried another article title from today’s headlines and the model nailed it as being from TechCrunch:
While this article is primarily about text classification, the general conclusions and advice carry over to most ML problems:
¹ There are a few other ways to do machine learning on GCP. You can do xgboost or scikit-learn in ML Engine. The Deep Learning VM supports PyTorch. Spark ML works well on Cloud Dataproc. And of course, you can use Google Compute Engine or Google Kubernetes Engine and install any ML framework you want. But in this article, I’ll focus on these three.
²Thanks to Greg Mikels for improving my original AutoML query to remove duplicates and cross-posted articles.
Data Analytics & AI @ Google Cloud
See all (63)
572 
3
Thanks to Rasmi, Gonzalo Gasca Meza, and Chris Rawles. 
Every Thursday, the Variable delivers the very best of Towards Data Science: from hands-on tutorials and cutting-edge research to original features you don't want to miss. Take a look.
572 claps
572 
3
Your home for data science. A Medium publication sharing concepts, ideas and codes.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/running-a-serverless-batch-workload-on-gcp-with-cloud-scheduler-cloud-functions-and-compute-86c2bd573f25?source=search_post---------245,"There are currently no responses for this story.
Be the first to respond.
This quick-start guide is part of a series that shows how to leverage Google Cloud Platform components to run batch workloads in a simpler way. Those familiar with AWS, there’s a great tool called AWS Batch, but looking at GCP products, how are we able to run a batch job in a similar manner? Let’s dig into the GCP documentation: aws-comparison
Going deeper into the documentation… A way that GCP recommends this use case to be accomplished is: reliable-task-scheduling-compute-engineA quick summary: Cloud Scheduler -> Pub/Sub -> Start VM
An example of starting and stopping VM’s that’s quoted on the documentation: start-and-stop-compute-engine-instances-on-a-schedule
But there’s a flaw in this approach, the VM is not deleted after the batch workload is completed, so we are paying for the storage costs. How can we make it more efficient, and turn into a more serverless solution?
Wait a minute, how can you talk about VM’s and serverless in the same sentence? Something smells fishy…
It sure does, but those questions will be answered shortly, so stay with me…
To begin with, let me introduce the solution we are going to use to make the batch execution more serverless, using the GCP components:
Btw, all GCP components that will be used in this solution have a free tier =)
This is the starting point of our batch process:
“Cloud Scheduler is a fully managed enterprise-grade cron job scheduler. It allows you to schedule virtually any job, including batch, big data jobs, cloud infrastructure operations, and more. You can automate everything, including retries in case of failure to reduce manual toil and intervention. Cloud Scheduler even acts as a single pane of glass, allowing you to manage all your automation tasks from one place.”
Go to this page to start your Cloud Scheduler configuration.
To configure our starting point, we need to begin creating a Cloud Scheduler Job, it’s really straightforward:
The important fields here are Name, Frequency, Timezone, Target, and Topic. For now, we don’t need to worry about the payload, just leave an empty JSON there because the field is required.
Pay attention to the Timezone and the cron expression at Frequency, that’s when your Target is going to be executed.
As a Target, we are going to use Pub/Sub, and when we press Create, we will be presented with our Scheduler Job information:
Cloud Scheduler enables you to even manually trigger the Target execution with the “Run now” option, but running it now will return an error. The execute-batch-process topic doesn’t exist yet, let’s fix that…
We will fix that error by creating our Pub/Sub topic, it’s going to be our mediator:
“Cloud Pub/Sub is a fully-managed real-time messaging service that allows you to send and receive messages between independent applications.”
Go to this page to start your Pub/Sub configuration.
Creating a Pub/Sub topic is as simple as filling the topic name!
When we press Create Topic our Pub/Sub mediator is ready to extend the hand to our next component.
“Google Cloud Functions is a lightweight compute solution for developers to create single-purpose, stand-alone functions that respond to Cloud events without the need to manage a server or runtime environment.”
So, let’s revise… once the Cloud Scheduler is triggered, it will publish a message to a Pub/Sub topic that will start the Cloud Function.
And finally, our Cloud Function is going to create a Compute Engine VM that will be responsible for running our batch workload.
This is the code we will use to run the Cloud Function.
Notice that we are using the @google-cloud/compute client library, this is the core of our execution engine, and the method createInstance is the one responsible for spinning up the machine that will run our batch process. The vmConfig attribute will contain all the instructions for our VM workload, let’s dig into it…
For our VM workload, we have chosen a really simple use case, which is sending a “Hello World” message to Stackdriver Logging.
“Stackdriver Logging allows you to store, search, analyze, monitor, and alert on log data and events from Google Cloud Platform and Amazon Web Services (AWS). Our API also allows ingestion of any custom log data from any source. Stackdriver Logging is a fully managed service that performs at scale and can ingest application and system log data from thousands of VMs. Even better, you can analyze all that log data in real time.”
Let’s see the code that the VM is going to execute:
Basically, we are doing 3 things here…
Line 1: sending our “Hello World“ message to Stackdriver Logging and telling it to write to the batch-execution key, so we can track it later.
Line 2: retrieving the gcp_zone from the internal metadata endpoint that will be needed on the next line.
To know more about Retrieving instance metadata go to the official documentation.
Line 3:
Who’s the best actor to know that the VM finished executing? That would be the VM itself!
So here we are deleting the VM after it’s done with the batch workload. This is how we make it serverless, or as close as you can get.
Sounds good, but how are we going to put that script inside the VM and then configure it on our Cloud Function?
Go to this page to start your Compute Engine configuration.
By going to the UI, we are going to input the following fields:
Note: Remember to use a f1-micro instance, because it has a free tier.
On the image above, everything was left with the default value, we are just going to select a pre-configured service account named compute-execute-batch-job, this is important because the default service account doesn’t have the right permissions to delete Compute Engine VM’s.
Looking at how the service account was configured, best practices were followed by adding only the necessary roles for this kind of workload:
This is how the Service Account was created using the gcloud command line, you are also able to do this by using the Cloud Console.
To know more about Service Accounts and Roles go to the official documentation.
And after the service account, scrolling down on our compute engine UI, we have a Startup Script field where we are going to paste our workload script:
Remember one thing, it’s the Cloud Function that’s going to create our VM, so we are not going to click on the Create command, we will instead click on the Equivalent REST option, pointed on the image above.
By doing that, it will present us the full JSON that can be used in an API call:
Now that we have what we need to wrap up our Cloud Function with the VM configuration, let’s go back and update it, with the JSON from the REST call:
Note: Some fields from the JSON can be removed to use the default values, so it becomes smaller, but I want to show how simple it is, you can just copy and paste.
Remember to remove the double quotes from the key of the JSON attributes and make sure that the zone value being used in the vmConfig attributes are the same as the const zone in line 5. Also, replace the const projectId to use your project.
We are almost there! We just need now to deploy our Cloud Function and connect it to the Pub/Sub topic, let’s do it using the UI.
Go to this page to start your Cloud Function configuration.
Looking at the UI:
Mainly we have 3 important fields: Trigger, Index.js, and Function to Execute.
Trigger: here we can select options to call the Cloud Function, we will choose our Pub/Sub Topic execute-batch-process.
Index.js tab: this field contains the code that will be executed when the Cloud Function is called, so paste the create_instance_function code here.
Function to execute: use the name of the method that will be called when the Cloud Function run, createInstance.
One small thing that sometimes is forgotten, since Google Cloud is managing everything for us with Cloud Function, we also need to provide the package.json like a standard NodeJS application, since we are using javascript in this case.
Here’s the code:
We are adding it to the package.json tab:
To be good citizens, we will follow the best practices again and use a service account with just the amount of permissions that are needed:
The Service Account function-create-vm was configured with a Service Account User Role and a Custom Role containing the following permissions:
Ok now we can press Create 😄, we are ready to test everything together!
We will see something like this:
I love it when I see a ✔️ icon!
If we go back now to our Cloud Scheduler Job, and trigger it manually we can see everything working together.
Go to the Compute Engine page after a few seconds and you will see a new VM running with the prefix batch-job-executor followed by the execution time, it’s a little trick so we always have a unique name, if we need to track problems later.
After a few more seconds you will see that the icon before the VM name changed, that’s because the VM is being deleted, once the deletion is done the VM will be gone from the instances page.
Finally, to make sure it actually did something, we are going to Stackdriver Logging page, and when we filter the batch-execution key we can see our Hello World message! 👌🏻
Remember that this batch workload will be running on a scheduled basis according to the cron expression programmed at the Chould Sheduler Frequency.
This is the first post of a series showing how to run batch workloads in a simpler way, using Google Cloud Platform. It’s important to point out that here we used Compute Engine, but you are also able to run batch processes using other components like App Engine with task queue, GKE and the newest member of GCP’s Compute family Cloud Run.
On this post, we showed a really simple batch workload to help you get started, and to make it serverless we made sure that we deleted the VM after it was done executing. Thinking about it we used other serverless components such as Cloud Scheduler, Pub/Sub, and Cloud Functions, the only difference was that GCP was managing the resources and creating and deleting those for us… Using a few words from a friend of mine: “A serverless solution is never serverless. there is always a server behind the scenes…”
Thank you for your time! And stay tuned for the next post, where we will show a more complex workload adding Docker and Container Registry to this solution. Cheers!
[Update] Part 2 has been posted:
Adding Docker and Container Registry to the mix
[Update] Google announced serverless workflows
Google Cloud community articles and blogs
763 
7
763 claps
763 
7
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
software engineer & google cloud certified architect and data engineer | love to code, working with open source and writing @ alvin.ai
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://open.nytimes.com/play-by-play-moving-the-nyt-games-platform-to-gcp-with-zero-downtime-cf425898d569?source=search_post---------246,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Recently I wrote about moving the platform behind The New York Times Crossword to the Google Cloud Platform and mentioned we were able to cut costs in the process. I did not get to mention the move occurred during a timeframe where our traffic more than doubled and that we managed to do it with zero downtime.
Even from the start, we knew we wanted to move away from our LAMP stack and that its replacement would likely be written with the Go programming language, leaning on GCP’s abstractions wherever possible. After much discussion, we came up with a microservice architecture and a four-stage process for migrating public traffic over to it. We drafted an RFC and distributed it internally to get feedback across the company and from our Architecture Review Board. Before long, we were ready for stage 1 and about to run into our first round of surprises.
For the initial stage, we wanted to simply introduce a new pure proxy layer in Google App Engine (GAE). Since all nytimes.com traffic flows through Fastly, we were able to add a rule to point all crossword traffic at a new *.appspot.com domain and proxy all traffic into our legacy AWS stack. This step gave us ownership over all of our traffic so that we could move over to the new stack, one endpoint at a time, and monitor the improvements along the way.
Of course, right off the bat we ran into issues, but for the first time ever, we also had an array of tools to let us peer into our traffic. We found that some web customers were unable to access the puzzle, and found the cause of the problem to be App Engine’s limit on the size of outbound request headers (16KB). Users with a large amount of third-party cookies had their identity stripped from the proxied request. We made a quick fix to proxy only the headers and cookies we needed and we were back in action.
The next problem came from our nightly traffic spike, which occurs when the next day’s puzzles are published at 10pm Eastern time. One of App Engine’s strengths is auto-scaling, but the system was still having problems scaling up fast enough for our 10x+ jump over the course of a few seconds. To get around this, we use an App Engine cron task combined with a special endpoint that utilizes an admin API to alter our service’s scaling settings right before we expect a surge in traffic. With a handle on these two problems, we were ready to move to the next stage.
Between all of NYT’s puzzles and game progress for all of our users, there was a lot of data in our existing system. In order to smooth the transition to the new system, there needed to be a mechanism to replay all of our data and keep it in sync. We ended up using Google PubSub to reliably push data into our new stack.
While we were able to rely on PubSub’s push-style subscriptions and App Engine for the majority of our data, we did have one use case that was not a good fit for GAE: generating PDFs for our puzzles. Go has a nice PDF generation library but some of the custom fonts we needed to use led to unacceptable file sizes (>15MB). To get around this, we had to pipe the PDF output through a command-line tool called ghostscript. Since we could not do this on App Engine, we added an extra hop in our PubSub flow and created a small process running on Google Container Engine (GKE) that listens to PubSub, generates the PDF, and then publishes the file back out to PubSub, where it is consumed by the “puzzles” service and saved to Google Datastore.
This is the stage where we learned a lesson on managing costs when doing heavy work in Google Datastore. The database uses the count of entity reads and writes to determine costs and, while replaying all of our historical game play, our user statistics were getting signalled to be reaggregated almost constantly. This reaggregation led to many collisions and recalculation failures which unexpectedly resulted in us spending thousands of dollars one weekend. Thanks to Datastore’s atomic transactions, we were able to toss a locking mechanism around statistics calculations, and the next time we replayed all user progress to the new environment, it was a fraction of the cost.
With our data reliably synced in near-realtime, it was time to start turning on actual endpoints in GCP.
Soon after data began to sync over to the new stack, we started making changes at the “edge” service to point to our newer implementations, one endpoint at a time. For awhile we were at a pace where we were confidently switching over one endpoint a day.
Rewriting existing endpoints to the new stack wasn’t our only job during this timeframe. We also had a new, read-only endpoint to implement for the new iOS home screen. This new screen required a mix of highly cacheable data (i.e. puzzle metadata) and personalized game data (i.e. today’s puzzle solve time). We have two different services for hosting those two different styles of data in our new stack and we needed to combine them. This is where our “edge” service became more than a dumb proxy and enabled us to combine information from our two sub-services.
In this stage, we also replatformed the endpoints in charge of saving and syncing game progress across multiple devices. This was a major step as all related endpoints dealing with user statistics and streaks also had to be migrated. The initial game progress launch was a little rockier than we had hoped. One endpoint was experiencing much higher than expected latency and a plethora of odd edge cases popped up. In the end, we were able to cut out an unneeded query to remove the extra latency on the slow endpoint but the edge cases were a bit tougher to chase down. Once again, thanks to the observability tooling available in Google App Engine, we were able to track down the worst of the bugs and we were back to smooth sailing.
Once the systems around puzzle data and game progress were stable and running purely on Google’s infrastructure, we were able to set our sights on the final component to be rewritten from the legacy platform: user and subscription management.
Users of the crossword app are allowed to purchase their subscription directly through their device’s app store. (For example, an iPhone user can purchase an annual NYT Crossword subscription directly from the iTunes store.) When they do so, their device is given a receipt and our games platform uses that receipt to verify the subscription when the app is loaded.
Since verifying such a receipt is a task that could possibly be used by other teams at The New York Times, we decided to build our “purchase-verifier” service with Google Cloud Endpoints. Cloud Endpoints manages authentication and authorization to our service so another team in the company could request a API key and start using the service. Given an iTunes receipt or a Google Play token, this service tells us if the purchase is still valid and when it will end. To authenticate direct NYT subscribers and to act as an adapter to translate our existing authorization endpoints to match the new verification service, we add a small “ecomm” service in the mix.
The final public endpoint went live on GCP a little over 2 months ago and we’ve been actively resolving small edge cases and tuning the system for maximum efficiency and costs. Thanks to GCP’s observability tooling, it’s not uncommon for the platform to have a day with 99.99%+ success rates and far lower latencies than we had in the past.
We still have a PHP admin component running in AWS for managing our system’s assets and feeds, but we’re currently redesigning and rewriting it to run on App Engine. Our next iteration is already reading and writing to a Google Cloud SQL instance so we hope to be out of AWS completely in the coming months.
For the future of the games platform, we’re looking into adding some new exciting features like social leaderboards and real-time collaborative multiplayer crosswords. By leaning on managed solutions from Google Cloud Platform like Firebase’s Realtime Database, we’ve had some very successful prototypes and we hope to have them available to the public sometime next year.
If you’ve been intrigued by some of the engineering behind The New York Times, you may be like to know that we’re currently hiring for a variety of roles and career levels:
How we design and build digital products at The New York Times
735 
7
735 claps
735 
7
Written by
Senior Software Engineer at Datadog. A user of Go. A lover of cats.
How we design and build digital products at The New York Times.
Written by
Senior Software Engineer at Datadog. A user of Go. A lover of cats.
How we design and build digital products at The New York Times.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://read.acloud.guru/deploy-a-docker-swarm-cluster-on-gcp-with-terraform-dc1c40bb062e?source=search_post---------247,NA
https://medium.com/paypal-tech/400-days-paypals-data-warehouse-migration-to-google-bigquery-8c3b845eb6c9?source=search_post---------248,"There are currently no responses for this story.
Be the first to respond.
The first in a multi-step Google Cloud Platform journey
By Romit Mehta, Vaishali Walia, and Bala Natarajan
“Take the first step in faith. You don’t have to see the whole staircase. Just take the first step.” — Dr. Martin Luther King Jr.
PayPal has experienced record growth since the beginning of the global pandemic. To keep up with the demand from growth, we decided to migrate PayPal Analytics platforms to the public cloud. The first big migration of a warehouse workload to BigQuery in Google Cloud took less than a year. Along the way, the PayPal team built a platform which would support many other use cases as well.
This writeup captures a milestone migration experience. We migrated half of our data and processing from Teradata systems to Google Cloud Platform’s BigQuery.
As organizations and consumers ventured into new ways of doing business during the pandemic, PayPal experienced record-high transaction volumes. This put a lot of pressure on the offline analytics systems used for compliance, risk processing, product and financial analytics, marketing, customer success, and fraud protection. These analytics systems were all in on-premises data centers. The systems were powered by Teradata and Hadoop at the core, with additional software and workflows in place to manage resources across these systems.
Demands for processing data were far outstripping the existing capacity on-premises. Adding capacity quickly during the pandemic had its own share of challenges. Data platform teams managed the crisis with manual intervention to prioritize various workloads that demanded additional processing time. Given the business outlook of continuing growth, PayPal realized that the analytics ecosystem required a change.
Additionally, we recognized the opportunity to modernize our data strategy in accordance with the ideas of better agility, discoverability, shareability and ecosystem integration. BigQuery allows us to centralize our data platform without losing capabilities such as SQL access, Spark integration, and advanced ML training. Also, BigQuery has some advanced features such as ML and real-time analytics that can be leveraged without moving data out to another system.
There were multiple factors that PayPal took into consideration in choosing cloud over on-premises expansion. PayPal’s data team started a blueprint for migration to public cloud, to keep up with data demands for the next five years based on Google Cloud Platform’s capabilities.
PayPal’s analytics infrastructure is based on a constellation of technologies for various use cases. Data analysts and a portion of data scientists predominantly depended on a data warehouse for their data work. The data in the warehouse was semi-structured, making it easier for teams to do analysis and reporting.
A simplified view of the data flow is provided in the following figure. Data from site databases first goes into the data warehouse. A copy of some of the data from the warehouse is made into a data lake which is powered by open source technologies. Data is then embellished with other data sources like tracking, experimentation, and data from PayPal’s adjacencies to get transformed and loaded back into the analytics warehouse for consumption.
PayPal managed two vendor-based data warehouse clusters on-premises, with a total storage of 20+ petabytes serving over 3,000 users. The capacity requirement was constantly growing as data became critical for business decisions. The analytics warehouse was limited by storage and CPU, and the main warehouse was limited by IO and storage.
Warehouse use cases could broadly be classified into interactive and batch workloads. Interactive workloads include ad-hoc queries from users using Jupyter Notebooks, and reports and dashboards using BI tools like Tableau and Qlikview. Batch workloads are scheduled using Airflow and UC4. Workloads are mostly written in SQL and executed using shell or Python scripts.
Due to challenges that arose as traffic grew, many of the transformation jobs and batch loads were running behind schedule. PayPal analysts and data scientists were seeing data way behind their service-level agreements (SLAs), with a degraded experience, and all of that delayed decision-making. Of the two major warehouses, PayPal decided to first migrate the analytics warehouse to BigQuery to experience using the service as a replacement for Teradata, and in the process build a platform around Google Cloud Platform services for PayPal’s data users. The main warehouse would be migrated later based on the learnings and experience from the analytics warehouse.
Technological challenges
Improving data users’ experience at PayPal involves addressing the following technological challenges:
Adoption Challenges
Changes to infrastructure need to overcome the following adoption challenges:
Given the list of challenges PayPal had to address, it was very clear that creating new on-premises solutions would be a problem. The building blocks for robust solutions were focused around the cloud with less support for on-premises infrastructure. Additionally, scaling requires buying hardware and the long lead times were a hindrance to business enablement. PayPal was already moving significant workloads to Google Cloud Platform which made the choice of moving the analytics platform to Google Cloud Platform easier. We evaluated various vendors that offered their services on Google Cloud Platform to see if they could solve some of the technological challenges mentioned earlier, and we narrowed down the choice to BigQuery. We ran a 12-week evaluation of BigQuery to cover different types of use cases. It performed well against the success criteria we targeted. A summary of the evaluation results is provided below.
We will be writing about the evaluation process, success criteria and the results in a separate post.
As part of our blueprint, we decided to tackle the “Analytics Warehouse” shown in Figure 1.
Once we chose which cloud and warehouse to explore, we identified the following tracks and started getting to the next phase.
Customer Contact
We reached out to users of the warehouse based on their usage stats over the past 12 months, as well as the data providers in that cluster. We set up time, walked them over the decision and sought their buy-in for this migration. This stakeholder buy-in was important for us to have a successful migration. We explained the rationale and how we planned to approach the problem. Some users were excited and wanted to be closely involved with the migration effort. We identified one team within one business unit as an early adopter and focused our migration effort on their use cases and data requirements.
Secure Infrastructure Buildout
We built a secure infrastructure to move data to the cloud. We kept data within BigQuery as US, multi-region to be accessed from other regions in the US. We implemented a secure private interconnect between our data center and the region in Google Cloud Platform nearest the analytics warehouse. Since we expected to operate in a hybrid mode (other connected systems remain on-premises for the foreseeable future), a private interconnect with no egress costs was a better option.
We decided to secure our data using PayPal-provided private keys in BigQuery, within a service perimeter offered by Google Cloud Platform. This ensured that data was secure and within a perimeter which cannot be accessed from outside. We deployed automation to prevent accidental creations of data sets that lack encryption keys. This way, we had encryption enabled by default for all data stored in Google Cloud Platform in a way that was compliant with our internal policies and external regulations.
We have used this infrastructure to copy more than 15 petabytes for BigQuery and 80+ petabytes into Google Cloud Services for various use cases. We use the same network infrastructure for users to access BigQuery through Jupyter Notebooks, Tableau, or from their scheduled jobs.
Regulatory Compliance and Pen-Testing
As a fintech organization that deals with PCI and PII data elements in our datasets, we worked with various regulators to file our intent to move data to the cloud. PayPal’s InfoSec, regional business units and legal teams worked overtime with our teams to prepare paperwork for regulators. We followed up with rounds of pen-tests to ensure there were no gaps. This helped us validate what we had designed into the infrastructure to secure the data and access to the data.
DDL (Data Definition Language) and SQL Conversion
Given that we were taking our data users to the cloud with a new technology, we wanted to ease the pain of transitioning from Teradata to BigQuery. To achieve this, we evaluated various options and selected a tool from CompilerWorks. Its transpiler allowed us to create DDLs in BigQuery, and use that schema to convert DMLs and user SQLs from Teradata flavor to BigQuery. PayPal worked to harden the transpiler configurations to generate performant, clean BigQuery-compatible SQLs.
This automated code conversion was a very critical step for us to get right as we wanted to ease the migration for our users. In addition to the code conversion, we also extracted valuable lineage data from CompilerWorks’s tool. We created an automation framework as well as a portal for interactive use and self-service code conversion. The automation kept polling for changes in on-premises infrastructure and create the equivalent in BigQuery as new artifacts got created. We requested users to use the portal to convert their existing or known SQLs to BigQuery-compatible SQLs for their testing and validation. We leveraged the same framework to convert users’ jobs, Tableau dashboards, and Notebooks for testing and validation. This automation helped us convert more than 10K SQLs.
Workloads, Schemas and Tables Identification
To scope the workloads, the team went through all Notebooks in our repository, Tableau dashboards and UC4 logs. Based on the tables we identified, we created a lineage graph to come up with a list of tables and schemas used, active scheduled jobs, notebooks, and dashboards. We validated the scope of work with users and confirmed that it was a true representation of the workloads on the cluster. This helped the team greatly reduce the number of workloads that we needed to migrate. Here is a breakdown of what was deprecated from the overall inventory.
Spending time on automation helped us separate out the used ones from unused ones and gain validation from users at the last step. Having users manually find this out would be tedious and error prone.
Data Movement, Loading and Validation
As we worked through this project, it became clear that data movement is very contextual to our setup, and off-the-shelf tools had limitations in being able to seamlessly copy data to Google Cloud Platform. This was the hardest part of the whole project. It was not the volume but accidental complexities that made it harder. Here is a quick list of problems we encountered:
Dry Runs and Wet Runs
Dry runs, an execution with no data, ensured that there were no syntax errors with the queries that were transformed. If the dry run was successful, we loaded data into the tables and requested users to conduct a wet run. Wet runs were one-off executions to test whether the result sets were all correct. We created test data sets for users to do their wet run, before they qualified their workload for production. All of this were enabled for users using our application lifecycle management portal which our users were accustomed to for deploying applications. We laid lots of emphasis on fitting our testing into an ecosystem that our users were used to.
Visibility into Progress
A number of the above activities were happening in parallel. This required coordination, which is harder for humans or coordinated spreadsheets. We kept track of all the data in BigQuery which was automatically updated as executions happened. We created dashboards to track sequences of activities and to report consistently to our execs and stakeholders on progress. These dashboards tracked the data copy progress for multiple milestones, workload rationalization and progress on readiness of notebooks, scheduled jobs and BI dashboards for dry and wet runs. A sample report looked like this. Users were able to search by DB name and table names for checking status.
Team Work Makes the Dream Work
This was very true in our case since many teams across PayPal came together to get to this landmark. We believe the following makes our story unique, and that helps us succeed:
Our user community at PayPal has transitioned and taken to BigQuery really well. Users needed initial help with project conventions (a new concept for them compared to Teradata) and with some help, they got productive very quickly. Users absolutely love the query performance, faster data load times and full visibility through BigQuery logs.
Taking PayPal through this migration helped us evaluate and observe what BigQuery and Google Cloud Platform can bring to PayPal. Data users now use SQL, as well as Spark through Notebooks and Google’s Dataproc over BigQuery. This helps us maintain a single copy of data along with visibility that Google Data Catalog provides for our data.
Plans are afoot to consolidate multiple datasets from finance, HR, marketing and third party systems like Salesforce along with site activity into BigQuery to enable faster business modeling and decision making. Teams are looking at streaming capabilities to inject site data sets directly to BigQuery for near real time usage to our analysts. In addition to BigQuery, some of our teams are also leveraging Google DataProc and Google Cloud Storage to consolidate many pieces of our open-source based data lake shown in Figure 1.
There are many employees at PayPal who worked directly and indirectly on this effort. Many employees in our India offices spent time on this effort while dealing with the raging pandemic. Our thanks to all of them!
Article credits:
Many thanks to Vaishali Walia who leads this program, and the entire Deloitte team for helping keep the migration on track.
Also thanks to Bala Natarajan who provided input for this article, and Melissa O’Malley, Michael Davis, and Parviz Deyhim who helped make it publication ready.
The PayPal Technology Blog
629 
6
629 claps
629 
6
The PayPal Technology Blog
Written by
Product Manager, Data Platform Products @ PayPal
The PayPal Technology Blog
"
https://medium.com/the-node-js-collection/processing-a-large-dataset-in-less-than-100-lines-of-node-js-with-async-queue-9766a78fa088?source=search_post---------249,"There are currently no responses for this story.
Be the first to respond.
Sara is a Developer Advocate on Google’s Cloud Platform team, focusing on big data and machine learning. She helps developers build awesome apps through demos, online content, and events. When she’s not programming she can be found on a spin bike, listening to the Hamilton soundtrack, or finding the best ice cream in New York.
As a developer advocate at Google, one of my favorite things to do is explore new datasets. Node.js is usually my go-to platform for analyzing this data and building apps on top of it. Last week, I was working with a particularly large dataset of images and needed a way to send each image to an image analysis API and write the JSON response to a local file.
The problem? My first version of a script to process the images worked fine for ten images and even hundreds of images. But when I tried to run it on the entire dataset of 200,000 images I quickly ran up against the call stack limit. Luckily I sit next to Myles (resident Node.js expert and core team member) who recommended I try the npm package async, not to be confused with the language feature of the same name.
If you’re more of a skip to the code person, check out the gist here.
To fix the call stack issue I needed to manage my API calls by pushing them into a queue where they could be processed in parallel. To be completely honest, having never worked with queues in Node.js before I was slightly intimidated by the thought of rewriting my script from scratch. But once I started queueing, I had something working in a few minutes (seriously) and my queue fears quickly disappeared.
My image IDs are in a newline delimited JSON file. First I convert this file into a JSON object using readFileSync. The object contains a list of image IDs and in my queue I want to send each image to the Vision API. The queue takes a task (in this case my object of image IDs) and a callback function, called when the worker is finished processing:
The queue takes a function and a concurrency number as parameters. Let’s start with the function: we pass it a task (our image ID from above) and a callback, which will be called when the worker completes a task. Inside the function is where I do my image processing. This function should return some JSON about the image which I want to write to a local JSON file. I’ll define that in the next step.
Concurrency tells Node.js the maximum number of workers to process our task in parallel. I played with this number until I found a balance of something that wasn’t too slow, but also didn’t result in API limits or call stack errors. The number will vary depending on what you’re doing, so it’s definitely ok to fine tune it by hand until you find your “magic number.” Here’s my queue:
Last, it’s time to write the callVision() function referenced above. This part isn’t exactly async.queue specific, but it’s still important because it’s the meat of my queue task. Here I’m using Google’s Cloud Vision API for image analysis, and I use the Google Cloud Node.js module to call it. Once I get a JSON response for each image, I create a JSON string of the response to write to a newline delimited JSON file (I’m using this format because it’s what BigQuery expects, which is where I’ll be storing the data eventually). Once this function completes, the data is sent back to the queue where it is written to my local JSON file. You can find all of the callVision() code in the gist.
That’s it! Let me know if you’ve done something interesting with async.queue in the comments, or find me on Twitter @SRobTweets.
Community-curated content for the millions of Node.js
781 
7
781 claps
781 
7
Community-curated content for the millions of Node.js users.
Written by
Connoisseur of code, country music, and homemade ice cream. Helping developers build awesome apps @googlecloud. Opinions = my own, not that of my company.
Community-curated content for the millions of Node.js users.
"
https://medium.com/google-cloud/the-hidden-costs-of-cloud-ddb702495e93?source=search_post---------250,"There are currently no responses for this story.
Be the first to respond.
At Server Density, we just completed a 9 month project to migrate all our workloads from Softlayer to Google Cloud Platform. This started with a single service using Cloud Bigtable for large scale storage of time series monitoring data and culminated with our entire product now running on GCP.
The move went smoothly and we’re very happy to be on Google Cloud. Although they have fewer products compared to the market leader (AWS) we think that Google Cloud products are better designed, the pricing models are easier to work with and there are regular releases of innovative new features…
"
https://towardsdatascience.com/scheduling-data-ingest-using-cloud-functions-and-cloud-scheduler-b24c8b0ec0a5?source=search_post---------251,"Sign in
There are currently no responses for this story.
Be the first to respond.
Lak Lakshmanan
Nov 20, 2018·4 min read
As Google Cloud continues to evolve, some of the solutions I presented in my book “Data Science on the Google Cloud Platform” get superseded because easier, more powerful solutions become available.
For example, a few months ago, I showed how to build regression and classification models using just SQL. The availability of highly scalable machine learning without having to move your data means that we can explore the value of machine learning quite easily. Were I to write the book today, I would have inserted a section on BigQuery ML into Chapter 5 (interactive data exploration).
In this article, I will talk about a second update: a better way to do periodic data ingest than what I presented in the last section of Chapter 2.
In the last section of Chapter 2 of the book, I presented a solution to schedule monthly downloads. This consisted of five steps:
Step 1 was done in the book, and I can simply reuse that Python program. The rest of the steps have gotten easier. A lot easier.
Instead of building a Flask web app and running it in AppEngine, there is a simpler way to create a web endpoint that is accessible via Http. The new way is to use Cloud Functions. I can use the same Python code that I used for ingest, but wrap it in a file named main.py (all the code in this article is on GitHub):
Essentially, my main.py has a single function that receives a Flask request object, from which I can extract the JSON payload of the HTTP Post by which the Cloud Function will be triggered.
I get the next month by looking to see what months are already in the bucket and then ingest the necessary data using the existing code in ingest_flights.py.
Once I have the main.py written, deploying the Cloud Function can be done via gcloud:
We can test the Cloud Function by sending it a curl request:
As the code above suggests, the URL of the Cloud Function is wide open. To somewhat secure the URL against denial-of-service attacks, we should change the URL to be something unguessable.
To make the URL unguessable, use the openssl library to generate a 48-character string, remove non-alphanumeric characters and trim the result to 32 characters:
This, by itself, is insufficient. We should also insist that legitimate callers provide us a token as part of the payload. Again, we can generate a token using the openssl program and add the check to main.py:
Doing both these things — an unguessable URL and checking for a token inside the Cloud Function — help to secure the Cloud Function.
Now that the Cloud Function provides an http endpoint that will launch the ingest job, we can use Cloud Scheduler to access this endpoint once a month:
The scheduler takes many formats for the schedule, including the format of Unix’s crontab, but I find the plain-language format supported by AppEngine’s cron the most readable. So, our endpoint will be accessed on the 8th of every month at 10am US Eastern time.
If you look at ingest_flights.py, the ingest method does quite a few things. It downloads the file, unzips it, cleans it up, transforms it and then uploads the cleaned up, transformed file to Cloud Storage.
Now that we are using Cloud Functions, it might be better to redesign this to be less monolithic. Besides being triggered by http calls, Cloud Functions can also be triggered by the addition of files to a bucket.
So, we could have the first Cloud Function simply upload the unzipped files to Cloud Storage, and then have a second Cloud Function carry out the extract-transform-load (ETL) part of the ingest. This might be more maintainable especially if it turns out that we have a bug in the ETL section. The original, raw data is available to rerun the ETL job.
Data Analytics & AI @ Google Cloud
376 
12
376 
376 
12
Your home for data science. A Medium publication sharing concepts, ideas and codes.
"
https://medium.com/@asanoop24/deploying-angular-6-app-on-google-app-engine-b6259d4c16c2?source=search_post---------252,"Sign in
There are currently no responses for this story.
Be the first to respond.
Anoop Sharma
Jun 21, 2018·3 min read
I am relatively new to the Cloud Platforms such as Google Cloud Platform, AWS, Microsoft Azure and had been struggling to deploy my applications over there. This article is for everyone who’s going through the same issue.
I had developed an Angular application recently with NodeJS as back-end and had been trying to deploy that on Google Cloud Platform (GCP). It took me about 50 articles and a hell lot of googling to find out exactly what needed to be done to deploy a simple Angular App on GCP. Hope you avoid going through all that trouble after reading this article.
Basically, Google offers 2 options to deploy your application:
In this article, I will be focusing on deployment using Google App Engine, where Google manages the infrastructure for you. You just need to upload your files to the Storage and then let Google do all the heavy-lifting for you. So let’s get started!
Go to your Angular App directory and run the following command to make your app production ready
This will create a dist folder in your project directory. The contents of the folder should basically look like this:
Now you have to create a config file for your project that will be used by Google Cloud App Engine to deploy the project.
Create a blank file in your project directory and name it as app.yaml
Paste the following contents as it is in the file:
Now, your app is ready to be deployed. The next step would be to make your Google Cloud ready for deployment. So let’s go ahead and do that.
Now your cloud is almost setup. You just need to upload your angular production build into this bucket and deploy it. Let’s see how to do that.
Hope this was helpful and saved you a lot of time googling all this stuff. There are a couple of more ways to deploy your angular application. Will cover those in next articles. Please post any questions below in the comments section. Will be glad to answer.
See all (21)
664 
24
Public domain.

664 claps
664 
24
About
Write
Help
Legal
Get the Medium app
"
https://towardsdatascience.com/data-lake-on-gcp-using-terraform-469062a205ad?source=search_post---------253,"Sign in
There are currently no responses for this story.
Be the first to respond.
Tuan Nguyen
Aug 15, 2020·9 min read
Back in the old days, dealing with physical infrastructure is a huge burden, which not only requires teams of experts to manage but also is time-consuming. In the modern cloud computing era, however, you can deploy hundreds of computers instantly to solve your…
"
https://towardsdatascience.com/how-to-use-jupyter-on-a-google-cloud-vm-5ba1b473f4c2?source=search_post---------254,"Sign in
There are currently no responses for this story.
Be the first to respond.
Lak Lakshmanan
Apr 11, 2019·8 min read
Note: The recipes in this article will still work, but I recommend that you use the notebook API now. Do:
gcloud beta notebooks --help
The simplest way to launch a notebook on GCP is to go through the workflow from the GCP console. Go to AI Platform and click on Notebook Instances. You can create a new instance from the user interface:
Once the instance is launched, you can click on a link to open JupyterLab:
When the instance is launched, it has a persistent disk. That disk will hold your notebooks. You can stop and restart the VM (from the GCP web console) without losing those notebooks.
Note that you can attach a GPU to a notebook instance from the user interface:
Enjoy!
This article is a collection of a few of my “recipes” for working with Notebook instances.
The instance is a Compute Engine image, so if you want to script things out, customize the machine, change its firewall rule, etc. you can use Compute Engine capabilities. The notebook instance is a Deep Learning VM, which is a family of images that provides a convenient way to launch a virtual machine with/without a GPU on Google Cloud. It has Jupyter Lab already installed on it and you can access it without the need for proxies or ssh.
The simplest approach is to specify an image family (see the docs for what the image families are available). For example, you can get the latest image in the tensorflow-gpu family with a P100 GPU attached using:
The URL to access Jupyter Lab is part of the metadata of the VM that you just launched. You can get it using:
Here’s a script that will do steps #1 and #2, waiting until the Jupyter notebook server has started:
Simply navigate to that URL and you’ll be in JupyterLab.
Click on the last icon in the ribbon of icons in the left-hand pane and you will be able to git clone a repository. Use the one for my book:
https://github.com/GoogleCloudPlatform/data-science-on-gcp
Navigate to updates/cloudml and open flights_model.ipynb. You should be able to run through the notebook.
You can also open up a Terminal and use git clone, git checkout, git push, etc. I tend to find it easier than using the built-in Git UI. But your mileage may vary!
You can specify a set of operations to run after Jupyter launches. These will be run as root.
In general, use the image-family approach for development (so that you are always developing with the latest of everything), but pin down to a specific image once you move things to production. The reason you want to pin down to a specific image in production is that you want to run on a version that you have actually tested your code with.
Get the list of images and find the one you were using (the latest in the image family you specified above):
Then, specify it when creating the Deep Learning VM (lines you might want to change are bolded):
The key aspect here is to launch papermill with a startup script and exit the notebook VM using TERMINATE without a restart-on-failure once papermill is done. Then, delete the VM. See this blog post for more details.
To create a Deep Learning VM attached to a TPU, first create a Deep Learning VM and then create a TPU with the same TensorFlow version:
The only difference when creating the Deep Learning VM is that you are specifying the TPU_NAME in the startup script.
If you create a Deep Learning VM and you specified a GCP login name (all my examples above, except for the production one did so), then only you (and project admins) will be able to ssh into the VM.
All Jupyter notebooks will run under a service account. For the most part, this will be fine, but if you need to run operations that the service account doesn’t have permission to do, you can have code in Jupyter run as you by doing the following:
Note: Do not use end-user credentials unless you started the machine in ‘single user mode’.
Creating an image in the tf-latest family uses the latest stable TensorFlow version. To work with TF-nightly (e.g. this is how to get TensorFlow 2.0-alpha), use:
Restarting Jupyter: Usually, all you need to do is to restart the kernel by clicking on the icon in the notebook menu. But once in a long while, you might completely hose the environment and want to restart Jupyter. To do that, go to the Compute Instances section of the GCP Console and click on the SSH button corresponding to your Notebooks instance. In the SSH window, type:
Startup logs: If Jupyter failed to start, or you don’t get a notebook link, you might want to look at the complete logs (including startup logs). Do that using:
The TensorFlow images use pip, but the PyTorch images use conda. So, if you want to use conda, the PyTorch images are a better starting point.
If you want to develop using the Deep Learning VM container image on your local machine, you can do that using Docker:
If you have a GPU on your local machine, change the image name from tf-latest-cpu to tf-latest-cu100.
__________________________________________________
I’m interested in expanding on these recipes. Contact me if you have a suggestion on a question/answer that I should add. For your convenience, here’s a gist with all the code.
Data Analytics & AI @ Google Cloud
See all (63)
417 
9
Every Thursday, the Variable delivers the very best of Towards Data Science: from hands-on tutorials and cutting-edge research to original features you don't want to miss. Take a look.
417 claps
417 
9
Your home for data science. A Medium publication sharing concepts, ideas and codes.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/updating-google-container-engine-vm-scopes-with-zero-downtime-50bff87e5f80?source=search_post---------255,"There are currently no responses for this story.
Be the first to respond.
I often hang out on the Google Cloud Platform Slack, which is a great community for learning and discussing GCP. Here is a common situation I’ve seen multiple people run into:
“I want to use Cloud SQL / Datastore / PubSub / etc. from my pods running in Kubernetes Engine. I want to use the automatic VM credentials, but my VM doesn’t have the right scopes or permissions and I can’t change it! How do I fix this?”
All Google Compute Engine VMs come with a built in OAuth2 Service Account that can be used to automatically authenticate to various GCP services. You can choose which services this service account has permission to access by assigning different “scopes” to the account.
For example, you might give the service account the “devstorage.read_only” scope so it can only read data from Google Cloud Storage. You might give another service account the “devstorage.read_write” so it can read and write data. An account can have any mix of scopes, so you can give it the exact permissions it needs to get the job done. You can find a list of all Google scopes here.
This service account is often referred to as “Application Default Credentials”
̶O̶n̶c̶e̶ ̶y̶o̶u̶ ̶c̶r̶e̶a̶t̶e̶ ̶t̶h̶e̶ ̶i̶n̶s̶t̶a̶n̶c̶e̶,̶ ̶y̶o̶u̶ ̶c̶a̶n̶’̶t̶ ̶c̶h̶a̶n̶g̶e̶ ̶t̶h̶e̶ ̶s̶c̶o̶p̶e̶s̶!̶ ̶T̶h̶e̶r̶e̶ ̶i̶s̶ ̶n̶o̶ ̶w̶a̶y̶ ̶t̶o̶ ̶f̶l̶i̶p̶ ̶o̶u̶t̶ ̶t̶h̶e̶ ̶A̶p̶p̶l̶i̶c̶a̶t̶i̶o̶n̶ ̶D̶e̶f̶a̶u̶l̶t̶ ̶C̶r̶e̶d̶e̶n̶t̶i̶a̶l̶s̶ ̶w̶i̶t̶h̶ ̶a̶n̶o̶t̶h̶e̶r̶ ̶o̶n̶e̶.̶ (This is no longer the case, Compute Engine now supports changing scopes on running instances. However, I still recommend following this guide. If the VM crashes and respawns or you change the cluster size, the new VMs will not have the updated scopes)
There are two solutions:
The first method is definitely more robust, as each pod can have its own service account if needed, but has the additional overhead of forcing you to manage these accounts. I won’t be covering this method in this post, but it is a great option if you want more control.
The second method means you don’t have to make any code changes or manage accounts, but you risk downtime while the new VMs start up.
With Google Kubernetes Engine, you can avoid this downtime if you follow some simple steps. Let’s take a look!
For this blog post, we are going to have a small 3-node Kubernetes cluster on Google Kubernetes Engine running a service backed by a deployment. The deployment will have 6 replicas.
Here are the nodes:
Here are the pods (modified to fit the screen):
You can see that the pods are distributed across the nodes.
Oh nooooo. The nodes don’t have the right permissions!
The first thing to do is create new nodes with the correct permissions. You can do this by creating a new node pool the same size as the old pool. This new node pool will sit alongside the old one, and new pods can be scheduled onto it.
Let’s say that your code needs the “devstorage.read_write” and “pubsub” scopes.
To create the new node pool, run the following command:
As always, you can customize this command to fit your needs.
Now if you check the nodes, you will notice there are three more with the new pool name:
However, the pods are still on the old nodes!
At this point, we could simply delete the old node pool. Kubernetes will detect that the pods are no longer running, and will reschedule them to the new nodes.
However, this will introduce some downtime to the application, since it will take time for Kubernetes to detect the nodes are down start the containers on the new hosts. This may only be a few seconds or minutes, but that might be unacceptable!
The better way would be removing the pods from the old nodes one at a time, and then remove the node from the cluster. Thankfully, kubernetes has a built in commands to do this.
First, cordon each of the old nodes. This will prevent new pods from being scheduled onto them.
Then, drain each node. This will delete all the pods on that node.
Warning: Make sure your pods are managed by a ReplicaSet, Deployment, StatefulSet, or something similar. Standalone pods won’t be rescheduled!
After you drain a node, make sure the new pods are up and running before moving on to the next one.
Once that is done, you can see that all the pods are running on the new nodes!
Now that all the pods are safely rescheduled, it is time to delete the old pool.
Replace “default-pool” with the pool you want to delete.
That’s it! You have updated your cluster with new scopes, and did it with zero downtime!
Google Cloud community articles and blogs
606 
4
606 claps
606 
4
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://estl.tech/deploying-redis-with-persistence-on-google-kubernetes-engine-c1d60f70a043?source=search_post---------256,"Google Kubernetes Engine (GKE) makes it very easy to create a Kubernetes cluster. It’s a few clicks away through Google Cloud Platform’s (GCP) web console, which even provides you with the corresponding gcloud command so you can just copy it into a script for next time.
After that comes the challenge of actually deploying stuff onto Kubernetes. The documentation isn’t too bad, but there are lots of concepts to pick up at once and it’s not always immediately clear how they all come together. Some of the names have changed too. For example, ReplicationControllers have been superseded by Deployments. Sometimes, the documentation says different things. A lot of step by step tutorials use very simple toy applications, usually stateless ones.
Perhaps I have been unfortunate with the tutorials I’ve come across. They usually go something like this:
For me, these tutorials were lacking because my app does have state to worry about. Tutorials that cover stateful applications usually don’t worry about how to keep the state beyond the lifecyle of the pod. For demonstration purposes, they declare a volume mount onto the host, so if the pod goes down and gets rescheduled onto another node, the data is toast. Not a problem in the tutorial, but unacceptable in real life.
This post will document how I deployed Redis with persistence for my own app. It’ll cover my exploration process, including what I found confusing, and links to the documentation which I found helpful.
Future posts will come as I get the rest of the app onto Kubernetes.
My colleagues who already have their apps on Kubernetes don’t really need persistence for their Redis instance. They use it for sessions and caching and if it’s gone, it’s fine. They deploy Redis as a StatefulSet with one replica and only allow it to run on one node.
For Coursemology, Redis is used to store the ActiveJob queue. Rails makes this so easy for developers. Here’s an example from Coursemology:
What this hides from developers is that the job has to be kept somewhere. In our case, it’s held in Redis and can be accessed through Sidekiq’s Scheduled Queue API. If the Redis data is lost, the scheduled job is lost and students will no longer be reminded that their assignments are due. Then my boss will send me an email asking what happened to the reminder emails. Not good.
Redis does not have SSL support . Coursemology’s current deployment uses SSH tunnels to secure communications between the app and the Redis server, which is deployed on a separate VM. This could probably be accomplished with a sidecar container in the same pod maintaining the tunnel, but I wanted to reduce the number of pets.
Redis also holds session information so users can maintain their login session even if the load balancer assigns their current request to a different app server.
You’ll need a Kubernetes cluster. The tutorials from the official documentation always have a section with instructions on how to get access to one to play with.
kubectl should also be installed and configured to talk to your cluster.
The examples below show what happens when Coursemology communicates with Redis, although some elements have been removed from the various listings for brevity. You can use your own app which needs Redis, or just use redis-cli to get and set keys for testing.
Some lines in the sample YAML files have dashes, while others don’t. Some of them seem to have more than one object inside because there’s more than one declaration of apiVersion . What’s that --- separating the sections?
I found answers to those questions in this blog post from Mirantis. If you’re new to YAML, it’s a great guide with specific examples for Kubernetes.
Let’s start with getting a Redis container running on the cluster. To ensure that everything is reproducible, all the configuration has been done with YAML files and sent to the cluster with the kubectl create -f somefile.yml command, even when kubectl could be used directly to achieve an objective.
I wrote this redis.yml file by referencing my colleague’s version, and also with the help of the StatefulSet tutorial. In particular, I was using a newer version of Kubernetes so the pod selectors are compulsory. The documentation talks about the necessity of pod selectors outside of the StatefulSet tutorial.
Copy the YAML above into a file named redis.yml, then run kubectl create -f redis.yml to create the Service and the StatefulSet on the cluster.
Run kubectl get statefulsets, kubectl get services and kubectl get pods to check the status of the Redis service.
When everything is up and running, you should see output similar to the one shown below.
Let’s get a shell to the container and try it out.
One question you might have now is what apiVersion to use for each object? I just followed the examples, but there’s a reference here with some explanation of the changes.
The location of the Redis host should be set by an environment variable in the app. In Kubernetes, environment variables can be set in config maps and passed to pods.
The relevant line should look something like this:
This was where I got confused by the documentation. According to the explanation of Services, environment variables and an optional DNS add-on are the two main ways of finding services. The environment variables worked but the documentation warns that there is an ordering requirement. That’s not ideal so I hoped to be able to use DNS instead.
DNS looked like it isn’t there by default and it looked rather tricky to install add-ons. However, at the bottom of the paragraph where it talks about ExternalName services, there’s a link to DNS pods. This was where I found the correct form for the service address.
It turns out that GKE does have DNS support, as mentioned on a GCP Solutions page. So while all the necessary documentation does exist and individual pages are fairly well written, it does take some repeated reading or prior knowledge of the concepts to figure out what parts are applicable.
According to the WordPress tutorial which I leaned on quite heavily for setting up persistence, I should have been able to just use the name redis. That’s also what the DNS section for the Services documentation suggests. However, I could not get it to work without the full hostname. Having just re-read the documentation, I finally noticed that the name lookup returns the cluster IP for the service. Since my Redis service definition has no cluster IP, that could be why using redis as the Redis host address did not work for me.
No persistence has been configured yet, but Redis is running and the app can connect to it. I can check that jobs can be scheduled and that they are added to the queue.
Let’s try deleting the pod by running kubectl delete pod redis-0. The StatefulSet will notice that there are now no Redis instances running and bring up the pod again. As the StatefulSet documentation notes, pods in a StatefulSet have an ordinal index and a stable network identity. Another pod named redis-0 will come up to replace the deleted one.
After it’s up, I tried reloading the dashboard and noted that the job had disappeared.
All the documentation for Redis persistence is available in one convenient place. There’s a dump file configured with the save option, and an Append Only Log. More details and a discussion of the trade-offs can be found in Redis’ documentation.
First, let’s figure out what save options have already configured. Get a shell to Redis then run config get <option> to see what’s going on. The relevant options for Redis persistence are save and appendonly:
Note that this configuration is not the same as Redis’ default configuration, which does specify some values for save. This is because we specified --requirepass to Redis. The Dockerfile for Redis’ official image points to an issue comment; it seems like specifying any argument to redis-server makes it assume you will specify everything. Thus, specifying a password has removed the configuration for persistence.
To specify persistence options, add them to Redis’ args in the redis.yml file. You can set multiple save points by specifying the option multiple times. For example:
This will create a snapshot after 900 seconds if at least 1 key changed, and after 30 seconds if at least 2 keys changed. It will also enable the Append Only File (AOF). For testing purposes, I have set a low save configuration of 30 2so the dump file gets created with just two changes.
Run kubectl replace -f redis.yml to update the StatefulSet and restart the Redis container. Now when you check Redis’ configuration, you should see:
Let’s view the files on the Redis container to see what’s going on.
When the container first comes up, a 0 byte appendonly.aof file is created:
After visiting the login page (which creates some session related keys):
The AOF file grows a little.
Wait a while (for the checkpoint to run):
The dump file is created.
After logging in:
The file sizes change.
Now delete the pod:
The StatefulSet recreates another pod with the same name. Let’s look inside again:
The data is gone. This is expected as no persistent storage was configured. I’ve also lost my session and have to login again, and the job has disappeared from Sidekiq’s dashboard.
Now let’s make the data persistent!
The persistent disk tutorial gives a nice example of how to create and mount a persistent disk.
You can create the disk through the console. When you’ve filled in the details, click on the command line link at the bottom to get a helpful popup with the gcloud command to run.
On the web console, the minimum disk size is 10 GB. However, this minimum is not enforced on the command line so you can create a 1 GB disk for testing purposes.
My Redis instance does not need much storage so I can run the following command to get a disk. Fill in your own values for — project and — zone:
If you used the command above to create the disk, the following message will appear when the disk has been provisioned.
There is no need to do this. GCP seems to automagically handle disk formatting when the disk is mounted to the container.
Edit redis.yml to add the disk to the StatefulSet under the volumes key. Add thevolumeMounts key to the Redis container so it will use the persistent disk.
The last part of redis.yml now looks like this:
Replace the Redis setup with kubectl replace -f redis.yml . This takes a bit longer now as the disk has to be attached to the cluster.
Create a shell on the Redis container. There should be an additional lost+found folder in the /data directory.
Repeat the tests for Round 2. Check the file sizes in the /data directory of the Redis container. Try logging in and creating jobs through the app.
Delete the pod with the command kubectl delete pod redis-0. In the previous tests, the pod comes up again automatically but the session and job data was gone.
This time, when the pod comes up again, refresh the app and the Sidekiq dashboard. I’m still logged in and the scheduled jobs are still there.
Success!!
Google Cloud Platform and Google Kubernetes Engine make it very easy to spin up a Kubernetes cluster. Once you get the hang of the various API objects available and how to use them, it is very satisfying to put together a resilient, self-healing system.
However, getting started can be a bit confusing. I hope this post has helped to clear things up a little.
ESTL is a leading full-stack engineering shop in…
589 
11
589 claps
589 
11
Written by

ESTL is a leading full-stack engineering shop in Singapore’s Ministry of Education. Our engineers build cutting-edge software to simplify the everyday duties of teachers and admin staff alike.
Written by

ESTL is a leading full-stack engineering shop in Singapore’s Ministry of Education. Our engineers build cutting-edge software to simplify the everyday duties of teachers and admin staff alike.
"
https://medium.com/google-cloud/5-principles-you-need-to-know-before-using-google-cloud-dataprep-for-data-preparation-8b639bd844b5?source=search_post---------257,"There are currently no responses for this story.
Be the first to respond.
Google Cloud Dataprep is an intelligent data service on Google Cloud Platform for exploring, cleaning, and preparing structured and unstructured data.
There are 5 principles important to know before your data preparation with Dataprep.
Before you get started cleaning your dataset, it is helpful to create a virtual profile of the source data…
"
https://towardsdatascience.com/how-to-enable-pandas-to-access-bigquery-from-a-service-account-205a216f0f68?source=search_post---------258,"Sign in
There are currently no responses for this story.
Be the first to respond.
Lak Lakshmanan
May 30, 2018·6 min read
Service accounts are a way to keep a tight leash on what your applications in Google Cloud Platform are doing. Instead of running your applications with all the permissions your user account has, you typically want the application to have extremely constrained access to your organization’s resources.
Let’s say that you’d like Pandas to run a query against BigQuery. You can use the the read_gbq of Pandas (available in the pandas-gbq package):
This will work if you run it locally (because it uses your identity, and presumably you have the ability to run queries in your project).
But if you try running the above code from a bare-bones account, it won’t work. You will get asked to go through an OAuth2 workflow to specifically authorize the application. Why? Because you don’t want an arbitrary application running up a BigQuery bill (or worse, accessing your corporate data), so you have to provide it the permissions it needs.
Let’s try it out.
To follow along with me, use the GCP web console and create a Google Compute Engine instance with the default access (this typically includes only the bare basics and doesn’t include access to BigQuery):
From the GCP web console, create a new service account. This is the account for which you will be generating a private key. For extra security and auditing, I recommend creating a brand new service account for each application and not reusing service accounts between applications.
Go to IAM & Admin, select “Service accounts” and click on +Create Service Account. Fill out the form as follows:
The roles above allow the service account to run queries and have those be billed to the project. However, the dataset owner still needs to allow the service account to view their datasets.
If you are using a non-public BigQuery dataset, give the service account the appropriate (typically just View) access to it by going to the BigQuery console and sharing the dataset with the service account’s email address. For the purposes of this tutorial, I will use a public BigQuery dataset, so we can skip this step.
SSH to the GCE instance and on the command-line, type in the following commands:
Change the project id in nokey_query.py and then run it:
It will ask you to go through an OAuth2 workflow. Hit Ctrl-C to exit. You don’t want this application running with *your* credentials.
A common suggestion you will hear when you run into such an error is to run:
and go through the interactive OAuth 2 workflow in the shell before launching the application. Be careful about doing this: (1) the above command is a bazooka and allow the application to do anything you can do. (2) It is not scriptable. You will have to do it every time you run the application.
The best approach is to create the Compute Engine VM, not with the bare-bones service account, but with the service account to which you have given BigQuery viewer permissions. In other words, swap steps 1 and 2, and when you create the GCE instance, use the newly created service account.
But what if you are not on GCP (so your machine isn’t created by the service account auth) or you are using a managed service such as Cloud Dataflow or Cloud ML Engine (and so the VMs are created by that service’s service account)? In that case, a better approach is to change the Pandas code as follows:
The application will now use the permissions that are associated with this private key.
Change the project id in query.py and then run it:
It will fail saying the private key wasn’t found
Remember that you created a JSON private key when you created the service account? You need to upload it to the GCE VM. (In case you didn’t create the key file, navigate to IAM > Service Accounts and create a private key for the pandas service account. This will create a JSON file and download it to your local computer. You can also revoke keys from here.)
In the top-right of the SSH window, there is a way to upload files. Upload the generated JSON file to the GCE instance and move it into place:
It will work now and you will get back the results of the query.
What if you are not using GCE, but are using a managed service (Cloud Dataflow, Cloud ML Engine, etc.) instead?
In that case, you will be submitting a Python package. Mark the private key as a resource in the setup.py using the package_data attribute:
Then, instead of hardcoding the path to the privatekey.json file, do this:
Here is a more complete example . Note that Python doesn’t have a way of marking packages as private, so you should be careful not to mistakenly publish this package in a public repository such as PyPI.
The private key essentially unlocks, for whoever presents it, the resources that have been made available. In this case, anyone who presents the private key will be able to make BigQuery queries. No second form of authentication (IP address, user login, hardware token, etc.) is required. So, be careful about how/where you store the private key. What I recommend:
(1) Make sure to have a .gitignore in the Python package that explicitly ignores the private key:
(2) Use git secrets to help safeguard against key leakage
(3) Rotate your keys. See this blog for details.
(4) Make sure to not publish the Python package to any repository of Python packages, as yours contains a private key.
Here is the example code of this article in GitHub. Happy coding!
Acknowledgment: Thanks to my colleagues Tim Swast and Grace Mollison for their help and suggestions. Any errors in the article are my own.
Data Analytics & AI @ Google Cloud
260 
260 
260 
Your home for data science. A Medium publication sharing concepts, ideas and codes.
"
https://medium.com/tensorflow/how-to-serve-an-embedding-trained-with-estimators-681d5fa2fbe2?source=search_post---------259,"There are currently no responses for this story.
Be the first to respond.
By Lak Lakshmanan, Technical Lead, Google Cloud Platform
When you have a sparse categorical variable (a variable that can take many possible values), it can be helpful to embed it into a lower dimension. The most well-known form of embedding is word-embedding (as in word2vec or Glove embeddings) where all the words in the language are represented by a vector of, say, 50 elements. The idea is that similar words are close-by in the 50-dimensional space. You can do the same thing with your categorical variables — train the embedding on one problem, and reuse that embedding instead of just one-hot encoding the categorical variable in related problems. The lower dimensional space of your embedding is continuous and so, it can also function as the input to a clustering algorithm — you can find natural groupings of the categorical variable.
In order to serve an embedding trained with an Estimator, you can send out the lower dimensional representation of your categorical variable along with your normal prediction outputs. Embedding weights are saved in the SavedModel, and one option is to share that file itself. Alternatively, you can serve the embedding on demand to clients of your machine learning team — which may be more maintainable,because those clients are now only loosely coupled to your choice of model architecture. They will get an updated embedding every time your model is replaced by a newer, better version.
In this article, I will show you how to:
The entire code of this article is on GitHub and it contains much more context. I’m only showing you key snippets here.
Let’s build a simple demand forecasting model to predict the number of bicycle rentals at a station, given that we know the day of the week and whether it is a rainy day. The data for this comes from a public dataset of New York City bicycle rentals and NOAA weather data:
The inputs to the model are:
The label we’ll want to predict is num_trips.
We can create the dataset by running this query in BigQuery to join the bicycle and weather datasets and do the necessary aggregations:
To write the model, we will use a custom estimator in TensorFlow. Although this is just a linear model, we can not use the LinearRegressor because the LinearRegressor hides all the underlying feature column arithmetic. We need access to the intermediate output (the output of the embedding feature column), and so we will write the linear model explicitly.
To implement a custom estimator, you have to write a model function and pass it into the Estimator constructor:
The model function in a custom estimator has 5 parts:
We are taking the station column, and putting it into a bucket based on its hashcode. This is a trick to avoid having to build a full vocabulary. There are only about 650 bicycle rental stations in New York, so by having 5000 hashbuckets, we greatly reduce the chance of collisions. By then embedding the station id into a smaller number of dimensions, we will also get to learn which stations are like each other, at least in the context of rainy-day-rentals. Ultimately, every station-id is represented by just a 2-dimensional vector. The number 2 controls how accurately the lower-dimensional space represents the information in the categorical variable. My choice of 2 here was arbitrary — realistically, we will need to tune this hyperparameter for best performance.
The other two categorical columns are created using their actual vocabulary and then one-hot encoded (the indicator column one-hot encodes the data).
The two sets of inputs are concatenated to create one wide input layer and then passed into a dense layer with one output node. This is how you program a linear model at a relatively low-level. This is equivalent to writing a LinearRegressor as:
Note that input_layer, indicator_column, etc. are all hidden away by the LinearRegressor. I am, however, exposing it because I want access to the station’s embeddings.
For a regression problem, we can minimize the mean squared error using the Ftrl optimizer (this is the default one used by the LinearRegressor, so I’m also using it):
Normally, we will send out only the predictions, but in our case, we want to send back both the predictions and the embeddings:
The ability to change the export_outputs is the other reason that we need to use a custom estimator here.
Now, we train the model as normal.
The exported model can then be served using TensorFlow Serving, or optionally deployed to Cloud ML Engine (which is essentially hosted TF Serving), and then invoked for predictions. You can also invoke the local model using gcloud (which provides a more convenient interface for this purpose than saved_model_cli):
What’s in test.json?
{“day_of_week”: 4, “start_station_id”: 435, “rainy”: “true”}
{“day_of_week”: 4, “start_station_id”: 521, “rainy”: “true”}
{“day_of_week”: 4, “start_station_id”: 3221, “rainy”: “true”}
{“day_of_week”: 4, “start_station_id”: 3237, “rainy”: “true”}
As you can see, I am sending 4 instances, corresponding to stations 435, 521, 3221 and 3237.
The first two stations are in Manhattan, in an area where rentals are quite frequent (and serve both commuters and tourists). The last two stations are in Long Island, in an area where rentals are somewhat less common (and perhaps only on weekends). The resulting output contains both the predicted number of trips (our labels) and the embedding for the stations:
In this case, the first dimension of the embedding is almost zero in all cases. So, we only need a one dimensional embedding. Looking at the second dimension, it is quite clear that the Manhattan stations have positive values (0.0081, 0.0011) whereas the Long Island stations have negative values (-0.0025, -0.0031).
This was learned purely by the machine learning model looking at bicycle rentals on different days at the two locations! If you have categorical variables in your TensorFlow models, try serving out the embeddings from them. Perhaps they will lead to new insights!
TensorFlow is an end-to-end open source platform for…
169 
3
169 claps
169 
3
TensorFlow is an end-to-end open source platform for machine learning.
Written by
TensorFlow is a fast, flexible, and scalable open-source machine learning library for research and production.
TensorFlow is an end-to-end open source platform for machine learning.
"
https://itnext.io/where-google-cloud-is-going-wrong-729e78f6c160?source=search_post---------260,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Despite a solid product offering, Google Cloud Platform is struggling to find its audience.
Comparing the big three cloud providers is like measuring high performance sports cars— you can argue over the subtleties of various models but ultimately they are in a completely different class to a Toyota.
Among these cloud titans, while AWS has sped off into the distant future, there’s fierce fighting for second and third place — and it’s a race Google is currently losing.
On the surface, this table makes my argument look baseless— GCP grew by 100% in 2016. But percentage comparisons only work when sizes are roughly similar. It’s a $22 billion marketplace that’s exploding and Google only has $500 million of this pie. Microsoft is 3 times the size and Amazon is 20 times larger. Even Alibaba (they have cloud!) has displaced Google in terms of revenue share.
A peek at the 2017 numbers should also make Googlers worry:
The smaller players are shrinking. Amazon’s percentages are hard to compare when their presence is so massive but among the rest, Microsoft is clearly running away for the number two prize in cloud.
In this massive market the runner-up prize in cloud is a big deal. There are some companies that simply won’t use Amazon, mainly because it’s seen as a competitor or other clouds might be a better fit for the existing tool-set. Microsoft has launched a formidable full scale assault in the “anyone but Amazon” space which has been staggeringly successful, and Google is frankly stalling.
Google was the original global scale, high-availability platform — it should easily take the crown in this business. Graphs aside, I also hear from clients and other cloud practitioners the growing pessimism about whether they will continue to use GCP. So what is going wrong?
Back in 2010, I was a huge Google Apps fan and spent much of my time convincing companies to drop Office and head to the browser. I even wrote a freaking book about Google Apps, in between foaming at the mouth at how Google Apps would soon be eclipsing Microsoft for all email and productivity.
In my blind optimism, the reasons for Google’s glorious victory in Apps were obvious compared to Office:
And, boy, how wrong was I! People looked at me like I was insane. Google Apps just never gained a serious foothold. Years later, Microsoft got mildly aggravated that Apps was stealing single-digit market share and crushed it with Office 365, which is now the lion share of SaaS office usage.
How did that happen? Office 365, if you’ve been lucky enough to use it, combines the worst of Outlook, a dramatically reduced feature-set of Office and none of the cool things about Google Apps. And yet corporate users have flocked to it in droves.
When I ask corporate execs why they went with Microsoft, the answers are always the same — 365 offered a natural upgrade path, an evolution for users, didn’t cost anything for existing licenses and it was blessed by their IT. In short, it was an enterprise upgrade, and the underlying tone was Google wasn’t really enterprise enough.
And in a nutshell this is the same problem with Google Cloud. If you go to the conferences, look at the marketing materials and check out the partner eco-system, they are simply not effectively targeting an enterprise audience. It’s techie, it’s nerdy, it’s academic, it’s esoteric, and whatever technical advantages they have over the competition are being completely ignored by the large adopters and eclipsed by competitors.
Does it matter? Emphatically, yes. You need the large corporate customers for volume, revenue, scale and ultimately the credibility that gets you more large customers.
I am 100% sure that DynamoDB is not going away in the next 10 years. But honestly I am not at all sure that FireBase will be around by next quarter. And this is a really common concern I hear from companies: “Yeah, Google <insert product name> is great but are they really committed?”
It’s a direct side-effect of overly-aggressive retirements in recent years, from great ideas nobody used (Google Wave) to okay-ish software everybody used (Google Reader) and critical software that people built businesses around (Insights for Search). Even fantastic products have no development for a decade (Google Voice). iGoogle, Google Talk, Google Health, Picnik, Google Buzz —all gone, gone, gone. It’s a virtual graveyard.
The acceleration between “This is the future” and “Nah, we’ll killing it” has reached the point where soon they’ll be launching and retiring in the same sentence. “We pleased to announce our new beta and you have until December to download your data before shutdown.”
And then there’s the zombie product problem, as shown by Google+. Is it dead, alive or what? There has been conflicting official quotes from Google executives saying they are shutting down… nope, now it will be “more focused” —wait, we haven’t given up yet, it’s still around. Is this any way to treat a platform that was officially launched to compete with Facebook? There are even articles on how to delete your Google+ account without affecting Gmail, that’s how screwed up this is.
Google at its core has a major Product Management problem that directly impacts its cloud adoption. Companies will not commit to your platform when you shut down services unpredictably. Both Amazon and Microsoft understand this and have a solid track record of supporting their cloud products.
Google’s infamous retirement ‘strategy’ makes every product look like a hobby. (If you think this is overstated, just ask anyone who built software around Google Realtime API.)
Google has the most successful advertising platform in the world but if you’ve ever used AdWords you’ll see it was designed by developers not marketers. The average company struggles to launch AdWords campaigns and use the core Google product that literally everybody wants, so much so that Google once launched AdWords Express to solve the problem (it didn’t).
Facebook, on the other hand, has shown how to sell pretty much the same product to non-techies and has built a wildly successful platform that average companies can use. Google missed this huge opportunity because it’s not a sales company and I’ve seen this over and over in various launches.
Case in point: the Google Next conference last year was the only tech conference I’ve ever seen that became cheaper as the date approached. You could buy a $1500 ticket six months ahead or wait until 2 weeks prior for an identical $500 ticket (admittedly with a widely distributed vendor discount). From airlines to music concerts, nobody operates this way. It created zero buzz for the event and flies in the face of the scarcity model of marketing. (For comparison, try finding a discount coupon for re:Invent.)
The same happened with Premier and Standard network tier pricing confusion. Instead of launching the Google-network backbone as a premium upgrade, it was presented (bizarrely) as a standard network option downgrade. You wouldn’t believe the number of people I spoke to who thought Google had increased the price of its network service and didn’t understand what it was. It would be like if AWS had announced Glacier but left customers with the impression that now you pay for data you need immediately.
In cloud, AWS and Azure have enterprise-grade sales efforts with entire vendor ecosystems ready to sell for them. Dealing with these companies, from a sales perspective at least, looks and smells just calling the big tech companies of yesteryear and, at a certain size, target customers like the familiarity. It’s all new but it feels just the same. By comparison Google’s sales effort is amateurish at best.
Google practically created the idea that software doesn’t need support — Gmail has no 1800 number and it doesn’t need one because it’s simple to use and always works. It’s easy to forget before Google, software pretty much always came with support. Unfortunately, this same approach doesn’t work with cloud.
As with many Google products, their free (read: developer) cloud support tends to focus on thinly-contributed user forums rarely visited by Google engineers. The couple of times I ventured into GCP for work projects, when I hit arcane snags (like the phantom database disconnection problem), I quickly realized I would never get an answer and went running back to AWS.
I know, I know, all cloud vendors have terrible documentation and spotty support. But when I turn to the developer’s official fire extinguisher, Stack Overflow, you can see there isn’t much support for GCP there either, which only amplifies the need for Google to provide more support.
Google Cloud does many things better than either Microsoft or Amazon, but don’t expect Google to tell you what they are.
Again, Google Next (their cloud conference) is firmly stuck in the “isn’t cloud great?” conversation of five years ago, whereas attendees should have had their eyelids taped open to be brainwashed into the awesomeness of their AI powerhouse. Instead the audience was given once of the worst tech keynotes in Silicon Valley history and I remember exactly none of the announcements.
Seriously, Google’s AI and deep learning tools absolutely demolish the competition so why isn’t this the primary message we’re hearing? “We’ve got per-second billing now,” they meekly announced — days after Amazon beat them to the punch (Google actually implemented sooner but lost the PR battle). Seriously, where is the value in the race to the bottom for cheaper virtual servers? Dammit, show us your cool toys!
Even simpler tools like FireBase aren’t promoted as differentiators. Building the equivalent of FireBase in AWS is not trivial (and why would you?). A Cloud Guru’s tutorial on building a video sharing website shows how mindlessly easy it is to use Firebase. It’s a great, great product with broad application. But frequently all I hear is that it’s too expensive (really?) and massive unexpected price changes last year screwed some devs.
I once worked with a former Google exec who said the company struggled to focus on anything not on the first page of its revenue charts, which I thought explained its casual attitude towards cloud. Seeing as advertising is upwards of 90% of the entire company’s revenue, that might make sense when their cloud sales are so small.
But as an enormous Google Fanboy, it pains me to see this. Amazon doesn’t ignore AWS because its real focus is on retail, and Microsoft isn’t making Azure a side project because of Windows and Office. It should be possible to overcome these hurdles since the technology is good and the infrastructure is there.
Unfortunately, I’m not hopeful. People are excited about AWS and there’s significant, growing chatter about Azure. Where are the people boasting about Google Cloud Certifications? Who can’t wait for the Google Next 2018 conference? GCP has the feeling of a niche provider that is going backwards at a time when the major clouds are iterating features at lightning speed.
Reluctantly I find myself taking pause with Google and more routinely using AWS for everything. With limited time to learn new tools and clients who want dependable enterprise-grade solutions, it’s not a platform I talk about much anymore. But I’d love — just love — for Google to prove me wrong because I’m still their number one fan.
ITNEXT is a platform for IT developers & software engineers…
435 
6
435 claps
435 
6
Written by
Developer advocate. Opinions here are my own. See my other blogs at http://bit.ly/jbeswick.
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
Written by
Developer advocate. Opinions here are my own. See my other blogs at http://bit.ly/jbeswick.
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@yufengg/how-to-upgrade-colab-with-more-compute-64d53a9b05dc?source=search_post---------261,"Sign in
There are currently no responses for this story.
Be the first to respond.
Yufeng G
Aug 14, 2019·4 min read
On a previous episode of AI Adventures, we looked at Colab as a great way to get started in the data science and machine learning world. But there are some models out there that need to run for a long time (Colab instances will reset after several hours), you want to get more memory or GPUs than is provided for free.
The question then becomes: How can we hook up Colab’s frontend with some more compute power? We’re going to use Google Cloud Platform’s Deep Learning VMs to power up your Colab environment. A while back, we looked at how to create Deep Learning VMs with your choice of machine learning frameworks. We’ll use that example today and connect Colab to it so we can utilize the resources on that machine.
If you want to catch this episode in video form, I’ve linked it here:
To start out, we’ll of course need to make our VM, and let’s make it a big one! Head over to the Cloud Marketplace and find the Deep Learning VM and select it. I’m going to call minecolab-vm. And for good measure let’s include 16 CPUs, with 60 GB of memory! And for the GPUs, I’ll choose the V100s, and put in two of them! Now we’re really cooking!
While we’re waiting for that to spin up and launch JupyterLab, let’s talk a bit about how we’re going to get Colab to talk to your VM. There are 2 tricks that we are going to use, which together make this connection possible. The first is the fact that Colab can connect to a local runtime. So if you have a local Jupyter notebook server running, you can use Colab as the frontend to that server, rather than the usual Jupyter Notebook frontend. You can access this option by clicking the drop-down menu in the upper right-hand corner of Colab and selecting “Connect to local runtime”.
The second trick we’ll take advantage of is that we can do port forwarding on the Deep Learning VM we just created, so that it can be accessed from our local machine. You can do it with this command (just swap out colab-vm for your VM’s name):
gcloud compute ssh colab-vm -- -L 8080:localhost:8080
Be sure to run it from your local terminal, not Cloud Shell. In this particular situation, since we wish to forward the port to our local machine, running the command in Cloud shell does not achieve our goals. Additionally, notice that the command features a double-dash: --. This command causes everything that comes after it to be sent to the “next layer” of the commands given, so to speak. In this case, it means that the -L 8080:localhost:8080 is given to the ssh command, rather than to gcloud compute. It configures what the mapping of the ports should be from the virtual machine to your local machine.
Just having the port-forwarding on its own would just mean we can access the Jupyter server from a local frontend, which you can do by going to http://localhost:8080. But if we open up Colab and tell it to connect to our “local” runtime, the local runtime it will see is actually the port-forwarded one from the Deep Learning VM! And thus the two become connected.
On my V100-powered Colab instance, I can run nvidia-smi to confirm that yes indeed, we have our two V100 GPUs powering this instance:
So now you understand how Colab and Deep Learning VMs can work together, and how to get your setup just how you want it as your needs change, both in terms of what the frontend looks like, and what backend system is driving it. For example, you develop against a local backend initially, and when you want to get more power, you can connect it to your Deep Learning VM and run a big training job.
How do you set up your data science environment, and how do you allow it to change, flex, and grow as needed? Share your approach in the comments below!
For now, head over to the Deep Learning VMs and try connecting it with your Colab notebook!
Thanks for reading this episode of Cloud AI Adventures. If you’re enjoying the series, please let me know by clapping for the article. If you want more machine learning action, be sure to follow me on Medium or subscribe to the YouTube channel to catch future episodes as they come out. More episodes coming at you soon!
Applying machine learning to the world. Developer and Advocate for @googlecloud. Runner, chef, musician. Opinions are solely my own.
See all (58)
202 
3
202 claps
202 
3
Applying machine learning to the world. Developer and Advocate for @googlecloud. Runner, chef, musician. Opinions are solely my own.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/ontologynetwork/you-can-now-develop-ontology-smart-contracts-on-google-cloud-aws-and-azure-4c7425e0cfb7?source=search_post---------262,"There are currently no responses for this story.
Be the first to respond.
Today the Ontology Development Platform (ont_dev_platform) was released on Google Cloud Platform Marketplace, making Ontology one of the first public blockchains to have a development platform on the leading cloud provider marketplaces: Google Cloud, Amazon Web Services, and Microsoft Azure. Using the Ontology Development Platform on one of these cloud providers allows you to play around with and develop smart contracts without having to go through the fuss of configuring and setting up an environment locally.
Ontology has also joined the Google Cloud Technology Partner program, which gives Ontology the opportunity to collaborate with Google in marketing activities. With this new relationship and the development platform releases, Ontology hopes to grow the tech community and make developing dApps more accessible for all.
Are you a developer or want to start out? Please check out the:
Are you a developer? Make sure you have joined our tech community on Discord. Also, take a look at the Developer Center on our website, there you can find developer tools, documentation, and more.
Ontology website / Ontology GitHub / ONTO website / OWallet (GitHub)
Telegram (English) / Discord
Twitter / Reddit / Facebook / LinkedIn
A high performance, open-source blockchain specializing in digital identity and data.
973 
973 claps
973 
Ontology is a high performance, open source blockchain specializing in digital identity and data. ONTO: http://onto.app/downloadpage/TW  Telegram: http://t.me/OntologyNetwork
Written by
Active project domain: https://ont.io/
Ontology is a high performance, open source blockchain specializing in digital identity and data. ONTO: http://onto.app/downloadpage/TW  Telegram: http://t.me/OntologyNetwork
"
https://towardsdatascience.com/step-by-step-tutorial-pyspark-sentiment-analysis-on-google-dataproc-fef9bef46468?source=search_post---------263,"Sign in
There are currently no responses for this story.
Be the first to respond.
Ricky Kim
Dec 24, 2018·11 min read
I recently had a chance to play around with Google Cloud Platform through a specialization course on Coursera; Data Engineering on Google Cloud Platform Specialization. Overall I learned a lot through the courses, and it was such a good opportunity to try various services of Google Cloud Platform(GCP) for free while going through the assignments. Even though I’m not using any of GCP’s services at work at the moment, if I have a chance I’d be happy to migrate some parts of my data works to GCP.
However, one thing that the course lacks is room for your own creativity. The assignments of the course were more like tutorials than assignments. You basically follow along already written codes. Of course, you can still learn a lot by trying to read every single line of codes and understand what each line does in detail. Still, without applying what you have learned in your own problem-solving, it is difficult to make this knowledge completely yours. That’s also what the instructor Lak Lakshmanan advised at the end of the course. (Shout out to Lak Lakshmanan, thank you for the great courses!)
*In addition to short code blocks I will attach, you can find the link for the whole Git Repository at the end of this post.
So I have decided to do some personal mini projects making use of various GCP services. Luckily, if you haven’t tried GCP yet, Google generously offers a free trial which gives you $300 credit you can use over 12 months.
The first project I tried is Spark sentiment analysis model training on Google Dataproc. There are a couple of reasons why I chose it as my first project on GCP. I already wrote about PySpark sentiment analysis in one of my previous posts, which means I can use it as a starting point and easily make this a standalone Python program. The other reason is I just wanted to try Google Dataproc! I was fascinated by how easy and fast it is to spin up a cluster on GCP and couldn’t help myself from trying it outside the Coursera course.
If you have clicked “TRY GCP FREE”, and fill in information such as your billing account (Even though you set up a billing account, you won’t be charged unless you upgrade to a paid account), you will be directed to a page looks like below.
On the top menu bar, you can see “My First Project” next to Google Cloud Platform. In GCP, “project” is the base-level entity to use GCP services, enable billing, etc. On the first login, you can see that Google automatically created a “project” called “My First Project” for you. Click on it to see ID of the current project, copy it or write it down, this will be used later. By clicking into “Billing” on the left-side menu from the web console home screen, “My First Project” is automatically linked to the free credit you received.
In GCP, there are many different services; Compute Engine, Cloud Storage, BigQuery, Cloud SQL, Cloud Dataproc to name a few. In order to use any of these services in your project, you first have to enable them.
Put your mouse over “APIs & Services” on the left-side menu, then click into “Library”. For this project, we will enable three APIs: Cloud Dataproc, Compute Engine, and Cloud Storage.
In the API Library page, search the above mentioned three APIs one by one by typing the name in the search box. Clicking into the search result, and enable the API by clicking “ENABLE” button on the screen.
When I tried it myself, I only had to enable Cloud Dataproc API, since the other two (Compute Engine, Cloud Storage) were already enabled when I clicked into them. But if that’s not the case for you, please enable Compute Engine API, Cloud Storage API.
If this is your very first time to try GCP, you first might want to install the Google Cloud SDK so that you can interact with many services of GCP from the command-line. You can find more information on how to install from here.
By following instructions from the link, you will be prompted to log in (use the Google account you used to start the free trial), then to select a project and compute zone (project: choose the project you enable the APIs from the above steps if there are more than one, compute zone: To decrease network latency, you might want to choose a zone that is close to you. You can check the physical locations of each zone from here.).
Since you have installed Google Cloud SDK, you can either create a bucket from the command-line or from the web console.
Click into “Storage” from left-side menu, then you’ll see a page like the above. Click “Create bucket”
For convenience, enter project ID you checked at the end of “Creating a Free Trial Account on GCP” stage. You can just click “create” without changing any other details, or choose the same location as your project.
Replace your_project_id with the project ID that you copied and run the below line on your terminal to set BUCKET_NAME variable to your project ID and make it available to sub-processes. (A Bash script you need to run later will make use of this)
Then create a bucket by running gsutil mb command as below.
The above command will create a bucket with the default settings. If you want to create a bucket in a specific region or multi-region, you can give it -l option to specify the region. You can see available bucket locations from here.
Now clone the git repository I uploaded by running below command in terminal.
Once you clone the repository, it will create a folder named pyspark_sa_gcp. Go into the folder and check what files are there.
You will see three files in the directory: data_prep.sh, pyspark_sa.py, train_test_split.py. In order to download the training data and prepare for training let’s run the Bash script data_prep.sh. Below is the content of the script and I have added comments to explain what each line does.
The original dataset for training is “Sentiment140”, which originated from Stanford University. The Dataset has 1.6million labelled tweets.50% of the data is with negative labels and the other 50% with positive labels. More info on the dataset can be found from the link. http://help.sentiment140.com/for-students/
In the above Bash script, you can see it’s calling a Python script train_test_split.py. Let’s also take a look at what it does.
Now we can run the Bash script to prepare the data. Once it’s finished, it will have uploaded prepared data to the cloud storage bucket you created earlier. It will take 5~6 mins to upload the data.
Go to the Storage from the left side menu and click into your bucket -> pyspark_nlp -> data. You will see two files are uploaded.
Or you can also check the content of your bucket from your terminal by running below command.
Cloud Dataproc is a Google cloud service for running Apache Spark and Apache Hadoop clusters. I have to say it is ridiculously simple and easy-to-use and it only takes a couple of minutes to spin up a cluster with Google Dataproc. Also, Google Dataproc offers autoscaling if you need, and you can adjust the cluster at any time, even when jobs are running on the cluster.
Go to Dataproc from the left side menu (you have to scroll down a bit. It’s under Big Data section) and click on “Clusters”. Click “Create clusters”, then you’ll see a page like below.
Give it a name (for convenience, I gave the project ID as its name), choose Region and Zone. To decrease the latency, it is a good idea to set the region to be the same as your bucket region. Here you need to change the default settings for worker nodes a little, as the free trial only gives you permission to run up to 8 cores. The default setting for a cluster is one master and two workers all with 4 CPUs each, which will exceed the 8 cores quota. So change the setting for your worker nodes to 2 CPUs, then click create at the bottom. After a couple of minutes of provisioning, you will see the cluster created with one master node (4 CPUs, 15GB memory, 500GB standard persistent disk) and two worker nodes (2 CPUs, 15GB memory, 500GB standard persistent disk each).
Since we need to change the default setting a little bit, we need to add one more argument to the command, but it’s simple enough. Let’s create a cluster and give it the same name as the project ID, and set worker nodes to have 2 CPUs each.
You can change the zone to be close to your bucket region.
Finally, we are ready to run the training on Google Dataproc. The Python script (pyspark_sa.py) for the training is included in the Git repository you cloned earlier. Since I commented on the script to explain what each line does, I will not go through the code. The code is a slightly refactored version of what I have done in Jupyter Notebook for my previous post. Below are a few of my previous posts, in case you want to know more in detail about PySpark or NLP feature extraction.
And let’s take a look at what the Python script looks like.
Since I commented inside the script to explain what each line does, I will not go through the code extensively. But in a nutshell, the above script will take three command line arguments: Cloud Storage location where the training and test data are stored, a Cloud storage directory to store prediction result of the test data, and finally a Cloud storage directory to store the trained model. When called, it will first do the preprocessing of the training data -> build a pipeline -> fit the pipeline -> and make predictions on the test data -> print the accuracy of the predictions -> save prediction result as CSV -> save fitted pipeline model -> load the saved model -> print the accuracy again on the test data (to see if the model is properly saved).
In order to run this job through the web console, we need to first upload the Python script to our cloud storage so that we can point the job to read the script. Let’s upload the script by running below command. (I’m assuming that you are still on pyspark_sa_gcp directory on your terminal)
Now click into Dataproc on the web console, and click “Jobs” then click “SUBMIT JOB”.
From the above screenshot replace the blurred parts of the texts to your project ID, then click “submit” at the bottom. You can inspect the output of the machine by clicking into the job.
The job is finished after 15 minutes, and by looking at the output, it seems like the cluster struggled a bit, but nonetheless, the prediction looks fine and the model seems to be saved properly.
If you submit a job from the command-line, you don’t even need to upload your script to Cloud Storage. It will be able to grab a local file and move to the Dataproc cluster to execute. (Again I’m assuming that you are still on pyspark_sa_gcp directory on your terminal)
Again the cluster seemed to struggle a bit, but still got the result and model saved properly. (I have tried to submit the same job on my paid account with 4 CPUs worker nodes, then it didn’t throw any warnings)
Go to your bucket, then go into pyspark_nlp folder. You will see that the results of the above Spark job have been saved into “result” directory (for the prediction data frame), and “model” directory (fitted pipeline model).
Finally, don’t forget to delete the Dataproc cluster you have created to ensure it will not use up any more of your credit.
Through this post, I went through how to train Spark ML model on Google Dataproc and save the trained model for later use. What I showed here is only a small part of what GCP is capable of and I encourage you to explore other services on GCP and play around with it.
Thank you for reading. You can find the Git Repository of the scripts from the below link.
https://github.com/tthustla/pyspark_sa_gcp
The Rickest Ricky. Love data, beer, coffee, and good memes in no particular order.
206 
7
Every Thursday, the Variable delivers the very best of Towards Data Science: from hands-on tutorials and cutting-edge research to original features you don't want to miss. Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
206 
206 
7
Your home for data science. A Medium publication sharing concepts, ideas and codes.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/cloud-and-servers/bulut-hizmetlerinde-iaas-paas-saas-nedir-3bb5620fd357?source=search_post---------264,"There are currently no responses for this story.
Be the first to respond.
Amazon(AWS), Google(Cloud Platform), Microsoft(Azure), IBM(SoftLayer, Bluemix) gibi büyük firmaların bulut hizmetlerindeki servis katmanları birbirleriyle aynı mantıktadır.
Amaç katman katman olan bu yapıların geliştiriciden soyutlanarak herkesin rahatça kullanabileceği ortamlar oluşturmaktır. Aşağıda bu sorumlulukların kimin sorumluluğunda olduğunu anlatan bir resim görmektesiniz.
On-Premises: Yazılımlarının sizin veya firmanın bilgisayarlarına yüklendiği kısımda tüm katmanların sorumluluğu sizin ekibinizin üzerindedir. Bir sistem ekibiniz olması gerekir. Bu sistem ekibi sunucuları, veritabanlarını, güvenliği, network’ü bilmesi ve kurması gerekmektedir. Veritabanının yedeklerinin alınmasını sağlaması, işletim sisteminin güncel sürümlerinin yüklenmesini sağlaması gerekmektedir. Ayrıca sistem ekibinin JVM, dll, plugin gibi yazılımın ihtiyacı olan Run-Time sisteme kurmaları gerekmektedir.
Infrastructure As A Service: Size bulut üzerinden sanal Compute, Storage, Networking satıldığı, kiralandığı bulut hizmeti olarak düşünebilirsiniz. Bilgisayar, Disk ve Network kartları almak yerine bunları Sanal olarak bulut’tan kiralayıp üzerine istediğiz işletim sistemini kurup yolunuza devam edebilirsiniz.
Platform As A Service: Bulut üzerinde direk bir java, ruby, node uygulaması geliştirmek istiyorsunuz ve işletim sistemi, network, sunucu gibi sistemler ile uğraşmak istemiyorsunuz, Sadece uygulamanızı geliştirmek ile uğraşıyorsunuz sonrada uygulamanızın run-time dosyalarını ilgili platforma atıp çalışmasını sağlıyorsunuz.
Software As A Service: Uygulamaların bulut’tan hizmet vermesine SaaS denir. Kullanıcılar sadece uygulama arayüzlerine erişebilir. Kendilerine ait bilgileri bu yazılımlara girerek, bilgilerini bulut üzerinde saklar ve buradan kullanırlar.
Örneğin: GoogleDocs, Evernote uygulamalar SaaS olarak düşünebiliriz.
Uzun süredir farklı sektörlerde (Askeri, Telekomünikasyon, Devlet, Bankacılık, Sigortacılık, Tübitak, SaaS) yazılımlar geliştiriyorum. Bu süreçte Havelsan, Milsoft, T2, Cybersoft ve Thundra firmalarında yönetici ve yazılım mühendisi olarak çalıştım. Deneyimlerimi ve teknolojik bilgi birikimi mi olabildiğince OnurDayibasi.com adresinde toplamaya çalışıyorum. Tüm yazılarıma ve daha fazlasını bu site üzerinden erişebilirsiniz.
AWS, Azure, OpenStack
104 
1
104 claps
104 
1
AWS, Azure, OpenStack
Written by
Senior Frontend Developer at Thundra
AWS, Azure, OpenStack
"
https://medium.com/net-core/deploy-an-asp-net-core-app-to-google-cloud-d5ff3ff99b2d?source=search_post---------265,"There are currently no responses for this story.
Be the first to respond.
In this post, I will show how to deploy an ASP.NET Core web application to Google Cloud Platform (GCP).
I will use the following tools:
The sections of this post will be as follows:
If you are ready, let’s get started.
First, go to Google Cloud web site and click Get started for free button. Sign up with your Gmail account and get $300 (for free) to spend on Google Cloud Platform over the next 12 months.
Now, download the Google Cloud SDK.
After installation has completed, accept the following options:
Next, a web browser will be opened and you will be requested to login to your Google account (select the one you used to register to GCP).
For future reference, the following command accomplishes the same mission:
Then, you will be prompted to use an existing project or create a new one, select the latter one and create a new project.
The project ID must be a unique name and keep in mind that at the end your application’s URL will be like:
https://[Your-project_id].appspot.com
You can also use the following command to create a project:
I created the following project:
We can verify the project was created with the following command:
Now, we will initialize our App Engine app with our project and choose its region:
Run the following command and select a region when prompted:
Now, we will enable billing for our project.
A billing account needs to be linked to your project in order for the application to be deployed to App Engine.
Go to Google Cloud Console and select the project you created from the top menu and click Open:
Next, in the search box type Billing and choose the Billing menu:
In the next dialog, select Link a billing account and then set your account.
Your flexible environment deployment will incur costs while it is active. We will clean up our project when we are finished to avoid ongoing costs at the end of this tutorial.
Open Visual Studio and go to Tools -> Extensions and Updates
Search for Google Cloud Tools for Visual Studio and Install (It’s already installed on my computer):
You should restart Visual Studio after this operation.
As a side note, Cloud Tools for Visual Studio is not supported in VS 2019. Cloud Tools for Visual Studio is an open source project and there is an issue about this subject. The last message in this issue is as follows:
So, I am using VS 2017 for this tutorial.
Now, we will create our web application. Open File -> New -> Project and select ASP.NET Core Web Application and then name the project and click OK.
In the next dialog, select Web Application:
Next, we will link our application to our GCP project.
Open Tools -> Google Cloud Tools -> Manage Accounts:
and select Add account. Login to the account that you used to register to the GCP.
You will see your account logged on the top right corner of Visual Studio. There is a No Project label near this account and click that and click Select Project:
Select the project that you created above.
app.yaml
Next, add a new file called app.yaml to the project and add the following code to this file:
The app.yaml file describes an app's deployment configuration. Here, app.yaml specifies the runtime used by the app, and sets env: flex, specifying that the app uses the flexible environment.
At this point, we should build the project.
Now, right-click on the project and select Publish To Google Cloud…
In the next dialog, select App Engine Flex:
Next, you will get the following dialog:
Enable the services link did not work for me. If you experience the same issue, then go to the Google Cloud Console and write APIs & Services in the search box and then search for app engine admin api in the library:
and Enable this API.
Do the same operation for the following API too:
Next, select Publish to Google Cloud again and click Publish in the next dialog:
I got the following error in this step:
I followed the solution in this StackOverflow post and changed InProcess in csproj file with lowercase as follows:
I published again and deployment succeeded, finally :) It took 9–10 minutes for the deployment operation to complete. After it is completed, a web browser is launched automatically :
As I mentioned above, your application’s URL is
The web page is delivered by a web server running on an App Engine instance on GCP.
To avoid incurring charges, we can delete our GCP project to stop billing for all the resources used within the project.
In the Google Cloud Console, search for Manage Resources. In this menu, select your project and click Delete:
I hope you found this post helpful and easy to follow. Please let me know if you have any questions and/or corrections.
And if you liked this post, please clap your hands 👏👏👏
If you want to find out more about Google Cloud Platform, you can have a look at this GCP Essentials quest. After completing this quest, you will have a monthly subscription for free and you can take other quests in Qwiklabs.
Bye!
UPDATE: Check the following post if you are interested in learning how to deploy an ASP.NET Core application with database access to Google Cloud:
medium.com
References
https://cloud.google.com/appengine/docs/flexible/dotnet/quickstart
Posts related to .NET Core (ASP.NET Core, MVC, Web API…)
256 
2
256 claps
256 
2
Posts related to .NET Core (ASP.NET Core, MVC, Web API…)
Written by
A software developer who loves learning new things and sharing these..
Posts related to .NET Core (ASP.NET Core, MVC, Web API…)
"
https://medium.com/google-cloud/introduction-to-google-container-builder-dbe6645f9421?source=search_post---------266,"There are currently no responses for this story.
Be the first to respond.
Google Container Builder is an interesting tool that allows you to use the power of the Google Cloud Platform to enable a fast and reliable build for your container images.
The name is a bit confusing to understand in terms of the things that it can do but I am going to talk about a process scenario that I have been using and which I believe applies to most of us and see how Google Container Builder makes it more efficient.
If you have been using Docker, one of the key things is to build a Docker image and make it available for others to pull the image to use in their local environment. The process goes something like this:
The process is no doubt simple but there are a few things that you would have observed if you have been using it for a while. These points are based on my experience, doing both local development and while teaching Docker to a wide range to developers:
What we are going to see next is how Google Container Builder addresses the above by giving you an environment in the Cloud that will speed up your Container builds, automatically push the images to Google Container Registry. We are only scratching the surface of Google Container Builder that has the capabilities for you to integrate with your build process, trigger events to notify external users, custom build scripts and more.
We are going to assume here that you already have your application and the associated Dockerfile to help build your Docker image. If not, you can pull a simple project from here (instructions on doing that are given later in this post):
We are going to be using the Google Cloud SDK that provides us command line utilities (commands) to work with Container Images. It contains commands to build, list and perform other operations with Container images.
I shall assume that you are familiar with the Google Cloud Platform (GCP) to the extent that you have a billing account, can navigate the GCP Console, have a Google Cloud Platform project and most importantly have downloaded and initialized the Google Cloud SDK.
If the above is true, we need to ensure a few things as given below:
Type in Container builder and it should bring up the Google Container Builder API. Click that and make sure that you enable it.
2. From the console or terminal, we will assume that you have authenticated your Google Cloud SDK and set the default project with a set of commands as shown below: $ gcloud auth login and $ gcloud config set project [YOUR_PROJECT_ID]
The first thing that we are going to do is have a simple application with its associated Dockerfile. I have provided a project that is just about enough to see this process in action.
Simple pull in the project from here:
github.com
Assuming that you have git installed on your local machine, do the following in a specific directory of your choice:
Great! All we need to do next is to submit our image build to Google Container Builder. That’s it.
The gcloudtool command to submit a build to the Container Builder environment in the Google Cloud is as follows:
The command is easy to understand:
In my case, my GCP project is named mindstormclouddemo and I wish to name my Docker image as myapp and the version is 1.0. The command for that is as follows:
This will trigger the process as shown below and it makes for an interesting series of steps to see what is going on (I have annotated and formatted some parts of it with my comments):
As you can see from the build process above, every step of it is provided to you from the process that Google Container Builder has launched in the cloud. I have annotated the output above with some comments to help you understand what is going on and if you have dealt with a Docker build before, this process should be familiar.
Do note that each build process thus fired from the gloud container buildcommand is unique. You can get a status/logs on the build too via the gcloud container buildscommand via the list or log sub command.
One important thing to note from here is the time that it took to complete the build. In my case, it was 49 seconds as seen in the final statement that it printed in the build output. This is important because there is a free tier that Google Container Builder provides i.e. First 120 build-minutes per day per billing account are free.
You can view the list of Container images via the gcloud command as shown below:
Do explore the gcloud container command for multiple other operations that you can do with Container images like tagging, untagging, deleting and more.
If you use the Google Cloud Console more than the command line (I do) — you can take a look at Google Container Registry option in the console.
This brings up the main screen of Container Registry, where you can see your image that just got built:
What is interesting here is that you can also view the Build History for the builds that you submitted. You can view the build process output in detail over here:
Click on any of the Build instances and you should see the build process output.
You could also use Stackdriver Logging to check out your build logs. Container Builder is a first class citizen with StackDriver too:
You can view the build output logs here too:
You might be wondering if this whole process is free for you as a user. It sounds too good to be true since there is a compute environment that is being provisioned for you to execute the build, your files are zipped and stored in Google Cloud Storage and there could be network egress for your Container images.
I suggest that you should look at the pricing page for full details, but here are some points at a glance.
Hope you enjoyed this tutorial. What we have seen is one area that Container Builder can streamline for you. It has a lot more capabilities like custom builds, triggers, notifying users in external apps like Slack when certain Build events happen, tying into Google Source Repositories to kick in builds when a code is pushed, etc. Do check those details out too.
Google Cloud community articles and blogs
166 
2
166 claps
166 
2
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/tensorflow/how-to-write-a-custom-estimator-model-for-the-cloud-tpu-7d8bd9068c26?source=search_post---------267,"There are currently no responses for this story.
Be the first to respond.
By Lak Lakshmanan (@lak_gcp), Technical Lead, Google Cloud Platform
Tensor Processing Units (TPUs) accelerate a wide range of machine learning workloads within Google and are available to Google Cloud customers. You can find TPU-enabled versions of state-of-the-art image models like ResNet and AmoebaNet in the Cloud TPU reference model repository; text summarization and question answering tasks can be performed on TPUs using the powerful Tensor2Tensor library. These tutorials walk you step-by-step through using many of the most popular Cloud TPU reference models.
But what if you have a custom TensorFlow model? In this article, I will step you through the process of writing a custom Estimator to run on the Cloud TPU. Along the way, I will point out gotchas to watch for and best practices to follow. The full code for the solution is on GitHub; I will show only pertinent snippets here.
A custom TensorFlow Estimator consists of a base Estimator that is passed in a model function:
The model function takes in features, labels and mode and returns an EstimatorSpec. For example, the model function for an image classification problem might consist of
The tf.contrib.tpu package in TensorFlow provides wrapper classes to help you write the code in such a way that you can run the code on CPU, GPUs, and Cloud TPUs. Let’s walk through the process of writing a custom Estimator in this accelerator-agnostic way.
Cloud TPUs are so fast that if you aren’t careful, your training will be dominated by reading and writing data (“infeed” and “outfeed”) and by the saving of checkpoints. Because it is wasteful to have TPUs wait on input/output, we will do several things to maximize the amount of time that the TPU spends on computation.
The first of these is to avoid parsing and data wrangling in the input function to the Estimator. Instead, transform the data beforehand into TF Records. TF Records are easier to batch than individual image files, and because the labels are in the record itself, this cuts down on the number of small files that have to be read. I used Apache Beam to carry out this transformation — you can find a script to read JPEGs and write out TF Records in the official TPU repository. The Apache Beam program can be executed at scale on Cloud Dataflow, but if your data source is not currently on Google Cloud, you can simply execute the program locally on a large VM (make sure to pip install apache-beam).
TF Records are dictionaries. For image classification, there are two entries written by the above pipeline that are important: ‘image/class/label’ which is an int64 and ‘image/encoded’ which consists of the content of the JPEG files.
As with any Estimator, you will need to write an input function to read in these TF Records. This task is considerably simplified when using the Dataset API, but there are a few things to keep in mind. I’ll point them out as we go along.
Here’s my input function:
Note that the input function takes a parameter — the params. In practice, this will be the command-line parameters passed to your training program so that we can extract details about the dataset such as the number of training and evaluation images.
The batch_size is special — because TPUs have multiple cores, the batch_size is set by the TPU Estimator and is the effective batch size. You have to return exactly batch_size records — you can not send back a partially filled batch. This is not a problem during training, since you will be looping over the training data indefinitely. However, it means that it’s simplest to round down the evaluation dataset to a multiple of the number of cores. If the number of cores is 8 and if you have 1026 images in your evaluation set, you will only use the first 1024 of them for evaluation. The remaining 2 will be dropped. (There are ways to process the final partial batch on Cloud TPU as well, but I won’t cover that detail here.)
As with any distributed training, you should ensure that each worker sees a different subset of the data — this is handled by the parallel interleaving of all your files and shuffling of records within the buffer itself.
A common need for image classification is to augment your original data by adding random crops, flips, etc. That is done by my read_and_preprocess function. Note that I apply this function to each TF Record and create 8 parallel batches, dropping any remaining records (again, this has no effect during training, since you repeat indefinitely).
The next part is of transposing. It turns out that, on TPUs, transposing the data to have the batch size last greatly improves the performance. So, we do that if necessary. The transpose_input flag will be false if we are running on a GPU or CPU.
TPUs require statically sized tensors. Although we have ensured that this is the case (by dropping the remainder), the Dataset API is written for core TensorFlow, which is more general. So, we call a function that changes the batch_size in the shape from None to, well, the batch_size.
The final bit of optimization is important. We need to prefetch the data. In other words, while the TPU is crunching one batch of records, we have the I/O threads go out and fetch the next batch. This keeps the TPU (or GPU) at maximum utilization. There is no impact on a CPU.
The input function (above) sets up how the input is handled, but defers the actual parsing to a method that I called read_and_preprocess(). Here’s how that looks like:
There are two key things to note here. One is the use of parse_single_example — because this function is called from a map(), it will be called on a single TF Record. We pull out the pertinent information (encoded image and label) from the record and use them to construct the necessary tensors. The second thing to note is that the data have to be numeric. I can not, for example, send back the label string because TPUs handle only numeric data. It is necessary to have computed the index of the label in the preprocessing pipeline so that the label, at this point, is simply an integer.
After you train the model, you will want to deploy the model and serve it with TF Serving. The code here is the same as you would have with any Estimator:
The TPU is optimized for batch inference — if your use case requires online prediction, you are currently better off serving from a CPU or a GPU (depending on the size and complexity of your model). The way I have written the input function, I am assuming that I am sent only one image, so this is really meant for CPU/GPU serving.
The model function needs to create and return a TPUEstimatorSpec. Here’s the implementation:
The features that are passed in might either be the image (my training and evaluation input functions) or a dictionary (my serving input function). I check, and retrieve the image from the features.
Then, I invoke my actual model math on the image. This should be familiar TensorFlow code that uses tf.layers. Browse the full source code to see how this looks.
Because this is a classification problem, I compute the integer label and the string label based on the logits for each of the classes using softmax followed by argmax and gather. I compute the cross entropy loss. This is like any Estimator.
The one difference is that while a regular Estimator requires the evaluation metrics as a dictionary, the TPUEstimator asks for a function that can be invoked either on the controlling CPU or on the TPU. Hence, the way you specify the eval metrics is a bit different.
The optimizer that you use has to be wrapped in a CrossShardOptimizer if you are using a TPU. This distributes the optimization across the cores.
The training operation is the minimization of this cross-shard-optimized loss. Use optimizer.minimize() and not layers.optimize_loss().
Put all these together and return a TPU Estimator Spec.
You may be familiar with Estimator’s train_and_evaluate loop. Unfortunately, it does not (yet) work effectively with TPUs. Fortunately, it is not too difficult to roll your own to gain more control in terms of how often and what you checkpoint (recall that you want to minimize the context switching and I/O overhead associated with overly frequent checkpointing).
The first thing is to pull out some of the command-line parameters and use them to specify the maximum number of steps and the batch sizes of training and evaluation.
The next bit is to find the TPU. If you created a Cloud TPU yourself on Google Compute Engine, you would have given it a name. I’m assuming that this name is passed in as the command-line parameter named ‘tpu’. If you are using Cloud ML Engine, the TPU name, zone, etc. are automatically inferred. Make sure to do this only if the use_tpu flag is set. If the user is running on a CPU or GPU, just create an empty RunConfig.
Next, create a TPUEstimator with model function, config, parameters and batch sizes. With the estimator created, we can move on to the actual training and evaluation loop:
The way that TensorFlow Estimators work is that they do a warm start from previously existing checkpoints. We can replicate that by loading in the checkpoint found in the output directory. Then, until we reach the maximum number of steps specified, we step through the training data train_batch_size steps at a time.
In my case, I am evaluating at every checkpoint on the full evaluation set, but obviously, you can make this less computationally intensive.
Finally, once the training is complete, I export a saved model. The saved model can be deployed for prediction using TF Serving or Cloud ML Engine.
At this point, we have a custom Estimator model that can be trained on Cloud TPUs. We wrote it in such a way (honoring the use_tpu flag and making the transpose optional, for example) that the same code also supports a variety of hardware, including CPUs and GPUs — so we actually have an Estimator model that works on all three types of hardware.
Next steps:
Take the Machine Learning with TensorFlow specialization on Coursera — it steps you through TensorFlow concepts and how to train, tune and deploy ML models at scale on Google Cloud.
TensorFlow is an end-to-end open source platform for…
180 
2
180 claps
180 
2
TensorFlow is an end-to-end open source platform for machine learning.
Written by
TensorFlow is a fast, flexible, and scalable open-source machine learning library for research and production.
TensorFlow is an end-to-end open source platform for machine learning.
"
https://medium.com/@lestrrat/taming-google-container-builder-22a6dded155c?source=search_post---------268,"Sign in
There are currently no responses for this story.
Be the first to respond.
Daisuke Maki
Dec 21, 2017·11 min read
Google Container Builder is Google Cloud Platform’s way of building container images for your projects.
cloud.google.com
It’s a basically a pipeline of “steps” that you can configure to build a chain that ultimately creates a container image. However, as of this writing (Dec 2017), it’s decidedly very… primitive.
I had already dabbled with Container Builder about a year ago, and could not make it do what I wanted. But recently I had to make a decision to leave my current CI/CD pipeline service, so I took a fresh look, and this time around, voila. I was able to do everything I wanted.
This post is basically my notes on the most useful and important points (read: where I stumbled) from my porting work.
Please note that I expect the reader to have a basic understanding of how to work with Google Cloud Platform, and that you have at least glanced through Google’s official documentation for the basics of how to use Container Builder.
Compared to other Continuous Integration/Continuous Delivery services such as Circle CI, Travis, and the like, the conceptual model that Container Builder provides is very minimal.
travis-ci.com
circleci.com
docs.gitlab.com
(And BTW, I use all those services in my other projects)
Like other services, Container Builder expects a YAML configuration file to describe the steps to build your software, cloudbuild.yaml . However, the documentation is a little bit lacking. I believe this is the only page where the cloudbuild.yaml is described in its entirety:
cloud.google.com
The simplest way to describe the contents of cloudbuild.yaml is this: “You are specifying a list of arbitrary container images to be executed”. I believe this is the conceptual equivalent of what you are writing to cloudbuild.yaml:
The cloudbuild.yaml is just a way to write this in an alternate form.
Notice that there are no “before_script”, “artifacts”, “cache” or whatever in cloudbuild.yaml . It’s very… barebones.
Before describing my pipeline, I’m going to start with what turned out to be the crucial revelation that finally allowed me to do what I wanted in Google Container Builder.
It was the entrypoint parameter in the step configuration.
The basic premise of the Container Builder is to break the entire process into containerized steps, such as “git”, “go”, “bazel”, etc. The stumbling block here is that when you read the docs, it seems as if you can only supply arguments to the principal command that the container was build for. For example, for docker step, you see something like the following:
So what happens when you run this step and get an error? is it because I have my files in the wrong place? Am I lacking some environment variables? This is frustrating because it looks as though the only thing you can do there is to provide more arguments to the docker command — and if you have done it before, you know that the interesting bits of a docker failure logs are all with the dockerd, not the docker client command. It almost looks as if there’s no way to diagnose the situation.
But do not despair.
This is where the entrypoint option comes in (I could swear this option did not exist previously, but the important thing is that I know of it now). This parameter in the cloudbuild.yaml step entry allows you to change the ENTRYPOINT command to an arbitrary executable in the container.
Therefore, you can run diagnostics, helper commands, or anything that you want for that matter, if you insert a shell in the entrypoint . Below sample prints out the environment variables before executing go build :
So with entrypoint, your steps are basically the equivalent of the follwing:
The entrypoint parameter and the use of shells in there gives you great, great, flexibility to make the Container Builder what you want to do, and quite frankly, I don’t think I could have done it without it.
I have one quick note about the ./builder/prepare_workspace.inc and prepare_workspace || exit bit above. One of the great things about Container Builder is that the basic steps are available for you on Github:
github.com
So if you ever need to peak, tweak, and/or customize their steps, you can always refer to their Dockerfiles and other helper scripts. This is how I learnt that the gcr.io/cloud-builders/go container was calling those two scripts before executing the go command.
Seeing your jobs being processed through Google’s system may give you sone thrills at the beginning, but you will soon realize the pain of waiting for remote jobs to execute, especially when you are initially developing your pipeline.
You will also most likely be wanting to run many jobs in succession as you tweak your parameters: so those extra seconds and minutes that the remote jobs require starts adding up.
The answer is to run the jobs locally. Luckily for us, Google provides us with a simplified version of the Container Builder pipeline that you can run on your machine.
https://github.com/GoogleCloudPlatform/container-builder-local
You can install this via the gcloud components command.
As far as I could tell, this command will give you exactly what you would see in the production environment, up to the service account that will normally execute your jobs, which allows you to debug even those nasty permission problems.
Throughout this entry, I will use the real gcloud command for examples, but always remember that you can substitute container-builder-local for gcloud container builds submit calls.
Finally, getting back to my Container Builder pipeline: The basic premise is as follows:
There are a few pain points that I’m sure everybody will stumble upon in this scenario. Let’s see what I did.
First, we need to clone the repository. But the repository is hosted outside of the Google environment, and is no supported by the likes of GCP’s Cloud Source Repositories (which, if I were hosting on Github, would give me an automatic mirroring option to make authentication a non-problem)
cloud.google.com
To start configuring, first we need a deploy key. Head over to your service, in this case Gitlab, and they should tell you how to register an SSH key to used as a deploy key. (Newbie reminder: Create a new key only for this purpose. You register the public key to the code repository service, and keep you private key)
Just for the heck of it, I created a key with ed25519 encryption. Remember to provide an empty password.
Now, in order for Container Builder to clone the repository, we need to somehow give it your private key, so that cloning over SSH works properly.
It would be rather insecure to just hand your private key to the service by sending the raw file or adding it in the source code repository, so we should be using a Key Management Service (KMS) to at least encrypt it.
Turns out Google also has a KMS. I mean, Google has everything.
cloud.google.com
The instructions on how to do this are pretty straight forward, so it should be easy to follow the guide below. Note that they are already using the entrypoint hack that I describe earlier in this article:
cloud.google.com
For those who are not already familiar with how KMS works, it’s a comprised of keys to encrypt/decrypt arbitrary data. The keys belong to keyrings, which are logical grouping of such keys.
What the above document is describing is how to take an SSH private key, encrypt it using a KMS key, and send it to the Container Builder environment for further processing. Because the private key is now encrypted, it is, um, safe to be sent to Container Builder.
In my case I had to encrypt my Gitlab key (obviously, I created my keyring and key beforehand)
I suppose that once encrypted, one could add this private key in the repository, but I personally opted to store the master version in Cloud Storage. When the build process starts, the necessary files are downloaded via gsutil, and processed.
Now that you have the encrypted private key (and other files) accessible from Container Builder, you can decrypt it and unpack it under /root/.ssh/ .
The document then tells you to patch the SSH configuration so that when git attempts to access the source repository, it will use that private key to authenticate against the server.
One thing to note is that you need to give your Container Builder service account (which is automatically created when you enable the service) the proper permissions to decrypt your data. You should be able to do this by going to IAM console, and giving the service account that looks like $your-numeric-project-id@cloudbuild.gserviceaccount.com the authorization to perform KMS decryption.
Having done that you can decipher your key like this. Don’t forget to keep the results in a “volume” so that it persists between steps.
Also, do not forget to properly configure git to use this key. In my case, I’m going to configure git clone against gitlab.com to use this key. In the following example, the container gcr.io/cloud-builders/git doesn’t really mean anything — all you need is a container with bash and the required commands. Again, do not forget to put the results in a persistent volume
Once you have the necessary files, you can now run git clone git@gitlab.com:... or whatever else that requires an SSH key to be present.
Because we’re just building from a particular commit/branch, all that the pipeline needs from git are the files from that 1 commit. So a regular clone, which downloads all of the history, is overkill here.
I opted to workaround this by explicitly telling Container Builder which branch we want to work with through the command line. In Container Builder, what you do is to provide Substitution Rules.
This sends the variable $_BRANCH to Container Builder with the current branch name. These variables will be substituted before your cloudbuild.yaml is processed, so in your git clone step, you can write something like below to get a shallow clone:
This will clone just the target branch, and it will not download the history.
Because I could not use the automated kicks to build from a git push, I had to manually find out the current commit SHA. Normally you probably want to embed a call like the following close to where you would use it:
But remember that we’re using containers here: The git command may not be available in other container images, so it’s safer to just save it in the /workspace
Then all you need later is the cat command to get this value.
I will be the first to admit that Container Builder is not for everybody. It’s opinionated, and it’s primitive.
While it gives you enough leverage to do whatever you want, nice-to-haves such as hooks and integrations, a real Web UI, and automated wrappers for things that I had to painstakingly explain in this article are glaringly missing.
If you are comfortable with your current CI/CD pipeline, I do not think you need to consider switching, unless you have your reasons.
But on the other hand, now that running CI/CD on containers re becoming ubiquitous the approach taken by Container Builder does not feel strange.
However, one place that Container Builder absolutely shines is its ability to run your pipeline locally from a simple binary.
If you have ever crafted pipelines in other services you know what I am talking about: the long waits, the errors that occur because you made one typo, and then the long waits again...
Heck, on another service I once had to wait over 5 hours for one of my pipelines to kick (and this is precisely one of the main reasons I had to look for alternatives). Understandably, it was apparently caused by an infrastructure glitch, but that’s nonetheless 5 hours of development time gone.
Container Builder takes away much of the above frustration by allowing you to run you pipeline right in front of your eyes, right when you want to run it. This, I feel, is a big win for the developer.
To complement the local runs, I must say that the pipelines that run on GCP are blazingly fast too. Not just the execution speed, but you practically have no queuing.
This can only be done by services that actually owns the infrastructure. YMMV but it feels nice not be completely stuck on that “pending” icon when all you want to know is if your build has passed.
I think the Container Builder is a very useful framework. It’s fast, development times are shorter because of local builds, and generally it gets the job done. After all, if you need more functionality, you can just create your own containers to do what you want.
On the other hand, if you’re not vested in the Google infrastructure, and if you absolutely need that fancy Web UI, it’s probably not for you.
Go/perl hacker; author of peco; works @ Mercari; ex-mastermind of builderscon; Proud father of three boys;
366 
5
366 
366 
5
Go/perl hacker; author of peco; works @ Mercari; ex-mastermind of builderscon; Proud father of three boys;
"
https://towardsdatascience.com/how-to-do-serverless-machine-learning-with-scikit-learn-on-google-cloud-ml-engine-db26dcc558a2?source=search_post---------269,"Sign in
There are currently no responses for this story.
Be the first to respond.
Lak Lakshmanan
Aug 3, 2018·4 min read
On Google Cloud Platform, Cloud ML Engine provides serverless machine learning, for training, hyperparameter optimization and predictions. Until recently, that was only for TensorFlow. Recently, though, the team has implemented all three capabilities for scikit-learn. In this article, I will take it for a spin.
The problem is to predict the weight of a baby given some information about the pregnancy. This is a problem that I solve end-to-end with TensorFlow in a bootcamp that I developed for GCP NEXT 2018. Let me now do with scikit-learn. Follow along with me by looking at this Jupyter notebook on GitHub.
The input data is in BigQuery, and I pull 1/1000th of the full dataset into a Pandas dataframe:
It is possible to use the full dataset if I use a larger VM, and I will do that in the final step, when training on the service. For developing the model, though, it is helpful to have a small dataset.
To train the model, I first wrote a function to get the x and y for training (I’m calling them features and label):
Then, I created a RandomForestRegressor and passed the x and y into its fit() method:
Evaluating the performance of the model can be done by calling predict() on the evaluation dataframe and compute the RMSE:
Once I had the code working, I added the following Jupyter magic to the cells in the notebook to write the cells out to Python files:
I also made many of the hardcoded numbers (such as the number of trees in the random forest) a command-line parameter. By making them command line parameters, I made it possible to hyperparameter tune them.
I tested the package locally, still on the 1/1000th of the dataset:
Training on Cloud ML Engine is as simple as submitting the Python package:
This time, I am training on 1/10 of the full dataset. To train on the full dataset, I will need a larger machine. I can do that by changing to a custom tier:
and passing in the above configuration file:
How did I get the max depth and numTrees? For that, I did hyperparameter tuning.
For hyperparameter tuning in ML Engine, write out a summary metric after evaluation. This is the code, assuming you have a variable named rmse which holds the final evaluation measurement:
To submit the hyperparameter tuning job, write a configuration file with the parameters you want to optimize:
Then, pass it to ML Engine with:
The outputs of the trials will start to populate and if you look at the GCP web console, the trial with the lowest RMSE will be listed, along with its runtime parameters.
Once trained, the scitkit learn model can be deployed:
The deployed model has an endpoint that you can access to get predictions:
Currently, the request has to be in the form of a text line, and has to be preprocessed data. Naturally, this imposes some caveats on how much you can expect to operationalize a scikit-learn model.
Again, my full code is on GitHub. Happy coding!
Data Analytics & AI @ Google Cloud
See all (63)
96 
Every Thursday, the Variable delivers the very best of Towards Data Science: from hands-on tutorials and cutting-edge research to original features you don't want to miss. Take a look.
96 claps
96 
Your home for data science. A Medium publication sharing concepts, ideas and codes.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/net-core/deploy-an-asp-net-core-app-with-ef-core-and-postgresql-to-google-cloud-be8a06978eb0?source=search_post---------270,"There are currently no responses for this story.
Be the first to respond.
In this post, I will show how to deploy an ASP.NET Core web application with EF Core and PostgreSQL to App Engine on Google Cloud Platform (GCP). Besides, I will show how to use Cloud SQL for PostgreSQL instance from this application.
I will use the following tools:
The sections of this post will be as follows:
In the previous post, I demonstrated how to deploy an ASP.NET Core web application starter project to GCP. As it has no database access, it is a simpler version of this one and I recommend you start from there if this is your first deployment to GCP.
If you are ready, let’s get started.
The application that we will deploy manages a database of TV Shows and its main page looks like below:
The app is an ASP.NET Core MVC web application and it uses EF Core to perform CRUD operations on a PostgreSQL database.
You can download the source code from this GitHub repository.
If you don’t have a Google Cloud account, go to Google Cloud web site and click Get started for free button. Sign up with your Gmail account and get $300 (for free) to spend on Google Cloud Platform over the next 12 months.
Next, go to Google Cloud Console and type App Engine in the search box and click Create in the dashboard of the App Engine to create a new project:
Name your project and then select your region and enable billing in the next page:
Now, we will create our PostgreSQL instance in the Cloud SQL.
Cloud SQL is a fully-managed database service that makes it easy to set up, maintain, manage, and administer your relational databases on Google Cloud Platform. You can use Cloud SQL with either MySQL or PostgreSQL.
First, select APIs & Services -> Library
and in the next page search for Cloud SQL API and Enable this API:
Next, search for Cloud SQL and click Go To Cloud SQL in the following page:
Click Create Instance in the next page:
and then choose PostgreSQL.
On the next page, give an ID to your instance and set your password for the default (postgres) user.
We will use this Id and password to connect to this instance later in the tutorial.
Next, the Compute Engine API creates the instance and it takes a few minutes.
After you see the green tick on the instance ID, click on the instance to go to Instance Details page:
In the Overview tab, you can see the IP and connection name for the instance:
Next, go to the Connections tab and click Add Network:
Security Issues
As you see, we gave access to public IPs to make login attempts to the instance. SSL encryption is recommended when using Public IP to connect to the instance.
For the sake of simplicity, I will not implement this part in this tutorial. If you want to implement this, please follow the instructions here. Besides, you will need to modify your app too. If you need help in this part, let me know in the comments.
In Visual Studio, open the application and modify the connection string in appsettings.json:
If you haven’t created your migration file already, run the following command in the Package Manager Console:
and then run the below command to create the database on the instance:
Now, we will check the database created on Cloud SQL using pgAdmin.
Launch pgAdmin and then select Servers -> Create -> Server:
and then fill in the fields shown in the red box below:
As you see below, our database and tables were created on the Cloud SQL:
Now, we will test the application on our local computer.
In this test configuration, our application server is on localhost and the database server is on the cloud.
Run the application on the Visual Studio and click TVShowsApp on the web page and then Create New on the next page.
After creating our first record, it is shown below on the main page:
We can check the record created using pgAdmin as well:
Now, we will publish our application to Google Cloud.
First, install Google Cloud Tools for Visual Studio if you don’t have it already.
Open Visual Studio and go to Tools -> Extensions and Updates. Search for Google Cloud Tools for Visual Studio and click Install (It’s already installed on my computer):
As a side note, Cloud Tools for Visual Studio is not supported in VS 2019. So, I am using VS 2017 for this tutorial.
Next, we will link our application to our GCP project.
Open Tools -> Google Cloud Tools -> Manage Accounts:
and select Add account. Login to the account that you used to register to the GCP.
You will see your account logged at the top right corner of Visual Studio. There is a No Project label near this account and click that and click Select Project:
Select the project that you created above.
Enable the following APIs from Google Cloud Console:
Next, right-click on the project in Visual Studio and select Publish To Google Cloud. Then select App Engine Flex in the next dialog.
After the deployment operation is completed, the web page is launched automatically and the application’s URL is as follows:
I created a new record as we did in the previous section and the main page of the application running on the cloud looks like below now:
Again, we can check the new record from the database using pgAdmin:
To avoid incurring charges, we can delete our GCP project to stop billing for all the resources used within the project.
In the Google Cloud Console, search for Manage Resources. In this menu, select your project and click Delete:
That’s the end of the post. I hope you found this post helpful and easy to follow. If you have any questions and/or comments, please let me know in the responses section below.
And if you liked this post, please clap your hands 👏👏👏
Bye!
https://cloud.google.com/appengine/docs/flexible/dotnet/using-cloud-sql-postgres
Posts related to .NET Core (ASP.NET Core, MVC, Web API…)
152 
3
152 claps
152 
3
Posts related to .NET Core (ASP.NET Core, MVC, Web API…)
Written by
A software developer who loves learning new things and sharing these..
Posts related to .NET Core (ASP.NET Core, MVC, Web API…)
"
https://medium.com/trendyol-tech/kubernetes-best-practices-c51137ec0e1e?source=search_post---------271,"There are currently no responses for this story.
Be the first to respond.
Merhaba, bu yazımızda Google Cloud Platform kanalının Kubernetes Best Practices adlı oynatma listesinden öğrendiklerimizi paylaşacağım , bu çoğu pratikleri Trendyol içerisinde de uygulamaya gayret ediyoruz.
* Kubernetes uygulamaları deploy etmenin yolu uygulamayı container içerisine koymaktır.
* Default base image kullanmak büyük image oluşmasına ve bir sürü güvenlik açığına sebebiyet verebilir.
* Çoğu Docker varsayılan image base image olarak Debian veya Ubuntuyu kullanır ve bu base imageler yüzlerce MB ek yük getirebilir.Tüm bu ek yük , güvenlik açıkları ve buglar için harika bir saklanma yeri olabilir.
* Image boyutlarını azaltmanın iki yolu var.- “Small Base Images”- “Builder Pattern”
* Küçük base imageler kullanmak(alpine versiyonlar) muhtemelen image boyunu düşürmenin en kolay yolu.
* Senin uygulamanın kullandığı dilin küçük base image seçeneği yoksa imageni, “raw linux alpine” imagesini base olarak kullanarak inşa edebilirsin.Bu sayede container içerisine nelerin gideceği konusunda tamamen kontrol sahibi olur
* Küçük base imagelerin ve builder patternin ölçülebilir avantajlarının görüldüğü iki alan var.- “Security”- “Performance”
* “Performance” tarafına bakarsak imagelerin build edilmesi , pull edilmesi ve push edilmesi süreleri çok daha azalacaktır.Imagelerin build ve push sürelerinin çok önemi olmasa da pull edilme süresi oldukça önemlidir çünkü şöyle bir senaryoda clusterdaki down olan bir node tekrardan tüm imageleri pull etmek zorunda ve bu süre ne kadar kısa olursa node kendini o kadar kısa sürede toparlayacaktır.
* “Security” tarafına bakarsak küçük imageler daha küçük yüzeye(surface) e sahip olduklarından ataklar konusunda daha güvenli olacaktır.(Google Container Vulnerability Scanner)
* K8s üzerinde daha fazla servis geliştirmeye başladığımız zaman en basit işler bile karmaşıklaşmaya başlar.Örneğin takımlar aynı isimli deployment veya service oluşturamaz, eğer binlerce podun varsa bunları listelemek uzun zaman alabilir.Bu gibi durumları basitleştirmek için “namespaceler” kullanılır.
* Namespace’i k8s clusterındaki sanal bir cluster olarak düşünebilirsin.
* Tek k8s clusterı içerisinde namespacelerin olabilir ve bunlar birbirinden tamamen izoledir ayrıca sana güvenlik ve performans açısından yardımcı olur.
* k8s temel olarak 3 namespace ile birlikte gelir. default , kube-system ve kube-public.
* kube-system ve kube-public genelde Kubernetes ile alakalı workloadları(pod) barındırır dolayısıyla bizim workloadlarımız için tek yer “default” kalıyor.
* kubectl create namespace test* kubectl get namespaces* kubectl apply -f test.yaml — namespace=test
* normalde tüm kubectl komutları current active namespace de çalışır.(eğer herhangi bir namespace belirtilmez ise bu “default”’dur.)
* namespaceler arasındaki serviceler ile konuşmak mümkündür.Normalde uygulaman bir Kubernetes servicesine erişmek isterse yapısal olarak desteklenen “DNS Service Discovery” i kullanabilir ve uygulamaya bu service ismi ile erişebilir.Fakat farklı namespacelerde aynı service ismine sahip uygulamalar olabilir bunu da “Expanded DNS Address” ile çözebiliriz.(Cross Namespace Communication)- <service name>.<namespace name>.svc.cluster.local şeklinde, buna common DNS pattern de denir.
* Healthcheckler sistemin , uygulamanın çalışıp/çalışmadığını anlamasına imkan verir.
* Eğer uygulaman çalışmıyorsa , diğer servisler bu uygulamaya erişememeli ve request iletememelidir.Bunun yerine requestler uygulamanın diğer sağlıklı instancelarına iletilmelidir.Ayrıca system uygulamayı tekrardan sağlıklı hale getirebilmelidir.
* Varsayılan olarak k8s , pod içerisindeki tüm containerlar çalıştığı zaman poda trafik yollamaya başlar ve containerlar crash olduğu zaman da restart eder.
* Deploymentlarını custom health checkler ile daha güçlü hale getirebilirsin.
* k8s’de iki tip health-check vardır.
- “Readiness”* k8s’e uygulamanın ne zaman trafik almaya hazır olduğunu bilgisini verir.
* k8s pod trafik iletmeye başlamadan önce “Readiness Probe”’dan başarılı şekilde geçtiğinden emin olur.
* Eğer “Readiness Probe” fail ederse k8s uygulamaya trafik yollamayı tekrardan ilgili health-check başarılı olana kadar keser.
- “Liveness”* k8s’e uygulamanın canlı olup/olmadığı bilgisini verir.
* Eğer uygulaman sağlıksız ise k8s podu siler ve podun yerine yeni bir tane başlatır.
* Şimdi ise sıra bu health checkleri test edeceğimiz probelara geldi , 3 tip probe var.
* Probelar konfigüre edilebilir.- initialDelaySeconds: ne kadar süre sonra bu health checkler test edilmeye başlanacak.- periodSeconds: ne kadar süre aralıklarıyla test için bu istekler iletilecek.- timeoutSeconds: ne kadar süre isteğin cevabı beklenecek.- successThreshold: kaç tane başarılı istekten sonra başarılı sayılacak.- failureThreshold: kaç tane fail istekten sonra hatalı sayılacak.
* Uygulamaların çalışabilmesi için containerlara yeterli kaynakların verilmesi gereklidir.Eğer büyük bir uygulamayı küçük kaynaklarla çalıştırmaya çalışırsan out of memory hataları alınabilir.
* Requests ve Limits , örneğin CPU ve memory gibi kaynakları kontrol etmek için kullandığı mekanizmalardır.
* Requests , containerların ilgili kaynağı almayı garanti ettiği tanımdır.Eğer container bir kaynak talep ederse , talep ettiği kaynağı k8s ona vererek node üzerine schedule eder.
* Limit ise containerin asla üstüne çıkmayacağı değeri belirler.2 tip resource vardır.- “CPU” ve “Memory”.
!* Eğer “requests” edilen kaynaklar sahip olunan nodeun veya nodeların toplam kaynaklarından büyük olursa pod asla schedule edilemez.
* Pod içerisindeki tüm containerlar kendi requests ve limit’lerini belirleyebilir.
* CPU resourceları milicore olarak tanımlanır.Eğer container 2 full core a ihtiyaç duyuyorsa çalışmak için sen bu değeri “2000m” olarak vermelisin.“2 core == 2000m”Eğer 1/4 ü kadar core ihtiyacı varsa o zaman da “250m” olarak bu değeri set etmelisin.
* Memory resourceları byte olarak tanımlanır.
* CPU sıkıştırabilir bir kaynak olarak bilinir,dolayısıyla uygulaman bu CPU limitlerine dayandığında k8s uygulamanın CPU kullanımını kısmaya başlar böylece uygulamanın yalnızca performansı düşecektir uygulama terminate edilmeyecektir.Fakat Memory sıkıştırabilir bir kaynak olmadığından verilen limite dayandığında veya bu limiti aştığında uygulama terminate edilecektir.
* ResourceQuota ve LimitRange benzer işler için kullanılır.ResoureQuota aslında namespace bazlı kaynak sınırlandırmasının tanımıdır.
aslında tüm bu değerler namespace altında tanımlı olan tüm podların containerlarının maks sahip olabileceği ve maks talep edebileceği kaynak değerleri.
LimitRange ise namespace içerisindeki bireysel olarak containierin maks ve min request/limit kaynak değerlerini belirler, yani bu değerden düşük talep edemezsin veya bu kadarden daha yüksek talep edemezsin gibi.
* Tüm bu requests ve limits aslında scheduler tarafından bir node a podun schedule edilmesi için kullanılır , scheduler aslında round robin load balancing kullanarak podu çalıştırmak için bir node arar ve node podun talep ettiği kadar yeterli kaynağı var mı yok mu kontrol eder, eğer hiçbir node da yeterli kaynak yoksa pod “Pending State”de bekler.
* Uygulamalarımız SIGTERM mesajını handle etmeli ve bu mesajı gördüğünde shutdown işlemini başlatmalı.Kubernetes Termination Lifecycle — — — — — — — — — — — — — — — — * bir podu terminate etmek istediğimizi düşünelim, bu noktada pod trafik almayı durduracaktır.Terminating statedeki bir pod için önce“preStop” hook çalıştırılır, SIGTERM signal yollanır poda.
terminationGracePeriodSeconds: 60 → paralelde olur bu diğer işlemlerle.yani preStop hook veya sigterm signal bitmesini beklemez.Tüm bu işlemlerden sonra uygulama halen kapanmadıysa SIGKILL signal yollanır ve force kill e zorlanır.
* External servicelere de erişmek için service ve endpoint oluşturabiliyorsun.Endpoint içerisinde bu external servicenin ipsini tanımlamak zorundasın.
Örneğin bir MongoDB serverimiz olsun external olarak.IP’si 10.12.232.424 olsun.Önce bir service oluşturuyoruz.
görüldüğü gibi herhangi bir pod selector yok burada , peki service trafiği nereye yönlendireceğini nereden bilecek ? işte bu noktada manual olarak bir Endpoint oluşturuyoruz.
veya external services using domain name diye bir yapı var bu konuda da service type olarak “ExternalName” kullanılır.
küçük bir not ilk işlemden sonra yani node üzerinde tüm podları sildikten sonra bu podların diğer nodelar üzerine schedule olduğundan emin olduktan sonra nodeu yönetimden çıkarmamızda fayda var.
Trendyol Tech Team
311 
311 claps
311 
Trendyol Tech Team
Written by
I do mostly Go, Kubernetes, and cloud-native stuff ⛵️🐰🐳
Trendyol Tech Team
"
https://medium.com/google-developer-experts/trimming-down-the-cost-of-running-google-cloud-dataflow-at-scale-1c796f72c002?source=search_post---------272,"There are currently no responses for this story.
Be the first to respond.
Google Cloud Dataflow is one of the products provided by Google Cloud Platform which helps you ingest and transform data coming from a streaming or a batched data source.
At Roobits, we extensively use Dataflow pipelines to ingest events and transform them into desirable data that is to be used by our customers.
Dataflow is also serverless and auto-scales based on the input load, which is an added bonus to the flexibility it already provides.
Dataflow essentially requires you to write the logic that’s to be performed on the incoming events from a source (which could be PubSub, Apache Kafka, or even a file!) and then deploy that logic on Google’s servers.
Dataflow allows you to write this logic either in Java, Kotlin or Python.
A very simple example of a Dataflow Pipeline that takes an input paragraph and counts the words in it, is as follows :
While the code here might look complicated, you can go to the documentation page of Apache Beam to know more about what’s happening here.
To deploy this code on your Google Cloud Project, you can do so as follows :
While it looks good, there are certain concerns when it comes to pricing as you plan on scaling this pipeline as it is.
Let’s look at them one by one.
By default, the disk size for the dataflow pipeline is set to 250GB for a batch pipeline and 400GB for a streaming pipeline.
If you are processing the incoming events in memory, this is mostly a wasted resource, so instead, I’d suggest reducing this parameter to 30GB or less (the min recommended value is 30GB but we faced no issues while running the pipeline at 9–10GB of PD)
You can do so by specifying the disk size as follows while deploying your pipeline :
Now looking at Google Cloud Pricing calculator, reducing this value saves us around 20$ per month per worker.
Micro batching a streaming pipeline helped us cut down on the number of writes our dataflow pipeline made into BigQuery, thereby reducing the cost of BigQuery writes.
You can look at the article below for more insights on how to do this :
medium.com
By default, Dataflow supports the n1 machine types for the pipeline and while these machines cover a variety of use cases, however, you might often want to use a custom machine of your own with either a powerful CPU or a large RAM.
To do this, you can add the following parameter while deploying the pipeline :
The value above would correspond to 8 cores and 7424 MB of memory and you can tweak this according to your will instead of being locked into using the presets.
Streaming Engine is a new addition to the Dataflow family and has several benefits over a traditional pipeline, some of them being :
As of now, the streaming engine is only available in the regions mentioned in the list here, but more regions will be added as the service matures.
To enable Streaming Engine, just pass the following flag to your pipeline execution and that’s it!
By default, the Dataflow service assigns your pipeline both public and private IP addresses.
Now if you don’t want your data to be made available to the general public, it’s a good idea to disable public IPs as that not only makes your pipeline more secure but might potentially also help you in saving a few bucks on your network costs.
Adding the following flag to the pipeline execution disables public IPs :
While it might be a no brainer for some, but I see a lot of people (including myself) paying extra for data that is transferred between the GCP services, just because they are not in the same region.
For instance, we ended up paying around 500$ in a week one of our projects, because the dataflow pipeline and the source AppEngine were in different locations (US and Europe)
Not only AppEngine and Dataflow, but a lot of GCP services have free ingress/egress from/to the same region!
To set the region while deploying your Dataflow pipeline, you can add the following execution parameter :
The supported regions by Cloud Dataflow are listed here :
cloud.google.com
And that’s it!Using a combination of the tips mentioned above, we were able to save a substantial amount from our spendings on Dataflow.
You can visit my Medium profile to read more blogs around Dataflow and Google Cloud; starting with this one that I wrote last week!
medium.com
Thanks for reading! If you enjoyed this story, please click the 👏 button and share to help others find it! Feel free to leave a comment 💬 below.
Have feedback? Let’s connect on Twitter.
Experts on various Google products talking tech.
349 
1
349 claps
349 
1
Experts on various Google products talking tech.
Written by
Has an *approximate* knowledge of many things. https://aftershoot.co
Experts on various Google products talking tech.
"
https://read.acloud.guru/cloud-based-ci-cd-on-gcp-6b07fde7222d?source=search_post---------273,NA
https://codeburst.io/vpc-networking-gcp-v-s-aws-77a80bc7cfe2?source=search_post---------274,"Comparing and contrasting Google Cloud Platform’s (GCP) with Amazon Web Services’ (AWS) virtual private cloud (VPC) networking.
TL:DR: Google Cloud Platform VPCs are relatively flat with controls targeting the instance, whereas Amazon Web Services VPCs are hierarchical with multiple layers of control at the region, zone, subnet, and instance.
The GCP and AWS solutions illustrated in this article are available for download as Terraform configuration files.
To explore the commonalities and differences between the GCP and AWS VPC features, we will walk through each one’s solution to a scenario consisting of three instances:
We can further describe the scenario using network flows in the following color-coded network diagram:
Internet to Frontend and Frontend to Internet (red)Internet to Bastion and Bastion to Internet (blue)
These flows allow for clients on the Internet to access the frontend instance (using HTTP) and the bastion instance (using ssh). These flows also allow for these instances to access servers on the Internet, e.g, performing software updates.
Backend to the Internet (green)
This flow allows for the backend instance to access servers on the Internet.
Bastion to Frontend (dark purple)Bastion to Backend (light purple)
These flows allow for the bastion instance to access any instance, including the frontend and backend, using ICMP and ssh.
Frontend to Backend (aqua)
This flows allows for the frontend instance to access the backend instances using HTTP.
The routes and firewall rules details are provided after the flow sections.
Internet to Frontend and Frontend to Internet (red)Internet to Bastion and Bastion to Internet (blue)
The frontend and bastion instances have both an internal IP address, e.g., 10.128.0.2, and external IP address, e.g., 35.206.100.29.
Please note: The instances’ network interface here is configured with the Standard Tier, i.e., traffic traverses the public Internet to and from the GCP region, e.g., us-central1. GCP’s premium tier networking is another differentiator between GCP and AWS; which is outside the scope of this article.
Inbound traffic to the instances’ external IP address published on the Internet gateway (IGW) is forwarded to the instances with their internal IP address.
The allow-http-target-frontend firewall rule allows HTTP traffic from any address, i.e., 0.0.0.0/0, to instances tagged with frontend; the frontend instance is tagged with frontend.
The allow-ssh-target-bastion firewall rule allows SSL traffic from any address, i.e., 0.0.0.0/0, to instances tagged with bastion; the bastion instance is tagged with bastion.
For outbound traffic, the VPC’s implied egress firewall rule, allows any instance to send most any traffic to any destination, i.e., 0.0.0.0/0.
The route “Default route to the internet” sends Internet-bound traffic to the IGW for instances with an external IP address. Otherwise, it sends it to a regional network address translation gateway (NAT) if it exists.
Backend to the Internet (green)
The backend instance has only an internal IP address; without an external IP address there can be no inbound Internet traffic.
For Internet-bound traffic, the route “Default route to the internet” in this case sends Internet-bound traffic to the regional network address translation gateway (NAT). The NAT uses its own external IP address to send traffic to servers on the Internet
Bastion to Frontend (dark purple)Bastion to Backend (light purple)
The allow-icmp-source-bastion firewall rule allows ICMP traffic from instances tagged with bastion to any instance.
The allow-ssh-source-bastion firewall rule allows ssh traffic from instances tagged with bastion to any instance.
Frontend to Backend (aqua)
The allow-http-target-backend firewall rule allows HTTP traffic from instances tagged with frontend to instances tagged with backend; the backend is tagged with backend.
Routes
Firewall Rules
The routing tables and security group details are provided after the flow sections.
Internet to Frontend and Frontend to Internet (red)Internet to Bastion and Bastion to Internet (blue)
The frontend and bastion instances have both an internal IP address, e.g., 172.16.0.189, and an external IP address, e.g., 3.81.119.142. The subnet housing these instances is configured to assign instances external IP addresses.
Inbound traffic to the instances’ external IP address published on the Internet gateway (IGW) is forwarded to the instances on their internal IP address.
While subnets’ network access control list (NACL) can be used to further control traffic flow, in this project we leave them with the AWS default VPC behavior; an open NACL.
All of the instances are associated with the security groups:
The frontend instance is also associated with the project-frontend security group. The project-frontend security group allows HTTP traffic from any address, i.e., 0.0.0.0/0, to instances with it.
The bastion instance is also associated with the project-bastion security group. The project-bastion security group allows HTTP traffic from any address, i.e., 0.0.0.0/0, to instances with it.
Because the frontend and bastion instances are in a public subnet, Internet-bound traffic is sent to the IGW. A public subnet is a subnet with a route table that has a default route to an Internet gateway; in this case the project-public route table. A private subnet is not a public subnet.
Backend to the Internet (green)
The backend instance has only an internal IP address; without an external IP address there can be no inbound Internet traffic.
Because the backend instance is in the subnet associated with the project-private-0 route table, Internet-bound traffic is sent to the NAT in the same zone as the subnet. The NAT uses its own external IP address for traffic to servers on the Internet.
Bastion to Frontend (dark purple)Bastion to Backend (light purple)
The project-instance security group, associated with all instances, allows both ICMP and ssh traffic from instances associated with the bastion security group.
Frontend to Backend (aqua)
The project-backend security group, also associated with the backend instance, allows HTTP traffic from instances associated with the frontend security group.
Route Tables
project-public (associated with both public subnets)
project-private-0 (associated with one private subnet)
Please note: The third route table is project-private-1 that is associated with the second private subnet; with a different NAT; not used in this scenario.
Security Groups
project-frontend
project-bastion
project-egress
project-instance
project-backend
In general, Google Cloud Platform VPCs are relatively flat with controls targeting the instance where-as Amazon Web Services VPCs are hierarchical with multiple layers of control: at the region, zone, subnet, and instance.
While not relevant to this example scenario:
Also not relevant to this example scenario:
One relevant difference:
Another related one:
Yet another difference not relevant to this scenario:
Lastly, one relevant difference:
Trying to remember two solutions to the same problem (in this case, networking) is always challenging. I am going to guess that I will often come back to this article to remind myself of them.
Bursts of code to power through your day.
69 
69 claps
69 
Written by
Broad infrastructure, development, and soft-skill background
Bursts of code to power through your day. Web Development articles, tutorials, and news.
Written by
Broad infrastructure, development, and soft-skill background
Bursts of code to power through your day. Web Development articles, tutorials, and news.
"
https://towardsdatascience.com/how-to-deploy-docker-containers-to-the-cloud-b4d89b2c6c31?source=search_post---------275,"Sign in
There are currently no responses for this story.
Be the first to respond.
James Briggs
Sep 18, 2020·8 min read
Docker containers are brilliant little things. They are essentially self-contained applications that can run across any OS.
Imagine you have a Python application, you bundle it, along with everything you need to run it into a Docker container — that container can now…
"
https://medium.com/hackernoon/hosting-a-free-static-website-on-google-cloud-storage-d0d83704173b?source=search_post---------276,"There are currently no responses for this story.
Be the first to respond.
This guide walks you through setting up a free bucket to serve a static website through a custom domain name using Google Cloud Platform services.
Sign in to Google Cloud Platform, navigate to Cloud DNS service and create a new public DNS zone:
By default it will have a NS (Nameserver) and a SOA (Start of Authority) records:
Go to you domain registrar, in my case I purchased a domain name from GoDaddy (super cheap). Add the nameserver names that were listed in your NS record:
PS: It can take some time for the changes on GoDaddy to propagate through to Google Cloud DNS.
Next, verify you own the domain name using the Open Search Console. Many methods are available (HTML Meta data, Google Analytics, etc). The easiest one is DNS verification through a TXT record:
Add the TXT record to your DNS zone created earlier:
DNS changes might take some time to propagate:
Once you have verified domain, you can create a bucket with Cloud Storage under the verified domain name. The storage class should be “Multi-Regional” (geo redundant bucket, in case of outage) :
Copy the website static files to the bucket using the following command:
gsutil rsync -R . gs://www.serverlessmovies.com/
After the upload completes, your static files should be available on the bucket as follows:
Next, make the files publicly accessible by adding allUsers entity with Object Viewer role to the bucket permissions:
Once shared publicly, a link icon appears for each object in the public access column. You can click on this icon to get the URL for the object:
Verify that content is served from the bucket by requesting the index.html link in you browser:
Next, set the main page to be index.html from “Edit website configuration” section:
Now, we need to map our domain name with the bucket we created earlier. Create a CNAME record that points to c.storage.googleapis.com:
Point your browser to your domain name, your website should be served:
While our solution works like a charm, we can access our content through HTTP only (Google Cloud Storage only supports HTTP when using it through a CNAME record). In the next post, we will serve our content through a custom domain over SSL using a Content Delivery Network (CDN).
Drop your comments, feedback, or suggestions below — or connect with me directly on Twitter @mlabouardy.
#BlackLivesMatter
192 
3
192 claps
192 
3
Written by
CTO & Co-Founder @Crew.work — Maker & Indie Entrepreneur @Komiser.io, Writer/Author and Speaker — Blog: https://labouardy.com
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
CTO & Co-Founder @Crew.work — Maker & Indie Entrepreneur @Komiser.io, Writer/Author and Speaker — Blog: https://labouardy.com
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
"
https://rominirani.com/you-have-700-gcp-credit-now-what-9a0defd8f6b9?source=search_post---------277,"Several folks have approached me that they received Google Cloud Platform credits during Google IO 2017 or in the last few months and they are not sure how to use the credit. This blog post is a direct result of that, where I thought of putting together 10 ways in which you could start utilizing that credit and learn more about Google Cloud Platform.
Note: Make sure that you have redeemed your GCP credits via the link that was given to you and that you have created a Google Cloud Platform Billing Account with a credit card associated with it. Here is a link on how you do that.
So what are things that you can do to put your credit to good use (in no order of importance):
If you are a new to the platform, the first question on your mind is where to begin? It is not an easy path to navigate and your point is well taken. But there is good help on the way in the form of Google Cloud Courses that are available on MOOC platforms like Coursera. I suggest that you start with those and the first course that you should do is Google Cloud Platform Fundamentals, which should take about 6–10 hours to complete.
Multiple courses on Google Cloud, including this basic one have just got started on Coursera and you should join one. Check out my post on the courses available to take today.
Before you jump on to beginning a course, I suggest you watch a video that helps you navigate the Google Cloud Platform. This is important and will hold you in good stead for the points that come up next.
Always wanted to learn Linux but have not got around to learning it? Or just want to practice a few Linux commands? Or better still, you need a Virtual Machine to try out a few things?
Well , Google Compute Engine can come to your rescue. It is the core piece of the Infrastructure as a Service (IaaS) that provides scalable and high-performance virtual machines.
Hey, what if you just want a Windows machine in the cloud? Sure — Google Compute Engine provides that too!
Check out these quick starts on both Linux and Windows (pick your choice .. both are good!):
One of the coolest features (tools) that is available to you as you get more familiar with the Google Cloud Platform is your own Linux Developer Machine in the cloud that you can use to manage all your Google Cloud Platform Resources. It is called the Google Cloud Shell and it sure is available to you. Check it out.
You must have heard of how containers are revolutionizing the way to build, ship and run our applications. You have definitely heard about Docker but are still itching to try out some commands to see if it is working. Or you are already familiar with Docker and have now started to understand that you need to have Container Orchestration software like Kubernetes to let you define your application state and let Kubernetes take care of managing the fleet of machines to help serve your application (Distributed Systems Challenges — not an easy problem to solve but Kubernetes does it!).
If any of the above sounds interesting, you should get started and here are some pointers:
Who does not like code labs? These are fantastic hands-on tutorials that have been crafted by Google to introduce you to multiple areas of Google Cloud Platform. They range from 10 minutes to 120/150 minutes and you can take them from a place and at a time of your choice.
The Code Labs for Google Cloud are available here:
Google Codelabscodelabs.developers.google.com
Try a few ones out and bookmark this for sure. Anytime you want to learn more about a service available in GCP, come back here.
Access them anytime at g.co/codelabs
Developers are getting excited about Serverless Architectures or Functions as a Service (FaaS). They are a new way in which you can architect your applications using functions, events and fully-managed cloud services at economies of scale and cost, that is fundamentally different from before.
Amazon Web Services with its AWS Lambda offering took a lead and has gained significant mind share, but did you know that GCP supports this via the Google Cloud Functions service?
The premise is simple on the surface. You can write your functions in JavaScript and then string them together in a process driven primarily via asynchronous events like a file getting uploaded to Google Cloud Storage, or triggering on demand, etc.
Currently, Google Cloud Functions supports JavaScript language only and you can get started with it via a couple of tutorials that I would suggest:
If you are a mobile developer, chances are that you have heard about Firebase. Firebase is now a complete platform and gone are the days when you only looked at it as a real-time database in the cloud. It provides a platform now for not just developing & testing your mobile application but also for growing and engaging your audience.
For those of you who would like an introduction to Firebase and understand the whole picture, take a look at this video first:
But what is key here is to understand that Firebase is now closely integrated with multiple Google Cloud Platform services. What this means is that since you have the GCP credits, start using them in conjunction with your Firebase driven application to take it to the next level.
You can use Google Cloud Storage aka GCS (this is mentioned in a later part of this article) in conjunction with Firebase. Store and serve large object content (attachments, audio, video, etc) from GCS in your Firebase App. We talked about Google Cloud Functions in the previous section. Guess what, Firebase Cloud functions is the perfect integration of Serverless functions in conjunction with events that you would expect from your Firebase database. For e.g. Send an email (written as a function) if a new record is added to the subscriber database.
Look at all the use cases that Firebase Cloud Functions can enable:
firebase.google.com
Check out this video too on building modern applications with Google Cloud Platform and Firebase:
The world loves data and lots of it. But making sense out of data is the tough but extremely rewarding part. If you have been working with Big Data, the challenge is in handling large amounts of data in batch/stream mode, analyzing them, transforming it and more. Doing this at scale while running the complex infrastructure and software is not for everyone.
This is where Google Cloud Platform really shines with a fully-managed set of services in the Big Data space. Do you use Hadoop and Spark in your projects? If yes, try out Cloud Dataproc that spins up a Hadoop cluster in seconds, allowing you to run your Spark jobs and even tearing down the cluster once you are done with it. Magical? Try out this codelab:
codelabs.developers.google.com
Or better still, check out Graham Polley’s article on Google Cloud Dataproc and the 17 minute train challenge:
shinesolutions.com
Heard about BigQuery? If not, you must take a look at it. Consider it as your Analytics Warehouse in the Cloud (again fully managed) where you simply load your data and perform SQL queries on it that run magically in seconds across GBs and TBs of data. Behind the scenes, the queries are run for you across clusters but you don’t have to worry about that. You ask questions of your Big Data and you get the answers.
Don’t believe me? Check out this video on what BigQuery is about?
Google also makes available large datasets for you to simply query. Check these out:
cloud.google.com
Machine Learning is all over the place and it is only a matter of time before it finds it way through most applications. But where do you start? You must have heard about the tons of mathematics that one needs to know to get started with it. The fact of the matter is that Google Cloud Platform provides a spectrum ranging from just using the Machine Learning APIs and then moving on to hosting your own Machine Learning models in the cloud.
I suggest that you first start off with using existing Machine Learning APIs that provides powerful ammunition to anyone looking at addressing challenges that could involve:
These APIs provide you a dead simple REST API to embed Machine Learning in your applications. Make no mistake that these REST APIs are front-ending powerful Machine Learning models that have been built over years with tons of training data thrown at them. The best part is that it is going to get better and more accurate over time, with improvements in terms of accuracy and latency being a given.
If you are still on the fence, I suggest that you give this Cloud Vision API page a try. Go to Try the API section on the page. You will be impressed with the capability and then imagine if you had to build something like that. Instead, I suggest to stand on the shoulders of these great APIs and build the next Wow! application.
Get started with these videos to see the Machine Learning APIs in action:
Say you want to run a Wordpress on Google Cloud or a MongoDB on Google Cloud or another popular application? The Google Cloud Launcher is just that. It is the fastest way to get started with GCP and provides you production grade installations in just a few clicks.
Here are some popular solutions:
At the time of writing, there are 200+ solutions in the Google Cloud Marketplace that you can deploy with a few clicks. It includes Databases, Blog & CMS, Developer Tools, Operating Systems and more.
Google Cloud Storage provides multiple data storage options depending on your needs. It provides the following:
Pick your choice of storage based on the requirements that you might have. For example, if you need a relational database for your hobby project, go ahead and spin up a Cloud SQL instance.
Choosing a storage option is not an easy task. Google Cloud makes available a guide to help you make the decision. Definitely take a look, even if you don’t end up using this service for a refresher on storage options. Here is a great flowchart from the official documentation:
App Engine in my book was the original PaaS that led to a huge number of applications being written on the platform. The standard run time version of App Engine supports Java, Python, Go and PHP.
If you are looking to write an application and let Google (App Engine) do the hard work of provisioning the app for you and running it for you, give App Engine a serious look. Like all PaaS , there are certain restrictions in the way that you write your code, libraries that you can use,etc. but if you are fine with that adjustment, you can huge scale and a serverless way of running your application. And best of all, it is hosted on the Google network that spans the globe and is the best in class.
Want to know how easy it is to deploy a App Engine application to the cloud? Check out this video:
Another talk that is very important is to understand what kind of application is best suited for the multiple options that are there on Google Cloud Platform i.e. Google Compute Engine, Google App Engine and Google Container Engine. This talk explains it well:
Web APIs changed the way applications talk to each other. It also allowed API only companies to thrive and chances are that you have definitely used some API or the other in your application.
But what about writing your own API and releasing it for your other applications (think mobile applications) or even the world? If you have an interesting idea for an API, I suggest that you go for it. Google Cloud Endpoints provides a great way for you to start writing and deploying your API for the world to use.
Google Cloud Endpoints provides you the ability to use their standard framework to write APIs or even using frameworks like Swagger. They are then hosted on Google infrastructure with logging and security all handled for you. Check out the Cloud Endpoints Quick Start that provides you multiple options of writing your APIs hosted on App Engine, Compute Engine, running in Docker or even in Kubernetes.
cloud.google.com
If you believe that your idea for a utility API is too small, take inspiration from this article where a developer identified a need, built out an API and now sees over 250 million API requests for day:
blog.ipinfo.io
To scale to that level, you will need to go with a platform that can scale and Google Cloud Platform is definitely one of those.
Remember that Google Cloud Platform comes with a generous free tier for most of its services, which means that if you are well within that tier in terms of resource usage, you will not be charged for it or in other words, you credit will remain intact. But my whole purpose here is to get you started with some services and eventually you end up liking it so much, that you move up the chain in terms of your usage and that’s when your credits will come in real handy.
If you are from the AWS or Azure world, one of the things that you absolutely need to help you jumpstart things on the Google Cloud Platform is a guide that can tell you what equivalent services are there on GCP. Check these guide out:
cloud.google.com
cloud.google.com
The above 10 suggestions form a short list of much more that you can do with Google Cloud Platform. In my experience, $700 in Cloud Credits is an extremely generous amount to learn and familiarize with various services available with Google Cloud Platform. As one of the leading Cloud Platforms available today, it pays to utilize that credit. Start now … you still have nearly 6 months to go for 2017 :-)
Technical Tutorials, APIs, Cloud, Books and more.
266 
4
266 claps
266 
4
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
Technical Tutorials, APIs, Cloud, Books and more.
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
Technical Tutorials, APIs, Cloud, Books and more.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://rominirani.com/build-a-serverless-online-quiz-with-google-cloud-functions-and-cloud-firestore-1e3fbf84a7d8?source=search_post---------278,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
This post will take a look at how you can build a Serverless online quiz using a couple of services on the Google Cloud Platform, namely Cloud Functions and Cloud Firestore.
If you are new to Google Cloud Functions, I suggest that you take a look at my tutorial series on Google Cloud Functions. It gets you up-to speed on the basics, types of functions and there is a Github repository to support it too.
Cloud Firestore, as the documentation states, is a NoSQL document database built for automatic scaling, high performance, and ease of application development. It is currently in Beta and I decided to give it a go and demonstrate how we could power some of our functionality wrapped inside Google Cloud Functions via a persistence layer in Cloud Firestore.
Note that I could have used Firebase Functions too, but I prefer to show this via the plain vanilla Cloud Functions in Google Cloud Platform.
Before we jump into how we build the online quiz, it is nice to see it in action. Since we are powering our quiz via Cloud Functions, all we will have is a HTTP URL to launch in the browser that will trigger the Cloud Function.
Assuming that you will visit that URL, you will get a screen that looks like this in your browser. The quiz that we have created is “Countries and their Capitals”. A sample screen is shown below:
You can select one of the options as the answer and click on submit. It will provide you with the results as shown below, along with an option to continue taking the quiz.
That is all there is to it but you get the picture. All of the above will be driven via Cloud Functions and the set of quiz questions, along with the 4 options and answer will be there in Cloud Firestore. No servers to run for you at all.
Let’s get started.
Here are a few steps to complete to begin our journey:
If you would like to see detailed screenshots on how to do the above, please follow my tutorial on Setting up a Local Environment for Google Cloud Functions development, which is part of my Google Cloud Functions tutorial series.
Now that we have the setup in place, let us move to the first part of our Online Quiz application, a place to hold our questions and answers or in other words, build our bank of questions and understanding how we can invoke it via any other application.
Firestore is a NoSQL document database built for automatic scaling, high performance, and ease of application development.
Having said that, the best way of think about it is a document store. Think of a document store as a collection of different kinds of documents or types. Examples of kinds or types can be a questions that will hold a list of question or employees, which will hold a list of employees.
So there are 3 things to keep in mind, if I have to simplify : Collection, Document and Attributes (Properties). So a quiz can be modelled as follows:
Now that we have got that out of our way, let us go setup Firestore for our project.
The first step is to visit the Firebase Console and sign-in with the same account. Once signed in, click on Add Project as shown below:
This will bring up a screen, where you should select the same GCP Project that you created in the earlier section and then click on ADD FIREBASE button as shown below:
This completes your step to add Firebase database to your GCP Project. It will lead you to the Firebase console as shown below:
But we are not done yet since you have both Firebase and Firestore and by default, this project adds Firebase to your project, whereas we need Firestore.
To do that, click on the Database link, once you are logged in. You will find that in the Develop → Database section in the left menu, as shown below:
This will show you that Firebase database is selected. What we need to do is select the Firestore database. Click on the Database dropdown list at the top and select Cloud Firestore BETA as shown below:
Wait for the initializations to get done. At the end , you should see the Cloud Firestore BETA selected for your project as shown below:
Now that we have our database selected, it is time to create our Collections as described earlier.
The Cloud Firestore documentation is excellent and I suggest to read that, but for now, our minimal understanding from the earlier section will be enough.
To re-iterate, our collection will be named questions. This collection will contain documents, where each document will contain the question details (Attributes/properties) that we explained earlier.
We will create our collection and add a few sample questions as given below. This can be done via the client libraries too, but we will keep with the web console for now.
Go ahead and click on ADD COLLECTION as shown below:
This will bring up a form where you can give a name for your collection. In our case we will call it questions.
Click on NEXT. This step will give us the opportunity to also add our first record i.e. document.
As you can see, we can give a unique id to help identify our document within the collection. We can either go with the AUTO-ID presented over here or we can click on that and give our own generated id. In my case, I will go with unique ids that are in sequential order like 1,2,3… and so on.
Once you provide your id, you can then start creating the fields. Remember that each question i.e. document of ours has the following fields:
Here is a sample screen that shows the 2 questions (documents) that I created in the collection with the fields for the second question shown:
Go ahead and create as many questions as you would like. For the purpose of our demo, 2 or 3 questions in total will suffice.
Great ! We now have our Firestore collection ready for serving to applications. We shall be using the Node.js Firebase npm modules to access them from our Cloud Functions as demonstrated in the next section.
Let’s move now to the final puzzle in our project i.e. write Google Cloud Functions to serve a random question to the user and check the answer. In short, we shall be writing two functions:
The names of the functions are self-explanatory.
This Cloud Function will get a random question from the Firestore collection i.e. questions. It will serve a HTML form with the question and the 4 options. The form will then on submit action trigger the checkanswer function.
This function will check the answer and inform the participant if the answer is correct or not. It will also put in a link to the getRandomQuestion again, so that the participant can continue to take more quiz questions.
On your local machine, create a folder in which our files will result. Inside that folder, create a index.js file with the code shown below:
Here are a few comments on the code above:
The package.json file is shown below. Place it in the same folder as the index.js file.
To deploy the function, launch a terminal session and go to the folder in which the index.js and package.json files reside. Assuming that you installed the local gcloud tools and the beta dependencies, you can deploy the two functions via the two commands as shown below:
Ensure that both the functions are deployed correctly via the command shown below:
Since both of these functions are HTTP Trigger based functions, we can get their HTTP endpoint via the describe command as shown below. Remember that we just need the HTTP Trigger Endpoint url for the getRandomQuestion function . The property httpsTrigger is important.
Copy the value of the url and you are all set to invoke that from the browser and see it in Action as given below:
Enjoy your own Quiz ! :-)
You can monitor your Cloud Functions via Stackdriver Logging. It is integrated into both the Google Cloud Console and also Firebase console.
From Google Cloud Console, you can visit Stackdriver Logging and can see the Cloud Functions execution as shown below:
Alternately, if you prefer the Stackdriver logging to be visible via the Firebase console, go to Functions option in the main menu as shown below:
You can then click on the Logs tab, to see the Functions execution and any log statements for your function execution.
This concludes our tutorial on building out a serverless online quiz application via Cloud Functions and Cloud Firestore. There are definitely improvements that we can make to this and here are a few suggestions to work on:
Hope this gives you a good idea on how to use Cloud Functions along with Firestore. Thank you for reading this tutorial. Hope you enjoyed it. If you have any feedback, please let me know.
Technical Tutorials, APIs, Cloud, Books and more.
89 
1
89 claps
89 
1
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
Technical Tutorials, APIs, Cloud, Books and more.
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
Technical Tutorials, APIs, Cloud, Books and more.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@jaychapel/4-ways-to-get-google-cloud-credits-c4b7256ff862?source=search_post---------279,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Feb 24, 2020·4 min read
Google Cloud credits are an incentive offered by Google that help you get started on Google’s Cloud Platform for free. Like Amazon and Microsoft, Google is trying to make it easy and in some cases free to get started using their Cloud Platform or certain services on their platform that they believe are “sticky” — which is beneficial if you’d like to try the services out for personal use or for a proof-of-concept. There is both a spend and a time limit for Google’s free credits, but then they also offer “always free” products that do not count against the free credit and can be used forever, or until Google decides to pull the plug, with usage limits.
The most basic way to use Google Cloud products is the Google Cloud Free Tier. This extended free trial gives you access to free cloud resources so you can learn about Google Cloud services by trying them on your own.
The Google Cloud Free Tier has two parts:
The Google Cloud 12-month free trial and $300 credit is for new customers/trialers. Be sure to check through the full list of eligibility requirements on Google’s website. (No cryptomining — sorry!)
Before you start spinning up machines, be sure to note the following limitations:
Your free trial ends when 12 months have elapsed since you signed up and/or you have spent your $300 in Google Cloud credit. When you use resources covered by Always Free during your free trial period, those resources are not charged against your free trial credit.
At the end of the Free Trial you either begin paying or you lose your services and data, it’s pretty black and white, and you can upgrade at any time during your Free Trial with any remaining credits being applied against your bill.
The Always Free program is essentially the “next step” of free usage after a trial. These offerings provide limited access to many Google Cloud resources. The resources are usually provided at monthly intervals, and they are not credits — they do not accumulate or roll over from one interval to the next, it’s use it or lose it. The Always Free is a regular part of your Google Cloud account, unlike the Free Trial.
Not all Google Cloud services offer resources as part of Always Free program. For a full list of the services and usage limits please see here — a few of the more popular services include Compute Engine, Cloud Storage, Cloud Functions, Google Kubernetes Engine (GKE), Big Query and more. Be sure to check the usage limits before spinning up resources, as usage above the Always Free tier will be billed at standard rates.
Google is motivated to get startups to build their infrastructure on Google Cloud while they’re still early stage, to gain long-term customers. If you work for an early-stage startup, reach out to your accelerator, incubator, or VC about Google Cloud credit. You can get up too $100,000 in credit — but it will come at the price of a large percentage of equity.
Options that don’t require you to give up equity include Founder Friendly Labs, StartX if you happen.
Google offers several options for students, teachers, and researchers to get up and running with Google Cloud.
There are also several offerings related to making education accessible without associated credits. See more on the Google Cloud Education page.
Various vendors that are Google Cloud partners run occasional promotions, typically in the form of a credit greater than $300 for the Google Cloud Free Trial, although we’ve also seen straight credits offered. For example, CloudFlare offers a credit program for app developers.
Also check out events that might offer credit — for example, TechStars startup weekends offers $3,000 in Google Cloud credits for attendees. Smaller awards of a few hundred dollars can be found through meetups and other events.
Google Cloud Credits do offer people and companies a way to get started quickly, and the Always Free program is a unique way to entice users to try different services at no cost, albeit in a limited way. Be sure to check out the limitations before you get started, and have fun!
Further reading:
Originally published at www.parkmycloud.com on February 18, 2020.
CEO of ParkMyCloud
133 
1
133 
133 
1
CEO of ParkMyCloud
"
https://medium.com/google-cloud/how-i-containerised-my-resume-api-91bb2d2f92f4?source=search_post---------280,"There are currently no responses for this story.
Be the first to respond.
…or (This) Idiot’s Guide to running Docker and Kubernetes on Google Cloud Platform
As I am currently looking for interesting new opportunities and am particularly interested in APIs, I created an API for my own CV/resume a few week ago, and wrote about it here: https://medium.com/@mariomenti/everything-is-an-api-including-me-and-my-cv-674ea433f283
Since then, I’ve spent some time looking into microservices and containers (namely Kubernetes and Docker), because I felt I wanted to get more familiar with some of these concepts. As ever hands-on even if I barely know what I’m doing, I thought it would be fun to describe how I moved my monolithic API into separate microservices, all running on Kubernetes on the Google Cloud Platform.
I was totally new to the oncept of containers when I started writing this, so this will hopefully serve as a useful idiot’s guide / starting point for people in a similar situation — but at the same time, I’ve almost certainly done things in a less than optimal (if not outright ugly or even wrong) way, so if any experts read this and feel sick, please comment and let me know where I’m barking up the wrong tree!
Why would I do this? Good question — in my particular case there probably isn’t that much reason, since my CV API is hardly groaning under the load (if it was, I wouldn’t have time to write articles like this!). But imagine this was a production-grade API: at the moment, everything, each endpoint of the API as well as the code that loads the resume data and API tokens, is all contained within one Go program. If there are changes needed to one endpoint, it means recompiling and redeploying the entire thing. And, say, there was one API endpoint (for example the “/contact” endpoint to send me an email or SMS message) that suddenly gets hugely popular and the API starts to struggle, in order to scale in the existing setup, I would have to increase the capacity of the entire Compute Engine instance I’m running the API on. By contrast, in a containerised world, because each API endpoint runs as its own microservice, I will be able to scale up the “/contact” endpoint independently, while leaving every other aspect of the API unchanged. So you suddenly have much more fine-grained control over the different parts of your application. Similarly, if an update is required to one API endpoint, it can be deployed independently of the rest of the API, without any downtime. In addition, Kubernetes is self-healing, so if the so-called pods that run an application go down, Kubernetes automatically replaces them.
So, without further ado, the pre-requisites (I’m working on a Ubuntu Linux laptop, and details may vary depending on the OS you are on, so I’m linking to fairly generic documents):
Good? Good. The first thing to do is to create a Kubernetes cluster to run our applications. I’m going with the default here, which creates a 3-machine cluster:
This will take a couple of minutes — once the cluster has been created, authenticate the Google Cloud SDK using your Google account by typing:
Let’s take a step back now and look at my existing API setup, and how we want to transform it into a collection of microservices. It’s very simple, and contains these parts:
All this is contained in the one Go program file. So to pick this apart, what I’m trying to do is to create these separate microservices:
Let’s look at the resume-server first. This is about a simple (and unrealistic) as it gets — a “real” API would of course have a datastore/database of some kind in place, but for the purposes of this demo, I’m just reading my resume/CV data from a JSON file.
We next need to create a docker container for this, so we can then upload this to Kubernetes. Again, the Dockerfile is simplicity itself:
… the COPY line makes sure we copy the resume.json file to the container, so the pod application can read it. Build the docker container like so:
You can get the value you should use in place of <YOUR-GOOGLE-CLOUD-PROJECT> by typing…
Whenever you see <YOUR-GOOGLE-CLOUD-PROJECT> in any examples here, replace with your actual Google Cloud project name. Once the docker container has been built, we need to push it to Google Cloud:
Next, we want to deploy the application to Kubernetes. To do this, we create a deployment.yaml file for the app:
In this example, we want to always run 2 instances of the resumeserver, but if we wanted to be more fault-tolerant, we could specify a higher number of replicas. As I mentioned above, Kubernetes will make sure that at least X are always running, and restart them as required.
Deploy the application:
This creates 2 running pods in Kubernetes, and you can see details via these commands — the first lists the deployments on your cluster, the second the pods (as we specified, there are 2 pods running for our resumeserver deployment).
So far so good, but in order to be able to communicate with these running pods, we need to create a named service. To do this, we use this service.yaml definition:
This defines a service called “resumeserver”, and it includes all pods that are running with the name “resumeserver-pods” (we specified this in the deployment.yaml file above). To create the service, run…
We can now check and see if our service is running:
You can see resumeserver is there, and has been allocated an internal IP address. The nice thing is that Kubernetes has a built in DNS service (more below), so from now on, when we want to connect to it from a different pod within our cluster, we’ll be able to refer to our resumeserver service by name.
OK, we now have the resume-server part up and running, and setting up the token-server is very similar, so I won’t go into details here, but all the files are on Github. Essentially, for each service we want to create, we follow these steps:
So after we created the tokenserver, following the above steps, we see the resumeserver and tokenserver both running on our cluster:
Next, let’s create one of the API endpoints. The most simple is the “/full” endpoint, which essentially just returns the entire resume/CV in JSON format. Some of the other endpoints are slightly more complex (but not much, this is a very simple demo :))
If you look at full.go, you can see that we refer to the tokenserver and resumeserver:
Because we created the services called “resumeserver” and “tokenserver”, we can now access them using their names (e.g. “http://resumeserver”) — Kubernetes’ internal DNS service takes care of that. Note this is internal only, these pods are not accessible from anywhere in the outside world (and neither do we want them to be), only from other pods within our cluster. In this demo we access them via HTTP, but if they were (say) redis or mySql servers instead of http servers, it would work in pretty much the same way.
To create the full-endpoint service, we go through the same steps again:
Once we’ve done this, we can see the new endpoint (full-endpoint) in the list of services:
The detailed code of each API endpoint is slightly different (you can see all the files in Github), but we go through the exact same process for each endpoint, until we end up with our cluster containing a service for each API endpoint:
We can also check on our deployments and pods (after the resumeserver service, I just specified 1 replica per service, but this could of course be scaled as appropriate):
Psssst… wanna know a secret? Before we go to the final part (actually exposing all these services to the outside world), let’s take a look at the “/contact” endpoint. This one is more interesting than the others (which TBH doesn’t say much) because it contains some code that sends SMS messages via Twilio, and emails via SendGrid. Not *that* interesting you say — but the interesting part here is figuring out how we best pass “secret” information like API keys to a container on Kubernetes, with the least risk of exposing this confidential data. Luckily Kubernetes has a concept of a Secret, which makes this very easy and secure.
To pass secret information to a Kubernets pod/deployment, we first create a Secret that contains our confidential API information. Here’s my secrets.yaml:
Then we create the Secret by running:
(and obviously don’t make the secrets.yaml file public in any way, e.g. by checking into source control or similar…)
Now, when we create the deployment for our contact-endpoint service, we can refer to our Secret we named “contact-secret”, and pass on these values as environment variables:
And in the contacts.go program in the pod we just deployed, we can easily pick up these environment values:
This is pretty neat, right? ;)
OK, so now to the final part — we have all these services running, but nothing can access them. So what we need is something that is exposed to the outside world, and that can handle our API requests and pass them onto the relevant service to be processed. Enter nginx (of course).
Again, this is simple in the extreme — we just create a custom nginx.conf, and then create a docker container that uses it. The contents of nginx.conf look like this:
On the first line, we specify the DNS resolver we want to use — this may be a different value in your case, to get the value that you should use, run:
The rest is pretty self-explanatory — each service is known by its service name, so we simply proxy requests to different locations to different services.
The Dockerfile to create the container:
Let’s build and push our container:
Next we create the deployment (this is again pretty much identical to the above services, I think you’re starting to see a pattern), and then create the service. This is where you see one difference to the services we previously created:
Unlike in previous service definitions, here we specify the type as “LoadBalancer”. This will give the service an external IP address. So after running “kubectl create -f service.yaml”, you should see something along these lines:
(It may take some time for the external IP address to appear, if it says “<pending>”, just re-run the command until you can see the external IP address.)
So now that our nginx service has an external IP address, we are ready to make some requests to the API… for example, call the /tags/golang endpoint:
POST to the /contact endpoint to send me an email:
And that’s it! (I think.) Our API is now fully containerised, yay!
Once everything is running, and you want to scale up or down a particular microservice, it’s as easy as running the kubectl scale command:
This will create 5 replicas of the pod running the /contact API endpoint. Scaling down is the same, just replace 5 with a lower number (if you specify replicas=0, there will be no pods left running the app).
You can even auto-scale a deployment depending on CPU demand:
To update a service, without any downtime at all, simply make changes to your program, content and/or Dockerfile, then go through the steps of building the docker container, push it to Google Cloud, and apply the deployment.yaml with the new container (no need to do anything to the service). This will replace the running pods in the deployment with the new versions.
If you screw up when updating a deploy, you can quickly roll back to the previous version of a given deployment:
There’s of course plenty more stuff you can do with Kubernetes deployments, and I barely scratched the surface — if you want to know more, see the docs here: https://kubernetes.io/docs/concepts/workloads/controllers/deployment/.
Ouch, this has turned into a bit of a monster of a post — if you’re still with me, I hope it proved useful in giving a hands-on walkthrough of some of the concepts in Docker and Kubernetes. As I said before, I’m very new to this, so no doubt there’s some clangers in here — if you spot any, please let me know and I’ll update the post!
Google Cloud community articles and blogs
86 
1
86 claps
86 
1
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
inquisitive hacker. swiss-brit. runner. things I love: cats, japan, drums, vegan curries, noisy & mathy music. tech stuff: APIs, chat & voice interfaces
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/swlh/cloud-dataflow-a-unified-model-for-batch-and-streaming-data-processing-43f48b3f3c3d?source=search_post---------281,"There are currently no responses for this story.
Be the first to respond.
Dataflow is a fully managed service to execute pipelines within the Google Cloud Platform ecosystem. It is a service that is fully dedicated to transforming and enriching data in stream (real-time) and batch (historical) modes. It is a serverless approach where users can focus on programming instead of managing server clusters, can be integrated with Stackdriver…
"
https://towardsdatascience.com/troubleshooting-gcp-cuda-nvidia-docker-and-keeping-it-running-d5c8b34b6a4c?source=search_post---------282,"Sign in
There are currently no responses for this story.
Be the first to respond.
Thushan Ganegedara
Jan 15, 2018·7 min read
I had a Google Cloud Platform (GCP) instance which was all setup well and running fine a day ago, which was set up following my previous tutorial.
medium.com
Let me tell you a bit about the machine I had,
But, something really weird happened to me when I started my GCP instance and tried running the docker container yesterday, using,
Nope, it didn’t work as I wanted it to and took me inside the container. Instead, it gave me the following error
This is likely to have happened because my GCP instance bravely decided that it would be totally fine to go ahead and update everything itself and things will magically work out so much better! Well, I got news! doesn’t happen that way. So it would be much appreciated if GCP gives us a way to turn off automatic updates during initial setup. In fact there has been many reports about NVIDIA drivers going bananas (or missing) with other updates (evidence 1, 2, 3).
Alright, before getting into the details of troubleshooting, let me summarize what I’m going to do step by step.
First and foremost, before checking if libraries are properly installed, see of your machine physically sees the GPU by typing in,
This should give something like,
If not, this could be due to GPU being unplugged or just nudging out from the socket because you moved the machine or something.
The first thing to do is, NOT jump to conclusions and rigorously start typing sudo apt-get install <this-and-that> hoping for the best! In fact the best thing in such a situation (and mostly ignored by many) is to identify what is the issue. First let us see what we’ve got. Go ahead and type,
You should ideally get something like,
If you get something like,
This could be due to two reasons,
So for NVIDIA-SMI to work properly, you need few things properly setup. They are,
Let’s check if these are installed, try typing
You should get
Next try,
this should give
Note that, actual list is much longer. But if get quite a few hits for CUDA, things should be alright. But don’t get too comfy! I had encounters things didn’t work even with these installed.
Let’s do another check to see if NVIDIA kernel modules are properly loaded by,
Ideally you should see,
If you don’t see anything, that is problematic! This means that NVIDIA drivers haven’t been loaded properly.
If you get both these outputs as they are shown, but not the correct dmesg message, you have the things you need all in the machine. So it could be due to some simple mis-configuration of PATH variables. So open up the .bashrc file and append the following two lines to it.
Then exit the text editor and run,
Reboot the machine and see if things are working fine by trying out nvidia-smi again. (PS: Remember to try with sudo privileges)
If you are reading this part, you weren’t lucky as some of the folks out there. Alright, let’s go ahead and toil the soil! In my opinion, it is not worthwhile anymore to dig deeper and try to find the single grain of mistake in a bowl full of libraries. In fact, it would be much easier, if we remove the current corrupted libraries and install things correctly from the scratch.
First we need to figure out which goes along with which. By this I mean we need to make sure we download specific (and correct) versions of CUDA and NVIDIA drivers correct for the Graphic card you have. So let’s go ahead and find out what is the latest.
Go to,
www.nvidia.com
And type in the details about,
Here’s the results I got,
Now we’ll make sure we stick to these particular versions when doing the installation, to avoid any discrepancies.
First let us remove any existing CUDA/NVIDIA libraries with,
First get the CUDA toolkit .deb with,
Reboot the system and try,
and you should see the correct output as shown in figure 2 and figure 3.
I just realized the NVIDIA driver 387.xx which is shipped with CUDA 9.1 didn’t work for my NVIDIA Tesla P100. So I had to first uninstall that and install NVIDIA 384.xx. This could be different for you depending on the GPU card you have in your instance.
You know what is weird! I was totally fine working with CUDA 9.1 and NVIDIA 387.xx as seen in figure 1. But now, NVIDIA 387.xx wasn’t compatible with CUDA 9.1 any longer. I’m not sure why but hopefully can get to the bottom of that!
Let’s do this by,
Now, let us install NVIDIA 384.xx driver manually by,
You will need nvidia-modprobe for nvidia-docker. Now do a quick check on path variables to see if they are properly set
Now try nvidia-smi and you should see something similar to figure 1, meaning things are back to normal (hopefully!).
One more thing, don’t forget what started this whole ordeal in the first place. It was the automatic updates. Updates are important to keep your machine secure from outside threats and everything, but if it is going to break my machine every 5 seconds I update, NO THANK YOU! I’ll manually update it myself. So to do this, open the following file in a text editor,
and set
This should stop those “pesky” (however, important) automatic updates off. But remember to update your OS consistently because, you don’t want someone to hack into your machine.
So in this post, we discussed how to troubleshoot the GCP instance if you ever get into trouble of broken configuration, missing drivers, etc. And the process I recommend is,
Cheers!
ML @ Canva | Educator (Manning/Packt/DataCamp) | PhD. Youtube: @DeepLearningHero Twitter:@thush89, LinkedIN: thushan.ganegedara
117 
1
117 
117 
1
Your home for data science. A Medium publication sharing concepts, ideas and codes.
"
https://medium.com/firebasethailand/cloud-storage-for-firebase-%E0%B9%80%E0%B8%88%E0%B9%89%E0%B8%B2%E0%B8%82%E0%B8%AD%E0%B8%87%E0%B9%80%E0%B8%94%E0%B8%B5%E0%B8%A2%E0%B8%A7%E0%B8%81%E0%B8%B1%E0%B8%9A-firebase-storage-%E0%B9%84%E0%B8%87%E0%B8%88%E0%B8%B0%E0%B9%83%E0%B8%84%E0%B8%A3%E0%B8%AB%E0%B8%A5%E0%B8%B0-3d88171383bd?source=search_post---------283,"There are currently no responses for this story.
Be the first to respond.
จากงาน Google Cloud Next 2017 ที่ผ่านมา จะเห็นได้ว่าทีม Firebase และทีม Google Cloud Platform (GCP) ได้ทำงานใกล้ชิดกันมากขึ้น ดูจากชื่อบริการที่เปิดใหม่อย่าง Cloud Functions for Firebase ที่ดึงบริการ Cloud Functions ใน GCP มาใช้งานร่วมกับ Firebase หรือการเปลี่ยนชื่อ Firebase Storage เป็น Cloud Storage for Firebase นั่นทำให้ผมเชื่อว่าน่าจะมีบริการอื่นๆใน GCP มาร่วมกับ Firebase เพิ่มขึ้นอีกในอนาคตเป็นแน่ จับตาดูไว้เลย
ก่อนจะไปต่อใครยังไม่รู้จักบริการ Firebase Storage ก็แนะนำให้เข้าไปอ่านที่บทความนี้ก่อนนะครับ
developers.ascendcorp.com
ส่วนคนที่รู้จักกันแล้ว ก็มาดูกันว่า Cloud Storage for Firebase นั้นมันมีอะไร ที่ทำให้คุณนั้นต้องอยากใช้…เกาะสมุย(ไม่เกี่ยว)
ขอโฟกัสไปที่งาน Google Cloud Next ก่อน ที่ได้มีการเปิดตัว Storage Class ออกมา 4 แบบ ตามภาพด้านล่างนี้
โดยปกติเวลาเราจะเก็บไฟล์ไว้ที่ Firebase Storage ไฟล์จะถูกเก็บไว้ที่ US เท่านั้น แต่จากนี้เราจะสามารถเลือก Region และความถี่ในการเข้าถึงไฟล์ที่เราต้องการเก็บได้ แล้วมันดีอย่างไร มาดูรายละเอียดของแต่ละประเภทของ class กัน
เป็นการเก็บไฟล์ที่ region ต่างๆไว้ตั้งแต่ 2 region เป็นต้นไป แบบนี้เหมาะมากกับไฟล์ที่มีการเข้าถึงที่บ่อย เช่น ไฟล์วิดีโอ ภาพ เป็นต้น ซึ่งปัจจุบัน Multi-regional ก็มีอยู่ 3 region หลักตามภาพด้านล่าง
เป็นการเก็บไฟล์ไว้ภายใน region ที่เลือกเท่านั้น โดยไฟล์ที่เก็บก็มีการเข้าถึงบ่อยเช่นกัน เช่น ข้อมูลสำหรับทำ analytics หรือจะเป็นข้อมูลที่เราคิดว่าจะให้บริการในเฉพาะ region นั้น
เป็นการเก็บไฟล์ไว้ภายใน region ที่เราเลือกเท่านั้น โดยไฟล์ที่เก็บมีการเข้าถึงไม่บ่อยอย่างเดือนละครั้ง เช่น ข้อมูล backup
เป็นการเก็บไฟล์ไว้ภายใน region ที่เราเลือกเท่านั้น โดยไฟล์ที่เก็บมีการเข้าถึงน้อยมากอย่าง 3 เดือนครั้ง เช่นไฟล์ที่เป็น archive, ไฟล์ log ที่เกี่ยวข้องทางกฎหมาย หรือไฟล์พวกที่เป็น disaster recovery
คำถามคือ ประโยชน์ของการเลือกเก็บไฟล์ตาม class ต่างๆได้มันดีอย่างไร ลองดูภาพด้านล่างนี้
จากรูปให้เราโฟกัสไปที่ Price/GB-mo กับ Ops Fee จะเห็นว่าราคามันสวนทางกัน เช่น ถ้าเราเก็บไฟล์ที่ Multi-regional ค่าใช้จ่ายในเก็บจะสูงกว่าประเภทอื่น แต่ค่าใช้จ่ายในเรื่อง transaction การใช้งาน upload, download จะถูกกว่าแบบอื่น
ดังนั้นหากเราเลือก class เก็บที่เหมาะสมแล้ว เราก็จะบริหารค่าใช้จ่ายได้ดีมากขึ้นไปด้วยนั่นเอง ส่วนเรื่องของ Latency ทีม GCP เขาเคลมว่าแต่ละ class นั้นเท่ากันนะจ๊ะ
คราวนี้กลับมาโฟกัสที่ฝั่งของ Firebase บ้าง ณ วันนี้ที่ผมเขียนบทความอยู่นี้ หากใครใช้ Blaze Plan ของ Firebase อยู่ เมื่อเข้าไปเมนู Storage จะเห็นตุ่ม 3 ตุ่มด้านบนขวา กดตุ่มมาจะเจอเมนูที่ให้ เพิ่ม bucket ได้ ในส่วนนี้เราสามารถตั้งชื่อ bucket ได้ เลือกประเภทของ region, เลือกความถี่ในการเข้าถึงไฟล์ ซึ่งเมื่อเลือกเรียบร้อยเราก็จะได้ค่าของ storage class มาให้เห็นละ
เรื่องต่อมาก็คือการ import ไฟล์จาก Google Cloud Storage (GCS) มาใช้งานใน Firebase เรื่องนี้ผมว่าหลายคนคงเคยประสบปัญหานี้ เช่น เมื่อก่อนเราเก็บไฟล์ทั้งหมดไว้บน GCS วันดีคืนดี ทาง business ต้องการให้เอาไฟล์เหล่านี้มาแสดงบนมือถือ หรือทำงานร่วมกับ Firebase ถ้าเป็นคุณเจอโจทย์นี้เข้าไปจะทำไงดี ถ้าไฟล์มีน้อยก็คงไม่มีปัญหา อัพโหลดใหม่ก็ได้ แต่ถ้ามีเยอะ อันนี้ไม่ง่ายแน่เลย
แต่ไม่ต้องกังวลใจอีกต่อไป เพราะวันนี้ Cloud Storage for Firebase มีบริการดีๆมาช่วยเราเชื่อมต่อไฟล์จาก GCS มาใช้งานร่วมกับ Firebase ได้เพียงไม่กี่คลิกเท่านั้น
ตัวอย่างผมมี bucket ใน GCS ชื่อ jirawatee-gcs ซึ่งมีไฟล์ 1 ไฟล์ดังรูปด้านล่าง
จากนั้นเมื่อผมต้องการเชื่อม bucket ใน GCS เข้ากับ Firebase ก็แค่เข้าไปที่ Console จากนั้น เลือกเมนู Storage แล้วคลิกตุ่ม 3 ตุ่มขวามือ จากเห็นเมนู add bucket ให้กดตรงนั้นเบาๆ คราวนี้จะมีหน้าต่างมาให้ เลือก Import existing Google Cloud Storage buckets แล้วก็เลือก bucket ของเรา อย่างของผมก็คือ jirawatee-gcs
เมื่อ import มาแล้ว ทุกคนก็น่าจะเจอหน้าต่างแบบนี้ ให้ปฏิบัติตามขั้นตอนเพื่อที่จะ grant สิทธิ์ให้เราสามารถจัดการไฟล์จาก GCS ได้ ซึ่งทำครั้งเดียวในแต่ละ bucket นะครับ ขั้นตอนคือไปดาวน์โหลด Cloud Tools มา แล้วทำตาม step 1–6 จากลิงค์นี้ https://cloud.google.com/sdk/docs/ เมื่อเสร็จแล้วคุณจะอยู่ใน folder bin ให้ run คำสั่งในการ grant
ถ้าคุณทำสำเร็จ สิ่งแรกที่พิสูจน์ว่าสำเร็จได้คือ เมื่อคุณเข้าไปดูรูปใน bucket ท่ีมาจาก GCS จะต้องแสดงได้
และสำหรับโค้ดในการระบุ bucket นี่ก็ง่ายเลย แค่ระบุ bucket ที่ต้องการ ตอน getInstance แบบนี้
ยังไม่พอๆ ในเมื่อไฟล์ของคุณมาเชื่อมต่อกับ Firebase ได้แล้ว คุณสามารถที่จะปกป้องไฟล์ของคุณด้วย Firebase Security Rules ได้ด้วย เช่น
รายละเอียดเพิ่มเติมเรื่อง Firebase Secuiry Rules สำหรับ Storage ตามอ่านได้ที่นี่
developers.ascendcorp.com
เรื่องสุดท้ายก็คือการ integrate กับ Cloud Functions for Firebase นั่นเอง สำหรับเรื่องนี้เล่าให้ฟังเป็นน้ำจิ้มไปก่อนว่า เมื่อไฟล์ใน Storage มีการเปลี่ยนแปลง มันจะมี Listerner วิ่งไปบอก Cloud Functions for Firebase ให้ทำงานอะไรสักอย่างได้ เช่น resize ภาพ เป็นต้น ซึ่งจะขอเล่าในบทความ Cloud Functions for Firebase ต่อไป
สำหรับบทความนี้ ผู้เขียนได้บินข้ามน้ำข้ามทะเลมาเขียนถึงประเทศสหรัฐอเมริกา เลยทีเดียว เนื่องจากได้รับเชิญมางาน Android O-MG และด้วย time zone ที่ต่างกันมาก บอกตรงๆ นอนไม่หลับ เลยได้โอกาสเขียนพอดี ก็หวังว่าจะมีประโยชน์กับ Firebase Developers ที่ใช้ Firebase Storage อ๊ะ อ๊ะ ต้องเรียกว่า Cloud Storage for Firebase สิ ถึงจะถูก ตอนนี้ 06:40 ที่นี่ละ ขอตัวไปอาบน้ำ เตรียมตัวไปงานก่อน แล้วพบกันใหม่กับบทความถัดไปครับ
Let you learn and share your Firebase experiences with each…
38 
38 claps
38 
Written by
Technology Evangelist at LINE Thailand / Google Developer Expert - 🔥Firebase
Let you learn and share your Firebase experiences with each other.
Written by
Technology Evangelist at LINE Thailand / Google Developer Expert - 🔥Firebase
Let you learn and share your Firebase experiences with each other.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@nathenharvey/ok-google-95fa8617293f?source=search_post---------284,"Sign in
There are currently no responses for this story.
Be the first to respond.
Nathen Harvey
Sep 18, 2018·1 min read
I am beginning a new adventure this week as a Developer Advocate with Google Cloud Platform (GCP).
I have built a career based on learning from those around me and helping share ideas, stories, and ways of working with others. Together, we can create more humane workplaces where learning and collaboration help create better customer experiences.
I have spent the last decade in and around the DevOps community learning from and being influenced by those around me. I am a practitioner of Chef-style DevOps which defines DevOps as
A cultural and professional movement, focused on how we build and operate high velocity organizations, born from the experiences of its practitioners.
I’ll be working for Mandy Waite (@tekgrrl) and, once again, with my good friend, Seth Vargo (@sethvargo). Over the coming weeks, I’ll be meeting the rest of my new teammates and learning about our practices for helping and learning from you.
Are you using GCP? I would love to hear from you!
Get in touch with me on twitter @nathenharvey, DMs are open.
Developer Advocate at Google
56 
56 
56 
Developer Advocate at Google
"
https://medium.com/@stevenc81/self-hosted-ghost-blog-on-gcp-free-tier-d91cd38b3bc4?source=search_post---------285,"Sign in
There are currently no responses for this story.
Be the first to respond.
Steven Cheng
Sep 11, 2017·5 min read
This weekend I got bored and decided to give GCP (Google Cloud Platform) Free Tier a try. In my experience the best way to learn things is doing it, and having a good purpose. I’ve been reflecting my writng habit and relaized I probably don’t write enough to share all the learnings. So this is it, I’ve decided to host my own Ghost installation with GCP Free Tier. Why? Becuase I like free stuff, to me a meal taste better if it’s free!
Here are the offerings https://cloud.google.com/free/
Since Ghost remommended installtion on Ubuntu 16.04 LTS I figured it might be a good idea to follow that advice. So the first step is getting a free Google Compute Engine
Sure, let’s launch a free VM instance within these limitations.
[IMPORTANT] Please remember to increase the Boot disk size from 10 to 30. I mean, why not? It’s free! Another important thing to note is to avoid creating VM in us-east4 as it's in North Virginia, and NOT FREE. I learned this after the blog was launhed so I had to recreat another one. With the Docker installation I was able to get a new one up in 10 minutes.
Wait for it to finish until you see this
Now select Open in browser window and get rocking!
I opted in for Docker installation because it’s easier and I can’t go back to the days of installing all dependencies one by one to make sure things are running well.
DigitalOcean has a nice post about this already so I’m just going to power through tis part. These are the 4 commands absolutely needed.
Now run this $ sudo docker run -d -p 80:2368 --name ghost -e url=""https://<your blog domanin>"" -v ~/ghost:/var /lib/ghost/content ghost
Few important things
Alright, you may see it installed locally on port 80. Do a curl localhost on the host OS (VM instance) to see it.
Wait! Why am I seeing this Moved Permanently. Redirecting to https://localhost/
It’s because Ghost assumes your installation is on the domain name you provided. If you don’t have a domain name read off hand and is dying to see something happens. Remove -e parameter from the docker run command to install on localhost instead.
Are you still with me?
Okay, assuming you have a domain name already now is a good time to point your domain name to your Ghost installation. Go back to the Google Compute Engine to find out the External IP is easy, and you probably saw it too.
Yeah, the red box with the external IP is what you need. Now let’s do DNS. I will use my Namecheap console as an example.
The second red box for value should be the external IP you see from GCP. Wait 20 minutes (or longer) for it to propogate.
This looks so exciting! Oh how the hell am I going to add new posts so it isn’t a static page? I had this confusion as well. It turned out we have to use a client to connect to the Ghost installation to create the first account so you can start wring cool stuff and be awesome.
Download the client
If you are seeing this as a result. You are doing something right. Congratulations!
Enjoy blogging with Ghost!
Good one if are thinking about these kind of things. I think that deserve a sperate post on it’s own. I’m happy to do another write up on how to
If you have any questions in the meantime, hit me up on Twitter
Cheers!
Originally published at steven.news on September 10, 2017.
https://twitter.com/stevenc81
50 
3
50 claps
50 
3
https://twitter.com/stevenc81
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/restarting-cloud-dataflow-in-flight-9c688c49adfd?source=search_post---------286,"There are currently no responses for this story.
Be the first to respond.
I’m not easily impressed by magic… you get used to magic when working daily with the Google Cloud Platform. But today I was impressed by the Cloud Dataflow service and it’s ability to be update your job without stopping it.
Cloud Dataflow is Google’s managed execution engine for running data processing workflows written with Apache Beam. Our main raison to using the Beam model is the ability to write a pipeline once and run it either streaming or batch. But when you dive deeper in the model the real power lies in it’s ability to work with event time and different types of windows.
An example is the “Session Window”: with a single line in your pipeline you can create sessions from your data. The code above groups your data in sessions with gaps of 20 minutes.
With such a powerful model for defining windows you can imagine that it’s not trivial to update a running “streaming” dataflow, or even recover for failures. You just can’t pull the plug and restart… what do you do with all the in-flight data that is already acknowledged and not written yet?! Well it turns out that Cloud Dataflow support updates.
But today we used this mechanism to give a failing dataflow a kick so it could recover from a glitch due to a BigQuery table that wasn’t created in time. The dataflow had about 8 hours of data in it’s buffers that we didn’t want to loose. The events where acknowledged so they where gone from PubSub but didn’t get written to BigQuery either, they where in the dataflow pipeline.
Using the — update flag on the dataflow switches the provisioning to a special mode that makes sure that no data is lost. It stops the running dataflow, makes sure that the data that is already acknowledged and in the pipeline is persisted to disk (ex. sessions that are not complete) and re-attach the disks to the new dataflow and continue running.
Nothing like a real emergency with a good outcome to raise the confidence in the product. Only downside: the uptime counter is reset from 200+ days to 0. But that’s a small price to pay for not loosing any data.
cloud.google.com
Google Cloud community articles and blogs
38 
3
38 claps
38 
3
Written by
Google Developer Expert for Cloud
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Google Developer Expert for Cloud
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://blog.rittmananalytics.com/date-partitioning-and-table-clustering-in-google-bigquery-and-looker-pdts-2bab9ec3be19?source=search_post---------287,"Google BigQuery is a data warehousing-orientated “table-as-a-service” product from Google Cloud Platform that, like Oracle Exadata, optimizes for full-table scans rather than selective row access and stores data organized into columns, rather than rows, to align better with filtering and aggregation workloads associated with data warehousing workloads.
Read more at the new home of the MJR Analytics blog.
A Blog about Cloud Analytics and Data Management from the…
189 
1
189 claps
189 
1
Written by
CEO of Rittman Analytics, host of the Drill to Detail Podcast, ex-product manager and twice company founder.
A Blog about Cloud Analytics and Data Management from the team at Rittman Analytics
Written by
CEO of Rittman Analytics, host of the Drill to Detail Podcast, ex-product manager and twice company founder.
A Blog about Cloud Analytics and Data Management from the team at Rittman Analytics
"
https://medium.com/javarevisited/7-free-online-courses-to-crack-google-cloud-associate-cloud-engineer-ace-certification-exam-in-2cf0b297aed?source=search_post---------288,"There are currently no responses for this story.
Be the first to respond.
Hello guys, if you are aiming for the Google Cloud platform and preparing for GCP Cloud Engineer Associate certification, the first certification to get yourself going with the Google Cloud platform and looking for free online courses to start your preparation then you have come to the right place.
In the past, I have shared the best courses to learn Google Cloud as well as certification courses to pass cloud engineer, data engineer, and cloud architect certifications, and today, I am going to share free GCP Cloud engineer courses for beginners and experienced developers.
These courses are created by experts and people who have already gone through the ordeal of this prestigious google cloud certification and passed it.
They are also picked from sites like Udemy, Pluralsight, and Coursera, three of the best online learning platform for programmers and IT professionals. You can also join these courses to better prepare for this certification and pass it on the very first attempt.
Before I share these free courses with you, I must congratulate you on making a great decision by learning cloud computing and choosing Google Cloud Platform, and when it comes to learning GCP, what else can be better than a Google Certified Cloud engineer?
Google is one of the top three market leaders when it comes to cloud computing, and if you have a certification of excellence from them, then you are going to have the upper hand among your competitor candidates.
In this tutorial, I have shortlisted 7 free online courses that will help you prepare for your GCP Associate Cloud Engineer Exam within a limited span of time. Apart from this, all these courses are created by the market leaders; therefore, not even a single moment will be wasted.
By the way, if you are serious about passing this prestigious Google cloud certification on the very first attempt then I highly recommend you join the Google Associate Cloud Engineer: Get Certified 2021 course on Udemy.
udemy.com
It’s not free but it’s the best course to pass the Cloud engineer certification and created by experts like Dan Sullivan, the guy who wrote the Official Certification Guide for Google. You can also get in just $10 on Udemy sales which happens every now and then.
Here is the list of the best free online courses to prepare for Google Cloud Professional Associate Cloud Engineer certification. This certification tests your ability to work with the Google cloud console and command line. You should be able to provision resources, create servers, deploy apps, and monitor them.
This exam is similar to AWS Cloud Practitioner and Azure Fundamentals exam and should be your first step on Google cloud certification. After passing this exam, you can prepare for other advanced Google Cloud certifications like professional cloud architect, professional data engineer, and professional DevOps engineer.
Anyway, without wasting any more of your time, here are the free courses for Google Cloud Associate Cloud Engineer certification:
This is one of the best free Udemy course to learn Cloud Computing. If you are just starting with your preparation and don’t want to leave any stone unturned, then this course is going to be a base for you.
It will work as a foundation for you. Xavier Corbett created it, and it’s a 1-hour long video course. So far over 2 lakh, students have enrolled in this course, and it is rated as 4.4 stars out of 5.
In this course, you’ll learn about the followings:
Apart from this, the instructor will give you brief information about various cloud platforms. If you are looking for a foundation course, then this one’s for you.
Here is the link to join this free cloud computing course — Introduction to Cloud Computing
Once you are done with the foundation course, then you are all set to learn about the platform that you are preparing for. This course is created by Djanaji Musale and Google Cloud Platform Gurus!. So far over 40 thousand students have enrolled in this course, and it is among the top-rated courses on Udemy about GCP.
In this course, you’ll learn about the followings:
If you are looking for a course that can help you get enough information about your platform then this course is for you.
Here is the link to join this free GCP course — GCP — Google Cloud Platform Concepts
If you are out on the internet looking for a single course that can prepare you in every aspect, then this course is for you. It’s a long video course available on Pluralsight.com.
Though this is a paid platform, since it offers a trial period, you can complete this course in that period without paying a single amount to the platform.
The Google Cloud team itself created this course in partnership with Pluralsight; thus, quality won’t be a concern anymore. In this course, you will learn about the following concepts of Cloud computing and GCP:
Apart from these, the instructor will also teach you how to maintain the security of the cloud and its applications. This course is specifically designed for beginners, and that’s why illustrations and graphics are majorly used in the videos.
Here is the link to join this online course — Google Cloud Certified Associate Cloud Engineer
By the way, you would need a Pluralsight membership to join this course which costs around $29 per month or $299 per year (14% discount).
I highly recommend this subscription to all programmers as it provides instant access to more than 7000+ online courses to learn any tech skill. Alternatively, you can also use their 10-day-free-pass to watch this course for FREE.
pluralsight.pxf.io
Another course created by the Google Cloud team, but this one is more of a limited and short course than the one available on Pluralsight, and this is available on Coursera which is an entirely free platform.
In this course, the instructors have live illustrations of various implementations, and you’ll be assigned with some project that needs to be finished to move forward in the course.
Besides this, if you’ll complete the course on time, then you’ll be rewarded with a certificate of competition. So far in this course, over 32 thousand students have enrolled, and based on the rating of over 1500 students, it has 4.7 stars out of 5.
This course is specifically designed for your preparation for the GCP Associate Cloud Engineer exam. If you move as per the instructor, then it will take four weeks, but if you are running short on time, then you can attend all the videos in just 8 hours.
This course is divided into three modules:
If you are looking for a course that can quickly prepare you for the GCP Associate Cloud Engineer Exam, then this one’s for you. The next three courses are interlinked with each other, and it is not recommended to skip any of the next three courses. Pursue them in serial order.
Here is the link to join this free Coursera course — Preparing for the Google Cloud Associate Cloud Engineer Exam
By the way, If you are planning to join multiple Coursera courses or specializations then consider taking a Coursera Plus subscription which provides you unlimited access to their most popular courses, specialization, professional certificate, and guided projects. It cost around $399/year but it's completely worth your money as you get unlimited certificates.
coursera.com
So far, it is assumed that you have completed the courses mentioned above, now it’s time to prepare for the particular topics that you are going to appear for.
This course is also available on udemy, and Google Cloud Platform Gurus created it, so far over 72 thousand students have enrolled in this course. It has a rating of 4.3 out of 5 based on the reviews of 1400 students.
In this course, you will learn about tactics to prepare for your GCP Associate Cloud Engineer certification course. Though this course is focused on a different certification exam, all the topics covered in this course are directly related to your GCP Associate Cloud Engineer Exam.
Instructors have smartly used explanatory illustrations and graphics. Even if you are a beginner, understanding things won’t be a problem for you. In this 16-hour long video course, every concept of GCP is explained in detail.
If time permits you, then it is recommended to pursue all the above mentioned three courses, you’ll be prepared enough to pass the GCP Associate Cloud Exam with flying colors.
Here is the link to join this Google Cloud course — Ultimate Google Certified Professional Cloud Developer
This is another free Udemy course for Google Cloud certification including Associate Cloud Engineer. This course shares some useful strategies to prepare for Google Cloud certification and it is prepared by none other than Dan Sullivan, author of some of the best Google cloud courses and the man who designed Google’s official certification guide.
There is no doubt that Google Cloud certification exams are challenging. Even if you deep knowledge of Google Cloud services, you could fail a certification exam if you are unfamiliar with the structure of the tests.
This course will help you understand how Google Cloud certification exams are structured, the rules for taking these exams, and the kinds of questions you can expect.
Perhaps most importantly this course shows how to analyze questions and precisely identify what is being asked and how to reason about each possible answer so you can choose the best option.
The course begins with a review of Google Cloud certification topics followed by a detailed discussion about the structure of certification exams. It then looks at the limitations of certification exams and how someone can fail an exam even if they are knowledgeable about the topic.
Here is the link to join this free course — How to Pass Google Cloud Certification Exams
This one is a relatively newer course to learn essential concepts of Google Cloud Platform which is necessary to pass the Google Cloud Associate Cloud Engineer (ACE) Exam.
In this free udemy course, you will learn about what is Cloud Computing, what are different Cloud computing models, Important GCP services, and some hands-on demo for creating a virtual machine, creating a bucket, and how to use Bigquery for Machine learning.
This course will also help you are planning to build or change your career to GCP. This course will also help to plan one of highly valued Google Certifications like “Google Associate Cloud Engineer”, “Professional Cloud Architect” etc.
Here is the link to join this free course — Google Cloud Fundamentals 101
That’s all about the best free course to prepare for Google Cloud Associate Cloud Engineer certification in 2021. In this guide, you have learned about the five best free courses to pass the GCP associate cloud engineer exam.
All these courses picked after detailed research, and all these courses are top sellers on their respective platforms. If you have ample time, then it is recommended to enroll in each of these courses and then decide which one you should finish and which instructor is more understandable.
For better preparation, I also recommend solving practice questions like the ones given in Google Cloud Associate Cloud Engineer Practice Exams by none other than Dan Sullivan, the author of Google’s official certification guide. It’s not free but in $10 completely worth it.
udemy.com
Other Cloud Computing and IT Certification Courses and Articles you may like
Thanks for reading this article so far. If you find these free Google Cloud Platform and Cloud Engineer certification courses useful, then, please share them with your friends and colleagues. If you have any questions or feedback, then please drop a note.  P.S. — If you are serious about learning the Google Cloud platform and passing Cloud engineer certification then I also suggest you join the Google Certified Associate Cloud Engineer Certification by A Cloud Guru and Ryan Kroonenburg. It’s not free but it’s the best course to pass the Cloud engineer certification and created by experts.
udemy.com
Medium’s largest Java publication, followed by 14630+ programmers. Follow to join our community.
253 
253 claps
253 
Written by
I am Java programmer, blogger, working on Java, J2EE, UNIX, FIX Protocol. I share Java tips on http://javarevisited.blogspot.com and http://java67.com
A humble place to learn Java and Programming better.
Written by
I am Java programmer, blogger, working on Java, J2EE, UNIX, FIX Protocol. I share Java tips on http://javarevisited.blogspot.com and http://java67.com
A humble place to learn Java and Programming better.
"
https://medium.com/google-cloud/google-cloud-sdk-dockerfile-861a0399bbbb?source=search_post---------289,"There are currently no responses for this story.
Be the first to respond.
The Cloud SDK is the command-line interface to the Google Cloud Platform. It is a flexible utility which uses GCPs own Cloud APIs to perform many different tasks such as deploying code on AppEngine, to creating Compute Engine VMs to checking IAM permissions and so on. It is the command line interface to pretty much every Google Cloud Platform API and service.
This article describes some more ways customers can use it by demonstrating several use cases involving containerizing the Cloud SDK. The following shows Dockerfiles targeted towards various operating systems and some sample usecases (Its nothing new, just a sample set to document the baseline dockerfiles.
EDIT 4/24: Updated blog post with changes from the source github Repo.
EDIT 9/4: Update content to point to official docker.io/google/cloud-sdk image
Dockerfile
Usecases
To use any of the images above, simply download the image directly (except alpine, you have to create that as shown later)
If you want to use a prebuilt image, see
github.com
specifically
or via tagged version of the SDK:
NOTE, with latest image set, all components are installed by default. If you would rather have a minimal image to use, try the slim image
The alpine based images do not yet exist in the official google cloud sdk but I am listing the docker generation file here incase you want to create one
The SHA256 checksum is listed on the SDK documentation page.
Note: you can pass in the ARG value of the sdk version and checksum as overrides:
The following describes building and running the Cloud SDK as an Alpine package. In other words, its a package you can use Alpine’s installer to setup instead of sourcing from a base image and then installing Cloud SDK as docker commands.
For more information see:
At the time of writing (3/16), the package is not in the official alpine community repository. Although building a version of the package is pretty straightforward, you can find it also on the private repository shown below.
Note: the package below is pinned to SDK 147.0.0. You can either update gcloud post-install or regenerate a new package.
APKBUILD
Running the SDK using an untrusted private repository (https://storage.googleapis.com/mineral-minutia-820/alpine/v3.5/community)
The following describes the abridged steps to creating the image locally in a Dockerfile for alpine
You’re an App Engine developer and you want to keep your workstation in as much of a consistent state as possible. That means you would rather not install the Cloud SDK or run dev_appserver.py directly from your laptop. What you would rather do is spin up any components for your local development without having Cloud SDK installed on the laptop.
What you’d like to do is run Cloud SDK and dev_appserver.py in a container. To do that, you need to run dev_appserver.py from the Cloud SDK docker image but map your deployable sources to that container.
For example with python, I’m mapping my current source directory to the container (under /apps) and instructing it to run the dev_appserver.py
You can also configure your container image if you’d like to use Maven You can also do this with maven but you will need to install the dependencies into the extended image itself as shown in the following Dockerfile that sets up your execution environment for maven:
Build your containerized runtime environment:
Then just launch with your app:
(Note: you’ll need to specify the host/port for the dev_appserver in the pom.xml section for the appengine-maven-plugin)
Don’t want to install, update and maintain a local SDK install? Install the SDK requires python installed locally. If you containerize the SDK, all you need to run is docker. If you pull the published google/cloud-sdk:latest image, you are guaranteed to have the latest release of the SDK. Note that once you pull an image down, it will remain cached in your local repository until you delete the local image and pull again. Alternatively, if you want to remain on a given version, you can always specify the docker image to pull. Note: we support only the last two releases of the SDK from ‘latest’.
For example, first initialize the volume to use by authorizing it with your credentials:
Then reuse that volume but now use any gcloud command:
You can also use this technique to initialize a volume with a service account. This is useful if you want to have several service accounts handy as volumes where each service account is scoped with a different IAM Role.
To setup a service account credential in a volume, first download the JSON certificate file and map a volume to the Cloud SDK container:
In the following example, my service account certificate file is stored on my local system at $HOME/certs/serviceAccountFile.json
Warning: the volume gcloud-config now has your credentials/JSON key file embedded in it; carefully control access to it!
Then run a new container but specify the volume. You’ll see that the configured credentials already exist
Now run some gcloud command on behalf of that service account.
You can continue to do this with other restricted service accounts in volumes. This will allow you to easily control which service accounts and its capabilities you use by having it already defined in a redistributable the container image (vs. using gcloud’s — configuration= parameter in each command).
Running emulators in a container provides an easy, predictable configuration for an emulator. You can always reuse a given configuration without needing to initialize the SDK with credentials. For example, the following starts up the pubsub emulator from a baseline SDK:
You can even extend and link some code you run in an container to connect with this emulator. In this mode, you run your emulator in one container, acquire the container’s internal address, then separately run your application in another container but link to the emulator via docker networking. There are several ways to do this securely too with docker custom networks.
First run the emulator:
We need to pass in the internal IP address for the emulator into your application container. The following command returns the internal IP address which we will use later.
Now run your application container with credentials but link it back to your emulator by passing in an environment variable for the emulators internal IP address
Note: you need to pass in GOOGLE_APPLICATION_CREDENTIALS because the PubSub client library tries to acquire an access token before contacting the emulator.
Suppose you want to run different automated tasks with service account with restricted access. First create a service account and initialize a volume as shown in the “Reuse service account credential” use case. Once you’ve done that, you can invoke any script that uses gcloud-cli. We described some of the scripting you can do with gcloud in a previous blog post here.
As a concrete example, suppose you have a script which lists out the service accounts and their keys:
$ svc.sh
If you want to run that script using credentials initialized and attached to a volume in a container, simply run the gcloud-sdk container, reference the volume with credentials and map your script to the running container. In this example, I initialized gcloud-config with a given service account and then mapped svc.sh script from my local workstation to the gcloud-sdk image. The entrypoint for the image is the script itself
For more info on scripting gcloud see previous blog post on this topic.
Hopefully, these basic use cases have given you some ideas on how to containerize the Cloud SDK and extend the image and customize in ways to suit your need.
Google Cloud community articles and blogs
44 
1
44 claps
44 
1
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/cloud-logging-though-a-python-log-handler-a3fbeaf14704?source=search_post---------290,"There are currently no responses for this story.
Be the first to respond.
I’ve been doing Big Data on the Google Cloud Platform for a while now and I really saw the platform mature. And if you’re doing Big Data you probably love Python. It’s really a language to get things done.
One of those build in features is Logging. Out of the box Python has a few Log Handlers for logging to console, files and streams. But I was really surprised nobody wrote a decent log handler for Google Cloud Logging.
So I decided to write a build-in handler for my luigi-gcloud extension, giving the user full control for what they want to log though the logging configuration.
But because it’s so small and easy to use I want to share it so you can build it in your Python scripts that suppose to run on Compute. Include it in your project and you have the handler ready to use.
The only thing left is to configure it with a ini file. A caveat though is to make sure that the root logger doesn’t use our handler. This is because the client library also does logging and you will get a reclusive loop.
The important parts in the ini file is the [logger_app] and [handler_cloudLoggingHandler] section. This will route the loggers with name ‘my-app’ to a the respective handler. In the handler section you need to specify the project_id of our cloud project.
Everything is configured, and now we can start logging directly to Cloud Logging. Make sure you look into the Custom Logs in the console.
I have to point out that for each log call an API call is done which can impact performance. But as a lot of Python scripts are written for automation task that are not performance critical this should not be a problem to them.
Happy Logging!
Google Cloud community articles and blogs
37 
1
37 claps
37 
1
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Google Developer Expert for Cloud
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/introducing-gcp-essentials-a-new-video-series-65a3d469ff1c?source=search_post---------291,"There are currently no responses for this story.
Be the first to respond.
Every day new users start using Google Cloud Platform (GCP). You might be one of them or know of one. If so, here’s a new video series for you on YouTube — GCP Essentials
You may be using a public cloud for the first time or looking to learn how GCP compares to other solutions you’ve used before. You may consider yourself a developer, may be passionate about containers and Kubernetes, may be excited about Serverless computing, or looking to migrate existing workloads to the Cloud. You may be on the (Dev)Ops side of the house and more concerned about provisioning, monitoring, observability, and alerting. And of course you may also be looking at GCP’s capabilities as some variant of data or ML engineers.
In all cases you’ve come to the right place and this series is meant as a set of short gentle videos to make sure GCP doesn’t look intimidating. Hopefully the short format also works for more seasoned users who want to make sure that they haven’t missed out on some basic information. I know I’ve certainly learned a few things in the process!
Here’s how to subscribe: bit.ly/GCloudPlatform
In the above opening video, we discuss cloud.google.com and Cloud Console as the two most-visited GCP destinations and what to expect from them. We also briefly cover gcloud, our almighty command line, billing and pricing considerations, free tutorials, and we even touch on developer support options.
Upcoming videos will cover GCP’s Free Trial and Free Tier (and why you should not confuse the two), Cloud Console and Cloud SDK in more detail, a platform overview, how to find “stuff”, and a lot more.
Expect new videos every other week. Feel free to provide comments (here or in the videos) and suggest topics you’d like to see covered!
Google Cloud community articles and blogs
98 
1
98 claps
98 
1
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Google Cloud Developer Relations
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/@iamgique/create-gce-on-gcp-9e5565ecdbf?source=search_post---------292,"Sign in
There are currently no responses for this story.
Be the first to respond.
Sakul Montha
Oct 28, 2017·3 min read
มาสร้าง GCE (Google Cloud Engine) บน Google Cloud Platform กันเถอะ
บทความนี้จะทำการทดลองสร้าง GCE (Google Cloud Engine) กันนะครับ ใครที่ยังไม่รู้จัก ให้ไปอ่านบทความนี้ก่อนครับ https://medium.com/@iamgique/google-cloud-platform-vs-amazon-web-services-gce-and-ec2-c7ddb6b93e7a ผมจะขอข้ามขั้นตอนการสมัครไปเลยนะครับ หวังว่าทุกท่านจะ happy กับการ สมัครสมาชิกที่แสนจะง่ายดาย พร้อมรับเงิน $300us มาใช้งาน อีกปีนึง
ข้ามกันไปที่หน้า Dashboard กันเลยให้ท่านไปที่ IAM & admin > Manage resources ก่อน
ทีนี้ก็รอครับ ใช้เวลานิดนึง …… รอจนเสร็จ
หากท่านมาถึงหน้านี้แล้วนั่นแปลว่าท่านมาถูกทางแล้วครับ แต่จะสังเกตุเห็นว่า Compute Engine ยังว่างเปล่าอยู่เลย เนื่องจาก เรายังไม่ได้ทำการสร้าง GCE นั่นเอง ให้ท่านกดไปที่ Go to the Compute Engine dashboard แล้วรอสักแปปใหญ่ ๆ …
ระบบจะถามเราว่าจะสร้าง หรือ นำเข้าไฟล์ VM instance ให้เราเลือก Create โลดด
Name: เป็นค่าชื่อของ instance ที่เราต้องการตั้งZone: ตอนที่เขาเลือกมาให้จะอยู่ใน zone us ถ้าผู้อ่านอยู่ ประเทศไทย แนะนำให้เลือก asia-east ครับ อันไหนก็ได้Machine type: ตอนที่ผมเขียนนี่มีเครื่องเล็กกว่าที่เลือกอีก 2 type ราคาจะถูกลงไปอีก อันนี้แล้วแต่ท่านจะสะดวกเลยBoot disk: แล้วแต่ท่านสะดวก ผมสะดวกแบบนี้Identity and API accessService account: ตรงนี้จะมีให้ท่านเลือกกรอก แต่ตอนนี้ท่านเลือก default ไปก่อนครับAccess scopes: ส่วนนี้จะเป็นการเลือกว่าจะใช้ service cloud ของ Google อะไรบ้าง ถ้าอยากเลือกเองให้ติ๊กที่ Set access for each API จะมีให้เลือกอีกเยอะเลย ส่วนผมขอเป็น default ไปก่อน เอาไว้จะใช้อันไหนที่มันไม่อยู่ใน default ค่อยกลับไปแก้ครับFirewall: ผมติ๊ก allow ทั้ง http และ httpsส่วนของการ auto restart ก็ default ไปครับCREATE โลด
หลังจากที่ท่าน Create VM instances success ระบบจะนำท่านมายังหน้า Dashboard ของ VM instances ตัว Dashboard นี้จะแสดงให้เห็น พวกค่าต่าง ๆ ที่จำเป็นให้ดู แล้วก็สามารถสั่ง Stop, Start หรือ Restart ได้จากหน้านี้เลย IP ภายใน ภายนอก ก็จะอยู่ในหน้านี้
ท่านสามารถทดลอง เข้าเครื่องได้ง่ายมากโดย คลิ๊กที่ SSH ที่จริงมีวิธีเข้าอีกหลายวิธี วิธีที่ผมยกตัวอย่างคือวิธีที่ง่ายที่สุดแล้วครับ เมื่อเข้ามาภายในจะเห็นว่า มีคำอธิบายเอาไว้ว่าเราใช้ OS อะไร ตัวที่เราเลือกมาจะเป็น Debian GNU/Linux ซึ่งก็สามารถใช้คำสั่ง Linux ได้ตามปกติเหมือนมันเป็นเครื่อง เครื่องนึงจริง ๆ
กลับมาที่ Dashboard ของ VM instances ให้ท่านลองคลิ๊ก เข้าไปที่ ชื่อ instances ของท่าน ระบบจะพาท่านไปหน้า VM instance details ท่านจะเห็นค่า metrics ซึ่งสามารถเลือก ได้ว่าจะให้แสดง ค่าอะไร เช่น cpu, network หรือ disk นอกจากนี้ก็ยังจะแสดงค่าอื่น ๆ ที่ท่านควรทราบ อันนี้ไม่ขออธิบายทั้งหมดนะครับ
มาถึงตรงนี้ท่านก็จะได้ instances มาแล้ว 1 เครื่อง พร้อมกับระบบปฏิบัติการ บทความต่อไป เราจะทำการนำ Application ไปลงใน Bucket แล้วทำการ deploy เข้า GCE กันครับ
Technology Advocacy Manager, a man who’s falling in love with the galaxy.
See all (63)
20 
20 claps
20 
Technology Advocacy Manager, a man who’s falling in love with the galaxy.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/swlh/backup-firestore-data-to-storage-bucket-on-a-schedule-in-gcp-7d910b2dea70?source=search_post---------293,"There are currently no responses for this story.
Be the first to respond.
In this article we are going to use four features of Google Cloud Platform (in four simple steps) to backup our Firestore database to a storage bucket.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@DazWilkin/visual-studio-code-editing-gce-files-563cdf52f2ca?source=search_post---------294,"Sign in
There are currently no responses for this story.
Be the first to respond.
Daz Wilkin
Jul 7, 2017·3 min read
Like many, I’m mostly completely converted to Visual Studio Code and Google Cloud Platform (full disclosure: Googler). Until today, I suffered through nano (very occasionally vi) when editing files on Google Compute Engine (GCE) instances.
Another editor — Textmate — has a feature called ‘rmate’ that enables remote editing by leveraging an ssh tunnel to the remote machine. Rafael Maiolla developed an extension that brings this experience to Visual Studio Code. Thanks, Rafael!
This post explains how to use this extension to remotely edit files on GCE instances.
You will need Visual Studio Code, ruby, rmate and Rafael’s remote-vscode extension. I’m running a Linux (Ubuntu) workstation, ymmv. Restart Visual Studio Code. Open “settings” (CTRL-,) and search for “remote.”. I left the settings at their defaults:
Then press “F1” to open Visual Studio Code’s Command Palette and find “Remote: Start Server”, click it. You should see (briefly) “starting server” (perhaps “restarting server”) in the bottom left hand corner of Visual Studio Code’s screen. Code is ready.
We’ll now set up a secure reverse (remote machine is the client) tunnel between your local machine and a remote GCE instance. To do this, you will, of course, need to have a remote GCE instance. I’ll assume you’ve done that and you have a machine called ‘my-gce-instance’.
You should replace the $VARIABLES in the following command but use something similar to:
All being well, you should find yourself logged in to a secure session on ‘my-gce-instance’.
You must install rmate on the GCE instance before you can use it. I’m running Ubuntu on the GCE instance and the commands I used are:
When you wish to use Visual Studio Code (on your local machine) to edit a file (on the GCE instance), run this command on the GCE instance:
For example, to edit index.html in /www, you would type:
Don’t be disappointed when it appears nothing has happened…
Something useful has happened!
NB: You must maintain the ssh session while you are editing. You may issue multiple ‘rmate’ commands, one for each file you wish to edit.
Return to Visual Studio Code and you should see that the remote file has been opened!
When you close each file, the associated (remote) rmate process will be terminated. When you’re done editing, you should exit the ssh session.
See all (138)
77 
2
No rights reserved
 by the author.
77 claps
77 
2
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/martinomburajr/building-a-go-web-app-from-scratch-to-deploying-on-google-cloud-part-0-intro-a6bf26972ce5?source=search_post---------295,"There are currently no responses for this story.
Be the first to respond.
I recently wrote an article describing the Google Cloud Platform (GCP) Compute stack which described how different layers of cloud, FaaS, PaaS, CaaS, IaaS are utilized in GCP (See link below).
"
https://medium.com/google-cloud/ethereum-helm-kubernetes-engine-25a9552f8e3d?source=search_post---------296,"There are currently no responses for this story.
Be the first to respond.
I wrote recently of a self-developed way to deploy Ethereum’s “Go Ethereum” (aka “geth”) two-ways to Google Cloud Platform:
https://medium.com/google-cloud/ethereum-on-google-cloud-platform-8f10c82493ca
I read last night on Microsoft’s Developer Blog “Using Helm to Deploy [Ethereum] to Kubernetes”.
One benefit of Kubernetes (and Helm) is that, if a Chart works — as in this case — for Azure Kubernetes Service, it should work for any Kubernetes cluster including, Kubernetes Engine. Hint: It does.
So, if you want to deploy a private Ethereum network to Kubernetes Engine, look no further than the stable (!) Ethereum chart:
https://github.com/kubernetes/charts/tree/master/stable/ethereum
If you have an existing Google Cloud Platform project, a Kubernetes Engine cluster and Helm installed, skip ahead.
Follow the instructions provided by the authors of the Ethereum chart:
https://github.com/kubernetes/charts/tree/master/stable/ethereum
You’ll need a wallet and the instructions explain how to create one if needed. My only recommendation would be to wrap the commands in virtualenv:
which should generate something of the form:
Make a (secure) note of these values as they represent your wallet.
You’re now ready to deploy the Chart to Kubernetes Engine:
NB Generate a secret. For testing purposes, perhaps using Secure Password Generator.
When the Helm deployment completes, it will provides guidance on next steps:
You should see the several resources deployed to your cluster.
There should be 4 Deployments:
And 3 Services:
And, you can confirm that a Network Load-Balancer has been created:
The Helm Chart provisions a Network Load-Balancer for the Ethereum deployment. You can access the EthStats Dashboard (referring back to the “Notes” provided when you deployed the chart) by browsing ${SERVICE_IP}:
and:
On Google Cloud Platform, it’s possible (preferable) to use an HTTP/S Load-Balancer for this HTTP-based service. To do this, you can munge your existing deployment or clone the Ethereum Chart repo and make the following changes.
Remove the Network Load-Balancer by changing type: LoadBalancer to type: NodePort in line 18 of the Chart’s values.yaml. You may apply this to the running deployment or change the template file.
Then apply an Ingress to this NodePort with:
NB You’ll need to replace [[HELM-RELEASE]] with the release name of your Ethereum Deployment.
If you do this manually, you can eyeball the release name. Mine is “wondering-guppy”. Yours will be different.
If you do this by changing the template, you can use the following template:
This will result in an HTTP/S Load-Balancer being provisioned:
And, similarly to the previous command, you can:
And then browse ${INGRESS_IP}.
The primary intent of this story is to show you that, if it blends, it’ll blend on Google Cloud Platform too. We’re often asked by developers whether X runs on Google Cloud Platform. The question is really “How can I run X on Google Cloud Platform?”
In this case, we’re able to ride the de facto container orchestration standard of Kubernetes and its companion ‘package manager’ of Helm. Given a Helm Chart, the Helm Chart should work on Kubernetes Engine.
That’s all!
Google Cloud community articles and blogs
36 
No rights reserved
 by the author.
36 claps
36 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/syncedreview/spotlight-on-ai-at-google-cloud-next-18-f4309c93beb9?source=search_post---------297,"There are currently no responses for this story.
Be the first to respond.
Artificial intelligence has become a sort of secret weapon in the battle to build the best cloud service platform. Google Cloud Platform is currently the underdog, trailing both Amazon Web Services and Microsoft Azure. But Google is betting robust AI will give it the edge it needs to catch up. At the annual Google Cloud Next conference which kicked off July 24 in San Francisco the company unveiled a series of AI-based product releases and enhancements for its analytics and machine learning tools, additional applications on G Suite, and new IoT products.
Earlier this week, Google parent company Alphabet reported its Q2 earnings, which were ahead of Wall Street’s expectations. The company’s “other revenue” category, which includes the Google Cloud business, rose 37 percent over last year. CEO Sundar Pichai emphasized that “machine learning has been a major focus and a key differentiator for Google, and that’s true for our Google Cloud customers as well.”
Synced is onsite at the Moscone Center in San Francisco to bring you Google Cloud Next ’18 news and updates.
AutoML is one of Google Cloud’s standout innovations: A suite of machine learning tools designed for developers who have limited machine learning expertise and resources. Powered by neural architecture search and transfer learning, AutoML can help developers build custom AI models, or more specifically, find the best neural network architecture and automatically set weights.
Earlier this year Google Cloud announced AutoML Vision, which provides pre-trained models via an API, and the ability to build vision-based custom models. Google has now introduced AutoML Natural Language and AutoML Translation, which enable model customization and integrate with existing Google Cloud APIs. All three AutoML tools are currently available in beta.
To build a custom model, developers need only feed labeled text data into AutoML Natural Language or translated language pairs into AutoML Translation. AutoML will then produce a trained and optimized machine learning model.
Google provides labeling and annotation services, a critical data processing step which ensures training data is high-quality. AutoML Vision’s in-house human team manually classifies images with labels. Natural Language API can analyze text data, such as content classification, sentiment analysis, and entity recognition. Translation API can directly convert texts in more than 100 languages.
The Tensor Processing Unit (TPU), introduced two years ago by Google, is a custom application-specific integrated circuit (ASIC) tailored for machine learning workloads on TensorFlow. In February Google made its TPU V2.0, or Cloud TPU, available in beta for researchers and developers on the Google Cloud Platform. Built with four custom ASICs, Cloud TPU delivers a robust 64 GB of high-bandwidth memory and 180 TFLOPS of performance.
At Google Cloud Next the company announced that its TPU 3.0 — a next-generation AI chip announced two months ago that is eight times more powerful than its predecessor and can achieve up to 100 petaflops performance — are now available in alpha. Google Cloud Chief AI Scientist Fei-fei Li comments, “TPUs allowed eBay to reduce the training time of their visual search model by a factor of almost 100 — from months to days.”
Google is applying its conversational AI technologies in customer service. The company announced Contact Center AI, a cloud-based machine operator that can answer customers’ questions, fulfill basic tasks, and transfer the calls to a human representative if necessary.
The Contact Center AI adopts some of the technologies behind Duplex, an advanced system announced two months ago that can for example call restaurants for reservations or schedule appointments at a hair salon.
Contact Center AI also leverages capabilities from Dialogflow, a technology that enables conversational AI and helps developers build chatbots. Google announced new Dialogflow features at Google Cloud Next, including text-to-speech capability via DeepMind’s WaveNet, the Dialogflow Phone Gateway for telephony integration, Knowledge Connectors to enrich the response database, and Automatic Spelling Correction.
Today, over four million businesses subscribe to Google G Suite, a set of cloud-based productivity and collaboration tools that includes G-mail, Google Calendar, Hangout, Google Drive, etc. Google is now using AI to promote its cloud-based enterprise businesses to the next level.
Google also announced that Smart Compose for Gmail will be soon available to G Suite customers. The program can automatically complete emails by filling in greetings or signing off, etc. It was first introduced with Gmail’s redesign this May as an experimental access feature.
Hangout, a Google communication platform that enables messaging and video chatting, added the new feature Smart Reply to help users respond to messages more quickly. Users are given automatic response suggestions on a Hangout Chat interface.
But the most exciting new feature might be the AI-powered grammar suggestions on Google Docs. Using machine learning, Google Docs can perform corrections from “simple grammatical rules like how to use articles in a sentence (‘a’ versus ‘an’), to more complicated grammatical concepts such as how to use subordinate clauses correctly.” The new feature was made available this week in Google’s Early Adopter Program.
Other announcements include a security center investigation tool (available in the Early Adopter Program for G Suite Enterprise customers), data regions (available now for G Suite Business and Enterprise customers), and voice commands in Hangouts Meet hardware (coming to select Hangouts Meet hardware customers later this year).
The Cloud and the Internet of Things (IoT) are inseparable, and Google has already invested heavily in IoT with products such as Android Things, Nest, Google Home, etc. Several months ago, the company announced Cloud IoT Core, a service that connects data from millions of dispersed devices using the Google Cloud Platform, and provides data-intensive processing, visualization, and analysis.
Without edge computing however, the back-and-forth communication between devices and the Google Cloud Platform would still cause high latency. Google took a step to fill that void yesterday with two new products: Edge TPU, and Cloud IoT Edge.
Edge TPU is a cut-down Google ASIC designed to complement Cloud TPUs, and will be embedded into gateways that bridge the Google Cloud Platform and devices such as sensors. Edge TPU empowers TensorFlow Lite machine learning models and accelerates inference at the edge so that edge devices can make local, real-time, intelligent decisions.
Cloud IoT Edge is a software that allows edge devices to run pre-trained machine learning models on the Edge TPU or on GPU- and CPU-based accelerators. Cloud IoT Edge can run on Android Things or Linux OS-based devices, and its runtime environment should include gateway class devices, the Edge IoT Core, and the TensorFlow Lite.
CTO of LG CNS Shingyoon Hyun says Google Cloud AI and IoT technologies will allow the South Korean tech company to “make a better working place, raise the quality of products, and save millions of dollars each year.”
Google will release an Edge TPU development kit, including a system on module (SOM) that combines Google’s Edge TPU, a NXP CPU, Wi-Fi, and other microchip’s elements Google has teamed up with semiconductor companies like NXP and Arm as well as gateway vendors and edge computing companies for manufacturing. The development kit will be available for order this October.
Most new products announced at Google Cloud Next ’18 so far are based on existing Google technologies such as AutoML, TPU, and other AI features. They are perfectly integrated with Google cloud business and designed to solve the pain points of businesses.
Google Cloud’s quarterly revenue crossed a billion dollars in 2017, prompting Google Cloud CEO Diane Greene to proclaim it the world’s fastest growing cloud. There is nothing to suggest that growth will slow in the future, as the cloud plays an important role in Google’s grand strategy to transition from an Internet giant into an AI mega-power.
Journalist: Tony Peng | Editor: Michael Sarazen
Follow us on Twitter @Synced_Global for more AI updates!
Subscribe to Synced Global AI Weekly to get insightful tech news, reviews and analysis! Click here !
We produce professional, authoritative, and…
141 
141 claps
141 
Written by
AI Technology & Industry Review — syncedreview.com | Newsletter: http://bit.ly/2IYL6Y2 | Share My Research http://bit.ly/2TrUPMI | Twitter: @Synced_Global
We produce professional, authoritative, and thought-provoking content relating to artificial intelligence, machine intelligence, emerging technologies and industrial insights.
Written by
AI Technology & Industry Review — syncedreview.com | Newsletter: http://bit.ly/2IYL6Y2 | Share My Research http://bit.ly/2TrUPMI | Twitter: @Synced_Global
We produce professional, authoritative, and thought-provoking content relating to artificial intelligence, machine intelligence, emerging technologies and industrial insights.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/private-access-to-gcp-apis-through-vpn-tunnels-ab0217918e38?source=search_post---------298,"There are currently no responses for this story.
Be the first to respond.
—
10/3/19: Please instead see the official solution here:
cloud.google.com
—
This tutorial demonstrates how to use APIs for Google Cloud Platform (GCP) services from an external network, such as your on-premises private network or another cloud provider’s network. This approach allows your on-premises servers that are connected to your private network to access GCP services without using public IP addresses.
GCP customers often have workloads spanning cloud proividers and their on-premises datacenters connected via VPN. In many of those situations, customers need to access various Google Cloud APIs where arbitrary outbound traffic from any system is restricted or acutely controlled.
This not a problem while the workload is running on GCP: accessing GCP APIs from within a VPC is directed towards Google-internal interfaces where those services resides. When accessing these same APIs from other restricted cloud providers or even your own datacenter the situation is a bit different: customers need to either enumerate and selectively allow wide Google API IP ranges or apply the same treament to the traffic as if the workload is within GCP: send the traffic securely through the VPN Tunnel.
This article is a baseline walkthrough of how to setup an strongswan ipsec tunnel to Google and then access GCP APIs and your VMs residing on Google.
VPC Service Controls are also enabled to demonstrate how to lock down specific API access to just authorized projects. That is, once the private API is enabled, you can optionally lock down access to APIs like Google Cloud Storage such that it is only accessible via the tunnel. This is an optional security measure you can employ to further restrict access.
The following diagram summarizes the overall architecture that you create in this tutorial.
Connect the two networks through a VPN tunnel and set the routing to resolve and emit GCP APIs through that tunnel.
Notes:
This tutorial uses the following billable components of Google Cloud Platform:
You can find the full git repo of this article here:
github.com
In the tutorial, we will setup the following remote an local networks.
Local:
GCP:
The steps outlined below sets up the following (in order):
This section details setting up the project on GCP.
It is advised to run all these commands in the same shell to avoid resetting environment variables.
First export some variables (feel free to specify your own projectIDs, ofcourse):
Create project that will host the VPN gateway and VPC API access (substitute the projectID with your own; this article uses GCP_PROJECT_NAME in the examples below)
Export the envionment variables for the project and network region to setup
Create the custom network within this project that will host the VPC and private API access:
This VM is used for testing connectivity from the remote VM to GCP. This VM will not have a public IP allocated and is accessible via VPC>
Verify no public address is allocated:
This example sets up another GCP project to ‘simulate’ a remote network. This project does NOT use Cloud VPN and instead terminates the VPN traffic directly on a VM. In reality, you can setup any cloud provider VPN or appliance while this article shows raw, low-level ipsec configuration
As before, the following command uses a project called ONPREM_PROJECT. In your case, you should setup this project as (for example): ONPREM_PROJECT-[randomcharacters]
Export some environment variables and substitue the variables with your project names:
Configure a new custom network with a specific CIDR range. You can configure any range but for consistency with the ipsec configuration below, the CIDR ranges below are used.
Configure firewall rules on the ‘simulated’ network to allow ipsec traffic inbound and internal traffic to the privateIP ranges for GCP.
This VM will host the ipsec tunnel and provide DNS resolution for the onprem network.
Since our onprem network is yet another GCP project, setup a route via gcloud
Normally, you would use BGP or directly set routetables but on GCP, these routes need to get setup on control plane.
Setup another VM instance on the onprem network that will send traffic and DNS resolution through the VPN Gateway (instance-1)
In the end, you should have two VMs setup on your onprem project:
NOTE public_IP of ONPREM VPN Gateway) is: >>>> 35.192.118.145 <<<<
Create VPN using Cloud Console.
Use the following specifications for the VPN. The remote peer IP is the IP address of the VPN host VM we setup previously 35.192.118.145
Use a new shared secret (eg: $SHARED_SECRET = python -c ""import uuid; print str(uuid.uuid4())"")
NOTE public_ip of GCP VPN GATEWAY that gets allocated. In this article, its: 35.184.203.133
NOTE THE VPN GATEWAY PUBLIC IP >>>> 35.184.203.133 <<<<*
export the ip as an environment variable for later use
On the GCP network, allow traffic inbound from your onprem network range 192.168.0.0/20
Configure the “ONPREM” VPN gateway
Remember to replace leftid= with the IP address of the VPN Gateway VM on $ONPREM_VPN_IP
Remember to replace right= with the IP address of the VPN Gateway VM on $GCP_VPN_IP
then
Configure strongswan secrets:
In the current example, the values would be:
Configure ipsec.conf:
Remember to replace leftid= with the IP address of the VPN Gateway VM on $ONPREM_VPN_IP
Remember to replace right= with the IP address of the VPN Gateway VM on $GCP_VPN_IP
Verify tunnels are created
2f. [Remote] Add next-hop route thorugh VPN Gateway
Verify the tunnels are up:
Open up a new window in instance-1, and run ip xfrm monitor.
You should see ipsec traffic through the gateway vm instance-1
Private access for Google APIs from remote systems requries a special CNAME map:
www.googlapis.com: CNAME=restricted.googleapis.com --> 199.36.153.4/30
On remote gateway VM (instance-1), open up two new shells.
In one window, run ip xfrm monitor, In another window, access the GCE VM and a private API:
Also verify the traffic through to 199.36.153.7 is via the tunnel
The session with ip xfrm monitor shows activity which indicates data sent via tunnel.
The following sequece will configure and test DNS resolution and direct connectivity to:
On remote gateway VM (instance-1):
Note ONPREM_PROJECT, the settings in your file will be different.
NOTE: GCP VMs overrides certain host entries likes /etc/resolv.conf so you may need to reset this if you leave the 'onprem' VMs running.
This should resolve to the IPs provided locally with CNAME and resolve to the 199.36.153. range
Note the IP address you connected to: 199.36.153.5
At this point, we have verified GCS API calls through the tunnel
We have verified the remote gateway sends traffic to GCP via the tunnel. We will now configure another host ‘on prem’ which will also send its traffic through to GCP via this gateway:
Normally, routes are added directly if using BGP or static route:
However, since this tutorial simulates ‘onprem’ on GCP, add routes via gcloud instead:
Verify API traffic is through the tunnel by running ip xfrm monitor on the VPN Gateway host (eg, instance-1 on project=ONPREM_PROJECT_NAME)
At this point, the VPN connectivity to GCP APIs and VM is established via the tunnel. Optionally enable API access to GCP services such that it must go through a trusted project (in our case, via the tunnel).
See Service Perimeters as well as Cloud IAM Roles for Administering VPC Service Controls.
First enable a security perimeter such that GCS access is only available via project=gcp-project:
Note: once you enable this, all acess to GCS for the given bucket is blocked except via this specific project.
on a host ‘on prem’ force traffic for www.googleapis.com to not go through the tunnel. Do this buy making the DNS resolve to the external address. On instance-2, comment the name resolution entry:
Then attempt to access GCS:
(note the address is outside of the tunnel route: 74.125.70.95)
Now add in name resolution such that the traffic transits the gateway and tunnel:
(note the address resolved is 199.36.153.4)
When you have completed this tutorial, delete your project to avoid incurring further costs.
Google Cloud community articles and blogs
58 
1
58 claps
58 
1
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://itnext.io/using-environment-variables-with-google-cloud-functions-e9948f70f6cd?source=search_post---------299,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Google’s Cloud Functions have been around for a little while now. The product works well with other Google Cloud Platform offerings and provides a scalable, event-driven runtime for serverless applications or microservices. Developers can write functions using (a relatively old version of ¯\_(ツ)_/¯) Node.js (other language runtimes aren’t available at this point ¯\_(ツ)_/¯) and attach them to…
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://rominirani.com/a-working-guide-to-understanding-google-compute-engine-pricing-options-part-1-d71f2cf2bd14?source=search_post---------300,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
This is series of articles that I plan to write to help understand the various pricing levers that you have in Google Cloud Platform’s core compute offering : Google Compute Engine. I plan to explain this vis easy to understand and practical examples with the help of the Google Cloud Cost Calculator, that is available publicly.
Let’s get going.
Google Compute Engine provides a core infrastructure component : Virtual Machines (VM). These VMs are classified into different families
Each family has various machine types. How do you select which machine type, what kind of workloads are they best suited for? You can use this guide as a start for the same:https://cloud.google.com/compute/docs/machine-types and I produce a screenshot from there that covers the various workloads that are best suited for which machine types.
Each of the machine types (e.g. N1 general purpose) will have several *fixed* or *pre-defined* configurations (instance types) of VPU and RAM. You can choose the appropriate instance types depending on your requirements.
A set of instance types for N1 (General purpose) machine types is shown below:
An important thing to consider when you choose a machine type is whether it is available in the GCP Region (actually zone) since VMs are zonal resources.
Check out the list of regions that GCP is available in and more importantly, the availability of machine types per region. Bookmark this since this can change. https://cloud.google.com/compute/docs/regions-zones
VM is a rented resource and hence you are charged for the time that you use the Virtual VM. Let’s qualify that. You are charged for the amount of time that you keep a VM in running state. If a VM is stopped, you are not charged for it. https://cloud.google.com/compute/vm-instance-pricing
Is there any minimum charge and then charged by second/minute/hour — whats the granularity by which you are charged? I reproduce this from the official documentation linked above:
Each VM instance type has a pricing associated with it i.e. per hour pricing. As you consider higher vCPU and more RAM, the pricing inches upwards. Take a look at pricing for N1 instance types (https://cloud.google.com/compute/vm-instance-pricing#n1_predefined).
Remember* the prices are not the same in each region. When we get to using the Google Cloud Calculator in a while, ensure that you choose the correct region for the pricing to avoid any surprised.
In the screenshot below, you will find that there is a dropdown for the region that you can select and see the prices.
The pricing is classified into:
This can be confusing but let’s understand it with an example that we shall work through with the help of the GCP Cost Calculator. We will also throw in some other variations.
First up, we will finalize that we are interested in provisioning a Compute Engine VM instance type (n1-standard-1), which has the following configuration:
Remember that I mentioned that GCP charges you for duration that you keep the VM running. So you run it 24 x 7 i.e. 7 days a week and for 24 hours a day i.e. you keep the VM running. GCP charges you by the rate per hour for that instance type.
But do consider that not everything needs to run 24 x 7. For e.g. consider organizations that have a development , test and production environments. Sure the production environments run 24 x 7 but my development / test environment might only run for 5 days a week (Mon-Fri) and 8 hours a day (9:00AM — 5:00 PM).
So I should be charged less right ? Correct. Let’s go step by step.
First up, we will use the GCP Cost Calculator available at (https://cloud.google.com/products/calculator) and see what it costs to run this VM.
Visit the URL and enter the values as:
Ensure that you select 24 hours per day and 7 days per week as shown below. Click on “Add to Estimate”
This will show the pricing to you on the right side:
Note 3 things here first:
Notice one of the entries in the billing details “Sustained Use Discount : 30%” . What is going on here? Google Cloud provides an automatic discount (upto a certain %) off the list price, when you use the VM for more time , in our case the full duration of the month.
The more you use, the more discount you get. Check this graph out.
If you use it for just 15 days a month, we still give you a % discount. This is Sustained Use Discount (SUD) and it is covered here: https://cloud.google.com/compute/docs/sustained-use-discounts
That’s great , isn’t it ? No need to do complex calculations — we pass on the discount as per your usage.
Next up, remember the dev and test environments? What if you wanted to run this for the following:
Let’s click on the Edit button for the pricing as shown below. This will populate the entries in the form of the GCP Calculator, where you can then tweak any settings.
Now go ahead and change the values as shown below:
And click on “Add to Estimate” again.
This will bring up the revised pricing as shown below:
That’s a nice discount , isnt it ? So always plan out your different environments (Production, Development , Testing) and how long the VMs need to be running.
Don’t pay for resources that are idle and you are not using. Best Practice : https://cloud.google.com/compute/docs/instances/schedule-instance-start-stop
Now, what let’s come to Committed Use Discounts (CUD). If you are willing to commit to running these instances all the time for a period of 1 yr or 3 yr, Google Cloud can give additional discounts. Read about CUD here (https://cloud.google.com/compute/docs/instances/signing-up-committed-use-discounts )
Let’s see how to do that in the calculator. Let’s edit the previous entry back to running it 24x7 , so that the total is shown as below. This time, we will also label this line item as : SUD Server.
Next up, add another VM resource with the following:
Click on “Add to Estimate”
Next up, add another VM resource with the following:
Click on “Add to Estimate”
This will bring up the following pricing and you can see how it is beneficial to go for CUD pricing, if you plan to commit to GCP.
Hope this was useful.
I plan to cover more some nuances next on Compute Engine pricing vis-a-vis Custom Machine Types and Preemptible VMs in another thread.
Till then, Happy Pricing !
Technical Tutorials, APIs, Cloud, Books and more.
42 
42 claps
42 
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
Technical Tutorials, APIs, Cloud, Books and more.
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
Technical Tutorials, APIs, Cloud, Books and more.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/javarevisited/my-favorite-free-google-cloud-platform-gcp-professional-cloud-developer-certification-courses-856ef69a56bb?source=search_post---------301,"There are currently no responses for this story.
Be the first to respond.
Hello guys, if you are preparing for Google Cloud Professional Cloud Developer certification in 2021 and looking for free online Google cloud courses then you have come to the right place.
In the past, I have shared the best courses to learn Google Cloud Platform as well as certification courses to pass cloud engineer, data engineer, and cloud architect certifications, and today, I am going to share free GCP Cloud Developer certification courses for both beginners and experienced cloud professionals.
This is one of the most difficult and prestigious exams, similar ot the AWS Developer Associate and Azure Developer (AZ-200) exam, once you pass this exam, you will have sufficient knowledge and skills to propose Google cloud-based solution, which is a very in-demand skill.
Being a Certified Cloud Developer could help you get more success in your professional career. In this era, basic certificates won’t work; you need to have a certificate of expertise. These days, companies themselves offer certificates to the candidates, but students have to clear their respective exams in return.
As I said, In this guide, I’ll share five free courses to pass the GCP Professional Cloud Developer Exam. You might find some courses that are only available on premium platforms, but they are included because such platforms offer a trial period. After research, I concluded that the Trial Period is more than enough to complete those courses.
Btw, If you need more comprehensive and focused certification courses then I also highly recommend you to join Ultimate Google Cloud Certifications: All in one Bundle (4) course on Udemy.
udemy.com
This is not free but the most comprehensive online courses for Google cloud certifications and provide study material for all four Google Cloud certifications like — Associate Cloud Engineer, Cloud Architect, Cloud Developer, Network Engineer. It also contains 500+ practice questions to hone your speed and accuracy skills.
Without wasting any more of your time, here is the list of best free online courses to prepare for the Google Cloud Professional Developer Associate exam.
These online training courses have been created by experts and made free for educational purposes. Thousands of developers shave already joined these free Google cloud courses and you can do the same too and pass this prestigious Cloud certification.
This is another free Udemy course you can use to prepare for the Google Cloud Developer Certification exam. This course will also help you are planning to build or change your career to GCP.
This is an introductory course to get started on the Google Cloud Platform (GCP). During this course, you will learn about what is Cloud Computing, what are different Cloud computing models, Important GCP services, and hands o
You can also use these to prepare for other highly valued Google Certifications like “Google Associate Cloud Engineer”, “Professional Cloud Architect” etc.
In short, a great free online course for beginner Google Cloud learners who are curious to learn about GCP, planning to get started with Google Cloud Certification.
Here is the link to join this free GCP course — Google Cloud Fundamentals 101: A quick guide to learn GCP
The next three courses on this list are interlinked with each other. If you don’t want to learn from the above two-course, you can enroll in these three courses.
These courses are designed for absolute beginners, and if you want to create a strong foundation in cloud computing, you can enroll in these courses.
This first course is created by Xavier Corbett, and so far, over 2 lakh students have enrolled in it. This course only talks about the basics of Cloud Computing and covers the following topics:
As mentioned above, throughout the course, only basics are covered, and the instructor has covered every aspect of cloud computing in detail and explained everything using graphics and proper illustrations.
If you are just starting to learn about cloud computing, this course will be a perfect start. More than 200K students have already joined this course.
Here is the link to join this course — Introduction to Cloud Computing
udemy.com
Once you have completed the basic course about cloud computing, you will understand How cloud computing can be used and what GCP offers. Now you are ready enough to start your platform-specific learning.
This course will teach you about various GCP concepts that will help you pass the GCP Professional Cloud Developer Exam with flying colors.
This course is created by Dhanaji Musale and Google Cloud Platform Gurus!, who has authored some of the most popular and comprehensive courses on the Google Cloud platform like Ultimate Google Certified Professional Cloud Developer 2021.
And so far, over 40 thousand students have enrolled in this course. Its a 6-hour long video course that will cover the following topics:
Apart from this, the instructors will also share a small trick through which you can get a reward of $300 in your Google Cloud Platform. Through this reward, you can practice. Throughout the course, graphics and visuals are widely used, and understanding the concepts is quite easy from the videos.
Here is the link to this free Google Cloud course — Google Cloud Platform Concepts
udemy.com
This course is available on CloudAcademy.com. It is among the newest and top-rated courses available on the internet for GCP Professional Cloud Developer Exam. So far, over 187 students have enrolled in this course.
In this course, there is a total of 13 videos, 12 Assignments. This is an 18-hour long video course that will teach you every bit of the GCP platform. One of the most exciting things about this course is that every topic is taught by experts.
For instance, In this course, a total of 5 instructors will deliver the lectures. Apart from this, at the end of the course, a sample exam is created for you. You can test your knowledge through this exam.
This course also includes three labs where you need to perform practicals like creating a Linux Virtual Machine on GCP, etc.
If you are looking for a course that can teach theory and practicals, this course is the best pick for you.
The above course is of 6 hours, but if you are running short on time and need a quick revision sort of course, then this one is for you. In this course, Linux Academy has covered major topics in-depth and briefly talked about less important topics.
It’s a 1-hour long video course, and over 17 thousand students have enrolled in it. Based on 621 Students’ reviews, this course has a rating of 4.4 out of 5 on Udemy.
This course is full of visuals and graphics, and that’s the reason why instructors have covered a wide range of topics in a limited period.
Graphics and visuals made the learning process quite easy and quick. If you are looking for a short course that can give you a brief introduction to GCP, this course is a perfect pick for you. It will deliver the best possible information in an hour.
Here is the link to join this free GCP course — Google Cloud Concepts
This is a meta course on Google Cloud Professional certification as it will provide you strategies and Tips for cracking Google Cloud Certifications in 2021.
The USP of this course is that it's prepared by Dan Sullivan, a cloud architect, systems developer, and author of the Official Google Cloud Professional Data Engineer Study Guide. He is an experienced trainer and his online training courses have been viewed over 1 million times.
The course begins with a review of Google Cloud certification topics followed by a detailed discussion about the structure of certification exams. After that, you explore the limitations of certification exams and how someone can fail an exam even if they are knowledgeable about the topic.
The final three lectures of the course focus on the Associate Cloud Engineer, Professional Data Engineer, and Professional Architect exams. In each lecture, you will analyze example questions and consider strategies for identifying key pieces of information and tips on eliminating incorrect options. Overall, a great free course for anyone preparing for Google cloud platform certification exams.
here is the link to join this course — How to Pass Google Cloud Certification Exams
This a Udemy Course with a rating of 4.3 out of 5. This is a detailed course on the GCP Professional Cloud Developer exam. It is created by Google Cloud Platform Gurus!, in every video of this course, graphics are used quite smartly to ease up the understanding process for candidates.
In this 16-hour long video course, every aspect of GCP is explained in detail. For instance, the following topics are covered in this course:
So far, over 72 thousand students have enrolled in this course, and if you are looking for a course that covers every aspect of GCP even beyond the course of the GCP Professional Cloud developer exam, this course is for you.
Here is the link to join this free course — Ultimate Google Certified Professional Cloud Developer 2021
That’s all about the free online courses to prepare for the Google Cloud Professional Cloud Developer certification exam. In this guide, I have shared 5 Free Courses that will help you prepare for the GCP Professional Cloud Developer exam.
If you think that I have missed a worthy course, then do let me know. I’ll check it out, and if it meets will meet my criteria, then no doubt, you’ll find it here in this list.
Other IT and Cloud Certification Articles you may like:
Thanks for reading this article so far. If you find these Google Cloud Platform Professional Cloud Developer certification courses useful then please share them with your friends and colleagues. If you have any questions or feedback then please drop a note.
P. S. — If you need more comprehensive and focuses certification courses then I also highly recommend you to join Ultimate Google Cloud Certifications: All in one Bundle (4) course on Udemy. This is not free but the most comprehensive online courses for Google cloud certifications and provide study material for all four Google Cloud certifications including Cloud Developer.
udemy.com
Medium’s largest Java publication, followed by 14630+ programmers. Follow to join our community.
220 
220 claps
220 
A humble place to learn Java and Programming better.
Written by
I am Java programmer, blogger, working on Java, J2EE, UNIX, FIX Protocol. I share Java tips on http://javarevisited.blogspot.com and http://java67.com
A humble place to learn Java and Programming better.
"
https://medium.com/@lemaysolutions/locked-in-a-box-machine-learning-without-cloud-or-apis-76cc54e391c8?source=search_post---------302,"Sign in
There are currently no responses for this story.
Be the first to respond.
Daniel Shapiro, PhD
Jul 5, 2017·4 min read
APIs are great. Machine learning APIs are even more awesome. From api.ai to the Google Cloud Platform APIs like Google vision, and IBM’s analytics tools, there is some serious muscle available out there. When calling an API, the need for a cloud-based instance (g or p instance on AWS), or an on-premise GPU in a box. You just call the API from your instance and like magic you get results. Sometimes your own GPU server in the cloud makes sense, if you are doing something custom that an API does not offer.
HOWEVER…
I have had several clients this year that do not want to call APIs at all. Some would not even ship the data off premise to AWS or Azure. Sometimes their data is too sensitive to share at all. For example, it may be classified. In other cases they may be obligated to keep the data in a certain country, and the API is not in their country. Lastly they may be hesitant to let go of their infrastructure investment. Edge computing (saving money by running the a.i. inside client machines like phones) is not something I have run into yet on a contract. And the cost savings of doing this stuff on premise versus on API calls and AWS are minor for most of our clients. The nightmare scenario for me is deploying AI into a box with no internet. Doable, but so incredibly not fun. You run apt-get update and it just hangs…..
Why are these businesses avoiding APIs? Well, some clients simply don’t want to pay the fees for API calls and want to host everything inside their facility as a secondary concern. I always advise against this approach where a solution could be available. I advocate for cloud solutions. The primary concerns I hear, as mentioned above, are regulatory or contractual, not financial.
After fighting the good fight to overcome these barriers to cloud/APIs, those companies that can’t or won’t end up provisioning a nice shiny new server. Sometimes with hyperV for taking snapshots. The system needs lots of RAM, WITH LOW LATENCY. Low latency means the 4 timing numbers on the purchasing page. They are not usually printed on the sticks of RAM. Here is a link to more info on RAM timing.
At the time I’m writing this, decent RAM specs for a good price and a cheap motherboard are DDR3 1600 with timing 7-8-8-24. Features in DDR4/5/6 are actually really nice. The number of channels matters too. This is a whole topic unto itself. The key here is to have at least 32 GB. Size matters. Models like word2vec eat RAM like you would not believe, and loading and storing data, especially image processing data, can be heavy on the RAM and the system bus.
Speaking of the system bus, you also need to have a motherboard with high speed. Obviously, it almost goes without saying that you need SSDs for your hard drives. I’ll leave it at that.
This rig also needs a strong GPU. Think $10,000–20,000 for one card. Obviously Nvidia. Until the VOLTA comes out the best stuff is Tesla. P100 to K40/K80 and others are the sort of GPU you want. Obviously it depends on what you want to do and how much you want to spend. Sometimes multiple GPUs and/or multiple machines are needed. Sometimes but not always, you want a strong CPU like an i7 to complement the GPU computing. This happens when the application already needs lots of CPU, or sometimes there are nice operations live AVX that make sense to leverage in CPU. The GCC compiler targets these instructions for you, and I have been involved in a Java project that uses this type of Single Instruction Multiple Data (SIMD) instruction. Lots of software needs a CPU (e.g. multithreading, hyper-V, …) so an i7/XEON is a good idea to have on your side. If you are going with a server rack, and money is not tight, why not just spend the $30,000 and buy a server. I myself went with a PC-based box for our dev environment, for way less than 30 thousand, but that’s just me.
OK. Let’s get back on track. Next, you need to install lots of frameworks and tools that your project will need. Tensorflow and keras, word2vec and glove. Lots and lots of whatever you need. This would be an AMI on AWS… Faceplant.
I always try and set these systems up with python 2 AND 3 versions of the toolchains, just in case you need either one. After all, this is infrastructure. Some models only work out of the box in python 2. Don’t start with the python subversion thing. 2.7, 3.4, 3.5, 3.6 … Just … Don’t.
I tend to build these systems on Ubuntu server unless the client want something special like CentOS. I also put in LEMP and some other server stuff to communicate with the rest of the client’s infrastructure. Usually this AI machine is one small part serving some larger purpose. Those other internal systems think of this server as an internal service where they submit jobs. It would be great if all the core company code was “AI first,” but most often existing companies want to add AI as a capability rather than overturning the apple cart and rewriting their code.
Once the basics are up and running we need to find machine learning libraries to replace the APIs we would have called. One tiny example is replacing Google vision with a tensorflow CNN like VGG16, inception, squeezenet, etc.
Well, hopefully this post on on-premise machine learning infrastructure setup has been informative.
Happy coding!
-Danieldaniel@lemay.ai ← Say hi.Lemay.ai1(855)LEMAY-AI
Other articles you may enjoy:
Passionate About Machine Learning R&D and Value Creation. ✍ daniel@lemay.ai ⬱ https://lemay.ai
71 
1
71 
71 
1
Passionate About Machine Learning R&D and Value Creation. ✍ daniel@lemay.ai ⬱ https://lemay.ai
"
https://medium.com/google-earth/mapping-species-for-half-earth-321a4b2f7624?source=search_post---------303,"There are currently no responses for this story.
Be the first to respond.
By Jeremy Malczyk, Michelle Duong, Ajay Ranipeta, Chris Heltne, Walter Jetz of Map of Life, Yale University, and the E.O. Wilson Biodiversity Foundation Half-Earth Project
Biodiversity is the variety of life on Earth, the building blocks of functioning ecosystems that provide the natural services on which all life depends, including people. Species, the fundamental units of biodiversity, are in the midst of an extinction crisis, losing ground globally at a rate 1,000 times greater than at any time in human history due to factors like habitat loss and climate change.
How do we stop this? Knowing where species live and the pressures threatening them is paramount in reversing the extinction crisis and maintaining the health of our planet, for ourselves and for future generations. As the impact of humans increasingly encroaches on critical habitats everywhere, determining ‘where’ to protect is just as critical as ‘how much’ to protect.
In his book, Half-Earth, acclaimed biologist Edward O. Wilson proposed a solution commensurate with the problem: conserve half the Earth’s land and sea to protect the bulk of biodiversity from extinction. Scientists agreed that this proposal was both necessary and possible.
“In order to stave off the mass extinction of species, including our own, we must move swiftly to preserve the biodiversity of our planet.” — E.O. Wilson
Born from his book and built on a solid scientific foundation, the Half-Earth Project is working to conserve half the Earth by protecting sufficient habitat to reverse the species extinction crisis and ensure the long-term health of our planet.
The next question the Half-Earth Project needed to answer was, which Half?
Enter Map of Life. Map of Life is a core tool of the Half-Earth Project, which is working to identify and prioritize areas of greatest biodiversity value, and communicate this information in new, dynamic, and engaging ways.
Based out of Yale University and the University of Florida, Map of Life assembles, integrates, and analyzes data on global species distributions. It brings together a wealth of information, assessing information on nearly 100,000 species from hundreds of data sources with multiple data types.
Building on several years of close collaboration with Google, Map of Life leverages Google Cloud Platform services to support biodiversity research, monitoring, education, and decision-making. By leveraging an enormous biodiversity database and a suite of spatial modeling tools, Map of Life is able to capture detailed patterns of species distributions at planetary scale.
Today, the Half-Earth Project Map is using new and existing data and applying cutting-edge capabilities of Google Cloud Platform with the goal of mapping terrestrial, marine, and freshwater species at up to 1 kilometer resolution.
For an initial set of analyses we leveraged the PostGIS suite of spatial functions to measure expected species presence by overlaying species range map polygons with a global grid composed of approximately 110 km x 110 km cells.
Outputs from these intersections are stored on Google Cloud Storage and imported to the BigQuery data warehousing service. The speed at which BigQuery can aggregate across large tables and compute metrics has been vital for analyzing large volumes of biodiversity data. This is increasingly important as the Half-Earth Project continues to add new species groups and generates higher resolution predictions of where species occur. Tables currently in the hundreds of millions of rows may scale to billions or trillions of rows as our taxonomic and spatial resolution increases. The low storage cost and transaction-based pricing BigQuery offers allows us to query and aggregate tables of such size without the maintenance and overhead required by a traditional data warehouse solution.
One aspect of our work measured biodiversity data in two ways: richness and range rarity.
Richness is the number of species occurring, or expected to occur, within a given area. Richness is the simplest way to measure biodiversity.
Range rarity is a continuous metric of range-restrictedness and crucial for considering species with very small ranges that are often of greatest conservation concern. Range rarity is a close proxy for the irreplaceability of a location when the goal is to conserve as many species as possible.
Another aspect of our analysis included estimating the amount of land already protected within any given ca. 100km cells grid cell (mentioned above). To map the protected area network, we filtered the World Database of Protected Areas, which became available as an Earth Engine public table asset in 2017, to remove redundant reserves and so called “paper-parks” that lack on-the-ground biodiversity protection. For areas that lack a geometry and only include a point location and reserve area, we generated a polygon by buffering the point provided to the size of the park. We then computed the area protected within each grid cell, and exported the results to Cloud Storage.
While no habitat loss is ideal, species with larger range sizes can generally afford to lose more habitat than those with smaller ranges. Accordingly, we determined individual species “protected” status based on range size and the proportion of their range that is protected. As the amount of protected area increases, the number of species protected also increases.
But what is good for one species group is not necessarily ideal for another, and what is good for the whole of biodiversity may leave small groups vulnerable.
To address this, we leveraged BigQuery’s support for user defined functions (UDF) and computed adequate protection levels with a simple javascript function. Given the ability to apply this test to each species, we ranked grid cells for each species group and tested to see how many species’ global protections met the criteria via a BigQuery window function run across the ranked grid.
Progress toward Half-Earth will be measured as a running total of conservation protections, with the ultimate goal being half the Earth’s land and sea. BigQuery window functions allow this to be computed quickly for each cell in the grid. As we selected breakpoints up to 50% of the Earth’s area, we tested each species with our function to see whether it met its minimum protected area, then counted the number of species that met the criteria for each step in the scenario.
To ensure rapid map tile delivery to the globe, we generated and exported static tilesets to Cloud Storage using Earth Engine’s Export.map.toCloudStorage() feature. This meant exporting compiled data as CSV from BigQuery, which can be re-joined to the grid Shapefile using OGR command line tools and ingested to Earth Engine as a table asset. From there we were able to visualize and explore the data in the Earth Engine code editor and export tiles once we were satisfied with the appearance.
Finally, with our Cloud Storage bucket populated with map tiles and data to drive charts and infographics, the front-end wizards at Vizzuality plugged in to our API to bring it all to life on the Half-Earth Map.
The Half-Earth Map pieces together species distribution data, the protected areas map, and a mask of human activities into a single map useful to scientists, conservationists, communities, decision-makers and anyone interested in biodiversity and the health of our planet.
For developers, scientists, explorers and storytellers
99 
99 claps
99 
Written by
The whole world in your browser.
For developers, scientists, explorers and storytellers
Written by
The whole world in your browser.
For developers, scientists, explorers and storytellers
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/building-a-slack-reminder-app-with-google-cloud-functions-and-google-cloud-scheduler-4046f4c9c19?source=search_post---------304,"There are currently no responses for this story.
Be the first to respond.
Google Cloud Platform provides awesome tools to help engineers perform automation with ease.
In this article, we will build and deploy a serverless application that sends messages to Slack by leveraging on Google Cloud Function. We will also use Google Cloud Scheduler to periodically run our application at an interval of 3hours.
Google Cloud Functions is a lightweight compute solution for developers to create single-purpose, stand-alone functions that respond to cloud events without the need to manage a server or runtime environment.
Google Cloud Functions can be written in Node.js, Python, and Go, and are executed in language-specific runtimes.
Cloud Functions can be associated with a specific trigger. The trigger type determines how and when your function executes. Cloud Functions supports the following native trigger mechanisms:
Google Stackdriver provides a suite of monitoring tools that help you understand what’s going on in your Cloud Functions. Logs for Cloud Functions are viewable in the Stackdriver Logging UI.
Cloud Function can be used for multiple cases such as Serverless application backends, Real-time data processing systems or Artificial Intelligence applications.
Google Cloud Scheduler is a fully managed, scalable, and fault-tolerant cron job scheduler that allows engineers to automate all their scheduled tasks in one place.
Google Cloud Scheduler allows you set up fully managed scheduled units of work to be executed at defined times or regular intervals with support for Unix cron format.
Cloud Scheduler can be associated with a target which can be either of the following:
In addition, Stackdriver integrates with Cloud Scheduler providing powerful logging for greater transparency into job execution and performance.
Cloud Scheduler can be used for multiple cases such as sending out a report email on a daily basis, updating some cached data every 10 minutes, or making requests to an endpoint.
Sign in to your Slack workspace and Create a new Slack app as follows:
Visit Cloud Functions and Create Function
Paste the following code snippet into the Inline editor and replace the value of url to your Webhook from Slack and Deploy your function.
Once done, you can visit the URL on your Cloud Function page to test.
Visit (Cloud Scheduler)[console.cloud.google.com/cloudscheduler] and Create Job
Great! Your should get the following message from the RestReminder Bot on your Slack Channel every 3 hours.
Thanks for reading through! Let me know if I missed any step, if something didn’t work out quite right for you or if this guide was helpful.
Google Cloud community articles and blogs
94 
94 claps
94 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Software / DevOps Engineer | Google Developer Expert for Cloud | https://timtech4u.dev
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/@yahyaibnmohamed/why-cloud-customers-can-have-it-all-with-ncloud-swiss-9680ba0880ed?source=search_post---------305,"Sign in
There are currently no responses for this story.
Be the first to respond.
Yahya Mohamed Mao
Apr 23, 2018·2 min read
Telecommunications experts came together in Barcelona for the Mobile World Congress taking place at the beginning of this year. As the industry’s premier event, technology companies have here the opportunity of showing off their latest innovations in diverse areas from cutting-edge smartphone technologies over artificial intelligence and cloud computing which is gaining enormously in importance. Considered as the digitization tool par excellence, the cloud’s capabilities are only now beginning to be realized. As business development progresses, the infrastructure must be able to cope with new modern initiatives and steady growth. Cloud computing is thereby the promise of having a modern and state-of-the-art IT infrastructure without the need for substantial capital investments and personnel increases. It is therefore not surprising that the cloud market revenue is expected to double in the next three years reaching up to 162 billion USD.
More than 65% of the cloud computing market is occupied by only a few leading giant providers such as Amazon AWS, Microsoft Azure and Google Cloud. The remaining 35% market share is in the hands of thousands of cloud providers scattered around the world. The question arises as to after which criteria customers decide for a cloud provider. The answer is: They do not decide for one. Many companies pursue a multi cloud strategy to maintain the ability and flexibility to select different cloud services from different providers. In fact, cloud computing is distinguished by a set of deployment models which are private, public and hybrid. Today, there is no single multi-cloud infrastructure vendor with the exception of n’cloud.swiss.
Praised as a Swiss and European alternative to the major cloud providers, n’cloud.swiss is a cloud platform running in one of the world’s most secure data centers in Switzerland. The idea is to enable customers to design a cloud according to their specific requirements with the same product, either as a service model, an on-prem version in existing IT environments or as a hybrid variant. In addition, all cloud service models from Infrastructure as a Service (IaaS), over Platform as a Service (PaaS) to Software as a Service (SaaS) are part of the platform. All this is surrounded by an innovative internal n’cloud.swiss application catalogue. Within the latter, n’cloud.swiss offers more than 142 applications from 30 different IT categories “free and ready to go” as well as the opportunity to upload also other development applications and tools easily.
Along personal support and competitive pricing models, API connectivity for easy and fast transfers of existing developments from or to other major cloud platforms award n’cloud.swiss a unique selling point and a competitive advantage.
Dynamic and data-driven marketing leader. Author, Mentor & Speaker. Top 50 Global Thought Leader. See www.yahyamohamedmao.com
See all (795)
558 

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
558 
558 
Dynamic and data-driven marketing leader. Author, Mentor & Speaker. Top 50 Global Thought Leader. See www.yahyamohamedmao.com
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/developing-on-gcp-getting-started-with-cloud-shell-editor-e074f2fa49d9?source=search_post---------306,"There are currently no responses for this story.
Be the first to respond.
Google Cloud Shell is an interactive shell environment for Google Cloud Platform. It makes it easy for you to manage your projects and resources without having to install the Google Cloud SDK and other tools on your system. With Cloud Shell, the Cloud SDK gcloud command-line tool and other utilities you need are always available when you need them.
Google Cloud Shell provides the following:
In this article, we’ll be using Google Cloud Shell Built-in Editor to modify source codes on a Google Cloud Source Repository (CSR), if you’re not familiar with CSR, check out my article — Managing Source Codes with Cloud Source Repositories 📁
The Built-in Editor is currently in Beta stage and runs Theia, a Cloud & Desktop IDE Platform. It has lots of features as a regular Code Editor.
You can use the code editor to browse file directories as well as view and edit files, with continued access to the Cloud Shell. The code editor is available by default with every Cloud Shell instance.
To easily modify files using the Editor, you can simply Activate Cloud Shell and execute the commands cloudshell edit FILEPATH , in my case, I am editing the ~/.bashrc file.
cloudshell edit ~/.bashrc
You’ll get a prompt to confirm if you want to Open in Editor, on approval, the Editor would be displayed with the file.
Cloud Shell Editor allows us easily modify files on CSR, to do so, navigate to your Cloud Source Repository, and select a repository of choice. Click on Edit Code at the Top-Right Corner of the page
You can also go ahead to perform git operations and run regular terminal commands on the Shell Terminal.
If you’ll like to switch to the Dark theme, on the Menu, Navigate through to File > Settings > Change Color Theme and Select the Dark Theme
If you’re using a Mirrored Cloud Source Repository, you’ll probably get an error like this when you try to push code:
A quick fix is to add the remote URL of the repository you’re mirroring, in my case, I am mirroring a GitHub repository so I’ll run the following commands on my Shell Terminal:
Once that’s done, you’ll be able to push to your main repository.
Also, Google Cloud Shell is free. 🕺🕺
Thanks for reading through! Let me know if I missed any step, if something didn’t work out quite right for you or if this guide was helpful.
Originally published at https://fullstackgcp.com.
Google Cloud community articles and blogs
67 
1
67 claps
67 
1
Written by
Software / DevOps Engineer | Google Developer Expert for Cloud | https://timtech4u.dev
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Software / DevOps Engineer | Google Developer Expert for Cloud | https://timtech4u.dev
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@gmusumeci/how-to-use-gcp-secret-manager-to-manage-your-secrets-using-terraform-488a75a65b28?source=search_post---------307,"Sign in
There are currently no responses for this story.
Be the first to respond.
Guillermo Musumeci
Jun 29, 2020·4 min read
You are going to start building your first application in GCP (Google Cloud Platform) and the question popup into your mind: How am I going to manage my credentials and secrets?
There are several options, however in this story, we are going to use GCP Secret Manager, so we can securely store and access API keys, passwords, certificates, and other…
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/spinnaker-on-gcp-with-gke-edfa994652f7?source=search_post---------308,"There are currently no responses for this story.
Be the first to respond.
After using Kubernetes on Google Cloud Platform (GCP) with Google Kuberenetes Engine (GKE) for a while and I have become very comfortable with rolling out new versions of my applications. Now that I am working with a growing team of developers with a variety of skillsets I knew that it was time to get away from the command line and implement a better release process.
Spinnaker is the easiest way to release complicated pipelines with little-to-no engineering interaction. Originally built by Netflix, Spinnaker has grown to support multiple clouds and a variety of architectures. Where I feel Spinnaker shines though is it’s integration with Kubernetes.
For our environment, I had a very specific list of requirements:
There are many other posts that you can go read more about the background to Spinnaker. In this post, I’m going to go through how I used many of Spinnaker’s features to create a build system that I could be proud of. Along with some commentary, I’ll walk you through the exact steps I took and provide code. There are some very specific places that I got stuck and I’ll include how I got over those issues in this post. Ready, set, GO!
If you’ve read my earlier articles then you know that I am a fan of Helm. When I first started with Spinnaker I saw that there were a few different ways to install, the recommended way being with Halyard, and other options including Helm and deployment manager. Let me just cut to the chase, I’m sure that the other options works, but it was quite the pain and I have a hard time seeing them as great options. After quickly ruling out deployment manager and many hours of back and forth with Helm, I dropped the other options and started with Halyard. Though Halyard was foreign, it is easy to install and easy to understand.
Just use Halyard.
Halyard installs easily on your local machine and remotely connects to the cluster you are running Spinnaker. Now you can install, change settings, and manage your Spinnaker instance from your computer.
For the remainder of this article, I will be going step by step, detailing learnings of Spinnaker and gotchas. I will assume that you have a GKE Cluster already spun up ready for your Spinnaker install. If you need some help creating a Kubernetes Cluster, I recommend checking out some of my earlier articles around creating Kubernetes Clusters on GKE.
medium.com
The first step after installing Halyard is to connect Halyard to your Kubernetes Cluster so you can install Spinnaker. With GCP this task is fairly simple, but it does require that you provide a Service Account to Halyard with the needed permissions to access your Cluster. To do this we will need to access GCP from your local computer, get the Cluster’s .kubeconfig and then provide that to Halyard. Let’s do take it one step at a time. The following code steps through each part.
You may notice that I named my account “spinnaker-account”. This is because I ultimately wanted to have multiple accounts for different Clusters. One Cluster for Spinnaker, and other Clusters for each of my various development environments. This is a personal preference. If you want to add more accounts to your Spinnaker instance then you need to rerun the last few steps for each Cluster as outlined below.
Checkout ~/.halOn your local computer, there is a new folder available that stores all of your Halyard settings. I recommend taking a moment now and finding this file and checking out ~/.hal/config. You’ll need to get used to looking at this config yaml file. If anything gets out of whack, it is here that you’ll be able to read what the actual settings are. Just get used to it.
With your provider accounts setup, we just need to deploy Spinnaker to our Spinnaker cluster and then we are ready to customize our installation.
Give it a moment. It will take a moment but when the dust settles you can go to your Kubernetes Cluster’s workloads and you’ll see the following very satisfying screen.
We can now start customizing Spinnaker to being our very own. Sadly, this is also where I started to hit various problems. Don’t worry, you won’t have the same problems I did, that’s why you’re reading this.
With Spinnaker running our first area of customization is to set where Spinnaker saves its own data. By default, Spinnaker doesn’t include persistence storage, so that is the first thing we need to set up.
To do this the first thing we need to do is create another Service Account that includes permissions to edit Google Cloud Storage buckets and then enable GCS.
You can go back to Google Cloud and slowly watch the update go through. Just be patient, it will take a few minutes. Spinnaker will create the GCS bucket for you if it doesn’t exist already. Basically, this is one less thing to worry about, time to move on to the next step.
For our pipelines, we will need to give Spinnaker the ability to connect to GCS so that we can pull Helm charts, YAML files, and anything else we may store in GCS. Here we go.
Again, the docs handle this explanation really well. This is easy. Now Spinnaker can pull artifacts when necessary.
Okay, this is when things started to fall apart. I was going strong and feeling like I couldn’t make a mistake till I hit this step, but you won’t hit those same problems.
As you can see from the previous diagram, you can see that we need to actually use SSL to secure Spinnaker for incoming to the Spinnaker Ingress. We will be using Cert-Manager for our Spinnaker Cluster.
With one part of the security handled, we need to move on to securing Spinnaker’s ingress. To do this we will need to add Helm into our Cluster so we can install Cert-Manager.
Because this step can be complicated I broke it out into its own article. Go check it out now!
medium.com
Okay, this is where things got really complicated for me. Nothing seemed to be working, certain backend systems were failing, things were going badly. When I looked through my Kubernetes Services I found the Spinnaker Ingress had some failing Backend Services. But why!?!
At this point, I felt that the ability to setup Spinnaker may be beyond me. After plenty of research (and some lucky Google Searches) I found the issue. The load balancer is setup to look for / when the actual healthcheck path that it needs to find is /health once you make the change everything starts to work again. The following images are going to show you how to determine which healthcheck to update and the update necessary.
Once this process is complete you need to give it a few minutes for the health check to stabilize and everything to be ready. Just be patient and come back when your backend services are all green.
Awesome, one step down, just a few more to go. We need to next setup our login methods.
With my system working again it was time to set up authentication so only people in the organization could access Spinnaker. This was pretty simple and the Spinnaker documentation basically handled all of the authentication questions. I put it into one bash script to make it easy.
Now whenever I hit my Spinnaker link only people within my organization can see my pipelines. Very nice.
The last thing I wanted was to trigger my builds whenever a Cloud Build was complete. Again, this is fairly straight forward so I’m just going to share my bash script.
Now whenever Cloud Build completes a build the resulting pubsub message will be picked up by Spinnaker and can be used as a trigger for my pipelines.
Setting up Spinnaker is admittedly not easy. Once done I’ve found it to be the easiest way to automate the deployment of Kubernetes workloads. If you went through all that I just wrote out, congrats, you’ve done Spinnaker the hard way. Recently, Google has added a simpler way to install Spinnaker with GCP. I honestly haven’t tried it so I don’t know if it provides all the features or the ease to manage after the installation but I would check it out if I were to do it all over again before going the hard way. Check it out!
cloud.google.com
Jonathan Campos is an avid developer and fan of learning new things. I believe that we should always keep learning and growing and failing. I am always a supporter of the development community and always willing to help. So if you have questions or comments on this story please add them below. Connect with me on LinkedIn or Twitter and mention this story.
Google Cloud community articles and blogs
31 
3
31 claps
31 
3
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Excited developer and lover of pizza. CTO at Alto. Google Developer Expert.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://towardsdatascience.com/how-to-deploy-a-flask-api-8d54dd8d8b8a?source=search_post---------309,"Sign in
There are currently no responses for this story.
Be the first to respond.
James Briggs
Nov 2, 2021·4 min read
Building that first API is for many of us, a significant step towards creating impactful tools that may one day be used by many developers. But often those APIs don’t make it out of our local machines.
Fortunately, it’s incredibly easy to deploy APIs. Assuming you have no idea what you’re doing right now — you will probably be…
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/simplifying-granular-access-control-on-kubernetes-gke-using-iam-and-rbac-19a627e3aa18?source=search_post---------310,"There are currently no responses for this story.
Be the first to respond.
Google Kubernetes Engine(GKE), a managed Kubernetes Engine provided by Google Cloud Platform is easy to use production grade service that enables to create and run K8s cluster in no time. Access control of GKE is well integrated with Cloud Identity & Access Management (IAM) which abstracts the authorization and authentication to Kubernetes cluster resources.
Few roles available on Cloud IAM for GKE are:
But still Cloud IAM is not sufficient for granular access control because IAM works on project-level which cannot manage access to namespace level. For eg, if we have to grant access for user to view pods’ logs, edit deployment, edit configmap, this level cannot be controlled by IAM. For that, we need RBAC(Role Based Access Control). But if we use Cloud IAM and RBAC combined, we can configure secure way for authentication and authorization.
In this post, I am going to consider situation of granting Kubernetes pod log viewer access to a google account(G Suite or Gmail or Service Account) with Cloud IAM authentication for cluster credentials and RBAC for log only view access to the same account(email).
Currently, GKE has three ways of Kubernetes authentication:
We are dealing with first two ways in this post.
Let’s add a google account to Cloud IAM and give Read-Only access to Kubernetes Engine Resources access.
Likewise, if we need to grant access to service account, we have to create a service account for the project from IAM Service Accounts page and add the role while creating account.
For a google account, gcloud sdk can be authenticated by simple commandgcloud auth login which get credentials through web-based authorization flow.
For service account, we first activate the account
Time to fetch credentials for the running cluster mycluster in us-central1-b zone.
Confirm the cluster access
Role-based access control (RBAC) is a method of regulating access to Kubernetes resources with right permissions based on roles in the cluster. For granting log viewer only access to a user in a namespace, we need to create role and role binding.
Change the name with the google account ID or service account and apply the manifest.
Now, the user can view logs, list pods only.
In this way, we can give granular access to Kubernetes resources to user by integrating with Google Cloud IAM.
Google Cloud community articles and blogs
79 
79 claps
79 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
DevOps | SRE | #GDE
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://developers.ascendcorp.com/%E0%B9%80%E0%B8%A2%E0%B8%B5%E0%B9%88%E0%B8%A2%E0%B8%A1%E0%B8%8A%E0%B8%A1%E0%B8%AD%E0%B8%AD%E0%B8%9F%E0%B8%9F%E0%B8%B4%E0%B8%A8%E0%B9%83%E0%B8%AB%E0%B8%A1%E0%B9%88%E0%B8%82%E0%B8%AD%E0%B8%87-google-asia-pacific-%E0%B8%81%E0%B8%B1%E0%B8%99%E0%B8%84%E0%B8%A3%E0%B8%B1%E0%B8%9A-da45ccf27b13?source=search_post---------311,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
พอดีที่ Google Asia Pacific มี Training ในหัวข้อเรื่อง Google Cloud Platform Big Data & Machine Learning Fundamentals ทาง Google เลยชวนทีมงานของ Ascend Analytics Hub ไปเรียน เพราะเราใช้ Google BigQuery เป็น Cloud Data Warehouse สำหรับงานทาง Business Intelligence และ Analytics มาประมาณ 1–2 ปีได้แล้ว แต่ก็เป็นแบบ trial-and-error คือ ลองผิดลองถูกกันมาเอง ผู้ใหญ่ใจดีอย่าง Ascend ก็เลยตกลงปลงใจส่งพวกเราไปเรียนให้รู้พื้นฐานกันเป็นเรื่องเป็นราว
เราไปเรียนกัน 2 คอร์ส ก็คือ CPB100: Google Cloud Platform Big Data & Machine Learning Fundamentals และ CPB200: BigQuery for Data Analysts คอร์สแรก 1 วัน คอร์สหลัง 2 วัน เช้ายันเย็น มีทั้ง Lecture และ Lab คนสอนเก่งมาก สุดแสนประทับใจ แต่ที่ประทับใจมาก จนอยากเอามาเล่าก่อน ก็คือ ออฟฟิศใหม่ของ Google Asia Pacific ใหญ่โตโอ่โถง มีสภาพแวดล้อมที่ทำให้คนอยากมาทำงานมากกกกกก (ก ไก่ ล้านตัว)
ออฟฟิศ Google Asia Pacific ตั้งอยู่ที่ 70 Pasir Panjang Road, #03–71, Mapletree Business City, Singapore 117371 ซึ่งย่านนี้ก็จะมีบริษัท IT อื่นๆ อยู่ใกล้เคียง เช่น SAP หรือ HP
ทางขึ้นของแขกผู้มีเกียรติ จะอยู่แยกกับพนักกงานนะครับ ขึ้นมา เราจะรออยู่ในส่วนของ Lobby เพื่อรอให้ Host เรามาพาไปยังห้อง Training ก็ค่อนข้างมี Security แน่นหนานะครับ มี Security Guards คอยสอดส่องดูความปลอดภัย โดยพี่ Security นี้ ก็จะแต่งกายด้วยเสื้อโปโลธรรมดานี่แหละครับ
ออฟฟิศก็ออกแบบดูทันสมัย โปร่งสบาย ไม่อึดอัด สว่าง บรรยกาศเหมือน Campus มากๆ ครับ
ห้องนี้เป็น ห้อง Brainstorm ก็จะไม่มีเก้าอี้นั่งครับ มีแต่ Whiteboard ที่เอาไปแขวนตรงไหนก็ได้ ชอบมาก อยากได้ซักห้องไว้ที่ออฟฟิศ
ห้อง Training ครับ มี Food Corner ด้วย ติดกับห้อง Training มีห้องยิม ไว้ให้เล่า Googler มาออกกำลังกายกัน ตอนเช้า เราก็ Skip Breakfast ที่โรงแรม มากินข้าวเช้าที่ Google … French Toast อร่อยมากกกกกก
เค้าบอกว่าอาหารที่ Google ก็จัดโดยนักโภชนาการ แล้ววิธีการจัดวาง ก็จะจัดให้อาหารที่ Healthy นั้น Visible กว่าอาหารที่ไม่ Healthy พวกน้ำเปล่า ผลไม้ ก็จะอยู่ด้านบนของตู้แช่ แต่พวกน้ำอัดลม จะอยู่ด้านล่าง ซึ่งจะมีสติ๊กเกอร์ขุ่นแปะไว้ ไม่ให้เห็น
นอกจากนี้ ก็ยังมีพวก Snacks ต่างๆ เต็มทุกเก๊ะเลย ถ้าเก๊ะบนๆ ก็จะเป็นขนมที่ Healthy สุดชีวิต มี Protein Bars ยกเวทเสร็จ กระดกนม แล้วกิน Protein Bars เป็นอันจบเลย
ส่วนพวก Sneakers อะไรพวกนี้ ก็มีนะครับ อยู่ล่างสุดเลย แอบซ่อนไว้
มาดูอาหารเที่ยงกัน ยิ่งใหญ่อลังการดั่งไลน์บุฟเฟต์ในโรงแรมหรู ก็จะมี Salad Bar, Indian Food, Chinese Food, International Food เป็นต้น มีซุป ผลไม้ หรือ ถ้าเป็นคน Healthy มาก ก็มีเครื่องสกัดน้ำผลไม้แยกกากไว้ให้ใช้ด้วยครับ
ที่ฟินมาก คือ มีฝรั่ง กับ ผงบ๊วย … ฟินเป็นการส่วนตัว
ทาง Googlers ก็พาเราเดินทัวร์ออฟฟิศในวันสุดท้ายก่อนกลับเมืองไทย มี Section หนึ่งเป็น Wellness ครับ (เอ๊ะ หรือ Well-being พอดีมี Memory Problem เล็กน้อย) ก็มีทั้ง Massage Room, Salon, Nap Room ทำเล็บ ทำผมกันไป
และก็มี Rooftop Garden ครับ สวนสวย ลมเย็นสบาย เช้าๆ ฝรั่งก็ออกมาวิ่งจ๊อกกิ้งกัน
มี Coffee Bar ครับ มี ฺBarista คอยทำกาแฟสุดอร่อย ให้เรามีเรี่ยวแรงทำงานต่อไป
ในห้องน้ำ ก็จะมีน้ำยาบ้วนปากไว้ แต่ที่พิเศษ คือ มีหัวปั๊ม และ มีกรวยกระดาษไว้ให้ด้วย
อยากมาทำงานที่ Google กันยังครับ ^^
Creating opportunities for all through world-class digital…
10 
1
10 claps
10 
1
Written by
Lecturer at Management of Analytics and Data Science Program, National Institute of Development Administration, Thailand and Data Analytics Consultant
Creating opportunities for all through world-class digital platforms & services
Written by
Lecturer at Management of Analytics and Data Science Program, National Institute of Development Administration, Thailand and Data Analytics Consultant
Creating opportunities for all through world-class digital platforms & services
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://itnext.io/infrastructure-as-code-using-pulumi-to-provision-and-bootstrap-a-gcp-instance-318f06c61a03?source=search_post---------312,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
This post will show you how to use Pulumi as the infrastructure-as-code tooling for provisioning an instance in the Google Cloud Platform. I will use Python for the Pulumi code examples. I ran the examples below on MacOS.
I used brew to install the pulumi CLI:
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/komiser/komiser-detect-potential-cost-savings-on-gcp-406136328334?source=search_post---------313,"There are currently no responses for this story.
Be the first to respond.
I’m super excited to annonce the release of Komiser:2.1.0 with beta support of Google Cloud Platform. You can now use one single open source tool to detect both AWS and GCP overspending.
With the GDPR becoming real in EU, logging and storage of (potentially) personally identifiable information now need to be reduced in many organizations. Komiser allows you to analyze and manage cloud cost, usage, security, and governance in one place. Hence, detecting potential vulnerabilities that could put your cloud environment at risk.
It allows you also to control your usage and create visibility across all used services to achieve maximum cost-effectiveness and get a deep understanding of how you spend on the AWS, GCP and Azure.
Below are the available downloads for the latest version of Komiser (2.1.0). Please download the proper package for your operating system and architecture.
Note: make sure to add the execution permission to Komiser chmod +x komiserand update the user’s $PATH variable.
Komiser is also available as a Docker image:
Note that we need to provide the three environment variables AWS_DEFAULT_REGION, AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY set in the container such as that the CLI can automatically authenticate with AWS.
Create a service account with Viewer permission, see Creating and managing service accounts docs.
If you point your favorite browser to http://localhost:3000, you should see Komiser awesome dashboard:
The versioned documentation can be found on https://docs.komiser.io.
Komiser is written in Golang and is MIT licensed — contributions are welcomed whether that means providing feedback or testing existing and new features.
Drop your comments, feedback, or suggestions below — or connect with me directly on Twitter @mlabouardy.
Analyze and manage cloud cost, usage, security, and…
120 
120 claps
120 
Analyze and manage cloud cost, usage, security, and governance in one tool.
Written by
CTO & Co-Founder @Crew.work — Maker & Indie Entrepreneur @Komiser.io, Writer/Author and Speaker — Blog: https://labouardy.com
Analyze and manage cloud cost, usage, security, and governance in one tool.
"
https://blog.lelonek.me/how-to-configure-google-kubernetes-engine-using-terraform-cccb67625946?source=search_post---------314,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
In this article, I’d like to go through my minimal setup of Google Kubernetes Engine (GKE) with infrastructure as code (IaC) in Terraform.
Before doing that, I want to define some concepts of what we are planning to do. On the other hand, I assume you are already pretty familiar with all of them, as they are quite advanced from my perspective.
We will combine all the above to eventually have k8s running on GCP.
In the file below, I present you the entire config for the kubernetes cluster in Google Cloud Platform. For now, I will skip all additional configuration (e.g. terraform setup or gcloud CLI installation and authentication).
Have a look at how our YAML is organized:
At the very beginning, we are enabling container.googleapis.com Service API which is required to build and manage container-based applications powered by the kubernetes technology.
Once we have that, we are defining the actual k8s cluster. We provide:
There’s nothing more than we need. We are ready to provision our infrastructure and have k8s cluster up and running!
However, if you want to see the entire repository, it is available here:
github.com
You will find there more fine-grained setup, configuration options and detailed description explaining step-by-step what, why and how to do everything that is needed to deploy the entire infrastructure.
Subscribe to get the latest content immediatelyhttps://tinyletter.com/KamilLelonek
With the knowledge you’ve just get, you shouldn’t have any problems with provisioning your Google Cloud Platform with infrastructure as code using Terraform to have there kubernetes cluster up and running.
You can start using it right away and deploy your first containers there. It is as simple as that so go ahead and try it yourself.
If you’ve been interacting with some advanced setups previously, you may have noticed we omitted a couple of components usually used in more complex systems. I haven’t introduced them for the sake of simplicity and to limit the new things you would have to get familiar with.
I definitely skipped networking in this setup. Not only didn’t we use any specific subnetwork for the cluster, but also we provided only a default one without any possibility to have external connections available. Moreover, I also omitted some specific node configuration including node pool associated with a cluster.
Nevertheless, everything works fine for our purposes and, until you interact with more composite services, this config is sufficient to be used as is.
Lean Programming | Software Development | Project…
79 
Thanks to Sebastian Muszynski. 
79 claps
79 
Written by
https://lelonek.me/ | https://tinyletter.com/KamilLelonek https://blog.arkency.com/authors/kamil-lelonek/ https://www.linkedin.com/in/KamilLelonek/
Lean Programming | Software Development | Project Management
Written by
https://lelonek.me/ | https://tinyletter.com/KamilLelonek https://blog.arkency.com/authors/kamil-lelonek/ https://www.linkedin.com/in/KamilLelonek/
Lean Programming | Software Development | Project Management
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@kamilklkn/amazaon-web-services-aws-nedir-ef733ecf6292?source=search_post---------315,"Sign in
There are currently no responses for this story.
Be the first to respond.
Kamil Kalkan
Jan 20, 2016·2 min read
Amazon web services yani AWS dünyanın en hızlı büyüyen bulut hizmetlerinin genel adıdır. Google(Cloud Platform), Microsoft(Azure), IBM(SoftLayer, Bluemix) gibi rakipleri olsa da, şu anlık bu alanın büyük abisi AWS.
Tüm Cloud sistemlerde olduğu gibi, AWS içindeki tüm modülleri ayrı ayrı, ama bir araya geldiğin de performaslı çalışacak şekilde tasarlamış. Hepsinin ayrı birer modül olması sistemin ölçeklenebilirlik oranını sağlıyor.
Ölçeklenebilirlik ne kadar önemli olduğunu anlamanız için Onur Dayıbaşı kendi medium bloğunda çok güzel açıklamış.
ÖSYM sınavı sonrasında sınavlar açıklandığında öğrencilerin sonuçlarını öğrenmek için sisteme girdiğinde Genel’de sistem çöker ve cevap veremez veya Üniversite öğrencilerinin ders seçme zamanlarında sistem çöker ve ögrenciler gece boyunca o sistem başında beklerler.Problem sistemin bir anda bu kadar çok öğrenciyi kaldıramamasıdır. Halbuki yılın diğer zamanlarında bu tip ihtiyaçlara gerek duyulmamaktadır. Bu kadar büyük durumlar için sistemlerin tasarlanması yılda gerçekleşecek 1 haftalık bir durum için tüm sistemin boşta yatması anlamına gelir.Amazon veya Alışveriş sitesinin yılbaşında, sevgililer gününde çok ziyaretçi alma durumlarına göre otomatik olarak büyümesi daha sonrasında kullanımının az olduğu dönemlerde otomatik olarak küçülmesidir.
Amazon S3 (Simple Storage Service) de dosyaları, HTML sayfaları, resimler veya aklınıza gelebilecek dosyaları, nesneleri depolamak için Amazon AWS içinde sunulan bir modül. Amazon S3 üzerinde ister public ister private olarak dosya barındırabilirsiniz.
Amazon Elastic Compute Cloud (EC2) ile kendi sanal sunucu(cluster)’larınızı oluşturma olanağı sağlayan web servisidir. EC2 Bize sanal sunucu Oluşturma, EBS Yönetimi, sunucu güvenlik grubu yapılandırmalarını kolaylıkla yapacağımız bir arayüz sunmakta. Eğer free olarak kullanacaksanız t2 mikro tam size göre. Sanal makineye ssh ile bağlanıp kullanabiliriz.
AWS de veri tabanının tutulduğu sanal server(instance) diyebiliriz. Yönetimi en güc olan mödül çünkü otomatik ölçekleme(auto scale) özeliği yok. Tamamen sizin manuel ayarlama yapmanız gerekmekte. Tabiki bunu kullanmak zorunda değilsiniz. Direk EC2 üzerine MySql benzeri kurulumları yapıp, oradanda devam edebilirsiniz fakat böyle bir niyetiniz varsa AWS kullanmak çokta mantıklı olmadığını düşüncesindeyim.
Not: Bu yazı daha önce kamilklkn sitesinde yayınlanmıştır.
Designer | Developer | UX Enthusiast | • Creative Problem Solver. — Everyone has a story to tell. https://linktr.ee/kamilklkn
23 
1
23 claps
23 
1
Designer | Developer | UX Enthusiast | • Creative Problem Solver. — Everyone has a story to tell. https://linktr.ee/kamilklkn
About
Write
Help
Legal
Get the Medium app
"
https://larrylu.blog/gcp-datastore-crud-api-server-109012071b16?source=search_post---------316,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
作者我對 GCP（Google Cloud Platform）提供的服務一直都滿有興趣，最近幾天用了 GCP 的 Cloud Datastore 跟 Node.js 實作了一個有基本 CRUD 功能的 API server，趁我還記得的時候趕快寫文章跟大家分享一下
Cloud DataStore 是一個具高度擴充性的 NoSQL 資料庫，他會自動備份資料確保資料完整性，而且資料一旦寫入之後在任何地方都會讀到最新的資料，不用擔心 consistency 的問題，資料量大的話還會自己做 scale，所以不管是要做大規模的系統或是小型應用都很適合，想看更多介紹的話可以到這裡
Cloud Datastore 是 GCP 提供眾多服務中的其中一個，要開始使用 GCP 的話可以點網站右上角的「免費試用」，點進去填完資料之後就可以獲得一年內 300 美元的試用額度
開始免費試用之後接著就是要新增專案，我這邊新增了一個專案叫做 nodejs-crud，他會隨機給你一個專案 ID，像我這邊就是 nodejs-crud-187502
填完之後就按下建立，之後到專案內的資訊主頁
建立好專案之後，可以點左邊的資料儲存庫，然後建立實體
點了建立實體之後，他會讓你選擇你的資料要放在哪裡，選一個離你比較近的地方就可以了，選完位置之後會要求你填一些東西，這邊都不用填，只要在「種類」那邊填上 todo，這樣我們就建立好我們的 datastore 實體了 🎉
有了 Datastore 實體之後，我們還需要一個 key 才可以用程式去存取我們的 Datastore，所以需要建立一個服務帳戶
到服務帳戶頁面之後點擊「建立服務帳戶」，帳戶名稱 admin，角色選擇「Cloud Datastore Owner」，然後記得把「提供一組新的私密金鑰」打勾，建立之後我們就會拿到一組 keyfile，這個 keyfile 可能會叫做 nodejs-crud-xxxxx.json，記得把它留好，不小心外洩出去的話別人就可以存取你 datastore 裡面的資料
GCP 上的設定就到此告一段落，接下來就要開始寫扣實作出 API server，因為我是用 Node.js 實作，會用到 google-cloud-node 這個 library，比較熟悉 python 的人可以用 google-cloud-python，或是其他語言也可以找到相對應的 library
先寫一小段扣把資料存進去再拿出來，確認我們可以存取 datastore，記得把 projectId跟 keyFilename 換成自己的 ID 跟 keyfile 路徑，懶得自己動手寫的話可以直接 clone 這個 repo 下來跑
如果剛剛設定都沒問題的話應該會輸出 I am data，確認之後就開始實作 API server，這邊會用到 Node.js 的 express，對 express 這個框架不熟的話也可以用自己熟悉的，先把 API 格式都開出來，接著就是慢慢把 CRUD 實作完，這邊有完整的 source code
要新增一個 item 首先要先讀取 body，隨機產生一個 id 之後把 body 當做內容存到 datastore，這裡會使用 uuid 產生 id，存完內容之後再把 id 丟到前端，詳細的 datastore API 可以參考 datastore.save
在 datastore 中不論讀寫都是使用 key，只要用同樣的 key 就會讀到一樣的東西，所以如果要根據 id 讀取的話要先用 id 生成一個 key，再用 key 去讀資料。另外，因為 datastore.get 一次可以讀好幾筆資料，我們只需要第一筆，所以只回傳 results[0].content
在 datastore 的操作中沒有真正的 update，雖然有提供 datastore.update 這個方法可以使用，但他其實是 map 到 datastore.save，所以沒有真正的 update，要更新一筆資料，就直接把新的資料存到同樣的 key 中就可以了
delete 的話大概是 CRUD 裡面最簡單的，直接使用 datastore.delete，把 key 傳進去就可以刪掉了～
不知道大家做完有沒有一種很輕鬆的感覺 XD，我自己是覺得還滿方便的，因為以往如果要用資料庫的話還要自己架，而且常常會遇到 server 上的 mongo 沒裝好或跑不起來等情況，用 GCP 的 datastore 就沒這個困擾
但方便也是有代價的，就是要花 $$$，不過我自己算一算其實不貴，如果只是做小專案的話還可以用它的永久免費額度，大型系統的話雖然要花錢，但因為他會 auto scale 而且保證 consistency，用它應該也可以省下不少維護上的成本，想自己算算看的話可以試試 GCP calculator
希望大家喜歡這次 Google Cloud Datastore 的分享，以後如果有機會也會再介紹 GCP 上的其他服務
我是 Larry Lu，我熱愛技術、喜歡與人分享，專長是 Web 前後端還有 CI/CD，歡迎大家來跟我聊聊技術～
83 
83 claps
83 
Written by
我是 Larry 盧承億，傳說中的 0.1 倍工程師。我熱愛技術、喜歡與人分享，專長是 Javascript 跟 Go，平常會寫寫技術文章還有參加各種技術活動，歡迎大家來找我聊聊技術～
我是 Larry Lu，我熱愛技術、喜歡與人分享，專長是 Web 前後端還有 CI/CD，歡迎大家來跟我聊聊技術～
Written by
我是 Larry 盧承億，傳說中的 0.1 倍工程師。我熱愛技術、喜歡與人分享，專長是 Javascript 跟 Go，平常會寫寫技術文章還有參加各種技術活動，歡迎大家來找我聊聊技術～
我是 Larry Lu，我熱愛技術、喜歡與人分享，專長是 Web 前後端還有 CI/CD，歡迎大家來跟我聊聊技術～
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-developers/the-developer-show-004-android-for-work-chrome-dev-summit-web-developer-training-167fc540d97a?source=search_post---------317,"There are currently no responses for this story.
Be the first to respond.
Engineering and technology articles for developers, written…
11 
11 claps
11 
Engineering and technology articles for developers, written and curated by Googlers. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Developer Advocate for Google. Improving life through science and art.
Engineering and technology articles for developers, written and curated by Googlers. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://itnext.io/gcp-command-line-cheatsheet-5e4434ca2c84?source=search_post---------318,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
A cheatsheet with various commands for the Google Cloud Platform (GCP) command-line tool (gcloud).
I plan to extend this list further as I encounter more commands.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/geekculture/polygon-news-google-cloud-offers-blockchain-insights-for-the-matic-network-902b415dc428?source=search_post---------319,"There are currently no responses for this story.
Be the first to respond.
Blockchain data for Polygon, an Ethereum scaling solution, has arrived on Google’s cloud platform.
Integration with Google BigQuery allows developers to analyze on-chain data on Polygon (formerly Matic) more efficiently. With BigQuery support, Polygon’s datasets are now listed in the Google Cloud Marketplace under the public financial services category.
The potential benefits of the integration include monitoring gas charges and smart contracts and identifying the most popular…
"
https://medium.com/@ratrosy/set-up-serverless-store-part-1-auth-storage-event-streaming-and-third-party-apis-c4274a408574?source=search_post---------320,"Sign in
There are currently no responses for this story.
Be the first to respond.
Ratros Y.
Jan 24, 2019·6 min read
This document is a part of the Serverless on Google Cloud Platform: an Introduction with Serverless Store Demo How-to Guide. It discusses the authentication, storage, event streaming solutions, and third-party APIs Serverless Store uses.
Serverless Store uses two Google Cloud Platform solutions, Google Cloud Storage and Google Cloud Firestore, for storage.
Google Cloud Storage is a unified storage for objects with global caching; in Serverless Store you will use it to store product images. The API is enabled by default when you create the Google Cloud Platform; follow the steps below to upload an image to Cloud Storage, which we will use in later steps:
Additionally, you will need to enable public access to the bucket:
When users create a new product, the Filepond script reads an image from their local drives (see app/static/enableFilepond.js for configuration), send it to a Cloud Function upload_image. The function processes the data with ImageMagick, then saves it to Cloud Storage:
Cloud Firestore is a NoSQL database. If you are not familiar with the concept, think of Cloud Firestore as a large-scale dictionary where data is stored in key-value pairs. The solution provides cheap, blazing fast storage for structured data, though it lacks the flexibility of traditional SQL databases. In Serverless Store you will use it to store structured data, such as product details and order information.
To set up Firestore:
Document is the unit of data storage in Firestore, which can be grouped into collections. It is schemaless, which means that you do not have to specify the structure of data beforehand. The steps below create a document in the Products collection, which we will use later:
Additionally, set up a few indexes to help with the query performance:
Cloud Firestore is essentially the database backend of Serverless Store. It is used throughout the code; the app/helpers/product_catalog/helpers.py file, for example, includes a collection of helper methods for product related operations, such as getting a product from Cloud Firestore and listing all products from Cloud Firestore:
Serverless Store uses Firebase Authentication. Firebase is a Google developed mobile/web app development platform that solves common app development challenges, such as authentication, analytics, storage, etc. In the Serverless Store demo app you will only use its authentication solution, more specifically, its Sign-in With Google support.
The Firebase Authentication integration has the following workflow:
The system returns with a token (JSON Web Token, JWT) if authentication completes successfully. The token is saved in the browser cookie and passed to Serverless Store in every request; it is a secure proof of identity that can be verified easily with Firebase Admin SDK. Firebase keeps profiles of authenticated users in the database for your future reference.
To set up Firebase Authentication:
When users open the /signin page, the app/static/signInOutWithGoogle.js script handles all the authorization and authentication workload. It redirects users to Google and collects the data and writes the token to the browser cookie when users successfully sign in:
For verification, the flask app uses Firebase Admin SDK. app/middlewares/auth.py includes two wrappers, auth_required and auth_optional; they are responsible for decoding the token, passing the authentication context (user ID, username, and email) to the app, and redirecting customers without valid tokens back to the homepage (if applicable).
Serverless Store streams events via Google Cloud Pub/Sub. Cloud Pub/Sub takes events (or any type of data) from an arbitrary number of sources (publishers) and delivers them to an arbitrary number of destinations (subscribers). It scales automatically, requires little configuration, and promises at-least-once delivery.
To set up Cloud Pub/Sub:
In Cloud Pub/Sub, events are published to a Pub/Sub topic. The following steps specifies how to create a topic, publish some data to it, then retrieve the data back.
Now you can try Cloud Pub/Sub out:
You should see an output, Hello World!, in the terminal.
Create the following topics, which Serverless Store uses:
In the flask app, the stream_event method in app/helpers/eventing/helpers.py is responsible for publishing events:
Many Cloud Functions subscribe to these events, which you will set up in the later parts.
Serverless Store uses Stripe and SendGrid for payment processing and emailing respectively (you can set up Google Pay for payment as well). To use these third-party APIs:
Payment is an asynchronous operation in Serverless Store: when the app receives a request, it publishes an order_created event via the stream_events method above, in the payment-process topic (app/blueprints/charge/blueprint.py):
The pay_with_stripe Cloud Function subscribes to the topic, listens to the events, and call the Stripe API to process the payment:
The asynchronous nature allows the demo app to process payments as fast as possible, even in the busiest holiday seasons. When the payment is processed, pay_with_stripe function will publish a payment_processed (or payment_failed) event to the payment-completion topic. Cloud Function sendOrderConfirmation (functions/sendOrderConfirmation/index.js) subscribes to the topic, listens to the events, and call the SendGrid API to send notification emails:
sendOrderConfirmation prepares order confirmation emails using pug templates from Cloud Firestore. Create an emails collection in Cloud Firestore and add the following documents:
Continue the setup in Set up Serverless Store: Part 2.
Developer Relations @ Google Cloud Platform
18 
18 
18 
Developer Relations @ Google Cloud Platform
"
https://medium.com/google-developers/the-developer-show-tl-dr-030-20fc0d2dda46?source=search_post---------321,"There are currently no responses for this story.
Be the first to respond.
Highlights: Android N APIs, Android Security Rewards, Google Cloud Platform and Location, Google Cloud Platform CLI, Google Sheets API v4, Grow Revenue with AdMob
The Developer Show is where you can stay up to date on all the latest Google Developer news, straight from the experts.
Have a question? Use #AskDevShow to let us know!
The Android N APIs are now final! It’s time to get your apps ready, so if you haven’t already, download the final SDK for Android N through the SDK manager in Android Studio. Also, you can now publish updates compiling with, and optionally targeting, API 24 to Google Play. Which means you can start testing your app on the alpha, beta, or even production channels with the Google Play Developer Console. Hit up the post for all the links.
One year ago, we added Android Security Rewards to the long standing Google Vulnerability Rewards Program. Since then, we’ve paid over half a million dollars to eighty two researchers. And you know what, for reports filed after June 1st, we’re starting to pay more. The post has all the details.
Have you ever wanted your own scalable location analysis platform? Well, check out the post for a full tutorial and docker images for one built with The Google Cloud Platform and Google Maps APIs.
The gcloud tool is your gateway to manage and interact with the Google Cloud Platform from the command line. Check out the post for some details including a demonstration of filters, formatting, and projections.
At Google I/O 2016, we launched a new Google Sheets API. The update includes new features including access to functionality found in the Sheets desktop and mobile user interfaces. Follow the link for an introduction and some code.
Within one month of implementing AdMob native ads, Holaverse saw its total revenue increase by 10%. From just one native ad slot, they earned over four thousand dollars in a single day. To learn more, read the full case study.
Engineering and technology articles for developers, written…
23 
23 claps
23 
Engineering and technology articles for developers, written and curated by Googlers. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Developer Advocate for Google. Improving life through science and art.
Engineering and technology articles for developers, written and curated by Googlers. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/@educative-inc/cracking-the-gcp-certification-exam-how-to-prepare-c07f612065b9?source=search_post---------322,"Sign in
There are currently no responses for this story.
Be the first to respond.
The Educative Team
Jun 25, 2021·7 min read
Google Cloud Platform (GCP) is a suite of cloud services that run on the same infrastructure. GCP offers a range of services such as cloud storage, database management, developer tools, and more. Companies can improve their own products, services, and technologies by using the Google Cloud Platform. With cloud skills in such high demand, a Google Cloud certification is a great way to boost your resume and help you stand out from the competition. Today, we’re going to dive deeper into the topic of Google Cloud certifications and discuss benefits, exam questions, preparation, and more.
We’ll cover:
A Google Cloud certification demonstrates that you have experience with Google Cloud technologies. It shows hiring managers that you can maintain and implement Google Cloud products, services, and technologies into a company’s workflow.
The demand for cloud skills is high and will only continue to grow. Over 80% of hiring managers say that cloud certifications make applicants more desirable. A certification will also expand your career opportunities and most likely result in a significant pay increase.
Here’s what you need to do to get Google Cloud certified:
A Google Cloud certification will only be attractive to companies that actually use (or plan to use) the technologies. Microsoft Azure and Amazon Web Services (AWS) have a larger market share in the public cloud than Google Cloud, so getting a certification from one of those companies may be better depending on what the hiring company uses.
Although Google Cloud is behind Azure and AWS in market share, it’s used by many Fortune 500 companies because of its simplicity and great features. Google’s Professional Cloud Architect certification is currently the highest-paying cloud certification available.
Tip: If you have experience with other cloud services, you may consider implementing a multi-cloud infrastructure.
Google Cloud currently offers ten certificates that fall under three different levels: foundational, associate, and professional. Each level of certification is geared toward a different type of user and requires different background knowledge and experience. Let’s take a look at the certificates!
Foundational-level certifications confirm that you have general knowledge of cloud concepts and Google Cloud technologies, features, services, capabilities, and more. The certificate available at this level requires no prerequisites with Google Cloud. This certification level is ideal for you if you’re in a non-technical position and want to add value to your organization by learning more about Google Cloud.
Foundational certification:
Foundational Cloud Digital Leader
A Cloud Digital Leader knows and can explain the abilities of Google Cloud’s main products and services and can explain how cloud solutions support businesses.
This is a great certification for you if you:
Associate-level certifications focus on the core skills of monitoring, deploying, and maintaining projects on Google Cloud. The certificate available at this level recommends you have at least 6+ months of experience building on Google Cloud. This certification level is ideal for you if you’re newer to the cloud and want to use this learning path to eventually transition to professional-level certifications.
Associate certification:
Associate Cloud Engineer
An Associate Cloud Engineer deploys apps, controls operations, and handles business solutions. An Associate Cloud Engineer can also use Google Cloud Console and the command-line interface to execute tasks.
This is a great certification for you if you:
Professional-level certifications cover core technical job duties and evaluate developed skills in design, implementation, and management. The certificates available at this level recommend you have at least 3+ years of industry experience with at least 1+ years of experience on Google Cloud. The certifications at this level are ideal for you if you have industry experience and experience with Google Cloud products and solutions.
Professional certifications:
Professional Cloud Architect
A Professional Cloud Architect helps organizations leverage Google Cloud services and technologies. They have a strong understanding of cloud architecture and can create and manage strong, safe, scalable solutions to support business goals.
This is a great certification for you if you:
Professional Cloud Developer
A Professional Cloud Developer builds, tests, and deploys scalable cloud-native apps using Google’s practices, technologies, and tools.
This is a great certification for you if you:
Professional Data Engineer
A Professional Data Engineer makes data-driven decisions by performing data analytics and creates, builds, secures, and monitors data processing systems with a focus on security and compliance. They also deploy and train machine-learning models.
This is a great certification for you if you:
Professional Cloud DevOps Engineer
A Professional Cloud DevOps Engineer handles development operations that balance speed and reliability. They use Google Cloud tools and technologies to build software delivery pipelines, deploy services, and track incidents.
This is a great certification for you if you:
Professional Cloud Security Engineer
A Professional Cloud Security Engineer helps organizations build and implement secure infrastructures using Google’s security technologies.
This is a great certification for you if you:
Professional Cloud Network Engineer
A Professional Cloud Network Engineer enforces and maintains cloud network architecture.
This is a great certification for you if you:
Professional Collaboration Engineer
A Professional Collaboration Engineer uses business objectives to create and implement policies, practices, and configurations. They use their knowledge of mail routing and identity management to support secure data access and communication.
This is a great certification for you if you:
Professional Machine Learning Engineer
A Professional Machine Learning Engineer builds and uses machine learning models to solve business problems. They work with other job roles to collaborate on development processes and successful model solutions.
This is a great certification for you if you:
Note: For more in-depth information about each certificate, you can visit Google’s Cloud certification site. Each certification provides exam guides where you can view topics and case studies that may be included in the exam.
There are many jobs that benefit from a cloud certification. Let’s take a look at some of the most in-demand, along with their average salaries:
Note: Cloud certifications can help even non-cloud engineers, IT professionals, and others stand out from other applicants.
The exam question types vary in form and subject. Questions can be multiple-choice, coding questions, or case studies. Depending on your certification, you could answer questions on subjects like:
The Google Cloud certification exams are known to be difficult and rigorous, so it’s very important to engage in certification training prior to taking your exam. The exam doesn’t provide you with a score at the end, only a Pass or Fail result. These results don’t provide you with much insight into which areas you performed well in and which areas you need to improve in. This is a major reason why you need to take your preparation seriously.
Let’s discuss some ways you can do some Google Cloud training and prepare for your exam:
When studying theory, Google Cloud documentation is a solid resource. Dedicate time to studying and understanding the How-to and Concepts sections of the documentation.
For the practical skills, you’ll have to practice often and thoroughly. Use the Google Cloud Platform practice exams, and check out other resources, tutorials, and hands-on labs available online.
Read through the featured products and use the exam guides to determine which ones will be relevant to your preparation. Read the documentation for those products and focus on developing a strong understanding of them.
For the Professional Cloud Architect exam, Google provides you with sample case studies that you can use for practice. Read through the case studies and try to create solutions for them.
Google Cloud Platform provides you with sample questions that help familiarize you with the format and potential content that may be covered on your exam. It’s important to note that the sample questions don’t claim to fully represent the range of topics or level of difficulty of the actual exam. You shouldn’t use your results from the sample questions to predict your exam result.
The questions you’ll practice will depend on the certification you plan to get. We took a look at the certification exam guides and pulled some questions from them. Let’s take a look:
Cloud skills are in high demand and that demand continues to grow. A Google Cloud certification will demonstrate to potential employers that you have proficiency in the Google Cloud Platform. The certification exams are known to be difficult, so it’s important to prepare with the best resources available.
Happy learning!
Coding is like skateboarding: you can’t learn new skills just by watching someone else. Master in-demand coding skills through Educative’s interactive courses.
12 
12 
12 
Coding is like skateboarding: you can’t learn new skills just by watching someone else. Master in-demand coding skills through Educative’s interactive courses.
"
https://pub.towardsai.net/how-i-passed-my-associate-cloud-engineering-exam-in-three-months-90b85e6c3da1?source=search_post---------323,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
I took my Associate Cloud Engineering exam recently, and I passed on my first try. I prepared for three months, and before…
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/hackernoon/deploy-private-docker-registry-on-gcp-with-nexus-terraform-and-packer-1af2b6a2a9c9?source=search_post---------324,"There are currently no responses for this story.
Be the first to respond.
In this post, I will walk you through how to deploy Sonatype Nexus OSS 3 on Google Cloud Platform and how to create a private Docker hosted repository to store your Docker images and other build artifacts (maven, npm and pypi, etc). To achieve this, we need to bake our machine image using Packer to create a gold image with Nexus preinstalled and configured. Terraform will be used to deploy a Google compute instance based on the baked image. The following schema describes the build workflow:
PS : All the templates used in this tutorial, can be found on my GitHub.
To get started, we need to create the machine image to be used with Google Compute Engine (GCE). Packer will create a temporary instance based on the CentOS image and use a shell script to provision the instance:
The shell script, will install the latest stable version of Nexus OSS based on their official documentation and wait for the service to be up and running, then it will use the Scripting API to post a groovy script:
The script will create a Docker private registry listening on port 5000:
Once the template files are defined, issue packer build command to bake our machine image:
If you head back to Images section from Compute Engine dashboard, a new image called nexus should be created:
Now we are ready to deploy Nexus, we will create a Nexus server based on the machine image we baked with Packer. The template file is self-explanatory, it creates a set of firewall rules to allow inbound traffic on port 8081 (Nexus GUI) and 22 (SSH) from anywhere, and creates a google compute instance based on the Nexus image:
On the terminal, run the terraform init command to download and install the Google provider, shown as follows:
Create an execution plan (dry run) with the terraform plan command. It shows you things that will be created in advance, which is good for debugging and ensuring that you’re not doing anything wrong, as shown in the next screenshot:
When you’re ready, go ahead and apply the changes by issuing terraform apply:
Terraform will create the needed resources and display the public ip address of the nexus instance on the output section. Jump back to GCP Console, your nexus instance should be created:
If you point your favorite browser to http://instance_ip:8081, you should see the Sonatype Nexus Repository Manager interface:
Click the “Sign in” button in the upper right corner and use the username “admin” and the password “admin123”. Then, click on the cogwheel to go to the server administration and configuration section. Navigate to “Repositories”, our private Docker repository should be created as follows:
The docker repository is published as expected on port 5000:
Hence, we need to allow inbound traffic on that port, so update the firewall rules accordingly:
Issue terrafrom apply command to apply the changes:
Your private docker registry is ready to work at instance_ip:5000, let’s test it by pushing a docker image.
Since we have exposed the private Docker registry on a plain HTTP endpoint, we need to configure the Docker daemon that will act as client to the private Docker registry as to allow for insecure connections.
You should now be able to log in to your private Docker registry using the following command:
And push your docker images to the registry with the docker push command:
If you head back to Nexus Dashboard, your docker image should be stored with the latest tag:
Drop your comments, feedback, or suggestions below — or connect with me directly on Twitter @mlabouardy.
#BlackLivesMatter
82 
1
how hackers start their afternoons. the real shit is on hackernoon.com. Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
82 claps
82 
1
Written by
CTO & Co-Founder @Crew.work — Maker & Indie Entrepreneur @Komiser.io, Writer/Author and Speaker — Blog: https://labouardy.com
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
CTO & Co-Founder @Crew.work — Maker & Indie Entrepreneur @Komiser.io, Writer/Author and Speaker — Blog: https://labouardy.com
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/swlh/how-to-use-google-cloud-and-gpu-build-simple-deep-learning-environment-c6eadff2a569?source=search_post---------325,"There are currently no responses for this story.
Be the first to respond.
Google Cloud Platform provides us with a wealth of resources to support data science, deep learning, and AI projects. Now all we need…
"
https://medium.com/@qwiklabs/whats-new-in-qwiklabs-cedb510d279?source=search_post---------326,"Sign in
There are currently no responses for this story.
Be the first to respond.
Qwiklabs
Feb 21, 2019·2 min read
Our engineers have been hard at work to make Qwiklabs and the Google Cloud Platform fun, educational, and accessible as we can. Login to your account, and check out some of your new features!
And your menu will appear! Whether it’s a lab or how your badges and quests work, you can search an answer for any topic you have questions about. (Qwiklabs related, of course. We can’t tell you the meaning of life.) You can also use the Chat Button at the bottom of your screen for immediate help 24/7.
In the upper right corner, click View Public Profile. Open the “share your badges with a public profile” dialog box, and click Make Profile Public. Now you’re a loud and proud member of the Qwiklabs community!
26 
1
26 claps
26 
1
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/monitoring-stackdriver-with-bigquery-9a49331f7aaf?source=search_post---------327,"There are currently no responses for this story.
Be the first to respond.
I have done several talks about BigQuery over the past two years. It was the first Google Cloud Platform product that I fell in love with. It is both simple to use (everyone knows SQL) and incredibly powerful (multiple TB with aggregation in < 30 secs?!?!?). One thing I have wondered is how I would monitor BigQuery to ensure that everything was working as expected. In the past, I ran data warehouse and log analysis tools on their own VMs and then monitored them the same way I would monitor any VM. However, I cannot do that when I am using a managed service like BigQuery. Managed services have so many advantages over running my own VMs that this week I sat down to learn the tools that were available for monitoring BigQuery using Stackdriver.
You can access Stackdriver Monitoring from Cloud Console. If you have not used Stackdriver before you will need to link Stackdriver to your Cloud Platform account. The UI walks you through the process; it just takes a minute or two. Once you are on the Stackdriver dashboard, you should see BigQuery under the resources menu. If BigQuery is not a listed resource, you may need to wait a bit. There can be a bit of a delay between creating new resource types and having them appear in the Stackdriver UI. I connected Stackdriver to my GCP account before I created my dataset and it took about an hour for it to appear in the UI.
I used the GitHub dataset I used in this blog post and the Iris dataset that is popular in machine learning textbooks. Monitoring tools are boring if there isn’t any load, so I wrote a quick Ruby script to run queries against both datasets as I explored. Once I verified that BigQuery and Stackdriver saw the traffic, I dove in.
The default dashboard shows the live query count, query times, and slot utilization. If you switch to dataset metrics you can see the number of tables, stored bytes, and uploaded bytes over time. You can also see this data by clicking on the dataset name.
I used static datasets, so the dataset metrics were pretty boring for my project. However, I could see how the query time changed as I changed the queries my script was using. Also, as I spun up more instances of my script, I saw the query count increase.
For most folks monitoring and alerting are one and the same. To see what kind of alerting metrics are available for BigQuery I created some alerting policies under “Alerting” -> “Create Policy.” With VMs the types of metrics I use most are Metric Threshold and Metric Rate of Change. Both these types are supported for BigQuery as well.
I set up a threshold metric that none of my queries should take more than 30 seconds and another that the 50% percentile should not be more than 5 seconds. I based these alerts on problems I had had with data warehouse and custom report generation in the past. Without the ability to get real-time metrics out of the database I had to put alerts on page render times and then step through the pipeline to figure out if the problem was in the HTML view, the business logic, or a slow query.
I also set up a few metric rate of change alerts. I set up one that detects if the raw number of queries was decreasing significantly over time. If I had more users doing ad hoc queries against my datasets I would set up an alert on the rate of change for slots allocated. That way I could track down the queries that were causing the increase before I hit the slot quota.
Stackdriver lets you create custom dashboards as well. When I am setting up monitoring for a website, I like seeing the 95%, 50%, and 5% response times overlaid. I feel like that gives me a better feel for overall system health and user experience than just seeing the average. I created a similar dashboard for my BigQuery datasets.
You can also mix BigQuery charts and charts for Compute Engine, App Engine, and other resource types on the same dashboard so that you can see your system as a whole in one place.
For more information about setting up Stackdriver for BigQuery and the types of metrics available, please check out the documentation.
02/23/17
Originally published at www.thagomizer.com on February 23, 2017.
Google Cloud community articles and blogs
9 
9 claps
9 
Written by
Rubyist, Data Nerd, Lazy Dev, Stegosaurus. Cloud Developer Advocate @ Google. Disclaimer: My opinions are my own. *RAWR*
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Rubyist, Data Nerd, Lazy Dev, Stegosaurus. Cloud Developer Advocate @ Google. Disclaimer: My opinions are my own. *RAWR*
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/google-cloud-functions-framework-9fbd899c201c?source=search_post---------328,"There are currently no responses for this story.
Be the first to respond.
Cloud Functions is a managed service for serverless functions on Google Cloud Platform. Functions scale dynamically, charge per use and eliminate the toil of infrastructure management. So why would I want to run functions in different environments?
In this article, we will cover the basics of function portability using the functions framework. We’ll show you how you can use functions on Google Cloud Functions locally, from the gcloud CLI, a URL, and applications like Google Sheets.
The Functions Framework allows you to:
Google Cloud Functions for Node 10 uses the Functions Framework for it’s underlying invoker module.
It’s open-source on GitHub and npm: @google-cloud/functions-framework
The below demos show calling a Cloud Function from localhost, the gcloud CLI, a URL, and Google Sheets.
We set COINBASE_TOKEN as an environment variable. Create your key here.
Code
In Node, export a function that makes a request to api.coinbase.com BTC in USD. We can optionally provide a specific date as a URL query parameter.
This function can be called in multiple environments thanks to the Functions Framework:
Localhost
gcloud CLI
Deployed URL
Google Sheets
You can Cloud Functions from Google Sheets with a few lines of Apps Script glue.
In Google Sheets, you can type = and autocomplete your function.
When you run gcloud functions deploy for Node 10, we…
Run any. Run anywhere. Integrate anything.
Google Cloud community articles and blogs
14 
1
14 claps
14 
1
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Google • Explorer
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://codeburst.io/google-cloud-operations-for-gke-by-example-a4a828e583f6?source=search_post---------329,"Exploring Kubernetes monitoring on Google Cloud Platform (GCP) through a concrete example.
Please note: For exploring Kubernetes logging on GCP, there is another article: Google Kubernetes Engine Logging by Example.
When we create a Kubernetes cluster on Google Kubernetes Engine (GKE), Google Cloud Operations for GKE is enabled by default.
Google Kubernetes Engine (GKE) includes native integration with Cloud Monitoring and Cloud Logging. When you create a GKE cluster, Cloud Operations for GKE is enabled by default and provides a monitoring dashboard specifically tailored for Kubernetes.
— GCP — Overview of Google Cloud’s operations suite for GKE
Please note: As Legacy Logging and Monitoring is deprecated and will be decommissioned on March 31, 2021, this article does not cover it.
One area of confusion is the relationship between Stackdriver and Google Cloud Monitoring and Cloud Logging.
Our suite of operations products has come a long way since the acquisition of Stackdriver back in 2014. The suite has constantly evolved with significant new capabilities since then, and today we reach another important milestone with complete integration into the Google Cloud Console. We’re now saying goodbye to the Stackdriver brand, and announcing an operations suite of products, which includes Cloud Logging, Cloud Monitoring, Cloud Trace, Cloud Debugger, and Cloud Profiler.
— GCP — All together now: our operations products in one place
While the following video, Kubernetes Apps on Day 2: Effective Monitoring and Troubleshooting with Stackdriver (Cloud Next ‘18), is a couple of years old, it provides an excellent overview of Google Cloud Operations for GKE.
From the video, we learn that can explore three different aspects of Google Cloud’s operations suite for GKE:
If you wish to follow along, you will need to have access to a GKE cluster with Cloud Operations Suite GKE is enabled and have downloaded the hello-cloud-ops-gke project.
To explore the basic usage of Cloud Operations Suite GKE, we will deploy a sample workload running two Apache web servers exposed by the app-1 service.
From the project’s root folder, we start by creating the namespace-1 namespace by executing:
We then create the sample workload in the namespace by executing:
We can load a web page served by the workload by first port forwarding our workstation’s port 8080 to the app-1 service by executing:
We then load the web page in a browser.
While we could repeatedly reload the browser to generate traffic to the Apache web servers, it is more efficient to automate the process by creating a job.
This job generates 100 queries per second of traffic to the Apache web servers for five minutes.
We can monitor the sample workload using the Kubernetes Engine > Workloads menu in Google Cloud Console.
The CPU graph charts various sums of metrics of the two apache containers:
The Memory graph similarly charts the sum of memory metrics of the two apache containers; each container contributing 16Mi for both memory limits and requests. The combined usage is approximately a constant 12Mi.
We can also monitor the sample workload using the Monitoring > Dashboards > GKE menu. We can quickly drill down to monitor the sample workload; here we filter by cluster and namespace.
From the filtered screen, we select the app-1 workload and select Metrics.
Things to observe:
Finally, we can create custom graphs using the Monitoring > Metrics Explorer menu. Here we recreate the CPU usage graph from above.
Things to observe:
In the previous examples, we used Cloud Monitoring to both store and visualize the metric data. Here we use Grafana to visualize the metric data stored in Cloud Monitoring.
Grafana is open source visualization and analytics software. It allows you to query, visualize, alert on, and explore your metrics no matter where they are stored. In plain English, it provides you with tools to turn your time-series database (TSDB) data into beautiful graphs and visualizations.
— Grafana Labs — Getting Started
Grafana ships with built-in support for Google Cloud Monitoring. Just add it as a data source and you are ready to build dashboards for your Google Cloud Monitoring metrics.
— Grafana Labs — Cloud Monitoring
In this example, we follow the instructions on using a Google Service Account Key to authenticate to Cloud Monitoring instead of using a GCE Default Service Account (or in particular using Workload Identity as we will run the Grafana workload on the cluster). The latter is the preferred approach; it, however, is a bit more complicated.
With the Google Service Account JSON key file on hand, we will deploy a Grafana workload exposed by the grafana service.
We then create the Grafana workload in the namespace by executing:
We can load the Grafana UI by first port forwarding our workstation’s port 3000 to the grafana service by executing:
Using a browser we log in to the Grafana UI with the username/password of admin/admin.
Using the Google Service Account JSON, we can add the Google Cloud Monitoring data source.
Please note: The Google Cloud Monitoring data source ships with a pre-built GKE Cluster Monitoring dashboard that can be imported; we, however, are not going to use it for this example.
To make the graph more interesting, we can first generate traffic to the Apache web servers.
We create a new dashboard and panel; we will work to recreate the graph of the app-1 workload CPU usage as we did in Cloud Monitoring. The query is constructed by updating:
Service: KubernetesMetric: CPU usage timeFilter: resource.label.cluster_name = cluster-1 AND resource.label.namespace_name = namespace-1Aggregation: noneAdvanced Options > Aligner: rateAlias By: {{resource.label.pod_name}}
Things to observe:
So far we have been monitoring Kubernetes container metrics; how might we monitor workload-specific metrics? As an example, we might want to monitor the rate of requests handled by the Apache servers.
The solution we will consider here is to have the Apache servers provide metrics in the Prometheus exposition format and configure Cloud Monitoring to consume them.
Prometheus is a monitoring tool often used with Kubernetes. If you configure Cloud Operations for GKE and include Prometheus support, then the metrics that are generated by services using the Prometheus exposition format can be exported from the cluster and made visible as external metrics in Cloud Monitoring.
— GCP — Using Prometheus
As we will have to modify our workload a bit for this example, we delete the workload that we have been using.
Unfortunately, the official instructions, Using Prometheus, are fairly terse; here we will go through the steps in more detail.
First, we need to enable Workload Identity for the GKE cluster as documented in Using Workload Identity. If one is using a test cluster, it is easier to recreate the cluster with Workload Identity enabled.
Next, we need to create a GCP service account with permissions to interact with Cloud Monitoring and Cloud Logging as documented in Hardening your cluster’s security; we name the service account prometheus.
We then need to allow the Kubernetes service account (we will create later) to impersonate the GCP service account as documented in Using Workload Identity. Here the namespace is namespace-1 and the Kubernetes service account name will be prometheus.
The sample workload is updated with a sidecar container, apache-exporter, that provide Apache server metrics in the Prometheus exposition format.
We then create the updated sample workload in the namespace by executing:
As before, the app-1 service exposes the Apache servers on port 80; in addition, it exposes the Apache servers’ metrics in Prometheus exposition format on port 9117.
We can examine these metrics by first port forwarding our workstation’s port 9117 to the app-1 service’s port 9117 by executing:
We then load the metrics in a browser:
We now need to run a Prometheus server in the cluster; the workload is a bit complicated.
Things to observe:
We then create the Prometheus workload in the namespace by executing:
We can load the Prometheus web UI by first port forwarding our workstation’s port 9090 to the prometheus service by executing:
We then load the Prometheus web UI in a browser and inspect the rate of Apache server access of the app-1 workload.
Time series are identified by name, here apache_accesses_total, and series of labels, here instance and job. Under the hood, however, we can observe that these metrics also have label names beginning with __ that are reserved for internal use.
The sidecar container of the Prometheus workload is the process that uploads these Prometheus metrics into GCP Cloud Monitoring.
The Stackdriver collector for Prometheus constructs a Cloud Monitoring MonitoredResource for your Kubernetes objects from well-known Prometheus labels.
— GCP — Using Prometheus
The metrics are accessible as the Kubernetes Container resource type and with metric name prefixed with external/prometheus/, e.g., external/prometheus/apache_accesses_total.
Through trial and error, we can determine that these metrics also include the following labels constructed from the related internal use Prometheus metric labels.
To make the graph more interesting, we can first generate traffic to the Apache web servers.
We can now create a custom graph using the Monitoring > Metrics Explorer menu to chart the rate of Apache server access of the app-1 workload.
The custom graph reflects this traffic; again 100 queries per second across both apache workloads.
Hope you found this useful; it was a rather challenging article to write.
Bursts of code to power through your day.
66 
1
66 claps
66 
1
Written by
Broad infrastructure, development, and soft-skill background
Bursts of code to power through your day. Web Development articles, tutorials, and news.
Written by
Broad infrastructure, development, and soft-skill background
Bursts of code to power through your day. Web Development articles, tutorials, and news.
"
https://blog.rittmananalytics.com/oracle-big-data-cloud-event-hub-cloud-and-analytics-cloud-data-lake-edition-pt-1-84961cd4274f?source=search_post---------330,"There are currently no responses for this story.
Be the first to respond.
Some time ago I posted a blog on what analytics and big data development looked like on Google Cloud Platform using Google BigQuery as my data store and Looker as the BI tool, with data sourced from social media, wearable and IoT data sources routed through a Fluentd server running on Google Compute Engine.
Read more at the MJR Analytics blog.
A Blog about Cloud Analytics and Data Management from the…
29 
1
29 claps
29 
1
A Blog about Cloud Analytics and Data Management from the team at Rittman Analytics
Written by
CEO of Rittman Analytics, host of the Drill to Detail Podcast, ex-product manager and twice company founder.
A Blog about Cloud Analytics and Data Management from the team at Rittman Analytics
"
https://medium.com/the-infinite-machine/python-app-engine-2017-prioritisation-of-tasks-using-services-eddbfb4252cc?source=search_post---------331,"There are currently no responses for this story.
Be the first to respond.
In my last post I went through getting the quickstart helloworld app going with the latest tooling and whatnot for Google Cloud Platform.
On reflection, it was surprisingly difficult, and that’s coming from someone who’s been using App Engine for 7+ years. Google Cloud Platform is big, the gcloud sdk is a whole universe. I guess that’s what you’ve got to expect of a mature platform.
Anyway, look, that beginner level is interesting and I’ll come back to it, but I need to jump ahead because I’m doing this for a reason, and the reason is Services.
App Engine is a misunderstood thing. I think people mostly see it as a weirdly proprietary and restrictive cloud based web server, and Google themselves seem to be pushing it as a useful coordination point for GCP based systems.
I think of it as a high level distributed computing based virtual machine that I can program to, a vast sea of computation that I can use for pennies. A wonder of the world.
—
One of the things I do is to use task queues extensively, and I’ll be writing a whole lot about that. I send fully serialised python functions, lexical closures included, from task to task, mapping over objects and performing all kinds of weird and wonderful background processing.
One side effect: I have some new algorithms that spit out thousands of tasks in seconds. This is the kind of thing App Engine excels at dealing with, but, you can’t be doing that in your main application. Why? Because if the UI handlers for your front end are handled by the same instances that handle these mountains of tasks, they’ll get lost in the noise, and starve. ie: the UI will become unresponsive.
What I’m trying to solve in this post is a prioritisation problem. How can I have bulk background processing happening in the same App Engine application, without starving my UI?
The approach I’ll try is is to have different sets of instances for the UI and for background tasks. And I think the right abstraction for this is Services.
—
A modern App Engine app’s architecture looks like this:
This is from the microservices guide.
Think of a version as a codebase + configuration.
You can just have one default service and one default version for an App Engine project, and that’s often enough. But one of the great features of services (well versions ultimately) is that they maintain different sets of instances:
What’s an instance? Think of it as a VM which includes your codebase, some processor cores, some RAM and so on. It’s the machine that actually runs your code.
One thing that Services and Versions let you do is to run different code bases in the same application, which is an excellent thing when you need it. But I’m trying to solve a different problem today, a prioritisation problem.
When you run a task, be it a handler that is run in response to an outside call (like someone hitting your web page / api), or a worker task you’ve kicked off in the background, that task runs on a task queue.
App Engine has Push Queues and Pull Queues. Push Queues are automatically handled by App Engine; they are just assigned to an instance to run, and come into your app as a web request. These are what I’m looking at today. When I talk about tasks and queues, I’m always talking about push queues.
Pull queues are a different beast; tasks are enqueued and you need to provide separate, explicit logic to pull tasks off the queue and handle them. I’m not addressing those in this post.
Also, there are different kinds of scaling for Push Queues; I’m using Automatic Scaling (where App Engine figures out how to scale instances up and down magically).
The prioritisation problem is this:
Say I have 1000 background tasks enqueued, some are running. Then, a user hits my api or web handler. That becomes task 1001 on a queue. If the same set of instances are handling both types of task, and are overloaded, App Engine is madly trying to provision more instances. But this can take a while (minutes, if you’ve really started a lot of tasks). Meanwhile the user’s task is lost in the pile of tasks that are delayed.
If I use two services, a UI service and a Background service, it would look like this…
I have 1000 background tasks enqueued, some are running. All this is happening in the Background service, on instances for that service. App Engine is madly trying to provision more instances for Background. Tasks are delayed. Then, a user hits my api or web handler. That goes to the UI service, which has its own instances (and is relatively quiet at the moment). The UI task is handled by one of those instances, quickly, and unaffected by the chaos happening in the Background service.
That’s what I want.
So, let’s try to get this happening.
—
I’m going to try to modify the quickstart app, to do the following:
1 —I’ll give it two services; the default service for the UI, and a “background” service for background tasks. I’ll use the same codebase for both services.
2 — I’ll modify the Hello World handler to have two buttons, Foreground and Background. Foreground will kick off, say, 10000 background tasks in the default, UI service, and Background will kick off the same number of tasks in the background service.
3 — I should be able to see the UI be affected by Foreground, by just refreshing the page; it’ll be really unresponsive. Background meanwhile should have no such affect.
Ok.
—
First, let’s turn the quickstart app into something with two services.
What we’ve got so far looks like this:
One service, with 3 versions interestingly. Apparently the uploads I did in the last article all created new versions. Do I want that? Probably not.
Here are my running instances:
One instance, listed by version. Hmm, I think I want to get this version thing under control.
Ok, so what do I do to make a new service?
So the first thing is, I’ve pecked around in the documentation and I can’t figure that out. I’ve read the opinionated docs about microservices, which are conceptual, but how do I actually do this thing?
I’m guessing I need to do this:
So first, let’s get the project set up. I’ll copy the hello_world project from the last post into its own folder called servicesdemo, then add it to Eclipse as a Pydev project. (yell out if you want more detail on how I do this)
Ok. So first, I’ll duplicate app.yaml, and just call it background.yaml .
Ok, now how do we deploy? I’m going to need some sort of switch for uploading a service.
In the last post I deployed like this:
Maybe there’s some kind of service switch? Looking in gcloud app deploy --help:
Ok, ok, just try to understand this thing.
Firstly, this looks promising:
There’s no service switch, but there’s a version switch. I’ll use that and just set my version to “default”, maybe I can calm down the version thing that way.
I faintly recall seeing something about setting a service name in the app.yaml file. Bit of googling, yup, it’s here:
Ok, so let’s add this servicename to background.yaml:
Righto. Now the deploy command should be this:
uhhuhuhrrr
ok, let’s try another version name:
Ok, that might have worked! What can I see in the cloud console?
Oh, that’s totally a thing! Success!
I need to be able to click a button and run 10000 background tasks. Ok.
The way to do this is to create 2 task queues, which will run their tasks in the “default” service and “background” service, respectively. For this, we need a queue.yaml file. It’ll look like this:
We don’t need to define the “default” queue, because App Engine does this for us, but I want the processing rate to be high, so I’ve added it here.
What’s target?
Ok, ok. Let’s try some stuff. First, this:
In the logs, I can see the request:
At A, you can see the request url, at B you can see the version is defaultversion, at C you can see the service name is background (called module id because services used to be modules).
So maybe we can just say background, and it’ll refer to the background service? Like this:
Now how do I deploy? I think deploying both the default service and the background service will upload the queue.yaml file. Let’s try deploying the default service:
Ok, let’s go look:
Totally did nothing.
<elided lots of hunting through logs and other horror>
Look, it’s working in the local environment:
I’m experienced in App Engine development, but I’m new to gcloud. Something is wrong here.
Ok, let’s strip it back to fundamentals. First, let’s simplify the queues:
There should also always be a queue called default. Let’s actually use that.
Now, I’m going to add code for enqueuing a task, to the helloworld request handler. It should either work, or blowup if the queues aren’t configured.
Here’s the new code for main.py:
I’ve used the deferred library to run a task. Don’t use the longhand way they show you in the docs, this is the best way to run a task in python.
To make deferred work, you also need to modify app.yaml (and background.yaml, don’t forget to add it there too):
That’s adding a built-in appengine handler at the route /_ah/queue/deferred to handle deferred requests.
Ok. In main.py, you can see I’ve added a HelloWorld function that just logs Hello World, and I run it in the background (it’ll go to the default task queue, because I haven’t specified anything).
So if queueing is working, I should be able to visit that page, and see the Hello World log entry.
So running the local server, then visiting localhost:8080, I see the hello world webpage, and I see this in the terminal:
See “Hello World” logged? That’s success.
Now I’ll deploy this to the default service, and try it there. Let’s visit the web page:
Ok! What’s in the logs?
Look, it’s run the background task using /_ah/queue/deferred, and you can see the line near the bottom saying “Hello World”. Good. What’s the task page look like now?
Ooh, we have a task list, instead of that annoying documentation screen. Ok! And you can see the default queue, and the task we ran.
But where is the background queue? Something is wrong here.
Googling, googling, googling, … then this:
stackoverflow.com
Let’s try that!!! My command should be
And…
Ok, let’s change the app.yaml to make the background queue truly run tasks against the background service:
And I’ll change main.py to use the background task queue:
Now deploy both the default and background services.
Yes, it turns out I can deploy both services and the queue definition in one line. Nice.
And visit http://emlyn-experiments.appspot.com/, I see “Hello, World!”, so let’s look at the console:
There’s an instance running for the background service, good!
How about the log?
You can see module_id is background, and it’s logged Hello World. That’s success!
Now let’s get back to those two buttons.
Here’s a new main.py:
What’s happening here is that the get handler is presenting some html with two buttons, foreground and background. Then the post handler is checking which button was pressed, and enqueueing 10,000 tasks to either the default or background queues, which are serviced by either the default or background apps, respectively.
Also, the HelloWorld function now loops uselessly, to make it actually use some resources.
I’ve uploaded this final version of the code to github: https://github.com/emlynoregan/servicesdemo
So now I deploy the services, then go to emlyn-experiments.appspot.com :
This is it, this is the experiment.
Ok. I predict that pressing “foreground” will fill the default queue with tasks, kick off a bunch of default service instances, and refreshing the browser will be tough because the default app will be unresponsive.
Here’s a video of pressing the “foreground” button.
You’ll see I press the button, lots of task build up, then I try to refresh the page. Sometimes it’s ok, but sometimes it’s laggy, slooow.
Try the background button:
I do the same thing here, and you’ll see that refreshing the page is fast, never any lag. That’s because the default app is doing that work, while the horrible slow tasks are all in the background app. Success!
—
Thanks for hanging in there for this. I feel like I learned more about gcloud, and now I know how to break my app into multiple services, and use those services to do different kinds of jobs.
—
Addendum: Running these tests (including screwing it up a few times) cost a bit over $7.
Idiosyncratic Incantations in Python for Google App Engine
19 
1
No rights reserved
 by the author.
19 claps
19 
1
Written by
I make things out of bits. Great and terrible things, tiny bits.
Idiosyncratic Incantations in Python for Google App Engine
Written by
I make things out of bits. Great and terrible things, tiny bits.
Idiosyncratic Incantations in Python for Google App Engine
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/swlh/using-a-cluster-in-the-cloud-for-data-science-projects-in-4-simple-steps-9ee067238448?source=search_post---------332,"There are currently no responses for this story.
Be the first to respond.
The Wreckers already said it::
“Why do they make it hard to love you?
Why can’t they even start to try?
’Cause now I feel the bridge is burnin’
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://blog.elegantmonkeys.com/stream-your-hapi-js-logs-to-google-cloud-e060e43a016d?source=search_post---------333,NA
https://medium.com/google-cloud/cloud-identity-beta-overview-bdd5e410b527?source=search_post---------334,"There are currently no responses for this story.
Be the first to respond.
Cloud Identity is identity as a service (IDaaS) for Google Cloud Platform. If you are already familiar with GCP IAM, Cloud Identity enables extended organizational user management features for organizations using GCP resources. Think active directory or G Suite without Gmail, Drive or Apps, just the user services and admin controls at no additional cost to GCP projects. Most interesting to me, Cloud Identity allows organizations to migrate outlier GCP billing accounts and projects that were set up by individuals or teams to an owned organizational domain to meet corporate security or compliance requirements.
With your active GCP account, after you sign up for Cloud Identity in IAM and admin -> Identity you will have to verify your domain ownership. If you have set up analytics or adwords it’s the same process. Upload a html file to the root server on the domains web server or add a meta tag in the index page.
As I mentioned, my favorite thing about Cloud Identity is that it allows you to migrate personal GCP projects and billing accounts into an organization. So say you have a few teams in your organization using their own credit cards and their own email accounts for GCP projects. Now you can corral and organize those accounts under one domain.
In order to move a project from a non domain GCP account (personal), you need the main admin account (destination) to be added as a Owner in the source GCP account. In my case my admin account is admin@mikekahn.net and my source is my personal gmail GCP account with the project mike-kahn-sandbox.
After the main admin account is added to the source project in the individual’s account, the main admin can view the project. Next step is to migrate that project to the domain. Go to the project view select the project and click migrate.
Now in the organization you can see my personal GCP sandbox project in my newly created organization.
Note this only changes the projects ownership and hierarchy. In the case above, billing would not be moved to my domain account only the project. Next step would be to Migrate existing billing accounts to the domain.
❗️To complete the migration, dont forget to remove the source project user ID, probably the individual’s personal email account from the migrated account’s IAM. This will ensure that the project is fully moved over to the new organization.
You have a few options here with migrating projects to the organization. If you wish you can keep billing in the individual’s account and only migrate the project and take ownership over it. This scenario could apply if the individual is expensing resources for their team on their corporate card but still corporate security requires ownership of the cloud project.
Read more: Migrating Existing Projects into the Organization
If your organization is using another main identity management platform service such as LDAP or AD, you can still use Cloud Identity today to help organize multiple GCP projects in your company or try out G Suite with your team. You can also use Google Cloud Directory Sync (GCDS) to maintain consistency between your AD or LDAP server. More info on GCDS here.
Google Admin with Cloud Identity is like G Suite without the apps. If you decide you want to start using Docs, Drive, and other G Suite Services you can easily add within Google Admin for the company and domain you just created. G Suite basic starts around $6/per user account.
As of writing this article, Cloud Identity is a free subscription that can be cancelled at anytime.
Cloud Identity is an excellent new feature of GCP to help organize multiple individuals or teams GCP projects and cloud platform users in your organization. Cloud Identity can help your organization comply with corporate security or compliance policies for cloud users and resources. If you have an existing identity management system such as AD or LDAP setup in your organization you can still utilize Cloud Identity and synchronize both services if you wish.
More details:Google Cloud Platform Identity and Access ManagementWhat is Cloud IdentityCompare Cloud Identity features
Google Cloud community articles and blogs
10 
3
10 claps
10 
3
Written by
Customer Engineer, Google Cloud. All views and opinions are my own. @mkahn5
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Customer Engineer, Google Cloud. All views and opinions are my own. @mkahn5
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://tincture.io/life-in-the-cloud-17e00baab706?source=search_post---------335,"The cloud is big. Amazon (AWS), Google (Cloud Platform), IBM (Cloud) and Microsoft (Azure), among others, have made big bets in the space. Indeed, virtually all of Amazon’s 2017 operating income came from AWS. Alphabet’s former Chairman Eric Schmidt had some important advice about the cloud for attendees at HIMSS18:
The thing is, I’m not sure we’re thinking about the cloud quite right yet. As evidence, I want to point to the Blade Shadow PC.
Blade is a France company, and Shadow PC is an app designed specifically for gaming. It offers users a virtual machine, giving them access to a high-end PC that is constantly upgraded.
Gamers always want to have PCs with the fastest processor, best graphics card, and sharpest display. It can get very expensive, and as soon as you’ve upgraded your device, it starts to become outdated. With Shadow, in theory, you’ll always have the latest and best.
Shadow doesn’t even care what device you run it on. Your monthly subscription lets you run it on any device you want (one at a time). You could start a game on your home PC, continue playing on your smartphone while you are taking an Uber to work, then pick up playing it on your work computer (during your breaks, of course). The machine you use is pretty much just needed for the connectivity and the screen.
Shadow has received a lot of media attention. The consensus seems to be that it may not always quite live up to the hype, yet, but it is pretty cool.
Here’s CNET’s review:
I have to admit, I don’t really care about video games, except as a business and cultural phenomena, but the implication of Shadow PC is not so much for video games as it is for everything else we do on devices. As Blade CEO Asher Kagan told The Next Web:
If we can do something to prove this is working for the gamer market, it’ll show it can work for anyone. Gamers are very demanding, they’re very sensitive to latency.
The Wall Street Journal’s David Pierce sees it leading to a new future:
Someday soon, what Uber did for cars and Netflix for TV will happen with computers. Rather than buy a suite of gadgets — a PC, phone, Xbox, etc. — you could just access their features when you need them. They won’t need to be distinct, powerful devices with their own hefty allocations of processor and memory. Instead, you’ll have a single virtual computer with all your data and preferences. You’ll reach for a touch screen when you’re on the go, sit down to a larger one with keyboard and mouse when you’re at your desk. Maybe you’ll have a wall-size one, too.
Similarly, Rob Enderle writes in Computerworld:
Blade Shadow PC has the potential to become a Netflix for PCs but so does Netflix and all the other streaming providers — including Amazon. I expect by 2020 we’ll have a lot of compelling alternatives to a running apps on locally on a PC and that this will drive a trend similar to what happened to Blockbuster and Netflix. The old will give way to the new and we’ll never have to worry about patches, replacing hardware, or even buying apps the way we do now.
Mr. Enderle’s article is headlined “By 2020, we’ll be using Windows in the cloud,” so we’re not just talking about games or apps or websites. We’re talking about operating systems. It takes the idea behind Chromebooks to the next level.
At this point, it might seem that everything will go to the cloud. After all, with cyberthieves getting ever more sophisticated, it almost seems negligent for you to have your own PC or for your company to hosts its own machines. Let the experts worry about hacking and security, about hardware and software upgrades. We only care that things work.
I don’t think it is going to be quite that simple.
I’m old enough to remember mainframe dumb terminals with arcane commands. I’m thus old enough to remember when PCs came along to help wean us from mainframes, when local computing power was something to be prized, not avoided.
Moving everything back to a centralized place, this time in the cloud, thus leaves me with a little trepidation.
I don’t think that is what is going to happen because of ubiquitous computing (also known as pervasive computing), and dispersed computing. They sound similar, and they are related, but they are distinct.
The former is often thought of in the Internet-of-Things gold rush, where everything is connected to the Internet, communicating all the time. It is happening already, and it will develop exponentially over the next few years. We’ll know more, about more things, than we ever could have guessed we might have needed.
The latter, though, is an even newer idea (Darpa is spurring it): all those devices aren’t just connected but they are all also computing. You draw your computing power from whatever is handy and appropriate to/necessary for the task(s) at hand.
You might access a distant cloud server farm for some needs, but your smart clothes for others. If you lose a connection, or if your computing needs shift, you seamlessly pick up another, or add more. Your screen might just be something you see through your AR contact lens and your “keyboard” might just be your hand gestures, or even a direct implant in your brain.
Think of computing power almost like electricity: it will just be there, everywhere, and you won’t even always need wires. You use what you need, and you don’t really care where it comes from.
Think about your computers as, well, you won’t have to think about them at all. As I said in a previous post,
If you’re aware of your device, that’s the past… We’re going to have to get past our fascination with the latest and greatest devices — a new iPhone! a 4D television! — and let their technology fade into the background. As it should.
Healthcare is very proud about how it is finally adopting (if not quite embracing) computers, and many of its thought-leaders are taking Mr. Schmidt’s advice by starting to move to the cloud as the “next” big thing. That’s good.
I just would like to see them spending more time thinking about the next next big thing.
Follow Kim on Medium and on Twitter (@kimbbellard)!
Translating 'medicine' into plain english.
25 
25 claps
25 
Written by
Curious about many things, some of which I write about — usually health care, innovation, technology, or public policy. Never stop asking “why” or “why not”!
Translating 'fortress medicine' into plain english. A digital town square for ideas & new perspectives. Open-source Thoughtware. Accessible. Important. Exciting. Trying hard to remember the future.
Written by
Curious about many things, some of which I write about — usually health care, innovation, technology, or public policy. Never stop asking “why” or “why not”!
Translating 'fortress medicine' into plain english. A digital town square for ideas & new perspectives. Open-source Thoughtware. Accessible. Important. Exciting. Trying hard to remember the future.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@cloudacademy/kickstart-your-tech-training-with-a-free-week-on-cloud-academy-f06da4490a79?source=search_post---------336,"Sign in
There are currently no responses for this story.
Be the first to respond.
Cloud Academy
Jun 22, 2020·4 min read
Original article published by Joe Nemer on cloudacademy.com.
Are you looking to make a jump in your technical career? Want to get trained or certified on AWS, Azure, Google Cloud Platform, DevOps, Kubernetes, Python, or another in-demand skill?
Then you’ll want to mark your calendar. Starting Monday, June 22 at 12:00 a.m. PDT (3:00 a.m. EDT), Cloud Academy is offering FREE access to our industry-leading Training Library.
Register here for immediate access to our free week.
During this free time period, you’ll have seven days of unlimited access to:
You’ll also get access to our most popular Cloud Academy labs that help you build practical experience by using live cloud environments:
Want to make the most out of your free week? Jump into a certification learning path and test drive the labs.
Getting certified shows that you have some of the most in-demand and profitable skills in the industry, though there are countless certifications to choose from. Cloud providers — such as AWS and Azure — offer foundational certifications, which are designed to validate a candidate’s overall understanding. Foundational cloud certifications for beginners include AWS Cloud Practitioner Certification and Microsoft Azure Fundamentals.
Theory is great, but there’s nothing like getting reps on real deployments to make you ready to ace the next cert exam — and as you might know, more and more exams have lab components.
Check out some of our most popular certification learning paths:
And try out a learning path to get a solid start on coding, general cloud, DevOps, and more, helping you operate across any cloud ecosystem:
Sign up on our free week registration page. No credit card or paid account required.
Download the app to help you study anywhere. You can even start on our website and transition to your phone within the same course seamlessly.
Believe it or not, there is no credit card required to gain full access to our mobile app and training library that delivers the theory, technical knowledge, and hands-on practice to get certified on industry-leading cloud platforms and technologies. We’re so confident that you will love our training platform and our mobile app, that you’ll want to stick around for a while.
Our training library is open from Monday, June 22 at 12:00 a.m. PDT and will be locked down on Sunday, June 28 at 11:59 p.m. PDT. Make sure you sign up now to create your free account in advance!
At Cloud Academy, we’re customer- and product-obsessed. It’s our mission to understand what our users want and need, so we can help to empower their learning goals. Our goal is to assist both personal and enterprise users to build hands-on technical skills that drive measurable results. With our enterprise-grade platform, you can assess, develop, and validate technical skills with structured learning paths that provide the theory and hands-on practice to master the skills you need in the real world.
Our training library — loaded with 10,000+ hours of technical content — is helping users build tech skills at the core of innovation.
You can explore what’s new on AWS, Azure, Google Cloud, containers, security, IoT, data science, programming, machine learning, big data, and more.
We built Cloud Academy around the needs of our Fortune 100 clients. From demonstrable security to fast, flexible deployments of thousands of seats, we provide total control and predictability in up-leveling your organization’s skills and enabling you to deliver on challenging technical projects.
Cloud Academy is the leading digital skills development platform that enables every enterprise to become a tech company through guided Learning Paths, Hands-on Labs, and Skill Assessment. Cloud Academy delivers role-specific training on leading clouds (AWS, Azure, Google Cloud Platform), essential methodologies needed to operate on and between clouds (DevOps, security, containers), and capabilities that are unlocked by the cloud (big data, machine learning).
Companies like Turner, Cognizant, SAS, and ThermoFisher customize Cloud Academy to contextualize learning and leverage the platform to assign, manage, and measure cloud enablement at scale.
*Free access starts Monday, June 22 at 12:00 a.m. PDT and ends Sunday, June 28 at 11:59 p.m. PDT. This promotion cannot be combined with any other promotions or applied in any way to existing personal or enterprise licenses. After free access ends, you will be given the option of upgrading your account to a paid subscription. To see what people are saying about us, read our reviews on G2.
The proven platform to assess, develop, and validate hands-on technical skills.
75 
75 claps
75 
The proven platform to assess, develop, and validate hands-on technical skills.
"
https://medium.com/google-cloud/advantages-of-running-sap-on-google-cloud-2632a862feeb?source=search_post---------337,"There are currently no responses for this story.
Be the first to respond.
Google Cloud Platform (GCP) is certified to run SAP Applications, SAP Cloud Platform (PaaS), and SAP HANA. Read on to learn some highlights of what makes GCP different especially for SAP workloads.
Google Cloud offers per second billing and pay as you go for compute and storage and flexible innovative pricing and for all types of workloads and application use cases.Sustained usage discounts automatically apply up to a 30% discount for instances running for a full month. This is a great default value-add for standard workloads that will run for a undetermined amount of time.Committed usage discounts have no up front costs and offer up to a 57% discount on regular pricing in exchange for a 1 or 3 year commitment. This is most likely where you will want to be if you are considering running SAP on GCP. More on committed usage discounts here.Preemptible VM (pVM) instances are ideal for stateless, batch jobs and fault tolerant processing workloads. PVMs are discounted to about 80% of regular pricing.
Custom machine types allow you to select exactly how much vCPU and memory you need allowing you to break free from a predefined instance type at other vendors.GCP has the fastest instance startup time out of any cloud provider and unparalleled uptime for regions and zones.Performance is underpinned by the private distributed backbone network with over 100 global network point of presence throughout the world putting cloud locations closer to you. GCP has various flexible and quick turn up private interconnect and peering options for your internal mission critical applications, or use Cloud Identity-Aware proxy (Cloud IAP) for identity verification without the need of a VPN.
HANA and BigQuery (BQ) are two very unique data services and can be used together to handle hot, warm, and cold data tiers. Operating HANA DBs can be costly and usually require upkeep with variable OLTP and OLAP workloads. Infrequently accessed HANA OLAP data can be moved from in-memory to a data warehouse like BQ to provide more cost efficiency. BigQuery is pay as you go, regional, has cached queries, SQL query and easy integration. Use SAP Data Services to integrate BQ with HANA and display it all with Tableau or Data Studio to refine and reduce your HANA system footprint.
Do you have Google Analytics 360 running in your business? Use BigQuery for predictive analysis and better insight into your customers. Automate data flows several times a day and visualize with Google Cloud Data Studio. More here.
Google Cloud does not charge for agreements such as the business associate agreement (BAA) required for HIPAA compliance. Google Cloud holds common and uncommon compliance audits to protect clients and enable all industries to do more.
Google has a very unique approach to security working the past 15 years keeping customers safe on Gmail, search and ads. Everything is encrypted on the platform in transit and at rest at no extra cost. Defense at depth at each layer of the infrastructure and network stack. Read more in the Google security white paper.
In 2017 the public cloud is more secure than most customer owned on-premises data centers. More on Google Cloud security here.
Does your organization use G-Suite (Google Apps like Gmail, docs, or drive?) If so it is very easy to integrate Google Cloud projects and users with Google Admin in G-Suite. More here.
Its easy to try the Google Cloud Platform. Check the free tier program here. There is no up front cost or no commitment to try.
Do you work with a system integrator or partner? Google Cloud has partnerships with most major, regional, and boutique specialty SAP partners.
Don’t worry about deploying and managing instances or complex data warehouse clusters anymore. Focus on productive systems and getting profitable insights from your data. Nearly all services on Google Cloud are fully managed by default.
More info:
The Keyword Blog Post- SAP on Google Cloud Platform, Certifications and MoreSAP on Google CloudSAP HANA on Google Cloud
Google Cloud community articles and blogs
18 
18 claps
18 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Customer Engineer, Google Cloud. All views and opinions are my own. @mkahn5
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.datadriveninvestor.com/qwiklabs-baseline-data-ml-ai-7424313d64ae?source=search_post---------338,"There are currently no responses for this story.
Be the first to respond.
I have been wanting to find out more about Google Cloud Platform (GCP) and was shown by a Google Community Manager, Qwiklabs, a website where they have several “quests” as they call it. Each of these quests allows you to either learn more about GCP or AWS. I had some time to work on a quest and chose to work on Baseline: Data, ML, AI quest.
Overview
No explanation on why I start with that quest, since it will be fantastic for me to see what are the Data, ML and AI features that are made available at GCP. It was a great learning experience coupled with hands-on labs (Credits need to be purchased to activate most of the hands-on labs). In this particular quest, it allows you to see BigQuery (SQL) and BigTable (NOSQL) briefly so you can pick up basic SQL. You get to check out Google’s Entity Recognition and Speech-to-Text capabilities, coupled with data transformation/preparation and report creation. Not forgetting the most essential, Google’s ML engine.
www.datadriveninvestor.com
The hands-on labs are available for a limited period of time and they will create practice accounts for each lab. Although the labs are timed, you are given ample time to complete them. My experience thus far is they give about 20 to 30% buffer, enough to take a short washroom break.
They do provide, for some of the sections, “Next Steps/Learn More” which I encourage you to try it out.
Below, is an overview and some thoughts on the hands-on lab to help readers with their own learning. You may skip this part and move to the conclusion where I share quick thoughts on the interesting features in GCP for data scientists.
Sections
There are 13 sections altogether and it took me about 1.5 calendar days to complete, so readers have to commit a weekend perhaps to run through the sections.
1-Introduction to SQL for BigQuery and Cloud SQL
You will learn this hierarchy, Projects -> Database -> Tables. Here you will learn some basics of SQL, how to write simple queries using Select, From and Where clauses, sorting and creating summary statistics. There is also a sub-section tables management as well, like Union and Insert Into clauses.
My Note: Try to take advantage of the lab to look at other clauses as well. For more information, you can go to W3 School and practice it at the lab (Remember it uses credits).
2-Big Query: Qwik Start
BigQuery is the Enterprise Data Warehouse (EDW) in GCP. For this lab, it provides either a Web Interface or Command Line to interact with BigQuery. I tried out Command Line as I want to get myself familiar with it again, rather than point-and-click which can be difficult to replicate.
Here you will learn how to access the metadata of the table, list the datasets available in a project. Another interesting hands-on (at least to me), is the part about creating a dataset (“babynames”) and loading a raw file into a table (names2010). Subsequently, you will perform a simple SQL query on the table that you have loaded.
My Note: Try to load in other raw files into the dataset as well, having some repetitive exercise, loading raw files from other years, helps to ‘internalize’ your learning.
3-BigTable: Qwik Start
BigTable is GCP’s NoSQL database. In this lab, you will get to connect to a Cloud BigTable instance, perform basic administrative tasks, and read and write data in a table.
For me, there is an interesting concept here and that is called the “column families”, where it defines groups of columns together. I am not from a CS background and do not work with databases that often except for pulling data from it, this was an interesting point to me and I will look into it further.
My Note: Unfortunately, I feel the lab is too short and could have allowed the participants to try some hands-on on querying a NoSQL database, if possible. Overall, it is a pretty short hands-on.
4-Cloud Natural Language and Cloud Speech API
This is a very interesting lab, both of them, but especially the Cloud Natural Language API.
In the Cloud Natural Language API, you get to pass in a sentence through a “gcloud” command and use the API to recognize entities in the statement. Of course, the Natural Language API can do more than entity recognition, like sentiment analysis, syntax analysis and so on. What is interesting to me at least is that if there is a Wikipedia Page on the entity it recognizes, the link will be provided as well.
For the Cloud Speech API, there is a ready-made audio file which you will be given encoding details. The details will be entered into a JSON file which you will “curl” into the API with the key.
This hands-on I find it lacking as I will prefer Qwiklabs point to a site to create more audio files and use these audio files with encoding details provided as further exercises.
My Note: For the Cloud Natural Language API, I had more fun with it as I parse in different sentences to see how good the API is and was good…limited to the examples I tried with, of course. So come up with other sentences and have fun with it. For the Cloud Speech challenge, I was technically challenged to try further, moreover, the time for the lab is pretty short to experiment further.
5-DataProc (GUI or Command Line)
DataProc is a cloud service for spinning Apache Spark and Apache Hadoop clusters. It's a straight forward hands-on that requires domain details to set up the computation cluster. You will be asked to change the number of “workers” as well. It's a very straightforward lab, not much variation you can introduce in to make the hands-on more fun (i.e. more learning points).
6-DataPrep
DataPrep is a data service for visually exploring, munging, and preparing data for analysis. This is a third-party software that resides in GCP. The software is called “Trifacta” Not to give too much away, but you will be building a data flow, that is very good visually, to see what are the steps taken to transform the data. What I like about it is that you can explore the data and record down the data transformation steps you want. The data visualization helps a lot in determining how the data is after certain transformations. It cuts down a lot of work compared to a programming language (not saying that it is bad, just the weakness it has) to determine how to clean the data. You can also do quick summary statistics as well if needed, all within the same interface.
My Note: The time given by the lab is ample thus after the last task, do try to navigate and play around with the data pipeline/flow you have created.
7-Google Cloud Datalab
In this section, it is presenting GCP’s ability to have notebooks in its environment. In addition, you will still have the git capability like commit and push etc.
8-Cloud ML Engine
So this is the fun and critical part of the whole quest…. machine learning! Here you can train a ready model from Tensorflow and run it locally or into the cloud (but my poor computer science background still cannot figure out the difference since both of them are on the cloud rather. Someone can explain to me?).
In this section, the lab uses a familiar dataset, the US Census Income data…well you know the above 50K or below 50K….that dataset? Remember…..? So if you are familiar with the dataset, we are doing a classification problem. In the lab, only one model is used and that is DNNCombinedLinearClassifier. You will not be learning a lot about machine learning, just to let you know first, but rather more on the ML function and features in GCP…just to let you know. You will get to deploy the model that you have trained too.
GCP provides TensorBoard (of course!) for you to look at your model training progress so that you can have an idea if there is a need for further tweaking or management of the training process.
I was looking through the steps and trying to look for ways to implement other classification models but do not see where I can change it though. My suspicion is that I have to nano/vim into trainer.task? So if you can try to use other machine learning models offered by TensorFlow.
9-Data Studio
Data Studio is the place where you can either build up your reports or dashboards (By the way, its FREE to use). In the hands-on, you get to pull a dataset from BigQuery to create a time-series data followed by styling the report (well it is just adding a background color to the visual) and adding a textbox at the top of the ‘report’. I can sense that pushing out Data Studio is not a priority in the whole quest so they only showed very few features in it. Pretty meh…if you asked me. There is really so much opportunity to show how a dashboard can be quickly created and layout.
Anyway, the interface is pretty similar to QlikSense and SAS Visual Analytics. My preferred visual tools are still R or Python. But point-click interface will definitely have a market.
10-Google Genomics: Qwik Start
Here you will be asked to work with the Cloud Genomic Pipelines API. I do not work with Genomics often so was not able to relate much to it but went with the motion and complete the lab because I want to complete the quest! :)
11-DataFlow
In this section, participants get to set up Python development environment and implement Python code. Again, the lab is more of going through the motion to completion rather. I would have preferred they get the participants to download python codes from a Google Drive perhaps, have it uploaded and then have it executed. Its closer to the real actual environment rather and it builds a more concrete and relatable experience.
12-Cloud Filestore
Personally, I feel this section is more about data engineering rather than data science. Think it will be great to have the lab done in a business scenario/setting so participants will understand why certain steps are taken. Having that understanding will help participants to understand the value of Filestore.
For the last section of this quest, it was pretty much going through the motion of clearing it and know the features of Filestore. The learning experience here does not ‘stick’ because of the lack of data engineering background, unfortunately.
Tips for Learning
Below are some tips from me on making this a great learning experience.
Conclusion
Although some of the sections are more of going through the motion, I still enjoyed it and have a better idea of GCP. In some areas, I can see the ease of using it like DataPrep. The visualization really enables better and faster data preparation. The quest also makes me want to improve my “command line” skill. (I started in the DOS era but went to Windows pretty quickly). I enjoyed the section on DataPrep, ML Engine, Natural Language and Speech API.
I will definitely be keen to work through other quests with the necessary support and resources available. As mentioned, I run a tech community (together with a group of friends) and I have seen a higher take-up of GCP for tech firms and for readers that are interested in the field, do try out Qwiklabs to familiarize yourself with GCP.
Also, I had the pleasure of leading a Cloud Study Jam with members from the following groups participating. They are BigDataX, DataScience SG (my co-founded data science community) and PyData Singapore. It was a great experience and I had a lot of fun doing it. :)
I hope the blog has been useful to you. I wish all readers a FUN Data Science learning journey and do visit my other blog posts and LinkedIn profile.
empowerment through data, knowledge, and expertise.
60 
60 claps
60 
empowerment through data, knowledge, and expertise. subscribe to DDIntel at https://ddintel.datadriveninvestor.com
Written by
AI & DS Advocate Newsletter@https://koopingshung.substack.com/ Website@https://koopingshung.com/
empowerment through data, knowledge, and expertise. subscribe to DDIntel at https://ddintel.datadriveninvestor.com
"
https://medium.com/@qwiklabs/sharing-is-caring-c4b624cfd423?source=search_post---------339,"Sign in
There are currently no responses for this story.
Be the first to respond.
Qwiklabs
Apr 24, 2020·3 min read
Training is a very important part of your career and getting trained in Google Cloud Platform can definitely take you places. If you are one of those people who needs to train their team and upgrade their #gcpskills, now is the right time to do it. In case you are wondering how “Share Groups” is the answer to your question.
Share groups help you in sharing the Qwiklabs credits in your account with your friends and colleagues. If you want to use Qwiklabs for training your team, you can purchase credits in bulk here.
Let us see how to create share credits step by step.
3. You will be directed to the “Share Credits” section where you need to click on the “CREATE A SHARE GROUP” button to create a new Credit Sharing Group.
4. Add a Group Name for the new share group, provide a brief description, and then click the Update button.
5. You can create as many groups as you like. Once you’ve created your group (or groups), open it by clicking on the Group Name to add Qwiklabs credits and share group members.
6. Add credits to the share group by typing the number of credits you want to share. Then click Share credits.
7. Add users to the share group by entering email addresses. You can enter as many email addresses as you like, one per line. You can add people who are not yet Qwiklabs users.
Optionally, you will be able to send a notification prompting share group members to create a Qwiklabs account to access credits. This will be added to the automatic system notification.
Important: Click Add email filters. If you do not click this button, your changes will not be saved.
If you are facing any difficulties with share groups and share credits you can reach out to support@qwiklabs.com. They are available 24/7 in five languages.
And speaking of sharing the wealth, you can enter code 1q-share-038 for 3 free credits (valid through May 1st).
64 
64 claps
64 
About
Write
Help
Legal
Get the Medium app
"
https://codeburst.io/kubernetes-on-gcp-simplicity-vs-flexiblity-a7e9511dcc7f?source=search_post---------340,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Exploring different ways of running Kubernetes on Google Cloud Platform (GCP).
Upon reading the Kubernetes documentation, you will observe that there are three fundamentally different ways of running Kubernetes on GCP.
Please note: There are other solutions that are outside of the scope of this article; RedHat OpenShift comes to mind.
Broadly speaking, the trade-off when selecting a choice is between simplicity and flexibility. Interestingly, cost is not a significant factor as running your own high-availability control plane on GCE is comparable to the cost of GKE that provides it for you.
Both the GKE and turnkey solutions provide you with simple automated mechanisms to perform common tasks (for example, scaling up the number of worker nodes). The kubeadm solution does not; you have to build them yourself.
In the kubeadm and turnkey solutions, you have direct access to the control plane nodes and thus can change fundamental Kubernetes behavior. In the GKE solution, you do not.
Having used the GKE and played with the kubeadm solutions, I recently explored using the open-source turnkey solution hoping to get the best of both worlds. Rather, in my opinion, I got the worst of both. For example, I was surprised to find that the open-source turnkey solution forced you into using the Google Compute Engine (GCE) pod network add-on with no clear way to switching to using another one.
I was also surprised by the amount of complexity in both the installation and maintenance scripts, and — more importantly — in the node startup and shutdown scripts. Being supported by the open-source community, I had visions of myself having to scour thousands of lines of shell scripts for a fix under the gun.
Being disappointed with the open-source turnkey solution, I decided to explore what it would take to implement an automated mechanism to perform a common task; scaling up worker nodes, on top of a kubeadm solution.
Without any automation, bringing up a worker node entails:
Bringing down a worker node entails draining and deleting it.
Much of the automated solution is easily provided by leveraging a number of GCP features, including:
The challenge, however, is dealing with the bootstrap token; creating it, and getting it to the instance’s startup script.
The following solution handles this by running a recurring workload in the Kubernetes cluster to regularly create bootstrap tokens (you can manage them using ConfigMaps) and store them as a secret in GCP Secrets Manager. The instance startup scripts can then retrieve this secret and thus be authenticated/authorized to join as a worker node.
If you wish to follow along, you will need:
We first create a GCP project for our Kubernetes cluster and enable the following APIs:
We initialize gcloud by executing:
To validate that we can ssh into instances, we create an instance by executing:
We SSH into it by executing:
And then we delete the instance:
Here we create a custom image that implements all the requirements outlined in the Kubernetes documentation Installing kubeadm. This image will serve as the base for our Kubernetes control plane and worker node instance templates.
While, earlier, we authenticated the gcloud tool, here we also authenticate SDK libraries:
Changing out the project_id (to your project’s id)and zone (your preferred zone), from the project’s k8s-node folder, we execute:
While the k8s-node.json Packer script at first seems complicated, it simply automates the commands provided in the Kubernetes documentation.
We create the infrastructure that supports the Kubernetes cluster.
The resources are:
Please note: In a later step we will create a second instance group, k8s-wk, to manage the instances that will automatically become worker nodes (not shown).
We first need to create a service account, e.g, terraform, with the Owner role to enable Terraform to manage the resources in our GCP project. For this account, we create a new key pair and download the generated JSON file. Copies of this file get placed into the cp and wk project folders as account.json.
Please note: While we are preparing the Terraform configuration in the wk folder here, we will actually be using it in a later section.
We create a file in the project folder, cp/terraform.tfvars, changing it with the specifics for our project:
We similarly create a file in the project folder, wk/terraform.tfvars:
From the cp folder we execute the following to initialize the Terraform configuration:
We then create the infrastructure:
Under the hood, the main.tf file does all the heavy lifting.
In this section, we will bring up the Kubernetes control plane on the single instance that was created in the previous section.
In preparation for this, we observe the external static IP address used by the load balancer; we need this in a following step.
We ssh into the instance; changing the instance name to match ours:
Please note: Depending on your gcloud configuration, you may be required to provide the zone option in this command (seems weird to me to have to do this).
Using the observed external static IP address to update the control-plane-endpoint option, we execute:
Things to observe:
This command outputs a lot of information. We need to keep the discovery-token-ca-cert-hash value in a safe location (we will be using it later).
To allow us to manage the cluster resources using our non-root account, we execute:
We verify that we have access:
Our control plane node is not ready as we have not yet installed a pod network add-on; in this case, we will use Calico by executing:
Things to observe:
After a few minutes, our control will be ready:
As you may have noticed, bringing up the control plane also provided us a token that we did not save; this is a bootstrap token that is used to authenticate /authorize when we want to join additional control plane or worker nodes. The problem with these tokens is that they are short-lived (24 hours).
We are going to address this problem by running a recurring workload every 12 hours in the cluster that generates bootstrap tokens (that expire in 24 hours) and stores them as a secret in GCP Secret Manager (technically two secrets). We start by creating a container image for the workload.
First, we need to create a Docker Hub repository, e.g., in my case sckmkny/bootstrapper.
From the bootstrapper-image project folder, we execute (updating the Docker Hub repository value):
We then push the image to Docker Hub (updating the Docker Hub repository value):
Under the hood, the bootstrapper file does all the heavy lifting. This shell script creates Kubernetes bootstrap tokens and stores them as GCP Secret Manager secrets: bootstrapper-id and bootstrapper-secret.
Here we schedule the workload.
We first need to be able to manage cluster resources from our workstation by executing (replacing the IP address with our control plane node’s external ephemeral IP address):
We confirm that we have access from our workstation:
We observe our kubectl configuration context; it should be the same as shown:
We create a file, bootstrapper-tf/terraform.tfvars, updated with the specifics for our project:
From the bootstrapper-tf folder we execute the following to initialize the Terraform configuration:
And then schedule the workload:
The heavy lifting is done in the main.tf file. It creates:
While the workload manages the bootstrapper-id and bootrapper-secret GCP Secret Manager secrets, we need to manually create three others:
We create the third, bootstrapper-config, by executing:
Now that we have all the necessary secrets in place, we can bring up a worker node.
From the wk folder we execute the following to initialize the Terraform configuration; remember we prepared this folder in an earlier step:
And bring up the worker node:
The heavy lifting is done in the main.tf file. It creates an
The startup-script and shutdown-script metadata values are the key to these instances automatically joining / leaving as worker nodes.
We validate that we do indeed have a worker node:
From this point forward, we can scale up / down the number of worker nodes by simply adjusting the instance count of the k8s-wk instance group.
While tangential to the stated problem, for completeness we bring up another control plane node to make for a highly available control plane (the Kubernetes documentation actually suggests that it best to have three control plane nodes).
We first ssh (changing instance name for our project) onto the existing control plane node to upload the certificates (they are removed after two hours):
and execute:
Take note of the provided certificate key that we will use next. We also observe (and save for later) a bootstrap token using:
We temporarily update the k8s-cp load balancer backend to send all traffic to the existing control plane node; rather than to the k8s-cp instance group.
Please note: Unfortunately, I lost an hour or so (and a bit of hair) before realizing that we needed the previous step.
We then scale up the k8s-cp instance group, e.g., from one to two instances.
We ssh into the new instance (changing the instance name and zone as appropriate):
We then execute the following (updating the IP address, token, discovery-token-ca, and certificate-key values as appropriate) to join the cluster:
We can confirm that we need to have two control plane nodes:
We finally revert the k8s-cp load balancer backend to send all traffic back to the k8s-cp instance group.
To clean up all the work that was done, we essentially go through the steps in reverse (deleting instead of creating):
Hope you found this useful.
Bursts of code to power through your day.
22 
22 claps
22 
Written by
Broad infrastructure, development, and soft-skill background
Bursts of code to power through your day. Web Development articles, tutorials, and news.
Written by
Broad infrastructure, development, and soft-skill background
Bursts of code to power through your day. Web Development articles, tutorials, and news.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/gcp-essentials-the-console-b2e7f9be1921?source=search_post---------341,"There are currently no responses for this story.
Be the first to respond.
GCP console is a powerful graphical tool to manage all your Google Cloud Platform and this new short video covers some of the core features to get you started building and managing your applications on GCP :
Do you know about the resource/project/folder/organization hierarchy?
Have you use the search bar to find resources or projects?
Do you use the console activity stream to see who’s done what on projects and resources?
Have you used the GCP mobile app?
Find out about all of this and more in this short video and subscribe to the GCP Essentials series: bit.ly/GCP-essentials
Google Cloud community articles and blogs
5 
5 claps
5 
Written by
Google Cloud Developer Relations
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Google Cloud Developer Relations
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alexismp/new-gcp-essentials-episode-developer-and-management-tools-30dd114f3944?source=search_post---------342,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alexis MP
Mar 11, 2019·1 min read
This latest episode in the GCP Essentials series on the Google Cloud Platform YouTube channel is dedicated to the command line tools (mostly gcloud) and Cloud Shell.
gcloud is pretty well thought out and what I like above all is how GCP Web Console offers you gcloud equivalents to steps you’re taking in the Web UI.
I personally find myself using both tools a lot and the latest Cloud Shell web editor has me building most of my apps in a browser; something I would have found silly just a few years ago.
One thing I did not mention in the video is the new and pretty awesome ability to “Open in Cloud Shell” — this offers your users the ability to clone and open a GitHub repo in Cloud Shell by simply clicking on a URL.
Also, if you’ve used Cloud Shell before, this video could still teach you a couple of advanced features. Check it out!
Google Cloud Developer Relations
8 
8 
8 
Google Cloud Developer Relations
"
https://medium.com/@jaychapel/looking-for-a-google-cloud-instance-scheduling-solution-as-you-wish-49237af25712?source=search_post---------343,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Oct 14, 2020·8 min read
Like other cloud providers, the Google Cloud Platform (GCP) charges for compute virtual machine instances by the amount of time they are running — which may lead you to search for a Google Cloud instance scheduling solution. If your GCP instances are only busy during or after normal business hours, or only at certain times of the week or month, you can save money by shutting these instances down when they are not being used. So can you set up this scheduling through the Google Cloud console? And if not — what’s the best way to do it?
This post was originally written by Bill Supernor in 2018. I have revised and updated it for 2020.
As mentioned, depending on your purchasing option, Google Cloud pricing is based on the amount of time an instance is running, charged at a per-second rate. We find that at least 40% of an organization’s cloud resources (and often much more) are for non-production purposes such as development, testing, staging, and QA. These resources are only needed when employees are actively using them for those purposes — so every second that they are left running when not being used is wasted spend. Since non-production VM instances often have predictable workloads, such as a 7 AM to 7 PM work week, 5 days a week, the other 64% of spend is completely wasted. Inconceivable!
The good news is, that means these resources can be scheduled to turn off during nights and weekends to save money. So, let’s take a look at a couple of cloud scheduling options.
If you were to do a Google search on “google cloud instance scheduling,” hoping to find out how to shut your compute instances down when they are not in use, you would see numerous promising links. The first couple of references appear to discuss how to set instance availability policies and mention a gcloud command line interface for “compute instances set-scheduling”. However, a little digging shows that these interfaces and commands simply describe how to fine-tune what happens when the underlying hardware for your Google virtual machine goes down for maintenance. The options in this case are to migrate the VM to another host (which appears to be a live migration), or to terminate the VM, and if the instance should be restarted if it is terminated. The documentation for the command goes so far as to say that the command is intended to let you set “scheduling options.” While it is great to have control over these behaviors, I feel I have to paraphrase Inigo Montoya — You keep using that word “scheduling” — I do not think it means what you think it means…
The next thing that looks schedule-like is the GCP Cron Service. This is a highly reliable networked version of the Unix cron service, letting you leverage the Google App Engine service to do all sorts of interesting things. One article describes how to use the Cron Service and Google App Engine to schedule tasks to execute on your Compute Instances. With some App Engine code, you could use this system to start and stop instances as part of regularly recurring task sequences. This could be an excellent technique for controlling instances for scheduled builds, or calculations that happen at the same time of a day/week/month/etc.
While very useful for certain tasks, this technique really lacks flexibility. Google Cloud Cron Service schedules are configured by creating a cron.yaml file inside the app engine application. The GCP Cron Service triggers events in the application, and getting the application to do things like start/stop instances are left as an exercise for the developer. If you need to modify the schedule, you need to go back in and modify the cron.yaml. Also, it can be non-intuitive to build a schedule around your working hours, in that you would need one event for when you want to start an instance, and another when you want to stop it. If you want to set multiple instances to be on different schedules, they would each need to have their own events. This brings us to the final issue, which is that any given application is limited to 20 events for free, up to a maximum of 250 events for a paid application. Those sound like some eel-infested waters.
Google Cloud Platform and ParkMyCloud — mawwage — that dweam within a dweam….
Given the lack of other viable instance scheduling options, we at ParkMyCloud created a SaaS application to automate instance scheduling, helping organizations cut cloud costs by 65% or more on their monthly cloud bill with AWS, Azure, and, of course, Google Cloud.
We aim to provide a number of benefits that you won’t find with, say, the GCP Cron Service. ParkMyCloud’s cloud management software:
While it depends on your exact schedule, many non-production Google Cloud VMs — those used for development, testing, staging, and QA — can be turned off for 12 hours/day on weekdays, and 24 hours/day on weekends. For example, the resource might be running from 7 AM to 7 PM Monday through Friday, and “parked” the rest of the week. This comes out to about 64% savings per resource.
Currently, the average savings per scheduled VM in the ParkMyCloud platform is about $245/month. That does not account for any additional savings gained from rightsizing.
If you’re not quite ready to start your own trial, check out this interview with Workfront, a work management software provider. Workfront uses both AWS and Google Cloud Compute Engine, and needed to coordinate cloud management software across both public clouds. They required automation in order to optimize and control cloud resource costs, especially given users’ tendency to leave resources running when they weren’t being used.
Workfront found that ParkMyCloud would meet their automatic scheduling needs. Now, 200 users throughout the company use ParkMyCloud to:
Google has done a great job of creating offerings for customers to save money through regular cloud usage. The two you’ll see mentioned the most are sustained use discounts and committed use discounts. Sustained use discounts give Google Cloud users automatic discounts the longer an instance is run. This post outlines the break-even points between letting an instance run for the discount vs. parking it. Sustained use discounts have also been expanded with resource-based pricing, which allows the sustained use to be applied based on your use of individual vCPUs and GB of memory regardless of the machine type you use.
Committed use discounts, on the other hand, require an upfront commitment for 1 or 3 years’ usage. We have found that they’re best applicable for predictable workloads such as production environments. There are also the pre-emptible VMs, which are offered at a discount from on demand VMs in exchange for being short-lived — up to 24 hours.
In addition to these discounts, you can also save money by rightsizing your instances. Provisioning your resources to be larger than you need is another form of cloud waste. On average, rightsizing a resource reduces the cost by 50%. Google Cloud makes it easy to change the CPU or the Memory amounts using custom instance sizes. If you’d rather use standard sizing, they offer that as well. By keeping an eye on the usage patterns of your servers, you can make sure that you’re getting the most use of the resources you are paying for.
Getting started with ParkMyCloud is easy. Simply register for a free trial with your email address and connect to your Google Cloud Platform to allow ParkMyCloud to discover and manage your resources. A 14-day free trial free gives your organization the opportunity to evaluate the benefits of ParkMyCloud while you only pay for the cloud computing power you use. At the end of the trial, there is no obligation on you to continue with our service, and all the money your organization has saved is, of course, yours to keep.
If you do choose to continue, our Google Cloud scheduler/optimizer pricing is straightforward. You will choose a functionality tier and pay per resource per month. There is a free forever tier available — so have at it.
Have fun storming the castle!
Originally published at www.parkmycloud.com on October 12, 2020.
CEO of ParkMyCloud
18 
18 
18 
CEO of ParkMyCloud
"
https://medium.com/serverlessguru/google-cloud-build-push-docker-images-via-github-trigger-522441505e05?source=search_post---------344,"There are currently no responses for this story.
Be the first to respond.
In this article, we are going to dive into the world of CI/CD pipelines. There are a lot of different options for incorporating a CI/CD into your development workflow. This article will focus on leveraging Google Cloud Build to handle the heavy lifting around creating and storing docker images.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@ratrosy/set-up-serverless-store-part-3-computing-cron-jobs-and-management-tools-34d51475df70?source=search_post---------345,"Sign in
There are currently no responses for this story.
Be the first to respond.
Ratros Y.
Jan 24, 2019·6 min read
This document is a part of the Serverless on Google Cloud Platform: an Introduction with Serverless Store Demo How-to Guide. It discusses the computing, cron jobs, and management tool solutions Serverless Store uses.
Now with all the products and services set up you are ready to deploy the app and the functions.They are deployed to Google App Engine and Google Cloud Functions respectively. App Engine is a managed serverless platform for highly scalable applications, and Cloud Functions is event-driven serverless platform for small, independent units of code. To learn more, see App Engine Documentation and Cloud Functions Documentation.
Serverless Store consists of one App Engine app and 6 Cloud Functions. The easiest way to deploy them is to use Google Cloud Build. Cloud Build lets you build software quickly across all languages; you can set up a custom workflows for building, testing, and deploying across multiple environments such as VMs, serverless, Kubernetes, or Firebase.
If you prefer not using Cloud Build, you may use Cloud SDK to deploy the artifacts one by one instead; for more information, see Deploying Serverless Store without using Cloud Build.
First, enable Cloud Build API in your project:
At this moment, the Serverless Store project uses placeholder configurations. Cloud Build can help you replace them with your project-specific values; in this tutorial, you will upload those custom configurations in a Cloud Storage bucket:
Go back to the root folder. cloudbuild.yaml specifies each step Cloud Build takes to build and deploy your app. In this tutorial, it takes Cloud Build 10 steps to deploy everything:
To trigger a build, run command
Give Cloud Build a few minutes to get things done. You can view the progress via the link in the output.
Your app is now available at YOUR-PROJECT-ID.appspot.com. You can view all the App Engine services via App Engine Services page in Google Cloud Console. Before you open the app, there are two more things to do:
Now, open the app and try signing in with your Google account, adding some items, purchasing an item, and watch the events flow via Google Data Studio.
Google Cloud Platform offers a product, Google Cloud Scheduler, for scheduling tasks to run periodically, either on Google Cloud Platform or on a local server. You can, for example, ask Cloud Scheduler to run a Cloud Function every week to remind people of the items they left in the cart in Serverless Store.
To set up this task with Cloud Scheduler:
Google Cloud Platform offers a variety of tools to help you manage your deployments inside and outside Google Cloud Platform. You can use Stackdriver Logging to collect logs, Stackdriver Monitoring to get real-time updates about your app, Stackdriver Trace to identify performance bottlenecks in your code, and many more. See here for a full list of Google Cloud Platform Management Tools.
Logs from Serverless Store are automatically collected in Stackdriver Logging.
To view the logs from the flask app:
To view the logs from the Cloud Functions:
You can view, search and group the logs. They can also be exported to Cloud Storage and Google BigQuery. See Stackdriver Logging Documentation for more information.
To view the status of your deployments, click Monitoring in the left side bar of the Cloud Console. You will be redirected to Stackdriver Console, where you can check the metrics of the Serverless Store app and related functions.
See App Engine Metrics and Cloud Functions Metrics for a complete list of metrics you can monitor.
Stackdriver Trace allows developers the check how long one specific block of code takes to run, and helps identify the slowest link in an execution path. App Engine deployments have Stackdriver Trace enabled by default; you can view the times App Engine takes to serve each request in the Stackdriver Trace page of the Cloud Console. Alternatively, you can also create custom traces non-specific to requests; Serverless Store, for example, includes a custom trace that tracks the three steps of payment processing: order preparation, event streaming, and payment processing:
Note that the custom trace spans across two different products (payment processing happens in a Cloud Function) as if it happens inside the same deployment. Every time a customer makes a payment, you can view the trace in Cloud Console and see which step takes the most time to finish, and make optimizations accordingly to create a better user experience.
To view the custom trace:
See Further Discussion for some tips and notes about Serverless Store.
Developer Relations @ Google Cloud Platform
32 
32 
32 
Developer Relations @ Google Cloud Platform
"
https://medium.com/google-cloud/automate-building-android-apks-with-google-cloud-build-ci-cd-and-a-gradle-docker-image-%EF%B8%8F-44e48f76756f?source=search_post---------346,"There are currently no responses for this story.
Be the first to respond.
Google Cloud Build(GCB) is a service that executes your builds on Google Cloud Platform infrastructure. Cloud Build can import source code from Google Cloud Storage, Cloud Source Repositories, GitHub, or Bitbucket, execute a build to your specifications, and produce artifacts.
You can write a build config to provide instructions to Cloud Build on what tasks to perform and can either use the build step/builders provided by Cloud Build , Cloud Build community or write your own custom build steps/builders.
You can also manually start builds in Cloud Build using the gcloud command-line tool or the Cloud Build API, or use Cloud Build’s trigger feature to create an automated continuous integration/continuous delivery (CI/CD) workflow that starts new builds in response to code changes on your code repository.
In this tutorial, you would setup a Cloud Build trigger that builds your android application and uploads it to a Cloud Storage Bucket.The builds of new APK bundle would also be automatically triggered once code is pushed code to your code repository.
As at the time of this writing, GCB Builders supports Gradle 4.6 as it’s latest Gradle Cloud Builder which is not compatible with the Android project. However, I created an updated Gradle Docker Image which we will use to perform gradle builds for our Android project.
This is where your project APKs will be stored, you are expected to provide a unique Bucket name, I will be using fullstackgcp-apk-builds as my Bucket name.
Make Bucket Public (Optional)
If you would like to access your APKs from your bucket publicly ( click here for mine), then you can set Bucket-level permission to allow anyone on the internet access to your Bucket files.
To do so, click Add Members on the *Permissions Tab of your Bucket and perform the following:
Note that: Your bucket is public and can be accessed by anyone on the internet
Cloud Build service account’s default permissions do not allow the account to manage a Cloud Storage bucket. To grant Cloud Storage IAM role to a Cloud Build service account, perform the following steps:
You need to ensure your application codes and all necessary files needed for building an APK are available on a code repository. Cloud Build currently supports Cloud Source Repositories, GitHub, or Bitbucket.
The demo source codes for this tutorial is available on GitHub, you can access them here.
In your repository, create a build configuration file: cloudbuild.yaml, which contains instructions for Cloud Build. The configuration file for this tutorial is:
In a nutshell, Cloud Build helps you run the docker command below:
docker run -v $(pwd):/home/app --rm gcr.io/fullstackgcp/gradle /bin/bash -c 'cd /home/app && ./gradlew clean assembleDebug'
In the command, we specify: -v which mounts our current directory as the volume, — rm which removes the container on exit.
You can change the -c command on your cloudbuild.yaml file if you would like to use other Gradle commands.
Cloud Build also copies the output: app-debug.apk into your GCS Bucket as app-debug-$SHORT_SHA.apk , where $SHORT_SHA is the first seven characters of COMMIT_SHA of the commit which triggered Cloud Build, it is meant to tag the APK builds on your GCS Bucket.
Cloud Build trigger listens to changes in your code repository, follow the steps below to create a GCB trigger.
To prevent unnecessary charges, clean up the resources created for this tutorial.
If you want to learn more about Cloud Build check out the following resources:
Thanks for reading through! Let me know if I missed any step, if something didn’t work out quite right for you or if this guide was helpful.
Google Cloud community articles and blogs
8 
5
8 claps
8 
5
Written by
Software / DevOps Engineer | Google Developer Expert for Cloud | https://timtech4u.dev
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Software / DevOps Engineer | Google Developer Expert for Cloud | https://timtech4u.dev
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@qwiklabs/you-tensorflow-for-poets-a-match-made-in-heaven-3d487fa86390?source=search_post---------347,"Sign in
There are currently no responses for this story.
Be the first to respond.
Qwiklabs
Feb 14, 2019·3 min read
Happy Valentine’s Day from all of us here at Qwiklabs and Google Cloud Platform! We brought you flowers — that is, we brought you Tensorflow for Poets, a lab that trains a machine learning Tensorflow classifier to identify images of daisies, roses, tulips, sunflowers, and dandelions. (Okay, that last one is technically a weed, but it’s still pretty.) Use code 1q-valentine-888 to get 36 free credits — almost as good as 3 dozen roses — to use on Qwiklabs.
– and you’ll have access to the variety of flowers in your beautiful bouquet, all ready for classification!
These labels are important. Tensorflow doesn’t require you to go through the laborious work of writing code that detects any possible variation in features of these pictures — instead, it uses deep learning, which classifies the data it is given simply by using the names of the files. The learning process for the network takes about twenty minutes, so you can polish off some chocolate kisses while you wait.
99% certainty? Not too shabby.
Try out Tensorflow for Poets for yourself! And if you’re interested in taking a deeper dive into using Tensorflow to build on machine learning, check out the Tensorflow website and their Github project.
10 
3
10 claps
10 
3
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@thanachart-rit/managing-data-lifecycle-%E0%B8%95%E0%B8%AD%E0%B8%99%E0%B8%97%E0%B8%B5%E0%B9%88-2-store-bdcd98094d91?source=search_post---------348,"Sign in
There are currently no responses for this story.
Be the first to respond.
Thanachart Ritbumroong
Jan 22, 2017·2 min read
มาต่อกันกับตอนที่ 2 ของการจัดการ Data Lifecycle ด้วย Google Cloud Platform นะครับ ใครยังไม่ได้อ่านตอนแรก ก็แวะไปอ่านกันได้ครับ เป็นเรื่องเกี่ยวกับ Data Ingestion
Managing Data Lifecycle ตอนที่ 1: Ingest
บน GCP ก็มี Storage หลากหลายให้เราเลือกใช้ครับ แต่ละอันก็จะเหมาะสมกับรูปแบบ Data ที่หลากหลาย โดยแบ่งตามโครงสร้างของข้อมูล และ purpose ของการใช้งาน ตามรูปด้านล่างเลยครับ
ไล่ไปตั้งแต่ซ้ายสุดไปขวาสุดนะครับ อันแรก ก็คือ Cloud SQL อันนี้ไม่มีไรมาก ก็คือ MySQL ธรรมดาครับ แต่ Google จะเรียกว่า Managed MySQL ก็คือ เราไม่ต้องไปดูแลเครื่องอะไรทั้งสิ้น เค้าจัดการให้หมด วิธีใช้ก็ง่ายๆ ก็เข้าไป create instance ขึ้นมา แล้วก็ใช้ SQL commands ในการสร้าง table หรือ query ดูข้อมูล
ตัวอย่าง ก็คือ เราไปสร้าง App บน App Engine, Container Engine, หรือ Compute Engine เป็น Layer หนึ่ง แล้วก็มาสร้าง Database Layer บน Cloud SQL เพื่อเก็บข้อมูลพวก transaction ต่างๆ
คอนเซปก็คือ Data Warehouse พร้อมใช้บน Cloud ครับ มีคนดูแลจัดการให้เสร็จสรรพ เน้นรองรับการ Query หนักๆ เยอะๆ ครับ ยุคแรกนี่ทำตัวเป็น DW สมบูรณ์แบบมากเลย คือ insert ได้อย่างเดียว update ไม่ได้ ถ้าเผลอ insert ผิด ก็ลบทำใหม่กันเลยทีเดียวครับ แต่ตอนนี้ยอมให้ update ได้แล้ว (ตอนที่เขียนยังเป็น beta version อยู่นะครับ)
เป็น NoSQL database บน Cloud ซึ่งก็จะไม่เหมือนพวก RDBS ตรงที่ จะไม่มีการ define schema ให้ชัดเจนครับ หรือที่เรียกกันว่า schema-on-read ครับ คือ จะมาสร้าง schema ตอนจะเรียกดูข้อมูล ตัวอย่างก็เช่นพวก JSON หรือ XML ครับ ข้อมูลที่เหมาะจะเก็บในรูปแบบพวกนี้ก็เช่น product catalog หรือพวก content บน web ครับ พวกนี้จะมีกิ่งก้านของข้อมูลที่ไม่ balanced เท่าไหร่
อันนี้ คือ high-performance NoSQL ครับ เป็นเทคโนโลยีที่ Google ใช้ในการจัดเก็บพวก Google Search, Google Analytics, Google Maps, และ Gmail ก็จะเหมาะกับพวก real-time application เอาไปเก็บพวก IoT data ก็ได้ครับ
เป็นส่วนสำหรับจัดเก็บ object storage สำหรับ structured และ unstructured data ไม่ว่าจะเป็น log files, database backup, export files, images, หรือ binary files เอาไว้ทำเป็น Data Lake ก็ได้ครับ ทำ Archive ของ ข้อมูลก่อนที่เราจะโยนเข้า Data Warehouse
Reference
cloud.google.com
Lecturer at Management of Analytics and Data Science Program, National Institute of Development Administration, Thailand and Data Analytics Consultant
See all (501)
17 
17 claps
17 
Lecturer at Management of Analytics and Data Science Program, National Institute of Development Administration, Thailand and Data Analytics Consultant
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@qwiklabs/networking-you-3179d82758cc?source=search_post---------349,"Sign in
There are currently no responses for this story.
Be the first to respond.
Qwiklabs
Apr 18, 2019·2 min read
Networking on the Google Cloud Platform certification is one of the most in-demand qualifications on the market — employers on LinkedIn say that becoming certification can increase an applicant’s chances of being hired exponentially. Across GCP and Qwiklabs, there are countless hands-on labs and events you can attend that help practice the skills needed to ace your exam.
Networking in the Google Cloud — You just can’t cloud compute without networking. It’s the structure that connects all your resources across the GCP! Learn the basics with this quest on networking tools like VPCs and enterprise-grade load balancers.
Network Performance and Optimization — Cover real life issues like network bottlenecks and content caching, and learn how to overcome them. This quest is optimal for anyone who wants to work on the kinds of problems they’ll troubleshoot on the exam and in real life.
It’s not a secret that Qwiklabs quests are a great resource for test preparation — and it’s not the only great resource on the scene! Check out what technical curriculum developer Phillipp Maier had to say on getting certified:
Self-paced learning is convenient and efficient, but sometimes people prefer to study in groups. Why not join in on your local study jam and work your way through quests with other learners in your community. If there isn’t a study jam in your area, why not create your own? Networking in the Google Cloud can be the first quest on your agenda!
Finally, if you’re brand new to Qwiklabs, there’s no better introduction to the future of Google Platform than Next ’19, which took place earlier in April. Check out the keynotes and panels, and keep up with all Google Cloud has to offer on the GCP YouTube channel; be it networking or otherwise.
58 
58 claps
58 
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/twigcp-2019-week-7-800582196ac1?source=search_post---------350,"There are currently no responses for this story.
Be the first to respond.
Welcome to the weekly Google Cloud Platform newsletter, served every Monday from medium.com/google-cloud/weekly !
Here are the main stories for this past week :
“Process paperwork pronto with the new Google Docs API”(Google blog). Google Docs is a great place to collaborate and now you can use the flexibility of an API to do even more!
GCP Network tiers (GCP Documentation). Premium or Standard? Get control over network costs if you’d like!
“From CCIE to Google Cloud Network Engineer: four things to think about” (Google blog). Networking in the cloud era and key GCP best practices.
“How Box Skills can optimize your workflow with the help of Cloud AI” (Google blog). Vision API and Cloud Functions at work, all with code provided.
“Predicting Customer Lifetime Value with Cloud ML Engine: introduction” (Google blog). As with any GCP Solution this one comes with code and full details.
“Jib 1.0.0 is GA — building Java Docker images has never been easier” (Google blog). Idiomatic down to how you build docker images without installing docker or even writing a Dockerfile.
From the “lift-and-shift or cloud-native, there’s a solution for you” department :
From the “certifications from those who take them” department :
From the “MNIST on TPU and Cloud ML Engine serving” department :
From the “how about spending some time on a (free) quality codelab?” department :
From still my favorite “Customers and partners talk best about GCP” department :
From the “automate with scheduling” department :
From the “this week in Serverless” department :
From the “Beta, GA, or what?” department :
From the “all things multimedia” department :
That is all for this week!-Alexis
Google Cloud community articles and blogs
5 
5 claps
5 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Google Cloud Developer Relations
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/@qwiklabs/vpc-network-peering-is-all-your-projects-need-43da87ba1055?source=search_post---------351,"Sign in
There are currently no responses for this story.
Be the first to respond.
Qwiklabs
Jun 24, 2020·4 min read
Google Cloud Platform (GCP) Virtual Private Cloud (VPC) Network Peering enables private connectivity across two VPC networks irrespective of whether or not they belong to the same project or the same organization. In the lab VPC Network Peering you will build SaaS (Software-as-a-Service) ecosystems in GCP, making services which are available privately across different VPC networks within and across organizations, allowing workloads to communicate in private space.
Before we begin with this lab, let us look at the advantages which VPC Network Peering provides:
Now that you know what are the advantages of VPC Network Peering let us take a look at what all you are going to do in this lab
Before you move ahead with this lab let’s have a brief look at the set up of this lab. In order to perform this lab, you just need the Google Chrome browser a stable internet connection and some time to go through this lab step by step.
In order to go through with this lab, the first step is activating the Cloud Shell. In order to do this all you need to do is follow the instructions given. Once your cloud shell is activated you can move on to your next objective of setting up VPC Network Peering.
To proceed with the VPC Network Peering Setup, the first step is to create a custom network in projects. If you notice in this lab you will find that there are 2 project ids which are provisioned unlike other labs
The project ID 1 will be referred to as the Project A and the project ID 2 will be referred to as Project B throughout the lab. You will be managing two projects in this lab.
Your Cloud Shell is already open as you have activated it earlier. Now you need to create a new tab by clicking on the plus icon to set your Project B
You will be switching between tabs in the cloud shell as you are working on two projects. First you will create a custom network in Project A and follow all the instructions step by step as given in the lab instructions. You will be going through the following steps in both Project A and Project B
In this step you will be setting up a VPC Network peering session between Project A and Project B. Make sure you take a note of the project ID which is displayed at the top of the screen.
Let’s start with project A
Follow the lab instructions as given and you should see this screen
You will see the status of the peering will remain inactive because there is no network-b in the B project
Now you will work on Project B and repeat the same steps as you did for Project A. You should see a screen similar to this
Copy the Internal IP address of vm-a and change the project to Project B, you will automatically land on the VM instances page for Project B. Click on the SSH button of vm-b. Once you do that you will observe that you have successfully setup VPC peering across projects in a cloud environment.
Enter code 1m-freelabs-511 for 30 days free access to this lab and countless others. Happy learning!
20 
20 claps
20 
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@thanachart-rit/managing-data-lifecycle-%E0%B8%95%E0%B8%AD%E0%B8%99%E0%B8%97%E0%B8%B5%E0%B9%88-3-process-and-analyze-593d36b96b8a?source=search_post---------352,"Sign in
There are currently no responses for this story.
Be the first to respond.
Thanachart Ritbumroong
Feb 2, 2017·2 min read
ตอนที่ 3 แล้วครับของการจัดการ Data Lifecycle ด้วย Google Cloud Platform ใครยังไม่ได้อ่านตอนแรกๆ ก็แวะไปอ่านกันได้ครับ เป็นเรื่องเกี่ยวกับ Data Ingestion กับ Data Storage
Managing Data Lifecycle ตอนที่ 1: Ingest
Managing Data Lifecycle ตอนที่ 2: Store
มาเริ่มกันที่ Data Processing ครับ ทุกคนคงเคยอ่าน ได้ยิน หรือ ร่ำเรียนกันมาว่า ในการทำ Data Warehouse นั้น จะต้องทำ ETL — Extract, Transform, Load ในยุคที่ก้าวเข้าสู่ ฺBig Data เทคโนโลยีพวก ETL ธรรมดา ก็เริ่มจะรองรับไม่ไหวครับ เพราะข้อมูลมันมหาศาลมาก แล้วเวลาที่ใช้ในการ Process ข้อมูลก็ไม่เยอะ
ปกติการทำ ETL ก็จะมีการ run job ตอนกลางคืนครับ แต่เราก็ต้องรอพวก ERP หรือ Legacy System พวกนี้ ทำ End-of-day processing ให้เสร็จ ซึ่งบางที กว่าจะเสร็จก็ตีสอง ตีสาม ETL ก็ต้องรีบจัดการทุกอย่างให้เสร็จก่อนทุกคนเริ่มงาน เปิดดูรายงาน … ที่เจ็บปวด คือ บางที End-of-day processing เกิดล่าช้า … หรือ ถ้าเจอจำนวน transactions แบบก้าวกระโดด ETL ก็จะวิ่งหมุนวนไป หรือไม่ก็ตายไปเลยก็มี
บน GCP ก็จะมีทางเลือกในการ Process Data ได้ สองแบบ แบบแรก คือ Cloud Dataproc และแบบสองคือ Cloud Dataflow
ใครที่รู้สึกว่าทำ ฺBig Data แล้วไม่ได้ใช้ Hadoop หรือ Spark มันไม่ใช่ Big Data ขอเชิญใช้ Dataproce ครับ Service นี้เป็น Managed Hadoop และ Spark กล่าวคือ ไม่ต้องไปซื้อเครื่องมาติดตั้งเอง มานั่ง Set Up อะไรให้วุ่นวาย เวลาที่ใช้ในการสร้าง Cluster ใหม่แค่ 90 วินาทีเท่านั้นเอง!!
การสร้าง Cluster ใหม่ ก็ง่ายมากครับ Click ๆ เลือกเอาเลยจะเอากี่ node กี่ตัว กดปุ๊ปเสร็จปั๊ป โดยเราก็แค่ submit job โดยที่เราสามารถ submit job ได้หลากหลายแบบ ได้แก่
เป็น service สำหรับการทำทั้ง batch และ stream processing ซึ่งรองรับ programming language 2 ภาษา คือ Java และ Python (แล้ว R ของเค้าล่าาา) รองรับการทำ ETL และ MapReduce ต่างๆ มากมาย
ส่วนนี้บน GCP ก็มีให้เลือกมากมายหลากหลาก ตัวที่ใช้กันบ่อยๆ ก็คือ
Reference:
cloud.google.com
Lecturer at Management of Analytics and Data Science Program, National Institute of Development Administration, Thailand and Data Analytics Consultant
See all (502)
6 
6 
6 
Lecturer at Management of Analytics and Data Science Program, National Institute of Development Administration, Thailand and Data Analytics Consultant
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@jrodthoughts/technology-fridays-dataprep-completes-google-cloud-data-science-pipeline-ee7caf078d92?source=search_post---------353,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jesus Rodriguez
Oct 6, 2017·3 min read
Technology Fridays: DataPrep Completes Google Cloud Data Science Pipeline
Welcome to Technology Fridays! Today, I would like to discuss a recent addition to the Google Cloud platform that enables a fundamental building block for data science applications and one that is missing in most platform as a service(Paas) stacks. Data wrangling and curation has been the missing element in Paas technologies such as Google Cloud. Now the search engine giant is addressing that limitation with the launch of Cloud DataPrep and a very intriguing partnership with one of the new generation of data quality technology providers.
Conceptually, Cloud DataPrep is a native cloud service that enable the preparation and curation of datasets. In terms of the Google Cloud platform, DataPrep sits in the middle between data storage services such as BigQuery or BigTable and advanced data science such as DataView or Cloud Machine Learning.
In a somewhat surprising move, Google didn’t built DataPrep from the ground up. Instead, the cloud incumbent decided to adapt the technology from data wrangling market leader Trifacta to its suite of cloud services. We’ve previously discussed Trifacta in this blog before but you should think about it as one of the new generation data quality management platforms that has emerged as an alternative to traditional solutions from incumbents such as Informatica Data Services or SQL Server Data Quality Services. Trifacta leverages machine learning algorithms to streamline the traditionally manual data curation processes. The result is an incredibly sophisticated data wrangling suite of tools that streamline the quality curation of data across an enterprise data pipeline. Cloud DataPrep is a version of the Trifacta suite optimized for Google Cloud data services.
Developers can start using Google Cloud DataPrep directly from the Google Cloud Console. The first step in a Cloud DataPrep solution is selecting the datasets that need to be wrangled. DataPrep supports two main types of datasets: Imported and Wrangled. Imported datasets are a reference to source data residing in data storage systems such as Files, databases, big data file systems, etc. Complementarily, Wrangled datasets are used to transform the source data using Recipes.
Atomic data cleansing and transformation steps in Cloud DataPrep are abstracted using Recipes. Typically, Recipes are authored using the Transform Editor tool and converted into Wrangle which is DataPrep’s domain-specific language for data transformations. Wrangle provides a flexible syntax that express structural transformations across single and multiple data source models. At runtime, Cloud DataPrep intercepts the Recipes and executes the corresponding Wrangle code.
While Recipes represent atomic data cleansing steps, DataPrep Flows are orchestrations involving multiple Recipes. Flows are typically used in more complex data cleansing tasks such as merging multiple datasets, identifying statistical outliers in the source data or many other data curation processes.
Visual Profiling is another key capability of Cloud DataPrep . The platform’s profiling tools include interactive visualizations that highlight key statistical patterns in datasets and recommend the appropriate transformations. A rich user experience combined with sophisticated statistical and machine learning algorithms make Visual Profiling and almost enjoyable experience in Cloud DataPrep certainly contrasting with the cumbersome profiling process in traditional data quality management stacks.
Cloud DataPrep completes the circle in Google Cloud’s data science pipeline that includes services in areas such as storage, transformation, analytics and data science. More importantly, data wrangling services such as DataPrep are a strong differentiator of Google Cloud against competitive platform such as Azure or AWS.
Competition?
Google Cloud DataPrep is a unique offering in the Peas market. However, the platform faces competition from a new fast growing group of data quality management platform startups such as Paxata, Alation or Tamr. Additionally, Cloud DataPrep is likely to be perceived as an alternative to traditional data quality management incumbent solutions such as Informatica Data Services or Microsoft SQL Server Data Quality Services.
CEO of IntoTheBlock, Chief Scientist at Invector Labs, I write The Sequence Newsletter, Guest lecturer at Columbia University, Angel Investor, Author, Speaker.
4 
4 
4 
CEO of IntoTheBlock, Chief Scientist at Invector Labs, I write The Sequence Newsletter, Guest lecturer at Columbia University, Angel Investor, Author, Speaker.
"
https://medium.com/google-cloud/this-week-in-google-cloud-gke-1-7-f51f9877c990?source=search_post---------354,"There are currently no responses for this story.
Be the first to respond.
Google Cloud Platform news over the past couple of weeks included :
Continuing with the “The network is the computer” department :
This week’s Big Data news covers Apache Airflow, Dataflow, BigQuery, but also Genomics and our latest Data Engineering Coursera courses :
From the “Containers, Kubernetes, …” department :
From the “we’re stronger and smarter with partners” department :
From the “customers and users talk best about GCP” department :
From the “what would a weekly recap be without Machine Learning updates?” department :
From the “ICYMI” department :
If you haven’t caught up with the GCP Podcast recently, you probably missed three very fine episodes :
The pic of the week is said to be an example of the dynamic nature of protein target and is taken from the Silicon Therapeutics story above :
That’s it for this week!
-Alexis
Google Cloud community articles and blogs
3 
3 claps
3 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Google Cloud Developer Relations
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://blog.devgenius.io/how-to-provision-configure-deploy-to-google-cloud-platform-97dbbe36fcde?source=search_post---------355,"There are currently no responses for this story.
Be the first to respond.
Provision, Configure & Deploy to Google Cloud Platform
Pre-requisites:
* This demo requires basic knowledge, understanding and familiarity of Google Cloud Plaftform, Docker, Kubernetes, terraform and Github Actions. This tutorial demo is the next demo to a series of demos I have been publishing about building and deploying a NodeJS app. I highly recommend that you first complete these two tutorials before getting started on this:
So assuming that you have now developed a new cool NodeJS application either by following my tutorial and whatever. You now want to run deploy that app to Google Cloud Platform (GCP).
In this tutorial, I’m going to demo how to deploy and run an application on GCP from my Mac. So to get started, let’s install gcloud command-line tool on macOS
Run the following brew command:
I’m using Zsh terminal. Z shell (Zsh) is a Unix shell built on top of bash (the default shell for macOS) — So I need to run the following command to be able to use the gcloud command line:
command to authorizing the SDK tools to access Google Cloud Platform using your user account credentials and setting up the default SDK configuration.
Follow all the prompts and go from there!
Set a default compute region appropriate to your location (GCP regions and zones), I set mine to us-central1:
Set a default compute zone appropriate to the zone:
Verify the configuration settings:
For more command lines, check out The gcloud command-line tool cheat sheet
The first thing we wanna do is testing that you can in fact run commands from your mac to provision resources to your google cloud. So let’s go ahead and provision a virtual machine (VM) inside GCP. Use the following gcloud command in your terminal to launch a VM with Ubuntu 20.04 distro:
Yay! Now you’re all set and can properly connect and create resources on your GCP from your mac. You can go ahead delete the VM we just created.
Now that we have set up gcloud SDK are able to create resources. let’s provision our project on the cloud with the resources need to deploy our NodeJs application.
At this point, I’m assuming that you have completed these tutorials first:
And you have docker images, GitHub actions job, and Kubernetes deployment set up for the demo app. Now we need to:
let’s use Terraform to provision and configure these resources in our GCP project. To do this we need to set the terraform template in main.tf and the providers.tf files.
Read the comments to get isignt on what each lines in the files does
Now let’s initialize terraform
If you are using terraform on your workstation, like I am in this demo; you will need to authenticate using User Application Default Credentials by running the command:
Then initialize terraform by running:
After the initialization is completed, run the following command to create resources on GCP:
deploy kubernetes deployment
Now that we have the cluster created. let get into it the cluster and run the Kubernetes deployment command:
First, let’s connect to the cluster by running:
Then, run the Kubernetes deployment:
After the deployment is completed, copy-paste your service external IP address and navigate to your browser to access your demo app as shown below:
Important Note:
for an actual project, additional setups need to be done where you can set up a static External IP address to your cluster from your Kubernetes deployment template and set up a proper DNS where users can access our application from the internet.
Now that we can deploy our app as expected. Let automate that process by setting up our CI/CD pipeline to deploy to GCP. This is a very simple GitHub action that allows you to deploy to Google Could Plaftrofm. The Action Details can be found here: setup gcloud GitHub Action
So, let’s get started with the following:
My GitHub actions Job looks like this:
Important Note: You’ll need to set up your deployment properly.
- Running kubectl apply will fail to update or recreate your resources.
- Force replace, delete, and then re-create the resource but this will cause a service outage.
Let’s make some modifications to our app, push the change code and watch your CI/CD pipeline in action:
You can review the full demo code here: github repo.
If you enjoy this, you might also like: “Provisioning vs Configuration Management with Terraform”
cheers!!!
Coding, Tutorials, News, UX, UI and much more related to development
64 
Get the latest news and update from DevGenius publication Take a look.
64 claps
64 
Written by
CIO @ITOT Africa | Lead Senior Site Reliability Engineer @ICF󠁧󠁢󠁳󠁣󠁴 | ""Learning is experience; everything else is just information!”
Coding, Tutorials, News, UX, UI and much more related to development
Written by
CIO @ITOT Africa | Lead Senior Site Reliability Engineer @ICF󠁧󠁢󠁳󠁣󠁴 | ""Learning is experience; everything else is just information!”
Coding, Tutorials, News, UX, UI and much more related to development
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/analytics-vidhya/7-hints-and-10-tips-for-coursera-essential-google-cloud-infrastructure-foundation-986975b9033f?source=search_post---------356,"There are currently no responses for this story.
Be the first to respond.
I recommend Architecting with Google Cloud Platform (GCP) or Developing applications with GCP Coursera specializations. Going through these specializations will significantly help you obtain the Google Cloud Architect certification or Google Cloud Developer certification. More importantly, hands-on labs enable you to build GCP applications.
"
https://medium.com/@jaychapel/google-cloud-machine-types-comparison-e3df3f155cbd?source=search_post---------357,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Feb 17, 2021·6 min read
Google Cloud Platform offers a range of machine types optimized to meet various needs. Machine types provide virtual hardware resources available to a virtual machine that vary by virtual CPU (vCPU), disk capability, and memory size, giving you a breadth of options. Within every machine family there’s a set of machine types that offer a combination of memory and processor configuration. With so much to choose from, finding the right Google Cloud machine type for your workload can get complicated.
Since we’ve gone over EC2 instance types and Azure VMs, we’re doing an overview of each GCP machine type. The image below shows the basics of what we will cover, but remember that you’ll want to investigate further to find the right machine type for your particular needs.
A version of this article was published in 2018. It has been completely revised and updated for 2021.
General-purpose machine types are resources managed by Google Compute Engine. Each machine type in the general-purpose machine type family is curated for specific workload types. You’ll find that these machine types are suitable for a variety of common workloads. Some examples of these workloads include: development and testing environments, databases, mobile gaming and web applications. These machines are known for offering the best balance of performance and price. Within the General-purpose family, you can choose from four general-purpose machine types: E2, N2, N2D, and N1.
E2 VMs provide a variety of compute resources for the lowest on-demand pricing across all general-purpose machine types. E2 machine types also utilize dynamic resource management, which offers numerous benefits for workloads that prioritize cost savings. These machine types offer the lowest cost of ownership on Google Cloud — you’ll see up to 31% of savings compared to N1. Additionally, pricing for E2 VMs already includes sustained use discounts and are also eligible for committed use discounts — which increases potential savings of up to 55%.
Best fit: Workloads such as small-to-medium databases, web serving, and application development and testing environments that don’t require large instance sizes, GPUs or local SSD would all be good fits for E2.
N2 machine types are the second generation general-purpose machine types that support up to 80 vCPUs and 640 GB of memory. N2 VMs offer you the ability to get about 30% higher performance from your VMs, and shorten many of your computing processes. These machine types offer higher memory-to-core ratios for VMs created with the extended memory feature.
Best fit: General purpose workloads including web and application servers, enterprise applications, gaming servers, content and collaboration systems, and most databases.
N2D machine types are the largest general-purpose machine type with up to 224 vCPUs and 896 GB of memory. These VMs are designed to provide you with the same features as N2 VMs.
Best fit: Web applications, databases, workloads, and video streaming.
N1 VMs are first-generation general-purpose machine types that support up to 96 vCPUs and 624GB of memory. Though most would recommend using one of the second-generation general-purpose machine types, N1 VMs do offer a larger sustained use discount than N2 machine types. Additionally, they have support for Tensor Processing Units (TPUs) in select areas.
Compute-optimized machine types are ideal for compute-intensive workloads. These machine types offer the highest performance per core and most consistent performance on Compute Engine. Compute-optimized machine types are suitable for workloads such as game servers, latency-sensitive API serving and high-performance computing (HPC). These machines have 40% greater performance than previous generation N1.
C2 machine types provide full transparency into the architecture of the underlying server platforms which will allow you to fine-tune the performance. C2 machine types run on a newer platform, offer more computing power, and are typically more powerful for compute-intensive workloads compared to the N1 high-CPU machines.
C2 VMs also offer up to 20% sustained use discounts. Additionally, they are eligible for committed use discounts, which would bring potential savings up to 60%.
Memory-optimized machine types are fit for tasks that require intensive use of memory with higher memory-to-vCPU ratios due to the fact that they offer the highest memory configurations across our VM families with up to 12 TB for a single instance. It is important to note that these machine types do not support GPUs. Memory-optimized machine types offer up to 30% sustained use discounts. Additionally, they are eligible for committed use discounts, bringing additional savings up to greater than 60%.
These machine types are best fit for in-memory databases and in-memory analytics.
M2 VMs support the most demanding and business critical database applications with up to 12TB of memory. These machine types offer the lowest cost per GB of memory on Compute Engine, which makes them a perfect choice for workloads that utilize higher memory configurations and have low compute resources requirements.
M1 machine types are the first generation memory-optimized machine types that offer 4TB of memory. Similar to M2 machine types, M1 machine types offer the lowest cost per GB of memory on Compute Engine.
Accelerator-optimized VMs were added in July 2020. This machine type family is optimized for demanding compute workloads — this would be workloads such as high-performance computing and CUDA-enabled machine learning.
Every A2 VM has a set amount of A100 GPUs that offer 20x improvements in computing speed in comparison to previous generation NVIDIA V100 GPUs. These machine types are currently available through Google’s alpha program.
Share-core machine types are a cost-effective option that works well with small or batch workloads that only need to run for a short time. They use partial vCPUs that run on one hyper-thread of the host CPU running your instance. These machine types use context-switching to share a physical core between vCPUs so they can multitask.
The GCP shared-core family provides bursts of physical CPU for brief periods of time in moments of need. They’re like spikes in compute power that can only happen in the event that your workload requires more CPU than you had allocated. These bursts are only possible periodically and are not permanent.
Predefined machine types vary to meet needs based on high memory, high vCPU, a balance of both, or both high memory and high vCPU. If none of the machine types meet your needs, Google has one more option for you — custom machine types. With custom machine types, you can define exactly how many vCPUs you need and what amount of system memory for the instance. They allow you to independently configure CPU and memory to find the right balance for your applications, so you’re only paying for what you need. They’re a great fit if your workloads don’t quite match up with any of the available predefined types, or if you need more compute power or more memory, but don’t want to get bogged down by upgrades you don’t need that come with predefined types.
On top of your GCP instance types, Google also offers graphics processing units (GPUs) that can be used to boost workloads for processes like machine learning and data processing. You can only attach GPUs to predefined or custom machine types. In general, the higher number of GPUs attached to your instances, the higher number of vCPUs and system memory available to you.
Between the predefined options and the ability to create custom Google Cloud machine types, Google offers enough variety for almost any application. Cost matters, but with the resource-based pricing structure, the actual machine you chose matters less when it comes to pricing.
With good insight into your workload and usage trends you have the resources available to find the machine type that fits your business needs.
Originally published at www.parkmycloud.com on January 27, 2021.
CEO of ParkMyCloud
See all (317)
11 
11 claps
11 
CEO of ParkMyCloud
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@ratrosy/set-up-serverless-store-part-2-machine-learning-api-data-analytics-and-data-visualization-c1399f92b4c8?source=search_post---------358,"Sign in
There are currently no responses for this story.
Be the first to respond.
Ratros Y.
Jan 24, 2019·7 min read
This document is a part of the Serverless on Google Cloud Platform: an Introduction with Serverless Store Demo How-to Guide. It discusses the machine learning/AI, data analytics, and data visualization solutions Serverless Store uses.
Serverless Store uses three Google Cloud Platform machine learning/AI solutions:
Google Cloud Vision API is an image analysis tool using a machine learning model pre-trained by Google. You can try the API out here. Serverless Store uses Cloud Vision API to label product images.
To use Cloud Vision API:
When a new product is added, the flask app publishes an event to the new-product topic. Cloud Function detect_labels (functions/detect_labels/main.py) subscribes to the topic, listens to the events, and call the Cloud Vision API to analyze the image of new products. Returned labels from Cloud Vision API are then written to Cloud Firestore.
Cloud Vision API uses a pre-trained model; for labeling tasks, the API always return a subset of labels predefined by Google. Useful as it is, many use cases require a more precise, or specific, solution, hence the creating of Cloud AutoML Vision. Cloud AutoML Vision allows developers to train a custom model for image classification and integrate it in their apps. Little machine expertise is required; Cloud AutoML Vision takes labeled images and handles everything else automatically behind the scenes.
Serverless Store uses Cloud AutoML Vision to train a model that tells pet products apart from other products, and promotes all the pet products automatically on the front page. To set up AutoML Vision for Serverless Store:
Cloud Function automl (functions/automl/main.py) is responsible for predicting images using the model. It also subscribes to the new-product topic, and analyze the image of the new product when events arrive.
DialogFlow is a Google provided solution for building natural and rich conversational experiences. In Serverless Store you will use DialogFlow to add a simple custom action to Google Assistant so that people can check all the trending items on sale interactively via conversation.
To set up the trending items Google Assistant action for Serverless Store:
Cloud Function functions/dialogFlow/index.js is a simple code snippet using the DialogFlow SDK to prepare responses. At this moment it returns nothing but one fixed response; you, however, can easily modify it to prepare sentences using product names in your Cloud Firestore.
As introduced in the opening piece, events delivered via message queues such as Cloud Pub/Sub are perfect sources for real-time data analytics. You can, for example, stream these events to Cloud Dataflow, where event contexts are extracted and analyzed in real-time, and save the insights to Google BigQuery, a data warehousing solution for future reference at ease without ever interrupting the normal operations of your app.
In Serverless Store, for simplicity reasons, you will send these events directly to BigQuery instead and use Google Data Studio to create a dynamically updated data report. The report prepared can help you easily track the number of orders and payments processed by the app.
Capable of storing hundreds of PBs of data and more, Google BigQuery allows developers and data analysts to query massively large dataset fast and efficiently. For this demo, however, you will only use a small subset of its features. For more information on BigQuery, see Google BigQuery Documentation.
To set up Google BigQuery for Serverless Store:
An App Engine service, extras/streamEventsApp/server.js accepts events from all the Pub/Sub topics you create earlier and save them in BigQuery:
If you have you own domain, it is also possible to use a Cloud Function for streaming events into BigQuery. See functions/streamEvents for details.
Google Data Studio gathers data from a variety of sources, including Google BigQuery, and helps developers and data analysts build interactive visual data report/dashboard. With all the events saved to BigQuery, in Serverless Store you can create a dashboard updated in real-time with Google Data Studio.
To set up Google Data Studio with Serverless Store:
Continue the setup in Set up Serverless Store: Part 3.
Developer Relations @ Google Cloud Platform
3 
3 
3 
Developer Relations @ Google Cloud Platform
"
https://medium.com/google-cloud/using-puppet-w-google-compute-engine-1eeed425aa7c?source=search_post---------359,"There are currently no responses for this story.
Be the first to respond.
In this article, you’ll learn how to use Puppet to provision virtual machine instances in Google Compute Engine on the Google Cloud Platform.
First, install Puppet locally. I followed the installation instructions on my laptop (OS X). This was simple enough.
The gce_compute module allows you to manage Compute Engine instances, disks, load balancers, and more, as native Puppet resources.
Next, create a ~/.puppet/device.conf file (alternatively, /etc/puppet/device.conf):
Next, create a puppet manifest file ~/www.pp with the following content:
Apply this manifest:
Find the IP address:
Lastly, take a look at http://xxx.xxx.xxx.xxx/ in your browser =) and you should see a simple webpage with the content that was specified in the www.pp file.
Check out a full demo with source that sets up a Puppet server, and spins up four Puppet managed nodes at https://github.com/GoogleCloudPlatform/compute-video-demo-puppet!
Compute Engine Management with Puppet, Chef, Salt, and Ansible — Detailed information on ways to keep your instances up-to-date with different tools, pro/cons, etc. The Appendix has the actual steps and the sample manifest file that I used.
Using Puppet to Automate Google Compute Engine — Good examples on using gce_compute with sample manifest files.
gce_compute — Puppet Forge. Very detailed information on everything you want to know about gce_compute!
cloud_provisioner — Puppet Forge. Not too much information here. Refer to the installation documentation instead.
Google Cloud community articles and blogs
7 
Thanks to Jack Wilber. 
7 claps
7 
Written by
Loves #Java. Lives in NYC. Creator of @JDeferred. Ex @Google @JBoss, @RedHatSoftware, @Accenture. Opinions stated here are my own.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Loves #Java. Lives in NYC. Creator of @JDeferred. Ex @Google @JBoss, @RedHatSoftware, @Accenture. Opinions stated here are my own.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-developers/the-developer-show-tl-dr-040-a0d53810f63e?source=search_post---------360,"There are currently no responses for this story.
Be the first to respond.
Highlights: Android 7.0 Nougat, Direct Boot, Launchpad Accelerator, AdWords, Chrome 56, Google Cloud Platform, HTTPS
The Developer Show is where you can stay up to date on all the latest Google Developer news, straight from the experts.
Have a question? Use #AskDevShow to let us know!
Now that Nougat has begun to roll out, it’s a good time to review the many security enhancements included. Check out the post for info on Direct Boot and encryption, media stack and platform hardening, App security improvements, and more.
We’re now taking applications for the third class of the Launchpad Accelerator. If you’re a late-stage app startup from Brazil, India, Indonesia, or Mexico — check out this post. Applications are open until October 24th for the equity-free program that begins in January and includes two weeks of all-expense-paid training at the new Google Developers Launchpad Space in San Francisco.
If you’re about to launch a new holiday campaign, you could, for example, use TrueView ads on YouTube along with banner ads on the Google Display Network to drive brand awareness and more holiday sales. To make it easier for you to track and forecast the performance of these campaigns against your advertising goals, we’re introducing campaign groups and performance targets. More info and a screenshot are on the post.
Beginning with Chrome 56 in January of 2017, we’ll mark HTTP sites that transmit passwords or credit cards as non-secure, as part of a long-term plan to mark all HTTP sites as non-secure. HTTPS is easier and cheaper than ever before. Check the post for details on our upcoming change as well as setup guides to help you get started with HTTPS.
Cloud Tools for PowerShell is a collection of “command-lets” for accessing and manipulating GCP resources. It’s currently in beta and already allows access to Google Compute Engine, Google Cloud Storage, Google Cloud SQL and Google Cloud DNS — with more to come! For more info on how to get started, check out the post.
If you’ve been considering moving your web serving infrastructure to the cloud, our recently published “Serving Websites Guide” is for you. It covers static websites, virtual machines, containers, and managed platforms. For each, the guide provides information about scalability, load balancing, DevOps, logging and monitoring, and more.
Engineering and technology articles for developers, written…
5 
5 claps
5 
Engineering and technology articles for developers, written and curated by Googlers. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Developer Advocate for Google. Improving life through science and art.
Engineering and technology articles for developers, written and curated by Googlers. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://lab.wallarm.com/wallarm-node-now-as-a-google-cloud-image-e7235fa774dc?source=search_post---------361,"
					We're sorry, but we can't find the page you were looking for. It's probably some thing we've done wrong but now we know about it and we'll try to fix it. In the meantime, try one of these options:					
© 2021 Wallarm				

			Type above and press Enter to search. Press Esc to cancel.		
"
https://medium.com/@cloudacademy/how-to-unlock-complimentary-access-to-cloud-academy-447bb9c7354b?source=search_post---------362,"Sign in
There are currently no responses for this story.
Be the first to respond.
Cloud Academy
Aug 22, 2019·3 min read
Are you looking to get trained or certified on AWS, Azure, Google Cloud Platform, DevOps, Cloud Security, Python, Java, or another technical skill? Then you’ll want to mark your calendars for August 23, 2019. Starting Friday at 12:00 a.m. PDT (3:00 a.m. EDT), Cloud Academy is offering complimentary access* to our entire Training Library.
During this free time period, you’ll have 72 hours of unlimited access to our industry-leading:
Just go to our landing page, enter your email address, and we’ll send you a friendly reminder when complimentary access to our content can be unlocked.
When access is unlocked, getting started is as easy as 1,2,3…
Believe it or not, there is no credit card required to gain full access to our mobile app and training library that delivers the theory, technical knowledge, and hands-on practice to get certified on industry-leading cloud platforms and technologies. We’re so confident that you will love our training platform and our mobile app that you’ll want to stick around for awhile.
Just remember that our training library will be locked down on Sunday, August 25 at 11:59 p.m. PDT, so what are you waiting for? Go to our Training Library and start a new Learning Path, Course, Quiz, Exam, Hands-on Lab, or Lab Challenge today.
According to Queen Latifah:
I don’t care how much money you have, free stuff is always a good thing.
And we believe it — that’s why we’re so excited to share this good news! Feel free to pass along the link to our landing page or share our posts on your favorite social media platform:
Cloud Academy is the leading digital skills development platform that enables every enterprise to become a tech company through guided Learning Paths, Hands-on Labs, and Skill Assessment. Cloud Academy delivers role-specific training on leading clouds (AWS, Azure, Google Cloud Platform), essential methodologies needed to operate on and between clouds (DevOps, security, containers), and capabilities that are unlocked by the cloud (big data, machine learning).
Companies like Turner, Cognizant, SAS, and ThermoFisher customize Cloud Academy to contextualize learning and leverage the platform to assign, manage, and measure cloud enablement at scale.
Our library is loaded with 10,000+ hours of technical content that is helping teams build the tech skills at the core of innovation.
You can explore what’s new on AWS, Azure, Google Cloud, CI/CD, containers, security, IoT, data science, programming, machine learning, big data, and beyond. Check out some of our new content from this month:
*Complimentary access starts Friday, August 23 at 12:00 a.m. PDT and ends Sunday, August 25 at 11:59 p.m. PDT. This promotion cannot be combined with any other promotions or applied in any way to existing Personal or Enterprise licenses. After complimentary access ends, you will be given the option of upgrading your account to a paid subscription. To see what people are saying about us, read our reviews on G2Crowd.
The proven platform to assess, develop, and validate hands-on technical skills.
7 
7 
7 
The proven platform to assess, develop, and validate hands-on technical skills.
"
https://medium.com/@jaychapel/google-cloud-machine-types-comparison-4c6b3f95da92?source=search_post---------363,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Feb 23, 2019·4 min read
Google Cloud Platform offers a range of machine types optimized to meet various needs. Machine types provide virtual hardware resources that vary by virtual CPU (vCPU), disk capability, and memory size, giving you a breadth of options. But with so much to choose from, finding the right Google Cloud machine type for your workload can get complicated.
In the spirit of our recent blog on EC2 instance types, we’re doing an overview of each Google Cloud machine type. This image shows the basics of what we will cover, but remember that you’ll want to investigate further to find the right machine type for your particular needs.
Predefined machine types are a fixed pool of resources managed by Google Compute Engine. They come in five “classes” or categories:
Standard machine types work well with workloads that require a balance of CPU and memory. The n1-standard family of machine types come with 3.75 GB of memory per vCPU. There are 8 total in the series and they range from 3.75 to 360 GB of memory, corresponding accordingly with 1 to 96 vCPU.
High memory machine types work for just what you’d think they would — tasks that require more system memory as opposed to vCPUs. The n1-highmem family comes with 6.50 GB of memory per vCPU, offering 7 total varieties ranging from 13 to 624 GB in memory, corresponding accordingly with 2 to 96 vCPUs.
If you’re looking for the most compute power, the n1-highcpu series is the way to go, offering 0.90 GB per vCPU. There are 7 options within the high cpu machine type family, ranging from 1.80 to 86.6GB and 2 to 96 vCPUS.
Share-core machine types are cost-effective and work well with small or batch workloads that only need to run for a short time. They provide a single vCPU that runs on one hyper-thread of the host CPU running your instance.
The f1-micro machine type family provides bursts of physical CPU for brief periods of time in moments of need. They’re like spikes in compute power that can only happen in the event that your workload requires more CPU than you had allocated. These bursts are only possible periodically and are not permanent.
For more intense workloads that require high memory but also more vCPU than that you’d get with the high-memory machine types, memory-optimized machine types are ideal. With more than 14 GB of memory per vCPU, Google suggests that you choose memory-optimized machine types for in-memory databases and analytics, genomics analysis, SQL analysis services, and more. These machine types are available based on zone and region.
Predefined machine types vary to meet needs based on high memory, high vCPU, a balance of both, or both high memory and high vCPU. If that’s not enough to meet your needs, Google has one more option for you — custom machine types. With custom machine types, you can define exactly how many vCPUs you need and what amount of system memory for the instance. They’re a great fit if your workloads don’t quite match up with any of the available predefined types, or if you need more compute power or more memory, but don’t want to get bogged down by upgrades you don’t need that come with predefined types.
On top of your virtual machine instances, Google also offers graphics processing units (GPUs) that can be used to boost workloads for processes like machine learning and data processing. GPUs typically can only be attached to predefined machine types, but in some cases can also be placed with custom machine types depending on zone availability. In general, the higher number of GPUs attached to your instances, the higher number of vCPUs and system memory available to you.
Between the predefined options and the ability to create custom Google Cloud machine types, Google offers enough variety for almost any application. Cost matters, but with the new resource-based pricing structure, the actual machine you chose matters less when it comes to pricing.
With good insight into your workload, usage trends, and business needs, you have the resources available to find the machine type that’s right for you.
Originally published at www.parkmycloud.com on August 9, 2018.
CEO of ParkMyCloud
See all (317)
3 
3 claps
3 
CEO of ParkMyCloud
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/twiggcp-2019week9-4167e00f34ba?source=search_post---------364,"There are currently no responses for this story.
Be the first to respond.
Welcome to the weekly Google Cloud Platform newsletter, served every Monday from medium.com/google-cloud/weekly !
Here are the stories for this past week :
“Google Cloud Platform Fundamentals: Core Infrastructure Training”. Every day new users, maybe your colleagues, take their first GCP steps. With presentations, demos, and hands-on labs, this class is for them.
“Coursera — Google Cloud”. The self-paced training courses (GCP Fundamentals and many others) from Coursera remain very popular, especially if you’re planning on a certification.
“Automating infrastructure with Cloud Composer”. Meet Apache Airflow managed by Google (using GKE) in this new step-by-step solution. This tutorial shows you how to automate backups of Compute Engine VM instances.
“Kickstart your cryptography with new Cloud KMS client libraries and samples”. The new GCP KMS client libraries are here! These gRPC-based libraries are more language-idiomatic and include a large collection of sample code for all languages!
“Making AI-powered speech more accessible”. Speech-to-text, Text-to-speech — more features, more voices, more languages, better accuracy, and lower prices!
“Introducing the Cloud IoT Device SDK” (Google blog). Embedded C libraries to connect, provision, and manage devices with Cloud IoT Core.
“Take control of your Kubernetes clusters with CSP Config Management” (Google blog). The logical next step after last week’s Cloud Services Platform (CSP) beta announcement.
From still my favorite “Customers and partners talk best about GCP” department :
From the “considering your options” department :
From the “we can make those ML stacks even more efficient” department :
From the “are you paying attention to statistical computing?“ department :
From the “taking ML to fields and places” department :
From the “new landing page for everything GCP and Gaming” department :
From the “GCP is meeting customers where they are” department :
From Doug’s “Firebase & Google Cloud” department :
From the “Beta, GA, or what?” department :
From the “all things multimedia” department :
That is all for this week!-Alexis
Google Cloud community articles and blogs
3 
3 claps
3 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Google Cloud Developer Relations
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/@traderbagel/https-jupyter-notebook-on-gcp-cb45101e7f69?source=search_post---------365,"Sign in
There are currently no responses for this story.
Be the first to respond.
Bagel
Dec 10, 2018·6 min read
紀錄一下載 Google Cloud Platform上面開一個 HTTPS的 Jupyter Notebook
這篇真的只是記錄 完全不負教學責任
開一個 Debian 9的機器, 4CPU, 16G RAM, 100G 硬碟, 開的時候要記得打開`允許HTTP, HTTPS`的選項。開完後到GCP選單中 `VPC網路`的地方給剛剛開的機器一個固定 IP, 這邊我有在 Godaddy買了一個域名, 把剛剛拿到的 IP綁定到 Godaddy \@下面 … 用圖片可能比較好解釋
到這邊 可以使用 host demo.domain.com 來確認是否綁定成功
接下來要安裝 Python3.6.7 (3.6的最終版本)先安裝一些需要的東西
下載 Python版本並且 Build
開啟一個 virtualenv
sudo vim /etc/nginx/sites-avaliable/default
差點忘了也要修改 jupyter設定檔
最後就可以執行 &連上惹
$ nohup jupyter notebook &> notebook.log &
真的很間單
主要參考 >>>> 他
自己的醫藥費自己賺
See all (85)
4 
4 claps
4 
自己的醫藥費自己賺
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@john-tucker/event-driven-application-by-example-ba937e58bf08?source=search_post---------366,"Sign in
There are currently no responses for this story.
Be the first to respond.
John Tucker
Aug 3, 2021·2 min read
A representative example of an event-driven application on Google Cloud Platform.
The example is an application that automates the creation / deletions of A records in a Google Cloud DNS internal Zone based on Google Compute Engine Instance insert, start, stop, and delete events.
The following diagram illustrates the Google Cloud Platform resources used.
Things to observe:
Nothing fancy here; that is about it.
Broad infrastructure, development, and soft-skill background
See all (26)
54 
54 claps
54 
Broad infrastructure, development, and soft-skill background
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/ruby-meets-bigquery-part-1-d8aa1f9e68f7?source=search_post---------367,"There are currently no responses for this story.
Be the first to respond.
As we’ve been improving support for Ruby on Google Cloud Platform one thing that comes up in our discussions is which gems are used frequently. We want to ensure that common gems install cleanly on the Ruby containers we provide. This means including libraries in the container image for many gems that have native extensions. While we were discussing this, the tester in me wondered, what are the most common 500 gems? Luckily there are multiple sources of data to figure this out.
Rubygems.org provides sanitized weekly dumps of their PostgreSQL database. The dump includes download count by gem and version. The downloads table is called gem_downloads and here’s the schema:
rubygem_id and version_id are foreign keys to other tables that contain the gem name, version string, and other pertinent information.
This isn’t a particularly large dataset but I wanted to put the imported data in a place where others on the team could easily run ad hoc queries. For me, the best place was BigQuery. BigQuery is a managed data warehouse that is part of Google Cloud Platform. It is astonishingly fast at querying large datasets and I can query it using standard SQL.
I used the example load script provided by rubygems.org to load the data into a local PostegreSQL database. Loading the data into BigQuery from there was pretty straightforward. For some of the tables I used the streaming method. Here is the code for loading the gems table.
Line 21 is a bit confusing. PostgreSQL returns each row as an array of values. BigQuery expects key value pairs. The zip and Hash[] transform the array of values into a Hash that will be sent to BigQuery in the correct format.
The rest of this example is pretty simple. First I require the pg and gcloud gems. Then I initialize a connection to BigQuery and PostgreSQL. After that, I create the BigQuery table if it doesn’t exist. Finally, I connect to PostgreSQL, extract the data, and insert into BigQuery.
For the rest of the tables in the rubygems.org data I used batch processing. I exported the tables to CSV and then loaded them directly from CSV into BigQuery using the UI. You can also load a CSV, json, or avro file into BigQuery using the gcloud gem.
Once the data is loaded into BigQuery analyzing it is as simple as writing SQL queries. My primary question was “Which gems have the most downloads?” This query gets the five most downloaded gems:
Here are the results:
I had expected Rails to be the most downloaded gem but it was only number 14.
The gem_downloads table has both a gem_id and a version_id column. Downloads are counted for a specific version of a specific gem so the above query doesn’t give an accurate count. This query sums the counts for all versions of each gem.
This yielded the same top five gems but with higher download counts.
I was also curious which testing library was most popular. It often feels like test library discussions are the editor wars of the ruby community. I’m quite fond of minitest but many folks still use rspec.
One last thing the Google Cloud Ruby team was interested in was which versions of Rails are most popular. We all knew someone who was still using an old version of Rails 3. To answer that question I needed to get download counts for each version of Rails.
What we call version numbers, like 3.1.0, are actually strings. In order to get counts for each version I had to extract the major version part of the version string and ignore the minor and patch portions. I used a regular expression to grab everything before the first dot in the version string.
This shows that Rails 3 is the most downloaded and Rails 4 has quite a few downloads as well.
This data gave us a reasonable metric for which gems were the most popular. However downloads is not necessarily the best way to measure usage. One company with a lot of servers that they update frequently could skew the data pretty significantly if they download their gems from the web on each install. I wanted to find another way to measure gem usage.
While I was working on this blog post, and the talk that goes with it. I found out that Google released information on nearly 3 million open source repositories from GitHub as a BigQuery public dataset. The next post in this series uses that data to analyze gem popularity and compares how the results differ between the two sources.
07/13/16
Originally published at www.thagomizer.com on July 13, 2016.
Google Cloud community articles and blogs
5 
5 claps
5 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Rubyist, Data Nerd, Lazy Dev, Stegosaurus. Cloud Developer Advocate @ Google. Disclaimer: My opinions are my own. *RAWR*
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/@qwiklabs/to-start-using-google-cloud-729610693eff?source=search_post---------368,"Sign in
There are currently no responses for this story.
Be the first to respond.
Qwiklabs
Feb 6, 2019·2 min read
Let’s start with an confession; I’m not the most technologically gifted. When I started my Qwiklabs tutorial on Google Cloud Platform’s essential tools and services, I was nervous I wouldn’t be able to keep up with all the Cloud has to offer.
But there was no reason to worry. The Google Cloud Platform (GCP) Essentials quest requires no prerequisites or background knowledge in the field of cloud computation. (There’s a pop-up that asks for a code when you click the link — guess you’ll have to read to the end of the article to get it!) Everyone starts somewhere, and Qwiklabs is excited to help you take that first step. This series of hands-on labs offered in this beginner’s course are accessible, easy to navigate, and pretty fun to complete — no matter how novice a learner might be, you can finish the GCP Essentials quest and earn your first Google Cloud badge in one day!
It was thrilling to finish a lab and feel the satisfaction of building something both useful and user-friendly — I even got a cool badge out of it.
Want to earn your own? Use this link and enter code 1q-gcpe-976 to get a one month pass for everything in the Qwiklabs catalog at no charge. Hurry, code expires Feb. 10, 2019. Hope to see you there!
Originally published at blog.qwiklabs.com on February 6, 2019.
6 
6 claps
6 
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@jaychapel/how-google-preemptible-vms-work-and-save-you-money-389968ffcb3b?source=search_post---------370,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Jun 15, 2018·2 min read
When it comes to virtual machines, Google Cloud Platform is anything but standard, which is no wonder why they mix things up by offering Google preemptible VMs. Traditionally, VMs only shut down when told to, but preemptible VMs only last up to 24 hours (if available at all), and you can stop them at any time. So the big question is — what’s the point?
First things first — cost. By opting to use preemptible VMs, you could save as much as 80% in comparison to on-demand (the same reason why AWS users opt for spot instances, and Azure users can run with low priority VMs). 80% is a massive discount, and a great option if your workload doesn’t have a deadline or your processes are short-lived, like financial modeling, rendering and encoding, and even some of your CI/CD processes like code testing.
Use the Google Cloud API or the ‘gcloud’ command line tool in to the GCP console to create your own preemptible VM. It’s just like the standard process— pick your instance size, networking options, disk setup, so on and so forth. The only difference is that you’ll need to enable the ‘preemptible’ flag while you’re setting up. Just in case, you should also come up with a shutdown script to have a process in place for what happens if that instance stops unexpectedly, which is quite possible in this case.
To get the most out of your workload in performance, capability, and cost effectiveness, take advantage of the fact that you can attach local SSD drives and GPUs to preemptible VMs. Put your VMs in managed instance groups for the benefit of scalability while the instances are available to you.
The smartest and most cost optimal way to use Google Preemptible VMs is to mix them with other types of instances that pair well with your workload. If your instances need to be on all day and night, try committed-discounts and save up to 57% on those servers. For instances that don’t need to run 24/7, try ParkMyCloud — an automated scheduling tool that saves you up to 65% by turning your instances when you’re not using them (and really — why waste money leaving something on when you don’t need it?). Finally, for your batch workloads or non-urgent jobs, go for the Google preemptible VMs and save up to 80%.
CEO of ParkMyCloud
4 
4 
4 
CEO of ParkMyCloud
"
https://medium.com/google-developers/the-developer-show-tl-dr-038-5f1c9dcd63da?source=search_post---------371,"There are currently no responses for this story.
Be the first to respond.
Highlights: Android 7.0 Nougat, Street View renderer, Google Santa Tracker open source, TensorFlow, Google Cloud Platform, Strackdriver, Maven and Gradle, Google Cloud Tools for IntelliJ, ASP.NET on Google Cloud Platform.
The Developer Show is where you can stay up to date on all the latest Google Developer news, straight from the experts.
Have a question? Use #AskDevShow to let us know!
Android 7.0 Nougat has begun rolling out to users. For more on what’s included, how we’re rolling it out, and what’s next… check out the post.
There’s a new Street View renderer in the Google Maps JavaScript API that brings rendering improvements and better mobile support. Street View helps make apps more unique and exciting by giving users a sense of what it’s like to visit a place in real life. Check out the post for all the improvements including before and after screen-caps.
The open source version of Google’s Santa Tracker has been updated with the Android and web experiences that ran last December. We extended, enhanced and upgraded our code… and you can see how we used our developer products — including Firebase and Polymer — by following this link.
Check out this post for highlights from the latest release of TF-Slim. The TF-Slim library provides common abstractions which enable you to define models quickly and concisely, while keeping the model architecture transparent and its hyperparameters explicit.
Also from TensorFlow: we’re opening sourcing TensorFlow model code for generating news headlines on Annotated English Gigaword which is a dataset often used in summarization research. Check out the post for details and the GitHub link.
Stackdriver is great for troubleshooting issues in production cloud applications. And it now has logs panel integration. Not only can you gather production application state and link to its source, you can *also* view the raw logs associated with your Google App Engine projects, all on one page. More details and a screenshot are on the post.
I’ve got two Google Cloud Platform updates for Java developers. First, the beta release of two new build tool plugins: one for Apache Maven and another for Gradle. These plugins allow you to test your applications locally and then deploy them to the cloud from the Command Line Interface or through integration with an IDE such as Eclipse and IntelliJ.
And, on that note, you can now use the new Google Cloud Tools for IntelliJ plugin to deploy your application in App Engine standard and App Engine Flexible and use Google Stackdriver Debugger and Google Cloud Source Repositories without leaving that IDE.
Here’s another bundle of posts for the Google Cloud Platform, these are about working with windows workloads. Everything from setting up a Windows Server and SQL Server on Google Compute Engine to Cloud Tools for Visual Studio and PowerShell. Check out all three posts:
Engineering and technology articles for developers, written…
6 
6 claps
6 
Engineering and technology articles for developers, written and curated by Googlers. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Developer Advocate for Google. Improving life through science and art.
Engineering and technology articles for developers, written and curated by Googlers. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/iot-5g-extreme-ideas-lab/seeed-studio-beaglebone-green-wireless-board-69d95e03ab00?source=search_post---------372,"There are currently no responses for this story.
Be the first to respond.
If you have been waiting for a board that could be integrated with The Google Cloud Platform, then here is one, BeagleBone Green Wireless Board. The BeagleBone Green Wireless IoT Developer prototyping kit enables developers to get data from the environment directly onto GCP . Once in GCP, developers can utilize its features such us Google Cloud Pub/Sub, Google Cloud Dataflow, Google Bigtable, Google BigQuery among others.
Seeed Studio BeagleBone Green Wireless board is a joint effort by Google, Seeed Studio and BeagleBoard.org. It has has been derived from Beaglebone Green which was also derived from Beaglebone Black. With this system developers are able to build applications without soldering and jump wires.
Beaglebone has produced more than 30 Groves that work well with the BeagleBone Green Wireless board, and the number is increasing. The Grove sensors are integrated into the standard grove connector allowing developers to connect the board to them with a simple cable. The 5V barrel of the Beaglebone Black has also been updated to micro USB host. This enables developers to power the board more conveniently.
Here are some of the functionalities that make the new prototyping kit ideal for IoT designs.
Built in Wi-Fi and Bluetooth radios
Seeed Studio BeagleBone Green Wireless has included Wi-Fi 802.11b/g/n 2.4GHz, a high-performance wireless module on board which add 2x2 MIMO Wi-Fi and Bluetooth 4.1 with BLE
802.11 High Rate or Wi-Fi, it is an extension to 802.11 specification developed by the IEEE for wireless LAN (WLAN) technology that applies to wireless LANS and provides 11 Mbps transmission (with a fallback to 5.5, 2 and 1 Mbps) in the 2.4 GHz band. 802.11b uses only DSSS.
This is the First Wi-Fi BLE Board from BeagleBoard Community.
A fully open hardware design
Seeed Studio BeagleBone Green Wireless is based on the open-source hardware design of BeagleBone Black.
Built in Grove connectors
Seeed Studios has removed the on-board HDMI connectors previously on Beaglebone Black to make room for the following two Grove connectors.
I2C - The Inter-integrated Circuit (I2C) Protocol is a protocol intended to allow multiple “slave” digital integrated circuits (“chips”) to communicate with one or more “master” chips. It only requires two signal wires to exchange information. It is only intended for short distance communications within a single device.
UART — A universal asynchronous receiver/transmitter, is a computer hardware device that translates data between parallel and serial forms. It provides the computer with the RS-232C Data Terminal Equipment ( DTE ) interface so that it can “talk” to and exchange data with modems and other serial devices.
The two Grove connectors, making it easier to connect to the large family of Grove sensors.
Built in programmable real-time units (PRU)
BeagleBone’s system-on-a-chip (SoC) combines ARMv7 CPU with two smaller 32-bit microcontroller Programmable Realtime Units (PRUs) that have access to all 512MB of system RAM. This lets the developer dedicate the PRUs to the time-sensitive and repetitive task of reading each sample out of an external ADC, while the main CPU lets us use the data with the GNU/Linux tools we’re used to.
Built in on board flash memory
This board has built-in onboard flash storage that lets you treat SD cards as optional, removable storage
Built in analog-to-digital conversion
It also has Built-in analog-to-digital conversion.
Headers
2x 46 pin header
Supports AP STA Mode for Wireless LAN Networking & A2DP & MRAA Library
Welcome to The IoT Xtreme Ideas Lab.
5 
5 claps
5 
Welcome to The IoT Xtreme Ideas Lab. In this publication, I share do-it-yourself Electronics, Embedded systems & Internet of Things projects. I believe Education should be free & accessible to all. I am currently plotting world domination through Open Source, Software & Hardware.
Written by
Electronic Engineer. Software Innovator, Intel. Deep Learning Abantu. IoT Champion for GDGs. Co-lead, GDG Nairobi #AI #IoT #5G Freak. Opinions = Mine
Welcome to The IoT Xtreme Ideas Lab. In this publication, I share do-it-yourself Electronics, Embedded systems & Internet of Things projects. I believe Education should be free & accessible to all. I am currently plotting world domination through Open Source, Software & Hardware.
"
https://medium.com/aifrontiers/google-to-offer-tensorflow-training-at-ai-frontiers-conference-b46fceb54189?source=search_post---------373,"There are currently no responses for this story.
Be the first to respond.
On November 11 at the AI Frontiers Conference in San Jose Convention Center, California, Google Cloud Platform (GCP) will present full-day training on image understanding with TensorFlow.
Image understanding is an AI technique that interprets images to figure out its context, including what the objects are, their spatial relationship to each other, and so on. It is now playing an essential role in numerous fields, for example, from Facebook automatically creating hashtags for your photos to personal album organization on your smartphones.
With increasing adoption of image understanding across various services and products, the industry now demands a large number of talents who can master the skills of image understanding.
Paring with Google machine learning library TensorFlow and AI accelerator application-specific integrated circuit TPUs, GCP is now one of the best cloud platforms for AI researchers. It also offers both pre-trained models via an API and the ability to build custom models using AutoML Vision to provide flexibility depending on different use case.
This advanced course on TensorFlow will include five sections:
The training course has some prerequisites: Basic SQL, familiarity with Python and TensorFlow. Attendees unpracticed in TensorFlow can take this Coursera course before attending this boot camp: https://www.coursera.org/learn/serverless-machine-learning-gcp.
AI Frontiers Conference brings together AI thought leaders to showcase cutting-edge research and products. This year, our speakers include: Ilya Sutskever (Founder of OpenAI), Jay Yagnik (VP of Google AI), Kai-Fu Lee (CEO of Sinovation), Mario Munich (SVP of iRobot), Quoc Le (Google Brain), Pieter Abbeel (Professor of UC Berkeley) and more.
Get your tickets now at aifrontiers.com. For question and media inquiry, please contact: info@aifrontiers.com
Showcasing the frontier technologies and people in…
2 
2 claps
2 
Written by
We showcase the cutting-edge AI research results that are deployed in large scale. Join our online weekly meetings at http://meetup.com/aifrontiers
Showcasing the frontier technologies and people in artificial intelligence.
Written by
We showcase the cutting-edge AI research results that are deployed in large scale. Join our online weekly meetings at http://meetup.com/aifrontiers
Showcasing the frontier technologies and people in artificial intelligence.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/thipwriteblog/google-cloud-function-%E0%B8%81%E0%B8%B1%E0%B8%9A-gcloud-interactive-cli-%E0%B8%84%E0%B8%B9%E0%B9%88%E0%B8%AB%E0%B8%B9%E0%B8%97%E0%B8%B5%E0%B9%88%E0%B8%81%E0%B8%B3%E0%B8%A5%E0%B8%B1%E0%B8%87%E0%B8%88%E0%B8%B0%E0%B9%80%E0%B8%9B%E0%B8%A5%E0%B8%B5%E0%B9%88%E0%B8%A2%E0%B8%99%E0%B9%84%E0%B8%9B%E0%B9%83%E0%B8%99%E0%B8%A7%E0%B8%B1%E0%B8%99%E0%B8%97%E0%B8%B5%E0%B9%88-1-%E0%B9%80%E0%B8%A1%E0%B8%A9%E0%B8%B2%E0%B8%A2%E0%B8%99%E0%B8%99%E0%B8%B5%E0%B9%89-3ba037ed055d?source=search_post---------374,"There are currently no responses for this story.
Be the first to respond.
หลังจากที่ Google Cloud Platform (GCP) ได้เปิดตัว Command LineInterface (CLI) ตัวใหม่ที่ชื่อ gcloud interactiveซึ่งตัวนี้ออก version beta หรือสถานะ Public Alpha ไปเมื่อต้นปี 2018โดยมีความ friendly กับเหล่า Developer มากขึ้นอย่างการมี Auto-prompts แสดงคำสั่งต่างๆขึ้นมาให้กดเลือกดูทันทีในขณะที่เรากำลังพิมพ์คำสั่ง ทำให้การใช้งานสะดวกและรวดเร็วขึ้นมากแถมบางครั้งยังทำให้เราเห็น command อื่นๆที่โผล่แว๊บขึ้นมาผ่านตาให้เรารู้สึกคุ้นเคยยิ่งขึ้น เมื่อมีชุดคำสั่ง CLI ใหม่ๆ เพิ่มเข้ามาเราก็จะไม่ต้องคอยเปิด release manual นั่งก๊อปปี้ไปมาแล้ว
ก่อนเฉลย.. เราขอพูดถึงอีกหนึ่ง Product จาก GCP ที่เราเองใช้งานตั้งแต่สมัยยังเป็น Beta นั่นคือ Cloud Functions ที่อยู่ใน Firebaseตัวนี้ก็เพิ่งเปลี่ยนสถานะจาก Beta เป็น Stable เมื่อช่วงกลางปี 2018ที่ผ่านมาเหมือนกัน
เรื่องมีอยู่ว่า… เมื่อช่วงเย็นที่ผ่านมานี้เราเพิ่งได้รับ email จากทีม Google Cloud Platform ส่งมาแจ้งเรื่อง
ให้เปลี่ยน runtimes version ที่ใช้กับ Cloud Functions ก่อนวันที่ 1 เมษายน 2562 นี้
ใจความคร่าวๆคือบอกว่า ตัว Cloud Function กำลังจะเปลี่ยนมาใช้gcloud interactive CLI ซึ่งหมายถึง Function ที่สร้างด้วย Cloud Functions API และ Firebase CLI ด้วย
เราสามารถสร้าง functions ด้วยการใช้ gcloud CLI, Cloud Functions APIหรือ Firebase CLI โดยที่ไม่ต้องระบุตัว runtimeแล้ว GCP จะ default ค่าให้เป็น Node.js 6
ตั้งแต่วันที่ 1 เมษายน 2562 เป็นต้นไป เมื่อเราต้องการ create functionโดยใช้ gcloud CLI หรือ Cloud Function APIจะต้องระบุค่ารันไทม์จึงจะสามารถสร้างได้เป็นข้อบังคับที่เพิ่มเข้ามานั่นเอง
แต่หากสร้างผ่าน Firebase CLI จะยังใช้แบบที่ไม่ต้องระบุ runtimes ได้แต่ค่ารันไทม์ default จะถูกระบุให้เป็น Node.js 8
## ต้องสร้าง Functions โดยระบุตัว runtimes เป็นตัวใดตัวหนึ่งใน 4 ตัวนี้แล้วแต่ที่เราต้องการใช้งาน ได้แก่ go111, nodejs6, nodejs8 หรือ python37
## วิธีการระบุ runtims จะแตกต่างกันไปตามวิธีการสร้าง functions ดังนี้
ซึ่งมันจะบังคับให้ระบุรันไทม์โดยใช้ flag ‘ — runtime’ เช่น$ gcloud functions deploy myCloudFunction — trigger-http — runtime nodejs6
2. การสร้างฟังก์ชั่นใหม่โดยใช้ Cloud Function API
ก็ต้องระบุรันไทม์เป็น property ในการสร้างเหมือนในตัวอย่างด้านล่างค่ะ
{ “name”: “myFunction”, … “runtime”: “nodejs6”, …}
3. การสร้างฟังก์ชั่นใหม่โดยใช้ Firebase CLI
ถ้าใช้ Firebase CLI เราจะมี 2 ตัวเลือก
3.1) ใช้ค่า runtimes default ต่อไป
3.2) แก้ไขค่า runtimes ในไฟล์ package.jsonแต่ต้องแน่ใจว่า function ใหม่ที่สร้างใช้ได้กับ Node.js 8
วิธีการ ให้เพิ่ม property ที่ชื่อ “engines” ในไฟล์ package.jsonเหมือนตัวอย่างด้านล่างนี้การระบุค่า property value ทำได้แค่สองค่า คือ “6” หรือ “8” เท่านั้น
“engines”: { “node”: “6” // or 8}
ด้านล่างนี้เป็น Link สำหรับอ่านเพิ่มเติมที่เก็บมาฝากค่ะ
cloud.google.com
cloud.google.com
เท่าที่อ่านมายังไม่เจอจุดที่ระบุว่าเปลี่ยน status เป็น Stable version แล้วหรือเราอาจจะตกข่าว กรี๊ดดด ถ้าใครมีอัปเดทรบกวน comment บอกกันด้วยน๊าแต่ถ้าลองมีอีเมล์มาแจ้งให้ update Project ก่อนวันที่ 1 เมษาขนาดนี้ก็คงจะใกล้คลอดเต็มตัวแล้วหล่ะม๊าง หรือไม่ก็เป็นแค่การเตรียมความพร้อม
สิ่งที่อยากฝาก
ขอตัวไป renovate project ของตัวเองก่อนนะคะแล้วพบกันใหม่ใน Blog หน้าจ้า Miss Youuu!
Programming, Technology, Work Life, Finance, Storyteller, Lifestyle, Content Creator, Podcast
3 

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
3 claps
3 
Written by
Software Engineer & Freelance Writer | thipwriteblog@gmail.com
พื้นที่รวมเรื่องราว เรื่องเล่า ของผู้หญิงคนหนึ่งที่ชอบอ่าน ชอบเขียน ชอบเรียนรู้ และอยากแบ่งปัน หากชื่นชอบกันก็กด Follow ไว้นะ ❤
Written by
Software Engineer & Freelance Writer | thipwriteblog@gmail.com
พื้นที่รวมเรื่องราว เรื่องเล่า ของผู้หญิงคนหนึ่งที่ชอบอ่าน ชอบเขียน ชอบเรียนรู้ และอยากแบ่งปัน หากชื่นชอบกันก็กด Follow ไว้นะ ❤
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@SandeepDinesh/gcp-will-be-getting-sydney-and-singapore-regions-this-year-19147abe19e5?source=search_post---------375,"Sign in
There are currently no responses for this story.
Be the first to respond.
Sandeep Dinesh
Jan 18, 2017·1 min read
Chris Dawes
GCP will be getting Sydney and Singapore regions this year:
https://cloudplatform.googleblog.com/2016/09/Google-Cloud-Platform-sets-a-course-for-new-horizons.html
I apologize if any of my pricing numbers are wrong, but everything I quoted is what I found on the public calculators (as of Jan 17, 2017).
Azure D5 V2 with Premium Storage:
GCE 16 Core 56 GB instance with SSD Storage:
I will gladly update my post if you share a better config on Azure that can perform the same.
See all (155)
3 
3 claps
3 
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@yahyaibnmohamed/ncloud-swiss-is-now-already-an-internationally-known-cloud-provider-6825e4580ab1?source=search_post---------376,"Sign in
There are currently no responses for this story.
Be the first to respond.
Yahya Mohamed Mao
Jul 17, 2018·2 min read
n’cloud.swiss AG famously known as the “Swiss made” alternative to Amazon AWS, Microsoft Azure, Google Cloud Platform and other major cloud providers is proud to publish an overview of its business follower count as well as their top LinkedIn influencers. The latter are Founder and Chairman of the Board of Directors André Matter, Chief Digital Officer Ralph Mattli and Chief Information and Operations Officer Matthias Imhof.
We would like to say THANK YOU to all of you for being part of the success stories we are writing not only in Switzerland but across the globe as well. We are proud to be now already an internationally known brand and company and are even more motivated to establish our cloud platform as the “Swiss made” alternative to the major cloud providers most notably Amazon AWS, Microsoft Azure and Google Cloud Platform.
n’cloud.swiss AG is an international operating and highly qualified service provider in the areas of IT and cloud solutions and is one of the leading providers in various markets. The owner-managed IT company was founded in 2001. Since then, it has been one of the very few companies in the cloud sector worldwide that can tailor all cloud models and services from Public, On-PREM or Hybrid clouds as well as offering “managed”, “semi-managed” or “unmanaged” support services. n’cloud.swiss AG also supports partners with its n’world innovation center in the implementation of new IT and cloud technologies with exciting projects for numerous business fields and use cases. These include, for example, blockchain projects, machine learning, edge computing, IA, big data, etc.
Safety through quality — quality through specialization and innovation. For n’cloud.swiss AG, these are not just goals, but lived values. For effective protection and the highest level of safety and innovation for our current and future customers at home and abroad.
Dynamic and data-driven marketing leader. Author, Mentor & Speaker. Top 50 Global Thought Leader. See www.yahyamohamedmao.com
See all (793)
51 

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
51 claps
51 
Dynamic and data-driven marketing leader. Author, Mentor & Speaker. Top 50 Global Thought Leader. See www.yahyamohamedmao.com
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@jaychapel/can-custom-machine-types-improve-your-life-a9ca3b721316?source=search_post---------377,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
May 1, 2019·3 min read
Today we’re going to look at an interesting trend we are seeing toward the use of custom machine types in Google Cloud Platform. One of the interesting byproducts of managing the ParkMyCloud platform is that we get to see changes and trends in cloud usage in real time. Since we’re directly at the customer level, we can often see these changes before they are spotted by official cloud industry commentators. They start off as small signals in the noise, but practice has allowed us to see when something is shifting and a trend is emerging — as is the case with these custom machine types.
Over the last year, the shift to greater use of Custom Machine Types (launched in 2016 on Google Compute Engine) and to a lesser extent Optimized EC2 instances(launched in 2018 on AWS) are just such a signal that we have observed growing in strength. Interestingly, Microsoft has yet to offer their equivalent version on Azure.
Custom machine types let you build a bespoke instance to match the specific needs of your workload. Many workloads can be matched to off-the-shelf instance types, but there are many workloads for which it is now possible to build a better matching machine which delivers a more cost effective price. The growth in adoption of this particular instance type supports the case and likely benefits of their availability.
So what are the benefits of these new customized machines? First, they provide a granular level of control to match the needs of your specific application workloads. In practice, this leads to compromise as you select the closest instance type to your optimal configuration. Such compromises typically lead to over-provisioning, a situation we see across the board among our customer base. We analyzed usage of the instances in our platform this summer, and found that across all the instances in our platform, the average vCPU utilization was less than 10%!
Secondly, they allow you to finely tune your machine to maximize the cost effectiveness of your infrastructure. Google claims savings of up to 50% when utilizing their customized options compared to traditional predefined instances, which we believe to be a reasonable assessment as we see the standard instance types are often massively overprovisioned.
On GCE, the variables that you can configure include:
Sustained Use Discounts and Preemptible VM Discounts are also available for these customized instances on GCE which also make this an attractive option.
On AWS, customized options are currently more limited and include only the number and type of vCPU’s, and the options are focused on per-core licensed software problems, rather than cost optimization. It will be interesting if they follow Google and open up cost-based customization options in the coming months, and allow the effective unbundling of fixed off-the-shelf instance types.
So just because customization is an option, is this something you should actually pursue? In fact, you will pay a small premium compared to the size of standard instances/VMs, albeit you can optimize for specific workloads, which oftentimes will mean an overall lower cost. To make such an assessment will require that you examine your applications resource use and performance requirements. Such determinations require that you carefully analyze your infrastructure utilization data. This quickly gets complex, although there are a number of platforms which can support thorough analytics and data visualization. Ideally, such analytics would be combined with the ability to recommend specific cost-effective customized instance configurations as well as automate their provisioning.
Watch this space for more news on custom machine types!
Originally published at www.parkmycloud.com on November 13, 2018.
CEO of ParkMyCloud
9 
1
9 
9 
1
CEO of ParkMyCloud
"
https://medium.com/@joachim8675309/bootstrapping-the-cloud-with-knife-gcp-b4ef9b26266?source=search_post---------378,"Sign in
There are currently no responses for this story.
Be the first to respond.
Joaquín Menchaca (智裕)
Apr 9, 2018·4 min read
This article is about knife tool’s role within the Chef workflow and how to get started with Google Cloud by creating a SSH deploy key that knife will use for its bootstrap process.
Knife is an orchestration tool that interacts with the Chef Server to update chef components, such as nodes, environments, roles, and cookbooks, and for bootstapping newly created nodes.
The bootstrap process will do the following:
All of this magic happens because of SSH, and as long as you configure SSH and enabled a deploy key on the target system, you can then remotely configure them to their initial desired state.
This guide will show you how to do this using with GCP (Google Cloud Platfrom) and AWS (Amazon Web Services).
If you are new to GCP, you can get started with a free tier, and after download and install Google Cloud SDK. This will create a default project, Google’s organizational category for managing cloud resources.
At this point we’ll need to run through the following steps:
This is the typical process to generate a key pair, which will create a private key of gce.key and a public SSH key of gce.key.pub.
We need to create a special keysfile format that gcloud tool requires for this. process. The format of this keysfiles will look like this (with users foo, bar, and baz):
We can easily craft this file in shell with the following:
Once we have our keysfile, we can upload the data to our default project, so that future systems will create the Ubuntu account with the installed public key:
Now we can create some systems, such as a three node ElasticSearch cluster (or whatever you desire). In Google Cloud, you can find this under Compute Engine area, and the VM Instances sub-section. There’s a CREATE INSTANCEbutton that gives you some graphical interface like this:
Select Ubuntu 14 image, and create systems, e.g. es-01, es-02, es-03. The defaults are fine for this category. We can view the results with gcloud compute instances list, and inspect the metadata about the system with gcloud compute instances describe <instance_name>.
With this information, we can log into the system using it’s public external IP address and our generated private SSH key for the ubuntu user:
Now that we can access our systems using our generated and installed key, we can easily bootstrap with a script (bash v4 required, e.g. brew install bash for macOS users) like this:
This above snippet is a simplified example, and needs to be tailored to your environment. There are some assumptions and requirements for your chef repository:
Also, these were not addressed for brevity:
A note about the script itself: the script has ephemeral state embedded into the code logic, which is an anti-pattern in my view. Specifically, the script responds to a hard coded static run-list based on the node name. A better solution, omitted for brevity, would be to:
I kept the script as is to illustrate the process involved, we can expand upon this in future articles.
There you have it, the process to bootstrap systems with knife through a SSH deploy key installed into your Google Project. In Part II, I will document how to do the same process with AWS.
Linux NinjaPants Automation Engineering Mutant — exploring DevOps, o11y, k8s, progressive deployment (ci/cd), cloud native infra, infra as code
See all (124)
1 
1 clap
1 
Linux NinjaPants Automation Engineering Mutant — exploring DevOps, o11y, k8s, progressive deployment (ci/cd), cloud native infra, infra as code
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@yogeshmalik/public-cloud-choosing-between-aws-gcp-azure-cloud-war-125a89c64777?source=search_post---------379,"Sign in
There are currently no responses for this story.
Be the first to respond.
Yogesh Malik
Jan 24, 2019·4 min read
◼️ Interconnect◼️ Network Performance & Latency◼️ Price◼️ SD-WAN◼️ Free-tier ◼️ Developer’s view◼️ IoT Development ◼️ API Management◼️ Network-as-a-Service◼️ Backup◼️ Disaster Recovery◼️ Managing Kubernetes ◼️ Container Registry
www.datamation.com
www.networkcomputing.com
www.networkworld.com
www.infoworld.com
www.datamation.com
www.techrepublic.com
www.crn.com
searchcloudcomputing.techtarget.com
www.channele2e.com
www.computerworlduk.com
www.sitepoint.com
www.infoworld.com
searchcloudcomputing.techtarget.com
www.business2community.com
www.networkworld.com
www.cbronline.com
www.techrepublic.com
www.crn.com
Exponential Thinker, Lifelong Learner #Digital #Philosophy #Future #ArtificialIntelligence https://FutureMonger.com/
See all (778)
1 

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
1 clap
1 
Exponential Thinker, Lifelong Learner #Digital #Philosophy #Future #ArtificialIntelligence https://FutureMonger.com/
About
Write
Help
Legal
Get the Medium app
"
https://digitizingpolaris.com/google-cloud-acquires-cask-to-help-simplify-application-enablement-f0fd7e8382c2?source=search_post---------380,"Google Cloud Platform (GCP) CEO Diane Greene wants to make it easier and less of a hassle to build big data applications on her company’s cloud, so she bought Cask Data Inc.
The Palo Alto-based startup makes CDAP (Cask Data Application Platform,) an open source unified integration platform for big data that abstracts the complexity out from Apache Hadoop and Apache Spark and provides developers with easy-to-use APIs with which to build large scale analytics frameworks.
This is exactly the type of thing that Google needs, according to Constellation Research vice president and principal analyst Holger Mueller. Acquisitions like Cask, Velostrata, and Apigee before it, “make it easier to consume new technologies — and that’s the know-how GCP needs to apply to simplify its solution,” he says.
There may be other reasons that Google wanted Cask as well. Consider that it is integrated with all three independent Hadoop distributions — Cloudera, Hortonworks and MapR — making it easier for enterprises to move workloads from one to the next if they fall out of love with the one they are using.
It’s worth considering too that Cask is a hybrid solution; in fact it was initially called Continuuity and built for on-premises. With Cask, as with Google’s other recent acquisition, Velostrata, companies can move their big data workloads to the cloud on their own terms. They can also easily move their workloads between clouds (or from Amazon and Azure to GCP, where Google might like them to stay.)
Mueller says that Google is working hard to make “GCP more palatable to the enterprise decision maker,” who dislikes complexity and doesn’t want to be locked-in.
He also doesn’t hesitate to point out that there is talent — also known as “acquihires” — involved in these acquisitions. The team at Cask, for example, is made up of several Apache Software Foundation committers and members who have not only spent years building Cask but also products at eBay, Facebook, LinkedIn, Microsoft, Pivotal, Yahoo and more.
News and narratives on the trek to the digital enterprise.
3 
3 claps
3 
Written by
Narrating the trek to the digital economy from ActBrilliant.com
News and narratives on the trek to the digital enterprise.
Written by
Narrating the trek to the digital economy from ActBrilliant.com
News and narratives on the trek to the digital enterprise.
"
https://medium.com/@ratrosy/set-up-serverless-store-further-discussion-ea6c4d04167a?source=search_post---------381,"Sign in
There are currently no responses for this story.
Be the first to respond.
Ratros Y.
Jan 24, 2019·2 min read
This document is a part of the Serverless on Google Cloud Platform: an Introduction with Serverless Store Demo How-to Guide. It provides some tips and notes about Serverless Store.
As explained in the opening piece, serverless computing is not necessarily a replacement for VM-based solutions. Some individuals and organizations may favor VMs over serverless solutions for their portability, manageability, and security. Last year in Next 18’, Google announced an open-source project, Knative, which allows developers and operators to build a custom serverless platform on Kubernetes. Since Kubernetes solutions are VM-based, the Knative project can be loosely seen as a middle ground between serverless and VMs; if properly configured, it may offer the best of two worlds. You can learn more about Knative here.
Cloud Pub/Sub guarantees at-least-once delivery. In other words, it is possible that some messages may get delivered more than once. When using Cloud Pub/Sub for event delivery, developers should
Additionally, Cloud Pub/Sub does not guarantee the ordering of messages. For more information, see Ordering messages.
Caching is still critical for serverless apps running on managed services, especially in production, even though serverless systems scale themselves automatically and may not require caches for performance improvement. Most managed products and services, such as Cloud Firestore and Google BigQuery, charges for API calls and requests, and have project-specific quotas and limitations; if necessary, implement a caching layer in your project to keep the number of requests sent in control.
Cloud-native event-driven computing is still fairly new and the pattern showcased in the Serverless Store is far from perfect. For example, you may have realized that the specifications of events is still hard-coded in the app, rather than loaded from a central registry or a specification file, as in the case of OpenAPI and gRPC service development. Google is actively contributing to the CloudEvents project and adding event support to more Google Cloud Platform products and services at this moment; you may want to check these experimental projects out when you build your own serverless event-driven app.
The demo app keeps credentials in environment variables for simplicity reasons, which is a highly discouraged practice for production apps. Google Cloud Platform provides Cloud KMS (Cloud Key Management Service) for managing encryption keys in the Cloud; you may want to integrate it in your serverless app for better security and easier management.
Developer Relations @ Google Cloud Platform
1 
1 clap
1 
Developer Relations @ Google Cloud Platform
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/hashgraph/mirror-node-software-beta-and-tools-now-available-including-one-click-deploy-in-google-cloud-3830b040ad2?source=search_post---------382,"There are currently no responses for this story.
Be the first to respond.
Mirror node software (beta) and tools now available — including one-click deploy in Google Cloud Platform Marketplace
The Hedera mirror node software, during its alpha phase of development, was run by Hedera, exchanges, and other third-parties (such as network explorers like Kabuto, Hashlog, & DragonGlass, and auditing tools like Armanino’s TrustExplorer). These third parties had express permission to access real-time and historical network data buckets in Google Cloud Platform (GCP) and Amazon Web Services (AWS) in order to run their services.
Starting today, we’re happy to announce that everyone now has access to real-time and historical raw data files for the Hedera testnet and mainnet via public AWS and GCP buckets. Early access users experienced the brunt of early mirror node software — they provided Hedera with invaluable feedback, which brought us to beta and public availability today.
Access to these buckets enables developers to configure and deploy their very own beta Hedera mirror node, as well as analyze network data using a brand-new Hedera-ETL software tool that works with Google BigQuery.
In addition, everyone now has the ability to independently verify the authenticity of Hedera mainnet and testnet data. This can all be done without needing any permission from Hedera — an important step in our path towards a fully decentralized network.
Found below is information on and setup instructions for:
The beta Hedera Mirror Node software exposes Hedera network transaction records and account balances generated by the Hedera mainnet or testnet via a REST & gRPC API.
Found below are a few documented ways in which anyone can deploy and configure a Hedera mirror node:
The beta version of the Hedera mirror node software requires the use of public buckets in GCP or AWS to retrieve network data. These buckets record every transaction record, balance (every account and its balance) and events coming from every node on the Hedera mainnet and testnet.
At a high level, the mirror node software, prior to its ingestion of data to the mirror node database, validates every file and signature from AWS or GCP buckets to ensure their validity. It does so by checking:
Technical details on beta mirror nodes can be found in the Hedera documentation.
In the future, the full Hedera mirror node software (v1) will maintain the consensus state of the network and its historical transactions (or a defined subset thereof), in addition to always watching the network for new transactions. By maintaining the consensus state, mirror nodes will be able to record and be queried for mathematically verifiable state proofs for any transaction.
A full mirror node will be capable of doing everything a regular network node can do, sans accepting Hedera API transactions, creating events, or influencing consensus. Full mirror nodes will primarily be used to provide value-added services such as audit support, access to historical data, transaction analytics, and more.
With the availability of network data buckets in AWS and GCP, anyone can integrate both real-time and historical network data directly into their business applications — whether it’s for audit support, transaction analytics, visibility services, security threat modeling, data monetization services, or something entirely different.
The network data stored in GCP and AWS buckets are configured for “requester pays”. This means the beta mirror node operator is responsible for the operational costs and egress from the buckets, but isn’t required to pay a fee for the data itself.
For instruction on how to access these buckets, check out the documentation.
The Hedera team has partnered with the blockchain ETL project to create Hedera-ETL. Hedera-ETL populates a BigQuery dataset with transactions and records generated by the Hedera mainnet or testnet, pulled from public AWS and GCP buckets.
The ETL tool uses the same ingestion software as the mirror node software but, instead of publishing the data to the mirror node database, it’s pushed straight into Google BigQuery. The ETL works like this:
Like any typical batch processing system, Hedera-ETL is most suitable for applications that want to run big, scalable queries and analysis on Hedera network data. For applications that need real-time responsiveness, an aforementioned traditional mirror node would be more suitable. Here are some example queries to help get you started:
Which accounts were newly created in the month of March 2020 and at what exact time?
Which consensus service submit-message transactions were successfully processed on April 1st, 2020?
Originally published at https://www.hedera.com on July 7, 2020.
Hello future
24 
24 claps
24 
Hedera is the most used, sustainable, enterprise-grade public network for the decentralized economy.
Written by
Hedera is the most used, sustainable, enterprise-grade public network for the decentralized economy.
Hedera is the most used, sustainable, enterprise-grade public network for the decentralized economy.
"
https://medium.com/neo4j/neo4j-youtube-channel-onwards-and-upwards-106198bdd0d9?source=search_post---------383,"There are currently no responses for this story.
Be the first to respond.
When I worked on the Google Cloud Platform, we produced many developer-focused videos. While my videos have over 1M views in total, the Android Developer Relations team produced some of the best developer video content on the web.
Upon starting at Neo4j, my VP wanted to see Android-quality videos on a startup budget. I was slightly hesitant because I wanted to make sure we were able to have a high ROI on our content production and video content is notoriously hard to develop and maintain
Instead of a bunch of professional studios with full-time staff, I needed to retrofit a windowless and airflow-less conference room to start filming.
With the video studio in place, I started doing single-handed filming of a lot of content about Graph Databases and Neo4j.
The most important videos I produced starring myself were the Intro to Graph Databases Series. This series now has close to 170k total views and 828 likes.
Through the help of many other producers, interviewees and engineers, we now have 700 videos on the channel, 1.1M views and 10k subscribers.
My colleague, Mark Needham, started the Neo4j Online Meetup in the middle of this journey. This YouTube Live stream now has 39 episodes where he has hosted himself and other developers talking about GraphQL, data science in practice, enterprise data silos, software analytics, database querying standards and even learning Chinese.
Just last week, Michael Hunger started a series of HowTo videos on the APOC Utility Library.
Additionally, we’ve had the opportunity to interview dozens of developers, including Ashley Sun of Lending Club and David Meza of NASA.
This has been an incredible learning experience. When I first started, I knew nothing about video editing or production. Nowadays, I find myself fairly competent but forever learning.
A few (big) things I realized along the way:
There are many other learnings on the software and equipment side that I hope to be able to share in future posts.
You tell us! We certainly have more videos coming as part of the Online Meetup, Intro series and APOC series. And we’ll continue to do interviews, webinars and more. Let us know if there’s anything else you’d like to see!
p.s. check out one of the latest popular videos on our channel
Developer Content around Graph Databases, Neo4j, Cypher…
5 
1
5 claps
5 
1
Written by
Lover of dogs, wine and good food. Intermittent adventurer. Father. Engineer and Director of DevRel @neo4j. Former Googler. Author of O'Reilly OAuth 2.0 book.
Developer Content around Graph Databases, Neo4j, Cypher, Data Science, Graph Analytics, GraphQL and more.
Written by
Lover of dogs, wine and good food. Intermittent adventurer. Father. Engineer and Director of DevRel @neo4j. Former Googler. Author of O'Reilly OAuth 2.0 book.
Developer Content around Graph Databases, Neo4j, Cypher, Data Science, Graph Analytics, GraphQL and more.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.datadriveninvestor.com/architecture-of-kubernetes-and-its-components-ec9bff693293?source=search_post---------384,"There are currently no responses for this story.
Be the first to respond.
Kubernetes architecture has mainly 3 components and they are the Master Nodes, the Worker Nodes and the distributed key-value stores like etcd.
The Master Node is responsible for managing the Kubernetes cluster and it is the entry…
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@dzakyputra/hi-rauf-thank-you-for-asking-ab68f7b6e1c2?source=search_post---------385,"Sign in
There are currently no responses for this story.
Be the first to respond.
Dzaky Widya Putra
Feb 21, 2019·1 min read
Rauf javid
Hi Rauf, thank you for asking.
To host and run your code you can use cloud servers like Amazon Web Services (AWS) or Google Cloud Platform (GCP), but I personally use GCP.
You can use “screen” to run the program even if you close the terminal.
— — — — Hosting python code in GCP: https://www.youtube.com/watch?v=o8XxAWZwnOg
Use screen: https://linuxize.com/post/how-to-use-linux-screen/
Software Engineering | Data Science
1 
1 clap
1 
Software Engineering | Data Science
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@adron/just-another-sunday-ea73bf759f3?source=search_post---------386,"Sign in
There are currently no responses for this story.
Be the first to respond.
Adron Hall
Oct 2, 2017·3 min read
I sit here at the moment watching two Kubernetes Clusters build. One is building on Azure and one on Google Cloud Platform (GCP). I’ve got a presentation coming up this Tuesday and Thursday, both I’ll be digging into Kubernetes, Terraform, and a number of other technologies. Those are the two hot technologies for the talks though. Albeit, the continuous integration, languages, and tooling that Terraform builds via configuration and Kubernetes runs in containers is what is actually the meat of this whole sandwich. Which is where I ponder what all of this goo is that wires together things in this virtual programmatic realm in which I’ll build something on top of.
It seems messy from inception. But then of course, all programming and related ecosystem elements in which programming takes place is a messy bag of guts.
Here I sit then, waiting the rather unknown pseudo random amount of time for the Kubernetes Clusters to finish building. A few moments pass and sure enough, as always, very inconsistent build times. The Azure Kubernetes Cluster took 7 minutes to build and the GCP Kubernetes Cluster took just 4 minutes. Last night the Azure cluster was taking 20 minutes or more while the GCP cluster was consuming about 3–4 minutes to build. I’m not sure, as I’ve not dug into the matter deep enough, but something seems awry within the way Azure needs to build out its instances, networking, and related cluster mechanisms. I’m not surprised though, Azure has always behaved and felt slow and cumbersome during the build out of infrastructure. GCP on the other hand clearly comes from Google’s thoroughbred engineering focus on things. It generally builds in a much smaller range of time, consuming much less time overall.
As I build all of this, to work out what will and won’t be in the demo, I find myself next fiddling with presentation material. I really don’t even like to have presentation material, I’d much rather have an interesting enough talk and respective code, samples, and demo to just show the whole thing. Presentation slide decks always fell like, and almost always are, just a crutch for the inability to form ideas, show concepts, or otherwise actually engage the audience around what is being presented. It’s a frustrating dichotomy to say the least. Eventually, with these latest efforts, I actually intend to get down to two slides: one for my information when ending the talk, the requisite contact information and such, then two would be the intro slide with a fancy title for whatever the meat of the talk will be about.
All of this work however is going to be interrupted by the dramatically more important bike ride I’ll take later to clear my thoughts and get the blood flowing through my veins. As things go, I actually dislike sitting still for more than a few hours. I like to chunk my time into brackets, get the work done, and then go for a ride, walk, or something to get my mind cleared back up. I hear it’s healthier for us humans too, but I’ve not set the research to memory to make that argument.
Until later… fini.
Software dev, data, heavy metal, transit, economics, freethought, atheism, cycling, livability, beautiful things & adrenaline junkie.
Software dev, data, heavy metal, transit, economics, freethought, atheism, cycling, livability, beautiful things & adrenaline junkie.
"
https://medium.com/@garystafford/building-serverless-actions-for-google-assistant-with-google-cloud-functions-cloud-datastore-and-e68b90f1c4ef?source=search_post---------387,"Sign in
There are currently no responses for this story.
Be the first to respond.
Gary A. Stafford
Aug 12, 2018·20 min read
In this post, we will create an Action for Google Assistant using the ‘Actions on Google’ development platform, Google Cloud Platform’s serverless Cloud Functions, Cloud Datastore, and Cloud Storage, and the current LTS version of Node.js. According to Google, Actions are pieces of software, designed to extend the functionality of the Google Assistant, Google’s virtual personal assistant, across a multitude of Google-enabled devices, including smartphones, cars, televisions, headphones, watches, and smart-speakers.
Here is a brief YouTube video preview of the final Action for Google Assistant, we will explore in this post, running on an Apple iPhone 8.
If you want to compare the development of an Action for Google Assistant with those of AWS and Azure, in addition to this post, please read my previous two posts in this series, Building and Integrating LUIS-enabled Chatbots with Slack, using Azure Bot Service, Bot Builder SDK, and Cosmos DB and Building Asynchronous, Serverless Alexa Skills with AWS Lambda, DynamoDB, S3, and Node.js. All three of the article’s demonstrations are written in Node.js, all three leverage their cloud platform’s machine learning-based Natural Language Understanding services, and all three take advantage of NoSQL database and storage services available on their respective cloud platforms.
The final architecture of our Action for Google Assistant will look as follows.
Here is a brief overview of the key technologies we will incorporate into our architecture.
According to Google, Actions on Google is the platform for developers to extend the Google Assistant. Similar to Amazon’s Alexa Skills Kit Development Console for developing Alexa Skills, Actions on Google is a web-based platform that provides a streamlined user-experience to create, manage, and deploy Actions. We will use the Actions on Google platform to develop our Action in this post.
According to Google, Dialogflow is an enterprise-grade Natural language understanding (NLU) platform that makes it easy for developers to design and integrate conversational user interfaces into mobile apps, web applications, devices, and bots. Dialogflow is powered by Google’s machine learning for Natural Language Processing (NLP). Dialogflow was initially known as API.AI prior being renamed by Google in late 2017.
We will use the Dialogflow web-based development platform and version 2 of the Dialogflow API, which became GA in April 2018, to build our Action for Google Assistant’s rich, natural-language conversational interface.
Google Cloud Functions are the event-driven serverless compute platform, part of the Google Cloud Platform (GCP). Google Cloud Functions are comparable to Amazon’s AWS Lambda and Azure Functions. Cloud Functions is a relatively new service from Google, released in beta in March 2017, and only recently becoming GA at Cloud Next ’18 (July 2018). The main features of Cloud Functions include automatic scaling, high availability, fault tolerance, no servers to provision, manage, patch or update, and a payment model based on the function’s execution time. The programmatic logic behind our Action for Google Assistant will be handled by a Cloud Function.
We will write our Action’s Google Cloud Function using the Node.js 8 runtime. Google just released the ability to write Google Cloud Functions in Node 8.11.1 and Python 3.7.0, at Cloud Next ’18 (July 2018). It is still considered beta functionality. Previously, you had to write your functions in Node version 6 (currently, 6.14.0).
Node 8, also known as Project Carbon, was the first Long Term Support (LTS) version of Node to support async/await with Promises. Async/await is the new way of handling asynchronous operations in Node.js. We will make use of async/await and Promises within our Action’s Cloud Function.
Google Cloud Datastore is a highly-scalable NoSQL database. Cloud Datastore is similar in features and capabilities to Azure Cosmos DB and Amazon DynamoDB. Datastore automatically handles sharding and replication and offers features like a RESTful interface, ACID transactions, SQL-like queries, and indexes. We will use Datastore to persist the information returned to the user from our Action for Google Assistant.
The last technology, Google Cloud Storage is secure and durable object storage, nearly identical to Amazon Simple Storage Service (Amazon S3) and Azure Blob Storage. We will store publicly accessible images in a Google Cloud Storage bucket, which will be displayed in Google Assistant Basic Card responses.
To demonstrate Actions for Google Assistant, we will build an informational Action that responds to the user with interesting facts about Azure, Microsoft’s Cloud computing platform (Google talking about Azure, ironic). Note this is not intended to be an official Microsoft bot and is only used for demonstration purposes.
All open-sourced code for this post can be found on GitHub. Note code samples in this post are displayed as Gists, which may not display correctly on some mobile and social media browsers. Links to gists are also provided.
This post will focus on the development and integration of an Action with Google Cloud Platform’s serverless and asynchronous Cloud Functions, Cloud Datastore, and Cloud Storage. The post is not intended to be a general how-to on developing and publishing Actions for Google Assistant, or how to specifically use services on the Google Cloud Platform.
Building the Action will involve the following steps.
Let’s explore each step in more detail.
The conversational model design of the Azure Tech Facts Action for Google Assistant is similar to the Azure Tech Facts Alexa Custom Skill, detailed in my previous post. We will have the option to invoke the Action in two ways, without initial intent (Explicit Invocation) and with intent (Implicit Invocation), as shown below. On the left, we see an example of an explicit invocation of the Action. Google Assistant then queries the user for more information. On the right, an implicit invocation of the Action includes the intent, being the Azure fact they want to learn about. Google Assistant responds directly, both verbally and visually with the fact.
Each fact returned by Google Assistant will include a Simple Response, Basic Card and Suggestions response types for devices with a display, as shown below. The user may continue to ask for additional facts or choose to cancel the Action at any time.
Lastly, as part of the conversational model, we will include the option of asking for a random fact, as well as asking for help. Examples of both are shown below. Again, Google Assistant responds to the user, vocally and, optionally, visually, for display-enabled devices.
The following steps assume you have an existing GCP account and you have created a project on GCP to house the Cloud Function, Cloud Storage Bucket, and Cloud Datastore Entities. The post also assumes that you have the Google Cloud SDK installed on your development machine, and have authenticated your identity from the command line (gist).
First, the images, actually Azure icons available from Microsoft, displayed in the responses shown above, are uploaded to a Google Storage Bucket. To handle these tasks, we will use the gsutil CLI to create, upload, and manage the images. The gsutil CLI tool, like gcloud, is part of the Google Cloud SDK. The gsutil mb (make bucket) command creates the bucket, gsutil cp (copy files and objects) command is used to copy the images to the new bucket, and finally, the gsutil iam (get, set, or change bucket and/or object IAM permissions) command is used to make the images public. I have included a shell script, bucket-uploader.sh, to make this process easier. (gist).
From the Storage Console on GCP, you should observe the images all have publicly accessible URLs. This will allow the Cloud Function to access the bucket, and retrieve and display the images. There are more secure ways to store and display the images from the function. However, this is the simplest method since we are not concerned about making the images public.
We will need the URL of the new Storage bucket, later, when we develop to our Action’s Cloud Function. The bucket URL can be obtained from the Storage Console on GCP, as shown below in the Link URL.
In Cloud Datastore, the category data object is referred to as a Kind, similar to a Table in a relational database. In Datastore, we will have an ‘AzureFact’ Kind of data. In Datastore, a single object is referred to as an Entity, similar to a Row in a relational database. Each one of our entities represents a unique reference value from our Azure Facts Intent’s facts entities, such as ‘competition’ and ‘certifications’. Individual data is known as a Property in Datastore, similar to a Column in a relational database. We will have four Properties for each entity: name, response, title, and image. Lastly, a Key in Datastore is similar to a Primary Key in a relational database. The Key we will use for our entities is the unique reference value string from our Azure Facts Intent’s facts entities, such as ‘competition’ or ‘certifications’. The Key value is stored within the entity’s name Property.
There are a number of ways to create the Datastore entities for our Action, including manually from the Datastore console on GCP. However, to automate the process, we will use a script, written in Node.js and using the Google Cloud Datastore Node.js Client, to create the entities. We will use the Client API’s Datastore Class upsert method, which will create or update an entire collection of entities with one call and returns a callback. The script , upsert-entities.js, is included in source control and can be run with the following command. Below is a snippet of the script, which shows the structure of the entities (gist).
Once the upsert command completes successfully, you should observe a collection of ‘AzureFact’ Type Datastore Entities in the Datastore console on GCP.
Below, we see the structure of a single Datastore Entity, the ‘certifications’ Entity, containing the fact response, title, and name of the image, which is stored in our Google Storage bucket.
With the images uploaded and the database entries created, we can start building our Action. Using the Actions on Google web console, we first create a new Actions project.
The Directory Information tab is where we define all the metadata about the Action. This information determines how the Action will look in the Actions directory and is required to publish your Action. The directory is where users discover published Actions on the web and mobile devices.
When developing the Actions, you will want to switch from the Actions on Google console to the Dialogflow console. Actions on Google provides a link for switching to Dialogflow in the Actions tab.
The first thing you will notice when switching to Dialogflow is what was referred to as Actions in the Actions on Google console now appears to be referred to as Intents in Dialogflow. According to Google, an Action is ‘an interaction you build for the Assistant that supports a specific intent and has a corresponding fulfillment that processes the intent.’ There is a direct relationship between an Action and an Intent. The word Intent, used by Dialogflow, is standard terminology across other voice-assistant platforms, such as Alexa and LUIS. All we need to know is that we are building Intents to support Actions — the Azure Facts Intent, Welcome Intent, and the Fallback Intent.
Below, we see the Azure Facts Intent. The Azure Facts Intent is the main Intent, responsible for handling our user’s requests for facts about Azure. The Intent includes a fair number, but certainly not an exhaustive list, of training phrases. These represent all the possible ways a user might express intent when invoking the Action. According to Google, the greater the number of natural language examples in the Training Phrases section of Intents, the better the classification accuracy.
Each of the highlighted words in the training phrases maps to the facts parameter, which maps to a collection of @facts Entities. Entities represent a list of intents the Action is trained to understand. According to Google, there are three types of entities: ‘system’ (defined by Dialogflow), ‘developer’ (defined by a developer), and ‘user’ (built for each individual end-user in every request) entities. We will be creating ‘developer’ type entities for our Action’s Intent.
An entity contains Synonyms. Multiple synonyms may be mapped to a single reference value. The reference value is the value passed to the Cloud Function by the Action. For example, take the reference value of ‘competition’. A user might ask Google about Azure’s competition. However, the user might also substitute the words ‘competitor’ or ‘competitors’ for ‘competition’. Using synonyms, if the user utters any of these three words in their intent, they will receive the same response.
Although our Azure Facts Action is a simple example, typical Actions might contain hundreds of entities or more, each with several synonyms. Dialogflow provides the option of copy and pasting bulk entities, in either JSON or CSV format. The project’s source code includes both JSON or CSV formats, which may be input in this manner.
Not every possible fact, which will have a response, returned by Google Assistant, needs an entity defined. For example, we created a ‘compliance’ Cloud Datastore Entity. The Action understands the term ‘compliance’ and will return a response to the user if they ask about Azure compliance. However, ‘compliance’ is not defined as an Intent Entity, since we have chosen not to define any synonyms for the term ‘compliance’.
In order to allow this, you must enable Allow Automated Expansion. According to Google, this option allows an Agent to recognize values that have not been explicitly listed in the entity. Google describes Agents as NLU (Natural Language Understanding) modules.
Another configuration item in Dialogflow that needs to be completed is the Dialogflow’s Actions on Google integration. This will integrate the Azure Tech Facts Action with Google Assistant. Google provides more than a dozen different integrations, as shown below.
The Dialogflow’s Actions on Google integration configuration is simple, just choose the Azure Facts Intent as our Action’s Implicit Invocation intent, in addition to the default Welcome Intent, which is our Action’s Explicit Invocation intent. According to Google, integration allows our Action to reach users on every device where the Google Assistant is available.
When an intent is received from the user, it is fulfilled by the Action. In the Dialogflow Fulfillment console, we see the Action has two fulfillment options, a Webhook or a Cloud Function, which can be edited inline. A Webhook allows us to pass information from a matched intent into a web service and get a result back from the service. In our example, our Action’s Webhook will call our Cloud Function, using the Cloud Function’s URL endpoint. We first need to create our function in order to get the endpoint, which we will do next.
Our Cloud Function, called by our Action, is written in Node.js 8. As stated earlier, Node 8 LTS was the first LTS version to support async/await with Promises. Async/await is the new way of handling asynchronous operations in Node.js, replacing callbacks.
Our function, index.js, is divided into four sections: constants, intent handlers, helper functions, and the function’s entry point. The Cloud Function attempts to follow many of the coding practices from Google’s code examples on Github.
The section defines the global constants used within the function. Note the constant for the URL of our new Cloud Storage bucket, on line 30 below, IMAGE_BUCKET, references an environment variable, process.env.IMAGE_BUCKET. This value is set in the .env.yaml file. All environment variables in the .env.yaml file will be set during the Cloud Function’s deployment, explained later in this post. Environment variables were recently released, and are still considered beta functionality (gist).
The npm package dependencies declared in the constants section, are defined in the dependencies section of the package.json file. Function dependencies include Actions on Google, Firebase Functions, and Cloud Datastore (gist).
The three intent handlers correspond to the three intents in the Dialogflow console: Azure Facts Intent, Welcome Intent, and Fallback Intent. Each handler responds in a very similar fashion. The handlers all return a SimpleResponse for audio-only and display-enabled devices. Optionally, a BasicCard is returned for display-enabled devices (gist).
The Welcome Intent handler handles explicit invocations of our Action. The Fallback Intent handler handles both help requests, as well as cases when Dialogflow cannot match any of the user’s input. Lastly, the Azure Facts Intent handler handles implicit invocations of our Action, returning a fact to the user from Cloud Datastore, based on the user’s requested fact.
The next section of the function contains two helper functions. The primary function is the buildFactResponse function. This is the function that queries Google Cloud Datastore for the fact. The second function, the selectRandomFact, handles the fact value of ‘random’, by selecting a random fact value to query Datastore. (gist).
Let’s look closer at the relationship and asynchronous nature of the Azure Facts Intent intent handler and buildFactResponse function. Below, note the async function on line 1 in the intent and the await function on line 3, which is part of the buildFactResponse function call. This is typically how we see async/await applied when calling an asynchronous function, such as buildFactResponse. The await function allows the intent’s execution to wait for the buildFactResponse function’s Promise to be resolved, before attempting to use the resolved value to construct the response.
The buildFactResponse function returns a Promise, as seen on line 28. The Promise’s payload contains the results of the successful callback from the Datastore API’s runQuery function. The runQuery function returns a callback, which is then resolved and returned by the Promise, as seen on line 40 (gist).
The payload returned by Google Datastore, through the resolved Promise to the intent handler, will resemble the example response, shown below. Note the image, response, and title key/value pairs in the textPayload section of the response payload. These are what are used to format the SimpleResponse and BasicCard responses (gist).
To deploy the Cloud Function to GCP, use the gcloud CLI with the beta version of the functions deploy command. According to Google, gcloud is a part of the Google Cloud SDK. You must download and install the SDK on your system and initialize it before you can use gcloud. You should ensure that your function is deployed to the same region as your Google Storage Bucket. Currently, Cloud Functions are only available in four regions. I have included a shell script, deploy-cloud-function.sh, to make this step easier. (gist).
The creation or update of the Cloud Function can take up to two minutes. Note the .gcloudignore file referenced in the verbose output below. This file is created the first time you deploy a new function. Using the the .gcloudignore file, you can limit the deployed files to just the function (index.js) and the package.json file. There is no need to deploy any other files to GCP.
If you recall, the URL endpoint of the Cloud Function is required in the Dialogflow Fulfillment tab. The URL can be retrieved from the deployment output (shown above), or from the Cloud Functions Console on GCP (shown below). The Cloud Function is now deployed and will be called by the Action when a user invokes the Action.
With our Action and all its dependencies deployed and configured, we can test the Action using the Simulation console on Actions on Google. According to Google, the Action Simulation console allows us to manually test our Action by simulating a variety of Google-enabled hardware devices and their settings. You can also access debug information such as the request and response that your fulfillment receives and sends.
Below, in the Action Simulation console, we see the successful display of the initial Azure Tech Facts containing the expected Simple Response, Basic Card, and Suggestions, triggered by a user’s explicit invocation of the Action.
The simulated response indicates that the Google Cloud Function was called, and it responded successfully. It also indicates that the Google Cloud Function was able to successfully retrieve the correct image from Google Cloud Storage.
Below, we see the successful response to the user’s implicit invocation of the Action, in which they are seeking a fact about Azure’s Cognitive Services. The simulated response indicates that the Google Cloud Function was called, and it responded successfully. It also indicates that the Google Cloud Function was able to successfully retrieve the correct Entity from Google Cloud Datastore, as well as the correct image from Google Cloud Storage.
If we had issues with the testing, the Action Simulation console also contains tabs containing the request and response objects sent to and from the Cloud Function, the audio response, a debug console, and any errors.
In addition to the Simulation console’s ability to debug issues with our service, we also have Google Stackdriver Logging. The Stackdriver logs, which are viewed from the GCP management console, contain the complete requests and responses, to and from the Cloud Function, from the Google Assistant Action. The Stackdriver logs will also contain any logs entries you have explicitly placed in the Cloud Function.
We also have the ability to view basic Analytics about our Action from within the Dialogflow Analytics console. Analytics displays metrics, such as the number of sessions, the number of queries, the number of times each Intent was triggered, how often users exited the Action from an intent, and Sessions flows, shown below.
In simple Action such as this one, the Session flow is not very beneficial. However, in more complex Actions, with multiple Intents and a variety potential user interactions, being able to visualize Session flows becomes essential to understanding the user’s conversational path through the Action.
In this post, we have seen how to use the Actions on Google development platform and the latest version of the Dialogflow API to build Google Actions. Google Actions rather effortlessly integrate with the breath Google Cloud Platform’s many serverless offerings, including Google Cloud Functions, Cloud Datastore, and Cloud Storage.
We have seen how Google is quickly maturing their serverless functions, to compete with AWS and Azure, with the recently announced support of LTS version 8 of Node.js and Python, to create an Actions for Google Assistant.
As an Engineer, I have spent endless days, late nights, and thankless weekends, building, deploying and managing servers, virtual machines, container clusters, persistent storage, and database servers. I think what is most compelling about platforms like Actions on Google, but even more so, serverless technologies on GCP, is that I spend the majority of my time architecting and developing compelling software. I don’t spend time managing infrastructure, worrying about capacity, configuring networking and security, and doing DevOps.
¹Azure is a trademark of Microsoft
All opinions expressed in this post are my own and not necessarily the views of my current or past employers, their clients, or Google and Microsoft.
Originally published at programmaticponderings.com on August 11, 2018.
AWS Senior Solutions Architect | AWS Certified Pro | Polyglot Developer | Data Analytics | DataOps | DevOps
1 

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
1 clap
1 
AWS Senior Solutions Architect | AWS Certified Pro | Polyglot Developer | Data Analytics | DataOps | DevOps
About
Write
Help
Legal
Get the Medium app
"
https://deephunt.in/deep-hunt-issue-30-2fd7602d5ff?source=search_post---------388,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
GPUs are now available for Google Compute Engine and Cloud Machine Learning
cloudplatform.googleblog.com
These instances support popular machine learning and deep learning frameworks such as TensorFlow, Theano, Torch, MXNet and Caffe, as well as NVIDIA’s popular CUDA software for building GPU-accelerated applications.
Paying With Your Face
www.technologyreview.com
Face-detecting systems in China now authorize payments, provide access to facilities, and track down criminals. Will other countries follow?
When computers learn to swear: Using machine learning for better online conversations
www.blog.google
Google has an ambitious plan to use artificial intelligence to weed out abusive comments and defang online mobs. Jigsaw in collaboration with media companies tries to determine toxicity of comments on web. Here’s MIT Technology Review’s take on this.
The Rise of AI Makes Emotional Intelligence More Important
hbr.org
As machine learning continues to grow, we all need to develop new skills in order to differentiate ourselves. But which ones?Develop the skills machines can’t replicate.
Inside Facebook’s AI Machine
backchannel.com
Facebook’s Applied Machine Learning group helps Facebook see, talk, and understand. It may even root out fake news. At Backchannel, Steven Levy has an inside look at Facebook’s AI machine.
Supporting the AI Talent Pipeline
medium.com
A perfect storm is brewing. Professors are leaving academia leaving gaps in university faculties to be filled. More and more graduating Ph.D. students are leaving academia, making it harder and harder to fill faculty vacancies. What can be done?
Paper notes on Wasserstein GAN
www.alexirpan.com
What is Wasserstein GAN all about and why it got so much publicity — a read-through that explains the paper in simple terms.
Prophet: forecasting at scale
research.fb.com
Facebook open sources Prophet — a forecasting tool available in Python and R that has been a key piece to improving Facebook’s ability to create a large number of trustworthy forecasts used for decision-making and even in product features.
How Did Spotify Get So Good At Machine Learning?
How did Spotify get so good at machine learning? Was machine learning important from the start, or did they catch up over time?
Automatic Handgun Detection Alarm in Videos Using Deep Learning
arxiv.org
A simple CNN based model that will detect handguns in a video and raise an alarm that can be used in surveillance and control systems that still require human supervision and intervention.
If you like what you are reading, please follow and recommend to your friends or give a shoutout on Twitter! I’m glad to hear your suggestions and recommendations @deephunt_in or in comments below!
Originally published as Deep Hunt newsletter.
Your weekly newsletter on the hottest things in Artificial…
1 
1 clap
1 
Written by
Dreamer, @iitguwahati alum. Creator of @deephunt_in, Organiser @ DeepLearningDelhi | Interested in all things data and machine learning.
Your weekly newsletter on the hottest things in Artificial Intelligence carefully curated by Avinash Hindupur!
Written by
Dreamer, @iitguwahati alum. Creator of @deephunt_in, Organiser @ DeepLearningDelhi | Interested in all things data and machine learning.
Your weekly newsletter on the hottest things in Artificial Intelligence carefully curated by Avinash Hindupur!
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/opsops/small-big-wtf-in-dynamic-inventory-for-gcp-c64cc57e5286?source=search_post---------389,"There are currently no responses for this story.
Be the first to respond.
GCP stands for Google Cloud Platform.
They provides their own dynamic inventory source, which allows to query GCP for existing VMs and put them into groups based on filters. The inventory plugin for this is called ‘gcp_compute'.
It allows to set host variables based on available instance data (labels, name).
The most common is this:
Boring and simple. I decided to add their something more. For example, a username.
It looked innocent and it almost worked. GCP wants not a string here, but a Jinja expression.
The problem is that bot variable is not defined. And gcp_compute is really nasty here. Instead of reporting the issue and rejecting to proceed, it’s just ignore the line. So, no bot variable, no ansible_user defined. In my case the problem was that ansible_user is defined, but I expected it to be redefined, and it wasn’t. What a mess.
Anyway, the proper way to fix this is to use jinja expression (without mustaches):
Note, there are nested quotes, because first layer is stripped by yaml parser (Both ansible_user: bot and ansible_user: ""bot"" are the same), and only adding second layer of quotes I get “a string” for Jinja.
If I wasn’t that much deep into Ansible machinery, that would have been a big fat WTF. At my current understanding it’s a minor quirk of google modules (which are unexpectedly lax toward Ansible best practices), and some eye rolling.
Tech blog on Linux, Ansible, Ceph, Openstack and…
1 
1 clap
1 
Tech blog on Linux, Ansible, Ceph, Openstack and operator-related programming
Written by
I work at Servers.com, most of my stories are about Ansible, Ceph, Python, Openstack and Linux.
Tech blog on Linux, Ansible, Ceph, Openstack and operator-related programming
"
https://medium.com/@thanachart-rit/managing-data-lifecycle-%E0%B8%95%E0%B8%AD%E0%B8%99%E0%B8%97%E0%B8%B5%E0%B9%88-4-explore-and-visualize-bbd01a4085c2?source=search_post---------392,"Sign in
There are currently no responses for this story.
Be the first to respond.
Thanachart Ritbumroong
Feb 7, 2017·1 min read
ตอนสุดท้ายแล้วครับของการจัดการ Data Lifecycle ด้วย Google Cloud Platform ใครยังไม่ได้อ่านตอนแรกๆ ก็แวะไปอ่านกันได้ครับ เป็นเรื่องเกี่ยวกับ Data Ingestion, Data Storage, Process and Analyze
Managing Data Lifecycle ตอนที่ 1: Ingest
Managing Data Lifecycle ตอนที่ 2: Store
Managing Data Lifecycle ตอนที่ 3: Process and Analyze
เครื่องมือสุดที่เลิฟของ Data Scientist บน GCP รองจาก BigQuery ก็คือ Datalab ครับ เค้าก็คืออออ iPython บน Cloud นี่เองงง แต่อย่าคิดว่าบน Cloud จะ run ข้อมูลได้มากมายมหาศาลนะครับ เค้าบอกว่าตัวนี้เป็นเหมือนเครื่องมือให้เรา develop code ให้เสร็จเรียบร้อย แล้ว limit จำนวน records มา run ดู เสร็จแล้วค่อยเอา code ไป run บน Cloud Dataflow
อย่างที่เกริ่นไปครับ ตัวนี้ก็คือ iPython ถ้าใครไม่รู้จัก ก็คือ เหมือนตัวที่เขียน code ไป แล้วดู output เป็นทอดๆไปได้ สะดวกง่ายมาก แบบรูปด้านล่างอ่ะครับ
ตอนนี้รองรับ 3 ภาษา Python, SQL, และ JavaScript
สามารถนำมาทำได้หลายอย่างเลยนะครับ เอามาเขียน TensorFlow ก็ได้ plot graph ก็ได้ ไว้บทความหน้าๆ จะมาเจาะลึกเรื่อง Datalab ให้ครับ
cloud.google.com
Reference:
cloud.google.com
Lecturer at Management of Analytics and Data Science Program, National Institute of Development Administration, Thailand and Data Analytics Consultant
See all (501)
Lecturer at Management of Analytics and Data Science Program, National Institute of Development Administration, Thailand and Data Analytics Consultant
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@LawrenceHecht/regarding-your-pricing-complaints-i-hear-you-c2ed986a5b8e?source=search_post---------393,"Sign in
There are currently no responses for this story.
Be the first to respond.
Lawrence Hecht
Mar 1, 2016·1 min read
Andrew McLagan
Regarding your pricing complaints, I hear you. How do you think Docker should charge? Is a subscription best? Obviously, they’re trying to discriminate (in an economic sense) based on size of implementation.
I’ve been looking at pricing models for containers. It is fascinating. Google Cloud Platform charges per node. IBM Bluemix charges based on container size (memory). However, AWS and Azure are currently only charging for the underlying infrastructure and instance. And then there are lots of pure-play container hosting companies.
I look forward to reading your future posts.
Edit/analyze/curate. Interest in #techpr/AR, #techpolicy, #blockchain #opendata, #analytics. Know about politics, econ, #enterpriseIT and surveys
2
2
Edit/analyze/curate. Interest in #techpr/AR, #techpolicy, #blockchain #opendata, #analytics. Know about politics, econ, #enterpriseIT and surveys
"
https://medium.com/altoros-blog/cloud-foundry-and-gcp-machine-learning-storage-and-debugging-b8d6ca13d1f3?source=search_post---------394,"There are currently no responses for this story.
Be the first to respond.
With a recently implemented service broker for Google Cloud Platform (GCP), Cloud Foundry users got access to a variety of APIs. The blog post overviews such machine learning APIs as Translation, Natural Language, Cloud Speech, Video Intelligence, and Cloud Vision, highlighting the benefits of combining them. In addition, the article features the Stackdriver tool — offered as part of GCP — which can be used to enable live debugging for an app already pushed to Cloud Foundry.
https://www.altoros.com/blog/cloud-foundry-and-gcp-machine-learning-storage-and-debugging/
Driving digital transformation with cloud-native platforms, blockchain, ML/RPA.
Written by
Altoros provides consulting and fully-managed services for cloud automation, microservices, blockchain, and AI&ML.
Driving digital transformation with cloud-native platforms, such as Kubernetes and Cloud Foundry. Helping to disrupt industries with blockchain and ML/RPA.
Written by
Altoros provides consulting and fully-managed services for cloud automation, microservices, blockchain, and AI&ML.
Driving digital transformation with cloud-native platforms, such as Kubernetes and Cloud Foundry. Helping to disrupt industries with blockchain and ML/RPA.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@qwiklabs/google-cloud-training-for-developers-start-your-quest-1aa9e0fc6cf6?source=search_post---------395,"Sign in
There are currently no responses for this story.
Be the first to respond.
Qwiklabs
Aug 16, 2017·2 min read
Developers, learn how Google Cloud Platform (GCP) can make your life easier with the latest online training from Qwiklabs. Enroll in the Developing Applications Quest and practice building and deploying apps on the Google Cloud Platform. This 7-lab Quest walks you through build and deploy scenarios using Node.js, Ruby, Python, and even ASP.Net Core Frameworks. When you complete the labs, you can use what you learned to run your own apps on GCP. And you can add the Developing Applications badge to your resume — a credential that confirms your “flight time” with the GCP Console.
Here’s some of what you’ll do:
Complete this Quest and use what you learned to use GCP for your next project. And don’t forget to take advantage of GCP’s free offerings to get you started qwikly!
Originally published at blog.qwiklabs.com on August 16, 2017.
"
https://medium.com/@dzakyputra/hi-rauf-thank-you-for-asking-f29f85848dd7?source=search_post---------396,"Sign in
There are currently no responses for this story.
Be the first to respond.
Dzaky Widya Putra
Feb 21, 2019·1 min read
Rauf javid
Hi Rauf, thank you for asking.
To host and run your code you can use cloud servers like Amazon Web Services (AWS) or Google Cloud Platform (GCP), but I personally use GCP.
You can use “screen” to run the program even if you close the terminal.
— — — — Hosting python code in GCP: https://www.youtube.com/watch?v=o8XxAWZwnOg
Use screen: https://linuxize.com/post/how-to-use-linux-screen/
Software Engineering | Data Science
1
1
Software Engineering | Data Science
"
https://medium.com/@jaychapel/do-google-sustained-use-discounts-really-save-you-money-parkmycloud-623c4eac3a72?source=search_post---------397,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
May 3, 2018·4 min read
When looking to keep Google Cloud Platform (GCP) costs in control, the first place users turn are the discount options offered by the cloud service provider itself, such as Google’s Sustained Use discounts. The question is: do Google Sustained Use discounts actually save you money, when you could just turn the instance off?
The idea of the Sustained Use discount is that the longer you run a VM instance in any given month, the bigger discount you will get from the list price. The following shows the incremental discount, and its cumulative impact on a hypothetical $100/month VM instance, where the percentages are against the baseline 730-hour month.
I have to say here that the GCP prices listed can be somewhat misleading unless you read the fine print where it says “Note: Listed monthly pricing includes applicable, automatic sustained use discounts, assuming the instance runs for a 730 hour month.” What this means to us is that the list prices of the instances are actually much higher, but their progressive discount means that no one ever actually pays list price. That said — the list price is what you need to know in order to estimate the actual cost you will pay if you do not plan to leave the instance up for 730 hours/month.
For example, the price shown on the GCP pricing link for an n1-standard-8 instance in the Iowa region is (as of this writing) $194.1800. The list price for this instance would be $194.1800/0.7 = $277.40. This is the figure that must be used as the entry point for the table above to calculate the actual cost, given a certain level of utilization.
Here at ParkMyCloud, we’re all about scheduling resources to turn off when you’re not using them, i.e., “parking” them. With this mindset, I wondered about the impact of the sustained use discounts on the schedule-based savings. The following chart plots the cost of that n1-standard-8 VM instance, showing Google sustained use discounts combined with a parking schedule.
We can definitely see progressively more sustained use savings added to progressively less schedule-based savings. I am sure this would end up getting described as the typical hype of “the more you spend, the more you save!” But, the reality of the matter must intrude here and show the more you spend…the more you spend!
Looking at what this means for ParkMyCloud users, here is the monthly uptime for a few common parking schedules, and the associated cost:
These are a far cry from the $277.40 list price, and even the $194.18 max discounted price. From this, it can be seen that even with the most wide-open “work day” schedule of 12 hours per weekday, the schedule is barely nudging over the 182.5 hours needed to hit the first price break of 20%. And even then, the 20% discount is only applied to those hours above 182.5 hours. A welcome discount to be sure, but not very enormously impactful to the bottom line.
Another way our users keep these utilization hours low is by keeping their VM instances “always parked” and temporarily overriding the schedule for a set number of hours (such as for an 8-hour workday) when their non-production resources are needed. When the duration of the override expires, the instance is automatically shut down. Giving the best possible savings, and usually never even hitting the first GCP discount tier.
In short: definitely! At least, they do save you money over the price listed by Google. Do they save you the maximum amount of money possible? No, not if it’s a non-production VM instance that is only needed during a regular workday (although it’s close).
To get the optimal savings on your resources, keep them running only when you’re actually using them, and park them when you’re not. If you meet the threshold of 25% usage for the month, Google’s Sustained Use discounts will kick in, and further lower your cost from the list price. These two savings options combined will optimize your costs and provide the maximum savings.
Originally published at www.parkmycloud.com on May 3, 2018.
CEO of ParkMyCloud
See all (317)
CEO of ParkMyCloud
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@dzakyputra/hi-syed-thank-you-15a5d73158be?source=search_post---------398,"Sign in
There are currently no responses for this story.
Be the first to respond.
Dzaky Widya Putra
May 2, 2019·1 min read
Syed Meesam Ali
Hi Syed, thank you!
I personally choose Google Cloud Platform because I also use another Google’s services like BigQuery, Google Sheets, etc to easily integrating with them. I’ve been using it for almost a year now, you can run your bot in Google Compute Engine.
But if your program is not too heavy and doesn’t require massive data processing maybe you can consider AWS as an cheaper alternative :)
Software Engineering | Data Science
Software Engineering | Data Science
"
