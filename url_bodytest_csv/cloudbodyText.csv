story_url,bodyText
https://medium.com/@amirhermelin/im-leaving-google-and-here-s-the-real-deal-behind-google-cloud-1b86513be01b?source=search_post---------0,"Sign in
There are currently no responses for this story.
Be the first to respond.
Top highlight
Amir Hermelin
Oct 5, 2018·8 min read
Note: this is my first post on Medium — be gentle :). I’m staring at my badge that I’ll be turning in tomorrow, and decided I must share my thoughts before my next gig consumes 100% of my time. These thoughts are my own personal opinions, and do not reflect or represent Google’s opinions or plans.
Let’s start from the end: after almost 6.5 years, I’m leaving what’s IMO the best company in the world to work for. This was my longest stint at any one company. I’m leaving to pursue a high-risk high-reward opportunity with a company that’s disrupting personal finance.
I joined Google Cloud before it was even a platform, and I’m one of the PMs that has spent the longest tenure in Cloud. I’d like to share what I’ve seen over the years and what I expect to see in years to come.
But first, I’d like to tell you why leaving Google is so difficult.
Many people mention great perks and benefits, like free food and four month maternity leaves (unfortunately my 3 boys were born before I’ve joined Google — I only had a couple of days to bond with them!). Not to mention great salaries and a stock that’s rock solid. Young professionals might place a larger weight on these, but they get so much more “free” stuff that’s going to greatly impact their development and future careers. I wish on all my boys to start their careers at Google — because Google provides accelerated learning and development into the most important fields of tech.
1.Heaven for engineers and product managers: the engineering and PM levels at Google are very high. Don’t get me wrong — not everyone is a rockstar. But on average, you can rely on your colleagues to tackle tough problems together, and get hard sh*t done. This is because:
2. The best culture money can’t buy: the first weeks after I joined two things struck me:
In addition, the culture rewards excellence and innovation, encourages saying “thank you” in public, and promotes great ideas and efforts.
There’s a feeling that people are generally “good”, which why I was completely blindsided (like many others) by the infamous memo of last year. Regardless of where you stand in the conversation, Google treats its employees with unbelievable fairness, especially when compared to other tech firms. Trust me, I’ve been around the block.
3. Innovation and scale all wrapped into one: I crack up whenever I read that some people think Google isn’t innovative anymore. Firstly, the most important field in tech today — Machine Learning — is led by Google, which is several years ahead of its nearest competition. I don’t want to put other companies down — they’re also very innovative. But in the field of AI/ML, nobody comes close. Not in technology and not in the raw numbers of quality engineering talent. Now think of all the things Google applies ML to, like self driving cars, assistant, search, etc. If that’s not innovation — what is?
To find scale, all I need to do is look at the main apps I use throughout my day: Maps, Photos, Chrome, YouTube, Gmail, Search. AFAIK all of these are irreplaceable. Yes, there are alternatives* — but for me, switching means negative impact on my day-to-day.
*I don’t really think there’s an alternative to YouTube. It’s unique. I’ve also spent a short stint working on YouTube data infrastructure, and I can say that the org culture/vibe and the people are pretty amazing.
So no room for improvement? All is perfect in Google-Land? Of course not.
In some areas across Google the execution could’ve been better. Google is willing to bet big on success, and also assumes the risk of big failures. Messaging efforts of past didn’t take off like they should’ve. Google+ was a huge effort that didn’t succeed (but did give birth to the amazing Photos app). There are other examples. But the organization is a learning one, and I’m hopeful that future efforts will integrate these lessons learned.
Another hurdle is due to the (good) fact that there’s a lot of openness to ideas and opinions. Sometimes discussions and debates feel like they’re taking too long, and decisions need to be made sooner. Even if consensus wasn’t achieved or not all opinions were heard. Once we’re done understanding the data, and we’re only left with opinions — time to march on.
Lastly, promotion and perf are areas that finding the right balance is an ongoing process. Every cycle, complaints surface around promo fairness and perf overhead. There’s an entire team working on improving the process, but I feel there’s still work to do. To be fair, it’s tough to find balance in an 80k+ sized company. Google is no longer a startup.
Which brings me to Cloud.
Google Cloud — No Longer a Startup
My first PM task at Google was to launch Monarch — Google’s planet scale monitoring service for Google’s apps and services (Maps, Gmail, etc). Talk about the opposite of “easy early wins”! But with the help of others (see above “Heaven for..”) I managed to successfully launch it, and found my way into the cloud org circa early 2013.
Only back then, it really felt like a startup. We were pressed to find product market fit amidst fierce competitors that had years of head start (AWS) and armies of sales and marketing teams (Azure). And users still had questions of whether or not we were here to stay.
Not surprisingly we’ve found success with customers that were similar to Google. When I first engaged with Snapchat I believe they were less than 10 people, but the scale and automation they were looking for were not unlike what we knew in other parts of Google.
So we’ve made some mistakes. Two meaningful ones to be precise.
Our first — taking too long to recognize the potential of the enterprise. We were led by very smart engineering managers — that held tenures of 10+ years at Google, so that’s what grew their careers and that’s what they were familiar with. Seeing success with Snapchat and the likes, and lacking enough familiarity with the enterprise space, it was easy to focus away from “Large Orgs”. This included insufficient investments in marketing, sales, support, and solutions engineering, resulting in the aforementioned being inferior compared to the competitors’.
The second mistake was chasing the competition. For example, AWS was having great success with EC2 (VMs). And customers were asking for it on GCP. So our native internal way of running things — containers — was to take a backseat for a few years, until a small startup by the name of Docker managed to hype up containers enough to make them relevant. Google took notice, and the rest is history. Another example is App Engine — predating today’s “serverless” hotness by a few years, and arguably a successful business even back then. Neither AWS or Azure had anything like it, but we had to divert too many resources to satisfy customers that were asking for features similar to what our competitors offered at the time.
But all that is in the past. Over the last ~3 years, things have changed substantially. With the current leadership in place (grounded with the right experience), strong focus on enterprise, and marketing/sales/support teams that are correctly leveled and staffed, we’re left with the meat and potatoes: the products.
My friends ask me if I think Google Cloud will catch up to its rivals. Not only do I think so — I’m positive five years down the road it will surpass them. Because today, Cloud is about helping other companies build software like Google does. All those great things about working at Google? Making them available to other companies — that’s the product market fit.
Take Kubernetes as an example. It broke many adoption records and is very successful, and not because of any new concepts that were introduced. Kubernetes is really an externalization of container orchestration that’s been battle hardened inside of Google for many years (called Borg), and built to scale the largest, most distributed web services. And of the three cloud vendors, Google is best positioned to leverage these technologies and innovate first with (much) better offerings.
Security is another area. If I asked you which company had the fewest breaches, or where you feel your information is safest — what would your answer be? And so, it’s no wonder that Google Cloud leads and will continue to lead in this area. Externalizing security features and practices to our customers will prove of significant value — sometimes too valuable to pass for any other reason.
Machine Learning is the third strong pillar. Helping companies utilize advanced ML technologies the way Google does can give businesses an unfair advantage. And so, pretty soon every industry will have to deploy and use it, one way or another.
There are other things, like externalizing what we know about running production at scale (think monitoring, logging and SRE practices), CI/CD, externalizing previously internal services such as Spanner, BigQuery, and BigTable. The list goes on, and the not-yet-realized opportunity is huge.
Fast forward 5 years. What company wouldn’t want to build their service <X> with the same agility, scale, and security that Google does? Scale it to billions of users with minimal disruption and rock solid stability and reliability?
It’s almost midnight, and the post reads more like a love story than someone who’s leaving the company :) I’ll end here and not venture into my next chapter yet — I’ll leave that for my next post. For now I’ll just say that, starting next week my commute is about to get 3x longer ;)
Update: I’ve finally got around to writing about what’s exciting about the next chapter, here: https://medium.com/@amirh1/and-heres-why-i-m-joining-sofi-3bb71ded10c
📝 Read this story later in Journal.
👩‍💻 Wake up every Sunday morning to the week’s most noteworthy stories in Tech waiting in your inbox. Read the Noteworthy in Tech newsletter.
Father, husband, product person, mountain biker, ex-Google
See all (6)
23K 
112
23K claps
23K 
112
Father, husband, product person, mountain biker, ex-Google
About
Write
Help
Legal
Get the Medium app
"
https://blog.sketchapp.com/prototyping-libraries-on-sketch-cloud-and-an-official-ios-ui-kit-in-sketch-49-bf090c70796c?source=search_post---------1,"There are no upcoming events.
Want to host your own event? Get in touch.

        The latest update introduces Prototyping, the ability to subscribe to Shared Libraries in Sketch, and a built-in Apple UI Design Resource.
      
Sketch 49 has arrived and, hot on the heels of our Libraries update, we’re adding yet another huge and highly anticipated feature to Sketch. We understand design, so we know that you can’t always tell if something works based on static screens alone. Sometimes you need to see the entire flow in action and the best way to do this is to turn your designs into interactive prototypes. With our latest update, we’re making that whole process seamless — say hello to Prototyping in Sketch.
That’s not all though. Alongside Prototyping we’re adding new ways to share Libraries via Sketch Cloud and the official iOS 11 Apple UI Design Resource is now built right into Sketch as a Shared Library. Here’s an overview of the headline features in Sketch 49:
In Sketch 49, we’re introducing Prototyping, letting you transform your designs into interactive prototypes and preview them, without ever leaving the app.
This new set of tools allows you to connect your Artboards, apply transitions, and then preview your designs right inside Sketch, on your mobile devices with Mirror, or on Sketch Cloud, where you can share your prototypes with colleagues, clients and the world.
To turn your static designs into clickable prototypes, simply select a layer and add a Link to an Artboard. Once you have two Artboards connected, you can add a simple animation for a smooth transition from one state to the next. Rinse and repeat across your whole project and you can quickly and easily build a lightweight, working prototype of your app or website — perfect if you’re working on a project and need to share a quick demo or you want to check the usability of a specific workflow.
To view your Prototype in action just click the Preview button in the toolbar and your prototype will launch in a dedicated window, allowing you to interact with your app or web design as if it were the real thing.
Prototyping makes presenting your work and getting sign-off on projects a whole lot easier. If you want to share a working prototype with a developer, colleague or client, just upload to Sketch Cloud, send them the link and they can interact with and comment on your designs, right in the browser. You can even create Start Points that let you launch your preview at a specific Artboard, so they know exactly where to begin.
If you’re working on designs for iPhone or iPad, we’ve now added Prototyping support to Sketch Mirror for iOS, so you can preview your prototypes on the screens they were designed for. Don’t forget to download the new Sketch Mirror update to try out your Prototypes there.
With Prototyping built right into Sketch, it’s now easier than ever to take your designs from ideation through to realization and to share your concepts with the people that matter.
You can find out more about how to make the most of Prototyping in our documentation.
In Sketch 47 we introduced Libraries and they’ve empowered you and your teams to work better, together. In Sketch 49 we’re building on this foundation and bringing you a new way to share and access Libraries.
With Shared Libraries, you can now download, and subscribe to, Libraries that have been uploaded to Sketch Cloud and, because all of this is built on open web technologies, these Libraries can be stored and accessed from anywhere on the web. We’re excited to say that we’ve taken advantage of this feature and teamed up with Apple to build their official iOS 11 UI kit right into Sketch, as a Shared Library.
With Sketch Cloud, you can already upload a Document and share your designs with colleagues or clients, all from inside the app. With the latest update, you can now subscribe to Documents uploaded to Sketch Cloud and they’ll be added directly to Sketch, as Shared Libraries.
When you subscribe to a Shared Library, your Library file will remain linked to the original Sketch Cloud Document so, if the creator uploads a new version, you’ll get a notification and you can choose to update your local Library as well. This is really useful if you’re working with a team and need a single source of truth for your design system or style guide. Working on a free UI Kit or another cool project you want to shout about? Sketch Cloud is now the easiest place to share your Library with the community.
If you want to allow your Documents to be added as Shared Libraries, simply select the Allow others to download this Document option when you upload your file to Sketch Cloud.
If you want to add a Document from Sketch Cloud as a Shared Library, just click Download › Add Library to Sketch and the Document will be added to the Libraries tab in Preferences, and you’ll be notified if the original Document is updated.
Apple’s design templates have always been the gold standard when it comes to iOS design resources and the obvious go-to when designing for iPhone or iPad.
We’re really excited to say that we’ve integrated Apple’s iOS 11 design template into Sketch, and can now offer this invaluable resource as a built-in Library.
The Apple iOS UI library has all of the components you’ll need to start working on your next project, carefully crafted for Sketch. This expansive Library includes everything, from tab bars and status bars to buttons and switches, ready to be inserted into your latest designs.
You can download the Apple iOS UI Library from the Libraries tab in Preferences and, when Apple updates their Document, you’ll be notified and can update your designs to the latest version.
As always we’ve been listening to your feedback and made some other, smaller improvements and bug fixes since Sketch 48. Here are some of the highlights:
You can find a full list of bug fixes and improvements on our updates page
Sketch 49 is a free update for everyone with an active license. If you need to renew your license, you’ll get Sketch 49 and a whole year’s worth of updates after that.
We always love to see what you’re creating with Sketch so if you’ve made an awesome Library and uploaded it to Sketch Cloud, let us know — we might even feature it online or in our newsletter. If you’ve got questions or feedback, you can get in touch with us via our support page or join in the conversation on Twitter, or on our Facebook group.
We’re already working on Sketch 50 and we’ll tell you all about it soon.
Turn your ideas into incredible products with a 30-day trial.
A monthly digest of the latest Sketch news, articles, and resources.
©
        2022
        Sketch B.V.
"
https://medium.com/productivity-in-the-cloud/6-links-that-will-show-you-what-google-knows-about-you-f39b8af9decc?source=search_post---------2,"There are currently no responses for this story.
Be the first to respond.
(Photo by Alex Koloskov at www.photigy.com )
Want to find out all the things Google knows about you? Here are 6 links that will show you some of the data Google has about you.
In order to serve relevant ads, Google collects data about you and creates a profile. You can control and review the information Google has on you here:
http://www.google.com/settings/ads/
Google also has a tool called Google Analytics, that helps publishers see what pages you have viewed on their website, how many times you have visited it, how long did you stay etc. You can opt out if you don’t want this type of data to be collected:
http://tools.google.com/dlpage/gaoptout
If you use Android, your mobile device may be sending your location to Google. You can see your entire location history here:
https://maps.google.com/locationhistory
Google saves every single search you have ever done. On top of that, they record every Google ad you have clicked on. This log is available in Google web history controls:
https://www.google.com/history/
Google offers an Account activity page that tells you about all the Google services you are using. You can even enable a monthly report that will be sent to your email:
https://www.google.com/settings/dashboard
The Account activity page also offers a list of all the apps that have any type of access to your data. You can see the exact type of permissions granted to the app and revoke access to your data here:
https://security.google.com/settings/security/permissions
Google lets you export all your data: bookmarks, emails, contacts, drive files, profile info, your youtube videos, photos and more here:
https://www.google.com/takeout
Google also keeps a history of your YouTube searches. You can find it here:
https://www.youtube.com/feed/history/search_history
Source: http://www.google.com/goodtoknow/online-safety/security-tools/
Brought to you by the team behind Cloud Fender. Originally published at blog.cloudfender.com.
A collection of tips & tricks about productivity in the…
2.8K 
24
2.8K claps
2.8K 
24
Written by
Keep all your files in one place. Sync Dropbox, Google Drive, Box & more to the cloud service you love at www.cloudfender.com
A collection of tips & tricks about productivity in the cloud from all over Mediu
Written by
Keep all your files in one place. Sync Dropbox, Google Drive, Box & more to the cloud service you love at www.cloudfender.com
A collection of tips & tricks about productivity in the cloud from all over Mediu
"
https://itnext.io/the-cloud-skills-shortage-and-the-unemployed-army-of-the-certified-bd405784cef1?source=search_post---------3,"Why it’s so hard to find roles in cloud technology, while jobs go unfilled.
In the last year I’ve met dozens of people who have taken many of the cloud certifications available yet been unable to land jobs in this space. Many have long resumes with impressive backgrounds, others have switched careers from science or math, while some are recent graduates trying to get into the newest area of IT. None of the people I met was obviously unqualified.
If you talk to recruiters or Human Resources, you’ll hear that AWS certifications are the hottest words on any technical resume . “There aren’t enough bodies to put in seats!” they say. Then if you turn around and talk to the crowds of the freshly-certified jobless — “Nobody’s responding to our job applications!” the chorus goes. You wonder, are these two groups living in the same universe? What’s going on?
Typically we have shortening 5-year cycles in tech that cause clamoring for new skills. First it was the internet (“HTML! Perl! PHP!”) then mobile apps (“Android! Objective C! React Native! Flutter!”) and now cloud (“AWS Certification! DevOps! SecOps!”). IoT and machine learning are probably next. The technology itself doesn’t really matter — companies just want an army of tactical workers to build this stuff tirelessly.
Back in the mobile apps craze, you could find a job with minimal experience — truly, it was the wild west. But in this cycle, companies aren’t throwing the offers at the newly-certified or ambitious. This is a very different space where the actual number of new cloud jobs created is going to be surprisingly small, a byproduct of the automation and commoditization of IT.
Each new technology has its S-curve and early adopters tend to overestimate how far we’ve progressed. In the cloud arena, adoption rates are nowhere near as high as many believe — in charting industry users against growth, I estimate it looks like this:
Right now in cloud, the real jobs are available at large companies who have either been early adopters or have new projects that were started in the cloud while the rest of their infrastructure is not. Anyone in their immediate orbit will see roles appearing in the cloud too — principally consulting firms and support vendors. These are where the best cloud jobs exist in 2019.
There are only a few mid-size companies in the cloud meaningfully at this point, though you will see many jobs posted. Some of these firms realize they need to renovate their IT but they attempt to do this through hiring “Digital Transformation” roles which do little but frustrate the revolving door of hires because they have no executive buy-in and budget. They also have entrenched IT that will not support these upstart attempts and the organizations need new personnel in IT leadership before anything significant can happen.
I realize there are many exceptions to these broad statements but generally I think the placement of cloud-adoption on the curve is fairly accurate, and we have yet to see a huge explosion in the demand for cloud talent. A little longer, when the mid-caps get into this space and the adoption curve matures, cloud skills will become mainstream requirements for IT staff.
But that’s not the whole story, and doesn’t account for why the newly trained and qualified aren’t getting into positions, and why corporate HR is claiming nobody is out there with the required skills. There’s another more important factor.
When mobile apps appeared, the new skillsets had no impact on the existing IT environment. Sure, the UI/UX industry blossomed and there was movement around the development space, but you were unaffected if you worked in databases, security, server administration or the myriad other roles than keep the lights on at corporations.
With cloud, this is entirely different, and the language used in job descriptions is a clue to what’s happening. We’re seeing an aggregation of jobs hiding behind cool-sounding new buzzwords. Who doesn’t want to be a 10x coder or a Full-Stack Developer? Roles that have been separate for years — database admin, server admin, quality engineer, software developer — are being smashed together because the headcount is vanishing. These roles are asking you to do the work of several people but using your attraction to intellectual challenges to make it sound more appealing.
It’s noticeable right now because automation and cloud have hollowed out the core of once-venerable professions. Database admin is hard and almost redundant now with RDS; server admin is very hard, and mostly gone with virtualization. QA/QE has been gutted by TDD, CI/CD and automation which effectively stick the developer with the work. The jobs opening up include a requirement to know all these skills in addition to everything else.
In larger companies that are adopting cloud, generally they are taking the existing back-end staff and retraining them as cloud architects or scalability engineers. And this works well because the back-end people have a perfect skillset to learn about scalability, redundancy and availability techniques. It also avoids layoffs and they have employees already familiar with the systems and culture.
The NoOps movement gives the impression that all these server admins are going to be out on the street fairly soon. But the reality is that the cloud will be very good for these groups, and their roles are becoming more generalized and coming towards the front-end. It’s fundamentally much easier to take an experienced back-end engineer and bring her to the front-end than vice versa. And this will be extremely obvious as serverless gains traction, since serverless is basically programming for infrastructure engineers.
The key thing here is that businesses are not hiring fresh-out-of-certification AWS experts to take over the infrastructure of their established operations teams. They want people with solid experience, a risk-free bet that the new hire can execute the tech flawlessly. This often means insane job requirements (I’ve seen “10 years’ serverless” as a hard must-have recently) and an existing track-record of success.
These companies have no interest in paying people who have learned, studied or recently certified unless it’s the icing on the cake of many years’ real-world success. This is probably no surprise in retrospect but it does mean you have to question the value of telling hopefuls that enormous opportunity exists unless they have this existing infrastructure experience.
For people without this background, the best AWS jobs available right now are in sales, account management and roles that aren’t directly technical. This is especially true in the vendor ecosystem, where they are less interested in a previous life in the backend.
If you are looking for business transformation, your best options are startups or consulting firms. Both come with their own set of problems but are the only places where you will be allowed to be a revolutionary. These roles don’t exist in larger, established companies, except to a privileged few who have already served their time working through the middle management hierarchy.
If you are a coding superstar, the sort of developer who soaks up new languages and frameworks for breakfast and has the capacity to put cloud skills under your belt, you’re already working for a FAANG or soon will be. The very small number of engineers who don’t use Stack Overflow twenty times a day and can remember the differences between a stack and a heap, they are being sniffed out by the big tech firms in an unstoppable hiring frenzy. This will be an extinct breed in regular companies very soon.
Finally, there’s going solo — the nuclear option. One respected tech thought leader recently Tweeted that there’s a “Huge arbitrage opportunity for Cloud DevOps people to go and make these deals to companies needing to make cuts in expense.” On the surface, this seems right — cloud professionals are the magicians in technology right now, and can seemingly take advantage of automation to slash expenses, leading to riches.
But this is the consulting trap, and it’s alluring to people who are smart, qualified and want to get out of their full-time employment rut to drive change independently. It doesn’t work usually thanks to the difficulty of finding willing customers, and I can tell you this from personal experience and the sea of talented tech people who have tried and failed at the same thing.
For most in IT, the paradox of the cloud is going to heavily influence our roles going forward. You will have to keep learning new skills at a much faster rate than peers in other industries, but you’re running up a down escalator that’s speeding up. The rate at which cloud is gobbling up the entire ecosystem creates opportunities for the experienced but ultimately will make it much harder for those switching careers or entering at a junior level.
ITNEXT is a platform for IT developers & software engineers…
5.8K 
48
Thanks to Kiarash Irandoust. 
5.8K claps
5.8K 
48
Written by
Developer advocate. Opinions here are my own. See my other blogs at http://bit.ly/jbeswick.
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
Written by
Developer advocate. Opinions here are my own. See my other blogs at http://bit.ly/jbeswick.
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
"
https://medium.com/swlh/a-design-analysis-of-cloud-based-microservices-architecture-at-netflix-98836b2da45f?source=search_post---------4,"There are currently no responses for this story.
Be the first to respond.
Netflix has been among the best online subscription-based video streaming services in the world ([12]) for many years, accounting for over 15% of the world’s Internet bandwidth capacity. In 2019, Netflix already acquired over 167 million subscribers, with…
"
https://medium.com/@serverpunch/why-you-should-not-use-google-cloud-75ea2aec00de?source=search_post---------5,"Sign in
There are currently no responses for this story.
Be the first to respond.
Top highlight
Punch a Server
Jun 29, 2018·4 min read
Update (18-July-2018): GCP has updated their account management policies to be more friendlier to GCP customers. https://cloudplatform.googleblog.com/2018/07/improving-our-account-management-policies-to-better-support-customers.html
UPDATE (2-July-2018): Thanks to the people from GCP support team who have reached out and assured us these incidents will not repeat. Here’s a direct message from them … “there is a large group of folk (within GCP) interested in making things better, not just for you but for all GCP customers.”
Follow discussions here. HACKERNEWS: https://news.ycombinator.com/item?id=17431609REDDIT: https://www.reddit.com/r/programming/comments/8v4wrh/why_you_should_not_use_google_cloud_this_is_about/
Note: This post is not about the quality of Google Cloud products. They are excellent, on par with AWS. This is about the “no-warnings-given, abrupt way” they pull the plug on your entire systems if they (or the machines) believe something is wrong. This is the second time this has happened to us.
We have a project running in production on Google Cloud (GCP) that is used to monitor hundreds of wind turbines and scores of solar plants scattered across 8 countries. We have control centers with wall-to-wall screens with dashboards full of metrics that are monitored 24/7. Asset Managers use this system to monitor the health of individual wind turbines and solar strings in real time and take immediate corrective maintenance. Development and Forecasting teams use the system to run algorithms on data in BigQuery. All these actions translate directly to revenue. We deal in a ‘wind/solar energy’ — a perishable commodity. If we over produce, we cannot store and sell later. If we under produce, there are penalties to be paid. For this reason assets need to be monitored 24/7 to keep up/down with the needs of the power grid and the power purchase agreements made.
Early today morning (28 June 2018) i receive an alert from Uptime Robot telling me my entire site is down. I receive a barrage of emails from Google saying there is some ‘potential suspicious activity’ and all my systems have been turned off. EVERYTHING IS OFF. THE MACHINE HAS PULLED THE PLUG WITH NO WARNING. The site is down, app engine, databases are unreachable, multiple Firebases say i’ve been downgraded and therefore exceeded limits.
Customer service chat is off. There’s no phone to call. I have an email asking me to fill in a form and upload a picture of the credit card and a government issued photo id of the card holder. Great, let’s wake up the CFO who happens to be the card holder.
“We will delete your project unless the billing owner corrects the violation by filling out the Account Verification Form within three business days. This form verifies your identity and ownership of the payment instrument. Failure to provide the requested documents may result in permanent account closure.”
What if the card holder is on leave and is unreachable for three days? We would have lost everything — years of work — millions of dollars in lost revenue.
I fill in the form with the details and thankfully within 20 minutes all the services started coming alive. The first time this happened, we were down for a few hours. In all we lost everything for about an hour. An automated email arrives apologizing for ‘inconvenience’ caused. Unfortunately The Machine has no understanding of the ‘quantum of inconvenience’ caused.
I understand Google’s need to monitor and prevent suspicious activity. But how you handle things after some suspicious activity is detected matters a lot. You need a human element here — one that cannot be replaced by any amount of code/AI. You just can’t turn things off and then ask for an explanation. Do it the other way round.
This is the first project we built entirely on the Google Cloud. All our previous works were built on AWS. In our experience AWS handles billing issues in a much more humane way. They warn you about suspicious activity and give you time to explain and sort things out. They don’t kick you down the stairs.
I hope GCP team is listening and changes things for better. Until then i’m never building any project on GCP.
12.2K 
49
12.2K claps
12.2K 
49
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@yesdeepakverma/how-i-cleared-all-3-google-cloud-certifications-in-3-weeks-f5591aa22572?source=search_post---------6,"Sign in
There are currently no responses for this story.
Be the first to respond.
Deepak Verma
Dec 11, 2018·6 min read
Yes, you read it right!
I have got my 4th Certificate Google Cloud Developer and 5th Certificate Google Cloud Network Engineer each in 1 week again.
I was able to clear all 3 Google Cloud certifications in 3 weeks. In this post, I will be sharing the resources and my approach to help you prepare for GCP certifications.
A bit about me:
I am a Python+Django developer with hands-on experience of Python-based scripting and Google Cloud Platform working with MediaAgility. I have worked on designing and developing projects to production scale from scratch. At MediaAgility, I am in the Machine Learning team. Since my organization is a Google Cloud Premier Partner, I have also worked on GCP migrations — setting production grade networking, Kubernetes, and GCP infra automation (IAM, networking, Kubernetes, Dataproc, VM, and more).
The prerequisites…
Google Cloud certifications need a right balance of both theory and practical skills. For the theory, Google Cloud documentation is a great resource and to increase your practical skills, you will have to practice frequently and extensively. You can use free $300 credit from GCP and try different services or join Coursera Google Cloud Architect or Data Engineer course and watch all the videos and complete all the Labs.
I started with Coursera’s Architecting with Google Cloud Certification. This exam checks your ability to provide a solution on the Google Cloud. It is almost 80% conceptual and 20% practical. You must know beforehand -
I watched all the videos in just 2–3 days and skipped the concepts that I knew already owing to my experience on Google Cloud. Sharing the important topics below -
This certification tests you for the ability to deploy a solution on Google Cloud. You must have a strong practical experience with Google Cloud — working knowledge of GCloud SDK and Google cloud console.
More details about the exam can be found here.
You must be able to perform the below actions -
This certification tests your ability to design big data solutions on GCP. This exam expects that you are familiar with the big data products (storage, processing, display) and their open source alternatives as well. This is required because some of the questions expect you to answer GCP alternatives for open source big data products.
BigQuery is the product that you must understand clearly. If you understand BigQuery, you can answer 40% of this exam.
Sharing an overview of other important topics and a few more resources:
My Schedule -
I studied from 10 p.m. to 2 a.m during those 3 weeks of my preparation. This needed me to take care of my health so that I remain focused. So, I would suggest you maintain a healthy diet and not let the stress get the better of you!
If you thoroughly understand the ‘How-to’ and ‘Concepts‘ sections of Google Cloud documentation, you easily have 70% of what it takes to clear GCP certification; remaining 30% is your practice, experience, and your state-of-mind during exams. So, take the exam with a relaxed state of mind.
All The Best!
Here are the links to my certificates.
Professional Data Engineer, Associate Cloud Engineer, Professional Cloud Architect
P.S. Don’t hesitate to click on clap button as many time as you can. :-)
Data Analytics | Kubernetes | Cloud Architect | Data Architect | Python
7K 
24
7K 
7K 
24
Data Analytics | Kubernetes | Cloud Architect | Data Architect | Python
"
https://medium.com/@steve-yegge/dear-google-cloud-your-deprecation-policy-is-killing-you-ee7525dc05dc?source=search_post---------7,"Sign in
There are currently no responses for this story.
Be the first to respond.
Steve Yegge
Aug 15, 2020·23 min read
God dammit, I didn’t want to blog again. I have so much stuff to do. Blogging takes time and energy and creativity that I could be putting to good use: my novels, my music, my game, and so on. But you get me riled enough, and I have to blog.
Let’s get this over with, then.
I’ll begin with a small but enlightening story from my early days at Google. For the record, I know I’ve said some perhaps unkind things about Google lately, because it’s frustrating when your corporate alma mater makes incompetent business decisions on the regular. But Google’s internal infrastructure is truly extraordinary, and you could argue that there is still none better today. The people who built Google were far better engineers than I will ever be, as this anecdote should serve to illustrate.
First a wee bit of background: Google has a storage technology called Bigtable. Bigtable was a remarkable technical achievement, being one of the first (if not the first) “infinitely scalable” key-value stores: the beginning of NoSQL, basically. These days Bigtable still holds up well in the rather crowded space of K/V stores, but back in the day (2005) it was breathtakingly cool.
One fun bit of trivia about Bigtable is that they had these internal control-plane entities (as part of the implementation) called tablet servers, which had large indexes, and at some point they became a scaling bottleneck. So the Bigtable engineers scratched their heads over how to make it scale, and realized that they could replace the tablet servers with Bigtables to unblock the scaling. So Bigtable is part of its own implementation. It’s Bigtables all the way down.
Another cool bit of trivia is that for a time, Bigtables became popular and ubiquitous inside Google and everyone and their dog had one. So at one Friday’s TGIF all-hands, Larry Page casually asked in passing, “Why do we have more than one Bigtable? Why isn’t there just one?” Because in theory, one Bigtable should have sufficed for all Google’s storage needs. Of course they never did migrate to just one, for practical software engineering reasons (e.g. blast radius), but the theory was interesting. One database for the whole universe. (Side note: Anyone know if Sable does this at Amazon?)
Anyway, here’s my story, to get us started on my rant.
One day, after I’d been working at Google for just over 2 years, I got an email from the Bigtable engineering team. It said something along the lines of:
Dear Steve,
Greetings from the Bigtable team. We wanted to let you know that you are running a very, very old Bigtable binary in the [some data center name] data center. That version is no longer supported, and we would like to work with you to help you upgrade to the latest version.
Please let us know if you can schedule some time to work with us on this.
Best,
Bigtable Team
You get a lot of email at Google, as you can imagine, and when I glanced at this one, this is what I first perceived it to be saying:
Dear RECIPIENT,
Greetings from Some Team. We wanted to let you know blah blah blah blah blah blah blah. Blah blah blah blah blah blah blah BLAH, and blah blah blah blah immediately.
Please let us know if you can schedule some of your precious time to blah blah blah.
Best,
Some Team
I almost deleted it on the spot, but there was this lingering, nagging feeling that it didn’t quite feel like a form letter, even though it obviously wasn’t for me, since I didn’t have a Bigtable.
But it was weird.
For the rest of the day, as I was alternating between working and deciding which species of gummy shark to try next in the micro-kitchens, of which there were at least three close enough to hit from my seat with a well-aimed biscuit, I thought about that email with a growing sense of mild anxiety.
They had used my name specifically. And the email had been sent to my email address and nobody else’s, and not by cc: or bcc:. The tone was very personal and pointed. Maybe it was some sort of mistake?
Curiosity finally got the better of me, and I went to look at my Borg console in the data center they’d mentioned in the email.
And sure enough, I was running a Bigtable there. Whaaaaat? I looked at its contents, and lo! It was from the Bigtable codelab I’d run back in my first week as a Noogler, in June 2005. The codelab had you fire up a Bigtable so you could programmatically write some values to it, and I had apparently never shut it down afterwards. It was still running there, over 2 years later.
There are several remarkable aspects to this story. One is that running a Bigtable was so inconsequential to Google’s scale that it took 2 years before anyone even noticed it, and even then, only because the version was old. As a point of comparison, I considered using Google Cloud Bigtable for my online game, but it cost (at the time) an estimated $16,000/year for an empty Bigtable on GCP. I’m not saying they’re gouging you, but in my own personal opinion, that feels like a lot of money for an empty fucking database.
Another remarkable aspect is that it was still running after 2 years. WTF? Data centers come and go; they experience outages, they undergo routine maintenance, they change all the time. Hardware gets upgraded, network switches swap out, everything is constantly being improved. How the heck were they able to keep my software running for 2 years in the face of all that change? It may feel like a humdrum achievement in 2020, but in 2005–2007 it was pretty impressive.
And then there is in my opinion the most remarkable aspect of all, which is that an unrelated engineering team in some other state was reaching out to me, the owner of some tiny mostly-empty Bigtable instance which had had zero traffic for the past 2 years, asking if they could help me upgrade.
I thanked them and shut it down and life went on. But I still think about that letter, thirteen years later. Because I sometimes get similar letters from the Google Cloud Platform. They look like this:
Dear Google Cloud Platform User,
We are writing to remind you that we are sunsetting [Important Service you are using] as of August 2020, after which you will not be able to perform any updates or upgrades on your instances. We encourage you to upgrade to the latest version, which is in Beta, has no documentation, no migration path, and which we have kindly deprecated in advance for you.
We are committed to ensuring that all developers of Google Cloud Platform are minimally disrupted by this change.
Besties Forever,
Google Cloud Platform
But I barely skim them, because what they are really saying is:
Dear RECIPIENT,
Fuck yooooouuuuuuuu. Fuck you, fuck you, Fuck You. Drop whatever you are doing because it’s not important. What is important is OUR time. It’s costing us time and money to support our shit, and we’re tired of it, so we’re not going to support it anymore. So drop your fucking plans and go start digging through our shitty documentation, begging for scraps on forums, and oh by the way, our new shit is COMPLETELY different from the old shit, because well, we fucked that design up pretty bad, heh, but hey, that’s YOUR problem, not our problem.
We remain committed as always to ensuring everything you write will be unusable within 1 year.
Please go fuck yourself,
Google Cloud Platform
And the thing is, I get these about once a month. It happens so often and so reliably that I have been inexorably pushed away from GCP, towards cloud agnosticism. I no longer take dependencies on their proprietary service offerings, because it actually winds up being less DevOps work, on average, to support open-source systems running on bare VMs, than to try to keep up with Google’s deprecation treadmill.
Before I return to shitting on Google Cloud Platform, because boyo I am nowhere near finished yet, let’s go visit how software engineering works in some other domains. Google engineers pride themselves on their software engineering discipline, and that’s actually what gets them into trouble. Pride is a trap for the unwary, and it has ensnared many a Google team into thinking that their decisions are always right, and that correctness (by some vague fuzzy definition) is more important than customer focus.
I’m going to pick a few somewhat arbitrary examples from other big software systems, but hopefully you’ll be able to start spotting the pattern everywhere; that pattern being: Backwards compatibility keeps systems alive and relevant for decades.
Backwards compatibility is a design goal of all successful systems that are designed for open use; that is, implemented as open source, and/or guided by open standards. I feel like I’m stating something that’s so obvious that we should all be awkwardly embarrassed, but no. It’s a political issue, so I need examples.
The first system I’ll pick is the oldest: GNU Emacs, which is a sort of hybrid between Windows Notepad, a monolithic-kernel operating system, and the International Space Station. It’s a bit tricky to explain, but in a nutshell, Emacs is a platform written in 1976 (yes, almost half a century ago) for writing software to make you more productive, masquerading as a text editor.
I use Emacs every single day. Yes, I’m also using IntelliJ every day, and that has grown into a powerful tooling platform in its own right. But writing software extensions for IntelliJ is a much more ambitious and complex undertaking than writing extensions for Emacs. And more importantly, stuff lasts forever on Emacs.
I’m still using software that I wrote for Emacs back in 1995. And I’m sure there are people who are still using software they wrote for Emacs from the mid-80s, if not earlier. Every once in a while it might require a minor tweak, but it’s really quite rare. I’m not aware of anything I’ve ever written for Emacs (and I’ve written a lot) that was ever forced into re-architecture.
Emacs does have a deprecation facility called make-obsolete. Emacs’ terminology for fundamental software engineering concepts (like what is a “window”) are often different from the industry conventions, because Emacs invented them long, long ago. The perils of being before your time: Your names are all wrong. But Emacs does indeed have deprecation, called obsolescence in their lingo.
However, the Emacs folks seem to have a different working definition. A different underlying philosophy, if you will.
In the Emacs world (and in many other domains, some of which we’ll explore below), when they make an API obsolete, they are basically saying: “You really shouldn’t use this approach, because even though it works, it suffers from various deficiencies which we enumerate here. But in the end it’s your call.”
Whereas in the Google world, deprecation means: “We are breaking our commitments to you.” It really does. That’s what it ultimately means. It means they are going to force you to do some work, possibly a large amount of rework, on a regular basis, as punishment for doing what they told you to do originally — as punishment for listening to their glossy marketing on their website: Better software. Faster! You do everything they tell you to do, and you launch your application or service, and then, bang, a year or two later it breaks down.
This is like selling you a used car that they know is going to break down in under 1000 miles.
These are two very, very different philosophical definitions of “deprecation”. Google’s definition reeks of planned obsolescence. I don’t believe that it’s actually planned obsolescence in the same sense that, say, Apple perpetrates. But Google definitely plans to break your stuff, in a roundabout way. I know because I worked there as a software engineer for 12+ years. They have loose internal guidelines about how much backwards compatibility to bake into service offerings, but in the end it’s up to each individual team or service. There are no corporate-level or engineering-level guidelines, and the longest anyone has the courage to recommend, in terms of deprecation cycles, is “try to give your customers 6–12 months to upgrade before you drape them over the barrel.”
This is hurting them far more than they realize, and it will continue to hurt them for years to come, because it’s not part of their DNA to care about customers. More on this below.
For the moment, I’m going to make the bold assertion that Emacs is successful in large part, perhaps even mostly, because they take backwards compatibility so seriously. In fact, that’s the thesis of this essay. Successful long-lived open systems owe their success to building decades-long micro-communities around extensions/plugins, also known as a marketplace. I’ve ranted about Platforms before, and how important they are, and how Google has never once in their entire corporate history ever really understood what goes into making a successful open Platform, not counting Android or Chrome.
Actually I should talk about Android briefly, because you’re probably thinking, hey, what about Android?
First, Android is not Google. They have almost nothing to do with each other. Android is a company that was purchased by Google in July 2005, and that company has been allowed to run more or less autonomously, and in fact has remained largely intact through the intervening years. Android is an infamously hairy tech stack, and a just-as-infamously prickly organization. As one Googler put it, “One does not simply walk into Android.”
I’ve done my share of ranting about how bad some of Android’s early design decisions have been. Heck, at the time I was doing that ranting, they were busy rolling out shit like Instant Apps, which is now (surprise!) deprecated, and haha on you if you were dumb enough to listen to them when they told you to port all your stuff to Instant Apps.
But there’s a difference here, a material difference, which is that the Android folks actually DO understand how important Platforms are, they go well out of their way to prevent breaking your old Android code. In fact, their efforts to keep backward compatibility are so extreme that even I, during my brief stint in Android-land a few years back, found myself trying to convince them to drop support for some of the oldest devices and APIs. (I was wrong, as I’ve been about many other things past and present. Sorry Android folks! Now that I’ve visited Indonesia, I see why we need them.)
The Android folks take backwards compatibility to almost unimaginable extremes, which piles on massive amounts of legacy technical debt in their systems and toolchains. Oh boy, you should see some of the crazy stuff they have to do in their build system, all in the name of compatibility.
For this, I award Android the coveted You’re Not Google award. You really don’t want to be Google. They don’t know how to build platforms that can last, whereas Android does know how to do it. And so Google is being very wise in one respect: letting the Android folks do things their way.
Instant Apps was a pretty dumb idea, though. You know why? Because it required you to rewrite and re-architect your application! As if people are just going to up and rewrite 2 million apps. I’m guessing Instant Apps was probably a Googler’s idea.
But you see the difference here. Backwards compatibility comes with a steep cost, and Android has chosen to bear the burden of that cost, whereas Google insists that you, the paying customer, bear that burden.
You can see Android’s commitment to backwards compatibility in their APIs. It’s a sure sign, when there are four or five different coexisting subsystems for doing literally the same thing, that underlying it all is a commitment to backwards compatibility. Which in the Platforms world, is synonymous with commitment to your customers, and to your marketplace.
Google’s pride in their software engineering hygiene is what gets them into trouble here. They don’t like it when there are lots of different ways to do the same thing, with older, less-desirable ways sitting alongside newer fancier ways. It increases the learning curve for newcomers to the system, it increases the burden of supporting the legacy APIs, it slows down new feature velocity, and the worst sin of all: it’s ugly. Google is like Lady Ascot in Tim Burton’s Alice in Wonderland:
Lady Ascot: Alice, do you know what I fear most?
Alice Kingsley: The decline of the aristocracy?
Lady Ascot: Ugly grandchildren.
To explore the tradeoff of Pretty vs Practical, let’s take a peek at a third successful platform (after Emacs and Android) and see how it fares: Java itself.
Java has tons of deprecated APIs. Deprecation is super popular with Java programmers, more so than for most programming languages. Java itself, the core language and libraries, deprecates APIs all the time.
To take just one of thousands of examples, killing threads is deprecated. It’s been deprecated since Java 1.2, released in December 1998. It’s been 22 years since they deprecated it.
My live production code still kills threads every day. Is that a good thing? Absolutely! I mean, obviously if I were to rewrite the code today I’d do it differently. But my game code, which has been able to make hundreds of thousands of people happy over the past 2 decades, was written to kill worker threads that take too long, and I’ve never had to change it. I know my system best, and I have literally 25 years of experience with running it in production, and I can tell you: In my use case, killing these particular worker threads is harmless. It is not worth it to focus my time and energy on rewriting that code, and praise be unto Larry Ellison (I guess), since Oracle has never made me rewrite it.
I guess Oracle understands Platforms too. Go figure.
You can see evidence all through Java’s core APIs of waves of deprecation, like glacier lines in a canyon. There are easily five or six different keyboard focus managers in Java Swing. In fact it’s hard to find a Java API that isn’t deprecated. But they all still work! I think the only time the Java core team will actually remove an API is if it causes a blatant security problem.
Here’s the thing, folks: We software developers are all super busy, and we are also faced with competing alternatives in every software domain. At any given time, programmers in language X are looking at language Y as a possible replacement. Oh, you don’t believe me? What about Swift? Everyone’s migrating *to* Swift, not away from it, right? Oho, how little you know. Businesses are taking a mercenary’s accounting of their dual mobile teams (iOS and Android), and starting to realize that those phony-sounding dog-and-pony-show cross-platform development systems like Flutter and React Native have real teeth, and using them could cut their mobile team sizes in half, or alternately, make them twice as productive. There’s real money at stake here. Yes, there are trade-offs, but on the other hand, mooooooooney.
Let’s say hypothetically that Apple was dumb enough to pull a Guido van Rossum, and declare that Swift 6.0 is backwards-incompatible with Swift 5.0, much in the way that Python 3 is incompatible with Python 2.
I probably told this story ten years ago, but about fifteen years ago I was at O’Reilly’s Foo Camp with Guido, sitting in a tent with Paul Graham and a bunch of other at-the-time mucky-mucks, waiting for Larry Page to fly out in his personal helicopter, and Guido was droning on tonelessly in the sweltering heat about “Python 3000”, which he had named in honor of the number of years it would take everyone to migrate to it. We kept asking him why he was breaking compatibility, and he’d answer, “Unicode”. And we’d ask him, hey, if we have to rewrite our code, then what other benefits are we going to see? And he’d answer, “Yoooooooooooooouuuuuuuniiiiiiicoooooooode”.
If you install the Google Cloud Platform “gcloud” SDK, you’ll get this notice:
Dear RECIPIENT,
We would like to remind you that support for Python 2 is deprecated, so fuuuuuuck yoooooooooooooooooooouuuuuu
…and so on. The Circle of Life.
But the thing is, every single developer has choices. And if you make them rewrite their code enough times, some of those other choices are going to start looking mighty appealing. They’re not your hostages, as much as you’d like them to be. They are your guests. Python is still a very popular programming language, to be sure — but golly did Python 3(000) create a huge mess for themselves, their communities, and the users of their communities’ software — one that has been a train-wreck in progress for fifteen years and is still kicking.
How much Python software was rewritten in Go (or Ruby, or some other alternative) because of that backwards incompatibility? How much new software was written in something other than Python, which might have been written in Python if Guido hadn’t burned everyone’s house down? It’s hard to say, but I can tell you, it hasn’t been good for Python. It’s a huge mess and everyone is miserable.
So let’s say Apple pulls a Guido and breaks compatibility. What do you think will happen? Well, maybe 80–90% of the developers will rewrite their software, if they’re lucky. Which is the same thing as saying, they’re going to lose 10–20% of their user base to some competing language, e.g. Flutter.
Do that a few times, and you’ve lost half your user base. And like in sports, momentum in the programming world is everything. Anyone who shows up on the charts as “lost half their users in the past 5 years” is being flagged as a Big Fat Loser. You don’t want to be trending down in the Platforms world. But that’s exactly where deprecation — the “removing APIs” kind, not the “warning but permitting” kind — will get you, over time: Trending down. Because every time you shake loose some of your developers, you’ve (a) lost them for good, because they are angry at you for breaking your contract, and (b) given them to your competitors.
Ironically, I played a role in helping Google become the deprecation-happy prima donnas that they are today, when I built Grok, which is a source-code understanding engine that facilitates automation and tooling on source code itself — similar to an IDE, but as a cloud-based service that stores materialized representations of Google’s entire multi-billion-line source graph in a big datastore.
Grok provided Googlers with a powerful foundation for doing automated refactorings across the entire code base (literally all of Google). Grok can figure out not just your upstream dependencies (who you depend on), but also your downstream dependencies (who depends on you), so when you change an API, you know everyone you’re breaking! So you can make a change and know that every consumer of your API is being updated to the replacement version; in fact, often, via a tool they wrote called Rosie, you can automate it entirely.
This permits Google’s code base internally to be almost preternaturally “clean”, as they have these automated mice scurrying about the house, There Will Come Soft Rains-style, cleaning up the dust balls as they rename SomeDespicablyLongFunctionName to SomeDespicablyLongMethodName because someone decided it was an ugly grandchild and it needed to be euthanized.
And honestly it works pretty well for Google… internally. I mean, yes, the Go community within Google does get some good-natured laughs at the expense of the Google Java community over the latter’s habit of gratuitous continuous refactoring. If you keep twiddling with something N times, then it implies that not only did you fuck it up N-1 times, but after a while it’s pretty clear you’ve probably fucked it up on the Nth try as well. But by and large, they stay on top of it, and stuff stays “clean”.
The problem begins when they take that attitude and try to impose it on their Cloud customers and other API users.
I’ve walked you a bit through Emacs, Android, and Java; let’s look at one last successful long-lived platform: The Web itself. Boy, HTTP sure has gone through a lot of iterations since 1995 when we were all using blink tags and under-construction signs in our handwritten HTML pages.
But it still works! And those pages still work! That’s right, folks, browsers are some of the world champions at backwards compatibility. Chrome is another example of a rare Google Platform that has their heads screwed on straight, and, you guessed it, Chrome acts effectively like a separate company from the rest of Google.
I’ll also give a shout-out to our friends in the Operating Systems business: Windows, Linux, NOT APPLE FUCK YOU APPLE, FreeBSD, and so on, for doing such a great job of backwards compatibility on their successful platforms. (Apple gets like a C-minus at best, since they break shit all the time for no good reason, but somehow the community papers over it on each release, and so far, containers haven’t completely obsoleted OS X… yet.)
But wait, you say. Aren’t you comparing apples to oranges, with standalone single-machine software systems like Emacs/JDK/Android/Chrome to multi-machine systems and APIs like Clouds?
Well, I tweeted this yesterday, but as a Larry Wall “sucks/rules”-style yardstick, I searched for “deprecated” on Google and Amazon’s developer sites, respectively, and even though AWS has hundreds more service offerings than GCP, Google’s developer docs mention deprecation around 7x as often.
At this point, if anyone at Google is reading this, they’re probably ready to pull out charts, Donald Trump interview style, showing me how they’re actually doing really well, and that I can’t do unfair comparisons like “deprecation mentions as a function of number of service offerings”.
But after all these years, Google Cloud is still #3 (I still haven’t written my “How to Aim For #2 and Miss” blog post about this), and according to some internal sources, there’s some concern that they may sink to #4 soon.
I don’t have a slam-dunk silver-bullet argument for you here, to “prove” my thesis. All I have are the colorful examples I’ve shared, which I’ve accumulated over 30 years as a developer. I’ve alluded to the deeply philosophical nature of this problem; in a sense, it’s politicized within the software communities. Some folks believe that platform developers should shoulder the costs of compatibility, and others believe that platform users (developers themselves) should bear the costs. It’s really that simple. And isn’t politics always about who has to shoulder costs for shared problems?
So it’s political. And there will be angry responses to this rant.
As a user of Google Cloud Platform, and also (at Grab) of AWS for 2 years, I can tell you that there’s a world of difference between the philosophies of Amazon and Google when it comes to priorities. I’m not actively developing on AWS, so I don’t have as much of a sense for how often they sunset APIs that they have previously dangled alluringly before unwitting developers. But I have a suspicion it’s nowhere near as often as happens at Google, and I believe wholeheartedly that this source of constant friction, and frustration, in GCP, is one of the biggest factors holding it back.
I know I haven’t gone into a lot of specific details about GCP’s deprecations. I can tell you that virtually everything I’ve used, from networking (legacy to VPC) to storage (Cloud SQL v1 to v2) to Firebase (now Firestore with a totally different API) to App Engine (don’t even get me started) to Cloud Endpoints to… I dunno, everything, has forced me to rewrite it all after at most 2–3 years, and they never automate it for you, and often there is no documented migration path at all. It’s just crickets.
And every time, I look over at AWS, and I ask myself what the fuck I’m still doing on GCP. They clearly don’t want customers. They want shoppers. Do you see the difference? Let me show you.
Google Cloud has a Marketplace in which people can offer their software solutions, and in order to avoid the empty-restaurant effect, they had to populate it with some offerings, so they contracted with a company called Bitnami to create a bunch of “click to deploy” solutions, or perhaps I should write “solutions”, because they don’t solve a fucking thing. They’re just there as checkboxes, as marketing filler, and Google never gave a shit whether any of them worked from Day One. I know the PMs who were driving it and I can assure you, those men do not give a shit.
Take click-to-deploy Percona, for instance. I was getting fed up with Google Cloud SQL’s shenanigans, and started looking into setting up my own Percona cluster as an alternative. And for once, Google was going to have done something right, and they were going to save me some time and effort with the click of a button!
Go ahead, I dare you. Follow the link and click the button. Choose “yes” to get all the default parameters and deploy the cluster to your Google Cloud project. Haha, joke’s on you; it doesn’t work. None of that shit works. It’s never tested, starts bit-rotting the minute they roll it out, and it wouldn’t surprise me if over half the click-to-deploy “solutions” (now we understand the air quotes) don’t work at all. It’s a completely embarrassing dark alley that you don’t want to wander down.
But Google is straight-up encouraging you to use it. They want you to buy it. It’s transactional for them. They don’t want to support anything. It’s not part of Google’s DNA. Yes, the engineers support each other, as evidenced by my Bigtable anecdote. But for their customer-facing products and services, they have always been ruthless in shutting down any offering that doesn’t meet their money bar, even if it has millions of users.
And this presents a real problem for GCP, because that DNA is behind all their Cloud offerings. They aren’t committed to supporting anything; it’s well-known that they refuse to host (as a managed service) any third-party software until after AWS has already done the same thing and built a successful business around it, at which point their customers hold them at gunpoint. But that’s the bar, to get Google to support something.
This lack of a support culture, combined with a “let’s break it in the name of making it prettier” deprecation treadmill, is alienating their developers.
And that’s not a good thing if you want to build a long-lived platform.
Google, wake the fuck up. It’s 2020. You are still losing. It’s time to take a hard look in the mirror and answer for yourselves whether you really want to be in the Cloud business.
If you do, then stop breaking shit. You guys are rich. We developers are not. So when it comes to shouldering the burden of compatibility, you need to pay for it. Not us.
Because there are at least three other really good Clouds out there. They are beckoning.
And now I’ll get back to trying to fix all my broken stuff. Sigh.
Tune in next time!
p.s. An update, after having read some of the discussions (which were great, btw). Firebase is not deprecated, nor are there any plans that I know of. However, they have a nasty threading bug that causes the Java client to stop in App Engine, which one of their engineers helped me with while I was at Google, but they never actually fixed it outright, so I have a crummy workaround to restart my GAE app every day. Four years later! Now they have Firestore, which will take work to migrate to as it’s totally different, and the Firebase bug’s never gonna be fixed. What can we learn from this? You can get help from them if you work there. It’s frightening that I appear to be the only one using Firebase on GAE, since I’m literally writing fewer than 100 keys in a 100% vanilla app, and it stops working every couple days from an acknowledged bug. What can I tell you, except use it at your own risk. I’m switching to Redis.
I’ve also seen some folks more experienced with AWS saying that AWS basically never deprecates/sunsets anything, SimpleDB being a great example. My speculation about AWS not having Google’s deprecation disease seems to have been justified.
It was also brought to my attention that 20 days ago, Google’s App Engine team broke a critical Go library hosting service by deprecating and killing a GAE app being run by one of the core Go engineers. Egg on face indeed.
Finally, I’ve heard that Googlers are busy discussing this already, and are by and large agreeing with me (love you guys!)—with the caveat that they seem to think that it’s not fixable because Google’s culture has never had the right incentive structure in place. I think it would be good if I could carve out some time to discuss the absolutely astonishing experience I had working with AWS engineers while I was at Grab. Sometime soon, I hope!
Oh yeah, and they really did have different species of gummy sharks in a giant self-serve bin in Building 43 in MTV in 2005, hammerhead being my favorite flavor. Larry & Sergey got rid of all the unhealthy snacks by 2006 though. So at the time of my Bigtable story in 2007, there were indeed no gummy sharks and I’m a big fat liar.
When I looked at Cloud Bigtable 4 years ago (give or take), that was the cost. It seems to have come down a bit, but is still an awful lot for an empty datastore, especially given that my first story shows how inconsequential an empty Bigtable is in their grand scheme.
Sorry for offending all the Apple folks, and for not saying enough nice things about Microsoft, etc. I’ve read all the threads online and nobody has said anything unfair, in my opinion. I appreciate all the discussion this has generated! But sometimes you have to make a big splash to kick the discussions off, you know?
Thanks for reading.
Update 2, Aug 19 2020: Stripe does it right! https://stripe.com/blog/api-versioning
Update 3, Aug 31 2020: A Google engineer in Cloud Marketplace who happens to be an old friend of mine contacted me to find out why C2D didn’t work, and we eventually figured out that it was because I had committed the sin of creating my network a few years ago, and C2D was failing for legacy networks due to a missing subnet parameter in their templates. I guess my advice to prospective GCP users is to make sure you know a lot of people at Google…
Steve Yegge is ex-Geoworks, ex-Amazon, ex-Google, and ex-Grab, with nearly 30 years of tech industry experience. Nowadays he’s pretty much retired.
7.9K 
62
7.9K 
7.9K 
62
Steve Yegge is ex-Geoworks, ex-Amazon, ex-Google, and ex-Grab, with nearly 30 years of tech industry experience. Nowadays he’s pretty much retired.
"
https://medium.com/free-code-camp/going-serverless-how-to-run-your-first-aws-lambda-function-in-the-cloud-d866a9b51536?source=search_post---------8,"There are currently no responses for this story.
Be the first to respond.
A decade ago, cloud servers abstracted away physical servers. And now, “Serverless” is abstracting away cloud servers.
Technically, the servers are still there. You just don’t need to manage them anymore.
Another advantage of going serverless is that you no longer need to keep a server running all the time. The “server” suddenly appears when you need it, then disappears when you’re done with it. Now you can think in terms of functions instead of servers, and all your business logic can now live within these functions.
In the case of AWS Lambda Functions, this is called a trigger. Lambda Functions can be triggered in different ways: an HTTP request, a new document upload to S3, a scheduled Job, an AWS Kinesis data stream, or a notification from AWS Simple Notification Service (SNS).
In this tutorial, I’ll show you how to set up your own Lambda Function and, as a bonus, show you how to set up a REST API all in the AWS Cloud, while writing minimal code.
Note that the Pros and Cons of Serverless depend on your specific use case. So in this article, I’m not going to tell you whether Serverless is right for your particular application — I’m just going to show you how to use it.
First, you’ll need an AWS account. If you don’t have one yet, start by opening a free AWS account here. AWS has a free tier that’s more than enough for what you will need for this tutorial.
We’ll be writing the function isPalindrome, which checks whether a passed string is a palindrome or not.
Above is an example implementation in JavaScript. Here is the link for gist on Github.
A palindrome is a word, phrase, or sequence that reads the same backward as forward, for the sake of simplicity we will limit the function to words only.
As we can see in the snippet above, we take the string, split it, reverse it and then join it. if the string and its reverse are equal the string is a Palindrome otherwise the string is not a Palindrome.
In this step we will be heading to the AWS Console to create the Lambda Function:
In the AWS Console go to Lambda.
And then press “Get Started Now.”
For runtime select Node.js 6.10 and then press “Blank Function.”
Skip this step and press “Next.”
For Name type in isPalindrome, for description type in a description of your new Lambda Function, or leave it blank.
As you can see in the gist above a Lambda function is just a function we are exporting as a module, in this case, named handler. The function takes three parameters: event, context and a callback function.
The callback will run when the Lambda function is done and will return a response or an error message.For the Blank Lambda blueprint response is hard-coded as the string ‘Hello from Lambda’. For this tutorial since there will be no error handling, you will just use Null. We will look closely at the event parameter in the next few slides.
Scroll down. For Role choose “Create new Role from template”, and for Role name use isPalindromeRole or any name, you like.
For Policy templates, choose “Simple Microservice” permissions.
For Memory, 128 megabytes is more than enough for our simple function.
As for the 3 second timeout, this means that — should the function not return within 3 seconds — AWS will shut it down and return an error. Three seconds is also more than enough.
Leave the rest of the advanced settings unchanged.
Press “Create function.”
Congratulations — you’ve created your first Lambda Function. To test it press “Test.”
As you can see, your Lambda Function returns the hard-coded response of “Hello from Lambda.”
Now add the code from isPalindrome.js to your Lambda Function, but instead of return result use callback(null, result). Then add a hard-coded string value of abcd on line 3 and press “Test.”
The Lambda Function should return “abcd is not a Palindrome.”
For the hard-coded string value of “racecar”, The Lambda Function returns “racecar is a Palindrome.”
So far, the Lambda Function we created is behaving as expected.
In the next steps, I’ll show you how to trigger it and pass it a string argument using an HTTP request.
If you’ve built REST APIs from scratch before using a tool like Express.js, the snippet above should make sense to you. You first create a server, and then define all your routes one-by-one.
In this section, I’ll show you how to do the same thing using the AWS API Gateway.
Go to your AWS Console and press “API Gateway.”
And then press “Get Started.”
In Create new API dashboard select “New API.”
For API name, use “palindromeAPI.” For description, type in a description of your new API or just leave it blank.
Our API will be a simple one, and will only have one GET method that will be used to communicate with the Lambda Function.
In the Actions menu, select “Create Method.” A small sub-menu will appear. Go ahead and select GET, and click on the checkmark to the right.
For Integration type, select Lambda Function.
Then press “OK.”
In the GET — Method Execution screen press “Integration Request.”
For Integration type, make sure Lambda Function is selected.
For request body passthrough, select “When there are no templates defined” and then for Content-Type enter “application/json”.
In the blank space add the JSON object shown below. This JSON object defines the parameter “string” that will allow us to pass through string values to the Lambda Function using an HTTP GET request. This is similar to using req.params in Express.js.
In the next steps, we’ll look at how to pass the string value to the Lambda Function, and how to access the passed value from within the function.
The API is now ready to be deployed. In the Actions menu click “Deploy API.”
For Deployment Stage select “[New Stage]”.
And for Stage name use “prod” (which is short for “production”).
The API is now deployed, and the invoke URL will be used to communicate via HTTP request with Lambda. If you recall, in addition to a callback, Lambda takes two parameters: event and context.
To send a string value to Lambda you take your function’s invoke URL and add to it ?string=someValue and then the passed value can be accessed from within the function using event.string.
Modify code by removing the hard-coded string value and replacing it with event.string as shown below.
Now in the browser take your function’s invoke URL and add ?string=abcd to test your function via the browser.
As you can see Lambda replies that abcd is not a Palindrome. Now do the same for racecar.
If you prefer you can use Postman as well to test your new isPalindrome Lambda Function. Postman is a great tool for testing your API endpoints, you can learn more about it here.
To verify it works, here’s a Palindrome:
And here’s a non-palindrome:
Congratulations — you have just set up and deployed your own Lambda Function!
Thanks for reading!
We’ve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
3.4K 
32
3.4K claps
3.4K 
32
We’ve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
Written by
Helping build financial progress for all @ Credit Karma. I speak Javascript, PHP, GO, English, Parle le Francais, Hablo Espanol & 会说中文! 嗯嗯，真的.
We’ve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
"
https://medium.com/thinking-design/the-evolution-of-ui-ux-designers-into-product-designers-623e4e7eaab3?source=search_post---------9,"There are currently no responses for this story.
Be the first to respond.
Top highlight
What is a product? Until recently, the term was only used in relation to something material and often found in a retail store, but nowadays it is coming to mean digital products as well. Apps are modern products.
When it comes to building great products, design is the most important “feature.” We’ve moved into the stage where product design dominates — it’s what sets companies apart and gives a real edge over competitors.
The design industry has evolved quite a lot in the last few years and today there are a number of different responsibilities encompassed by the umbrella term designer: UX Designers, UI Designers, and Product Designers. “What is the difference between these job roles?” is a fairly common question in design industry. Let’s attempt to distill what each of these titles really mean, and look at why the evolution of UI/UX Designers into Product Designers is a logical development in the era of modern technology.
All of these roles have one thing in common — they all design how a user interacts with a product. But these people all perform slightly different functions to reach the same goal.
UX designers are primarily concerned with how the product feels. The goal is make the user’s interaction as efficient and simple as possible. UX designers look at design from the mindspace of a user and squash potential problems by:
UI designers, on the other hand are the people who are primarily concerned how the product looks. They responsible for how we see the product in its final version. They are in charge of designing each screen or page with which a user interacts and ensuring that the user interface visually communicates the logic that a UX designer has offered (for example, a UI designer creating a data dashboard can front load the most important content at the top). UI designers are also responsible for creating a style guide and unified visual language that is applied across the product.
Product Designer is a sum-up term used to describe a designer who is generally involved in the creation of the look and feel of a whole product. Many product designers consider themselves to be designers who design experiences. This means that a product designer is the one who gives real insight to UX and UI designers when it comes to how certain features should work, or how a certain UI control should look.
Companies use the term ‘Product Designer’ differently; the most general definition is a person who’s a champion of a user’s needs. As Justin Edmund said, “A product designer oversees product vision from a high level (how does this feature make sense for where we want to be in 6 months) to a low execution level (how does styling this button this way impact how the user flows through this function).”
Design and its component practices are like any other craft: you can always develop a deeper familiarity with the minutiae and master your skills by taking more active role in whole design process. There’s a general positive trend of designers taking on a larger scope of responsibilities within the product development process, and here are some drivers for this trend:
Design thinking has become a popular approach for designing a product. The best design reflects the product’s goals. Good designers have always applied design thinking to product design, whether physical or digital, because it’s focussed on end-to-end product development, not just the “design phase” part.
When thinking in products, designers should understand business objectives and be able to answer the following questions first:
Answering these questions helps designers understand the user experience of a product as a whole; not purely as interaction (feel) or visual (look) design part. Only after that it makes sense to move to the actual state offinding a design solution which includes following 6 phases:
More and more companies try to unify designers and developers into the development process. This new way of product development has two major benefits:
Unlike more traditional forms of design, design process for digital products isn’t a one-time thing and designers should never assume to get everything right on the first go. Implementation often reveals gaps in the design: undocumented conditions or bad assumptions about product usage, which hard to predict without shipping the product.
To design a successful product you need to adopt a process of continuous improvement. Iterative design follows the idea that design should be done in repeated cycles: it’s a process of constantly refining and improving the product based on both qualitative and quantitative feedback data from your users. This is a great opportunity for designers to see a bigger picture, improve their work based on user feedback and make the product inherently more valuable to the user.
Product design represents the next enlargement of design scope, from just user experience design towards an even broader state of design for an entire product.
The best products are built by people who understand the whole product, not just their silo. In order to make such products, UI/UX Designers should look to evolve into Product Designers who are able to generate and process a additional information to get the best result.
Nick Babich is a developer, tech enthusiast, and UX lover. He has spent the last 10 years working in the software industry with a specialized focus on development. He counts advertising, psychology, and cinema among his myriad interests.
Originally published at blogs.adobe.com.
More about Adobe XD:
Stories and insights from the design community.
1.7K 
18
1.7K claps
1.7K 
18
Written by
New Tools for New Creatives. Get all the latest creative apps plus seamless ways to share and collaborate. All right on your desktop.
Stories and insights from the design community.
Written by
New Tools for New Creatives. Get all the latest creative apps plus seamless ways to share and collaborate. All right on your desktop.
Stories and insights from the design community.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://towardsdatascience.com/running-jupyter-notebook-in-google-cloud-platform-in-15-min-61e16da34d52?source=search_post---------10,"Sign in
What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Top highlight
Amulya Aankul
Sep 8, 2017·5 min read
Recently, while I was doing my research project on Computer Vision using Convolutional Neural Network, I found out that my 8GB RAM laptop is useless. It took me an hour to learn from just 1 epoch. Therefore, rather than spending 1500$ on a new GPU based laptop, I did it for free on Google Cloud. (Google Cloud gives 300$ credit, and I have 3 gmail accounts and 3 credit cards :D)
So lets not waste anymore time and move straight to running jupyter notebook in GCP.
For this step, you will have to put your payment information and verify your account. It’s the most simple step. If you fail this step, close your laptop and think where you are going in life.
Click on the three dots shown in the image below and then click on the + sign to create a new project.
Click on the three lines on the upper left corner, then on the compute option, click on ‘Compute Engine’
Now click on ‘Create new instance’. Name your instance, select zone as ‘ us-west1-b’. Choose your ‘machine type’. (I chose 8v CPUs).
Select your boot disk as ‘Ubuntu 16.04 LTS’. Under the firewall options tick both ‘http’ and ‘https’ (very important). Then, choose the disk tab and untick ‘ Delete boot disk when instance is deleted’.
If you click on ‘customize’, you will be able to find options for using GPUs. You can choose between 2 NVIDIA GPUs.
Some firewall settings:-
Now click on ‘Create’ and your instance is ready!
Your new VM instance should look something like this. Note down the External IP.
IMPORTANT : DON’T FORGET TO STOP YOUR GPU INSTANCE AFTER YOU ARE DONE BY CLICKING ON THE THREE DOTS ON THE IMAGE ABOVE AND SELECTING STOP. OTHERWISE GCP WILL KEEP CHARGING YOU ON AN HOURLY BASIS.
By default, the external IP address is dynamic and we need to make it static to make our life easier. Click on the three horizontal lines on top left and then under networking, click on VPC network and then External IP addresses.
Change the type from Ephemeral to Static.
Now, click on the ‘Firewall rules’ setting under Networking.
Click on ‘Create Firewall Rules’ and refer the below image:
Under protocols and ports you can choose any port. I have chosen tcp:5000 as my port number. Now click on the save button.
Now start your VM instance. When you see the green tick click on SSH. This will open a command window and now you are inside the VM.
In your SSH terminal, enter:
and follow the on-screen instructions. The defaults usually work fine, but answer yes to the last question about prepending the install location to PATH:
To make use of Anaconda right away, source your bashrc:
Now, install other softwares :
Open up a SSH session to your VM. Check if you have a Jupyter configuration file:
If it doesn’t exist, create one:
We’re going to add a few lines to your Jupyter configuration file; the file is plain text so, you can do this via your favorite editor (e.g., vim, emacs). Make sure you replace the port number with the one you allowed firewall access to in step 5.
It should look something like this :
To run the jupyter notebook, just type the following command in the ssh window you are in :
Once you run the command, it should show something like this:
Now to launch your jupyter notebook, just type the following in your browser:
where, external ip address is the ip address which we made static and port number is the one which we allowed firewall access to.
Congratulations! You successfully installed jupyter notebook on GCP!
Lets connect : https://www.linkedin.com/in/aankul
Follow me on medium: https://medium.com/@aankul.a
Edit :All those facing ‘ERR_CONNECTION_TIMED_OUT’ error. Please try Bastardized Eloquence’s solution in the comments.
Data Engineer at Amazon
See all (170)
5.2K 
80
Some rights reserved

Every Thursday, the Variable delivers the very best of Towards Data Science: from hands-on tutorials and cutting-edge research to original features you don't want to miss. Take a look.
5.2K claps
5.2K 
80
Your home for data science. A Medium publication sharing concepts, ideas and codes.
About
Write
Help
Legal
Get the Medium app
"
https://towardsdatascience.com/automated-machine-learning-on-the-cloud-in-python-47cf568859f?source=search_post---------11,"Sign in
There are currently no responses for this story.
Be the first to respond.
Will Koehrsen
May 22, 2018·9 min read
Two trends have recently become apparent in data science:
This article will cover a brief introduction to these topics and show how to implement them, using Google Colaboratory to do automated machine learning on the cloud in Python.
Originally, all computing was done on a mainframe. You logged in via a terminal, and connected to a central machine where users simultaneously shared a single large computer. Then, along came microprocessors and the personal computer revolution and everyone got their own machine. Laptops and desktops work fine for routine tasks, but with the recent increase in size of datasets and computing power needed to run machine learning models, taking advantage of cloud resources is a necessity for data science.
Cloud computing in general refers to the “delivery of computing services over the Internet”. This covers a wide range of services, from databases to servers to software, but in this article we will run a simple data science workload on the cloud in the form of a Jupyter Notebook. We will use the relatively new Google Colaboratory service: online Jupyter Notebooks in Python which run on Google’s servers, can be accessed from anywhere with an internet connection, are free to use, and are shareable like any Google Doc.
Google Colab has made the process of using cloud computing a breeze. In the past, I spent dozens of hours configuring an Amazon EC2 instance so I could run a Jupyter Notebook on the cloud and had to pay by the hour! Fortunately, last year, Google announced you can now run Jupyter Notebooks on their Colab servers for up to 12 hours at a time completely free. (If that’s not enough, Google recently began letting users add a NVIDIA Tesla K80 GPU to the notebooks). The best part is these notebooks come pre-installed with most data science packages, and more can be easily added, so you don’t have to worry about the technical details of getting set up on your own machine.
To use Colab, all you need is an internet connection and a Google account. If you just want an introduction, head to colab.research.google.com and create a new notebook, or explore the tutorial Google has developed (called Hello, Colaboratory). To follow along with this article, get the notebook here. Sign into your Google account, open the notebook in Colaboratory, click File > save a copy in Drive, and you will then have your own version to edit and run.
Data science is becoming increasingly accessible with the wealth of resources online, and the Colab project has significantly lowered the barrier to cloud computing. For those who have done prior work in Jupyter Notebooks, it’s a completely natural transition, and for those who haven’t, it’s a great opportunity to get started with this commonly used data science tool!
Automated machine learning (abbreviated auto-ml) aims to algorithmically design and optimize a machine learning pipeline for a particular problem. In this context, the machine learning pipeline consists of:
There are an almost infinite number of ways these steps can be combined together, and the optimal solution will change for every problem! Designing a machine learning pipeline can be a time-consuming and frustrating process, and at the end, you will never know if the solution you developed is even close to optimal. Auto-ml can help by evaluating thousands of possible pipelines to try and find the best (or near-optimal) solution for a particular problem.
It’s important to remember that machine learning is only one part of the data science process, and automated machine learning is not meant to replace the data scientist. Instead, auto-ml is meant to free the data scientist so she can work on more valuable aspects of the process, such as gathering data or interpreting a model.
There are a number of auto-ml tools — H20, auto-sklearn, Google Cloud AutoML — and we will focus on TPOT: Tree-based Pipeline Optimization Tool developed by Randy Olson. TPOT (your “data-science assistant”) uses genetic programming to find the best machine learning pipeline.
To use TPOT, it’s not really necessary to know the details of genetic programming, so you can skip this section. For those who are curious, at a high level, genetic programming for machine learning works as follows:
(For more details on genetic programming, check out this short article.)
The primary benefit of genetic programming for building machine learning models is exploration. Even a human with no time restraints will not be able to try out all combinations of preprocessing, models, and hyperparameters because of limited knowledge and imagination. Genetic programming does not display an initial bias towards any particular sequence of machine learning steps, and with each generation, new pipelines are evaluated. Furthermore, the fitness function means that the most promising areas of the search space are explored more thoroughly than poorer-performing areas.
With the background in place, we can now walk through using TPOT in a Google Colab notebook to automatically design a machine learning pipeline. (Follow along with the notebook here).
Our task is a supervised regression problem: given New York City energy data, we want to predict the Energy Star Score of a building. In a previous series of articles (part one, part two, part three, code on GitHub), we built a complete machine learning solution for this problem. Using manual feature engineering, dimensionality reduction, model selection, and hyperparameter tuning, we designed a Gradient Boosting Regressor model that achieved a mean absolute error of 9.06 points (on a scale from 1–100) on the test set.
The data contains several dozen continuous numeric variables (such as energy use and area of the building) and two one-hot encoded categorical variables (borough and building type) for a total of 82 features.
The score is the target for regression. All of the missing values have been encoded as np.nan and no feature preprocessing has been done to the data.
To get started, we first need to make sure TPOT is installed in the Google Colab environment. Most data science packages are already installed, but we can add any new ones using system commands (preceded with a ! in Jupyter):
After reading in the data, we would normally fill in the missing values (imputation) and normalize the features to a range (scaling). However, in addition to feature engineering, model selection, and hyperparameter tuning, TPOT will automatically impute the missing values and do feature scaling! So, our next step is to create the TPOT optimizer:
The default parameters for TPOT optimizers will evaluate 100 populations of pipelines, each with 100 generations for a total of 10,000 pipelines. Using 10-fold cross validation, this represents 100,000 training runs! Even though we are using Google’s resources, we do not have unlimited time for training. To avoid running out of time on the Colab server (we get a max of 12 hours of continuous run time), we will set a maximum of 8 hours (480 minutes) for evaluation. TPOT is designed to be run for days, but we can still get good results from a few hours of optimization.
We set the following parameters in the call to the optimizer:
There are other parameters that control details of the genetic programming method, but leaving them at the default works well for most cases. (If you want to play around with the parameters, check out the documentation.)
The syntax for TPOT optimizers is designed to be identical to that for Scikit-Learn models so we can train the optimizer using the .fit method.
During training, we get some information displayed:
Due to the time limit, our model was only able to get through 15 generations. With 100 populations, this still represents 1500 different individual pipelines that were evaluated, quite a few more than we could have tried by hand!
Once the model has trained, we can see the optimal pipeline using tpot.fitted_pipeline_. We can also save the model to a Python script:
Since we are in a Google Colab notebook, to get the pipeline onto a local machine from the server, we have to use the Google Colab library:
We can then open the file (available here) and look at the completed pipeline:
We see that the optimizer imputed the missing values for us and built a complete model pipeline! The final estimator is a stacked model meaning that it uses two machine learning algorithms ( LassoLarsCV and GradientBoostingRegressor ), the second of which is trained on the predictions of the first (If you run the notebook again, you may get a different model because the optimization process is stochastic). This is a complex method that I probably would not have been able to develop on my own!
Now, the moment of truth: performance on the testing set. To find the mean absolute error, we can use the .score method:
In the series of articles where we developed a solution manually, after many hours of development, we built a Gradient Boosting Regressor model that achieved a mean absolute error of 9.06. Automated machine learning has significantly improved on the performance with a drastic reduction in the amount of development time.
From here, we can use the optimized pipeline and try to further refine the solution, or we can move on to other important phases of the data science pipeline. If we use this as our final model, we could try and interpret the model (such as by using LIME: Local Interpretable Model-Agnostic Explainations) or write a well-documented report.
In this post, we got a brief introduction to both the capabilities of the cloud and automated machine learning. With only a Google account and an internet connection, we can use Google Colab to develop, run, and share machine learning or data science work loads. Using TPOT, we can automatically develop an optimized machine learning pipeline with feature preprocessing, model selection, and hyperparameter tuning. Moreover, we saw that auto-ml will not replace the data scientist, but it will allow her to spend more time on higher value parts of the workflow.
While being an early adopter does not always pay off, in this case, TPOT is mature enough to be easy to use and relatively issue-free, yet also new enough that learning it will put you ahead of the curve. With that in mind, find a machine learning problem (perhaps through Kaggle) and try to solve it! Running automatic machine learning in a notebook on Google Colab feels like the future and with such a low barrier to entry, there’s never been a better time to get started!
As always, I welcome feedback and discussion and can be reached on Twitter @koehrsen_will.
Data Scientist at Cortex Intel, Data Science Communicator
4.1K 
12
4.1K 
4.1K 
12
Your home for data science. A Medium publication sharing concepts, ideas and codes.
"
https://towardsdatascience.com/passing-the-google-cloud-professional-data-engineer-certification-87da9908b333?source=search_post---------12,"Sign in
There are currently no responses for this story.
Be the first to respond.
Daniel Bourke
Apr 27, 2019·10 min read
*Note: This article is dedicated to the Google Cloud Professional Data Engineer Certification exam before March 29, 2019. After this date, there were some updates. I have included these in the Extras section*
"
https://medium.com/free-code-camp/why-learning-to-code-alexa-skills-is-the-gateway-to-a-cloud-computing-job-fa13c1c0c853?source=search_post---------13,"There are currently no responses for this story.
Be the first to respond.
There are radical economic shifts underway. Society is moving from commodity-based capital toward intellectual capital. Not only will repetitive manufacturing jobs will be wiped-out — mundane white-collar work will also be eliminated.
The jobs of the future will be those which can’t be done by robots or replaced by artificial intelligence.
“We have an opportunity to fill the hundreds of thousands of vacant American jobs that aren’t being automated. These are the jobs that involve telling those machines what to do. The programming jobs.” — Quincy Larson
So where will these programming jobs exist? For those willing to invest in developing the intellectual capital necessary to compete in the new economy, all trends point to the public cloud.
In 2017, an estimated $122.5 billion will be invested in public cloud services. The cloud segment is growing fast — with spend expected to increase 30% annually to $203 billion by 2020.
Expectations are most bullish for cloud applications and cloud infrastructure to over perform. Customer demand for IaaS and SaaS cloud components will accelerate as they pursue digital business strategies.
All of this spending on public cloud is creating a massive demand for skills. In CompTIA’s recent outlook, they noted IDC’s anticipation of 7 million cloud-related jobs — many from existing positions which will be re-engineered with a heavy cloud focus.
But cloud talent is in short supply — and the scarcity of cloud computing skills is now the #1 impediment for companies with major cloud initiatives. As a result, cloud-fluent developers are now commanding some of the highest paying jobs.
In 2016, the top paying IT certification was cloud-related Amazon Web Services, with an average salary of $125,871.
“Many people in the U.S. and around the world lack the education and skills required to participate in the great new companies coming out of the software revolution. There’s no way through this problem other than education, and we have a long way to go.” — Marc Andreessen
“Today, another shift is taking place, to event driven functions, because the underlying constraints have changed, costs have reduced, and radical improvements in time to value are possible.” — Adrian Cockcroft
The cloud is also going through a radical change — one that mimics the economic shift from commodity-based capital toward intellectual capital. Repetitive virtual machines will be wiped out, and mundane white-collar maintenance scripts will be eliminated. The cloud of the future will be driven by “functions as a service” that can’t be performed by server-based infrastructure or platform services.
A simple and engaging way to explore cloud computing services and the new concept of event driven “serverless” functions is by developing a custom Alexa skill. It’s a fun and engaging entry point into the API-driven world of cloud computing and emerging serverless architecture patterns.
While Alexa is still in the early stages of adoption, it’s clear that voice interaction is a breakout technology. Since Amazon opened Alexa to developers, over 10,000 skills have been developed for the service.
Underlying the growth of Alexa is a key innovation by Amazon Web Services that is fueling a majority of the custom skills — a service called Lambda.
AWS Lambda is an event-driven, serverless computing platform provided by Amazon as a part of the Amazon Web Services. It is a compute service that runs code in response to events and automatically manages the compute resources required by that code.
When a user invokes an Amazon Alexa skill, it triggers an event via an API call. AWS Lambda makes it easy to execute a function in response to the event. And since AWS Lambda currently supports functions written in Node.js, Python, C#, and Java — it’s easy for developers that are already familiar with those common languages to build functions.
To get started with Amazon Alexa and AWS Lambda, I encourage you take a few easy steps toward building marketable cloud skills.
acloud.guru
After publishing your skill, just fill out a simple form to get a free Alexa swag. Amazon offers a new promotion each and every month.
Thanks for reading! If you like what you read, hold the clap button below so that others may find this. You can follow me on Twitter.
We’ve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
1.4K 
15
1.4K claps
1.4K 
15
Written by
migrating talent to the cloud at acloud.guru
We’ve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
Written by
migrating talent to the cloud at acloud.guru
We’ve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
"
https://towardsdatascience.com/maximize-your-gpu-dollars-a9133f4e546a?source=search_post---------14,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jeff Hale
Oct 29, 2018·13 min read
I wanted to figure out where I should train my deep learning models online for the lowest cost and least hassle. I wasn’t able to find a good comparison of GPU cloud service providers, so I decided to make my own.
Feel free to skip to the pretty charts if you know all about GPUs and TPUs and just want the results.
I’m not looking at serving models in this article, but I might in the future. Follow me to make sure you don’t miss out.
"
https://medium.com/firebase-developers/should-i-query-my-firebase-database-directly-or-use-cloud-functions-fbb3cd14118c?source=search_post---------15,"There are currently no responses for this story.
Be the first to respond.
Just about every app you use has to query a database and show the results on screen. Firebase makes this easy by providing SDKs for apps to directly read and write the databases provided by the platform (Realtime Database and Cloud Firestore). But there are some situations when you want to route all requests through a server side component, such as Cloud Functions, to manage the query. But how do you make that decision? When is it better to directly query from the client, and when is it better to route the request through a backend? There’s no right or wrong answer here, so let’s weigh each option by the properties that matter the most.
In this article, I’ll use the word “direct” to talk about database access using the Firebase SDKs that query a database without going through a backend. And I’ll use the word “indirect” for access going through Cloud Functions, or some other backend you control.
Here’s an example of direct access to Cloud Firestore from a web client using JavaScript. It’s simply requesting all documents in a collection, sorted by a field with a timestamp, limited to the first 100 results. The returned snapshot object contains all the query results, ready to use:
And here’s an example of indirect access, via an HTTP type Cloud Function also written in JavaScript. It’s almost exactly the same, except now the client has to invoke it via an HTTP request, and process all the results returned in JSON format:
I’ll compare these options by some important characteristics.
Of course, everyone wants their database access to be fast. This might be the first factor to consider when choosing between direct or indirect access. It’s intuitive that direct access is usually going to be faster than indirect access. Here’s why.
The Firebase SDKs provide a local persistence layer (cache) that stores query results for future use. When a client app issues a query, if the SDK determines that the cache contains up-to-date results for that query, the results can come directly from the cache. The obvious benefit here is that network bandwidth and latency are reduced. The results appear faster, even while offline, and the end user pays less data costs to see those results.
It’s worth noting that the local cache is not always available on the mobile client, and sometimes it might have to be explicitly enabled. Be sure to check the documentation for your database (Realtime Database or Cloud Firestore) for your specific mobile platform to understand the requirements and limitations.
If you make the request via Cloud Functions, there is absolutely no client-side caching done by default. If you want to cache the results, you’ll have to do that on the client, using some mechanism you choose. For example, Android apps might choose to use Room to persist query results. You’ll have to write and test all the code to make sure it works. You’ll also have to figure out if and when cached query results become stale.
The case is similar for database writes. If you write a document using the SDK while the client is offline, the write will be persisted locally, then automatically synchronized later when connectivity returns. However, if you write via a call to Cloud Functions, the HTTP connection will obviously fail fast while offline, and the client will have to retry as needed.
Performance (and convenience) wins obviously goes to the Firebase client SDK. But there is one case to be aware of when performance can be poor for the local cache. If the SDK’s local cache becomes very large, and a complex query has to sort through thousands of records in order to satisfy the query, the cost of doing that on the client might become worse than the same query executed on the server. I would expect that most apps won’t run into this situation, but you should be aware that it can occur. It’s good to measure the performance of your queries, and you can do that in production, on your users’ devices, with Firebase Performance Monitoring.
One important behavior for Firebase databases, when accessed from the client SDKs, is the fact that if you read any node or document, the client always transfers the entire contents of that entity. The client SDKs don’t support the limiting of child nodes or document fields in the response, which is sometimes called a “projection” in SQL. To work around this constraint, sometimes developers will structure their database to support the minimal transfer of data for a query. Sometimes this involves the duplication of data in various places, which is common for NoSQL type databases.
Imagine you have a collection for blog posts in a collection called posts. Notice it has a field for text that could be very long.
If you queried this collection on the client to display a list of posts that match some criteria, the client would necessarily download the entire blog post just to satisfy that query. So, in order to speed things up, you could move the large text field into a separate collection, “posts-text”:
Now, queries against “posts” will execute faster on the client, and the document with the text of the post can be fetched only as needed.
However, if you’ve already committed to a schema that structures documents that cause performance problems when queried, and you can’t change it, using a Cloud Function might be the best choice. The function could perform the query using its fast connection to the database, extract only the necessary fields for display, and send the minimal results to the client.
There is not just one correct way to decide whether or not to use direct or indirect access for performance reasons. You’ll need to weigh (and hopefully benchmark!) your options to figure out what’s best.
The overall pricing for both Firebase databases (Cloud Firestore, Realtime Database) is tied (partly) to how much data your app reads. As mentioned previously, the Firebase SDK’s local cache can prevent many data reads from happening on the server. Data coming from cache, unchanged on the server, prevents the cost of the query and the cost of the bandwidth to bring that data to the app.
If you query indirectly through a Cloud Function, you will pay for the cost of the query in addition to the cost of the execution of the function. The server SDKs you use in Cloud Functions do not cache data, so each execution pays the full cost of the query, and its data transfer. Some developers may opt to implement a custom caching layer in memory or in another Google Cloud product (such as Cloud Memorystore) in order to reduce costs.
Both Firebase databases provide a way for you to control access to data coming from apps using security rules (Cloud Firestore, Realtime Database). Implementing these rules correctly and comprehensively for your app is crucial to its security. But this only works for traffic originating from the provided client SDKs (and the REST APIs, when provided with a Firebase Authentication token or no authentication mechanism).
However, when querying indirectly through Cloud Functions, the client SDKs can’t be used. You’re required to use the Firebase Admin SDK, or one of the other server SDKs. These SDKs are initialized using a service account instead of an end user Firebase Authentication account. Queries from the server SDKs are considered “privileged” and completely bypass all security rules. So, if you need to control the data coming in and out of your code deployed to Cloud Functions, you’ll have to write that logic separately from your security rules. (Note that Realtime Database has a provision for initializing the Admin SDK with a given Firebase Authentication UID, which then limits its access using the security rules that apply to that UID. No equivalent feature is currently provided for Cloud Firestore.)
Firebase security rules are great for limiting direct client access, and you should definitely start there for security. You’ll discover that security rules are backed by a special expression language, which is not a full programming language. In order to promote speed and security, there are limitations to what you can do. If you run into one of these limitations, you might have to route client access through Cloud Functions in order to perform whatever checks are necessary to allow a particular read or write operation. For example, if you want to implement strict rate limits for queries, you would have to use Cloud Functions for that, and force clients to call the function instead of using direct access. Or, if clients should never be able to read certain fields in a document, a function could filter out that data before it reaches the caller, similar to the earlier example.
I’ll also mention that if security rules get you most of the way to your security requirements for a database write, you could also use a Cloud Function to implement a database trigger (Cloud Firestore, Realtime Database) to run after a database write completes, and perform further checks there as needed. If the data found isn’t acceptable, you could then simply delete it or move it somewhere out of the way for auditing. Just bear in mind that the data will still exist in its original written location for a brief period of time.
Firebase databases have a very special feature that lets you listen for realtime updates to data (Cloud Firestore, Realtime Database). So, if a client is interested in a particular location in the database, it can attach a listener at that location, and the listener will receive callbacks whenever the underlying data changes. This also works with queries that receive multiple child nodes or documents — if the results of the query would change over time, those deltas are also received by the listener. When no more updates are needed, the client just removes the listener. For example, with Cloud Firestore, you can attach a listener to the query at the start of this post by using onSnapshot() instead of get():
This ability to receive realtime updates works great in client app code, but is almost always not appropriate for code deployed to Cloud Functions. Functions need to run quickly and get their work done in a finite amount of time. There are very few good use cases for adding a persistent listener to the database inside a function. For HTTP type functions, there is also no streaming of results back to the caller (also known as HTTP chunked transfer encoding). The entire HTTP response is delivered as a unit, and if the response isn’t delivered before the function’s configured timeout, the connection is closed. So, realtime data is not really possible withCloud Functions.
If you need to build a public API for your data in Firebase (such as Hacker News did), you could allow global read access to your data via security rules, and ask developers to use a Firebase client SDK. This makes a lot of sense if you intend to expose very “live” data, as the client APIs make it easy as access that realtime data, as described in the last section.
It gets tricky, however, if there is no client SDK that supports realtime listeners for your client’s platform. You can send people to the REST API, but streaming realtime updates is only supported by Realtime Database. Cloud Firestore has a REST API, but there’s no support for streaming.
It’s possible that the product’s REST APIs won’t work well for your case, or you want to provide something easier for clients to consume. In that case, you’ll definitely want to look into building your own API with Cloud Functions. This is actually quite common. Of course, you’ll be paying for your usage of both products, but there is one huge optimization you can apply here. Cloud Functions integrates with Firebase Hosting which you can use as an edge-caching proxy. What you do here is effectively direct web requests to Firebase Hosting, which checks to see if it already has a response in its cache. It will either serve previously-cached content, or forward the request to Cloud Functions.
You can read more about the integration between Cloud Functions and Firebase Hosting in the documentation.
It’s impossible to say without knowing your specific use case! It’ll take some benchmarking, cost estimation, and an understanding of your requirements in order to select the best option. It could also be a matter of preference, for example, if you’d rather hide the details of a series of database queries behind a more simple web API. If you’re looking for a conversation to help hash these things out, I’ll recommend posting to the Firebase Google Group firebase-talk or the Firebase subreddit. The Firebase community is very active and helpful!
Tutorials, deep-dives, and random musings from Firebase…
3.7K 
19
3.7K claps
3.7K 
19
Tutorials, deep-dives, and random musings from Firebase developers all around the world. Views expressed are those of the authors and don’t necessarily reflect those of Firebase or its parent companies.
Written by
firebase-consultant.com, Firebase GDE, engineer, developer advocate, Xoogler
Tutorials, deep-dives, and random musings from Firebase developers all around the world. Views expressed are those of the authors and don’t necessarily reflect those of Firebase or its parent companies.
"
https://medium.com/free-code-camp/how-i-designed-developed-and-deployed-a-chatbot-entirely-in-the-cloud-a60614eb94f2?source=search_post---------16,"There are currently no responses for this story.
Be the first to respond.
It all started with a YouTube video I recorded few months back. In it, I talked about the importance of deliberate revision. This helps you retain things in your mind for a longer period of time, and gives you techniques to revise important projects. If you haven’t, please watch it here.
In the video, I talked about how frequently you should revise, what the heck is the forgetting curve, and why you should care.
I wanted to give you guys a proper tool, in addition to the video, so that you can revise better. Being a developer, my natural response was “Let’s write an app!”
But if you’ve read my other article about why native apps are doomed, you know I was a bit reluctant to write a standalone app for this. I took a step back and analyzed the situation. I needed a back-end to store users’ data and a front-end to collect and show that data.
I wanted the user on-boarding to be as frictionless as possible. Forcing users to download a new app is hard. If I built a chatbot, it would serve that purpose and I wouldn’t have to convince anyone to download anything. I would also save some time since I wouldn’t have to build a standalone client app and go through app stores’ processes.
You can try the bot I built here.
Let’s cut to the chase and talk about the process. Read on to see how my chatbot went from an idea to a fully working product — entirely using cloud-based tools.
Natural Language Processing (NLP) and AI are integral parts of any smart chatbot. So, I knew from the start that I’d require AI and NLP to make my bot “smart” and something you could talk to. It should also understand what you are asking it to do. I come from a full stack development background and I have zero experience with Machine Learning, AI or NLP. But for this bot, all of these things were necessities.
Being a tech enthusiast, I always keep tabs on what tools and libraries the Biggies are launching. I was aware of Wit.ai, an online API, released by Facebook for enabling NLP in your apps and bots. I played around with it for a while but found it particularly hard.
I quickly searched for other alternatives and found Api.ai. I played around with it and found it more developer-friendly, so I went with it.
Here’s what exactly you do with these ai APIs:
Note: You can call on custom logic, which resides in your secure back-end, if API.ai’s built-in handlers can’t handle your use case. In the case of Revisebot, I was storing each user’s learning history and calculating what topics the user should revise next. This required custom calculations and persistence mechanisms.
Api.ai also offers some pre-built agents, such as small talk and weather agents, which can answer users’ queries about weather and other topics. These are plug-n-play things which you can readily use in your chatbots.
Since Revisebot needed to handle custom use cases, I had to write some code. Time to churn out some JavaScript/Node.js code. Yay!
I am a long time user of Digital Ocean, but it costs around $6/month at a minimum. Since I wasn’t hoping to make money off of Revisebot, hosting it on Digital Ocean didn’t make sense. I’d be losing money on a monthly basis.
I needed a free cloud host for this project. I knew Firebase offered free hosting (as I’ve used it in the past). I have also used Open Shift as well, for other projects (mostly Laravel). But I thought it would be a great idea to Google some other alternatives, at least for the sake of Node.js.
That’s when I came across Heroku and its free plan.
In no time, I learned that Heroku’s Node.js integration is awesome. So I read their docs and quickly spun up a Node.js app on their free dynamo. It was enough for my needs. Its only limitation was that it sleeps after a while, so the first API call might fail while the dynamo is waking up from sleep. But I adapted my chatbot to respond to such scenarios.
I had been contemplating learning some MongoDB. So I decided to use MongoDB as the database for my chatbot. A chat app is a good use case for MongoDB’s document-based storage system.
My plan ran into a little roadblock when I discovered that Heroku does not offer MongoDB integration for free. No worries — I went back to my friend Google and searched for a “Free MongoDB cloud.”
That’s how I came to know about mLabs, which offers free MongoDB instances in the cloud.
Their free plan is not recommended for production ready apps, but that’s OK. I’m gonna run my chatbot on the free plan anyway.
My plan was to code the entire thing up in whatever free time I had after my full time job. Because of this, I needed the flexibility of coding from anywhere. So my developer workspace needed to reside in the cloud, which I could load up from anywhere I had internet.
I’ve been using cloud-based IDEs for quite a while and the experience is mixed. Nitrous.io was awesome but they shut it down. :( After trying some online IDEs like cloud9 and codeanywhere, the one that I found most stable and developer-friendly was Codenvy. It offers workspaces which you can create or destroy at your own will.
So I created a new Ubuntu-based workspace in Codenvy and installed node, npm, git and curl right away. Codenvy offers a terminal as well, so Linux users feel right at home. My developer workspace in the cloud was all set.
Next, I git-cloned my project’s repository from Heroku, and set up the DB integration with mLab’s MongoDB instance using .env files. As you can see in the screenshot below, blooming-escarpment-58368 was my Heroku Node.js project.
The chatbot was supposed to work with Facebook Messenger and Slack. I would have to learn the developer APIs for both platforms and set up my development machine for testing the API calls. Luckily, Api.ai also offers easy one-click integration with most of the social media platforms. You just have to follow their documentation to bring your chatbot to the specified platform.
As you can see in the screenshot above, I’ve integrated Revisebot with Facebook Messenger and Slack, as of now. This step won’t take long, believe me.
Using these tools, I was able to write, test and deploy the entire ecosystem of my chatbot (the DB, the application layer, the front-end and the AI agent) to react to users’ queries.
But there were still some pieces left in order to make Revisebot a complete, finished product.
Although I was the only developer working on this chatbot, I needed to store the code somewhere safe. Git was an obvious choice for source code and version control management, but GitHub does not offer a free, private repository. Revisebot was not supposed to be an open-source venture, so I could not host the source code there. Additionally, as I was not using a local development machine, I couldn’t use any local git repo to store my code on.
Back in the day, I played around with bitbucket.org. I had some idea that they offered a free private repository, but wasn’t sure if they still offered any such plans. I went to their site and found that they did. The rest is pretty self-explanatory.
Design and graphics sit at the core of any digital product. I needed a logo, background images, and cover images for my chatbot’s Facebook page, Slack app store listing, and homepage.
I am not a designer by any means, so I needed some help. I had to choose the color palette and icons, mix shapes together to create a logo, and more.
Luckily, there is a helpful tool for this called Canva.
It offers ready-made design templates for social media, YouTube, and logos which you can customize according to your needs. I created Revisebot’s logo, entirely in Canva, using free shapes and some creativity. I think I did fine.
I also used some of their free templates to create other visual assets for Revisebot like a Facebook cover image.
So that’s how I coded and deployed a fully working chatbot, which can help you schedule your revision, entirely in the cloud.
It costs me exactly $0 to run this service.
Let me know if you have any questions regarding my project.
*No local machines were engaged in the making of this chatbot.
If you liked this post, kindly give me some claps and follow me for more posts like this one. You should also subscribe to my YouTube channel, if you like developing digital things.
Till next time…
We’ve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
2.8K 
16
2.8K claps
2.8K 
16
We’ve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
Written by
Software Engineer; Builds Internet Based Businesses & Apps; Solopreneur
We’ve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
"
https://medium.com/google-developers/whats-the-relationship-between-firebase-and-google-cloud-57e268a7ff6f?source=search_post---------17,"There are currently no responses for this story.
Be the first to respond.
Top highlight
If you’re a mobile app developer, I imagine there’s a good chance that you know something about Firebase, Google’s mobile application development platform. Or, if you’re an enterprise systems developer, you might know something about Google Cloud Platform (GCP), a broad suite of products and services that host your data and code at planetary scale, and more. While both platforms can be used without knowledge of the other, there are some ways in which they overlap. Knowledge of this relationship is important in two circumstances:
Even if these situations don’t (yet!) apply to you, understanding how the tools and services are related should help reduce the friction you might encounter with some tasks, especially regarding your Firebase app.
So, what do you need to know? Let’s start with the most important detail.
When you go to create a Firebase project, this fact is mostly hidden. To get started with Firebase, it’s simply not necessary to know anything about GCP. The onboarding path is optimized to get you to a working solution with minimal effort. I know that many developers appreciate this!
Here’s a screenshot of the Firebase console after you’ve created a new project:
You can see a scrollable list of products on the left, organized by top-level categories that expand and collapse. In the middle, you have some buttons that help you get started adding your app to the project. It’s pretty clear what you’re expected to do next. Later, after you add an app and start using some of the products, the main area changes into a dashboard that shows you some stats on the products you use.
By contrast, here’s a screenshot of the same newly-created project in the Google Cloud console:
The look and feel is almost completely different. Here, I’ve also opened the hamburger menu in the upper top left, and it’s hovering over the main content. This menu also scrolls, and there are a LOT more products and options here than you see in the Firebase console.
You can think of a GCP project as a virtual container for data, code, configuration, and services, regardless of how it was created. When you create a Firebase project, you are actually creating a Google Cloud project behind the scenes. This means that you can view and manage many aspects of your Firebase project in the Cloud console.
In some cases, the Firebase console actually delegates to the Cloud console in order to handle some common tasks, such as billing management and administrative user management (known as Identity and Access Management, or IAM in the Cloud console). So if you’re working with the Firebase console, then somehow click through to something with a blue and white UI theme, you’ve just been sent to the Cloud console. Same project, different console and UI. Here’s what it looks like if you try to click on a billing account from the Firebase console. It’s definitely the Cloud console, even if it says “Firebase” at the top:
There’s one super-important thing to know about these project containers. Since the underlying project is the same for both Firebase and GCP, if you delete the project using either Firebase or the Cloud console, you will delete everything in that container, no matter where it was configured or created. So, if you created a project with the Cloud console, then add Firebase to it, then delete the project in the Firebase console, all your Cloud data will also be deleted.
Now let’s imagine, instead, that you’ve created a project in the Cloud console. At the outset, your project won’t have anything directly related to Firebase configured in it. After all, the Cloud console doesn’t know if you intend to build a mobile app, so why set that up? But if you have an existing Cloud project, you can very easily add Firebase to it.
To add Firebase services to an existing project, go to the Firebase console, click the “add” button. When it asks for your project name, you have the opportunity to choose an existing project from the dropdown that shows your existing projects that don’t have Firebase added.
When you select a project and proceed from this point, all the APIs and services that power Firebase products will be automatically enabled in your project, and you’ll be able to use the Firebase console to work with those products.
If you’re wondering what exactly I mean by “APIs and services”, this is a GCP concept that’s only visible in the Cloud console. Here’s a screenshot of the APIs and services dashboard from the Cloud console after Firebase has been added to a project:
Here, you can see a number of APIs (enabled by default), along with some Firebase product APIs highlighted in the red box. This detail of enabled APIs is hidden from developers in the Firebase console, because it’s not really necessary to know. However, knowledge of GCP APIs and services gains importance as an app’s backend becomes more sophisticated. For example, an app developer might want to make use of the Cloud Vision API to extract text from images captured by the device camera. And then, go further and translate the text discovered in that image using the Cloud Translation API. To use these APIs (and get billed for them), you have to enable them in the Cloud console. Once enabled, you can call them from your backend code (deployed to Cloud Functions, for example).
As you dig around in each console, one thing you might notice is that the set of products you can manage in the Firebase console has three items in common with the set of products in the Cloud console. These products are Cloud Storage, Cloud Firestore, and Cloud Functions. While each product is the same at its core, regardless of where you’re viewing it, they are each organized and managed in very different ways between the Firebase console and the Cloud console. This leads me to my next point.
As you might guess from their names, Cloud Storage, Cloud Firestore, and Cloud Functions are Google Cloud products. Technically, they are not Firebase products, even though you can work with them in the Firebase console and manipulate them in your app using Firebase SDKs and tools. First, some quick definitions:
Without Firebase in the picture, these Cloud products are typically used in enterprise environments, where data and processes are mostly controlled within Google Cloud, or some other backend. To work with these products programmatically, Google Cloud provides client APIs meant for backend code, along with the command line tools gcloud and gsutil.
With Firebase in the picture, these three products are enabled to work seamlessly with mobile apps by providing additional SDKs for mobile clients, additional tooling with the Firebase CLI, and a way to configure security rules to control access to data through the provided SDKs. I’ll talk about some of the specifics of these Firebase additions in future posts.
(Since I mentioned Cloud IAM earlier, I should also mention that Firebase offers additional IAM roles for some Firebase products that give other members of your team granular access to those products, without the risk of them making a dangerous change elsewhere in your project.)
Note that the names of these three Cloud products don’t change from a Firebase perspective. I know it’s tempting (and natural!) to say things like “Firebase Storage” and “Firebase Functions”, but these names aren’t accurate. Am I being pedantic about this? Perhaps, but you won’t find these names anywhere in formal documentation! However, you will see names like “Cloud Storage for Firebase” and “Cloud Functions for Firebase” when dealing with the Firebase add-ons for these Cloud products.
If you’re a Firebase app developer, you probably created your project in the Firebase console. But, at some point, you might need to jump over to the Cloud console for some administrative tasks, to expand your cloud infrastructure, or make use of Cloud APIs. The Firebase console is just the beginning to build out the infrastructure of your mobile app.
If you’re a Cloud infrastructure developer, and you want to build mobile or web apps against the data you’ve already stored, you’ll need to jump into the Firebase console to deal with configurations and tasks that are unique to the Firebase additions to some Cloud products.
In fact, Actions on Google projects are also GCP projects (if you’re working with DialogFlow). These projects have Firebase enabled by default, so that’s another way you could end up with a new perspective on a GCP project. In any case, no matter how your project came into existence, the console you started with might not end up being the only console you use. Thinking of a project primarily as a container for services and APIs makes this transition easier. Each console is just giving you a view of those services and APIs in a different way.
Read more about the differences between Firebase and Google Cloud with respect to these products:
Engineering and technology articles for developers, written…
3.2K 
15
3.2K claps
3.2K 
15
Written by
firebase-consultant.com, Firebase GDE, engineer, developer advocate, Xoogler
Engineering and technology articles for developers, written and curated by Googlers. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
firebase-consultant.com, Firebase GDE, engineer, developer advocate, Xoogler
Engineering and technology articles for developers, written and curated by Googlers. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/predict/cloud-cities-of-venus-367710d1e26c?source=search_post---------18,"There are currently no responses for this story.
Be the first to respond.
Top highlight
Before we set our sights on the chalky red horizons of Mars, there was Venus. Named after the goddess of love and beauty, Venus is by no means a meek and pleasant planet. Beneath its wispy gold surface are rocky riverbeds and hellish, deserted plains. Acidic clouds roam over the valleys where oceans of carbon dioxide could have existed in the past, the immense pressure causing the gas to go into a liquid supercritical state. Its thick atmosphere has caused a runaway greenhouse effect with temperatures as high as 863 degrees Fahrenheit (462 degrees Celsius). If it’s described as Earth’s sister, it’s certainly not for its ability to host life. Instead the planet is a barren and suffocating landscape appearing as one of the brightest objects in our night sky.
And yet, Venus is in many ways a better option than Mars.
Up until about 60 years ago, we thought our future colonies would be built on Venus. It was close to the sun and had what appeared to be welcoming clouds in its sky. Similar in size and mass to Earth, scientists even thought it might already be home to alien life forms. It wasn’t until we sent probes that we realized just how hostile the planet really was. In fact, many of the first probes imploded in mid-air as they were subject to an atmospheric pressure 90 times higher than here on Earth. Later probes were reinforced and able to land on the Venusian surface, but even then they only lasted 2 hours before malfunctioning. We began to look for a better prospect.
But the problem with this thinking is that it only takes into consideration the planet’s surface. 31 miles (50 km) above, Venus has the most Earth-like conditions in the entire solar system. The pressure and temperature drops dramatically and the gravity is 90% of Earth’s or .9g. On Mars, gravity is a dangerous .4g, leading to a lot of complications for human settlers. Low gravity decreases muscle strength, affects coordination, and causes mineral loss in bones. Astronauts can experience drops in bone density of up to 1% per month. Even advanced cases of bone loss for elders here on Earth only see drops of 1% per year. Sometimes this bone loss can be irreversible. Not only that, in low gravity the cardiovascular muscles start to weaken and fluids shift towards your head where they can cause vision problems.
Venus’s proximity is another advantage. It’s our closest neighboring planet and a roundtrip here could take half as long as one to Mars. This would not only cut costs but also lower exposure to radiation during travel. On the planet, Venus’s thick atmosphere is great at protecting from radiation all around, whereas this is another major concern on Mars whose thin atmosphere and lack of magnetic field would expose humans to extremely high levels of radiation.
While plans to colonize Mars involve protective domes, the colonies of Venus would be cloud cities. Floating structures and blimps made of light yet durable material (graphene, for example) would allow the cities to be mobile and resilient amidst Venus’s harsh winds. The blimps would be tiled overhead with solar panels that could collect energy. Because the planet is made almost entirely of CO2, any number of gasses could be used for flotation, including oxygen and nitrogen which are safe for human beings. That is to say, a blimp filled with regular Earth air would float well in the super-dense atmosphere. The sulfuric clouds contain hydrogen and water for crop irrigation and special suits would protect against the veils of acid.
Researchers estimate that cloud cities would, in many respects, be easier to set up than their Martian counterparts. Less material, more mobility, and great access to solar power means maintenance would be easier too, with Venus getting 4 times as much solar energy as Mars.
This potential spurred NASA’s HAVOC — or High Altitude Venus Operational Concept — which outlines the steps to colonizing the warm yellow skies. The first step would be a robotic test mission using a powerful rocket such as the Space Launch System currently in development. Two spacecraft, one without a crew and one with two people onboard, would take 100 days to arrive on Venus where the unmanned craft would promptly settle among the clouds and make sure the technology was safe and working appropriately. It would then be joined by the crew which would stay there for a full month before deploying a smaller capsule and coming back home. That is, of course, the downside to having a gravity similar to Earth’s: rockets must be powerful enough to break out of Venus’s hold. If the mission went well, a group of people would then be sent to live on our neighboring planet for a year. Altogether the mission would only take 450 days whereas a mission to Mars would last around 2 years.
Despite what it is now, astrobiologists propose that Venus might have been the first habitable planet in the solar system. According to the most recent climate models, the planet should have been capable of hosting life for over half of its existence — meaning that if we don’t find traces of past organisms on the planet, it would make life on Earth an even bigger and more mysterious question. But it’s possible that there exist organisms there even today, living among the sulfuric clouds we would colonize. Microbes here on Earth have been known to live in acidic, extreme conditions not entirely different from those on Venus.
Why then this focus on the red planet when it appears to have more disadvantages than the nearby Venus?
There is this idea that Mars won out over Venus for one simple reason: a surface. From our days exploring continents and planting flags onto newfound shores, humankind has seen surfaces as a marker of innovation and discovery. We pursued a surface even in our venture to the moon with the iconic images of the American flag on lunar soil or the footprint left by the astronauts we greeted and cheered for along the way. Making history wouldn’t quite feel the same if we couldn’t touch the face of our home away from home. And should that be enough to leave cloud cities as a figment of our science fiction?
where the future is written
4.4K 
30
Science and technology shaping our future. Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
4.4K claps
4.4K 
30
Written by
Astrophysics student, writer for over a decade. A passion for language and the unexplored universe. I aim to marry poetry and science. ella.aldrsn@gmail.com
where the future is written
Written by
Astrophysics student, writer for over a decade. A passion for language and the unexplored universe. I aim to marry poetry and science. ella.aldrsn@gmail.com
where the future is written
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/thinking-design/10-ux-design-predictions-for-2018-54bebb8d9767?source=search_post---------19,"There are currently no responses for this story.
Be the first to respond.
Top highlight
We’re living in a time when design and user experience have never mattered more. The past decade of change shows us that design must constantly adapt as a discipline in order to meet user needs.While it’s impossible to see the future, we can still make some educated guesses about it. In this article, I go through some of the trends that will shape UX design in 2018 and, possibly, for several more years to come.
Recent trends like minimalism and flat design focus on one thing. They remove the distraction from what’s really important: content. Content-focused experiences are experiences in which content shapes design (or design emphasizes content). A designer’s mission is to make sure that nothing impedes a viewer’s experience of the content. In an attempt to follow this trend, designers focus on the following visual aspects of design:
Medium is a prime example of the trend for content-focused experiences
A content-focused experience isn’t limited to the visual part of design; the content itself has to be more helpful and engaging. That’s why, in 2018, we’ll see a growing importance of content strategy. Well-curated content will be a huge part of a brand’s success in digital products.
Today’s most successful websites and apps provide much more than relevant information in an easy-to-consume format. They create an intuitive experience that reduces friction and saves user’s time.Time is of the essence for success in both mobile and desktop environments. Users want the products that help them reach their goal as fast as possible and time-saving designs are key in helping them with that. Time-saving design allows users to take a limited number of steps from the moment they install an app/visit a site until the moment they take an action.This design has following characteristics:
These elements represent only a portion of possible design features that can save a user time.
Time-saving design is all about designing in the interest of saving time. Airbnb is a great example of such straight-to-the-point design.
While personalization is related to time-saving design, it deserves its own part of the list. We’re moving from one-size-fits-all experiences towards individually tailored experiences where technology is adapted to people. With the power of personalized data and significant progress in AI and machine learning, we’re leaning towards systems that are capable of changing user experiences based on users themselves.In an attempt to get to know users on a deeper level, businesses will continue to search for new ways to offer a more personalized brand experience. This will move personalization to a whole new level.
One of the most common examples of personalization in modern apps/websites is personalized suggestions based on user actions. They increase the likelihood that a consumer will take action during any given site visit.
The trend of focusing exclusively on the mobile audience should be re-evaluated. The growing number of connected devices will push the industry to create more dynamic and continuous digital experiences. With the rise of Internet of Things (IoT) devices, we’re moving from mobile-first experiences towards omnichannel experiences.The root of this trend lies in the nature of user interaction. To a user, it doesn’t matter where or how an interaction occurs. In fact, the interaction itself is largely invisible because a user has a goal she or he wants to achieve using the most relevant medium. That’s why a true user experience is device-agnostic. Users need experiences that match a context, defined by a user’s current device. Omnichannel UX brings consistency to a multi-device world. A designer who wants to create an omnichannel UX should create a seamless flow for the user’s journey by making it possible for the users to transition smoothly between devices when they use the product.We already have a few good examples of omnichannel UX among today’s apps. Uber is one of them. Requesting a ride from Uber can start on an Amazon Echo and end on an iPhone. That’s a good example of an experience that crosses two platforms, from two different vendors, with two different interfaces, yet is seamless to the user.
Uber’s omnichannel UX. Image credits: Businessinsider
In 2018 we won’t necessarily be designing whole ecosystems, but we’ll pay more attention to the ways people transition from one touchpoint to the other. As users, we’ll see more digital experiences that are both dynamic and continuous.
People expect to interact with digital products the way they normally would with each other, and designers should prepare digital products to deliver on those expectations.
The trend of humanizing digital experiences is directly related to user emotions. The way a user feels about an interaction with a product has a great impact on whether or not they’ll use the product on a long-term basis.
Designers can focus on humanizing digital interactions by focusing on satisfying fundamental human needs (such as trust, transparency, and security):
The checkout flow in the Stripe app. The app uses animation to reassure the user: when the user clicks “Pay,” a spinner briefly appears before a user sees the success state. The checkmark animation encourages the user to feel like they easily completed the purchase. Image credits: Michaël Villar
As well as delighting users:
The trend of humanizing digital experiences will result in more demand for psychologists, UX researchers, and other specialists who will help create more human experiences.
In 2018 we’ll continue moving away from designing for clicks and taps towards the domain of screenless experiences–a domain of Voice User Interfaces (VUIs). VUIs are already being deployed in a range of technologies: Apple has Siri, Google has OK Google, Microsoft has Cortana, and Amazon has Alexa. The rapid development of voice interaction capabilities in our daily lives makes it clear this technology will soon become either an alternative or even a full replacement to traditional graphical user interfaces (GUIs). According to Gartner, by 2018, 30 percent of our interactions with technology will happen through conversations with voice-based systems. At the same time, VUIs will likely to continue living alongside GUIs in 2018. There are two reasons for this. Firstly, it’s not always appropriate to use voice input (e.g. giving voice commands to devices in crowded place is socially unacceptable). Secondly, there is still a lot of improvement to be done in the conversational systems itself: VUIs require a better understanding of human’s conversation flow–not only the topics people talk about but how they talk about them.
2018 will be the year we see how biometrics help make tasks like authentication and identity management easier for both businesses and end-users. Many progressive apps and services won’t require users to create and remember passwords, but will instead use biometric authentication methods for this purpose. As effortless authentication becomes a user expectation, other companies will overhaul the UI design of the authentication process.
With the facial recognition in Apple’s Face ID, your face becomes a password
AR will break through into mainstream culture. At the recent Facebook F8 conference for developers, Mark Zuckerberg claimed that soon all screens will be replaced by lenses for the ultimate AR experience. Considering the efforts of Google, Apple, Facebook, and Microsoft in this field, it sounds like a realistic possibility in the next few years. While the dominance of mobile phones won’t change anytime soon, we’ll see dramatic growth in augmented reality apps for mobile devices. There are three elements in mobile technology which will have a significant impact on mobile AR, and these are faster processors, higher quality displays, and better cameras.In the upcoming year, we’ll start to see more useful applications in augmented reality, like ones that aid translation between cultures and languages:
Or ones that provide helpful details during complex procedures:
Of course, there will be also a lot of apps for pure entertainment:
One of the most significant changes in the future in the way we use our apps will be the emergence of virtual reality technology. However, there are a lot of challenges that must be addressed before VR becomes a standard way of interaction.2018 won’t be a year of VR, yet; while some of us will work on VR products, VR itself won’t’ be a mass-market effect. In 2018 we will see more progress in defining ‘best design’ practices for VR. Gestures will be in focus–natural gestures with similar meanings in the real world will help with translating actions in the virtual space.
UX has always been a broad category but, in 2018, a designer’s specialization will trend towards new technologies (like augmented reality, artificial intelligence, and virtual reality). The role of a UX designer is about to expand again.At the same time, being a UX designer in 2018 will be less about ‘doing all of the stuff yourself,’ and more about ‘connecting people together.’ This will put the focus on collaboration, fast prototyping, and automation in some steps of the UX process that previously were manual (such as the design handoff).
An emerging set of new tools like Adobe XD will significantly improve the workflow for designers.
As with trends of any nature, some come and go, whereas others stay the course and become fundamentals. But even though tools and ideas change and our understanding of design transforms, the mission of a designer stays the same: design exists to make people’s lives better.
Nick Babich is a developer, tech enthusiast, and UX lover. He has spent the last 10 years working in the software industry with a specialized focus on development. He counts advertising, psychology, and cinema among his myriad interests.
Originally published at blogs.adobe.com.
Learn about Adobe XD, our all-in-one design and prototyping tool:
Stories and insights from the design community.
3.1K 
6
3.1K claps
3.1K 
6
Written by
New Tools for New Creatives. Get all the latest creative apps plus seamless ways to share and collaborate. All right on your desktop.
Stories and insights from the design community.
Written by
New Tools for New Creatives. Get all the latest creative apps plus seamless ways to share and collaborate. All right on your desktop.
Stories and insights from the design community.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://blog.sketchapp.com/sketch-39-brings-symbol-resizing-and-cloud-beta-a74d3aa0611a?source=search_post---------20,"There are no upcoming events.
Want to host your own event? Get in touch.

        We’re thrilled to introduce Sketch 39. This update brings Smart Resizing, so you can experiment with flexible layouts in your Canvas. We’re also introducing Sketch Cloud, letting you share your Sketch documents online.
      
Adapting the same design for different device sizes just got easier. You can now select a layer, and use a dropdown in the Inspector to set how it should behave when its container is resized.
These are the four resizing rules you can apply:
Furthermore, you can now resize individual Symbol instances and apply the smart rules to layers inside the Symbol. Now you no longer need to create multiple Symbols when designing for different screen sizes. To see it in action, download the 39 update and use elements from our iOS UI design template (File › New From Template › iOS UI Design). To learn more about this new feature, read more in our Documentation.
Sketch 39 introduces the first glimpse into an exciting feature — Sketch Cloud, a service that allows you to share your designs from Sketch to the world. We have many plans for its future, and we’re excited to hear your feedback.
Sketch Cloud is currently in beta and we’ll be rolling out access gradually over the coming weeks. You can sign up for our waiting list now (via the Cloud panel in Sketch’s preferences), and you’ll receive sign in details as we roll it out. Once signed in, you will be able to upload your document via the Cloud Toolbar item and receive a link you can share. In addition, you’ll be able to manage your shared documents. Please note that in the beta, uploaded documents will be viewable for 30 days.
We also made a few other improvements in response to your feedback. Some highlights include:
As always, you can check out the complete list of updates.
We hope you’ll download this free update, and see what’s new for yourself! Sketch 39 also introduces our new system for numbering updates, and follows on from Sketch 3.8.3. Read more about these changes in our previous blog post. Please note Sketch 39 requires Mac OS 10.10 or higher to run.
Don’t hesitate to reach out to us with any feedback, feature requests, or bug reports via our support page. You can also join the conversation in our Facebook group or Twitter.
Let’s continue to make Sketch better, together!
Turn your ideas into incredible products with a 30-day trial.
A monthly digest of the latest Sketch news, articles, and resources.
©
        2022
        Sketch B.V.
"
https://towardsdatascience.com/scalable-efficient-big-data-analytics-machine-learning-pipeline-architecture-on-cloud-4d59efc092b5?source=search_post---------21,"Sign in
There are currently no responses for this story.
Be the first to respond.
Satish Chandra Gupta
Mar 4, 2020·9 min read
"
https://debugger.medium.com/google-has-finally-convinced-me-to-create-my-own-cloud-85a80b0e8f40?source=search_post---------22,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jason Howell
Dec 10, 2020·6 min read
As a superuser of Google goods and services, the last few months, like 2020 in general, has had me rethink how I approach my digital life. Of all the things I use my smartphones for, music and photos are near the top of that list. I’ve had a system that works, and recent changes by Google have forced me to reevaluate…
"
https://faun.pub/scaling-applications-in-the-cloud-52bb6dfbac4e?source=search_post---------23,"Scalability is an important part when architecting a web app. There are multiple options on how to scale the web app tier and the database tier. Those options will be explained with examples from services from Microsoft Azure. If you are beginner and want to understand the fundamentals of scalability and resiliency, then this article is for you.
Let’s imagine you have a business web app hosted on a VM. At first, your website gets some ten requests per second. But now, after you launched a new cool product or service, it is getting multiple thousands of requests per second. The VM will receive all the load, in certain point it will reject requests and become slow if not down, that is bad news for your growing business! How to solve it? You might say: I need a more powerful VM! Well, that is called Vertical scaling.
When the 8GB RAM, I3 processor and HDD disk are not enough anymore, then you spin up another VM. The new one have 512 GB RAM, Xeon processor and the latest SSD disk. This is Scale Up. It is the easiest and fastest way to scale a web app. It requires only moving the web app content to the bigger new VM, without changing the source code. Azure provides VMs up to 448GB dedicated RAM. (more)
When moving to another VM, the app might be down for a certain time and might lose user’s HTTP sessions and Cookies.
If even with this monster VM, the app cannot handle all the load, then Scale Up reaches its limits because VMs at the end cannot have unlimited RAM and CPU. So, what is if we can share the load on multiple VM instances? Well, that is Scale Out.
While Scale Up focuses on making the single machine bigger, Scale Out is creating multiple ones. This way we can have much more RAM and CPU, not on one single VM, but on a cluster of VMs. The same solution used to get more computation power with processors, when moved from one single to multiple processors/threads.
Scale Out requires re-thinking about the architecture of the app and in some scenarios changing the source code.
This approach needs a solution to choose to which VM instance to send the user or HTTP request to. Well, that is the Load Balancer or the Traffic Manager.
As the meaning of their names, Load Balancer and Traffic Manger are used to distribute network traffic load between multiple instances so that no one will fail.
They use multiple algorithms, these are some of them:
Round Robin: rotating sequential manner.
Least Connection: the current request goes to the server that is servicing the least number of active sessions at the current time.
Chained Failover: redirects to the next server only if the previous one cannot accept any further requests.
Weighted Response Time: redirects to the server with the current fastest response.
With this solution, you can go to up to 20 VM instances in Azure. But what if the request takes too long because it tries to access images, videos, Html or any static content on a server that is too far away from the user? The solution here is to use CDN.
CDN is used to reduce the latency for getting static content from the server to the user’s location. This latency is mainly caused by 2 reasons. The first reason is the physical distance between the user and the server. CDNs are located in multiple locations called Point-of-Presence (POP) around the world. So that it is possible to get one that is closer to the user location than your servers are. The second is accessing the file on the disk, so CDN might use a combination of HDD, SSD or even RAM to cache these data, depending on the frequency of data access frequency. A time-to-live (TTL) can be applied to the cache to say it should expire at a certain time.
CDN caches static content files. But, what if you need to cache some dynamic data? This can be solved by using Cache.
When lots of SQL requests to the database gives the same result, then it is better to cache this data in memory to ensure faster data access and reduce the load on the database. The typical case is the top 10 products displayed on the home page for all users. Because it uses RAM memory and not disks, it can save as much data as the RAM do. Data is stored as key-value pairs. Cache can be distributed across multiple regions.
Azure Redis Cache can save up to 530 GB. Azure doesn’t provide a service for Memcached.
In case of underlying infrastructure issues there can be potential data loss. Redis Cache provide a solution for data persistence, while MemCached doesn’t.
When users search for a certain information in the web site, a request will hit the server to get the data from the database. Most requests are similar in their form. So why not caching those results? Well, here where ElasticSearch comes to play.
Elasticsearch can be used to store a predefined search queries and their results. Because it saves data in-memory, it is so much faster than querying the data from database. It also can provide a near real-time search-as-you-type suggestions. This reduces the load on the database, which itself reduces the load on the server as the request will be processed in less time. ElasticSearch is accessible via REST API and requires changing the app’s source code to use it.
Elasticsearch could also be used for log analytics and real-time application monitoring.
When creating multiple copies of the server hosting the hole monolith web application is still not enough or not efficient to handle all users, then let’s think about splitting the application into two parts: web app and web API.
Monolith web apps are typically composed of 2 tiers: Web API and frontend web app. This is the case for ASP.NET MVC apps where the views and business logic live together in one server. In this case, the server will not only process the request to get data to the user but it also renders web pages to generate HTML content. The second role could be done by the client using SPA (Single Page Application) approach. So, the Views part of the app will be moved on a separate server. Consequently, two servers instead of one are now serving users.
This approach requires rewriting the app.
Splitting the application into 2 parts is not enough? What about splitting it into multiple parts? Well, this is where microservices comes to play.
Microservices are an approach to developing a single application as a suite of small services, instead of one single giant app. Each small service runs in its own process, instead of relying on one single process to run the entire app. The typical example is the e-commerce app split into microservice for payment, one for front end user interface, one for email notifications, one for managing comments and another one for recommendations.
Think about it as applying the separation of concerns and the single responsibility principle in OOP not only to the class level but also to the different components.
With this way, it is possible to scale out only and exactly the specific part of the application that receives more load. So, you can have 5 instances running the front end microservice and 2 instances running the payment microservice.
These microservices communicate with lightweight mechanisms, often an HTTP resource API. Because they are small parts, they don’t need the entire VM instance to run. Instead, they can run on a Container. A container is another form of OS virtualisation. Unlike the VM, it just has the minimum resources needed to run the app. This results in a light weight image. Hence, it is more scalable than VMs.
Those multiple containers can be managed by an orchestrator like Kubernetes or Swarm.
Going from monolithic to microservices requires lots of change in the app source code. It is not unusual to be required to rewrite the entire app from scratch.
Azure supports Docker containers and also container orchestrators like Kubernetes, Service Fabric and Swarm. Learn more about AKS.
With microservices, the monolithic app is split into small pieces based on business domains. Can we go deeper more than that to split the app to smaller pieces? Well, Serverless apps makes it possible.
Serverless app is a small piece of the app hosted on its own instance. This instance is managed for you, so you don’t need to take care of any container or VM. It can scale out automatically depending on the load. Typically, you can use it for resizing or processing images, starting a job on a database etc., anything that is more often independent from the business logic.
Now that we have many tiers running, each one on a different endpoint, the client app will ask: where do I go? Well, API Management is designed to help in this case.
As Load Balancer distributes load on VMs, API Management can distribute load on different API endpoints or microservices. The distribution mechanism can take into account the load on each endpoint.
We split the monolith app into multiple small modules. These modules need to communicate with each other. They obviously can use REST web services. Some of these communications doesn’t need to be synchronous. So why keep waiting for a response if it is not needed right now! Well, here Queues comes to play.
Queues provides an asynchronous solution to communication between software components. When using REST web services for communication, the requested server must be available at that time or the app will fail. While with Queues, if the server is not available, then it doesn’t matter. The request can wait in the Queue to be processed when the server will be available later. This approach help to decouple the different components and makes them easily scalable and resilient.
There are some other techniques to reduce the load on the server which doesn’t need using cloud services. Other than the server, who else can do data processing? Well, the front-end website might be a good candidate in some scenarios.
Many tasks could be processed without the need for the database or the server. For example, if the front-end web site can resize images instantly and effectively, then why bothering the server? For several years ago, the server always tried to do all the heavy work for the client because the latter is not powerful enough. This fact has changed now: 4GB Ram with 8 cores processor is not a surprising spec for a mobile device.
Another solution to not bother the server is to not send the same request more than once if we are sure we’ll get the same response! Well, caching the HTTP responses on the client is a good practice.
The browser can intelligently cache the HTTP requests and their responses if the web app wants to. In addition to that it can provide a TTL (Time-To-Live) for each stored data. This way the web app will reach the server only for the first time, then the next time it will get the response from the cache. This not only reduces the load on the server, but also makes the client app more responsive. This approach is relatively easy to be implemented as it only requires adding HTTP Headers to the requests. The browser will interpret those and take the responsibility to either return data from its own cache or routing the request to the server.
Until now, we have explained the options for scaling a web app. But, almost all apps connect to a database server. When the app gets more load, it typically affects the database. Like the web app, the database cannot receive infinite requests/queries per second. Not only that, the SQL database instance have a maximum amount of data to save. Hence, it needs to scale. So how to scale a database?
Most of the principles we used for scaling a web app are applied for scaling a database. Those are all about vertical and horizontal scaling, caching and replicates. We’ll explain them one by one and start with the easiest option to the more complex.
The easiest option to enhance database response time is to use a built-in feature added to do exactly that. Well, I’m talking about caching data.
SQL databases have a good feature for caching data: buffer cache. It allows caching the most frequent queries in-memory. As a result, access to data is faster.
Caching queries is limited by the size of the available memory. And when lots of queries cannot benefit from the cache and needs to query the big tables, then we need an option to retrieve data as fast as possible. Well, indexes are made for that!
Relying on the ID to retrieve data from the tables requires looping through almost all the rows. That is because the ID is a non-ordered primary key. The customer with ID equal to 5 could be on the row number 7. With using index, it’ll be on the 5th row. As a result, there’s no need to loop the entire table if we know where exactly we can find it.
Even with indexes, you might still below the required response time. The cause might be non-optimized SQL queries. Well, we need to optimize the queries!
Performance testing reveals that Stored Procedures are faster than queries coming from source code or ORM because they are pre-compiled in the database. There’s a tradeoff here as in the other hand ORM are easier to deal with, can handle multiple databases and even can optimize the SQL query. Nothing keeps us from mixing both to get the best out from the two.
If the limits of a one single database are reached, then why not creating multiple instances. The duplication of can be based on multiple criteria. Well, let’s start by the one based on Read or Read-Write operations.
SQL queries are either reading or writing data. From here, engineers asked: why not to write to a database and read from another one? This way they can reduce the load to a half by balancing it on 2 databases instead of only one. The RW DB will take the responsibility to update the Read DB so that they have almost the same exact data.
With the web app, when we wanted to go further more than duplicating instances, we split the app into small components. Can the same logic be applied to database? Another criterion for replicating DB is based on splitting the table itself. This is called Partitioning or Sharding. Well, let’s start by Partitioning.
Partitioning divides a table into multiple tables with fewer columns. The customer table with 20 columns will be split into 2 or more tables. The first table will have columns from 1 to 7 and the second one will have the columns from 8 to 20. Of course, each table contains the primary-key to join data from both tables. This is useful when usually only the first 7 columns are needed. So, it takes less time run as it’ll bring less data. Those 2 tables could be on the same database or in 2 separate ones. SQL databases support this case by providing features to recognize from which partition to get the data.
Vertical partitioning should be considered carefully, because analysing data from multiple partitions requires queries that join the tables. That is the case where we need all the 20 columns.
We split a table vertically by splitting the columns. Can we split it based on the rows? Well, that is called Sharding.
Sharding divides a table into multiple tables. Each table then contains the same number of columns, but fewer rows. For example, the customer table can be partitioned to 5 smaller tables each representing the continent for a group of customers. Knowing the customer location will help to redirect the query to right partition to process less rows. The smaller tables can live in the same SQL instance or in separate ones, the same as with Partitioning. Horizontal and Vertical partitioning can be mixed together.
Tables should be partitioned so that queries reference as few tables as possible. Otherwise, excessive UNION queries, used to merge the tables logically at query time, can affect performance.
Until now, we have seen splitting tables based on their columns’ or rows’. But, can we split tables into group of tables? Well, let’s see DDD.
We have split the web app into smaller microservices based on the context/domain. The same thing could be applied to DDD. So that each domain has its own set of tables: tables for payment, comments, etc.
Imagine that now each microservice have its own domain tables living in the same container!
The objective of DDD is not to scale the database. But that is a consequence of its application. And if it is not built from the design phase of the project, it will require a huge amount of change to the source code.
Still, the SQL databases are not enough for handling all the load? Well, why not trying NoSQL?
SQL databases are based on schema and relationships between tables. This is at the heart of its limit to reach infinite scalability. On the other side, NoSQL databases save data as key-value pairs, no need for schema neither for table relationships. For that reason, tables can be split horizontally and infinitely!
NoSQL eliminates relationships between tables. You still can do some tweaks to establish relationships, but that is not recommended. So, you have better to make the decision if it still suits your needs or not.
Scaling an application on the cloud is not only the responsibility of the architect, but it also requires the developers to think about the stateless aspect and DBA to think about partitioning the database. This work should be done at first. One important note to think about is also Metrics and Analytics. Because those who can tell if we need to scale up or down and especially what exactly needs to be scaled.
Join our community Slack and read our weekly Faun topics ⬇
The Must-Read Publication for Creative Developers & DevOps Enthusiasts
2.5K 
4
2.5K claps
2.5K 
4
Written by
Premier Field Engineer at Microsoft, ex MVP, Cloud & DevOps enthusiast. I share what I learn in my professional experience through articles and videos.
The Must-Read Publication for Creative Developers & DevOps Enthusiasts. Medium’s largest DevOps publication.
Written by
Premier Field Engineer at Microsoft, ex MVP, Cloud & DevOps enthusiast. I share what I learn in my professional experience through articles and videos.
The Must-Read Publication for Creative Developers & DevOps Enthusiasts. Medium’s largest DevOps publication.
"
https://medium.com/thinking-design/ux-process-what-it-is-what-it-looks-like-and-why-its-important-290640e69531?source=search_post---------24,"There are currently no responses for this story.
Be the first to respond.
As a UX designer, I am sure you have been asked many times “What is your UX design process? What and how many steps does it have?” There is a simple reason why this question so popular among designers: UX process is a cornerstone of UX design, it’s a make-it-or-break-it aspect of UX design. Without a solid UX design process, a designer could be completely moving in the dark. A clear and concise UX process, on the other hand, makes it possible to craft amazing experiences for users.
In this article, we’ll define a general UX design process, as well as the order in which specific UX phases should be taken. We will also see what methods can be used by UX designers during each phase.
The answer to this question is: it depends. A UX design process is something that everyone has in the UX industry, but something that everyone does differently. This happens because UX process depends heavily on the project. Different projects require different approaches: the approach to a corporate website differs from the way we design a dating app. And while there are some practices UX designers should follow for each project (such as conduct product research before moving on to prototyping), there are principles in every part of the process that have to be custom designed for the specific project.
At its core, every UX process should consist of the following 5 key phases:
One of the most important phases in UX design is actually done before the UX design process even starts. Before you can build a product, you need to understand its context for existence. Product definition phase sets the stage for the success of a product. During this phase, UX designers brainstorm the product at the highest level (basically, the concept of the product) with stakeholders.
This phase usually includes:
Once the product idea is defined, product research (which naturally includes user and market research) provides the other half of the foundation for great design. Good research informs your product and the fact that it comes early in design process save a lot of resources (time and money) further down the road (as fewer adjustments will need to be made).
The product research phase is probably the most variable between projects — the phase varies based on the complexity of the product, timing, available resources and many other factors. This phase can include:
The aim of the Analysis phase is to draw insights from data collected during the Product Research phase. Capturing, organizing and making inferences from the “what” users want/think/need can help UX designers begin to understand the “why” they want/think/need that. During this phase, designers confirm that the most important assumptions being made are valid.
This phase usually includes:
When user expectations from the product are established (it’s clear what their goals are and how they like to operate with it), UX designers move to the design phase. An effective design phase is both highly collaborative (it requires input from all team players involved in product development) and iterative (meaning that it cycles back upon itself to validate ideas and assumptions).
The design phase usually includes:
Usually, the validation phase starts when the high-fidelity design is fleshed out. A product is validated with stakeholders and end-users through the series of user testing sessions.
Similar to the product research phase, this phase is also variable between projects. Validation phase can include:
Now you’ve seen how each phase is connected to each other, let’s consider some helpful tips for improving the UX design process:
It’s important to understand that UX design isn’t a linear process. The phases of the UX process often have considerable overlap and usually there’s a lot of back-and-forth. As the UX designer learns more about the problem being solved, the users and details about the project (especially, constraints), it may be necessary to revisit some of the research undertaken or try out new design ideas.
Communication is a key UX design skill. While doing great design is one thing, communicating great design is equally as important, as even the best concepts will fail if they don’t accept by the team and stakeholders. That’s why the best UX designers are great communicators.
UX designers should be flexible with every project — the process employed should be tailored to fit specific project needs, both business and functional. A process tailored to the capabilities of the business and the clients proved to be generally effective.
When it comes to UX design process, there’s no one fits all solution. But whether your UX process lightweight or it’s full of a lot of activities, the goal of each UX design process is the same — create great a product for your users. Thus, use what works the best for your project, get rid of the rest, and evolve your UX process as your product evolves.
Nick Babich is a developer, tech enthusiast, and UX lover. He has spent the last 10 years working in the software industry with a specialized focus on development. He counts advertising, psychology, and cinema among his myriad interests.
Originally published at blogs.adobe.com.
Learn about Adobe XD, our all-in-one design and prototyping tool:
Stories and insights from the design community.
2.7K 
5
2.7K claps
2.7K 
5
Written by
New Tools for New Creatives. Get all the latest creative apps plus seamless ways to share and collaborate. All right on your desktop.
Stories and insights from the design community.
Written by
New Tools for New Creatives. Get all the latest creative apps plus seamless ways to share and collaborate. All right on your desktop.
Stories and insights from the design community.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@copyconstruct/monitoring-in-the-time-of-cloud-native-c87c7a5bfa3e?source=search_post---------25,"Sign in
There are currently no responses for this story.
Be the first to respond.
Cindy Sridharan
Oct 4, 2017·41 min read
Note — Huge thanks to Jamie Wilkinson and Julius Volz, Google SREs present and past, for reading a draft of this and giving me invaluable suggestions. All mistakes and opinions however, are solely mine. This was, in essence, my Velocity 2017 talk.
The infrastructure space is in the midst of a paradigm-shifting change. The way organizations — from the smallest of startups to established companies — build and operate systems has evolved.
Containers, Kubernetes, microservices, service meshes, immutable infrastructure and serverless are all incredibly promising ideas which fundamentally change the way we run software. As more and more organizations move toward these paradigms, the systems we build have become more distributed and in the case of containerization, more ephemeral.
While we’re still at the stage of early adoption, with the failure modes of these new paradigms still being very nebulous and not widely advertised, these tools are only going to get increasingly better with time. Soon enough, if not already, we’ll be at that point where the network and underlying hardware failures have been robustly abstracted away from us, leaving us with the sole responsibility to ensure our application is good enough to piggy bank on top of the latest and greatest in networking and scheduling abstractions.
No amount of GIFEE (Google Infrastructure for Everyone Else) or industrial-grade service mesh is going to fix the software we write. Better resilience and failure-tolerant paradigms from off-the-shelf components now means that — assuming said off-the-shelf components have been configured correctly — most failures will arise from the application layer or from the complex interactions between different applications. Trusting Kubernetes and friends to do their job makes it more important than ever for us to focus on the vagaries of the performance characteristics of our application and business logic. We’re at a time when it has never been easier for application developers to focus on just making their service more robust and trust that if they do so, the open source software they are building on top of will pay the concomitant dividends.
In order to manoeuvre this brave new world successfully, gaining visibility into our services and infrastructure becomes more important than ever before to successfully understand, operate, maintain and evolve these.
Fortunately for us, a new crop of tools have emerged to help us rise to this challenge. While one might argue that these tools suffer from the selfsame problem they assist us in solving — viz, the tools themselves are every bit as nascent and emergent as the infrastructural paradigms they help us gain visibility into — strong community interest, community driven development and an open governance model do a lot to promote the sustainability and development of these tools.
In addition to a surge in open source tooling, commercial tooling modeled along the lines of Google, Facebook and Twitter’s internal tools have emerged to address the real need felt by the the early adopters of cloud native paradigms. Given how far both categories of tools have evolved in recent years, we now have a veritable smorgasbord of choices.
A plethora of tools at our disposal to adopt or buy, however, presents an entirely different problem — one of decision making.
How do we choose the best tool for our needs? How do we even begin to tell the difference between these tools when several of these tools more or less do the same thing? We might’ve heard that monitoring is dead and observability is all the rage now. Does that mean we stop “monitoring”? We hear a lot about the “three pillars of observability”, but what even is observability and why should we care? What really is the difference between logs and metrics, except where we send them to? We might’ve heard a lot of tracing — but how useful can tracing really be if it’s only just a log with some context? Is a metric just a log or trace that occurs too frequently for a backend system to store? Do we really need all three of them?
I’ve said this before in one of my posts, but it bears reiterating. It’s tempting, especially when enamored by a new piece of technology that promises the moon, to retrofit our problem space with the solution space of said technology, however minimal the intersection. Before buying or building a tool, it becomes important to evaluate the maximum utility it can provide for the unique set of engineering challenges specific teams face. In particular when it comes down to choosing something as critical as a monitoring stack, in order to be able to make better technological choices, it becomes absolutely necessary for us to first fully understand:
— the strengths and weaknesses of each category of tools — the problems they solve— the tradeoffs they make— their ease of adoption/integration into an existing infrastructure
Most importantly, it becomes important to make sure that we are solving the problems at hand, not using the solutions these new crop of tools provide. Starting over from scratch isn’t a luxury most of us enjoy and the most challenging part about modernizing one’s monitoring stack is iteratively evolving it. Iterative evolution — of refactoring, if you will — of one’s monitoring stack in turn presents a large number of challenges from both a technical as well as an organizational standpoint.
The goal of this post is to shed light on these technologies and primarily frame the discussion in the context of the problems that we will be solving and the tradeoffs we might be making. It’s important for me to state upfront that the main purpose of this post isn’t to provide catch-all answers or solutions or demos of specific tools. What I hope to achieve with this post is leave you with some ideas and hopefully some questions that you can try to answer as you design systems with the goal of bringing better visibility to them.
— What even is observability and how is it different from Monitoring? — An overview of the “three pillars of modern observability”: logging, metrics collection, and request tracing  — The pros and cons of each in terms of resource utilization, ease of use, ease of operation, and cost effectiveness — An honest look at the challenges involved in scaling all the three when used in conjunction  — What to monitor and how in a modern cloud native environment; what is better-suited to be aggregated as metrics versus being logged; how and when to use the data from all the three sources to derive actionable alerts and insightful analysis  — When it makes sense to augment the three aforementioned tools with additional tools
This post is titled Monitoring in the time of Cloud Native. I’ve been asked why I chose to call it monitoring and not observability. I was expecting more snark about the buzzword that’s actually in the title — Cloud Native — than the one conspicuous by its absence. I chose not to call it observability for this very same reason — two buzzwords was one too many for my liking.
In all seriousness, I do believe there’s a difference between the two. The reason I believe so is because the nature of failure is changing, the way our systems behave (or misbehave) as a whole is changing, the requirements these systems need to meet are changing, the guarantees these systems need to provide are changing. In order to rise to these challenges successfully, it becomes necessary to not just change the way we build and operate software, but also gain better visibility into our services, which in turn gives us a shorter feedback loop about the performance of our services in production, which in turn enables us to build better services. In order to craft this virtuous cycle, it becomes important to understand what’s observability and how it differs from Monitoring.
When I type “monitoring” into a search engine, the first two results that come up are the following:
— observe and check the progress or quality of (something) over a period of time; keep under systematic review.
— maintain regular surveillance over.
Monitoring, to me, connotes something that is inherently both failure as well as human centric. Let’s talk a bit more about this because this forms the bedrock of this post.
In the past, we might’ve first tested our application. This might’ve been followed by a QA cycle. Then we might’ve released our code, followed by “monitoring” it. Followed by leaving a lot to chance.
To be fair, I don’t believe this was how everyone has been managing software lifecycle, but it makes for a good caricature of what’s often considered “the old way”.
We “monitored” something because we expected something to behave a certain way. What’s worse, we expected something to fail in a very specific manner and wanted to keep tabs on this specific failure. An “explicit, predictable failure” centric approach to monitoring becomes a problem when the number of failure modes both increases and failure itself becomes more implicit.
As we adopt increasingly complex architectures, the number of “things that can go wrong” exponentially increases. We often hear that we live in an era when failure is the norm. The SRE book states that:
It turns out that past a certain point, however, increasing reliability is worse for a service (and its users) rather than better! Extreme reliability comes at a cost: maximizing stability limits how fast new features can be developed and how quickly products can be delivered to users, and dramatically increases their cost, which in turn reduces the number of features a team can afford to offer.
Our goal is to explicitly align the risk taken by a given service with the risk the business is willing to bear. We strive to make a service reliable enough, but no more reliable than it needs to be.
Opting in to the model of embracing failure entails designing our services to behave gracefully in the face of failure. In other words, this means turning hard, explicit failure modes into partial, implicit and soft failure modes. Failure modes that could be papered over with graceful degradation mechanisms like retries, timeouts, circuit breaking and rate limiting. Failure modes that can be tolerated owing to relaxed consistency guarantees with mechanisms like eventual consistency or aggressive multi-tiered caching. Failure modes that can be even triggered deliberately with load shedding in the event of increased load that has the potential to take down our service entirely, thereby operating in a degraded state.
But all of this comes at the cost of increased overall complexity and the buyer’s remorse often acutely experienced is the loss of ability to easily reason about systems.
Which brings me to the second characteristic of “monitoring” — in that it’s human centric. The reason we chose to “monitor” something was because we knew or suspected something could go wrong, and that when it did go wrong there were consequences. Real consequences. High severity consequences that needed to be remedied as soon as possible. Consequences that needed human intervention.
I’m not someone who believes that automating everything is a panacea, but the advent of platforms like Kubernetes means that several of the problems that human and failure centric monitoring tools of yore helped “monitor” are already solved. Health-checking, load balancing and taking failed services out of rotation and so forth are features these platforms provide for free. That’s their primary value prop.
With more of the traditional monitoring responsibilities being automated away, “monitoring” has become — or will soon be — less human centric. While none of these platforms will truly make a service impregnable to failure, if used correctly, they can help reduce the number of hard failures, leaving us as engineers to contend with the subtle, nebulous, unpredictable behaviors our system can exhibit. The sort of failures that are far less catastrophic but ever more numerous than before.
Which then begs the question — how do we design monitoring for such systems?
It really isn’t even so much about how to design monitoring for these systems, than how to design the systems themselves.
I’d argue that “monitoring” should still be both hard failure as well as human centric, even in this brave new world. The goal of “monitoring” hasn’t changed, even if the scope has shrunk drastically, and the challenge that now lies ahead of us is identifying and minimizing the bits of “monitoring” that still remain human centric. We need to design our systems such that only a small sliver of the overall failure domain is now of the hard, urgently human actionable sort.
But there’s a paradox. Minimizing the number of “hard, predictable” failure modes doesn’t in any way mean that the system itself as a whole is any simpler. In other words, even as infrastructure management becomes more automated and requiring less human elbow grease, application lifecycle management is becoming harder. As the number of hard failure modes shrink at the expense of a drastic rise in implicit failure modes and overall complexity, “monitoring” every failure explicitly becomes infeasible, and not to mention, quite unnecessary.
Observability, in my opinion, is really about being able to understand how a system is behaving in production. If “monitoring” is best suited to report the overall health of systems, “observability”, on the other hand, aims to provide highly granular insights into the behavior of systems along with rich context, perfect for providing visibility into implicit failure modes and on the fly generation of information required for debugging. Monitoring is being on the lookout for failures, which in turn requires us to be able to predict these failures proactively. An observable system is one that exposes enough data about itself so that generating information (finding answers to questions yet to be formulated) and easily accessing this information becomes simple.
For the uninitiated, blackbox monitoring refers to the category of monitoring derived by treating the system as a blackbox and examining it from the outside. While some believe that with more sophisticated tooling at our disposal blackbox monitoring is a thing of the past, I’d argue that blackbox monitoring still has its place, what with large parts of core business and infrastructural components being outsourced to third-party vendors. While the amount of control we might have over the performance of the vendor might be limited, having visibility into how services we own are impacted by the vagaries of outsourced components becomes exceedingly crucial insofar as it affects our system’s performance as a whole.
Even outside of third-party integrations, treating our own systems as blackboxes might still have some value, especially in a microservices environment where different services owned by different teams might be involved in servicing a request. In such cases, being able to communicate quantitatively about systems paves the way toward establishing SLOs for different services.
It seems pragmatic for individual teams to treat services owned by other teams as blackboxes. This enables individual teams to design better integrations with other systems owned by different teams based on the contracts they expose and guarantees they offer.
“Whitebox monitoring” refers to a category of “monitoring” based on the information derived from the internals of systems. Whitebox monitoring isn’t really a revolutionary idea anymore. Time series, logs and traces are all more in vogue than ever these days and have been for a few years.
So then. Is observability just whitebox monitoring by another name?
Well, not quite.
The difference between whitebox monitoring and observability really is the difference between data and information. The formal definition of information is:
Data are simply facts or figures — bits of information, but not information itself. When data are processed, interpreted, organized, structured or presented so as to make them meaningful or useful, they are called information. Information provides context for data.
The distinction between monitoring and observability isn’t just about if the data is being reported from the bowels of the system or if it’s collected via treating the system as a blackbox. The distinction, in my opinion, is more purpose-driven, than origin-driven. It’s not so much about where this data comes from than what we plan to do with it and how easily we can achieve this.
Whitebox monitoring is a fantastic source of data. Observability is our ability to easily and quickly find germane information from this data when we need it. Observability is more about what information we might require about the behavior of our system in production and whether we will be able to have access to this information. It doesn’t matter so much if this information is pre-processed or if it’s derived from the data on the fly. It’s also not about how we plan to process and use the raw data. This raw data we’re collecting could have various uses —
Both of these cases, I’d argue, fall under “monitoring”.
These are all different goals. We could optimize for some of these or maybe even all of these. Whitebox monitoring is an important component (possibly the most important component) that helps us achieve all of these aforementioned goals, but whitebox monitoring, per se, isn’t observability.
Different organizations might have different requirements for what falls under “monitoring”. For some, dependency analysis might be an active part of their “monitoring”. For others, security auditing might be an indispensable part of their Monitoring goals. As such, I see observability as a spectrum and something constantly evolving as a service evolves.
Another way of looking at what falls under “monitoring” as opposed to what’s “observability” is by differentiating what we do reactively as opposed to what we do proactively.
Again, this might be different for different organizations, but I think it’s important to differentiate between the two purposes. Proactively generating information from data because we feel we might need them at all times is different from generating information on the fly at the time of debugging or analysis from data proactively collected.
Yet another way of looking at this spectrum is to perhaps distinguish based on the Ops versus Dev responsibilities. I see “monitoring” as something that requires being on-call. Observability I see as something that mandatorily requires developer/software engineer participation.
Furthermore, it’s worth noting at there’s a cause/effect relationship at play here in the spectrum. A lot of times what people “monitor” at one layer (metrics like error rate and duration of request) are often the “symptoms”, with the cause being several layers down the spectrum.
Being able to troubleshoot a problem involves often starting with a symptom reported by a coarse-grained metric (increased error rate or response time) or a trace (service B is slow for certain types of requests from downstream service C) that provides a bird’s eye view of the problem and then iteratively drilling down to corroborate or invalidate our theories thereby reducing the search space at every iteration, until we finally reach the information needed to until we arrive at the root cause.
Which brings me to my next point about —
While having access to data becomes a requirement if we wish to derive information from it, observability isn’t just about data collection alone. Once we have the data, it becomes important to be able to get answers/information from this data easily.
While it’s true that raw data is more malleable than pre-processed information, deferring processing of information until we actually need it incurs other overheads, namely that of collection, storage and on-the-fly processing. While it might sound all very well in theory to state that implementation details don’t matter so long as we can get to our observability goals, how the data that is being gathered can be best processed and stored becomes a key practical consideration if we wish to achieve the dream of establishing and sustaining the virtuous cycle. Usability of data becomes a key concern as well, as does the barrier to data collection.
And lastly, I’d argue that the most overarching aspect of observability isn’t data collection or processing. Having data at our disposal alone doesn’t solve problems. Problem solving also involves the right amount of engineering intuition and domain experience to ask the right questions of the data to be able to get to the bottom of it. In fact, good observability isn’t possible without having good engineering intuition and domain knowledge, even if one had all the tools at one’s disposal. And that really is what the rest of this post aims to address, by hopefully giving you some food for thought in terms of how to build systems to make it possible to gain better insights from them.
A more concrete example would help us understand logs, metrics and traces better. Let us assume the architecture of our system or sub-system looks like the following:
A log is an immutable record of discrete events that happened over time. Some people take the view that events are distinct compared to logs, but I’d argue that for all intents and purposes they can be used interchangeably.
Event logs in general come in three forms:
1. Plaintext — A log record might take the form of free-form text. This is also the most common format of logs.2. Structured — Much evangelized and advocated for in recent days. Typically this is logs emitted in the JSON format.3. Binary — think logs in the Protobuf format, MySQL binlogs used for replication and point-in-time recovery, systemd journal logs, the pflogformat used by the BSD firewall pf which often serves as a frontend to tcpdump.
Logs, in particular, shine when it comes to providing valuable insight along with ample context into the long tail that averages and percentiles don’t surface. Coming back to the example we saw above, let us assume that all of these various services also emit logs at varying degrees of granularity. Some services might emit more log information per request than others. Looking at logs alone, our data landscape might look like the following:
The very first thing that jumps out to me when I look at the above diagram is abundance of data points. Recording anything and everything that might be of interest to us becomes incredibly useful when we are searching at a very fine level of granularity, but simply looking at this mass of data, it’s impossible to infer at a glance what the request lifecycle was or even which systems the request traversed through or even the overall health of any particular system. Sure, the data might be rich but without further processing, it’s pretty impenetrable.
What we require, in short, is information. The interesting aspect of information in the context of this discussion is what information we’re looking for? Do we want information about the lifecycle of a request? Or do we want information about the resource utilization of a specific service? Or do we want information about the health of a specific host? Or do we want information about why a specific service crashed? Or do we want information about the replication lag in a distributed key value store? Or are we looking for information about how long it took an eventually consistent system to converge? Or are we looking for information about GC pauses? Or are we trying to glean information about the symptoms or are we trying to find the root cause? There is, quite frankly, an endless amount of data points we can collect and an endless number of questions we can answer, from the most trivial to the most difficult.
Two very important pieces of information, however, pertains to the fate of requests throughout their lifecycle (which is usually short lived) and the fate of a system as a whole (measured over a duration that is orders of magnitudes longer than request lifecycles). I see both traces and metrics as an abstraction built on top of logs that pre-process and encode information along two orthogonal axes, one being request centric, the other being system centric.
A trace is a representation of a series of causally-related distributed events that encode the end-to-end request flow through a distributed system. A single trace can provide visibility into both the path traversed by a request as well as the structure of a request. The path of a request allows us to understand the services involved in the servicing of a request, and the structure of a trace helps one understand the junctures and effects of asynchrony in the execution of a request.
Albeit discussions around tracing pivot around their utility in a microservices environment, I think it’s fair to suggest that any sufficiently complex application that interacts with — or rather, contends for — resources such as the network or disk in a non-trivial manner can benefit from the benefits tracing can provide.
The basic idea behind tracing is straightforward — identify specific points in an application, proxy, framework, library, middleware and anything else that might lie in the path of execution of a request, instrument these points and have these coordinate with each other. These points are of particular interest since they represent forks in execution flow (OS thread or a green thread) or a hop or a fan out across network or process boundaries that a request might encounter in the course of its lifecycle.
Usually represented as a directed acyclic graph, they are used to identify the amount of work done at each layer while preserving causality using happens-before semantics. The way this is achieved is by adding instrumentation to specific points in code. When a request begins, it’s assigned a globally unique ID, which is then propagated throughout the request path, so that each point of instrumentation is able to insert or enrich metadata before passing the ID around to the next hop in the meandering flow of a request. When the execution flow reaches the instrumented point at one of these services, a record is emitted along with metadata. These records are usually asynchronously logged to disk before being submitted out of band to a collector, which then can reconstruct the flow of execution based on different records emitted by different parts of the system.
Collecting this information and reconstructing the flow of execution while preserving causality for retrospective analysis and troubleshooting enables one to understand the lifecycle of a request better. Most importantly, having an understanding of the entire request lifecycle makes it possible to debug requests spanning multiple services to pinpoint the source of increased response time or resource utilization. As such, traces largely help one understand the which and sometimes even the why — like which component of a system is even touched during the lifecycle of a request and is slowing the response?
The official definition of metrics is:
a set of numbers that give information about a particular process or activity.
Metrics are a numeric representation of our data and as such can fully harness the power of mathematical modeling and prediction to derive knowledge of the behavior of our system over intervals of time in the present and future— in other words, a time series. The official definition of time series :
a list of numbers relating to a particular activity, which is recorded at regular periods of time and then studied. Time series are typically used to study, for example, sales, orders, income, etc.
Metrics are just numbers measured over intervals of time, and numbers are optimized for storage, processing, compression and retrieval. As such, metrics enable longer retention of data as well as easier querying, which can in turn be used to build dashboards to reflect historical trends. Additionally, metrics better allow for gradual reduction of data resolution over time, so that after a certain period of time data can be aggregated into daily or weekly frequency.
One of the biggest drawback of historical time series databases has been the identification of metrics which didn’t lend itself very well toward exploratory analysis or filtering. The hierarchical metric model and the lack of tags or labels in systems like Graphite especially hurt in this regard. Modern monitoring systems like Prometheus represent every time series using a metric name as well as additional key-value pairs called labels.
This allows for a high degree of dimensionality in the data model. A metric is identified using both the metric name and the labels. Metrics in Prometheus are immutable; changing the name of the metric or adding or removing a label will result in a new time series. The actual data stored in the time-series is called a sample and it consists of two components — a float64 value and a millisecond precision timestamp.
Let’s evaluate each of the three in terms of three criteria before we see how we can leverage the strengths of each to craft a great observability experience:
— Ease of generation/instrumentation— Ease of processing — Ease of querying/searching— Quality of information— Cost Effectiveness
Logs are, by far, the easiest to generate since there is no initial processing involved. The fact that it is just a string or a blob of JSON makes it incredibly easy to represent any data we want to emit in the form of a log line. Most languages, application frameworks and libraries come with in built support for logging. Logs are also easy to instrument since adding a log line is quite as trivial as adding a print statement. Logs also perform really well in terms of surfacing highly granular information pregnant with rich local context that can be great for drill down analysis, so long as our search space is localized to a single service.
The utility of logs, unfortunately, ends right there. Everything else I’m going to tell you about logs is only going to be painful. While log generation might be easy, the performance idiosyncrasies of various popular logging libraries leave a lot to be desired. Most performant logging libraries allocate very little, if any, and are extremely fast. However, the default logging libraries of many languages and frameworks are not the cream of the crop, which means the application as a whole becomes susceptible to suboptimal performance due to the overhead of logging. Additionally, log messages can also be lost unless one uses a protocol like RELP to guarantee reliable delivery of messages. This becomes especially important if one is using log data for billing or payment purposes. Lastly, unless the logging library can dynamically sample logs, logging has the capability to adversely affect application performance as a whole. As someone mentioned on a Slack:
A fun thing I had seen while at [redacted] was that turning off most logging almost doubled performance on the instances we were running on because logs ate through AWS’ EC2 classic’s packet allocations like mad. It was interesting for us to discover that more than 50% of our performance would be lost to trying to control and monitor performance.
On the processing side, raw logs are almost always normalized, filtered and processed by a tool like Logstash, fluentd, Scribe or Heka before they’re persisted in a data store like Elasticsearch or BigQuery. If an application generates a large volume of logs, then the logs might require further buffering in a broker like Kafka before they can be processed by Logstash. Hosted solutions like BigQuery have quotas you cannot exceed. On the storage side, while Elasticsearch might be a fantastic search engine, there’s a real operational cost involved in running it. Even if your organization is staffed with a team of Operations engineers who are experts in operating ELK, there might be other drawbacks. Case in point — one of my friends was telling me about how he would often see a sharp downward slope in the graphs in Kibana, not because traffic to the service was dropping but because ELK couldn’t keep up with the indexing of the sheer volume of data being thrown at it. Even if log ingestion processing isn’t an issue with ELK, no one I know of seems to have fully figured out how to use Kibana’s UI, let alone enjoy using it.
While there is no dearth of hosted commercial offerings for log management, they are probably better known for their obscene pricing. The fact that a large number of organizations choose to outsource log management despite the cost is a testament to how operationally hard, expensive and fragile running it in-house is.
An antidote often proposed to the problem of the cost overhead of logging is to sample or to only log actionable data. But even when sampled aggressively, it requires us to make decisions a priori as to what might be actionable. As such, our ability to log “actionable” data is entirely contingent on our ability to be able to predict what will be actionable or what data might be needed in the future. While it’s true that better understanding of a system might allow us to make an educated guess as to what data now gathered can prove to be a veritable source of information in the future, potentially every line of code is point of failure and as such could become the source of a log line.
By and large, the biggest advantage of metrics based monitoring over logs is the fact that unlike log generation and storage, metrics transfer and storage has a constant overhead. Unlike logs, the cost of metrics doesn’t increase in lockstep with user traffic or any other system activity that could result in a sharp uptick in data.
What this means is that with metrics, an increase in traffic to an application will not incur a significant increase in disk utilization, processing complexity, speed of visualization and operational costs the way logs do. Metrics storage increases with the number of time series being captured (when more hosts/containers are spun up, or when new services get added or when existing services are instrumented more), but unlike statsd clients that send a UDP packet every time a metric is recorded to the statsd daemon (resulting in a directly proportional increase in the number of metrics being submitted to statsd compared to the traffic being reported on!), client libraries of systems like Prometheus aggregate time series samples in-process and submit them to the Prometheus server upon a successful scrape (which happens once every few seconds and can be configured).
Metrics, once collected, are also more malleable to mathematical, probabilistic and statistical transformations such as sampling, aggregation, summarization and correlation, which make it better suited to report the overall health of a system.
Metrics are also better suited to trigger alerts, since running queries against an in-memory time series database is far more efficient, not to mention more reliable, than running a query against a distributed system like ELK and then aggregating the results before deciding if an alert needs to be triggered. Of course, there are systems that strictly query only in-memory structured event data for alerting that might be a little less expensive than ELK, but the operational overhead of running large distributed in-memory databases, even if they were open source, isn’t something worth the trouble for most when there are far easier ways to derive equally actionable alerts. Metrics are akin to blackbox frontends of a system’s performance and as such are best suited to furnish this information.
The biggest drawback with both logs and metrics is that they are system scoped, making it hard to understand anything else other than what’s happening inside of a particular system. Sure, metrics can also be request scoped, but that entails a concomitant increase in label fanout which results in an increase in storage. While the new Prometheus storage engine has been optimized for high churn in time series, it’s also true that metrics aren’t the best suited for highly granular request scoped information. With logs, without fancy joins, a single log line or metric doesn’t give much information about what happened to a request across all components of a system. Together and when used optimally, logs and metrics give us complete omniscience into a silo, but nothing more. While these might be sufficient for understanding the performance and behavior of individual systems — both stateful and stateless — they come a cropper when it comes to understanding the lifetime of a request that traverses through multiple systems.
Traces
Tracing captures the lifetime of requests as they flow through the various components of a distributed system. The support for enriching the context that’s being propagated with additional key value pairs makes it possible to encode application specific metadata in the trace, which might give developers more debugging power.
The use cases of distributed tracing are myriad. While used primarily for inter service dependency analysis, distributed profiling and debugging steady-state problems, tracing can also help with chargeback and capacity planning.
Tracing is, by far, the hardest to retrofit into an existing infrastructure, owing to the fact that for tracing to be truly effective, every component in the path of a request needs to be modified to propagate tracing information. Depending on whom you ask, you’d either be told that gaps in the flow of a request doesn’t outweigh the cons or be told that these gaps are blind spots that make debugging harder.
We’ve been implementing a request tracing service for over a year and it’s not complete yet. The challenge with these type of tools is that, we need to add code around each span to truly understand what’s happening during the lifetime of our requests. The frustrating part is that if the code is not instrumented or header is not carrying the id, that code becomes a risky blind spot for operations.
The second problem with tracing instrumentation is that it’s not sufficient for developers to instrument their code. A large number of applications in the wild are built using open source frameworks or libraries which might require additional instrumentation. This becomes all the more challenging at places with polyglot architectures, since every language, framework and wire protocol with widely disparate concurrency patterns and guarantees need to cooperate. Indeed, tracing is most successfully deployed in organizations where there are a core set of languages and frameworks used uniformly across the company.
The cost of tracing isn’t quite as catastrophic as that of logging, mainly owing to the fact that traces are almost always sampled heavily to reduce runtime overhead as well as storage costs. Sampling decisions can be made:
— at the start of a request before any traces are generated— at the end once all participating systems have recorded the traces for the entire course of the request execution— midway through the request flow, when only downstream services would then report the trace
All approaches have their own pros and cons.
Given the aforementioned characteristics of logs, any talk about best practices for logging inherently embodies a tradeoff. There are a couple of approaches that I think can help alleviate the problem on log generation, processing, storage and analysis.
We either log everything that might be of interest and pay a processing and storage penalty, or we log selectively, knowing that we are sacrificing fidelity but making it possible to still have access to important data. Most talk around logging revolves around log levels, but rarely have I seen quotas imposed on the amount of log data a service can generate. While Logstash and friends do have plugins for throttling log ingestion, most of these filters are based on keys and certain thresholds, with throttling happening after the event has been generated.
If logging is provided as an internal service — and there are many companies where this is the case — then establishing service tiers with quotas and priorities can be a first step. Any user facing request or service gets assigned the highest priority, while infrastructural tasks or background jobs or anything that can tolerate a bounded delay are lower on the priority list.
With or without quotas, it becomes important to be able to dynamically sample logs, so that the rate of log generation can be adjusted on the fly to ease the burden on the log forwarding, processing and storage systems. In the words of the aforementioned acquaintance who saw a 50% boost by turning off logging on EC2:
The only thing it kind of convinced me to is the need for the ability to dynamically increase or decrease logging on a per-need basis. But the caveat there is always that if you don’t always run the full blown logging, eventually the system can’t cope to run with it enabled.
Data isn’t only ever used for application performance and debugging use cases. It also forms the source of all analytics data as well. This data is often of tremendous utility from a business intelligence perspective, and usually businesses are willing to pay for both the technology and the personnel required to make sense of this data in order to make better product decisions.
The interesting aspect to me here is that there are striking similarities between questions a business might want answered and questions we might want answered during debugging. For example, a question that might be of business importance is the following:
Filter to outlier countries from where users viewed this article fewer than 100 times in total.
Whereas, from a debugging perspective, the question might look more like:
Filter to outlier page loads that performed more than 100 database queries.
Or, show me only page loads from Indonesia that took more than 10 seconds to load.
While these aren’t similar queries from a technical perspective, the infrastructure required to perform these sort of analysis or answer these kinds of queries is largely the same.
What might of interest to the business might be the fact that:
User A viewed Product X.
Augmenting this data with some extra information might make it ripe for observability purposes:
User A viewed Product X and the page took 0.5s to loadUser A viewed Product X whose image was not served from cacheUser A viewed Product X and read review Z for which the response time for the API call was 300ms.
Both these queries are made possible by events. Events are essentially structured (optionally typed) key value pairs. Marrying business information along with information about the lifetime of the request (timers, durations and so forth) makes it possible to repurpose analytics tooling for observability purposes.
If you think about this, log processing neatly fits into the bill of Online Analytics Processing (OLAP). Information derived from OLAP systems is not very different compared to information derived for debugging or performance analysis or anomaly detection at the edge of the system. Most analytics pipelines use Kafka as an event bus. Sending enriched event data to Kafka allows one to search in real time over streams with KSQL, a streaming SQL engine for Kafka from the fine folks at Confluent.
KSQL supports a wide range of powerful stream processing operations including aggregations, joins, windowing, sessionization, and much more. The Kafka log is the core storage abstraction for streaming data, allowing same data that went into your offline data warehouse is to now be available for stream processing. Everything else is a streaming materialized view over the log, be it various databases, search indexes, or other data serving systems in the company. All data enrichment and ETL needed to create these derived views can now be done in a streaming fashion using KSQL. Monitoring, security, anomaly and threat detection, analytics, and response to failures can be done in real-time versus when it is too late. All this is available for just about anyone to use through a simple and familiar SQL interface to all your Kafka data: KSQL.
Enriching business events that go into Kafka anyway with additional timing and other metadata required for observability use cases can be helpful when repurposing existing stream processing infrastructures. A further benefit this pattern provides is that this data can be expired from the Kafka log regularly. Most event data required for debugging purposes are only valuable for a relatively short period of time after the event has been generated, unlike any business centric information that normally would’ve been evaluated and persisted by an ETL job. Of course, this makes most sense when Kafka already is an integral part of an organization. Introducing Kafka into a stack purely for real time log analytics is a bit of an overkill, especially in non-JVM shops without any significant JVM operational expertise.
The fact that logging still remains an unsolved problem makes me wish for an OpenLogging spec, in the vein of OpenTracing which serves as a shining example and a testament to the power of community driven development. A spec designed ground up for the cloud-native era that introduces a universal exposition as well as a propagation format. A spec that enshrines that logs must be structured events and codifies rules around dynamic sampling for high volume, low fidelity events. A spec that can be implemented as libraries in all major languages and supported by all major application frameworks and middleware. A spec that allows us to make the most of advances in stream processing. A spec that becomes the lingua franca logging format of all CNCF projects, especially Kubernetes.
Prometheus is much more than just the server. I see Prometheus as a set of standards and projects, with the server being just one part of a much greater whole.
Prometheus does a great job of codifying the exposition format for metrics, and I’d love to see this become the standard. While Prometheus doesn’t offer long term storage, the remote write feature that was added to Prometheus about a year ago allowed one to write Prometheus metrics to a custom remote storage engine like OpenTSDB or Graphite, effectively turning Prometheus into a write-through cache. With the recent introduction of the generic write backend, one can transport time-series from Prometheus over HTTP and Protobuf to any storage system like Kafka or Cassandra.
Remote reads, though, is slightly newer and I’ve only been seeing efforts coalesce into something meaningful in the last few months. InfluxDB now natively supports both Prometheus remote reads and writes. Remote reads allows Prometheus to read raw samples from a remote backend during query execution time and compute the results in the Prometheus server.
Furthermore, improvements to the Prometheus storage engine in the upcoming 2.0 release makes Prometheus all the more conducive to cloud-native workloads with vast churn in time-series names. The powerful query language of Prometheus coupled with the ability to define alerts using the same query language and enrich the alerts with templated annotations makes it perfect for all “monitoring purposes”.
With metrics, however, it’s important to be careful not to explode the label space. Labels should be so chosen so that it remains limited to a small set of attributes that can remain somewhat uniform. It also becomes important to resist the temptation to alert on everything. For alerting to be effective, it becomes salient to be able to identify a small set of hard failure modes of a system. Some believe that the ideal number of signals to be “monitored” is anywhere between 3–5, and definitely no more than 7–10. One of the common pain points that keeps cropping up in my conversations with friends is how noisy their “monitoring” is. Noisy monitoring leads to either metric data that’s never looked at — which in other words is a waste of storage space of the metrics server — or worse, false alerts leading to a severe case of alert fatigue.
While historically tracing has been difficult to implement, the rise of service meshes make integrating tracing functionality almost effortless. Lyft famously got tracing support for all of their applications without changing a single line of code by adopting the service mesh pattern. Service meshes help with the DRYing of observability by implementing tracing and stats collections at the mesh level, which allows one to treat individual services as blackboxes but still get incredible observability onto the mesh as a whole. Even with the caveat that the applications forming the mesh need to be able to forward headers to the next hop in the mesh, this pattern is incredibly useful for retrofitting tracing into existing infrastructures with the least amount of code change.
Exception trackers (I think of these as logs++) have come a long way in the last few years and provide a far superior UI than a plaintext file or blobs of JSON to inspect exceptions. Exception trackers also provide full tracebacks, local variables, inputs at every subroutine or method invocation call, frequency of occurrence of the error/exception and other metadata invaluable for debugging. Exception trackers aim to do one thing — track exceptions and application crashes — and they tend to do this really well. While they don’t eliminate the need for logs, exception trackers augment logs — if you’ll pardon the pun — exceptionally well.
Some new tools also help achieve visibility by treating the network packets as the source of truth and using packet capture to build the overall service topology. While this definitely has less overhead than instrumenting all application code throughout the stack, it’s primarily useful for analyzing network interactions between different components. While it cannot help with debugging issues with the asynchronous behavior of a multithreaded service or unexpected event loop stalls in a single threaded service, augmenting it with metrics or logs to better understand what’s happening inside a single service can help one gain enough visibility into the entire architecture.
Observability isn’t quite the same as monitoring. Observability connotes something more holistic and encompasses “monitoring”, application code instrumentation, proactive instrumentation for just-in-time debugging and a culture of more thorough understanding of various components of the system.
Observability means having the ability — and the confidence — to be able to build systems knowing that these systems can turn into a frankensystem in production. It’s about understanding that the software we’re building can be — and almost always is — broken (or prone to break soon) to varying degrees despite our best efforts. A good analogy between getting code working on one’s laptop or in CI to having code running in production would be the difference between swimming in an indoor pool versus swimming in choppy rivers full of piranhas. The feeling of being unable to fix one’s own service running in a foreign environment for the want to being able to debug isn’t acceptable, not if we want to pride ourselves on our uptime and quality of service.
I want to conclude this post with how I think software development and operation should happen in the time of cloud-native.
It’s important to understand that testing is a best effort verification of the correctness of a system as well as a best effort simulation of failure modes. Unit tests only ever test the behavior of a system against a specified set of inputs. Furthermore, tests are conducted in very controlled (often heavily mocked) environments. While the very few who do fuzz their code benefit from having their code tested against a set of randomly generated input, fuzzing can only comprehensively test against the set of inputs to one service. End-to-end testing might allow for some degree of holistic testing of the system and fault injection/chaos engineering might help us gain a reasonable degree of confidence about our system’s ability to withstand these failures, but complex systems fail in complex ways and there’s is no testing under the sun that enables one to predict every last vector that could contribute towards a failure.
Despite these shortcomings, testing is as important as ever. If nothing else, testing our code allows us to write better and more maintainable code. More importantly, research has proven that something as simple as “testing error handling code could have prevented 58% of catastrophic failures” in many distributed systems. The renaissance of tooling aimed to understand the behavior of our services in production does not obviate the need for testing.
Testing in production isn’t really a very new idea. Methodologies such as A/B testing, canary deployments, dark traffic testing (some call this shadowing) have been around for a while.
Being able to test in production however absolutely requires that the release can be halted and rolled back if the need arises. This in turn means that one can only test in production if one has a quick feedback loop about the behavior of the system one’s testing in production. It also means being on the lookout for changes to key performance indicators of the service. For an HTTP service this could mean attributes like error rate and latencies of key endpoints. For a user facing service, this could additionally mean a change in user engagement. Testing in production essentially means proactively “monitoring” the change in production. Which brings me to my next point.
Monitoring isn’t dead. Monitoring, in fact, is so important that I’d argue it occupies the pride of place in your observability spectrum.
In order to test in production, one needs good, effective monitoring. Monitoring that is both failure centric (in that we proactively monitor for changes to KPI’s) as well as human centric (we want the developer who pushed out the change to test in production to be alerted as soon as possible).
I chose to call this Tier I Monitoring, for the want of better word, since I believe these are table stakes. It’s the very minimum any service thats going to be in production needs to have. It’s what alerts are derived from and I believe that time-series metrics are the best suited for this purpose.
However, there are several other bits and pieces of information we might capture but not use for alerting. There’s a school of thought that all such information isn’t of much value and needs to be discarded. I, however, believe that this is the sort of information I often find myself requiring often enough that I want it presented to me in the form of a dashboard.
A good example of this sort of tier II monitoring would be this dashboard of GitLab’s which is aptly named fleet overview or this one which gives information about the running Go processes. I picked these examples because GitLab is famously known for its transparency and these are real, live production dashboards of a real company. I find analyzing these dashboards more interesting than cooking up toy examples for the purpose of a blog post.
While these metrics don’t particularly help with debugging of gremlins or problems we don’t even know exist, having such dashboards gives me a bird’s eye view of the system, which I find invaluable especially after a release, since it gives me extremely quick feedback about how known key metrics might have been impacted by the change, but weren’t severe enough to trigger an alert. Measuring heap usage for a potential memory leak would be a good example of such Tier II monitoring. I would really like to know if I pushed out a code that’s leaking memory, but I don’t consider it something I necessarily want to be alerted on.
Then there’s exploration, which I find useful to answer questions one could not have proactively thought about. This often involves querying of raw events or log data rich in context and is extremely powerful for surfacing answers we couldn’t have predicted beforehand.
The problem with all of the three approaches seen until now is that they require that we record information about our systems a priori. What this means is the the data we need is generated before we can derive any useful information from it.
Dynamic instrumentation techniques aren’t new. However, implementations like DTrace were primarily machine centric and mostly correlate events that remain confined to an address-space or specific machine. Recent academic research has married these ideas with some of the ideas pioneered by distributed tracing, allowing one to “to obtain an arbitrary metric at one point of the system,while selecting, filtering, and grouping by events meaningful at other parts of the system, even when crossing component or machine boundaries”.
The primary breakthrough the Pivot Tracing paper proposed is the baggage abstraction.
Baggage is a per-request container for tuples that is propagated alongside a request as it traverses thread, application and machine boundaries. Tuples follow the request’s execution path and therefore explicitly capture the happened-before relationship. Using baggage, Pivot Tracing efficiently evaluates happened-before joins in situ during the execution of a request.
The idea of baggage propagation has been incorporated into the OpenTracing spec, which now enables “arbitrary application data from a mobile app can make it, transparently, all the way into the depths of a storage system”. While this still isn’t quite the same as what the whitepaper describes, it still gets us one step closer to true end-to-end tracing and the ability to dynamically enrich tracing data for better visibility. Facebook’s Canopy further takes ideas pioneered by the Pivot Tracing paper and marries it with an underlying event model pioneered by Scuba, making exploration of data more dynamic than ever.
And finally there are the unknowables.
Things we can’t know about or don’t need to know about. Even if we have complete omniscience into our application and hardware performance, it’s simply not feasible — or required — to have complete visibility into the various layers of abstractions underneath the application layer. Think ARP packet losses, BGP announcements or recursive BGP lookups, OSFP states and all manner of other implementation details of abstractions we rely on without a second thought. We simply have to get comfortable with the fact that there are things we possibly cannot know about, and that it’s OK.
Which brings me to my final point —
Observability — in and of itself, and like most other things — isn’t particularly useful. The value derived from the observability of a system directly stems from the business value derived from that system.
For many, if not most, businesses, having a good alerting strategy and time-series based “monitoring” is probably all that’s required to be able to deliver on the business goals. For others, being able to debug needle-in-a-haystack type of problems might be what’s needed to generate the most business value.
Observability, as such, isn’t an absolute.
Pick your own observability target based on the requirements of your service.
@copyconstruct on Twitter. views expressed on this blog are solely mine, not those of present or past employers.
2.5K 
8
2.5K 
2.5K 
8
@copyconstruct on Twitter. views expressed on this blog are solely mine, not those of present or past employers.
"
https://faun.pub/kubernetes-vs-docker-swarm-whos-the-bigger-and-better-53bbe76b9d11?source=search_post---------26,"There are currently no responses for this story.
Be the first to respond.
Container orchestration is fast evolving and Kubernetes and Docker Swarm are the two major players in this field. Both Kubernetes and Docker Swarm are important tools that are used to deploy containers inside a cluster. Kubernetes and Docker Swarm has many their prominent niche USPs and Pros in the field and they are here to stay. Though both of them have quite a different and unique way to…
"
https://medium.com/@ste.grider/serverless-showdown-aws-lambda-vs-firebase-google-cloud-functions-cc7529bcfa7d?source=search_post---------27,"Sign in
There are currently no responses for this story.
Be the first to respond.
Stephen Grider
Apr 24, 2017·11 min read
June 8 Update: Jason Polites from Google (https://www.linkedin.com/in/polites/) helpfully clarified a couple issues around my analysis of Google Cloud Functions. These updates are added in line with the article text below. Thanks Jason!
If 2016 was the year of microservices, 2017 is shaping up to be the year of serverless computing, most notably through AWS Lambda and Google Cloud Functions created through Firebase.
Cloud Functions for Firebase were announced a month ago, bringing them into direct competition with AWS’s offerings. This, of course, inevitably invites benchmarks and comparisons between AWS’s and Google’s offerings. Let’s walk through the two.
Wait, what is serverless computing?
Ah, the requisite explanation.
Traditional backends have been created using monolithic servers, where a single server may have several different responsibilities under a single codebase. Request comes in, server executes some processing, response comes out. The same server might be responsible for authentication, handling file uploads, and keeping track of user profiles. The key mechanic is that if two different requests come in for two different resources, it gets handled by a single codebase. This server might run on dedicated or virtualized machinery (or several machines!), and persistently runs over the span of days, weeks, or months.
More recently, we’ve seen the introduction of microservices as a popular architectural decision. With a microservices approach, there are still distinct servers, but many different servers, which of which handles a single purpose. A single service might be in charge of user authentication, and another one may handle file uploads. Microservice architectures are characterized by many separate codebases and incremental deployments of each individual service. The idea here is that a service which isn’t modified often is less likely to break, along with providing a more logical separation of responsibilities. Like monolithic deployments, microservices are traditionally long-running processes being executed on dedicated or virtualized machinery.
Finally, serverless architectures. Think of them as a natural evolution or extension to microservices.
This is a microservice architecture driven to the extreme. A single chunk of code, or ‘function’ is executed anytime a distinct event occurs. This event might be a user requesting to login, or a user attempting to upload a file. These functions are traditionally very short running in nature — the function ‘wakes up’, executes some amount of with a duration of 10 milliseconds to 10 seconds, and is then terminated automatically by the service provider. No persistence, no dedicated machinery — in effect, you have no idea where your code is running at any given time. The benefit to serverless architectures shares some of the benefits of a microservices based approach, where each function has some distinct responsibility and logical separation.
The Test App
To compare the two services, I wrote a small React Native application with the intent of providing one-time-password authentication.
Rather than expecting a user to enter a tedious email and password combination, the user is expected to enter just their phone number. Once we have their phone number in hand, we generate a short six-digit token then text it to the user via SMS. The user then enters the code into our app, after which we expect them to enter the code back into our app. If they enter the correct code, great, they are now authenticated.
Given that the code is the key authenticating factor, its something that clearly shouldn’t be generated or stored directly on the user’s mobile device. Instead, we should generate and store the code somewhere else, somewhere that the user doesn’t have any type of read access to. Enter our serverless functions!
Its always important to plan out the different cloud functions that will be created. In this case, I see three clear phases of the login process where some amount of logic must be executed in a secure environment:
Each function we create is assigned a unique name, usually to identify its purpose. I followed a simple nomenclature, opting for ‘createUser’, ‘requestOneTimePassword’, and ‘verifyOneTimePassword’.
With these three functions in mind, let’s walk through the deployment process
Creation of functions with Lambda can take two forms, either direct access of the Lambda Console or through the Serverless framework. I chose to use the Serverless framework, as it made deployment (later) much easier.
Serverless encourages centralizing all configuration of your functions into a single YML file. The YML file requires the function name as it will be displayed on the Lambda console, the name of the function in your code base, and some configuration on when to execute the function. In our case, we wanted to execute the function on an incoming HTTP request with a method of POST.
Here’s the relevant snippet of config from the YML file for creating a new user:
One of the interesting aspects of AWS Lambda is that it is truly built assuming that you’ll have any type of event driving a function invocation, not just an incoming HTTP request issued by a client device. Other valid triggers might be a file upload to S3, or a deploy to some other service on AWS. Even though its clear to you and me that we only want to run the function with an incoming HTTP request, we still have to be awfully explicit.
I found writing the actual function to require a little more boilerplate than I’d like:
You will notice a reference to firebase in here; I am still using Firebase for user management, even though the app is hosted on AWS infrastructure.
Yep, the request body has to be manually parsed. You’ll also notice that I made some ‘handleSuccess’ and ‘handleError’ helpers, to avoid some otherwise awful boilerplate. Here’s ‘handleSuccess’:
Again, don’t expect Lambda to handle JSON encoding or decoding for you, this is all manual.
Project creation with Cloud Functions was clearly easier. Its clear that the managers around this project assume that the most common use case is handling incoming HTTP requests, so there wasn’t a tremendous amount of configuration to route a particular event to a particular function.
Generation of the initial project was done by using the firebase CLI, which I hadn’t been previously familiar with. The CLI generates an entire Firebase project, which allows hosting important configuration like your security rules in a VCS, rather than relying entirely upon the console rule editor.
Definition of the functions took place inside of a Javascript file, where each export is essentially assumed to be a deployable function. For example:
The actual function creation was far more straightforward.
Fans of Express JS will immediately be at home with the req, res function signature. The request and response objects use an identical API to Express’, which makes for a straightforward learning curve. Also notice no need for complicated boilerplate around handling responses.
Winner: Google Cloud Functions
Creating functions with Firebase is a clear winner. There’s less upfront configuration required, along with a far more palatable API. Of course, the caveat is that Firebase’s amount of configuration is smaller because there are fewer function triggers available on Firebase. No need to specify that a function should be executed on an incoming HTTP request when there are only six different ways of triggering them
Certainly not much to say here, as the deployment process is nearly identical on both platforms. Having set up the initial project with Serverless, deployment on the AWS side was as easy as a terminal command:
Firebase deployment was similar by using the Firebase CLI
In both cases, the time from initiating the deployment to seeing the function go live was about forty seconds. Nothing to lose sleep over.
Winner: Tie
If function creation was easier on Firebase, I can confidently say that testing your functions in a staging environment is far easier on AWS.
For the above project, I spent around two hours from start to finish on AWS, whereas the same exact project took around five hours, simply because of of the atrocious debug cycle. It all comes down to the presence of a simple tool on the AWS side — the beautiful blue Test button.
Once your function has been deployed, you can create a ‘test’ event, by manually creating a request to be sent directly to your function. In this case, I wanted to manually test the creation of a new user by providing a unique phone number. Using one of the sample templates, I manipulated the body of the request to include a phone number, then saved the test event.
Once your test event is created, that beautiful blue Test button will execute your function instantaneously and immediately show output from the execution in plain text, including not only the function’s request response, but also any log output coming from the function.
June 8 update: There is a testing mechanism for Cloud Functions, but it’s not (currently) available in the Firebase console. If you access the “Cloud Console” (https://console.cloud.google.com) you’ll see Cloud Functions there with a range of capabilities, including quick testing. There is also a local emulator which allows you to debug functions locally, and Cloud Platform also has a (free) Cloud Debugger which actually lets you put a breakpoint on live code!
Original writeup: Let me be clear: manual testing of Cloud Functions is a pain, stemming from two aspects:
To the first point, manual testing of Cloud Functions revolves around your favorite HTTP request utility, be it curl or Postman. If your function fails to execute due to some hidden typo, rest assured that you’ll get a 50x status code without much more information, rather than any helpful debug output.
If you do want to get information out, you’ll be using Firebase’s Function console.
At the console, you’re limited to seeing only logged information, as opposed to AWS’s console which shows both log statements and function response bodies.
But the biggest gripe I have is how long it takes to see logs appear here. With stopwatch in hand, it would take one to five minutes of waiting to see any log information pop up from a single request. That terrible feedback loop lead to a lot of confusion as I tried to keep the order in which I’d execute test requests in mind. Let’s face it; when you have a long feedback loop like that, you may immediately execute one to five manual tests, then try to decipher the output you receive a few minutes later. Not fun.
Winner: AWS Lambda
In general, you can count on paying for function invocations based on two metrics: the number of invocations, and the amount of time each invocation takes to execute, modified by the hardware that the function is executed upon.
June 8 Update: I have neglected to include Amazon’s API Gateway price, which is $3.50 per million requests and is necessary if you want to have HTTP invocation of the function. Cloud Functions includes this for no extra charge. So the 19,193,857 requests you quoted for AWS would actually cost ~$65, not $1, which is a pretty large difference.
Original: At the time of this writing, Cloud Functions cost $0.40 per million invocations (after two million that are free), while Lambda clocks in at $0.20 per million invocations (after one million that are free).
Execution environment refers to the hardware that is used to run the function. More powerful hardware, more cost. Its a bit of an exercise in engineering economics, however. If you’re running a computation heavy function that takes some non-zero amount of time to execute, you might think to use a less powerful machine, as it costs less money per millisecond of execution time. But its a double edged sword; the slower the machine, the more milliseconds you’re spending! I’d love to do some followup work to figure out the sweet spot in machine size for compute-heavy tasks.
Google Cloud Function’s invocation time pricing is a function of the CPU plus RAM size, whereas AWS is a function of the RAM size only.
For example, a function that takes 100ms to execute on a 256mb memory machine with a 400mhz cpu would cost the following on Google :
Or, put another way, you’d get 432,432 requests for $1 on Google, not including the free tier or flat cost of invocation.
On AWS Lambda, a similar setup would cost
Or, put another way, you’d get 19,193,857 invocations for $1, not including the free tier or flat cost of invocation. A factor of four, really? Someone check my math, please.
Winner: AWS
At this point, AWS Lambda is head and shoulders above Google Cloud Functions. The testing cycle feels much tighter, and the pricing is currently no-contest. Function creation is a bit easier with Google Cloud, but as soon as you get that boilerplate down you’re good to go.
Officially, Google Cloud Functions are still in beta, so we might see price reductions at some point in time, or better tooling, but for now I can’t help but point friends over to AWS Lambda.
2.6K 
10
2.6K claps
2.6K 
10
"
https://blog.gruntwork.io/cloud-nuke-how-we-reduced-our-aws-bill-by-85-f3aced4e5876?source=search_post---------28,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
At Gruntwork, we write lots of automated tests for the code in our Infrastructure as Code Library. Most of these are integration tests that deploy our code into a real AWS account, verify the code works as expected, and then clean up after themselves. Sometimes, the cleanup doesn’t work when the tests run into errors or hit timeout limits.
In the early days, these leftover idle resources didn’t matter much. But by the start of 2018, our AWS bill was up to $2000/month. For a company of just 3 people (at the time), and no cloud-hosted product, this was outrageously expensive.
To solve this, we set out to build a tool that periodically goes through our AWS account and deletes all idle resources. We started out by poring through the AWS Billing Dashboard to identify the resources costing us the most money and then built out the tool to support deleting those resources. We had to be sure that we didn’t actively delete any resources while a test was running, so we designed the tool so it could delete only resources older than a specified time period (e.g. older than 24h).
We call this tool cloud-nuke. It’s an open source, cross-platform CLI application released under the Apache 2.0 License. Here’s an example of how you can use cloud-nuke to destroy all the (supported) resources in your AWS account that are more than 1 hour old:
This will delete all Auto Scaling Groups, (unprotected) EC2 Instances, Launch Configurations, Load Balancers, EBS Volumes, AMIs, Snapshots and Elastic IPs (more resources coming soon!). There’s also an extra option to exclude resources in certain regions:
We ran cloud-nuke on a nightly basis and after a few weeks of usage, in March, we saw that our bill had increased by 75% — whoops! After some digging into why, we figured out that there were two reasons: first, the team had grown to 5 people. Second, we realized that timing played an important role.
After further analyzing our spending, we realized that our testing fell into two categories:
Since we were running cloud-nuke on a nightly basis, that meant that all the automated tests could leave resources hanging around and costing us money for as long as 23 hours before being deleted. But how to delete those resources without affecting users doing manual testing? We fixed this by creating separate AWS accounts for manual testing and automated testing. We then run cloud-nuke every 3 hours in the automated testing account and every 24 hours in the manual testing account. This cut our monthly spending in half.
We were excited at the progress and wanted to see just how far we could push it. We noticed that while the spending on compute resources like EC2 instances had greatly reduced, our monthly CloudWatch requests spend was close to $500. This was particularly surprising because none of our tests made a large number of requests to the CloudWatch API.
What followed was a few days of searching through CloudTrail logs to figure out the source of the requests. After a while, we reached out to AWS support and they pointed us to an IAM role belonging to DataDog. Apparently, for DataDog to perform its primary monitoring functions it needed to make a lot of requests to the CloudWatch API — which it did very quietly. Since the integration was for a previous experiment that was no longer being actively pursued, we cut it out.
For the first time, in June, we got a bill that was less than $1000 and we’ve been spending only $500 on average since then.
There are a few main takeaways for how to reduce the cost of your testing environments:
We’re constantly looking for more ways to reduce cost as the products we build continue to evolve. In the meantime, cloud-nuke is open source and available to be used in your own cloud environment. Here are some features we’d love to add in the future (PRs are welcome!):
Take it for a spin and let us know what you think!
Get your DevOps superpowers at Gruntwork.io.
The Gruntwork Blog
1.6K 
8
Thanks to Yevgeniy Brikman. 
1.6K claps
1.6K 
8
Written by
Microsoft MVP | .NET Contributor | C# Enthusiast | Mildly Uninteresting
The Gruntwork Blog
Written by
Microsoft MVP | .NET Contributor | C# Enthusiast | Mildly Uninteresting
The Gruntwork Blog
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/hackernoon/cloud-is-not-a-digital-transformation-strategy-unless-you-are-amazon-aec1c470cf8f?source=search_post---------29,"There are currently no responses for this story.
Be the first to respond.
Top highlight
How did an online bookstore eat the cloud?
In 2002, Jeff Bezos did an odd, remarkable thing. He applied an IT architecture (SOA) to the culture, processes, and organizational structure at Amazon.
Bezos sent an internal mandate demanding that Amazon teams communicate with each other by exposing “their data and functionality through services interfaces.” Anyone who disobeyed the mandate would be fired.
Unbeknownst to the rest of the world, Bezos had quietly refactored Amazon into a platform company.
And it worked. A few years later, Amazon’s transformation gave rise to S3, the first of Amazon’s web services, which we now know collectively as the cloud.
A year after the Bezos mandate, Nicholas Carr published an article in the Harvard Business Review titled, “IT Doesn’t Matter.” Carr argued that IT had already transformed nearly every industry, and the lack of opportunity for differentiation meant companies should be focused on playing defense: risk management and cost containment.
IT organizations became incredibly cost conscious, creating layers of decision making and architectural review for every new technology considered — all in a battle to increase standardization and minimize costs.
But information technology can’t be compared to the transition from steam engines to railroads or from telegraphs to telephones — technologies that became commoditized utilities that drive little differentiation to businesses today.
Information technology builds upon itself, each layer of innovation opening up new avenues to transform the world.
Smartphones proliferated, GPS and mapping technologies matured, and startups began building on the infinite capacity of the cloud. These layered innovations enabled Netflix, Airbnb, Spotify, and even Amazon itself to break across legacy industries like tidal waves.
The Bezos mandate changed the world. He turned a tactical technology stack into one of the most strategic products of all time.
It was time to play offense, not defense.
Today, you’ll be hard pressed to find a company that does not have a cloud strategy. “Cloud First” and “Cloud Shift” are the popular mantras espoused by Gartner analysts and IT executives, all fueling the trillion dollars forecasted in cloud spending over the next four years.
On the surface, the narrative is clear and compelling. If you’re not in the cloud business, managing data centers and IT systems isn’t a core competency. Instead, pay for what you use from a superior service at lower costs due to shared economies of scale. In addition, you can take advantage of an ecosystem of tools and services designed to accelerate development.
But enterprises can’t compete with leaner, faster startups by migrating to the cloud. Following the herd is not a survival strategy for the digital era.
First, migrating to the cloud (private, public, or hybrid) is a big IT undertaking. If IT is under delivering for application development and the goal is to improve time to market, then adding yet another big IT program only pushes the goal farther away.
Second, even if companies do get to the cloud, they often find themselves releasing software at legacy speeds.
Releasing software faster requires changes in architecture (e.g. micro services), processes (e.g. Agile, CI/CD), tools (e.g. DevOps), and new skillsets. In addition, if you want high quality releases, you need fast, representative environments (e.g. Docker) and datasets (e.g. Delphix).
Third, most enterprises invest in incremental, marginal innovation, building more features for their largest existing customers.
But there’s a fundamental equation at work in the world today:
Legacy Industry + Digital Era = Digitally Refactored Industry
You can’t increment your way into the future. You can’t survive by playing defense from a legacy position. You have to play offense to win the future.
Offense looks like Amazon and Jeff Bezos buying Whole Foods to instantly acquire a critical mass of customers and physical stores as local distribution hubs to disrupt the grocery business. Defense looks like yet another bank adding mobile check deposit to their iPhone or Android app.
More than ever, enterprises need to focus on what really drives revolutionary innovation: the few great ideas inside or outside a company that will refactor an industry for the digital era.
Take the biggest, best idea from all of your programs, and ask yourself a simple, honest question:
Will this program define the future of your industry?
If the answer is no, then it’s time to revisit your strategy for the future.
Cloud doesn’t matter. It wasn’t even the goal for Amazon — just a byproduct of cost-effectively scaling retail operations. If you ask your teams about innovation, and their best idea is migrating to the cloud, then you know you’re on the path to obsolescence.
Instead, focus and execute on the great ideas that will win the future. Or you’ll end up just another legacy company, lost in the clouds.
If you’d like to learn more about the frameworks I’ve used to drive disruptive innovation, sign up for updates on the launch of the book and get the first few chapters now.
And here’s a preview of the hardcover for the book:
#BlackLivesMatter
2K 
11
how hackers start their afternoons. the real shit is on hackernoon.com. Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
2K claps
2K 
11
Written by
Bestselling Author of Disrupt or Die, Delphix executive chairman and founder, Avamar founding CEO
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
Bestselling Author of Disrupt or Die, Delphix executive chairman and founder, Avamar founding CEO
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@earlg3/google-cloud-architect-exam-study-materials-5ab327b62bc8?source=search_post---------30,"Sign in
There are currently no responses for this story.
Be the first to respond.
Earl Gay
Jul 18, 2017·4 min read
(Update: I’ve added a new post highlighting my re-certification: Google Cloud Architect Exam Study Materials — Updates for 2019 Re-Certification. Be sure to check it out for some additional notes and prep material!)
After recently completing the Google Cloud Architect certification, I wanted to share the preparation materials that I used. Due to the newness of the exam, one challenge is there is not the same abundance of preparation material as there is for other Cloud exams. The official Exam Guide leaves a bit to be desired and there is no official Practice Exam at the current time, so I hope this material is helpful for folks preparing.
I prepared through a combination of methods outlined below (in addition to real-world GCP usage):
This is probably a little overkill, but none of the aforementioned alone went into the depth in all of the areas that I had hoped. The answer was using the combination, and skipping areas in each that may have been redundant.
This is the just the official Google Cloud Platform documentation: https://cloud.google.com/docs/.
Side-note: One thing I liked about the exam is that I didn’t feel like any of the questions asked me for point-in-time questions (i.e. what did this feature do when the exam was released versus what it may do now as of July of 2017). As a result, you can prepare well by reviewing the most up-to-date documentation without a fear that your knowledge will be too accurate. That’s kind of a silly thing to say, but other exams from other vendors do have questions where you have to answer them based on a previous point in time, even if the version of the exam and product still match.
Most of these documents are overviews or FAQs. You may want to branch off of them into deeper areas, but I felt the Product Overview and FAQs were solid. I reviewed the product documentation last of all of the study material I used, and mostly used it to fill in gaps of knowledge, though; if you review this material first, you may want to go deeper.
The sessions from Google Cloud Next are on YouTube now (217 of them), and if there are any areas where you feel you’d like a little more depth (e.g. App Engine, Cloud Storage, Datastore, Stackdriver), these sessions can be helpful. They’re also just all really good sessions in general. Even if you’re not preparing for the exam, I’d recommend watching as many of the videos as you can. I like to watch them at 2.0x speed for maximum productivity.
The Coursera courses are made available by Google to partners, but are available for anyone. They have a couple of options for courses, but if you are a current AWS Certified Architect Professional then there is a course available based on that, which is what I took. It compares GCP products with AWS products and is slimmed down from the other option.
I thought it was a valuable course, but lecture videos were fairly short. Total lecture time was 121 minutes, but there are quizzes and labs. The non-AWS specific course appears to be a little longer.
I subscribe to Linux Academy because they have a ton of great courses in general even outside of GCP, but they also have several Google Cloud Platform courses available. Honestly, I only skimmed through the course because I had done all of the other preparation prior, but the material seems solid.
Overall, I thought the exam was done well. It’s not too long, and the questions are good Architect-level questions: it’s about being able to architect solutions, not necessarily memorizing every low-level command. With that said, a few final thoughts as you prepare for the exam:
Customer Engineer, @GoogleCloud | Mobility, Cloud, and Random Technology | Posts are mine and don’t represent my company.
1.7K 
9
1.7K 
1.7K 
9
Customer Engineer, @GoogleCloud | Mobility, Cloud, and Random Technology | Posts are mine and don’t represent my company.
"
https://medium.com/google-cloud/cloud-run-vs-cloud-functions-whats-the-lowest-cost-728d59345a2e?source=search_post---------31,"There are currently no responses for this story.
Be the first to respond.
Top highlight
In my previous story, Gabriel Flores asked me about cost comparison. Indeed, choosing a solution from a technical point of view is important, but the cost of a service isn’t to forget because, in auto scaling environment, it can blow up without warning.
The comparison performed in only based on the pricing and Quotas & limits of each product. The performance of processing are supposed equals in both solutions.
Cloud functions pricing is based on the vCPU in GHz/s and the memory in Gb/s, rounded up to the nearest 100ms. The specificity of functions is that you can adjust the memory and the CPU speed, but not separately. Both are linked, increase the memory if you want more MHz and vice versa
Network egress price is a flat rate of $0.12/Gb
Finally, requests are billed at $0.40 the million of requests after the 2 millions of free quota
Cloud Run pricing is based on vCPU/s and the memory in Gb/s, rounded up to the nearest 100ms. It’s not possible to use partial vCPU per instance. Your instance has always 1 vCPU assigned. Memory can be adjusted from 128Mb to 2Gb
An instance can handle up to 80 concurrent request. Thereby, for a given instance, you are only charged when at least one request is being processed by the instance.
Network egress is based on Network Premium Tier pricing after 1Gb of free tier.
Finally, requests are billed at $0.40 each million of requests after 2 million requests of free quota
The network cost is slightly different with 5Gb of egress free tier for Cloud Functions and only 1 for Cloud Run -> about $0.50 of difference. But the egress Gb price isn’t the same, and more complex on Cloud Run, based on Network Premium Tier. In Europe and North America, it’s quickly cheaper with Cloud Run and comparable for Asia.
Cloud Functions allow to adjust the vCPU speed accordingly with the memory used. But can handle only 1 request per instance.
Cloud Run always use 1 vCPU; you can only adjust the memory. But Cloud Run can process concurrent requests on the same instance.
Finally the cost of the number of requests is strictly the same.
By the way, we can consider 2 main cases of comparison: With only 1 concurrent request, and with several concurrent requests.
I built a GSheet for playing with memory, concurrency and process requirement values.
In this case, only the processing of 1 request is compared. In this case, Cloud Run and Cloud Functions have exactly 1 instance up for processing this unique request, and thus only this instance is charged.
This case can happen if you set the currency to 1 in Cloud Run, or if you have sparse and sequential requests to your service.
Thereby, in the same condition, with 1 vCPU (2.4Ghz) and 2Gb of memory, the pricing and the performance are strictly the same between Cloud Run and Cloud Functions.
Most of time, a service handles several requests in the same time. Cloud Run has the capability to handle up to 80 concurrent requests with the same instance.
At the opposite, Cloud Functions create as many instances as the concurrent requests.
Thereby, more you can process request concurrently with Cloud Run, cheaper is the service
The best choice depends on what you want to optimize, your use-cases and your specific needs.
If your objective is the lowest latency, choose Cloud Run.
Indeed, Cloud Run use always 1 vCPU (at least 2.4Ghz) and you can choose the memory size from 128Mb to 2Gb.
With Cloud Functions, if you want the best processing performance (2.4Ghz of CPU), you have to pay 2Gb of memory. If your memory footprint is low, a Cloud Functions with 2Gb of memory is overkill and cost expensive for nothing.
Cutting cost is not always the best strategy for customer satisfaction, but business reality may require it. Anyway, it highly depends of your use-case
Both Cloud Run and Cloud Function round up to the nearest 100ms. As you could play with the GSheet, the Cloud Functions are cheaper when the processing time of 1 request is below the first 100ms. Indeed, you can slow the Cloud Functions vCPU, with has for consequence to increase the duration of the processing but while staying under 100ms if you tune it well. Thus less Ghz/s are used and thereby you pay less.
For example, with a low processing requirement (20Mhz required) that required only 128Mb of memory, you have:
Sure, Cloud Functions is 10 times cheaper, but your customers wait 10 times more. And this pricing comparison is true ONLY if you have sequential requests.
In this example, with more than 10 concurrent requests, Cloud Run is cheaper! Indeed, Cloud Run handles up to 80 concurrent requests on the same instance before creating (and billing) a new one.
At opposite, Cloud Functions handles concurrent requests on different instances, and thus, you have to pay each running instance in addition of the cold start overcost for each instance created.
There is an other case where Cloud Functions are cheaper and where it’s a good idea to slow the vCPU: When an app calls APIs and waits the responses. Having a function with 200Mhz is enough and a full vCPU of Cloud Run is overkill (for no or few processing) and thereby expensive.
In the Limitation and Quota page of each product, the number of parallel instances is limited to 1000.
However, it’s possible to set a custom upper bound to limit the number of parallel instances. Initially released for limiting resource usage (for example, database connection), it’s also a great functionality to limit expenses and to set the upper bound of your service cost. But always with a negative impact on user satisfaction in case of saturation.
Cloud Functions can be triggered by HTTP requests (called HTTP Functions) but also by events triggered on the Google Cloud environment (called Background Functions)
At the opposite, Cloud Run containers can be only called by HTTP requests. However, it’s possible to use PubSub push subscription for pushing PubSub events to be processed by Cloud Run. In addition, it’s also possible to publish Storage event to PubSub topic and thus, to process Storage events with Cloud Run again through a PubSub push subscription. But, no more.
Thereby, except for PubSub and Storage events, Cloud Functions are unavoidable for all others types of events. No choice possible.
Your serverless app can require some private resources only available in your VPC. For example, a memory store for low latency key-value storage, or a VPN/direct connect access for reaching external resources (OnPremise or on other cloud).
Cloud Functions allow to set up a VPC connector for connecting your serverless functions to your VPC, and thus to route the traffic as required.
Until now, it’s not possible with managed Cloud Run but it will change in 2020.
Before my conclusion, I would like to add some additional considerations, not directly linked to the platform pricing, but also in the whole process of development, serving, and monitoring.
Public clouds come with a new capability: The real knowledge of cost. In my company, Cloud cost generated fears: Woah, it’s expensive ! No, it isn’t, but it’s known, visible. On premise environment, it’s very hard to know the real cost per service, especially human cost or on mutualized resources (network, storage, hosting,…).
When you choose a technical solution, it’s important to take into account all the pieces of the project, and first of all your human resources. Cloud Functions seem cheaper because your use case fit them well. But, is your team is able to develop functions in the available languages?
In my company, our legacy stack (and expertise) was PHP and C++ and this not fit with Cloud Functions capability. That’s why, containers were our choice, even before Cloud Run product exists.
Do you really know the cost of your team ?
All these reasons are, most of time, far more expensive than the cloud provider cost.
In addition of this human aspect, what will be the cost of refactoring in case of portability ? What will be the cost of X% more bugs because of difficulties of tests or lack of experience in a new language?
In November 2019, Cloud Run turns to GA after 6 month of Beta.
Cloud Functions are in GA for Python 3, Node 8 and Go 1.11
Apparently, there no difference in term of Google commitment between the 2 products. However, if your app run on Python 3.8, Node 10/12 or Go 1.12/1.13, there is no GA version on Cloud Functions (or even not support at all!). With Cloud Run, the GA commitment is applied on all containers, independently of the version of the language in it.
There is few differences between the 2 services but they can imply organizational change or technical solution design redesign. This is indirect cost but to take into account if you plan to deploy hundreds of Cloud Functions with a high rollout velocity.
Indeed, Cloud Functions are limited to 1000 functions per project, and to 120 minutes of compilation per days and per project.
In Cloud Run, there is no limit on compilation because it’s performed outside the platform, and there is the same limit of 1000 services per project. BUT a service can serve several endpoints.
Other limits are hard to compare or are not relevant for a large majority of projects.
As you can see, the cost comparison between Cloud Functions and Cloud Run goes further than simply comparing a pricing list. Moreover, on your projects, you often will have to use the 2 solutions for taking advantage of their strengths and capabilities.
My first choice for development is Cloud Run. Its portability, its testability, its openess on the libraries, the languages and the binaries confer it too much advantages for, at least, a similar pricing, and often with a real advantage in cost but also in performance, in particular for concurrent requests. Even if you need the same level of isolation of Cloud functions (1 instance per request), simply set the concurrent param to 1!
In addition, the GA of Cloud Run is applied on all containers, whatever the languages and the binaries used.
However, some reasons, VPC capabilty or eventing, can force you to choose Cloud Functions, but, most of time, it won’t be for a pricing reason.
Finally, the variety of use-cases, differences in organization and the complexity of human resources (skills, wishes, motivations) generate too many combinaisons and I can only provide tips, and things not to forget in your decision process. It’s not possible to provide a unique answer.
In any case, be sure to choose the right partner, with an external point of view, to help you in this journey, talk with your teams and try by yourself for finding the best answer to your use cases!
Google Cloud community articles and blogs
2.1K 
16
Thanks to Steren Giannini and Stewart Reichling. 
2.1K claps
2.1K 
16
Written by
GDE Google Cloud Platform, scrum master, speaker, writer and polyglot developer, Google Cloud platform 3x certified, serverless addict and Go fan.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
GDE Google Cloud Platform, scrum master, speaker, writer and polyglot developer, Google Cloud platform 3x certified, serverless addict and Go fan.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@phoneum/cloud-earning-pht-reduction-2f2eafa47f10?source=search_post---------32,"Sign in
There are currently no responses for this story.
Be the first to respond.
Phoneum
Apr 2, 2021·2 min read
Hello Everyone,
We have an important announcement to make today!
Cloud Earning PHT becomes 2 years old! This marks the beginning of a new chapter for PHT and is a huge milestone for the app as well as the PHT Token.
When Cloud Earning PHT first came out, the earning rate was between 10 and 15 PHT daily. Then in August 2019, we started a promotion and bumped the earning rate to ~100 PHT a day. This was an important move in order to give the ability to early investors and adopters to earn more than the normal rate. In 2020, we have introduced the PHT Staking functionality. It came with a promotional functionality to earn between 12% and 60% yearly.
Today, we are adjusting the staking to a 12% yearly rate (still paid out on a monthly basis at 1%), but we are removing the cap on the maximum amount able to be staked.
We are also starting the PHT earning reduction process. The earning is going to be gradually decreased by 10 PHT for each 250 MIL PHT that goes in circulation.
HUGE NEWS FOR VIP MEMBERS!
All active memberships and anyone who activates VIP by the 1st of May 2021, will not be affected by the PHT reduction and will still be earning ~100 PHT/daily times the multiplier rates (x2, x5, x10) until the VIP expires next year.
On behalf of the Phoneum team, we want to thank you all for your continued support and for being active in the early stages of Phoneum development.
Don’t miss the chance to activate VIP and earn more PHT before the rates get reduced!
Start Earning and Staking: https://play.google.com/store/apps/details?id=com.cloud.earning
Stay tuned for more news and updates!
The Phoneum Team
Phoneum is a decentralized cryptocurrency, that operates on mobile devices via app ONLY. For more information, please visit our website: https://phoneum.io
1.96K 
16
1.96K 
1.96K 
16
Phoneum is a decentralized cryptocurrency, that operates on mobile devices via app ONLY. For more information, please visit our website: https://phoneum.io
"
https://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193?source=search_post---------33,"There are currently no responses for this story.
Be the first to respond.
By Sara Robinson, Aakanksha Chowdhery, and Jonathan Huang
What if you could train and serve your object detection models even faster? We’ve heard your feedback, and today we’re excited to announce support for training an object detection model on Cloud TPUs, model quantization, and the addition of new models including RetinaNet and a MobileNet adaptation of RetinaNet. You can check out the announcement post on the AI blog. In this post, we’ll walk you through training a quantized pet breed detector on Cloud TPUs using transfer learning.
The whole process — from training to on-device inference on Android — takes 30 minutes and costs less than $5 on Google Cloud. When you’re done, you’ll have an Android app (iOS tutorial coming soon!) that performs real-time detection of dog and cat breeds and requires no more than 12Mb of space on your phone. Note that in addition to training an object detection model in the cloud, you can alternatively run training on your own hardware or in Colab.
We will first set up some of the libraries and prerequisites needed for training and serving our model. Note that this setup process may take significantly longer than training and serving the model itself. For convenience, you may use the Dockerfile here that provides the dependencies for installing Tensorflow from source and downloading the necessary datasets and models for this tutorial. If you decide to use Docker, you should still work through the “Google Cloud Setup” section and then skip to the “Uploading dataset to GCS” section. The Dockerfile will also build the Android dependencies for the Tensorflow Lite section. See the attached README file for more information.
First, create a project in the Google Cloud Console and enable billing for that project. We’ll use Cloud Machine Learning Engine to run our training job on Cloud TPUs. ML Engine is Google Cloud’s managed platform for TensorFlow, and it simplifies the process of training and serving ML models. To use it, enable the necessary APIs for the project you just created.
Second, we’ll create a Google Cloud Storage bucket to store the training and test data for our model, along with the model checkpoints from our training job.
Note that all of the commands in this tutorial assume you’re running Ubuntu. We’ll use Google Cloud gcloud CLI for many of the commands in this tutorial, along with the Cloud Storage gsutil CLI to interact with our GCS buckets. If you don’t have those installed, you can install gcloud here and gsutil here.
Run the following to set your current project to the one you just created, replacing YOUR_PROJECT_NAME with the name of your project:
Then we’ll create a Cloud Storage bucket with the following command. Note that Storage bucket names must be globally unique, so you may get an error if the first name you choose is taken.
This may prompt you to first run gcloud auth login, after which you will need to provide a verification code sent to your browser.
Then set up two environment variables to simplify commands throughout this tutorial:
Next, to give our Cloud TPU access to our project we need to add a TPU-specific service account. First, get the name of your service account with the following command:
When this command completes, copy the value of tpuServiceAccount (it will look something like your-service-account-12345@cloud-tpu.iam.gserviceaccount.com) and then save it as an environment variable:
Finally, grant the ml.serviceAgent role to your TPU service account:
If you don’t have TensorFlow installed, follow the steps here. To follow the on-device section of this tutorial, you’ll need to install TensorFlow from source using Bazel following the instructions here. Compiling TensorFlow may take a while. If you’d just like to follow the Cloud TPU training section of this tutorial, you don’t need to compile TensorFlow from source and can install a released version via pip, Anaconda, etc.
If this is your first time using TensorFlow Object Detection, welcome! To install it, follow the instructions here.
Once you’ve installed Object Detection, be sure to test your installation by running the following:
If installation is successful, you should see the following output:
To keep things simple, we’ll use the same pet breeds dataset from our last post on training an object detection model. This dataset includes around 7,400 images — ~200 images for 37 different cat and dog breeds. Each image has an associated annotations file, which includes the bounding box coordinates where the specific pet is located in the image. We can’t feed these images and annotations directly to our model; we need to convert them into a format our model can understand. For this we’ll use the TFRecord format.
To dive right in to training, we’ve made the pet_faces_train.record and pet_faces_val.record files publicly accessible here. You can either use the public TFRecord files, or if you’d like to generate them yourself, follow the steps here.
You can download and extract the public TFRecord files using the following command:
Note that these TFRecord files are sharded, so once you’ve extract them you’ll have 10 pet_faces_train.record files and 10 pet_faces_val.record files.
Once you’ve got your TFRecord files available locally, copy them into your GCS bucket under a /data subdirectory:
With your TFRecord files in GCS, move back to the models/research directory on your local machine. Next you’ll add the pet_label_map.pbtxt file in your GCS bucket. This maps each of the 37 pet breeds we’ll be detecting to an integer, so that our model can understand them in a numerical format. From the models/research directory, run the following:
At this point you should have 21 files in the /data subdirectory of your GCS bucket: the 20 sharded TFRecord files for training and testing, and the label map file.
Training a model to recognize pet breeds from scratch would take thousands of training images for each pet breed and hours or days of training time. To speed this up, we can make use of transfer learning — a process where we take the weights of a model that has already been trained on lots of data to perform a similar task, and then train the model on our own data, fine tuning the layers from the pre-trained model.
There are many models we can use that have been trained to recognize a wide variety of objects in images. We can use the checkpoints from these trained models and then apply them to our custom object detection task. This works because, to a machine, the task of identifying the pixels in an image that contain basic objects like tables, chairs, or cats isn’t so different from identifying the pixels in an image that contain specific pet breeds.
For this example we’ll use SSD with MobileNet, an object detection model optimized for inference on mobile. First, download and extract the latest MobileNet checkpoint that’s been pretrained on the COCO dataset. To see a list of all the models that the Object Detection API supports, check out the model zoo. Once you’ve extracted the checkpoint, copy the 3 files into your GCS bucket. Run the commands below to download the checkpoint and copy it into your bucket:
When we train our model, it’ll use these checkpoints as its starting point for training. Now you should have 24 files in your GCS bucket. We’re almost ready to run our training job, but we need a way to tell ML Engine where our data and model checkpoints are located. We’ll do this with a config file, which we’ll set up in the next step. Our config file provides hyperparameters for our model, the file paths for our training data, test data, and the initial model checkpoint.
Machine learning models have two distinct computational components: training and inference. In this example, we’re making use of Cloud TPUs to accelerate training. There are a few lines in the config file that relate specifically to TPU training. We can use a larger batch size when training on TPUs since they make it easier to handle large datasets (when experimenting with batch size on your own dataset, make sure to use multiples of 8 since data needs to be divided evenly for each of the 8 TPU cores). With a larger batch size for our model, we can reduce the number of training steps (in this example we use 2000). The focal loss function we use for this training job, defined in the following lines in the config, is also a great fit for TPUs:
This loss function computes loss for every example in the dataset and then reweights them, assigning more relative weight to hard, misclassified examples. This logic is better suited for TPUs than the hard example mining operation used in other training jobs. You can read more about focal loss in Lin et al. (2017).
Recall from above that the process of initializing a pre-trained model checkpoint and then adding our own training data is called transfer learning. The following lines in the config tell our model that we’ll be doing transfer learning for object detection, starting from a pre-trained checkpoint.
We also need to consider how our model will be used after it’s been trained. Let’s say our pet detector becomes a global hit, used by animal lovers and pet stores everywhere. We need a scalable way to handle these inference requests with low latency. The output of a machine learning model is a binary file containing the trained weights of our model — these files are often quite large, but since we’ll be serving this model directly on a mobile device we’ll need to make it as small as possible.
This is where model quantization comes in. Quantization compresses the weights and activations in our model to an 8-bit fixed point representation. The following lines in our config file will generate a quantized model:
Typically with quantization, a model will train with full precision for a certain number of steps before switching to quantized training. The delay number above tells ML Engine to begin quantizing our weights and activations after 1800 training steps.
To tell ML Engine where to find our training and test files and model checkpoint, you’ll need to update a few lines in the config file we’ve created for you to point to your bucket. From the research directory, find object_detection/samples/configs/ssd_mobilenet_v1_0.75_depth_quantized_300x300_pets_sync.config. Update all the PATH_TO_BE_CONFIGURED strings with the full path of the data directory in your GCS bucket. For example, the train_input_reader section of the config would look like the following (make sure to replace YOUR_GCS_BUCKET with the name of your bucket):
Then copy this quantized config file into your GCS bucket:
Before we kick off our training job on Cloud ML Engine, we need to package the Object Detection API, pycocotools, and TF Slim. We can do that with the following command (run this from the research/ directory, and note that the parentheses are part of the command):
We’re ready to train our model! To kick off training, run the following gcloud command:
Note that if you receive an error saying that no Cloud TPUs are available, we recommend simply trying again in a different zone (Cloud TPUs are currently available in us-central1-b, us-central1-c, europe-west4-a, and asia-east1-c).
Right after we kick off our training job, run the following command to start an evaluation job:
Both training and evaluation should complete within about 30 minutes. While they are running, you can use TensorBoard to see the accuracy of your model. To start TensorBoard, run the following:
Note that you may need to first run gcloud auth application-default login.
Navigate to localhost:6006 to view your TensorBoard output. Here you’ll see some common ML metrics used to analyze the accuracy of your model. Note that these graphs only have 2 points plotted since the model trains quickly in very few steps (if you’ve used TensorBoard before you may be used to seeing more of a curve here). The first point here is early in the training process and the last point shows metrics at the last step (step 2000).
First, let’s look at the graph for mean average precision at 0.5 IOU (mAP@.50IOU):
Mean average precision measures our model’s percentage of correct predictions for all 37 labels. IoU is specific to object detection models and stands for Intersection-over-Union. This measures the overlap between the bounding box generated by our model and the ground truth bounding box, represented as a percentage. This graph is measuring the percentage of correct bounding boxes and labels our model returned, with “correct” in this case referring to bounding boxes that had 50% or more overlap with their corresponding ground truth boxes. After training, our model achieved 82% mean average precision.
Next, look at the Images tab in TensorBoard:
In the left image, we see our model’s predictions for this image and on the right we see the correct, ground truth box. The bounding box is very accurate, but our model’s label prediction is incorrect in this particular case. No ML model can be perfect. 😉
At this point you have a fully trained pet detector, which you can use to test your own images in the browser with zero setup using this Colab notebook. To run this model in real time on a phone requires some extra work — — in this section, we will show you how to use TensorFlow Lite to get a smaller model and allow you take advantage of ops that have been optimized for mobile devices. TensorFlow Lite is TensorFlow’s lightweight solution for mobile and embedded devices. It enables on-device machine learning inference with low latency and a small binary size. TensorFlow Lite uses many techniques for this such as quantized kernels that allow smaller and faster (fixed-point math) models.
As mentioned above, for this section, you will need to use the provided Dockerfile, or build TensorFlow from source (with GCP support) and install the bazel build tool. Note that if you’d like to only work through this second part of the tutorial without training a model, we’ve made a pre-trained model available here.
To make these commands easier to run, let’s set up some environment variables:
We start by getting a TensorFlow frozen graph with compatible ops that we can use with TensorFlow Lite. First, you’ll need to install these python libraries. Then to get the frozen graph, run the export_tflite_ssd_graph.py script from the models/research directory with this command:
In the /tmp/tflite directory, you should now see two files: tflite_graph.pb and tflite_graph.pbtxt (sample frozen graphs are here). Note that the add_postprocessing flag enables the model to take advantage of a custom optimized detection post-processing operation which can be thought of as a replacement for tf.image.non_max_suppression. Make sure not to confuse export_tflite_ssd_graph with export_inference_graph in the same directory. Both scripts output frozen graphs: export_tflite_ssd_graph will output the frozen graph that we can input to TensorFlow Lite directly and is the one we’ll be using.
Next we’ll use TensorFlow Lite to get the optimized model by using TOCO, the TensorFlow Lite Optimizing Converter. This will convert the resulting frozen graph (tflite_graph.pb) to the TensorFlow Lite flatbuffer format (detect.tflite) via the following command. Run this from the tensorflow/ directory:
This command takes the input tensor normalized_input_image_tensor after resizing each camera image frame to 300x300 pixels. The outputs of the quantized model are named ‘TFLite_Detection_PostProcess’, ‘TFLite_Detection_PostProcess:1’, ‘TFLite_Detection_PostProcess:2’, and ‘TFLite_Detection_PostProcess:3’ and represent four arrays: detection_boxes, detection_classes, detection_scores, and num_detections. The documentation for other flags used in this command is here. If things ran successfully, you should now see a third file in the /tmp/tflite directory called detect.tflite (sample tflite file is here). This file contains the graph and all model parameters and can be run via the TensorFlow Lite interpreter on the Android device and should be less than 4 Mb in size.
To run our final model on device, we will need to use the provided Dockerfile, or install the Android NDK and SDK. The current recommended Android NDK version is 14b and can be found on the NDK Archives page. Please note that current version of Bazel is incompatible with NDK revisions 15 and above. Android SDK and build tools can be downloaded separately or used as part of Android Studio. To build the TensorFlow Lite Android demo, build tools require API >= 23 (but it will run on devices with API >= 21). Additional details are available on the TensorFlow Lite Android App page.
Before trying to get the pets model that you just trained, start by running the demo app with its default model, which was trained on the COCO dataset. To build the demo app, run this bazel command from the tensorflow directory:
The apk above will be built for 64-bit architecture and you may replace it with-- config=android_arm for 32-bit support. Now install the demo on a debug-enabled Android phone via Android Debug Bridge (adb):
Try running this starter app (called TFLDetect) and holding your camera to people, furniture, cars, pets, etc. The working test app should look something like this. You will see boxes around the detected objects with their labels. The working test app was trained using the COCO dataset.
Once you’ve got the generic detector working, replacing it with your custom pet detector is fairly simple. All we need to do is point the app to our new detect.tflite file and give it the names of our new labels. Specifically, we will copy our TensorFlow Lite flatbuffer to the app assets directory with the following command:
We will now edit the BUILD file to point to this new model. First, open the BUILD file tensorflow/contrib/lite/examples/android/BUILD. Then find the assets section, and replace the line “@tflite_mobilenet_ssd_quant//:detect.tflite” (which by default points to a COCO pretrained model) with the path to your TFLite pets model “//tensorflow/contrib/lite/examples/android/app/src/main/assets:detect.tflite”. Finally, change the last line in assets section to use the new label map. Your final assets section should look like this:
We will also need to tell our app to use the new label map. In order to do this, open up the tensorflow/contrib/lite/examples/android/app/src/main/java/org/tensorflow/demo/DetectorActivity.java file in a text editor and find the definition of TF_OD_API_LABELS_FILE. Update this path to point to your pets label map file: “file:///android_asset/pets_labels_list.txt”. Note that we have already made the pets_labels_list.txt file available for your convenience. This new section of DetectorActivity.java (around line 50) should now look as follows:
Once you’ve copied the TensorFlow Lite file and edited your BUILD and DetectorActivity.java files, rebuild and reinstall your app with the following commands:
Now for the best part: find the nearest dog or cat and try detecting it. On a Pixel 2, we get greater than 15 frames per second.
Want to use your own training data to train an object detection model on Cloud TPUs? Dive into the object detection docs here. For labeling your own image library, check out these resources for generating TFRecord files for other well known datasets. We’d love to have you contribute and hear your feedback. Leave a comment on this post, or submit a PR or issue on GitHub.
Thank you to everyone who worked on this release: Derek Chow, Aakanksha Chowdhery, Jonathan Huang, Zhichao Lu, Vivek Rathod, Ronny Votel, Pengchong Jin, Xiangxin Zhu as well as the following colleagues for their guidance and advice: Vasu Agrawal, Sourabh Bajaj, Chiachen Chou, Tom Jablin, Wenzhe Li, Tsung-Yi Lin, Hernan Moraldo, Kevin Murphy, Sara Robinson, Andrew Selle, Shashi Shekhar, Yash Sonthalia, Zak Stone, Pete Warden, Menglong Zhu.
TensorFlow is an end-to-end open source platform for…
1.8K 
51
Thanks to Sara Robinson and Aakanksha Chowdhery. 
1.8K claps
1.8K 
51
Written by
TensorFlow is a fast, flexible, and scalable open-source machine learning library for research and production.
TensorFlow is an end-to-end open source platform for machine learning.
Written by
TensorFlow is a fast, flexible, and scalable open-source machine learning library for research and production.
TensorFlow is an end-to-end open source platform for machine learning.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@devesu/how-to-upload-data-to-firebase-firestore-cloud-database-63543d7b34c5?source=search_post---------34,"Sign in
There are currently no responses for this story.
Be the first to respond.
Devesu
Jan 23, 2019·5 min read
Firestore is a simple sophisticated database which helps to store our data in document format in cloud. It provides very easy to use API for performing all CRUD (Create, Read, Update, Delete) operations. But during initial phase of project we may wish to upload some master or setup data in bulk. Currently there is no direct data upload option provided in Firestore database, so we need to do bit of coding to achieve that. Lets start.
Kindly note whatever name you are using as column header will be used as name of the fields, except 1st column “dockey”. “dockey” column values will be used as key of the documents in Firestore database.
Now export this file as CSV and save it somewhere.
You can use any online tool to convert your CSV data into JSON format, but I found https://www.csvjson.com/csv2json more suitable for our job. Open that website. Select your CSV file. Make sure you have selected Hash as Output option. Press Convert button below, now you will able to see your JSON data, download the file.
Inside Settings click on “Service accounts”. From left-hand side tab-bar select “Firebase Admin SDK”, and then press on “Generate Private Key” button at the bottom of the page, it will download your service account key JSON file. Please take note of “databaseURL” link mentioned in configuration code snippet. We will use that value in our upload program.
Our data and service account key is ready so lets prepare the upload program.
After running above command you can see one “package.json” file is created inside your project folder. Place “data.json” & “serviceAccountKey.json” files inside your project folder. Open your project folder in any of your favourite IDE editor ( my favourite is Visual Studio Code ), and create “index.js” file.
We need to write our upload program inside this “index.js” file. Before we proceed we need to install “firebase-admin” library. Install it by using below command from terminal, you must run the command from your project’s root folder.
After installation you can see one “node_modules” folder created inside your project folder, and one dependency (firebase-admin) added inside your package.json file.
Now copy below code and paste inside your index.js file. Please remember to replace collectionKey (name of the collection) and databaseURL (which you noted down during service account key download) values with your own.
Code Explanation
Line 1: Importing firebase-admin library
Line 2: Importing service account key information
Line 4: Importing JSON data file
Line 5: Setting up collection name. please modify this line and setup your own collection name
Line 7–10: Initialising admin sdk
Line 12–14: Setting up Firestore property
Line 16: Checking if any data exists in data file and if it is object type
Line 17: Loop on data object
Line 18: Calling Firestore API to store each document
Line 19: Setting collection name
Line 20: Setting document name/key. Please do not provide document key if you want auto generated document key.
Line 21: Setting document data
Line 23: Success message, after document successfully stored in Firestore database
Line 26: Error message if fail
If everything is fine you should see an output like below, after running your program.
Thats all, we are done.
happy coding …
a passionate coder …
See all (7)
1.8K 
33
1.8K claps
1.8K 
33
a passionate coder …
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/thinking-design/6-reasons-why-graphic-designers-should-learn-ux-design-7f129c530d7c?source=search_post---------35,"There are currently no responses for this story.
Be the first to respond.
More than ever, it seems UX design is top of mind for many organizations. The importance of good UX design continues to transform the design industry. Previously, we discussed tips on how to move from graphic designer to UX designer (5 Tips for Graphic Designers Switching to UX Design). Today we’re going to talk about less radical change — why it’s important for a graphic designer to gain UX design skills.
There are many reasons why graphic designers should learn UX design, and here are 6 of them.
A key difference between graphic design and UX design is the ability to translate user needs into product-based solutions. While graphic design surely does take user needs and preferences into account, UX design really drills down on what the user really needs, and why they need it. This requires UX designers to spend significantly more time with users. Direct involvement in the UX design process leads to the better design solutions and satisfaction from seeing the impact of your work.
Graphic design and UX design are dramatically different when it comes to the end result of the work being done. Graphic designers create visual appeal, while UX designers create outcomes.
It’s possible to create a product with a great visual appeal, but with bad UX. For example, if a mobile app looks great, but it’s hard to use, that’s good graphic design with a bad UX. If the app works well but looks terrible, it’s good UX with bad graphic design. By learning UX design disciplines, graphic designers can create design solutions that both look and work great.
One of the most common misconceptions about UX design is that good usability is more important than aesthetics. On the contrary, attractive things work better. Good aesthetics can improve the overall user experience of a product: it creates a positive first impression and shows that the person behind the design cares. That’s why having a good graphic design and UX design skills help designers to create both a pixel-perfect and user-focused design at the same time.
Graphic design is a specialized discipline, and there is a certain level of craftsmanship required to produce great visuals (such as mastering typography, space, shape and color theory).
UX design, on the other hand, sits at the crossroads of a lot of fields and involves many schools of knowledge. UX designers need to have an understanding of human psychology, interaction design, information architecture, user research techniques and many other disciplines in order to create the right solutions to a user’s problems.
Learning UX design is an opportunity for graphic designers to the understand many different aspects of design and unpack big-picture thinking.
As you probably know, the UX design process is an iterative problem-solving process and it can be very different from the usual graphic designer process. The UX design process usually starts with the identification of a problem; the problem is often defined during a product research phase. This research informs the product’s design. There is no point in solving problems (i.e. creating a design solution) that users don’t care about; they won’t pay to solve those problems. Having UX research skills helps graphical design to focus on what’s really valuable for the users.
Also, there’s a significant difference in the prototyping phase. The prototyping phase of the UX design process can be best simplified with four words: build, test, learn, iterate. Incorporating this practice into the design process helps graphic designers to test their designs and improve them based on feedback.
A huge part of the UX design job involves stepping away from working in isolation and dealing with people. Great designs come from collaboration — with users, team members, stakeholders. The UX designer role requires stellar communication and collaboration skills. Whether at a startup or large corporation, it’s required for a designer to work intimately with almost everyone involved in creating the product. Practicing UX design will help you to develop good communication skills — you’ll be able to communicate effectively, fairly, and clearly to people at all levels of knowledge and experience.
Learning UX design is important to keep pace with the shifting job market. There is increased demand for UX designers today: the UX design job market has grown significantly in the last year, and it’s showing no signs of slowing down. In fact, 73% of managers and department heads working in UX design said they plan to double the number of UX designers in their organization in the next five years. Companies realize the ROI when they hire good designers: utilizing top design talent helps companies grow faster through better user experiences. Learning UX design skills can, therefore, boost your career or help you find a new opportunity.
UX design goes far beyond canvases and into real life experiences that change the way we do everyday things. Learning UX design is an opportunity for a graphic designer looking to switch up their career path, or expand their skillset. It’s important to remember that today’s most in-demand designers don’t just create products that look good — they craft amazing experiences.
Nick Babich is a developer, tech enthusiast, and UX lover. He has spent the last 10 years working in the software industry with a specialized focus on development. He counts advertising, psychology, and cinema among his myriad interests.
Originally published at blogs.adobe.com.
Learn about Adobe XD, our all-in-one design and prototyping tool:
Stories and insights from the design community.
1.8K 
8
1.8K claps
1.8K 
8
Stories and insights from the design community.
Written by
New Tools for New Creatives. Get all the latest creative apps plus seamless ways to share and collaborate. All right on your desktop.
Stories and insights from the design community.
"
https://medium.com/linedevth/%E0%B8%AA%E0%B8%A3%E0%B9%89%E0%B8%B2%E0%B8%87-line-bot-%E0%B8%94%E0%B9%89%E0%B8%A7%E0%B8%A2-messaging-api-%E0%B9%81%E0%B8%A5%E0%B8%B0-cloud-functions-for-firebase-20d284edea1b?source=search_post---------36,"There are currently no responses for this story.
Be the first to respond.
Top highlight
ยุคนี้หลายคนคงรู้จัก Bot หรือ Chatbot บนแพลตฟอร์มของ LINE กันแล้ว เพราะทาง LINE ได้เปิด Messaging API ให้เราใช้งานมาตั้งแต่ปี 2016 และมีนักพัฒนาในไทยสนใจสร้าง Bot เพิ่มขึ้นเรื่อยๆ
ในการพัฒนา LINE Bot เราจำเป็นต้องมี Webhook ที่เป็น SSL หรือ HTTPS ซึ่งหลายบทความที่เคยอ่านมา ก็จะใช้ Heroku และ ngrok ซะส่วนใหญ่ แต่บทความนี้ผมจะเลือกใช้ Cloud Functions for Firebase มาสร้าง เพราะ…ผมชอบ เอ้ย! เพราะสะดวกในการเชื่อมต่อ database ทั้ง Cloud Firestore หรือ Realtime Database และที่สำคัญคือเชื่อมั่นได้กับ Google infrastructure ที่พร้อม auto scale เมื่อ Bot เรามีผู้ใช้เพิ่มมากขึ้น
ขั้นตอนในการพัฒนา
สำหรับใครที่ยังไม่เคยสร้าง Provider และ Channel มาก่อน แนะนำให้อ่านบทความปฐมบทด้านล่างนี้ แต่ถ้าใครเคยละ ให้ข้ามไปข้อ 2 ได้เลยจ้า
medium.com
ขั้นตอนนี้ ถ้าใครมีโปรเจคกับ Firebase อยู่แล้วให้ข้ามขั้นตอนนี้ไป
ส่วนใครที่ยังไม่เคยมีหรือต้องกาสร้างใหม่ให้ไปที่ Firebase Console จากนั้นกด add project ระบุรายละเอียด และกด Creat project
เมื่อกดเข้าไปในโปรเจคที่เราสร้าง สิ่งที่แรกที่ต้องทำคือ…ให้ไปเปลี่ยนแพลนจาก Spark เป็นเป็น Blaze(Pay as you go) ก่อน เนื่องจาก Cloud Functions for Firebase มีเงื่อนไขว่า หากต้องการไป request ตัว APIs ที่อยู่ภายนอก Google จำเป็นจะต้องใช้แพลนนี้(เราจะต้องไปเรียก Messaging API ของ LINE)
เมื่อใช้แพลน Blaze แล้ว โควต้าฟรีจากแพลน Spark ก็สมทบให้เราเหมือนเดิม
ขั้นตอนนี้ใครลงทั้ง 2 ตัวแล้วให้ข้ามไป หรือตรวจสอบให้แน่ใจอีกครั้งผ่าน command line
สำหรับใครที่ยังไม่ได้ลง หรือไม่พบเลขเวอร์ชัน ให้ไปที่ https://nodejs.org ดาวน์โหลดและติดตั้ง Node.js ตามแต่ละ OS ที่เราใช้งานให้เรียบร้อย (การติดตั้ง Node.js จะได้ npm มาด้วยอัตโนมัติ)
เนื่องด้วยการ deploy และ Test ตัว Cloud Functions for Firebase ทั้งหมดต้องทำผ่าน Firebase CLI ดังนั้นเอา npm ที่ได้มา ติดตั้งโลด
ตรวจสอบสักหน่อยว่าเราติดตั้ง Firebase CLI เรียบร้อยด้วยคำสั่ง
ให้ run คำส่ัง login โดยจะมี browser เด้งมาให้เรา login ก็ให้เรา login ด้วย account ที่เราใช้สร้างโปรเจคใน Firebase
จากนั้นสร้าง directory ว่างๆมาสักอัน(ตัวอย่าง directory ผมชื่อ bot) แล้วให้ shell เข้าไปในนั้นซะ
จากนั้นเริ่มการ init ตัว Cloud Functions for Firebase แบบที่แท้ทรูได้
จะเจอหน้าตาใน terminal แบบนี้ ให้เลือกโปรเจคที่เราต้องการใช้งาน(กรณีมีหลายโปรเจค) จากนั้นกด enter เบาๆ
จากนั้นจะมีคำถามมาอีก 3 คำถามคือ
ซึ่งเมื่อตอบคำถามครบ 3 ข้อ ก็ถือว่าเสร็จขั้นตอน Initial ละ โดย Structure ของโปรเจคที่เราได้จะมีหน้าตาแบบนี้
หลักๆคือเราจะเขียน code ผ่าน index.js เนี่ยหละ
การอัพเดททั้ง Firebase CLI และ firebase-functions SDK นั้นถือเป็นเรื่องสำคัญ ทั้งการใช้งาน feature ใหม่และการแก้ bug ดังนั้นเราจะควรหมั่นอัพเดททั้งคู่ให้เป็นปัจจุบันอยู่เสมอ และก่อนจะ run คำสั่งด้านล่างนี้ อย่าลืม shell ไปที่ directory ชื่อ functions/ ก่อนนะจ๊ะ
ขั้นตอนนี้เราจะพัฒนาระบบให้ตอบสนองกับผู้ใช้ เมื่อผู้ใช้ได้ส่งข้อความมาให้ Bot ก็จะมี Message event วิ่งเข้ามาที่ Webhook พร้อมกับก้อน Object ซึ่งภายในนั้นจะมี replyToken ที่ไว้สำหรับการตอบกลับได้ทั้งแบบ 1:1, group และ room
เริ่มต้นเปิดไฟล์ index.js ขึ้นมา ภารกิจแรกของเราคือ การสร้าง URL ที่มี HTTPS ขึ้นมา ด้วย code ตัวอย่างด้านล่างนี้
ต่อไปก็ deploy โดยการ shell ไปที่ directory ชื่อ functions/ แล้ว run คำสั่งนี้
รอ deploy สัก 10–20 วินาที เราจะได้ URL คืนมาใน terminal เลย ซึ่งมี HTTPS อยู่ และ path แรกก็ชื่อ LineBot ด้วย ซึ่งหากไม่ได้ copy ไว้ ก็ยังสามารถเข้าไปที่ Firebase Console เมนูชื่อ Functions จะเจอชื่อ Function และ URL ใน Dashboard
ทดสอบเอา URL มาเปิดซะหน่อย จ๊ะเอ๋ Hello Wolrd!
เมื่อได้ URL มาอย่างสมใจปอง ก็เอา URL นี้ไปกรอกใน LINE Developer Console ในช่องของ Webhook URL ที่เราได้ปล่อยว่างไปในขั้นตอนที่ 1 จากนั้นกด Update และ Verify ตามลำดับ
เอาหละ ก่อนที่เราจะเขียนโค้ดเพิ่มเติม เรามาศึกษาโครงสร้างของ object ที่เมื่อผู้ใช้ส่ง text หา Bot ในกรณีที่เป็น 1:1 กันก่อน
จากโครงสร้างด้านบน จะมีจุดที่ต้องสนใจ 3 จุดเพื่อนำไป coding ดังนี้
เนื่องจากเราจะต้องมีการ curl ตัว Messaging API ดังนั้นให้เราไปเพิ่ม dependencies ชื่อ request และ request-promise ในไฟล์ package.json ก่อนเป็นอันดับแรก
กลับมาที่ไฟล์ index.js ให้เรา import ตัว dependency เข้ามา
มือปืนลงมือได้ โดยอ้างอิงจากสเปคที่ได้มา เขียนในรูปแบบ Node.js โลด ซึ่งจากโค้ดด้านล่างผมจะเช็คก่อนว่า message->type เป็น text หรือไม่ ถ้าไม่ใช่ text ตัว Bot ก็จะไม่โต้ตอบใดๆกลับไป แต่หากเป็น text ก็จะตอบข้อความเดียวกับที่ผู้ใช้พิมพ์มากลับไป หรือที่เขาเรียกกันว่า Echo Bot นั่นเอง
อย่าลืมเปลี่ยน xxxxx เป็น Channel Access Token จากขั้นตอนที่ 1 ถ้าเรียบร้อยละ จะรอรัยอะ deploy ยาวๆไป
ทดสอบคุยกับตัวเองหน่อยซิ
Response จากการ reply จะเป็นค่าว่าง และมี status = 200 (กรณีไม่มีปัญหา)
ขั้นตอนนี้เราจะพัฒนาระบบส่งข้อความไปหาผู้ใช้แบบ 1 shot ต่อ 1 คน โดยการส่งจะต้องระบุ User ID ของผู้ใช้คนนั้นๆ และสำหรับตัวอย่างนี้ผมจะไปดึงข้อมูลสภาพอากาศจาก OpenWeatherMap ด้วยพิกัดจากรหัสไปรษณี ดังนั้นให้ทุกคนไปสมัครสมาชิกแล้ว copy ตัว API Key ออกมา
การได้มาซึ่ง User ID ปกติจะเก็บมาจาก 2 เหตุการณ์
ตัว userId จะอยู่ภายใน object ชื่อ source ที่มากับ event ซึ่งจะมีลักษณะประมาณนี้
ให้เราแอบเก็บ userId นี้ไว้ในฐานข้อมูลกรณีที่ต้องการส่งข้อความหาผู้ใช้ในภายโอกาสต่อไป
อย่าลืมแก้ค่า x, y, z จากตัวอย่างด้วย
เสร็จแล้วก็ deploy โลด
เมื่อ deploy ไปรอบได้จะได้ URL ใหม่ที่ต่อท้ายด้วย /LineBotPush ให้เอา URL ดังกล่าวไป request ซะ แล้วรอดูผลลัพธ์
Response จากการ pushจะเป็นค่าว่าง และมี status = 200 (กรณีไม่มีปัญหา)
ขั้นตอนนี้เราจะพัฒนาระบบส่งข้อความไปหาผู้ใช้แบบ shot เดียวหลายคน สูงสุด 500 คน โดยการส่งจะต้องระบุ User ID ของกลุ่มผู้ใช้เป้าหมายลงใน array
การได้มาซึ่ง User ID ก็เหมือนกับในข้อที่ 4 เด๊ะ จากนั้นก็มาลงโค้ดกัน โดยตัวอย่างนี้ผมจะรับค่า query string ที่ชื่อว่า text แล้วตรวจสอบว่ามีข้อความส่งมาจริง จากนั้นจึงจะนำข้อความนั้นไปส่งแบบ multicast ให้ผู้ใช้ 2 คนพร้อมกัน ซึ่งจะได้ response กลับมาเป็น JSON
อย่าลืมแก้ค่า x, y, z จากตัวอย่างด้วย
เสร็จแล้วก็ deploy โลด
เมื่อ deploy เรียบร้อยแล้วจะได้ URL ใหม่ที่ต่อท้ายด้วย /LineBotMulticast ให้เอา URL ดังกล่าวไป request ซะ แล้วรอดูผลลัพธ์ได้เลย
Response จากการ multicast จะเป็นค่าว่าง และมี status = 200 (กรณีไม่มีปัญหา)
ขั้นตอนนี้เราจะพัฒนาระบบส่งข้อความไปหาผู้ใช้แบบ shot เดียวโดนทุกคน โดยไม่ต้องระบุ User ID ของกลุ่มผู้ใช้เป้าหมายเลย สะด๊วก สะดวก
หมายเหตุ: เฉพาะ LINE Official Account ที่สามารถใช้งาน Broadcast ได้
โดยตัวอย่างนี้ผมจะรับค่า query string ที่ชื่อว่า text แล้วตรวจสอบว่ามีข้อความส่งมาจริง จากนั้นจึงจะนำข้อความนั้นไปส่งแบบ broadcastให้ผู้ใช้ทุกคนพร้อมกัน ซึ่งจะได้ response กลับมาเป็น JSON
อย่าลืมเปลี่ยน xxxxx เป็น Channel Access Token จากขั้นตอนที่ 1 ถ้าเรียบร้อยละ จะรอรัยอะ deploy ยาวๆไป
เมื่อ deploy เรียบร้อยแล้วจะได้ URL ใหม่ที่ต่อท้ายด้วย /LineBotBroadcast ให้เอา URL ดังกล่าวไป request ซะ แล้วรอดูผลลัพธ์ได้เลย
Response จากการ broadcast จะเป็นค่าว่าง และมี status = 200 (กรณีไม่มีปัญหา)
เอาหละหลังจากเรารู้จักทั้งการ Reply และการ Push แล้ว เรามาลองดูสถานการณ์จริงกันหน่อยว่าทาง LINE จะคำนวนโควต้าข้อความที่ส่งออก(one-way-communication) กันอย่างไร มาดูกัน
จากรูปด้านบนมีลำดับเหตุการณ์ดังนี้
ถึงเวลาคำนวนละว่าเหตุการณ์ตัวอย่างนี้ จะนับโควต้าข้อความที่ส่งออกเท่าไรซึ่งคำตอบก็คือ…นับแค่ 1 ข้อความเท่านั้นคร้าบ จุดนี้ถ้าใครเข้าใจเรื่อง Reply + Push แล้ว ทาง LINE จะนับ 1 แค่ข้อความสุดท้าย(“ผลลัพธ์การค้นหา”) เท่านั้น เนื่องจากข้อความสุดท้ายที่ส่งออกมาไม่มี replyToken เพื่อใช้ในการตอบกลับนั่นเอง…จากตัวอย่างนี้มันจะมีโอกาส ฟรี เลยได้ไหม คำตอบคือ ได้ แค่ไม่ให้ Bot ตอบว่า “รอสักครู่นะคะ” ทุกอย่างก็ไม่มีการคำนวนโควต้าแล้ว ว้าววว!!!
หมายเหตุ: ถ้าเหตุการแชท 1-on-1 นี้เกิดขึ้นบน CMS จะไม่มีค่าใช้จ่ายใดๆทิ้งสิ้น
ขั้นตอนนี้ผมจะขอแนะนำวิธีการตั้งเวลาให้ Bot ทำงานตามที่เรากำหนด…ขอแนะนำให้รู้จัก Cron-Job.org บริการที่จะทำให้เราตั้งเวลา curl ตัว API ได้อัตโนมัติและ FREE(ทีม Firebase ยังแนะนำ)
เข้าไปที่เว็บ https://cron-job.org แล้วสมัครสมาชิกให้เรียบร้อย จากนั้นเข้าไปที่เมนู Members -> Cronjobs -> Create cronjob
ระบุ API ที่ต้องการให้ไป curl ได้เลย หาก API ที่เตรียมมาต้องมีการทำ HTTP authentication ก็ใส่ username และ password ได้ ถัดมาก็ตั้งเวลาละ จากตัวอย่างผมตั้งให้ curl ทุกวันตอน 1 ทุ่ม นอกจากนี้ยังสามารถส่งอีเมลกรณีที่ Cronjob fail และบันทึก response ได้อีกด้วย
คราวนี้ก็ลองนึก use case กับ Bot ของเราดูว่า จะตั้งเวลาให้ Bot ไปหาข้อมูลอะไรมาให้เราบ้าง เช่น ค่าน้ำมัน, ราคาทอง, อัตราแลกเปลี่ยน และอื่นๆอีกล้านแปด
ของดีและฟรีมีอยู่จริงบนโลก ชอบมะ ถ้าชอบก็ให้แม่มาขอเสะ เอ้ย ถ้าชอบก็ใช้ซะ!
Source code ฉบับสมบูรณ์ทั้ง Reply, Push, Multicast และ Broadcast ของบทความนี้สามารถ clone ไปเล่นกันได้ที่
github.com
สำหรับบทความนี้ก็เหมาะมากสำหรับผู้เริ่มต้นที่สนใจพัฒนา Chatbot บนแพลตฟอร์มของ LINE ซึ่งมีผู้ใช้ในไทยกว่า 44 ล้านคน หลังจากนี้ผมจะปล่อยบทความที่เป็น series ของ Messaging API ออกมาเรื่อยๆ ขอให้รอติดตามชมกัน ก่อนจากไปวันนี้ขอฝากคำคมไว้สักหน่อย…มีด…คมมะ…ตึ่งโป๊ะ ไปเล่นที่บ้านไป๊…เอาจริงๆ “การแพ้ที่น่ากลัวที่สุด ก็คือการแพ้ใจตัวเอง” ราตรีสวัสดิ์พี่น้องชาวไทย
Deliver world-class developer experiences and learning through LINE API
987 
29
987 claps
987 
29
Written by
Technology Evangelist at LINE Thailand / Google Developer Expert - 🔥Firebase
Closing the distance. Our mission is to bring people, information and services closer together
Written by
Technology Evangelist at LINE Thailand / Google Developer Expert - 🔥Firebase
Closing the distance. Our mission is to bring people, information and services closer together
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-developers/firebase-google-cloud-whats-different-with-cloud-functions-612d9e1e89cb?source=search_post---------37,"There are currently no responses for this story.
Be the first to respond.
In my prior post about Firebase and & Google Cloud Platform (GCP), I talked about some of the ways that Firebase relates to GCP. The main points were:
I promised to expand more on the third point for each of the products that Firebase augments. Today, it’s Cloud Functions!
If you’re not familiar with Cloud Functions (Firebase, GCP), it’s Google’s serverless compute product that lets you deploy code that responds to HTTP requests and events from other Google products in your project. Event sources include Cloud Storage, Cloud Firestore, Firebase Realtime Database, and many others. So, for example, when a file gets uploaded to your Storage bucket, your code can receive an event for that. And when a document changes in Cloud Firestore, you can respond to that as well.
I, for one, love working with Cloud Functions. In my early app-building days, I always found it to be a pain to try to manage a backend for my app, even if it didn’t have to scale. Today, if I want to make a web API or some other backend, I just write and deploy code to Cloud Functions, and it all Just Works and scales. Now, without the responsibility of managing a backend, I get to spend more time working on the app.
Since my world of development exists primarily in the mobile space, I also spend a bunch of time with Firebase. This means that I prefer a Firebase-centric view of my software development tools. So, when I work with Cloud Functions, I typically do so with the tools and APIs provided by Firebase. However, I find it’s helpful to keep in mind that Cloud Functions is actually a Google Cloud product, visible from Firebase.
I have a project where I’ve already deployed some functions with the Firebase CLI (more on that later). Here’s what the Cloud Functions dashboard looks like in the Firebase console:
You can see all the deployed functions in this project, along with some tabs at the top for more diagnostics. Between all these tabs, you can do all your typical work with Cloud Functions. For the function where the mouse pointer is hovering, there’s an overflow menu with additional options. One of those options is called “Detailed usage stats”, and you can see an icon there that indicates that you’re going to leave the Firebase console if you click it. Clicking the link opens a tab and takes you to the Cloud console (easy to spot with its blue and white UI theme), with even more detailed diagnostics for just that one function. There are also tabs for viewing the source code and other files deployed with the function, as well as a way to test the function.
If you click the “Edit” button at the top, you can change some of the properties of the function:
It’s worth noting that this console page is the only place you can set retries on failure for a background function that was deployed with the Firebase CLI. This is an important configuration that helps make sure your functions are as reliable as possible, assuming you’ve coded it correctly. (The checkbox for retries isn’t visible here — you have to scroll down a bit further.) This console page is also the only place where you can change the memory and timeout configured for your function after it’s already been deployed.
Essentially what we have here is a situation where the Firebase console delegates to the Cloud console for some deeper information and tasks that aren’t normally required during development. From here you can also navigate to the Cloud Functions dashboard inside the Cloud console, which is similar to the equivalent Firebase screen:
If you want to navigate to this dashboard directly, you can always navigate to the Cloud console directly, select your project from the dropdown at the top, then choose the Cloud Functions product in the left menu.
The Firebase console typically seeks to give you a simplified view of Cloud products. As you perform more advanced tasks, you’ll likely end up spending more time in the Cloud console. I find that it’s common to switch between them frequently, depending on my task at hand. Your typical daily work with Cloud Functions won’t be in either console, however. The most common work is writing code and deploying it with a CLI.
As I mentioned before, my view of software development is mostly through the eyes of a mobile developer who uses Firebase for most backend functionality. As such, I tend to prefer the Firebase CLI to deploy to Cloud Functions. You should probably know that there is also a Cloud CLI called gcloud that you can also use to deploy functions. Both are effective at deploying functions.
The Firebase CLI offers some unique conveniences:
gcloud gives you these benefits:
These two CLIs serve mostly different needs, and you can’t use one CLI to deploy all the same code as the other, simply because their deployment configurations are different. However, there’s nothing really preventing you from using both CLIs in tandem, as each provides its own advantages. For example, if you enjoy working with the customized APIs provided for each event source by firebase-functions and the Firebase CLI, you are almost certainly going to use the Firebase CLI for deployment (the APIs don’t work with gcloud). And did I mention those TypeScript type bindings? 😁 However, gcloud gets you closer to the Cloud Functions product itself and lets you perform more power-user actions. So give them both a try and see how they could work well for you.
It’s very common for mobile apps to invoke backend APIs via an HTTP request. Firebase makes this easier for developers by providing a special kind of “callable” function, which is a wrapper around normal HTTP functions. For client apps, Firebase provides a library that makes it easy to directly invoke callable functions without having to manage the details of an HTTP client library. When you use the Firebase SDK to invoke a callable function, these tasks are performed automatically, reducing the amount of boilerplate code you have to write on both the client and backend:
Since Cloud Functions offers HTTP triggers, you can easily use them for webhooks and REST APIs. What your API consumers may not appreciate, however, is the URL that your function is given by default. Assuming that your Cloud project name is “your-project”, and you’ve deploy to region “us-central1”, a function called “helloWorld” will look like this:
https://us-central1-your-project.cloudfunctions.net/helloWorld
It’s not exactly the most memorable URL. If you have a public API for other developers to consume, you might want to attach that to your domain instead. With Firebase Hosting, you can do that. Firebase Hosting is normally used to distribute static web content around the world, but you can also use it as a proxy for Cloud Functions. All you have to do is connect your domain with Firebase Hosting, then connect Firebase Hosting with Cloud Functions and rewrite a path forward to your Cloud Function endpoint. With this, you can have a more strongly branded endpoint:
https://api.your-domain.com/helloWorld
On top of that, you can configure Firebase Hosting to enable edge caching of your function’s response so that it’s served faster, and you don’t pay the cost of a function invocation each time.
Firebase offers unique tools and SDKs for mobile developers who want to use Cloud Functions, building on top of the core product. So, if you do any mobile development, be sure to check out both the Firebase and Cloud perspectives of Cloud Functions.
Engineering and technology articles for developers, written…
1.6K 
5
1.6K claps
1.6K 
5
Engineering and technology articles for developers, written and curated by Googlers. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
firebase-consultant.com, Firebase GDE, engineer, developer advocate, Xoogler
Engineering and technology articles for developers, written and curated by Googlers. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://betterprogramming.pub/are-cloud-certifications-worth-the-sweat-and-tears-d948dbe09352?source=search_post---------38,"Sign in
What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Greg Farrow
Aug 29, 2019·8 min read
If you’ve recently started using a cloud service, such as AWS, GCP or Azure, you’ve probably discovered their certification program. Maybe you’ve been using the cloud for a while, but aren’t sure if taking the certifications is worth the effort.
Taking a certification is hard work, but it can be well worth the effort. At ResponseTap 75% of our Engineers now have one or more certifications, with the other 25% actively…
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/technoesis/about-the-google-professional-cloud-architect-exam-gcp-2019-36f56da9f2d0?source=search_post---------39,"There are currently no responses for this story.
Be the first to respond.
[Updated on January 2021]
The content below was initially published in April 2019, right after I took the Google Professional Cloud Architect exam.
What has happened since 2019? …. Of course, the world has changed quite significantly, but the PCA certification has not. Topics covered in the exam guide are similar and the case…
"
https://coder.today/software-engineer-from-monolith-to-cloud-think-small-f828effc6afc?source=search_post---------40,
https://codeburst.io/organizing-your-firebase-cloud-functions-67dc17b3b0da?source=search_post---------41,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Top highlight
Organize your Cloud Functions for Firebase with scale in mind. Handle a small or huge amount of Cloud Functions without a big index file. Keep a clear overview over all of your triggers.
It’s not long ago that Firebase released Cloud Functions for Firebase. If you never heard of them please take a look here and catch up after you feel ready for a deeper experience or watch the video below.
The shortest explanation of Firebase Cloud Functions in a node environment I can write is: The Firebase Cloud Functions are just a folder in your project with it’s own node modules and a index file where you run your backend code. Your code can be triggered by url requests or realtime-database, storage, and authentication triggers.
Here is an example how a small index file could look like
It sounds great and believe me it is. They take away from us the need to manage a “real” server and focus just on coding. Still we have the freedom to organize our Cloud Functions (backend code) however we want. This freedom can lead to some weird situations like a huge index file or lots of files with strange names to keep in track witch one is for what. No matter how you organize your cloud functions they have to end in the index file. This can be treated as a problem or used as solution.
Like everyone else when we started working with the Cloud Functions there where just few of them. All fit easily into the index file. Why bother? With time more and more functions came to the file so it become pretty large.
Separating the functions in different files was the next step. It was a good solution for a while. You can imagine what happened next. Even this was hard to maintain. We made our first huge mistake. We named the files on what they are doing and not what is triggering them. This made it very hard to find something. This is how our Cloud Functions folder was organised:
Even if you write the functions on your own. After time you forget what they where for or what you want to say with the name. With more developers working on the same project this becomes more confusing. We tried to use some conventions but they can’t be easily predicted to cover all use-cases so they changed over time and old names remained. This resulted in a huge number of files where some where named with older and others with newer conventions.
At that moment we realized that we had to move all the files into separate folders according to different scopes or functionality. Even if we find a way to move them to different folders we would have to make a huge refactoring over all our functions. We realized that as the number of functions raised we need to refactor all of them because our concept of organisation changed. Therefore I started to search for a solution that should remain the same from zero functions to hundreds+ functions. Prepared for easy scaling.
While searching the right way to just put the functions in separate folders I cam over a interesting GitHub discussion. There I found the solution to our problem with organizing the Firebase Cloud Functions. We found there how to make the separation into folders but also how to automate the addition of all functions from those folders to the index file. Without that we would have to add each function from every folder and file to the index file and still end with a huge index file on large projects.
The solution for our problem was the comment from David King in the GitHub discussion mentioned above and a combination of all other comments.
This is the first of four parts of our solution. The second one also comes from David King. It allows us to give the functions camel case names from the path where they are saved.
From the comment we can see that David is probably not using this way to automatically name his functions. At this point I got a clear plan how to organize our Cloud Functions for Firebase.
There we can also see from Nicolas Garnier the right way to write functions outside the index file without having trouble with the admin initialization:
The trick with the admin initialization is to putt it into a try catch .
At this point I wan’t to thank David King and Nicolas Garnier for their comments and help on that GitHub issue and give them credit. They deserve a huge credit for this! Without them I would still have a painful work on handling all my Cloud Functions.
After the initial idea from the comment above I had a very clear plan on how to organize my Cloud Functions. There are four main parts of the solution:
The main concept is that:
The naming of functions should indicate the trigger and not what they are doing.
We already saw the code we can use to automatically load all functions in our index file and name your functions from their location. Here is the whole code of our Firebase Cloud Functions index file:
Don’t forget to install the modules with npm i -S glob, camelcase .
The small changes I made was to add the ignore option to the glob initialization, the modified naming call and the modified file names from functions.js to f.js. The functions are renamed to f just to reduce the typing amount. This is the whole code you need to have in your index file.
To separate the main Cloud Function triggers all realtime database functions should be in a folder called db and the folder structure in it should have exactly the same structure as your Firebase realtime database. The auth and storage triggers should have also their own folders auth and storage .
The filenames in all of the folders should be named as the trigger they are watching. This is how the folder and files should be named if we use the functions.auth.user().onCreate and functions.auth.user().onCreate triggers:
If we watch only the auth functions they would be deployed with the names authOnCreate and authOnDelete .
Let us now deep down to the database functions. Here is the structure if we add some functions to the db folder:
If we deploy this kind of folder structure we would have this Cloud Functions deployed to Firebase:
Because the storage triggers can’t be triggered on a specific path we can name them according to the conditions on witch their code will run. A complete implementation of the mentioned concepts would look like this:
They would deployed look like this:
We can see that just files that end with .f.js will be deployed as functions. That allows us to have helper functions in a separate file in the root of the functions folder or even between the triggers itself. You are also not forced to use only this kind of naming. If you really need specific names you can use them
If you start using a solution like this there should be a clear list of upsides and downsides you have by using it. The upsides are:
The downsides would be:
That are all upsides and downsides I found for now.
If you want to see all this in action we provide a full functional open source demo application called React Most Wanted. It doesn’t have a lot of functions but the best thing about this solution is that it’s the same for small and large projects.
The code from this instruction was working for years now in all of our projects. Now we have a new way to load our functions and while doing that we also enable es6 import and export syntax in all of your functions without breaking the old ones using the old syntax.
The new solution is basicaly the same but now under a npm package called firebase-function-tools . That makes bugfixes and updates to the function loading process much easier.
Before we can use the new method we need to install our package with this command: npm i -S firebase-function-tools
Now the code for loading our firebase functions would look like this:
As you can see now we have just 6 lines of code and the best part is that all of that is enough to use the es6 import and export syntax in every of your functions. We can stey with the naming as we defined it above but now we can have much much less code:
The code above shows a firebase cloud function that deletes data created under a specific path.
For more complex function calls don’t forget that the functions import looks different in es6 :
Ohhh and one more thing ;) The same package will have some helper functions to avoid writing the same code over and over again in different projects. It already supports a thumbnail generator, users maper, counter and I will add more and more usefull tools to it.
If you think that you have a great solution for a common problem with firebase functions feel free to send a PR. They are always welcome :)
Because the latest Firebase version have now a limit for the number of functions you can deploy at once and some of our producton apps have more than allowed we had to find s smooth solution for this. The great people at Firebase already made great solution for this by allowing grouped functions. We just need to implement it in our library.
If you already us hits library it’s just a little parameter away to have grouped functions. Just add a true to your loadFunctions call like here:
That’s it :)
Your functions will be gruped by the first folder. If you follow the instructions in this article you probably have as first level folders db , auth , storage , firestore , https , etc.. That means that all functions in auth will be in the groups “auth” , all in the folder db will be in the group “db” and so on. You can now deploy only (for example) the db functions by calling:
WARNING: If you already deployed all your functions without grouping and enable it later on. Be avare that all your current functions will get renamed because the grouped ones hava a “-” after the group name!!! For example the function authOnCreate will be renamed to auth-onCreate . If you rename a function it will first get deleted and then the new one deployed. I would recommend to do this migration on a time when your application is not used at all or has the minimum usage. Also don’t forget to adopt the names for the callable functions and those use setup in the firebase.json file.
If you still have to much functions in a group I would recommend just to split the group into multiple parts like db1 , db2 , db3 .
While Cloud Functions for Firebase provide a huge flexibility on how to use them, they allow all of us to get messy with the way we organize and store our code. Using the above guidelines you should be able to organize your Cloud Functions code in a scalable and clear way so you can find everything you search for and just forget about the index file. It is very important to name your functions by the triggers and not by their purpose.
I hope that this can help others to organize their Firebase Cloud Functions. It would be great to hear how you organize your Cloud Functions and what you think about this solution. Any comment is welcome.
If you want to keep up with my new articles on Firebase you can follow me on Twitter here. The next one would be probably about authorization with roles and grants.
Bursts of code to power through your day.
2.4K 
37
2.4K claps
2.4K 
37
Written by
Organiser of Google Developer Group Berchtesgadener Land • Creator of React Most Wanted • Working at ICS Logistik • External teacher at UAS Salzburg
Bursts of code to power through your day. Web Development articles, tutorials, and news.
Written by
Organiser of Google Developer Group Berchtesgadener Land • Creator of React Most Wanted • Working at ICS Logistik • External teacher at UAS Salzburg
Bursts of code to power through your day. Web Development articles, tutorials, and news.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@guyernest/building-a-successful-modern-data-analytics-platform-in-the-cloud-4be1946b9cf5?source=search_post---------43,"Sign in
There are currently no responses for this story.
Be the first to respond.
ML-Guy
Oct 20, 2019·12 min read
I worked with dozens of companies migrating their legacy data warehouses or analytical databases to the cloud. I saw the difficulty to let go of the monolithic thinking and design and to benefit from the modern cloud architecture fully. In this article, I’ll share my pattern for a scalable, flexible, and cost-effective data analytics platform in the AWS cloud, which was successfully implemented in these companies.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@davidmytton/how-much-is-spotify-paying-google-cloud-ebb3bf180f15?source=search_post---------44,"Sign in
There are currently no responses for this story.
Be the first to respond.
David Mytton
Mar 7, 2016·4 min read
Two weeks ago, Spotify announced it was migrating from its own datacentres to Google Cloud Platform.
This is a huge win from Google because Spotify is the first major service running at huge scale that is deploying across many of its cloud products (and talking about it). We all know that Snapchat has been running on Google for a while, but since it is primarily on App Engine, Google needed a credible use case for its other services. Now it has one. This is similar to AWS’s Netflix.
"
https://netflixtechblog.com/netflix-cloud-security-detecting-credential-compromise-in-aws-9493d6fd373a?source=search_post---------45,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Top highlight
Will Bengtson, Netflix Security Tools and Operations
Credential compromise is an important concern for anyone operating in the cloud. The problem becomes more obvious over time, as organizations continue to adopt cloud resources as part of their infrastructure without maintaining an accompanying capability to observe and react to these compromises. The associated impacts to these compromises vary widely as well. Attackers might use this for something as straightforward as stealing time and CPU on your instances to mine Bitcoin, but it could be a lot worse; credential compromise could lead to deletion of infrastructure and stolen data.
We on the Netflix Security Tools and Operations team want to share a new methodology for detecting temporary security credential use outside of your AWS environment. Consider your AWS environment to be “all those AWS resources that are associated with your AWS accounts.”
In this post, we’ll show you how to detect compromised AWS instance credentials (STS credentials) outside of your environment. Keep in mind, however, that you could do this with other temporary security credentials, such as ECS, EKS, etc.
Attackers understand where your applications run, as well as common methods of detecting credential compromise. When attacking AWS, attackers will often try to use your captured AWS credentials from within their AWS account. Perhaps you’re already paying attention to invocations of the “dangerous” AWS API calls in your environment — which is a great first step — but attackers know what will get your attention, and are likely to try innocuous API calls first. The obvious next step is to determine if API calls are happening from outside of your environment. Right now, because the more general AWS IP space is well-known, it’s easy to detect if API calls originate from outside of AWS. If they originate from AWS IPs other than your own, however, you’ll need some extra magic. That’s the methodology we’re publicizing here.
To understand our method, you first need to know how AWS passes credentials to your EC2 instance, and how to analyze CloudTrail entries record by record.
We’ll first build a data table of all EC2 assumed role records, culled from CloudTrail. Each table entry shows the instance ID, assumed role, IP address of the API call, and a TTL entry (the TTL helps keep the table lean). We can quickly determine if the caller is within our AWS environment by examining the source IP address of the API call from an instance.
When you launch an EC2 instance with an IAM Role, the AWS EC2 service assumes the role specified for the instance and passes those temporary credentials to the EC2 metadata service. This AssumeRole action appears in CloudTrail with the following key fields:
eventSource: sts.amazonaws.comeventName: AssumeRolerequestParameters.roleArn: arn:aws:iam::12345678901:role/rolenamerequestParameters.roleSessionName: i-12345678901234sourceIPAddress: ec2.amazonaws.com
We can determine the Amazon Resource Name (ARN) for these temporary instance credentials from this Cloud Trail log. Note that AWS refreshes credentials in the EC2 metadata service every 1–6 hours.
When we see an AssumeRole action by the EC2 service, let’s store it in a table with the following columns:
Instance-IdAssumedRole-ArnIPsTTL
We can get the Instance-Id from the requestParameters.roleSessionName field. For each AssumeRole action, let’s check to see if a row already exists in our table. If not, we will create one.
If the row exists, let’s update the TTL to keep it around. At this point we update the TTL; since the instance is still up and running, we don’t want this entry to expire. A safe TTL in this case is 6 hours, due to the periodic refreshing of instance credentials within AWS, but you may decide to make it longer. You can construct the AssumedRole-Arn by taking the requestParameters.roleArn and requestParameters.roleSessionName from the AssumeRole CloudTrail record.
For example, the resulting AssumedRole-Arn for the above entry is:
arn:aws:iam::12345678901:assumed-role/rolename/i-12345678901234
This AssumeRole-Arn becomes your userIdentity.arn entry in CloudTrail for all calls that use these temporary credentials.
Now that we have a table of Instance-IDs and AssumeRole-ARNs, we can start analyzing each CloudTrail record using these temporary credentials. Each instance-id/session row starts without an IP address to lock to (remember, we claimed that with this method, you won’t need to know all your IP addresses in advance).
For each CloudTrail event, we will analyze the type of record to make sure it came from an assumed role. You can do this by checking the value of userIdentity.type and making sure it equals AssumedRole. If it is AssumedRole, we will grab the userIdentity.arn field which is equivalent to the AssumeRole-Arn column in the table. Since the userIdentity.arn has the requestParameters.roleSessionName in the value, we can extract the instance-id and do a lookup in the table to see if a row exists. If the row exists, we then check to see if there are any IPs that this AssumeRole-Arn is locked to. If there aren’t any, then we update the table with the sourceIPAddress from the record and this becomes our IP address that all calls should come from. And here’s the key to the whole method: If we see a call with a sourceIPAddress that doesn’t match the previously observed IP, then
we have detected a credential being used on an instance other than the one to which it was assigned, and we can assume that credential has been compromised.
For CloudTrail events that do not have a corresponding row in the table, we’ll just have to discard these, because we can’t make a decision without a corresponding entry in the table. However, we’ll only face this shortcoming for up to six hours, due to the way AWS handles temporary instance credentials within EC2. After that point we’ll have all of the AssumeRole entries for our environment, and we won’t need to discard any events.
To prevent false positives, you’ll want to consider a few edge cases that impact this approach:
If you look in your CloudTrail records, you may find that you see a sourceIPAddress that shows up as <servicename>.amazonaws.com outside of the AssumeRole action mentioned earlier. You will want to account for these appearing and trust AWS in these calls. You might still want to keep track of these and provide informational alerting.
When you make an API call in a VPC that has a VPC endpoint for your service, the sourceIPAddress will show up as a private IP address instead of the public IP address assigned to your instance or your VPC NAT Gateway. You will most likely need to account for having a [public IP, private IP] list in your table for a given instance-id/AssumeRole-Arn row.
You might have a use case where you attach additional ENI(s) to your EC2 instance or associate a new address through use of an Elastic IP (EIP). In these cases, you will see additional IP(s) show up in CloudTrail records for your AssumedRole-Arn. You will need to account for these actions in order to prevent false positives. One way to address this edge case is to inspect the CloudTrail records which associate new IPs to instances and create a table that has a row for each time a new IP was associated with the instance. This will account for the number of potential IP changes that you come across in CloudTrail. If you see a sourceIPAddress that does not match your lock IP, check to see if there was a call that resulted in a new IP for your instance. If so, add this IP to your IP column in your AssumeRole-Arn table entry, remove the entry in the additional table where you track associations, and do not alert.
You might be asking the question: “Since we set the lock IP to the first API call seen with the credentials, isn’t there a case where an attacker’s IP is set to the lock IP?” Yes, there is a slight chance that due to this approach you add an attacker’s IP to the lock table because of a compromised credential. In this rare case, you will detect a “compromise” when your EC2 instance makes its first API call after the lock of the attacker’s IP. To minimize this rare case, you might add a script that executes the first time your AWS instance boots and makes an AWS API call that is known to be logged in CloudTrail.
The methodology we’ve shared here requires a high level of familiarity with CloudTrail, and how AssumeRole calls are logged. However, there are several advantages, including scalability, as your AWS environment grows and your number of accounts increases, and simplicity, since with this method you needn’t maintain a full list of IP addresses allocated to your account. Do bear in mind the “defense in depth” truism: this should only constitute one “layer” of your security tactics in AWS.
Be sure to let us know if you implement this, or something better, in your own environment.
Will Bengtson, for Netflix Security Tools and Operations
Learn about Netflix’s world class engineering efforts…
1.2K 
7
1.2K claps
1.2K 
7
Written by
Learn more about how Netflix designs, builds, and operates our systems and engineering organizations
Learn about Netflix’s world class engineering efforts, company culture, product developments and more.
Written by
Learn more about how Netflix designs, builds, and operates our systems and engineering organizations
Learn about Netflix’s world class engineering efforts, company culture, product developments and more.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/every-google-cloud-product-described-in-4-words-or-less-4d3f37f4567b?source=search_post---------46,"There are currently no responses for this story.
Be the first to respond.
28 new products — the Github link shows which ones are new
Download PDFs, text, and hi-res PNGs from https://github.com/gregsramblings/google-cloud-4-words
‪Includes Google Cloud, Firebase, Apigee, Google Maps Platform, and G Suite APIs
Also tweeted at https://twitter.com/gregsramblings/status/1151881674361204738
Check my blog for other resources — https://gregsramblings.com
Google Cloud community articles and blogs
1.3K 
16
1.3K claps
1.3K 
16
Written by
Google Developer Relations — Living in San Francisco https://gregsramblings.com | https://cloud.google.com | http://instagram.com/gregsimages
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Google Developer Relations — Living in San Francisco https://gregsramblings.com | https://cloud.google.com | http://instagram.com/gregsimages
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/@khreniak/cloud-firestore-security-rules-basics-fac6b6bea18e?source=search_post---------47,"Sign in
There are currently no responses for this story.
Be the first to respond.
Kacper Hreniak
Mar 21, 2019·4 min read
Articles:
Cloud Firestore Security Rules is a tool to define access control to your Firestore. You don’t have to worry about creating an authorization or authentication code for your database. In the dashboard of the Cloud Firestore Security Rules define matches to your collections or subcollections and create conditions for each of them to manage access to the Firestore.
"
https://medium.com/@odedia/comparing-kubernetes-to-pivotal-cloud-foundry-a-developers-perspective-6d40a911f257?source=search_post---------48,"Sign in
There are currently no responses for this story.
Be the first to respond.
Oded Shopen
Dec 29, 2017·21 min read
Author’s disclosure: I’m currently a Sr. Platform Architect at Pivotal, however I’ve written this article almost a whole year before joining the company. It is based on my own unsolicited experience working with the platform as a vendor for a third party.
I’ve been developing on Pivotal Cloud Foundry for several years. Working with the Spring boot stack, I was able to create CI/CD pipelines very easily and deployments were a breeze. I found it to be a truly agile platform (is that still a word in 2018?).
On the other hand, Kubernetes is gaining serious traction, so I’ve spent a few weeks trying to get a grasp on the platform. I’m in no means as experienced working with Kubenertes as I am in Cloud Foundry, so my observations are strictly as a novice to the platform.
In this article I will try to explain the differences between the two platforms as I see them from a developer’s perspective. There will not be a lot of internal under-the-hood architecture diagrams here, this is purely from a user experience point of view, specifically for a Java developer that is novice to either platform.
Such a comparison is also inherently subjective, so although I will try to provide objective comparison for any aspect, I will also give my personal thoughts on which aspect works best for me. These may or may not be conclusions you would agree with.
Cloud Foundry is a cloud-agnostic platform-as-a-service solution. The open source cloud foundry is developed and supported by the cloud foundry foundation, which includes the likes of Pivotal, Dell EMC, IBM, VMWare, and many others. There are enterprise versions developed based on the open source project, such as IBM Bluemix and Pivotal Cloud Foundry (PCF for short).
Kubernetes is an open source cloud platform that originated from Google’s Project Borg. It is sponsored by the Cloud Native Computing Foundation, whose members include top names of the industry such as AWS, Azure, Intel, IBM, RedHat, Pivotal and many others.
Kubernetes is first and foremost a container runtime. Although not limited to it, it is mostly used to run docker containers. There are several solutions that offer a PaaS experience on top of Kubernetes, such as RedHat OpenShift.
First and foremost, Cloud Foundry is a PaaS. I don’t feel Kubernetes fits this description. Some regard it as an IaaS+. Even Kubernetes’ own documentation describes itself as “not a traditional, all-inclusive PaaS”.
As a developer, the biggest differentiator for me is how Cloud Foundry takes a very Spring-like, opinionated approach to development, deployments and management.
If you ever used Spring Boot, you know that one of its strengths is the ability to auto-configure itself just by looking at your maven/gradle dependencies. For example, if you have a dependency on mysql JDBC driver, your Spring Data framework would auto-configure to use mysql. If no driver is provided, it would fallback to h2 in-memory database.
As we’ll see in this article, PCF seems to take a similar approach for application deployment and service binding.
Kubernetes takes a different approach. It is inherently a generic container runtime that knows very little about the inner-workings of your application. Its main purpose is to provide a simple infrastructure solution to run your container, everything else is up to you as a developer.
Kubernetes runs Docker containers. As such, it supports a very wide range of applications, from a message broker to a Redis database to your own custom java application to anything you can find on Docker Hub.
Anyone who had a chance to write a Dockerfile knows it can be either a trivial task of writing a few lines of descriptor code, or it can get complicated rather quickly. Here’s a simple example I pulled off of github, and that’s a fairly simple example:
This example should not seem intimidating to the average developer, but it does immediately show you there is a learning curve here. Since Docker is a generic container solution, it can run almost anything. It is your job as the developer to define how the operating system inside the container will execute your code. It is very powerful, but with great power comes great responsibility.
Cloud Foundry takes a very opinionated approach to containers. It uses a container solution called garden. The original container in earlier versions of PCF was called warden, which actually predates docker itself.
Cloud foundry itself actually predates Kubernetes. The first release was in 2011, while kubernetes is available since 2014.
More importantly than the container runtime being used, is how you create a container.
Let’s take the example of a developer that needs to host a Spring Boot Java application.
With docker, you should define a Dockerfile to support running a java-based application. You can define this container in many different ways. You can choose different base operating systems, different JDK versions from different providers, you can expose different ports and make security assumptions on how your container would be available. There is no standard for what a java-based Spring Boot application container looks like.
In Cloud Foundry, you have one baseline buildpack for all java-based applications, and this buildpack is provided by the vendor. A buildback is a template for creating an application runtime in a given language. Buildpacks are managed by cloud foundry itself.
Cloud Foundry takes the guesswork that is part of defining a container out of the hands of the developer. It defines a “standard” for what a java-based container should look like, and all developers, devops teams and IT professionals can sync-up with this template. You can rest assured that your container will run just like other containers provided by other developers, either in your existing cluster or if you’ll move to a public cloud solution tomorrow.
Of course, sometimes the baseline is not enough. For example — you might want to add your own self-signed SSL certificates to the buildpack. You can extend the base buildpack in these scenarios, but that would still allow you to use a shared default as the baseline.
Continuing with the opinionated approach, Cloud foundry can identify which buildpack to use automatically, based on the contents of the provided build artifact. This artifact might be a jar file, a php folder, a nodejs folder, a .NET executable etc. Once identified, cloud foundry will create the garden container for you.
All this means that with PCF, your build artifact is your native deployment artifact, while in Kubernetes your build artifact is a docker image. With Kubernetes, you need to define the template for this docker image yourself in a Dockerfile, while in PCF you get this template automatically from a buildpack.
PCF separates the web dashboard to two separate target audiences.
Kubernetes takes a different approach. You get one dashboard to manage everything. Here’s a typical Kubernetes dashboard:
As you can see from the left-hand side, there is a lot of data to process here. You have access to persistent volumes, daemons, definition of roles, replication controllers etc. It’s hard to focus on what are the developer’s needs and what are the IT needs. Some might tell you this is the same person in a Devops culture, and that’s a fair point. Still, in reality-it is a more confusing paradigm compared to a simple application manager.
Cloud foundry uses a command line interface called cf. It is a cli that lets you control all aspects of the developer interaction. Following in the footsteps of simplicity that you might have already noticed, the idea is to take an opinionated view to practically everything.
For example, if you are in a folder that contains a spring boot jar file called myapp.jar, you can deploy this application to PCF with the following command:
That’s it! That’s all you need. PCF will lookup the current working directory and find the jar executable. It will then update bits to the platform, where the java buildpack would create a container, calculate the required memory settings, deploy it to the currently logged-in org and space in PCF, and set a route based on the application name:
Although you can start with barely any intervention, this doesn’t mean you give up any control. You have a lot of customizations available in PCF. You can define your own routes, set the number of instances, max memory and disk space, environment variables etc. All of this can be done in the cf cli or by having a manifest.yml file available as a parameter to the cf push command. A typical manifest.yml file can be as simple as the following:
The main takeaway is this: with PCF, provide the information you know, and the platform will imply the rest. Cloud Foundry’s haiku is:
Here’s my code
Run it on the cloud for me.
I don’t care how.
In kubernetes, you interact with the kubectl cli. The commands are not complicated at all, but there is still a higher learning curve from what I’ve experienced so far.
For starters, a basic assumption is that you have a private docker registry available and configured (unless you only plan to deploy images available on public registries such as docker hub). Once you have that registry up and running, you will need to push your Docker image to that registry.
Now that the registry contains your image, you can initiate commands to kubectl to deploy the image. Kubernetes documentaiotn gives the example of starting up an nginx server:
The above command only spins up a kubernetes pod and runs the container.
A pod is an abstraction that groups one or more containers to the same network ip and storage. It’s actually the smallest deployable unit available in Kubernetes. You can’t access a docker container directly, you only access its pod. Usually, a pod would contain a single docker container, but you can run more. For example, an application container might want to have some monitoring dameon container in the same pod.
In order to make the container accessible to other pods in the Kubernetes cluster, you need to wrap the pod with a service:
Your container is now accessible inside the kubernetes cluster, but it is still not exposed to the outside world. For that, you need to wrap your service with an ingress.
Note: Ingress is still considered a beta feature!
I could not find a simple command to expose an ingress at this point (please correct me if I’m wrong!). It appears that you must create an ingress descriptor file first, for example:
Once that file is available, you can create the ingress by issuing a command
Note that unlike the single manifest.yml in PCF, the deployment yml files in Kubernetes are separated — there is one for pod creation, one for service creation and as you saw above — one for ingress creation. A typical descriptor file is not entirely overwhelming but I wouldn’t call it the most user friendly either. For example, here’s a descriptor file for nginx deployment:
All this to say — with kubernetes, you need to be specific. Don’t expect deployments to be implied. If I had to create a haiku for Kubernetes, it’ll be probably something like this:
Here’s my code
I’ll tell you exactly how you should run it on the cloud for me
And don’t you dare make any assumptions on the delployment without my written consent!
Both platforms support the ability to deploy applications with zero downtime, however this is one area where Kubernetes wins in my opinion, since it provides a built-in mechanism for zero downtime deployments with rollback.
2019 update: As of Pivotal Cloud Foundry 2.4, native zero-downtime deployments are available out of the box!
With Pivotal Cloud Foundry, t̶h̶e̶r̶e̶’̶s̶ ̶n̶o̶ ̶b̶u̶i̶l̶t̶-̶i̶n̶ ̶m̶e̶c̶h̶a̶n̶i̶s̶m̶ ̶t̶o̶ ̶s̶u̶p̶p̶o̶r̶t̶ ̶a̶ ̶r̶o̶l̶l̶i̶n̶g̶ ̶u̶p̶d̶a̶t̶e̶, you’re basically expected to do some cf cli trickery to perform the update with zero downtime. The concept is called blue-green deployment. If I had to explain it in step-by-step guide, it’ll probably be something like this:
Note: The cf cli supports plugin extensions. Some of them provide automated blue-green deployments, such as blue-green-deploy, autopilot and zdd. I personally found blue-green-deploy to be very easy and intuitive, especially due to its support for automated smoke tests as part of the deployment.
kubectl has a built-in support for rolling updates. You basically pass a new docker image for a given deployment, for example:
The command above tells kubernetes to perform a rolling update between all pods of the kubernetes-bootcamp deployment from its current image to the new v2 image. During this rollout, your application remains available.
Even more impressive — you can always revert back to the previous version by issuing the undo command:
As we saw previously, both PCF and Kubernetes provide load balancing for your application instances/pods. Once a route or an ingress is added, your application is exposed to the outside world.
If we’ll take an external view of the levels of abstraction that are needed to reach your application, we can describe them as follows:
PCF supports two methods of load balancing inside the cluster:
Eureka runs as its own service in the PCF environment. Other applications register themselves with Eureka, thereby publishing themselves to the rest of the cluster. Eureka server maintains a heartbeat health check of all registered clients to keep an up-to-date list of healthy instances.
Registered clients can connect to Eureka and ask for available endpoints based on a service id (the value of spring.application.name in case of Spring Boot applications). Eureka would return a list of all available endpoints, and that’s it. It is up to the client to actually access one of these endpoints. That is usually done using frameworks like Ribbon or Feign for client-side load balancing, but that is an implementation detail of the application, and not related to PCF itself.
Client-side load balancing can theoretically scale better since each client keeps a cache of all available endpoints, and can continue working even if Eureka is temporarily down.
If your application already uses Spring Cloud and the Netflix OSS stack, PCF fits your needs like a glove.
Kubernetes uses DNS resolution to identify other services within the cluster. Inside the same namespace, you can lookup another service by its name. In another namespace, you can lookup the service’s name followed by a dot and then the other namespace.
The major benefit that Kubernetes’ load balancing offers is not requiring any special client libraries. Eureka is mostly targeted at java-based applications (although solutions exist for other languages such as Steeltoe for .NET). With Kubernetes, you can make load-balanced http calls to any Kubernetes service that exposes pods, regardless of the implementation of the client or the server. The load-balancing domain name is simply the name of the service that exposes the pods. For example:
And the API would load-balance over the available instances. It doesn’t matter if your client is written in Java, PHP, Ruby, .NET or any other technology.
2018 update: Pivotal Cloud Foundry now supports polyglot, platform-managed service discovery similar to Kubernetes, using Envoy proxy, apps.internal domain and BOSH DNS.
PCF offers a services marketplace. It provides a ridiculously simple way to bind your application to a service. The term service in PCF is not the same as a service in kubernetes. A PCF service binds your application to things like a database, a monitoring tool, a message broker etc. Some example services are:
Third party vendors can implement their own services as well. Some of the vendor offerings include MongoDB Enterprise and Redislabs for Redis in-memory database. Here’s a screenshot of available services on Pivotal Web Services:
IBM Bluemix is another Cloud Foundry provider that offers its own services such as IBM Watson for AI and machine learning applications.
Every service has different plans available based on your SLA needs, such as a small database for development or a highly-available database for a production environment.
Last but not least, you have the option to define user-provided services. These allow you to bind your application to an existing service that you already have, such as an Oracle database or an Apache Kafka message broker. A user provided service is simply a group of key-value pairs that you can then inject into your application as environment variables. This offloads any specific configuration such as URLs, usernames or passwords to the environment itself — services are bound to a given PCF space.
Kubernetes does not offer a marketplace out of the box. There is a service catalog extension that allows for a similar service catalog, however it is still in beta.
Note that since it can run any docker container — Dockerhub can be considered as a kubernetes marketplace in a way. You can basically run anything that can run in a container.
Kubernetes does have a concept similar to user-provided services. Any configuration or environment variables can exist in ConfigMaps, which allow you to externalise configuration artifacts away your container, thus making it more portable.
Speaking of configuration — One of the features of the Spring Cloud Services service is Spring Cloud Config. It is another service that is targeted specifically for Spring Boot applications. The config service serves configuration artifacts from a git repository of your choosing, and allows for zero-downtime configuration changes. If your Spring beans are annotated with @RefreshScope, they can be reloaded with updated configuration by issuing a POST /refresh API call to your application. The property files that are available as configuration sources are loaded based on a pre-defined loading order, which provides some sort of an inheritance-based mechanism to how the configuration is loaded. It’s a great solution, but again assumes that your applications are based on the Spring Cloud (or .NET Steeltoe) stack. If you’re already using spring boot with a config server today — PCF fits like a glove.
In Kubernetes, you can still run a config server as a container, but that would probably become an unneeded operational overhead since you already have built-in support for ConfigMaps. Use the native solution for the platform you go with.
A big differentiator of Kubernetes is the ability to attach a storage volume to your container. Kubernetes uses etcd as a means to manage storage volumes, and you can attach such a volume to any of your containers. This means you get a reliable storage solution, which lets you run storage-based containers like a database or a file server.
In PCF, your application is fully stateless. PCF follows the 12-factor apps model and one of these models assumes that your application has no state. You should theoretically take the same application that runs today in your on-prem data center, move it to AWS, and provided there is adequate connectivity, it should just work. Any storage-based solution should be offloaded to either a PCF service, or to a storage solution outside the PCF cluster itself. This may be regarded as an advantage or a disadvantage depending on your application and architecture. For stateless application runtimes such as web servers, it is always a good idea to decouple it from any internal storage facility.
Getting started with Kubernetes was not easy. As mentioned above, you can’t just start with a 5-minutes quick start guide, there are just too many things you need to know and too many assumptions about what you already have (docker registry and a git repository are often taken for granted).
Just taking a look at the excellent Kubernetes Basics interactive tutorial shows the level of knowledge required on the platform. For a basic on-boarding, there are 6 steps, each one of them containing quite a few commands and terminologies you need to understand. Trying to follow the tutorial on a local minikube vm instead of the pre-configured online cluster is quite difficult.
Getting started with PCF is easy. Your developers already know how to develop their spring boot / nodejs / php / ruby / .NET application. They already know what its artifacts are. They probably already have some jenkins pipeline in place. They just want to run the same thing in a cloud environment.
If we’ll take a look at PCF’s “Getting Started with Pivotal Cloud Foundry”, it’s almost comical how little is required to get something up and running. When you need more complex interaction, it’s all available for you, either in the cf cli, as part of a manifest.yml, or in the web console, but this doesn’t prevent you from getting started quickly.
If you mainly develop server-based applications in java or nodejs, PCF gets you to the cloud simply, quickly and more elegantly.
Kubernetes is truly a great open source platform. Kudos to Google for giving up control and letting the community do its thing. That’s probably the number one reason why Kubernetes has taken off so quickly while other solutions like Docker swarm are falling behind. Other vendors also offer solutions that offer a more PaaS-like experience on top of Kubernetes, such as RedHat OpenShift.
But with such a diverse and thriving eco-system, the path forward can be one of many different directions. It really does feel like a Google product in a way — maybe it will remain supported by Google for years, maybe it will change with barely any backwards compatibility, or maybe they’ll kill it and move to the next big thing (Does anyone remember Google Buzz, Google Wave or Google Reader?). Any AngularJS developer who’s trying to move to Angular 5 can tell you that backwards compatibility is not a top priority.
Cloud Foundry is also a thriving open source platform, but it is pretty clear who sets the tone here. It is Pivotal, with additional contributions from IBM. Yes, it’s open source, but the enterprise play here is Pivotal Cloud Foundry, which provides added value like the services marketplace, Ops Manager etc. And on that side, it’s a limited democracy. This is a product that is meant to serve enterprise customers, and the feature set would first and foremost answer those needs.
If Kubernetes is Google, then PCF is Apple.
A little more of a walled garden, more controlled, better design/experience layer, and a commitment for delivering a great product. I feel like the platform is more focused, and focus is critical in my line of work.
The real surprise of the recently announced PCF 2.0 was that all I’ve been talking about throughout this article is now just one part of a larger offering. The application runtime (everything that is referred to as PCF in this article) is now called Pivotal Application Service (PAS). There is also a new serverless solution called Pivotal Function Service (PFS), and lastly — a new Kubernetes runtime called Pivotal Container Service (PKS). This means that Pivotal Cloud Foundry now gives you the best of both worlds: A great application runtime for fast onboarding of cloud-native applications, as well as a great container runtime when you need to develop generic low-level containers.
In this article I tried to share my personal experiences of working with both platforms. Although I am a bit biased towards PCF, it is for a good reason — it has served me well. I approached Kubernetes with an open mind, and found it to be a very versatile platform, but also one that requires a steeper learning curve. Maybe I got spoiled by living in the Spring eco-system for too long :). With the latest announcement of PKS, it appears that Pivotal Cloud Foundry is set to offer the best integrated PaaS — one that lets you run cloud native applications as quickly and simply as possible, while also exposing the best generic container runtime when that is needed. I can see this becoming very useful in many scenarios. For example, Apache Kafka is one of the best message brokers available today, but this message broker still doesn’t have a PCF service available, so it has to run externally on virtual machines. Now with PCF 2.0, I can run Apache Kafka in docker containers inside PCF itself.
The main conclusion is that this is definitely not a this or that discussion. Since both the application runtime and the container runtime now live side by side in the same product, the future seems promising for both.
Thank you for reading, and happy coding!
Oded Shopen
Visit my homepage: http://odedia.org
Follow me on twitter: https://twitter.com/odedia
Follow me on LinkedIn: https://www.linkedin.com/in/odedia/
Follow me on 500px: https://500px.com/odedia
Sr. Platform Architect @pivotal . I enjoy 📷, 🎥 editing, and the 🎵 of the King of Pop. Views are my own. http://odedia.org
1.2K 
16
1.2K 
1.2K 
16
Sr. Platform Architect @pivotal . I enjoy 📷, 🎥 editing, and the 🎵 of the King of Pop. Views are my own. http://odedia.org
"
https://medium.com/starts-with-a-bang/is-humanity-ignoring-our-first-chance-for-a-mission-to-an-oort-cloud-object-51205de499b5?source=search_post---------49,"There are currently no responses for this story.
Be the first to respond.
In 2003, scientists discovered an object beyond Neptune that was unlike any other: Sedna. While there were larger dwarf planets…
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@dabit3/aws-appsync-up-and-running-560a42d96ba7?source=search_post---------50,"Sign in
There are currently no responses for this story.
Be the first to respond.
Top highlight
Nader Dabit
Jan 3, 2018·9 min read
The AWS AppSync GraphQL service offers real-time updates and built-in offline support along with first class integration with ElasticSearch, DynamoDB, and AWS Lambda.
The way that I would describe AppSync is that it is basically a Firebase like service but with more comprehensive database query and search features, as well as a fully managed GraphQL API for your application.
Right now, AppSync has sdks for native iOS, web, and React Native, with Android coming soon.
In this post, I will walk through using AWS AppSync to create a new API endpoint that will provision a DynamoDB table, and walk through the starter project that AppSync provides and discuss how everything is wired together.
We will then look at how you can use AppSync to create new Schemas that will automatically provision additional tables to correlate to the new resources. We will also take a look at how to use resolvers and mapping templates to create custom requests and responses from your GraphQL schemas.
This post will assume you have an AWS account.
The first thing you need to do is log in to the AWS Console and click on AWS AppSync in the Mobile Services section.
Once you are logged into the AppSync console, you will see a screen, which will allow you to go ahead and create a new API (figure 1). If this is your first time here and you have no APIs created, this screen will look different, but will still have the orange Create API button.:
I will click the orange Create API button and create a new API called EventsApp, and will choose the Event App template which will automatically create a nice basic example schema which we can use to get started, then scroll down and click the orange Create button.
Next, we will be brought to the dashboard for the API we just created. Here, we will see a menu on the left for us not only to explore and edit the schema, but also to test out Queries, add and view existing data sources related to the API, and manage settings (Figure 3).
In this view, we see our API endpoint URL, the auth mode (set to API KEY by default), the API ID and a quick getting started guide for iOS, React Web, and React Native.
We will start by cloning the Web project and plugging in our newly created credentials.
Go ahead and scroll to the bottom of the Getting Started guide and choose Web (React), and copy the cloned repo url (Figure 3).
In a working folder from your command line, let’s go ahead and clone the project:
Next, go ahead and install the dependencies using either npm or yarn:
or
Now, we need to add our our AppSync configuration file.
To download the file, click the orange Download button right below where we cloned the repo and download the file into the src folder.
Now, we should be able to go ahead and run the project.
From the root of the directory, run npm start.
We should see our project running in the browser, and we should now be able to add new events (Figure 4)! Go ahead and add at least one event.
Now, we should be able to go into our Data Sources and see the new item added to our database.
In the dashboard, click Data Sources and then click on AppSyncEventTable and then click on the items tab (Figure 5).
Let’s now open up our editor to see how all of this is put together, then we’ll dive deeper into the AWS AppSync dashboard to view the queries and schema powering this application!
In src/App.js, let’s take a look at what is going on:
Next let’s take a look at src/Components/AllEvents.js and src/GraphQL/QueryAllEvents.js. In these files, you will see a typical GraphQL / Apollo configuration.
The GraphQL query in QueryAllEvents look like this:
This query will fetch an array of event items, each containing an id along with some other information.
Where exactly is this data coming from? Let’s go back into our AppSync console to see what’s going on! Go to the dashboard for the events app and let’s take a look at the Schema that was generated for us when we created the API (Figure 6).
In this schema, we will see the correlating type called Event along with the listEvents query.
We can also see additional types, queries, and mutations for things like deleting an event, getting a single event by id, among other things.
The next question is probably “How does the database deal with the queries and mutations?”.
The answer is resolvers.
Resolvers offer a way to link database tables (in our case, DynamoDB) to individual queries and mutations.
You can view, attach, and edit resolvers from the Schema view by either clicking on an existing resolver or clicking the Attach button next to an existing data type (Figure 7).
Resolvers use a templating language called Velocity Templating Language (VTL) to map GraphQL queries into a format specific to your data source.
A few of the most most basic queries and mutations that you would need to perform, like getting an item by id or putting in item into an array, are already written for you and available in the dropdown menu when you are creating or editing your resolver (Figure 8).
Let’s try walking through the process of creating a new type and query in our schema, and then attaching and testing out a resolver.
In the schema editor, let’s add a new User type then click save (Figure 9).
After clicking save, click the Create Resources button in the top right corner of the screen above the data types.
In the Create Resources screen, choose User as the type from the dropdown menu, and scroll to the bottom and click the Create button (Figure 10).
This do two things:
You should be able to now view your data sources and see a new table, automatically created for you, called UserTable. You should also be able to view the new definitions in the schema.
Now, let’s test out the new table and query!
In the left menu of the dashboard, click on Queries. AppSync has an in-browser tool for writing, validating, and testing GraphQL queries!
Create a new mutation query called createUser:
Then click the orange play button (Figure 11).
Now, you should be able view the new data in the UserTable!
We can also fetch this data using the getUser query.
Try the following query to fetch the newly created user:
You can use AWS IAM policies or Cognito User Pools for their directory, as well as custom rules that are set in the GraphQL resolvers to perform runtime checks of authorization rules.
This allows some pretty powerful capabilities like being able to check if a specific user has access to perform an action on a single piece of data — even at the database row level. For example, you can check if a user named Bob was the creator of a record and only allow Bob to be able to read or edit that record. These access checks can be done before the resolver communicates with a data source or after to perform filtering prior to sending data back to the client.
The Cognito User Pools capabilities also allow for “grouping” of users and configuring the resolvers based on these groups. For example, maybe you have Engineering and HR users but only the Engineering users can invoke a specific resolver.
To learn more, check out the docs here.
If you’re looking to integrate your client UX with AppSync easily, check out AWS Amplify.
This new library also released last year lets you get things like user Registration and Login easily in your app. It manages the AWS IAM and Cognito User Pools objects (like JWT tokens) so that you can pass them to the AppSync service when you want users to authenticate before running resolvers as outlined in the earlier section.
Amplify works great with AWS resources, for instance it will send Analytics to Amazon Pinpoint and also has a Storage component for uploading & downloading data from S3 which can be locked down on a per-user basis with a “private” setting.
AWS designed it to be open and pluggable for usage with other cloud services that wish to provide an implementation or custom backends which is a nice addition to the Open Source community.
Check out this YouTube clip for an overview of Amplify.
To see a general walkthrough of how to create a new AppSync application along with a custom schema, see the following video:
To learn more about AppSync, check out the developer guide as well as the API docs.
Thanks for reading!
I will be doing my best to keep this up to date with the most recent documentation, so feel free to use it as a reference, or to reach out to me at dabit3@gmail.com if you ever have any questions.
Developer Relations Engineer at Edge & Node working with The Graph Protocol
See all (1,974)
2.1K 
7
2.1K claps
2.1K 
7
Developer Relations Engineer at Edge & Node working with The Graph Protocol
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/robinfood/how-we-built-latin-americas-largest-cloud-restaurant-company-during-a-global-pandemic-1c61c2b28045?source=search_post---------51,"There are currently no responses for this story.
Be the first to respond.
I have been an entrepreneur for as long as I can remember, and have experienced my fair share of both successes and failures. In February 2020 RobinFood (formerly Muy) certainly felt like a success.
We were riding the perfect wave, having just raised USD$15.6 MM in our Series B while growing at 600% YOY. During early March, we were hitting new sales records every day, but suddenly the music stopped. By March 24th we found ourselves with all of our stores closed and our operations at a complete standstill.
How it all began
Our story with food-tech started more than a decade ago in 2007. My co-founder, Miguel Mc Allister, and I met in college, where we started Domicilios.com — a company to help restaurants promote their food online. We had no funding, so the two of us did everything — from coding all the way to uploading menus. Our first product focused on restaurants in Bogota, Colombia. In exchange for a monthly fee, restaurants could display their menu, logo and information on our platform.
For a couple of years this was a side project alongside our day jobs, but in 2011 we both decided to quit our jobs to go all-in on this venture. Given the continued growth in web traffic to the site, it felt like the right time to turn this promotional online tool into a transactional one. We moved from only giving information to consumers to actually helping them sell to customers and execute their food orders. We took our first order on April 19th 2012, which turned out to be the starting point for years of uninterrupted growth.
There were several key drivers for this growth. Importantly, we were able to learn how to take advantage of the two biggest internet growth hacking waves of the decade: SEO and mobile apps. Also, we were successful in expanding inorganically throughout LatAm, by acquiring and merging with BuenosAiresDelivery in Argentina, LimaDelivery in Peru, and DeliYami in Ecuador. We gained deep experience in the market while working with the biggest brands in the world. In addition, we were the main partners of McDonald’s, KFC, Pizza Hut and many others in the region.
In 2014 we made what we believe was the right call — partnering with DeliveryHero (DH) and other entrepreneurs that decided to come together to create a dominant global food service player (DH is currently a +$20B USD public company). We all had similar deal terms and ambitions: 100% dedication to grow a great business which we could IPO, allowing us to monetize all of our collective hard work. We were entrepreneurs from Australia, Korea, Turkey, SEA, Greece, Uruguay, MENA, the Nordic countries, Germany, among others, all under Niklas Otsberg’s (DH co-founder and CEO) amazing leadership.
By the time we were preparing our transition from co-founders to executives of a public company, Domicilios.com had more than 90% market share in the online food delivery market in Colombia, Peru, and Ecuador — a region of 100M inhabitants. Nevertheless, Niklas, Delivery Hero´s CEO, was always challenging us, specifically around one fact: while in some MENA regions DH had one order per inhabitant, in our region we barely had 2.5M users (vs the 100M population). Why was the penetration so low? We struggled for years trying to come up with the right answer.
Domicilios.com had achieved milestones that very few had before us: we were among the 20 biggest food delivery companies in the world, reaching more than 1M orders a month, and we had sold our company in the biggest exit of a tech company in Colombian history. Yet we felt frustrated, because we still didn’t have a clear-cut answer to Niklas’s question. We had succeeded beyond our dreams as entrepreneurs but had effectively failed to improve the daily life of Latin America’s emerging middle class.
RobinFood (formerly Muy) was born
With this question lingering in our minds, we started thinking about a potential new startup that could answer it. We knew that the main barrier for Domicilios.com was the price point, where the majority of the offering could only be afforded by the more affluent segments of society, or as an occasional treat. However we wanted to give access to good affordable food to everyone in Latin America. Ultimately, we came to the conclusion that to do so, someone would need to produce and deliver fresh, hot, and delicious food for under 2.5 USD. So we decided to do it.
We found the opportunity to be in both channels: on-premise and off-premise. Unlike the main 10 brands in the U.S. which account for 22% of the total market, in Colombia, Brazil, and Mexico the 10 largest players represent less than 7%. Most of the market is informal and lacks economies of scale and strong processes.
Our business model is based on our Cloud Restaurants, which merge a traditional restaurant and a cloud kitchen by leveraging our proprietary tech ecosystem that integrates all of our customer and supply chain processes. This seamless integration allows us to offer our customers a world-class, multi-brand, omni-channel experience, like no other in our region, while optimizing our food supply chain and operations. In simple terms, we gain advantage by: 1) Using the physical store both as a restaurant, and as a cloud kitchen for multiple brands (increase utilization of real estate); 2) Best in class tech to improve prediction and automatization that reduces costs; 3) Better product and user experience increases loyalty and demand.
As we expand our regional footprint, optimized end-to-end processes will allow us to offer our customers the BEST PRICES in every market, in order to deliver on our goal of making good affordable food available for everyone in Latam.
To start, we decided on a multi-step execution: Step 1 is to create the cloud restaurants network, the physical infrastructure; Step 2 is to increase utilization by launching new virtual and on-site brands and partnerships. Our initial focus was on weekday lunch occasions — the biggest but also the most fragmented segment in Latin America.
Major players in the region are global foodservice brands that don’t target local food and customs. That’s why we created our first brand “Muy”, an everyday food offering where customers have full flexibility to mix and match any of 30 different homemade local ingredients, in order to create more than 6,000 potential combinations. More importantly, they can do this for less than 2 USD per serving, which of course quickly gained a lot of traction (and is something none of the global players match easily, as they target more affluent demographics). We started expanding our footprint and building technology to automate the user experience and production capacity.
By February 2020, RobinFood had expanded its reach to cover a large part of Bogota (a city of 10M inhabitants) with more than 30 cloud restaurants, opened three locations in Mexico (with a menu tailored to the local taste), were ready to launch in Brazil, had launched our first virtual brand in Colombia, and had created a complete tech stack that ranged from AI demand forecasting (that enabled a reduction in food waste from 30% to 2%) to amazing new user products such as our CMS (Contactless Muy Store).
COVID Outbreak
When Covid-19 started spreading across Latam in early March, the key priority for us was the safety and well-being of all RobinFooders (people who work at RobinFood), but obviously at the same time we had to prepare for a situation where the long-term viability of our company was at stake. Massive layoffs were being announced daily by major foodservice players, something we wanted to avoid at all costs as one of our core values is that we all succeed as a team.
We decided to go with a more human approach, knowing that we had to reduce expenses, but also cognizant that everyone was living through a difficult personal situation. We decided to voluntarily reduce the executive teams’ salaries first and wait for the response from the rest of the team. As CEO I started by reducing my salary by 50%. What came next exceeded our most optimistic expectations. We were expecting only people with high salaries to help in this situation, but at the end of the day 85% of all RobinFooders voluntarily reduced their salary, and we were able to meet our burn target without massive layoffs. We knew we had created something worth fighting for, when even the teams in the cloud restaurant operations were making their support clear. This is one of many messages we unexpectedly received:
“Good night, as an employee of Muy (currently RobinFood) and wanting to contribute even a grain of sand to this company that has given me the opportunity to grow in it, I would like to give up 10% of my salary to be able to reduce costs. I would like to be able to help more, but my responsibilities as a single mother prevent me from giving more, since my son depends economically on me almost entirely. However, I hope that this small percentage can help the company in some way, I have faith and hope that everything will improve soon in our country.”
Being pioneers in the online food delivery industry, we have always believed that delivery will account for at least 50% of the total food service market (before Covid-19 penetration was ~10%), and thus we were building the infrastructure to serve the hyperlocal needs of emerging middle class neighborhoods. Delivery penetration jumped to almost 100% in some areas. This was our moment to shift gears from “footprint expansion” to “virtual brand creation”, devoting 90% of our resources to brand creation and partnerships.
From April, we accelerated our brand launches. Currently we have five brands in Colombia, three in Mexico, and we are ready to launch in Brazil within the next few weeks. Despite this uncertain time, we have proven that our Cloud Restaurant model works, combining on-premise and off-premise models with a multi-brand approach. It is the most efficient way to improve foodservice with incredible returns on our investments.
During the last couple of months our technology has advanced faster than ever before:
Our most important achievement is that we have now surpassed our historic peak revenue and are growing consistently again (based on our September figures and how we are performing so far in October). To date these are our results:
Our goal of having more than 1,000 cloud restaurants and 4,000 brand locations by 2024 in LatAm, as well as over 1 billion USD in annual revenue before 2026, is clear and we feel we are making strong and consistent progress towards it.
In more than 15 years of running startups, I have never had to steer a company through a storm that was potentially so destructive as COVID-19. We are still not sure whether we are closer to the end than to the beginning. However, we are certain that we have created better products and processes, building brands that our customers love, a stronger team that knows the company has their back, and a resilient business model able both to weather this storm and to take advantage of the significant growth opportunity that exists in the market. We will continue to build great local brands and products to achieve our goal to make good affordable food available for everyone in Latin America.
If you want to know more about us, visit RobinFood.com
Latin America’s largest cloud-restaurant company.
8.2K 
1
8.2K claps
8.2K 
1
Written by
CEO & Co-Founder of RobinFood, Latin America´s largest cloud restaurant company
We want to make good affordable food available for everyone in Latam.
Written by
CEO & Co-Founder of RobinFood, Latin America´s largest cloud restaurant company
We want to make good affordable food available for everyone in Latam.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@rupak.thakur/aws-vs-paperspace-vs-floydhub-choosing-your-cloud-gpu-partner-350150606b39?source=search_post---------52,"Sign in
There are currently no responses for this story.
Be the first to respond.
Rupak Kr. Thakur
Oct 24, 2017·8 min read
With deep learning making its mark on almost every industry today, the demand as well as interest for roles like “Data Scientist”, “ML/DL engineer”, “AI Scientist” etc. have seen an unprecedented increase. More and more students, fresh graduates and industry professionals are realizing the need to stay abreast of these emerging technologies and are taking up courses, certifications and jobs in these fields. Once you decide to jump into the domain, the first thing you need to get your hands on is high computing power. That’s where GPUs come in.
However, building your own deep learning rig is a pricey affair. Factor in costs of a fast and powerful GPU, CPU, SSD, compatible motherboard and power supply, air-conditioning bills, maintenance and damage to components. On top of it, you run the risk of falling behind on the latest hardware in this rapidly advancing industry.
Moreover, just assembling the components is not enough. You need to setup all the required libraries and compatible drivers before you can start training your first model. People still go along this route, and if you plan to use deep learning extensively (>150 hrs/mo), building your own deep learning workstation might be the right move.
A better and cheaper alternative is to use cloud-based GPU servers provided by the likes of Amazon, Google, Microsoft and others, especially if you are just breaking into this domain and plan to use the computing power for learning and experimenting. I have been using AWS, Paperspace and FloydHub for the past 4–5 months. Google Cloud Platform and Microsoft Azure were similar to AWS in their pricing and offerings, hence, I stuck to the previously mentioned three.
AWS : Most popular cloud service provider. Offers secure and scalable GPU instances along with additional AI integrations such as Polly, Rekognition, Lex and AWS Machine Learning (available in some regions).
Paperspace : Cloud VMs with GPU support for gaming, designing and programming (ML/DL) needs. Offers latest NVIDIA GPUs along with pre-installed packages and a few DL frameworks at competitive prices.
FloydHub : Marketed as “Heroku for DL”, Floyd promotes open-source collaboration by introducing public projects and datasets. Has its own CLI for training models using Caffe, PyTorch, Chainer, MxNet, TF, Keras and others.
Choose a p2.xlarge instance with elastic IP and 30GB EBS volume (part of Free Tier) on AWS, Ubuntu ML-in-a-box GPU+ VM with 50GB SSD on Paperspace and Base Data Scientist Plan without any Powerups on FloydHub.
The comparison between the three can be extensive with each offering unique benefits. However, I’ll keep it limited to six key aspects which would be most relevant to a beginner in this domain or someone who plans to use these platforms for small-scale hobby projects.
[UPDATE| May 2018] : This post is now more than 6 months old. In this era of ever-changing technology with hardware/software upgrades, any comparison between different technological platforms quickly become outdated. Thus, I’ve added snippets of UPDATE sections in relevant places of this post and one to summarize it all, at the end. However, the updates, by no means, should be considered exhaustive.
Setting up a fully-configured instance on AWS is difficult, inspite of having extensive setup tutorials on the web. Appropriate shell scripts need to be run to configure EBS volume, set up dedicated IPs and also install the required packages, software tools and DL libraries. Of course, you can use some of the freely available Deep Learning AMIs. Nonetheless, they still require a little bit of effort.
On the other hand, Paperspace and FloydHub pride itself in allowing its users to setup the instances within minutes. With FloydHub, you have to install a separate CLI. However, the instructions provided are pretty clear and once you login, you find yourself being welcomed to a host of different DL environments. Installing additional packages are not much difficult either. On Paperspace as well, you can run your instance within a few clicks, though some additional packages and frameworks might need manual installation for a complete experience.
Uploading/downloading datasets is the biggest pain point while using cloud GPU services. With AWS, FileZilla Client can be used to transfer files. Using commands like curl and wget from the terminal do not always work and other open-source hacks have to be relied upon. AWS, however, does allow easy data download/upload for Kaggle competitions through kaggle-cli. Paperspace provides 1Gbps fiber internet and a web browser. Currently, it also provides a drag-and-drop feature for Windows machines (coming soon for Linux) to transfer files from your local machine to the VM directly. When using FloydHub, one has to download the dataset locally and then upload it to their account. The code and data have to be kept separately on your local system, as every time the script runs, the entire folder contents are uploaded.
Paperspace and FloydHub being new entrants on the block fall behind AWS in terms of open-source community support, availability of tutorials and video experiments. However, their official documentation and examples are quite comprehensive.
Things to note : Floyd CLI takes some time getting used to. Lots of processes are different than standard terminal or desktop-based usage. Hence, it is a good idea to religiously go through the FloydHub documentation and FAQs. If you are a Paperspace user far away from US (Eastern Europe/Asia), expect some latency while using the desktop environment.
AWS and FloydHub use Tesla K80 GPUs (12GB vRAM) and 61GB RAM, whereas Paperspace has options for Quadro M4000 (8GB vRAM), a couple from Pascal series (16–24GB vRAM) and even the latest Volta series, Tesla V100 (16GB vRAM), each with 30GB RAM. To give a rough estimate, the Pascal series GPUs are 3x faster than K80s, while the V100 is 6x faster than K80s. AWS and Paperspace also use SSD and dedicated GPU instances, whereas FloydHub offers a choice between pre-emptible and dedicated GPUs.
The usual way of running scripts on these services are through Jupyter notebooks or directly executing them on the terminal. Paperspace, by virtue of providing a desktop environment also allows IDEs like Spyder and other utility softwares. The presence of a Linux desktop is highly convenient.
[UPDATE | May 2018] : All three of them (AWS/Paperspace/FloydHub) have now upgraded themselves to NVIDIA Volta GPUs, thus making superfast training and inference possible now. In terms of software and frameworks, AWS has updated its Deep Learning AMI, which includes pre-installed frameworks like Chainer, TensorFlow, Keras, PyTorch. FloydHub already has the latest versions of all these frameworks.
As a benchmarking exercise, I compared training of multiple models on all the three platforms under the same environment (Keras+Theano on Jupyter).AWS - p2.xlarge (Tesla K80, 12GB vRAM, 61GB RAM)Paperspace - GPU+ VM (Quadro M4000, 8GB vRAM, 30GB RAM)FloydHub - Tesla K80, 12GB vRAM, 61GB (equivalent to Base plan)
Two models were trained — A deep CNN model with Dropout on Fashion MNIST dataset and a fine-tuned pre-trained VGG16 network on a grocery product image classification task. Their performance is depicted below.
AWS p2.xlarge and Paperspace GPU+ have almost equivalent performance with AWS just inching ahead. If we use the Pascal versions on Paperspace, which are still cheaper than AWS, the model performance is expected to be 3x as fast as AWS. Despite using the same hardware, FloydHub is at ~0.75x of AWS, most probably due to slower disk reading speed.
[UPDATE | May 2018] : This is probably the most interesting update. On running the same experiments/scripts as mentioned above, recently, I found a huge improvement in the training time on FloydHub. Latest numbers show that they are at par with AWS or Paperspace GPU+ or even better. FloydHub seems to have fixed the I/O issues and having upgraded to the latest TensorFlow, Keras and PyTorch versions, it seems to have done wonders for this platform. The Fashion MNIST script now takes 8s/epoch while training, whereas the Pre-trained VGG16 script now takes much lesser (~100s/epoch). While I haven’t checked if Paperspace too has brought about some improvements on the same, AWS definitely hasn’t. So, for now, FloydHub emerges as the fastest among the three.
Both Paperspace and FloydHub offer custom plans for teams. However, Floyd’s associated features like centrally sharing datasets/projects, versioning of various job runs for easy reproducibility and support for fast.ai and Udacity MOOCs aid collaboration and a conducive open-source atmosphere. Floyd also allows concurrent job runs. AWS offers multi-GPU instances, whereas FloydHub and Paperspace only support single GPU systems.
[UPDATE | May 2018] : While AWS has focused more on lateral applications favouring enterprise and production systems, Paperspace and FloydHub both have introduced a lot of new features to improve upon the ease of usage and ease of access of GPUs to the general public, at large. Some of these have been :
[FloydHub] : Slack integration, favouring usage across teams
[FloydHub] : Job management UI, metrics dashboard
[FloydHub] : A beta version of a new interactive environment (similar to a VM on cloud), called Workspace
[Paperspace] : Collaboration as the official partner for Jeremy Howard’s fast.ai course
[Paperspace] : Paperspace Gradient and API, along with their own CLI, which are respectively tools to run your jobs efficiently on the cloud and a devkit to automate your VM/jobs (suitable for DevOps!)
With Workspace, Gradient and fast.ai collaboration, FloydHub and Paperspace have moved closer to offering similar features.
Pricing is probably the most important selection criteria. Currently, billing is prorated on a per second basis for AWS and FloydHub and at millisecond granularity for Paperspace.
AWS GPU instances start at $0.9/hr with 30GB free EBS volume under the Free Tier program. A 100GB SSD volume+ elastic IP would cost an additional $13/month. AWS also provides spot instances which are much cheaper, but highly susceptible to price fluctuations and hence, not a reliable option.
Paperspace offers Maxwell series GPUs at $0.4/hr and Pascal GPUs from $0.65/hr. A 100GB SSD with public IP will cost $7/month. Additional utility services are also provided.
FloydHub recently moved away from a pay-as-you-go model to well-defined monthly plans. The Base Data Scientist plan costs $14/month for 10 GPU hours and 100GB storage. Additional pre-emptible GPU hours can be bought starting from $0.59/hr. A premium is charged for dedicated GPU instances.
[UPDATE | May 2018] : FloydHub’s pricing structure has changed significantly, while that of AWS and Paperspace remains almost the same. Paperspace still remains the most affordable option.
I didn’t try deploying a model on either of them. Floyd provides a one-line command to deploy your model as a REST API. AWS has a host of associated services to further improve your app deployment experience. Will update once I explore them.
Summarizing the key aspects in the table below.
[UPDATE | May 2018] : There are not much changes in the chart above, except the Performance section, where FloydHub is the fastest now. On the Hardware/Software front, all three are at a level pegging.
If you have used either of these services before, please share your experience. If you haven’t, go for it now. It would be nice to have your suggestions below.
See all (9)
1.93K 
20
1.93K claps
1.93K 
20
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/arway/ar-developers-need-the-ar-cloud-right-now-ff56cf7d6a01?source=search_post---------53,"There are currently no responses for this story.
Be the first to respond.
ARwayKit: Why & How we’re building tools to empower the AR-Cloud that we’ve all been waiting for…
We are a team of creators who care about your safety, well-being, and experience through location + augmented reality services.
At ARWAY, we are enabling developers with the tools they need to build hyper-accurate location apps through their own AR-Cloud. So we can finally see the fruition of the killer apps we’ve all been waiting for…
Think, indoor AR navigation and virtual tours to search & rescue operations and service/maintenance tools.
Picking up where the other toolkits (SDKs) left off, starting indoors, with GPS-denied environments.
We are bridging the gap of scalability by removing the hardware dependency and leveraging mobile cameras to safely interpret and navigate the GPS-denied world. Solving the problems that existing visual positioning solutions have failed to address.
As AR developers & content creators ourselves, we understand the pain points and frustrations that need to be addressed;
“Augmented reality is the ‘boy who cried wolf’ of the post-Internet world — it’s long been promised but has rarely been delivered in a satisfying way. — Om Malik
First, we need to talk about Mapping…
For any kind of visual positioning tool, mapping is a crucial process — since the quality of maps directly affects the reliability and accuracy of localization.
For an effective mapping tool, ease-of-use is essential, as there is no doubt that mobile mapping is a tedious task for the user. This is why real-time feedback assistance and quality control for maps streamline this process. Allowing the possibility for collecting city-scale maps, in a single session.
(Here’s where we have more of a deep dive into the technicals)
Under the hood of our ‘Mapping Engine’, we capture unique ‘feature points’ in each camera keyframe and then match those features continuously in each frame to estimate the camera position in real-time. These matched feature points are converted into a point cloud map that is stored in our cloud infrastructure.
During ‘localization’, when a device needs its localized Pose for AR, our SDK allows developers to request localization data from previously-stored 3D maps.
This seems easy, right? So what’s REALLY the problem we are solving?
Since the early 2000s, image registration has mostly used traditional feature-based approaches.
These approaches are based on three steps:
In brief, we select points of interest in both images, associate each point of interest in the reference image to its equivalent in the sensed image and transform the sensed image so that both images are aligned.
We use internally modified ORB algorithms to create a state-of-the-art toolkit for the image registration problem, ORB is basically a fusion of FAST keypoint detector and BRIEF descriptor with many, many modifications to enhance the performance. First, it uses FAST to find key points, then applies the Harris corner measure to find top N points among them. It also uses a pyramid to produce multi-scale features.
Beyond this, we are pushing the boundaries by experimenting with unique deep learning techniques for image registration, as deep learning has allowed for state-of-the-art performance in computer vision tasks such as image classification, object detection, and segmentation. There is no reason why this couldn’t be the case for image registration.
For a visual positioning tool to be powerful, handling changes in the physical environments and lighting conditions is very important.
Imagine having to constantly recreate or update a map after every time its environment or lighting condition changes. It would be near impossible to do and completely impractical for building a large scale AR-Cloud experience.
We’re enabling developers to update maps dynamically when a user tries to localize in any of the pre-mapped environments. During localization, the camera keyframes are collected and if the localization is successful then those keyframes are used to extract feature points. This further updates the current maps if needed.
Environmental lighting changes from daylight to artificial light indoors or night lights outdoors, these changes affect the accuracy and reliability of any of the feature point-based algorithms that store direct pixels containing light information. This can be resolved by using geometry and shapes to represent ‘feature points’ instead of pixels. HOG feature descriptor is one of the algorithms showing how this problem can be resolved.
Running localization locally on a variety of devices might be challenging and processor expensive.
For a large scale AR-Cloud, users might need to download MBs of a map data file upfront which would degrade and slow the user experience. Our perspective is to make a seamless experience by enabling On-Cloud localization on a variety of devices. During localization, a device sends compressed raw sensor data to the nearest located ‘ARWAY Cloud service’ which returns the local pose of the device in a pre-mapped environment.
This approach enables developers to integrate a localization service into their existing apps with minimal coding efforts and removes the dependency of writing platform-specific code, hence why we are proud to announce that ARWAY is able to support WebAR compatibility, out-of-the-box.
We are continuously doing research towards the various visual localization algorithms and systems, with the aim of providing tools to the masses for creating the AR-Cloud.
Additionally, we will be presenting our research paper on visual localization soon in the upcoming IEEE and CVPR conferences.
If you are interested in partnering with us on research or want to work with our team, please reach out to hello@arway.io
Baran is the Founder & CEO of ARWAY, a computer vision startup in London recently accepted into Techstars, working to build a digital framework for the GPS-denied world.
Connect with Baran and the company on Twitter.
Nikhil Sawlani leads the AR-Cloud research at ARWAY, he is a published AR researcher. Focusing on all problems related to AR+Location.
We send a newsletter every 1–2 months, gathering our thoughts of the AR industry and sharing our progress.
Sign Up to our newsletter & early access waiting list.
Join our growing AR developer community.
Making your mobile camera smarter for hyper-accurate location + augmented reality apps.
6.5K 
Updates + Progress + Thoughts Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
6.5K claps
6.5K 
Written by
Founder & CEO @ ARWAY (https://arway.app), building a part of the Metaverse with Spatial Computing.
Through Unity, optimized for Google and Apple, ARWAY grants you an Augmented Reality Software Kit to frame the digital world in a few minutes!
Written by
Founder & CEO @ ARWAY (https://arway.app), building a part of the Metaverse with Spatial Computing.
Through Unity, optimized for Google and Apple, ARWAY grants you an Augmented Reality Software Kit to frame the digital world in a few minutes!
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://codeburst.io/next-js-on-cloud-functions-for-firebase-with-firebase-hosting-7911465298f2?source=search_post---------54,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Top highlight
Welcome to a series exploring Cloud Functions for Firebase with a modern application stack (React, Apollo, GraphQL, Next.js & Firebase). If you are not familiar with this stack, read on! If you are familiar, checkout the TOC above.
Most apps today are built as Single Page Applications (SPAs) using a JavaScript framework to enable rich user interactions. With a traditional SPA the client makes a single request to the server for the HTML, CSS and JavaScript required for the app. Because the app is a bunch of JS scripts, it must wait until some of the JS bundles are completely downloaded before it can make any requests to the server for data to populate the UI. This means that there are two round trips to the server before the user can interact with the app. Not ideal.
Server-side Rendering (herein SSR) in a JS SPA is used to evaluate the JS on the server (server-side 😉) and then request the appropriate data before sending back the computed HTML page along with the CSS. Any JS bundles can then be loaded asynchronously since the UI will appear completed before the JS finishes downloading and parsing. This gives the appearance of a faster app.
The image below defines common terms for each step in the web app loading sequence from a user’s perspective:
For more detail on the metrics used to measure page loads I would recommend watching this Google IO Lighthouse video from the Google Lighthouse web auditing and performance tool website.
The main benefit to SSR is that the app appears to load completely, more quickly. These images illustrate the difference between Client-side Rendering to Server-side Rendering of an SPA:
With SSR the TTFB can be increased due to the time spent on the server evaluating HTML and populating the page with data before being sent to the client. If you are using a common back-end service for processing and data storage (say, Firebase), the data request is made inside the same data center and is extremely fast. So while the TTFB can be slower, it’s shouldn’t be any order of magnitude slower than making two round trips across the internet.
Another condition of SSR is that the TTFB, First Paint, FCP and FMP are compressed to the same point on the timeline. They’re essentially the same thing. The JS bundles enabling client-side interactivity may still be loading, but the user sees a page closer to the finished product sooner.
The latter is why SSR is so desirable.
A way around the TTFB being slow in both CSR and SSR is to use the App Shell model or client-side caching. This can significantly improve the user experience in all cases. We will cover this in a future post.
For more discussion on SSR vs CSR I would recommend these articles:
Tom Dale — You’re Missing the Point of Server-side Rendered JS Apps
Juan Vega — Client-side vs. server-side rendering: why it’s not all black and white
Hacker News discussion
A condition of SSR apps is that the host serving the app requires a runtime to perform the JS evaluation. This means you cannot simply use a CDN with static files and an API server, you must have a server to host your application. We’ll explore how to use Firebase to achieve this.
So now that we have an understanding of SSR and why you would decide to use it, we can talk about the framework of choice.
a small framework for server-rendered universal JavaScript webapps, built on top of React, Webpack and Babel — Zeit.co
Place React components in a pages directory and running next, and you'll get automatic code splitting, routing, hot code reloading and universal (server-side and client-side) rendering. — Zeit.co
If you want to learn about the motivations behind Next.js I recommend the official blog post.
Personally, I find that routing, code-splitting and hot-module reloading for React applications too complex to get right when learning. Having infinite flexibility to implement any routing method or code-splitting is great, if you’re a pro. There’s conflicting information about which method is best and it’s all just to much to configure, let alone for a universal SSR React app as a beginner.
Next.js solves these issues by having opinions (they aren’t always bad) about how to manage routing in a React app and abstracts away any of the complexities with handling SSR. Then you get the bonus of HMR and extensible Webpack and Babel configurations for when your skills develop.
Simple Routing — React components inside pages/ become page routes 👍 There’s no need to try and figure out which version of which router library the project uses, or unravel the mesh of routes mixed with pages.
Code Splitting — Routes are code-split. There’s an algorithm in Next that performs common module bundling/splitting and it works quite well 👍 KISS
SSR — it just works 👍
Data-fetching on Server-side Render — it’s a function you implement so you can use any data-fetching or storage you prefer.
Pre-fetching pages — you specify when this happens.
Oh, and the soon to be released Next.js 3.0 has support for outputting a static SPA as well. So if you have no desire to use SSR, but like the features listed above, then you can simply output a static app and use a CDN as your host. But let’s do something new and drop SSR on Cloud Functions.
Traditionally hosted SPAs use static resources on a CDN as mentioned earlier. Firebase Hosting is such a service and therefore cannot do back-end processing. However, with the addition of Cloud Functions for Firebase in March 2017 an integration was made with Firebase Hosting to allow serving dynamic content. This allows us to do server-side processing with Cloud Functions and use a custom domain with the Cloud Function through Firebase Hosting. This Q/A with David East goes through a quick example.
Regular Cloud Functions have the following URLs:
https://us-central1-<project-name>.cloudfunctions.net/<function-name>
After using Firebase Hosting redirects, we can get the following URL format:
<project-name>.firebaseapp.com/
Unfortunately the above video only covers redirecting a sub-route of your Firebase Hosting URL to a single Cloud Function returning custom HTML. We don’t want to write a different Cloud Function for each URL in our app. And we also don’t want to host our app on a sub-route of our domain. We want the app hosted on the root of our domain. Like so:
<project-name>.firebaseapp.com/
We’ll get to clean URLs with Firebase Hosting Rewrites later. First let’s host our app on Firebase!
Create a folder structure like this:
We will build our Next.js app in the src/app folder.
Navigate to the src/app/ folder and install our dependencies like so:
I use Yarn, feel free to use NPM instead. I would recommend npm@5.2.0 or greater as it’s finally stable and super speedy.
Now we will add our Next.js pages and components. This will create a simple site that has a Home and About page with a standard header that has links to each page.
N.B.: For a complete guide on learning Next.js there’s no going past https://learnnextjs.com/. It’s quick and covers everything you need to get started with the framework.
Add the following scripts to the package.json file in the src/app/ folder.
These scripts will help us run local development with features like HMR etc from our project root later. It will error at the moment thanks to our next.config.js build directory redirect.
To start, create a project in the Firebase web console — call the project nextonfirebase.
Then go to the root of your local project nextonfirebase/ and run the following few commands:
yarn init -y
yarn global add firebase-tools
firebase login — login to the Firebase CLI.
firebase init — initialise a Firebase project.
Unlike with Firebase Hosting, the Firebase CLI does not ask us for a directory to keep our Cloud Functions, it just creates a folder at the current dir: nextonfirebase/functions/. Before we delete this folder, we need to move the contents to our existing nextonfirebase/src/functions/ folder:
Now we need to update the firebase.json file to point the Firebase CLI tool to the correct location of our Cloud Function code. Replace the file’s contents with the JSON below:
When setting up our Next.js app earlier we created a file called next.config.js. This file is used to tell the Next.js framework what destination directory to output our built app to. You will see that it says to output the built app to a ../functions/next directory. This is a significant step in getting Next.js to operate correctly. The default settings of Next.js specify a destination directory with the format .next. That little . in the name has been the cause of some headaches when getting resources to be uploaded through the CLI and to be found once hosted on FaaS services (Solutions are in the works in various places to solve this, but it’s best to go with this method for now. There’s no real downside this way). I haven’t run into any problems since trying this renaming method that just specifies a directory without a . in the name.
Thanks to @geovanisouza92 (GitHub) for their help and collaboration with this (if you use the serverless framework, check out their serverless-next example of this on AWS Lambda with API Gateway).
Getting to the code.
We don’t want to commit our Next.js build folder to our repo so add a .gitignore file in our functions folder.
Since we’re in our src/functions/ folder, run yarn to install the dependencies. We’ll also need to add new dependencies for next.
This is an annoying issue where the node_modules/ for the Next app are required to be packaged alongside the next build folder in our Cloud Functions, but we can’t simply copy the src/app/node_modules/ to src/functions/node_modules/ as that would defeat the purpose of using a package manager with a lockfile and probably cause a myriad of other problems (who wants to manually merge node_modules 😑) . There’s probably a better solution to this, but for now just add all dependencies from src/app/package.json into src/functions/package.json (please comment if you find a nice cross-platform solution to this and I will update the article to spread the word!).
I’ve recently found that firebase-tools doesn’t install the latest releases of the Cloud Functions dependencies; firebase-admin and firebase-functions. To make sure we’re using the latest and most compatible versions run the following:
Now we can create our Cloud Function that serves our Next app. In src/functions/index.js paste the following:
This code simply sets up the Next.js app to be returned from a Cloud Function called next. Here you can see the (req, res) variables passed from the Cloud Function through to the Next.js app to handle.
Before we can go any further we need some build scripts to manage local Firebase testing and deploying. I’m a fan of NPM scripts because of their ease of use and simplicity. Run yarn init -y in the project root directory so we can manage our project from there and add the code below:
The scripts we have here are not magic. They simply allow us to manage the project from the root directory easily. The list below shows what goals we want to achieve with the scripts, which scripts align to each goal and explains how each script does so:
The pre scripts just ensure all deps are installed before running serve or deploy. These are scripts using NPM script hooks.
Now that we have our scripts sorted, let’s test out our local development workflow and see if it works as expected:
It all works as expected, cool!
Let’s now see how our app works when hosted on Cloud Functions for Firebase alone. Run the following command to deploy (we won’t use this again so it’s not in the NPM scripts):
N.B.: This deployment will take some time ~2–3minutes depending on your machine and internet connection.
Open a browser to a new tab with DevTools open on the network tab. Copy the Cloud Function’s URL from the Terminal. It should look like this:
https://us-central1-<project-name>.cloudfunctions.net/<function-name>/
Load the page.
N.B.: If you get Internal Server Error then you forgot the trailing slash. Yes, that again (repeat readers will understand)!
When you try to navigate to the /about page you will see that Next.js expects to route to the root of the URL, not some /next/ subroute. This affects two things:
2. Next.js routing does not work for our bundled JS files.
This is the crux of the problem people are having with SSR Next.js on ephemeral compute services like Cloud Functions and AWS Lambda etc.
There are probably some workarounds for getting this all to work within Next.js itself. I tried many permutations of assetPrefix with other config settings, but could only solve some of the problems. Either way, there’s a better solution!
Our Next.js app wants to be hosted on the root URL, and that’s what we want too! We also want to use the clean URL from our Firebase Hosting service instead of the long Cloud Function URL.
<project-name>.firebaseapp.com/
Hosting rewrites use a glob pattern to match URLs. The docs state that
** matches any file or folder in an arbitrary sub-directory. Note that * just matches files and folders in the root directory.
The key word here is “an”. This routing rule does not solve the issues with files under the /_next/* path or nested subroutes. We want to match
any file or folder in ALL arbitrary sub-directories.
**/** — actually matches any file in any sub-directory.
Update your firebase.json to contain the following:
Don’t try to test the rewrite rules yet. There’s one more thing standing in our way. Firebase Hosting has a set priority for resolving content:
As you can see, static content gets matched before rewrites.
Simply deleting all the .html files in the src/public/ folder seems like a good idea, until firebase deploy is run.
A way around this is to put your static assets into the src/public/ folder, or simply leave the auto-generated 404.html file (because it won’t be routed to as Next.js has it’s own 404 page) and delete the index.html.
The deployment upload still gives a warning informing you that the Hosting directory doesn’t contain an index.html file. This can be ignored (unless your CI/CD pipeline requires no warnings, in which case I cannot help you, sorry).
Re-deploy the app with yarn deploy and you should now be able to use the Hosting URL for the Next.js App. Woo! 📦
Firebase can also run a local web server with your Firebase Hosting configuration. It even runs our Cloud Function if we have rewrites setup. So we can actually test our Firebase Hosting SSR locally to determine if everything works as expected. Just run yarn serve from your project’s root directory for the correct script.
A more up to date example can be found in this repo:
github.com
So SSR on Firebase Hosting is now possible thanks to the integration with Cloud Functions. It’s worth noting that Cloud Functions is still in Beta, so performance is set to improve over time. Once the Firebase Hosting rewrite rules are in place, Next.js’s internal page routing just works. I’m excited for static support to come to the framework as it means I can use the same JS framework for SSR and static CSR sites on the same Firebase infrastructure! It’s a exciting future!
In my previous post about GraphQL on Firebase, I touched on why I go to the trouble getting everything working on Firebase. I like that it’s an all-in-one managed and hosted service. It’s ideal for serverless development which I believe to be the future of all cloud/web based applications. Code packaging, billing, logging & metrics, just everything you would need is in one place. There’s no need to deal with multiple user accounts for team members on multiple platforms/services.
It may be more difficult to get up and running, but you only have to deal with those problems once. Using multiple tools you’ll have to deal with the myriad of ecosystems for the entire project, or go through the hell of migrating to consolidate. I’d rather put in the effort here.
Plus, it’s fun solving new problems.
Thanks to the Next.js and Firebase teams for building such cool products. And thanks to all those who’ve created content that I’ve linked throughout this post.
A special thanks to Callum Gardner for reviewing this post numerous times.
Web app performance improvements:
Dev Tools:
YouTube Videos:
More by me:
If you found this useful, please recommend and share with your friends & colleagues.
Bursts of code to power through your day.
2.1K 
30
Thanks to Callum Gardner. 
Some rights reserved

2.1K claps
2.1K 
30
Written by
GCP, Firebase, Sveltejs & Deno fan! @jthegedus on all platforms.
Bursts of code to power through your day. Web Development articles, tutorials, and news.
Written by
GCP, Firebase, Sveltejs & Deno fan! @jthegedus on all platforms.
Bursts of code to power through your day. Web Development articles, tutorials, and news.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://blog.sketchapp.com/color-management-sketch-cloud-updates-and-symbol-scaling-in-sketch-48-f4188f4ec6bb?source=search_post---------55,"There are no upcoming events.
Want to host your own event? Get in touch.

        Sketch 48 has arrived and with the latest release we’ve added support for color management, improved Symbol scaling and introduced an exciting, new feature to Sketch Cloud.
      
Sketch 48 has arrived and with the latest release we’ve added support for color management, improved Symbol scaling and introduced an exciting, new feature to Sketch Cloud. Here’s an overview of the headline features in the latest update:
For a long time sRGB was the only color space digital designers really needed to worry about. But with the introduction of Wide Gamut displays, that can render more vibrant colors, times have changed.
We know you’re designing for all kinds of devices and displays so in version 48, Sketch now supports working in, and exporting documents in sRGB and Display P3 color profiles.
By default, Sketch uses an Unmanaged color profile so if you don’t want to worry about Color Management, you don’t have to — you can just leave the preferences as they are. You’ll see better performance in the app but remember that when you export files the colors may look different from what you see on screen in Sketch.
If you do want more control over how Sketch renders your colors, you can now easily change your default color profile to Display P3 or sRGB, in Preferences › General › Color Profile.
Already working on something? If you need to change the profile of an existing document, don’t worry, you can do that too. Just choose File › Change Color Profile… in the menu and select the color profile you want to use.
To find out more about color management you can check out our documentation or this excellent article by Marc Edwards. For a more in-depth look, we suggest picking up a copy of Craig Hockenberry’s brilliant e-book “Making Sense of Color Management”. We’re even offering 15% Off when you use the discount code SKETCH at checkout.

While we’ve been hard at work making improvements to Sketch itself, this update also brings a welcome new feature to Sketch Cloud. Now, when you share with Sketch Cloud, your entire document and all its contents will be uploaded, rather than just a preview.
This means that anyone you share your designs with can now download and open your document in Sketch — perfect for making iterations or collaborating remotely.
If you want to make your document available for download you can choose to do so after it’s been uploaded to Sketch Cloud by checking the box that says Allow this document to be downloaded.
We’ll be adding some more big features to Sketch Cloud in our next release so keep an eye out for future updates.
This is one of our most requested features and we totally understand why — there’s nothing more frustrating than getting to the end of a project, sitting back and looking at your vast collection of Artboards and realising a color you’ve used across the whole document just isn’t quite right.
Having to manually change colors throughout your project will be a thing of the past with our new Find and Replace Color options.
You can now find a color you’re using in your document and replace it with another color throughout your entire document. We’ve even taken opacity into consideration and allow you to both find and replace colors with full sRGB values.
You’ll probably be as excited as we are to hear that in Sketch 48 Symbol instances can now be scaled, independently from the Symbol master, using the Scale command.
Most Sketch users will know that simply resizing a Symbol, like resizing a group or layer, doesn’t affect properties such as corner radius and border thickness. This is normally fine but not so hot if you’ve got an icon or component you’d like to use at varying sizes across your document while retaining its stylistic properties. Previously you had to detach your Symbol, scale it to the appropriate size and create a whole new Symbol.
In Sketch 48 you can now use the Scale command to scale Symbol instances — just as you can with any other layer or group — and any properties such as borders will increase or decrease in size, in proportion with the Symbol itself, without affecting the master.
So instead of having a bunch of Symbols for the same icon at different sizes, you can now have just one Symbol in your Library and scale it to fit anywhere, while ensuring a consistent style across your project. This feature is especially useful if you’re using Libraries as it ensures your Symbols will stay up-to-date, no matter where or at what size they’re being used.
To resize a Symbol and all of its properties (corner radius, border thickness, shadow size, etc.) choose Layer › Transform › Scale… from the menu (or press Command-K).
As always we’ve taken your feedback and suggestions on board and made some smaller improvements and bug fixes since the last update. Here are some of the highlights:
You can find a full list of bug fixes and improvements on our updates page.
Sketch 48 is a free update for everyone with an active license. If you need to renew your license, you’ll get Sketch 48 and a whole year’s worth of updates after that.
We’d love to hear what you think of this update so please let us know. If you’ve got questions or feedback, you can get in touch with us via our support page or join in the conversation on Twitter, or on our Facebook group.
We’re working hard on Sketch 49 right now. Keep your eyes peeled for more news on it soon.
Turn your ideas into incredible products with a 30-day trial.
A monthly digest of the latest Sketch news, articles, and resources.
©
        2022
        Sketch B.V.
"
https://blog.sketchapp.com/introducing-cloud-inspector-free-developer-handoff-in-the-browser-59917220334a?source=search_post---------56,"There are no upcoming events.
Want to host your own event? Get in touch.

        Today we’re launching Cloud Inspector in beta and bringing developer to Sketch for free. Here’s what’s in the beta and what’s coming next.
      
In our 2020 teaser video, we announced that developer handoff was coming to Cloud in January 2020. And today, we’re excited to make good on that promise with a beta launch of Cloud Inspector.
Cloud Inspector makes it easy for developers to get the information they need to turn pixels into code — all in the browser and, most importantly, for free. It’s another step on our journey to build great collaboration tools for teams, and an important part of our mission to help people design better products.
With today’s launch, the first version of Cloud Inspector is officially in beta. We’ve been using it internally and testing it with real users for a while now, and we’re excited to get it into more hands. Here’s what you can expect today, as well as what’s coming next.
Cloud Inspector is all about handoff and collaboration within teams, but we know that solo designers often work with developers, too. So we’ve made Cloud Inspector available to everyone.
If you have a Team workspace on Cloud, every design you share there will be ready to inspect right away. And for designs you save to a personal workspace, you can enable Inspector in their document settings.
To fire up Cloud Inspector, click on the new Inspect tab in the sidebar next to any Artboard on Cloud. When you do, you’ll see something that hopefully feels a little familiar — an Inspector-like view of that Artboard’s attributes.
Just like the Inspector in the Mac app, Cloud Inspector changes to display relevant information, depending on the layer or Artboard you’ve selected. We’ve also made sure it hides anything unnecessary, such as corner radius being set to 0, a blend mode set to Normal or opacity set to 100%. In other words, Cloud Inspector hides the defaults and presents the most important information developers need, right when they need it.
If you’re inspecting a file and need to copy an attribute, all you have to do is hover over it and click the copy icon. Right now we’ll copy a simple, plain text version of that attribute to your clipboard, but in the future we’ll let you set a language to copy to (such as CSS).
For layers with multiple attributes such as different fills and blend modes, or text properties, you can click on a section heading to copy them all to your clipboard at the same time. As an example here’s what copying a group of text attributes looks like:
Copying attributes to the clipboard is helpful, but with colors specifically we decided to take things a step further. Hover over any color and click on the arrow that appears to bring up a formats menu. From here you can copy colors in HEX, RGB and HSL formats, as well as NSColor and UIColor formats for Objective-C and Swift.
When you pick a color format, we’ll save that preference for the rest of your browsing session, so any other color you copy will be in that same format.
We think the option to pick a format for copying color attributes will be really handy during handoff, and we can’t wait to bring more development-focussed details to future version of Cloud Inspector.
When you select a layer to inspect, hovering any other layer on your Artboard will show you relative measurements to it. Combined with the position and size attributes in the Inspector, sizing up designs should be a breeze.
This first beta version of Cloud Inspector is just the start of our developer handoff efforts, and we’re already hard at work on the next set of features, including:
Cloud Inspector is out in beta and ready for you to try in Cloud today. You can find full details about how to make the most of it in our documentation. As ever, we appreciate your feedback and we’d love to hear what you think of it. Stay tuned for more updates and additions in the coming months
Turn your ideas into incredible products with a 30-day trial.
A monthly digest of the latest Sketch news, articles, and resources.
©
        2022
        Sketch B.V.
"
https://blog.sketchapp.com/private-sharing-in-sketch-cloud-sync-shared-styles-shortcuts-and-more-in-sketch-46-62ede30b0ab3?source=search_post---------57,"There are no upcoming events.
Want to host your own event? Get in touch.

        With Sketch 46, we’ve introduced a major new update to Sketch Cloud, vertical text alignment and a few other enhancements to help improve the way you work.
      
We’re back again with Sketch 46. In this update, we’ve introduced a major new update to Sketch Cloud, vertical text alignment and a few other enhancements to help improve the way you work. Here are the headline features in this release:
Our biggest update in Sketch 46 isn’t to Sketch itself, but Sketch Cloud. That said, we think you’ll find it incredibly useful. In short, now when you upload a document to Sketch Cloud, you get the option to share it privately with others.
All you need to do is enter the email address(es) of anyone you want to invite and they’ll get a link to view your document. You’ll also now find a ‘Shared with me’ section within Sketch Cloud — separate from your own document overview — with easy access to documents you’ve previously been invited to view.
Following on from the improvements we made to resizing in Sketch 45, this release brings the ability to align text layers vertically within their now fully-adjustable bounding box.
Now, you can set whether text aligns to the top, middle or bottom of its layer and, when you resize that layer vertically, the alignment settings you’ve made will come into play.
Getting things pixel perfect when you’re working with type isn’t always easy. Up until now, Sketch didn’t account for cap heights and the baseline when it came to measuring the distance between text layers and other elements. That changes with Sketch 46.
Now, holding the Control key along with Option measures the distance from the baseline and cap height of text, rather than the bounding box of the layer. In other words, you can be more precise with your text placement than ever before.
We want you to get the most out of Sketch, so we’ve been steadily building up our online documentation to help. It’s a treasure trove of information and advice, and with Sketch 46, it’s easier to access than ever.
Like most Mac apps, using the search box within the Help menu will filter down menu items to help you find the one you want. Sketch goes one step further now, listing out relevant online docs where you search term is mentioned. For new users (or even seasoned pros who want to delve deeper), we reckon this will prove really useful.
It wouldn’t be a new release of Sketch without a few bug fixes and improvements made as a result of your feedback (as ever, thank you — keep it coming!). Here are the highlights:
You can find a full list of bug fixes and improvements on our updates page.
Sketch 46 is a free update for everyone with an active license. If you need to renew your license, you’ll get Sketch 46 and a whole year’s worth of updates after that.
Do let us know what you think of this update — we’d love to hear from you. If you’ve got questions or feedback, you can get in touch with us via our support page or join in the conversation on Twitter, or on our Facebook group.
Sit tight — Sketch 47 is on its way! We’ll have more news on that soon.
Turn your ideas into incredible products with a 30-day trial.
A monthly digest of the latest Sketch news, articles, and resources.
©
        2022
        Sketch B.V.
"
https://levelup.gitconnected.com/cloud-design-patterns-explained-simply-113c788b33ff?source=search_post---------58,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
“There are only patterns, patterns on top of patterns, patterns that affect other patterns. Patterns hidden by patterns. Patterns within patterns…”
- Chuck Palahniuk
Perhaps the simplest way to describe a pattern would be a regularity that predictably repeats itself. The beauty of patterns lies in their ubiquity — From the way organelles are arranged in the smallest of micro-organisms to the way stars line up in galaxies…
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://lugassy.net/why-we-moved-from-amazon-web-services-to-google-cloud-platform-726c412fd667?source=search_post---------59,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Top highlight
TL;DR: AWS’s awesome, but Google is Googol awesome.
Participate in any AWS re:Invent conference ($1,600 admission) or follow head evangelist Jeff Barr and you’ll fall in love instantly with Amazon Web Services.
100s of new features every year and an all-you-can-eat, elastic, no-ops bouffe of on-demand services. Well, until you get to actually taste the food…
Amazon’s awesome, but Google Cloud is built by developers, for developers, and you see it right away.
GAE just works, has auto-scaling, load-balancers and free memcache all built in. Want to connect to Cloud SQL? use the virtual linux socket /cloudsql. Want to write custom logs and see them instantly? Append /var/log/app_engine/custom_logs or simply console.log() and console.error(). Want to profile and debug your application IN PRODUCTION? put breaking points in StackDriver or SSH to the managed instance.
Under the hood you’ll see GAE is 100% docker. Use it to run your 20 microservices with *.appspot.com service discovery or run one mamooth application at scale.
Update: I’ve received horror stories about what it was like with GAE at the early days, especially with the built-in datastore. I had no experience with GAE or Google Cloud back then so can’t really comment. As before any blind date, be prepared to leave anytime. Code a wrapper around your cache, data and message stores so you can switch technologies/providers.
Google lets you create custom machine types with any cpu/memory configuration. They let you opt for cheaper, preemptible (a-la spot) instances with a single click and no bidding/auction code whatsoever.
Google connects each and every VM to its super-fast, low-latency networking. Amazon requires you to buy expensive 10G-capable instances and/or enable enhanced networking.
Google lets you set up simple Firewall rules. Amazon gives you VPC, security groups, network access control lists and a big, fat headache.
Update: Some have commented AWS VPC is great and lets you tightly secure instances, create sandboxes and control internal networking. I still found it confusing since I rarely need anything beside basic ports/home IP rules. However if you’d like to investigate every connection issue like a murder mystery go ahead.
Google bills by the minute (not hour) and apply AUTOMATIC DISCOUNTS for long-running workloads, with absolutely no reserved pricing nonsense (warning: AWS EC2 pricing page might crash your browser).
Want to run a message bus? AWS will make your head spin with SNS, SQS, Kinesis, Kinesis Streams and Kinesis Firehose. GCP has only Pub/Sub which just works and is insanely scalable.
Update: I realize SQS (~Hosted RabbitMQ?) and Kinesis (~Hosted Kafka?) are two different buses, but getting GCP to work with one messaging product regardless of volume/velocity sounds better for me.
Google BigQuery is nicely priced by the GB stored and TB queried, has day partitioner built-in, 50% reduction in price for unmodified partitions (so you can keep data for longer) and full SQL support.
Google DataFlow is an amazing framework for consuming and processing data in batch or streams, with windowing, automatic triggering/speculative data and easy to use transformations.
Update: AWS started adding “Streams” to each service and spun them as new products, further increasing confusion. Dataflow is easier to comprehend as it treats all I/O targets as either sources and/or sinks.
Amazon has one of the most confusing IAM. While it is nice to set up a role to only allows usage for a particular resource from a specific device and times of day, you end up spending most of your time debugging policies.
Google security is more leaned back, assumes all resources are allowed within each trusted “project”.
Moreover, people you invite to projects must have a Google Account which are secure by default and usually already set up.
Update: Apparently you can have a Google Account with any email address, but if you are like me and using Google for Work (Google Apps) for SSO, you are already set. Amazon also supports MFA but you have to create a new set of users.
We moved to GCP because we wanted to work on infrastructure that runs YouTube, Gmail and Google Analytics. We moved because Google is fair, much more tech-savvy and launch products that just works.
Update: It is unclear if Google (the search engine, youtube, gmail) is using GCP. It has certainly created it and happily released it as they are working internally on the next big thing.
AWS is still fantastic. I just hope they would close issues before releasing new, half-baked features in time for re:invent.
Someone wrote: “I guess nobody ever got fired for using AWS. It’s the IBM of the cloud” and I couldn’t agree more, but if you are your own boss and like to look further, go with GCP.
Your mileage may vary.
Follow me here or on Twitter as I plan to write deeper posts battling specific products (i.e Pub/Sub vs. Kinesis, Google CDN vs. CloudFront).
Updated based on some love and hate from HackerNews.
Thoughts about Startups, Development & Ops by Michael…
636 
20
636 claps
636 
20
Written by
Co-Founder, HashUnited.com. Built 7 other companies, sold 3, worked with dozens more. Writing about startups, dev & ops.
Thoughts about Startups, Development & Ops by Michael Lugassy
Written by
Co-Founder, HashUnited.com. Built 7 other companies, sold 3, worked with dozens more. Writing about startups, dev & ops.
Thoughts about Startups, Development & Ops by Michael Lugassy
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/walmartglobaltech/cloud-native-application-architecture-a84ddf378f82?source=search_post---------60,"There are currently no responses for this story.
Be the first to respond.
Cloud native is an approach for building applications as micro-services and running them on a containerised and dynamically orchestrated platforms that fully exploits the advantages of the cloud computing model. Cloud-native is about how applications are created and deployed, not where. Such technologies empower organisations to build and run scalable applications in modern, dynamic environments such as public, private, and hybrid clouds. These applications are built from the ground up, designed as loosely coupled systems, optimised for cloud scale and performance, use managed services and take advantage of continuous delivery to achieve reliability and faster time to market. The overall objective is to improve speed, scalability and, finally, margin.
Speed — Companies of all sizes now see a strategic advantage in being able to move quickly and get ideas to market fast. By this, we mean moving from months to get an idea into production to days or even hours. Part of achieving this is a cultural shift within a business, transitioning from big bang projects to more incremental improvements. At its heart, a Cloud Native strategy is about handling technical risk. In the past, our standard approach to avoiding danger was to move slowly and carefully. The Cloud Native approach is about moving quickly by taking small, reversible and low-risk steps. Scalability — As businesses grow, it becomes strategically necessary to support more users, in more locations, with a broader range of devices, while maintaining responsiveness, managing costs and not falling overMargin — In the new world of cloud infrastructure, the strategic goal is to be to pay for additional resources only as needed — as new customers come online. Spending moves from up-front CAPEX (buying new machines in anticipation of success) to OPEX (paying for additional servers on-demand).
Cloud Native Computing Foundation is an open source software foundation housed in the Linux Foundation and includes big names such as Google, IBM, Intel, Box, Cisco, and VMware etc, dedicated to making cloud-native computing universal and sustainable. Cloud native computing uses an open source software stack to deploy applications as microservices, packaging each part into its own container, and dynamically orchestrating those containers to optimize resource utilisation. According to the FAQ on why CNCF is needed — Companies are realising that they need to be a software company, even if they are not in the software business. For example, Airbnb is revolutionising the hospitality industry and more traditional hotels are struggling to compete. Cloud native allows IT and software to move faster. Adopting cloud-native technologies and practices enables companies to create software in-house, allows business people to closely partner with IT people, keep up with competitors and deliver better services to their customers.
Microservice is an approach to develop a single application as a suite of small services, each running in their own process and communicating using lightweight protocols like HTTP. These services are built around business capabilities and are independently deployable by fully automated deployment machinery.
Each service of a cloud-native application is developed using the language and framework best suited for the functionality. Cloud-native applications are polyglot. Services use a variety of languages, runtimes and frameworks. For example, developers may build a real-time streaming service based on WebSockets, developed in Node.js, while choosing Python for building a machine learning based service and choosing spring-boot for exposing the REST APIs. The fine-grained approach to developing microservices lets them choose the best language and framework for a specific job.
Cloud-native services use lightweight APIs that are based on protocols such as representational state transfer (REST) to expose their functionality. Internal services communicate with each other using binary protocols like Thrift, Protobuff, GRPC etc for better performance
A cloud-native app stores its state in a database or some other external entity so instances can come and go. Any instance can process a request. They are not tied to the underlying infrastructure which allows the app to run in a highly distributed manner and still maintain its state independent of the elastic nature of the underlying infrastructure. From a scalability perspective, the architecture is as simple as just by adding commodity server nodes to the cluster, it should be possible to scale the application
According to Murphy’s law — “Anything that can fail will fail”. When we apply this to software systems, In a distributed system, failures will happen. Hardware can fail. The network can have transient failures. Rarely, an entire service or region may experience a disruption, but even those must be planned for. Resiliency is the ability of a system to recover from failures and continue to function. It’s not about avoiding failures, but responding to failures in a way that avoids downtime or data loss. The goal of resiliency is to return the application to a fully functioning state following a failure. Resiliency offers the following:
One of the main ways to make an application resilient is through redundancy. HA and DR are implemented using multi node clusters, Multi region deployments, data replication, no single point of failure, continuous monitoring etc.
Following are some of the strategies for implementing resiliency:
Testing for resiliency — Normally resiliency testing cannot be done the same way that you test application functionality (by running unit tests, integration tests and so on). Instead, you must test how the end-to-end workload performs under failure conditions which only occur intermittently. For example: inject failures by crashing processes, expired certificates, make dependent services unavailable etc. Frameworks like chaos monkey can be used for such chaos testing.
Containers make it possible to isolate applications into small, lightweight execution environments that share the operating system kernel. Typically measured in megabytes, containers use far fewer resources than virtual machines and start up almost immediately. Docker has become the standard for container technology. The biggest advantage they offer is portability.
Cloud-native applications are deployed using Kubernetes which is an open source platform designed for automating deployment, scaling, and management of containerised applications. Originally developed at Google, Kubernetes has become the operating system for deploying cloud-native applications. It is also one of the first few projects to get graduated at CNCF.
DevOps, the amalgamation of “development” and “operations describes the organisational structure, practices, and culture needed to enable rapid agile development and scalable, reliable operations. DevOps is about the culture, collaborative practices, and automation that aligns development and operations teams so they have a single mindset on improving customer experiences, responding faster to business needs, and ensuring that innovation is balanced with security and operational needs. Modern organisations believe in merging of development and operational people and responsibilities so that one DevOps team carries both responsibilities. In that way you just have one team who takes the responsibility of development, deployment and running the software in production.
Continuous integration (CI) and continuous delivery (CD) is a set of operating principles that enable application development teams to deliver code changes more frequently and reliably. The technical goal of CI is to establish a consistent and automated way to build, package, and test applications. With consistency in the integration process in place, teams are more likely to commit code changes more frequently, which leads to better collaboration and software quality.
Continuous delivery picks up where continuous integration ends. CD automates the delivery of applications to selected infrastructure environments. It picks up the package built by CI, deploys into multiple environments like Dev, QA, Performance, Staging runs various tests like integration tests, performance tests etc and finally deploys into production. Continuous delivery normally has few manual steps in the pipeline whereas continuous deployment is a fully automated pipeline which automates the complete process from code checkin to production deployment.
Cloud-native apps take advantage of the elasticity of the cloud by using increased resources during a use spike. If your cloud-based e-commerce app experiences a spike in use, you can have it set to use extra compute resources until the spike subsides and then turn off those resources. A cloud-native app can adjust to the increased resources and scale as needed.
We’re powering the next great retail disruption.
863 
7
863 claps
863 
7
We’re powering the next great retail disruption. Learn more about us — https://www.linkedin.com/company/walmartglobaltech/
Written by
Distinguished Engineer — Building the next generation Tech platform for Sams club
We’re powering the next great retail disruption. Learn more about us — https://www.linkedin.com/company/walmartglobaltech/
"
https://medium.com/firebase-tips-tricks/how-to-use-cloud-firestore-in-flutter-9ea80593ca40?source=search_post---------61,"Sign in
There are currently no responses for this story.
Be the first to respond.
Top highlight
Peter Haddad
Apr 4, 2020·10 min read
Note: This post was originally published on April 2020 and has been completely revamped and updated for accuracy and comprehensiveness.
In this article, we will add Cloud Firestore to a Flutter application, perform different read, write operation and use some queries to retrieve data.
This is the fourth article related to Firebase in Flutter, you can check the previous articles in the below links:
medium.com
medium.com
medium.com
To know how to download the google-service.json file, you can check the first article in the above list. In the last two articles, I created a form using Flutter performed queries for the realtime database and authenticated users with Firebase, but in this article I'm not going to create a form, its mostly going to be code snippet related to Firestore and explaining each one.
Note: You can find the source code for all the Firebase/Flutter tutorials in the following repository: Firebase-Flutter-tutorials. Support me by starring the repository and following me on Github for more awesome content! You can also subscribe to my newsletter and join me at Discord! Let’s get started 😁
Both Cloud Firestore and realtime database are nosql database, their are no joins, their are no columns or tables, and you don't have to worry about duplicating your data. The main difference between the two is that Cloud Firestore contains collections and inside these collections you have documents that may contain subcollections or fields mapped to a value while realtime database can be considered as a big json that will contain all the data. The other also important difference to take into consideration is the queries, in realtime database as you can tell from previous articles we can only use one orderByChild().equalTo() (we cannot chain) while in Cloud Firestore as we will see later in the article we can chain queries.
As I said before, to check how to create a flutter project and add the google-service.json file which is used for android, then please check this article How To Use Firebase In Flutter. Next, you need to add the following dependency to the pubspec.yaml file:
Click CTRL + S to save, and you have successfully added Cloud Firestore to your Flutter application!
There are two ways to add data to the Cloud Firestore, first way is to specifiy the document name and the second way Cloud Firestore will generate a random id, let us see both cases. So first in your State class you need to get an instance of Cloud Firestore:
Now, you can add the data:
As, you can see in the above, on press of a button, we create a user collection and we use the add() method which will generate a random id. Since the add() method returns Future<DocumentReference> therefore we can use the method then() which will contain a callback to be called when the Future finishes. The variable value which is a parameter passed to the callback is of type DocumentReference therefore we can use the property id to retrieve the auto generated id.
As you can see, we added a map, string, int and we can also add an array.
Note: If you have alot of data, then do not use all the data inside a map inside a collection, instead create a subcollection if the data is related to the top level collection, if not then just create another top level collection. Remember a document has a size limit of 1mb.
Now let’s say you are using Firebase Authentication In Flutter, instead of using an auto generated id, you can use the userId as the document id that way it will be easier to retrieve the data:
Here, since we use the userId as the document id, therefore we use the method set() to add data to the document. If a document already exists and you want to update it, then you can use the optional named parameter merge and set it to true:
This way the existing data inside the document will not be overwritten.
To update fields inside a document, you can do the following:
So, here we update the age to 60, we can also add a new field while updating existing field:
You can also update field, add a new field, update the map and add a new field to the map:
If we run the above, we would get:
Note: set() with merge:true will update fields in the document or create it if it doesn't exists while update() will update fields but will fail if the document doesn't exist
Now let’s say we want to add characteristics for each user in Cloud Firestore, we can do that by using a array:
As you can see now, using FieldValue.arrayUnion(), you can either add an array if it doesn’t exist or you can update an element in the array. If you want to remove an element from the array, then you can use FieldValue.arrayRemove([""generous""]), which will remove the element generous
Now let’s say that all our users will have pets, but we don’t want to retrieve those pets when we retrieve a list of users. In that case we can create a subcollection for pets. Remember, queries are shallow, meaning if we retrieve the documents inside the user collection, then the documents inside the pet collection wont be retrieved.
To create a pet subcollection, we can do the following:
Now this specific document will have a subcollection pet connected to it, which will make it easier if you want to retrieve the pet in relation with the user document.
To delete data from a document, you can use the method delete() which returns a Future<void>:
To delete a field inside the document, then you can use FieldValue.delete() with update():
To retrieve data from Cloud Firestore, you can either listen for realtime updates or you can use the method get():
So here we retrieve all the documents inside the collection users, the querySnapshot.docs will return a List<DocumentSnapshot> therefore we are able to iterate using forEach(), which will contain a callback with a parameter of type DocumentSnapshot and then we can use the property data to retrieve all the data of the documents.
Result:
You can also use result.exist which returns true is document exist.
So, as you can see above we didnt retrieve data from the pets collection since queries are shallow, therefore to retrieve the data from subcollections, you can do the following:
So here we retrieve the id and then use get() again to be able to retrieve the data inside the pet collection. Result:
To retrieve only one document, instead of all documents in a collection. You can do the following:
which will give you the following:
If you want to access the city inside the map or if you want to access the name, then you can use the get operator:
You can also use where, which retrieves data by ascending order, to retrieve documents that satisfy a condition. For example:
Here we use where() to check if the country attribute inside the address map is equal to USA and retrieve the document. Result:
To constantly listen for changes inside a collection, you can use the method snapshots():
The snapshots() method returns a Stream<QuerySnapshot>, therefore you can call the method listen() that will subscribe to the stream and keep listening for any changes in Cloud Firestore.
If you want see which document was modified or added or removed, then you can do the following:
This first will retrieve all the documents and then if you added, modify or remove it will retrieve that document. Example:
Cloud Firestore uses index to improve the performance of retrieving the data from the database. If there is no index then the database must go through each collection to retrieve the data which will make the performance bad. There are two index type single index which are automatically indexed by Firestore and composite index which you need to manually create. Therefore, you have to create an index whenever you are using more than one where() in a single query or if you are using one where() and orderBy() so basically when it is two different fields.
Note: You can only have 200 composite index
First let us create a sample data:
So now we can do the following queries:
Query 1:
Result:
Query 2:
Result:
Other queries on a single field, that you can perform are:
If you want to query on an array value, then you can do the following:
If you want to query on an array value, then you can do the following:
which will give you the following documents:
You can also perform or queries by using whereIn or arrayContainsAny. For example:
This will return every document where countryName is either italy or lebanon.
You can also chain where() queries, but if you are using isEqualTo with any other range comparison or with arrayContains, then you need to create a composite index. Example:
This will return an error, which will also include a link to create a composite index:
Therefore you can create the index in the console:
You will get the following result:
You cannot perform range queries on different fields, for example:
This will return the following error:
You can also order the retrieved documents, for example:
This will retrieve the first 3 countryName in ascending order, result:
Now if you use .orderBy(""countryName"", descending: true), then this will retrieve the last 3 countryName, result:
You can also combine where() with orderBy(), but if you are using a range query then both where() and orderBy() should contain the same field.
I hope you enjoyed this Flutter/Firebase article, in the next article I will use Firebase storage and to store images and connected to Firestore.
Originally published at https://petercoding.com on April 4, 2020.
Software Developer. Actively helping users with their Firebase questions on Stack Overflow. Occasionally I post on medium and other platforms.
2.4K 
13

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
2.4K claps
2.4K 
13
A collection of articles and tips & tricks on how to develop with Firebase, by a group of Firebase GDEs and developers active on Stack Overflow.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@deb_xpand/distributed-teams-are-the-new-cloud-for-startups-89b9dcada75a?source=search_post---------62,"Sign in
There are currently no responses for this story.
Be the first to respond.
Deb Bardhan
Dec 12, 2018·11 min read
By Anupam Rastogi & Deb Bardhan
We have seen a marked increase in the number of startups being built outside traditional technology hubs such as Silicon Valley. We’ve also experienced firsthand how startups are building seamless distributed teams spread across many geographies. The conventional wisdom has been that it is a significant inhibitor for a startup to have team members based out of multiple geographies or fully distributed. Many mainstream venture investors have historically been loathe to invest in such companies. And this has been for a good set of reasons.
We posit that it is time to change these beliefs. We’ve reached a tipping point where the advantages of building a distributed team can offset the inherent challenges. Further, the ability to execute rapidly with a distributed or remote team can provide a startup with significant competitive advantages over those limited to one region.
We are beginning to see breakout examples of this trend. InVision has built out a billion dollar business with 700 employees and zero offices. Popular, high-growth SaaS products such as Automattic, Buffer and Gitlab have been built by engineers spread across many cities. Companies such as Atlassian, Freshworks and Algolia (originated from Australia, India and Europe respectively) have leveraged remote offices since their early days and are now prominent in their respective market segments globally. With skyrocketing costs and high employee turnover in the SF Bay Area, we are seeing many startups from here open offices in more stable locations with tech talent availability. At Emergent Ventures, we meet multiple early stage startups with distributed or remote teams every week, and have invested in several of them.
Just as the Cloud had a proverbial 10X multiplier effect on the technology ecosystem by exponentially reducing startup costs, we believe that distributed teams — by removing the location barrier to team growth — are the next game-changer for the startup world. When done right, distributed teams can be faster to set up and scale, can cost less to run, and can be more resilient and available. Distributed teams are also a great way to overcome regional talent barriers and immigration constraints.
Why now?
We have seen a marked increase in startups with truly distributed teams in recent years. The snowball is now building up. The advent of the Internet and video conferencing tools provided the necessary backdrop for distributed teams to be successful. Yet, those are not exactly recent developments. Here are reasons we believe startups with distributed teams are taking off now:
The Pitfalls
Distributed teams are not for every startup — at least not yet. And there are certainly some fundamental challenges that only a few have demonstrated they can master. Shaping the culture of a startup in its early days can be an order of magnitude harder with a distributed team. Team communication can be a challenge in the absence of nonverbal cues. Some have found that distributed teams may not be able to move as fast as a dozen early startup employees cranking away, ‘locked’ for days in the proverbial Silicon Valley garage. Motivation and morale can be hard to sustain in a virtual environment. Training and onboarding acquire a new set of challenges. Some functions can be harder to execute well in a fully distributed structure, e.g. creative pursuits that rely on brainstorming and serendipity. And for teams spread across the globe, conference calls and other synchronous communication may be limited to a few inconvenient hours of the day.
Getting it Right
Over the years, we have observed several practices shared by successful distributed teams that help them overcome the inherent challenges of the model. This can be the topic of an entire book (or at least a blog post of its own), however, here are some of the big ones:
Once a startup has the distributed team engine going, it can optimize the location of each hire for:
(Talent Availability * Employee Retention *Speed to Scale) / Cost
Distributed Team Models
There isn’t a generally accepted definition of a “distributed team”. For instance, if two out of a team of ten people are based in different locations, then it would likely be seen as a core local team with some remote members. When does a team go from being a core+remote to distributed? Our opinion is that if the ratio of remote members to total team size is greater than a third then it begins to have the dynamics of a distributed team. We include teams with two or more offices or locations in our definition of distributed teams.
Based on our observation and study of distributed teams, we classify them into a few different structures and models. We have not come across widely accepted nomenclature describing these models and will take the creative license to suggest names for them as we reflect on the key dynamics for these models. Distributed team setups that we see most frequently include:
1. Hub and Spoke model: In the Hub and Spoke model, there is a clear core or headquarter location, which often houses some of the leadership. There are one or more “satellite” locations with team members focused on specific functions or needs, e.g. software development, data wrangling or inside sales.
This structure has been used by larger corporations for a long time. It is now increasingly used by early-stage startups to build product away from the noisy and expensive tech hubs. Some startups use this model to tap into specific competency areas, such as Montreal for AI or Pittsburgh for Autonomous Vehicles. This model is also seen where startups with a high-touch field sales model add regional sales offices to be closer to the critical mass of customers. Our own decision of making Xpand a bi-coastal team early on was driven by the decision to be close to our two core customer segments — financial services and technology.
2. Reverse Offshore model: This is an important variant of the Hub and Spoke model, and has the team focussed around two core geographies. The primary location is often focused on building the product and its delivery/operations, while the other is focused on selling the product.
The genesis of the traditional offshore model was with larger corporations offshoring certain functions, primarily as a cost saving measure. In that format, the core team is close to the customer base, and parts of product development seen as non-core are often offshored.
Many startups are now reversing the traditional offshore model, where they start in a location suitable for building a high-quality product, but not in close proximity to the target customer base. These startups then establish a go-to-market team in their target market. Israeli enterprise software and security startups pioneered this model over the past couple of decades. We are now seeing high-quality B2B software companies emerge out of India, Europe and elsewhere. Freshdesk and Criteo are examples of this model, where they ‘offshored’ part of their go-to-market teams to the US after building product elsewhere.
3. Polygon model: In this model, a startup begins with a central seeding location. Over time, it expands to other locations that are equally important, with no clearly defined headquarter. Each of these locations is ‘full-stack’, has a cross-section of important functions, and is relatively self-sufficient.
Amazon’s recently announced move to a three headquarter format (Seattle + New York City + Arlington) aligns with this model. Atlassian is another example where the founding vertex was Sydney, and other equally important vertices were developed in SF and Ireland. This model can work well with a configuration where different businesses or product lines are managed out of different offices. We believe this model is better suited to growth-stage startups and large corporations. Early-stage startups benefit from a clearly identified central node that can drive decisions and agility.
4. Truly Distributed model (“Beans” model): In this model, team members are spread across cities and countries. Often there is no formal office at all. The number of locations such a startup has members in could approach the number of its team members. The emergence of this model is the culmination of extensive usage of seamless collaboration tools and maturation of remote hiring/management practices.
Buffer is an example of this model, where a team of 79 members is dispersed across 50+ locations. Locations are not organized by strategic competency and team members contribute from wherever they want to work. In recent years, we’ve been seeing a surge of startups with this model. Many successful fully distributed startups we’ve seen are focused on consumers, developers or SMBs as customers. Those focused on larger Enterprise customers and/or with a complex technology core have so far been underrepresented here. But this may change as the distributed team model becomes more mainstream and remote collaboration tools evolve further.
This model is not easy to pull off. But those who are able to get this right in the early days of a startup have a significant competitive advantage in our opinion. We expect to see many more successful startups emerge with this model, especially in combination with the Hub and Spoke model. One format we are seeing frequently has a small central leadership team based in San Francisco or Silicon Valley, with rest of the team working remotely from anywhere in the world.
Final Thoughts
As you may have already concluded, each distributed team setup has its own benefits and shortcomings. Different structures lend themselves well to different evolution points in a team/startup’s trajectory. As you think about your team’s structure, evaluate how it aligns with your company’s competitive advantages, focus segment and customer base. Key factors to consider are around optimizing cost structure, attracting and retaining the right type of talent, taking advantage of certain tax benefits, developing proximity to customers, and access to future financing, strategic partners and potential acquirers. The distributed team model brings its own challenges, but there are known strategies and tactics around team building, conflict resolution, creative co-development, and driving goal alignment for the different types of distributed teams. We hope to cover some of these areas in future posts.
We welcome your feedback and look forward to learning about your experiences of building distributed teams.
Authors
Deb Bardhan is an entrepreneur and early stage investor based in Silicon Valley. He is the co-founder and CEO of Xpand, an enterprise SaaS startup, tackling the problem of employee onboarding with customers like Allianz, ING, Tata. He’s also an Entrepreneur-In-Residence at Pre-Hype and co-founder and board member of the Wharton Alumni Angels. He can be reached via LinkedIn.
Anupam Rastogi is a venture capital investor and advisor based in the San Francisco Bay Area. He has been associated over two dozen startups and growth-stage companies from around the globe. Anupam focuses on SaaS, AI and Smart Mobility investments at Emergent Ventures. He can be reached via Linkedin or Twitter
All images owned by respective authors under Creative Commons
Deb Bardhan is an entrepreneur and early stage investor based in Silicon Valley. He is the co-founder and CEO of Xpand, an enterprise SaaS startup.
644 
644 
644 
Deb Bardhan is an entrepreneur and early stage investor based in Silicon Valley. He is the co-founder and CEO of Xpand, an enterprise SaaS startup.
"
https://medium.com/@alibaba-cloud/redis-vs-memcached-in-memory-data-storage-systems-3395279b0941?source=search_post---------63,"Sign in
There are currently no responses for this story.
Be the first to respond.
Top highlight
Alibaba Cloud
May 18, 2018·15 min read
Redis and Memcached are both in-memory data storage systems. Memcached is a high-performance distributed memory cache service, and Redis is an open-source key-value store. Similar to Memcached, Redis stores most of the data in the memory. It supports operations on various data types including strings, hash tables, and linked lists among others. The Alibaba Cloud ApsaraDB family supports these two popular data storage systems: ApsaraDB for Redis and ApsaraDB for Memcache. In this article, we will examine the difference between Redis and Memcached.
Salvatore Sanfilippo, the author of Redis, shared the following points of comparison between Redis and Memcached:
Let us now discuss some points to support the above comparisons.
Unlike Memcached which only supports data records of the simple key-value structure, Redis supports much richer data types, including String, Hash, List, Set and Sorted Set. Redis uses a redisObject internally to represent all the keys and values. The primary information of the redisObject is as shown below:
The type represents the data type of a value object. The encoding indicates the storage method of different data types in the Redis, such as type=string represents that the value stores a general string, and the corresponding encoding may be raw or int. If it is int, Redis stores and represents the associated string as a value type. Of course, the premise is that it is possible to represent the string by a value, such as strings of “123″ and “456”. Only upon enabling the Redis virtual memory feature will it allocate the vm fields with memory. This feature is off by default. Now let us discuss some data types.
Frequently used commands: set/get/decr/incr/mget and so on.
Application scenarios: String is the most common data type and general key/value belong to this category.
Implementation method: String is a character string by default in Redis referenced by redisObject. When called for the INCR or DECR operations, the system will convert it to the value type for computation. At this time, the redisObject’s encoding field is int.
Frequently used commands: hget/hset/hgetall and so on.
Application scenarios: Storing user information object data, including the user ID, user name, age and birthday; retrieving the user name, age or birthday through the user ID.
Implementation method: Hash in Redis is a HashMap of the internally stored value and provides the interface for direct access to this map member. As shown in the figure, the Key is the user ID and the Value is a map. The key of this map is the member attribute name, and the value is the attribute value. In this way, one can directly execute changes and have access to the data through the internal map’s key (In Redis, the internal map key is called field), that is, through the key (user ID) + field (attribute tag) to perform operations on the corresponding attribute data.
There are two ways of implementation for the current HashMap: when there are only a few members in the HashMap, Redis opts for one-dimensional arrays for compact storage to save memory, instead of the HashMap structure in the real sense. At this time, the encoding of the corresponding value redisObject is zipmap. When the number of members increases, Redis will convert them into the HashMap in the real sense and the encoding at this time will be ht.
Frequently used commands: lpush/rpush/lpop/rpop/lrange.
Application scenarios: The Redis List is the most important data structure in Redis. In fact, it is possible to implement Twitter’s following list and fans list using the list structure of Redis.
Implementation method: Through a two-way linked list which supports reverse lookups and traversing to facilitate operations. But it also brings about some additional memory overhead. Many implementations in Redis, including sending buffering queues, also adopt this data structure.
Frequently used commands: sadd/spop/smembers/sunion and so on.
Application scenarios: Redis set provides a similar external list function as list does. It is special in that the set can automatically remove duplicates. When you need to store a list of data without any duplication, the set is a good option. In addition, the set provides an important interface to judge whether a member is within a set, a feature not provided by list.
Implementation method: The internal implementation of set is a HashMap whose value is always null. It actually removes duplicates quickly through calculating the hash values. In fact, this is also the reason why set can judge whether a member is within the set.
Frequently used commands: zadd/zrange/zrem/zcard and so on.
Application scenarios: The application scenarios of Redis sorted set are similar to those for the set. The difference is that while set does not automatically sort the data, sorted set can sort the members through a priority parameter (score) provided by the user. What’s more, the latter also sorts inserted data automatically. You can choose the sorted set data structure when you need an orderly set list without duplicate data, such as Twitter’s public timeline, which can take the posting time as the score for storage with automatic sorting of the data obtained by time.
Implementation method: Redis sorted set uses HashMap and SkipList internally to ensure efficient data storage and order. HashMap stores the mappings between the member and the score; while the SkipList stores all the members. The sorting relies on the score stored in the HashMap. Using the SkipList structure can improve the search efficiency and simplify the implementation.
In Redis, not all data storage occurs in memory. This is a major difference between Redis and Memcached. When the physical memory is full, Redis may swap values not used for a long time to the disk. Redis only caches all the key information. If it discovers that the memory usage exceeds the threshold value, it will trigger the swap operation. Redis calculates the values for the keys to be swapped to the disk based on “swappability = age*log(size_in_memory)”. It then makes these values for the keys persistent into the disk and erases them from memory. This feature enables Redis to maintain data of a size bigger than its machine memory capacity. The machine memory must keep all the keys and it will not swap all the data.
At the same time, when Redis swaps the in-memory data to the disk, the main thread that provides services, and the child thread for the swap operation will share this part of memory. So, if you update the data you intend to swap, Redis will block this operation, preventing the execution of such a change until the child thread completes the swap operation. When you read data from Redis, if the value of the read key is not in the memory, Redis needs to load the corresponding data from the swap file and then return it to the requester. Here, there is a problem of the I/O thread pool. By default, Redis will encounter congestion, that is, it will respond only after successful loading of all the swap files. This policy is suitable for batch operations when there are a small number of clients. But if you apply Redis in a large website program, it is not capable of meeting the high concurrency demands. However, you can set the I/O thread pool size for Redis running, and perform concurrent operations for reading requests for loading the corresponding data in the swap file to shorten the congestion time.
For memory-based database systems like Redis and Memcached, the memory management efficiency is a key factor influencing the system’s performance. In traditional C language, malloc/free functions are the most common method for distributing and releasing memory. However, this method harbors a huge defect: first, for developers, unmatched malloc and free will easily cause memory leakage; second, the frequent calls will make it difficult to recycle and reuse a lot of memory fragments, reducing the memory utilization; and at last, the system calls will consume a far larger system overhead than the general function calls. Therefore, to improve the memory management efficiency, memory management solutions will not use the malloc/free calls directly. Both Redis and Memcached adopt their self-designed memory management mechanisms, but the implementation methods vary a lot. Next we introduce these two mechanisms.
Memcached uses the Slab Allocation mechanism for memory management by default. Its main philosophy is to segment the allocated memory into chunks of a predefined specific length to store the key-value data records of the corresponding length to solve the memory fragment problem completely. Ideally, the slab’s design of the allocation mechanism should guarantee external data storage, that is to say, it facilitates the storage of all the key-value data in the Slab Allocation system. However, the application of other memory requests of Memcached occur via general malloc/free calls. Normally, this is because the number and frequency of these requests determine that they will not affect the overall system performance. The principle of Slab Allocation is very simple. As shown in the figure, it first applies for a bulk of memory from the operating system and segments it into chunks of various sizes, and then groups chunks of the same size into the Slab Class. Among them, the chunk is the smallest unit for storing the key-value data. It is possible to control the size of each Slab Class by making a Growth Factor at the Memcached startup. Suppose the Growth Factor in the figure is 1.25. If the chunk in the first group is 88 bytes in size, the chunk in the second group will be 112 bytes. The remaining chunks follow the same rule.
When Memcached receives the data sent from the client, it will first select the most appropriate Slab Class according to the data size, and then query the idle chunk list containing the Slab Class in the Memcached to locate a chunk for storing the data. When a piece of data expires or is obsolete, and therefore discarded, it is possible to recycle the chunk originally occupied by the record and restore it to the idle list.
From the above process, we can see that Memcached has very high memory management efficiency that will not cause memory fragments. Its biggest defect, however, is that it may cause space waste. Because the system allocates every chunk in the memory space of a specific length, longer data might fail to utilize the space fully. As shown in the figure, when we cache data of 100 bytes into a chunk of 128 bytes, the unused 28 bytes go to waste.
Implementation of Redis’ memory management mainly proceeds through the two files zmalloc.h and zmalloc.c in the source code. To facilitate memory management, Redis will store the memory size in the memory block header following memory allocation. As shown in the figure, the real_ptr is the pointer returned after Redis calls malloc. Redis stores the memory block size in the header and the memory occupied by the size is determinable, that is, the system returns the length of the size_t type, and then the ret_ptr. When the need to release memory arises, the system passes ret_ptr to the memory management program. Through the ret_ptr, the program can easily calculate the value of real_ptr and then pass real_ptr to release the memory.
Redis records the distribution of all the memory by defining an array the length of which is ZMALLOC_MAX_ALLOC_STAT. Every element in the array represents the number of memory blocks allocated by the current program and the size of the memory block is the subscript of the element. In the source code, this array is zmalloc_allocations. The zmalloc_allocations[16] represents the number of memory blocks allocated with the length of 16 bytes. The zmalloc.c contains a static variable of used_memory to record the total size of the currently allocated memory. So in general, Redis adopts the encapsulated malloc/free, which is much simpler compared with the memory management mechanism of Memcached.
Although a memory-based store, Redis supports memory data persistence and provides two major persistence policies, RDB snapshot and AOF log. Memcached does not support data persistence operations.
Redis supports storage of the snapshot of the current data into a data file for persistence, that is, the RDB snapshot. But how can we generate the snapshot for a database with continuous data writes? Redis utilizes the copy on write mechanism of the fork command. Upon creation of a snapshot, the current process forks a subprocess that makes all the data cyclic and writes them into the RDB file. We can configure the timing of an RDB snapshot generation through the save command of Redis. For example, if you want to configure snapshot generation for once every 10 minutes, you can configure snapshot generation after each 1,000 writes. You can also configure multiple rules for implementation together. Definitions of these rules are in the configuration files of the Redis. You can also set the rules using the CONFIG SET command of Redis during the runtime of Redis without restarting Redis.
Redis’ RDB file is, to an extent, incorruptible because it performs its write operations in a new process. Upon the generation of a new RDB file, the Redis-generated subprocess will first write the data into a temporary file, and then rename the temporary file into a RDB file through the atomic rename system call, so that the RDB file is always available whenever Redis suffers a fault. At the same time, the Redis’ RDB file is also a link in the internal implementation of Redis’ master-slave synchronization. However, RDB has its deficiency in that once the database encounters some problem, the data saved in the RDB file may be not up-to-date, and data is lost during the period from last RDB file generation to Redis failure. Note that for some businesses, this is tolerable.
The full form of AOF log is Append Only File. It is an appended log file. Unlike the binlog of general databases, the AOF is a recognizable plaintext and its content is the Redis standard commands. Redis will only append the commands that will cause data changes to the AOF. Every command for changing data will generate a log. The AOF file will become larger and larger. Redis provides another feature — AOF rewrite. The function of AOF rewrite is to re-generate an AOF file. There is only one operation for each record in the new AOF file, instead of multiple operations for the same value recorded in the old copy. The generation process is similar to the RDB snapshot, namely forking a process, traversing the data and writing data into the new temporary AOF file. When writing data into the new file, it will write all the write operation logs to the old AOF file and record them in the memory buffering zone at the same time. Upon completing the operation, the system will write all the logs in the buffering zone to the temporary file at one time. Thereafter, it will call the atomic rename command to replace the old AOF file with the new AOF file.
AOF is a write file operation and aims to write the operation logs to the disk. It also involves the write operation procedure we mentioned earlier. After Redis calls the write operation for AOF, it uses the appendfsync option to control the time for writing the data to the disk by calling the fsync command. The three settings options in the appendfsync below have security strength from low to strong.
For general business requirements, we suggest you use RDB for persistence because the RDB overhead is much lower than that of AOF logs. For applications that cannot stand the risk of any data loss, we recommend you use AOF logs.
Memcached is a full-memory data buffering system. Although Redis supports data persistence, the full-memory is the essence of its high performance. For a memory-based store, the size of the memory of the physical machine is the maximum data storing capacity of the system. If the data size you want to handle surpasses the physical memory size of a single machine, you need to build distributed clusters to expand the storage capacity.
Memcached itself does not support distributed mode. You can only achieve the distributed storage of Memcached on the client side through distributed algorithms such as Consistent Hash. The figure below demonstrates the distributed storage implementation schema of Memcached. Before the client side sends data to the Memcached cluster, it first calculates the target node of the data through the nested distributed algorithm that in turn directly sends the data to the node for storage. But when the client side queries the data, it also needs to calculate the node that serves as the location of the data queried, and then send the query request to the node directly to get the data.
Compared with Memcached which can only achieve distributed storage on the client side, Redis prefers to build distributed storage on the server side. The latest version of Redis supports distributed storage. Redis Cluster is an advanced version of Redis that achieves distributed storage and allows SPOF. It has no central node and is capable of linear expansion. The figure below provides the distributed storage architecture of Redis Cluster. The inter-node communication follows the binary protocol but the node-client communication follows the ASCII protocol. In the data placement policy, Redis Cluster divides the entire key numerical range into 4,096 hash slots and allows the storage of one or more hash slots on each node. That is to say, the current Redis Cluster supports a maximum of 4,096 nodes. The distributed algorithm that Redis Cluster uses is also simple: crc16 (key) % HASH_SLOTS_NUMBER.
Redis Cluster introduces the master node and slave node to ensure data availability in case of SPOF. Every master node in Redis Cluster has two corresponding slave nodes for redundancy. As a result, any two failed nodes in the whole cluster will not impair data availability. When the master node exists, the cluster will automatically choose a slave node to become the new master node.
In this article, we have discussed the differences between Redis and Memcached. We started by listing few points of comparison suggested by Salvatore Sanfilippo, the author of Redis. Thereafter, we further elaborated, with key points of difference between Redis and Memcached being the data-types supported, cluster management, data persistence support, and memory management schemes.
To learn more, visit the Alibaba Cloud ApsaraDB for Redis page and the ApsaraDB for Memcache page.
Reference:
https://www.alibabacloud.com/blog/redis-vs-memcached-in-memory-data-storage-systems_592091?spm=a2c41.11544560.0.0
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
1.5K 
6
1.5K claps
1.5K 
6
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://engineering.opsgenie.com/comparing-api-gateway-performances-nginx-vs-zuul-vs-spring-cloud-gateway-vs-linkerd-b2cc59c65369?source=search_post---------64,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
As OpsGenie, we have been growing aggressively, both in terms of headcount and product features. To give you some idea, our engineering team grew from 15 to 50 just last year. To scale up the development teams, we divided the engineering power to eight-people teams by obeying the Two Pizza team rule.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.datadriveninvestor.com/realtime-database-vs-cloud-firestore-which-database-is-suitable-for-your-mobile-app-87e11b56f50f?source=search_post---------65,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Top highlight
Firebase from Google offers two types of cloud-based client-accessible database solutions for mobile apps that support realtime data syncing — Realtime Database and Cloud Firestore.
Realtime Database is the Firebase’s first and original cloud-based database. For the mobile apps requiring synced states across clients in realtime, it is an efficient and low-latency solution.
Cloud Firestore is Firebase’s newest flagship database for mobile apps. It is a successor to the Realtime Database with a new and more intuitive data model. Cloud Firestore is richer, faster, and more scalable than the Realtime Database.
Which database is a right choice for your mobile app?
Before choosing the one to go with, consider the following differences between Realtime Database and Cloud Firestore.
· Data Model
Realtime Database and Cloud Firestore, both are NoSQL Databases.
Realtime Database stores data as one large JSON tree which makes it easier to store simple data but harder to organize complex, hierarchical data at scale.
Cloud Firestore stores data in documents arranged in collections. Simple data is stored in documents, which is easy and similar to the way data is stored in JSON. Complex, hierarchical data is conveniently organized at scale using subcollections within documents. Cloud Firestore requires less denormalization and data flattening.
· Realtime and offline support
Both Realtime Database and Cloud Firestore have mobile-first, realtime SDKs and support local data storage for offline-ready apps.
Realtime Database offers offline support for mobile clients on iOS and Android only whereas Cloud Firestore provides offline support for iOS, Android, and web clients.
· Querying
Data can be retrieved, sorted, and filtered from any of the databases through queries.
Realtime Database supports deep queries with limited sorting and filtering functionality. In a single query, you can either sort or filter, not both, on a property. By default, queries are deep which always return the entire subtree.
Cloud Firestore support indexed queries with compound sorting and filtering. Unlike Realtime Database, Cloud Firestore allows chaining the filters and combining filter and sort on a property in a single query. You don’t have to write deep queries; instead of an entire collection or entire document, you can query subcollections within a document. The default query type is indexed wherein query performance is proportional to the size of your result set, not your data set.
· Writes and transactions
Realtime Database facilitates basic write and transaction operations. Data can be written as an individual operation while transactions in the native SDKs require a completion callback.
Cloud Firestore facilitates atomic write and transaction operations. Operations can be batched and completed atomically while transactions will repeat automatically until they’re completed.
· Reliability and performance
Realtime Database is a mature product and you can expect the stability from a battle-tested, tried-and-true product. Realtime Database demonstrates very low latency as databases are limited to zonal availability in a single region, which makes it a great option for frequent state-syncing.
Cloud Firestore is currently in beta version and stability in a beta product is not always the same as that of a fully launched product. However, Firebase assures that when Cloud Firestore graduates from beta, it will have stronger reliability than Realtime Database as it will house your data across multiple data centers in distinct regions, ensuring global scalability and strong reliability.
· Scalability
Scaling in Realtime Database requires sharding, meaning scaling beyond 100,000 concurrent connections and 1,000 writes/second in a single database requires distributing your data across multiple databases.
In Cloud Firestore (after beta), scaling will be fully automatic which means sharding your data across multiple instances won’t be needed.
· Security
Realtime Database cascade rules that require separate validation. The only security option for Realtime Database is the Firebase Database Rules. It also read and writes rules cascade. The data needs to be validated separately using the ‘validate’ rule.
Cloud Firestore provides simpler yet more powerful security for mobile, web, and server SDKs where mobile and web SDKs use Cloud Firestore Security Rules while server SDKs use Identity and Access Management (IAM). Also, data validation is automatic and rules don’t cascade unless a wildcard is used. However, rules can restrain queries which means, if a query’s results might contain data the user doesn’t have access to, the entire will query fail.
· Pricing
Both solutions are available on the Firebase’s Spark (Free), Flame ($25/month), and Blaze (Pay as you use) pricing plans.
Realtime Database charges only for bandwidth and storage, but at a higher rate.
Cloud Firestore charges based on operations performed in your database (read, write, delete) and, at a lower rate, bandwidth and storage. It supports daily spending limits for Google App Engine projects, to ensure you don’t go over the costs that you’re comfortable with.
Conclusion
It is possible to use both of the databases together within the same Firebase app or project. Both the databases are capable to store the same kind of data and the client libraries work in a similar manner for both of them.
If you are comfortable using a beta product, use Cloud Firestore for new projects. Cloud Firestore provides additional functionality, performance, and scalability on an infrastructure which is specifically designed to incorporate more powerful features in future releases.
You can expect to find new query types, more robust security rules, and improved performance with some other advanced features planned for Cloud Firestore. Just keep in mind the differences mentioned above if you decide to use both databases or choose one of them for your mobile app.
empowerment through data, knowledge, and expertise.
1.5K 
6
1.5K claps
1.5K 
6
Written by
Ashish Sharma is the Chief Marketing Officer at WeDigTech, a Mobile App Development Company in LA California US. He is responsible for marketing activities.
empowerment through data, knowledge, and expertise. subscribe to DDIntel at https://ddintel.datadriveninvestor.com
Written by
Ashish Sharma is the Chief Marketing Officer at WeDigTech, a Mobile App Development Company in LA California US. He is responsible for marketing activities.
empowerment through data, knowledge, and expertise. subscribe to DDIntel at https://ddintel.datadriveninvestor.com
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/hackernoon/aws-vs-digitalocean-which-cloud-server-is-better-1386499a6664?source=search_post---------66,"There are currently no responses for this story.
Be the first to respond.
Sixth annual State of the Cloud Survey conducted by RightScale on the latest cloud computing trends shows that AWS continues to lead in public cloud adoption.
The cloud is usually used to refer to a few servers associated with the web that can be contracted as a part of a product or software application service. Cloud-based services can incorporate web hosting, data sharing, and software use.
‘The cloud’ can likewise refer to distributed computing, where a few servers are connected to share the load. This connection means that as opposed to using one single powerful server, complex procedures can be distributed over different smaller servers
In a cloud, there are many distributed resources acting as one. This makes the cloud very tolerant of errors, due to the distribution of data. Use of the cloud tends to lessen the creation of different versions of files, due to shared access to records and data.
DigitalOcean and AWS are cloud service platforms that offer database storage, computer power among other functionalities. DigitalOcean versus AWS is as named a David versus Goliath story with a twist. The brave upstart, DigitalOcean, faces a setup behemoth. Like David, DigitalOcean has a technique that plays to its strengths while staying away from a fight with Amazon. But this isn’t a battle until the very end. Amazon and AWS address the necessities of various groups of audiences and realizing what each does well will empower you to pick between them.
DigitalOcean (spelled as a single word; “Digital Ocean” was a 90’s-era producer of wireless communications gadgets) is a new cloud hosting supplier. Launched in 2011, DigitalOcean concentrates solely on developers’ needs. The organization currently has 9 data centers positioned in San Francisco, Singapore, Amsterdam, New York and London.
DigitalOcean concentrates on three key selling points to stand out: simplicity, pricing, and high-performance virtual servers. They zero in on giving developers an easy and quick way to set up affordable Linux instances which they call droplets. DigitalOcean supports most of the modern Linux distros; Ubuntu, Fedora, Debian, and CentOS. It is straightforward to set up several applications on their droplets e.g. Ruby on Rails, LAMP, Ghost, Docker or stack.
DigitalOcean’s pricing is the most affordable among all cloud providers. Pricing starts at $0.007/hr or $5/mo and they provide an easy transition between the hourly and monthly tariffs. Their most popular package, called a droplet costs $0.015/hr or $10/mo while providing 1 core processor, 1Gm memory, 30GB SSD disk and 2TB transfer. On AWS the closest to this is a package known as t2.small instance which goes for $0.026/hr, doubling the cost of a droplet on DigitalOcean. One more benefit is that DigitalOcean doesn’t have hidden charges for extra services like more traffic or fixed IP addresses.
DigitalOcean is known for providing very high-performance servers. Their network speed is 1Gbps; all hard disks are SSD and an incredible start-up time of only 55 secs. DigitalOcean nodes are placed at the top by independent performance tests, way far above Amazon machines.
DigitalOcean has a massive stockpile of documentation accessible for its administration, and since it is as yet a moderately basic VPS host, there is generally not much of a need for support. So, they don’t offer help by telephone and response times by email can be slow, likely on account of the sheer number of clients they are supporting.
Looking at performance vs price, DigitalOcean had a notable edge over most of its rivals for years, but that notch has since been shut as everyone followed their lead and revised their network infrastructure while lowering pricing on entry level packages. DigitalOcean is a favorite among developers due to the super fast setup times user friendliness of their platform.
Finally, DigitalOcean prides itself with a simple, user-friendly setup. Targeting developers only, providing Linux virtual machines and DNS management. It lacks hosted databases, configuration management, analytics, load balancing among others. DigitalOcean proudly markets themselves as a bare-bones IaaS provider for Linux developers.
Amazon’s AWS is the market’s leader by far; It is estimated that Amazon has as much computing muscle as the next 11 rivals on the list combined. They offer an umbrella of various IaaS and PaaS solutions. The most celebrated of them all is the EC2 IaaS solution. Other services offered by AWS include load balancing, storage, content delivery, databases, networking and content delivery, deployment and configuration management and application development platforms. They own the largest data centers in the world located strategically in 9 regions around the world.
It is evident that DigitalOcean cannot compete with Amazon’s AWS concerning features. The only area where DigitalOcean can compete is against the EC2, but even here DigitalOcean’s capacity is about 1% that of the EC2.
Amazon also has a robust database of help documentation, but with the vast size of service offering available in the portal, most clients will need to engage with support at some point or another. They have a huge support team that can accommodate this, but support is not included with all packages. Technical assistance charges can take up to 10% of your monthly expenditure, which can add up for larger organizations.
If you are already of DigitalOcean, you should congratulate yourself for making the smarter choice. If you’re on AWS, running a couple of ECM virtual machines, with high costs of bandwidth each month, it might be deserving to switch and take advantage of the free bundled bandwidth.
It is clear that the need for developer centric cloud is rising. The intent towards a ‘NoOps’ environment is evident. However, the answer to which cloud server is better solely depends on your project requirements and compliance.
Written by Dmitry Budko
Want to learn more? Check out here
#BlackLivesMatter
1.6K 
12
1.6K claps
1.6K 
12
Written by
Collection of posts from those who build Dashbouquet https://dashbouquet.com/
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
Collection of posts from those who build Dashbouquet https://dashbouquet.com/
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
"
https://blog.coinbase.com/how-coinbase-builds-secure-infrastructure-to-store-bitcoin-in-the-cloud-30a6504e40ba?source=search_post---------67,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Top highlight
Three and a half years ago, Coinbase launched using a simple hosting platform: Heroku.
It was the right solution at the time. With just two technical founders building the product (neither with any serious dev-ops experience) we knew that Heroku would be more battle tested than any solution we could hack together on our own.
But we also knew this wouldn’t work forever. Early in our company’s history, we started to contemplate the next version of our infrastructure that would run inside AWS. It had to be built from the ground up with security in mind (the most common ways that bitcoin companies die is due to theft and hacking) but we didn’t want to compromise on engineering happiness and productivity.
After about a year, we finally completed the transition and we’ve been running inside AWS for quite some time now. This post outlines some of what we learned during the transition. It can be used as a starting point to building paranoid and productive infrastructure in the cloud.
Today, Coinbase securely stores about 10% of all bitcoin in circulation.
Disclaimer: Though we discuss some of our security measures below, our security measures are continually evolving. These are just a few measures that have existed at one point in our growth. For more on our approach to security, see this YouTube talk.
Two of the most important principles we followed when designing our infrastructure are layered security and eliminating single points of failure. Both of these concepts encourage you to not put all your eggs in one basket. Instead, strive for redundancy and consensus amongst multiple parties. These concepts are used heavily in bank security, nuclear launches, certificate authorities, corporate governance, and even human resources.
A simple example of this in practice is securing your administrator account on AWS with a two factor token that is controlled by a second person. If you have one person who controls the password to the account, give the second factor token to another party. Store the second factor in a vault or safe deposit box off site for some physical (in addition to crypto) based security.
It can prevent a single person maliciously (or accidentally) ending the company.
The developers on your team should not have (or need) production SSH access to do their regular work (deploying code, spinning up new services, debugging, etc).
However, it is difficult to entirely eliminate the need for SSH access. Some people in the company will always need a way to debug obscure problems. When people do need SSH access, here is how you can lock it down:
If you have any particularly sensitive keys (in our case, bitcoin private keys) try storing them entirely offline (air-gapped). Coinbase early on made a decision to store the vast majority (98%+) of customer bitcoin entirely offline, in safe deposit boxes.
Version one looked like this. Just some USB drives (and paper backups) stored in a safe deposit box at a local bank.
We’re now on version three of our cold storage and it has come a long way. Keys are generated offline in a secure environment, and split using Shamir’s secret sharing. Each private key is divided into parts, and some subset of the pieces are required to restore the secret. This way some pieces can be lost and the secret is still recoverable (redundancy). It also requires a quorum of key-holders to come together to restore a key (consensus).
Key holders are geographically distributed and follow a protocol during key signing ceremonies to verify their identity and assure the integrity of the ceremony.
Here is a simple example of generating a 2 of 3 key (where at least 2 of the 3 pieces are required to recombine the secret) using Hashicorp’s open source Vault Project. You can require 5 of 10 pieces or any threshold you’re comfortable with.
In addition to logging production access (as mentioned before), you should log everything happening across all containers in your infrastructure.
It is critical to have a good audit trail if there ever is an incident. The only thing worse than being hacked, is being hacked but not knowing how it happened. Your only option then is to hope you’ve patched the right thing, and relaunch with fingers crossed (guess and check).
Great logging also creates a deterrent against theft. People are less likely to steal if they feel there is a chance they will get caught.
Designing an environment focused on low-latency and high variety logs required a new design for our new infrastructure. To reduce the complexity of logging, we wanted to push all of our logs through one place that could be consumed in many ways. Running bitcoin nodes around the world required our logging endpoints to be accessible across many networks. To minimize the complexity of adding log producers and consumers, we now pipe every event across Coinbase through a streaming, distributed log (Kinesis) that provides flexible at-least-once guaranteed processing and a multi-day buffer of data that can be replayed as needed.
We run a fleet of Docker containers that process the entirety of this pipe to perform a variety of transformations, evaluations and transfer data to more permanent homes for archival, search and more.
Another piece of software that we built looks for irregularities in the logs flowing through Kinesis. It has three levels of alerts when it detects something:
Earlier I mentioned that developers shouldn’t need production access to do their regular work. One of the most common tasks in development is deploying new code.
To solve this, we’ve developed a number of tools in our infrastructure around the idea of consensus. We believe in a 3 phase process where anyone should be able to propose any change, but consensus should be achieved before that proposal can be applied. One of our tools enabling this is called Sauron which comments on every pull request. It requires approvals on pull requests (+1’s from other developers) before code can be deployed into production.
This particular branch, requires a +1 from two developers other than the author. But more sensitive services in our SOA can require even more approvers of special types. During periods of increased risk, such as if you suspect someone’s laptop has been compromised, you can dial the number of +1’s up as high as is needed system wide (without blocking all deploys). This protects against the case where one or more developers get malware on their laptop.
We also use this idea of consensus based changes when making updating our environment (such as the docker-compose files we use to launch our services). Anyone can propose a change, but no single person can push a change to production.
We’ve been running entirely on Docker in production for over a year now. Prior to the influx of new deployment tools (and looking to embrace a consensus approach to deployment), we started building our own tool, CodeFlow. Codeflow gives each developer the power to deploy their code by combining a Dockerfile, Docker-Compose file, and Envars to deploy 12-factor applications. We could probably write a whole blog post on this single tool, but the goal of it is to combine consensus based deployment with the developer productivity and happiness of Heroku.
This post just scratches the surface of what it takes to build secure/paranoid infrastructure in the cloud (as you can see there are a lot of moving parts!). And there are plenty of topics we didn’t have time to cover here, including:
Although we’ve come a long way, we still have much more to do as the most popular place to store digital currency.
If you’re interested in working with the awesome team behind some of these tools, we’d love to speak with you. We’re hiring remote and in house dev-ops and generalist engineers (you can work at our office in San Francisco, or remotely).
If you are a devops engineer or generalist engineer, we’ve set up a coding challenge that you can take in about 45 minutes if you’d like to show us your skills.
You can also apply through our careers site, or send any questions you might have to talent at coinbase dot com. We have flexible work hours and vacations, very competitive compensation (both in cash and equity), and fly everyone out to HQ in San Francisco once a quarter.
Learn about working at Coinbase…
835 
10
835 claps
835 
10
Written by
Co-Founder and CEO at @Coinbase. Increasing economic freedom in the world. Join us: https://www.coinbase.com/careers
Learn about working at Coinbase: https://www.coinbase.com/careers
Written by
Co-Founder and CEO at @Coinbase. Increasing economic freedom in the world. Join us: https://www.coinbase.com/careers
Learn about working at Coinbase: https://www.coinbase.com/careers
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://towardsdatascience.com/simple-way-to-deploy-machine-learning-models-to-cloud-fd58b771fdcf?source=search_post---------68,"Sign in
There are currently no responses for this story.
Be the first to respond.
Tanuj Jain
Apr 14, 2019·11 min read
The Machine learning world currently sees Data Scientists (DS) performing one or both of the following 2 prominent roles:
In this blog post, I’m attempting to display an example approach to the second aspect of a DS’s job i.e., creating some software that can be used by the stakeholders. Specifically, we would create a web-service that can be queried to obtain the predictions from a machine learning model. The post is mostly intended for machine learning practitioners who would like to go beyond only developing models.
Tech-stack: Python, Flask, Docker, AWS ec2
The workflow can be broken down into following basic steps:
DISCLAIMER: The system presented here is light years away from what a commercial production system should look like. The key takeaways from this blogpost should be the development workflow, acquaintance with the tech stack and getting the first taste of building an ML production system.
Let’s start with the first step.
We need some machine learning model that we can wrap in a web-service. For demo purpose, I chose a logistic regression model to do multiclass classification on iris dataset (Yep, super easy! #LazinessFTW). The model was trained on a local system using python 3.6.
Using the familiar scikit-learn, the above mentioned model can be trained quickly. For model development, refer the notebook ‘Model_training.ipynb’ in the github repo for this blog. There are only 2 important aspects of model development that I would like to highlight:
Now that we have the trained model file, we are ready to query the model to get a class label for a test sample. The inference is as simple as calling a predict() function on the trained model with the test data. However, we would like to build the inference as a web-service. For this purpose, we would use Flask.
Flask is a powerful python microwebserver framework that allows us to build REST API based web-services quickly with minimum configuration hassle. Let’s dive into the code:
a. First, let’s define a simple function to load the trained model file.
Here, we define a global variable called ‘model’ and populate it within the load_model() function. The purpose of using a global variable will become clear shortly.
b. Next, we instantiate a Flask object called ‘app’:
c. Now, we define a home endpoint, which when hit, returns a ‘Hello World!’ message.
Notice the use of app.route decorator.
d. Now, we define a ‘predict’ endpoint. The endpoint accepts a ‘POST’ request wherein the test data on which we wish to get a prediction is received by the endpoint. Keeping things simple, the function works only when a single test sample needs to be predicted (won’t work if multiple samples need to be predicted in a single call to the endpoint).
Notice the direct call to the predict function through the ‘model’ variable.
e. Finally, declare the main function:
Here, a call to the load_model() function ensures that the variable ‘model’ is populated with the trained model attributes (and hence the need for a global model variable). So, there is no need to load the model repeatedly with every call to the predict endpoint. This allows the web-service to be quick. The response is returned as a string which is the predicted class label.
The complete flask specific code is as below:
At this point, the web-service is ready to be run locally. Let’s test this.
Execute the command python app.py from the terminal. Go to the browser and hit the url 0.0.0.0:80 to get a message Hello World! displayed. This corresponds to the home endpoint return message.
NOTE: A permission error may be received at this point. In this case, change the port number to 5000 in app.run() command in app.py. (Port 80 is a privileged port, so change it to some port that isn't, eg: 5000)
Next, let’s test if we can get predictions using this web-service using the following curl post request on the terminal:
The curl request posts one test sample [5.9,3.0,5.1,1.8]to our web-server and returns a single class label.
Up to this point, we have a web-service that runs locally. Our ultimate intention is to be able to run this piece of code on a cloud virtual machine.
In the Software Development world, there is a famous justification given by a developer whose code was found to be broken by a tester: ‘But it worked on my machine!’. The problem portrayed here can usually be attributed to a lack of consistent environment that runs the software across different machines. Ideally, our code itself should be independent of the underlying machine/OS that runs it. Containerization allows developers to provide such isolation.
How is it important here?
Our intention is to run our web-service on a cloud VM. The cloud VM itself may run any OS. Containerization of our web-server allows us to avoid the trouble of running into environment related issues. If the containerized code works on one machine, it will surely run on another irrespective of the characteristics of the machine. Docker is the most famous containerized technology out there at this point and we will be using the same here. For a quick tutorial on docker, check this link.
Let’s dive into the Dockerfile that comprises a set of instructions for docker daemon to build the docker image.
We pull the base docker image from python dockerhub repo on which our specific build instructions are executed. The COPY commands are simply taking specific files from the current folder and copying them over to a folder called ‘deploy’ within the docker image we are trying to build. In addition to app.py and model file, we also need a requirements file that lists specific versions of python packages we use to run our code. The WORKDIR command changes the working directory to ‘deploy/’ within the image. We then issue a RUN command to install specific python packages using the requirements file. The EXPOSE command makes the port 80 accessible to the outside world (our flask service runs on port 80; we need this port inside the container to be accessible outside the container).
Issue the build command to end up with a docker image:
(Don’t forget the period at the end of the command).
Use command ‘docker images’ to see a docker image with a docker repository named ‘app-iris’ created. (Another repository named python will also be seen since it is the base image on top of which we build our custom image.)
Now, the image is built and ready to be run. We can do this using the command:
The above commands uses -p flag to map port 80 of the local system to the port 80 of the docker container for the redirection of traffic on local HTTP port 80 to port 80 of the container. (If you are using local port 5000 instead of port 80, change the port mapping part of the command to 5000:80).
Let’s test if this works by hitting the URL: http://0.0.0.0:80 on the browser which should display ‘Hello World!’ which is the home endpoint output message (If port 5000 is used, modify the http port to 5000 in the url). Also, use the curl request mentioned earlier to check if the predicted class label is returned.
We already have a containerized application that works on our local system. Now, what if someone else wishes to consume the service? What happens if we need to build an architectural ecosystem around the service that needs to be available, automated and scalable? It’s easy to see that having a web-service running locally would be a very bad idea. So, we wish to host the web-service somewhere on the internet to fulfil the requirements we listed. For this blog, we choose to host our service on an AWS ec2 instance.
As a prerequisite, one needs to have an AWS account for using the ec2 instance. For new users, there are several AWS resources that are available for free for a period of 1 year (usually up to some limit). In this blog, I would be using a ‘t2.micro’ ec2 instance type which is free tier eligible. For users who have exhausted their AWS free-tier period, this instance costs around 1 cent(USD) per hour at the time of writing this blog; a super negligible amount to pay.
Let’s start with the process.
Log into the AWS management console and search for ec2 in the search bar to navigate to EC2 dashboard.
Look for the below pane, select ‘Key Pairs’ and create one.
This will download a ‘.pem’ file that is the key. Save this file somewhere safely. Now navigate to the location of this file on your system and issue the below command with key file name replaced by yours:
This commands changes permissions on your key pair file to private. The use of key pairs will be explained later.
Next, click ‘Launch Instance’ on the EC2 dashboard:
Choose the Amazon Machine Instance (AMI) from the list of options. An AMI determines the OS that the VM will be running (plus some other stuff we don’t care about at this point). For this blog, I chose ‘Amazon Linux 2 AMI’ which was the default selection.
The next screen allows you to select the instance type. This is where the hardware part of the VM can be selected. As mentioned previously, we will work with ‘t2.micro’ instance.
You can select ‘Review and Launch’ that takes you to ‘Step 7: Review Instance Launch’ screen. Here, you need to click the ‘Edit Security Groups’ link:
You now have to modify the security group to allow HTTP traffic on port 80 of your instance to be accessible by the outside world. This can be done by creating a rule. At the end, you should end up with such a screen:
In the absence of this rule, your web-service will never be reachable. For more on security groups and configuration, refer AWS documentation. Clicking on the ‘Launch’ icon will lead to a pop up seeking a confirmation on having a key-pair. Use the name of the key pair that was generated earlier and launch the VM.
You would be redirected to a Launch screen:
Use the ‘View Instance’ button to navigate to a screen that displays the ec2 instance being launched. When the instance state turns to ‘running’, then it is ready to be used.
We will now ssh into the ec2 machine from our local system terminal using the command with the field public-dns-name replaced with your ec2 instance name (of the form: ec2–x–x–x–x.compute-1.amazonaws.com) and the path of the key pair pem file you saved earlier.
This will get us into the prompt of our instance where we’ll first install docker. This is required for our workflow since we will build the docker image within the ec2 instance (There are better, but slightly complicated alternatives to this step). For the AMI we selected, the following bunch of commands can be used:
For an explanation of the commands, check the documentation.
Log out of the ec2 instance using the ‘exit’ command and log back in again using the ssh command. Check if docker works by issuing the ‘docker info’ command. Log out again or open another terminal window.
Now let’s copy the files we need to build the docker image within the ec2 instance. Issue the command from your local terminal (not from within ec2):
We would need to copy requirements.txt, app.py, trained model file and Dockerfile to build the docker image as was done earlier. Log back into the ec2 instance and issue ‘ls’ command to see if the copied files exist. Next, build and run the docker image using the exact same commands that were used in the local system (Use port 80 at all locations in the code/commands this time).
Hit the home endpoint from your browser using the public dns name to see the familiar ‘Hello World!’ message:
Now send a curl request to your web-service from local terminal with your test sample data after replacing the public-dns-name by yours:
This should get you the same predicted class label as the one you got locally.
And you are done! You can now share this curl request with anyone who wishes to consume your web-service with their test samples.
When you no longer need the web-service, do not forget to stop or terminate the ec2 instance:
This is a super basic workflow intended for ML practitioners itching to go beyond model development. A huge number of things need to be changed to make this system into one that is more suited to a real production system. Some suggestions (far from complete):
A lot more could be added to the above list, but maybe that’s something for another blog post.
Github repo: https://github.com/tanujjain/deploy-ml-model
It would be great to hear your feedback!
I am a data scientist currently employed at Axel Springer AI, Berlin. My core interest lies with building machine learning solutions.
895 
12
895 
895 
12
Your home for data science. A Medium publication sharing concepts, ideas and codes.
"
https://medium.com/@edigleyssonsilva/cloud-functions-for-firebase-sending-e-mail-1f2631d1022e?source=search_post---------69,"Sign in
There are currently no responses for this story.
Be the first to respond.
Edigleysson Silva
Mar 6, 2019·5 min read
Cloud Functions is a Google Cloud feature that allows functions to run in the cloud. In this article I’ll show you the use of Cloud Functions in Firebase, creating an e-mail sending function that will work through web requests.
The first steps are to create the design and configuration of the Firebase CLI. So if you have already done this you can go straight to Step 3.
To create your project you must have a Google account and access the https://console.firebase.google.com link. You will see the Firebase welcome screen. On that same page you can see the “Add Project” button. Click on this button and fill in the information for your project. Okay, your project is set up, let’s move on.
From this step the operations will all be on your local machine. Let’s then configure the environment by installing the Firebase CLI. This tool allows us to do a lot of things, but here we will deal only with Cloud Functions.
First install firebase-tools using the command:
If this command fails, you may need to check the permissions issue.
Our environment is already set up and now it’s time to start developing. Create a folder called cloud-functions-sendmail (or the name that you want). In this folder run the command:
Once this command has been executed. A link will be displayed on your terminal. It is the authentication link, which will allow Firebase to identify the account and select the projects correctly.
So open this link in your browser and give access to your account.
After that, run the command:
This command will create a Cloud Functions project in your folder (in my case, cloud-functions-sendmail). Once this command has been executed you will see a screen like:
As you can see. Here we will select a project. In my case, I will select the byte-girl project. Then you will choose the preferred language, between JavaScript and TypeScript. Someone like:
Here we’ll use the JavaScript language.
Advancing you can tell if you will use ESLint and dependencies will also be installed. In this example, both of these options were set to NO.
After the command has finished executing. Some files will be created in your folder. We will only make use of the functions folder that contains the file index.js which is where we will put our functions.
The purpose here is to create an e-mail sending function. But for this we will need some packages, so let’s install them.
The packages are nodemailer that is to send e-mail, and cors that is to handle the CORS (Cross-Origin Resource Sharing) of requests.
I will not dwell here on any of these packages, for this is not the purpose of this article. See the references for more information.
To install these packages, you need to move to the functions folder and use the command:
Yeee! Everything working, lets write our functions!
Lets see our index.js file. The content is someone like:
Note that there is a commented code snippet. This excerpt shows an example of how you can define a function. Basically you only need to define a function for the exports object in such a way that this function is defined through functions. In our case it would look something like:
functions.http.onRequest defines that this function will be executed through requests. But calm down. It still does not do the body of this function. Let’s do some things before.
First we will delete these comments and leave only the first line. Now we will import the firebase-admin, cors, and nodemailer modules. As shown in the image below.
Note that in addition to importing we also initialize the app with admin.initializeApp ().
Now you need to create a transporter to send the email. With the help of nodmailer we will use a Gmail account to send emails. To create the transporter you only need to enter the credentials and the service, which in this case is Gmail.
Finally, the implementation of the function will use the transporter to send the e-mail. Setting options such as sender, subject, and e-mail content.
See how our index.js file was and how our function was defined.
Our function when executed will return the sent e-mail message, if all goes well and the error message in case there is a bug.
Once you have created the function, just send to the cloud To do this, inside the functions folder execute the command:
After executing this command the upload of your function will be started. In addition, the dependencies will be verified so that their function can execute correctly. See the output of this command:
As you can see, our function can be invoked by a URL which in this case is https://us-central1-byte-girl.cloudfunctions.net/sendMail. To execute our function we just have the call with the dest parameter in the URL For example: https://us-central1-byte-girl.cloudfunctions.net/sendMail?dest=someemail@teste.com.
As you noticed by the code, the email sent is with an image of Pickle Rick (Rick and Morty Character). The sender email is picklerick.firebase@gmail.com. This email was created to perform these tests.
Anyway, we’re done. You now have a mail sending function running in the cloud. Expand your horizons even more, integrate other Firebase tools into Cloud Functions, other modules, anyway. The sky is the limit!
See more about:
See my Github repo with many examples of Firebase
Fullstack developer with expertise in web and mobile development with Flutter, Firebase, Ionic, PHP, C#, JS and Python. Now I’m trying to learn Erlang/Elixir :D
2.2K 
20
2.2K 
2.2K 
20
Fullstack developer with expertise in web and mobile development with Flutter, Firebase, Ionic, PHP, C#, JS and Python. Now I’m trying to learn Erlang/Elixir :D
"
https://codeburst.io/graphql-server-on-cloud-functions-for-firebase-ae97441399c0?source=search_post---------70,"GraphQL is a data query language created by Facebook. Queries to the server specify the data they want to fetch while the GraphQL server exposes what is available and knows how to get it. The fundamental difference from REST is that we no longer expose predefined data. Therefore solving the problem of adding endpoints whenever we add new clients and versioning our REST endpoints.
Lee Byron’s blog post describes its use within Facebook and the advantages it offers. Another great resource is this Apollo post.
Firebase, although known predominantly for its real-time database, is a collection of services packaged, billed and deployed together. In my opinion, it offers the slickest developer experience of any cloud services available. My favorite features are:
I could go on.
So the developer experience (DX)is why, not because of anything to do with GraphQL. GraphQL can be hosted anywhere that responds to HTTP requests, so why not on Cloud Functions? Then we can utilize the DX of Firebase AND GraphQL!
While they do exist, I would argue that the reasons for not running GraphQL on serverless/FaaS are weak. The main argument for not using FaaS is the Cold Starts problem. The Serverless Framework community has covered this topic well (their blog is great! I recommend following them):
What is cold start in Serverless (FaaS)?
Cold start happens when you execute an inactive (cold) function for the first time. It occurs while your cloud provider provisions your selected runtime container and then runs your function. This process, referred to as cold start, will increase your execution time considerably.
While you’re actually running your function it will stay active (hot), meaning your container stays alive — ready and waiting for execution. But eventually after a period of inactivity, your cloud provider will drop the container and your function will become cold again.
So each request to our GraphQL endpoint would either, use a hot container, or a cold one. Since GraphQL Servers are a single API to your entire data graph, every request to fetch or mutate your data will go through here. Thus, you should have enough traffic to have some hot containers around making the Cold Start time irrelevant. If this does become an issue, you can simply run another Cloud Function to ping the desired Cloud Function and keep it warm.
There are multiple packages available to leverage GraphQL in a Node.js environment. Facebook (well, the GraphQL team at Facebook) has express-graphql which is a middleware implementation of GraphQL for the ExpressJS server package. As of writing, this is the most popular middleware with ~97,000 downloads in April, 2017 according to npmjs. The other prominent implementation is the Apollo developed graphql-server-express, also a middleware for Express. It has far fewer downloads at ~35,000 downloads in the same period.
The number of downloads is a bad reason to choose between two packages, so let’s dig deeper. Facebook’s implementation seems to be a standard middleware for Express. While Apollo distinctly addresses the difference between the two packages:
The following features distinguish GraphQL Server from express-graphql, Facebook’s reference HTTP server implementation:
- GraphQL Server has a simpler interface and allows fewer ways of sending queries, which makes it a bit easier to reason about what’s going on.
- GraphQL Server serves GraphiQL on a separate route, giving you more flexibility to decide when and how to serve it.
- GraphQL Server supports query batching which can help reduce load on your server.
- GraphQL Server has built-in support for persisted queries, which can make your app faster and your server more secure.
Click here for a deeper dive into the Apollo docs.
At this point you may be asking “Why’re we going to run an Express Server on Cloud Functions. Don’t they already expose an HTTPS endpoint?”.
Well, yes, they do already expose an endpoint. However we cannot customize it with the GraphQL middleware like we can Express. Fortunately, Cloud Functions can be used with the Express.js web server framework. For more information about how this works, please read my previous post about Express on Cloud Functions.
In this tutorial we’ll be using Apollo’s graphql-server-express package as I lean towards the community built around it. While express-graphql is open-source and well maintained, I find that that GraphQL community around the Apollo tools will enable better integration, support and ultimately more customization.
In the ./firebaseFunctions directory run:
Our ./firebaseFunctions/package.json should now look like:
Now we have all the requried packages we will implement the Server in ./firebaseFunctions/graphql/server.js:
And then use that server in ./firebaseFunctions/index.js:
Before we try and test this server, we will need to implement the GraphQL schema and the resolver methods to fetch the data. Instead of reinventing the wheel, we will use the same example data as the Apollo crew which is the basis for the dev.apollodata.com examples.
Create ./firebaseFunctions/graphql/data/schema.js:
And ./firebaseFunctions/graphql/data/resolvers.js:
We can deploy our server by running the now familiar yarn deploy.
Testing the URL <function-name>/api will yield a “Cannot GET null” 404 error, while adding the trailing “/” yields a “Cannot GET /” 404 error. Why does the trailing slash not solve our problem? Well, the Cloud Function is hosted on /api/ but our GraphQL server is hosted at /api/graphql as we defined in our Express server in ./firebaseFunctions/index.js. We must use our defined routes like so:
<function-name>/api/graphql: accepts a GraphQL POST query string.
<function-name>/api/graphiql: runs the GraphiQL browser IDE.
<function-name>/api/schema: returns our schema in pretty-JSON.
And yes, no trailing “/” this time, graphql-server-express knows what’s up!
To enable independent development of the GraphQL server and schema from the client apps, Facebook developed the GraphiQL in-browser IDE. It let’s you explore the schema by browsing the schema and constructing and executing queries!
Notice the “Documentation Explorer” on the right side of the IDE. Use this to browser the queries, mutations and their parameters and structures.
Each query that is run through GraphiQL simply constructs the appropriate request object and sends a request through to our /api/graphql endpoint. Thus, each time we run a query here it executes an instance (it may be new or use an existing one) of our Cloud Function. Pretty neat! Read more about it here.
A repository of the completed project is available here:
github.com
There you have it, GraphQL on Cloud Functions. This is only the beginning of my exploration into GraphQL on Firebase, so stay tuned by Following! Next up, GraphQL subscriptions while using GraphQL on Cloud Functions!
More by me:
If you found this useful, please recommend and share with your friends & colleagues.
Bursts of code to power through your day.
1.6K 
16
Some rights reserved

1.6K claps
1.6K 
16
Written by
GCP, Firebase, Sveltejs & Deno fan! @jthegedus on all platforms.
Bursts of code to power through your day. Web Development articles, tutorials, and news.
Written by
GCP, Firebase, Sveltejs & Deno fan! @jthegedus on all platforms.
Bursts of code to power through your day. Web Development articles, tutorials, and news.
"
https://medium.com/@sebastianengel/easy-push-notifications-with-flutter-and-firebase-cloud-messaging-d96084f5954f?source=search_post---------71,"Sign in
There are currently no responses for this story.
Be the first to respond.
Sebastian Engel
Aug 5, 2019·5 min read
Many apps are forgotten by its users very shortly after they got installed. Push notifications are a great possibility to bring your app back into your users mind and to increase the retention rate.
In this article I’ll show you how to integrate push notifications easily into your Flutter app and how to send simple notifications from the Firebase…
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@nnilesh7756/what-are-cloud-computing-services-iaas-caas-paas-faas-saas-ac0f6022d36e?source=search_post---------72,"Sign in
There are currently no responses for this story.
Be the first to respond.
Nilesh Suryavanshi
Nov 8, 2017·2 min read
Today, everyone are moving towards Cloud World (AWS/GCP/Azure/PCF/VMC). It might be a public cloud, a private cloud or a hybrid cloud.
But are you aware of what are Services Cloud Computing provides to us ????
Majorly there are three categories of Cloud Computing Services:
a) Infrastructure as a Service (IaaS) : It provides only a base infrastructure (Virtual machine, Software Define Network, Storage attached). End user have to configure and manage platform and environment, deploy applications on it.
AWS (EC2), GCP (CE), Microsoft Azure (VM) are examples of Iaas.
b) Software as a Service (SaaS) : It is sometimes called to as “on-demand software”. Typically accessed by users using a thin client via a web browser. In SaaS everything can be managed by vendors: applications, runtime, data, middleware, OSes, virtualization, servers, storage and networking, End users have to use it.
GMAIL is Best example of SaaS. Google team managing everything just we have to use the application through any of client or in browsers. Other examples SAP, Salesforce .
c) Platform as a Service (PaaS): It provides a platform allowing end user to develop, run, and manage applications without the complexity of building and maintaining the infrastructure.
Google App Engine, CloudFoundry, Heroku, AWS (Beanstalk) are some examples of PaaS.
Below fig 1.0 while give you more idea on it.
d) Container as a Service (CaaS): Is a form of container-based virtualization in which container engines, orchestration and the underlying compute resources are delivered to users as a service from a cloud provider.
Google Container Engine(GKE), AWS (ECS), Azure (ACS) and Pivotal (PKS) are some examples of CaaS.
e) Function as a Service (FaaS): It provides a platform allowing customers to develop, run, and manage application functionalities without the complexity of building and maintaining the infrastructure.
AWS (Lamda), Google Cloud Function are some examples of Faas
Hope this gives you clear idea on what are Cloud Computing Services provided in market!!!!!!
@nilesh7756 | VMware NSX-T | Kubernetes | Openshift | PCF/PKS | Docker | AWS | GCE/GKE | DevOps
935 
7
935 claps
935 
7
@nilesh7756 | VMware NSX-T | Kubernetes | Openshift | PCF/PKS | Docker | AWS | GCE/GKE | DevOps
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@howkhang/ultimate-guide-to-setting-up-a-google-cloud-machine-for-fast-ai-version-2-f374208be43?source=search_post---------73,"Sign in
There are currently no responses for this story.
Be the first to respond.
How Khang
Jan 3, 2018·9 min read
course.fast.ai
– – – – – – – – – – – – – – – – – – – – – –
The guide below is archived and no longer works.
Note 1: Please note that some people have reported that Google Cloud does not accept debit cards.
Note 2: If you are unable to find an Upgrade button at this point, just continue with the next steps. The Upgrade button should appear at Step 5 below when you request for an increase in quota.
Compute Engine → Settings
IAM & Admin → Quotas
Compute Engine → Marketplace
It is up to you to decide the number of cores and the amount of memory to allocate for your VM instance but if you have the budget for it, it is a good idea to go with 4 cores and > 12 GB memory, as the Tesla K80 GPU comes with 12 GB memory.
There appear to be warnings due to a deprecated image being used but so far I have not encountered any downstream problems so it may be safe to just ignore them for now.
There are a few methods of connecting to your Debian VM according to Google Cloud Platform’s official documentation (which can be found here). Two of those methods will be mentioned briefly.
While this method is the most convenient, it comes with a list of known issues that are officially documented here, which is why I recommend using the Google Cloud SDK command line method below.
Step a. Download and install Cloud Tools for your OS to use the Cloud SDK. (Windows users: (1) verify that the option to install bundled Python is checked and (2) ensure that you have PuTTY installed).
Step b. Run “gcloud init” to link your account, and select project and region.
Step c. Run “gcloud compute ssh <your instance name> -- -L 8080:localhost:8080”
Note: You should always stick to the same method of connecting to your VM because your login usernames for SSH from browser and for Cloud SDK may be different. This may result in confusion as you will end up in completely different home directories after connecting to your VM. If you intend to alternate between the two methods, first configure your SSH from browser and Cloud SDK login usernames to be identical.
*To switch users when connecting via Cloud SDK, use the command: “gcloud compute ssh <your user name>@<your instance name>”
Launch your browser and connect using “localhost:8080”.
Compute Engine → VM instances
— — — — — — — — — — — — — — — — — — — — — — — — — — — — — —
VPC Network → Firewall rules
Create a firewall rule if your project does not already have one for tcp:8888.
You may choose some other unused port number but the typical number is 8888. It is permissible to specify a range of ports such as in the above case.
Compute Engine → VM instances → Create
It is up to you to decide the number of cores and the amount of memory to allocate for your VM instance but if you have the budget for it, it is a good idea to go with 4 cores and 26 GB memory (ideally > 12 GB, as the Tesla K80 GPU comes with 12 GB memory).
Again, it is up to you to decide the type of persistent disk to use as the boot disk but SSD is recommended for its higher speed if you have the budget for it. (For the fast.ai version 2 course, you minimally need 20 GB.)
VPC network → External IP addresses
**Following this step results in ongoing charges for the static IP address. Skip this step if you wish to save on the cost of a static IP address (US$0.01/hour at time of writing).
There are a few methods of connecting to your Ubuntu VM according to Google Cloud Platform’s official documentation (which can be found here). Two of those methods will be mentioned briefly.
While this method is the most convenient, it comes with a list of known issues that are officially documented here, which is why I recommend using the Google Cloud SDK command line method below.
Step a. Download and install Cloud Tools for your OS to use the Cloud SDK. (Windows users: (1) verify that the option to install bundled Python is checked and (2) ensure that you have PuTTY installed).
Step b. Run “gcloud init” to link your account, and select project and region.
Step c. Run “gcloud compute ssh <your instance name>”
Note: You should always stick to the same method of connecting to your VM because your login usernames for SSH from browser and for Cloud SDK may be different. This may result in confusion as you will end up in completely different home directories after connecting to your VM. If you intend to alternate between the two methods, first configure your SSH from browser and Cloud SDK login usernames to be identical.
*To switch users when connecting via Cloud SDK, use the command: “gcloud compute ssh <your user name>@<your instance name>”
curl https://raw.githubusercontent.com/howkhang/fastai-v2-setup/master/setup.sh | bash
You will be automatically disconnected after the script has finished running in order for the VM to do a reboot.
Use this method only if you went through step 9 to obtain a static IP address.
Launch your browser and connect using your VM’s static IP address followed by the port number (in this case, “:8888”). For the token, copy the long string of characters shown after the “token=” .
Use this method if you skipped obtaining a static IP address in step 9 or if you encounter difficulties connecting to your Jupyter notebook using method 1.
Step a. Open another terminal (or Google Cloud SDK Shell) on your local machine and run the following command for port forwarding:
gcloud compute ssh <your instance name> --ssh-flag=“-L” --ssh-flag=“8888:localhost:8888”
You should see a new VM terminal pop up. You should now have a total of two local terminals (both showing your gcloud compute ssh commands) and two VM terminals (one showing the jupyter notebook server running and the other just showing the command line prompt).
Step b. Connect to Jupyter notebook using localhost:8888
Launch your browser and connect using “localhost” followed by the port number (in this case, “:8888”). For the token, copy the long string of characters shown after the “token=”.
And we are good to go! Let me know if you face any issues or have any feedback on this guide.
Lawyer + Coder
See all (44)
1.5K 
41
1.5K claps
1.5K 
41
Lawyer + Coder
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/firebase-developers/why-is-my-cloud-firestore-query-slow-e081fb8e55dd?source=search_post---------74,"There are currently no responses for this story.
Be the first to respond.
We like saying lots of impressive things about Cloud Firestore’s performance — “performance scales with the size of the result set, not the underlying data set”, and that “it’s virtually impossible to create a slow query.” And, for the most part, this is true. You can query a data set with billions upon billions of records in it, and get back results faster than your user can move their thumb away from the screen.
But with that said, we occasionally hear from developers that Cloud Firestore feels sluggish in certain situations, and it takes longer than expected to get results back from a query. So why is that? Let’s take a look at some of the most common reasons that Cloud Firestore might seem slow, and what you can do to fix them.
Probably the most common explanation for a seemingly slow query is that your query is, in fact, running very fast. But after the query is complete, we still need to transfer all of that data to your device, and that’s the part that’s running slowly.
So, yes, you can go ahead and run a query of all sales people in your organization, and that query will run very fast. But if that result set consists of 2000 employee documents and each document includes 75k of data, you have to wait for your device to download 150MB of data before you can see any results.
The best way to fix this issue is to make sure you’re not transferring down more data than you need. One simple option is to add limits to your queries. If you suspect that your user only needs the first handful of results from your employee list, add a limit(25) to the end of your query to download just the first batch of data, and then only download further records if your user requests them. And, hey, it just so happens I have an entire video all about this!
If you really think it’s necessary to query and retrieve all 2000 sales employees at once, another option is to break those records up into the documents that contain only the data you’ll need in the initial query, and then put any extra details into a separate collection or subcollection. Those other documents won’t get transferred on that first fetch, but you can request them later as your user needs them.
Having smaller documents is also nice in that, if you have a realtime listener set up on a query and a document is updated, the changed document gets sent over to your device. So by keeping your documents smaller, you’ll also have less data transferred every time a change happens in your listeners.
So Cloud Firestore’s offline cache is pretty great. With persistence enabled, your application “just works”, even if your user goes into a tunnel, or takes a 9-hour plane flight. Documents read while online will be available offline, and writes are queued up locally until the app is back online. Additionally, your client SDK can make use of this offline cache to avoid downloading too much data, and it can make actions like document writes feel faster. However Cloud Firestore was not designed as an “offline first” database, and as such, it’s currently not optimized for handling large amounts of data locally.
So while Cloud Firestore in the cloud indexes every field in every document in every collection, it doesn’t (currently) build any of those indexes for your offline cache. This means that when you query documents in your offline cache, Cloud Firestore needs to unpack every document stored locally for the collection being queried and compare it against your query.
Or to put it another way, queries on the backend scale with the size of your result set, but locally, they kinda scale with the size of the data in the collection you’re querying.
Now, how slow local querying ends up being in practice depends on your situation. I mean, we’re still talking about local, non-network operations here, so this can (and often is) faster than making a network call. But if you have a lot of data in one single collection to sort through, or you’re just running on a slow device, local operations on a large offline cache can be noticeably slower.
First, follow the best practices mentioned in the previous section: add limits to your queries so you’re only retrieving the data that you think your users will need, and consider moving unneeded details into subcollections. Also, if you followed the “several subcollections vs a separate top level collection” discussion at the end of my earlier post, this would be a good argument for the “several subcollections” structure, because the cache only needs to search through the data in these smaller collections.
Second, don’t stuff more data in the cache than you need. I’ve seen some cases where developers will do this intentionally by querying a massive number of documents when their application first starts up, then forcing all future database requests to go through the local cache, usually in a scheme to reduce database costs, or make future calls faster. But in practice, this tends to do more harm than good.
Third, consider reducing the size of your offline cache. The size of your cache is set to 100MB on mobile devices by default, but in some situations, this might be too much data for your device to handle, particularly if you end up having most of your data in one massive collection. You can change this size by modifying the cacheSizeBytes value in your Firebase settings, and that’s something you might want to do for certain clients.
Fourth, try disabling persistence entirely and see what happens. I generally don’t recommend this approach — as I mentioned earlier, the offline cache is pretty great. But if a query seems slow and you don’t know why, re-running your app with persistence turned off can give you a good idea if your cache is contributing to the problem.
So zig-zag merge joins, in addition to being my favorite algorithm name ever, are very convenient in that they allow you to coalesce results from different indexes together without having to rely on a composite index. They essentially do this by jumping back and forth between two (or more) indexes sorted by document ID and finding matches between them.
But one quirk about zig-zag merge joins is that you can run into performance issues where both sets of results are quite large, but the overlap between them is small. For example, imagine a query in a restaurant review app where you were looking for expensive restaurants that also offered counter service.
While both of these groups might be fairly large, there’s probably very little overlap between them. Our merge join would have to do a lot of searching to give you the results you want.
So if you notice that most of your queries seem fast, but specific queries are slow when you’re performing them against multiple fields at once, you might be running into this situation.
If you find that a query across multiple fields seems slow, you can make it performant by manually creating a composite index against the fields in these queries. The backend will then use this composite index in all future queries instead of relying on a zig zag merge join, meaning that once again this query will scale to the size of the result set.
While Cloud Firestore has more advanced querying capabilities, better reliability, and scales better than the Firebase Realtime Database, the Realtime Database generally has lower latency if you’re in North America. It’s usually not by much, and in something like a chat app, I doubt you would notice the difference. But if you have an app that’s reliant upon very fast database responses (something like a real-time drawing app, or maybe a multiplayer game), you might notice that the Realtime Database feels… uhh… realtime-ier.
If your project is such that you need the lower latency that the Realtime Database provides (and you’re anticipating that most of your customers are in North America), and you don’t need some of the features that Cloud Firestore provides, feel free to use the Realtime Database for those parts of your project! Before you do, I would recommend reviewing this earlier blog post, or the official documentation, to make sure you understand the full set of tradeoffs between the two.
Remember that even in the most perfect situation, if your Cloud Firestore instance is hosted in Oklahoma, and your customer is in New Delhi, you’re going to have at least 80 milliseconds of latency because of that whole “speed of light” thing. And, realistically, you’re probably looking at something more along the lines of a 242 millisecond round trip time for any network call. So, no matter how fast Cloud Firestore is to respond, you still need time for that response to travel between Cloud Firestore and your device.
First, I’d recommend using realtime listeners instead of one-time fetches. This is because using realtime listeners within the client SDKs gives you a lot of really nice latency compensation features. For instance, Cloud Firestore will present your listener with cached data while it’s waiting for the network call to return, giving you the ability to show results to your user faster. And database writes are applied to your local cache immediately, which means that you will see these changes reflected nearly instantly while your device is waiting for the server to confirm them.
Second, try to host your data where the majority of your customers are going to be. You have the option of selecting your Cloud Firestore location when you first initialize your database instance, so take a moment to consider what location makes the most sense for your app, not just from a cost perspective, but a performance perspective as well.
Third, consider implementing a reliable and cheap global communication network based on quantum entanglement, allowing you to circumvent the speed of light. Once you’ve done that, you probably can retire off of the licensing fees and forget about whatever app you were building in the first place.
So the next time you run into a Cloud Firestore query that seems slow, take a look through this list and see if you might be hitting one of these scenarios.
While you’re at it, don’t forget that the best way to see how well your app is performing is to measure its performance out in the wild in real-life conditions, and Firebase Performance Monitoring is a great way of doing that. Consider adding Performance Monitoring to your app, and setting up a custom trace or two so you can see how your queries perform in the wild.
Tutorials, deep-dives, and random musings from Firebase…
1.2K 
7
1.2K claps
1.2K 
7
Tutorials, deep-dives, and random musings from Firebase developers all around the world. Views expressed are those of the authors and don’t necessarily reflect those of Firebase or its parent companies.
Written by
Former Game Designer. Current Developer Advocate at Google. Dad. Often sleep deprived.
Tutorials, deep-dives, and random musings from Firebase developers all around the world. Views expressed are those of the authors and don’t necessarily reflect those of Firebase or its parent companies.
"
https://medium.com/hashmapinc/snowflakes-cloud-data-warehouse-what-i-learned-and-why-i-m-rethinking-the-data-warehouse-75a5daad271c?source=search_post---------75,"There are currently no responses for this story.
Be the first to respond.
Top highlight
by Ed Fron, Enterprise Architect
At Hashmap, clients often ask us to help them get the absolute best performance out of their data warehousing solutions.
After spending significant time recently in sizing, balancing, and tuning an on-premise data warehouse environment, performance just wasn’t where it needed to be for Tableau workbooks meant to analyze consumer device hardware data.
A maximum of 13 concurrent users could be handled at peak times. Simple queries could take as long as 5 minutes to execute with complex queries escalating to 30 minutes. Data loads were occurring only every 24 hours, but hourly loads were required. To top things off, a geographically dispersed user base across the US, Europe, and Asia wasn’t helping matters at all.
Was it time to look at replacing this data warehouse environment?
Waiting days or even weeks to analyze critical data for decision making is no longer acceptable. Most business teams want real-time insights that match the fast pace business and markets, and data scientists can become frustrated by the limitations placed on queries and an ability to load, transform and integrate both structured and semi-structured data.
Hang on a minute — it’s no small task to research and conduct a comprehensive assessment of all the data warehouse options out there. Where do you start? How do you select a solution that can outpace the existing platform or other traditional data warehousing solutions based on performance, scalability, elasticity, flexibility and affordability?
Managing and growing a data warehouse should be a core competency of any healthy IT organization. Standing up to a cloud data warehouse should not be. Don’t hesitate to bring in an outside team, like Hashmap, that has this as a core competency. They can help not only stand up the warehouse (which is deceptively easy), but they can help you configure it in ways to anticipate the future while keeping costs low.
Hashmap’s Data & Cloud Migration and Modernization Workshop is an interactive, two-hour experience for you and your team to help understand how to accelerate desired outcomes, reduce risk, and enable modern data readiness. We’ll talk through options and make sure that everyone has a good understanding of what should be prioritized, typical project phases, and how to mitigate risk. Sign up today for our complimentary workshop.
www.hashmapinc.com
A key factor driving the evolution of modern data warehousing is the cloud. The cloud creates access to:
Having worked numerous times over the years with everything from Hadoop to Teradata as well as having been deeply involved on migration projects moving workloads from on premise environments to the cloud, I must say I was super-excited for the opportunity to explore the options to architect and deploy this particular data warehouse.
Rather than letting the features of an existing DW solution set limits on evaluating a new solution, I focused on an approach to explore the possibilities that are available across the spectrum of cloud data warehousing today.
For this particular implementation, the existing data ingestion process was stable and mature. Daily, an ETL script loads the raw JSON file from the file system and inserts the file’s contents into a SQL table which is stored in ORC format with snappy compression.
In order to avoid reimplementing the ETL process, the first constraint was the cloud data warehouse needed to support the ORC file format. The second major constraint was to maintain backward compatibility with the existing Tableau workbooks.
Given the constraints, there are two cloud data warehouses that support ORC file format — Snowflake and Amazon Redshift Spectrum. Both Snowflake and Redshift Spectrum allow queries on ORC files as external files located in Amazon S3. However, Snowflake edged out Redshift Spectrum for its ability to also load and transform ORC data files directly into Snowflake.
Meeting the Tableau constraint was a wash as Tableau can connect to a variety of data sources and data warehouses including Snowflake and Redshift Spectrum.
One bonus for shorter term POCs — Snowflake (which runs in AWS today, but we expect it will run in other cloud providers soon) offers a $400 credit for 30 days that can be used for compute and storage.
As an update to this article, Snowflake is now available on Microsoft Azure as of June 2018 and GCP as of June 2019.
Snowflake is a cloud data warehouse built on top of the Amazon Web Services (AWS) cloud infrastructure and is a true SaaS offering. There is no hardware (virtual or physical) for you to select, install, configure, or manage. There is no software for you to install, configure, or manage. All ongoing maintenance, management, and tuning is handled by Snowflake.
Architecturally there are 3 main components that make up the Snowflake data warehouse.
The 3 main components are:
By design, each one of these 3 layers can be independently scaled and are redundant. If you interested in detailed information about the underlying architecture visit Snowflake’s documentation here.
Snowflake makes connecting to databases very easy and provides a few different methods to do so. One method is to use any of the supported ODBC drivers for Snowflake. Additionally, SnowSQL CLI (install instructions are found here) can be leveraged or use the web based worksheet within your Snowflake account.
For the project that I worked on, I started using the web based worksheet but quickly installed the SnowSQL CLI on my machine to use a full featured IDE with version control — a UI that I am more accustomed to for development.
Snowflake defines a virtual warehouse as a cluster of compute resources. This warehouse provides all the required resources, such as CPU, memory, and temporary storage, to perform operations in a Snowflake session.
The first step after logging into a Snowflake account is to create a virtual warehouse. I created two warehouses, the first for ETL and the second for queries.
To create a warehouse, execute the following CREATE WAREHOUSE commands into SnowSQL CLI or the web based worksheet:
In the code above I set different WAREHOUSE_SIZE for each warehouse but set both to AUTO_SUSPEND and AUTO_RESUME to help save on some of my $400 free credits. For more detailed information about virtual warehouses, visit Snowflake’s documentation here.
All data in Snowflake is maintained in databases. Each database consists of one or more schemas, which are logical groupings of database objects, such as tables and views. Snowflake does not place any hard limits on the number of databases, schemas (within a database), or objects (within a schema) that you can create.
As we go through creating a database, keep in mind that our working project constraints were as follows:
Snowflake provides two options that will impact data model design decisions needed to help meet the first constraint of loading ORC data into Snowflake.
The first option is that Snowflake reads ORC data into a single VARIANT column table. This allows querying the data in VARIANT column just as you would JSON data, using similar commands and functions.
The second option allows extraction of selected columns from a staged ORC file into separate table columns using a CREATE TABLE AS SELECT statement. I decided to explore both options and create two databases for each approach.
Execute the following command to create two databases:
The MULTI_COLUMN_DB database will be used for to create the tables with multiple columns.
The SINGLE_VARIANT_DB database will be used to store the tables with a single variant column.
Execute the following command to create a new schema in the specified database:
The multiple column table specifies the same columns and types as the existing DW schema.
Execute the following command to create a table with a multiple column:
To best utilize Snowflake tables, particularly large tables, it is helpful to have an understanding of the physical structure behind the logical structure. Refer to the Snowflake documentation Understanding Snowflake Table Structures for complete details on micro-partitions and data clustering.
Execute the following command to create a table with a single variant column:
CREATE TABLE “SINGLE_VARIANT_DB”.”POC”.DEVICEINFO_VAR (V VARIANT);
A view is required to meet the constraint of providing backward compatibility with the existing queries to access the single variant tables.
Execute CREATE VIEW to create a view:
Now that the database and table have been created, it’s time to load the 6 months of ORC data. Snowflake assumes that the ORC files have already been staged in an S3 bucket. I used the AWS upload interface/utilities to stage the 6 months of ORC data which ended up being 1.6 million ORC files and 233GB in size.
So the ORC data has been copied into an S3 bucket — the next step is to set up our external stage in Snowflake.
When setting up a stage you have a few choices including loading the data locally, using a Snowflake staging storage, or providing info from you own S3 bucket. I have chosen that latter as this will provide long term retention of data for future needs.
Execute CREATE STAGE to create a named external stage. An external stage references data files stored in a S3 bucket. In this case, we are creating a stage that references the ORC data files. The following command creates an external stage named ORC_SNOWFLAKE:
You will need to provide the S3 URL as well as AWS API keys. Once you’ve done this, your stage will show up in Snowflake.
A named file format object provides a convenient means to store all of the format information required for loading data from files into tables.
Execute CREATE FILE FORMAT to create a file format
Execute COPY INTO table to load your staged data into the target table. Below is a code sample for both a Multiple Column Table and a Single Variant Column Table.
The previous examples include the ON_ERROR = ‘continue’ parameter value. If the command encounters a data error on any of the records, it continues to run the command. If an ON_ERROR value is not specified, the default is ON_ERROR = ‘abort_statement’, which aborts the COPY command on the first error encountered on any of the records in a file.
NOTE: At the time this post was published, there is a known issue with the execution of the ON_ERROR = ‘continue’ statement when accessing external files on S3 and the default value ‘abort_statement’ is run. A bug has been submitted to Snowflake and a fix is pending.
Execute a SELECT query to verify that the data was loaded successfully.
Once the the data was all loaded, I started Tableau and created a connection to Snowflake, and then set data source to point to the correct Snowflake database, schemas, and tables.
Next, I was ready run some sanity tests. I identified 8 existing queries and then made 5 runs of each query/Tableau dashboard and captured the times using the Snowflake history web page.
The Snowflake environment is now ready for Tableau side-by-side testing. For side-by-side testing, end users will compare the performance of the Tableau datasource configured to connect to the existing data warehouse against another Tableau datasource configured to connect to the new Snowflake Cloud Data Warehouse.
To establish a baseline for the query performance benchmarks, I’m using a medium sized warehouse for the first round of testing. It will be interesting to see how the initial query benchmarks compare to the current DW using that size. I expect the results to be similar.
But this is where the fun begins!
I can then quickly experiment with the different types of queries and different Snowflake warehouse sizes to determine the combinations that best meet the end user queries and workload. Snowflake claims linear performance improvements as you increase the warehouse size, particularly for larger, more complex queries.
So when increasing our warehouse from medium to large, I would expect Run 1 times cut in half for the larger, more complex queries. It also helps to reduce the queuing that occurs if a warehouse does not have enough servers to process all the queries that are submitted.
In terms of concurrency testing for a warehouse, resizing alone will not address concurrency issues. For that, Snowflake’s multi-cluster warehouses are designed specifically for handling queuing and performance issues related to large numbers of concurrent users and/or queries.
Again, for the initial round of testing, multi-cluster warehouses will not be used to establish a baseline. To address any concurrency issues, I will configure the warehouse for multi-cluster and specify it to run in auto-scale mode, effectively enabling Snowflake to automatically start and stop clusters as needed.
Working on this Snowflake project was a great experience. The Snowflake architecture was designed foundationally to take advantage of the cloud, but then adds some unique benefits for a very compelling solution and addresses the .
First, Snowflake leverages standard SQL query language. This will be an advantage for organizations who are already use SQL (pretty much everyone) in that teams will not need to be “re-skilled”.
Importantly, Snowflake supports the most popular data formats like JSON, Avro, Parquet, ORC and XML. The ability to easily store structured, unstructured, and semi-structured data will help address the common problem of handling all the incongruent data types that exist in a single data warehouse. This is a big step towards providing more value on the data as a whole using advanced analytics.
Snowflake has a unique architecture for taking advantage of native cloud benefits. While most traditional warehouses have a single layer for their storage and compute, Snowflake takes a more subtle approach by separating data storage, data processing, and data consumption. Storage and compute resources are completely different and need to be handled separately. It’s really nice to ensure very cheap storage and more compute per dollar, while not drive up costs by mixing the two essential components of warehousing.
Snowflake provides two distinct user experiences for interacting with data for both a data engineer and a data analyst. The data engineer/s load the data and work from the application side, and is effectively the admin and owner of the system.
Data analysts consume the data and derive business insights from the data after it is loaded in the system by a data engineer. Here again, Snowflake separates the two roles by enabling a data analyst to clone a data warehouse and edit it to any extent without affecting the original data warehouse.
Lastly, Snowflake provides instant data warehouse scaling to handle concurrency bottlenecks during high demand periods. Snowflake scales without the need for redistributing data which can be a major disruption to end users.
Data warehousing is rapidly moving to the cloud and solutions such as Snowflake provide some distinct advantages over legacy technologies as outlined above.
In my opinion, traditional data warehousing methods and technologies are faced with a big challenge to provide the kind of service, simplicity, and value that rapidly changing businesses need and, quite frankly, are demanding, not to mention ensuring that initial and ongoing costs are manageable and reasonable.
Based on my testing, Snowflake certainly addressed the 2 key constraints for this project, namely, support for the ORC file format and maintaining backward compatibility with the existing Tableau workbooks.
Beyond addressing those constraints though, Snowflake delivered significant performance benefits, a simple and intuitive way for both admins and users to interact with the system, and lastly, a way to scale to levels of concurrency that weren’t previously possible — all at a workable price point.
Snowflake was super fun and easy to work with and is an attractive proposition as a Cloud Data Warehousing solution. I look forward to sharing more thoughts on working with Snowflake in future posts.
If you’d like additional assistance in this area, Hashmap offers a range of enablement workshops and consulting service packages as part of our consulting service offerings, and would be glad to work through your specifics in this area.
How does Snowflake compare to other data warehouses? Our technical experts have implemented over 250 cloud/data projects in the last 3 years and conducted unbiased, detailed analyses across 34 business and technical dimensions, ranking each cloud data warehouse.
www.hashmapinc.com
To listen in on a casual conversation about all things data engineering and the cloud, check out Hashmap’s podcast Hashmap on Tap as well on Spotify, Apple, Google, and other popular streaming apps.
www.hashmapinc.com
www.hashmapinc.com
www.hashmapinc.com
medium.com
medium.com
medium.com
medium.com
Ed Fron is an Enterprise Architect at Hashmap working across industries with a group of innovative technologists and domain experts accelerating high-value business outcomes for our customers. Connect with Ed on LinkedIn and be sure to catch Hashmap’s Weekly IoT on Tap Podcast for a casual conversation about IoT from a developer’s perspective.
Innovative technologists and domain experts helping…
889 
15
889 claps
889 
15
Written by
Innovative technologists and domain experts accelerating the value of Data, Cloud, IIoT/IoT, and AI/ML for the community and our customers http://hashmapinc.com
Innovative technologists and domain experts helping accelerate the value of Data, Cloud, IIoT/IoT, and AI/ML for the community and our clients by creating smart, flexible and high-value solutions and service offerings that work across industries. http://hashmapinc.com
Written by
Innovative technologists and domain experts accelerating the value of Data, Cloud, IIoT/IoT, and AI/ML for the community and our customers http://hashmapinc.com
Innovative technologists and domain experts helping accelerate the value of Data, Cloud, IIoT/IoT, and AI/ML for the community and our clients by creating smart, flexible and high-value solutions and service offerings that work across industries. http://hashmapinc.com
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://towardsdatascience.com/how-to-extract-the-text-from-pdfs-using-python-and-the-google-cloud-vision-api-7a0a798adc13?source=search_post---------76,"Sign in
There are currently no responses for this story.
Be the first to respond.
Silvia Zeamer
Feb 14, 2021·10 min read
This winter, I discovered that Wellesley College, where I am currently a senior studying Media Arts and Sciences, has an archive of over a hundred year’s worth of course catalogues, admissions guidelines, and yearly bulletins. I was immediately electrified by the potential for fascinating data which could be drawn from these documents, but the first step would have to be converting them to text, as there are not many analytical methods which can be run on scans of old, browned PDFs.
Thus began my search for a way to quickly and effectively run OCR on a large volume of PDF files while retaining as much formatting and accuracy as possible. After trying several methods, I found that using the Google Cloud Vision API yielded by far the best results of any of the publicly available OCR tools I tried.
As I could not find any single, comprehensive guide to using this amazing tool to run simple OCR applications, I decided to write this one, so that anyone with a little programming knowledge can put this wonderful tool to use.
In order to run optical character recognition using Google Cloud Vision, you first need to have a Google account. This will allow you to login to Google’s dashboard for cloud services. One of the many services which are accessible from this dashboard is file storage, which we will be using to host the PDF file we will be converting to text.
Because the advanced machine learning algorithms which we will be accessing via the Cloud Vision API run in the cloud, we will need to upload our PDF to a “bucket” of files hosted by Google, so that it will be accessible.
This tutorial will show you how to write the end result, a text file containing all the text in your PDF, to a location on your computer.
3. Click on the dropdown menu just to the right of the logo which says Google Cloud Platform. Mine says “OCR Test”, which is the name of my currently open project, but yours will say something different. A window will pop up with a list for recent projects and a “New Project” button in the top right corner. Click the button to make a new project. Give your project a name which will help you remember what you’re using it for. You don’t need to worry about any of the other fields. Click “Create”. Once your project has been created, make sure to select it by opening the window again and selecting it from the list of recent projects.
4. You should now see the Project Info, APIs, and other information panels for your newly created project, as in the screenshot above. In the “Getting Started” panel on the bottom left, click “Explore and Enable APIs”. This will allow you to choose the Google APIs you want to be able to use for this project.
5. In the menu bar at the top of the screen, click “Enable APIs and Services”. This will take you to the API Library. Search for “Cloud Vision API” and select it.
6. Click “Enable” to make the API available to your project. This will take you to your overview page for the Cloud Vision API. In the top right corner of the screen, click “Create Credentials”.
7. Choose “Cloud Vision API” from the drop down menu under “Which API are you using?” and under “Are you planning to use this API with App Engine or Computer Engine,” select “No, I’m not using them”. Click the blue “What Credentials Do I Need?” button.
8. Now you will be able to create a key so that you can authenticate yourself when you try to connect to the Cloud Vision API. Choose a service account name you will remember, and set your role to “Owner”. Set the key type to JSON. Click continue. You will now be able to download a JSON file containing your credentials.
You now have a project on the Google Cloud Platform, which will be able to use the Cloud Vision API. The next step is to upload your PDF document so that it is stored in the cloud. Then, you can write the script to convert it to text.
9. If it is not already open, click the navigation menu on the left side of the Google Cloud Platform, and scroll down until you see “Storage”. Click on it — this will open a drop-down menu. Select “Browser” from the dropdown menu. At this point, you will need to enable billing if you have not done so already. If you have Google Pay, you can use it here — otherwise, you will need to enter external payment information. This will vary depending upon how you pay, so I will not give instructions. Once you’re done, you should see a dialogue with the option to “Create a Bucket”.
10. Give your bucket a unique name. This is a storage repository within the project you created earlier. Set where to store your data to “multi-region” and the default storage class for your data to “standard”. Click “Create”.
You now have a bucket set up, where you can upload files so that they can be accessed by any APIs which are enabled for the current project. You can upload the PDF file you would like to transcribe by dragging and dropping it from wherever you keep it on your computer.
You are ready to write a program which can access both this file and the Cloud Vision API by connecting to Google Cloud services and providing the key you downloaded earlier.
Now that you have everything you need set up on the Google Cloud side of things, we will move to installing the necessary tools on your computer and using them to extract text from a PDF file.
First, you may need to make some installations. Open your terminal and navigate to a folder where you will keep the python script you write. Enter the following commands.
pip install google-cloud-vision
pip install google-cloud-storage
These use pip to install two Python libraries with tools for interacting with the Google Cloud Vision and Cloud Storage APIs, respectively. Next, run
pip freeze
This will check if you’ve installed everything you should have. You should have the following, although most will likely be newer versions.
If you don’t have any of them, use pip to install the ones missing.
Finally, you need to set your Google Application Credentials — that is, you need to register where you’re keeping the json key you downloaded earlier, so that when you run programs using Google Cloud services, your computer can authenticate itself as belonging to your Google account.
You can find excellent instructions on how to do this on any platform here. Once you have done this, you will be able to run programs which use Google Cloud Services from the command line.
Now we get to the fun part — writing a script to actually perform optical character recognition on our chosen PDF! Make a new Python file and open it with your preferred code editor. I will explain each part of the script I used so that you can understand it as you substitute in your information. You can find the whole of the script here, on my Github, as well. Try to follow along with each step before downloading it to tinker with.
We need to import json so that we can handle Cloud Vision’s outputs. re is a library which will allow us to use regular expressions to match particular patterns in strings.
Vision and storage from google.cloud will allow us to use the Google Cloud Vision and Google Cloud Storage APIs.
2. The next step is to write a function to detect all the places in our PDF file where there is readable text, using the Google Cloud Vision API. Make sure to read the comments in this function, so that you understand what each step is doing.
In addition to the comments explaining this function, here are some things to note. You may expect that when we run Google’s amazing OCR tools on a document, we will get a text file in return. Actually, this function will just output a json file — or several, depending on the size of your PDF — containing information about where there is text in the file. Actually getting the text so we can read it is the next step.
This function takes two inputs. The first, gcs_source_uri is the location of your PDF file in Google Cloud storage. The second, gcs_destination_uri is the location in Google Cloud Storage where you want the json files containing your file annotations to go.
URI is the term for a file location in Google Cloud storage. You can think of it as a URL within Google Cloud Storage, or like a path on your computer. It describes where, in the hierarchy of files you keep on google cloud, a particular file can be found. To find the URI of a file, you can double click on it to see details about it and copy the URI from the table of data you will thus open.
To generate your annotations, you will write a line at the bottom of your Python file calling the async_detect_document function. Mine looks like this.
The first URI is the path to a PDF document stored in my google cloud storage bucket, from which I want to read. The second leads to a folder in which I am saving all of my document annotations.
3. Now that we have annotated our PDF, we can finally use Cloud Vision to go to each location where there is text and read it into a text file! My code for doing this follows. Again, be sure to read the comments.
This function takes just one argument: the URI of the location where we stored our annotations. It will output the results of the transcription into a text file in your currently active directory, in addition to printing them in your terminal.
Here’s how I called it, using the same directory as before.
Congratulations! If all went well, you should now be in posession of a text file containing a line-by-line transcription of all the machine-readable text in your PDF. You may be surprised by how much could be read — it even works on some handwriting.
Here is a side-to-side comparison of some of my results. This is a page from a course catalogue I drew from the Wellesley College archives, dating from 1889. Despite the fact that I used a totally un-pre-processed PDF as an input file for this test, the results are highly accurate, even for names and foreign words.
In my next article, I will demonstrate some methods for pre-processing old text files in order to increase accuracy even more, so stay tuned. If you have any trouble or just want to chat, please get in touch — I love to talk!
Senior at Wellesley College studying Media Arts and Sciences. Future research scientist in HCI and security. Here for human connection <3
554 
10
Every Thursday, the Variable delivers the very best of Towards Data Science: from hands-on tutorials and cutting-edge research to original features you don't want to miss. Take a look.
554 claps
554 
10
Your home for data science. A Medium publication sharing concepts, ideas and codes.
About
Write
Help
Legal
Get the Medium app
"
https://blog.bitsrc.io/introduction-to-glitch-for-node-js-apps-in-the-cloud-cd263de5683f?source=search_post---------77,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Glitch is a platform made by the fine folks at Fog Creek Software, a firm known on the internet for making FogBugz, Trello, co-creating Stack Overflow. Most programmers are also familiar with the Joel Spolsky name, of Joel on Software fame.
This brief introduction should get you familiarized with Glitch and get to know how and why you can leverage it for your own projects.
Tip: Use Bit to build apps faster with components. Easily share and reuse components with your team, and use them to build new apps! Give it a try.
bit.dev
Glitch is a platform on which you can build Node.js applications in the cloud, public or private, and collaborate with other people on them.
At a first glance, you might be tempted to think it’s not that much serious, right? What’s all this images and colors, it’s not for serious programmers!
This is the key thing here. You’re not a less proficient or hardcore programmer if you don’t use Vim and instead use an editor with an interface that makes things more fun.
I’ve been using Glitch for some months now, and I must say that in my opinion it’s really an amazing platform.
What do I use it for? I use it to create quick prototypes, I use it in the tutorials I write on my blog, I also use it to host and run some scripts that automate parts of my business.
An example is a script I built to automatically send a special email to a person that buys one of my products on Gumroad, with a unique URL they can use to access the purchase.
I can write a little Node app, and Glitch gives me a unique URL that I can use to call it from webhooks.
Glitch is also a great way to demo projects, because with its embed functionality one can quickly add an entire app, or the source code visualization, into any web page. I’ve used it for some tutorials.
Glitch is (currently) focused on Node.js.When first starting out, you can start building from a sample project:
Or you can remix an existing project, which is just another name for forking:
When you fork a project, you get assigned a new URL automatically. Then you can change that URL, and pick any freely available URL that matches your preferences.
Your project will be glitch.com/edit/#/your-random-url and it will be publicly accessible at your-random-url.glitch.me.
I didn’t mention it’s completely free for individuals, while it provides paid plans for teams, that can use it with more special features that give the option for a more powerful private communication among project members.
In order to provide this free service to a big number of users, there are some limitations in place to keep servers running. Applications inactive for 5 minutes are put to sleep, and you can’t have long-running applications (after 12 hours of running they are stopped), but every time they receive a network request they are woken up again.
There are some resource consumption limits, as well, to prevent applications to use too much space or network calls.
Those limitations are very reasonable for a wide variety of applications, and I’m sure you already have some ideas for what you could use Glitch for.
You can create files, and store data in there because the filesystem is persistent.
There is no built-in database service, but you can, of course, rely on any 3rd part hosted database provider.
There are lots of sample projects to integrate external databases in your app at glitch.com/storage:
You can create a project, make it publicly available, and also hide some information when other people remix it.
This is especially important, as you might guess, with API keys, or anything that’s sensitive. You can’t just put those in publicly available code, of course, but Glitch provides us with a special place for that.
You can add those values to environment variables to the .env file, and they will only be accessible to you.
Notice the key image next to the filename, to give us a visual hint that this file is secure and will never be shown to other people even if your project is public.
If a project is private, of course, none of those files are shown to people, but your application is still available on a public URL if you decide to create a public endpoint for it.
Also, anything that’s stored in the .data/ folder is not copied over when the project is remixed.
Glitch integrates the Node debugger — only available on Chrome — which you can enable by pressing a button. From there, you can debug your application in the Chrome DevTools.
When you click the package.json file you have the option to install npm packages. By clicking Add Package, you have access to any package available on the npm registry:
Every time you make a change to one of your files, Glitch creates a new revision of the project. This under the hoods is powered by Git but presented in a really innovative and refreshing view, which can be used also by people not familiar with Git in the first place.
Clicking the two left arrows icon under your profile picture brings on the Rewind view. You can see when changes were made, by who:
Now if you move the vertical line over the changes, the editor switches to be a diff visualizer. An instant glimpse at which files changed on the left, and which lines of code changed in the opened file on the right:
This is really handy, especially when viewing your own code a few months from now, or other team members changes.
You can export any project to GitHub, as well as importing any project from GitHub.
Glitch gives you pretty tools that abstract over the underlying platform, but clicking “Log | Console” gives you access to the shell:
You can do any kind of console magic from here. Well, any magic that does not require root access.
The Glitch team made handy lists of sample applications you can start from.
Some examples?
I hope you will find a little app that’s useful to start a project you have in mind or that solves a need you have. Start by remixing it, and make it yours.
Glitch is a great platform to experiment, build things, see what sticks and in general play around with new technologies. feel free to comment below! :)
blog.bitsrc.io
blog.bitsrc.io
blog.bitsrc.io
The blog for advanced web development. Brought to you by the Bit community.
1.8K 
3
1.8K claps
1.8K 
3
Written by
I write tutorials for developers at https://flaviocopes.com
The blog for advanced web and frontend development articles, tutorials, and news. Love JavaScript? Follow to get the best stories.
Written by
I write tutorials for developers at https://flaviocopes.com
The blog for advanced web and frontend development articles, tutorials, and news. Love JavaScript? Follow to get the best stories.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://blog.invertase.io/getting-started-with-cloud-firestore-on-react-native-b338fb6525b9?source=search_post---------78,"Elliot Hesp
This blog post has been updated for version >=6.0.0 of React Native Firebase.
Late last year, Firebase announced Cloud Firestore, a NoSQL document database that compliments the existing Realtime Database product. React Native Firebase has since provided support for using it on both Android & iOS in React Native, right from day one of the release.
Let's go ahead and start building a TODO app with Cloud Firestore & React Native Firebase.
Assuming you’ve integrated React Native Firebase and added @react-native-firebase/app & @react-native-firebase/firestore to your project (using these docs), we can get started.
Create a new file Todos.js in the root of your React Native project and point your index.android.js & index.ios.js files to it:
Next; set up a basic React component in Todos.js:
Cloud Firestore allows documents (objects of data) to be stored in collections (think of them as containers for your documents). Our TODO app will hold a list of todo documents within a single “todos” collection only for simplicity. Each document contains the data specific to that todo -  in our case the title and complete properties.
The first step is to create a reference to the collection, which can be used throughout our component to query it.
We’ll import @react-native-firebase/firestore and create this reference in our component:
For simplicity, we'll use react-native-paper for our UI - a great library for React Native which provides pre-built React components that follow Googles Material Design guidelines. It's super easy to install, head over to their documentation on how to get started.
Let's now create a simple UI with a scrollable list of todos, along with a text input to add new ones:
You should now see the example scroll view, a text input, and a button which does nothing - something similar to the following:
We now need to connect the text input to our local state, so we can send the value to Cloud Firestore when the button is pressed; subsequently adding the new TODO item.
We'll use the useState hook here, and update state every time the text changes via the onChangeText prop from the TextInput component.
Modify our Todos component and add the new state item with an initial state of an empty string:
Then set the state item to be the value of our TextInput and the onChangeText to call out setTodo function; which will update our state whenever the user enters text into the input:
Your app should now respond to text changes, with the value reflecting local state:
To add a new document to the collection, we can call the add method on our collection reference.
Create a new function in our component called addTodo. This method will use our existing ref variable to add a new item to the Firestore database.
Update our button onPress to call this new addTodo method:
When the button is pressed, the new todo is sent to Cloud Firestore and added to the collection. We then reset the todo state variable to clear the TextInput box value.
The add() method on the CollectionReference is asynchronous and additionally returns the DocumentReference for the newly created document.
If we check our collection on the Firebase Console we should now see our todo records being added to the collection:
If you're having problems adding new documents; make sure your Cloud Firestore Security Rules allow writing to todos collection.
Even though we’re populating the collection, we still want to display the documents on our app.
For reading documents; Cloud Firestore provides two ways;
get() queries the collection once
onSnapshot() allows subscribing to updates to the query results (e.g. when a document changes) in realtime
As we want to subscribe to updates we'll want to use onSnapshot() so let's go ahead and setup some additional component state to handle the updates and the subscription.
Add a loading and todos state to the component. The loading state will default to true, and the todos state will be an empty array:
We need a loading state to indicate to the user that the first connection (and initial data read) to Cloud Firestore has not yet completed.
With the useEffect hook we can trigger a function to be called when the component first mounts. By returning the onSnapshot function from useEffect, the  unsubscribe function that onSnapshot() returns will be called when the component un-mounts.
The query returns a QuerySnapshot instance which contains the data from Firestore. We can Iterate over the documents and use it to populate state:
We use the snapshot forEach method to iterate over each DocumentSnapshot in the order they are stored on Cloud Firestore, and extract the documents unique identifier (.id) and data (.data()). We also store the DocumentSnapshot in state to access it directly later.
Every time a document is created, deleted or modified on the collection, this method will trigger and update component state in realtime, neat!
We also check if loading needs to be set back to false. On the first load, this will disable loading - however after initial loading is complete we update the state in realtime so there is no need for the loading state again.
Now we have the todos loading into state, we need to render them. A ScrollView is not practical here as a list of TODOs with many items may cause performance issues when updating. Instead, we’ll use a FlatList.
We'll want to render differently if loading state is true:
When not loading, render the todos in a FlatList using the todos state we're populating:
You may notice that we’ve got a Todo component rendering for each item. Let's create this as a PureComponent; this means each row will only need re-render if one of its props (title or complete) changes.
Create a Todo.js file in the root of your project:
This component renders out the title and whether the todo has been completed or not. Using react-native-paper we return a List.Item with an Icon on the left-hand side of the todo row/item. The icon changes based on the complete status of the todo.
When the row is pressed, the toggleComplete function is called; here we've set it to update the Firestore document with a reversed completion state.
Because our Todos component is subscribed to the todos collection, whenever an update is made on an individual todo; the listener is called with our new data - which then filters down via state into our Todo component.
Our final app should now look something like this:
At Invertase, we're proud to be contributing to open-source and the Firebase community, and we hope you'll love the new release of React Native Firebase.
Please get in touch via GitHub or Twitter if you have any issues or questions on this release.
With 💛 from the React Native Firebase team at Invertase.

"
https://medium.com/@xander51/the-hyperx-cloud-mix-is-too-expensive-53c123b68399?source=search_post---------79,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alex Rowe
Oct 10, 2018·5 min read
EDIT/UPDATE (10/21/2019): I eventually reviewed the launch version of this headset last year and I really liked it (Read it here!)…but I still thought the price point was a touch high. It’s a good product that I still think is just a little bit over a brilliant price sweet spot.
In a world where the only competition is standard headphones, the $199 price point of the HyperX Cloud Mix doesn’t look too bad. In a vacuum, getting a bluetooth headphone with a wired hi-res mode and a bonus gaming boom mic for under $200 isn’t so unreasonable.
Unfortunately for HyperX, the gaming headset market exists and is aggressively competitive, and already offers other options that do the same or more…for less money.
So this price is certainly a gamble.
It’s strange to see HyperX on the back foot, value-wise.
This is the second time I’ve felt this way, after their recent release of the $159 Cloud Flight (read my review here) came in at 10 dollars above the average for competing headsets.
UPDATE: That was only further complicated by the release of the exceptional Cloud Stinger Wireless.
The company is usually so good at offering extreme value for the money, and indeed, many of their other products still do, whether you’re looking at the bottom or the top end of their lineup.
You’ve got a couple of other options for Bluetooth/Gaming headset combos under the price of the Mix.
The $99 Arctis 3 Bluetooth matches the Cloud Mix feature-for-feature at half the price.
It’s not built quite as well. It doesn’t isolate as well. But it will mix audio from the wired and wireless sources, making it perfect for playing a game while listening to music or a call on your phone.
And the plastic nightmare of the Turtle Beach Stealth 700, even though it doesn’t have the world’s most robust build quality, also offers a comparable feature set to the Cloud Mix for $149…and then goes for it by also including noise cancelling and low latency direct-to-console connections.
“But what about the hi-res drivers Alex? Surely they’re going after the Arctis Pro wireless?”
Perhaps. But hi-res audio is sometimes a bit daft. It’s turned from a useful tool in the production space into a marketing trick to sell you a new thing. And even now, cleaning up this article months later, there aren’t any current games on the market that natively support hi-res audio. The launch of the new consoles next year might change that, but right now it’s a waiting game.
Also, those Arctis Pro models offer more in the features department. Yes, the GameDAC and Wireless variants are a fair bit more expensive, but their exceptional audio processors offer all kinds of connectivity options you don’t get with the Cloud Mix, and even the base $179 version offers a decent DAC and RGB lighting options.
Fundamentally, the Cloud Mix is a Cloud Alpha with added Bluetooth support. In a vacuum, that’s a really exciting sentence to me! It’s a wondeful product, but not HyperX’s most competitive one.
And the price premium over the wired model is a bit higher than the typical wired/wireless ratio.
There’s a couple of ways they could improve this and make the Cloud Mix the only headset I’d ever recommend.
Given HyperX’s excellent prior track record as a value leader, if the Cloud Mix also included the wireless dongle of the Cloud Flight, the price would make much more sense and their position would be nigh-unassailable.
2.4ghz USB dongles are often used on gaming headsets because they provide a much lower latency connection than what typical Bluetooth provides, and offer easy native support for all Windows, Mac, and PS4 machines.
Bluetooth isn’t a super great option for gaming, and I don’t know that it ever will be.
Most modern implementations prioritize audio quality over the hyper-fast latency that games and even movie soundtracks require. Not that long ago, it wasn’t common to experience sync drift with Bluetooth audio connections. They’re best suited to music even today.
Failing a dongle, if HyperX had priced this headset at $179, I wouldn’t have even written this article. $50-$80 for Bluetooth is a premium much more in line with the market standard…though they’d still have that Arctis 3 Bluetooth’s $99 price point to contend with.
I respect that HyperX is trying to cover a wider gamut with their products. But the premium headphone market is a challenging space.
The marketing blast for the Cloud Mix did include numerous deals with game streamers and influencers, who all tweeted and instagrammed about their new exciting awesome HyperX headsets within minutes of each other.
I still prefer that kind of marketing to the weird elitism of Sennheiser’s recent campaign, but I think it would be even better if someone tried to blend the famous people and the technical aspects of the product into one campaign.
It’s fine if you’d like to buy something based on its brand alone. I’m not going to judge you for that. That’s a totally viable marketing strategy that’s worked for many companies in the past. But many of HyperX’s other products offer more incredible value for the money.
I’ve never felt more like a HyperX product ignored their competitor’s pricing structures…or even their own pricing structures.
It’s true that the $200 price point for gaming headsets is still largely untapped, so perhaps they were trying to carve out a new niche there. In the year since launch, only the Corsair Virtuoso has really gone for this price, and I commend the Mix for at least fitting my head.
Fortunately, the headset goes on sale somewhat regularly, and again, in a vacuum I don’t think it’s a bad product. But at a slightly cheaper price it would top my recommendations lists instead of merely being present in the middle of them.
I write independent tech, game, music, and audio reviews and analysis from a consumer perspective. Support me directly: https://xander51.medium.com/membership
1K 
5
1K 
1K 
5
I write independent tech, game, music, and audio reviews and analysis from a consumer perspective. Support me directly: https://xander51.medium.com/membership
"
https://medium.com/@atul-sharma-94062/how-to-use-cloud-firestore-with-flutter-e6f9e8821b27?source=search_post---------80,"Sign in
There are currently no responses for this story.
Be the first to respond.
Top highlight
Atul Sharma
Jun 20, 2019·5 min read
In one of my previous blogs i had discussed about using Firebase realtime database with flutter. Now Firebase comes up with a more advanced version of it as Cloud Firestore. I will show you step by steps how we can use Cloud Firestore in Flutter.
Cloud Firestore
Cloud Firestore is also a NoSQL Database like realtime database, but the difference is in the structure of data storage. Realtime database stores data in JSON tree. In Cloud firestore we are having Collection. A collection can have any number of Documents, these Documents contains data. Cloud Firestore scales automatically, its current scaling limit is around 1 million concurrent connections and approx. 10,000 writes operations /second.
https://console.firebase.google.com
2. Add two apps one for android and other for IOS
iOS
Android
On firebase console from the left menu select Database option to create Cloud Firestore database.
Select security rules for database, for our demo purpose let us select test mode, it ill make the public database, any one can read and write to this database.
2. Open ios/Runner.xcworkspace. Save GoogleService-info.plist in Runner folder
Similarly open Android Studio and put google-services.json file in the app module of android project.
3. In your IDE or editor, open the file pubspec.yaml. Add dependency for cloud_firestore and save the file.
4. From the project directory, run the following command.
flutter packages get
2. Create databaseReference object to work with database.
3. Create a screen with buttons for CRUD operations.
In Firestore, data resides in document. Collections contains these document. In our case we will create a collection named “books” and add documents to this collection, than add data in document.
1. On click of “Create Record” button, createRecord() method is invoked.
2. In createRecord(), we create two demo records in database.
when we use collection(“books”).document(“1”) a document is created in collection named “books” and using setData() we have created data in that document.
when we use collection(“books”) .add(), it has done the same but the document name is randomly generated by firestore as we have not specified.
In order to view the data, we have to retrieve all documents or particular document of that collection. For retrieving particular document we have to specify document id.
3. Output on console is
It updates description of document id 1 to ‘Head First Flutter’
2. It deletes document 1 from the collection named books.
See all (11)
1.7K 
8
1.7K claps
1.7K 
8
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/build-a-weather-station-using-google-cloud-iot-core-and-mongooseos-7a78b69822c5?source=search_post---------81,"There are currently no responses for this story.
Be the first to respond.
Ok, there is a lot of tutorials teaching how to build a Weather Station because there many ways of doing it. It’s a simple project so I will try to focus on building an end to end solution, from collecting data to doing analytics on your data. All of it will use managed Google Cloud services, giving an overview on how to build a complete IoT solution. At the end, you could build reports on your data and access it through the web. Here you can see how it will look:
Our finished WebApp : https://weather-station-iot-170004.firebaseapp.com /
Data Studio report: https://datastudio.google.com/reporting/0B0w5dnm9bD8sdy1OR1lZQ0l4Vmc
In this tutorial we will build a weather station using a WiFi microcontroller running MongooseOS, that sends data securely via Cloud IoT Core using MQTT protocol, then the data is processed in an event-based way using Firebase Cloud Functions, that save the raw data in BigQuery and update the device current state in Firebase Realtime Database. The data then can be accessed through DataStudio and via a simple WebApp hosted on Firebase Hosting. It’s many products, but I will show how each one can be easily connected to deploy a product that scales on demand. Our architecture will look like this:
For ease of development, I’ll use MongooseOS, that already have a connector for Cloud IoT Core and helps with the process of provisioning devices with certificates, WiFi configuration and others custom configurations.
What we will learn:
So enough talk, let’s get started 🚀.
Google recently launched in public beta Cloud IoT Core, a managed service to securely communicate with your IoT devices using common protocols (MQTT and HTTP) and to manage those devices in an easy way. Basically, with this service, you can plug with many others Google services to process, store and analyze all the data generated by your devices. Here we can see an example of a recommended architecture using Cloud IoT Core.
Cloud IoT Core have a concept of registry of devices, wherein our project we will group a series of similar devices and associate with this registry. To get started with Google Cloud you can do all on the Cloud Console web interface, but the command line tools is a more powerful tool and it’s the one that I choose to use on this project.
To use the gcloud command line tools, follow the instructions here to download and install it.
cloud.google.com
After installing the SDK, you should install the beta tools to have access to the Cloud IoT Core commands. Also after this you should authenticate and create a project to use in this tutorial, exchange YOUR_PROJECT_NAME with a name that you want for this project:
Now on the Cloud IoT Core side, you first should configure some components related to Cloud PubSub, one of the main components used by Cloud IoT Core. In the commands below you will do the following:
If you access the Google Cloud Console you can validate that it’s all created and configured.
For this project I’ll be using the newest ESP32 WiFi microcontroller, for those who don’t know it yet, it’s the sucessor of the largely famous ESP8266 from ExpressIf, but now with much more capabilities, like built-in Bluetooth LE, dual-core processor clocked at 240MHz, touch sensor and support for flash encryption, so no one can get access to your code. One hell of an upgrade.
Adafruit sells an awesome kit to get started with the ESP32 and Google Cloud, it contains all you need for this project and many others, so if you want to go the easy way, you can buy one of this. (Just an idea for you adafruit industries, I do not have one of these, just saying …)
This project also works on an ESP8266, so the code and schematic provided here have a configuration to run on both microcontrollers. The circuit for this project is very simple, just connect the DHT sensor to the ESP32/ESP8266 like the following diagram:
To program the board we will use MongooseOS, that is an Operating System with many awesome features and made for commercial products. It has support for some microcontrollers like CC3200, ESP32 and ESP8266. One cool feature of it is the possibility to quickly prototype your embedded apps using Javascript and it has a tools called mos that make programming, provisioning and configuration really easy on those supported boards.
To use it we need to download and install it from the official website. Follow the installation instructions on https://mongoose-os.com/docs/quickstart/setup.html.
mongoose-os.com
With the tools installed, download the project code on Github repository linked here, so you can build and deploy it on the device.
github.com
The repository consists of 3 sub-projects:
Here some description of the firmware project:
To program the hardware, enter the firmware folder and run the following instructions to flash the firmware, configure WiFi and provision the device on Cloud IoT Core:
That’s it, your device will begin to send data to Cloud IoT Core. The projects come configured to send data each minute, but you can changes this later on the fs/init.js file or you can create a custom configuration variable to change the time. I will leave this as a homework. You can see whats happening on the device using the mos console tool. You will see it trying to connect with mqtt.googleapis.com.
To see the data on PubSub you can use gcloud command to query the subscription that we created:
If you see the data on the console, you can start celebrating, we are on the right path 🎉🏆.
Getting directly from the official website definition:
BigQuery is Google’s low-cost, fully manageable petabyte scalable data storage service. BigQuery is stand-alone, there is no infrastructure to manage and you do not need a database administrator as it scales with your data.
Here we will use it to store all of ours collected sensor data to run some queries and to build reports later using Data Studio. To start let’s create a Dataset and a Table store our data. To do this, open the BigQuery Web UI, and follow the instructions:
Now to insert data on BigQuery we will user Firebase Cloud Functions, that can be configured to execute based on many different triggers and events. One of those triggers are new data inserted on a PubSub Topic, so we will listen to our Topic associated with our Device Registry and with each data that arrives we execute a function that store the data in BigQuery and maintain the last device data on Firebase Realtime Database.
Firebase Realtime Database is a technology really useful to maintain realtime data, giving free and automagically sync between all connected clients. Even Google recommends it to maintain realtime state from IoT devices like we can see in the here.
The code for our function can be seen above, it basically react to PubSub events and insert into BigQuery then update the current state on Firebase.
The Firebase Command Line Tools requires Node.JS and npm, which you can install by following the instructions on https://nodejs.org/. Installing Node.js also installs npm.
Once Node and NPM is installed, run the following command to install Firebase CLI.
Now to configure firebase with our project and deploy the functions, in the project root folder, follow the above instructions:
With the deployed functions you have all setup to ingest the telemetry data sent by the device and store in both storages solution. You can see all deployed resource on the Firebase Console.
You can see the code for the functions above:
The firebase-tools have also a builtin server, you can start it on the project folder just running firebase serve, it will start a web server on port 5000 by default.
The webapp can be seen directly on the public directory, the logic is at public/app.js and the frontend at public/index.html. It’s pretty basic, just Javascript, Web Material Component and Chart.JS the charts.
If all it’s correctly setup, then you can celebrate again, because you developed an end to end solution for IoT without touching an advanced server setup.
Data Studio is a really intuitive tool and I will not explore it here so this tutorial became less extensive, but to let you know, Data Studio has a BigQuery connector, so just import your table and play with the different visualizations provided by this awesome tool. Go to the datastudio.google.com and play with it.
That’s it for this tutorial, hope that you got interested in Google Cloud IoT Core, it’s an awesome service that you can do powerful things with it. The post got a little bit longer than I expected, but I believe that gives a great overview of a lot of tools on Google Cloud Platform.
The code for this project can be found on my Github and some interesting are linked in the section bellow to read later:
github.com
Do you like this post ? So don’t forget to leave your clap on 👏 below, recommend and share it with your friends.
Did you do something nice with this tutorial? Show in the comments section below.
If you have any questions, post in the comments that I will try to help you.
Google Cloud community articles and blogs
1.3K 
26
1.3K claps
1.3K 
26
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Product Engineer @Leverege & Google Developer Expert for IoT. When I'm not cooking, I'm building fun stuff or helping my local dev community. GDG organizer.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/free-code-camp/rtdb-to-firestore-fd8da8149877?source=search_post---------82,"There are currently no responses for this story.
Be the first to respond.
Migrated to personal blog: https://alexsaveau.dev/2017/12/12/survival-guide-how-to-migrate-from-the-firebase-realtime-database-to-cloud-firestore/
Ever since Google’s new mobile SDKs were announced two years ago at I/O, the Firebase empire has been ever-expanding. It now supports more and more features, such as Cloud Functions, phone authentication, and performance monitoring. However, one SDK that hasn’t changed much is the Firebase Realtime Database (RTDB).
The RTDB hasn’t received any major updates — and not because it was a perfect API. Far from it. If you’ve read about Pier Bover’s experience, or have used the Firebase RTDB yourself, these problems might sound familiar:
No way to query your data properly […] and dumb data modelling.
So what’s next? How is Google going to solve these limitations? Instead of releasing a version 4.0 of the RTDB, which would be messy and painful for everyone, Google is using what it learned from the Firebase Realtime Database’s faults. And they’re completely redesigning and rewriting it from scratch into a new database: Cloud Firestore.
The RTDB isn’t going away — that would cause a huge crisis. But going forward, Cloud Firestore will receive most of the attention and love.
This article is going to delve deep into Google’s long-awaited database revamp, mostly from an Android RTDB developer’s point of view. In addition, the article is intended as a replacement for hours of pouring over documentation to build a mental model of the new SDK.
Unless you’ve just recently joined the Firebase party, you’ve probably heard of the Google Cloud Platform (GCP). Except for the RTDB, all other Firebase server products, like Cloud Functions and Firebase Storage, are usually rebrands of existing GCP solutions with extra bells and whistles — plus Firebase integration and branding.
However, the RTDB was ported over from the pre-Google Firebase. It turns out that the database actually used to be a chat service. They only decided to strip out the UI and turn it into an SDK after the company shifted its focus. With the never-ending swarm of chat app samples, you’d think they’re still a little bit nostalgic.
On the other hand, Cloud Firestore is built off GCP’s Google Cloud Datastore, a NoSQL database with near-infinite scalability and powerful querying abilities. The Firebase-branded Cloud Datastore adds the expected realtime capabilities and, of course, integration with other Firebase services such as Authentication and Cloud Functions.
For database fans, Cloud Datastore is a multi-region, synchronously-replicated database that uses ACID transactions. This means that once Google says your write is committed, a dinosaur-age meteor could take out a region and your data would still be safe and sound, ready to be queried.
Now I’m not saying we humans would fare so well…but at least your data would still be rock solid! Oh, and it uses atomic clocks — now if that isn’t cool, I don’t know what is!
Now that you have a basic understanding of why Google decided to create a completely new Firebase branded database and where it came from, let’s get started!
For the rest of this article, I’ll be using examples from apps I’ve built (so no chat app samples! 😁). Specifically, I’ll be using examples from Robot Scouter, an app to help FIRST Robotics Competition teams make data-driven decisions during competitions.
The app’s basic purpose is to allow users to collect data about other teams in units called scouts. These scouts can be based off customizable templates. Scouts and templates are composed of metrics, which are different types of data a user can collect. Templates are standalone objects, but scouts are implicitly owned by a team.
Individual teams and templates can be shared with other users, but the scouts will follow a team around wherever it goes since the team owns them.
Let’s start by looking at what the Robot Scouter data structure looked like with the RTDB. Take a deep breath, there’s a lot to scroll through:
All we really wanted was a teams, templates, and users collection. Instead, we had to denormalize our data to accommodate for the RTDB’s deep-by-default queries. Remember, if we query a node with the RTDB, we also get all child nodes.
Now let’s take a look at the equivalent Cloud Firestore data structure:
The Cloud Firestore data structure is easier to understand and much shorter than the RTDB structure. This is thanks to nested refs versus being forced to denormalize everything in the RTDB.
The first major difference you’ll notice is the lack of denormalization. In the Cloud Firestore data structure, we declare our teams ref and put each team directly inside that ref instead of flattening it out. Similarly, we’ve merged the template-indices directly into the templates ref.
You might have also noticed that our scouts are now directly placed inside a team instead of being in a separate indices ref. Your first reaction might have been “Wait, you’re going to waste all the user’s data! Don’t do that!” This is the beauty of Cloud Firestore: queries are “shallow” by default.
I put the word shallow in quotes because technically, you could nest a huge amount of data in your documents. But we’ll talk about why you shouldn’t do that later. Wait a sec, what’s a document? Cloud Firestore has two basic building blocks: collections and documents.
Collections in Firestore are the equivalent of a ref in the RTDB with a huge list of child nodes that each contain objects. If you scroll back up to the Firestore data structure, you’ll notice that teams, templates, and users are all collections. They each contain a whole bunch of objects — and in Firestore, these objects are called documents.
Documents would be your conventional object node in the RTDB. However, in Firestore they’re a little special: documents are explicitly owned by a collection. In the RTDB, you could put pretty much anything anywhere. Cloud Firestore brings in a little sanity and uses an alternating pattern of collections and documents which looks a little like this: collection1/document1/collection2/document2/....
While this pattern could feel constraining at first, I’ve found it helpful in forcing me to design a neatly organized data structure. You’ll notice that my scouts collection now properly resides within a team document. I only had to split it out in the RTDB so that my users wouldn’t have to download all their scouts when looking at a team. In Cloud Firestore, teams have explicit ownership of a set of scouts without having to download them when loading the team.
In the RTDB, you had a free-flowing model with 3 core data types: booleans, strings, and numbers. Things like lists and maps were either an afterthought or just part of how you queried for data in the RTDB.
On the other hand, with Firestore, you have a very clear cut structure: collections contain documents, and documents contain fields and links to one or more subcollections.
Subcollection is just a fancy term for another list of objects owned by a document — except you won’t get that list when querying the document. This is because documents don’t technically contain subcollections. They just link to them. Hence why we can put our scouts collection inside the team document — or link to it, if you will.
On top of containing subcollections, documents in Firestore support a vast array of data types with more on the horizon. For now, here are the supported types:
Yes, null is now an explicitly defined data type in Cloud Firestore. If you set a document equal to a Java object whose getter returns null, the field will still show up in the Firebase Console with the null data type.
Well ok, so what? Adding the null data type makes field deletion explicit. In the RTDB, setting something to null is the same as deleting it. But in Firestore, to delete a field you must set its value to FieldValue.delete(). On a similar note, ServerValue.TIMESTAMP has become FieldValue.serverTimestamp().
In addition, the null data type sort of enables migrations. Using the DocumentSnapshot#contains(…) method, you could check to see if a field exists and do something if it doesn’t. A better strategy would be to use a Cloud Function, but that’s beyond the scope of this article.
You’ll notice documents still support arrays and maps, but how does that work if documents can only contain fields? Remember how I said you could technically nest your data? Here’s that special case: Firestore lets you store explicitly defined arrays and maps within documents, as opposed to creating a subcollection.
Note: there are several limits to documents in Firestore. Specifically, there is a 1MB size limit, a maximum of 20,000 properties per document, and a 500 level deep object nesting limit.
Properties are different from fields in that they account for all nested fields, not just the conventional root level field. In addition, as of this writing, updating a large array or map rewrites the entire array/map and will have abysmal performance on large data structures. Please use subcollections instead!
Because Google loves to rename things, document “keys” from the RTDB are now called ids. The last path segment of a collection or document is called an id, meaning teams/teamId1 is a document with id teamId1 under a collection with id teams. Nothing too groundbreaking, but it’s always nice to be on the same page when it comes to terminology.
Finally, since documents are one of the fundamental building blocks of Firestore, you can only get a full document. This is unlike the RTDB where you could query for as specific a field as you liked.
Now that you have a basic understanding of Firestore’s two fundamental building blocks — collections and documents — it’s time to look at how we can store and then get our data.
The Cloud Firestore API surface is a massive improvement over the RTDB’s. So you’re unlikely to find any methods that were simply ported over (though some will look familiar).
The first distinction you’ll notice from the RTDB is the slightly messy and spread out way of creating and updating data. Not to worry, it’ll all make sense in just a bit.
Unlike our pets, there are no stray documents — they must all live under a collection. This means we have two places from where we can add data: a collection to add documents, and a document to add, update, or remove fields.
Let’s start by looking at the simplest way to add data, though collections:
We’re saying that within the teams collection, we want to add a document with all the fields from the Team POJO.
Now let’s look at the more interesting case where we change document data:
The first thing to note is our scoutRef: it creates a scout inside our scouts collection, which in turn exists under a team document. As a URL, it would look something like this: teams/teamId/scouts/newScoutId.
The document() method returns a new DocumentReference with a random id. It’s a truly random id in the sense that it’s no longer based off of a timestamp.
Those familiar with the RTDB will know that the push() method creates a pseudo-random key using a timestamp for native temporal sorting. Since Cloud Firestore aims to move away from being a chat-oriented database, it doesn’t make sense for them to use temporal sorting as the default mechanism.
As such, this means you’ll have to manually add a timestamp field when relevant. In theory, you could use the timestamp as a document id for sorting, but that reduces flexibility.
The DocumentReference contains a plethora of different ways to set and update data, from using maps and POJOs to supplying varargs. There’s a slice of pie for everyone! I’m going to focus on the POJO and specific field update methods, since those are the ones I’ve found to be most useful.
The first method you’ll notice and probably use most often is set(Object). This one is fairly simple: it behaves exactly like the Java’s Map#set(key, value) method. If no document exists, it will create a new one. Otherwise, if a document exists, it will be overwritten.
However, Google also provides SetOptions with various merge combinations to only overwrite some fields. I’ve found this to be useful when updating a user’s profile, for example. I’ll set/update their name, email, and photoUrl, but not the lastLogin field, since it isn’t part of my User POJO.
If you want to ensure a document exists before performing an update, the update(String, Object, Object…) method will be the right tool for the job. In this case, we are updating a specific field with a new value. If the document doesn’t exist prior to calling the update method, the update will fail. If you’d like, you can also update multiple fields at once by alternating key/value pairs in the varargs. (I personally prefer using multiple updates in a WriteBatch , which I’ll discuss later.)
What if you wanted to update a nested field inside an object? For this use case, Google provides the FieldPath#of(String…) method. Each item inside the varargs array brings you deeper inside a nested field’s path — technically an object. For example, FieldPath.of(""rootField"", ""child"") updates the following field: myDocument/rootField/child.
Similarly, Firestore also supports the dot notation syntax which lets you reference that same field like so: rootField.child.
Cloud Firestore also includes an awesome new way to batch writes with the WriteBatch class. It’s very similar to the SharedPreferences.Editor you’ll find on Android. You can add or update documents in the WriteBatch instance, but they won’t be visible to your app until you call WriteBatch#commit(). I’ve created the standard Kotlin improvement where the batch lifecycle is managed for you — feel free to copypasta.
The last important API change to note when managing data is how to delete it. Cloud Firestore has a method to delete a document—DocumentReference#delete()—but no easy way to delete an entire collection. Google provides a code sample with documentation on how to delete all documents in a collection, but they haven’t baked it into the SDK yet. This is because this method could easily fail under extreme conditions when attempting to delete thousands or even millions of documents buried in various subcollections. But Google does say they’re working on it.
In addition, their sample doesn’t delete subcollections either — only documents under the collection. Google doesn’t yet have a clear solution to that problem on Android either. Still, they’re providing a CLI/NodeJS API as part of firebase-tools that you can use to delete all subcollections manually or from a Cloud Function.
In my case, I don’t let users create random collection names so I can delete all my subcollections by getting their parent document ids.
I’ve rewritten their sample with more functionality and a cleaner API in Kotlin:
Whew, we’ve covered most everything you’ll need to know about storing data!
The first thing to note is that I’m using the word retrieving instead of reading. This is because Firestore provides two very clear-cut ways of retrieving data: either through a single read (aka a get), or through a series of reads (aka a snapshot listener).
Let’s start by exploring ways to read data once. In the RTDB, you had the addListenerForSingleValueEvent() method, but it was full of bugs and edge cases. I think Frank van Puffelen — a Googler — summed it up best:
The best way to solve this is to not use a single-value listener.
Yeah. There’s definitely a problem when you tell your own users not to use a product you’re selling.
Cloud Firestore completely revamps the entire data retrieval experience with better and more intuitive APIs.
First, a note on offline capabilities. The RTDB wasn’t designed as an offline first database — offline capabilities were more of an afterthought since the db was ported over from pre-Google Firebase. On the other hand, Cloud Firestore isn’t exactly an offline first database since it’s also designed to be realtime. But I would consider its offline capabilities to be first class citizens along with the realtime stuff.
Given those improvements, offline support is enabled by default (except for web), and data is stored in a SQLite database using Android’s native APIs. I don’t know about you, but I find it more than a little ironic that a NoSQL database needs a SQL database to work.
For the curious, Firestore’s SQL database is named firestore.$firebaseAppName.$projectId.(default). In addition, they lock it using PRAGMA locking_mode = EXCLUSIVE to improve performance and prevent multi-process access. If you’re really curious, here are the tables and queries I’ve found so far:
I did some more digging and found a few other things. For example, GRCP devs really like enums. You know what they say, “If something is bad for you, do more of it!”
With that aside, let’s explore our first method: DocumentReference#get(). This is the simplest and most basic way to retrieve data: it replaces the RTBD's addListenerForSingleValueEvent() method with several notable improvements.
First, it returns a Task<DocumentSnapshot> . This makes far more sense than using the same event model API as you would for snapshot listeners from the RTDB. Now, you can use all of Play Services’s lovely Task APIs to add your success and failure listeners. You can even attach them to an activity lifecycle if needed.
Second, offline support actually makes sense when using get(). If the device is online, you’ll get the most up-to-date copy of your data directly from the server. If the device is offline and has cached data, you’ll immediately get that cache. And finally, if there’s no cached data, you’ll immediately get a failure event with error code FirebaseFirestoreException.Code#UNAVAILABLE. TLDR: you’ll get the most up-to-date data that can be retrieved in the device’s current network state.
I’ll dive into queries in just a bit, but for now, I’ll just mention that the Query#get() method returning a Task<QuerySnapshot> is also available with the same behavior as described above.
In other notable news, the Query#getRef() method was removed to support a possible future where a query doesn’t depend on a CollectionReference. Just like in the RTDB, CollectionReference extends Query to support easily starting a query. But in the RTDB, you used to be able to jump back and forth between queries and refs. This is no longer the case in Firestore. I’ve found this to be a mild inconvenience, but nothing too major.
Of course, this is Firebase — so we also want our sweet, sweet realtime capabilities. The API surface for queries was also completely revamped to be cleaner and clearer.
Let’s start by looking at how you would get all the documents in a collection.
Do you remember the difference between addValueEventListener() and addChildEventListener() from the RTDB? And have you ever wished you could get a bit of both worlds? Me too. Thankfully, this is exactly what Google has done with Cloud Firestore: you’ll get the entire list of documents and a list of changes and possible exceptions all in one monolithic callback.
I’m not sure I like the combined data/exception model, but it makes sense in a Java 8 world with functional interfaces. For example, here’s a nice lambdazed callback:
Let’s start with the error case, since that’s what all good developers should think about first, right? 😁
FirebaseFirestoreException is relatively simple compared to the RTDB. First, it’s actually an exception! Whaaat? An error that actually extends Exception—who would have thought!? This makes crash reporting dead simple: just report the exception which includes error codes and everything. It’ll look nice and pretty like this:
Exception com.google.firebase.firestore.FirebaseFirestoreException: PERMISSION_DENIED: Missing or insufficient permissions.
With that aside, let’s move on to the exciting stuff: QuerySnapshot. It contains document changes, the full list of documents, and some other data I’ll explore in just a bit.
I’ve provided links to all the relevant classes, because I’m going to skip over those in favor of using FirebaseUI. I’ll explore this in depth later when we’re putting everything together.
As a quick summary, you can differentiate between different update types, iterate over the QuerySnapshot to get each DocumentSnapshot in pretty Java 5 for loops, convert the entire list to a bunch of POJOs (not recommended for performance reasons, will discuss later), and convert individual documents to a POJO or access specific field information. So basically everything you’d expect from a nice API.
However, I do want to explore listener registration and QueryListenOptions—a new way get information about your offline status.
Those two concepts will be easier to understand with a code sample, so here goes nothing:
The basic idea of this method is to wait until data is received directly from the server.
The first thing to note is listener registration — it’s kinda painful. I’ve spent some time thinking about it, and I’ve come to the conclusion that Google made the right choice. It should be painful to nudge you in the right direction.
Ok, let’s back up a bit. In the RTDB, you’re used to removing the listener callback instance directly from the Query class. This was a nice API, but it let you do terrible things like accidentally leak your Contexts. The new API returns a ListenerRegistration whose only method is remove()—pretty self-explanatory.
This new listener registration method forces you to rethink your approach to retrieving data. Here’s a simple guide to choosing which API to use:
Ok, so the listener registration API is painful, but intentionally so to nudge you into picking the right tool for the job.
Now let’s take a look at the QueryListenOptions. Remember how I said Cloud Firestore considers offline support a first class citizen? Here’s where they address the final pain points devs experienced with the RTDB. They still don’t offer a way to customize how your data is cached, but personally, I don’t see any value in that kind of customization: the API should be smart enough to manage that stuff for me—and it is with Firestore.
The first method you’ll find in your listen options is called includeQueryMetadataChanges() and the second is called includeDocumentMetadataChanges(). Both of these are tied to SnapshotMetadata`s isFromCache() and hasPendingWrites() respectively.
For a given QuerySnapshot, isFromCache() will have the same value for each DocumentSnapshot’s metadata and for query’s metadata itself. This means you can find out if your data is up-to-date with the server either from a QuerySnapshot or from a DocumentSnapshot—it doesn’t matter. Either the entire query is considered to be up-to-date, or not — there’s no in-between state like the API would have you believe. In theory, one of your documents could actually be up-to-date if another active listener includes that document in its results, but Google has opted for simplicity and doesn’t surface this information in the API.
On the other hand, hasPendingWrites() can have a different value for each DocumentSnapshot. This is what you’d expect, and there aren’t any special edge cases or tricks.
To summarize:
One last tidbit before I move on: all the addSnapshotListener methods are also duplicated in DocumentReference so you can get updates about just a single document if needed.
Ahhh… More than 3,000 words later, we finally get into the meat of Cloud Firestore.
I don’t have any statistics to back this statement, but I think that by far the biggest complaint about the RTDB is the lack of proper querying abilities. Here’s another quote from Pier Bover’s article:
Really? Google is providing a data service with no searching or filtering capabilities? Yeah. Really.
Since Cloud Firestore is backed by GCP’s Cloud Datastore, queries are first class citizens.
Let’s go back to our new and improved data structure. But to save you from aggressively scrolling up for a minute, here it is reposted:
Since we have an infinite list of teams, how do we get a specific user’s teams? In the RTDB, we would have stored the data following a pattern akin to this: teams/uid1/teamKey1. With Cloud Firestore, we flipflop the user’s id and the team id so that the pattern looks more like this: teams/teamKey1/owners/uid1.
Now we can query for the user’s teams like so:
We’re telling Firestore to look at the owners field in all documents under the teams collection for a document with id uid equal to true.
Unfortunately, this method doesn’t support ordering. So instead, we’ll write the following query:
This query has the advantage of supporting ordering, but it comes with similar issues to the RTDB: updating those sorting values is going to be a pain.
In my case, the sorting values are always static: they’re either the team number or the document creation timestamp. Because I’m never going to update those sorting values, this query works perfectly for me.
On the other hand, you might have different constraints — remember, I need my data to be structured in a way that supports easily sharing teams and templates across users. If this isn’t your case, you should take a look at Google’s suggested structures and their solutions to common problems.
Since the queries you write will depend on your app’s specific constraints, I’m not going to delve into them too much. But I will point out that Cloud Firestore supports compound queries.
One last notable change from the RTDB before I move on: priorities aren’t a thing anymore. Since Firestore properly supports ordering and querying, they opted to remove the .priority field you could find in the RTDB from Firestore documents.
However, if you still want to order your documents by id for some reason, Firestore provides the FieldPath#documentId() method for exactly that purpose.
Security rules in Firestore have gotten a bit worse for wear, in my opinion. However, for those familiar with Firebase Storage, you’ll feel right at home. Google has merged its database rules technology with the rest of GCP.
On the other hand, for those coming from a JSON world with the RTDB, Firestore’s new rules syntax is a bit convoluted. If you deploy rules in your CI build, you’ll have to either edit them in the Firebase Console and then copy the rules to your local editor, or edit them in a txt file. Gross.
Here’s what the simplest possible set of rules looks like:
Google actually has surprisingly good documentation on security rules — I’ve personally been able to solve nearly all of my problems just by reading the docs. I will still go over a few gotchas from the RTDB developer’s point of view (assuming you’ve at least skimmed the docs).
First, the read keyword is an array of get and list, and the write keyword is an array of create, update, and delete . Each keyword is self-explanatory except for list — it applies to queries, meaning not a single “get.” Each of these keywords can be used individually, but the read and write ones were provided for convenience.
On a related note, you’ll usually end up splitting up your write keywords to allow deletion. For example, using the request object to check write validity fails if a user is trying to delete the data in question. In addition, if you’re checking to see if someone is an owner, you’ve introduced a security flaw. Anyone can add themselves, since the new data is being checked instead of the old.
Here’s some sample rules to put those words into code:
There’s another major difference from the RTDB developer’s perspective: rule evaluation is shallow by default. This goes along nicely with the (sub)collection model, but requires a small shift in mindset.
For example, the request variable does not contain information about its parent document. At first, I wanted to check from a document inside a subcollection if a parent document had some field. But of course this doesn’t work, because the subcollection is just a link inside of the parent document.
Because rules are shallow, you have to be careful when using the double star operator (variable=**) since its resources won’t contain parent document information. In addition, there’s some funkiness with the variable:
Now that you have a complete understanding of Cloud Firestore’s capabilities along with its differences and improvements from the RTDB, let’s take a look at how we can put all of this together to build some UI.
FirebaseUI consists of several components including auth and storage, but we’ll focus on the firestore module.
In the querying section, I mentioned several times that FirebaseUI could help us. We’ll start by looking at how we can improve upon QuerySnapshot’s toObjects() method.
There are two main problems with using the toObjects() method:
While you might be thinking, “well, I’ll just create a list and update it whenever new objects come in,” FirebaseUI does exactly that for you so you don’t have to write boilerplate code.
FirestoreArray—as it’s aptly named — is an array of snapshots from Firestore converted to your POJO model objects. Its constructor takes in a Firestore Query, a SnapshotParser<T>, and optionally, query options. It starts listening for data whenever you add one or more ChangeEventListeners and will automatically stop listening when the last listener is removed.
The ChangeEventListener will notify you when each object changes, when an entire update has been processed, and when any errors occur. The SnapshotParser<T> has a single method — parseSnapshot—which is responsible for converting each DocumentSnapshot into your model POJO of type T.
Since FirestoreArray implements List<T> , this setup lets you easily listen for updates to your model objects with minimal hassle.
In terms of performance, FirestoreArray uses Android’s native LruCache to lazily parse objects as needed. For now, we’ve set the max cache size to 100, but if you feel you’ll need a bigger (or smaller) cache size, we’d love to know your use cases in a GitHub issue.
Since this is FirebaseUI, we let you easily map your FirestoreArray to a RecyclerView with the FirestoreRecyclerAdapter and its FirestoreRecyclerOptions.
There are a few interesting recycler options, notably the ability to pass in an Android Architecture Components LifecyleOwner with which we’ll automatically manage the FirestoreArray’s lifecycle for you.
Ok, that was a lot of words. Here’s what it would look like all put together with Architecture Components while taking auth states into consideration:
For web developers, Firestore comes complete with full offline support, unlike the RTDB which had… nothing? Yep. Cheers to offline support as a first class citizen for all mobile platforms!
Also, if you’d like extra info about migrating from the RTDB to Cloud Firestore, like how to keep your data synchronized during the transition period, you’ll find documentation here.
I hope you’ve enjoyed this deep dive into Firebase’s new database and are ready to start migrating your apps. Feel free to ask me any questions or clarifications! And if you found this article helpful, don’t hesitate to give me some claps 👏.
If you’ve been enjoying quotes bashing the RTDB, here’s one last quote for your viewing pleasure:
People have made [the RTDB] work for prod apps, but they are forcing a square peg into a round hole. - Eric Kryski
Ouch, that burns! While the RTDB may have been an uncontrollable wildfire, Cloud Firestore is a fiercely powerful flame you can wield with purpose to build better apps!
firebase.google.com
We’ve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
2K 
5
2K claps
2K 
5
We’ve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
Written by
Freelancer. All things 🔥base, Android, and 🔓-source. Also, 🐤 builds. https://alexsaveau.dev
We’ve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
"
https://medium.com/free-code-camp/how-to-build-a-web-application-using-flask-and-deploy-it-to-the-cloud-3551c985e492?source=search_post---------83,"There are currently no responses for this story.
Be the first to respond.
In each section, I will show pieces of code for you to follow along. All the code used in the tutorial is available in this GitHub Repository.
HTTP is the protocol for websites. The internet uses it to interact and communicate with computers and servers. Let me give you an example of how you use it everyday.
When you type the name of a website in the address bar of your browser and you hit enter. What happens is that an HTTP request has been sent to a server.
For example, when I go to my address bar and type google.com, then hit enter, an HTTP request is sent to a Google Server. The Google Server receives the request and needs to figure how to interpret that request. The Google Server sends back an HTTP response that contains the information that my web browser receives. Then it displays what you asked for on a page in the browser.
We will write code that will take care of the server side processing. Our code will receive requests. It will figure out what those requests are dealing with and what they are asking. It will also figure out what response to send to the user.
To do all this we will use Flask.
It makes the process of designing a web application simpler. Flask lets us focus on what the users are requesting and what sort of response to give back.
Learn more about micro frameworks.
The code lets us run a basic web application that we can serve, as if it were a website.
This piece of code is stored in our main.py.
Line 1: Here we are importing the Flask module and creating a Flask web server from the Flask module.
Line 3: __name__ means this current file. In this case, it will be main.py. This current file will represent my web application.
We are creating an instance of the Flask class and calling it app. Here we are creating a new web application.
Line 5: It represents the default page. For example, if I go to a website such as “google.com/” with nothing after the slash. Then this will be the default page of Google.
Line 6–7: When the user goes to my website and they go to the default page (nothing after the slash), then the function below will get activated.
Line 9: When you run your Python script, Python assigns the name “__main__” to the script when executed.
If we import another script, the if statement will prevent other scripts from running. When we run main.py, it will change its name to __main__ and only then will that if statement activate.
Line 10: This will run the application. Having debug=True allows possible Python errors to appear on the web page. This will help us trace the errors.
In your Terminal or Command Prompt go to the folder that contains your main.py. Then do py main.py or python main.py. In your terminal or command prompt you should see this output.
The important part is where it says Running on http://127.0.0.1:5000/.
127.0.0.1 means this local computer. If you do not know the meaning of this (like I didn’t when I started — this article is really helpful), the main idea is that 127.0.0.1 and localhost refer to this local computer.
Go to that address and you should see the following:
Earlier you saw what happened when we ran main.py with one route which was app.route(“/”).
Let’s add more routes so you can see the difference.
In lines 9–11. we added a new route, this time to /salvador.
Now run the main.py again and go to http://localhost:5000/salvador.
So far we have been returning text. Let’s make our website look nicer by adding HTML and CSS.
First create a new HTML file. I called mine home.html.
Here is some code to get you started.
The Flask Framework looks for HTML files in a folder called templates. You need to create a templates folder and put all your HTML files in there.
Now we need to change our main.py so that we can view the HTML file we created.
Line 1: We imported render_template() method from the flask framework. render_template() looks for a template (HTML file) in the templates folder. Then it will render the template for which you ask. Learn more about render_templates() function.
Line 7: We change the return so that now it returns render_template(“home.html”). This will let us view our HTML file.
Now visit your localhost and see the changes: http://localhost:5000/.
Let’s create an about.html inside the templates folder.
Let’s make a change similar to what we did before to our main.py.
We made three new changes:
Line 9: Change the route to""/about"".
Line 10: Change the function so it is now def about():
Line 11: Change the return so that now it returns render_template(""about.html"").
Now see the changes: http://localhost:5000/about.
To connect both pages we can have a navigation menu on the top. We can use Flask to make the process of creating a navigation menu easier.
First, let’s create a template.html. This template.html will serve as a parent template. Our two child templates will inherit code from it.
Line 13–14: We use the function called url_for(). It accepts the name of the function as an argument. Right now we gave it the name of the function. More information on url_for() function.
The two lines with the curly brackets will be replaced by the content of home.html and about.html. This will depend on the URL in which the user is browsing.
These changes allow the child pages (home.html and about.html) to connect to the parent (template.html). This allows us to not have to copy the code for the navigation menu in the about.html and home.html.
Content of about.html:
Content of home.html:
Let’s try adding some CSS.
In the same way as we created a folder called templates to store all our HTML templates, we need a folder called static.
In static, we will store our CSS, JavaScript, images, and other necessary files. That is why it is important that you should create a CSS folder to store your stylesheets. After you do this, your project folder should look like this:
Our template.html is the one that links all pages. We can insert the code here and it will be applicable to all child pages.
Line 7: Here we are giving the path to where the template.css is located.
Now see the changes: http://localhost:5000/about.
Now that you are familiar with using Flask, you may start using it in your future projects. One thing to always do is use virtualenv.
You may use Python for others projects besides web-development.
Your projects might have different versions of Python installed, different dependencies and packages.
We use virtualenv to create an isolated environment for your Python project. This means that each project can have its own dependencies regardless of what dependencies every other project has.
First, run this command on your command prompt or terminal:
Second, do the following:
Here you can give a name to the environment. I usually give it a name of virtual. It will look like this: virtualenv virtual.
After setting up virtual environment, check your project folder. It should look like this. The virtual environment needs to be created in the same directory where your app files are located.
Now go to your terminal or command prompt. Go to the directory that contains the file called activate. The file called activate is found inside a folder called Scripts for Windows and bin for OS X and Linux.
For OS X and Linux Environment:
For Windows Environment:
Since I am using a Windows machine, when I activate the environment it will look like this:
The next step is to install flask on your virtual environment so that we can run the application inside our environment. Run the command:
Run your application and go to http://localhost:5000/
We finally made our web application. Now we want to show the whole world our project.
(More information on virtualenv can be found in the following guides on virtualenv and Flask Official Documentation)
To show others the project we made, we will need to learn how to use Cloud Services.
To deploy our web application to the cloud, we will use Google App Engine (Standard Environment). This is an example of a Platform as a Service (PaaS).
PaaS refers to the delivery of operating systems and associated services over the internet without downloads or installation. The approach lets customers create and deploy applications without having to invest in the underlying infrastructure (More info on PaaS check out TechTarget).
Google App Engine is a platform as a service offering that allows developers and businesses to build and run applications using Google’s advanced infrastructure — TechOpedia.
You will need a Google Account. Once you create an account, go to the Google Cloud Platform Console and create a new project. Also, you need to install the Google Cloud SDK.
At the end of this tutorial your project structure will look like this.
We will need to create three new files: app.yaml, appengine_config.py, and requirements.txt.
Content of app.yaml:
If you were to check Google’s Tutorial in the part where they talk about content of the app.yaml, it does not include the section where I wrote about libraries.
When I first attempted to deploy my simple web app, my deployment never worked. After many attempts, I learned that we needed to include the SSL library.
The SSL Library allows us to create secure connections between the client and server. Every time the user goes to our website they will need to connect to a server run by Google App Engine. We need to create a secure connection for this. (I recently learned this, so if you have a suggestions for this let me know!)
Content of appengine_config.py:
Content of requirements.txt:
Now inside our virtual environment (make sure your virtualenv is activated), we are going to install the new dependencies we have in requirements.txt. Run this command:
-t lib: This flag copies the libraries into a lib folder, which uploads to App Engine during deployment.
-r requirements.txt: Tells pip to install everything from requirements.txt.
To deploy the application to Google App Engine, use this command.
I usually include — project [ID of Project]
This specifies what project you are deploying. The command will look like this:
Now check the URL of your application. The application will be store in the following way:
My application is here: http://sal-flask-tutorial.appspot.com
From this tutorial, you all learned how to:
I learned three important things from this small project.
First, I learned about the difference between a static website and a web application
Static Websites:
Web Applications:
Server Side and Client Side:
Second, I learned about Cloud Services
Most of my previous projects were static websites, and to deploy them I used GitHub Pages. GitHub Pages is a free static site hosting service designed to host projects from a GitHub Repository.
When working with web applications, I could not use GitHub Pages to host them. GitHub Pages is only meant for static websites not for something dynamic like a web application that requires a server and a database. I had to use Cloud Services such as Amazon Web Services or Heroku
Third, I learned how to use Python as a Server Side Language
To create the server side of the web application we had to use a server side language. I learned that I could use the framework called Flask to use Python as the Server Side Language.
You can build all sorts of things with Flask. I realized that Flask helps make the code behind the website easier to read. I have made the following applications during this summer of 2018 and I hope to make more.
Personal Projects
During my internship
Here is the list of resources that helped me create this tutorial:
If you have any suggestions or questions, feel free to leave a comment.
We’ve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
1.8K 
17
1.8K claps
1.8K 
17
We’ve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
Written by
Software Developer Intern at FinityOne | Recent Computer Science Graduate from UC Irvine
We’ve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
"
https://itnext.io/microservices-with-spring-boot-and-spring-cloud-16d2c056ba12?source=search_post---------84,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
In this guide, we will build a working app who will create users and authenticate/authorize users and clients in a microservices architecture using Spring Boot, Spring Cloud, and MongoDB.I won’t use the inMemory configuration that we always see in the most guides.
This guide will be composed of 3 parts:First part: Creating the config, registry and gateway services;Second part: Create the auth service…
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/firebasethailand/%E0%B9%80%E0%B8%82%E0%B9%89%E0%B8%A1%E0%B8%82%E0%B9%89%E0%B8%99%E0%B8%81%E0%B8%B1%E0%B8%9A-firebase-cloud-firestore-%E0%B8%A3%E0%B8%B0%E0%B8%9A%E0%B8%9A%E0%B8%90%E0%B8%B2%E0%B8%99%E0%B8%82%E0%B9%89%E0%B8%AD%E0%B8%A1%E0%B8%B9%E0%B8%A5%E0%B8%97%E0%B8%B5%E0%B9%88%E0%B9%80%E0%B8%9B%E0%B8%B4%E0%B8%94%E0%B8%95%E0%B8%B1%E0%B8%A7%E0%B9%83%E0%B8%AB%E0%B8%A1%E0%B9%88%E0%B8%A5%E0%B9%88%E0%B8%B2%E0%B8%AA%E0%B8%B8%E0%B8%94%E0%B8%88%E0%B8%B2%E0%B8%81-firebase-%E0%B9%81%E0%B8%9A%E0%B8%9A%E0%B8%88%E0%B8%B1%E0%B8%94%E0%B9%80%E0%B8%95%E0%B9%87%E0%B8%A1-d001e43e2be7?source=search_post---------85,"There are currently no responses for this story.
Be the first to respond.
เมื่อเดือนตุลาคม ปี พ.ศ. 2560 ได้เปิดตัวบริการใหม่ชื่อว่า Firebase Cloud Firestore ซึ่งเป็นบริการในส่วนของ Database ที่ใช้ระบบฐานของข้อมูลแบบ NoSQL ที่เป็นแบบ Document Database และเป็นการนำเอาข้อดีต่างๆของบริการด้านฐานข้อมูลรุ่นพี่อย่าง Realtime Database มาปรับปรุงพัฒนาต่อและเพิ่มความสามารถขึ้นไปมากขึ้น เช่น การออกแบบโครงสร้างฐานข้อมูลที่ง่ายขึ้นและซับซ้อนน้อยลง (Flexibility) , การสอบถามข้อมูล (Query) ที่ง่ายขึ้น มีการกรองข้อมูล (Filter) มากขึ้นและมีการทำดัชนี (Index) ได้หลากหลายขึ้น , รองรับการขยายตัวของข้อมูลที่มากขึ้น (Scale) , เพิ่มการระบุชนิดของข้อมูล (Type) , การคัดลอกข้อมูลภายในฐานข้อมูลของเราไว้ในหลายภูมิภาค (Multi-region) และยังคงจุดเด่นของ Realtime Database ไว้อย่างครบถ้วน เช่น การรับรู้กระทำของข้อมูลในเวลาเดียวกัน (Real-time data synchronization) , การเข้าถึงข้อมูลโดยไม่มีอินเตอร์เน็ต (Offline support) , การป้องกันและสร้างกฏรักษาความปลอดภัยการเข้าถึงข้อมูล (Security & rule)
อย่างที่เราทราบว่าทุกบริการของ Firebase นั้นเป็น Serverless นั้นหมายความว่าเราไม่ต้องจัดเตรียมพวกระบบ Back-end ใดๆ เองเลย แต่จัดเตรียม SDKs ของ Platform หรือ ภาษา ที่เราจะใช้ เพียงเท่านี้เราก็สามารถเข้าถึงบริการ Cloud Firestore ได้ทันที โดย SDKs ที่ทาง Firebase เตรียมไว้ให้เราก็มีอย่างครบครัน เช่น iOS, Android, Web, Node.js, Java, Python, Go, REST และ RPC APIs. โดยโครงสร้างจะเป็นแบบ NoSQL ที่เราสามารถจัดเก็บข้อมูลในรูปแบบ Document ที่จะผูก Fields กับ Values เข้าด้วยกัน ซึ่ง Document ก็จะถูกจัดเก็บใน Collections อีกทีนึง ซึ่งเราจะสามารถสร้าง Querie From ไปจัดการเอาข้อมูลที่เราต้องการได้ในแต่ละ Document โดยในบริการ Cloud Firestore สามารถระบุชนิดของข้อมูลได้ด้วย ไม่ว่าจะเป็น ข้อความ, ตัวเลขและในส่วนของข้อมูลที่มีความซับซ้อนมีการซ้อนกันของข้อมูลมากๆ เราก็สามารถสร้างเป็น Subcollections ภายใน Document และแบ่งข้อมูลเป็นลำดับชั้นเพื่อที่จะรองรับการเติบโตของข้อมูลในอนาคตได้ โดยเราสามารถออกแบบโครงสร้างได้ทุกรูปแบบที่จะสามารถทำงานได้อย่างดีที่สุดในของแอพเรา เพิ่มเติมอีกนิด ในกระบวนการ Query ข้อมูลใน Cloud Firestore มันดูแพง, มีประสิทธิภาพและทำให้เราทำงานสะดวกขึ้นด้วย เพราะ Syntax นั้นก็สั้น แถมมันยังสามารถไปเลือกเอาข้อมูลที่เราต้องการในระดับ Document ที่แตกต่างกัน โดยที่จะไม่เอาข้อมูลของระดับที่สูงกว่าหรือต่ำกว่าติดมาด้วยและยังเพิ่มการจัดเรียงข้อมูล (Sorting), การกรองข้อมูล (Filtering), การจำกัดข้อมูล (Limits), การแบ่งหน้าข้อมูล (Paginate) ที่มีความสามารถมากกว่าเดิม ซึ่งถ้าหากเราไม่อยากที่จะไปดึงข้อมูลทุกครั้งที่ข้อมูลมีการเปลี่ยนแปลง ก็ให้เราเพิ่ม Realtime listeners เอาไว้ โดยเราจะได้รับข้อมูลใหม่เฉพาะขณะที่ข้อมูลได้มีการเปลี่ยนแปลงเท่านั้นในส่วนของการป้องกันการเข้าถึงข้อมูลใน Cloud Firestore ก็สามารถผนวกกับบริการอย่าง Firebase Authentication และเรายังสามารถสร้างกฏการใช้งานของฐานข้อมูลเราได้เพียงที่เดียวก็จะสามารถใช้การในทุกๆ Platform หรือ Identity and Access Management (IAM) สำหรับภาษาฝั่ง server
การพัฒนา Cloud Firestore แบ่งออกแบบ 5 ขั้นตอน ดังนี้
ในขั้นตอนแรกเราจะมาเริ่มสร้าง Database เพื่อที่จะใช้งาน Cloud Firestore กันก่อน โดยอันดับแรก ทุกคนจะต้องมี gmail กันก่อน ใครที่มีแล้วก็เข้าสู่ขั้นตอนต่อไปกันเลย ลำดับต่อมา เราจะเข้าไปที่เว็บ https://firebase.google.com จัดการเกี่ยวกับทุกๆบริการเกี่ยวกับ firebase ในอนาคต
Sing in ของใครของมันให้เรียบร้อย จากนั้นคลิก GO TO CONSOLE เพื่อที่จะเข้าไปสู่หน้าถัดไป
สำหรับใครที่พึ่งเข้ามาใช้งานครั้งแรก เราจะไม่มีโปรเจคใดๆ ให้กดปุ่ม เพิ่มโครงการ
จะมีหน้าต่างขึ้นมาให้เราใส่ชื่อโครงการและเลือกประเทศไทย ส่วนรหัสโครงการ ทาง Firebase จะเป็นคนสร้างให้เรา ซึ่งเราจะไปแก้ไขเองก็ได้แต่ห้ามซ้ำกับคนใช้ทั่วโลกและแก้ไขภายหลังไม่ได้ หลังจากนั้นกดปุ่ม สร้างโครงการ
หลังจากรอให้ทาง Firebase สร้างโครงการของเราสักครู่ จะพาเรามาที่หน้า https://console.firebase.google.com ซึ่งต่อไปนี้จะมีหน้าที่เราจะเห็นบ่อยๆ เพราะเป็นหน้าที่จะทำให้เราแยกไปตามบริการต่างๆ ของ Firebase ในโครงการของเรา ในส่วนของ Cloud Firestore จะอยู่ในส่วนของ Database ก็ให้เราคลิกเข้าไปที่ Database ทางด้านซ้าย
ทาง Firebase จะถามเราว่า เราจะใช้ Realtime Database ซึ่งเป็นบริการรุ่นพี่ หรืออยากจะเป็นหน่วยกล้าตายใช้ Cloud Firestore ที่เป็นรุ่นเบต้าอยู่ (ณ ขณะที่เขียนบทความ) แล้วเราจะรออะไร ? กดเลยสิครับ ลองใช้ FIRESTORE เบต้า
จากนั้นจะมีหน้าต่างขึ้นมาถามเกี่ยวกับกฏความปลอดภัยของโครงการเรา ให้เราเลือก เริ่มต้นในโหมดทดสอบ ไปก่อน แล้วกดปุ่ม เปิดใช้
ถ้าหากขึ้นหน้าตามาแบบนี้ แปลว่าเราพร้อมที่จะใช้งาน Cloud Firestore กันแล้ว
หลักจากที่เราเปิดใช้งาน Cloud Firestore ในโครงการกันแล้ว ขั้นตอนต่อไป เราจะมาลง SDKs ที่ทาง Firebase ได้เตรียมไว้ให้ โดยในตัวอย่างจะเป็นการลง SDKs ของ Platform Web โดยใน Platform ก็มีการลงคล้ายๆกันแล้วไม่ยากจนเกินไปสามารถดูรายละเอียดได้ที่ https://firebase.google.com/docs/firestore/quickstart
เพื่อความสะดวกและรวดเร็ว เราก็นำ Link ที่เก็บ File g-zip จาก CDN มาใช้ได้เลยโดยนำไปแปะที่ไฟล์ HTML
หรือใครอยากจะลงผ่าน npm ก็ใช้คำสั่ง
จากนั้นให้เราย้ายมาที่ไฟล์ javascript แล้ว Import Firebase เข้ามาใช้งาน
มาถึงขั้นตอนที่เราต้องทำการ Initialize ตัว instance ของ Cloud Firestore เพื่อที่ทาง Firebase จะได้รู้ว่าเรากำลังจะเชื่อมต่อไปที่โครงการของใครโดยใช้ Method initializeApp โดยภายในระบุเป็น Object ของ Firebase key ลงไป
ซึ่งเราจะไปเอา Firebase key จากหน้าแรกโดยคลิกที่ Overview ทางด้านซ้าย
หลังจากนั้นคลิกที่คำว่า เพิ่ม Firebase ไปที่เว็บแอปของคุณ
เราจะได้พวก Firebase key ต่างๆ ของโครงการของเรา ให้เรากดปุ่ม คัดลอก และ นำมาวางแทนที่ Code ด้านบน หากทำครบตามขั้นตอนก็เป็นอันเสร็จสิ้น การติดตั้ง SDKs ของเราเพื่อใช้งานได้แล้ว
ก่อนที่เราจะเริ่มการเขียนข้อมูลลงฐานข้อมูล เราต้องเข้าใจพื้นฐานของการเก็บข้อมูลบน Cloud Firestore กันสักหน่อย
อย่างที่เราได้เกริ่นไว้ตอนต้น ระบบฐานข้อมูลของ Cloud Firestore จะเป็น NoSQL แบบ Document ซึ่งจะไม่เหมือน ระบบฐานแบบ SQL โดยจะไม่มีตาราง ไม่มีแถว ใดๆ ทั้งสิ้น การเก็บข้อมูล ภายใน Document จะเก็บแบบ Key-value โดยแต่ละ Document จะถูกเก็บไว้ใน Collection ซึ่งใน Document สามารถมี subcollection ได้ด้วย
ลองให้เรานึกภาพว่าเรามีเอกสาร 1 แผ่น ข้างในนั้นก็จะมีเนื้อหาอยู่ภายในและเอกสารหลายๆแผ่น ก็จะเก็บอยู่ในแฟ้มอีกทีนึง
โดยที่เราไม่ต้องไปออกแบบตารางของฐานข้อมูลไว้ก่อนเหมือนอย่างระบบฐานแบบ SQL แต่เราสร้างสามารถสั่งเขียนข้อมูลลงไปได้เลย โดยระบุ Collection และ Document ถ้าหากไม่เคยมี Collection และ Document มาก่อนทาง Cloud Firestore จะสร้างให้เราเองโดยอัตโนมัติ ซึ่งทาง Cloud Firestore ก็ได้บอกไว้ว่า เหมาะสำหรับการเก็บข้อมูลโดยมี Collection จำนวนขนาดใหญ่ และ Document ขนาดเล็ก
เอาหล่ะเราลองมาดูตัวอย่างของการเก็บข้อมูลแบบต่างๆกัน
Document เป็นการเรียนชื่อแทนหน่วยการเก็บของข้อมูลใน Cloud Firestore ภายในจะประกอบไปด้วย ชื่อของ Document , ชื่อของ key และ ค่าข้อมูล (value) โดยชื่อของ Document ห้ามซ้ำกัน ซึ่งใน Cloud Firestore สามารถให้เราระบุ Type ของข้อมูลได้คือ boolean, number, string, geo point, timestamp, array, object, reference และ null
เช่นดังรูป
ชื่อของ Document คือ User1
ชื่อของ key คือ Username , Class , born
ค่าข้อมูล คือ “Thanongkiat” (string) , “highschool6” (string) , 2049 (number)
Collection เป็นการเรียกชื่อแทนของการเก็บหลายๆเอกสารไว้ด้วยกัน เช่น เราจะเก็บ ข้อมูลของ user หลายๆ Document ไว้ด้วยกัน จึงตั้งชื่อ Collection ว่า Users ซึ่งใน Collection เดียวกันเราสามารถใส่ข้อมูลที่แตกต่างชนิดกันในแต่ละ Key แต่ละ Document ก็ได้ โดยในแต่ละ Key และ Document จะมีอิสระในการใส่ข้อมูลต่างๆลงไป แต่เราควรใส่ข้อมูลในแต่ละ Key ของ Document เป็นประเภทเดียวกันเพราะจะทำให้การค้นหาและการจัดเรียงลำดับของข้อมูลนั้นง่ายขึ้น
ข้อที่เราควรรู้ ของการเก็บข้อมูลแบบ Collection คือ ชื่อของ Collection ห้ามซ้ำกัน, ห้ามมีข้อมูลดิบใดๆหรือ Collection อื่นๆ อยู่ใน Collection ภายในจะเก็บได้เพียง Document เท่านั้น, เราไม่ต้องสร้างหรือลบ Collection ถ้าหากมีการเพิ่มข้อมูลลงไปโดยระบุชื่อ Collection และ Document จะทำให้ Collection นั้นถูกสร้างขึ้นโดยอัตโนมัติและถ้าหาก Collection นั้น ไม่มี Document ใดๆ เหลืออยู่เลย Collection จะถูกลบโดยอัตโนมัติเช่นกัน
หากว่าเราอยากทำลำดับชั้นของข้อมูลทาง Cloud Firestore ก็ตอบสนองสิ่งนั้น โดยเราสามารถสร้าง Subcollection ไว้ข้างใน Document เท่านั้น ซึ่งใน Subcollection ก็สามารถมี Document ได้อีก เราซึ่งสามารถสร้าง Subcollection ของ Subcollection ไปได้อีกเรื่อยๆ โดยทาง Cloud Firestore ได้บอกไว้ว่าสามารถซ้อนกันไปได้เยอะสุดคือ 100 ลำดับชั้น เรียกได้ว่าถ้าเปลี่ยนมาเป็นถุงทรายน้ำคงไม่ท่วมกรุงเทพแล้ว ฮ่า
มาถึงวิธีที่เราจะเข้าถึงข้อมูลกัน เราสามารถสร้าง Path references ไว้ได้ สมมุติว่าเราจะเข้าถึงข้อมูลใน Document User1 เราก็เพียงระบุชื่อ Collection ตามด้วย ชื่อ Document ที่เราจะเข้าถึงดังนี้
หรือต้องการสร้าง Path ไว้เพื่อเข้าถึงเฉพาะแค่ Collection ก็สามารถทำได้
โดยทาง Cloud Firestore ได้บอกไว้ว่าสามารถ Path references ไว้ก่อนเยอะๆได้เพื่อความสะดวกในการใช้งานโดยไม่ใช้ทรัพยากรมากนัก (เทียบเท่ากับ Object ที่มีข้อมูลขนาดเล็กๆเท่านั้น) และ Path references ก็ไม่ได้ไปยุ่งเกี่ยวกับพวก Network operations เลยด้วย ว่ากันง่ายๆคือ แค่ประกาศ Path references ไว้เฉยๆ ไม่ได้ใช้อินเตอร์เน็ตไปเชื่อมต่อกับเซิพเวอร์เลยนั้นเอง
สมมุติว่าข้อมูลเรานั้นซ้อนกันลึกลงไปหลายๆชั้น การที่จะมา .collection() และ .doc() ทุกๆครั้งก็จะไปก็จะเป็นการเสียเวลาจนเกินไปโดยทาง Cloud Firestore ก็ได้ทำ Syntax ให้เราเข้าถึงได้ง่ายขึ้น
โดยจะเป็นการสลับกันระหว่างชื่อของ Collection และ ชื่อของ Document ไปเรื่อยๆโดยจะเริ่มต้นด้วยชื่อของ Collection ก่อนเสมอ
หลังจากที่เราเข้าใจพื้นฐานของการเก็บข้อมูลบน Cloud Firestore เรียบร้อยแล้ว ในส่วนนี้จะเป็นการอธิบายประเภทของโครงสร้างหลักๆ ใน Cloud Firestore ซึ่งจะแบ่งการออกแบบโครงสร้างของฐานข้อมูลออกเป็น 3 แบบใหญ่ๆ
การออกแบบโครงสร้างแบบ Nested data in documents จะเป็นการที่เราจะสร้างโครงสร้างย่อยๆลงไปใน Document เป็นชนิด Object หรือ Array
2. Subcollections
การออกแบบโครงสร้างแบบ Subcollections จะเป็นการที่เราจะสร้าง Subcollection ลงไปใน Document แต่จะมีจุดเริ่มต้นมาจาก root collection เดียวกัน
3. Root-level collections
การออกแบบโครงสร้างแบบ Root-level collections จะเป็นการที่เราจะสร้าง root collection หลายๆ ชนิด ซึ่งข้อมูลภายในแต่ละ collection อาจจะเกี่ยวข้องกันหรือไม่ก็ได้
โดยการออกแบบโครงสร้างของฐานข้อมูลนั้นเป็นเรื่องที่ละเอียดอ่อน แต่กลับมีความสำคัญมากกับตัวแอพพลิเคชั่นของเรา ถ้าหากเราออกแบบโครงสร้างของฐานข้อมูลไม่ดีจะมีผลต่อการทำงานของเราอย่างมาก อาจทำให้ประสิทธิภาพลดลงหรือการจัดการข้อมูลภายในเกิดความยากลำบากมากขึ้น โดยแต่ละโครงสร้างก็จะมีข้อดี ข้อเสียแตกต่างกันไป การเลือกใช้โครงสร้างแต่ละชนิดควรจะมีเหตุผลประกอบเพื่อที่จะเลือกใช้ด้วย ซึ่งจะขอยกเรื่องของนี้ไปไว้เป็นบทความเฉพาะซึ่งจะมีเจาะข้อดีข้อเสีย ตัวอย่างสถานการณ์ที่ควรจะใช้งานแต่ละชนิด
การเพิ่มข้อมูลไปที่ Cloud Firestore จะมีหลายกรณีแตกต่างกันไปตามสถานการณ์ของเรา โดยจะมีหลายวิธีในการเพิ่มข้อมูล
โดยเราจะใช้ set() และ add() method เพื่อที่จะเขียนข้อมูลไปที่ Cloud Firestore
การเขียนข้อมูลด้วย Method Set() มีกฏอยู่ว่า
2. การเขียนด้วย add()
การเขียนข้อมูลด้วย Method add() ทาง Cloud Firestore จะสร้าง Index ให้เราโดยอัตโนมัติ
แต่เรื่องมันไม่จบง่ายๆ แค่นั้นหน่ะสิ๊ ใครที่ข้ามหรืออ่านผ่านๆ โปรดระวังให้ดี เพราะทาง Cloud Firestore เน้นย้ำเลยนะว่ามันไม่เหมือนกับ Push ID ใน Realtime Database ซะทีเดียวนะ การสร้าง Index ของ Cloud Firestore มันจะไม่ได้เรียงลำดับให้นะ บางทีเรา add ไปทีหลัง อาจจะไปอยู่ลำดับบนๆ ก็ได้ ถ้าอยากให้มันเรียงลำดับกันด้วย ก็ให้ระบุชื่อของ Document โดยใช้ Timestamp จะดีกว่า
3. สร้าง Document เปล่าๆ ด้วย doc()
การสร้าง Document เปล่าๆ แล้วค่อยมาเพิ่มก็ข้อมูลทีหลัง ก็ทำได้ไม่ยาก เพียงแค่ใช้ Method doc() ซึ่ง Cloud Firestore ก็จะสร้าง Index ให้เองเหมือน add() ทุกประการ
เราก็สามารถเลือกใช้วิธีการเพิ่มข้อมูลทั้ง 3 วิธีได้ตามสถานการณ์จะนำพาไปโดยไม่มีความแตกต่างกันด้านประสิทธิภาพ
การแก้ไขข้อมูลคือการที่เราอยากจะเปลี่ยนแปลงข้อมูลบางส่วนของเอกสาร โดยที่จะไม่ไปเขียนทับทั้งหมด ซึ่งเราจะใช้ Method update()
ถ้าหากอยากจะ update ข้อมูลของก้อน Object ก็สามารถทำได้โดยใช้ . (เครื่องหมายจุด)
หรือจะเป็น update ของข้อมูล Timestamp
ก่อนเราจะมาเรียนรู้การเขียนแบบ Transactions and Batched ต้องขอเท้าความไปถึงความหมายของแต่ละคำกันก่อน
เอาหล่ะหลังจากเรียนรู้ความหมายของทั้งคู่ไปแล้ว เรามาดู Syntax ของการเขียนข้อมูลแบบ Transactions and Batched กัน
ขั้นตอนแรกคือเรียก Method runTransaction() จากนั้นก็ปฏิบัติตามขั้นตอนที่เคยอธิบายไว้ข้างบน คือ get() + doOperating() + update()
หรือว่าจะนำผลลัพธ์ที่ได้จากขั้นตอน doOperating() มาทำอะไรต่อก็สามารถทำได้
2. Batched
ขั้นตอนแรกคือเรียก Method batch() จากนั้น ก็นำมา .set() .update() .delete() หรือตามที่เราต้องการพอเสร็จแล้วก็ .commit() เพียงเท่านี้ คำสั่งของเราจะถูกส่งไปเป็นชุดพร้อมกันทันที
การลบข้อมูลของ Cloud Firestore โดยใช้ Method delete() โดยก็มีการลบ 3 กรณี
การลบ Document ก็เพียงแต่นำ .delete() ต่อท้าย .doc() ที่เราต้องการจะลบ
2. การลบ Field
การลบ Field ต้องกระทำผ่าน Method update() ก่อนแล้วจึงระบุ Key ที่เราต้องการจะลบแล้วระบุค่าเป็น firebase.firestore.FieldValue.delete()
3. การลบ Collection
การที่เราจะลบ Collection ถ้าหากจำกฏเดิมที่เคยบอกด้านบนของบทความได้ว่า หากไม่มี Document ใด เหลืออยู่ใน Collection นั้น Collection จะถูกลบไปเอง นั้นก็หมายความว่า ถ้าเราอยากจะลบ Collection เราจึงต้องลบ Document ออกให้หมดนั้นเอง ซึ่งกรณีนี้ยังเหมารวมไปถึง การลบ Subcollection ด้วย
โดยมีข้อกำหนดไว้คือการที่เราลบข้อมูลใน Document ใด แต่หากใน Document นั้นมี Subcollection จะไม่ถูกลบไปด้วย เราต้องไปไล่ลบด้วยตัวเองผ่านวิธีการที่กล่าวไว้ข้างบน
ในส่วนนี้จะพูดถึงการดึงข้อมูลและสอบถามข้อมูลจาก Cloud Firestore โดยจะมีการอยู่ 2 วิธี โดยทั้งคู่จะสามารถใช้ได้ทั้งการดึงข้อมูลและการสอบถามข้อมูล
การรับข้อมูลเพียงครั้งเดียวจะเป็นการรับข้อมูลเมื่อเรามีจุดประสงค์ที่จะไม่ต้องการรับรู้การเปลี่ยนแปลงของข้อมูล เมื่อ ณ ขณะนั้นข้อมูลมีค่าเป็นอะไรก็จะได้ค่านั้นมา หากมีการเปลี่ยนแปลงข้อมูลในภายหลัง เราต้องเป็นผู้จัดการรับข้อมูลล่าสุดเอง โดยวิธีการรับข้อมูลเพียงครั้งเดียวจะใช้ Method get()
หากต้องการรับข้อมูล Document เดียว ก็เพียงนำ Path reference ที่สร้างไว้มาต่อท้ายด้วย .get() โดยนำ Callback ที่ได้รับมา .data() เพื่อแกะเอาข้อมูลข้างในออกมา
หากต้องการรับข้อมูลหลายๆ Document โดยมีเงื่อนไขต่างๆ เราก็จะใช้ Method where() มาช่วยในการ Filter ระบุเงื่อนไขในการสอบถามข้อมูลของเรา จากนั้นก็ใช้ .get() เพื่อรับข้อมูลมา
หากต้องการรับข้อมูลทุก Document ใน Collection ก็เพียงแต่นำ Method get() ไปต่อท้ายจาก .collection() ที่เราต้องการได้เลย หรือ สามารถใช้ .where() แบบไม่ระบุ Parameter ของตัวกรองภายในก็ใช้ได้เหมือนกัน
โดย Method where() เราจะไปอธิบายกันละเอียดๆ ส่วนของการสอบถามข้อมูลด้านล่าง
2. การรับข้อมูลแบบ Realtime update
การรับข้อมูลแบบ Realtime update จะเป็นการรับข้อมูลเมื่อเรามีจุดประสงค์ที่จะต้องการรับรู้การเปลี่ยนแปลงของข้อมูล เมื่อ ณ ขณะที่ข้อมูลเกิดการเปลี่ยนแปลงจะมีการรับข้อมูลที่เกิดการเปลี่ยนแปลงโดยอัตโนมัติ โดยในครั้งแรกที่มีการรับข้อมูลจะสร้าง Initialize instance และทุกครั้งที่ข้อมูลมีการเปลี่ยนแปลงก็จะส่ง Callback ให้กับเราซึ่งเป็น Iistener โดยผ่าน Method onSnapshot()
ณ ขณะที่เขียนบทความอยู่ SDKs ที่ทาง Cloud Firestore มีให้จะยังไม่รองรับการรับข้อมูลแบบ Realtime update ในภาษา Java, Python และ Go นะครับ
หากต้องการรับข้อมูล Realtime update แบบ Document เดียว ก็เพียงนำ .onSnapshot() ไปต่อท้าย Path reference ที่เราสร้างไว้และนำ Callback ที่ได้รับมา .data() ดังเดิมเหมือนขั้นตอนรับข้อมูลครั้งเดียว
หากต้องการรับข้อมูลหลายๆ Document โดยมีเงื่อนไขต่างๆ เราก็เปลี่ยนจากใช้ Method .get() เป็น .onSnapshot() เท่านั้นเองที่เหลือเหมือนเดิม
แต่หากเราอยากจะแยกกรณีของการที่ข้อมูลมีการเปลี่ยนแปลง เช่น เพิ่ม , แก้ไข หรือ ลบ ก็สามารถทำได้โดยใช้ Property .docChanges ของ Callback แล้วจึงนำมา For loop เพื่อวนหากรณีที่เราต้องการ
โดยในบางครั้งหากเราเป็นผู้ที่เปลี่ยนแปลงข้อมูลที่เรากำลังเป็น Iistener อยู่พอดีอาจจะเกิดเหตุการณ์ที่เราจะได้รับการแจ้งเตือนข้อมูลนั้นก่อนการที่ข้อมูลจะถูกเขียนจริงๆบน Server จากเหตุที่อินเตอร์เน็ตของผู้ส่งที่ล่าช้า จึงเกิดเหตุการณ์ที่เรียกว่า “Latency compensation” ทำให้เราจะได้รับการอัพเดทข้อมูลนั้นถึงสองครั้งในช่วงเวลาที่ต่างกัน โดยทาง Cloud Firestore ก็จะมีวิธีให้เราได้ตรวจสอบว่าข้อมูลนั้นรับนั้นเป็นข้อมูลที่ส่งมาจาก Server หรือจาก Local เอง
แต่หากว่าเราใช้ ภาษา JavaScript เราสามารถใช้Promise เพื่อป้องกันการเกิดเหตุการณ์นี้ได้ หรือในภาษา Swift ก็จะมี Completion callback ใช้ให้ได้เหมือนกัน
เมื่อเราได้ทำการ Iistener การรับข้อมูลแบบ Realtime update แล้วนั้นหากว่าเราไม่มีความต้องการที่จะเรียกใช้การรับข้อมูลแล้ว เราควรที่จะยกเลิกจากสถานะการเป็น Iistener เพื่อหยุดการใช้งาน bandwidthในการรับการเปลี่ยนแปลงข้อมูล
และในบางครั้งการรับข้อมูลของเราก็ไม่ได้สำเร็จหรือผ่านไปด้วยดีเสมอไป เช่น เกิดจากการป้องกันความปลอดภัย หรือ เราสอบถามข้อมูลอย่างไม่ถูกวิธี เพราะฉะนั้นเราสามารถจะจัดการ Error ที่เกิดขึ้นกับเหตุการณ์ต่างๆได้
มาถึงตอนพระเอกของเราแล้ว โดยสิ่งที่ทำให้ Cloud Firestore น่าใช้กว่า Firebase Realtime Database ก็เพราะตัวมันเองมีประสิทธิการสอบถามข้อมูล (Query) ที่ดีกว่าเดิม ถึงแม้จะยังไม่ดีที่สุดแต่ก็ทำให้เรามีความหวังมากขึ้นเพราะ ณ ขณะนี้ก็ยังเป็นเวอร์ชั่นเบต้า อนาคตจะต้องมีการปรับปรุงในรายละเอียดและเพิ่มฟีเจอร์ต่างๆเพิ่มมาอย่างแน่นอน
1. การสร้างการสอบถามข้อมูลอย่างง่ายและแบบผสมใน Cloud Firestore
เอาหล่ะอย่างที่เราเห็นมาแล้วข้างบนว่ามี Method where() เพื่อใช้ในการ Filter ข้อมูลต่างๆ เรามาดูกันก่อนว่า Method where() ภายในประกอบตัว Parameter อะไรบ้าง
ภายใน where() จะเป็นประกอบไปด้วย Parameter 3 ตัวคือ
สรุปแล้วรูปร่างของ where() จะมีหน้าตาเป็นแบบนี้
where(fieldPath, opStr, value)
ถ้าหากเป็น Platform หรือ ก็จะมีหน้าตาคล้ายๆ แต่ว่าเปลี่ยน Syntax เท่านั้นเอง
ต่อไปก็ลองมาดูตัวอย่างการใช้งานแบบง่ายๆ กัน สมมุติว่าข้อมูลตัวอย่างเป็นดังนี้
หากเราต้องการค้นหา Document ของเมืองที่อยู่ในรัฐ California ย่อว่า CA
หากต้องการค้นหา Document ของเมืองหลวงทั้งหมด
หรือตัวอย่างอื่นๆ
ต่อไปจะเป็นตัวอย่างของการสอบถามแบบผสมหลายเงื่อนไขดูบ้าง โดยวิธีการก็ยังใช้ Method where() เหมือนเดิมแต่ให้เดิมต่อท้าย where() ตัวแรกไปได้เลย โดยจะมีค่าเท่ากับ logical AND โดยถ้าหากเราใช้การระบุเงื่อนไขแบบเฉพาะเจาะจง (==) กับ เป็นช่วง (“<”, “<=”, “==”, “>”, “>=”) พร้อมกันเราต้องไปจัดการเรื่อง Index ก่อน ซึ่งจะกล่าวถึงวิธีการในหัวข้อต่อไป
โดยการสอบถามแบบผสมหลายเงื่อนไขจะมีกฏอยู่ว่า เราจะระบุเงื่อนไขแบบช่วงได้แค่ Field เดียว เช่น
แบบที่ถูกต้อง
แบบที่ไม่ถูกต้อง
เพราะมีการระบุเงื่อนไขแบบช่วงถึง 2 Fields คือ “>=” ที่ state และ “>” ที่ population
2. การเรียงและจำกัดข้อมูล
หลังจากที่ดูตัวอย่างการ Filter โดยใช้ Method where() ไปเรียบร้อยแล้ว Cloud Firestore ก็ยังมีการ Query แบบการเลือกเรียงข้อมูล (OrderBy) ตาม Field ที่ระบุ หรือ การจำกัด (Limit) ผลลัพธ์ตามจำนวน
โดย Method orderBy() จะประกอบไปด้วย Parameter 2 ตัว
สรุปแล้วรูปร่างของ orderBy() จะมีหน้าตาเป็นแบบนี้
orderBy(fieldPath, directionStr)
และ Method limit() จะประกอบไปด้วย Parameter 1 ตัว
สรุปแล้วรูปร่างของ limit() จะมีหน้าตาเป็นแบบนี้
limit(limit)
ทีนี้มาดูตัวอย่างการใช้งาน orderBy() และ limit()
หากเราต้องการชื่อของ 3 เมืองแรก โดยการเรียงตามตัวอักษร
หรือเปลี่ยนเป็น 3 เมืองท้ายสุด
หากเราต้องการเรียงผลลัพธ์Document ที่เรียงตามรัฐและแต่ละรัฐเรียงตามจำนวนประชากรจากมากไปน้อย
โดยเราจะสามารถทำเงื่อนไขจาก Method where() มาใช้ร่วมกันได้ด้วย
หากต้องการผลลัพธ์ Document ที่ระบุเงื่อนไขของประชากรมากกว่าหนึ่งแสนโดยเรียงจากน้อยไปมากและจำกัดจำนวน 2 Document
อย่างไรก็ตามการใช้งาน orderBy() และ limit() ร่วมกับ where() ก็มีกฏอยู่ว่าถ้าเป็นการระบุเงื่อนไขเป็นช่วงใน where() และ orderBy() จะต้องเป็น Field เดียวกันด้วย เช่น
แบบที่ถูกต้อง
แบบที่ไม่ถูกต้อง
เพราะมีการระบุเงื่อนไขเป็นช่วงโดยใช้ “>” และ Field ที่ระบุใน where() คือ population ซึ่งใน orderBy() เป็น country
3. การสอบถามข้อมูลโดยตัวชี้เป้าหมายและแบ่งช่วง
เราได้เรียนรู้การสอบถามข้อมูลของ Cloud Firestore โดยใช้ Method where(), orderBy() และ limit() กันไปแล้ว ในส่วนนี้จะพูดถึงการที่เราจะ Filter ผลลัพธ์ของการสอบถามข้อมูลของเราโดยวิธีการระบุจุดเริ่มต้นหรือจุดสิ้นสุดของการค้นหาซึ่งใช้การชี้เป้าหมายและการแบ่งช่วงของผลลัพธ์ของการสอบถามออกเป็นกลุ่มย่อยๆ โดยเราจะใช้ Method startAt(), startAfter(), endAt() และ endBefore()
Method startAt() จะประกอบไปด้วย Parameter 1 ตัว
สรุปแล้วรูปร่างของ startAt() จะมีหน้าตาเป็นแบบนี้
startAt(snapshotOrVarArgs)
Method startAfter() จะประกอบไปด้วย Parameter 1 ตัว
สรุปแล้วรูปร่างของ startAfter() จะมีหน้าตาเป็นแบบนี้
startAfter(snapshotOrVarArgs)
Method endAt() จะประกอบไปด้วย Parameter 1 ตัว
สรุปแล้วรูปร่างของ endAt() จะมีหน้าตาเป็นแบบนี้
endAt(snapshotOrVarArgs)
Method endBefore() จะประกอบไปด้วย Parameter 1 ตัว
สรุปแล้วรูปร่างของ endBefore() จะมีหน้าตาเป็นแบบนี้
startAfter(snapshotOrVarArgs)
4. การชี้เป้าหมายการสอบถามข้อมูล
สมมุติเรา มีตัวอักษร A-Z หากเราใช้ startAt(A) ก็จะได้ผลลัพธ์เป็น A-Z แต่หากใช้ startAfter(A) ก็จะได้ผลลัพธ์เป็น B-Z ซึ่งการใช้ endAt() และ endBefore() ก็จะให้ผลลัพธ์คล้ายกันแต่สลับเป็นการระบุจุดสิ้นสุดแทน
โดยกฏการจะใช้ Method startAt(), startAfter(), endAt() และ endBefore() จะต้องอยู่ต่อท้าย Method orderBy() เสมอ
ตัวอย่างเช่นหากต้องการผลลัพธ์ Document ที่เรียงตามจำนวนประชากรจากน้อยไปมากโดยเริ่มที่ 1 ล้านคนเป็นต้นไป
หรือต้องการผลลัพธ์ Document ที่เรียงตามจำนวนประชากรจากน้อยไปมากโดยสิ้นสุดที่ 1 ล้านคน
หรือหากเราจะใช้ผลลัพธ์จากการรับข้อมูลมาใช้ในการระบุตัวชี้เป้าหมายโดยการระบุค่าที่ได้จาก Callback ในตัวชี้เป้า เช่น ต้องการผลลัพธ์ Document ของเมืองที่มีประชากรใหญ่กว่าเมือง San Francisco
5. การแบ่งช่วงการสอบถามข้อมูล
การแบ่งช่วงการสอบถามข้อมูลจะเป็นการแบ่งการผลลัพธ์ของเราออกเป็นส่วนย่อยๆ โดยจะมีประโยชน์ในการที่เราต้องการจะแบ่งหน้าของการแสดงผลลัพธ์หรือแบ่งจำนวนการแสดงผลลัพธ์เป็นจำนวนที่กำหนดไว้ซึ่งใช้จุดเริ่มต้นหรือจุดสิ้นสุดจากครั้งก่อนมาเป็นตัวชี้เป้าหมายในครั้งต่อไปโดยเราจะใช้ Method limit() เพิ่มไปกับการชี้เป้าหมายการสอบถามข้อมูล
ตัวอย่างเช่นต้องการแสดงผลลัพธ์จากการสอบถามข้อมูลโดยเรียงจากจำนวนประชากรจากน้อยไปมากทีละ 25 และให้ผลลัพธ์ตัวสุดท้ายเป็นจุดเริ่มต้นในครั้งต่อไป
6. การชี้เป้าหมายการสอบถามข้อมูลแบบหลายเป้าหมาย
หากการระบุเป้าหมายของการสอบถามข้อมูลยังไม่ได้ผลลัพธ์ที่ตรงกับจุดประสงค์ที่เราตั้งไว้เราสามารถเพิ่มการชี้เป้าหมายเพิ่มเข้าไปได้อีกเพื่อที่การระบุเป้าหมายจะทำได้ละเอียดมากขึ้นและให้ผลลัพธ์ตรงกับที่เราต้องการมากขึ้น
ตัวอย่างเช่นหากเราต้องการค้นหารัฐที่มีมหาลัยวิทยาลัย “Springfield” แต่เผอิญมหาลัยนี้เปิดในหลายรัฐ
หากเราต้องการเริ่มต้นตั้งแต่ Springfield ที่เริ่มต้นด้วย Missouri
ในการเก็บข้อมูลของ Cloud Firestore นั้นต้องการที่เราจะต้องไปกำหนดการจัดการกับ Index (ดัชนี) ของข้อมูลด้วยเพื่อให้ได้ประสิทธิภาพการค้นหาที่ดีที่สุด จริงๆ แต่ใน Realtime Database ก็มีการแนะนำให้เราไปจัดการกับ Index เหมือนกันแต่ไม่บังคับ ซึ่งพอมาเป็น Cloud Firestore ทำให้เรามีตัวเลือกของการสอบถามข้อมูลมากขึ้นเราจึงต้องจัดการกับ Index ด้วยซึ่งทาง Cloud Firestore ก็ได้กล่าวไว้ว่า หากเราต้องการสอบถามโดยเงื่อนไขเพียงครั้งเดียวหรือว่ากันง่ายๆคือใช้ Method where() ครั้งเดียว เราไม่จำเป็นต้องจัดการกับ Index ก็ได้ แต่ว่าถ้าเราต้องการสอบถามโดยเงื่อนไขแบบผสมเราจึงถูกบังคับต้องจัดการกับ Index ด้วย ไม่เช่นนั้นจะไม่สามารถสอบถามข้อมูลได้ แต่ยังไงเสียเราก็ต้องเลือกการเรียงข้อมูลจากน้อยไปมาก หรือ มากไปน้อยถึงแม้ว่าเราจะทำการสอบถามแบบใช้เงื่อนไข “==” ก็ตาม
โดยการเพิ่ม Index นั้น เราต้องเข้าไปทำใน Firebase console
ถ้าเราต้องการจะ ลบ ก็เพียงนำเม้าส์ไปแช่ไว้แถวๆด้านขวาของรายการ Index ที่เราต้องการจะลบก็จะมีเมนูแสดงขึ้นมา
การป้องกันและความรักษาปลอดภัยของข้อมูลใน Cloud Firestore ก็ได้มีการออกแบบให้เราสามารถกำหนดกฏของความปลอดภัยต่างๆได้โดยผ่าน Firebase Console ได้ทันที ซึ่งหากเราใช้ Cloud Firestore คือเราสามารถมาทำเรื่องของการป้องกันและความรักษาปลอดภัยของข้อมูลเพียงที่เดียวก็สามารถใช้ได้ทั้งหมดโดยไม่ต้องมาทำแยกกันสำหรับ Web และ Mobile ส่วนฝั่ง Server ก็สามารถใช้ IAM ใน Google Cloud Platform มาจัดการเรื่องนี้สำหรับ Cloud Firestore ได้อีกด้วย
ในส่วนของบทความนี้เราจะกล่าวถึงในส่วนของพื้นฐานการใช้ Security & rule ของเฉพาะเว็บและมือถือก่อน ส่วนในบทความต่อไปไว้เราจะมาลงลึกของการใช้งานจริงโดยเฉพาะกันอีกที
การป้องกันและความรักษาปลอดภัยของข้อมูลใน Cloud Firestore สามารถผนวกกับ บริการอย่าง Firebase Authentication และ การเขียนกฏความปลอดภัยใน Cloud Firestore ด้วย Firebase Console ในการตรวจสอบการระบุตัวตน , สิทธิ์ในการเข้าถึงข้อมูลและการตรวจสอบความถูกต้องของข้อมูลโดยที่เราไม่ต้องมีการวางระบบ Infrastructure หรือ การเขียน Code ฝั่ง Server อะไรเองเลยด้วย โดยเราก็สามารถกำหนดได้เลยว่าจะให้ User ที่ผ่านการ Authentication มาให้คนไหนสามารถที่จะเข้าถึงข้อมูลส่วนไหนได้บ้างและข้อมูลที่เขียนลงไปที่ฐานข้อมูลต้องผ่านการตรวจสอบความถูกต้องด้วย
เริ่มต้นการเขียน Security & rule เราต้องไปเปิด Firebase Console ก่อนเพื่อเริ่มเขียนกันครับ
ให้เราไปที่คลิกที่เมนู Database ทางซ้ายมือ หลังจากนั้นเลือก Cloud Firestore เข้ามาแล้วคลิกที่เมนู กฏ จะปรากฏดังภาพ
หากใครที่ได้เลือกตามที่ผมบนไว้ข้างบนกฏของฐานข้อมูลปัจจุบันของเราจะเป็นแบบที่ไม่ว่าใครๆสามารถมาเขียนหรืออ่านข้อมูลของเราได้ทั้งหมดเลย
Syntax ที่ควรรู้ในการเขียน Security & rule
ก่อนที่เราจะมาเขียนกฏของฐานข้อมูลของเราเอง เราจะมาเข้าใจ Syntax ต่างๆ ของ Security & rule กันก่อน
ทีนี้ลองมาดูตัวอย่างการสร้างกฏแบบต่างๆกัน
การสร้างกฏแบบอ้างอิงไปที่ Path myCollection/myDocument โดยอนุญาติให้เข้าถึงการเขียนหรืออ่านหากผ่านเงื่อนไข และ อีกกฏคือการสร้างแบบอ้างอิงในทุกๆ Document ภายใน Path myCollection โดยอนุญาติให้เข้าถึงการเขียนหรืออ่านหากผ่านเงื่อนไข
โดยหากเราจะใช้เงื่อนไขของการ Authentication เข้ามาตรวจสอบผู้ที่มีสิทธิ์ก็สามารถทำได้โดยใช้ request.auth หรือถ้าหากเป็นเงื่อนไขเกี่ยวกับเวลาก็ใช้ request.time
หากเราต้องการจะตรวจสอบจากข้อมูลที่เขียนเข้ามาก็ใช้คำสั่ง resource.data.field และเรายังสามารถอ้างอิงไปยังข้อมูลที่อยู่ใน Documents อื่นได้ด้วย get(/myCollection/otherDocument).data.field
หากเราต้องการให้ผู้ใช้คนอื่นมีสิทธิ์เข้าถึงข้อมูลของเฉพาะ User ตัวเอง หรือให้ผู้ใช้ที่ผ่านการ Authentication สามารถเข้าถึงข้อความในห้องการพูดคุยได้เท่านั้น
ในส่วนต่อมาเราจะมาลงลึกกันไปอีกหน่อย ถ้าหากเราอยากจะเขียน Security & rule กับข้อมูลที่มีลำดับชั้นเราจะต้องเขียนกฏอย่างไรได้บ้าง ? ก่อนอื่นเราต้องเข้าใจในทางทฤษฎีว่า Collection และ Document ทั้งหมดที่อยู่ใน Database จะขึ้นต้นด้วย /databases/ชื่อของDatabase/documents แต่ Path นี้จะไม่ปรากฏในฝั่งของ Client เช่น ต้องการเข้าถึง employees/stanley แต่ที่เขียนใน Security rule จริงๆ จะเป็น databases/ชื่อของDatabase/documents/employees/stanley โดยหากเราเข้าใจพื้นฐานตรงส่วนนี้ก็จะทำให้เราสามารถเขียนกฏเฉพาะเจาะจงไปที่ Document ไหนในฐานข้อมูลที่เราต้องการก็ได้
มีการเขียนกฏอีกวิธีที่เราสามารถเขียนกฏซ้อนกันเข้าไปได้โดยไม่ต้องอ้างอิง Path ตั้งแต่เริ่มต้น แต่ให้เราเปลี่ยนวิธีการเขียนไปไว้ข้างในนั้นอีกทีเลย เช่น /databases/{database}/document/spaceships/serenity/crew/jayne ซึ่งตามตัวอย่างจะเหมือนกันเราอ้างอิงไปที่ Path โดยตรง
กฏใน Cloud Firestore นั้นเป็นแบบ Rules don’t cascade โดยในกฏที่เราได้กล่าวมาทั้งหมดนั้นจะไม่ไปมีผลต่อ Subcollection หากเราไม่ได้ใช้ =** ต่อท้าย เช่นหากเราจะเขียนกฏเพื่อเข้าถึง /spaceships/serenity แต่เราจะไม่สามารถเข้าถึง /spaceships/serenity/crew/hoban ได้นั้นเอง
โดยสิ่งที่เราต้องพึงระวังอีกอย่างนึงก็คือ หากเราเขียนกฏในระดับ Collection กฏนั้นก็จะไม่ไปมีผลกับ Document ภายในเช่นกัน
เพราะฉะนั้นหากเราเขียนกฏตามตัวอย่างข้างบนก็จะไม่มีใครสามารถเข้าถึง Document ภายใน spaceships ได้นั้นเอง
มาถึงกรณีที่เราจะได้ใช้ในการเขียนกฏบ่อยๆ นั้นก็คือการเขียนกฏแบบ wildcards เพราะในส่วนใหญ่เราต้องการที่จะให้สิทธิ์ภายใน Collection เดียวกันเหมือนกันโดยที่เราไม่ต้องมาเขียนแยกไปในแต่ละ Document ซึ่งการเขียนลักษณะแบบนี้จะไม่ได้เจาะจงไปใน Document ใด Document นึงภายใน Collection เหล่านั้น โดยหากชื่อของ Document คือ “enterprise” ค่าของ String ที่อยู่ภายใน wildcards คือ spaceship หากมีการอ้างอิงถึงก็จะมีค่าเท่ากับ enterprise
ถ้าหากเราได้เพิ่ม =** ต่อท้าย String ที่อยู่ภายใน wildcards เข้าไปจะทำกฏจะไปมีผลต่อ ทุก Document และ Subcollection ที่อยู่ด้านล่างลงไปอีกด้วย เช่น เขียนกฏดังตัวอย่างจะทำให้ spaceships/serenity และ spaceships/enterprise/crew/troi มีผลไปด้วยนั้นเอง
แต่ถ้าหาก Client ต้องการ Update ข้อมูลใน spaceships/enterprise/crew/troi จะทำให้ spaceship มีค่าเป็น enterprise/crew/troi แต่สมมุติว่าจุดประสงค์ของเราต้องการแค่ชื่อของ spaceship เช่น enterprise เราจะต้องเขียนโครงสร้างของกฏใหม่
หากเราเขียนกฏใหม่เป็นเหมือนตัวอย่างข้างบน จะทำให้เราสามารถได้ค่าของ spaceship เป็นเพียงแค่ชื่อของ Document นั้นเท่านั้น โดยที่เรายังใส่ =** ต่อท้ายไปใน wildcards ระดับ Collection ก็จะทำให้เราสามารถเข้าถึง ทุกๆ Document และ Subcollection ได้เหมือนเดิม
หากภายในกฏของเรามีกฏที่อ้างอิงถึง Document เดียวกันและมีกฏข้อใดอนุญาติให้เราสามารถเข้าถึงได้ เราจะสามารถเข้าถึงข้อมูลในส่วนนั้นได้ดังตัวอย่าง เช่น ผู้ใช้ที่ผ่านการ Authentication เข้ามาจะมาสามารถเข้าถึงข้อมูล Subcollection ต่างๆ ภายใน wildcards spaceship ได้ แต่หากเป็นผู้ที่ไม่ได้ทำการ Authentication มาใช้งานก็จะสามารถเข้าถึงการอ่านข้อมูลได้หากเป็น Subcollection ที่ชื่อว่า publicData เช่น spaceships/pegasus/publicData/manifest
การประเมินว่ากฏไหนจะสามารถให้ผู้ใช้คนไหนดำเนินการใดๆได้หรือไม่ จะเป็นประเมินจาก Boolean expression ถ้าหากผ่านเงื่อนไขจะมีค่าเป็น True ก็จะอนุญาติให้ดำเนินการได้หรือหากไม่ผ่านเงื่อนไขก็จะมีค่าเป็น False ทำให้ผู้ใช้ไม่มีสิทธิ์ได้รับอนุญาติดำเนินการตามที่เรากำหนดได้
ซึ่งถ้าเราไม่ได้กำหนดกฏใดๆไว้เลยของแต่ละ Path นั้นๆ จะมีค่ากำหนดเริ่มต้นคือ False ทำให้ทุกการดำเนินการใดๆจะไม่ได้รับการอนุญาติ
ลองมาดูดังตัวอย่าง เช่น หากเราอนุญาติแต่ Read ก็จะไม่สามารถดำเนินการเขียนได้ , หากเราอนุญาติแต่ Write การเขียนกฏ allow read: if false ลงไปด้วยก็ไม่จำเป็นต้องทำก็ได้ แต่เราสามารถเขียนกฏลงไปเพื่อเตือนความจำของเราเอง , หากเราไม่ได้เขียนกฏใดๆเลยก็จะหมายความว่าไม่อนุญาติให้ดำเนินการใดๆทั้งสิ้นใน Path นี้
ในการสร้างกฏของ Cloud Firestore นั้นหากเราจะต้องมีการสร้างเงื่อนไขของกฏที่ซับซ้อนมากขึ้น เราก็สามารถใช้ Function ที่ทาง Cloud Firestore ได้เตรียมไว้ให้ในการนำมาประเมินการอนุญาติการดำเนินการใดๆกับกฏของฐานข้อมูลของเราได้ โดยในบทความจะแนะนำกฏที่เป็นพื้นฐานและจะได้มีโอกาศใช้บ่อยๆ
หากเราต้องการเข้าถึงตัวแปรประเภท Map เช่น Array List หรือ Object ขณะสร้างกฏเราสามารถเข้าถึงค่าต่างๆในนั้นโดยผ่านสัญลักษณ์ (.) จุด หรือ เป็นเครื่องหมาย [ ] วงเล็บ แต่หากเราใช้แบบเครื่องหมายวงเล็บจะต้องระบุชื่อตัวแปรภายในเครื่องหมาย คำพูด ( “ ” )
ถ้าเราอยากจะร้องขอค่าในตัวแปรต่างๆภายในฐานข้อมูลของเรา เราสามารถทำได้ผ่านคำสั่ง request โดยปกติเราก็จะเอาไว้ใช้ตรวจสอบผู้ใช้จากการ Authentication ซึ่งจะมีกรณีต่างๆ ดังนี้
ตัวแปร resource จะคล้ายกับตัวแปร request.resource แต่ว่าจะเป็นการอ้างอิงถึง Document ที่มีอยู่แล้วภายในฐานข้อมูล ในกรณีที่เราดำเนินการอ่าน จะเป็นการอ่าน Document ขณะที่เราดำเนินการเขียน Document เก่าที่ทำการเปลี่ยนแปลงค่า
โดยทั้ง resource และ request.resource เป็น Object ชนิด Maps ทั้งคู่ ซึ่งภายในจะประกอบด้วย Property หลายๆตัวให้เราสามารถนำมาใช้ได้ ดังตัวอย่าง
หรือเราจะได้ค่า Metadata ของข้อมูลต่างๆก็สามารถเรียกได้โดยผ่าน สัญลักษณ์ _ _ เช่น __name__, __created_at__, and __updated_at__
หากว่าเราใช้ Wildcards เพื่อกำหนดกฏของชุดของ Document จะทำให้ค่าในตัวแปร Wildcards เปลี่ยนไปตามชื่อของ Document โดยเราสามารถนำค่าที่ได้ไปใช้อ้างอิงกับเงื่อนไขของกฏได้ เช่น หากเราต้องการนำค่าจาก userIDFromWildcard ไปตรวจสอบดูว่าตรงกับ request.auth.uid หรือไม่ หากถูกต้องจะทำการอนุญาติการเขียนและการอ่านได้
ถ้าหากเราใช้ resource เราก็จะสามารถตรวจสอบข้อมูลที่จะเขียนเข้ามาใน Path ที่เราอ้างอิง แต่อาจจะมีบางกรณีที่เราจะต้องไปตรวจสอบโดยการอ้างอิงข้อมูลใน Document อื่นที่อยู่ในฐานข้อมูลเดียวกัน โดยเราจะใช้ Method get() และ exists() เข้ามาช่วย
ใน Method get() ภายในก็จะระบุ Path เป็น Parameter ลงไปและวิธีการเข้าถึงข้อมูลภายในก็ต้องทำการ .data เข้าไปเหมือนกับ resource
Method exists() ภายในก็จะระบุ Path เป็น Parameter ลงไปเช่นกัน แต่จะเป็นการตรวจสอบข้อมูลว่ามีอยู่หรือไม่ใน Path นั้น
ทั้งสอง Method หากเราต้องการใช้ Wildcards ใน Path เราต้องเปลี่ยนจาก { } เปลี่ยนเป็น $ เพื่อเข้าถึงตัวแปร Wildcards นั้นๆแทน
หากว่าเราต้องมีการเขียนกฏคล้ายๆกันเพื่อใช้ในหลายๆที่ เราสามารถลดขั้นตอนการทำงานของเราโดยการสร้างฟังก์ชั่นของกฏขึ้นมาใช้งานเองได้ โดยฟังก์ชั่นที่สร้างมาจะสามารถนำไปใช้ได้ภายใน match block เดียวกัน ซึ่งฟังก์ชั่นที่เราเขียนขึ้นมาก็สามารถใช้ requestและ resource หรือ การเข้าถึง Wildcards ต่างได้อีกด้วย
ตัวอย่างของกฏหากเราไม่ได้เขียนเป็นฟังก์ชั่น
หากเปลี่ยนวิธีโดยการเขียนแบบสร้างฟังก์ชั่น
หรือมีอีกวิธี
ในการป้องกันการเขียนจากฝั่ง Client จะใช้ Method create, update และ delete ในการสร้างกฏ เพื่อที่จะไปจับคู่กับ set(), add(), update(), remove() และ transaction() ที่ทางฝั่ง Client ได้ส่งคำสั่งเข้ามา โดยจะทำให้เราสามารถแยกในการสร้างกฏออกเป็นกรณีต่างๆได้ละเอียดมากขึ้น
ซึ่งหากเราไม่ต้องการที่จะแยกออกเป็นกรณีๆก็สามารถใช้ write ได้เหมือนเดิม
ในการป้องกันการอ่านจากฝั่ง Client จะใช้ Method get และ list ในการสร้างกฏ เพื่อที่จะไปจับคู่กับ get() และ where().get() ที่ทางฝั่ง Client ได้ส่งคำสั่งเข้ามา โดยจะทำให้เราสามารถแยกในการสร้างกฏออกเป็นกรณีต่างๆได้ละเอียดมากขึ้น
ซึ่งหากเราไม่ต้องการที่จะแยกออกเป็นกรณีๆก็สามารถใช้ read ได้เหมือนเดิม
โดยมีข้อควรระวังคือหากเราจะมีการสอบถามข้อมูลภายใน field นั้นๆ จะต้องการสิทธิ์ในการเข้าถึงการอ่านภายใน Document นั้นๆ ก่อน เช่น
โดยใช้คำสั่งเข้าไปสอบถามข้อมูลดังนี้
ก็จบลงไปแล้วกับพื้นฐานการพัฒนาระบบฐานข้อมูล Cloud Firestore อย่างไรก็ตามก็อยากแนะนำให้ผู้อ่านได้ศึกษารายละเอียดต่างๆทั้ง 5 ขั้นตอนก่อนนำไปประยุกต์ใช้งานจริงกันนะครับเพราะทุกขั้นตอนก็มีความสำคัญทั้งสิ้นไม่ว่าจะเป็นการสร้าง Cloud Firestore เพื่อใช้งานในโครงการเพื่อเริ่มดำเนินโครงการ, การติดตั้ง SDKs เพื่อใช้งาน Cloud Firestore เพื่อติดต่อกับ Server , การออกแบบโครงสร้างและการจัดการของข้อมูลที่หากออกแบบโครงสร้างเป็นไปอย่างดีก็จะทำให้เราทำงานได้ราบรื่นและมีประสิทธิภาพมากขึ้น , การดึงและสอบถามข้อมูลก็จะทำให้เราผลลัพธ์ของข้อมูลอย่างถูกต้องตามที่เราต้องการ และส่วนสุดท้ายเป็นการป้องกันและความปลอดภัยของข้อมูลที่จะทำให้เราสามารถป้องกันผู้ที่ไม่ประสงค์ดีมากระทำการใดๆที่เราไม่ต้องการภายในฐานข้อมูลของเราได้ โดยหากเราทำเข้าใจในทุกๆขั้นตอนเป็นอย่างดีแล้วก็จะสามารถนำไปต่อยอดในการสร้างแอพพลิเคชั่นของเราได้อย่างสมบรูณ์
ซึ่งหากใครที่ต้องการศึกษาด้วยตนเองก็สามารถเข้าไปอ่านรายละเอียดจาก Document ของ Firebase เองได้ที่ https://firebase.google.com/docs/firestore/ หรือ https://firebase.google.com/docs/reference/js/firebase.firestore และหากใครอยากติดตามบทความของผมต่อก็สามารถรออ่านต่อกันได้ในเร็วๆ นี้ จะเป็นการออกแบบโครงสร้างของฐานข้อมูลอย่างละเอียดฉบับใช้งานจริง ยังไงฝากติดตามกันต่อและช่วยกดปรบมือเป็นกำลังใจให้ด้วยนะครับ
Let you learn and share your Firebase experiences with each…
1.1K 
2
1.1K claps
1.1K 
2
Written by
CTO @ Flagfrog # Full-stack Developer # Everything i can do , but it maybe not cool
Let you learn and share your Firebase experiences with each other.
Written by
CTO @ Flagfrog # Full-stack Developer # Everything i can do , but it maybe not cool
Let you learn and share your Firebase experiences with each other.
"
https://medium.com/google-cloud/what-are-the-google-cloud-platform-gcp-services-285f1988957a?source=search_post---------86,"There are currently no responses for this story.
Be the first to respond.
See Also: What is Google’s Cloud Platform (described using on the 1,000 most commonly used words)?
Google Cloud Platform (GCP) offers dozens of IaaS, PaaS, and SaaS services. Sadly, the Wikipedia entry for GCP is garbage, and while the official docs are pretty good, the marketing-dust sprinkled on them gives me a toothache. For my own reference, I pulled together an objective description for each of the services available on GCP.
Big hat-tip to Greg Wilson, the Google Cloud Developer Relations team, and the Google Cloud Developer Experts for the 4-word descriptions.
Something missing or wrong? Add a comment!
Google Cloud community articles and blogs
774 
6
774 claps
774 
6
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Developer Advocate @ Google, software engineer, and author of “Professional Android” series from Wrox. All opinions are my own.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/@saveyourtime/firebase-cloud-firestore-add-set-update-delete-get-data-6da566513b1b?source=search_post---------87,"Sign in
There are currently no responses for this story.
Be the first to respond.
Aaron Lu
Mar 14, 2019·2 min read
When you use set() to create a document, you must specify an ID for the document to create.
Specify that the data should be merged into the existing document.
Let Cloud Firestore auto-generate an ID.
.add(...) and .doc().set(...) are completely equivalent
Software Engineer with experience in languages such as HTML5, CSS3, JavaScript, Node.js and MySQL, frameworks such as React, React-Native, express and Nest.js
1.6K 
17
1.6K 
1.6K 
17
Software Engineer with experience in languages such as HTML5, CSS3, JavaScript, Node.js and MySQL, frameworks such as React, React-Native, express and Nest.js
"
https://towardsdatascience.com/how-to-train-machine-learning-models-in-the-cloud-using-cloud-ml-engine-3f0d935294b3?source=search_post---------88,"Sign in
There are currently no responses for this story.
Be the first to respond.
Chris Rawles
May 24, 2018·5 min read
And how to artfully write a task.py using the docopt package
Training ML models in the cloud makes a lot of sense. Why? Among many reasons, it allows you to train on large amounts of data with plentiful compute and perhaps train many models in parallel. Plus it’s not hard to do! On Google Cloud Platform, you can use Cloud ML Engine to train machine learning models in TensorFlow and other Python ML libraries (such as scikit-learn) without having to manage any infrastructure. In order to do this, you will need to put your code into a Python package (i.e. add setup.py and __init__.py files). In addition, it is a best practice to organize your code into a model.py and task.py. In this blog post, I will step you through what this involves.
The task.py file
As a teacher, one of the first things I see students, particularly those newer to Python, get hung up on is creating a task.py file. Although it’s technically optional (see below), it’s highly recommended because it allows you to separate hyperparameters from the model logic (located in model.py). It’s usually the actual file that is called by ML engine and its purpose is twofold:
There are many different ways you can write a task.py file — there are even different names you can give it. In fact the task.py and model.py convention is merely that — a convention. We could have called task.py aReallyCoolArgument_parser.py and model.py very_deeeep_model.py.
We could even combine these two entities into a single file that does argument parsing and trains a model. ML Engine doesn’t care as long as you arrange your code into a Python package (i.e. it must contain setup.py and __init__.py). But, stick with the convention of two files named task.py and model.py inside a trainer folder (more details below) that house your argument parsing and model logic, respectively.
Check out the Cloud ML samples and Cloud ML training repos for full examples of using Cloud ML Engine and examples of model.py and task.py files.
Writing clean task.py files using docopt
Although many people use argparse, the standard Python library for parsing command-line arguments, I prefer to write my task.py files using the docopt package. Why? Because it’s the most concise way to write a documented task.py. In fact, pretty much the only thing you write is your program’s usage message (i.e. the help message) and docopt takes care of the rest. Based on the usage message you write in the module’s doc string (Python will call this __doc__), you call docopt(__doc__), which generates an argument parser for you based on the format you specify in the doc string. Here is the above example using docopt:
Pretty nice, right? Let me break it down. The first block of code is the usage for your task.py. If you call it with no arguments or incorrectly call task.py this will display to the user.
The line arguments = docopt(__doc__) parses the usage pattern (“Usage: …”) and option descriptions (lines starting with dash “-”) from the help string and ensures that the program invocation matches the usage pattern.
The final section assigns these parameters to model.py variables and then executes the training and evaluation.
Let’s run a job. Remember the task.py is part of a family of files, called a Python package. In practice you will spend the bulk of your time writing the model.py file, a little time creating the task.py file, and the rest is basically boilerplate.
Because we are using docopt, which is not part of the standard library, we must add it to setup.py, so we insert an additional line into setup.py:
This will tell Cloud ML Engine to install docopt by running pip install docopt when we submit a job to the cloud.
Finally once we have our files in the above structure we can submit a job to ML Engine. Before we do that, let’s first test our package locally using python -m and then ml-engine local predict. These two steps, while optional, can help you debug and test your packages functionality before submitting to the cloud. You typically do this on a tiny data set or just a very limited number of training steps.
Once we have tested our model locally we will submit our job to ML Engine using gcloud ml-engine jobs submit training
These two lines are relevant to our discussion:
The first line indicates the location of our package name, which we always call trainer (a convention). The second line indicates, in the trainer package, that we will call the task module (task.py) in the trainer package.
Conclusion
By building a task.py we can process hyperparameters as command line arguments, which allows us to decouple our model logic from hyperparameters. A key benefit is this allows us to easily fire off multiple jobs in parallel using different parameters to determine an optimal hyperparameter set (we can even use the built in hyperparameter tuning service!). Finally, the docopt package automatically generates a parser for our task.py file, based on the usage string that the user writes.
That’s it! I hope this makes it clear how to submit a ML Engine job and build a task.py. Please leave a clap if you found this helpful so others can find it.
Additional Resources
Software Engineer @ Google
967 
4
967 
967 
4
Your home for data science. A Medium publication sharing concepts, ideas and codes.
"
https://medium.com/get-it-working/get-googles-firestore-working-with-react-c78f198d2364?source=search_post---------89,"There are currently no responses for this story.
Be the first to respond.
Top highlight
In their own words, Firestore is Google’s flexible, scalable NoSQL cloud database to store and sync data for client- and server-side development.
So, if you’re looking to create a fully fledged app, web or native, Google’s Firestore offers a ‘path of least resistance’ route to getting up and running with a real-time database, quickly.
Great for those of us working towards an MVP and scalable for those ideas which gain traction (shouldn’t we be so lucky).
In October last year, Google released the Firestore beta (a pretty stable beta). Prior to that you may be familiar the Firebase product (Google acquired Firebase in 2014 — link to Techcrunch announcement) Here’s a quick two minute intro to Firestore:
Step 1: Open up your preferred terminal and install firebase with npm(that’s firebase not firestore — bare with me here)
You should now have firebase listed in your package.json file — At time of writing, we’re currently on version 5.1.0
Step 2: Lets create our database — Head over to the Firebase website: https://firebase.google.com and hit the ‘Get Started’ button — use a preferred gmail account to sign-up. When you’re done with the signup you’ll land on the welcome page with a prompt to create your first project. 👇👇
Select the ‘Add project’ card to create a new project. You’ll be presented with a modal asking you to create a name for your project and identify the region + the obligatory ‘sign your analytics life away’ should you wish to disclose.👇👇
Next — we want to get the tracking code for our web app, hit the ‘Add Firebase to your web app’ option…👇👇
You’ll be presented with your config code — copy and paste the code within the <script> tags (script tags not needed) 👇👇
Step 3: Head back over to your React project and let’s create a new Firestore.js component (or whatever you would like to name it). Import firebase into the component and then paste-in the config code. Don’t forget to export the component.
You should have something similar to this👇👇 don’t worry, the below keys are fictitious, I’m not that trusting! 😲
Step 4: Almost there with the Firestore setup, now we need to create our database. Back in Firebase, select the ‘Database’ option from the left-hand navigation and select ‘Create database’ on the Firestore cta. 👇👇
Whilst in development, we’ll relax the security preferences to allow ‘read’ & ‘writes’ to our database. This will need to be addressed prior to any publishing of your app for obvious security reasons.
Step 5: Once enabled, you’ll be presented with your new database. Select the ‘+ Add collection’ option, and create a name for said collection. As highlighted in the tooltip, a collection is a set of documents which contain data. For our example, we’ll use the suggested ‘users’ for creating a database of user details.
After selecting ‘Next’ we have the option to add a Document id (unique reference) or weleave it blank for Firestore to create a random unique id per document. We’ll leave it blank for now.
For this example project, we’ll capture two pieces of data from our users, their full name and email address — both are set to string with blank values.
Once you’ve specified the data you’re looking to capture, click ‘save’. You’ve now created your database, populated with an example document.Time for a sneaky 🙌
Step 6: Ok, with our database created let’s get some data in there. In our project we’ll create a new component called User.js which contains a simple form.
Step 7: Looking good. Now we need to get the data from the form, into the state object (If your not familiar with React’s component state, have a quick run-through the React docs here).
First we’ll create our initial ‘empty’ state inside a constructor function:
Step 8: Next we’ll add a custom attribute to both input fields, which will call a function ‘updateInput’ to set our component state, when a user enters data into the form.
Let’s test this: open up the page in the browser and using React developer tools (Chrome) search for the User component. Now you’ll be able to see the state updated as you type in the form fields — great job!
Step 9: Now we’ll add another custom attribute to our input fields. This will assign the data (in state) to our object value which get’s passed into Firestore. (that was succinct…!) like so:
Almost there with the form, we just need to add an attribute to call a function which does the magic when the form is submitted.
Step 10: Still with me…? Awesome, we’re nearly there.Let’s create the addUser function which will send data to our Firestore database. We’ll use an arrow function to avoid having to bind this to the function in the constructor — if that doesn’t make sense, read this.
Ok, here we’re doing two simple things.1. preventDefault() method stops the page from refreshing (it’s default behaviour)
2. After the user submits the form, the state is updated to an empty string to remove the user input across both fields.
Simple enough, now let’s get that data into Firestore:
Ok, let’s run through what we’re doing here:
Hit save, open up the browser and complete the form, hit submit and…
Magic — you did it! 👇👇
As always, this is by no means the only way, right way, best way to get Firestore working with React. This is just one simple route to get things up and running, break down the learning barriers and either move on to another challenge or dive-in further. The Firestore documentation is fantastic, so definitely take a peek.
Go build something brilliant…👊
Short tutorials to get things working with JavaScript
1.7K 
20
1.7K claps
1.7K 
20
Written by

Short tutorials to get things working with JavaScript
Written by

Short tutorials to get things working with JavaScript
"
https://medium.com/@sathishvj/writing-and-passing-the-google-cloud-associate-engineer-certification-a60c2f6d99c2?source=search_post---------90,"Sign in
There are currently no responses for this story.
Be the first to respond.
sathish vj
Nov 15, 2018·6 min read
Subscribe to my YouTube channel that teaches you to apply Google Cloud to your projects and also prepare for the certifications: youtube.com/AwesomeGCP. Check out the playlists I currently have for Associate Cloud Engineer, Professional Architect, Professional Data Engineer, Professional Cloud Developer, Professional Cloud DevOps Engineer, Professional Cloud Network Engineer, and Professional Cloud Security Engineer.
Associate Cloud Engineer Playlist — https://www.youtube.com/playlist?list=PLQMsfKRZZviRwqJwNmh1eAWnRMvlrk40x
I wrote the GCP Cloud Associate Engineer exam and passed. Yaay! Here are my immediate impressions and notes. Hope it is useful to future test takers.
www.credential.net
For those appearing for the various certification exams, here is a list of sanitized notes (no direct question, only general topics) about the exam.
Overall notes across all GCP certification exams
Notes from the Professional Cloud Architect exam
Notes from the beta Professional Cloud Developer exam
Notes from the Professional Data Engineer exam
Notes from the Associate Cloud Engineer exam
Notes from the beta Professional Cloud Network Engineer Exam
Notes from the beta Professional Cloud Security Engineer Exam
Notes from the Professional Collaboration Engineer Exam
Notes from the G Suite Exam
Notes from the Professional DevOps Engineer Exam
Notes from the Professional Machine Learning Engineer Exam
Main Link — https://cloud.google.com/certification/cloud-engineer
Topics Outline — https://cloud.google.com/certification/guides/cloud-engineer
Practice Exam — https://cloud.google.com/certification/practice-exam/cloud-engineer
A collection of posts, videos, courses, qwiklabs, and other exam details for all exams: https://github.com/sathishvj/awesome-gcp-certifications
I’ve collected here a bunch of free Qwiklabs codes which are awesome to get lots of hands-on practice. Use them well.
medium.com
Check the FAQs here: https://medium.com/@sathishvj/frequently-asked-follow-up-questions-on-google-cloud-gcp-certifications-438e1addb91d.
Wish you the very best with your GCP certifications. You can reach out to me at LinkedIn and Twitter, especially for training for the certifications, short term consulting on GCP, and anything related to GoLang.
tech architect, tutor, investor | GCP 12x certified | youtube/AwesomeGCP | Google Developer Expert | Go
918 
8
918 
918 
8
tech architect, tutor, investor | GCP 12x certified | youtube/AwesomeGCP | Google Developer Expert | Go
"
https://medium.com/6d-ai/why-is-arkit-almost-useless-without-the-arcloud-6ee1e7affc65?source=search_post---------91,"There are currently no responses for this story.
Be the first to respond.
Top highlight
If you were asked what is the single most valuable asset in the tech industry today, you’d probably answer that it’s Google’s search index, or Facebook’s social graph or maybe Amazon’s supply chain system. I believe in 15 years time there’ll be another asset at least as valuable as these, that doesn’t exist today. Probably more valuable, if you look at what Microsoft’s Windows O/S asset (easily the most valuable tech asset in the 1990s) is worth in 2017 vs 1997.
Will one company eventually own (a huge profitable part of) it? History says probably. Will it be a new company? Also probably. Just as it was hard to imagine Microsoft losing its position in 1997, its hard to imagine in 2017 Google or Facebook losing their position. But nothing is guaranteed. I’ll try to lay out the arguments supporting each of three sides playing here (incumbents, startups, open web) in the last post of this series.
My last couple of posts talked about how ARKit and ARCore work. Basically discussing what’s available today and how did we get here. This post (well, series of posts) is going to get into what’s missing from ARKit and ARCore, and how those missing pieces will work.
To get beyond ARKit and ARCore we need to start thinking bigger than ourselves. How do other people on other types of AR devices join us & communicate with us in AR? How do our apps work in areas bigger than our living room? How do our apps understand & interact with the world? How can we leave content for other people to find & use? To deliver these capabilities we need cloud based software infrastructure for AR. I’ve been hearing people (including my SV partner Ori Inbar) refer to all this stuff as the ARCloud, and I like that name.
The ARCloud can be thought of as a machine-readable 1:1 scale model of the real world. Our AR devices are the real-time interface to this parallel virtual world which is perfectly overlaid onto the physical world.
When ARKit was announced at WWDC this year Apple Chief Executive Tim Cook touted augmented reality, telling analysts: “This is one of those huge things that we’ll look back at and marvel on the start of it.”
A few months went by. Developers worked hard on the next big thing, but the reaction to ARKit at the iPhone launch keynote was “meh”. Why was that?
It’s because ARKit & ARCore are currently at version 1.0. They only give developers three very simple AR tools:
In our excitement over seeing one of the hardest tech problems solved (robust 6DoF pose from a solid VIO system) and Tim Cook saying the words “augmented” and “reality” together on stage, we overlooked that you really can’t build anything too impressive with just those 3 tools. Their biggest problem is people expecting amazing apps before the full set of tools to build them existed. Note that it’s not the if but the when that we’ve gotten wrong.
Clay Bavor referred to the missing pieces of the AR ecosystem as connective tissue which I think is a great metaphor. In my post on AR product design I highlighted that the only reason for any AR app to exist (vs a regular smartphone app) is if it has some interaction or connection with the real world. With physical people, places or things.
For an AR app to truly connect to the world, there are three things it has to be able to do. Without this connection, it can never really be AR-Native. These capabilities are only possible with the support of the ARCloud:
How do we support multiple users sharing an experience? How do we see the same virtual stuff at the same time, no matter what device we hold or wear, when we are in the same place (or not). You can choose a familiar term to describe this capability based on what you already know e.g. “multi-player” apps for gamers, or “social” apps or “communicating” apps. It’s all the same infrastructure under the hood and built on the same enabling technology. Really robust localization, streaming of the 6dof pose & system state, 3D mesh stitching & crowdsourced mesh updating are all tech problems to be solved here. Don’t forget the application level challenges like access rights, authentication etc (though they are mostly engineering problems now).
GPS just isn’t a good enough solution, even the forthcoming GPS that’s accurate to 1 foot. I’ll explain why in the future post on this topic. How do we get AR to work outside in large areas? How do we determine our location both in absolute coordinates (Lat/Long) & also relative to existing structures to sub-pixel precision? How do we do achieve this both indoors & out? How do we ensure content stays where its put, even up days or years later? How do we manage so much data? Localizing against absolute coordinates is the really hard tech problem to solve here.
How do our apps understand both the 3D structure or geometry of the world (the shape of things) e.g. that’s a big cube-like structure my Pokemon can hide behind or bounce into, and identify what those things actually are e.g. the blob is actually a couch & my virtual cat should stay off couches. Real-time on device dense 3D reconstruction, real-time 3D scene segmentation, 3D object classification (Don’t worry, I’ll explain what all these terms mean in the post on this subject), backfilling local processing with cloud trained models are the challenges here.
Like much in AR, it’s not that hard to build something that demoes well, but it’s very hard to build something that works well in real world conditions.
I’d hoped to get all this out into one post… but it would have been an epic, even in relation to my other posts. So I’ll do one post on each of the above 3 points. What I hope to achieve is to communicate both how important and how difficult it is to build this infrastructure to deliver a consumer grade AR UX.
Just when you thought you were getting your head around the difference between AR, VR and MR, it all goes another level deeper! Vendors will use identical terms that mean completely different things, like:
When I worked in telecom infrastructure, there was a little zen-like truism that said “there is no cloud, it’s just someone else's computer”. We always ended up working with the copper pairs or fibre strands (or radio spectrum) that physically connected one computer to another, even across the world. It’s not magic, just difficult. What makes ARCloud infrastructure different from the cloud today, powering our web and mobile apps, is that AR (like self-driving cars & drones & robots) is a real-time system. Anyone who has worked in telecom (or on fast-twitch MMO game infrastructure) deeply understands that real-time infrastructure and asynchronous infrastructure are two entirely different beasts.
So while many parts of the ARCloud will involve hosting big data and serving web APIs and training machine learning models, just like today’s cloud, there will need to be a very big rethink of how do we support real-time applications and AR interactions at massive scale. Basic AR use-cases like: streaming live 3D models of our room while we “AR Skype”; updating the data & applications connected to things, presented as I go by on public transport; streaming (rich graphical) data to me that changes depending on where my eyes are looking, or who walks near to me; maintaining & updating the real-time application state of every person & application in a large crowd at a concert. Without this type of UX, there’s no real point to AR. Lets just stick with smartphone apps. Supporting this for eventually billions of people will be a huge opportunity. 5G networks will play a big part & are designed for just these use-cases. If history is any guide, some if not most of today’s incumbents who have massive investments in the cloud infrastructure of today will not cannibalize those investments to adapt to this new world.
Ultimately it’s up to the users of AR apps to decide this. Useless was a provocative word choice. So far, 1 month in, based on early metrics, users are leaning towards “almost useless”. My personal belief is that useful apps can be built on today’s ARKit, but they will only be useful to some people, occasionally. They might be a fun novelty that makes you smile when you share it. Maybe if you are buying a couch you’ll try it in advance. But these aren’t the essential daily-use apps that define a new platform. For that we need AR-Native apps. Apps that are truly connected to the real-world. And to connect our AR-Apps to each other & the world, we need the infrastructure in place to do that. We need the ARCloud.
Next: AR and connecting people (coming soon)
PS: special thanks to Sam Dauntesq from @madewitharkit, and Mark Billinghurst for the insightful feedback to an early draft of this post
Building the 3D Map of the World
1.6K 
21
Thanks to Sam Dauntesq and Mark Billinghurst. 
1.6K claps
1.6K 
21
Written by
CEO @6D_ai, @Super_Ventures. Building the AR Cloud
Building the 3D Map of the World
Written by
CEO @6D_ai, @Super_Ventures. Building the AR Cloud
Building the 3D Map of the World
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/thinking-design/what-does-a-ux-designer-actually-do-fe9e1e4c1342?source=search_post---------92,"There are currently no responses for this story.
Be the first to respond.
Top highlight
User Experience (UX) Designer is a job title you’re likely hearing more and more these days. While UX design is a field that feels essential to product development, its function still remains a mystery to many because of its relative newness. Thus, when someone says “I’m a UX designer,” it is not always immediately clear what they actually do day-to-day.
This article is dedicated to those who are genuinely still unsure what a UX designer does.
For a long time design has been associated with graphic design (“the look” of a product). As digital technology and our expectations about digital interactions has grown, we’ve begun focusing more and more on “the feel” part of a design, also known as the user experience. If UX is the experience that a user has while interacting with a product, then UX Design is the process by which a designer tried to determine what that experience will be (Note: We can’t really design experiences as a formal entity. However, we can design the conditions of an intended experience).
A UX designer’s role is directly involved in the process of making a product useful, usable and enjoyable for its users. If you want to learn more about UX design, consider reading the article, What You Should Know About User Experience.
How do UX designers work on a day-to-day basis? The answer to this question, as with many questions, is: it depends. A UX designer’s responsibilities can vary dramatically from company to company and sometimes even from project to project within one company. Despite the variety the role offers, there are some general functions a UX designer can be expected to perform irrespective of the company they work at.
Below I’ve summarized the 6 main responsibilities of UX designer:
Product research (which naturally includes user and market research) is every UX designer’s starting point for a UX design project. It provides the foundation for great design as it allows designers to avoid assumptions and make information-driven decisions.
Product research is important because:
From the technical side, product research is a data collecting process through channels like:
Collected data is analyzed and converted into quantitative and qualitative information. This valuable information will be used for decision making.
Based on the product research results, the next step for a UX designer is to identify key user groups and create representative personas. A persona is a fictitious identity that reflects one of the user groups for whom they are designing.
Persona is a fictional character that highlights behaviors, needs and motivations of the target users. Image credit: xtensio
Personas aren’t the users they want, but the users they actually have. And while personas are fictional they should represent a selection of a real audience and their behaviors. The goal of creating personas is to reflect patterns that they’ve identified in their users (or prospective users).
When a UX designer has identified personas, they can write scenarios. A scenario is a narrative describing “a day in the life of” one of their personas, including how their website or app fits into their user’s lives. Whether they’re designing an app or a website, and whether this is a new product or a redesign of an existing product, it’s important to think through all of the steps that a user might take while interacting with their product.
An example of a user scenario presented in a format of a storyboard. Image credit: Chelsea Hostetter
Once a UX designer has done the research and created personas, it’s time to define the Information Architecture. Information architecture is the creation of a structure for a website, app, or other product, that allows users to understand where they are, and where the information they want is in relation to their current position. Information architecture results in the creation of navigation, hierarchies and categorizations. For example, when a UX designer sketches a top level menu to help users understand where they are on a site, s/he is practicing information architecture.
An example of information architecture. Image credit: Behance
Once the IA has been determined, it’s time to create wireframes. A wireframe is a design deliverable most famously associated with being a UX Designer. Basically, a wireframe is a low fidelity representation of a design. Wireframes should represent each screen or step that a user might take while interacting with a product.
Wireframes have following properties:
A well-created wireframe communicates design in a crystal clear way. Image credit: Behance
A lot of people use the terms “wireframe” and “prototype” interchangeable, but there’s a significant difference between two design deliverables — they look different, they communicate something different and they serve different purposes. While wireframes are similar to architectural blueprints (e.g. a building plan), prototype is a middle to high fidelity representation of the final product.
Prototypes have following properties:
A prototype is a simulation of the final interaction between the user and the interface. Prototypes should be clickable, although they don’t need to have full functionality.
Testing helps UX designers find out what problem users experience during the interaction with a product. One of the most common ways that a UX designer might do product testing is by conducting in-person user tests to observe one’s behavior. Gathering and analyzing verbal and non-verbal feedback from the user helps UX designers create a better user experience. Not to say that being in the same room while someone struggles to use your product is a powerful trigger for creating empathy with users.
User testing session. Image credit: Toronto Public Library
There are a lot of other testing methods available. If you’re interested in learning more information about user testing, read about The Top 5 User Testing Methods.
UX design is a process of constant iteration. A UX designer’s work doesn’t stop with the product release, in fact, UX designers continue to learn which drives future updates. They launch with the best possible product, but they’re always prepared to learn and grow.
If you overview different UX designer job descriptions, you’ll find that the list of responsibilities on each can vary significantly — in some descriptions, the UX designer role is all about research and usability testing, while in others it’s more technical role, responsible for building the prototypes and working more closely with the engineering team. All because the role of a UX designer depends heavily on the nature of the company and the difference between one UX designer role and another can be dramatic. The biggest difference is between startups and big companies:
While the UX designer role is complex, challenging and multifaceted, UX design is really fascinating and satisfying career path which could take you in many directions.
Nick Babich is a developer, tech enthusiast, and UX lover. He has spent the last 10 years working in the software industry with a specialized focus on development. He counts advertising, psychology, and cinema among his myriad interests.
Originally published at blogs.adobe.com.
Learn about Adobe XD, our all-in-one design and prototyping tool:
Stories and insights from the design community.
1.7K 
7
1.7K claps
1.7K 
7
Written by
New Tools for New Creatives. Get all the latest creative apps plus seamless ways to share and collaborate. All right on your desktop.
Stories and insights from the design community.
Written by
New Tools for New Creatives. Get all the latest creative apps plus seamless ways to share and collaborate. All right on your desktop.
Stories and insights from the design community.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/automation-generation/algorithmic-trading-automated-in-python-with-alpaca-google-cloud-and-daily-email-notifications-422b7c6b7c53?source=search_post---------93,"There are currently no responses for this story.
Be the first to respond.
The purpose of this article is to provide a step-by-step process of how to automate one's algorithmic trading strategies using Alpaca, Python, and Google Cloud. This example…
"
https://itnext.io/building-a-serverless-restful-api-with-cloud-functions-firestore-and-express-f917a305d4e6?source=search_post---------94,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
It’s obviously run on ‘a’ server somewhere in this earth; however, the upside is you don’t have to configure or maintain it.
I recently got an exercise on how to build a RESTful HTTP interface, and it intrigues me that I could try to build a temporary RESTful API to test the concept. Luckily, I am working on several projects that use Firebase Cloud Functions and Cloud Firestore. And it’s totally possible to create such endpoints for testing purposes or even a full working RESTful API project. Who knows?!
So, in this project, I will use Firebase Cloud Functions, Firebase Hosting, Cloud Firestore (database) and TypeScript. Yes, you see it right, Cloud Functions does support TypeScript! That means you can take advantage of ES6+ when writing the functions.
You can follow this guidance or check the git respository of this project.
https://github.com/dalenguyen/serverless-rest-api
I will use this API to manage the contact information as an example.
Step 1: Get your firebase project ready
Well, you need to create a firebase project to work with. Remember to choose Firestore as your database.
The rules are open to public. You need implement the security for firestore before going to production.
Step 2: Initiate Firebase Hosting and Functions on your local machine
Before getting started, you need to install firebase-tools thought NPM
After that, you log in and init firebase project
Remember to check both Functions and Hosting. The advantage of implementing hosting is that you will have a custom URL for your API project.
Please choose TypeScript as the language for writing functions. You can choose JavaScript if you want to.
Step 3: Install package dependencies
The packages will be in the functions folder
The firebase-functions-helper is the package that I wrote when working with Cloud Firestore. You can use the native guide if you want.
www.npmjs.com
Step 4: Write your functions
The functions logic locates at functions/src/index.ts
You first need to import the necessary packages
In this file, we also create CRUD route for the API
Step 5: Deploy your hosting and functions
Before deploying to firebase, we need to make some changes on firebase.json
We will map the functions to the URL, so the webApi will trigger we will call URL. (The URL is different in your project)
Now, you can deploy to firebase
This command will deploy hosting and functions to firebase. Next time, you just need to deploy functions only when you make some changes to your routing.
Step 6: Testing your API project
I will use Postman to test the API. First, we will create a contact by sending POST request together with the body.
View all contacts with GET request
We can pass the contact Id to view only one contact
Update my email address by sending PATCH request
Finally, we can delete a contact with DELETE request
There are some response handling that need to be improved; however, we now have a working API to experiment with.
If you are interested in building API, you can take a look a my other post:
Building RESTful Web APIs with Node.js, Express, MongoDB and TypeScript
Follow me on Twitter for the latest content on Angular, JavaScript & WebDevelopment 👐
ITNEXT is a platform for IT developers & software engineers…
1.4K 
30
1.4K claps
1.4K 
30
Written by
Full Stack Developer / JavaScript Enthusiast/ http://dalenguyen.me
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
Written by
Full Stack Developer / JavaScript Enthusiast/ http://dalenguyen.me
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://read.acloud.guru/what-you-need-to-get-aws-certified-5937e613b10f?source=search_post---------95,"The Amazon Web Services (AWS) certification offers individuals one of the most valued certifications in the marketplace. The exam is difficult, and there is no magic pill for bypassing the work required to pass.
We commonly get questions from our students wondering “what else is needed beyond watching your AWS Certification courses to pass the exam?”
Here’s some answers to some of your most frequently asked questions about preparing for the associate-level AWS certifications:
A Free Tier AWS AccountThe courses from A Cloud Guru offer hands-on labs — and you’ll need an AWS account to complete the interactive examples and exercises. The majority of our hands-on labs use the AWS Free Tier, so there is almost no additional cost for the account.
ComputerA PC (Windows or Linux) or Mac — You can’t do the course purely on a tablet or mobile device. You can view our courses on mobile devices and tablets, but you can’t do the exercises without a PC or Mac as you need to be able to run an SSH client.
BrowserIdeally a modern browser — such as Firefox or Chrome— to access A Cloud Guru.
If you have extensive AWS experience, we’ve seen students successfully earn the certification in less than a week with our course as the guide for exam preparation.
This depends on how much experience you have with AWS. If you’re new to AWS but have some IT experience, typically between 40 to 80 total hours of study is required. Our AWS Certified Associate Courses are 10 to 20 hours long, and require about 4 hours of study for every 1 hour of video.
The A Cloud Guru certification courses includes unlimited access to the training content, video lectures, quizzes and practice questions, sample code, example hands-on labs, and moderated discussion forums.
You can also access our training courses on your Apple or Android device!
No. AWS uses a third party to administer the exams. The training provided by A Cloud Guru is independent of AWS and their testing centers. You will need to register separately for the certification exam using the AWS Training and Certification Portal.
The free AWS whitepapers are the only additional study materials you’ll need to pass the certification exam. In particular, it’s important to know the AWS Security Whitepaper back to front — it applies to all the certifications. The AWS Storage Services Whitepaper is also useful, and the AWS Well-Architected Framework is worth a read.
This differs from person to person, but I personally prefer to split screens or use dual monitors to simultaneously display the video player and the AWS console. This allows you to perform the exercises and examples at the same time as the instructor — pausing and replaying the video whenever it goes too fast or if you missed something.
To pass the certification, you’ll need to be comfortable with AWS — and the only way to do that is by using all the AWS services covered in the certification training and exam.
My recommendation is to complete the course, and then destroy everything you’ve created during the course — then try it again without following the course. Try to create your own exercises to test yourself. Create a small personal project to practice building with AWS. There is no substitute for using AWS when studying and preparing for the exam.
This is where the questions in the course are important. They will help assess where you have gaps in your knowledge and need to study more.
When doing final preparations for the exam, I would go through the A Cloud Guru sample questions and take note of where you did poorly. Then revisit those sections in the course, and read the relevant whitepaper. Afterwards, repeat the process until you get 100% for the sample questions.
You can now simulate the exam experience on A Cloud Guru. We take a random selection from our large pool of questions and ensure that the domain breakdown satisfies that listed in the exam blueprint. Unlike a real certification exam, we’ll show you a detailed report upon completion.
In the exam, there will be a number of scenario based questions that often test your comprehension of the question as much as they test your knowledge. When you encounter a scenario based question — use the process of elimination before selecting a correct answer.
After students start training, many wonder “how do I know I’m ready to take the exam?” To alleviate test anxiety, we’ve combined hundreds of student testimonials, insights from AWS experts, and countless hours of our own studying to create the AWS Certification Prep Guide.
This exam guide is free to members, and helps you prepare for all AWS certifications — with tips, tricks and strategies that apply to all exams.
After viewing this quick but comprehensive guide to the AWS Certification exams, you’ll feel confident knowing when to step into the exam room.
Amazon Web Services launches new updates daily and it’s hard to find the time to keep up. To help you stay up-to-date after your certification, A Cloud Guru created the AWS This Week newscast to provide quick and current headlines from Amazon Web Services delivered to your inbox every week.
Slack BotThe engineering team at A Cloud Guru developed a fun quiz bot that can be integrated with your team’s Slack channel — including a leaderboard.
Alexa SkillA Cloud Guru is also available on Alexa in the US, UK, India, Canada, and Australia. Just say “Alexa, open A Cloud Guru” to access hundreds of practice questions that test your AWS knowledge on a variety of services including compute, storage, networking, security, databases, and management tools. Learn more about Alexa’s data mining disruptive history and how it has become the ML service we all know today.
I hope this answers the majority of questions we hear from our students. If you have any other questions, create an account on A Cloud Guru and join our on-line community of over 600,000 students in 160 countries!
A Cloud Guru makes it easy (and awesome) to get certified and master modern tech skills — whether you’re new to cloud or a seasoned pro. Check out ACG’s current free courses or get started now with a free trial.
Our discussion forums are a hive of activity, full of engineers helping each other learn cloud and prepare for exams. See you there!
A Cloud Guru – AWS Certification Courses — Join the community of 400,000 students who have taken our online AWS certification preparation courses and passed with ACG!
Check out these post to help you get ready for other cloud certifications:
Keep up with the latest A Cloud Guru announcements on Twitter and Facebook!
Get more insights, news, and assorted awesomeness around all things cloud learning.
"
https://codeburst.io/express-js-on-cloud-functions-for-firebase-86ed26f9144c?source=search_post---------96,"Express.js, or Express, is an unopinionated server-side JavaScript web framework that runs on Node.js. Simply, it provides an API to create and manage HTTP routes, payloads and sessions. It’s one of the most widely used projects by server-side JavaScript developers.
Running web servers is difficult due to fluctuating traffic as more users request data. Load balancing is the process of routing the traffic to multiple machines to spread the workload, maximizing throughput, minimizing response time and avoiding system overloads. While there are tools available to solve these problems, you must still manage the servers behind the Load Balancer and pay for the unused resources of each machine.
Functions as a Service (FaaS), like Google Cloud Functions, are small server containers that automatically scale as required, this makes it an ideal place to host an API without requiring any specialized knowledge of load balancing. It’s just 3 lines of code in your package.json to deploy your entire auto-scaling server. Just 3 lines! It can also be cheaper since you pay only when a request is being fulfilled, and no more.
At this point, you may be asking “Why’re we going to run an Express Server on Cloud Functions. Don’t they already expose an HTTPS endpoint?”.
Well, yes, they do already expose endpoint via HTTPS triggered Cloud Functions, but if we were to use Express, we could use the vast Express middleware ecosystem, or even host our entire API behind a single Cloud Function. Fortunately, the Google developers who implemented Cloud Functions know what they’re doing, and we can simply pass an Express instance directly into the Cloud Function HTTPS handler.
To start, in your functions folder, run yarn add express. Then update the index.js file to be the following:
N.B.: If you wish to use ES6+ syntax as I am here, please read my ES6 in Cloud Functions for Firebase article.
I asked this question on StackOverflow myself and got a response from a Firebase Engineer:
This all works because under the covers, an Express app is actually just a function that takes a Node.js HTTP request and response and acts on them with some automatic sugaring such as routing. So you can pass an Express router or app to a Cloud Function handler without issue, because Express’s req and res objects are compatible with the standard Node.js versions.
And the Cloud Function handler doesn’t have any issues because as the Cloud Functions for Firebase HTTP Triggers docs outline:
Use functions.https to create a function that handles HTTP events. The event handler for an HTTP function listens for the onRequest() event and supports two HTTP-specific arguments: request and response. These parameters are based off of the Express Request and Response objects, giving you access to their corresponding properties.
So it’s not complete magic, it was just good design. Thanks Google/Firebase engineers!
One of the most used Express middleware is cors. It’s a small package that enables Cross-Origin Resource Sharing (CORS) with various options. The code for adding middleware is the same here as in any other environment. As an example, we’ll add cors to our server.
Add the package in our ./functionsES6 directory by running:
yarn add corsnpm install cors — as of npm v5 --save is the default behaviour.
Then add cors to our app in ./functionsES6/index.js
Since we’re using Express you’ll need to add a trailing / to the URL as Express requires. Illustrated below:us-central1-<project-name>.cloud-functions.net/apius-central1-<project-name>.cloud-functions.net/api/
If you do not append the slash when calling the Cloud Function, you’ll find you get a 500 error:Error: could not handle the request
An Automated SolutionInstead of passing in the Express app directly to the Cloud Function, we can just append the / to the request object in the function and return the Express object while passing the new request/response pair through.
This code doesn’t look as clean, and the Cloud Function Express magic is removed, but we have more control and can give a better endpoint to our consumers.
The trailing slash issue aside, there are some other caveats to using an Express server within Cloud Functions, the biggest of which is around BodyParser.
All requests to HTTPS triggered Cloud Functions are automatically parsed by Cloud Functions with an Express Body Parser before the request reaches your code in the function. So even if you define your parser and middleware through Express, it’s still already automatically parsed. And you can’t explicitly define the parser that is used by Cloud Functions, the content-type of the request dictates that, as documented here. Thus, if you do not control the calling environment, you cannot control the parsing of the data.
In addition to this, not all parsing options through Body Parser are available to use even if controlling the calling environment. Again, the options for parsing are available here.
A repository of the completed project is available here:
github.com
If you like Firebase you might want to check out awesome-firebase:
github.com
So Express on Cloud Functions is simple! It’s easier to get going than a self-hosted Express server, as it doesn’t require all the knowledge that comes with infrastructure scaling. It’s flexible in that you can use all the technologies and practices you’ve come to expect from the Express ecosystem, assuming you have control over the calling environment.
Thanks to Sam Oliver for his help fleshing out the Body Parser issue.
More by me:
If you found this useful, please recommend and share with your friends & colleagues.
Bursts of code to power through your day.
1.1K 
11
Some rights reserved

1.1K claps
1.1K 
11
Written by
GCP, Firebase, Sveltejs & Deno fan! @jthegedus on all platforms.
Bursts of code to power through your day. Web Development articles, tutorials, and news.
Written by
GCP, Firebase, Sveltejs & Deno fan! @jthegedus on all platforms.
Bursts of code to power through your day. Web Development articles, tutorials, and news.
"
https://medium.com/zero-equals-false/integrating-firebase-cloud-functions-with-google-calendar-api-9a5ac042e869?source=search_post---------97,"There are currently no responses for this story.
Be the first to respond.
I’ve recently started using Firebase for my projects, and found some success with their hosting, Firestore, and authentication services. When I tried using their Cloud Functions, I saw value in integrating it with their Google Calendar API. I wanted to be able to trigger the cloud function and have it manage my personal Google Calendar, but ran into some problems with the function’s authorization since I didn’t want to need a sign-in each time it executed.
After a lot of headaches, I consulted a fellow developer who had some experience with using OAuth2 and Google APIs. (Be sure to check out his Medium article on Node.js, OAuth2, and Gmail here!) Be sure to have NPM installed before beginning this tutorial.
With this problem solved, I was finally able to get my Firebase Cloud Function to quickly add events to my primary Google calendar via an HTTP trigger with only minimal details in the request body. A repository (minus the OAuth2 credentials.json file) with the code used to create our cloud function can be found here.
Obviously, the first thing we need to is to create a Firebase app. This can be done through the Google Developer Console and integrating Firebase into your app later, but I’ll be using the Firebase Console. After signing in to your Google account, you should get a list of all your Firebase projects and an Add Project button. When you click Add Project, you’ll be prompted to fill in your project details.
The Project Name can be anything you like, but the Project ID must be unique through all Google projects. As you can see above, my project ID is calendar-cloud-function, but I’ll be using PROJECT_ID for the remainder of the article as an indication for you to replace with your specific Firebase project ID.
Be sure to accept all terms and conditions in the window. Once doing so, click Create project to finish creating your Firebase project. This process will take several seconds before confirming that your project was indeed created successfully. You can then enter your Firebase project.
NOTE: Behind the scenes, a project is also made in the Google Developer Console that will be used later.
If we go to the Functions tab on the left under Develop section, we’ll see our dashboard where our Cloud Functions will live.
Because we just created this project, there will be nothing listed here. Once you add a Function to your project, you can come back to this dashboard to see the logs, HTTP trigger, and other important information about your function.
Now that we have a Firebase project initialized on our account, we’ll be able to add/deploy Cloud Functions and integrate any other Firebase product with our project.
Open up the command prompt and navigate to a workspace where you want to house your project. Run mkdir PROJECT_ID to create a directory with the same name as your Firebase project and cd into the new directory.
First, you’ll need to download the Firebase CLI with npm install -g firebase-tools. Once installed, login to your Firebase account with the command firebase login. This will grant the CLI access to your account so that you can manage and deploy code to specific Firebase projects.
The Firebase CLI also provides helpful tools to initialize code for specific Firebase products. Run firebase init to begin the setup the project architecture for your cloud function.
For this tutorial, we are only working with Firebase Cloud Functions. Select Functions for your project setup and press enter to continue with the initialization of your project.
Next, the Firebase CLI will want to know which project you want your Cloud Function to be associated with. Select the correct PROJECT_ID for the Firebase project and press enter.
After pressing Enter, you will be given the option to write the Cloud Function in JavaScript or TypeScript. Feel free to choose either, but the code for this tutorial was written in JavaScript. Continue with the rest of the project setup. For ease of use later, be sure to install the npm dependencies and a linter during your project initialization.
Once the npm dependencies are downloaded, your Firebase project should be created and ready to use. Open up the root directory in your favorite IDE and we can begin writing code!
Your project should have the following architecture. If you enabled a linter for your project, we’ll need to make a minor change to the firebase.json file in the root directory.
In the predeploy array, edit the line ""npm --prefix \""RESOURCE_DIR\"" run lint"" to ""npm --prefix functions run lint"". The entirety the file should now be as follows:
In our index.js file, you should see some commented out code. At this point, let’s uncomment the helloWorld function and deploy it to Firebase in order to see if everything is configured correctly. After commenting out the function, index.js should look like this:
This will return a message of “Hello from Firebase!” when the Cloud Function is executed. In the command prompt, run firebase deploy --only functions in the project’s root directory. The deploy will take ~1 minute to complete, but should succeed. Once successful, navigate back to your project’s Firebase dashboard and see our newly created helloWorld function listed in the Functions dashboard with an HTTP endpoint for a trigger.
Copy the HTTP endpoint and paste it in a new tab of the browser. This will execute your function and you should get a “Hello from Firebase!” message back. This means that your project is configured correctly and now have a Cloud Function deployed!
This function doesn’t do much, but the opportunities for what it can do are endless. For this tutorial, we will be working with the Google Calendar API (v3). Google has hundreds of other APIs available, and I encourage you to check them out once you are done with this article. To work with Google APIs, you will first need to set up our OAuth2 credentials. This will authenticate your function when interacting with Google’s APIs.
For this part, we’ll be generating our API credentials that our Cloud Function will use to interact with the Google Calendar API. This eliminates the need for sign-in prompts and allows your cloud function to execute quickly and reliably with a fresh token to access your specific Google Calendar.
First, go to the Google Developer Console and select the project you created if it is not already selected. You should be on your project’s main dashboard, with cards for many different aspects of your project. Select Go to APIs overview in the section for APIs.
In the API Dashboard, select Credentials from the menu on the left. If you’ve made credentials for other apps, you may have some already listed here, otherwise you’ll be prompted to create your own credentials.
You may see a warning banner near the top of the page saying:
To create an OAuth client ID, you must first set a product name on the consent screen
To fix this, click the Configure consent screen button on the alert and pick a name for your “product”. Once you name your product, you may need to select an email for a Support Email before the Save button is enabled. Once enabled, click Save to be redirected to the OAuth Client ID creation screen.
On the credentials screen, select Web Application under Application Type to bring up the other criteria needed for OAuth credentials. Name the OAuth Client ID something meaningful so you can keep track of the credentials for your project.
Under Authorized redirect URIs in the Restrictions section, add the URI https://developers.google.com/oauthplayground. Once the redirect URI is added, click anywhere outside the input to ensure the URI was entered correctly. If everything looks correct, click the Create button at the bottom!
If your OAuth client credentials were created successfully, you will be redirected back to your credentials screen with the following alert window active:
If you click OK, you should see the credentials listed under the OAuth2.0 client IDs section on the Credentials page.
In the right-most column for your client ID is a download icon. This will download a JSON file of the OAuth client credentials. The file should contain the following data:
Rename this file to credentials.json and save it in the same directory as index.js file for the project you created in Part 2.
Now that we have our OAuth Client credentials, we’ll need to actually go into our redirect URI and configure the OAuth2. To do this, go to https://developers.google.com/oauthplayground. On the page, click the Settings icon in the top-right of the screen to pull up the configuration settings. Select Use your own OAuth credentials at the bottom to bring up inputs for the Client ID/Client Secret you just created.
After entering both the Client ID and Client Secret from your credentials.json file you created in Part 3, leave the above window open. On the left-side of the screen, search for the Calendar API v3 and select the appropriate scope that you want your cloud function to have.
Because we want our app to manage/edit/read from your Google Calendar, select the scope https://www.googleapis.com/auth/calendar for full read/write access. Once that scope is selected, click Authorize APIs at the bottom of Step 1.
You’ll be taken to a Google sign-in page where you sign in to the Google account who’s calendar you want to access. Once signed in, you’ll be asked to confirm access for your credentials to manage your Google calendars. Approve of this message to continue. This will redirect you to Step 2 of the OAuth2 Playground form.
You should have the Authorization code populated. This code is only valid for a short period of time (3600 seconds precisely). While it’s active, you’ll need to click Exchange authorization code for tokens to populate a refresh token that we will be able to use later in our cloud function.
For this tutorial, we will add this refresh token to the credentials.json file we created earlier to limit the number of files accessed by our cloud function later. An updated version is below, and your file should be the same format.
There you go! You now have OAuth2 configured for your cloud function so that the Refresh token, Client ID, Client Secret can get you a valid Access token when working with the Google Calendar API. If you initialize this project as a Git repository, be sure to specify credentials.json in your .gitignore file so that you don’t accidentally add your credentials to a public repository.
Now that we have OAuth2 configured for the Google Calendar API, we can begin writing code to interact with it! To do this, navigate to your project in the command line and go into the functions folder where your package.json lives. There, run npm i googleapis --save to install the necessary Google API Node.js tools.
First thing we’ll do is import all the required packages we’ll be using. The completed code for index.js can be found here. We’ll put this at the top of our file, before any methods we write later. Be sure that credentials.json is in the same folder as index.js, otherwise the path will be need to be updated.
Next, we’ll begin writing our Firebase function. In Part 2 of this tutorial, we wrote a helloWorld() Firebase function. We’ll replace that with the following addEventToCalendar() method.
The request and response parameters act as entry points and exits for our cloud function to interact with Firebase and the user. The request will have the data needed to make our calendar event, and we create that event in the first line of the function, storing it in eventData.
Next, we use our imported googleCredentials to create an oAuth2Client object with all the credentials we need to verify us as the correct user. We then set the refresh_token for the oAuth2Client credentials so that we have a fresh token when accessing the Google Calendar API.
Finally, we call a method addEvent() that has not been written yet, passing it the eventData and oAuth2Client details, waiting for either a Success or Failure. Once either is received, we pass the details of what happened back to the user with the response parameter given by Firebase.
The last method we need to write is the addEvent() method reference above. The full method is below:
As mentioned before, we passed our calendar event and authentication that were created in the initial addEventToCalendar() Firebase function. We then use the Google Calendar API by referencing the calendar object we created earlier. The full API for the Google Calendar can be found here. If successful, we pass the newly-created calendar event data back in the Promise with a message of Success that will be handled by the Firebase function we created above.
And that’s it! The Firebase function is now complete. Like before, we will deploy this function by running the command firebase deploy --only functions in the root directory of the project. After a minute or two, the Firebase CLI will say that the Cloud Function is deployed.
Once your function is deployed, go to the Google Developer Console and navigate to your Firebase project. In the menu on the left, select Cloud Functions from the list. There, you should see the Firebase function you just created in your Cloud Functions dashboard. Click the addEventToCalendar function to get more information about it.
Here, you should see four tabs for your cloud function: General, Trigger, Source, and Testing. Check out each one to learn more about your function, including its HTTP trigger endpoint.
We’ll start by going to the Testing tab though. Here, you’ll see the following input options:
The empty {} JSON object is your request body. If you remember, we access several elements of this request body in our cloud function to create the calendar event. Because of this, we’ll need to replace the Triggering Event to something along the lines of:
Click Test the function to call your Firebase function with the above request body. If everything is correct, you should get a response with details about your newly created event. If you navigate to your Google Calendar, you’ll be able to see your event, exactly where it’s supposed to be.
That’s it, we’ve officially got a Firebase Cloud Function deployed that will create events in your calendar with very minimal details needed in the request body. This function has an HTTP endpoint, so it can be hit by several different methods.
Further work could include using Alexa/Google Home to call that HTTP endpoint after adding specific voice prompts. Besides that, your interaction with the Calendar API could be extended by deleting events, modifying existing ones, reading out events occurring today, or others. Security for your Cloud Function could also be a next-step, so that you are sure that only you and your services are able to call your Cloud Function.
Now that you have a Cloud Function deployed in your Firebase project, I’d encourage you to look into several other Firebase services to expand your project even more. Good luck, and I hoped this help you learn something new about a cool, interesting technology!
Zero Equals False delivers quality content to the Software community.
950 
9
950 claps
950 
9
Zero Equals False delivers quality content to the Software community. For more quality content, head over to https://zeroequalsfalse.com
Written by
Just a hockey guy that does computer stuff. https://scottmcc.com/
Zero Equals False delivers quality content to the Software community. For more quality content, head over to https://zeroequalsfalse.com
"
https://towardsdatascience.com/machine-learning-with-tensorflow-on-google-cloud-platform-code-samples-7c1bc07cd265?source=search_post---------98,"Sign in
There are currently no responses for this story.
Be the first to respond.
Lak Lakshmanan
Feb 24, 2018·4 min read
Over the past few months, my team has been working on creating two 5-course specializations on Coursera called “Machine Learning on Google Cloud Platform” and “Advanced Machine Learning on GCP”. The full 10-course journey will take you from a strategic overview of why ML matters all the way to building custom sequence models and recommendation engines.
These courses provide an interactive, practical, pragmatic way to get started doing ML quickly and effectively. While there are many theoretical machine learning courses, my goal with this specialization is to provide practical training, so that you can hit-the-ground running. In order for you to get that jump start, the courses come with lots of open-source, example TensorFlow applications that you can take and train/deploy immediately.
The first course in the Machine Learning on GCP series. How Google Does ML, is now live on Coursera. Please go take the course, and take the remaining courses as they show up once every few weeks.
But even as you are waiting for the awesome team of presenters to teach the courses, the source code for the labs in the specialization is already available. In this blog post, I’ll list what is available in each folder of the GitHub repo:
data_analysis.ipynb shows you how to do data analysis on huge datasets:
mlapis.ipynb shows you how to invoke pre-trained ML models:
repeatable_splitting.ipynb illustrates the importance of repeatable splitting of data
create_datasets.ipynb shows you how to explore and create datasets using Pandas and BigQuery
a_tfstart.ipynb shows you how to use TensorFlow as a numeric software package
b_estimator.ipynb shows you how to write simple ML models in TensorFlow
c_batched.ipynb shows you how to work with large datasets in TensorFlow
d_traineval.ipynb shows you how to do distributed training with TensorFlow
debug_demo.ipynb shows you how to debug TensorFlow programs
e_cloudmle.ipynb shows you how to deploy TensorFlow models and predict with them in a serverless way, with Cloud ML Engine
a_features.ipynb illustrates the importance of representing features correctly
dataflow shows you how to use Apache Beam on Cloud Dataflow for preprocessing
taxifeateng shows you how to implement feature engineering in TensorFlow models
a_handtuning.ipynb shows you how to change a variety of parameters associated with TensorFlow models to gain greater accuracy
b_hyperparam.ipynb shows you how to autotune your TensorFlow models on Cloud ML Engine so that you don’t have do hand-tuning.
c_neuralnetwork.ipynb shows you how to train and predict with distributed neural network models in TensorFlow the easy way.
d_customestimator.ipynb shows you how to take a model that you find in a paper and implement in a way that is distributed and scaleable.
This set of notebooks:
1_explore.ipynb shows you how to explore data using Pandas and BigQuery
2_sample.ipynb shows you how to repeatably split the data
3_tensorflow.ipynb shows you how to build an Estimator model on the data
4_preproc.ipynb shows you how to preprocess data at scale using Dataflow
4_preproc_tft.ipynb shows you how to preprocess data at scale using TF.Transform
5_train.ipynb shows you how to train a TensorFlow model on Cloud ML Engine
6_deploy.ipynb shows you how to deploy a trained model to Cloud ML Engine
serving shows you how to access the ML predictions from web applications and from data pipelines.
Together the above 7 labs summarize the lessons of the previous six courses on a realistic problem, taking you from data exploration to deployment and prediction.
No labs with this one — this is about design and architecture considerations around ML models.
mnist_estimator.ipynb shows you how to build an image classifier using the Estimator API
mnist_models.ipynb shows you how to build a custom estimator with all the tricks (convolutional layers, augmentation, batch normalization, etc.) that go into a good image classification model
flowers_fromscratch.ipynb shows you how to apply the image model in the previous notebook to “real” images.
Labs on Transfer learning and AutoML are not in GitHub since they don’t involve any coding — just point & click!
sinewaves.ipynb show you how to build a time-series forecasting model in TensorFlow using a variety of techniques including CNNs and LSTMs.
temperature.ipynb illustrates how hard LSTMs can be to get right
txtcls1.ipynb shows you how to build a from-scratch text classification model using a variety of techniques including CNNs and LSTMs
txtcls2.ipynb shows you how to use pretrained word embeddings in a text classification model.
word2vec.ipynb shows you how to create a word embedding from your own dataset.
poetry.ipynb shows you how to use Tensor2Tensor to solve your own text problems, whether it is text summarization or text generation
content.ipynb shows an example of a content-based recommendation system
wals.ipynb shows you how to build a collaborative filtering recommendation system in TensorFlow
wals_tft.ipynb makes the collaborative filtering model production-ready by adding in a tf.transform pipeline to map unique user-ids and item-ids automatically.
As ML matures, there will be more labs, of course, and some of the labs above may prove unnecessary. The GitHub repo is a living repository and we plan to keep it up-to-date and reflective of recommended TensorFlow practice. So, if you are building your own ML models, the notebooks (and their corresponding model directories) are a great starting point.
Happy exploring!
Data Analytics & AI @ Google Cloud
826 
7
Every Thursday, the Variable delivers the very best of Towards Data Science: from hands-on tutorials and cutting-edge research to original features you don't want to miss. Take a look.
826 claps
826 
7
Your home for data science. A Medium publication sharing concepts, ideas and codes.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/firebase-developers/how-to-schedule-a-cloud-function-to-run-in-the-future-in-order-to-build-a-firestore-document-ttl-754f9bf3214a?source=search_post---------99,"There are currently no responses for this story.
Be the first to respond.
It frequently comes up that it would be very helpful to have a Cloud Function run at a particular point in time in the future. When the Firebase team released scheduled functions, that provided a solution for regularly recurring functions invocations, like a cron job. But that didn’t exactly address the case where only a single function invocation was needed at a very specific time, or on a delay.
You could certainly use a scheduled function to run frequently, checking some schedule of your creation (perhaps a database) to see if any work is overdue at the moment of invocation. This works for some cases, but has a few downsides:
In particular, here are some situations that don’t work very well with scheduled polling, and could be helped by more granular scheduling of one-off work to be executed on an exact delay:
For these types of situations, you’re definitely better off with a cheaper and more flexible approach than a scheduled function. The solution I recommend is using Cloud Tasks to schedule the invocation of a “callback” function at the time the followup work needs to be done.
Cloud Tasks is a fully managed service that allows you to manage the execution, dispatch and delivery of a large number of distributed tasks. It’s very versatile, but for the purpose of this post, we can essentially use it as a scheduling mechanism to invoke an Cloud Functions HTTP trigger at a specific time in the future.
With Cloud Tasks, you first create a queue, optionally configure it, then add tasks to the queue using the provided SDK. An individual task can be configured to make an HTTP request with a payload that describes the work to be done. I like to think of the HTTP function as a kind of “future callback” that receives instructions on what to do at the time it was invoked.
One generalized use case that’s a good fit for Cloud Tasks is giving a Firestore document a TTL (“Time To Live”), otherwise known as an expiration date. The idea is to have the document automatically deleted at some point in the future after it was created. Perhaps the document represented something temporary or transient, and needs to be cleaned up at the expiration time.
For the purpose of this blog, imagine implementing a message board that allows people to post messages to it by writing a document into a Firestore collection called “posts”. What we’ll do is allow for the user to arrange for their messages to expire and be removed from the collection after a configurable amount of time.
An important note about Cloud Tasks limitations: you should familiarize yourself with the documentation on quota and limits. For the purpose of this post, know that you won’t be able to schedule a task to execute greater than 30 days in the future.
I’ll assume you’re already familiar with Firestore and Cloud Functions. I’m using the Firebase CLI to deploy functions written in TypeScript. Here’s a diagram of the system I’m about to describe:
Cloud Tasks is a billed Cloud product, so you’ll need to have billing enabled on your project if it’s not already. A breakdown of the pricing for Cloud Tasks is here (IMO, it’s pretty cheap: you get to schedule 1 million tasks for free every month).
First, Enable the Cloud Tasks API in the Cloud console. You won’t be able to do this in the Firebase console, so you’ll have to get a little comfortable with the Cloud console for this one.
Follow the documentation to create a queue. You’ll need to have the gcloud CLI installed and configured for your Firebase project. It basically involves running a single command to create a named queue. I’ll call mine “firestore-ttl”:
gcloud might give you a warning, but you can ignore it as long as the output includes a statement that the queue was created.
With Cloud Tasks set up, you can now start using it in your function code. What we need are two functions, one to trigger when a document is created, and another for Cloud Tasks to invoke when it’s time for it to be deleted. Cloud Tasks provides a client SDK for Node. Install it with npm:
Now, for the function code, I’ll import the usual Firebase modules in addition to the newly added @google-cloud/tasks module. Notice that I’m using a JavaScript require instead of an import for @google-cloud/tasks because it doesn’t yet support import syntax.
Next, I’ll define the Firestore onCreate trigger that gets invoked whenever a document in the “posts” collection is created.
I need to know when the user intends to expire the document, so I’ll support the addition of two fields in the document that specifies the time of expiration. expiresIn is expressed as a number of seconds from the current time. expiresAt is a timestamp field that gives the exact time of expiration. The code that creates the document can use either one. I’ll express these additions as a TypeScript interface:
Inside the onCreate callback, I’ll pull the document data out, cast it, and perform some checks. What I want is for expirationAtSeconds to be the time of expiration expressed in epoch seconds, no matter which field was used:
Now I need some configuration for the task queue created earlier:
Then use the Cloud Tasks SDK to give me a fully qualified path to this queue. queuePath is going to be a string that uniquely identifes the task.
To fully configure the task, I need to also give it the URL to my callback function and the contents of the payload to deliver.
You can tell from the URL that the function is called “firestoreTtlCallback”, deployed to the same region and project as the task queue. If you’re using this code, you’ll have to make sure this URL string is correct for your deployment region and project. The actual URL will be available in the Firebase console after you’ve deployed the function.
ExpirationTaskPayload is just another interface that’s helping me stay typesafe when reading and writing its contents. It describes the JSON data structure that will be serialized into the payload:
Now, build up the configuration for the Cloud Task:
This configuration is saying the following:
Note that the encoding to base64 is just required by the Cloud Tasks API. The function will still receive the raw stringified JSON.
Now, all I have to do is enqueue the task in the queue I created earlier:
And that’s it for the Firestore trigger. Of course, we still need to write the HTTP callback function to be invoked by Cloud Tasks at the right time:
You can see that it pulls out the parsed JSON body, assumes that it contains an ExpirationTaskPayload type structure, and uses the Firebase Admin SDK to delete the document using the path given in the payload.
You can deploy these functions and exercise them by creating a document in the Firebase console, making sure to include either a expiresIn or expiresAt field in the document. Try an expiresIn value of 5 seconds to see very quickly that everything is working. One quick note about security — HTTP functions are deployed with public access by default when using the Firebase CLI, which could be a problem here, so you’ll want to read more about this later on in this post.
Let’s say we would like to allow the user to also be able to cancel the future expiration of the document. This is also relatively easy. I’ll implement that with the following additions:
First, the call to create the task must use the returned result:
The ID of the task is in the name property of response. I’ll expand the definition of ExpiringDocumentData to allow for this new field called expirationTask:
And write it back to the document:
Now all we need is the onUpdate trigger to look for field removal, delete the task, and also remove the expirationTask field from the document, which is no longer useful:
And that’s about all there is to it. The complete code can be found in this gist, and at the bottom of this article.
You might be thinking: “This is great, but now can’t anyone delete any document in my database if they just know the URL and payload format of my callback function?” And the answer to that is definitely yes. By default, the Firebase CLI deploys HTTP functions so that they can be invoked by anyone with an internet connection. But this is a solvable problem.
To lock down access to the callback function, you will need to make some changes to the configuration of both the function and the task so that they use a service account to authenticate that the caller of the function is allowed to invoke it. This involves making some advanced configuration changes that are not visible in the Firebase console. First, configure the task to use a service account for authentication when invoking the function. For the function, go to the Cloud console and change the Cloud Functions Invoker role to remove “allUsers” and replace it with that service account, or “allAuthenticatedUsers”. (Note that “allAuthenticatedUsers” doesn’t refer to Firebase Authentication users. It’s a Google Cloud Platform concept that stands for all service accounts.). It will be similar to what you see in step 3 of this tutorial. Learn more about IAM for Cloud Functions and security configuration for Cloud Tasks.
Also, if you are allowing users to specify expiration times directly from your app, you should strongly consider writing some security rules to place some limits on the values of expiresIn and expiresAt. For starters, these values only make sense if they specify a time in the future. You might also want to restrict the min and max times a document can expire. Lastly, you probably don’t want users to be able to modify the expirationTask field, which would effectively allow the user to prevent cancelation of the task (though allowing them to know the string value isn’t an immediate security problem).
If you don’t want to expose anything to users about the expiration of the document, you will probably want to use a second, private document to contain all this data, and change the code to create it through some other mechanism controlled by your backend. You’ll probably also want to change the payload of the task to contain both documents to delete, including both the user-facing document, and the private document with the expiration.
Tutorials, deep-dives, and random musings from Firebase…
1.6K 
15
A summary of what has happened on the Firebase Developers publication in the past quarter. Sent once a quarter. Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
1.6K claps
1.6K 
15
Written by
firebase-consultant.com, Firebase GDE, engineer, developer advocate, Xoogler
Tutorials, deep-dives, and random musings from Firebase developers all around the world. Views expressed are those of the authors and don’t necessarily reflect those of Firebase or its parent companies.
Written by
firebase-consultant.com, Firebase GDE, engineer, developer advocate, Xoogler
Tutorials, deep-dives, and random musings from Firebase developers all around the world. Views expressed are those of the authors and don’t necessarily reflect those of Firebase or its parent companies.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://debugger.medium.com/how-to-make-a-cloud-free-security-camera-for-3-a2f2b467bfa5?source=search_post---------100,"Sign in
There are currently no responses for this story.
Be the first to respond.
Dave Gershgorn
Mar 12, 2021·6 min read
I like the idea of having a security camera, but there are two drawbacks to most of the options on the market. They either send video to the cloud, meaning you’re not in control of that data and typically have to pay a monthly fee, or they cost more than…
"
https://blog.graph.cool/introducing-prisma-cloud-a-graphql-database-platform-ed591baa8737?source=search_post---------101,
https://medium.com/aws-enterprise-collection/6-strategies-for-migrating-applications-to-the-cloud-eb4e85c412b4?source=search_post---------102,"There are currently no responses for this story.
Be the first to respond.
“How emigration is actually lived — well, this depends on many factors: education, economic station, language, where one lands, and what support network is in place at the site of arrival.” -Daniel Alarcón
This post outlines 6 different migration strategies we see customers implement to migrate applications to the cloud. These strategies build upon the 5 R’s that Gartner outlined here in 2011. This is the final installment of a three-part series on migrations. The first post in this series introduces the concept of a mass migration, which we’ll simply refer to as “migration” throughout the series, and the second post of the series describes A Process for Mass Migrations to the Cloud. While each of these posts stands on its own, I believe they go better together.
Enterprises typically begin to contemplate how to migrate an application during the second phase of the “Migration Process” — Portfolio Discovery and Planning. This is when they determine what’s in their environment, what are the interdependencies, what’s going to be easy to migrate and what’s going to be hard to migrate, and how they’ll migrate each application.
Using this knowledge, organizations can outline a plan (which should be considered subject to change as they progress through their migration and learn) on how they’ll approach migrating each of the applications in their portfolio and in what order.
The complexity of migrating existing applications varies, depending on the architecture and existing licensing arrangements. If I think about the universe of applications to migrate on a spectrum of complexity, I’d put a virtualized, service-oriented architecture on the low-complexity end of the spectrum, and a monolithic mainframe at the high-complexity end of the spectrum.
I suggest starting with something on the low-complexity end of the spectrum for the obvious reason that it will be easier to complete — which will give you some immediate positive reinforcement or “quick wins” as you learn.
The 6 most common application migration strategies we see are:
We find that many early cloud projects gravitate toward net new development using cloud-native capabilities, but in a large legacy migration scenario where the organization is looking to scale its migration quickly to meet a business case, we find that the majority of applications are rehosted. GE Oil & Gas, for instance, found that, even without implementing any cloud optimizations, it could save roughly 30 percent of its costs by rehosting.
Most rehosting can be automated with tools (e.g. CloudEndure Migration, AWS VM Import/Export), although some customers prefer to do this manually as they learn how to apply their legacy systems to the new cloud platform.
We’ve also found that applications are easier to optimize/re-architect once they’re already running in the cloud. Partly because your organization will have developed better skills to do so, and partly because the hard part — migrating the application, data, and traffic — has already been done.
2. Replatforming — I sometimes call this “lift-tinker-and-shift.”
Here you might make a few cloud (or other) optimizations in order to achieve some tangible benefit, but you aren’t otherwise changing the core architecture of the application. You may be looking to reduce the amount of time you spend managing database instances by migrating to a database-as-a-service platform like Amazon Relational Database Service (Amazon RDS), or migrating your application to a fully managed platform like Amazon Elastic Beanstalk.
A large media company we work with migrated hundreds of web servers it ran on-premises to AWS, and, in the process, it moved from WebLogic (a Java application container that requires an expensive license) to Apache Tomcat, an open-source equivalent. This media company saved millions in licensing costs on top of the savings and agility it gained by migrating to AWS.
3. Repurchasing — Moving to a different product.
I most commonly see repurchasing as a move to a SaaS platform. Moving a CRM to Salesforce.com, an HR system to Workday, a CMS to Drupal, and so on.
4. Refactoring / Re-architecting — Re-imagining how the application is architected and developed, typically using cloud-native features.
This is typically driven by a strong business need to add features, scale, or performance that would otherwise be difficult to achieve in the application’s existing environment.
Are you looking to migrate from a monolithic architecture to a service-oriented (or server-less) architecture to boost agility or improve business continuity (I’ve heard stories of mainframe fan belts being ordered on e-bay)? This pattern tends to be the most expensive, but, if you have a good product-market fit, it can also be the most beneficial.
5. Retire — Get rid of.
Once you’ve discovered everything in your environment, you might ask each functional area who owns each application. We’ve found that as much as 10% (I’ve seen 20%) of an enterprise IT portfolio is no longer useful, and can simply be turned off. These savings can boost the business case, direct your team’s scarce attention to the things that people use, and lessen the surface area you have to secure.
6. Retain — Usually this means “revisit” or do nothing (for now).
Maybe you’re still riding out some depreciation, aren’t ready to prioritize an application that was recently upgraded, or are otherwise not inclined to migrate some applications. You should only migrate what makes sense for the business; and, as the gravity of your portfolio changes from on-premises to the cloud, you’ll probably have fewer reasons to retain.
What’s your migration experience been? I’d love to hear about it, and host it on my blog!
Keep building,- Stephenorbans@amazon.com@stephenorbanRead My Book: Ahead in the Cloud: Best Practices for Navigating the Future of Enterprise IT
Note: “Migration” is the third of four “Stages of Adoption” I’m writing about in the Journey to Cloud-First series. The first stage is “Project.” The second stage is “Foundation.” After “Migration” comes “Reinvention.” This series follows the best practices I’ve outlined in An E-Book of Cloud Best Practices for Your Enterprise. Stay tuned for more stories posts covering each of these stages.
Both of these series are now available in my book Ahead in the Cloud: Best Practices for Navigating the Future of Enterprise IT.
Tales of AWS in the Enterprise
600 
10
600 claps
600 
10
Written by
Husband to Meghan, father to Harper and Finley. GM of AWS Data Exchange (formerly Head of Enterprise Strategy for AWS). Author of “Ahead in the Cloud”.
Tales of AWS in the Enterprise
Written by
Husband to Meghan, father to Harper and Finley. GM of AWS Data Exchange (formerly Head of Enterprise Strategy for AWS). Author of “Ahead in the Cloud”.
Tales of AWS in the Enterprise
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@sathishvj/on-passing-all-google-cloud-certifications-54b2cc1e428c?source=search_post---------103,"Sign in
There are currently no responses for this story.
Be the first to respond.
Top highlight
sathish vj
Jan 8, 2019·15 min read
Subscribe to my YouTube channel that teaches you to apply Google Cloud to your projects and also prepare for the certifications: youtube.com/AwesomeGCP. Check out the playlists I currently have for Associate Cloud Engineer, Professional Architect, Professional Data Engineer, Professional Cloud Developer, Professional Cloud DevOps Engineer, Professional Cloud Network Engineer, and Professional Cloud Security Engineer.
My experience with and topics for the Profession Data Engineer, Professional Cloud Architect, beta Professional Cloud Developer, Associate Cloud Engineer exams, beta Professional Network Engineer, the beta Professional Security Engineer, the Professional Collaboration Engineer and the G Suite exams. Edit: it’s not “All” certifications anymore as some new ones have been released.
If there was one caption to describe my experience writing each of the google cloud certification exams, it is this: confident before, doubtful during, relieved after. I would also prefix a ‘very’ to each of those parts. That should give you a view into what the exam is like. There’s quite a bit to study before, for sure. Having covered that reasonably, I walk into the exam with confidence. But the exam is not easy — non-direct questions, multiple areas combined into one question, “out-of-syllabus” questions on general concepts, questions/options about obscure sub-services and details that I’ve never heard of before, and finally, answer options that are all right but we have to pick the ‘best or the recommended way’. During each exam, I honestly wasn’t sure if I would pass. And it was a big relief to see a “Provisional Result: PASS” at the end each time. Whew!
But let’s step back a bit. And let me take you through my experience in a little more detail. Today, I have all of the Google Cloud certifications: Professional Data Engineer, Professional Cloud Architect, and Associate Cloud Engineer. I also wrote the beta Professional Cloud Developer but the results will be out only a few months from now. (Update on Jan 24th 2019: the Professional Cloud Developer exam is out of beta now and I passed!)(Update on March 13th 2019: the Professional Network Engineer exam is out of beta now and I passed!)(Update on March 29th 2019: the Professional Cloud Security Engineer exam is out of beta now and I passed!)(Update in January 2020: I got the Professional Collaboration Engineer and also the G Suite certification.)
I wrote the associate engineer exam in mid November, 2018. Then bunched up the remaining 3 for the two weeks when I got time to myself to prepare around Christmas and New Year. It worked for me, luckily. And i think there is a reason this worked. Read on.
I don’t think so. I don’t think it is easy to pass all exams without wide knowledge of software development and systems in general and some knowledge of almost all offerings from Google Cloud — and there are over a hundred of them. The exams don’t go very deep in all of them (a mile wide and a few inches deep), but you will need to understand enough to know the concepts and best/recommended practices. People write blog posts only about their passing, but I really wonder how many failed. I’ve seen only a few people openly discussing how they flunked their exam the first time and how they are preparing again for it.
Without realising it, for quite a while. The certifications were not in my sights until the last quarter of 2018. However, I’d been using GCP for projects for a few years now. A bunch of my hackathon projects, personal projects, and static web sites (all from the early part of this decade) were made on Google AppEngine Standard. I had completed almost every Coursera specialisation on GCP starting from end 2017. I’d also used Firebase for a few of my Ionic/Angular projects. I owned domain names that I’d setup with GSuite and created AppEngine projects within them. I’d also done some work on Kubernetes and Docker during my stint at Red Hat. From early on in 2018, I had also gluttonously devoured (went super fast through) Andrew Ng’s machine learning courses on Coursera followed by Google’s specialisation in Machine Learning and also the Google Machine Learning Crash Course. So I had been learning merely for the sake of figuring out these technologies and using GCP for my projects for more than a year. The certifications weren’t yet a goal.
My original plan somewhere around October, probably, was to write the Professional Cloud Architect (PCA) only. Then I found the Associate Cloud Engineer (ACE) certification and thought it might be a simpler first step and digressed to write that first. Since the time I decided to attempt the certification, I prepared across two+ months for all four certifications. Within that, I actually got less than a month because I was traveling around visiting various clients. For the Architect exam, I had been preparing little by little with a burst of activity just prior to the exam. I hadn’t done too much prior work in the Professional Data Engineer (PDE) subjects, so I spent about two focused weeks, studded with a few Christmas and New Year parties, on preparing for the Data Engineer exam. I didn’t really prepare for the beta Professional Cloud Developer (bPCD) because there wasn’t any specific material to prepare with.
One for all, all for one.
An aspect that clearly stood out for me was the overlap across all the certifications. If you want to pass any one exam, you have to study for all of them. IAM, GCS, and Stackdriver are universal — every exam had these sections prominently. (Stackdriver questions were a little less in my Data Engineer exam, but there still were a few.) Beyond that, every exam had a piece of every other one. My Professional Architect exam had quite a few questions on Datastore. Usage and setup of Dataproc, Dataflow, and Pub Sub was everywhere. Firebase is not part of the exams, but they featured in some of the options. Dataprep and Datastudio featured in the developer exams. You might not fully understand the Data* products, but you have to know how they are deployed and the use cases where they are best employed. BigQuery was there in the ACE, PCA, PDE, bPCD, but probably a little less in the PCA. Kubernetes was there in all except for the PDE.
Given the considerable overlap, my key advice is to watch all tutorials/training videos for all certifications, and then focus on practice questions for the specific exam you intend to take.
To pick up from where I left off earlier, taking all certifications wasn’t my goal. Having skimmed through the courses though, I realised that there was way more to GCP than just the parts I was repeatedly using. So to get a better understanding of the possibilities and arm myself with a wider repertoire of tools, I decided that I’d try the Professional Architect exam. I then realised that there was considerable overlap between the exam content. I checked the ACE exam and realised that because I’d coded GCP projects already, maybe the AE exam would be an easier intermediate step combined with the learning I’d done for the PCA. So the ACE certification happened. Later when I got time, I wrote the PCA and passed that. Meanwhile, a client of mine wanted some help with data engineering on GCP and I wondered if I could do the PDE also. In learning for that, I again realised that I could, with the knowledge from ACE+PCA+PDE and maybe just a little more, I could probably attempt the bPCD exam. So that happened quite suddenly — I just booked it one day and went for it the next. For the bPCD, there wasn’t any material or questions or courses online yet, so there was nothing I could specifically prepare for. Finally, I also took the PDE certification, which required the most learning for me. But having written all the other exams, the PDE was way less daunting.
Edit 23rd April 2020: It has been more than a year since I wrote this. I see more courses now and more practice exams. These didn’t exist when I was writing the exam. Even the courses that I took at the time might have improved or become worse with time. So, please take this in the context of the time.
I went all out on courses, because I also had an agenda to evaluate them and suggest them to clients I’m working with. Coursera is the closest to the exam. My wild guess is that the original exams were created/reviewed by some of those who created the Coursera training material. However, often they barely skimmed the surface on some topics. But then, no course I have seen so far comprehensively covered all the material related to the exam questions. All of them were ‘introductory’ in nature. On the exam, every once in a while the questions were from a remote corner of the documentation. GCP products and services that were not even mentioned in the courses were part of the options. The documentation, therefore, is the most comprehensive but you clearly can’t go over it all. I also went through the Linux Academy and Cloud Academy courses. Both were useful; incrementally so from the Coursera one. Yet, some of those modules were very useful because the instructors there have taken the exam and they give quite a few tips based on their experience. (For tips based on my experience, follow the links at the end.) The Linux Academy course also has a good readout and analysis of the case studies which were very useful. The practice questions at the end of these courses were too simplistic and unlike the actual exam questions. The exam questions are convoluted and long, while the practice tests are straightforward. If you go into the exam with only that practice, you’re going to be badly surprised. The practice test on the google certification site is the closest to the real one. Take that and get used to those lengthy questions. I had to spend quite some time reading and re-reading some of the 50 questions in the real exam and took the full 120 minutes to finish, whereas I could finish a 50 question practice test on some of these online courses in 15 minutes. I tried a few courses and practice tests on Udemy. Almost all of them were derivative or ridiculously bad. There was one PCA practice test set which was reasonable. A bunch of questions in this were copied from other places and sometimes (I felt) the answers were wrong, but still.
After all the certifications, my best guess would be - below average. I thought I knew GCP reasonably well, but now that I know more, I realise I know less. There is so much more to GCP than I will ever be able to grasp and they are releasing new products and services and updates to them regularly. Some parts of the internal GCP engineering is nothing but astounding, but we’re not even talking about internals of that sort. Even at a high level, my ignorance is substantial. I haven’t personally worked on Hadoop+Spark=Dataproc except in small doses. I haven’t worked with Apache Beam yet, but I like it now and I’m keen to work on it especially since it also has a Go library now. My knowledge of Machine Learning is middling with only a little knowledge of Tensorflow, but I have coded full fledged applications that use Cloud Vision and AutoML. I’ve done projects on AppEngine standard but not Flexible. I’ve done a little bit of DataPrep and DataStudio, but not much. This list goes on and on. Then there are products that I’ve never used in any of my projects yet — like Cloud Armor, BigTable, Build. My hope is only to continue learning, work on small projects, and keep trying out what I see in blogs and tutorials. What the certifications have given me though, apart from a slap of humility, is a wide view of what is possible with GCP. Instead of a hammer that I used everywhere previously, I now have a wider array of tools and better knowledge about tools (if not in-depth knowledge of tools) to make architectural choices from.
What happens if the time runs out and I haven’t submitted the responses?
I didn’t know the answer to this, so I asked the Google Certifications support. I got this response: “your responses will be automatically be recorded and submitted. Please allow 7 days for Google to confirm receipt of your exam results. Upon receiving your record, Google will complete an evaluation of your exam record including compliance with the Terms and Conditions.”
I got a provisional result immediately and it’s been a few days. Why haven’t I got the mail with the certification?
I got some certification mails immediately (of course, this does not apply for any of the beta tests), while some others took a few days. As seen in the response to the above question, the final certificate could take as long as a week. When I didn’t receive the mail for a few days, I started questioning myself whether I had actually seen a provisional Pass result or whether I had just imagined it. If you want to calm yourself, you can login to the webassessor site where you registered for the exam and see your provisional result.
Edit 2019/11: Which exam should I take first?
I would strongly recommend that you take the Associate Engineer exam first.
I’ve seen people feeling devastated and demotivated after taking one of the Professional exams and flunking. My suggested path for them is to try the ACE first and then come back to the suitable professional exam.
How long will it take for me to study for the exam?
It depends. There are people who have apparently passed it with a week of study and others who have flunked after months of study. So it depends on how much you already know, whether you have the relevant experience, how much you prepare, and your ability to take multiple choice tests well. All of that, as you can see, depends on you.
For those appearing for the various certification exams, here is a list of sanitized notes (no direct question, only general topics) about the exam.
Overall notes across all GCP certification exams
Notes from the Professional Cloud Architect exam
Notes from the beta Professional Cloud Developer exam
Notes from the Professional Data Engineer exam
Notes from the Associate Cloud Engineer exam
Notes from the beta Professional Cloud Network Engineer Exam
Notes from the beta Professional Cloud Security Engineer Exam
Notes from the Professional Collaboration Engineer Exam
Notes from the G Suite Exam
Notes from the Professional DevOps Engineer Exam
Notes from the Professional Machine Learning Engineer Exam
What next for me on GCP? Keep using GCP and keep learning. Apart from that, I’m hoping to train more people in using Google Cloud, getting their certifications, and do short term consulting. If there are such opportunities, do reach out to me on LinkedIn.
p.s. for those writing their own recap after the exam, please note that you can only discuss the topics as given in the exam outline. I got a mail from the Google Certification team not to disclose points like the number of questions per topic or paraphrasing the questions. I edited out the few instances I’d mentioned the number of questions I got, even if it was descriptive phrases like “many questions”, “hardly any questions”, “no questions”.
A collection of posts, videos, courses, qwiklabs, and other exam details for all exams: https://github.com/sathishvj/awesome-gcp-certifications
Collection of free QwikLabs codes for practice: https://medium.com/@sathishvj/qwiklabs-free-codes-gcp-and-aws-e40f3855ffdb
Check the FAQs here: https://medium.com/@sathishvj/frequently-asked-follow-up-questions-on-google-cloud-gcp-certifications-438e1addb91d.
Wish you the very best with your GCP certifications. You can reach me at LinkedIn and Twitter. If you can support my work creating videos on my YouTube channel AwesomeGCP, you can do so on Patreon or BuyMeACoffee.
tech architect, tutor, investor | GCP 12x certified | youtube/AwesomeGCP | Google Developer Expert | Go
See all (32)
871 
16
871 claps
871 
16
tech architect, tutor, investor | GCP 12x certified | youtube/AwesomeGCP | Google Developer Expert | Go
About
Write
Help
Legal
Get the Medium app
"
https://blog.brand.ai/how-to-share-a-design-system-in-sketch-1-3-245308f2d7f1?source=search_post---------104,"InVisionApp, Inc.
InVisionApp, Inc.
Freehand is the visual collaboration tool disguised as an online whiteboard.
InVisionApp, Inc.
By using InVision brand assets you agree to follow our guidelines packaged with the media kit. For further information about use of the InVision brand assets, please contact support@invisionapp.com.

"
https://medium.com/hackernoon/cloud-computing-for-beginners-85d168959afb?source=search_post---------105,"There are currently no responses for this story.
Be the first to respond.
Introduction to Cloud
Following is an excerpt from the book “Cloud Is a Piece of Cake”. Learn more about the book here.
Consider you wanted to host a movie booking service. Before cloud, you had to buy a physical server and host it. So whenever a customer wants to book a ticket, he sends a request from his computer to your server. The server processes the request and sends data back to the client (customer’s computer). If the server and the client were having a conversation, it would look like this.
Client: I need tickets for Captain America: Civil War.
Server: Here are the available seats for the movie {P1, P2, P4, A3 …}
Client: I am booking P1, P2
Server: Blocking the seats, awaiting payment
Client: Here are my payment details {CardNo: XXXXXX, CVV…}
Server: Congrats, they are booked.
The customer’s computer and the server communicate with packets of data.
Your server would be able to handle 1000’s of such requests based on its configuration.
However, what happens when the demands exceed the server’s limit? Say 10k — 20k requests? It is the same case when ten to twenty people are pinging you on Whatsapp. You will attempt to answer them all one by one. However, you would take a lot of time. However, on the internet this delay is unacceptable.
Why can’t we add more servers?
Yes, that’s a solution. Say you buy or rent ten more servers. Now they can work as a team and share the load. That works well. However, your service won’t have such requests all the time. Compare the first day of the Civil War Movie to the fiftieth day. The number of requests drops down. The additional servers you bought are now jobless. You will be still paying for their electricity and bandwidth. In some situations, you can never predict when requests are at peak. So it’s necessary for the servers to be ready. When the next peak time arrives, your servers may be outdated. Hosting it physically has a lot of other problems like power outages, maintenance cost, etc. Since this involves wastage of resources, this isn’t an effective solution. The cloud was developed to tackle this.
How can the cloud help?
In the cloud, we have a shared pool of computer resources (servers, storage, applications, etc.) at our disposal. When you need more resources, all you need is to ask. Provisioning resources immediately is a piece of cake for the cloud. You can free resources when they are not needed. In this way, you only pay for what you use. Your cloud provider will take care of all the maintenance.
Where is the cloud?
The shared pool of computer resources exists in a physical location called data centers. Your cloud providers have multiple data centers around the world. So your data is replicated at multiple sites. Even if a data center goes down because of a natural calamity, it’s still safe in another location.
What are IaaS, PaaS and SaaS?
In IaaS (Infrastructure as a Service) you are given materials like cement, bricks, sheets, etc. to build a house. Similarly, here you get to choose the hardware you want to make the cloud service. You have got the flexibility to make it in the way you want. Ex: Amazon Web Services, Microsoft Azure, Google Compute Engine, etc.
In PaaS (Platform as a Service) the house is built for you, you only need to furnish it. Similarly, here you are provided preconfigured hardware. So it can only run applications it supports. You don’t get the flexibility when compared with IAAS. Ex: Heroku, Google App Engine, etc.
SaaS (Software as a Service) all you need to occupy. Here you are offered software on a subscription basis. Ex: Gmail, Yahoo, etc.
Enjoyed reading this? Do you want to learn to create virtual machines, build a cloud app, secure and scale it? The above is just an excerpt from the book Cloud Is a Piece of Cake .
Hacker Noon is how hackers start their afternoons. We’re a part of the @AMI family. We are now accepting submissions and happy to discuss advertising & sponsorship opportunities.
If you enjoyed this story, we recommend reading our latest tech stories and trending tech stories. Until next time, don’t take the realities of the world for granted!
#BlackLivesMatter
830 
10
how hackers start their afternoons. the real shit is on hackernoon.com. Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
830 claps
830 
10
Written by
Author of Cloud Is a Piece of Cake
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
Author of Cloud Is a Piece of Cake
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@scarygami/cloud-firestore-quicktip-documentsnapshot-vs-querysnapshot-70aef6d57ab3?source=search_post---------106,"Sign in
There are currently no responses for this story.
Be the first to respond.
Gerwin Sturm
Oct 11, 2017·2 min read
Since the Firebase team launched Cloud Firestore last week, one question/problem that keeps popping up on Stackoverflow is the result of there being two different ways to retrieve data that have to be handled slightly differently. So after answering several of those questions I though it would be a good idea to put together this short article.
The samples I’m using are in JavaScript, but the behavior is essentially the same for all the other SDKs as well, and I have relevant links at the end.
Retrieving a single document from Cloud Firestore is done with the following steps.
2. Call get() on the DocumentReference to get a DocumentSnapshot.
3. Check if the document you specified actually exists.
4. Retrieve and use the data.
When querying a collection you can get zero, one or more documents as a result, by going through the following steps.
2. (Optional) Create a Query based on the collection.
3. Call get() to get a QuerySnapshot.
4. Check whether there are any documents in the result.
or
5. Retrieve and use the data. Two different ways to do so:
or
Basic query documentation
Detailed references for the different SDKS: Android / GO / Java / JavaScript / Node.js / Objective-C / Python / Swift
Business applications and web developer, Google Developer Expert for Web Technologies until 2017, World traveller
1.5K 
5
Some rights reserved

1.5K 
1.5K 
5
Business applications and web developer, Google Developer Expert for Web Technologies until 2017, World traveller
"
https://blog.bitsrc.io/using-airtable-as-a-handy-cloud-storage-for-your-apps-ae6cce0efe11?source=search_post---------107,"Airtable is an online service that you can use as a very convenient backend storage option.
Why is it convenient? Because:
The Airtable company provides the basic service for free, with some limitations you need to be aware of.
Tip: Use Bit to reuse components between apps. It helps your team organize and share components, so you can build new apps faster. Give it a try.
bit.dev
The API has a limit of 5 requests per second, which is not high, but still reasonable to work with for most scenarios.
Using the Airtable Node.js library which I’ll introduce later on, you don’t have to worry about handling those rate limits yourself, since it comes with built-in logic to handle them, plus an automatic repeat of any rejected request.
The free plan has a limit of 1200 records per each base, which means across all the base tables. Paid plans allow more entries.
One of the things you’ll likely appreciate the most as a developer is a good API documentation.
AirTable has a very useful documentation and one of the those I like to point out when it comes to having a great example of docs for developers.
The examples are many and the parameters you can pass are all well documented.
They provide a set of URL and Node.js examples, and all of those are based on the actual data you have in your tables.
Also, any API call parameter set already comes with your API keys and tables and view names pre-filled. It’s great because you don’t have to look at a random example and switch context, and you can work with your own data.
When working with Airtable you’ll find some terms familiar if you used databases before, and some unique to Airtable.
A base is a database, a set of tables related to your application.
A table is one set of data, organized in columns
A view is a particular representation of a table. You can have the same data presented in different views.
You always reference one view, one table, and one base to fetch your data.
Tip for SQL users: a view is not what you are used to; every table has at least one view.
Once you create an Airtable account you can see the list of sample bases. You can create a new base by selecting a workspace, and clicking “Add a base”:
You’ll be presented with a list of choices.
You can start with a template, and Airtable provides a lot of different templates to choose from:
You can also import an existing spreadsheet, or start from scratch, which is what we’ll do now.
Choose a color and an icon:
And clicking the base icon will open the table view. Here you have the visualization of your “My base” base, showing the “Table 1” table, with the “Grid view” view:
Edit the table fields
From here, clicking the down arrow at the right of each column gives you many choices. Use “Rename field” and “Customize field type” to create the table structure you want.
Every field has a type. You can choose from many predefined fields, which should satisfy most of your needs:
Enough with the description of Airtable as a product. Let’s see how to work with its API.
For the sake of following the examples, let’s keep the default column names and types.
Airtable provides an official Node.js library, available on npm as airtable. Install it in your app using
Once the library is installed, you can use it in a Node.js app
where API_KEY is an environment variable you configure before running the program.
As you work with your API, you are likely to use other variables that set the name of a base, the name of a table, and the name of a view:
Use those to initialize a base, and to fetch a table reference:
The VIEW_NAME variable will be used when selecting elements from a table, using the table.select() method.
Let’s now see how to perform some very common data operations.
You can add a new record by calling the create() method on the table object. You pass an object with the field names (you don’t need to pass all the fields, and the ones missing will be kept empty).
In this case, if there is an error we just log it, and then we output the id of the row just entered.
You can get a specific record by using the find() method on the table object, passing a record id:
From the record object returned by find() we can get the record content:
You can get the record id using record.getId() and you can get the time the record was created using record.createdTime.
Update one or more fields of a record, using the update method of the table object:
In this case, fields not mentioned are not touched: they are left with whatever value they were before.
To clean the unlisted fields, use the replace() method:
To delete a record from the table, call the destroy() method of the table object:
So far we’ve seen how to operate with a single record object. How can you iterate upon all the records in a table? Using the select method of the table object:
notice we used the firstPage() method. This gives us access to the first page of records, which by default shows the first 100 items.
If you do have more than 100 items in your table, to get access to the other records you have to use the pagination functionality. Instead of calling firstPage() you call eachPage(), which accepts 2 functions as arguments.
The first, processPage(), is a function that is called upon every page of records. In there, we simply add the page records (available through the first parameter, filled automatically by the Airtable API).
We add those partial set to the allRecords array, and we call the fetchNextPage()function to continue fetching the other records.
The processAllRecords() function is called when all the records have been successfully fetched. There is one single parameter here, which contains an error object if there is an error, otherwise, we are safe to use the allRecords array values.
Airtable is a useful service to use for building prototypes and to build small applications that need a fair amount of data.
It’s great to be able to see the data and edit it through a well-thought user interface. It’s especially nice if you have non-technical people that need to interact with that data.
The free service allows up to 1200 records per base, with paid plans you can get up to 5000 if you have the Plus plan, and 50.000 records with the Pro plan, and more using the enterprise plan.
Feel free to comment below, suggest idea or ask anything! thanks 😃
blog.bitsrc.io
blog.bitsrc.io
blog.bitsrc.io
The blog for advanced web development. Brought to you by the Bit community.
1.5K 
4
1.5K claps
1.5K 
4
Written by
I write tutorials for developers at https://flaviocopes.com
The blog for advanced web and frontend development articles, tutorials, and news. Love JavaScript? Follow to get the best stories.
Written by
I write tutorials for developers at https://flaviocopes.com
The blog for advanced web and frontend development articles, tutorials, and news. Love JavaScript? Follow to get the best stories.
"
https://towardsdatascience.com/deploy-machine-learning-pipeline-on-cloud-using-docker-container-bec64458dc01?source=search_post---------108,"Sign in
There are currently no responses for this story.
Be the first to respond.
Moez Ali
May 8, 2020·12 min read
In our last post, we demonstrated how to develop a machine learning pipeline and deploy it as a web app using PyCaret and Flask framework in Python. If you haven’t heard about PyCaret before, please read this announcement to learn more.
In this tutorial, we will use the same machine learning pipeline and Flask app that we built and deployed previously. This time we will demonstrate how to deploy a machine learning pipeline as a web app using the Microsoft Azure Web App Service.
In order to deploy a machine learning pipeline on Microsoft Azure, we will have to containerize our pipeline in a software called “Docker”. If you don’t know what does containerize means, no problem — this tutorial is all about that.
In our last post, we covered the basics of model deployment and why it is needed. If you would like to learn more about model deployment, click here to read our last article.
This tutorial will cover the entire workflow of building a container locally to pushing it onto Azure Container Registry and then deploying our pre-trained machine learning pipeline and Flask app onto Azure Web Services.
PyCaret is an open source, low-code machine learning library in Python that is used to train and deploy machine learning pipelines and models into production. PyCaret can be installed easily using pip.
Flask is a framework that allows you to build web applications. A web application can be a commercial website, blog, e-commerce system, or an application that generates predictions from data provided in real-time using trained models. If you don’t have Flask installed, you can use pip to install it.
Docker is a tool designed to make it easier to create, deploy, and run applications by using containers. Containers are used to package up an application with all of its necessary components, such as libraries and other dependencies, and ship it all out as one package. If you haven’t used docker before, this tutorial also covers the installation of docker on Windows 10.
Microsoft Azure is a set of cloud services that is used to build, manage and deploy applications on a massive and global network. Other cloud services that are often used for deploying ML pipelines are Amazon Web Services (AWS), Google Cloud, IBM Cloud and Alibaba Cloud. We will cover most of them in our future tutorials.
If you haven’t used Microsoft Azure before you can sign up for a free account here. When you sign up for the first time you get a free credit for the first 30 days. You can utilize that credit in building your own web app by following this tutorial.
Have you ever had the problem where your python code (or any other code) works fine on your computer but when your friend tries to run the exact same code, it doesn’t work? If your friend is repeating the exact same steps, they should get the same results right? The one-word answer to this is the environment. Your friend’s Python environment is different than yours.
What does an environment include? → Python (or any other language you have used) and all the libraries and dependencies with the exact versions using which application was built and tested.
If we can somehow create an environment that we can transfer to other machines (for example: your friend’s computer or a cloud service provider like Microsoft Azure), we can reproduce the results anywhere. Hence, a container is a type of software that packages up an application and all its dependencies so the application runs reliably from one computing environment to another.
“Think about containers, when you think about containers.”
This is the most intuitive way to understand containers in data science. They are just like containers on a ship where the goal is to isolate the contents of one container from the others so they don’t get mixed up. This is exactly what containers are used for in data science.
Now that we understand the metaphor behind containers, let’s look at alternate options for creating an isolated environment for our application. One simple alternative is to have a separate machine for each of your applications.
(1 machine = 1 application = no conflict = everything is good)
Using a separate machine is straight forward but it doesn’t outweigh the benefits of using containers since maintaining multiple machines for each application is expensive, a nightmare-to-maintain and hard-to-scale. In short, it’s not practical in many real-life scenarios.
Another alternate for creating an isolated environment are virtual machines. Containers are again preferable here because they require fewer resources, are very portable, and are faster to spin up.
Can you spot the difference between Virtual Machines and Containers? When you use containers, you do not require guest operating systems. Imagine 10 applications running on a virtual machine. This would require 10 guest operating systems compared to none required when you use containers.
Docker is a company that provides software (also called Docker) that allows users to build, run and manage containers. While Docker’s container are the most common, there are other less famous alternatives such as LXD and LXC that provides container solution.
In this tutorial, we will use Docker Desktop for Windows to create a container that we will publish on Azure Container Registry. We will then deploy a web app using that container.
What is the difference between a docker image and a docker container? This is by far the most common question asked so let’s clear this right away. There are many technical definitions available, however, it is intuitive to think about a docker image as a mold based on which container is created. An image is essentially a snapshot of container.
If you prefer a slightly more technical definition then consider this: Docker images become containers at runtime when they run on a Docker Engine.
At the end of the day, docker is just a file with a few lines of instructions that are saved under your project folder with the name “Dockerfile”.
Another way to think about docker file is that they are like recipes you have invented in your own kitchen. When you share those recipes with somebody else and they follow the exact same instructions, they are able to produce the same dish. Similarly, you can share your docker file with other people, who can then create images and run containers based on that docker file.
Now that you understand containers, docker and why we should use them, let’s quickly set the business context.
An insurance company wants to improve its cash flow forecasting by better predicting patient charges using demographic and basic patient health risk metrics at the time of hospitalization.
(data source)
To build and deploy a web application where the demographic and health information of a patient is entered into a web-based form which then outputs a predicted charge amount.
Since we have already covered the first two tasks in our last tutorial, we will quickly recap them and focus on the remaining tasks in the list above. If you are interested in learning more about developing machine learning pipeline in Python using PyCaret and building a web app using Flask framework, you can read our last tutorial.
We are using PyCaret in Python for training and developing a machine learning pipeline which will be used as part of our web app. The Machine Learning Pipeline can be developed in an Integrated Development Environment (IDE) or Notebook. We have used a notebook to run the below code:
When you save a model in PyCaret, the entire transformation pipeline based on the configuration defined in the setup() function is created . All inter-dependencies are orchestrated automatically. See the pipeline and model stored in the ‘deployment_28042020’ variable:
This tutorial is not focused on building a Flask application. It is only discussed here for completeness. Now that our machine learning pipeline is ready we need a web application that can connect to our trained pipeline to generate predictions on new data points in real-time. We have created the web application using Flask framework in Python. There are two parts of this application:
This is how our web application looks:
If you would like to see this web app in action, click here to open a deployed web app on Heroku (It may take few minutes to open).
If you haven’t followed along, no problem. You can simply fork this repository from GitHub. If you don’t know how to fork a repo, please read this official GitHub tutorial. This is how your project folder should look at this point:
Now that we have a fully functional web application, we can start the process of containerizing the app using Docker.
You can use Docker Desktop on Mac as well as Windows. Depending on your operating system, you can download the Docker Desktop from this link. We will be using Docker Desktop for Windows in this tutorial.
The easiest way to check if the installation was successful is by opening the command prompt and typing in ‘docker’. It should print the help menu.
Kitematic is an intuitive graphical user interface (GUI) for running Docker containers on Windows or Mac. You can download Kitematic from Docker’s GitHub repository.
Once downloaded, simply unzip the file into the desired location.
The first step of creating a Docker image is to create a Dockerfile. A Dockerfile is just a file with a set of instructions. The Dockerfile for this project looks like this:
Dockerfile is case-sensitive and must be in the project folder with the other project files. A Dockerfile has no extension and can be created using any editor. We have used Visual Studio Code to create it.
If you don’t have a Microsoft Azure account or haven’t used it before, you can sign up for free. When you sign up for the first time you get a free credit for the first 30 days. You can utilize that credit to build and deploy a web app on Azure. Once you sign up, follow these steps:
Once a registry is created in Azure portal, the first step is to build a docker image using command line. Navigate to the project folder and execute the following code.
Now that the image is created we will run a container locally and test the application before we push it to Azure Container Registry. To run the container locally execute the following code:
Once this command is successfully executed it will return an ID of the container created.
Open Kitematic and you should be able to see an application up and running.
You can see the app in action by going to localhost:5000 in your internet browser. It should open up a web app.
Make sure that once you are done with this, you stop the app using Kitematic, otherwise, it will continue to utilize resources on your computer.
One final step before you can upload the container onto ACR is to authenticate azure credentials on your local machine. Execute the following code in the command line to do that:
You will be prompted for a Username and password. The username is the name of your registry (in this example username is “pycaret”). You can find your password under the Access keys of the Azure Container Registry resource you created.
Now that you have authenticated to ACR, you can push the container you have created to ACR by executing the following code:
Depending on the size of the container, the push command may take some time to transfer the container to the cloud.
To create a web app on Azure, follow these steps:
BOOM!! The app is now up and running on Azure Web Services.
Note: By the time this story is published, the app from https://pycaret-insurance2.azurewebsites.net will be removed to restrict resource consumption.
Link to GitHub Repository for this tutorial.
Link to GitHub Repository for Heroku Deployment. (without docker)
In the next tutorial for deploying machine learning pipelines, we will dive deeper into deploying machine learning pipelines using the Kubernetes Service on Google Cloud and Microsoft Azure.
Follow our LinkedIn and subscribe to our Youtube channel to learn more about PyCaret.
User Guide / DocumentationGitHub RepositoryInstall PyCaretNotebook TutorialsContribute in PyCaret
We have received overwhelming support and feedback from the community. We are actively working on improving PyCaret and preparing for our next release. PyCaret 1.0.1 will be bigger and better. If you would like to share your feedback and help us improve further, you may fill this form on the website or leave a comment on our GitHub or LinkedIn page.
As of the first release 1.0.0, PyCaret has the following modules available for use. Click on the links below to see the documentation and working examples in Python.
ClassificationRegressionClusteringAnomaly DetectionNatural Language ProcessingAssociation Rule Mining
PyCaret getting started tutorials in Notebook:
ClusteringAnomaly DetectionNatural Language ProcessingAssociation Rule MiningRegressionClassification
PyCaret is an open source project. Everybody is welcome to contribute. If you would like contribute, please feel free to work on open issues. Pull requests are accepted with unit tests on dev-1.0.1 branch.
Please give us ⭐️ on our GitHub repo if you like PyCaret.
Medium : https://medium.com/@moez_62905/
LinkedIn : https://www.linkedin.com/in/profile-moez/
Twitter : https://twitter.com/moezpycaretorg1
Data Scientist, Founder & Author of PyCaret
1K 
6
1K claps
1K 
6
Your home for data science. A Medium publication sharing concepts, ideas and codes.
"
https://medium.com/@shundoribou8/familiar-with-master-nodes-take-a-look-into-super-nodes-voter-nodes-and-cloud-nodes-19014ce2a39b?source=search_post---------109,"Sign in
There are currently no responses for this story.
Be the first to respond.
Stacy Green
Jul 13, 2018·4 min read
Allow me to begin with a question.
What is Apex Network?
APEX Network aims is to build stronger 1-to-1 relationships between the enterprise and consumers through proprietary blockchain technology — transforming the exchange process of interactions, information, and value throughout the B2C engagement process. APEX Network will help facilitate increased trust, privacy, and transparency for B2C interactions. It will also optimize efficiency, cost-effectiveness and help eliminate middlemen networks when necessary.
If you are looking into this article I must assume -you are familiar with the top master node coin i.e. DASH which was worth over a million bucks few months back paying you 7 % and you are also familiar with NEO, and how you can stake it and can earn a interest of 3 % annually.
Take a look into the Apex Node Ecosystem.
The role of the Apex Node Ecosystem is functionality,governance and infrastructural support for the Apex Ecosystem.
3 fundamental principles while designing the APEX NODE ECOSYSTEM are :-
Apex has introduced three types of nodes.
Supernodes in the Apex ecosystem can also be understood as a “delegate” or “block producers” as more commonly referred to as DPOS based blockchains. They has the authority to create a new side chains.
INCENTIVE AND REWARD STRUCTURE:-
Network fees=Transaction fees +System fees + Apex virtual machine workload + transfer of assets + enterprise Assets DEX fees.Super Nodes has to share 30–40% of their fees to the voter nodes.
Ecosystem fees= These can be thought as guaranteed rewards ,are the pre main net launch every year the reward pool for this is 1 % of the total supply,0.5% of the Apex ecosystem token pool and 0.5 % of the increased supply.This program is scheduled to end after 2 years ,when the ecosystem has grown big enough and already has been established.
Special Node Program= there are some special node program to incentivize and reward the community namely,Kratos and Apexion.
Supernode specifications-
Network Requirement-
There will be strict requirements such as-
NO special hardware or software is needed to run a voter node,just the standard CPX wallet.
There are three tiers in this system-
Genesis Tier-400000 CPX(Weight 40)
Tier 1–200000 CPX(Weight 30)
Tier 2–70000 CPX(Weight 20)
Tier 3-No Requirement needed(Weight 10)
Data Cloud Nodes:-
Data Cloud Nodes are required to stake at least 20,000 CPX. There is no minimum hardware or software requirement.
The Data Cloud Nodes can decide if they want to join the Community Nodes or the Enterprise Nodes.
Reward Structure-
The Node reward system is divided into three categories -
1) Nodes are rated based on their performance by data node scoring.
2) The data transactions, which according to preliminary calculations account for around 5% of the total transaction fee.
3) Data storage that is paid by the data owners ,a part of that storage fee is used to incentivize the Data Cloud Nodes.
Here comes the base point apart from the details mentioned above-
There is no fixed percent of return,example if you stake this many coins you are going to be rewarded with X% of return.
The reward is completely dependent on the amount of transactions on the network and adaptation of the Apex Ecosystem.
Let me assure you one thing,the revenue of this company is going to be dependent on this,so this is not going to something small.
Read the information issued by the team-http://cdn.chinapex.com.cn/med/videos/to/APEX-Node-Ecosystem-0622.pdf
Watch the review of the CEO by BlockchainBrad-https://www.youtube.com/watch?v=1y4_UxFxcjU&t=2008s
Link to their Telegram Channel- t.me/APEXcommunity
You are a winner!Cheers!
5.6K 
3
5.6K claps
5.6K 
3
You are a winner!Cheers!
About
Write
Help
Legal
Get the Medium app
"
https://servian.dev/google-cloud-data-engineer-exam-study-guide-9afc80be2ee3?source=search_post---------110,"Sign in
There are currently no responses for this story.
Be the first to respond.
Guang X
Jan 17, 2019·2 min read
I recently studied for and successfully passed the Google Cloud Professional Data Engineer certification/exam. This is a 12-page exam study guide that I personally compiled and used in the hope of covering all the keys points. Below you’ll find a preview of the study guide, but you can click here to download full PDF file.
I hope this helps anyone looking to sit their certification!
Note: This is a good way to recap, but you may still need to read original documentation for each product to learn new concepts etc. And remember — there’s no substitute for hands-on experience with the tools. 😀
To find out more about Servian’s GCP capabilities, see here.
Pdf File: https://github.com/xg1990/GCP-Data-Engineer-Study-Guide/blob/master/GCP%20Data%20Engineer.pdf
Thanks Graham Polley for helping prepare my first Medium article!
Data Platform Engineer @ New Aim Pty Ltd. https://linkedin.com/in/xu-guang/
See all (14)
824 
6
824 claps
824 
6
At Servian, we design, deliver and manage innovative data & analytics, digital, customer engagement and cloud solutions that help you sustain competitive advantage.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@guyernest/architecting-a-successful-modern-data-analytics-platform-in-the-cloud-f090b7a04696?source=search_post---------111,"Sign in
There are currently no responses for this story.
Be the first to respond.
ML-Guy
Feb 5, 2021·12 min read
After we discussed the concepts for Building a Successful Modern Data Analytics Platform in the Cloud, it is time to architect it. This post will review the reference architectures for our scalable, flexible and robust design in both Amazon Web Services (AWS) and Microsoft Azure.
I worked in the last 20 years with hundreds of companies to harness the ever-evolving technology tools to bring to life business ideas. Some companies were startups born to the cloud age…
About
Write
Help
Legal
Get the Medium app
"
https://blog.sketchapp.com/smart-distribute-cloud-documents-and-sketch-for-teams-whats-new-in-sketch-4de2e21e1d6f?source=search_post---------112,"There are no upcoming events.
Want to host your own event? Get in touch.

        We’ve got some really exciting updates to share with you— from big improvements to the Mac app, to the Sketch for Teams beta!
      
The last six months have been some of the busiest and most exciting in the history of Sketch. We had our first funding round, we announced our biggest update yet, and we’ve been travelling all over the world to share sneak previews of what’s next for the platform. And after a busy Summer, it’s finally time to share what we’ve been working on with all of you — from big improvements to the Mac app, to the Sketch for Teams beta.
With Sketch 55, we introduced Smart Distribute to make managing and creating complex layouts easier. We also promised we’d be back with updates that made Smart Distribute even more powerful. In today’s update, it gets a serious upgrade.
Originally, Smart Distribute only worked in a single direction — controlling the spacing between layers in a horizontal row or vertical column. But in this latest update, it supports grids, so you can adjust the horizontal spacing between each layer in a row, as well as the vertical spacing between the rows themselves.
Better still, you can now use negative spacing (so layers can overlap) and adjusting the spacing between layers inside a group is as simple as clicking on the group itself, grabbing a handle and dragging.
We’ve also introduced a new “Tidy” button in the Inspector. It’s the quickest way to organise multiple layers into a neat grid of rows and columns.
We’ve had a lot of requests for this one, so we’re very excited to say you can now edit text overrides on the Canvas. Simply double-click on any text in any Symbol instance and start typing. Now you can stay out of the inspector, stay in the moment and speed up your workflow.
Sketch Cloud is great for sharing designs, getting feedback and distributing Libraries, but we know uploading and opening documents can sometimes slow you down. So, we’re launching Cloud Documents — a new way to create, save and open documents from Sketch Cloud, right in the Mac app.
It starts with a new Cloud tab in the Welcome Window, where you can view and open any of your documents saved on Sketch Cloud. Firing up your browser and trawling through your Downloads to find the right file is a thing of the past.
You can choose whether you want to create Cloud or local documents by default (and change that setting at any time) in Preferences. Cloud documents regularly and automatically save locally as well, so you won’t have to worry about losing your work. When you want to save the latest version to Sketch Cloud, simply hit save.
This is just the start of our plans to bring Sketch Cloud and the Mac app closer together. And if you’re a regular Sketch Cloud user, we think it’s going to be a huge improvement to your workflow. You can find out more about using Cloud Documents in the documentation.
Along with our headline changes, the latest Mac app comes with plenty of small improvements and fixes:
And we’re not done yet. Today marks the official launch of our Sketch for Teams beta and we couldn’t be more excited to share it with you. As we mentioned in our previous post, the beta is an early look at what’s in store for you and your team, and we’ll be shipping regular updates between now and the full launch later in the year.
You can sign up today and get a fully-featured, 90-day free trial for your whole team. We hope this will give even the largest teams plenty of time to try it out and to see how it makes collaboration in Sketch (and beyond) easier, faster and more seamless.
In the beta, you’ll be able to:
We promised some big things when we announced our funding round earlier this year, and what you’re seeing today is just the start. With Cloud Documents and the Sketch for Teams beta we’re laying the groundwork for some even more exciting projects we’re working on right now. And we’ll be sharing them with you very soon.
Turn your ideas into incredible products with a 30-day trial.
A monthly digest of the latest Sketch news, articles, and resources.
©
        2022
        Sketch B.V.
"
https://levelup.gitconnected.com/running-a-scraping-platform-at-google-cloud-for-as-little-as-us-0-05-month-6d9658982f04?source=search_post---------113,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
I was recently faced with the problem of finding an apartment in Berlin. Following my previous experience in this same effort, I decided to automate the task and write a software to send me an alert of the best deals. In this article, I explain how I built the foundations of this platform.
The platform I've written is a Go application deployed to Google Cloud using Terraform. Also, it has Continuous Deployment from a private GitHub repository.
After a quick research, I came to the following list of platforms to monitor:
A few hours later, I have a Go binary that does everything I need to run the application locally. It uses a web scraping framework called Colly to browse all the platforms listings, extract basic attributes, and export to CSV files in the local filesystem.
Since I didn’t want to maintain the application running locally, my first choice would be to get a cheap instance at Google Cloud. Once I had this rented virtual machine, I could write a startup script to compile the app from GitHub, and set up a crontab to scrape the platforms on a daily basis.
Probably the best decision for this specific project, but could I use this personal problem as an opportunity to explore the integration of Google Cloud services?
Since, in the past, I was involved in multiple projects involving some sort of scraping application, I believed it was worth the effort. I could easily reuse this setup in the future.
My architecture started with a few premises:
My hypothesis was that I didn't need a virtual machine running 24/7; thus, it should not cost the same as a full month price. In fact, my application was able to download all the properties I was interested in under 3 minutes, so I expected something significantly lower.
My exploration through the latest Google Cloud services resulted in finding Cloud Run, a service that “run(s) stateless containers on a fully managed environment or in your own GKE cluster.” Still classified as a beta product by Google Cloud, it is built on top of Knative and Kubernetes. The key proposal is its pricing model: it charges in chunks of milliseconds rather than hours of runtime.
With a few tweaks, my Go application was wrapped in a Docker container to be runnable by Cloud Run. Once it gets a HTTP POST request, it collects attributes from all the advertised properties and publishes as CSV files to a Google Storage bucket. For my use case, I created two possible ways to hit this endpoint: an Internet-accessible address so I can trigger it whenever I want, and through Cloud Scheduler, which is configured to hit it once a day.
The application is fairly simple: it consists of an HTTP server with a single endpoint. On every hit, it scrapes all the platforms and saves results in CSVs inside a Storage bucket.
Other application files can be found in this Gist. All the feedback is appreciated, as this is one of my first Go projects.
Now with permissions already given, use Terraform to set up the rest of the infrastructure.
$ cd deployment$ terraform init$ terraform apply
The initial deployment may take about five minutes since Terraform waits for Cloud Run to build and start before configuring Cloud Scheduler.
Since Cloud Run is still in beta — with API endpoints in alpha stage —I was not able to declare all the infrastructure in Terraform files. As a temporary workaround, I’ve written a couple of auxiliary bash scripts that trigger the Cloud API through its CLI command. Fortunately, all this happens in background when a developer triggers terraform apply.
Every day, without any human interaction, Cloud Scheduler creates a new folder with a number of CSV files with the most recently available apartments in my city.
Not all the services in use are available in the official calculator. Either way, I've made a rough estimation for my personal use, considering an unrealistic number of one deployment each day.
For comparison, an f1-micro instance — with 0.6GB of RAM — running over a full month on Google Cloud, is included in the free tier; a g1-small instance, with 1.7GB, would cost US$ 13.80 per month. Also, it is reasonable to consider the cost could decrease or increase depending on how accurate were my initial assumptions and further optimizations.
Coding tutorials and news.
1K 
7
Public domain.

A monthly summary of the best stories shared in Level Up Coding Take a look.
1K claps
1K 
7
Written by
Everyday software engineer, forever mathematician. https://iriomk.com/blog/
Coding tutorials and news. The developer homepage gitconnected.com && skilled.dev
Written by
Everyday software engineer, forever mathematician. https://iriomk.com/blog/
Coding tutorials and news. The developer homepage gitconnected.com && skilled.dev
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/no-localhost-no-problem-using-google-cloud-shell-as-my-full-time-development-environment-22d5a1942439?source=search_post---------114,"There are currently no responses for this story.
Be the first to respond.
I recently switched to a Chromebook full time (the awesome Pixelbook), and the first question I get asked is “How do you do any development work on it?”
While Chromebooks are getting more powerful local development (a la Crostini), I’ve actually been using Google Cloud Shell as my primary development environment. In fact, I haven’t opened a local shell for months now.
So far, the experience has been awesome, and I’ve learned a bunch along the way. I wanted to share some tips, here they are!
Cloud Shell is a free terminal that you can use for whatever you want, and it runs 100% in a web browser.
Click this link to open Cloud Shell: https://console.cloud.google.com/cloudshell
Seeing how most of my work requires the internet, I’m completely fine with being tied to a browser.
Cloud Shell comes with most of the tools I use on a daily basis right out of the box. These include gcloud, node, kubectl, docker, go, python, git, vim, and more.
There is 5GB of persistent storage that is tied to your $HOME directory, and this is also completely free.
The other really nice thing is that you are automatically authenticated to the current Google Cloud project you’re working in. This makes setup super easy, everything just works!
Most people developing cloud apps usually run a web server of some sort. Usually, you would run it locally and just type in “localhost” into your web browser to access it.
This is not possible with Cloud Shell, so the team created a neat “web preview” function that creates a URL on the fly to point to your local server.
You can open up any port from 2000 to 65000, and your web traffic will come through!
Warning: There is some transparent auth done behind the scenes, so the URL the Cloud Shell opens might not work if you give it to someone else. I’d recommend a tool like ngrok if you want to share your “localhost” connection remotely.
By default, Cloud Shell runs on a g1-small VM, which can be under-powered for some tasks. You can easily upgrade to a n1-standard-1 by enabling “boost mode.” It’s like the TURBO button on an old PC, but this time it actually works :)
Yes yes, vim and emacs and nano are great and all. But sometimes you just want a nice, comfortable GUI to work with.
Cloud Shell ships with a customized version of the Orion editor.
While its not as good as VS Code or Eclipse, its actually a fully featured editor and I feel quite productive with it!
If you have files locally you want to upload to cloud shell, just click the “Upload” button in the menu and choose your file.
To download files, run this inside Cloud Shell:
And it will download the file!
Because Cloud Shell only persists your $HOME directory, if you install things that don’t come out of the box with Cloud Shell, chances are it will be gone the next time you use it.
If you install things with apt-get, there really is no good solution. However, if you are downloading the binary directly or are compiling from source, you can create a path in your $HOME directory (for example, /home/sandeepdinesh/bin) and add that to your PATH. Any binaries in this folder will run like normal, and they will be persisted between reboots.
If you have a git repository in a public place (like a public GitHub repo), you can actually create a link that will open the end user’s Cloud Shell and automatically clone the repo to their $HOME directory.
cloud.google.com
Because it is all free, it is a great way to get someone up and running with your project without having to worry about their local machine!
I’ve had a ton of success using Cloud Shell as my primary dev environment. If you are using a Chromebook or if you just want a free Linux shell that you can access from any browser, I’d definitely check out Cloud Shell.
There are a ton of new features coming down the line as well, I’ll be sure talk about them when they come out!
Google Cloud community articles and blogs
803 
11
803 claps
803 
11
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/thinking-design/xd-essentials-typography-in-mobile-apps-7048abfb1cc5?source=search_post---------115,"There are currently no responses for this story.
Be the first to respond.
Top highlight
Communication plays a vital role in design — in order to be successful your products have to clearly communicate their intent and purpose. When we talk about communication, we usually mean text, because the purpose of text in your app is to establish a clear connection between the app and user, and to help your users accomplish their goals. Typography plays a vital role in this process; good typography makes the act of reading effortless, while poor typography turns users off.
How do you make typography work for you and your users, and not against you and them? Unfortunately, there are a lot of conflicting opinions out there about best practices for typography on mobile, and there isn’t one strict set of rules that apply in every case. However, there are several things you can do to make sure the typography is honoring the content and improve readability on mobile devices.
Three of the most important considerations when designing type for mobile devices are font size, line length, and space. Type must be large enough to easily read, and there should be sufficient space between lines.
The size of your text has a huge impact on the experience of reading something on screen. It takes the user much longer to process smaller text. Text that is too small can cause the reader to strain, because it’s usually hard to find the beginning and end of lines of text when there are too many letters to digest at one time. As result, users will skip most of the information presented. This is especially true for mobile, where tiny type on a small, bright screen can be a real headache for users.
However, text that is too large can also cause problems. If a line is too short the eye will have to travel back too often, breaking a reader’s rhythm. This makes the message more difficult to comprehend.
In order to make your text legible you need to find the right balance between font size and line length.
Practical tip: There is no simple formula for selecting a font size, but a good starting point is Apple’s and Google’s guidelines:
The generally accepted, optimal length of a line of text for comfortable reading is around 50–60 characters, including spaces (“Typographie”, E. Ruder). This measure is good for desktop apps, since it’s rare that 60 characters will extend to the edge of a desktop screen, but on most mobile devices 60 characters extends beyond the boundaries of the screen. Also lines with 60 characters on mobile screens usually require fonts that are too small to be easily readable.
There’s no commonly accepted standard for measure on a mobile screen, however a good rule of thumb for line length is to use 30–40 characters per line for mobile (source).
Practical tip: Font-size is generally more important than line length so you should always go for a good font-size first. If it can be combined with an optimal line length, then that’s obviously the ideal solution.
When it comes to mobile screens, space is a key consideration: the smaller the screen size, the more important it is that you give letters room to breathe. By adding a little space to text — by increasing line height or letter spacing — you are helping users better interact with the words.
Practical tip: Smaller type is easier to read with additional spacing. Thus, add extra space — maybe as much as 10 or 20 percent — between lines of type.
“Consider using paragraph spacing as well since paragraphs may look longer on smaller screens. This additional space gives readers the perception that text is not too dense and will feel easier to read.” — Carrie Cousins
Contrast is particularly important when designing for mobile because of the potential for distracting glare when using a device outside. The two most common misconceptions about color contrast are:
Even people with perfect vision experience eye fatigue from staring at white text on a black background for too long.
It’s worth saying it again: the most important property of the text in your app is readability. Designers often like to use low contrast techniques, because low contrast makes things look beautiful and harmonious, but beautiful isn’t always the best for readability. When the text is hard to read the experience will be far from good, and the design simply won’t work.
Practical tip: The right amount of contrast is a tricky thing. Because of the variation between screens, a color contrast that seems good enough on a designer’s monitor could appear much different on a user’s screen. However, the situation is not hopeless — there is a good place to start and it’s a WC3’s Web Content Accessibility Guidelines.
The WC3 sets minimum standards for contrast ratios, which represent how different a color is from another color (commonly written as 1:1 or 21:1, the higher the difference between the two numbers in the ratio, the greater the difference in relative luminance between the colors). The W3C recommends the following contrast ratios for body text and image text:
Ensure sufficient color contrast for your text for best readability. You can use a contrast ratio tool to quickly find out if you’re within the optimal range.
You should try to use a single font throughout your app. Mixing several different fonts can make your app seem fragmented and sloppy. When designing an app think about how can you make the typography powerful by playing with weight and dimensions, not different typefaces.
Practical tip: A safe bet is to rely on the platform’s default font.
Apple uses the San Francisco family of typefaces to provide consistent reading experience across all platforms. Roboto & Noto fonts are the standard typefaces on Google Android and Chrome.
Don’t forget about testing. Once you’ve made your typography choices, it’s absolutely necessary to test them out with real users, on the most popular device screen widths. The more devices you try text styles on, the better you’ll understand how it looks and works for users. If any of the test participants have trouble reading your copy, then you can bet your regular users will have exactly the same problem.
Practical Tip: You should use A/B testing to find out how your typography choices reflected in your conversion.
As Oliver Reichenstein states in his essay, Web Design is 95% Typography:
“Optimizing typography is optimizing readability, accessibility, usability(!), overall graphic balance.”
This statement is relevant not only for web, but for all UIs, because it encapsulates the importance of readability. Typography is the backbone of a design. By optimizing your typography you’re also improving your UI.
Nick Babich is a developer, tech enthusiast, and UX lover. He has spent the last 10 years working in the software industry with a specialized focus on development. He counts advertising, psychology, and cinema among his myriad interests.
Originally published at blogs.adobe.com.
More about Adobe XD:
Stories and insights from the design community.
410 
7
410 claps
410 
7
Written by
New Tools for New Creatives. Get all the latest creative apps plus seamless ways to share and collaborate. All right on your desktop.
Stories and insights from the design community.
Written by
New Tools for New Creatives. Get all the latest creative apps plus seamless ways to share and collaborate. All right on your desktop.
Stories and insights from the design community.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@dynamicbalaji/oracle-cloud-infrastructure-foundations-2020-associate-certification-1z0-1085-20-preparation-ea9edb7d428b?source=search_post---------116,"Sign in
There are currently no responses for this story.
Be the first to respond.
Balaji Ashok Kumar
Apr 20, 2020·11 min read
Hello everyone, last week I passed the OCI foundations exam as it is now free due to COVID-19 as given here. I prepared for it and cracked it in a week. So, I am here to share my experience and guidance on how to prepare for it.
This certification provides candidates with foundational knowledge of core cloud computing concepts and an understanding of those services in Oracle Cloud Infrastructure.
Designed for professionals who is new to the cloud computing or want to know about Oracle Cloud Infrastructure.
Oracle University online learning platform provides a learning path for preparing to this certification and you can access it by following the below link:
Understand OCI Foundations
It comes with video tutorials for the certifications syllabus and a set of 25 questions practice exam too.
This section contains the notes for the certification topics for quick reference and its the biggest section in this article :) If you want, you can skip it and jump to the next section.
OCI Architecture
OCI Compute Services
OCI Storage Services
OCI Network Services
OCI IAM
OCI Database Services
OCI Security
OCI Pricing and Billing
Please follow below steps to book appointment for the certification exam:
I highly recommend you to perform system test from the same computer and location you will be testing from on exam day. Please follow below link for further steps:
http://www.pearsonvue.com/oracle/op/
Ensure you have administrative rights on your computer to be able to download the software and proceed further with the prompts.
The certification will be open 30 minutes before the booked time and please be ready at least before 15 minutes so that you will be ready for the booked time after completing the admission process.
There are totally 60 questions and you have 105 minutes — so you have approximately 2 minutes for each question. So take your time to read the question carefully before choosing the answer and also review the each question once you completed all questions.
Good that you read the whole article; hopefully it will be useful for you.
All the best for OCI Foundations Certification exam. Please contact me if you have any queries/need any details on this at Balaji Ashok Kumar
Hit the clap button 👏👏🏻👏🏼👏🏽👏🏾👏🏿 (as many times as you want!) :)
If you are planning to write Oracle Autonomous Database Cloud 2019 Specialist Certification, then you can refer my preparation guide through below link:
Oracle Autonomous Database Cloud 2019 Specialist | 1Z0–931 | Preparation Guide
Please feel free to share your feedback and I would very much welcome it :) Thanks for reading. Have a nice day!!!
Full Stack Web Developer — https://github.com/dynamicbalaji
1.1K 
10
1.1K 
1.1K 
10
Full Stack Web Developer — https://github.com/dynamicbalaji
"
https://medium.com/@sathishvj/notes-from-my-google-cloud-professional-cloud-architect-exam-bbc4299ac30?source=search_post---------117,"Sign in
There are currently no responses for this story.
Be the first to respond.
sathish vj
Jan 8, 2019·6 min read
Subscribe to my YouTube channel that teaches you to apply Google Cloud to your projects and also prepare for the certifications: youtube.com/AwesomeGCP. Check out the playlists I currently have for Associate Cloud Engineer, Professional Architect, Professional Data Engineer, Professional Cloud Developer, Professional Cloud DevOps Engineer, Professional Cloud Network Engineer, and Professional Cloud Security Engineer.
Immediately after the exam I do a memory dump as notes. Hence it is also quite unordered. This is a sanitized list that gives general topics and questions I encountered. The intention is not to give you the questions, but to give you topics that you can be prepared for. I was often stumped by some questions; hopefully you can be more prepared based on my experience. Wish you the very best!
This was the exam I originally planned to take first, but then I completed the Associate Cloud Engineer first. The notes I have on this seem to be fairly thin. So it’s kind of automatically sanitized and doesn’t divulge much details.
Google Cloud Certified — Professional CloudArchitect
For those appearing for the various certification exams, here is a list of sanitized notes (no direct question, only general topics) about the exam.
Overall notes across all GCP certification exams
Notes from the Professional Cloud Architect exam
Notes from the beta Professional Cloud Developer exam
Notes from the Professional Data Engineer exam
Notes from the Associate Cloud Engineer exam
Notes from the beta Professional Cloud Network Engineer Exam
Notes from the beta Professional Cloud Security Engineer Exam
Notes from the Professional Collaboration Engineer Exam
Notes from the G Suite Exam
Notes from the Professional DevOps Engineer Exam
Notes from the Professional Machine Learning Engineer Exam
Main Link — https://cloud.google.com/certification/cloud-architect
Topics Outline — https://cloud.google.com/certification/guides/professional-cloud-architect/
Practice Exam — https://cloud.google.com/certification/practice-exam/cloud-architect
A collection of posts, videos, courses, qwiklabs, and other exam details for all exams: https://github.com/sathishvj/awesome-gcp-certifications
I’ve collected here a bunch of free Qwiklabs codes which are awesome to get lots of hands-on practice. Use them well.
medium.com
Check the FAQs here: https://medium.com/@sathishvj/frequently-asked-follow-up-questions-on-google-cloud-gcp-certifications-438e1addb91d.
Wish you the very best with your GCP certifications. You can reach me at LinkedIn and Twitter. If you can support my work creating videos on my YouTube channel AwesomeGCP, you can do so on Patreon or BuyMeACoffee.
tech architect, tutor, investor | GCP 12x certified | youtube/AwesomeGCP | Google Developer Expert | Go
712 
21
712 
712 
21
tech architect, tutor, investor | GCP 12x certified | youtube/AwesomeGCP | Google Developer Expert | Go
"
https://medium.com/google-cloud/serverless-on-google-cloud-platform-an-introduction-with-serverless-store-demo-41992dec085?source=search_post---------118,"There are currently no responses for this story.
Be the first to respond.
This document is the opening piece of the Serverless on Google Cloud Platform: an Introduction with Serverless Store Demo How-to Guide. It discusses briefly serverless computing and its patterns in the context of Serverless Store, a web e-commerce demo app for showcasing serverless products and services on Google Cloud Platform. Serverless Store is not an official Google product.
Serverless Store is featured in Google Cloud Global Digital Conference 2019. View the keynote and breakout recordings to learn more.
Deploying a service used to be a tremendous commitment: to get everything up and running, developers and operators have to build a server, setup OS and network, install a variety of dependencies and prepare for years (if not longer) of upgrade and maintenance ahead. Nowadays many developers choose to use virtual machines (VMs) provided by Google, Amazon, and other cloud service providers instead, leveraging their experience in hardware and networking for better, more secure and more reliable service deployments. Nonetheless, VMs are still servers; the fact that they are running on the cloud in a virtualized form does not eliminate heavy server management workload. Additionally, VM deployments require careful capacity planning and they are charged per-second; in other words, every idling core and unit of RAM are a waste of budget.
Serverless computing promises a pay-as-you-go future with (almost) no server management at all. Serverless platforms take the code from developers and perform all the deployment tasks (networking, dependencies, maintenance, etc.) automatically behind the scenes. The greatest advantage serverless computing provides is that the deployment scales itself without additional configuration; your apps will always have exactly what they need computationally (within an understandable margin of error, of course) when running.
This is not saying that serverless computing is an ideal solution for every use case. Going serverless implies that you trust Google Cloud Platform, AWS, or other cloud service providers to manage a large portion of your computing stack for you and expect them to perform as you hope, which may not be the case in some scenarios. More importantly, the architecture of your app has a great impact on how well serverless computing keeps its no-server-management easily-scalable promise: for example, if you use a VM-based database backend solution, such as a Cloud SQL instance, with your serverless function deployment, its scalability will be heavily limited by the performance of the SQL instance, and unexpected errors may occur if you are not careful enough.
The ever growing popularity of AWS Lambda and Google Cloud Functions leads many to believe that FaaS (Function as a Service) is a synonym for serverless computing. FaaS platforms take a function from developers, build it into an app, and deploy it in the cloud; it advocates a quite special pattern for app development, where an app is broken down into a collection of functions, grouped by a (managed) gateway, and accessed by users via static HTML files served online:
Effectively transforming the backend into an API service, this signature architecture enjoys all the benefits of serverless computing (little to none server management, scalability, low cost), and enables team collaboration on a different level (UI design, for instance, is now decoupled; team members may each work on their own functions as opposed to a full app). Many developers have successfully adopted the pattern and created compelling experiences with minimal amount of effort.
But there is a catch. This distinct design is, as with serverless computing in general, not perfect for all use cases, and bears the risk of gruesome fragmentation: without careful coordination developers may end up with a large number of functions difficult to assess, monitor, maintain, orchestrate, and keep a complete picture of, in many ways akin to a 100,000-piece jigsaw puzzle. The architecture is also fairly new; it takes some time for developers and teams to adapt to, and they may need to overhaul their workflows to accommodate.
Serverless and FaaS are not necessarily the same, and it would be dangerous to force a function mindset. It is recommended that developers experiment with serverless solutions and integrate them into their apps to the extent they are most comfortable with. For teams with a comprehensive knowledge of serverless computing, it might be OK to build a service from ground up using nothing but functions; those who are interested in but not yet very comfortable with serverless computing should, however, consider taking a hybrid approach and migrating some select compatible workload to serverless platforms first. Serverless is, after all, a flexible misnomer.
Generally speaking, code in an app is executed sequentially. The sequence (or execution path) is essentially a contract crystalized in the deployment, triggered by an input (request) and finishes with an output (response). The input, output, and the sequence are bundled inseparably together in the contract; once deployed, it cannot be modified. Developers will have to modify the code and re-deploy.
Event-driven computing plans to break the contract and free all the parts involved in the sequence. Parts now emit an event (a piece of message with execution contexts) at the time of completion; they cares little about what happens next, and leaves the following step at the discretion of whatever delivers the event, which, for cloud-native applications, is usually a data/event streaming solution, such as Google Cloud Pub/Sub and Apache Kafka. Parts that send events are event publishers, and those that receive events become event subscribers.
This publisher/subscriber pattern grants greater development freedom and easier team collaboration. It is now possible to hot plug/swap blocks of code without redeployment, and developers can easily add multiple subscribers to the same event stream, creating a fan-out structure. One of the most prominent use cases of such structures is real-time data analytics:
Traditionally, data analysts process data in batch, usually via auto-generated log files. The application executes a sequence, writes the details to logging agents (as a step of the sequence), and data analytics team extracts insight from them, with an understandable delay. In event-driven systems, however, once connected to the event stream as one of the subscribers, data analytics team can get the data they need in real time without interrupting normal operations. If connected to real-time data processing and warehousing solutions such as Google Cloud Dataflow and Google BigQuery, developers can have analysis done in real-time as well.
The data/event streaming solutions themselves also offer great help in application development. You can set up Google Cloud Pub/Sub, for example, to reattempt delivery automatically when one of the subscribers is experiencing temporary technical difficulties. Cloud Pub/Sub also supports snapshot, enabling developers to go back in time and replay events, which can be particularly helpful when testing new pieces of code.
Event-driven computing works particularly well with serverless computing. Events are natural triggers for serverless deployments; more importantly, the (almost) infinite scalability of serverless computing platforms allows message queues to pass events around as fast as possible, saving the trouble of capacity planning commonly witnessed in VM-based deployments.
Breaking the contract of sequential code execution has some serious complications, with the most important being the dissociation of inputs (requests) and outputs (responses). In the world of event-driven computing, requests and responses arrive in two separate dimensions; whoever sends the request are not naturally guaranteed a response. For asynchronous operations it is usually fine: when people post a new photo in their social feeds they usually do not expect it to show up in the feeds of their friends immediately; quick feedback is appreciated but not required. Synchronous operations, however, are a different story. When people open a website, they expect the page to load as quickly as possible; no other actions can be performed until the contents show up. Event-driven computing, unfortunately, does not work well in this scenario, as the request is considered served when the event is published; developers have to manually retrieve the response.
There are many solutions that address this issue, though none of them is perfect. Common strategies include using statically generated pages throughout the app, polling the system for results right after event publishing, and setting up dedicated asynchronous routines for synchronous operations when required (for example, when people request a webpage, prepare it synchronously first, then publish an event to the message queue so that event subscribers can track the action). Once again, it is up to developers themselves to decide how far they would like to go with event-driven computing; for some apps it may be a heavenly solution.
As introduced earlier, Serverless Store is an e-commerce app designed to showcase serverless products and services on Google Cloud Platform. Note that it is for demonstration purposes only and does not necessarily reflect best practices in the app development.
Serverless Store features a fully serverless architecture. It runs on two Google provided serverless computing platforms, Google App Engine and Google Cloud Functions, with all of its components, storage, data analytics, machine learning/AI, etc., supported by managed services. There are no VMs involved at all. The app is scalable, and you pay only what the app uses.
Serverless Store consists of both app and functions. It is a hybrid, featuring a standard (traditional, if you may) Python flask web app as the centerpiece, with many critical and supplementary features served by Cloud Functions. For people who prefer the FaaS pattern, with a relatively small amount of effort it is possible to make Serverless Store purely function-based; you can also easily revert it back to a web app with no Cloud Function workers at all, ready for VM deployment.
Additionally, Serverless Store adopts event-driven computing for all asynchronous operations in the app. Synchronous operations, such as listing products, are still performed in a sequential manner. It is hoped that this approach can help interested developers ease into event-driven computing, a fairly new pattern, and better discover its benefits and limitations.
You can view live demos of Serverless Store in these keynotes.
Download the project here.
Guides below introduces briefly each product and service used in the Serverless Store demo app and explains how they are integrated in the app. Follow them to set up Serverless Store:
See Further Discussions for some tips and notes about Serverless Store.
Watch Google Global Digital Conference 2019.
Google Cloud community articles and blogs
734 
3
734 claps
734 
3
Written by
Developer Relations @ Google Cloud Platform
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Developer Relations @ Google Cloud Platform
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/android-news/serverless-notifications-with-cloud-functions-for-firebase-685d7c327cd4?source=search_post---------119,"Sign in
There are currently no responses for this story.
Be the first to respond.
Mahmoud AlyuDeen
Mar 16, 2017·4 min read
Sending notifications to users is a vital, and in many cases, a key process to any app. And for a long time, we’ve been lucky enough to have the delivery of notifications carried out by services like FCM and APN.
The thing is.. it required a server. You had to host some code on a cloud platform that would allow it to always be running, code that listens and determines what to send, to which users, and then makes the request to send the actual notification to your user.
Of course, that makes it harder for indie developers to consider including notifications in their apps, they might even disregard certain ideas altogether if it’s centered around notifications.
Enters Cloud Functions for Firebase. Google’s latest, and long awaited addition to the Firebase suite. Cloud Functions lets you write javascript functions and host them on Google Cloud Platform, they can be triggered by events on your Google Cloud, HTTP requests, and.. of course.. Firebase events! Be it a change in the Realtime database, storage, a new user or even an analytics event you’ve defined.
First, let’s consider a basic and very common use case.. Chat! I made a basic sample of a chat app that uses Firebase Realtime database to store messages.
The idea is that the app itself would take care of writing messages in both the sender’s and receiver’s end.
The whole project is available on Github. Both Android and Functions parts.
Yes, that was what’s missing. Telling someone they have a message when they’re not using the app.
To breakdown what’s happening here, the function is listening on any write operations on the notifications/messages node, when a new message is written there, it fetches the receiver’s instanceId, displayName and photoUrl from the database and authentication. It then constructs the notification and sends it to the receiver’s token. Finally, it deletes the message from the notifications node.
A couple of potential issues and some advice I thought to be worth mentioning:
Imagine if the sendNotification function above got triggered by the deletion it performs on the same node to which it listens. You might think: “Oh, that would cause an endless loop”.. You would be right!
Hence, the next point:
When a function listens to database write events on a node, it gets triggered by editing or deleting objects on that node. If you want to only react to what’s left after all write events are complete, you want to call .current on the event.
With your code running somewhere else, your only clue as to how things are going is the logs you make. So make sure to think in advance and log things that might go wrong, so you have enough information to fix it when they do.
As promising, and seemingly complete Cloud Functions is right now, it was just released last week. They have a warning themselves that they will likely introduce breaking changes in the future. So keep that in mind.
Well, as of 2 days ago, neither did I.. Actually it was hard to get excited when I found out about Cloud Functions as I didn’t have a clue how I would use it. But really, it’s not that hard. You just need a crash course on the syntax, and the samples on Github should get you up and running with the basic use-cases. And of course, the documentation will have you covered on the rest.
The Get started guide should get you up and running with your first Cloud project in no time. Happy coding!
It is my belief that: the best time to be a developer, is right now!Problems that used to be costly or required a lot of time and practice to tackle are getting to the point of non-existence. Giving us the opportunity to concentrate on the core logic of apps instead. Nothing proves that more than Firebase. With it, one developer can make the app, make its back-end, have it hosted, tested and monetized, for free! So what’s stopping you?
I am a diehard tech enthusiast, from gaming computers to smartphones and everything in between. I code for fun and for a living and I write every now and then.
1K 
9
1K 
1K 
9
The (retired) Pub(lication) for Android & Tech, focused on Development
"
https://towardsdatascience.com/pre-training-bert-from-scratch-with-cloud-tpu-6e2f71028379?source=search_post---------120,"Sign in
There are currently no responses for this story.
Be the first to respond.
Denis Antyukhov
May 10, 2019·7 min read
In this experiment, we will be pre-training a state-of-the-art Natural Language Understanding model BERT on arbitrary text data using Google Cloud infrastructure.
This guide covers all stages of the procedure, including:
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/automation-generation/build-a-day-trading-algorithm-and-run-it-in-the-cloud-for-free-805450150668?source=search_post---------121,"There are currently no responses for this story.
Be the first to respond.
Commission-free stock trading on a free Google Cloud Platform instance, step-by-step.
When I first started learning about how easy it’s become to take control of my money in the market, day trading jumped out at me as the thing to do. Of course, if it were really as easy as it sounds, everyone would be doing it. Finding a profitable strategy takes time and work, and sticking to it in turbulent market conditions takes discipline.
It’d be a lot easier to take the room for human error — and a whole lot of stress — out of the equation if we could have a computer execute the strategy for us. Just a few years ago, there were a lot of hurdles to doing that, even if you had the programming know-how. Most brokerages that the average person could access charged commission fees, which would eat away the profits of a day-trading strategy. Finding any sort of API access among those that offered commission-free solutions was a challenge to me.
In this article, I am using Alpaca’s commission-free trading API with their premium data service Polygon. (Please note that, according to their docs, you’ll need to sign up for a brokerage account in order to access the premium data feed used here.) I’ll provide a day-trading script that leverages this premium data for a little technical analysis, and I’ll go over what it does and how to run it yourself.
First off, go ahead and get the script from GitHub with this command:
Now, you can open it up in your favorite text editor and follow along. Note that near the top of the file, there are placeholders for your API information — your key ID, your secret key, and the URL you want to connect to. You can get all that information from the Alpaca dashboard.
Replace the placeholder strings with your own information, and the script is ready to run. But before we let it touch even your simulated account’s (entirely make-believe) money, let’s go over what it does. (If you’re more interested in how to get it running on GCP than what it’s doing, skip ahead to the next section.)
Broadly, this is a momentum-based algorithm. We’ll not trade for the first fifteen minutes after the market opens, because those are always pretty hectic. Between the fifteenth minute and the first hour, though, we’ll look for stocks that have increased at least 4% from their close on the previous day. If they’ve done that and they meet some other criteria, we’ll buy them, and we’ll hold them until they either rise high enough (meeting our price target) or fall too low (meeting our ‘stop’ level.)
You’ll notice that below the connection information in the code, there are some additional variables that can be configured. These can be tweaked easily to best suit your needs for the algorithm. There are thousands of stocks available to trade, but not all of them are suitable for a strategy like this.
We filter down the list by looking for a few things — we want a relatively low share price, but not one that’s so low that it behaves more like a penny stock. We also want to be sure that the stock is liquid enough that we’ll get our orders filled. We make sure that the dollar volume of the stock was at least min_last_dv on the previous trading day.
The default_stop and risk parameters are important to making sure that our algorithm stays within acceptable limits. Risk is what percent of our portfolio we’ll allocate to any given position. Since we sell when we hit the stop loss, the amount of cash from our portfolio at risk on a trade is default_stop * risk * account_balance .
I won’t go over how we get our initialization data here — if you want, you can take a look at the code and check out Polygon’s documentation on their ‘ticker’ data. What’s a little more interesting is the fact that we can also stream data in real time from Polygon. (This is also done in a recently-published “HFT-ish” example, another Alpaca day trading algorithm that trades much more frequently than this one and tries to profit from tiny order book imbalances.)
Using Alpaca’s Python SDK, we connect to three types of streaming channels. The first is trade_updates, which is simply a connection to Alpaca on which we can hear updates on our orders as they happen. We’ll use this to make sure we’re not submitting multiple open orders at once for a stock and to see whether or not our orders get filled.
The other two channels are A.<symbol> and AM.<symbol> . For each stock that we’re going to watch, we subscribe to those channels to receive updates from Polygon about the price and volume of the stock. The A channel updates every second, whereas the AM channel updates every minute. We aggregate the information from the A channel ourselves so we can do up-to-the-second calculations, but we consider AM to be the source of truth, replacing whatever we’ve aggregated with what comes through that channel. While we might get away with only watching A and relying on our own aggregation, trusting AM gives us a little extra resilience to hiccups in the connection and such.
Once we’ve added the incoming data to our aggregate, if we haven’t already ordered shares of a stock, we check to see if it looks like a good buy. We define a “good buy” as something with a positive, growing MACD that’s been trading at a decent volume and is up over 4% from yesterday’s close so far today. We also want to make sure that it’s maintained its momentum after the open, so we look to see that the price is higher than its highest point during the first fifteen minutes after the market to open. We hope that these stocks will continue to rise in value as the day goes on.
If we have a position in a stock, we also check with each bar that comes in for that stock if it’s time to sell. We sell when the stock has reached either our target price or our stop loss, or if the MACD suggests that the security is losing its momentum and it’s fallen back to our cost basis. Ideally, enough stocks hit the target price we set that we can recover the losses from those that hit the stop loss, with some extra profits on top.
At the end of the trading day, we liquidate any remaining positions we’ve opened at market price. The use of market orders is generally not ideal, but they are used in this case because the potential cost of holding overnight is greater than we were willing to risk on the position. Ideally, we have already liquidated our shares based on our defined stop losses and target prices, but this allows us to catch anything that sneaks by those by trading flat.
If you scroll down past the bottom of the long run() method, you’ll see how we check to see when the market will be opening and closing using the Alpaca Calendar API endpoint. Using this means that, if you like, you can set up a Cron job to run the script at the same time every day without having to worry about market holidays or late opens causing issues. Many people prefer to run their scripts manually, but it’s nice to have the option to just let it run on its own.
At this point, if you’ve plugged in your own API keys, you could just run python algo.py and be off to the races, watching it buy and sell stocks as its signals are triggered through the Alpaca dashboard. But you might not want to leave your own machine running all day — or maybe you’re just worried about a cat bumping the power button during market hours.
Fortunately, it’s easy to get a machine in the cloud — away from your cat — set up with Google Cloud Platform’s free tier. At the time of writing, they’re even offering several hundred dollars in free credit for trial accounts, so you don’t need to worry too much about running into minor incidental costs. Once you’ve signed up for a GCP account, head to the GCP console and create a project.
Once you’ve created a project, go to the sidebar and navigate to Compute Engine > VM Instances. In this panel, you can see any instances you’ve created. That might be blank for now, but let’s go ahead and make one. Hit Create and fill out your configuration like this.
Hit Create at the bottom, and after a short bit of watching a circle spin, you’ll have a machine up and running, visible in the VM Instances panel. It’ll look something like this:
You can click Open in browser window to get a shell to the machine. Go ahead and do that now — we’ll need it in a minute. But before we type anything into the terminal, we need to get our script onto the machine. (You might know of some ways to do that through the terminal — if so, go for it! Or, you can follow along to do it through the GCP console interface.)
In the sidebar, scroll to Storage > Browser. On the Storage page, go ahead and hit Create Bucket. You’ll get a prompt like this:
We’re not too concerned about the storage cost, since we’ll be storing much less than 1 GB and only sharing it with our one instance. (And we have a few hundred dollars in credit anyhow, if you’ve signed up for the trial.) Once your bucket is created, it’s easy to put your files into it.
Upload the requirements.txt and algo.py files you checked out from the GitHub repository and plugged your Alpaca API credentials into. We’re almost done! All that’s left is to get the files onto our instance. In the browser terminal you’ve opened, you can type gsutil cp gs://<your-bucket-name>/* . to download your files onto the VM.
Now that the script is on the cloud instance, we just need to run it. Python is already installed, but to make requirement management easy, go ahead and download and install pip. In the VM terminal, do this:
And then install the algorithm’s dependencies:
And that’s it! Now, when you’re ready to get the algorithm running, you can just type python3 algo.py into the VM’s terminal and watch it get to work. (At this point, you can also delete the bucket you created, if you’re worried about a few pennies in storage fees.) Now that you’re on your way to being a GCP pro; feel free to play around with the Python code in the script and see what works best for you! I hope this guide has been helpful, and good luck!
Technology and services are offered by AlpacaDB, Inc. Brokerage services are provided by Alpaca Securities LLC (alpaca.markets), member FINRA/SIPC. Alpaca Securities LLC is a wholly-owned subsidiary of AlpacaDB, Inc.
You can find us @AlpacaHQ, if you use twitter.
News and thought leadership on the changing landscape of…
1K 
18
1K claps
1K 
18
News and thought leadership on the changing landscape of automated investing. Changing the market one algorithm at a time.
Written by

News and thought leadership on the changing landscape of automated investing. Changing the market one algorithm at a time.
"
https://eng.lyft.com/introducing-flyte-cloud-native-machine-learning-and-data-processing-platform-fb2bb3046a59?source=search_post---------122,"By Allyson Gale and Ketan Umare
Today Lyft is excited to announce the open sourcing of Flyte, a structured programming and distributed processing platform for highly concurrent, scalable, and maintainable workflows. Flyte has been serving production model training and data processing at Lyft for over three years now, becoming the de-facto platform for teams like Pricing, Locations, Estimated Time of Arrivals (ETA), Mapping, Self-Driving (L5), and more. In fact, Flyte manages over 7,000 unique workflows at Lyft, totaling over 100,000 executions every month, 1 million tasks, and 10 million containers. In this post, we’ll introduce you to Flyte, overview the types of problems it solves, and provide examples for how to leverage it for your machine learning and data processing needs.
With data now being a primary asset for companies, executing large-scale compute jobs is critical to the business, but problematic from an operational standpoint. Scaling, monitoring, and managing compute clusters becomes a burden on each product team, slowing down iteration and subsequently product innovation. Moreover, these workflows often have complex data dependencies. Without platform abstraction, dependency management becomes untenable and makes collaboration and reuse across teams impossible.
Flyte’s mission is to increase development velocity for machine learning and data processing by abstracting this overhead. We make reliable, scalable, orchestrated compute a solved problem, allowing teams to focus on business logic rather than machines. Furthermore, we enable sharing and reuse across tenants so a problem only needs to be solved once. This is increasingly important as the lines between data and machine learning converge, including the roles of those who work on them.
To give you a better idea of how Flyte makes all this easy, here’s an overview of some of our key features:
Flyte frees you from wrangling infrastructure, allowing you to concentrate on business problems rather than machines. As a multi-tenant service, you work in your own, isolated repo and deploy and scale without affecting the rest of the platform. Your code is versioned, containerized with its dependencies, and every execution is reproducible.
All Flyte tasks and workflows have strongly typed inputs and outputs. This makes it possible to parameterize your workflows, have rich data lineage, and use cached versions of pre-computed artifacts. If, for example, you’re doing hyperparameter optimization, you can easily invoke different parameters with each run. Additionally, if the run invokes a task that was already computed from a previous execution, Flyte will smartly use the cached output, saving both time and money.
In the example above, we train an XGBoost model using the dataset provided here. The machine learning pipeline is constructed in Python and consist of the following four tasks, which align with a typical machine learning journey:
Note how each task is parameterized and strongly typed, making it easy to try different variants and use in combination with other tasks. Additionally, each of these tasks can be arbitrarily complex. With large datasets, for example, Spark is more preferable for data preparation and validation. Model training, however, can be done on a simple model coded in Python. Lastly, notice how we’re able to mark tasks as cacheable, which can drastically speed up runs and save money.
Below, we combine these tasks to create a Workflow (or “pipeline”). The Workflow links the tasks together and passes data between them using a Python based domain specific language (DSL).
Every entity in Flyte is immutable, with every change explicitly captured as a new version. This makes it easy and efficient for you to iterate, experiment and rollback your workflows. Furthermore, Flyte enables you to share these versioned tasks across workflows, speeding up your dev cycle by avoiding repetitive work across individuals and teams.
Workflows are often composed of heterogeneous steps. One step, for example, might use Spark to prepare data, while the next uses this data to train a deep learning model. Each step can be written in a different language and utilize different frameworks. Flyte supports heterogeneity by having container images bound to a task.
By extension, Flyte tasks can be arbitrarily complex. They can be anything from a single container execution, to a remote query in a hive cluster, to a distributed Spark execution. We also recognize that the best task for the job might be hosted elsewhere, so task extensibility can be leveraged to tie single-point solutions into Flyte and thus into your infrastructure. Specifically, we have two ways of extending tasks:
FlyteKit extensions: Allow contributors to provide rapid integrations with new services or systems.
Backend plugins: When fine-grained control is desirable on the execution semantics of a task, Flyte provides backend plugins. These can be used to create and manage Kubernetes resources, including CRDs like Spark-on-k8s, or any remote system like Amazon Sagemaker, Qubole, BigQuery, and more.
Flyte is built to power and accelerate machine learning and data orchestration at the scale required by modern products, companies, and applications. Together, Lyft and Flyte have grown to see the massive advantage a modern processing platform provides, and we hope that in open sourcing Flyte you too can reap the benefits. Learn more, get involved, and try it out by visiting www.flyte.org and checking out our Github at https://github.com/lyft/flyte.
Este artículo también está en español: eng-espanol.lyft.com
Stories from Lyft Engineering.
946 
5
Thanks to Ryan Lane and Mark Grover. 
946 claps
946 
5
Written by
Product manager for Flyte.org @Lyft. Previously @Google Search, Android. Curious about ethics and decision-making.
Stories from Lyft Engineering.
Written by
Product manager for Flyte.org @Lyft. Previously @Google Search, Android. Curious about ethics and decision-making.
Stories from Lyft Engineering.
"
https://medium.com/@agasthikothurkar/google-certified-professional-cloud-architect-study-resources-a66f8f52aac5?source=search_post---------123,"Sign in
There are currently no responses for this story.
Be the first to respond.
Agasthi Kothurkar
Jan 10, 2018·3 min read
I recently passed the Google Certified Professional Cloud Architect exam and wanted to share with you a collection of relevant exam resources and content which helped me prepare for it
The exam was “tough” to say the least with every aspect in the official exam study guide tested. For a given problem…
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/the-cloud-architect/the-quest-for-availability-771fa8a94a7c?source=search_post---------124,"There are currently no responses for this story.
Be the first to respond.
Top highlight
In this series of posts, I will walk you through architecting, building and deploying a large scale, multi-region, active-active architecture — all while trying to break it.
The idea is to split the series into the following structure:
Of course — the structure might and probably will change as I continue writing and get your feedback, so feel free to steer the course of (t)his (s)tory in the comments below :)
One of my favorite quotes that influenced my thinking of software engineering is from Werner Vogels, CTO at Amazon.com.
“Failures are a given and everything will eventually fail over time.”
Indeed, we live in a chaotic world where failure is a first-class citizen. Failure usually comes in three flavors; the early failures, the wear-out (or late) failure, and the random failures — with each coming at a different stage in the life of any given system.
Early failures are essentially related to programming and configuration bugs (typos, variable mutations, networking issues like ports and IP routing misconfiguration, security, etc…). Over time, as the product (or version) matures and as automation kicks-in, those failures tend to naturally diminish.
Note: I just mentioned “automation kicks-in”! This really means that you have to be using automation to experience this natural declining behaviour of early failures. Doing things manually won’t allow for that luxury.
Wear-out (or late) failures — you often read online that software systems, unlike physical components, are not subject to wear-out failures. Well, software is running on hardware, right? Even in the cloud, software is subject to hardware failure and therefore should be accounted for.
But that’s not all, wear-out failures also and most often are, related to configuration drifts. Indeed, configuration drift accounts for the majority of reasons why disaster recovery and high availability systems fail.
Random failures are basically, well, random. A squirrel eating your cables. A shark brushing its teeth on transatlantic cables. A drunk truck driver aiming at the data-centre. Zeus playing with lightings. Don’t be a fool, over time, you too will eventually fall victim to ridiculous unexpected failures.
We live in a environment where velocity is critical — and by that, I mean being able to deliver software continuously. To give you an idea of velocity at scale, Amazon.com, in 2014, was doing approximately 50 million deployments a year, that’s roughly 1.6 deployments per seconds.
Of course, not everyone needs to do that — but the velocity of software delivery, even at smaller scale, does have a big impact on customer satisfaction and retention.
So how does velocity impact our “bathtub” failure rate curve? Well, it now looks more like the mouth of a shark ready to eat you raw. And indeed, for each new deployment, new early failures will be thrown at you, hoping to take your system down.
As you can easily notice, this creates a tension between the pursuit of high availability and the speed of innovation. If you develop and ship new features slowly, you will have a better availability — but your customer will probably seek innovations from someone else.
On the other hand, if you go fast and innovate constantly on behalf of your customers, you risk failures and downtime — which they will not like.
To help you grasp what you are fighting against, I included the table of “The Infamous Nines” of availability. Let that table sink in for a minute.
If you want to have 5-nines of availability, you can only afford 5 minutes of downtime a year!!
Few years ago, I experienced first-hand a complete system meltdown. It took our team few minutes just to realize what was happening, another few minutes to get our sh*t together and slow our heart-rate down, and another couple hours to complete a full system restore.
Lesson learned: If __any__ humans are involved in restoring your system, you can say bye-bye to the Infamous Nines.
So how can you reconcile both availability and velocity for the greater good of your customers? There are three important things, namely:
Simply put, what you should aim for is having everyone in the team confident enough to push things into production without being scared of failure.
The best way to do so is by first having highly available and reliable systems, having the right tooling in place and by nurturing a culture where failure is accepted and cherished. In the following, I will focus more on the availability and reliability aspect of things.
It is worth remembering, that generally speaking a reliable system has high availability — but an available system may or may not be very reliable.
Consider you have 2 components, X and Y, respectively with 99% and 99.99% availability. If you put those two components in series, the overall availability of the system will get worse.
It is worth noting that the common wisdom “the chain is as strong as the weakest link” is wrong here — the chain is actually worsened.
On the other hand, if you take the worse of these components, in that case, A with 99% availability, but put it in parallel, you increase your overall system availability dramatically. The beauty of math at work my friends!
What is the take away from that? Component redundancy increases availability significantly!
Note: you can also calculate availability with the following equation:
Alright, now that we understand that part — let’s take a look at how AWS Regions are designed.
From the AWS website, you can read the following:
The AWS Cloud infrastructure is built around Regions and Availability Zones (“AZs”). A Region is a physical location in the world where we have multiple Availability Zones. Availability Zones consist of one or more discrete data centers, each with redundant power, networking and connectivity, housed in separate facilities.
Since a picture is worth 48 words, an AWS Region looks something like that.
Now you probably understand why AWS is always, always talking and advising its customers to deploy their applications across multi-AZ — preferably three of them. Just because of this equation my friends.
By deploying your application across multiple AZs, you magically increase, and with minimal effort, it’s availability.
This is also the reason why using AWS regional services like S3, DynamoDB, SQS, Kinesis, Lambda or ELBs just to name a few, is a good idea — they are by default, using multiple AZs under the hood. And this is also why using RDS configured in multi-AZ deployment is neat!
One thing to remember though is that availability does have a cost associated with it. The more available your application needs to be, the more complexity is required — and therefore the more expensive it becomes.
Indeed, highly available applications have stringent requirements for development, test and validation. But especially, they must be reliable, and by that, I mean fully automated and supporting self-healing, which is the capability for a system to auto-magically recover from failure.
They must dynamically acquire computing resources to meet demand but they also should be able to mitigate disruptions such as misconfigurations or transient network issues.
Finally, it also requires that all aspects of this automation and self-healing capability be developed, tested and validated to the same highest standards as the application itself. This takes time, money and the right people, thus it costs more.
While there are tens, or even hundreds of techniques used to increase application reliability and availability, I want to mention two that in my opinion stand-out.
Exponential backoffTypical components in a software system include multiple (service) servers, load balancers, databases, DNS servers, etc. In operation, and subject to potential failures as discussed earlier, any of these can start generating errors.
The default technique for dealing with these errors is to implement retries on the requester side. This simple technique increases the reliability of the application and reduces operational costs for the developer.
However, at scale and if requesters attempt to retry the failed operation as soon as an error occurs, the network can quickly become saturated with new and retired requests, each competing for network bandwidth — and the pattern would continue forever until a full system meltdown would occur.
To avoid such scenarios, exponential backoff algorithms must be used. Exponential backoff algorithms gradually decrease the rate at which retries are performed, thus avoiding network congestion scenarios.
In its most simple form, a pseudo exponential backoff algorithm looks like that:
Note: If you use concurrent clients, you can add jitter to the wait function to help your requests succeed faster. See here.
Luckily many SDKs and software libraries, including the AWS ones, implement a version (often more sophisticated) of this algorithms. However don’t assume it, always verify and test for it.
QueuesAnother important pattern to increase your application’s reliability is using queues in what is often called message-passing architecture. The queue sits between the API and the workers, allowing for the decoupling of components.
Queues give the ability for clients to fire-and-forget requests, letting the task, now in the queue, to be handled when the right time comes by the workers.
This asynchronous pattern is incredibly powerful at increasing the reliability of complex distributed applications — but is unfortunately not as straightforward to put in place as the exponential backoff algorithms since it requires re-designing the client side. Indeed, requests do not return the result anymore, but a JobID, which can be used to retrieve the result when it is ready.
Cherry on the cakeCombining message-passing patterns with exponential backoff will take you a long way in your journey to minimize the effect of failures on your availability and are in the top 10 of most important things I have learned to architect for.
That’s it for this installment — I hope you have enjoyed it! Be sure to check out the next part of this series! Please do not hesitate to give feedback, share your own opinion, or simply clap your hands.
-Adrian
Resilient, scalable, and highly available cloud architectures.
1.1K 
8

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
1.1K claps
1.1K 
8
Written by
Principal, EC2 Core @awscloud ☁️ I break stuff .. mostly. Opinions here are my own.
All you need to know about building resilient, scalable, and highly available architectures in cloud.
Written by
Principal, EC2 Core @awscloud ☁️ I break stuff .. mostly. Opinions here are my own.
All you need to know about building resilient, scalable, and highly available architectures in cloud.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://netflixtechblog.com/netflix-sirt-releases-diffy-a-differencing-engine-for-digital-forensics-in-the-cloud-37b71abd2698?source=search_post---------125,"Forest Monsen and Kevin Glisson, Netflix Security Intelligence and Response Team
The Netflix Security Intelligence and Response Team (SIRT) announces the release of Diffy under an Apache 2.0 license. Diffy is a triage tool to help digital forensics and incident response (DFIR) teams quickly identify compromised hosts on which to focus their response, during a security incident on cloud architectures.
Digital Forensics and Incident Response (DFIR) teams work in a variety of environments to quickly address threats to the enterprise. When operating in a cloud environment, our ability to work at scale, with imperative speed, becomes critical. Can we still operate? Do we have what we need?
When moving through systems, attackers may leave artifacts — signs of their presence — behind. As an incident responder, if you’ve found one or two of these on disk or in memory, how do you know you’ve found all the instances touched by the attackers? Usually this is an iterative process; after finding the signs, you’ll search for more on other instances, then use what you find there to search again, until it seems like you’ve got them all. For DFIR teams, quickly and accurately “scoping a compromise” is critical, because when it’s time to eradicate the attackers, it ensures you’ll really kick them out.
Since we don’t yet have a system query tool broadly deployed to quickly and easily interrogate large groups of individual instances (such as osquery), we realized in cases like these we would have some difficulty in determining exactly which instances needed closer examination, and which we could leave for later.
We’ve scripted solutions using SSH, but we’ve also wanted to create an easier, more repeatable way to address the issue.
Diffy finds outliers among a group of very similar hosts (e.g. AWS Auto Scaling Groups) and highlights those for a human investigator, who can then examine those hosts more closely. More importantly, Diffy helps an investigator avoid wasting time in forensics against hosts that don’t need close examination.
How does Diffy do this? Diffy implements two methods to find outliers: a “functional baseline” method (implemented now), and a “clustering” method (to be implemented soon).
How does the “functional baseline” method work?
When is the functional baseline useful?
How does the “clustering” method work?
When is the clustering method useful?
In environments supporting continuous integration or continuous delivery (CI/CD) such as ours, software is frequently deployed through a process involving first the checkout of code from a source control system, followed by the packaging of that code into a form combined (“baked”) into a system virtual machine (VM) image. The VM is then copied to a cloud provider, and started up as a VM instance in the cloud architecture. You can read more about this process in “How We Build Code at Netflix.”
Diffy provides an API for application owners to call at deploy time, after those virtual machine instances begin serving traffic. When activated, Diffy deploys a system configuration and management tool called osquery to the instance (if it isn’t already present) and collects a baseline set of observations from the system by issuing SQL commands. We do this on virtual machines, but osquery can do this on containers as well.
When an incident occurs, an incident responder can use Diffy to interrogate an ASG: first pulling the available baseline, next gathering current observations from all instances there, and finally comparing all instances within the ASG against that baseline. Instances that differ from the baseline in interesting, security-relevant ways are highlighted, and presented to the investigator for follow-up. If the functional baseline wasn’t previously collected, Diffy can rely solely on the clustering method. We’re not settled on the algorithm yet, but we see Diffy collecting observations from all instances in an ASG, and using the algorithm to identify outliers.
In today’s cloud architectures, automation wins. Digital forensics and incident response teams need straightforward help to help them respond to compromises with swift action, quickly identifying the work ahead. Diffy can help those teams.
We’ve characterized Diffy as one of our “Skunkworks” projects, meaning that the project is under active development and we don’t expect to be able to provide support, or a public commitment to improve the software. To download the code, visit https://github.com/Netflix-Skunkworks/diffy. If you’d like to contribute, take a look at our Contributor Guidelines at https://diffy.readthedocs.io/ to get started on your plugin and send us a pull request. Oh, and we’re hiring — if you’d like to help us solve these sorts of problems, take a look at https://jobs.netflix.com/teams/security, and reach out!
Learn about Netflix’s world class engineering efforts…
944 
4
944 claps
944 
4
Written by
Learn more about how Netflix designs, builds, and operates our systems and engineering organizations
Learn about Netflix’s world class engineering efforts, company culture, product developments and more.
Written by
Learn more about how Netflix designs, builds, and operates our systems and engineering organizations
Learn about Netflix’s world class engineering efforts, company culture, product developments and more.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/js-dojo/build-a-realtime-chat-app-with-vuejs-vuex-and-firestore-32d081668709?source=search_post---------126,"There are currently no responses for this story.
Be the first to respond.
Recently, Google’s Firebase platform released a new product, Cloud Firestore. Much like the Firebase real-time database, it is a NoSQL-based cloud database that can be used to build real-time applications. It addresses some of the problems Firebase has, like writing complex queries. You can read more about its features here.
In this post, I’ll be building a simple real-time chat application called Firechat which uses Vue and Vuex, and the new Cloud Firestore. I’ll look at how to integrate Firestore into a Vue.js application and some best practices with Vuex. You can get the final source code here on Github.
Note: this article was originally posted here on the Vue.js Developers blog on 2017/10/30
Let’s start by scaffolding a new Vue application using the vue-cli. I’ll use the webpack-simpletemplate.
Before going any further, I want to use the spread operator ... and async/await. We will also be using Vuex for state management, and uuid for generating random ids, so let's install those. We will also need the Firebase module.
Next, in the project root directory, edit .babelrc like so:
Now we can use the spread operator ... and await/async.
Before going any further, go to Firebase and sign up if you haven’t got an account. Once you do, click “Add Project” and enter a name.
Then click “Add Firebase to your web app”.
Grab the config object and create a file called config.js under src like below. Select ""test mode"" - that'll be fine for learning Firestore. It means your database will be public, so don't share the link on the internet anywhere.
Head back to Firebase, and under the database tab, click “Try Firestore Beta” and “Start in Test Mode”, which will take you to a UI where you view the database as it changes in real time.
Next, let’s set up Vuex and Firestore. Create a folder under src called store, and inside index.js. Enter the following:
Next, head over to main.js and import the store, and inject it into the app.
Lastly, visit App.vue, delete all the default content, add try console.log on the database to make sure everything is working.
Finally, run the server using npm run dev. If you didn't make any mistakes, you should see Firestore {__ob__: Observer} in the console! That means Firestore is configured and working.
Let’s create some initial data to work with. We can write using the Firestore API method, add. Start by creating a component called Initialize.vue in src. This component will give us an interface to seed the data. The actual creation of data will happen in the Vuex store.
We also stubbed a get() method which will get the data later on.
The application will have two Vuex modules: conversations.js and users.js. Messages will be saved in an array in conversations.js. If the application was to get bigger and more complex, I'd make a module for messages.js as well, but it will not be necessary at this stage.
Go ahead and create conversations.js and users.js inside of store.
Inside of users.js, add the following:
state simply declares the state, so Vue can add reactivity for us. currentUser will simulate having someone logged in, and used to set the sender property on messages.
db.collection('users') is part of the Firestore API. It returns a reference to the collection in the Firestore database. If it does not exist, it is created (or will be when you insert a document into it). Read more here: https://firebase.google.com/docs/firestore/data-model.
Firestore provides a set() method to add new data. You must provide a unique identifier. In this case, I'm using what would usually be known a username - mr_a, mr_b and so on. Even if a user was the change their firstName or lastName, the unique identified would remain the same. In the above snippet, I also set currentUser in the state. This will be used later to set a sender field when when sending messages. Easy.
Let’s see a bit more of the Firestore API by creating some seed data in conversations.js.
A bit more going on here than the users store. all will hold all the conversations in the application. allIds will be an array of conversation.id. Lastly, allMsgIds will be an array containing all the conversation.message ids. We will use allMsgIds to decide if a message is new or not later on when adding new messages.
There are actually two ways to add data to a collection in Firestore, set() and add(). When using set() you must specify an id for the document. You could make one using something like uuid (which we import at the top of the store). In users, we manually specified one.
Alternatively, we can have Firestore generate a random id for us using add(), which is what is happening above. The messages are nested in the conversation document, however, are Firestore cannot generate an id for us, so we created one using the uuid module. That's it for setting the seed data up.
Now we have some seed data ready to go, head back to index.js and import the conversationsand users modules, and add them the Vuex.Store constructor. Previous code has been omitted for brevity.
Modify App.vue to import and use the Initialize.vue component:
Now try clicking Initialize app base state. The data should be written to Firestore (you may need to refresh the Firestore page to see the new collections).
The next thing to do is display the Firestore data in the Vue app. Head to conversations.js and add a get() method. We will use the ES7 async/await syntax.
Again, we get a reference to the collection using db.colletion. Calling get() on the collection returns a promise. I don't check for failure here, but you can do so with a try/catch loop in a real application.
For each conversation, we then commit a mutation which we will make next, with conversation as the payload.
Let’s create the mutation now:
Some interesting stuff here. Firstly, to access the data in a Firestore document, you need to call data() on the object. If you simply do console.log(conversation), you won't see the data. conversation.id contains the unique id Firestore made for us.
Note that doing state.app[conversation.id] = conversation does not work! It will add the properties to the store, but they will NOT have Vue reactivity, which would prevent us from adding new message down the line. See https://vuejs.org/v2/guide/reactivity.html for more details.
Note that we also do not add the messages (although when we call data() we do have access to them). We'll see why later on. We also save the conversation.id in the allIds array.
Let’s do the same for users.js. We will also set mr_b as the currentUser.
That’s it for access the data. Time to create some components to display it.
Now create ConversationContainer.vue and Message.vue inside src, and enter the following. We will start with Message.vue.
Nothing exciting here. A simple check to decide if the message was sent by the current user and to position it accordingly. Let’s move on to ConversationContainer.vue.
Again, nothing special. Later on in created() we will load all the messages, and listen for new ones. This component will receive a conversations as a prop, which comes from $store.state.conversations.
Now import ConversationContainer.vue in App.vue. Note this is the final implementation for App.vue.
Pretty straightforward — we loop allIds, and pass each conversation as a prop to the ConversationContainer.vue. We can use the id from Firestore as a :key for the v-for loop as a bonus. The Vuex mapState helper function makes it easy to access the state.
Lastly, update Initialize.vue to actually fetch the data (we stubbed get() out earlier, remember?)
If everything went well, you should be able to click the “GET” button and see the following:
Finally, the bit we’ve all been waiting for — real-time messaging. In ConversationContainer.vue, update create() to look like the following:
In Firestore, you can listen to a document using the onSnapshot() method. More here: https://Firebase.google.com/docs/firestore/query-data/listen.
Firstly, refresh the Vue application and hit get to query Firestore for the conversations. Check the console. You should see Source: Server printed twice. onSnapshot() triggers immediately, returning the current state of the document. It also will trigger every time the document changes.
This is why we didn’t populate the messages in the conversations.js - module - we want to fetch the conversation once initially, to get the ID and members, but constantly be watching for new messages. We will take advantage of the initial onSnapshot() to get the current state of the conversation, specifically the messages, and then update anytime a new message is created.
Notice the line let source = convo.metadata.hasPendingWrites ? 'Local' : 'Server'. This is due to a feature called latency compensation. Basicially, when you write data, all clients listening to the document will receive a notification, before the data is even sent to Firestore. You could use perhaps use this to show a ""member is typing..."" notification. If hasPendingWrites is true, the data has not been written yet, and if it's false, it has.
Now, we need to add the messages to the store. Update created() like so:
You access the data in the document returned from onSnapshot() using the data() method. Let's now write the mutation to add the messages in conversations.js.
onSnapshot() returns the entire conversation, including the existing messages. By checking if allMsgIds includes the message.id, and pushing it as such, we can ensure only new message are added to the conversation. Vue's reactivity will automatically update the UI for us.
That should be enough to display the messages! Try refreshing, grabbing the data and you should see something like this.
Lastly, let’s send a message in real-time. Update ConversationContainer.vue:
Fairly straightforward. Using v-model, we bind to an <input>, and on keyup.enter send the message, passing the conversationId to the commit.
Header over to conversations.js and add the following action:
Firestore documents are updated using update(), which allows you to update certain fields, without touching the others. Read more here: https://firebase.google.com/docs/firestore/manage-data/update-data.
We simply update the messages property, with all the existing ones, plus the newly sent message. When the Firestore document is updated, onSnapshot() triggers again and adds the message to the conversation.
This should be enough to get the application working real-time! Try opening another browser and send some messages — they should be received real-time in any other clients.
Firestore is very easy to get started with, and extremely powerful. Some improvements to this application include:
Helping web professionals up their skill and knowledge of…
1.2K 
6
1.2K claps
1.2K 
6
Helping web professionals up their skill and knowledge of Vue.js
Written by
I write about frontend, Vue.js, and TDD on https://vuejs-course.com. You can reach me on @lmiller1990 on Github and @Lachlan19900 on Twitter.
Helping web professionals up their skill and knowledge of Vue.js
"
https://medium.com/firebasethailand/%E0%B8%A3%E0%B8%B9%E0%B9%89%E0%B8%88%E0%B8%B1%E0%B8%81-cloud-functions-for-firebase-%E0%B8%95%E0%B8%B1%E0%B9%89%E0%B8%87%E0%B9%81%E0%B8%95%E0%B9%88-zero-%E0%B8%88%E0%B8%99%E0%B9%80%E0%B8%9B%E0%B9%87%E0%B8%99-hero-1c94acbb55af?source=search_post---------127,"There are currently no responses for this story.
Be the first to respond.
ในการพัฒนาแอพพลิเคชันนั้น บางครั้งเราก็ไม่สามารถใส่ Logic ทั้งหมดไปไว้ใน Client แอพได้ ไม่ว่าจะด้วยเหตุผลเรื่องความปลอดภัย, การทำงานที่ซับซ้อน หรือจะเป็นการลดการใช้งานทรัพยากรในเครื่อง Client ก็ตาม ทางออกของเราโดยทั่วไปก็คือ จัดเตรียม server เองซึ่งอาจต้องคำนึงถึงการ scalable ความปลอดภัย จากนั้นก็ลง server script ให้เรียบร้อย แล้วก็ลงมือเขียนโปรแกรมไม่ว่าจะเป็น API หรือ SDK ที่ต้องรองรับทั้ง iOS, Android และ Web ตามภาพด้านล่างนี้
เป็นไง แค่คิดก็เยอะละ จริงไหม เอาหละในบทความนี้เรามาทำความรู้จักกับบริการที่จะช่วยเตรียมทุกสิ่งทุกอย่างที่เล่ามาให้คุณพร้อมใช้งาน เพียงแค่คุณเขียนโปรแกรม ตาม Logic หรือ Business logic ที่คุณต้องการก็พอ และนั่นก็คือ Cloud Functions for Firebase
คือบริการที่ทำงานในฝั่ง server เพื่อตอบรับการ trigger จากบริการต่างๆใน Firebase โดยฟังก์ชันที่เราเขียนขึ้นมาทั้งหมดจะถูกเก็บไว้ที่ Google Cloud ซึ่งจะดูแลเรื่องความปลอดภัย ความเสถียร และการ scalable ให้เราแล้ว
Run functions server side in a scalable Cloud triggered by Firebase events.
การทำงานหลังจากเรา deploy โค้ดขึ้นไปที่ Cloud Functions for Firebase แล้ว ตัว Cloud Functions จะทำตัวเป็นนักดักฟัง (Listener) และรอรับ trigger จากบริการของ Firebase ทันที และจะทำงานตามฟังก์ชันที่เราเขียนไว้ เช่น ส่งอีเมล, ส่ง Push Notification, สร้าง Thumbnail, กรองคำหยาบ, ต่อ Google APIs, ต่อ Third-party APIs(แพ๊คเกจที่ไม่ใช่ Spark), นับจำนวน Child ที่มี และอื่นๆอีกเพียบ
ปัจจุบันมีบริการของ Firebase ที่ Integrate เข้ากับ Cloud Functions for Firebase แล้วด้วยกัน 5 บริการตามภาพด้านล่างเลย
ก่อนจะไปดูตัวอย่างการทำงานของทั้ง 5 บริการที่ Integrate เข้ากับ Cloud Functions for Firebase แล้วเรามาดูวิธีการ ซีตุ๊บ(Setup) กันก่อน
ขั้นตอนนี้ ถ้าใครมีโปรเจคกับ Firebase อยู่แล้วให้ข้ามขั้นตอนนี้ไป
ส่วนใครที่ยังไม่เคยมีหรือต้องกาสร้างใหม่ให้ไปที่ Firebase Console จากนั้นกด add project ระบุรายละเอียด และกด Creat project
ปัจจุบัน Cloud Functions for Firebase รองรับการพัฒนาด้วย Javascript ใน Environment ที่เป็น Node.js เพราะฉะนั้นเราจะต้องมี Node.js และ npm(node package manager) ในเครื่องเราซะก่อน วิธีลงก็มี 2 วิธีคือดาวน์โหลดจากหน้าเว็บ https://nodejs.org/
หรือ จะ install ผ่าน command line ก็ได้ ด้วยคำสั่ง
จากนั้นก็เช็คหน่อยว่าเครื่องเรามีทั้ง node และ npm แล้ว(ปกติต้องมาด้วยกัน) ด้วยคำสั่ง
หมายเหตุ เวอร์ชันขั้นต่ำของ node ที่รองรับคือ 6.3.1
run คำสั่งด้านล่างนี้ใน command line เพื่อติดตั้ง Firebase CLI ได้เลย
เราสามารถใช้คำสั่งที่ install ในการอัพเดท Firebase CLI ในอนาคตได้ด้วยนะ
จากนั้นก็เช็คหน่อยว่าเราติดตั้ง Firebase CLI เรียบร้อยหรือไม่ด้วยคำสั่ง
เริ่มต้นให้เรา run คำส่ัง login ใน command line โดยจะมี browser เด้งมาให้เรา login ก็ให้เรา login ด้วย account ที่เราใช้สร้างโปรเจคใน Firebase นั่นหละตามนี้
จากนั้นให้เราสร้าง directory ที่จะไว้เก็บไฟล์ แล้วก็ command line เข้าไปที่ directory นั้น ตัวอย่าง
เมื่อเข้าที่ directory ที่ต้องการแล้ว ก็ให้ใช้คำสั่ง
จะเจอคำว่า Firebase เป็นไฟเท่ๆแบบนี้
ให้เราขยับ cursor เลือกเอาโปรเจคที่ต้องการจะทำงานด้วย(กรณีมีหลายโปรเจค) แล้วก็กด enter เบาๆ แล้วจะมีคำถามขึ้นมาถามว่าจะลง dependencies เริ่มต้นมั้ย Do you want to install dependencies with npm now? ให้รีบตอบ Y แล้ว enter ต่อไป ก็จะเสร็จละ
เมื่อ Initial เรียบร้อย ให้เข้าไปใน directory จะพบว่ามีโครงสร้างดังรูป เป็นอันว่าจบกระบวนการ Initial แล้วนะเออ
ในการพัฒนาโดยทั่วไป เราจะไปเกี่ยวข้องกับไฟล์ในโฟลเดอร์ functions นี่หละ จำแนกโดย
การอัพเดท dependencies ทั้งหลายที่เราระบุใน package.json ให้เราเข้าไปที่โฟลเดอร์ functions ผ่าน command line โดยให้มั่นใจว่า จะต้องพบไฟล์ package.json ในนั้น
เมื่อมั่นใจแล้ว ก็ใช้ให้คำสั่งตรวจสอบ dependency ใน command line โลด
หากมี dependency มีเวอร์ชันใหม่ก็จะแสดงตามตัวอย่างภาพด้านล่างนี้
จากนั้นให้เราอัพเดท dependency โดยใช้คำสั่ง
จากนั้นลองใช้คำสั่ง ncu เพื่อเช็คอีกครั้งก็จะพบรูปยิ้มแล้ว
แต่ถึงกระนั้นก็หาใช่ว่าจะจบไม่ แม้ตัวเลขเวอร์ชันในไฟล์ package.json จะอัพเดทแล้ว เราก็ต้อง install พวก dependencies เหล่านั้นด้วย โดยให้เรา run คำสั่งเหล่านี้
คราวนี้หละ สมบูรณ์อัพเดทจริงๆสักที
มาดูตัวอย่างการทำงานกับบริการต่างๆที่พร้อมส่ง Trigger มาให้ Cloud Functions for Firebase ทำงานต่อกันเลย
Firebase Authentication สามารถส่ง Trigger ให้ Cloud Functions for Firebase ได้ 2 กรณีคือ เมื่อผู้ใช้ Sign up เข้าสู่ระบบ(Create) และเมื่อผู้ใช้ถูกลบออกจากระบบ(Delete)
จากตัวอย่างโค้ดด้านบน sendWelcomeEmail กับ sendByeEmail จะเป็นชื่อของฟังก์ชันที่จะใช้อ้างอิงใน Firebase Console ส่วน onCreate และ onDelete คือ Listener ที่รอรับ Trigger จาก Firebase Authentication นั่นเอง
โค้ดตัวอย่างการทำงานระหว่าง Firebase Authentication และ Cloud Functions for Firebase
github.com
Firebase Realtime Database สามารถส่ง Trigger ให้ Cloud Functions for Firebase ได้โดยเมื่อมีข้อมูลใหม่ถูกเขียนลง database
จากโค้ดด้านบน makeUppercase ก็คือชื่อของฟังก์ชันที่ใช้อ้างอิงใน Firebase Console ส่วน onWrite คือ Listener ที่รองรับ Trigger จาก Firebase Realtime Database
{langId} และ {pushId} จากตัวอย่างด้านบน เป็น wildcard ที่สามารถระบุได้ตาม event ที่เกิดขึ้นใน object นั้นๆ
โดยหลักการเมื่อมีข้อมูลถูกเขียนลง Database ตัว RTDB ก็จะ Trigger ไปให้ Cloud Functions for Firebase ทำงาน ตัว Cloud Functions ก็จะอ่านข้อความที่เข้ามา จากนั้นก็ขึ้นอยู่กับเราว่าจะทำอะไรกับข้อความนั้น เช่น กรองคำหยาบ, ทำอักษรตัวใหญ่, ส่งไป Translate เป็นต้น จากนั้นก็จะ Write ข้อมูลกลับไปที่ RTDB ต่อไป
โค้ดตัวอย่างการทำงานระหว่าง Firebase Realtime Database และ Cloud Functions for Firebase
github.com
Cloud Storage for Firebase สามารถส่ง Trigger ให้ Cloud Functions for Firebase ได้โดยเมื่อไฟล์ใหม่เพิ่มเข้ามาใน Storage
จากโค้ดด้านบน generateThumbnail ก็คือชื่อของฟังก์ชันที่ใช้อ้างอิงใน Firebase Console ส่วน onChange คือ Listener ที่รองรับ Trigger จาก Cloud Storage for Firebase
โดยหลักการ เมื่อมีไฟล์เข้ามาใหม่ใน Storage ตัว Storage ก็จะ Trigger ไปหา Cloud Functions for Firebase ทำงาน จากนั้นตัว Cloud Functions ก็สามารถจัดการกับไฟล์ กรณีรูปภาพ เช่น ทำลายน้ำ, สร้าง thumbnail, Blur รูป, หรือตรวจสอบว่ารูปที่อัพโหลดมาเป็นรูปอะไร(ผ่าน Google Cloud Vision API) จากนั้นก็ upload ไฟล์กลับไป พร้อมกับ write ข้อมูลไปที่ Realtime Database ได้เช่นกัน
เรื่องการจัดการกับรูปใน Client แอพ ไม่ใช่เรื่องง่ายที่จะบริหารจัดการ memory เอง ซึ่งนักพัฒนาส่วนใหญ่คงเคยประสบปัญหา Out of memory กันมาแล้ว
*เคล็ดลับ ปกติเมื่อเราสร้างฟังก์ชันในการจัดการรูป เราจะได้ memory มา 256MB ซึ่งหากเราต้องการจัดการไฟล์ที่ใหญ่และมีความซับซ้อน และเจอปัญหา memory ไม่พอ เราสามารถเข้าไปที่ https://console.cloud.google.com/ เลือกเมนู Cloud Functions และเลือกชื่อฟังก์ชันที่เราต้องการจะปรับ memory จากนั้นก็ปรับ memory ได้แล้วจ้า
โค้ดตัวอย่างการทำงานระหว่าง Cloud Storage for Firebase และ Cloud Functions for Firebase
github.com
Firebase Analytics สามารถส่ง Trigger ให้ Cloud Functions for Firebase ได้โดยเมื่อมี event เกิดขึ้น
จากโค้ดด้านบน sendAppGreeting ก็คือชื่อของฟังก์ชันที่ใช้อ้างอิงใน Firebase Console ส่วน first_open คือ event ที่เกิดขึ้นเมื่อผู้ใช้คนนั้นๆเปิดแอพครั้งแรก และ onLog คือ Listener ที่รองรับ Trigger จาก Cloud Storage for Firebase
หมายเหตุ เนื่องด้วยปัจจุบัน Cloud Functions for Firebase ยังอยู่ในสถานะ beta บาง event จาก Firebase Analytics อาจต้องใช้เวลา 2–3 ชั่วโมง เพื่อ log event และ Trigger ตัว Cloud Functions ให้ทำงาน
โค้ดตัวอย่างการทำงานระหว่าง Firebase Analytics และ Cloud Functions for Firebase
github.com
มาถึง Firebase Cloud Messaging ตัวนี้จะเป็นการที่ Cloud Functions for Firebase รับ Trigger มาได้จาก 4 บริการข้างต้น จากนั้นเราเขียนฟังก์ชันเพื่อยิง Push Notification ไปหาผู้ใช้โดยอัตโนมัติทั้งแบบรายคน หรือ Topic ก็ได้ (Firebase Cloud Messaging ไม่ได้เป็นคน Trigger) ตามตัวอย่างรูปด้านล่างนี้
จากโค้ดด้านบน sendNotification ก็คือชื่อของฟังก์ชันที่ใช้อ้างอิงใน Firebase Console ส่วน onWrite คือ Listener ที่รองรับ Trigger จาก Firebase Realtime Database จากนั้น ก็สั่งยิง Push Notification ไปหา Topic ที่ชื่อ Sample
การเก็บ Server Key ไว้ใน Client แอพโดยเฉพาะ Android เป็นเรื่องที่เสี่ยง เนื่องจากถ้ามีคน decompile APK เราไป ก็มีโอกาสที่เขาจะได้ server key เราไป และหากเขาได้ server key ไป นั่นหมายความว่า เขาจะสามารถส่งข้อความหาผู้ใช้ของเราได้
โค้ดตัวอย่างการทำงานระหว่าง Firebase Realtime Database, Firebase Cloud Messaging และ Cloud Functions for Firebase
github.com
อีกหนึ่งความสามารถที่ Cloud Functions for Firebase เตรียมมาให้คือ บริการสร้าง REST API ให้เราเชื่อมต่อกับบริการของ Firebase อย่าง Realtime Database ได้ ซึ่งเราอาจใช้ในการ Migrate ข้อมูลจากฐานข้อมูลเดิม หรือใช้งานร่วมกันเลยก็ได้ในกรณีอยากให้ลูกค้าฝั่ง Mobile App ใช้งาน Firebase Realtime Database
จากโค้ดด้านบน addMessage ก็คือชื่อของฟังก์ชันที่ใช้อ้างอิงใน Firebase Console ส่วน onRequest คือ Listener ที่รองรับการ Request ซึ่งเมื่อเรา deploy ฟังก์ชันนี้แล้ว เราจะได้ endpoint คืนกลับมา ตัวอย่าง https://us-central1-your-project.cloudfunctions.net/addMessage
โค้ดตัวอย่างการสร้าง API และเขียนข้อมูลลง Firebase Realtime Database ด้วย Cloud Functions for Firebase
github.com
เมื่อเราเขียนฟังก์ชันตามต้องการเรียบร้อยแล้ว ก็มาถึงขั้นตอนการ deploy ละ เริ่มต้นให้เรา command line ไปที่ directory ที่เราสร้างไว้ จากนั้นพิมพ์คำสั่ง
จากนั้นก็นับหนึ่งถึงสามล้าน
ประมาณ 1–2 เลยทีเดียว นานนิสนึง(ทีม Firebase บอกกำลังพัฒนาให้การ deploy เร็วขึ้น)
เมื่อเราเข้าไปใน Firebase Console เลือกโปรเจคที่ต้องการจากนั้นเลือก Functions ที่เมนูซ้าย เราก็จะพบหน้า DASHBOARD ที่แสดงฟังก์ชันทั้งหมดที่เราได้ deploy ไป, จำนวนครั้งที่ทำงาน และระยะเวลาเฉลี่ยในการทำงานแต่ละครั้ง
นอกจากนั้นเราสามารถดู Log ที่เกิดขึ้นทั้งหมดใน tab ที่ชื่อ LOGS ซึ่งเราสามารถเลือกดูตามฟังก์ชัน หรือประเภทของ log ได้
และ Tab สุดท้ายคือ USAGE คือสถิติการใช้งานของเรา เพื่อไว้ใช้ประเมินค่าใช้จ่ายที่อาจเกิดขึ้นได้
สำหรับรายละเอียดแพลนของ Cloud Functions for Firebase
เพิ่มเติมที่ https://firebase.google.com/pricing/
เมื่อคุณอ่านบทความมาถึงตรงนี้แล้ว เกิดไฟลุกอยากลองเล่นแบบจริงจัง ผู้เขียนแนะนำให้ผู้อ่านไปดูตัวอย่างของ Cloud Functions for Firebase ใน GitHub ซึ่งมีตัวอย่างเกือบ 30 ตัวอย่างหลากหลายการใช้งานให้เป็นไอเดีย
github.com
และบทความนี้ไม่ได้สอนเรื่องการเขียน Node.js ก็ต้องรบกวนผู้อ่านไปศึกษาการพัฒนา Node.js กันเองนาจา(ผู้เขียนก็ศึกษาจาก sample ใน GitHub นี่หละ)หวังว่าบทความนี้จะทำให้ทุกท่านรู้จัก Cloud Functions for Firebase และเกิดไอเดียในการเอาไปใช้งานกับโปรเจคที่ท่านทำอยู่ วันนี้ขอตัวลาไปดูการถ่ายทอดสดงาน Google I/O ’17 ก่อน บทความหน้าคงได้มีอะไรใหม่ๆอัพเดทจากเวทีนี้ ราตรีสวัสดิ์พี่น้องชาว Firebase Developers
Let you learn and share your Firebase experiences with each…
612 
7
612 claps
612 
7
Written by
Technology Evangelist at LINE Thailand / Google Developer Expert - 🔥Firebase
Let you learn and share your Firebase experiences with each other.
Written by
Technology Evangelist at LINE Thailand / Google Developer Expert - 🔥Firebase
Let you learn and share your Firebase experiences with each other.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/firebasethailand/%E0%B8%A3%E0%B8%B9%E0%B9%89%E0%B8%88%E0%B8%B1%E0%B8%81-firebase-cloud-messaging-fcm-%E0%B8%95%E0%B8%B1%E0%B9%89%E0%B8%87%E0%B9%81%E0%B8%95%E0%B9%88-zero-%E0%B8%88%E0%B8%99%E0%B9%80%E0%B8%9B%E0%B9%87%E0%B8%99-hero-fb7900af92cd?source=search_post---------128,"There are currently no responses for this story.
Be the first to respond.
จากที่ผมได้มีโอกาสไปแชร์เนื้อหา Firebase Cloud Messaging ในงาน Google I/O 2016 Extended Bangkok ที่ผ่านมา ก็เลยอยากมาสรุปให้ฟังสำหรับคนที่ไม่ทัน และคนที่ไม่ได้ไปร่วมงานครับ
Firebase Cloud Messaging (FCM) คือ บริการส่งข้อความแจ้งเตือนแบบ ข้ามแพลตฟอร์ม (cross-platform messaging) ทั้ง Android, iOS และ Web แบบฟรีๆ ชื่อเดิมก็คือ Google Cloud Messaging (GCM) นั่นเอง
ในการพัฒนาเรื่อง FCM ผมจะขอแยกออกเป็น 3 parts ดังนี้
เมื่อพร้อมแล้ว…ก็มาเริ่มกันเลย เปิด Android Studio สร้าง Project ใหม่มารอไว้ก่อน แล้วไปสู่ส่วนแรกกัน
เรื่องการ Set up Firebase ให้ไปดูที่บทความนี้
medium.com
เมื่อ Set up แล้วก็ไปเพิ่ม FCM SDKใน build.gradle ของ app-level แล้วกด Sync ก็เป็นอันจบส่วนที่ 1 ละ
เริ่มต้นส่วนที่ 2 ด้วยการสร้างคลาส MyFirebaseInstanceIDService.java และ MyFirebaseMessagingService.java ไว้ที่ app-level โดยจะขออธิบายการทำงานของแต่ละคลาสกันก่อน
MyFirebaseInstanceIDService.javaคลาสนี้เอาไว้รับค่า token โดย method onTokenRefresh จะถูกเรียกเมื่อค่า Token มีการเปลี่ยนแปลง ซึ่งเราจะสามารถ handle ค่า token เองได้ เช่นการส่งไปเก็บหรืออัพเดทใน web server เรา
MyFirebaseMessagingService.javaคลาสนี้ใช้สำหรับรับ Notification แบบ foreground โดย method onMessageReceived จะถูกเรียกเพื่อรับ remoteMessage object จาก FCM ซึ่งผมทำตัวอย่าง ให้มีการรับทั้งแบบ notification payload และ data payload(จะลงรายละเอียดเรื่อง payload ใน Part 3) รวมถึงทำการ custom ตัว notification เพื่อให้เห็นความแตกต่างในการรับ notification แบบ background
เมื่อสร้าง 2 คลาสด้านบนเรียบร้อย ก็ให้ไปประกาศตัว service ภายใน <application> ที่ไฟล์ AndroidManifest.xml กันตามนี้
ในกรณีที่แอปเราอยู่ในสถานะ background เราสามารถกำหนดค่าเริ่มต้นให้กับ notification icon, notification color และ notification channel id ได้โดยกำหนดค่า meta-data ใน <application> ที่ไฟล์ AndroidManifest.xml ได้ดังนี้
ต่อไปเราสร้าง layout กันที่ activity_main.xml กัน โดยผมจะให้มันมีหน้าตาแบบนี้ (เอาที่สบายใจเลยละกัน)
คราวนี้เรามาใช้งานกันดู เปิด MainActivty.java ของทุกคนขึ้นมาฮ๊าว์ฟฟฟจะสังเกตว่า source code ของ layout ด้านบน ผมได้ประกาศ attribute ที่ชื่อว่า android:onClick ไว้ครบทั้ง 3 ปุ่มแล้ว ดังนั้นก็มาใส่ code กันเลย
เบื่องต้นเราก็พัฒนาเสร็จละ ต่อไปเราไปทดสอบยิง Notification จาก Firebase Console กัน โดยเมื่อเข้ามาที่ Console เราจะเจอเมนูทางซ้ายมือชื่อว่า Notifications คลิกเบาๆจะเจอ เจ๊คนนึงผิวคล้ำใส่แว่น มาตอนรับ โดยเจ๊จะเชิญชวนให้กดยิง message แรกซะ เอ้า รอรัยกดเลย
จากนั้นเราจะเข้าสู่หน้าที่ให้ระบุรายละเอียดที่เราจะส่งละเบื้องต้น จะส่งแบบ Simple มากๆ แค่ใส่ Message text และเลือก User segment แล้วส่งเลย เพื่อดูผลลัพธ์กันก่อน ว่า notification เข้าไหม หากพร้อมแล้วก็กดปุ่ม SEND MESSAGE สีฟ้าๆได้เลย
แทม ทาดาแดม แทมแถ่ม แถ่ม แทม แท้ม…
สำเร็จแล้ว คราวนี้เรามาทำความรู้จักกับเมนู notification ใน Firebase Console เพิ่มขึ้นอีกหน่อย
แบบแรก User segment : เป็นการเลือกแอพที่จะส่ง โดยเลือกจาก package ของแอพ และยังสามารถ fillter ได้อีกทั้ง กลุ่มผู้ใช้, ภาษา และเวอร์ชัน แถมยังเพิ่ม Target หลายๆแอพในการส่งครั้งเดียวได้อีก อะไรจะดีงามพระรามเก้า ขนาดนี้
แบบถัดมาคือ Topic การทำงานคือจะส่งให้ User ที่ได้ subscribe หัวข้อนั้นๆไว้ ถ้าคุณใช้มันถูกทางมันจะทรงอานุภาพมาก (เดี๋ยวอธิบายในส่วนที่ 3 เรื่อง Web server จะเห็นภาพเลย)
แบบสุดท้ายคือ Single device การทำงานคือให้เอา Token มาใส่แบบยิงปืนนัดเดียว ได้นกตัวเดียว
เมื่อรู้จักการส่งขั้นสูงแล้ว คราวนี้ก็ลองกดส่งอีก 2 ครั้ง
จะเห็นว่าเราสามารถ custom title ได้ละ และที่ส่ง 2 ครั้งเพราะต้องการให้เห็นความต่างของการรับ notification แบบ background และ foreground ว่ามันต่างกัน นั่นสิแล้วมันต่างกันยังไงบ้าง
เริ่มต้นด้วยการไปที่ Firebase Console กันอีกละ จากนั้นเข้าไปที่ Project ที่เราสร้างไว้ เลือก Project settings แล้วเลือก Tab ที่ชื่อว่า CLOUD MESSAGING ก็จะเจอ Server key ให้ Copy มาซะ
เรื่อง Protocol ในการส่ง Message จะมีด้วยกัน 2 แบบคือ HTTP และ XMPP และในตัวอย่างนี้ผมจะใช้ HTTP โดยการส่งจะประกอบไปด้วย 3 ส่วน คือ
คราวนี้เรามาโฟกัสกันที่ Messages (JSON) กันหน่อย เพราะมันสำคัญมากเลย ใน Message นั้นจะประกอบไปด้วย 3 ส่วน (อีกละ)
ตัวอย่างหน้าตา messages ก่อนส่งก็จะเป็นแบบนี้
สามารถดูรายละเอียดเพิ่มเติมแบบจัดเต็มที่ https://firebase.google.com/docs/cloud-messaging/http-server-ref
เอาหละเรามาสร้าง class FCM ด้วย PHP กันดีกว่า ซึ่งตัวอย่าง target ผมจะใช้ to เพื่อส่ง topic ชื่อ news นะครับ
สร้าง class เสร็จแล้ว ก็เขียน request ดูตามนี้
เอ้า ยิง!!!…ตู้ม
เมื่อเราทำได้ถึงจุดนี้ อาจมีคำถามว่า ถ้าส่งข้อความด้วย notification payload และเราไม่ได้เปิดแอพอยู่ (background) อยากให้กด notification แล้วกระโดดไปยังหน้าที่ต้องการ ไม่ใช่หน้า launcher จะทำได้ไหม…จุดนี้ทีม Firebase เค้าเตรียมมาให้ละครับ โดยเราจะต้องทำเพิ่ม 2 ส่วนด้วยกันคือ
ที่เหลือก็อาจจะส่งค่าแนบมาใน data payload เพื่อรับค่าและ handle งานบางอย่างที่ activity นั้นๆต่อไป
จบละ…แต่ช้าก่อน Android Developer คงไม่ใช่ทุกคนที่เขียน PHP ได้ช่ายมะ งั้นเอานี่ไปเลย ผมเพิ่ม code ให้คุณใน Android Project ให้ยิงผ่านแอพกันไปเลย
Source code ทั้งหมดของ Android ผมเอาขึ้น GitHub ให้ละนะครับ โหลดไปดูได้
github.com
Before: เมื่อก่อนผมเคยยิง notification จาก server รอบละ 1 ล้าน devices ซึ่งผมใช้วิธี query ตัว token มาจากฐานข้อมูลทั้งหมดก่อน แล้วแยก Thread ออกไป 10 Threads เพื่อให้กระจายยิงไปแบบเท่าๆกัน คือ Thread ละ 100,000 devices ในแต่ละ request เราสามารถบรรจุ token ได้ 1,000 tokens ซึ่งใช้เวลาทั้งหมดประมาณ 5–10 นาที แต่บางครั้ง Thread บางตัวมี error หรือ timeout เราก็จะต้องมา handle ต่อว่าจะ retry หรือ เพิ่ม Thread ต่อไป ซึ่งมันดูไม่ scalable แบบอัตโนมัติ
After : เมื่อผมมาทำความเข้าใจเรื่อง topic แล้วก็พบว่า Firebase จะ handle สิ่งที่ผมทำใน before ให้หมด โดยผ่านแค่ request เดียว เช่น ถ้าผมต้องการจะส่ง Message ให้ทุกคนที่เคย login แล้วเท่านั้น ผมก็จะไป subscribe กลุ่มผู้ใช้หลังจาก login ใน topic ชื่อ logined จากนั้นตอนส่งแค่เพียงเราเลือกส่ง topic ชื่อ logined ตัว Firebase ก็จะจัดการส่งให้หมดละ ทำให้ชีวิตเราดีงามพระราม 8.9 ขึ้นอีก
ท้ายที่สุดผมขอเสนอ “สิ่งที่คุณอาจไม่รู้มาก่อนใน Firebase Cloud Messaging” เพราะคุณอาจไม่เคยพบเจอ หรือยังไม่มีใครบอกคุณ ดังต่อไปนี้ นี้ นี้…
ผมได้แชร์ Slide ของหัวข้อ Firebase Cloud Messaging ที่ใช้ในงาน Google I/O 2016 Extended Bangkok ไว้ให้ด้วยครับ
สำหรับบทความนี้ใช้เวลาไป 3 วันกว่าจะเขียนเสร็จ เนื้อหาลงลึกในหลายประเด็น สำหรับ FCM ก็หวังว่าบทความนี้จะเป็นประโยชน์กับ Android Developer ที่ต้องการจะพัฒนาระบบ Push Notification ขึ้นมาใช้งาน ตั้งแต่ต้นน้ำยันปลายน้ำ ได้ลองดู แล้วพบกันใหม่บทความหน้าครับ…ราตรีสวัสดิ์ พี่น้องชาวไทย
Let you learn and share your Firebase experiences with each…
464 
27
464 claps
464 
27
Let you learn and share your Firebase experiences with each other.
Written by
Technology Evangelist at LINE Thailand / Google Developer Expert - 🔥Firebase
Let you learn and share your Firebase experiences with each other.
"
https://medium.com/google-cloud/google-cloud-functions-for-go-57e4af9b10da?source=search_post---------129,"There are currently no responses for this story.
Be the first to respond.
In January 2019, Google Cloud Functions finally announced beta support for Go. Check out the official blog post for more details.
Let me start with a simple “hello world” to introduce you to the overall build+deploy experience. GCF expects an http.HandlerFunc to be the entry point. Create a package called “hello” and add a trivial handler:
In order to deploy, use the following command. It will create a new function called hello and will use HelloWorld as the entry point. The Go runtime to be used will be Go.11.
Deploying may take a while as it is noted. Once deployed, you will be able to see the HTTP endpoints on your terminal. You can also view your functions at the Cloud Console.
On the console, you can see “hello” function is deployed. You can access to logs and basic metrics such as number of invocations, execution time and memory usage.
If you have external dependencies, go.mod file will be used to get the dependencies. You can also vendor them under the function module. I imported the golang.org/x/sync/errgroup package as an example. See GCF guideline on dependencies if you need more information.
The dependency is going to be retrieved when I redeploy the function again.
Function is redeployed at https://us-central1-bamboo-shift-504.cloudfunctions.net/hello. See it yourself. You can also call the function from command line:
I also generated some load from my laptop to the function to provide you a more realistic response time data. I made 1000 requests, 10 concurrently at a time. You can see that there are some outliers but most calls fall into the 213 milliseconds bucket.
In Go, we organize packages by responsibility. This also fits well with serverless design patterns — a function is representing one responsibility. I create a new module for each function, provide function-specific other APIs from the same module.
The main entry point handler is always in fn.go, this helps me to quickly find the main handler the way main.go would help me to find the main function.
Common functionality lives in a separate module and vendored on the function package because GCF CLI uploads and deploys only one module at a time. We are thinking about how this situation can be improved but, currently a module should contain all of its dependencies itself.
An example tree is below. Package config contains configuration-related common functionality. It is a module and is imported and vendored by the other functions (hello and user).
Unlike other providers, we decided to go with Go idiomatic handler APIs (func(ResponseWriter, *Request)) as the main entry point. This allows you to utilize existing middlewares available in the Go ecosystem more easily. For example, in the following example, I am using ochttp to automatically create traces for incoming HTTP requests.
For each incoming request, an incoming trace span is created. If you register an exporter, you can upload the traces to any backend we support including Stackdriver Trace of course.
Google Cloud community articles and blogs
859 
19
859 claps
859 
19
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
See rakyll.org for more.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/stackblitz-blog/google-cloud-meet-stackblitz-da13f4e4fc29?source=search_post---------130,"There are currently no responses for this story.
Be the first to respond.
Big news! Today at Google Cloud Next in San Francisco we announced an exciting new partnership with our friends at Google Cloud. Starting with Google Cloud Platform’s (GCP) latest serverless offerings, our integration enables developers to go from idea to production — in just one click.
That’s a pretty tall order- spinning up production grade apps is a lengthy & painful process. You have to scaffold a new project, create a repo, install CLIs, configure & provision a deployment service, create build & deploy scripts, cross your fingers... and then spend hours tearing your hair out on StackOverflow.
Instead, wouldn’t it be great if you could just click a stack, select a cloud provider, choose a repo name and boom- you’ve now got an app that’s deployed instantly & scales automatically?
No configuration, no installations, no hassles. With one click on StackBlitz.com your new app is created, a production build is instantly deployed live to GCP, and a new GitHub repo is initialized for you to start working in:
And with GCP’s brand new Cloud Run, a managed serverless execution platform, you can now use any language & any framework to write serverless applications. So whatever your dream stack is, you can instantly spin up production ready, infinitely scalable apps with it on StackBlitz- in a single click.
Getting your local dev environment set up can be a high barrier, especially if you’re just starting out on GCP. That’s why we automatically spin up a preconfigured, finely-tuned dev environment powered by vscode that’s already booted & running w/ hot reloading for you—right in your browser:
Once you’ve put the final touches on your app, just click “deploy” and your app is instantly built into a production container image, uploaded to the Container Registry and is live on prod in mere seconds—no CLI installs/logins required:
Commit changes, create branches, push to GitHub, and even open pull requests- your same exact git workflow “just works”:
Along with serverless, we’ll also be bringing this entire end-to-end experience to other core GCP offerings like Kubernetes Engine, AI & Machine Learning, and Compute & App Engine. The barrier to entry for these tend to be a bit higher, which makes that instant experience feel even more like magic.
We’re working around the clock to launch this experience to all GCP developers. Drop your info in the beta signup form and we’ll send you an invite as soon as we’re out of alpha!
News and Engineering from the StackBlitz Team
1.98K 
7
1.98K claps
1.98K 
7
Written by
Software engineer & builder of @StackBlitz, @GoThinkster (acq) & https://realworld.io 🤘 Code the future!
News and Engineering from the StackBlitz Team
Written by
Software engineer & builder of @StackBlitz, @GoThinkster (acq) & https://realworld.io 🤘 Code the future!
News and Engineering from the StackBlitz Team
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/swlh/eks-vs-ecs-orchestrating-containers-on-aws-9d49d3ff7f8d?source=search_post---------131,"There are currently no responses for this story.
Be the first to respond.
AWS announced Kubernetes-as-a-Service at re:Invent in November 2017: Elastic Container Service for Kubernetes (EKS). Since yesterday, EKS is generally available. I discussed ECS vs. Kubernetes before EKS was a thing. Therefore, I’d like to take a second attempt and compare EKS with ECS.
Before comparing the differences, let us start with what EKS and ECS have in common. Both solutions are managing containers distributed among a fleet of virtual machines. Managing containers includes:
"
https://blog.sketchapp.com/smart-layout-a-new-welcome-window-and-projects-on-the-mac-and-cloud-whats-new-in-sketch-9c1721a2cd3b?source=search_post---------132,"There are no upcoming events.
Want to host your own event? Get in touch.

        While we’re working hard on the next big Sketch for Teams features, we also wanted to share some equally important updates to the Mac app!
      
It’s been a busy summer for the whole team here at Sketch and we’re not slowing down yet. While we’re working hard on the next big features for Teams, we also wanted to share some equally important updates to the Mac app — Smart Layout, a new Welcome Window, and Projects support.
When we laid the foundations for design systems in Sketch, we started by creating a powerful way to build and reuse components with Symbols. Then, we made it easier to keep Symbols and styles consistent across teams with Shared Libraries. With Sketch 58, we’re taking things a leap further — introducing Smart Layout.
With Smart Layout, you can say goodbye to manually resizing Symbols to fit their overrides, or building multiple Symbols within the same design, just to accommodate different sizes of the same component. Now, Sketch can handle all of that.
Simply set your Layout properties for your Symbol master and it’ll resize automatically whenever you edit an override in one of its instances. Better still, Smart Layout maintains the padding and spacing between layers in your Symbol as it grows in the direction you’ve set.
A basic button symbol with a horizontal layout,growing from the center
For basic components like buttons, Smart Layout is going to be a real time-saver. But when it comes to complex components, we think it’ll be a true game-changer. By setting up groups inside your Symbol master, you can set different layout directions for different parts of the same Symbol. So a card component can grow vertically to fit its copy, while the buttons at the bottom grow horizontally to fit their labels.
That’s just the start of what Smart Layout can do. You can see a few more examples of it in action on our website. To see how it works in detail, check out the Smart Layout documentation, or grab some popcorn and watch our video guides.
If you’d prefer to get some hands-on Smart Layout experience, you can download the demo file and check it out for yourself. We can’t wait to see how you’ll use Smart Layout to supercharge your Symbols.
Last month we announced Projects on Sketch Cloud, so whether you’re part of a team or working on your own, you can organise your documents on Cloud. With Sketch 58, we’re bringing Projects to the Mac as well.
First of all, we’ve made it easier to save Cloud documents to a specific project. When you create a new project and save it for the first time, you’ll be able to choose a workspace and project to save it to, using the drop-down menus. We’ll also add the ability to create a new project from the Mac app very soon!
With the launch of Sketch for Teams, Cloud documents and the recent introduction of Projects on the web, it was only a matter of time before we brought full support for all of these features to the Mac. In our latest release, we’ve updated the Welcome Window to accommodate them all. The old Welcome Window served us well but it’s now been replaced with the all new Documents Window, and it’s bigger, better and snappier than ever.
Whether you’re a part of a team or a personal account user, you’ll be able to see all of your Teams and Projects in the sidebar and clicking on any of them will show you just the documents you’re looking for. We’ve made the Documents Window bigger, too, so there’s plenty of room for all those thumbnails.
Like the old Welcome Window, you’ll still be able to see recently opened documents and access templates using the relevant tabs in the Sidebar. We’ll be making further updates and improvements to the new Documents Window, so keep an eye out for future releases.
You might notice something missing from the new Documents Window right now and that’s Libraries. One of our next big steps is improving support for Libraries across Cloud, Teams and the Mac app, giving them their own place in your workflow, just like Documents & Projects. Stay tuned for updates on publishing your Libraries on Cloud, approval controls for accepting updates, instant update notifications, and, soon, the ability to open source your Libraries and UI Kits with other teams and users, through improved sharing features.
We’re on the road right now, bringing Sketch to design conferences across the world. If you’re heading to any of the events below, come and say hi, pick up some Sketch swag, catch up with the team and even get a sneak preview of some new features:
As ever, we’d love to hear what you think about Sketch, your feedback helps us improve whole the platform. Keep an eye out for future updates, we’ve got plenty more to show you before the year is out.
Turn your ideas into incredible products with a 30-day trial.
A monthly digest of the latest Sketch news, articles, and resources.
©
        2022
        Sketch B.V.
"
https://medium.com/aws-activate-startup-blog/hiring-a-cloud-engineer-questions-to-ask-and-what-you-should-hear-12a960d97163?source=search_post---------133,"There are currently no responses for this story.
Be the first to respond.
In this blog post, I provide tips for hiring an experienced cloud engineer who can help you run your startup or small business efficiently. I use the term “cloud engineer” as more of a quality/virtue than a position description or title. Whether you intend this person to be a CTO or an individual contributor, you need someone with the right skill set to architect your cloud. I show you how you can use five core principles to evaluate the strengths of candidates and make a great hire for the job. I also provide sample questions that you can ask during an interview as well as sample answers that you should hear from candidates.
In the following sections, I focus on five principles that are critical to most startups and their customers:
If you have a digital business, downtime is a worst-case scenario. Customers, revenue, and reputation all suffer every minute your application is down. Consider downtime, measured in minutes, as one of the key performance indicators for your engineer. If you start seeing spikes in that metric, you know you have a problem in your IT shop. Check out fellow Solutions Architect Slavik Dimitrovich’s post on High Availability for Mere Mortals.
“I architect defensively.” Your cloud engineer should know how to architect for failure at the following levels: application, server, architectural (app tier, database tier, etc.), and physical data center.
“I have 5 9s tattooed on my arm.” Your cloud engineer should be motivated and proud of his past availability metrics.
“I don’t mind pager duty.” This is a great sign. It indicates accountability and ownership of her work. You want an engineer who designs architectures and wants to own operating them. Another less obvious perspective is that an engineer who has designed a well-architected, self-healing stack sees pages as informational heads-ups, not drop-everything action items.
The Slashdot effect, aka the Reddit-hug-of-death, is a situation where news or social media sites link to a poorly architected site, leading to an overwhelming amount of traffic. These massive ramps in traffic are exactly what your company wants, but if your architecture isn’t scalable, it will crash and burn when you need it the most. Further, before and after those spikes your architecture should elastically downsize itself to keep your costs as low as possible.
“Elastic this, elastic that….” Elasticity is one of the most important advantages that cloud computing brings to the table. Elasticity is all about matching capacity to demand as closely as possible. Not all elements in an architecture can be elastic, but your architect should recognize the importance of elasticity and strive to take advantage of it at every opportunity.
“Horizontal scaling over vertical scaling.” In the modern cloud, adding additional compute/networking/storage to an existing server (vertical scaling) is a trivial task. But there are eventual performance and practical cost limits. Balancing load across an auto-scaling group that adds/removes smaller instances is a better approach from a scalability, cost, and elasticity standpoint. Your architect should want to instinctually architect for horizontal scale from the start.
“Internet scale services.” AWS has a handful of services that are implicitly scalable (for example, Amazon S3, Amazon DynamoDB, and Elastic Load Balancing). This means that it takes little, if any, manual intervention to scale those services from handling a few gigs to several petabytes. A good cloud engineer leverages services to simplify, or entirely mitigate, the effect of something like storage as a bounding factor for an application.
Page response time is one of the most important metrics affecting usability. Architects have many tools at their disposal to understand where each millisecond of load time is being spent. That load time is affected by several factors including your user’s physical location, server-side processing, and networking bandwidth. A good architect can prioritize load-time bottlenecks and tactically address them throughout the full stack.
“Caching data.” As you begin to scale, a database tends to be where you first feel growing pains. Offloading database queries into a caching tier like Memcached or Redis can give your application lightning fast reads and can also significantly reduce demand on your database. Your architect should have some experience caching data in a key-value store.
“Caching assets at the edge.” One of the most effective ways to reduce page load time (beyond compression) is to cache static assets like images, CSS, and JavaScript as physically close to your end users as possible. Content distribution networks (CDNs) make caching both static and dynamic content “at the edge” a fairly straightforward process. Your architect should have some experience with CDNs.
“I’ve used tools XXX and YYY.” One of the best indicators that an architect has been responsible for a real-life production workload is familiarity and preferences with logging and monitoring tools. An architect’s metrics are like a pilot’s instrument panel: she trusts and relies on them to keep the ship in the air.
The pulse of a digital company can be measured by its rate of feature releases. Retaining customer engagement and satisfaction is often correlated with how fast your product evolves to fix a bug or release a new feature. If your cloud engineer is blocking developer progress because he is “still provisioning that cluster” or “still configuring the database,” he is slowing down your developers instead of enabling them to innovate. A good cloud engineer looks for every opportunity to solve an easier problem to get the same job done.
“Why reinvent the wheel?” No matter the type of company, common sets of technical requirements evolve in an increasingly complex architecture, such as background workers, outbound email, or mobile push. Avoid engineers who believe they need to implement seemingly common architectural patterns from scratch.
“I don’t like managing my own clusters.” If candidates pride themselves in how they operated a 15-node cluster, it might be a red flag. Managing that cluster was probably a major focus, perhaps even a full-time job. Managing your own cluster is as in-the-weeds as you can get. Good cloud engineers let managed services sweat the management details so they can focus on optimizing the workload broadly across your stack.
“I like to avoid database administrators.” Managed database services like Aurora on Amazon RDS drastically reduce the operational complexity that DBAs have historically been responsible for. Database management experience is now much less important than the ability to select the right managed database for the right workload.
Companies that make physical products are laser focused on their physical supply chain. Similarly, a digital company should have intense focus on Development Operations (DevOps). Think of your company’s DevOps process as the supply chain responsible for making sure your product features ship to your customers. Tactically, DevOps means ensuring that code is written, well managed, tested, and deployed in a reliable and repeatable way. Disruptions or holes in your company’s DevOps procedures will inevitably cause downtime or service degradation for your customers.
“It’s a balance.” Adding too much DevOps process and technology can slow things down if there are not enough people to justify it. Your cloud engineer should recognize that balance and find the appropriate level of DevOps based on factors like company size, developer mix, and growth targets.
“Infrastructure as code.” In today’s cloud, one can define entire technical architectures with specially formatted text files like AWS CloudFormation templates and AWS Elastic Beanstalk ebextension configs. If a candidate can talk intelligently about keeping infrastructure managed at a source-code level, it’s a sign of a progressive and forward-thinking cloud engineer.
“I $#@%^& love/hate tool XXX.” It’s a good sign if candidates have strong opinions about which tool chains they prefer. Someone who has learned to love or hate a CI or CD tool has actually walked the walk down the DevOps path.
There are a variety of backgrounds that can develop people into great cloud engineers. I find that software development and DevOps experience translate nicely into the cloud. Don’t immediately disqualify candidates with limited hands-on cloud experience. If they value and understand the ideas discussed in this post, they typically can learn the cloud-specific nuances quickly. Above all, you want an individual who can holistically look at your architecture and continuously optimize.
Written by Paul Underwood, Solutions Architect, Amazon Web Services
For startups building on AWS.
402 
6
402 claps
402 
6
For startups building on AWS.
Written by
Amazon Web Services Startup Program. Follow @AWSstartups.
For startups building on AWS.
"
https://medium.com/@rohit_kapur/how-im-slowly-moving-off-the-cloud-with-nextcloud-460118a7723d?source=search_post---------134,"Sign in
There are currently no responses for this story.
Be the first to respond.
Rohit Kapur
Feb 1, 2019·16 min read
I bought a Raspberry Pi from Amazon with the goal of moving off of Google Photos to a self-hosted photo backup solution that’s nearly as seamless.
My goals for this project:
"
https://medium.com/google-cloud/linux-gui-on-the-google-cloud-platform-800719ab27c5?source=search_post---------135,"There are currently no responses for this story.
Be the first to respond.
Top highlight
Having servers on the cloud is great: you can access them from anywhere and at anytime! You can easily ssh into your server and do whatever you want, right?
Well, what if you want to browse the web? I want to use Chrome! Or you could use Lynx … but I’ve heard it’s not CSS3 compatible. This is a little tutorial that will take you through all the steps to have GUI access to a Google Compute Engine instance.
Important: if you start an instance you’ll be charged per minute. Go to the end of this post to see how to stop it and get $300 in credits!
Update: My next post will discuss how to make this connection secure by using VNC over VPN.
Visit the developers console, log in, and create a project if needed clicking on the Create Project button. Navigate the left menu to list the VM instances running on your project: Compute > Compute Engine > VM instances. If it’s the first time you do this for this project it might take a bit longer, since it’s setting some stuff up, don’t worry this happens only once.
Click on the Create Instance button to access the instance creation form. Choose a name for your instance. Any instance type and linux distribution would work, but if you want to go with something safe choose n1-standard-1 with backports-debian-7-wheezy-v20150423.
Choose a zone close to you to have the best latency in your connection.
If you’d like to use Windows the instances already come with support for RDP (Remote Desktop Protocol) so you don’t need any extra steps.
Once your instance is running you can SSH directly into it by simply clicking the SSH button. This also handles the authentication for you, really handy!
Once connected let’s update our sources list and install some extra packages:
Before we continue configuring the VNC server that will allow us to access our instance through a desktop environment we should install one. You can install your favorite one:
a) If you like Gnome and are not in a hurry:
Gnome is beautiful … and heavy! So be patient while everything gets installed, two to five minutes is completely normal.
b) If you prefer something faster to install you might like Xfce:
Now that our instance has a desktop environment let’s make it accessible via VNC. Start the vncserver, and follow the directions to create a password
Note: this password will grant access to your instance, so make it strong.
If everything went fine your VNC server is now running and listening on port 5901. You can verify this with netcat from the Google Compute Engine instance:
There’s many options available, my favorite one is RealVNC Viewer. Install one but don’t try to connect to your server just yet: it will fail as the firewall rules don’t allow it.
In order to communicate with our instance we need its external IP. You can find it on the Developers Console.
Let’s try to connect to it using netcat again:
Regardless of the tool you use the connection will fail, this is expected as the firewall rules block all communications by default for security reasons.Let’s fix that.
Navigate to the configuration for the default network “Compute > Compute Engine > Network” and then click on default. Or you could also click here and choose your project.
We’re going to add a new firewall rule, pressing the corresponding button.
Choose a descriptive name for the rule.
We will allow traffic coming from any source, which is why we use 0.0.0.0/0, the IP mask equivalent to a wildcard.
The traffic will be on the port 5901 for protocol TCP and going to instances tagged as vnc-server.
The last step is to tag our instance as a vnc-server, for that go back to the VM description page and click on “add tags”
Let’s first of all make sure that the connection is now allowed by the firewall:
Great! Everything seems ready for our VNC client to connect. Open your VNC viewer and connect to the IP of your Compute Engine instance on port 5901.
To connect you’ll need to provide the password you gave at the beginning of this tutorial.
And voilà! You can now use your favorite Desktop environment on your Google Compute Engine instance.
If you still cannot connect to VNC after you have created a firewall rule you should make sure that your IP has not been banned by sshguard.
To see if this is the case you can run:
If your output differs from this one flush the table and retry:
An instance running on the cloud has a cost but the good news is that you can simply stop it and restart whenever you need it again. Click on the Stop button and you’ll be charged only for the associated disk which at the moment of writing of this article is 40¢ per month. I dare you finding a cheaper cup of coffee in San Francisco!
Finally, if you’re new to the Google Cloud Platform make sure to get into the Free Trial to access $300 in credit so you can try it out and have some fun!
I hope this was useful. Feel free to add any comments for feedback or questions on twitter.
Google Cloud community articles and blogs
1K 
41
Some rights reserved

1K claps
1K 
41
Written by
VP of Product at Dgraph 🏳️‍🌈 Catalan
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
VP of Product at Dgraph 🏳️‍🌈 Catalan
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/insiden26/tech-at-n26-the-bank-in-the-cloud-e5ff818b528b?source=search_post---------136,"There are currently no responses for this story.
Be the first to respond.
If you are in Europe, chances are that you have heard of N26. It’s a #NoBullshit bank that gives you complete control over your finances from your smartphone. It’s one of the most successful startups of Europe and it is what a bank of 2018 should be.
Also, it is a bank that is completely hosted in the cloud!
I joined N26 sometime back as a developer in the core backend team. I had done microservices, cloud, CI/CD, containerisation etc before, but I imagined that a bank would be more “traditional” — I am glad my assumptions were wrong. Since then I have been amazed by the sheer brilliance of the tech architecture and technologies that work together to make it all possible. Here are a few technologies that are extensively used in N26.
I think it is worth sharing our experiences, learning and how we do things. So this post is a mix between what I have learnt here in short time.
In the beginning, there was a Monolith (and rumours are, there still is!). As we started growing, we quickly realised that we need to have services that can scale independently. Taking from Conway’s law, it became essential to have cross-functional teams that are centred around features. As the understanding of the domain matured, we started slicing up the monolith into smaller logical services.
Currently, there are about 15 cross-functional teams that are responsible for 60+ micro-services. These teams are structured around business sub-domains and functionality and are responsible for managing corresponding services. This gives us more freedom to choose the right tools and helps us focus on how we want to build a particular service. This also lets teams experiment with new ideas quickly and iteratively.
Coming to tech — for a bank, we do things differently. Here are the things we are proud of:
CI/CD pipeline: As soon as the code is pushed to a branch of the repository, the CI server gets kicked in and runs automated tests and reports the build status. Every merge to master triggers the CD pipeline. Here, is how a typical pipeline looks like with different stages.
Once all the tests pass and there are no security flaws (and we have the necessary business approvals), then the application is deployed to different environments in blue/green deployment manner.
There are about 900 builds and 100+ live deployments every week.
Infrastructure as code: The N26 Core Teams (Security, SRE and TechOps) live by the discipline that the infrastructure provisioning should be automated. We are big fans of Hashicorp and use:
Containerisation and Autoscaling services: The services are build as Docker containers. This ensures consistency across environments. The configuration is injected dynamically and the load balancers are configured to autoscale service instances up and down based on the load.
All services are designed in a way that they can be brought up or killed anytime. Our servers are Cattle and not pets . Containerisation also eases development and local testing. Setting up a dev machine identical to other environments is usually a matter of minutes.
Monitoring and logging: When you have a distributed system, you need to have an eye on everything happening in the system.
We Love to Experiment — A good company is not just about the tech but also about the culture it maintains. Yes, we are a bank. Yes, everything we do needs to be audited and pass through multiple gates of security. And yes, we are not afraid to try out new things!
Every 6 weeks, there are Getting Stuff Done Days —these are 2 days where everyone in the company is allowed to work on anything they want. At the company level that’s about 7500 hours of creative energy and motivated people doing what they feel improves things. We believe, if you have the right people and you trust them to do the right thing, it goes a long way.
As part of my first GSDD, I evaluated the newly released SpringBoot 2 against SpringBoot 1 and published my findings here that got published in the weekly Spring newsletter.
Another thing that came out from GSDD is that someone experimented with Kotlin, shared the learnings with others and now we are moving towards adapting Kotlin at a bigger level in the company — read why N26 loves Kotlin.
There is so much more to cover and I have just begun my journey here. I have just touched the different areas above that are oceans in themselves. I will continue learning and sharing.
There is so much that has been already done and still so much more to do. We are going to expand to UK and US soon. We are growing and if any of the above gets you excited — check out the positions that you can fill here.
It takes people of all makeups to create a bank that adapts…
977 
6
977 claps
977 
6
It takes people of all makeups to create a bank that adapts to the needs of millions. That's why we mix data, intuition, and heart. To build a powerful tool, to make life that much easier. We're some of the best at what we do. And want you to be too.
Written by

It takes people of all makeups to create a bank that adapts to the needs of millions. That's why we mix data, intuition, and heart. To build a powerful tool, to make life that much easier. We're some of the best at what we do. And want you to be too.
"
https://medium.com/thinking-design/simple-tips-to-improve-user-testing-4ce9dddbe462?source=search_post---------137,"There are currently no responses for this story.
Be the first to respond.
Top highlight
Testing is a fundamental part of the UX designer’s job and a core part of the overall UX design process. Testing provides the inspiration, guidance and validation that product teams need in order to design great products. That’s why the most effective teams make testing a habit.
Usability testing involves observing users as they use a product. It helps you find where users struggle and what they like. There are two ways to run a usability test:
We’ll focus on the first, but some of the tips mentioned can be applied to both types of testing.
The earlier you test, the easier it is to make changes and, thus, the greater impact the testing will have on the quality of the product. A lot of design teams use the excuse, “The product isn’t done yet. We’ll test it later,” to postpone testing. Of course, we all want our work to be perfect, which is why we try to avoid showing a half-baked design. But if you work too long without a feedback loop, the chances are higher that you’ll need to make a significant change after releasing the product to the market. It’s the classic mistake: thinking you’re the user and designing for yourself. If you can invest energy to learn early and prevent problems from happening in the first place, you will save a tremendous amount of time later.
The good news is that you don’t need to wait for a high-fidelity prototype or fully formed product to start testing. In fact, you should start testing ideas as soon as possible. You can test design mockups and low-fidelity prototypes. You’ll need to set the context for the test and explain to test participants what’s required of them.
Before starting usability testing, be crystal clear on your goals. Think of the reason you want to test the product. What are you trying to learn? Ask yourself, “What do I need to know from this session?” Then, once you understand that, identify exactly which features and areas you want feedback on.
Here are a few common objectives:
Once you have an objective, you can define which tasks you’ll need to test in order to answer your questions or validate your hypothesis and assumptions. The objective is not to test the functionality itself (that should be a goal of the quality assurance team), but to test the experience with that functionality.
When designing tasks, make them realistic and actionable. These could be specific parts of the product or prototype that you want users to test — for example:
Don’t squeeze in many subjects in your usability testing checklist. Conducting the tests and analyzing the results will take a lot of time. Instead, list the important tasks in your product, and order them by priority.
Testers need to know what to do. Make it easy. Users tend to become discouraged when tasks are unclear.
As a moderator, you should be very clear about the goal of a task (for example, “I expect that users will be able to complete the checkout within two minutes”). However, you don’t need to share that goal with participants.
Patrick Neeman of Usability Counts recommends assigning five tasks per participant. Considering the time of the session (usually 60 minutes), leave time for your questions, too.
People tend to perform more naturally if you provide them with a scenario, rather than dry instruction. Instead of asking them something like, “Download a book with recipes,” you could phrase it as a scenario, like, “You’re looking for some new ways to cook beans. Download an ebook with recipes.” A scenario provides some context and makes the task more natural for the user. The more naturally participants perform the task, the better the data you will get as a result.
Go through the task several times yourself, and work out appropriate questions to ask. It’s hard work but will definitely pay off.
Finding the questions you want to ask is important, but also, the people who participate in your test should be representative of your target audience (user persona). There’s no point in watching people use your product if they don’t match your target audience. Therefore, as soon as you have some idea of what to test, start recruiting. Carefully recruit people based on your goals. Be advised: Finding people for usability tests is not easy. In fact, recruiting is one of the biggest reasons why many companies don’t regularly talk to their users. Thus, put in the extra effort to find people who represent your target audience.
If your product already has a customer base, then a quick analysis of available information (for example, analytics data, customer support tickets, surveys, previous usability sessions) will help you assess what you already know or don’t know about your users.
Numbers provided by an analytics tool on how the user interacts with a product — clicks, user session time, search queries, conversion, etc. — will help UX designers to prepare for usability tests. Image: Ramotion
Of course, feedback from friends and family is better than nothing, but for better results, you’ll need independent and unbiased users, ones who haven’t used your product before. Your friends and family are too close to the product to know how real people would perceive it for the first time.
Before recruiting users, you’ll need to decide on the type of people to test your product. Define criteria and select testers according to it. For example, if you are testing a mobile app for ordering food, most probably you’ll need feedback from people who order food regularly. Translate this requirement into precise, measurable criteria, so that you can use it to screen prospective participants: people who order food at least once a week from different delivery services (participants should have experience with at least three services).
In addition to specifying the users you want to talk to, think about people you don’t want to see in any of your sessions. As a rule of thumb, avoid testing with tech-savvy users and early adopters, because such testing might not be as revealing as you’d like. Also, avoid participants who have conflicts of interest (such as ones who work for competitors).
Next, create a screener questionnaire to identify people for your testing sessions. As with any good survey or questionnaire, avoid leading questions. An example of a question that would reveal the “right” answer is, “Do you like ordering food using a smartphone?” Most people who want to join a testing session would surely answer yes to that question.
You can prepare a list of questions in the format of a survey and ask potential testers to fill it out.Google Forms is a great tool for creating screeners and collecting the responses in a spreadsheet. Because responses go right into a Google spreadsheet, you can sort and filter them.
Next, you’ll need to get people to fill out the screener. One way to achieve this is to create a job description with a link to your survey. In the description, explain your expectations, and offer an incentive to motivate people to show up (such as a $100 Amazon gift card for a 60-minute interview). Craigslist, Twitter and Facebook are the most obvious places to post the job description.
Things will be a bit harder when you need to recruit very specific and hard-to-find types of users. But even in this case, it’s totally solvable:
Tip: If your product is on the market, you could show a message — “Want to give us more feedback?” — somewhere in the user flow, which leads to your screener form. Also, if you use a service such as Intercom, you could automatically email new users after they have used the product five times, inviting participation in testing.
Some product teams think they need a lot of participants for usability testing. In fact, testing with five users generally unveils 85% of core usability problems. The most important problems are easy to spot for people who are new to your product, and difficult for you to spot because you no longer have fresh eyes. It turns out that you’ll learn a lot from the first person you talk to, a little less from the next, and so forth.
Once you collect the responses and filter the list of potential participants based on your criteria, select the five candidates who fit your criteria the best.
Clearly Instruct on How to Join the Session
When you schedule a test session, provide all details in a confirmation email to participants:
To minimize frustrating no-shows, you could ask users to reply to confirm. For example, your subject line in the confirmation email could be something like, “Usability session scheduled on May 14 at 3 pm. (Please reply to confirm).” You could also call participants to remind them about their appointment on the day before the session.
Hearing directly from users is one of the fastest ways to learn about and improve your product. By watching someone use your product, you can quickly identify areas where the product isn’t clear enough.
When a session begins, the participant might be nervous and unsure about what to expect. The quality of a usability session is directly related to the rapport you build with the participant. The deeper the participant’s trust in the moderator, the more frank their feedback will be. Conduct the test in a way that participants will feel comfortable giving you honest feedback.
A few things to remember:
Once you have presented the task, everything should be led by the participant. Your goal in this session is to understand how users will use the product. For example, if the participant takes an unplanned route through your app, don’t correct them! Wait to see what happens. This is valuable learning.
Your participants are there to teach you something, not the other way around! Judging users or trying to educate them during the test would be counterproductive. Your goal is to get as much information as possible in the time available and to understand it all from their perspective.
Thus, avoid phrases like, “That was obvious, right?” and “Do you really think so?” while raising your eyebrows, even if something seems obvious. Instead, ask something like, “How easy or difficult was it for you to complete this task?” or “Why do you think that?” There should never be any judgement or surprise in either your tone or body language.
When you explain how the product you’re testing functions, you’ll almost certainly be introducing bias to the test. In the real world, your product will live on its own. You won’t be there to guide users along and tell them exactly what to do and how to use it. Participants should have to figure things out based on the task’s description and what they see in the interface.
When participants start a task, try your best not to interrupt them. The more you interrupt, the less likely they’ll have the confidence to complete the task. They’ll lose their flow, and you won’t see anything resembling natural behavior.
Drawing attention to specific issues that you care about could cause people to change their behavior and focus their answers on the issues you’re emphasizing. This problem is particularly common in discussions on user interface design: If you were to ask people about a particular design element (such as the color of the primary call-to-action button), they’ll notice it thereafter much more than they would have otherwise. This could lead participants to change their behavior and focus on something that doesn’t matter.
The think-aloud method is critical to getting inside the participant’s head. In fact,Jakob Nielsen argues that it’s the best usability tool. Using the think-aloud technique, the moderator asks test participants to use the product while continuously thinking out loud — simply verbalizing their thoughts as they move through the user interface. Using this technique for the food-ordering app, most probably you’d get responses like, “Hm, this looks like a food-ordering app. I’m wondering how to order food. Maybe if I tap here, I’ll see a form to request a meal.” The technique enables you to discover what users really think about your design and will help you turn the usability session into actionable redesign recommendations. Responses like, “Oh, it loads too slowly”, “Why am I seeing this?” and “I expected to see B after A” can be translated into actionable design changes.
Tip: Because most users don’t talk while using a product, the test facilitator will have to prompt them to keep talking. Ask something like, “What’s going on here?” when test participants interact with the product.
Mind the distinction between listening and observing. While both methods will provide UX designers with valuable information, many UX designers focus too heavily on listening. Observing users can uncover a lot more in a lot less time. You can learn a lot by listening to people, but you can learn way more by seeing how they react to a product.
Most people want to look smart, which is why during testing sessions, you’ll notice participants struggle through a task but then tell you that it was easy for them. Thus, focus on their behavior, not their opinion.
When you’re not quite sure what a participant is talking about, ask for clarification. A simple question like “When you said… did you mean…?” will make things clear. Don’t leave it to the end of the session. The end of a session is too late to go back and figure out what someone was talking about.
Be eager and curious to learn as much as you can about the user’s experiences and perspectives. Don’t settle for the first answer you get. Always dig deeper by asking follow-up questions. Follow-up questions will give you a lot of insight into what has really happened. People often can’t clearly state their motivations without being prompted. A simple well-timed follow-up question will usually yield a more thorough explanation or valuable example.
During the session, participants will certainly ask you some questions. Here are some of the most common ones:
Resist the temptation to tell them all about it! Ask them a question right back. It’ll reveal a lot.
A lot of product teams think about the design process as a linear process that starts with user research, has a phase for prototyping and ends with testing. However, treat it as an iterative process.
Testing, as much as coding, designing and gathering requirements, has a place in the iterative loop of product design and development. It’s important to test at each interval of this process, if resources are available.
The best way to avoid having to rework a product is to inject feedback into the process. Regular user feedback (not necessarily in the form of usability testing, but also in online surveys or analysis of customer support tickets) should be at the heart of the UX design process.
Testing in-person is a great way to understand user behavior; unfortunately, it’s not always possible. What if you need to test only one small feature, or your test participants are dispersed (for example, if your product targets international customers), or you need results fast (ideally, today)? In this case, focus on remote testing. But how do you handle remote sessions?
Nowadays, a ton of tools are available for you to run remote unmoderated tests. Here are some:
You could conduct remote moderated sessions using Google Hangouts or Skype. Simply ask users to share their screen, and then see how they interact with your product. Don’t forget to record the session for further analysis. (Record both video and audio; without audio, it might be hard to tell why certain behavior occurred.)
The downside of remote testing is that many participants get tested so frequently that they’ve learned to focus on certain aspects of a design. To compensate for possible “professional” testers, you’ll need to analyze the test sessions (for example, by watching the video recordings), and exclude results from people who don’t seem to provide genuine feedback.
Involve the whole product team in the testing process. Having an opportunity to observe users will help the whole team understand the problems with usability and to empathize with users. Testing enables you to build shared understanding, even before the team starts designing.
Product design is a team sport. And because testing is an essential part of the design process, it should be discussed with all team players. Direct involvement in preparing the test will make team members more interested in the activity. As the person responsible for UX research, you should make it clear how your team will use the findings from the usability tests.
You can’t expect the entire team to join the testing sessions. In most cases, it isn’t necessary for everyone to observe all usability testing first-hand (although it might be desirable). But you can record the testing sessions on video and share it with colleagues. Video can be extremely helpful during design discussions.
One thing that slows down many forms of usability testing is analysis. Extracting findings from the data collected during testing sessions could take days or even weeks. But if the entire team watches the sessions and takes notes, they will be better able to summarize the findings and decide on next steps.
A common question among many product teams is, “When should we test?” The answer is simple: Test before a design or redesign, test during the design, and then test afterwards, too.
Trying to solve everything at once is simply impossible. Instead, prioritize your findings. Fix the most important problems first, and then test again. However, if that’s impossible (for example, if the problems are too big to tackle), then prioritize problems according to their impact on revenue.
You can’t afford to skip testing, because even a simple round of testing could make or break your product. Investment in user testing is just about the only way to consistently generate a rich stream of data on user behavior. Thus, test early, test often.
For the latest trends and insights in UX/UI design, subscribe to our weekly experience design newsletter.
Nick Babich is a developer, tech enthusiast, and UX lover. He has spent the last 10 years working in the software industry with a specialized focus on development. He counts advertising, psychology, and cinema among his myriad interests.
Originally published at blogs.adobe.com.
Learn about Adobe XD, our all-in-one design and prototyping tool:
Stories and insights from the design community.
1.2K 
1
1.2K claps
1.2K 
1
Written by
New Tools for New Creatives. Get all the latest creative apps plus seamless ways to share and collaborate. All right on your desktop.
Stories and insights from the design community.
Written by
New Tools for New Creatives. Get all the latest creative apps plus seamless ways to share and collaborate. All right on your desktop.
Stories and insights from the design community.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@LightspeedHQ/google-cloud-spanner-the-good-the-bad-and-the-ugly-5795f37a7684?source=search_post---------138,"Sign in
There are currently no responses for this story.
Be the first to respond.
Top highlight
Lightspeed HQ
Sep 27, 2018·15 min read
Originally published on the Lightspeed HQ Blog.
As a company that offers multiple cloud-based POS solutions to retailers, restaurateurs and ecommerce merchants around the globe, Lightspeed employs several different types of database platforms for a myriad of transactional, analytical and search use cases. Each of these database platforms have different strengths and weaknesses. So, when Google introduced Cloud Spanner to the market — promising features unheard-of in the space of relational databases, such as virtually unlimited horizontal scalability and 99,999% service-level agreement (SLA) — we couldn’t pass up the opportunity to get our hands on it!
To provide a comprehensive overview of our hands-on evaluation of Cloud Spanner, as well as the evaluation criteria we used, we’ll cover these main topics:
Before delving into the specifics of Cloud Spanner and its similarities and differences with other solutions on the market, let’s talk about the principal use cases we had in mind when considering where to deploy Cloud Spanner within our infrastructure:
Note: For simplicity and ease of comparison, this article compares Cloud Spanner against MySQL variants of the GCP Cloud SQL and Amazon AWS RDS solution families.
In a traditional database environment, when database query response times get close or even exceed pre-defined application thresholds (mostly due to an increase in the number of users and/or queries), there are several ways to bring response times down to acceptable levels. However, the majority of these solutions involve manual intervention.
For example, the first step one would initiate is to look at different performance related settings of the database and tweak them to best fit the applications’ use case patterns. If that proves insufficient, one may have the option to scale the database vertically or horizontally.
Vertically scaling an application entails upgrading the server instance, usually by adding more CPUs/cores, more RAM, faster storage, and so on. Adding more hardware resources translates into increased database performance, measured mostly in transactions per second and transaction latency for OLTP systems. Relational database systems (which benefit from a multithreading approach), such as MySQL, scale decently well vertically.
This approach has a few flaws, but the most obvious one is the maximum size of a server on the market. As soon as one reaches the limit of the biggest server instance, there is only one way to go: horizontal scaling.
Horizontal scaling is an approach where one adds more servers to the cluster, in order to ideally increase the performance linearly with the number of servers added. Most traditional database systems don’t scale well horizontally, or not at all. For example, MySQL can scale horizontally for read operations, by adding read slaves, but is unable to scale horizontally for write operations.
On the other hand, due to its nature, Cloud Spanner can easily scale horizontally with minimal intervention.
A fully featured RDBMS as a Service system needs to be evaluated from multiple angles. We took the most popular RDBMS on the cloud as our baseline — for Google, the GCP Cloud SQL and for Amazon, the AWS RDS. In our evaluation, we focused on the following areas:
Although Google does not explicitly claim that Cloud Spanner is designed for analytical processing, it does share some attributes with other engines, such as Apache Impala & Kudu and YugaByte, which are designed for OLAP workloads.
Even if there was a only small chance that Cloud Spanner included a consistent horizontally scalable HTAP (Hybrid transactional/analytical processing) engine with a (somewhat) usable set of OLAP features, we felt it was worth looking into.
With that in mind, we looked at the following areas:
Google Spanner is a clustered relational database management system (RDBMS) that Google uses for several of its own services. Google made it publicly available to Google Cloud Platform users in early 2017.
Here are some of Cloud Spanner’s attributes:
“Cloud Spanner chooses an index automatically only in rare circumstances. In particular, Cloud Spanner does not automatically choose a secondary index if the query requests any columns that are not stored in the index.“https://cloud.google.com/spanner/docs/secondary-indexes
Note: The Apache Tephra project adds extended transactional support to Apache HBase (also implemented within Apache Phoenix in Beta now).
Ok, so we’ve all read Google’s claims about Cloud Spanner’s advantages — almost limitless horizontal scaling while maintaining strong consistency and very high SLA. Although these claims, by any means, are an extremely hard thing to achieve, it was not the purpose of our evaluation to refute them. Instead, let’s focus on other things most database users are concerned with: feature parity & usability.
Both Google Cloud SQL and Amazon AWS RDS, the two most popular OLTP DBMS on the cloud market, have very large feature sets. However, to scale out those databases beyond the size of one node, you need to do application sharding. This approach creates additional complexity on both application and administration fronts. We looked at how Spanner fits in the scenario of unifying multiple shards into a single instance and the features (if any) that may need to be sacrificed.
Firstly, when starting with any database, one needs to create a data model. If you think that you can plug Spanner’s JDBC into your favourite SQL tool, you’ll discover that you can query your data with it but cannot use it to perform table creation or able alteration (DDL), or any insert/update/delete operations (DML). Google’s official JDBC supports neither.
“At present, the drivers do not support DML or DDL statements.”Spanner documentation
The situation isn’t any better with the GCP console, where you can submit only SELECT queries. Fortunately, there exists a community JDBC driver with the support of DML and DDL including transactions github.com/olavloite/spanner-jdbc. While this community driver is extremely valuable, the absence of Google’s own JDBC driver is surprising. Fortunately, Google is offering fairly broad support of client libraries (based on gRPC): C#, Go, Java, node.js, PHP, Python, and Ruby.
The almost mandatory use of Cloud Spanner’s custom APIs (due to the lack of DDL and DML in the JDBC) result in some limitations to related areas of the code, such as connection pools or database linking frameworks (for example, Spring MVC). Typically, while using JDBC, one has the liberty of grabbing a favourite connection pool (for example, HikariCP, DBCP, C3PO,…) that is production tested and performs well. In the case of Spanner’s custom APIs, we have to rely on connection/session pools/frameworks that we built in-house.
Primary-Key (PK) oriented design permits Cloud Spanner to be very fast when data is accessed via the PK, but also results in some query challenges.
Google’s Cloud Spanner has built-in support for secondary indexes. This is a very nice feature not always present in other technologies. Apache Kudu currently doesn’t support secondary indexes at all and Apache HBase doesn’t support indexes directly, but can add them through Apache Phoenix.
It is possible to simulate indexes in Kudu and HBase as a separate table with a different composition of primary keys, but atomicity of operations made to the parent table and linked index-tables needs to be done at the application level, and is not trivial to implement properly.
As mentioned in the Cloud Spanner overview, its indexes may have a behavior different from MySQL indexes. So, extra caution should be put on query building and profiling, to ensure appropriate index use where intended.
A very popular and useful object in a database is views. They can be used for a large number of use cases; my two favorites are as a logical abstraction layer and as a security layer. Unfortunately, Cloud Spanner does NOT support views. This is particularly limiting since there is not a column level granularity for access permissions, where views can be a viable workaround.
In the Cloud Spanner documentation, in the section that details quotas and limitations (spanner/quotas), there is one, in particular, that may be troublesome for some applications: out of the box, Cloud Spanner has a limit of a maximum of 100 databases per instance. Obviously, this can be a major setback for a database that is designed to scale out beyond 100 databases. Fortunately, after talking to our Google technical contact, this limit can be increased to almost any value via Google support.
Cloud Spanner offers fairly decent programming language support to operate with its APIs. Officially supported libraries are in C#, Go, Java, node.js, PHP, Python, and Ruby. The documentation is reasonably detailed, but similarly to other cutting-edge technologies, the community is quite small compared to the most popular database technologies, which may cause more time spent when less common use cases or issues needs to be addressed.
We didn’t find a way to create a Cloud Spanner instance in a local environment. The closest we got was a docker image of CockroachDB, which is similar in principle, but very different in practice. For example, CockroachDB can use PostgreSQL JDBC. As it is imperative for a development environment to be as close a match as possible to production, Cloud Spanner is not ideal as one needs to rely on a full Spanner instance. To save costs you can select a single region instance.
The creation of a Cloud Spanner instance is very easy. One just needs to choose between creating a single region or a multi-region instance and specify the region(s) and the number of nodes. After less than a minute, the instance is up and running.
A few rudimentary metrics are directly available on the Spanner page in the Google console. More detailed views are available via Stackdriver, where you can also set up metric thresholds and alerting policies.
MySQL offers vast and very granular user permissions/roles settings. One can easily set up access to a specific table or even just to a subset of its columns. Cloud Spanner uses Google’s Identity & Access Management (IAM) tool, which only allows the setting of policies and permissions on a very high level. The most granular option is a permission on a database level, which doesn’t fit a large chunk of production use cases. This limitation forces you to add extra security measures in your code, infrastructure or both, in order to mitigate unauthorized use of Spanner’s resources.
Simply put, backups are non-existent in Cloud Spanner. While Google’s high SLA claims may guarantee you will not lose any data, due to hardware or database failures, there’s no coming back from human error, application defects, and so on. We all know the rule: high availability doesn’t replace a sound backup strategy. For now, the only way you can back up data is to programmatically stream them out of the database to a separate storage environment.
For data loading and query testing, we used Yahoo! Cloud Serving Benchmark. The table below presents the YCSB workload B with a ratio of 95% read and 5% write.
*The load test was running on a compute engine n1-standard-32 (32 vCPUs, 120 GB memory) and the test instance was never the bottleneck in the tests.
** The maximum number of threads within a single instance of YCSB is 400. Total of six parallel instances of YCSB benchmark had to be run to get the total of 2400 threads.
Looking at the benchmark results, particularly the combination of CPU load and TPS, we can clearly see that Cloud Spanner scales quite well. More load generated by more threads is compensated by more nodes in Cloud Spanner’s cluster. While the latency looks fairly high, especially when run with 2400 threads, to get more accurate numbers it may be worth re-testing with 6 smaller compute engine instances. Each instance would each run one YCSB benchmark instead of one big CE instance with 6 benchmarks in parallel. This way it may be easier to distinguish between Cloud Spanner query latencies, and the latency added by the network connection between Cloud Spanner and the CE instance running the benchmark.
Splitting data into physically and/or logically independent segments, called partitions, is a very popular concept inherent in most OLAP engines. Partitions can greatly improve query performance and database maintainability. Delving further into partitions would be an article(s) on its own, so let’s just mention the importance of having a partitioning and sub-partitioning scheme. The ability to split data into partitions and even further into sub-partitions is key to the performance of analytical queries.
Cloud Spanner doesn’t support partitions per-se. It divides data internally into splits based on the primary key ranges. The splitting is done automatically to balance the load across the Cloud Spanner cluster. A very handy Cloud Spanner feature, is load base splitting of the parent table (the table that is not interleaved with another). Spanner automatically detects if a split contains data that is read more frequently then the data in other splits and may decide to further split it. This way more nodes can be involved in the querying and this effectively also increases bandwidth.
Cloud Spanner’s way to bulk data is the same as a normal load. To achieve maximum performance you’ll need to follow some best practices, including:
Loading data this way makes use of all of Cloud Spanner nodes.
We used the YCSB workload A to generate a 10M rows data set.
* The load test was running on a compute engine n1-standard-32 (32 vCPUs, 120 GB memory) and the test instance was never the bottleneck in the tests.
** 1-node setup is not recommended for any production load.
As mentioned above, Cloud Spanner automatically handles the splits based on their loads, so the results improved after several consecutive reruns of the test. The results presented here are the best results we obtained. Looking at the numbers above we can see how Cloud Spanner scales (well) with the increased number of nodes in the cluster. The numbers that stand out are the extremely low average latencies which are in contrast with results of mixed workloads (95% read 5% write), as described in the section above.
Scaling up and down the number of Cloud Spanner nodes is a one-click task. If you want to quickly load data, you may consider boosting the instance to the maximum (in our case it was 25 nodes in the US-EAST region) and then scale down to the number of nodes suitable for your usual load, once the data is in the database, while keeping in mind the 2TB/node limit.
We were reminded of this limit even with a much smaller database. After several runs of load tests, our database was about 155GB and when scaling down to 1 node instance we received the following error:
We managed to downscale from 25 to 2 instances, but were stuck with the 2 nodes.
Scaling up and down the number of nodes in a Cloud Spanner cluster can be automated through its REST API. This can be particularly useful to alleviate increased load on the system during busy hours.
Initially, we planned to put a significant amount of time into this part of our Spanner evaluation. After just a few SELECT COUNTs, we immediately knew that the benchmarking would be short and that Spanner is NOT an OLAP suitable engine. No matter the number of nodes in the cluster, a simple selection of a number of rows on a 10M row table took between 55 to 60 seconds. Additionally, any query that required a bigger amount of memory to store intermediate results failed with an OOM error.
SELECT COUNT(DISTINCT(field0)) FROM usertable; — (10M distinct values)-> SpoolingHashAggregateIterator ran out of memory during new row.
Some numbers for TPC-H queries can be found in Todd Lipcon’s article Nosql-kudu-spanner-slides.html, slides 42 & 43. The numbers are consistent with our own findings (unfortunately).
With the current state of Cloud Spanner features, it’s hard to think of it as an easy replacement for an existing OLTP solution, especially once your needs outgrow it. One would have to invest a significant amount of time to build a solution around Cloud Spanner’s shortcomings.
When we started the Cloud Spanner evaluation, we expected its management features to be on par with, or at least not so far from, other Google SQL solutions. But, we were surprised by the complete lack of backups and very limited resource access control. Not to mention its lack of views, no local development environment, unsupported sequences, JDBC without DML and DDL support, and so on.
So, where does this leave someone who needs to scale-out a transactional database? There doesn’t yet seem to be one solution on the market that fits all use cases. There are plenty of closed and open source solutions (a few of which are mentioned in this article), each with its strengths and weaknesses, but none of them offer SaaS with 99.999% SLA and strong consistency. If high SLA is your primary objective and you’re not inclined to build your own multi-cloud solution, Cloud Spanner may be the solution you are looking for. But, you should be aware of all of its limitations.
To be fair, Cloud Spanner was only released for general availability in the Spring of 2017, so it’s reasonable to expect that some of its current shortcomings may eventually disappear (hopefully) and when that happens, it may be a game changer. After all, Cloud Spanner is not just a side project for Google. Google uses it as the backbone for other Google products. And when Google recently replaced Megastore on Google Cloud Storage with Cloud Spanner, it allowed Google Cloud Storage to become strongly consistent for object listing on a world-wide scale (which is still not the case for Amazon’s S3).
So, there’s still hope…we hope.
Originally from Prague, Czech Republic, Ales Penkava has been a Data Architect in Montreal for almost ten years. For the past year and a half he has led Lightspeed’s Data Pipeline team. When not obsessing about big data solutions, Ales helps fledgling dancers grow at the Broadway Academy school that he co-founded with his wife.
Originally published on the Lightspeed HQ Blog.
Founded in 2005, Lightspeed builds tech that helps independent retailers, restaurateurs, and eCommerce merchants start, market, manage, and grow their business.
See all (51)
950 
4
950 claps
950 
4
Founded in 2005, Lightspeed builds tech that helps independent retailers, restaurateurs, and eCommerce merchants start, market, manage, and grow their business.
About
Write
Help
Legal
Get the Medium app
"
https://blog.expo.dev/how-to-build-cloud-powered-mobile-apps-with-expo-aws-amplify-2fddc898f9a2?source=search_post---------139,"AWS Amplify is a CLI & toolchain for the client that allows developers to quickly create & connect to AWS cloud services directly from the front-end environment. Amplify lowers the barrier to entry for developers looking to build full-stack applications by giving them an easy way to create & connect to managed cloud services.
"
https://medium.com/@jryancanty/stop-downloading-google-cloud-service-account-keys-1811d44a97d9?source=search_post---------140,"Sign in
There are currently no responses for this story.
Be the first to respond.
Ryan Canty
Jul 27, 2020·6 min read
TL;DR: Generating and distributing service account keys poses severe security risks to your organization. They are long-lived credentials that are not automatically rotated. These keys can be leaked accidentally or maliciously allowing attackers to gain access to your sensitive GCP resources. Additionally, when used actions cannot be attributable back to a human. You don’t actually have to download these long-lived keys. There’s a better way!
For some background, almost every change you want to make in Google Cloud from creating a GKE cluster to reading from a GCS bucket is handled using an API. This API is authenticated using the OAuth2 protocol, which basically means there’s a short lived (1 hour default) access token attached to every authenticated request. If you’re familiar with the whole “Sign in with Google” popup, that’s OAuth2 hard at work authenticating you with your Google credentials. Once you’re authenticated, an access token is attached to all your API requests whether you’re using gcloud, terraform, SDKs, or the console. In Google Cloud, we use a lot of automation and web services which similarly need those tokens, but robots aren’t very good at opening browsers and typing in passwords so they need some sort of verifiable identity. Enter Service Accounts.
Service accounts allow automated users to prove their identity using a public/private key pair in the form of a JSON file. A service account also has the same ability as users or groups to bind to IAM roles to do things in GCP. To make an API request, a service account will sign a JWT token with its private key and the Google authentication system will verify that signature with the public key, granting an access token. This basic (and oversimplified) concept is important for later parts of this post. If you’d like to read more about this flow, check out RFC 7523.
You should never need to generate and download a service account key to use a service account within Google Cloud infrastructure.
Service accounts are very easy to use within Google Cloud. Most, if not all, compute resources (i.e. GCE instances, GKE Pods, Cloud Functions, etc.) support the ability to attach a service account. This allows these resources to act as the service account, call Google SDKs and APIs within the bounds of permissions granted to the service account. You should never need to generate and download a service account key to use a service account within Google Cloud infrastructure. A risk emerges when developers think they need a service account to accomplish a task, so they generate and download a key.
I cannot tell you how often I see documentation or tutorials instruct folks to download these service account keys and use them indefinitely or worse, store them in their source code working directory. Doing this, you’re literally one line in a .gitignore from committing this highly sensitive secret to Github and getting breached.
Remember how I said that if you have the Service Account’s private key, you can sign a JWT token and be granted an API access token? Well there is a way to do that without ever needing to download the key.
Let’s say I have a service account that is used for GKE so it has the role roles/container.developer. We’ll call this service account k8s@project.iam.gserviceaccount.com. Let’s further say that my user ryan@example.com isn’t allowed to download the key to this service account and doesn’t have direct permissions to mess with GKE but what I do have is the magic role roles/iam.serviceAccountTokenCreator. Now all I have to do to setup my GKE credentials with the gcloud command is:
This is great because it allows this command to use a service account without actually having the key! Not only that but you always know you are impersonating because a warning message pops up letting you know.
But if you’re running multiple commands with the same service account, this can be annoying to type over and over. Instead, let’s set this with gcloud config.
This is much better! Now we can impersonate a user for multiple commands without having to constantly add that --impersonate-service-account flag. But what if, like me, you constantly switch back and forth between service accounts for different projects. We can write a very simple bash script to simplify typing and remembering service accounts that you use frequently.
Now you could execute this with the following before using gcloud:
You could make this more robust by reading from a config file if you like, but I think a single-file script gets the point across. Now for every new service account you want to use, simply add it and a short name for it to this script and you’re off.
We’ve spent all this time talking about a better way to consume service accounts through gcloud but what about the very common use case of Terraform? For development purposes, we need to test our infrastructure as code somehow. Thankfully, the google terraform provider supports directly passing an OAuth2 token as an environment variable. All you have to do to get this token and tell Terraform about it is this:
You could further simplify this by wrapping it in a Makefile such as this (special thanks to emalloy for putting this together):
Now you have a token that will only live in your environment for 1 hour and be useless to an attacker after that!
The SecOps folks may be thinking, how do I attribute and audit actions taken by a user impersonating a service account? In Cloud Logging, every API call executed by a service account that has been impersonated has the following structure within it:
You can also get a higher level of detail if you enable Data Access logs. The below Cloud Logging filter will include every API call that ryan@example.com made while impersonating. I’m sure you can find other clever filters as well.
I’ll also point out that this level of attribution is impossible if you allow users to download service account keys since they are effectively using shared credentials, assuming more than one person has access to download the same key. To illustrate, this is the only info you get in the logs if I could download the k8s service account key and used it.
If I’m a forensic analyst or an auditor, there’s no way I could figure out definitively what human executed this API request unless each user has their own service account or key and that’s defined somewhere. Even still, that is very difficult to trace back.
There are some use cases where downloading a service account key to your workstation is necessary, but they are not the norm. Keeping secrets like this short-lived locally should be the goal, the same way we should enable MFA and not use hunter2 as our password. For the obvious cases where service account keys must be downloaded to use GCP resources from your datacenter or another cloud, I’d recommend taking a look at HashiCorp Vault which has a plugin to checkout short-lived service account keys.
So now that you know, please stop downloading service account keys! :)
Cloud Security Engineer at Google Cloud http://github.com/onetwopunch
646 
11
646 
646 
11
Cloud Security Engineer at Google Cloud http://github.com/onetwopunch
"
https://blog.chain.com/wall-street-meet-block-chain-b2747909eb90?source=search_post---------141,"Since Chain.com is at the center of many of these discussions, I wanted to share our thoughts on why people are excited about digital assets, what the benefits of using the block chain are, and how it relates back to bitcoin.
It’s become widely known that, thanks to the bitcoin protocol, we can now send money over the Internet directly between two entities. The mechanism which enables this is the block chain, a shared ledger of linked addresses which correspond to privately controlled keys. To send money, a user takes her key and signs a transaction, directing funds from addresses she controls to a recipient’s address. No intermediaries — just the open network.
But here’s the big idea: using open source protocols which extend bitcoin, we can send any type of value the same way: stocks, reward points, prepaid minutes, stored value cards, dollars, etc.
Financial services companies have a lot to gain. By transferring assets digitally over Internet-based networks, they can avoid using costly clearing houses or doing complex integrations across entities. They’ll also be able to build more innovative products.
And that’s good for everyone else. When trusted institutions issue digital assets on the open Internet, financial services will be less expensive, more secure, work better together, and be accessible to more people around the world.
Here’s how it works:
After all, the world already has “digital assets” — the assets just happen to be entries in databases at various financial institutions, processors, and others. How is the block chain any better?
Here’s the question we like to ask instead: Under what conditions is the block chain the best solution for managing digital assets in a market?
Based on our learning from projects with payments and financial services companies, we’ve found the block chain is ideal in markets with the following characteristics:
In markets like these, moving assets with a database requires either:
The first approach is common when the entities perform similar functions and are similar in size (see: Securities, Telecom, Energy, Banks). But clearing houses can be expensive and inefficient because the third party intermediary tends to become a monopoly and act like it: high fees, slow to innovate, hard to replace. On the plus side, it’s easier to add new entrants, and it pushes a lot of complexity out of the entities.
The second approach, integrating systems directly, is more common when there are entities at different levels in a market, for example, a card network and a merchant, an issuer and a distributor, a business and its customers. It lets the parties involved be in control of their own destiny. But the problem with system integrations, even if they are API-based, is that they’re expensive to build, hard to change once operating at scale, and difficult to integrate new partners into. It also tends to multiply points of failure and create more opportunities for fraud.
What we ideally want is a system which moves the complexity of transferring, settling, and recording assets out of the entities — like a clearing house — but allows for direct transfer of assets between entities — like an integration. And unlike both of these systems, we want a solution that is flexible, transparent, extensible, and inexpensive.
The block chain offers the following unique benefits for asset transfer:
What this sums up to, in essence, is a financial cloud. We call it the financial cloud because it de-couples financial infrastructure from the products and services that run on it.
This enables innovation, since the product specs can dictate the implementation, as opposed to the current system where the particulars and history of the infrastructure dictate a great deal of the end-user experience. (Dissatisfaction with financial products is one reason fintech start-ups are “unbundling” the banks.)
Unlike traditional cloud computing, private data never leaves the company’s system thanks to the cryptographic model of the block chain.
Below are a few illustrative examples of assets that could soon be managed on the block chain.
As you’ll see, this isn’t about asking people to embrace bitcoin. Far from it, it’s about solving problems for consumers, businesses, and the institutions who serve them.
Here are the steps for building a block chain based service:
Chain.com helps our customers do all these things. After all, the above components are all about execution, and none is a competitive differentiator — that comes back to your specific idea or product, brand, customer orientation, and the technology platforms inside your company.
Specifically, Chain.com serves as a transaction co-signer to strengthen security, a data provider to ensure privacy and availability, and as a design partner to help companies think through the ins and outs of a particular implementation. We also operate teams of on-call engineers to ensure the reliability of all these systems.
We love helping institutions large and small, across different industries and countries, to move more quickly, operate more securely, and build truly innovative products on the emerging financial cloud.
Feel free to get in touch.
You can follow me and Chain on Twitter.
Ledger as a Service. Launch your blockchain in minutes not months with Sequence by Chain.
266 
4
Thanks to Ryan Smith, devon gundry, Tess Rinearson, Eric Wiesen, and Jeff Lee. 
266 claps
266 
4
Written by
CEO, Interstellar
Ledger as a Service. Launch your blockchain in minutes not months with Sequence by Chain.
Written by
CEO, Interstellar
Ledger as a Service. Launch your blockchain in minutes not months with Sequence by Chain.
"
https://medium.com/google-developers/firebase-google-cloud-whats-different-with-cloud-storage-a33fad7c2b80?source=search_post---------142,"There are currently no responses for this story.
Be the first to respond.
In my previous posts about the relationship between Firebase and Google Cloud Platform (GCP), I talked about what’s different between the two for both Cloud Functions and Cloud Firestore. In this post, the topic will be Google Cloud Storage (Firebase, GCP), a massively scalable and durable object storage system. In this context, an “object” is typically a file — a sequence of bytes with a name and some metadata.
It’s tempting to think of Cloud Storage like a filesystem. In many ways it’s similar, except for one important fact: there are actually no directories or folders! Here’s how it really works. The top level of organization is called a bucket, which is a container for objects. Each bucket is effectively a big namespace full of objects, each with unique names within that space. Object names can look and feel like they have a directory structure to them (for example, /users/lisa/photo.jpg), but there are no directories or folders behind the scenes. These path-like names are helpful for organization and browsing in both the Cloud and Firebase consoles. Sometimes, we just use the word “folder” to make it easy to describe these paths.
Here’s what the Cloud console looks like when you’re browsing the contents of a storage bucket:
And here’s what the Firebase console looks like for the same bucket:
You’ll notice that the Cloud console puts a lot more data and functionality on the screen than the Firebase console. The Cloud console exposes the full power of Cloud Storage, while the Firebase console exposes only those features that are likely to be important to Firebase developers. That’s because the use cases for Cloud Storage tend to be a little different, depending on your perspective.
Cloud developers are often also enterprise developers. And, as you might guess, enterprise developers have broad requirements for object storage. They’re using Cloud Storage for backups and archives, regional storage, hosting static content, controlling access to objects, versioning, loading and saving data in BigQuery, and even querying data directly. Cloud Storage is very flexible! A majority of the access to an object comes from backend systems, often other Cloud products and APIs, and the SDKs. But for Firebase developers, the use cases are typically more narrow.
The most common use case for using Cloud Storage in a mobile or web app is handling user-generated content. A developer might want to enable users to store images and videos captured on their devices, then let the users view the files on other devices or share them with friends. Firebase provides SDKs (Android, iOS, web, Unity, C++) for uploading and downloading objects in storage, directly from the app, bypassing the need for any other backend components. To help with privacy and prevent abuse, Firebase also provides security rules, paired with Firebase Authentication, to make sure authenticated users can only read and write the data to which they’ve been given permission. Collectively, these SDKs and security rules are referred to as “Cloud Storage for Firebase”.
When you create a Firebase project, or add Firebase to an existing GCP project, a new storage bucket is automatically created for that project. This helps reduce the amount of configuration required to get started with Cloud Storage in a mobile app. Since all Cloud Storage bucket names across the entire system must be unique, this new bucket is named “[YOUR-PROJECT-ID].appspot.com”, where YOUR-PROJECT-ID is the globally unique ID of your project. You normally don’t even need to know this bucket name, as it’s baked into your Firebase app configuration file. When Firebase gets initialized, it’ll automatically know which bucket to use by default. In fact, this bucket is usually referred to as the “default storage bucket” for Firebase.
All that said, Firebase developers are not limited to using only Firebase SDKs, and Cloud developers can opt into using Firebase to read and write existing buckets. If you started out with Firebase, you can use any of the Cloud-provided tools, SDKs, and configurations at any time. And if you have existing data in Cloud Storage to use in a mobile app, you can certainly add Firebase to your project to make that happen.
Everyone who wants to access a Cloud Storage bucket from the command line can use gsutil, provided their Google account is authorized to do so. gsutil provides commands to upload and download files locally, and to configure a storage bucket. It’s often more convenient to use this tool rather than either of the web consoles, especially for bulk operations.
There’s one interesting difference between Firebase and Cloud. As you’ve just read, they both provide access control to objects in storage buckets, but those rules are mutually exclusive from each other. You use Cloud IAM to control access to an object only from backend systems and SDKs, but you use Firebase security rules to control access only from mobile applications using the Firebase client SDKs. These access control mechanisms don’t overlap or interfere with each other in any way. Note that the Firebase Admin SDK is actually a server-side SDK, which can also be used to access Cloud Storage. In fact, the API it provides is actually just a wrapper around the Cloud SDKs, which are controlled by IAM.
How are you using Cloud Storage? I’ve used it in a couple experimental projects. There’s a connected video doorbell that stores images of guests who ring the doorbell and a “universal translator” that translates speech recorded on a mobile device. Check those out, and let me know what you’ve built as well!
Engineering and technology articles for developers, written…
932 
7
932 claps
932 
7
Engineering and technology articles for developers, written and curated by Googlers. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
firebase-consultant.com, Firebase GDE, engineer, developer advocate, Xoogler
Engineering and technology articles for developers, written and curated by Googlers. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/how-to-do-time-series-prediction-using-rnns-and-tensorflow-and-cloud-ml-engine-2ad2eeb189e8?source=search_post---------143,"There are currently no responses for this story.
Be the first to respond.
The Estimators API in tf.contrib.learn is a very convenient way to get started using TensorFlow. The really cool thing from my perspective about the Estimators API is that using it is a very easy way to create distributed TensorFlow models. Many of the TensorFlow samples that you see floating around on the internets are not distributed — they assume that you will be running the code on a single machine. People start with such code and then are immeasurably saddened to learn that the low-level TensorFlow code doesn’t actually work on their complete dataset. They then have to do lots of work to add distributed training code around the original sample, and who wants to edit somebody else’s code?
Note: Estimators have now moved into core Tensorflow. Updated code that uses tf.estimator instead of tf.contrib.learn.estimator is now on GitHub — use the updated code as a starting point.
So, please, please, please, if you see a TensorFlow sample that doesn’t use the Estimators API, ignore it. It will be a lot of work to make it work on your production (read: large) datasets — there will be monitors, coordinators, parameter servers, and all kinds of systems programming craziness that you don’t want to have to dive into. Start with the Estimator API and use the Experiment class. (Disclaimer: my views, not that of my employer).
The Estimators API comes with a Deep Neural Network classifier and regressor. If you have typical structured data, follow the tutorial linked above or take this training course from Google Cloud (soon to be available on Coursera) and you’ll be on your way to creating machine learning models that work on real-world, large datasets in your relational data warehouse. But what if you don’t have a typical structured data problem? In that case, you will often need to create a custom estimator. In this blog post, I will show you how.
A common type of data that you will want to do machine learning on is time-series data. Essentially, your inputs are a set of numbers and you want to predict the next number in that sequence. In this article, I will make it a bit more general and assume that you want to predict the last two numbers of the sequence. As the Computer Science proverb goes, if you can do two, you can do N.
The traditional neural network architecture that is used for sequence-to-sequence prediction is called a Recurrent Neural Network (RNN). See this article and this one for a very accessible introduction to RNNs. But you don’t need to know how to implement a RNN to use one, so once those articles go deeper than you want, quit.
To follow along with this article, have my Jupyter notebook open in another browser window. I am only showing key snippets of code here. The notebook (and the GitHub folder) contains all of the code.
It’s usually easier to learn with a small toy dataset that you can generate as much as you want of. Real data will come with its own quirks! So, let’s generate a bunch of time-series data. Each sequence will consist of 10 numbers. We will use the first eight as inputs and the last two as the labels (i.e., what is to be predicted):
The code to generate these time-series sequences using numpy (np):
Write a bunch of these time-series sequences to CSV files (train.csv and valid.csv) and we are in business. We’ve got data.
The way the Estimators API in TensorFlow works is that you need to provide an input_fn to read your data. You don’t provide x and y values. Instead, you provide a function that returns inputs and labels. The inputs is a dictionary of all your inputs (name-of-input to tensor) and the labels is a tensor.
In our case, our CSV file simply consists of 10 floating point numbers. The DEFAULTS serves to specify the data type for the tensors. We want to read the data 20 lines at a time; that’s the BATCH_SIZE. A batch is the number of samples over which gradient descent is performed. You will need to experiment with this number — if it is too large, your training will be slow and if it is too small, your training will bounce around won’t converge. Since we have only input, the name you give that input doesn’t really matter. We’ll call it rawdata.
The input_fn that the Estimators API wants should take no parameters. However, we do want to be able to provide the filename(s) to read on the command line. So, Let’s write a read_dataset() function that returns an input_fn.
The first thing that we do is to decide the number of epochs. This is how many times we need to go through the dataset. We’ll go through the dataset 100 times if we are training, but only once if we are evaluating.
Next, we’ll do wild-card expansion. Lots of times, Big Data programs produce sharded files such as train.csv-0001-of-0036 and so, we’d like to simply provide train.csv* as the input. We use this to populate a filename queue and then use a TextLineReader to read the data:
After this, we decode the data, treating the first 8 numbers as inputs and the last two as the label. The inputs, when we read it, is a list of 8 tensors each of which is batchsize x 1. Using tf.concat makes it a single 8xbatchsize tensor. This is important because the Estimators API wants tensors not lists.
If we we were using a LinearRegressor, DNNRegressor, DNNLinearCombinedRegressor, etc., we could have simply used the existing class. But because we are doing sequence-to-sequence prediction, we have to write our own model function. At least right now, the Estimators API doesn’t come with an out-of-the-box RNNRegressor. So, let’s roll out our own RNN model using low-level TensorFlow functions.
Recall that we had to package up the inputs into a single tensor to pass it as the features out of the input_fn. Step 0 simply reverses that process and gets back the list of tensors.
A Recurrent Neural Network consists of a BasicLSTMLCell to which you pass in the input. You get back outputs and states. Slice it to keep only the last cell of the RNN — we are not using any of the previous states. Other architectures are possible. For example, I could have trained the network to have only one output always and used rolling windows. I’ll talk about how to modify my example to do that at the end of this article.
The comments in the code above are pretty self-explanatory regarding the other steps. We are not doing anything surprising there. This is a regression problem, so I’m using RMSE.
The Experiment class is the smart one in the Estimators API. It knows how to take the model function, input functions for training and validation and do reasonable things regarding distribution, early stopping, etc. So, let’s hand off our pieces to it:
The code above works on a single machine, and if you package it up into a Python module, you can also submit it to Cloud ML Engine to have it trained in a serverless way:
In this article, I assumed that you have thousands of short (10-element) sequences. What if you have a very long sequence? For example, you might have the price of a stock or the temperature reading from a sensor. In such cases, what you could do is to break up your long sequence into rolling sequences of fixed length. This length is obviously arbitrary, but think of it as the “look-back” interval of the RNN. Here is TensorFlow code that will take a long sequence and break into smaller, overlapping sequences of a fixed length:
For example:
will result in:
Once you have these fixed length sequences, everything is the same as before.
Happy coding!
Google Cloud community articles and blogs
804 
17
804 claps
804 
17
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Data Analytics & AI @ Google Cloud
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/@juankb01/cloud-mining-a-year-in-review-65f507f2c835?source=search_post---------144,"Sign in
There are currently no responses for this story.
Be the first to respond.
Juan Bernal
Jan 10, 2019·8 min read
At the height of the late 2017 crypto bull run, I began researching ways of growing my stack of crypto. After losing almost a whole ETH due to exchange fees and realizing I wasn’t going to become the next WolfOfPoloniex anytime soon, I opted to looking into mining as a viable alternative.
Because of my limited capital, however, I soon turned to cloud mining, and even though I found an overwhelming amount of people saying all cloud mining providers were most definitely scams, I decided to take the plunge and find a yearly subscription to mine any one of the coins I saw a strong future for.
After doing a lot of research, I paid Hashflare $220 for a yearly ETH cloud mining contract with 10 MH/s of hashrate on January 4, 2018.¹ For the last year I have tracked its daily performance to see if I had truly been duped (as everyone claimed I would), or if cloud mining could be a viable source of crypto income. The entire year’s worth of payments (copied straight from my Hashflare dashboard) can be found here: Hashflare Mining Profits
This writeup is just a review of those last 365 days, including a brief analysis of payout trends and other stray statistics I calculated while trying to figure out how my investment had done. I hope it provides an unbiased firsthand experience so that others can make an informed decision if they believe cloud mining is right for them. Without further ado, here is what I found:
I “received” my daily ETH payout every day without fail. I even went as far as as sending the first 0.1 ETH mined to my Ledger to make sure it was indeed real ETH and I would at least have some of it in my possession if they decided to run, but I continued to receive payments for the entire term of the contract with no problem.
That being said, Hashflare still has ~0.16 of my mined ETH in their possession and I’m currently waiting for KYC verification so I can withdraw it in one transaction because I’m not sure if they still have a 0.1006 ETH withdrawal minimum and I am currently capped at withdrawing 0.1 ETH daily. As long as the verification goes through and I can get all the mined ETH into my wallet I can wholeheartedly say it wasn’t a scam, but I won’t count my chickens before they hatch.
However, Hashflare did stop mining bitcoin in mid-2018 and it revoked all BTC mining contracts due to “decreasing profitability” because of last year’s price crash. Even though the terms of the contract 100% gave them that option, I guess some people could argue that this is “scam-y”, and the fact that they implemented stricter withdrawal restrictions afterwards does leave a bad taste in my mouth. I’m willing to give them the benefit of the doubt because I can’t prove this move was purely malicious or if they faced regulator pressure to do so, but it does seem fairly unlikely that KYC would kick in coincidentally at around that time… However, because this article is about my personal experience, I’m staying in the not-a-scam side unless I positively cannot withdraw my ETH from their site.
UPDATE 1/23: After waiting almost 3 weeks and emailing support once, my KYC verification was finally accepted by Hashflare. I successfully withdrew the remaining ~0.16 ETH to my Ledger as of this morning (etherscan) so I can finally say with 100% confidence that cloud mining through Hashflare was not a scam.
2. Q: Second of all, is it profitable? A: Yes and no, it depends.
I hate giving such a “traditional” finance answer, but it really is true in this case. My investment was NOT profitable in USD terms due to the massive ETH price crash of last year. I mined a total of 0.26880820 ETH for $220, meaning I basically “bought” my ETH at a price of $818.43. With Ethereum’s current price hovering at around $150, that 0.26880820 ETH is only worth ~$40 today, meaning I lost 81.27% of my $220. As a simple comparison, if I had bought $220 worth of ETH on January 4 (which I will call “Equivalent ETH”) instead of paying Hashflare for my contract, my USD loss would have actually been slightly higher than that: around 83% or ~$185.²
However, using this idea of Equivalent ETH to calculate my total Ethereum profit, my investment actually returned 13% ETH, which is pretty decent for an asset you can just buy and forget for a year.³ Because my goal in crypto since day one has always been to grow my stack of coins regardless of my performance in fiat terms, I’m happy with this outcome… but if you are betting strictly on growing your dollar amount as quickly as possible you probably want to look far far away from cloud mining.
For a little bit of perspective, since I was looking for a brainless way to make a steady return year-round, I could have looked to invest in a traditional index fund. Crypto falls broadly into the technology space, so buying $220 worth of $QQQ (an ETF that tracks the NASDAQ 100) could have been a somewhat similar move. This $QQQ investment would currently be worth basically the same $220 meaning I would have avoided had a huge USD loss, but if that had been the case I would not have grown my ETH stack past what it was before.
3. Q: Ok, so it’s not a scam and it is actually profitable but you could have made WAY more doing X/Y/Z. A: Sure! However that wasn’t my goal.
That’s not really a question but it is still a valid point. Could I have turned the equivalent ETH into a whole lot more by aggressively trading it around the clock or by picking some cheap microcap crypto and riding it to the moon? Maybe. Could I have turned those $220 into way more USD by trading penny stocks all year or buying the Powerball or putting it all on black at a nearby casino? Potentially. However that wasn’t my goal.
I ventured into cloud mining because I wanted a steady source of crypto that wouldn’t cost an arm and a leg (I’m looking at you antminer and other hardware miners) and I actually got just that. I didn’t want to be glued to my computer 24/7 to see how my investment was doing, and I’m not the best or smartest trader out there so I guarantee you I would not have made 13% in last year’s crypto markets and still be here to talk about it. Instead, by opting for cloud mining I slowly but steadily earned ETH year-round, and my contract actually helped me survive last year’s massive bear market with a little bit more crypto than I started with.
4. Q: So, are you going to buy another Hashflare contract now? A: Ehh…
There seems to have been a whole lot of luck involved in how things panned out for the duration of my contract. From the simple fact that I decided to buy an ETH contract instead of putting my money into a BTC one that would have been revoked, things lined up in a weird way for my benefit and I’m not sure if things are going to be the same this year.
For example, just a few days after I bought the contract I learned about Ethereum Casper, the hybrid PoW/PoS consensus protocol that was supposed to be launched sometime in 2018. If it had been implemented on time, I’m almost certain it would have made cloud mining obsolete and Hashflare would have cancelled my contract, but delays in its launch made sure traditional PoW mining continued into this year and my contract was able to finish unharmed. I’m not sure if Casper will ever be released or if other major changes to the Ethereum network are going to affect the viability of cloud mining, but I wouldn’t want to be on the receiving end of that fallout, and since the ETH platform is trying to revitalize itself after losing a lot of steam last year I’m unsure of what to expect.
Furthermore, in a weird way I got “lucky” with last year’s bear market. If we had seen a bull market, my contract would have been wildly successful in USD terms but pretty disappointing in ETH terms.⁴ Because it is fairly easy for even the most brainless trader to make money in bull markets, I could have probably made more than a 13% return on $220 worth of ETH just trading on momentum, and because I will ALWAYS want to maximize my amount of crypto, being stuck in a year-long contract with $220 less to invest in that kind of market would not have been ideal. While I’m the last person that can tell you if we’re going to see a 2017 level bull run anytime soon, a period of accumulation due to dirt cheap crypto is the first step towards positive price movements and with most major projects being down 80% or more last year I believe it’s just a matter of when more than why.
With all of that being said, while I may not be interested in commiting to a yearly cloud mining contract with Hashflare, I’m definitely looking into Nicehash or other short-term providers to mine ETH or XMR this year, but I still have to do a lot of research before figuring out if I’m going to go down that road or if I’m just going to take advantage of the low prices and buy crypto directly.
Some stray observations I made in the past year
Footnotes
22. Future lawyer trying to find my place in the world. Passionate about: Technology | Entrepreneurship | Cooking | Unique Experiences | and Much More
637 
5
637 
637 
5
22. Future lawyer trying to find my place in the world. Passionate about: Technology | Entrepreneurship | Cooking | Unique Experiences | and Much More
"
https://itnext.io/how-to-use-netflixs-eureka-and-spring-cloud-for-service-registry-8b43c8acdf4e?source=search_post---------145,"One of the main tenets of the microservice architecture pattern is that a set of loosely-coupled, collaborating services works together to form a cohesive, whole application. In this way, it’s not so much about the individual services, it’s more about ensuring the interactions between the services remains reliable and failure-tolerant.
What this means in practice, is removing hard-coded information and replacing them with dynamically updating environment variables, having separate databases for each service, removing as much dependency between services as possible (so if necessary, a service that needs more instances can easily scale without impacting its counterparts), and generally decreasing the complexity of managing the whole by increasing the complexity of each individual service.
It sounds like a good strategy, but keeping track of all the smaller pieces of the whole makes it a burden on the client to manage all of this.
Which brings me to the topic of my blog today: the Netflix Eureka service registry — and how to use it with Spring Cloud to more effectively manage this complexity.
As the Github repo for Netflix’s Eureka registry says itself:
“Eureka is a REST (Representational State Transfer) based service that is primarily used in the AWS cloud for locating services for the purpose of load balancing and failover of middle-tier servers.” — Netflix: Eureka at a Glance, Github
Sounds good? Good. Without further ado, let’s get to setting up this Eureka service registry and a couple of services to see it in practice.
Setting up a Spring-based Eureka server is actually, very simple. Spring.io, itself, has a great walkthrough here, that helped get me up and running quickly with both a Eureka server and a sample Spring Boot project.
I won’t go through the step-by-step, as you can do that with the tutorial I linked to, or you can download my working example here. But I will highlight the important things to include for the server setup.
Build.gradle File
Inside of your build.gradle file, you must include the dependencies above: the spring-cloud-starter-netflix-eureka-server and the spring-cloud-dependencies. That’s done, and next up is the application.yml file in the resources folder.
Application.yml File
This file doesn’t need much, either. It needs a server port specified so the service doesn’t automatically start on port 8080, which would then conflict with our other Spring Boot client services, when running locally.
A couple more configurations are specified, as well, for convenience. The Eureka client is instructed not to register itself upon start up (eureka.client.register-with-eureka: false), and it is told not to search for other registry nodes to connect to, as there are none (at least not while running locally )(eureka.client.fetch-registry: false). A default URL is listed and the verbose logging for Eureka and any subsequent discovery of services is turned off.
Right, now there’s just one final step on the main class path.
EurekaServerApp.java File
The only extra the main file needs is the @EnableEurekaServer annotation, which tells the Spring Boot service to enable the server. Easy.
Ok, we can move on to the services now — this is where things start to get more interesting.
Once again, the Spring.io, walkthrough here, does a very good job of setting up the first Spring Boot server and client project, so I’ll just hit the highlights again of what you must include.
Build.gradle File
For the client service’s build.gradle file, just be sure to include the spring-cloud-starter-netflix-eureka-client, and the spring-cloud-dependencies. The spring-boot-starter-web dependencies are included so a web endpoint can be created in the service that will show us what the service’s information relative to the Eureka server looks like.
Bootstrap.yml & Application.yml Files
Once again, some minor configurations are needed for the boostrap.yml and application.yml files. The bootstrap.yml is pictured above, it is picked up before the application.yml file by Spring, so this is where we set the service’s name. I chose the very original spring.application.name: a-java-service, for this project. That’s how the Eureka server will reference it going forward.
And then the application.yml file has configs similar to the Eureka server setup. A port number and the default URL for the Eureka server.
EurekaClientApplication.java File
Finally, the service is annotated in the main class path file with @EnableDiscoveryClient, which tells the Spring Boot service to activate the Netflix Eureka DiscoveryClient implementation and register its own host and port with the Eureka server.
The REST endpoint defined below on the REST Controller can be used to see all the service instances registered in the Eureka registry at the http://localhost:8091/service-instances/a-java-service URL.
The JSON looks like this:
Now, that was a good start, but to be more real-world, I decided I wanted to also register a Node.js service to the Eureka server as well, and that proved a little more challenging.
Luckily, there’s a handy little NPM package out there for just such needs, called eureka-js-client, which is billed as a
JavaScript implementation of a client for Eureka (https://github.com/Netflix/eureka), the Netflix OSS service registry.
The documentation must be read through to the end because there’s some special gotchas when using the Spring implementation with Eureka, but I managed to get it working with some trial and error. Here’s what you need to get a sample Node.js project up and running.
Package.json File
For this Node.js service, we just need to install the eureka-js-client and I added express and nodemon so I could easily make some REST calls to the application, as well as have the Node server automatically reload as I made tweaks to the system.
Server.js File
I used just one server.js file for convenience and because this project is so small to begin with.
What’s important is the new Eureka() set up. Whereas with the Spring Boot projects, we set the application configurations in the bootstrap.yml and application.yml, for the JavaScript project, this config is set in the server.js file or injected with a config file (for larger projects or projects with multiple different configurations based on production lifecycle).
In here is where the application name is defined: app: 'a-node-service', the host name, IP address, port, data center info, etc. are defined. Every parameter up to the registerWithEureka and fetchRegistry options are required or else the server will crash, but the vipAddress and dataCenterInfo fields can be filed exactly as they’re written in the eureka-client-js documentation. They just have to be filled out.
Last, but not least, once the client service has been set up, it is started with the client.start() command, where I am console logging either the error message if it fails to start or the ‘Node.js Eureka Started’ message if connecting to the Eureka registry is successful.
Let’s get this service registry up and running now. I’ve got full details to run all the service in the README.md of my example project here, but here it is as well.
We’ll cd into each service contained within the one master project folder, if you have a file structure similar to the one prescribed by the Sping.io starter tutorial. Here’s what mine looks like for reference:
As I said, cd into each repo eureka-client-java, eureka-client-node and eureka-service, and for the two Spring Boot projects run gradle clean build and then gradle bootRun. For the Node.js project, run npm start. Give it a minute for all the projects to spin up and be found by the registry service, then go to http://localhost:8761 and this is what you should see.
This is the Eureka server homepage, and if you look in the second section down the page, you’ll see ‘Instance currently registered with Eureka’, and the two service we built registered with it.
Sweet, it worked. To verify these services are really running, you can go to http://localhost:8091/service-instances/a-java/service and http://localhost:3000/ to see info from each service.
Now, let’s take it a step further, because the whole point of having a service registry is to make it easier for clients registered there to ask questions about how to connect and communicate with the other available services:
“Each service registers itself with the service registry and tells the registry where it lives (host, port, node name) and perhaps other service-specific metadata — things that other services can use to make informed decisions about it. Clients can ask questions about the service topology (“are there any ‘fulfillment-services’ available, and if so, where?”) and service capabilities (“can you handle X, Y, and Z?”).” — Spring, Microservice Registration and Discovery with Spring Cloud and Netflix’s Eureka
The next step I wanted to take was making it so I could get the service instance JSON data supplied by the Java service available through an endpoint from the Node.js service — which it can only get by communicating with the info supplied by the Eureka service registry.
Here’s what to do.
Server.js File
Right before the client.start() command in the server.js file, I defined a variable called javaInstance, and then used the method provided by the eureka-client-js plugin getInstancesByAppId() calling the Spring Boot service registered as a-java-service to get its service information.
With this, I could reconstruct the URL information supplied by the Spring Boot REST endpoint to get that service’s information. In short, I was able to parse out all the necessary info from the Java instance supplied by the Eureka server to reach it through the Node.js endpoint.
Here’s what the URL comes out to be once the host name, port number and app name are acquired: http://localhost:3000/serviceInfo/192.168.1.18:8091/service-instances/A-JAVA-SERVICE .
And here’s what you’d see if both the Node.js and Java services are running and registered with the Eureka server:
Voila. The above JSON (not well formatted) is for the a-java-service, accessed through an endpoint on the Node.js service. Just be sure to make that URL call to the Spring Boot service inside of the client.start() command, otherwise you won’t be able to access all the instance information supplied from the getInstancesByAppId() call to the Eureka server.
At this point, Eureka is doing what it’s designed to do: it registers services and provides information from one to the other, minus the hardcoded complexity developers used to have to be responsible for remembering and implementing manually.
As I said at the beginning, building applications according to the microservice architecture pattern means more complexity managing the individual services, but greater fault tolerance and reliability of the application as a whole.
Service registries like Netflix’s Eureka server help manage this additional complexity, and when coupled with Spring’s Cloud technology it becomes infinitely easier to use. The next step after this would be using something like Zuul to assist with dynamic routing, load balancing between instances, handling security, and more. But that’s for another blog post.
Thanks for reading, I hope this proves helpful and gives you a better understanding of how to use Netflix’s Eureka server with the help of Spring Cloud, and discover Java and Node.js services. Claps and shares are very much appreciated!
If you enjoyed reading this, you may also enjoy some of my other blogs:
References and Further Resources:
ITNEXT is a platform for IT developers & software engineers…
598 
3
598 claps
598 
3
Written by
Staff Software Engineer, previously a digital marketer. Frontend dev is my focus, but always up for learning new things. Say hi: www.paigeniedringhaus.com
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
Written by
Staff Software Engineer, previously a digital marketer. Frontend dev is my focus, but always up for learning new things. Say hi: www.paigeniedringhaus.com
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
"
https://medium.com/google-cloud/cloud-iot-step-by-step-connecting-raspberry-pi-python-2f27a2893ab5?source=search_post---------146,"There are currently no responses for this story.
Be the first to respond.
Hi friends!
So you’ve got an awesome idea to connect a Raspberry Pi to a weather station in your backyard so you can automate opening and closing your windows when it’s going to rain. Oh and while you’re at it, you’re going to hook up a microcontroller like an ESP32 to turn your sprinklers on and off when it’s a bit too hot out. How about never forgetting to turn off your oven again!
So now what? There are plenty of tutorials out there on building a weather station. Nice list of components, some of them are pretty good about physically connecting it, others kind of gloss over it a bit and you have to search around to piece everything together. Some go a step further and tell you how to connect it up to the Cloud! Eureka! Now we’re talking. But a lot of the ones that go that far are really long, no table of contents so you have to kind of search through all the pieces to find what you need. And rarely do they fill in all the gaps for a specific use case.
The goal of this blog, is to start small, and focused (even though it still feels kinda long). I want you, by the end of this, to have a step-by-step guide to connecting your Raspberry Pi to Google’s Cloud Platform. There’s a TON of stuff to do beyond that, but like I said, start small. I’m not going to go into even close to the full features that our IoT platform has to offer. Even with regards to communication with devices. I’m ONLY covering one way, device to Cloud communication. But I want to enable you to bring your project to the Cloud. Taking that first step is all about being able to see your device talking to the Cloud.
After I get this covered, I’ll start building up similar tutorials. More things you can do with devices like two-way communication. How to move data around inside the Cloud Platform, how and where to store the data for different use-cases. How to run basic analysis. How to train a machine learning model in the cloud with your data.
In this tutorial, note that for some folks, the level of detail I go into will be way more than you need. For example, in the Pi setup basics, I go into a lot of detail around things like, changing the font of the terminal to make it easier to read. I have set up logical headings so it’s easy to bounce to the parts of the tutorials that you need to get up and running.
I used a Pi 3+ for the tutorial, but from the library dependency setup portion down, this will also work with a Raspberry Pi Zero W. If you want to setup your Pi Zero for this, but haven’t set one up before, I highly recommend this guide. And then skip down to the dependency section in this post.
Some of these steps might be slightly different for you, I have some things I like to do to my Pi just for quality of life. If you already know how to setup WiFi, and adjust your Pi to how you like it, then you can safely skip this bit and go straight to the library dependency setup.
Direct connect to your Pi, and hook the HDMI.
Depending on the monitor you’re on, or how old you are, I change the font in my terminal to Monospace 14. Makes it much easier to read. I also change my keyboard layout in the preferences to US->English. Default is UK->English which has symbols in unexpected places for me.
One thing you definitely need to do if you’re using WiFi, is be sure that in the settings on the Pi, your WiFi country is set to wherever you are. If it’s set to Germany, and you’re in the United States, it’s likely not going to work. Sometimes different combinations of countries work, but it’s pretty random, and better to just set it.
Next step is to be sure your Pi can connect to the internet. Either plug in an ethernet cable, or if you’re using WiFi, scan for networks your Pi can see. Run this from a terminal on the Pi:
Pick the one you want to use, remember the SSID.
Note that this will override any existing WiFi settings you had, so if you want to preserve the current settings, move the file to be a backup. This can be done with:
Then run:
You’ll replace the contents of that file if there’s anything in there with one of two things. If your WiFi has a password, then use this:
If there’s no password:
Now that you have a configuration in there for your WiFi, you need to restart that service to engage the WiFi:
Now you should be able to verify that you have WiFi by running:
You should have an IP address. Depending on your WiFi’s configuration it’s probably something that starts with 168.1 or 127.1 (as long as it’s not 127.1.1.1 which isn’t right for this).
If it fails, and you don’t have an IP address. You can try:
This hard resets the WiFi. If it STILL doesn’t work, try rebooting the Pi completely with:
Now that you’re Pi can talk to the internet, let’s get it setup to have all the Python dependencies we need.
If you’re new to Linux, Pi, and all things apt-get, each of these steps will do some variation of spew out a bunch of text about what you’re asking for, and what dependencies it relies on, to tell you what it’s going to install on your Pi. If you want to know what it all means, the internet pages for each library are pretty good about explaining what the library is all about. I’ll TL;DR each one just to blurb on what the particular library is used for.
First up, make sure we’re current:
This makes sure that the list of places your Pi will be getting its libraries from is current. Sometimes these repositories move around, get deprecated, etc, so it’s always good to run this when starting a project. Note that if you’re using Raspbian, that they’re constantly updating it as well, so some of these dependencies may just say they’re already there. That’s totally fine, this is just, for the sake of completeness, all the things you need.
Next up is to get the pieces we need to handle the secure connection with IoT Core. Authentication is done via JWT instead of user/password as it’s way more secure. To handle that, the library is pyjwt, which depends on a Python library called cryptography. Why am I telling you this? Because to install those pieces, you need certain base libraries. To get them, run each of these commands:
Note here, that depending on your version of Python, being able to install one of the libraries, cryptography, might require 3.x. Go ahead and try to go through everything with whatever version you have of Python, but if you get compile errors on the step when you get to sudo pip install cryptography then you’ll need to run all these pip installs with pip3 instead of just pip so you’re using Python 3.x. I’ve run it recently with Python 2.7.13 and it appears they’ve cleaned up a bunch, so you’ll get messages for a few of these like Requirement already satisfied: <library>. I’m just including all the manual library additions for completeness.
For this demo, we’re using MQTT, so we need the library that allows us to use that protocol, paho-mqtt. Run:
For our encryption we’re using pyjwt. Run:
For crypto, the cryptography library is the dependency for our JWT library. Run:
And finally, I’m using a Sense HAT for my telemetry data. Just makes it nice and easy since it provides telemetry sensors, has a nice API, and the library is pre-installed on the Pi model 3. If you’re not using a Pi that has it installed, just run:
Now your device should be all set (minus code, coming later).
If you’ve never done this before, then go to Google’s Cloud Platform landing page and click through the “Try now” button. It’s the most seamless if you have a gmail address, but you can still do this if you don’t have one. You can go here and create a Google account using any email address.
Once the account is all setup, head to the console. First step will be to set up billing. Keep in mind, setting up the Cloud project if it’s the first time you’re doing it, gives you $300 in Cloud credits, and there’s a generous free tier so while you’re trying things out, you’re unlikely to hit any pay walls.
First step is to enable Cloud IoT. Number of ways to get there:
1) Put IoT Core into the search bar in the console2) Select it from the hamburger menu in the upper left. IoT Core is down near the bottom. 3)Click this link.
Clicking the enable button will grant the appropriate permissions for the default security rules to allow your user to use IoT Core. If you’re wanting to do it by hand, be sure the user you’re going to use for your IoT work has permission to publish to Pub/Sub. IoT Core bridges the device messages to Pub/Sub for you, which means you need to have permission to write. These permissions are handled in the IAM part of the console. There be dragons. I would only suggest doing that if you’re really comfortable with Google Cloud Platform.
Next we’ll create your registries where you want your IoT devices to live. Registries are logical groupings of devices. So if you’re doing like, a smart building infrastructure, each registry might represent a building’s worth of devices.
Easiest path is to create a device registry from the IoT Core console page, and as part of that process, in the drop-down where it asks for a Default telemetry topic, you can elect to create a Pub/Sub topic inline. Select the region closest to you, and by default it enables MQTT and HTTP protocols. Easiest to leave that alone unless you know you’ll only be using one vs. the other. The example code in this blog uses MQTT.
Now we need to create out device representation in IoT Core. In the page that opened when you created your registry, click on the Create Device button.
Give it an id, and leave all the options alone. For the SSL certificate, we’ll create an RSA with x509 wrapper here, but if you want to look at the other options, you can see how to create them here. Be sure that you select the appropriate radio button for the type of cert you create. By default, the RS256 radio button is selected so if you’re just creating the cert using the code snippets below, select the RS256_X509 radio button for Public key format.
The default install of Raspbian has openssl installed. If you’re running a custom OS on the Pi and it doesn’t have openssl installed, you should be able to just run this off the Pi, and put the private key on it later. To create the key, run the following:
You can either upload the key directly, or copy/paste the contents of the key. There’s two radio buttons on the device page to pick which way you want to do it.
If you want to upload, click the radio button on the create device page for Upload. Browse to and select the demo.pub key. Scroll down and click the Create button. You should then see your newly created device in the list on the registry details page.
If you’ve run the openssl commands on a device that can’t run the webpage, leave the radio button on manual, and on your device run:
and copy everything (including the tags) between:
and paste it into the text box for the Public key value.
This handles the auth from the Google side confirming your device is okay to talk to Google. Last security piece is to grab the Google roots.pem file so your device knows it’s talking to Google. Again, on the Pi if possible, off the Pi and transfer it over if not, run:
One last thing to setup. Pub/Sub is efficient in that, if nothing is listening to the Pub/Sub topic, any messages sent to that topic won’t be stored. Since subscriptions don’t go into the past to pick up messages, there’s no reason to store them if no one’s listening. So enter Pub/Sub into the search field on the Cloud Platform console, or pick Pub/Sub from the hamburger menu to open up the Pub/Sub page. You should see at least the topic you created above listed here.
Click on the 3 dot menu on the right, and pick New subscription. Give it an id.
Now you have all the pieces setup to get your device talking to Google Cloud Platform. A few last bits with the code itself. We’re almost there!
The code can be found on my GitHub. 01_guide.html is an abbreviated version of this blog post. If you need to find things later, it may well be faster without the talking through things here to just access that guide and pick up what you need. 01_basics.py is the basic code you want to get and run. As I mentioned above, it uses the Sense HAT to gather the telemetry data. So if you have one, you should be able to modify the variable block in the code to point to the various pieces you’ve setup and just run it.
The ssl_private_key_filepath is the full path to the private half of the key we created: demo_private.pem. The root_cert_filepath is the full path to the roots.pem we grabbed using wget above. project_id is the Google Cloud Platform project id that your device was registered to. gcp_location is the region you picked when creating the registry. registry_id and device_id are the ids you gave above when creating those pieces.
If you want to use something other than the Sense HAT then comment out any reference to sense and replace with whatever you want to use.
Last piece, is I have commented out the actual publish line of code so you can test to be sure everything ELSE is working before you start spamming messages into the Cloud. That’s down around line 100 (as of the time of publishing this blog). I’d suggest running first, then uncommenting that line.
Now you should be able to run:
If it’s working, you should see messages on the console like:
And then:
If all that’s working right, then go ahead and uncomment the publish line in the code, re-run it, and then after a bit (there IS a delay before Pub/Sub will return values using the command line SDK, but the actual publish time is faster than this) you can verify by running this command anywhere you have the gcloud SDK installed (if you don’t, go here for instructions on installing it):
If all has gone well, you should see your messages showing up there!
Thanks for reading, now go connect your device to the Cloud! Think of a project, tell me your ideas.
If you have any questions, about a project idea, the Cloud or IoT, or anything, please don’t hesitate to ask in the comments below, or reach out to me on Twitter!
Next step: Sending communication back to your device from the Cloud.
Google Cloud community articles and blogs
818 
14
818 claps
818 
14
Written by
Husband, father, actor, sword fighter, musician, gamer, developer advocate at Google. Making things that talk to the Cloud. Pronouns: He/Him
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Husband, father, actor, sword fighter, musician, gamer, developer advocate at Google. Making things that talk to the Cloud. Pronouns: He/Him
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/flutter-vision-flutter-firebase-ml-vision-firebase-cloud-firestore-76f3794f5d38?source=search_post---------147,"There are currently no responses for this story.
Be the first to respond.
I’ve been a fan of code once, release many development platforms like Cordova (and associated app frameworks, like Ionic) as well as Xamarin. I’ve been keeping an eye on Flutter, Google’s entry into the fray. Recently they hit their 1.0 release and I decided to give it a spin. I was a bit apprehensive diving in as Flutter apps are written in Dart (of which I have little experience), but I was encouraged by Google’s statement:
You don’t need previous experience with Dart or mobile programming.
With that statement in mind, I decided to recreate an app I previously wrote using Ionic + Firebase + Google Cloud Vision. If you’d like to check that out, the tutorial is available as well as the source code.
In this tutorial we will build an app that takes pictures of items, uses ML Vision to label the items, and saves the image and labels to Firebase Cloud Firestore and Storage. If you’d like to skip to the source code it’s available here. Here is a video of the finished product:
Here we go! I followed the installation instructions provided for MacOS. While not as easy as a brew or npm install, I got through it with ease. Be sure to run flutter doctor and follow the instructions to install any missing dependencies.
My editor of choice is VS Code which fortunately has both Flutter and Dart plugins available. Running Flutter: Run Flutter Doctor prompted me to point to where my Flutter SDK was installed, then I was all set to start writing my app!
First step is to create a new project using the handy VS Code commands outlined here. Seconds later the project is setup including both the iOS and Android platform specific files. Coming from my experience with Cordova I found this quiet surprising as adding a single platform is a separate (and sometimes lengthy) process.
At this point we are ready to run the app. I don’t have any simulators (iOS) or emulators (Android) running at this point so I tap No Devices in the VS Code status bar and click iOS device which automatically launches an iPhone XR simulator. Back in VS Code I hit F5 and 16.2 seconds later the app is running in debug mode on the simulator. Fast! Per the tutorial I change the text and save main.dart which kicks off a hot reload. 1.5 seconds later I see the next text on the screen. Fast!
Like my previous app and tutorial, Flutter Vision is going to take a picture of an item, analyze the picture using ML Kit for Firebase on the device, then store the data in Firebase Cloud Firestore.
First, we need to be able to invoke the camera and save the image file after the user takes a picture. The camera plugin installation instructions are here. Be sure to make the platform specific changes outlined here. After looking at a simple sample and a more complex example I updated my app to simply show a camera preview along with a button to take a picture. After taking the picture the filename and path is shown via a toast. It’s a bit long to paste here, so check out the gist for the source. It’s best to test with an actual device since we are using the camera. Connect a device to your machine and you should see it listed in VS Code’s status bar. Hit F5 to kick off a build and debug session. Note: I was prompted to select a Team in Xcode — after selecting one I had the app running on my phone. Step one complete!
Next up we need a way to use Firebase ML Kit. After a quick search I see that Google has Firebase Flutter plugins — FlutterFire! To use Firebase services we need to first create a Firebase project. There is a great tutorial here that explains how to get this done. Next, we need to closely follow the steps outlined to handle the platform-specific configurations. Now that we have a Firebase project and have setup our platforms, we can start using Firebase ML Kit. Let’s add ML Kit to our app by following the installation instructions. For a quick test, let’s create a function to handle label detection and iterate through the list of labels returned by ML Vision and display them to the user. We will make this more useful later.
Full gist of changes here. Let’s fire up the app again by hitting F5. It works! See it in action:
Next we are going to save the image and label data to Firestore. Last time around we used Firebase’s Realtime Database — since then Firebase has released Firestore, a feature rich NoSQL datastore. To setup your Firestore database follow the instructions here, choosing Test Mode for the purposes of dev/test. Remember that this leaves you wide open, so be sure to lock it down with security rules before going to Production. Back to the code — Flutter has a plugin available here which we will install. Next, after we take a picture and analyze it, let’s store it as a document that we will call an item. Multiple documents will be organized into a collection that we will call items. Create a function that will create a new item:
Then change our label detection process to call this new function:
Now when we take pictures we can see items in our collection being added extremely fast. To be clear, this is taking a picture, analyzing the picture for labels using ML Vision, then saving it to a cloud database. Seriously, this is mind-boggling fast:
Now we need to store our images. In the previous app, we base64 encoded the image data and stored it directly in Firebase’s Realtime Database. We won’t be able to store image data in Cloud Firestore due to datatype restrictions. No worries, we will store the image file in Firebase Storage then get the URL and store in the item document using the Firebase Storage plugin. Here is our function to handle the uploads. It uploads the file then awaits the StorageUploadTask and grabs the downloadURL:
Update our _addItem function to save the URL:
We need a unique URL, so we will use the handy uuid package like the example:
F5 again to try it out:
At this point our image and data is stored in Firebase. Next we need to display the list of images with labels to the user. We are going to restructure our app to have 2 screens: one that is responsible for displaying a list of images and labels to the user; another that takes a picture and kicks off the ML Vision process. Let’s create a new screen, ItemListScreen, to display a list of cards. I’m going to adapt the examples here and here to fit our needs:
…and an ItemsList widget (sorry for the formatting, available in a gist here):
Let’s change the home screen to the ItemsListScreen:
We should also take the user back to the list after they take a picture so they can see the results by adding the following to the detectLabels function:
I also decided to do some cleanup by renaming FlutterVisionHome to CameraScreen since we now have more than one screen.
Run the app one last time to see the final results! Firebase ML Vision is crazy fast. Paired with Flutter we were able to get a snappy app that we can run on both iOS and Android in hours, not days. Incredible. If you were like me and hesitant to try Flutter because you didn’t know Dart — give it a go today!
The source code is available here. Feel free to open an issue there if you run into trouble or hit me up in the comments. Thanks for listening!
Google Cloud community articles and blogs
799 
2
799 claps
799 
2
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
CTO at United Effects
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/@metinalniacik/spring-cloud-spring-boot-ve-eureka-server-kullanarak-microservice-%C3%B6rne%C4%9Fi-1775cf220b2d?source=search_post---------148,"Sign in
There are currently no responses for this story.
Be the first to respond.
Metin Alnıaçık
Sep 13, 2018·2 min read
Spring Cloud, Spring Boot ve Eureka Server kullanarak microservice uygulaması geliştireceğiz.
Microservice kavramını, en basit şu şekilde tanımlayabiliriz; Bir uygulamanın atomik parçalara bölünüp küçük servislerin oluşturulduğu mimari bir yaklaşımdır. Bunun bir çok faydası vardır. Bu servisleri geliştirirken istediğimiz dili kullanabiliriz. Uygulamanın daha esnek ve ölçülebilir olması gibi bir çok fayda daha sayılabiliriz. Bunun karşıtı yaklaşım ise monolithic/layered mimarisidir.
Teorik anlatım üzerinde çok fazla durmayacağım. Basit bir uygulama yapacağım.
Service registration işlemi için Eureka Server’i kullanacağız. Eureka Server, Netflix tarafından geliştirilmiştir. Ayrıca açık kaynak kodlu bir projedir.
Uygulama üç projeden oluşacaktır. Bu projelerden birincisi Eureka Server, ikinci proje film listesi getiren movie server, son proje ise movie client olarak tasarlayacağız.
1- Eureka Server
Uygulamanın pom.xml dosyası aşağıdaki gibidir.
Server port bilgisini ve eureka server bilgilerini girmek için, application.yml veya application.properties dosyalarından biri tanımlanmalıdır.
Ben, üç projede de application.properties dosyasını kullanacağım.
Server çalıştırdıktan sonra http://localhost:9999 adresinden “Spring Eureka” bilgilendirme ekranını görüntüleyebilirsiniz. Kırmızı ile işaretlenmiş kısımda şu anda çalışmakta olan servisler görüntülenmektedir. Fakat çalışmakta olan bir servis olmadığından liste boştur.
2- Movie Server
Bu servisin amacı ise istek gönderdiğimizde bize iki filmden oluşan bir liste dönmesidir. Bu kısım sadece veri sağlayacak. Görüntüleme ile ilgili herhangi bir işlem yapılmayacaktır.
application.properties dosyasına port bilgisi girilmediğinde varsayılan port 8080 dir.
Server çalıştırdıktan sonra http://localhost:9999 adresine tekrar baktığımızda bootstrap.properties dosyasında uygulama adı (MOVIE_SERVICE) görüntülecektir.
3- Movie Client
Bu kısımda ise 2. kısımda yapmış olduğumuz Movie Server projesine istek göndereceğiz ve orada oluşturulmuş bilgi ekranda görüntülenecektir.
DiscoveryClient sınıfının getInstances metodu kullanılarak istediğimiz service ile veri alış verişi yapabiliriz.
Movie Client üzerinden istek gönderilerek elde ettiğimiz veriyi ekranda görüntülüyoruz.
http://localhost:9090/ şeklinde bir istek gönderdiğimizde MovieController altındaki handleRequest metodu çalışır. Bu metod içerisinde discoveryClient.getInstance kullanılarak Movie Server servisi bulunur ve http://localhost:8080/list adresine istek atılır, daha sonra alınan veri ekranda görüntülenir.
İlgili projelere aşağıdaki linklerden ulaşabilirsiniz.
Eureka Server: https://github.com/mtnaln/movie_eureka_server-
Movie Server: https://github.com/mtnaln/movie_server
Movie Client: https://github.com/mtnaln/movie_client
İyi günler,Bol kodlamalar :)
Kod mu? Severim
7.4K 
7.4K 
7.4K 
Kod mu? Severim
"
https://medium.com/google-cloud/google-cloud-functions-python-overview-and-data-processing-example-b36ebde5f4fd?source=search_post---------149,"There are currently no responses for this story.
Be the first to respond.
Serverless, FaaS (Functions-as-a-Service), and Python are important knowledge areas for anyone building or utilizing cloud services in 2019. Cloud Functions are a lightweight managed service that you can use to increase agility while using cloud. You will want to consider Cloud Functions for new architectures or when modernizing existing workflows using multiple cloud platform services.
Cloud Functions are serverless, so you run code without having to worry about managing or scaling servers. Cloud Functions integrate easily with Google Cloud Platform (GCP) services and you pay for resources only when your code runs. They are invoked by triggers that you specify and they standby waiting for specific events or HTTP endpoint calls.
Serverless does not mean that you take existing code chop it up into functions to receive a lower cost and instant autoscaling. Adopting a serverless approach means utilizing managed services and allowing your provider to handle base functionality of your service or application then use Cloud Functions to act like the glue between those services to deliver business value.
Cloud Functions support microservices architectures, data management pipelines, and allow you to easily integrate AI into applications. As this article will further demonstrate, they can act like glue pulling together multiple services in Google Cloud Platform to deliver a service, build insights through ML, or help data flow to BigQuery.
Since Cloud Functions execute in about 100ms, they can enable you can have a near real time streaming pipelines. They should be quick snippets of code that can fail easily if necessary. The main prerequisite is that you have a moderate skill level at either node.js or Python. In this article, we will focus on the Python runtime.
There are two different types of Cloud Functions: HTTP functions and background functions. HTTP functions are triggered by a HTTP trigger endpoint; and, background functions are triggered by event triggers from GCP services.
What can you use Cloud Functions for?
Triggers determine how and when the function executes. When you deploy a cloud function you must select a trigger that will invoke your code. These are the unique events that you will identify and plan for actions to occur after.
Event Trigger Types
Other triggersYou can get creative with GCP services native functionally to further extend triggers with services that support Pub/Sub or any service that provides webhooks.
Make a trigger from any service that supports Cloud Pub/SubBecause Cloud Functions can be invoked by message sent to a Pub/Sub topic, its easy to integrate Cloud Functions with other Google services that support Pub/Sub. For example you can export Stackdriver Cloud Logging to a Cloud Pub/Sub topic as a sink destination, once a message is posted to that Pub/Sub topic you can execute a function. This could be interesting for integrating with a service such a PagerDuty for staying ahead of critical logging notifications.
Gmail Push Notifications can be configured to send a message to a Pub/Sub topic when there are changes in Gmail inboxes. Are you using a Gmail inbox to manage invoices or activity for a department? Now you can kick off secondary workflows with Cloud Functions based upon events (messages received) in Gmail.
One example I like for data processing workflows is using a Cloud Function to perform serverless data warehouse updates. I have only been able to find this scenario done in node.js by Asa Harland, so here it is for you in Python.
The use case: Say you have a Python proficient data science team working in Jupyter notebooks or Cloud Datalab. Your organization has a data warehouse in BigQuery that you are working on via notebooks. You need fresh updated data as the sample set you are working on is static and hasnt been updated in awhile. By using Cloud Storage as your source, a Cloud Function in between, and BigQuery as your destination you have a basic data pipeline to give you fresh data for your analysis and insights.
Using the object.finalize trigger whenever a new CSV or AVRO is uploaded to a specified Cloud Storage bucket the Cloud Function then uses the BigQuery API to append rows to the specified table in the function code.
Here is how the architecture looks for this data processing pipeline:
Here is the function code to append new CSVs or AVROs in Cloud Storage to a BigQuery table:
Now, with your function set on your source bucket whenever a new CSV delta is uploaded the rows in that CSV will be automatically appended to the table you specified.
The goal of this article is to help you understand serverless options such as Cloud Functions when building out new modern architectures on Google Cloud Platform. Today there are many managed options that can give you different types of agility when building a new application or services. Cloud functions are the glue code that can make your managed service backed applications or workflows more efficient and insightful.
Credit to Zack Kanter for an excellent post on the business case for serverless. Some concepts and descriptions used in this post were adapted from his article.
Google Cloud community articles and blogs
638 
7
638 claps
638 
7
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Customer Engineer, Google Cloud. All views and opinions are my own. @mkahn5
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/bbc-design-engineering/moving-bbc-online-to-the-cloud-afdfb7c072ff?source=search_post---------150,"There are currently no responses for this story.
Be the first to respond.
Top highlight
This is the first in a series of posts about how BBC Online is changing, making use of the cloud and more. To stay informed, follow this ‘BBC Design + Engineering’ Medium space — the follow button is at the bottom of the page.
Over the past few years, we in the BBC’s Design+Engineering team have completely rebuilt the BBC website. We’ve replaced a site hosted on our datacentres with a new one, designed and built for the cloud. Most of the tools and systems that power the site have moved too. We’ve used modern approaches and technologies, like serverless. And we’ve refreshed the design, approach, and editorial workflow, ready for the future. Hundreds of people have been involved, over several years. And it’s just about complete.
This post is the first of several looking at the what, the why, and most importantly the how: the approach we took to creating a new site that’s ready for the future. Delivering quality technology change, quickly and effectively.
The BBC website is huge. Over half the UK population use it every week. Tens of millions more use it around the world. It has content in 44 different languages. And it offers over 200 different types of page — from programmes and articles, to games and food recipes.
As is the case with tech, if you stand still, you go backwards. Until recently, much of the BBC website was written in PHP and hosted on two datacentres near London. That was a sensible tech choice when it was made in 2010; but not now.
The BBC’s site is made up of several services (such as iPlayer, Sounds, News and Sport). For them all, we need to ensure they use the latest and best technology. That’s the only way to ensure they’re the best at what they do. They all need to be hugely reliable at scale, as well as fast, well-designed, and accessible to all.
And so, over the past few years, it’s been our strategy to recreate BBC Online. Almost every part has been rebuilt on the cloud. We’ve taken advantage of the many benefits that the cloud brings — such as the flexibility to provision new services almost instantly. And we’ve used best-practice tools and techniques — such as the React framework, and the DevOps model. We’ll look more at the approach in a moment, but first, let’s discuss the underlying principles.
Rebuilding a massive website could easily suffer from the Second System Effect. It’s all too easy for new projects to be over-ambitious with the requirements and the approach. A push for perfection makes it tempting to pick the most sophisticated solutions, rather than the simplest. We needed to prevent this, to ensure good value and delivery at pace. Here are some principles that helped us do this.
When building something as large as BBC Online, it might be tempting to consider everything from scratch. Doing so provides the most control, and leaves no stone unturned. But the cost in doing so can be huge. An off-the-shelf solution might only give you 90% of what you want, but if it can be delivered in 10% of the time, it’s probably a worthy trade-off. This applies to tech, UX, business analysis, and pretty-much everything else. Most problems have already been solved somewhere else; so don’t solve them again.
A great tech example of this is the use of serverless. Around half of the BBC’s website is rendered serverlessly with AWS Lambda. Managing virtual machines (or containers) is expensive — keeping them secure, reliable and scalable takes time. Serverless mostly solves that problem for us. And problems solved elsewhere mean we shouldn’t do them ourselves.
When there are many teams, duplication is inevitable. Two teams will each come across the same problem, and create their own solution. In some ways this is good — teams should be empowered to own and solve their challenges. But left unchecked, it can create multiple solutions that are incompatible and expensive to maintain.
As we’ve rebuilt BBC Online, we’ve removed a lot of duplication and difference that has built up over the years. Multiple bespoke systems have been replaced with one generic system. It’s a double win, because as well as being more efficient (cheaper), we can focus on making the new single approach better than the multiple old approaches. It’s because of this that the BBC website now has better performance and accessibility than ever before.
However, we must be wary of over-simplification. Replacing multiple systems with one looks great from a business point-of-view. But software complexity grows exponentially: each new feature costs more than the previous one. There reaches a point where two simple systems are better than one sophisticated system.
As an example, we chose to keep the BBC’s World Service sites separate from the main English-language BBC site. The needs of the World Service (such as working well in poor network conditions) were specialist enough to warrant a separate solution. Two simpler websites, in this case, are better than one complex site.
Creating a new BBC website has involved many teams. To be successful, we needed these teams to align and collaborate more than we’ve ever done before. Otherwise, we could easily create something that’s less than the sum of its parts.
It’s hard to overstate the value of communication. Without it, teams cannot understand how their work fits alongside that of other teams. And without that understanding, they cannot see the opportunities to share and align. Teams may even start to distrust each other.
Communication brings understanding, and that allows the culture to change. Instead of teams doing their own thing in isolation, they naturally share, collaborate, and flex to each other’s needs. They go beyond what is strictly their team’s remit, knowing that others will too. Which ultimately makes a better solution for everyone.
Over recent months I’ve heard teams say things like ‘that other team is busy so we’re helping them out’, or ‘we’re aligning our tech choice with the other teams’. It’s a level of collaboration I’ve not seen before. By understanding the bigger picture, and how everyone plays their part, we’ve created a level of trust and alignment that’s a joy to see.
Even with great culture and communication, multiple teams won’t necessarily collectively come together to build the right thing. This is, of course, the much-quoted Conway’s Law.
“Organizations which design systems […] are constrained to produce designs which are copies of the communication structures of these organizations.”
Melvin Conway
The BBC has historically had separate websites — for News, for Sport, and so on. Each one had a separate team. To change that, and build one website, we needed to reorganise. But how? One gigantic team wouldn’t work, so instead we split teams into the most efficient approach. We created teams for each page ‘type’ — a home page, an article page, a video page, and so on. We also created teams to handle common concerns — such as how the site is developed and hosted. Altogether, it minimised overlap and duplication, and allowed each team to own and become expert in their area.
When designing large software systems, we’ve a tricky balance to find. We must plan for the future, so that we meet tomorrow’s needs as well as today’s. But we also don’t want to over-engineer. We cannot be sure what the future will bring. Requirements will change. Cloud providers will offer new technologies. The rate of change in the world — particularly the technology world — is higher than ever before.
There is no substitution for good analysis, planning, and software design. Researching the opportunities and choices available is key in ensuring a project sets off in the right direction. But we must resist the danger in over-thinking a solution, because it may be for a future that never comes.
The beauty of agile software development is that we can discover and adapt to challenges as we go along. Business plans and architectures need to evolve too, based on what we learn as the project evolves. Don’t solve problems until you’re sure they’re problems you actually have.
Expanding on the above, we must be careful not to optimise too soon. Large software projects will inevitably run into performance issues at some point. It’s super-hard to predict when and how these issues will appear. So, don’t. Use the benefit of agile development to respond to performance issues only when they become real.
As mentioned above, much of the BBC website now renders serverlessly using AWS Lambda. At the start of the project, we were suspicious of how quickly Lambda could render web pages at scale. We had an alternative approach planned. But in the end, we didn’t need it. Performance using serverless has been excellent. By not optimising too early, we saved a huge amount of effort.
This principle is Gall’s Law:
“A complex system that works is invariably found to have evolved from a simple system that worked. A complex system designed from scratch never works and cannot be patched up to make it work. You have to start over with a working simple system.”
John Gall
Removing complexity from an existing system is hard. And in our case, we had multiple complex websites that we wanted to combine. The collective requirements of these sites would overload any one system. So, we had to start again, going back to the basics of what common abilities were needed.
Finally, a very practical principle: ensure you are able to move quickly, so that you can learn and adapt. Release early, and often — even if it’s only to a small audience. As discussed earlier, predicting the future is notoriously hard. The best way to understand the future is to get there quicker.
The counterargument to this is that change brings risk. And with a popular service like BBC Online, reliability is critical. The BBC has always had a strong operational process (including 24/7 teams managing services, and a DevOps approach to ensure those who develop systems are also responsible for maintaining them). We’ve continued to invest in this area, with new teams focussing on infrastructure, and on the developer experience (DevX). We’ll go into more details in a future blog post.
Smaller releases, done more often, are also an excellent way to minimise risk.
Putting the above principles into practice, here’s a super-high overview of how the BBC website works.
Let’s look at each layer.
First up, all traffic to www.bbc.co.uk or www.bbc.com reaches the Global Traffic Manager (GTM). This is an in-house traffic management solution based on Nginx. It handles tens of thousands of requests a second. Because of its scale, and the need to offer extremely low latency, it partly resides in our own datacentres, and partly on AWS.
For parts of our site, a second traffic management layer is sometimes used. (Internally, these are called Mozart and Belfrage). These services, hosted on AWS EC2s, handle around 2,000 requests per second. They provide caching, routing, and load balancing. They also play a key part in keeping the site resilient, by spotting errors, ‘serving stale’, and backing off to allow underlying systems to recover from failure.
The vast majority of the BBC’s webpages are rendered on AWS, using React. React’s isomorphic nature allows us to render the pages server-side (for best performance) and then do some further updates client-side.
Increasingly, the rendering happens on AWS Lambda. About 2,000 lambdas run every second to create the BBC website; a number that we expect to grow. As discussed earlier, serverless removes the cost of operating and maintenance. And it’s far quicker at scaling. When there’s a breaking news event, our traffic levels can rocket in an instant; Lambda can handle this in a way that EC2 auto-scaling cannot.
A new internal project, called WebCore, has provided a new standard way for creating the BBC website. It’s built around common capabilities (such as an article, a home page, and a video page). It’s created as a monorepo, to maximise the opportunity for sharing, and to make upgrades (e.g. to the React version) easier. By focussing on creating one site, rather than several, we’re seeing significant improvements in performance, reliability, and SEO.
As discussed earlier, we’ve kept our World Service site as a separate implementation, so that it can focus on the challenges of meeting a diverse worldwide audience. (This project, called Simorgh, is open source and available on GitHub.) Our iPlayer and Sounds sites are kept separate too, though there is still a considerable amount of sharing (e.g. in areas such as networking, search, and the underlying data stores).
The rendering layer focuses just on presentation. Logic on fetching and understanding the content is better placed in a ‘business layer’. Its job is to provide a (RESTful) API to the website rendering layer, with precisely the right content necessary to create the page. The BBC’s apps also use this API, to get the same benefits.
The BBC has a wide variety of content types (such as programmes, Bitesize revision guides, weather forecasts, and dozens more). Each one has different data and requires its own business logic. Creating dozens of separate systems, for each content type, is expensive. Not only do they need the right logic, but they also need to run reliably, at scale, and securely.
And so, a key part of our strategy has been to simplify the process of creating business layers. An internal system called the Fast Agnostic Business Layer (FABL) allows different teams to create their own business logic without worrying about the challenges of operating it at scale. Issues such as access control, monitoring, scaling, and caching are handled in a single, standard way. As per our principles, we’re making sure we don’t solve the same problem twice.
The final two layers provide a wide range of services and tools that allow content to be created, controlled, stored and processed. It’s beyond the scope of this post to talk about this area. But the principles are the same: these services are moving from datacentres to the cloud, tackling difference and duplication as they do, and ensuring they are best-placed to evolve as BBC Online grows.
So, BBC Online is now (almost) entirely on the cloud. And it’s faster, better and more reliable as a result. We’ve discussed the key principles on how we made that change happen. And we’ve seen a summary of the technology used. More on that in later posts.
Most excitingly, this isn’t the end; just the start of something new. The tech and culture ensure we’re in a brilliant place to make BBC Online the best it can be. And that’s what we’ll do.
I’m massively proud of all we’ve achieved — to everyone involved, thank you.
We’ll be delving into the detail over the coming weeks — to be notified, subscribe to this ‘BBC Design + Engineering’ Medium channel.
BBC Product & Technology
964 
2
Thanks to Johnathan Ishmael. 
964 claps
964 
2
Written by
Head of Architecture for the BBC's digital products.
Building the best BBC products, platforms and services for audiences in the UK and around the world
Written by
Head of Architecture for the BBC's digital products.
Building the best BBC products, platforms and services for audiences in the UK and around the world
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://codeburst.io/building-a-serverless-api-using-firebase-cloud-functions-e5a6be42144c?source=search_post---------151,"A couple of days ago, as I was taking a shower, I had a Eureka moment! It occurred to me how possible it is to build a complete REST API using Firebase Cloud Functions. At first I thought, “This is just your very ambitious brain mumbling Bakani”, but after really thinking about the possibilities, I was convinced. I even tweeted about it and Abe Haskins said it’s possible and actually wished me good luck. So I set out to prove my theory.
I have always wanted to build a news app. I’ve iterated over several different versions of what would this app would be, most of which remain permanent residents of Localhost Valley. With each trial, getting international news was not a challenge because a while ago I discovered NewsAPI.org (or @NewsAPIorg on Twitter). It is a well documented API for news articles from over 60 sources world wide. Being from Zimbabwe, it was a bit of a bummer that Zimbabwean news sites were not on that list of sources.
Fast forward to the launch of Puppeteer (a headless chrome node API), I knew this was how I could finally get local news onto my news app(s). So I built my first web scrapper and I was now able to get almost up-to-date news from Chronicle & The Herald. I simply setup a cron job that would curl the scrapper twice an hour. Then every time I opened my news app I’d have 30 minutes or less fresh local stories. Great right? Yes, but with Firebase I could do more.
Well, I definitely do not mean there are no servers involved. Actually according to Martin Fowler, Serverless architectures refer to applications that significantly depend on third-party services (known as Backend as a Service or “BaaS”) or on custom code that’s run in ephemeral containers (Function as a Service or “FaaS”). This simply means the application developer does not need to worry about provisioning servers and scaling them but simply relies on infrastructure and services already built for all of that. Instead, developers then focus on making their apps awesome and deliver more value to their users.
Firebase is one of the many FaaS vendors out there. Firebase Cloud Functions allow you to have Node.js code which gets run in response to a trigger from any of the suite of Firebase products (Real-time Database, Cloud Firestore, Hosting & Storage). You can setup a function that runs when an image is uploaded to your Storage bucket or a new document is stored in Cloud Firestore.
One of the triggers is HTTPs requests to a given endpoint which can run a function when specified routes are requested. So this means you could setup HTTPs functions that respond to requests using any of the supported HTTP methods which are GET, POST, PUT, DELETE & OPTIONS. The best part is you can test all your functions even locally, read the docs for more.
Since basically this is just node, you can even setup Express in your function. This will allow you to gracefully handle different routes and request methods to your API.
Largely inspired by NewsAPI.org, I built my own version which hopefully will get used by other developers as well. This API is basically built using two https functions, one that receives stories from the above mentioned scrapper, and the other that actually responds to traffic from the consumers of the API.
The Scrapa Function simply waits for stories from the web scrapper. It then stores these stories in a Cloud Firestore database. Because Firestore is a schema-less document database, the story document can easily adapt to the changes as they come from the scrapper. In general a story has a title, description, date when published, an author and source. Sources are also stored in the database and can be requested via the Main API function.
The Main API Function responds to requests made by clients consuming the API. At the time of this writing, the available API endpoints are simply GET /sources and GET /news and these requests are handled by the express routes setup in api.js.
When you deploy your function, Firebase gives you an endpoint like https://us-central1-your-project-id.cloudfunctions.net/function_name. This might not be the coolest of URLs to send to your developer friends for them to try out your API. It’s Ok for that background service no one will never know makes a request to such a URL. But… Firebase has got you covered. Just add the following lines to your firebase.json under “hosting”
Now you can use the more friendly Firebase Hosting domain which is https://your-project-id.firebaseapp.com/api or simply https://yourcustomdomain.com/api if you have setup a custom domain.
This means your Cloud Functions can not make requests to remote services/APIs. Well it does make sense because Firebase is giving you almost everything for free, and they are a business too.
Cloud Functions ship with Node v6.11.5. The docs say “The Cloud Functions Node.js execution environment follows the Node “LTS” releases, starting with the v6 LTS release published on 2016–10–18. The current Node.js version running in Cloud Functions is Node v6.11.5"". But this can be solved by using a transpiler like Babel.js to turn your ES6+ code into the ES5 or lower i.e Javascript understood by v6.11.5.
The below article is the best way you can achieve this. I highly recommend it.
codeburst.io
So after all my brain wasn’t mumbling, it is actually possible to develop a complete REST API using Firebase Cloud Functions. A complete scalable API with HTTPs, who knew?
I look forward to exploring this concept further by including authentication and possibly rate limiting because even though Firebase can handle the heat it would be cool to restrict our “Robot” friends from abusing the service.
Thank you for reading ❤.
Bursts of code to power through your day.
1.1K 
10
1.1K claps
1.1K 
10
Written by
Lover of God, code & food. Backend Developer. Guardian of the localhost.
Bursts of code to power through your day. Web Development articles, tutorials, and news.
Written by
Lover of God, code & food. Backend Developer. Guardian of the localhost.
Bursts of code to power through your day. Web Development articles, tutorials, and news.
"
https://netflixtechblog.com/how-data-inspires-building-a-scalable-resilient-and-secure-cloud-infrastructure-at-netflix-c14ea9f2d00c?source=search_post---------152,"By: Jitender Aswani (Data Engineering and Infrastructure), Sebastien de Larquier (Science & Analytics)
Netflix’s engineering culture is predicated on Freedom & Responsibility, the idea that everyone (and every team) at Netflix is entrusted with a core responsibility and they are free to operate with freedom to satisfy their mission. This freedom allows teams and individuals to move fast to deliver on innovation and feel responsible for quality and robustness of their delivery. Central engineering teams enable this operational model by reducing the cognitive burden on innovation teams through solutions related to securing, scaling and strengthening (resilience) the infrastructure.
A majority of the Netflix product features are either partially or completely dependent on one of our many micro-services (e.g., the order of the rows on your Netflix home page, issuing content licenses when you click play, finding the Open Connect cache closest to you with the content you requested, and many more). All these micro-services are currently operated in AWS cloud infrastructure.
As a micro-service owner, a Netflix engineer is responsible for its innovation as well as its operation, which includes making sure the service is reliable, secure, efficient and performant. This operational component places some cognitive load on our engineers, requiring them to develop deep understanding of telemetry and alerting systems, capacity provisioning process, security and reliability best practices, and a vast amount of informal knowledge about the cloud infrastructure.
While our engineering teams have and continue to build solutions to lighten this cognitive load (better guardrails, improved tooling, …), data and its derived products are critical elements to understanding, optimizing and abstracting our infrastructure. This is where our data (engineering and science) teams come in: we leverage vast amounts of data produced by our platforms and micro-services to inform and automate decisions related to operating the many components of our cloud infrastructure reliably, securely and efficiently.
In the next section, we will highlight some high level areas of focus in each dimension of our infrastructure. In the last section, we will attempt to feed your curiosity by presenting a set of opportunities that will drive our next wave of impact for Netflix.
In the Security space, our data teams focus almost all our efforts on detecting suspicious or malicious activity using a collection of machine learning and statistical models. Historically, this has been focussed on potentially compromised employee accounts, but efforts are in place to build a more agnostic detection framework that would consider any agent (human or machine). Our data teams also invest in building more transparency around our security and privacy to support progress in reducing threats and hazards faced by our micro-services or internal stakeholders.
In the Reliability space, our data teams focus on two main approaches. The first is on prevention: data teams help with making changes to our environment and its many tenants as safe as possible through contained experiments (e.g., Canaries), detection and improved KPIs. The second approach is on the diagnosis side: data teams measure the impact of outages and expose patterns across their occurrence, as well as provide a connected view of micro-service-level availability.
In the Efficiency space, our data teams focus on transparency and optimization. In Netflix’s Freedom and Responsibility culture, we believe the best approach to efficiency is to give every micro-service owner the right information to help them improve or maintain their own efficiency. Additionally, because our infrastructure is a complex multi-tenant environment, there are also many data-driven efficiency opportunities at the platform level. Finally, provisioning our infrastructure itself is also becoming an increasingly complex task, so our data teams contribute to tools for diagnosis and automation of our cloud capacity management.
In the Performance space, our data teams currently focus on the quality of experience on Netflix-enabled devices. The main motivation is that while the devices themselves have a significant role in overall performance, our network and cloud infrastructure has a non-negligible impact on the responsiveness of devices. There is a continuous push to build improved telemetry and tools to understand and minimize the impact of our infrastructure in the overall performance of Netflix application across a wide range of devices.
In the People space, our data teams contribute to consolidated systems of record on employees, contractors, partners and talent data to help central teams manage headcount planning, reduce acquisition cost, improve hiring practices, and other people analytics related use-cases.
See open source project such as StreamAlert and Siddhi to get some general ideas.
This was just a tiny glimpse into our fantastic world of Infrastructure Data Engineering, Science & Analytics. We are on a mission to help scale a world-class data-informed infrastructure and we are just getting started. Give us a holler if you are interested in a thought exchange.
Contributors: Sui Huang (S&A) is partnering on reimagining People initiatives.
Learn about Netflix’s world class engineering efforts…
729 
2
729 claps
729 
2
Written by
Learn more about how Netflix designs, builds, and operates our systems and engineering organizations
Learn about Netflix’s world class engineering efforts, company culture, product developments and more.
Written by
Learn more about how Netflix designs, builds, and operates our systems and engineering organizations
Learn about Netflix’s world class engineering efforts, company culture, product developments and more.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://itnext.io/kubernetes-networking-behind-the-scenes-39a1ab1792bb?source=search_post---------153,"One of the things I love the most about Kelsey Hightower’s Kubernetes The Hard Way guide— other than it just works (even on AWS!)—is that it keeps networking clean and simple; a perfect opportunity to understand what the role of the Container Network Interface (CNI) is for example. Having said that, Kubernetes networking is not really very intuitive, especially for newcomers… and do not forget “there is no such thing as container networking”
While there are very good resources around this topic (links here), I couldn’t find a single example that connects all of the dots with commands outputs that network engineers love and hate, showing what is actually happening behind the scenes. So, I decided to curate this information from a number of different sources to hopefully help you better understand how things are tied together. This is not only important for verification purposes, but also to ease troubleshooting. You can follow along with this example in your own Kubernetes The Hard Way cluster, as all of the IP addressing and settings are taken from it (May 2018 commits, before Nabla Containers).
Let’s start from the end; we have three controller and three worker nodes.
You might notice there are also at least three different private network subnets!. Bear with me, we will explore them all. Keep in mind that while we refer to very specific IP prefixes, these are just the ones chosen for the Kubernetes The Hard Way guide, so they have local significance and you can chose any other RFC 1918 address block for your environment. I will post a separate blog post for IPv6.
This is the internal network all your nodes are part of, specified with the flag — private-network-ip in GCP or option — private-ip-address in AWS when provisioning the compute resources.
Provisioning controller nodes in GCP
Provisioning controller nodes in AWS
Each of your instances will then have two IP addresses; a private one from the node network (controllers: 10.240.0.1${i}/24, workers: 10.240.0.2${i}/24) and a public IP address assigned by your Cloud provider, which we will discuss later on when we get to NodePorts.
GCP
AWS
All nodes should be able to ping each other if the security policies are correct (…and if ping is actually installed in the host).
This is the network where pods live. Each worker node runs a subnet of this network. In our setup POD_CIDR=10.200.${i}.0/24 for worker-${i}.
To understand how this is setup, we need to take a step back and review the Kubernetes networking model, which requires that:
Considering there can be multiple ways to meet these, Kubernetes will typically handoff the network setup to a CNI plugin.
A CNI plugin is responsible for inserting a network interface into the container network namespace (e.g. one end of a veth pair) and making any necessary changes on the host (e.g. attaching the other end of the veth into a bridge). It should then assign the IP to the interface and setup the routes consistent with the IP Address Management section by invoking appropriate IPAM plugin. [CNI Plugin Overview]
A namespace wraps a global system resource in an abstraction that makes it appear to the processes within the namespace that they have their own isolated instance of the global resource. Changes to the global resource are visible to other processes that are members of the namespace, but are invisible to other processes. [Namespaces man page]
Linux provides seven different namespaces (Cgroup, IPC, Network, Mount, PID, User and UTS). Network namespaces (CLONE_NEWNET) determine the network resources that are available to a process, “each network namespace has its own network devices, IP addresses, IP routing tables, /proc/net directory, port numbers, and so on”. [Namespaces in operation]
A virtual network (veth) device pair provides a pipe-like abstraction that can be used to create tunnels between network namespaces, and can be used to create a bridge to a physical network device in another namespace. When a namespace is freed, the veth devices that it contains are destroyed. [Network namespace man page]
Let’s bring this down to earth and see how all this is applied to our cluster. First of all, Network plugins in Kubernetes come in a few flavors; CNI plugins being one of them (why not CNM?). The Kubelet in each node will tell the container runtime what Network plugin to use. The Container Network Interface (CNI) sits in the middle between the container runtime and the network implementation. Only the CNI-plugin configures the network.
The CNI plugin is selected by passing Kubelet the — network-plugin=cni command-line option. Kubelet reads a file from — cni-conf-dir (default /etc/cni/net.d) and uses the CNI configuration from that file to set up each pod’s network. [Network Plugin Requirements]
The actual CNI plugin binaries are located in — cni-bin-dir (default /opt/cni/bin)
Notice our kubelet.service execution parameters include network-plugin=cni.
Kubernetes first creates the network namespace for the pod before invoking any plugins. This is done by creating a pause container that “serves as the “parent container” for all of the containers in your pod” [The Almighty Pause Container]. Kubernetes then invokes the CNI-plugin to join the pause container to a network. All containers in the pod use the pause network namespace (netns).
Our CNI config indicates we use the bridge plugin to configure a L2 Linux software bridge in the root namespace with name cnio0 (the default name is cni0) that acts as a gateway (“isGateway”: true).
It will also setup a veth pair to attach the pod to the bridge just created.
To allocate L3 info such as IP addressees, an IPAM-plugin (ipam) is called. The type is host-local in this case, “which stores the state locally on the host filesystem, therefore ensuring uniqueness of IP addresses on a single host” [host-local plugin]. The IPAM-plugin returns this info to the previous plugin (bridge), so any routes provided in the config can be configured(“routes”: [{“dst”: “0.0.0.0/0”}]). If no gw is provided, it will be derived from a subnet. A default route is also configured in the pod network namespace pointing to the bridge (which is configured with first IP of the pod subnet).
Last, but not least, we also requested to masquerade (“ipMasq”: true) traffic originating from the pod network . We don’t really need NAT here, but that’s the config in Kubernetes The Hard Way. So, for the sake of completeness, I should mention the entries in iptables the bridge plugin configured for this this particular example; All packets from the pod which destination isn’t in the range 224.0.0.0/4 will be NAT’ed, which is somehow not aligned with “all containers can communicate with all other containers without NAT”. Well, we will prove you don’t need NAT in short.
We are now ready to configure pods. We are going to take a look at all the Network namespaces in one of the worker nodes and analyze one of them after creating a nginx deployment as described in here. We will use lsns with the option -t to select the type of namespace (net).
We can find out the inode number of these with the option -i in ls.
Optionally, you can also list all the Network namespaces with ip netns.
In order to see all the processes running in the network namespace cni-912bcc63–712d-1c84–89a7–9e10510808a0 (4026532426), you could do something like:
This indicates we are running nginx in a pod along with pause. The pause container and rest of containers in the pod share the net and ipc namespace. Let’s keep the pause PID 27255 handy.
Let’s now see what kubectl can tell us about this pod:
Some more details:
We have the pod name nginx-65899c769f-wxdx6 and the ID of one of the containers in it (ngnix), nothing about pause yet. It’s time to dig deeper on the worker node to connect all the dots. Keep in mind Kubernetes The Hard Way doesn’t use Docker, so we will use the Containerd CLI ctr to explore the container details.
With the Containerd namespace (k8s.io), we can get the container ID’s for ngnix:
And pause:
The container ID for ngnix ending in 983c7 matches what we got with kubectl. Let’s see if we can find out which pause container belongs to the nginx pod.
Do you remember the PID’s 27331 and 27355 running in the network namespace cni-912bcc63–712d-1c84–89a7–9e10510808a0?
And
We now know exactly which containers are running in this pod (nginx-65899c769f-wxdx6) and network namespace (cni-912bcc63–712d-1c84–89a7–9e10510808a0):
So, how is this pod (nginx-65899c769f-wxdx6) actually connected to the network?. Let’s take the pause PID 27255 we got before to run commands in its network namespace (cni-912bcc63–712d-1c84–89a7–9e10510808a0).
We will use nsenter for this purpose with option -t to specify the target pid and also provide -n without a file in order to enter the network namespace of the target process (27255). Let’s see what ip link show,
and ifconfig eth0 say:
We confirm the IP address we got before from kubectl get pod is configured on pod’s eth0 interface. This interface is part of a veth pair; one end in the pod and the other in the root namespace. To find out what interface is on the other end, we use ethtool.
This tells us the peer ifindex is 7. We can now check what that is in the root namespace. We can do this with ip link:
to double-check, see:
Cool, the virtual link is clear now. We can see what else is connected to our Linux bridge with brctl:
So we have this:
How do we actually forward traffic?. Let’s look at the routing table in the pod’s network namespace:
So, we know how to get to the root namespace at least (default via 10.200.0.1). Let’s check the host’s route table now:
We know how to forward packets to the VPC Router (Your VPC has an implicit router, which normally has the the second address in the primary IP range for the subnet ). Now, does the VPC router know how to reach each pod network?; No it doesn’t, so you’d expect the CNI-plugin installs routes there or you just do it manually (as in the guide). Haven’t checked yet, but the AWS CNI-plugin probably handles this for us in AWS. Keep in mind there are tons of CNI-plugins out there, this example represents the simplest network setup.
Let’s create two identical busybox containers with a Replication Controller using kubectl create -f busybox.yaml.
We get:
Pings from one container to another should be successful:
To understand the traffic flow you can either capture packets with tcpdump or use conntrack.
The pod’s source IP address10.200.0.21 is translated to the node IP 10.240.0.20.
You can see the counters increasing in iptables as follows:
On the other hand if we removed “ipMasq”: true from the CNI-plugin config, we would see the following (we don’t recommend changing this config on a running cluster, this is only for educational purposes):
Ping should still work:
Without NAT in this case:
So, we just verified “all containers can communicate with all other containers without NAT”.
You probably noticed in the busybox example the IP addresses allocated for a busybox pod were different in each case. What if we wanted to make these containers available, so the other pods could reach?. You could take their current pod IP addresses, but these will eventually change. For this reason, you want to configure a Service resource that will proxy requests to a set of ephemeral pods.
“A Service in Kubernetes is an abstraction which defines a logical set of Pods and a policy by which to access them” [Kubernetes Services]
There are different ways to expose a service; the default type is ClusterIP, which will setup an IP address out of the cluster CIDR (only reachable from within the cluster). One example is the DNS Cluster Add-on configured in Kubernetes The Hard Way.
kubectl reveals the Service keeps track of the endpoints, and it will do the translation for you.
How exactly?… iptables again. Let’s go through the rules that were created for this example. You can list them all with the iptables-save command.
As packets are produced by a process (OUTPUT) or just arrived on the network interface (PREROUTING), they are inspected by the following iptables chains:
The following targets match TCP packets destined to 10.32.0.10 port 53 and translate the destination address to 10.200.0.27 port 53.
The following targets match UDP packets destined to 10.32.0.10 port 53 and translate the destination address to 10.200.0.27 port 53.
There are other types of Services in Kubernetes; NodePort in particular is also covered in Kubernetes The Hard Way. See Smoke Test: Services.
NodePort exposes the service on each Node’s IP at a static port (the NodePort). You can access the NodePort service from outside the cluster. You can check the port allocated with kubectl (31088 in this example).
The pod is now reachable from the Internet at http://${EXTERNAL_IP}:31088/. Where EXTERNAL_IP is the public IP address of any of your worker instances. I used the worker-0’s public IP address in this example. The request is received in the node with private IP 10.240.0.20 (the Cloud provider handles the public facing NAT), however the service is actually running in another node (worker-1, you can tell by the endpoint’s IP address10.200.1.18)
So the packet is forwarded to worker-1 from worker-0 where it reaches destination.
Is it ideal?. Probably not, but it works. The iptables rules programed in this case are:
In other words, the destination address of packets with destination port 31088 is translated to 10.200.1.18. The port is also translated from 31088 to 80.
We didn’t cover the Service type LoadBalancer that exposes the service externally using a cloud provider’s load balancer, as this post is long enough already.
While this might seem like a lot, we are only scratching the surface. I’m planning to cover IPv6, IPVS, eBPF and a couple of interesting actual CNI-plugins next.
I hope this has been informative. Please let me know if you think I got something wrong or any typo.
Further reading:
ITNEXT is a platform for IT developers & software engineers…
612 
7
Down-to-earth articles about Software, Cloud, Networking and Automation.
612 claps
612 
7
Written by
Proud dad working at Red Hat (CCIE, CCDE). Sharing content I create about: networking, automation, programming, golang, ipv6, and open source software.
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
Written by
Proud dad working at Red Hat (CCIE, CCDE). Sharing content I create about: networking, automation, programming, golang, ipv6, and open source software.
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://blog.sketchapp.com/our-plans-for-cloud-in-2020-604590145395?source=search_post---------154,"There are no upcoming events.
Want to host your own event? Get in touch.

        Here’s how we’re making Cloud the best place for collaboration — for solo designers and growing teams—in 2020 and beyond.
      
Sketch may have started life as a Mac app, but it has evolved to combine the best of native macOS technologies with browser-based collaboration tools that help over one million people to design better products.
We’ve been hard at work making our Cloud platform the best place for collaboration, whether you’re a solo designer or a growing team. Last month, we officially took Teams out of beta and brought free developer handoff to Sketch with Cloud Inspector. And with Sketch 63, we made saving documents to Cloud even faster.
Since we launched the Teams beta, we’ve heard from lots of you about your experiences using Cloud — from pain points, to features you wish existed. And we’ve been bringing that feedback into our roadmap. In this post, we thought we’d share some of our plans for Cloud in 2020 and beyond.
We’ve already kicked off the year by making Cloud a faster, smarter place to collaborate. When Cloud first launched, it was a simple way to share previews of your designs in the browser. When you uploaded a document to Cloud, the Mac app would send a set of static images to Cloud to display.
Now, things are a little different. From Sketch 63 onwards, we’re letting our servers do all the hard work of processing your document for Cloud instead of your Mac, making uploads much faster. On top of this, the processing work we do on our servers means we can do smarter things with your Sketch file — like allowing developers to inspect them.
Cloud Inspector launched in beta in January and brought free developer handoff to Cloud. And we’re already working on a round of exciting updates for it — like the ability to export production-ready assets in a single click, view Symbols and overrides, and export code in different languages.
With Teams, we introduced a Cloud workspace where every document is shared with your whole team by default. But we’ve heard from many of you who want more control over sharing documents and projects, as well as bringing in guests. This year, we’ll introduce more granular sharing controls and permissions, like disabling sharing for people outside your organization.
And because sharing designs always leads to more feedback, we’re making comments clearer and context-aware with Annotations. You’ll be able to click on any part of a design in the browser and leave a comment in the spot that makes most sense. When someone replies to your feedback, we’ll automatically thread and group comments to keep the conversation flowing.
Whether you’re working on your own, or as part of a large team, version control is always a challenge. Cloud comes with a full version history for every document, so you can quickly browse back through changes and even download older iterations.
We think we can do even better, though, so this year we’ll be making changes between versions more visual. We’ll also allow you set permissions for new versions, too. So, for example, you can choose who needs to approve a new version before anyone else in your workspace can see or download it.
We’re proud to have been a part of the conversation around design systems from the very beginning. But we also know that design systems are much more than just a Sketch Library. Symbols and Libraries are a great starting point, but for a design system to be truly successful, it needs to translate well into code, with guidelines that make it easy to apply across an entire product or brand. So this year, we’ll be bringing design systems to life, with much more detail, in Cloud.
Your design system might start with a Sketch Library, but with Cloud, we’ll give you ways to make it more dynamic and useful to everyone in your team. As well as being able to present individual components alongside usage guidelines and information, you’ll be able to link them up with code that helps developers easily implement them across your product.
Whether you’re using Cloud as a solo designer, or you’re part of a larger team, our goal is to make it the best place to share work and collaborate. The plans we’ve shared here are just the tip of the iceberg and there’s plenty more to come.
Turn your ideas into incredible products with a 30-day trial.
A monthly digest of the latest Sketch news, articles, and resources.
©
        2022
        Sketch B.V.
"
https://towardsdatascience.com/how-to-pass-the-google-cloud-professional-data-engineer-exam-f241d7191e47?source=search_post---------155,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jonathan Moszuti
Dec 13, 2019·15 min read
So you’ve probably googled the title above and now you’re here. Don’t worry though, you’re not the only one. I also did the same thing.
In this guide, I will give you an idea of what the exam is like and I how I prepared for it (and passed!)
"
https://towardsdatascience.com/10-days-to-become-a-google-cloud-certified-professional-data-engineer-fdb6c401f8e0?source=search_post---------156,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jeff Hale
Jun 20, 2019·11 min read
I recently took the updated Google Cloud Certified Professional Data Engineer exam. Studying for the test is a great way to learn the data engineering process with Google Cloud.
I recommend studying for the exam if you want to use Google Cloud products and:
"
https://medium.com/google-cloud/gps-cellular-asset-tracking-using-google-cloud-iot-core-firestore-and-mongooseos-4dd74921f582?source=search_post---------157,"There are currently no responses for this story.
Be the first to respond.
One of the biggest problems logistics industry faces today is the tracking of assets and vehicles. We have a bunch of ways to resolve this problem, GPS and Cellular technologies have been around for a long time and in the most cases are enough to track a vehicle in almost real-time. Our project will use both of these technologies to achieve this goal. Check out how our app will look like at the end of this tutorial:
You can access the WebApp at https://asset-tracker-iot.firebaseapp.com/
In this tutorial we will build an Asset Tracker using an ESP32 microcontroller running MongooseOS, that sends data securely via Cloud IoT Core using MQTT protocol over mobile network, the data is processed in an event-based way using Firebase Cloud Function, saves the data and current device state in Firestore. The data then can be accessed through a Progressive Web App hosted on Firebase Hosting, with the possibility to even configure the device remotely, showcasing Cloud IoT Core bi-directional communication.
If you are confused by the many terms, I recommend you to read another post that I made about using Google Cloud for an IoT project. In this post, I explain in more details most of these products. Here I’ll be more succinct in the configuration of the project on Google.
medium.com
Here we’ll be focusing on building an end to end architecture that handles the location data coming from our sensors in a secure way, store the data, show our fleet on a map in realtime. Our architecture will look like this:
Making a project that relies on mobile network and with security in mind is not trivial. I’ll show how easy is to do this using MongooseOS and their awesome PPPoS library, that abstracts the network over GPRS/Cellular serial modules, making easier for us to build our products that depend on this kind of communication.
The security channel to send data using MQTT will be handled by Cloud IoT Core. All data received will be stored in Firestore database. Here is the summary of what we will learn:
To use the gcloud command line tools, follow the instructions here to download and install it.
cloud.google.com
After installing the SDK, you should install the beta tools to have access to the Cloud IoT Core commands. Also after this you should authenticate and create a project to use in this tutorial, exchange YOUR_PROJECT_NAME with a name that you want for this project:
Create a Cloud IoT Registry where the devices will be registered to send data.
If you access the Google Cloud Console you can validate that everything is created and configured.
In this section, I’ll show how to setup the asset tracker, all modules needed and how to program the board with our code.
The heart of our project will be an ESP32 WiFi microcontroller, it has many flavors out there and as far as I known, any model will work.
Here is the project component list:
Our schematic is the following:
The schematic, in general, is pretty simple. Maybe the only weird part is the GSM module connections. I put a MOSFET to switch on and off the module because it consumes a lot of energy. Making this I can use less energy and put the device to sleep when I’m not sending data. Another detail is the usage of a diode to drop the voltage to power the GSM module with 4.2v, because powering it with 3.3v or 5v is considered under-voltage and over-voltage respectively by the module.
To program the board we will use MongooseOS. It has a tools called mos that make programming, provisioning and configuration really easy on the supported boards. To use it we need to download and install it from the official website. Follow the installation instructions on https://mongoose-os.com/docs/quickstart/setup.html.
mongoose-os.com
With the tools installed, download the project code on Github repository linked here, so you can build and deploy it on the device.
github.com
The repository consists of 3 sub-projects:
I will detail a little more each project in the next sections.
Here some description of the firmware project:
Now take a look a the device code, most of its functions have a little comment. Basically the device flow is the following:
To program the hardware, enter the firmware folder and run the following instructions to flash the firmware, configure WiFi and provision the device on Cloud IoT Core:
That’s it, your device will begin to collect location data, connect to the cellular network and send data to Cloud IoT Core. You can see whats happening on the device using the mos console tool. You will see it trying to connect to the cellular network and to mqtt.googleapis.com. Here is some output from mos console, I omitted some junk messages:
I recommend taking a walk with the device to generate data and also sometimes I have problems to get GPS and GPRS to connect inside my house. To see the data on PubSub you can use gcloud command to query the subscription that we created:
If you see the data on the console, you can start celebrating, we are on the right path 🎉🏆.
The Cloud Firestore is like a merge of Google Datastore and Firebase Realtime Database, with some features like document and collections definitions, real-time data synchronization, data replication and SDK for many languages. One of the most exciting news to me is that you have the same ease-of-use of all the Firebase products and in comparison with Firebase Database now we have a much better support for advanced queries and data modeling.
For this project I choose Firestore because of these features:
This project uses Firebase Cloud Functions to handle all the custom rules and backend services that our application need. The code can be seen below, but there are some settings that can be changed in the project:
Here are some details on each function that is running on the Cloud in reaction to our events:
To deploy our functions we need the Firebase Command Line Tools, it requires Node.JS and npm, which you can install by following the instructions on https://nodejs.org/. Installing Node.js also installs npm. Once Node and NPM is installed, run the following command to install Firebase CLI.
Now to configure firebase with our project and deploy the functions, in the project root folder, follow the above instructions:
With the deployed functions you have all setup to store our received location data sent by the device and execute our custom rules on how to store it. You can see and monitor all deployed resource on the Firebase Console.
Now it’s time to deploy our WebApp.
For this part of the project, I’ll not enter into too many details because it’s not the main subject for this post, but I’ll highlight some methods that make access to Cloud Firestore.
Here are some of the most important functions for the WebApp, where it access data on Firestore and call our endpoint to update device configuration:
For this app, I used an awesome tool called create-react-app that configures a really good project structure and tools to work with modern Javascript, in this case using React. For those who don’t know it, I recommend to check it out on their official blog.
reactjs.org
In the last section we already installed NPM, so now we only have to install all project dependencies and build it to production. Here are some commands that come configured with create-react-app :
After building our project we can now run firebase deploy and have all of our project deployed on Firebase infrastructure.
If all it’s correctly setup, then we have another end to end solution created using many awesome tools and better yet: Without touching an advanced and boring server setup.
That’s it for this project, we have a really cool asset tracker prototype that uses many new features of Google Cloud and some IoT tools. Hope you guys enjoy this project and I’ll continue to explore more projects using those tools.
The code for this project can be found on my Github and some interesting are linked in the section bellow to read later:
github.com
Do you like this post ? So don’t forget to leave your clap on 👏 below, recommend and share it with your friends.
Did you do something nice with this tutorial? Show in the comments section below.
If you have any questions, post in the comments that I will try to help you.
Google Cloud community articles and blogs
1.1K 
16
1.1K claps
1.1K 
16
Written by
Product Engineer @Leverege & Google Developer Expert for IoT. When I'm not cooking, I'm building fun stuff or helping my local dev community. GDG organizer.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Product Engineer @Leverege & Google Developer Expert for IoT. When I'm not cooking, I'm building fun stuff or helping my local dev community. GDG organizer.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-developers/5-aha-moments-with-the-google-cloud-platform-14b44b7ecdc3?source=search_post---------158,"There are currently no responses for this story.
Be the first to respond.
Posted by: Jerome Poudevigne, Startup Architect, Google Cloud
Throughout the past couple of years, I have helped a good number of companies, big and small, migrate their systems to the Google Cloud Platform (aka GCP). During the course of these migrations, there are always a few of those moments where people look at a specific Google Cloud feature and say, “now, that’s cool!”.
More often than not, it is because, coming from other platforms, they have gotten used to some features requiring multiple steps, or some operations being complicated, etc. And often they find out that in GCP you can do this specific operation in a couple of clicks, or by setting up a simple text-based configuration. Then you see that light bulb turning on in their head, and there you go… happy customer.
A few of these happen so often that I compiled them in a list to share with others who might also benefit from these “aha!” moments. You could say these are the five things I wish they told me when I started using Google Cloud.
A project is a namespace where resources live. Every resource you instantiate in GCP, from load balancers to Kubernetes clusters to virtual machines, belongs to a single project, and has no access (by default) to resources in other projects. User roles and authorisations can be defined per-project and trickle down to everything in it. This has two immediate benefits: you can group things that belong together in neat logical units, and things that don’t belong together are isolated from each other (and isolation is a Good Thing)
This is powerful and quite simple, but it often takes new users off-guard. I’ve had many clients call me and ask me “How can I make sure my developers cannot access the production machines? What’s the best way to create access policies? ”
The answer to this is actually super-simple:
Every machine/other resource in the production project won’t be accessible to developers.
Of course there is a lot more to it, and you can refine roles and permissions to a much greater degree using Organizations, Folders, etc. Not to mention all the crazy things you can do with per-project billing. But at least you can say “hey, if it’s a machine in the staging environment then it can be found in the “staging” project”.
Imagine you are using a Cloud provider and that you have servers in the US, and servers in Singapore, and that they need to communicate.
So you create a VPC (Virtual Private Cloud) network in the US data center, another one in the Singapore data center, and then you will connect them by setting up inter-region VPC peering or a VPN (Virtual Private Network) or a transit VPC or other routing magic.
Lots of work, right? And many moving parts, so lots of opportunities for things to break.
With GCP, however, what makes my clients go “aha!” is when they realize that in GCP a single VPC network covers the entire planet. Only subnets are attached to a geographic location, and virtual machines communicate between subnets on private IPs (good old RFC1918 addresses) — no extra routing needed.
So, to make your server communicate across continents on GCP, here are the steps:
That’s all there is to it. Your VPC network spanning 2 continents is ready to use. Below is a screenshot of how it looks on my account, for a VPC network called ‘my-global-network’ with 2 subnets. The first column (“us-central1” and “asia-southeast1”) contains the name of the GCP regions (read: data centers). The second column is the subnet name that I picked when I created them.
A machine in the US (on the “us-central” subnet) with IP 10.0.0.5 can communicate directly with a machine in Singapore (on the “singapore”) subnet with IP 10.10.0.8.
Nothing else to set up.
And thanks to the way these networks work, the Google Cloud Load Balancer can present a single IP to the world, and forward traffic to the instances that are the closest to you geographically without having to setup a tedious DNS-based load balancing. But that’s worth an entire blog. I’ll save it for another day.
There is no network security without a firewall so unsurprisingly GCP comes with one built-in.
Now, I don’t know about you, but nothing makes my brain hurt like a list of firewall rules displaying IP ranges and addresses and ‘Allow/Deny’ directives. It looks a bit like this:
If you imagine a normal network with a few dozen (hundred?) servers, you can quickly see how this can get out of control. You’d better have a solid printout of your network layout to refer to when you start adding and changing rules. And good luck debugging things!
Wouldn’t it be nice if, instead, you could just tell the firewall: “the HTTP traffic from outside can only reach the HTTP servers and the MySQL database is only reachable by the HTTP server(s) on the same network?”
Turns out it’s pretty simple on GCP by using a little thing called network tags. As the documentation says:
“Network tags are text attributes you can add to Compute Engine virtual machine (VM) instances. Tags allow you to make firewall rules and routes applicable to specific VM instances.”
So let’s see how it works. Firewall rules in GCP are defined in terms of source and target (the traffic flows from the source to the target). You can define filtering rules that apply to the source or the target, and in both cases you can use tags.
This is simpler shown with an example. The rule below states that on the default network, the traffic to the VMs with the tag mysql-server can come from the VMs with the tag http-appserver. Any other traffic is “Deny”-ed by default.
All you have to do is to tag your machines properly, and they will automatically be covered by the rule. You don’t need to enter their IP range.
That’s neat if you ask me. It makes it a lot simpler to grasp what’s happening.
Of course, there’s a TON more to firewalls in GCP. Tags also apply to routes and you can mix and match IP-based rules with tag-based rules. Not to mention that thing called service accounts, but I’ll leave those for another day.
The bottom line is that you can create most rules by just expressing a business need and not having to remember complicated network layouts. I have no hard stats, but I’m pretty sure this has saved me hours of work.
Easily access virtual machines (VMs) from the Google Cloud console was one of my first “aha!” moments when I started using GCP.
This is a screen capture of my Google Cloud console, with a virtual machine and its internal IP.
The last column has a header that says “Connect” and when you click on the word “SSH” a separate windows pops up. You wait for a few seconds, and… this is what you get. Your personal shell access — in a browser popup no less.
You are connected through ssh to the virtual machine of your choice. You did not have to download ssh keys and put them in the ~/.ssh directory, do the correct chmod command and run a long-winded ssh -i ~/.ssh/somekey me@<it-took-me-forever-to-copy-paste-the-address-here>
In addition, you have access to a few nifty features such as uploading and downloading files, changing the user etc. Just use the menu behind the cog icon at the top right.
In truth, you should not need to connect directly that often, but when you have to, this is a godsend.
The Google Cloud console has a cool trick: you can actually connect to a virtual environment that is managed by the Google Cloud console itself. It serves a bit as a jump host. You can access most resources from the projects from it, and you can activate it directly from the top menu with, no particular setup on your side. It’s called the Cloud Shell.
This is how it looks at the top right of the console:
When you activate the Cloud Shell, the session opens at the bottom of the console. You get a command line prompt and it’s fully configured with the gcloud command line tool (the jack-of-all-trades of Google Cloud scripting).
You can do a great many things from there, and this even includes uploading and downloading files, editing code or deploying it, a web preview for your AppEngine application, and more.
So you can get access to a fully configured shell environment in your project from any laptop where you can connect with your credentials. On top of this, it persists between connections so you can fine-tune it to your needs and have these changes available the next time you re-connect.
This has saved me many times during my previous life as a traveling consultant!
Did I say 5 “Aha!” moments ? Well, you’ve been patient reading all the way to here, so here’s one more for free.
Google Cloud has an amazing way to literally “teleport” a running virtual machine between physical hosts without stopping it. It’s called Live Migration. It allows Google to move your virtual machine away from a defective host, or a host that needs a patch or an upgrade, or for any other infrastructure related reason.
It’s all done in the background, and is totally transparent, so you never really see it happening. Unless you look VERY closely. I once did a demo to a client, where a machine was live migrated while he was simulating a solid network load — and we did not lose a single packet, with no noticeable degradation in latency.
So there you go. These are 5+1 things that made me go “Aha!” when I became more familiar with the Google Cloud Platform, and that still make my clients do the same.
There is a lot of depth to the platform, and my examples above only scratch the surface of our features. I encourage you to try it yourself. There is a generous free tier, and when you are ready to take the plunge and create that new company, please contact us at Google Cloud for Startups. We’ll get you up and running in no time.
Jerome is a Startup Architect at Google Cloud. Based in Singapore, he helps startups make the most of the Google Cloud Platform.
Engineering and technology articles for developers, written…
668 
3
668 claps
668 
3
Engineering and technology articles for developers, written and curated by Googlers. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
News about and from Google developers.
Engineering and technology articles for developers, written and curated by Googlers. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/@adrianco/cloud-native-computing-5f0f41a982bf?source=search_post---------159,"Sign in
There are currently no responses for this story.
Be the first to respond.
adrian cockcroft
Aug 9, 2017·4 min read
Amazon Web Services recently joined the Cloud Native Computing Foundation, and I’m representing AWS as the CNCF board member, with Arun Gupta from our open source team coordinating technical engagement with projects and working groups. To explain what this is all about, I think it’s useful to look at what we mean by “cloud native,” and where the term came from.
Back in 2009, I was working at Netflix, and the engineering teams were figuring out some new application architecture patterns we would need to migrate to AWS. Some of us had learned how to automate deployments at scale from time spent working at eBay, Yahoo, and Google. We also learned new ideas from Werner Vogels and the AWS team. The result was a new set of fundamental assumptions that we baked into our architecture. In 2010, we started talking publicly about our cloud migration, and in 2012 we got the bulk of the platform released as a set of open source projects, collectively known as NetflixOSS.
While we didn’t invent most of these patterns, the fact that we gathered them together into an architecture, implemented it at scale, talked about it in public, and shared the code was influential in helping define what are often referred to as cloud native architectures.
Cloud native architectures take full advantage of on-demand delivery, global deployment, elasticity, and higher-level services. They enable huge improvements in developer productivity, business agility, scalability, availability, utilization, and cost savings.
On-demand delivery, taking minutes instead of weeks, is often the first reason that people move to cloud, but it doesn’t just reduce the deployment time for a traditional application: it also enables a new cloud native pattern of ephemeral and immutable deployments. In the old deployment model, where it takes weeks to get a resource, you’re going to hang on to it, order extra capacity in advance, and be reluctant to give it back, so you’ll figure out how to update it in place. The cloud native pattern, instead, is to bake instances or build containers, deploy many identical copies just as long as they are needed, shut them down when you are done, and create new images each time the code changes. NetflixOSS pioneered these concepts by baking Amazon Machine Images (AMIs). Docker subsequently used it as a core element of the container deployment model.
Deploying applications that span multiple datacenters is a relatively rare and complex-to-implement pattern, but cloud native architectures treat multi-zone and multi-region deployments as the default. To work effectively in this model, developers should have a good understanding of distributed systems concepts; a discussion of the “CAP Theorem” became a common interview topic at Netflix. Despite huge improvements in technology, the speed of light is a fundamental limit, so network latency, and in particular cross-regional latencies, are always going to be a constraint.
Cloud native architectures are scalable. When I first presented about Netflix’s use of AWS in 2010, we were running front end applications on a few thousand AWS instances, supporting about 16 million customers in the USA. Nowadays, Netflix is fully migrated to AWS, has over 100 million global customers, and is running on over 100,000 instances. The implementation details have changed over the years, but the architectural patterns are the same.
Over time, components of cloud native architectures move from being experimental, through competing implementations, to being well-defined external services. We’ve seen this evolution with databases, data science pipelines, container schedulers, and monitoring tools. This is one place where the Cloud Native Compute Foundation acts as a filter and aggregator. The Technical Oversight Committee of the CNCF reviews projects, incubates them, and adopts projects as they move from the experimental phase to the competing implementation phase. For customers who are trying to track a fast-moving and confusing world, it’s helpful to regard CNCF as a brand endorsement, for a loose collection of interesting projects. It’s a loose collection, rather than a single, integrated cloud native architecture, so there’s no particular endorsement of any one project over another, for members of CNCF, or for users of projects.
The CNCF currently hosts ten projects, and is incubating many more: Kubernetes for container orchestration, Prometheus for monitoring, Open Tracing for application flow monitoring, Fluentd for logging, Linkerd for service mesh, gRPC for remote procedure calls, CoreDNS for service discovery, Containerd and Rkt for container runtimes, and CNI for container native networking.
From the AWS perspective, we are interested in several CNCF projects and working groups. AWS were founding members of the Containerd project; we are excited about participating in the Containerd community, and have lots of ideas around how we can help our customers have a better experience. Our forthcoming ECS Task Networking capabilities are written as a CNI plugin, and we expect CNI to be the basis for all container-based networking on AWS. In addition, a recent CNCF survey reports that 63 percent of respondents host Kubernetes on Amazon EC2, and Arun is blogging about his experiences with several Kubernetes on AWS installers, starting with Kops. We have plans for more Kubernetes blog posts and code contributions, and think there are opportunities to propose existing and future AWS open source projects to be incubated by CNCF.
The charter of the open source team we are continuing to build at AWS is to engage with open source projects, communities, and foundations, as well as to help guide and encourage more contributions from AWS engineering. AWS is already a member of The Linux Foundation, which hosts CNCF, and we look forward to working with old and new friends on the shared goal to create and drive the adoption of a new computing paradigm optimized for modern distributed systems.
Please follow @AWSOpen to keep up to date on open source at AWS.
Work: Amazon Sustainability (ex AWS, Battery Ventures, Netflix, eBay, Sun Microsystems, CCL)
401 
4
401 
401 
4
Work: Amazon Sustainability (ex AWS, Battery Ventures, Netflix, eBay, Sun Microsystems, CCL)
"
https://medium.com/google-cloud/running-a-serverless-batch-workload-on-gcp-with-cloud-scheduler-cloud-functions-and-compute-86c2bd573f25?source=search_post---------160,"There are currently no responses for this story.
Be the first to respond.
This quick-start guide is part of a series that shows how to leverage Google Cloud Platform components to run batch workloads in a simpler way. Those familiar with AWS, there’s a great tool called AWS Batch, but looking at GCP products, how are we able to run a batch job in a similar manner? Let’s dig into the GCP documentation: aws-comparison
Going deeper into the documentation… A way that GCP recommends this use case to be accomplished is: reliable-task-scheduling-compute-engineA quick summary: Cloud Scheduler -> Pub/Sub -> Start VM
An example of starting and stopping VM’s that’s quoted on the documentation: start-and-stop-compute-engine-instances-on-a-schedule
But there’s a flaw in this approach, the VM is not deleted after the batch workload is completed, so we are paying for the storage costs. How can we make it more efficient, and turn into a more serverless solution?
Wait a minute, how can you talk about VM’s and serverless in the same sentence? Something smells fishy…
It sure does, but those questions will be answered shortly, so stay with me…
To begin with, let me introduce the solution we are going to use to make the batch execution more serverless, using the GCP components:
Btw, all GCP components that will be used in this solution have a free tier =)
This is the starting point of our batch process:
“Cloud Scheduler is a fully managed enterprise-grade cron job scheduler. It allows you to schedule virtually any job, including batch, big data jobs, cloud infrastructure operations, and more. You can automate everything, including retries in case of failure to reduce manual toil and intervention. Cloud Scheduler even acts as a single pane of glass, allowing you to manage all your automation tasks from one place.”
Go to this page to start your Cloud Scheduler configuration.
To configure our starting point, we need to begin creating a Cloud Scheduler Job, it’s really straightforward:
The important fields here are Name, Frequency, Timezone, Target, and Topic. For now, we don’t need to worry about the payload, just leave an empty JSON there because the field is required.
Pay attention to the Timezone and the cron expression at Frequency, that’s when your Target is going to be executed.
As a Target, we are going to use Pub/Sub, and when we press Create, we will be presented with our Scheduler Job information:
Cloud Scheduler enables you to even manually trigger the Target execution with the “Run now” option, but running it now will return an error. The execute-batch-process topic doesn’t exist yet, let’s fix that…
We will fix that error by creating our Pub/Sub topic, it’s going to be our mediator:
“Cloud Pub/Sub is a fully-managed real-time messaging service that allows you to send and receive messages between independent applications.”
Go to this page to start your Pub/Sub configuration.
Creating a Pub/Sub topic is as simple as filling the topic name!
When we press Create Topic our Pub/Sub mediator is ready to extend the hand to our next component.
“Google Cloud Functions is a lightweight compute solution for developers to create single-purpose, stand-alone functions that respond to Cloud events without the need to manage a server or runtime environment.”
So, let’s revise… once the Cloud Scheduler is triggered, it will publish a message to a Pub/Sub topic that will start the Cloud Function.
And finally, our Cloud Function is going to create a Compute Engine VM that will be responsible for running our batch workload.
This is the code we will use to run the Cloud Function.
Notice that we are using the @google-cloud/compute client library, this is the core of our execution engine, and the method createInstance is the one responsible for spinning up the machine that will run our batch process. The vmConfig attribute will contain all the instructions for our VM workload, let’s dig into it…
For our VM workload, we have chosen a really simple use case, which is sending a “Hello World” message to Stackdriver Logging.
“Stackdriver Logging allows you to store, search, analyze, monitor, and alert on log data and events from Google Cloud Platform and Amazon Web Services (AWS). Our API also allows ingestion of any custom log data from any source. Stackdriver Logging is a fully managed service that performs at scale and can ingest application and system log data from thousands of VMs. Even better, you can analyze all that log data in real time.”
Let’s see the code that the VM is going to execute:
Basically, we are doing 3 things here…
Line 1: sending our “Hello World“ message to Stackdriver Logging and telling it to write to the batch-execution key, so we can track it later.
Line 2: retrieving the gcp_zone from the internal metadata endpoint that will be needed on the next line.
To know more about Retrieving instance metadata go to the official documentation.
Line 3:
Who’s the best actor to know that the VM finished executing? That would be the VM itself!
So here we are deleting the VM after it’s done with the batch workload. This is how we make it serverless, or as close as you can get.
Sounds good, but how are we going to put that script inside the VM and then configure it on our Cloud Function?
Go to this page to start your Compute Engine configuration.
By going to the UI, we are going to input the following fields:
Note: Remember to use a f1-micro instance, because it has a free tier.
On the image above, everything was left with the default value, we are just going to select a pre-configured service account named compute-execute-batch-job, this is important because the default service account doesn’t have the right permissions to delete Compute Engine VM’s.
Looking at how the service account was configured, best practices were followed by adding only the necessary roles for this kind of workload:
This is how the Service Account was created using the gcloud command line, you are also able to do this by using the Cloud Console.
To know more about Service Accounts and Roles go to the official documentation.
And after the service account, scrolling down on our compute engine UI, we have a Startup Script field where we are going to paste our workload script:
Remember one thing, it’s the Cloud Function that’s going to create our VM, so we are not going to click on the Create command, we will instead click on the Equivalent REST option, pointed on the image above.
By doing that, it will present us the full JSON that can be used in an API call:
Now that we have what we need to wrap up our Cloud Function with the VM configuration, let’s go back and update it, with the JSON from the REST call:
Note: Some fields from the JSON can be removed to use the default values, so it becomes smaller, but I want to show how simple it is, you can just copy and paste.
Remember to remove the double quotes from the key of the JSON attributes and make sure that the zone value being used in the vmConfig attributes are the same as the const zone in line 5. Also, replace the const projectId to use your project.
We are almost there! We just need now to deploy our Cloud Function and connect it to the Pub/Sub topic, let’s do it using the UI.
Go to this page to start your Cloud Function configuration.
Looking at the UI:
Mainly we have 3 important fields: Trigger, Index.js, and Function to Execute.
Trigger: here we can select options to call the Cloud Function, we will choose our Pub/Sub Topic execute-batch-process.
Index.js tab: this field contains the code that will be executed when the Cloud Function is called, so paste the create_instance_function code here.
Function to execute: use the name of the method that will be called when the Cloud Function run, createInstance.
One small thing that sometimes is forgotten, since Google Cloud is managing everything for us with Cloud Function, we also need to provide the package.json like a standard NodeJS application, since we are using javascript in this case.
Here’s the code:
We are adding it to the package.json tab:
To be good citizens, we will follow the best practices again and use a service account with just the amount of permissions that are needed:
The Service Account function-create-vm was configured with a Service Account User Role and a Custom Role containing the following permissions:
Ok now we can press Create 😄, we are ready to test everything together!
We will see something like this:
I love it when I see a ✔️ icon!
If we go back now to our Cloud Scheduler Job, and trigger it manually we can see everything working together.
Go to the Compute Engine page after a few seconds and you will see a new VM running with the prefix batch-job-executor followed by the execution time, it’s a little trick so we always have a unique name, if we need to track problems later.
After a few more seconds you will see that the icon before the VM name changed, that’s because the VM is being deleted, once the deletion is done the VM will be gone from the instances page.
Finally, to make sure it actually did something, we are going to Stackdriver Logging page, and when we filter the batch-execution key we can see our Hello World message! 👌🏻
Remember that this batch workload will be running on a scheduled basis according to the cron expression programmed at the Chould Sheduler Frequency.
This is the first post of a series showing how to run batch workloads in a simpler way, using Google Cloud Platform. It’s important to point out that here we used Compute Engine, but you are also able to run batch processes using other components like App Engine with task queue, GKE and the newest member of GCP’s Compute family Cloud Run.
On this post, we showed a really simple batch workload to help you get started, and to make it serverless we made sure that we deleted the VM after it was done executing. Thinking about it we used other serverless components such as Cloud Scheduler, Pub/Sub, and Cloud Functions, the only difference was that GCP was managing the resources and creating and deleting those for us… Using a few words from a friend of mine: “A serverless solution is never serverless. there is always a server behind the scenes…”
Thank you for your time! And stay tuned for the next post, where we will show a more complex workload adding Docker and Container Registry to this solution. Cheers!
[Update] Part 2 has been posted:
Adding Docker and Container Registry to the mix
[Update] Google announced serverless workflows
Google Cloud community articles and blogs
763 
7
763 claps
763 
7
Written by
software engineer & google cloud certified architect and data engineer | love to code, working with open source and writing @ alvin.ai
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
software engineer & google cloud certified architect and data engineer | love to code, working with open source and writing @ alvin.ai
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/uploading-resizing-and-serving-images-with-google-cloud-platform-ca9631a2c556?source=search_post---------161,"There are currently no responses for this story.
Be the first to respond.
Top highlight
More and more applications today require their users to upload photos. From simple things like updating a profile photo, to more complicated services like Snapchat where you share a bunch of them. These applications are running on all kind of devices with different resolutions and at different network conditions. In order to do it right we have to deliver the images as fast as possible, with the best available quality, taking into account the targeted device and screen resolution.
One option is to create an image service yourself, where you can upload the images, store them and possibly resize them in few different sizes. Unfortunately, doing so is usually very costly in terms of CPU, storage, bandwidth and can end up very pricy. It is also quite a complicated task and many things can go wrong.
By using Google App Engine and Google Cloud Storage though, you can easily achieve this seemingly difficult task by using their API. Start by completing a simple tutorial on how to upload files into the cloud and read the rest if you want to see it in action to understand why it’s one of the coolest things ever.
App Engine API has a very useful function to extract a magic URL for serving the images when uploaded into the Cloud Storage:
get_serving_url()
Returns a URL that serves the image in a format that allows dynamic resizing and cropping, so you don’t need to store different image sizes on the server. Images are served with low latency from a highly optimized, cookieless infrastructure.
In practice it’s the same infrastructure that Google is using for their own services like Google Photos. The magic URLs usually have the following form: http://lh3.googleusercontent.com/93u...DQg
If all that wasn’t enough for what’s coming out of the box, there are also no charges for resizing the images and caching them when using that Google’s magic URL. Yes, you read it correctly, this is a free of charge service and you pay only for the actual storage of the original image.
You don’t have to worry about anything when it comes to serving images for your next big idea today. All you need to do is to upload your images once, extract the magic URL and then use it directly on the client-side by updating the arguments depending on the environment. Prototyping applications similar to Snapchat or even more complicated ones could be implemented over a weekend.
Besides the already mentioned tutorial from the documentation, you can play with a live example on gae-init-upload (which is based on the open source project gae-init).
The photo in the example is taken by Aleksandra Kiebdoj.
Google Cloud community articles and blogs
713 
20
713 claps
713 
20
Written by
The world chico.. and everything in it...
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
The world chico.. and everything in it...
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/building-a-real-time-quant-trading-engine-on-google-cloud-dataflow-and-apache-beam-841a909d2c12?source=search_post---------162,"There are currently no responses for this story.
Be the first to respond.
Google Cloud has fully managed services that allow end users to build big data pipelines for their analytical needs. One of them is called Dataflow. It allows developers to build data pipelines based on Apache Beam SDK. It is fully managed in that gCloud takes care of auto scaling of distributed worker resources as well of redistributing work loads amongst available nodes. Together with Apache Beam it provides a unified programming model for both batch and streaming data alike. It also integrates conveniently with other gCloud offerings such as Pub/Sub and BigQuery.
In this post, we are going to build a data pipeline that analyzes real time stock tick data streamed from gCloud Pub/Sub, runs them through a pair correlation trading algorithm, and outputs trading signals onto Pub/Sub for execution.
Disclaimer: this post is intended to experiment with building real time data analytics on Dataflow, not to teach how to code quant trading strategies. Do not use this trading strategy for real trading without proper risk management.
A little background on the trading strategy before going into details of the pipeline. Our trading algorithm is based on the assumption that similar stocks move in tandem in the long term. For example, large cap tech stock prices like google, apple, facebook move in a correlated way. If for any reason they are not correlated any more, it might be caused by an unusual market event. We have an opportunity to open a long/short position of the pair of stocks and close that position when correlation reverts to mean. It’s a simple mean-reversion strategy for trading pairs of stocks.
For example, using a sliding window, we can calculate correlation of stock A and B’s most recent X tick prices every Y minutes . Under normal market condition, A and B should be positively correlated, which means their correlation should be close to 1. If for any reason their correlation becomes negative and falls below a threshold, it is our trading signal to open a position to long stock A and short stock B. We will continue to monitor A and B until they become positively correlated again, then we can close the position. As in any long/short trading strategy the pair can also move in adverse ways. Strict risk management triggers are put in place to prevent unconstrained loss, but that’s tangential to the focus of this post.
Our Dataflow based trading pipeline looks like this
Apache Beam SDK allows us to construct a data pipeline on gCloud Dataflow using a unified programming model. At the heart of it is the concept of PCollection. PCollection is an immutable container of data sets. It is the input and output of every transformer in the pipeline. It can be bounded which means the size of the data set is known, e.g. file IO from a text file, jdbc IO from a database query. It can also be unbounded, which is the case with streaming data, e.g. Kafka IO from messages off of a queue.
Beam SDK also offers several generic Transforms that work with PCollection. It’s based on the Map/Reduce programing model. In this post we will use
Now let’s take a look at some code.
First we defined our universe of stock symbols and pairwise combination. Let’s initialize our Beam pipeline instance
Then for each stock we have tick market data streamed in via gCloud Pub/Sub on topics in the form of “input_<symbol>”. Each tick data is just a tuple of timestamp and price. We then construct the pipeline by applying transforms one after another.
There is a lot to unpack here. Let’s go through one pipe at a time.
2. We then apply a Map transform and a Filter to decode the bytes into strings and filter out invalid data, if any
3. Then we apply a ParDo transform to add timestamps to each data point. The timestamp is the same as the timestamp of the tick data. This is necessary to make sliding windowing work in an unbounded PCollection. The AddTimestampDoFn extends beam.DoFn and overrides process function.
4. Then we apply a sliding window to the data stream. The sliding window is calculated every minute for 10 minutes worth of tick data for each symbol. This windowing affects any grouping by key operations downstream in the pipeline
5. Now comes the interest part. We apply another ParDo to pair the individual tick data stream up and output multiple streams of paired data. Our ParDo function has the same template as the add timestamp ParDo in step 3. Note in the output streams, the key is no longer individual stock symbol, but a pair of symbols
6. Now that we have paired up our input streams, we can group the values by the pair key so that all the market data for each symbol is in the value list. In this case our key is one of (‘goog’, ‘aapl’), (‘aapl’, ‘fb’) or (‘goog’, ‘fb’). The values are the iterator of tick data for the pair.
7. Now we are ready to calculate pairwise correlation in the sliding window. We construct Pandas Series out of the tick data and calculate correlation for each pair. The output is the pair key and correlation coefficient tuple.
8. The last step in the pipeline is to add a Filter for the pair that has negative correlation below a threshold and output that pair as a trading signal onto gCloud Pub/Sub. Again we use type hints to ensure Python uses binary type to publish data onto Pub/Sub.
Now if we want to run our awesome quant trading pipeline in Google Cloud Dataflow, we can follow this Quickstart to set up our GCP project, authentication and storage. Then all we need is to run the pipeline on Dataflow using this command
In conclusion, Google Cloud Dataflow together with Apache Beam offers an expressive API for building data pipelines and transforms. Apache Beam programming model has some learning curve in terms of getting used to the transforms and windowing semantics. It forces users to think through each stage of the data pipeline and the input/output data types, which is a good thing in our opinion. Google Cloud manages a lot of magic under the hood in terms of parallelize processing, distributing units of work and auto-scale. Dataflow integrates seamlessly with other GCP products like BigQuery, Pub/Sub and Machine Learning, which makes the ecosystem very powerful for big data analytics.
As always you can find the full code discussed in this post on Cloudbox Labs github.
Google Cloud community articles and blogs
552 
7
552 claps
552 
7
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Software engineer, data infrastructure, blog @CloudboxLabs.com
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/firebase-developers/patterns-for-security-with-firebase-offload-client-work-to-cloud-functions-7c420710f07?source=search_post---------163,"There are currently no responses for this story.
Be the first to respond.
One of the great advantages to working with a product suite like Firebase is its ability to provide robust backend services to your app, while minimizing the hassle of running your own backend servers. In fact, many types of apps can avoid the need to write any sort of backend code. This is because Firebase provides security rules for Cloud Firestore, Cloud Storage, and Realtime Database that work in tandem with Firebase Authentication, to help you enforce which authenticated users can read and write what data in your app.
However, for apps that place a value on security, sometimes security rules aren’t enough to provide maximal protection. One of the tenets of highly secure systems is called the principle of least privilege, which says that a user’s access to a system should include only those privileges which are essential to the tasks they’ve been given. What does that mean for an app that uses Firebase? Well, that depends on 1) what operations the storage system allows you to control, and 2) what your app intends for the user to do with those operations.
Let’s take Cloud Firestore as our storage system. Firebase gives direct access to Firestore from an app using the Firebase SDK, and that access is protected by security rules that you deploy to your project. As I mentioned before, you can control read and write access, but it gets more involved than that. Write access breaks down in to create, update, and delete operations. And read access breaks down into get and list. These are called granular operations (video tutorial), and you’ll want to make use of them as much as possible in order to give the user “least privilege” to the data they work with.
Here are a couple common requirements for an app:
I’ll use web client code samples here to illustrate the first task (other mobile client platforms will be similar):
Notice that the initial contents of the document include a timestamp field with the server timestamp for the moment the document was created, and a number of initial credits (if, say, this were a game with some form of currency).
Before I go any further, I should point out that no client code is safe from tampering. App developers should make the assumption that their code could be changed in some way, or maybe not even run at all.
Without any protection, the code above could be modified to lie about the date of addition, or worse, lie about the number of initial credits. I don’t know about you, but I’d really like the data in that document to be accurate, because I might want to query for users accounts who were created around a certain date range, and I definitely don’t want to give away free credits in the game! But I do want the user to be able to read this document so they can see how many credits remain.
With Firebase security rules, it’s possible to check both of those values before they actually get added:
These rules are saying that:
This is great! The granular permissions for create and get really help out here, because they enforce that the user has to create the document with specific fields, and they can’t be updated or deleted later.
But there are a couple problems. If I ever want to change the number of initial credits, I have to do it in two places simultaneously: both the code and the rule. This isn’t really possible to do smoothly, since not all clients can possibly load and use the code update at the exact same time that the new rules take effect. Also, if I want to change the initial credits based on some condition (e.g. day of signup, use of a promo code), that might be be difficult or impossible to express in security rules.
So, what’s a better way to do this?
Sometimes code is best deployed to a backend you control, so you can update it at will, and it can’t be tampered with by malicious clients. For the above case, Cloud Functions is a great fit, because you can take advantage of an authentication trigger that automatically runs when a user account is created in Firebase Authentication.
Here’s the code to create that document in an auth trigger (I use TypeScript, but if you prefer JavaScript, it should be easy to adapt):
With this function deployed to my project using the Firebase CLI, whenever a user account is created, the new document will be created with exactly the properties I choose. And, if I want, I could do extra work to determine how many initial credits the user should start with, and deploy that logic immediately, whenever I want.
This is even better now, but there’s a new kind of a problem lurking here. Since the client isn’t creating the document, how does it know when it’s been created, so it can start using it? This function fires asynchronously, some time after the account is created, and the client code that creates the account doesn’t wait for it to execute. The call to createUserWithEmailAndPassword will finish immediately when the account is created and not wait for the document.
The client can still build a reference to the document, since it’s based on the known account UID. But it would have to poll that document periodically until it finally exists. Polling is generally an undesirable option if it can be avoided. Fortunately we have a much better option.
The best solution here is to have the client listen for realtime updates to that document. Now you might be concerned if it’s an error to attempt to listen to a document that doesn’t exist. After all, the client will almost certainly start listening to the document before the Cloud Function creates it. Fortunately, this isn’t a problem at all. A Cloud Firestore client can listen to documents and queries that yield no data initially, but the listener will be triggered as soon as a matching document is available. This can be used on the client to know when all the work of the function is complete, so it can advance its UI and start using that new data.
Here’s the same client code from before, but updated to use a listener to know immediately when the document is created:
Note here that a new promise is created to track the document. userDocRef has a listener attached, and as soon as there’s an document available, the promise becomes resolved with the contents of the document. It’s important to remember that onSnapshot returns an unsubscribe function that should be called when the listener is no longer needed.
The security rules for this now reduce down to simply:
This ensures that a per-user document can only be read by that same user when signed into the app. There are no rules for document creation required, because backend SDKs used with Cloud Functions always bypass security rules completely. Rules only apply to access from mobile and web clients, and use of the Firestore REST API when provided a Firebase Authentication token.
The system diagram for this process looks like this:
If you don’t want that document hanging around after the user account is deleted, you can use an onDelete auth trigger as well:
This is especially important if your app needs to be GDPR compliant.
For a complete, runnable web page and source code that illustrates this in action, visit this gist to see the HTML, JavaScript, Cloud Functions, and security rules code.
Be mindful of application security! Always use Firebase security rules to minimize what a user can do in your app. For application logic that absolutely must not be tampered with, push that to Cloud Functions or some other backend you control.
Tutorials, deep-dives, and random musings from Firebase…
872 
5
872 claps
872 
5
Tutorials, deep-dives, and random musings from Firebase developers all around the world. Views expressed are those of the authors and don’t necessarily reflect those of Firebase or its parent companies.
Written by
firebase-consultant.com, Firebase GDE, engineer, developer advocate, Xoogler
Tutorials, deep-dives, and random musings from Firebase developers all around the world. Views expressed are those of the authors and don’t necessarily reflect those of Firebase or its parent companies.
"
https://medium.com/@coffeeandcloud/angular-clean-architecture-approach-fcfe32e983a5?source=search_post---------164,"Sign in
There are currently no responses for this story.
Be the first to respond.
Coffee & Cloud ☕️☁️
Dec 18, 2018·7 min read
If you are familiar with writing Angular applications, then you know that from early versions the framework supports you in creating and organizing your source files. This was more and more improved as the versions were going on. So why the heck should you care about using a more common architecture pattern for your web apps? Let’s first explain what all the rumors on clean architecture are about. If you’re already familiar with Clean Architecture, you may skip directly to the hands-on article.
In short: Clean Architecture is a pattern for structuring an application architecture in general. It was originally introduced by Robert Martin, also known as Uncle Bob, who also published a lot of articles and books about writing clean and reusable code. His last book, Clean Architecture, sums up his thoughts and suggestions on how to improve your application’s structure on a higher level then clean code.
He researched about common architecture design and found out, that most popular architecture principles (e.g., just to name a few: Hexagonal Architecture, Onion Architecture or Screaming Architecture) have one main goal: Building some kind of application layers and creating a separation of concerns in this way. He also saw that every architecture tries at least to separate the business logic from the rest of the application and that these architectures produce systems that are
For detailed information, you can refer to his article on 8thlight [1]. So what does this mean for our web app development with Angular? Let’s try to apply these points to our beloved framework:
If you already read about building applications with the Clean Architecture pattern, then you should have definitely seen the following diagram that explains the general structure.
Just to sum up the basic concepts, the spirit of clean architecture is based on building application layers which and shape, as already mentioned, a separation of concerns. Furthermore, there are some rules on how these layers should interact with each other:
Because the Clean Architecture diagram and the rules are very general, let’s try to break it down and put it more in our context of web application development. At first, I like to straighten out, that Clean Architecture is a general pattern which is supposed to work on different platforms and ranges from backend over mobile up to web applications. That means, there is no best way how to implement it. The shown approach in this article series was primarily inspired by an article on Speakerdeck by Marko Milos [2].
The best way to map the layer principle into a project and code structure is by using some kind of packages and bundle all layer related classes and interfaces together. Remember, only higher layers are allowed to access lower layers, not vice versa. To guaranty the interoperability between the layers, it is also a good idea to specify interfaces (e.g. repositories) in a very low layer.
Most web applications are some kind of CRUD applications, thus we will focus on CRUD repositories/APIs in this series. So let’s assume we have the following package layers:
So far so good, we know now about how our project can be structured. One last thing to mention is the importance of data objects over the whole architecture: Since our core package abstracts the business logic through usecases and entities, each application layer has to handle these. But the data objects on the data or presentation layer are mostly not the same which are used in the business logic. This is because APIs often provide more information, data repositories need some mapping functionality for object members or presentation data objects need some more fields to display the data as needed. To deal with this, it’s highly recommended to make use of layer-specific data objects and map these from and to the core entities when transferring data from usecases to repositories or presentation layers.
The next article shows in a hands-on manner how this theoretical approach will look like in practice.
Junior Fullstack Developer on his journey 🌎 currently in love with progressive web apps, flutter and micronaut 🤗
See all (21)
962 
2
962 claps
962 
2
Junior Fullstack Developer on his journey 🌎 currently in love with progressive web apps, flutter and micronaut 🤗
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/yale-sandbox/react-native-push-notifications-with-firebase-cloud-functions-74b832d45386?source=search_post---------165,"There are currently no responses for this story.
Be the first to respond.
Top highlight
Google’s new Cloud Functions service for Firebase is a huge addition to their offerings, and I’ve been focused on utilizing it for one feature in particular: serverless push notifications. I started working on a basic social networking app a few weeks ago, and from the onset implementing notifications was a definitive goal (quite honestly it was the purpose of the project). However, I did not want to go through the trouble of setting up a server in order to listen to my Firebase data. Luckily for me, with some basic knowledge of how to use Firebase SDKs, push notifications became quick and easy.
Setting up push notifications — even with this implementation — can be daunting, especially given the amount of set-up that is required. To follow my implementation, three Firebase services must be utilized:
Unfortunately, Google’s documentation for React Native users is fairly limited, and even the most popular integration packages skimp on thorough instructions, and have inconsistent features. I opted to use React Native Firebase (RNF). Luckily, their installation procedure is straightforward and well described.
Install node module, then link the dependency.
Create a Firebase project in the console, download the GoogleService-Info.plist and move it to: ios/[YOUR APP NAME] so the SDK can connect to your project. Open the project with XCode to ensure the file has been added properly. If it doesn’t show up in the sidebar, just drag the file in.
Then, open AppDelegate.m and add:
As well as:
at the beginning of the didFinishLaunchingWithOptions:(NSDictionary * )launchOptions method.
Set up CocoaPods.
Generate a Podfile.
Sometimes the Podfile generates with duplicate declarations, copy and paste the following into yours if errors are reported:
Run:
If any issues are encountered, refer to here.
I’m going to assume you already have a working application built with a backend database to save time — don’t worry, there are plenty of guides on how to do this, check out the docs here. At the very least, for directed push notifications, your database must store unique push tokens for each user, and presumably some data to send them.
First, in Xcode, add the following to AppDelegate.h:
And the following to AppDelegate.m:
You will also need to enable the capabilities Push Notifications, and Remote Notifications.
To set up FCM with RNF, there are two functions that you must call after user authentication. The first is to request permissions from the user, and the second is to get the device’s unique push notification token, which returns a promise with the token. This needs to be stored somewhere in your database; I opted for the user’s document under the collection “users”. See below:
That gives the client basic FCM functionality. There’s a lot more that can be done with the SDK, and I encourage further reading so that your push notifications become more interactive. On the Firebase side, a permission key must be supplied to the console so that Google’s servers are able to connect with APNS. This key is easily generated through Apple’s developer tools. Check out Google’s guide.
Test to make sure FCM is working by sending a notification from the Firebase console. You will need to install the release scheme of your application on a real device; the simulators don’t support APNS.
Debugging:
Once FCM is working, the final step is to write a cloud function to listen for database activity — this is the coolest part. To start, install the Firebase CLI and login.
Then cd into your project directory and run:
When it prompts you to install node modules return yes. Once the script completes, in your React Native project you should now have a new folder: functions. Open the index.js file and begin writing functions!
To start, two dependencies have to be imported:
Since this function will be running on Google’s servers, you have access to all Firebase services.
Make sure to export any function that you want Google to upload, but feel free to write auxiliary functions that aren’t exported.
Key things to note:
Once the function is complete:
This uploads your function to Google’s servers.
Also, check the logs! The Cloud Functions pane provides detailed error reports:
Once it uploads without errors, try performing a write operation to the node in your database that the function is listening to, either manually or by using a simulator. If the notification doesn’t come through at first, check the logs again to make sure it’s executing properly.
If a notification appears on your device congrats! Now it’s easy to expand the feature by writing more Cloud Functions, and to implement it in other projects.
I’ll try to throw together an example/starter app for this and will link it here when it’s ready. In the meantime, I’ll do my best to respond to issues below.
more guides:
incubating technical entrepreneurs and ventures at Yale
1.4K 
12
1.4K claps
1.4K 
12
Written by
Yale College Senior studying Computer Science. I like to build things.
incubating technical entrepreneurs and ventures at Yale
Written by
Yale College Senior studying Computer Science. I like to build things.
incubating technical entrepreneurs and ventures at Yale
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@imrenagi/serverless-lambda-architecture-with-google-cloud-platform-35cb3123206b?source=search_post---------166,"Sign in
There are currently no responses for this story.
Be the first to respond.
Imre Nagi
May 29, 2018·7 min read
I’ve been thinking about writing a technical blog post since I join Traveloka about 4 months ago. However, because of so many things went around: work’s stuff, commencement stuff (YES! I finally graduate and become #CMUGrad),etc. , I barely had enough time to work on small POC and write several things about it.
This work actually was inspired by my Big Data AWS training couple months ago with AWS Solution Architects in Traveloka Jakarta office. On that training, they proudly told us that they now have full set of serverless solution for Big Data. It includes AWS Kinesis Firehose (AWS Messaging Service solution), AWS Glue (AWS Serverless Batch Processing Solution), AWS S3 (Cloud Storage), AWS Firehose Analytics (AWS Realtime Streaming SQL Solution) and their new product named AWS Athena used for running Adhoc query to data stored in AWS S3. Go to this link to read the complete explanation about those things. Then, I’m thinking, why not doing the same thing in GCP?
First thing first. If you are not familiar with Lambda Architecture, you might need to read some articles on internet about it. Here is the some key concepts cited from http://lambda-architecture.net/ about lambda architecture:
I started this project by using public streaming data available on the internet and by picking one simple user story. The decision comes to Meetup.com RSVP streaming API. Fortunately, meetup.com provides free public streaming API that we can use to get all RSVPs that have been made world-wide. This should be enough for us because we just want to create speed layer and batch layer consuming those data. Then the user story chosen is:
As an user, given the range of date, I should be able to get the number of RSVP created for every 15 minutes.
You must be asking why it is for every 15 minutes while we can actually put the data somewhere, run the query and get the data for every minutes. Don’t think too fast. This is just for simplifying my further explanation.
Techinically in lambda architecture, the speed layer and batch layer are made so that they can provide (near) realtime analytics to the business decision maker or business analysts. In the real world, running batch job is expensive in terms of money and time consumed by the application. On the other hand, business stakeholder simply can’t wait to get the current data until the next batch job runs on the cluster. However, it is worth to note that batch processing should be the source of the most accurate data a company or organization can have. What we can do to solve this dilema?
Streaming or speed layer comes to the resque. Speed layer provides us with the estimated data in (near) realtime manner. Yes! It is estimated! It’s really hard to get the accurate data by using the speed layer. On the other hand, speed layer can provide the user with the current data easily. So, what we can do to get the realtime data? The answer is simply by combining the data from the batch job and realtime streaming job. Yes we need to take the trade off.
Let me give you example. Assume that now is 8 AM in the morning and CEO meetup.com wants to know the number of RSVP have been made up until now since yesterday midnight. However, the last batch job run 12 AM on last night so we clearly dont have accurate data from 12 AM until 8 AM. In this case, we need to combine the accurate data from the last batch job with the estimated data from the straming job runs from 12 AM until 8AM. Once we run the next batch job and get the accurate data for today, we can simply rewrite the result written by the streaming layer in the serving layer. The idea is simple, right? Don’t forget that we sacrifice the accuracy of the data a bit in this case, but to save the costs and for faster data driven decision, it is worth investment, IMHO.
I know that you have been waiting for the buzz words. Here we go.
As visually described on the diagram above, we can breakdown the component into several parts:
This is a simple java application I wrote to pull the data from RSVP Streaming API of meetup.com and push the data to GCP Pubsub. This application runs in Kubernetes Pod and deployed in Google Container Engine. For more detail, check in on my github in event-delivery/MeetupRSVPPublisher.java.
Google Cloud Pubsub is a centralized messaging system like Apache Kafka, RabitMQ, etc. If you are familiar with Topic and Consumer Group concept in Apache Kafka, it will be easier for you to understand the concept owned by Cloud Pubsub. It has topics (equivalent to Kafka’s Topic) and Subscription (equivalent to Kafka’s Consumer Group).
In Cloud Pubsub, all consumer should pull the message from the subscription instead of directly to the topic partition like what Kafka does. Once the subscription is created, the message will start flowing from the Pubsub and are ready to be consumed by the consumer subscribing the pubsub subscriptions.
In this case, Pubsub layer is not part of Lambda Architecture. However, to help getting clearer picture and creating scalable architecture, Cloud Pubsub has a very important role to achieve it.
From the data above, it is easy to see that the rate of published message and consumed message is not that huge. Pubsub only received about 3 messages per seconds and get about 3 pull operations per seconds.
Cloud Dataflow is used as the streaming engine in our implementation of speed layer. There are two responsibilities of the speed layer in our use case. First is to write the data pulled from the Pubsub to Google Cloud Storage by using TextIO so that the Batch layer can consume these data later and run batch processing on top of it. The second is to aggregate the number of RSVP comes to the system for every 15 minutes window. Once it gets the number, it will store the result to Google NoSQL technology named Cloud Bigtable by using BigtableIO. We can also dump the data to Google BigQuery by using BigqueryIO, however we don’t really need it in this use case.
You can go to the streamprocessor/RSVPStreamPipeline.java to see what is happening. :D
The DAG is pretty simple. First, rsvpParser used to serialize String given by Pubsub to Java Object. Then, every object parsed will be grouped by using 15 minutes fixed window in rsvpGroupWindow . In order to group the RSVP, I use rsvpKeyify and rsvpGroupByKey to give every RSVP a key representing the time window of its arrival timestamp. Then to aggregate the number of RSVP within the same fixed window, I used rsvpReducer to simply accumulate the count. Then transformation to a Hbase Put object is done and then the result is stored in our CloudBigtable using BigtableIO plugin.
Dataflow jon running for batch processing is not that different with the one for batch processing. The different is only the data source used, which is the Google Cloud Storage. Other than that, the batch processing as we know are not the long running process. It only runs once in a particular range of time. For instance once a day, a week or even a month.
To get the full picture of the code, you can take a look in streamprocessor/RSVPBatchPipeline.java.
We simply decided to use NoSQL technology for the serving layer provided by GCP called Bigtable.
In designing NoSQL schema, we need to think about how we gonna query the data. Since we want to query the data based on its date, we can simply use date yyyyMMdd as the partition key of the table. To get the data for every 15 minutes, we can create a column family called count containing many columns for storing the count for every 15 minutes. The way I do it is by using string 0000, 0015, 0030, 0100 and so on as column name to represents the time window of 15 minutes. By using this schema design, we can get additional benefits if:
Both of batch and speed layer will write to the same partition and to the column name and family. The speed layer will write the estimation count and the batch layer will write the corrected count of the data.
Once the data are stored in the bigtable, the other application such as backend RESTful API will be able to read the data and exposed it to the outside world.
All works mentioned in this blog post are made available in my github repository. Feel free to take a look, submit issues or even submit Pull Request for any kind of advancement.
Google Developer Expert, Cloud Platform Engineer @gojek
513 
513 
513 
Google Developer Expert, Cloud Platform Engineer @gojek
"
https://towardsdatascience.com/journey-into-the-clouds-how-to-become-a-aws-certified-solution-architect-719cae167e70?source=search_post---------167,"Sign in
There are currently no responses for this story.
Be the first to respond.
Bishr Tabbaa
Feb 12, 2020·6 min read
I have been interested in expanding my knowledge as an IT solution architect from applications to infrastructure, so I decided to prepare for the Amazon Web Services Certified Solution Architect Associate exam which is one of the most important and widely recognized certificates in the IT industry*. I wanted to share my methodology for successfully preparing for the exam and thereby assist others on this journey.
Examining the enterprise IT industry trends for the last 10 years, there are high-growth areas that continue to expand and reward engineering talent: Cloud Infrastructure, Data Analytics, and Security. Although one can enter either the Data Analytics and Security domains through certification, the financial and professional rewards are not significant unless one possesses years of experience and especially in Data Science, a minimum of a two-year master’s degree in STEM preferably with a specialization in Applied Mathematics, Computer Science, or Statistics. Cloud Infrastructure has a flatter incline and less costly learning curve thanks to the excellent, public documentation for the top platform vendors and the panoply of educational resources I will share below.
Furthermore, consider the inevitable progress of cloud computing as traditional data centers based on legacy hardware and software ebb due to their capex basis, inflexibility, and technology stagnation. As Gartner’s 2019 report suggests, organizations of all sizes are migrating workloads to the cloud. Data Center spending slowed from a 6% increase in 2017 to less than 2% in 2019; server purchases declined 1% in 2019 and are projected to further decline 3% annually for the next five years. Meanwhile, spending on IT services including cloud computing is growing twice as fast rising from 4% in 2017 to almost 5% in 2019.
Moreover, one must assess the cloud computing vendors to know where to place bets professionally. There are the Big Three and frankly everyone else: Amazon Web Services (AWS), Microsoft Azure, and Google Cloud Platform are the undisputed leaders in this category as of February 2020. AWS launched in 2006 and has had a major head start over its competitors such that AWS has 33% market share in 2019 and is growing more than 35% annually. Compare that success to Azure born in 2010 which has now reached 18% market share and to GCP which began in 2013 and has achieved 8% market share. What’s more there have been reports this year of leaked Google memo’s suggesting that Alphabet is considering a possible exit from the cloud platform space by 2024 if it cannot reach #1 or #2 in the market.
Finally, Solution Architecture represents an area of the IT labor market that is best suited to adaptation to the constant change that underlies the IT industry. The devil will always be in the details, however Architecture is a conceptual and logical activity that is aligned with the business, and cloud computing enables one to better reason about and assemble the computational, storage, data, and networking legos to design better distributed systems that can solve business problems. Putting all of this together, focusing on Solution Architecture for the top cloud platform (eg AWS) not only makes good business and common sense, it can be enjoyable since you are making a strategic impact on organizations and interact with a variety of stakeholders. It is not surprising then that the salaries for Cloud Solution Architects are comparable to and the jobs more plentiful than the other aforementioned domains.
There is no substitute for getting your hands dirty on real-world projects at work and you will get depth in some areas; however, many of the AWS service user guides have solid 10 minute tutorials and semi-realistic samples that will broaden your knowledge and expand your comfort zone. This step will be ongoing and may take a year which is approximately the minimum amount of experience before you can sit for the AWS exam and happens to be the length of time that the free tier of AWS services can last for an individual or corporate account.
Read the FAQs for RDS, EC2, Route 53, SQS, and VPC. Read the whitepapers about the AWS Well-Architected Framework and its pillars; then make sure you can design technology solutions that balance and satisfy their cost, performance, reliability, operations, and security constraints. Take handwritten notes and draw diagrams to strengthen your understanding of the material and construct a robust mental model of how AWS components fit together. Take the free AWS Digital Training video series. This step takes one (1) month.
Half a million people have taken Ryan Kroonenburg’s Solution Architect Associate course at Udemy. I highly recommend this course because Ryan is an excellent communicator, the A Cloud Guru material is current and thorough, and the two practice tests that simulate the AWS exam are well worth the price of the course itself which can range from $11.99 to $149.99 depending on the ongoing Udemy discounts and promotions. This step takes one (1) month.
The crib notes and cheat sheets published by Tutorials Dojo and Digital Cloud Training are outstanding and combine remarkable breadth and depth. They cover all the major AWS services that show up on the exam and simplify the review process during exam preparation. Take additional handwritten notes from these crib notes to supplement the earlier AWS material and further reinforce your memory. This step takes two (2) weeks.
I recommend taking at least 3 full-length practice tests before sitting to take the real exam. Ryan’s Udemy course has two such exams. You can also purchase an official practice exam from AWS itself for about $20. The best set of practice exams though is Jon Bonson’s suite of 6 practice exams hosted on Udemy which can be purchased for $12.99-$39.99. Both Ryan and Jon’s exams contain detailed explanations for the correct and incorrect answers, and their test reports help identify your strengths and weaknesses. This step takes two (2) weeks. You should also sign up for the exam once you are regularly getting scores above 75%.
In the final week before the official exam, I suggest taking another practice exam, reviewing your notes one more time, getting exercise and rest, and relaxing with family and friends so that you are full of confidence when you sit for the exam.
There are no shortcuts to success in Life. If you can invest at least three (3) months and roughly $80 for the two Udemy courses and the AWS practice exam, rest assured that that you will pass the AWS Solution Architect Associate exam as I did. I hope this article helps others in the IT community reach their professional goals. Good luck and let me know what you think!
Enjoy the article? Follow me on Medium and Twitter for more updates.
Solutions Architect @awscloud • Author • Board Director • AWS • Data Science • History of System Failure • Markets • @RiceAlumni • Tennis🎾☁️🖥️🧮
478 
2
Thanks to Yenson Lau. 
Every Thursday, the Variable delivers the very best of Towards Data Science: from hands-on tutorials and cutting-edge research to original features you don't want to miss. Take a look.
478 claps
478 
2
Your home for data science. A Medium publication sharing concepts, ideas and codes.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/thinking-design/the-importance-of-data-in-design-a536b2016590?source=search_post---------168,"There are currently no responses for this story.
Be the first to respond.
Top highlight
The face of UX is changing. Just a few years ago product decisions were often based on a designer’s wants and intuition. Today, they’re more often based on feedback and information provided by users. In a world where analytics rules, design is becoming data-driven. Data forms the cornerstone of our product development process; it can quickly inform development priorities for enhanced user experience, improved user satisfaction and increased adoption rates.
In order to keep up with the times, your designs need to be based on data. In this article, you’ll learn how to include data as a core component of your design process.
Steve Jobs famously said, “It’s really hard to design products by focus groups. A lot of times, people don’t know what they want until you show it to them.” So why not follow his advice, hire a top designer team and let them make all the decisions?
There are a couple problems with this approach. First, Steve Jobs wasn’t a designer, he was an inventor. He was strongly focused on creating new product categories and revolutionary technologies. While his approach might be good for inventors, it’s hardly applicable to the regular product designers because when you create a new product in an existing product category, your users will have a great sense of what they want and need.
Second, even if you’ve hired the best designers in the world, they can’t predict what your users want simply because designers aren’t users(except of course if they’re designing a product for designers, but that’s a rare exception).In our industry, the gap in knowledge between a designer and user is huge, and it’s simply wrong to think that designers understand exactly what users want and need without any proper user research, or without any real testing or data.
Thus, designers cannot make decisions based simply on what they think. Designers have to engage users to gain insights so they can effectively tailor user experiences.To build user-focused experiences, designers need to use a data-driven approach.
Data-driven design sounds like a great buzzword, but what does it really mean? Basically, data-driven design lets data drive many of the decisions made about design. The goal of data-driven design is to develop a better understanding of everyday experience.
Before getting straight into a data-driven design, we have to make sure we understand what counts as data. There are two types of data:
Most analytics tools provide a lot of quantitative data.
For example, in the context of a website, Google Analytics can tell you how many visitors has come to your website, how they got there and what actions they took. What this data doesn’t tell you is why. Why does a certain group of users take one action, while a different group prefer another? Why is one piece of content more interesting for your users than another? The ‘why’ aspect is of the highest importance because it opens the mystery of user logic.That’s when we turn to qualitative data.
Absolutely not! In fact, you should never trust just one type of data. Use both together to really understand your product’s usage patterns. Armed with quantitative and qualitative data, you can make a more informed decision; quantitative data tells you a current status, qualitative data gives perspective. For example, you can look at quantitative usage data to see how your product is actually being used. For features that aren’t being used as much as you expect, you need to dig deeper with qualitative feedback to understand the problem. Data will help you in formulating a hypothesis about a problem and how to solve it.
To use data to inform design in a meaningful way, we need to connect the dots between data and design improvements to use data to identify your specific users’ desires, problems and needs.
A data-driven design strategy is based on 3 things:
Data on its own is fairly useless. In order to create meaningful product experiences you need to analyse data to turn raw data into meaningful information and insights designers can use.
As noted in the article 6 myths of data driven design, if you are using data to inform design decisions, you have three ways to look at things:
Also it’s very important to be able to quickly identify and connect the most important data for use in analytics. The following guidelines should help:
It’s no surprise that the ability to empathize — to step outside of yourself and see the world as other people do — is a core to designing a meaningful product.
It’s important to understand that a data is generated by and about people, not machines. Data should represent the traces of human behaviour. Thus, seek human stories to give meaning to data:
When we understand the user’s perspective, we make products that are better suited to our users. One good example of using empathy in the design process is Tesla. Tesla has done things to package up its technologies in a way that is both new and interesting, but also familiar at the same time. The design intent behind the Model S is clear — create a “strangely familiar” product. If customers want to drive an adoption of electric cars they need them look like a traditional car. This makes the product feel more friendly.
Empathy can be built up by watching and interviewing your users, analyzing surveys and using more traditional research methods, such as diary studies and usability testing. Your ultimate goal should be set on creating a product people love.
When we think about data in terms of design and innovation we should think of data as something that helps guide our decisions of what to do next, but at the same time, we shouldn’t let data decide for us. There are a couple issues with following a purely data-driven approach. First, metrics are limited because they’re based on what you’ve already built (all available data is based on your current audience and how your current product behaves). Taking this into account, you understand that you only have a small subset of the information that you need to build a successful product. Second, no amount of data or empathy will replace the fact that a designer needs to make decisions on how to interpret data. This means that you as a designer need to be a curator of what is meaningful and what’s not. You need to have a vision for what you are trying to do. Vision is achievable. It’s built up over time through experience: by making decisions, by making mistakes and by learning along the way. Use data to validate and help you navigate your vision.
The danger in any product design environment is when designers rely on one part of a strategy too heavily and end up optimizing for the wrong thing. As in all things in the real world, there needs to be a balance. Good product design comes from finding the right balance between data, empathy, and vision.
The world is changing. Just a few years ago it would be difficult to imagine that we’d know the impact that an experience would have on the market before we shipped it. Today, this is a reality, and many product-based companies are building experiences this way. As designers, we have a mass of opportunities in front of us to fundamentally rethink how we work with data, and how we drive meaning and insights from it. We should use these opportunities to make better design decisions, ultimately creating better products for our users.
Nick Babich is a developer, tech enthusiast, and UX lover. He has spent the last 10 years working in the software industry with a specialized focus on development. He counts advertising, psychology, and cinema among his myriad interests.
Originally published at blogs.adobe.com.
Learn about Adobe XD, our all-in-one design and prototyping tool:
Stories and insights from the design community.
235 
1
235 claps
235 
1
Written by
New Tools for New Creatives. Get all the latest creative apps plus seamless ways to share and collaborate. All right on your desktop.
Stories and insights from the design community.
Written by
New Tools for New Creatives. Get all the latest creative apps plus seamless ways to share and collaborate. All right on your desktop.
Stories and insights from the design community.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/young-coder/could-microsoft-azure-actually-win-the-cloud-18c78b8780fe?source=search_post---------169,"There are currently no responses for this story.
Be the first to respond.
Being the first mover in a new field is a calculated risk. If you get it right (think Apple’s iPhone), you can own the market for years. But if you misjudge the technical challenges or your customers’ needs, you’ll end up with an expensive flop (think Apple Newton).
"
https://codeburst.io/how-to-build-a-command-line-app-in-node-js-using-typescript-google-cloud-functions-and-firebase-4c13b1699a27?source=search_post---------170,"Scotch.io taught me much of what I know in web development. Earlier this year, I came across an article “Build An Interactive Command-Line Application with Node.js” by Rowland Ekemezie. I was struck by the amount of knowledge I got from the article. I came to understand how much of these cli-apps like angular-cli, create-react-app, yeoman, npm works, So I decided to replicate his work and add more technologies to it here.
In this article, we are going to build a command-line contact management system in Node.js using TypeScript, Google Cloud Functions and Firebase.
We will use commander.js for command line interfaces, inquirer.js for gathering input, Node.js as the core framework, Google Cloud Functions as FaaS (Function as a Service) that executes our functions and Firebase for data persistence.
Make sure you have Node.js version ≥ 6. Let’s create a project directory and initialize it as a Node app.
As stated earlier we will be using TypeScript instead of the normal JavaScript. TypeScript is a strict syntactical superset of JavaScript and adds optional static typing to the language.
As you can see, we installed typescript and also installed ts-node globally. ts-node is an executable, which allows TypeScript to be run seamlessly in a Node.js environment.
As you can see with ts-node, we can actually run TypeScript files (*.ts) without first compiling it to plain JavaScript, then use node contact.js to run the compiled file.
The presence of a tsconfig.json file in a directory indicates that the directory is the root of a TypeScript project. The tsconfig.json file specifies the root files and the compiler options required to compile the project. A project is compiled in one of the following ways:
When input files are specified on the command line, tsconfig.json files are ignored.
Make your tsconfig.json look like this
We will need various Node modules to achieve our goal.
After the commands above are done, our package.json will look like this.
Now, TypeScript is now configured in our Node.js app. Let's create our project files.
The project directory contact-manager should look like this.
Going back to our tsconfig.json, you will see that we targeted the es5, es6, and es8 in the lib property of our tsconfig.json file.
We need to configure our TS to use the ES2107 library, and since ES6 and ES8 are not yet well supported by all browsers we definitely want a polyfill. core-js does the job for us. We installed core-js and its @types/core-js earlier on, so we import the module core-js before our app is loaded. Let's put the following line in our polyfills.ts file.
contact.ts is the entry point of our app, so we open it up and import our polyfills.ts file.
Now, we are set to use any ES8 or ES6 features. Before, we define our app logic. Let's first setup our Cloud Functions and Firebase.
Firebase Cloud Functions run in a hosted, private, and scalable Node.js environment where you can run JavaScript code. You simply create reactive functions that trigger whenever an event occurs. Cloud functions are available for both Google Cloud Platform and Firebase (they were built on top of Google Cloud Functions).
Here, we will be using the HTTP trigger. Visit Google Cloud Platform to read more about Cloud Function triggers. Before we begin creating Cloud Functions, we have to install the Firebase tools.
To begin to use Cloud Functions, we need the Firebase CLI (command-line interface) installed from npm. If you already have Node set up on your machine, you can install Cloud Functions with:
npm install -g firebase-tools
This command will install the Firebase CLI globally along with any necessary Node.js dependencies.
Let’s create a folder that will hold our Cloud Functions.
To initialize your project:
After these commands complete successfully, your project structure looks like this:
Now, our Cloud Functions are set. They are written in plain JavaScript but we want to write it in TypeScript, then compile to JavaScript before deploying it to the Cloud.
Rename index.js to index.ts, then move into the functions folder.
Install typescript
Create tsconfig.json
Make it look like this
Open package.json and modify the scripts tag section
We need two node modules: Cloud Functions and Admin SDK modules (these modules are already installed for us). So go to the index.ts and require these modules, and then initialize an admin app instance.
Now that the required modules for our project have been imported and initialized, let’s write our Cloud Functions code. As stated earlier, we are going to write functions that will be fired when an HTTP event occurs. We are going to write functions that will handle adding, updating, deleting and listing contacts.
Lets create the barebones of the following functions listed above
In the code above, each of the functions will execute when the corresponding names are called using cURL, an HTTP request or a URL request from your browser. Let’s try out a basic Cloud Function to see how it works.
Open file index.ts and insert the following implementation:
This is the most basic form of a Firebase Cloud Function implementation based on an HTTP trigger. The Cloud Function is implemented by calling the functions.https.onRequest method and handing over as the first parameter the function which should be registered for the HTTP trigger.
The function which is registered is very easy and consists of one line of code:
Here the Response object is used to send a text string back to the browser so that the user gets a response and is able to see that the Cloud Function is working.
To try out the function we now need to deploy our project to Firebase.
npm run deploy
Note: The above compiles the index.ts to index.js, then deploys the JavaScript file ‘index.js’.
The deployment is started and you should receive the following response:
If the deployment has been completed successfully and you get back the function URL which now can be used to trigger the execution of the Cloud Function. Just copy and paste the URL into the browser and you should see the following output:
Note: Google Cloud Functions is a Node.js environment, that means you can run npm install --save package_name and use whatever package you want in your functions.
If you’re opening up the current Firebase project in the back-end and click on the link Functions you should be able to see the deployed helloWorld function in the Dashboard:
Let’s add some flesh to our functions.
Wow… We did a whole lot of things here. If you noticed, Express was brought into the play to handle RESTful requests. This is possible because as stated earlier Google Cloud Function is like a Docker container with a Node.js environment.
Let’s deploy our Cloud Function. Run this command for deployment:
npm run deploy
We are done with our Cloud Functions. Let’s move back into the contact-manager folder.
In this section, we define our controller functions that handles user input and calls the corresponding Cloud Function.
Basically, we defined the URL of our Cloud Function, which will be used based on the type of action that is to be performed. Axios is used to send request alongside the payload and to receive a response from our Cloud Function. We see here that our Cloud Function is the heart of our logic. It does the actual adding, updating, deleting, etc work and all our app does is to print the result on our console.
We need a mechanism for accepting user inputs and passing it to our controller functions defined in the step above.
Commander.js comes to the rescue. commander.js is the complete solution for node.js command-line interfaces, inspired by Ruby’s commander.
.command() Initialize a new Command.
The .action() callback is invoked when the command 'a' or 'addContact' is specified via ARGV, and the remaining arguments are applied to the function for access.
When the command arg is ‘*’, an un-matched command will be passed as the first arg, followed by the rest of ARGV remaining.
Next issue after we complete the above is how do we get user inputs. Inquirer.js solves the issue for us.
Inquirer.js strives to be an easily embeddable and beautiful command line interface for Node.js (and perhaps the ""CLI Xanadu"").
Inquirer.js should ease the process of
Note: Inquirer.js provides the user interface and the inquiry session flow. If you're searching for a full blown command line program utility, then check out commander, vorpal or args.
contact.ts
The controller functions in questions.ts are imported here. The inquirer.prompt() launches the prompt interface (inquiry session) presenting to the user the questions passed to the inquirer. It returns a Promise, answers which is passed to our controller function addContact.
Now that our tool is complete, it is time to make it executable like a regular shell command. First, let’s add a shebang at the top of contact.ts, which will tell the shell how to execute this script.
Now, let’s configure the package.json to make it executable.
We have added a new property named bin, in which we have provided the name of the command from which contact.js will be executed.
We need to compile our scripts to JavaScript, we will modify our package.json.
Run npm run build .
Now for the final step. Let’s install this script at the global level so that we can start executing it like a regular shell command.
Before executing this command, make sure you are in the same project directory. Once the installation is complete, you can test the command.
This should print all of the available options that we get after executing node contact --help. Now you are ready to present your utility to the world.
One thing to keep in mind: During development, any change you make in the project will not be visible if you simply execute the contact command with the given options. If you run which contact, you will realize that the path of contact is not the same as the project path in which you are working. To prevent this, simply run npm link in your project folder. This will automatically establish a symbolic link between the executable command and the project directory. Henceforth, whatever changes you make in the project directory will be reflected in the contact command as well.
We’ve barely scraped the surface of what’s possible with command line tooling in Node.js. As per Atwood’s Law, there are npm packages for elegantly handling standard input, managing parallel tasks, watching files, globbing, compressing, ssh, git, and almost everything else you did with Bash.
The source code for the example we built above is liberally licensed and available on Github .
If you found this useful, found a bug or have any other cool Node.js scripting tips, drop me a line on Twitter (I’m @ngArchangel).
You can find the full source code in my Github repo.
Feel free to reach out if you have any problems.
Follow me on Medium and Twitter to read more about TypeScript, JavaScript, and Angular.
Bursts of code to power through your day.
881 
5
881 claps
881 
5
Written by
I am available for gigs, full-time and part-time jobs, contract roles | 📦:kurtwanger40@gmail.com | Author of “Understanding JavaScript” https://gum.co/LikDD 📕
Bursts of code to power through your day. Web Development articles, tutorials, and news.
Written by
I am available for gigs, full-time and part-time jobs, contract roles | 📦:kurtwanger40@gmail.com | Author of “Understanding JavaScript” https://gum.co/LikDD 📕
Bursts of code to power through your day. Web Development articles, tutorials, and news.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/factory-mind/angular-cloud-firestore-step-by-step-bootstrap-tutorial-ecb96db8d071?source=search_post---------171,"There are currently no responses for this story.
Be the first to respond.
Here is the step-by-step list to test the evolution of Firebase: Cloud Firestore (beta)! 🎉
Cloud Firestore is the “evolution” of the Firebase Realtime Database (see my previous article), which captures all the extraordinary flexible and scalable database, mobile and web development features by adding interesting query creation tools (the weakest feature of…
"
https://medium.com/google-cloud/using-a-gpu-tensorflow-on-google-cloud-platform-1a2458f42b0?source=search_post---------172,"There are currently no responses for this story.
Be the first to respond.
Warning before reading this: I am very happy this blog has been useful to lots of people. Since its now over a year old some of the commands are based on older versions of software. For an easier setup you may now want to use the Google Cloud optimized compute images:
I recommend reading this blog by Viacheslav Kovalevskyi instead of continuing with this one.
This blog was written for:
Google has some nice documentation here but there were a few additional steps I needed to take. So, lets start from the beginning:
There are two ways to set the instance up: 1) you use the command-line interface that Google Cloud offers or 2) you use their incredibly friendly web-ui to help you along. Since I am a big fan of the Google Cloud web interface this is what I’ll do. Setting up a server is very simple.
When in the Google Cloud platform make sure you have created a project and then navigate to the Compute Engine. There you will be asked if you want to create a new instance and once you get the popup dialog shown here on the left you can configure the number of cores, the memory (RAM) and a little option saying “GPU”. Click on this and the additional options show up that will allow you to indicate if and how many GPU’s you want to use. I then selected Ubuntu 16.04 as a boot disk, left all the other options the same and then click Create to start the instance.
Once the instance is ready you can connect to it by either using the web-shell Google Cloud offers or by copying the gcloud command to connect from your own command line.
Now that we are in we need to install some drivers for the GPU. The GPUs on Google Cloud are all NVIDIA cards and those need to have CUDA installed. To install CUDA 8.0 I used the following commands for Ubuntu 16.04 (taken from the Google Cloud documentation):
To verify its all working properly run the command below which will show you that the GPU is recognized and setup properly.
Yay! It exists and is being recognized. We’ll also need to set some environment variables for CUDA:
NVIDIA provides the cuDNN library to optimize neural network calculations on their cards. They describe it as:
The NVIDIA CUDA® Deep Neural Network library (cuDNN) is a GPU-accelerated library of primitives for deep neural networks. cuDNN provides highly tuned implementations for standard routines such as forward and backward convolution, pooling, normalization, and activation layers. cuDNN is part of the NVIDIA Deep Learning SDK.
Deep learning researchers and framework developers worldwide rely on cuDNN for high-performance GPU acceleration. It allows them to focus on training neural networks and developing software applications rather than spending time on low-level GPU performance tuning.
Summary: they have done a lot of work to make your life easier… You will need to register for the NVIDIA Developer Program and then you can download the latest version of the software. In this case I downloaded version 5.1 for CUDA 8.0 (I just noticed a newer version 6.0 is available as well). Once downloaded move it over to the instance using SCP or via Google Cloud Storage.
Once its on the instance install it using:
So the GPU instance is running and the drivers are in place, all that is left is to get TensorFlow installed to function for the GPU. You can see Google is trying to make it super simple to get all this working because you literally need to lines to get this last step done:
Installing tensorflow-gpu ensures that it defaults to the GPU for the operations where its needed. You can still manually move certain things to the CPU whenever you want to. Lets test if it all works…
Now to test if it was all successful you can use the python code below. It assigns two variables and one operation to the cpu and another two variables and an operation to the GPU. When starting the session we are telling it via the ConfigProto to log the placement of the variables/operations and you should see it printing out on the command line where they are placed.
Thats all…
based on feedback from ChrisAMancuso I have replaced the line
with
to ensure that it installs CUDA 8.0 (CUDA 9.0 is/was not supported by TensorFlow).
Google Cloud community articles and blogs
794 
12
794 claps
794 
12
Written by
Co-founder / CTO of @orbiit_ai. Data (Scientist) junky. All views my own.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Co-founder / CTO of @orbiit_ai. Data (Scientist) junky. All views my own.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/open-graphql/effortless-real-time-graphql-api-with-serverless-business-logic-running-in-any-cloud-8585e4ed6fa3?source=search_post---------173,"There are currently no responses for this story.
Be the first to respond.
In this post, I will show you how you can create effortless real-time GraphQL API with serverless business logic and deploy it to any cloud. Sounds a bit like a click-bait title right? What is effortless anyway? Obviously, some effort is involved. You may assume that we will create your own GraphQL server if you are familiar with GraphQL or you have heard about it and always wondered, what it is and how do I start writing GraphQL servers. Also, you may assume that we will be dealing with cloud deployments, serverless functions. In a nutshell — complex stuff.
Well, Effortless is the key word here. It is super simple to set up and run your own GraphQL API in any cloud of your choice on top of existing Postgres or it’s extensions. And no we won’t be setting up servers and talking about cloud deployments. Well, maybe a tiny bit. In this blog post, I will explain most of the feature set of Hasura.io open source engine and how it brings you Real-time GraphQL API without creating your own server. We will see how to navigate through its features to give you an in-depth overview and use cases for using it even with your existing server, non-Postgres database or serverless functions.
GraphQL is not only a buzzword but widely adopted way to interact with servers which is gradually replacing REST API. In a nutshell, GraphQL is a query language for your API and specific type system in which you define your data. After you defined your data shape as well as how to retrieve this data on the server, on the client you can query, change or even subscribe to data changes by using specific query format. The data that you will receive will be in the exact same shape as you’ve requested. In this blog post I won’t go in depth describing what GraphQL is, but if you’re new to GraphQL, I’m doing a free 4 day Bootcamp that you should totally subscribe to! We will cover how to use an existing GraphQL API in React, Angular or Vue and also learn how to create our own GraphQL API in NodeJS.
As you may assume from the title of this post, here we will be talking about effortless real-time GraphQL API. Sounds like magic, right? Well, it certainly feels so with hasura.io open source GraphQL engine! Let’s dive in!
Hasura engine gives you an engine running in docker container and exists as a layer on top of new or existing Postgres database.
Yeah, that’s right. You not only can create your GraphQL API with ease from scratch but run the engine on top of your existing Postgres Database.
Since it’s running in Docker container, you basically can run it everywhere where docker can run, meaning, on Heroku, Digital Ocean, AWS, Azure, Zeit, GCP or even your local environment.
Hasura engine comes with slick UI from which you can test your queries, mutations or subscriptions by using GraphiQL tool extended with various cool addons that we will overview in a bit
So why it’s open source? That was a question that I asked Tanmai Gopal — co-founder of hasura.io.
Because it’s a part of your stack. And in today’s age you need the transparency and flexibility of an open-source component. Open-source makes it easy to migrate into and away from, improve security, flexibility to extend and features you’d like to see, engage with a community. Communities also help you ensure that your open-source product can run in different environments. Hasura is multi-cloud and multi-platform with the help of our community running it in their favourite environments and contributing information back to the project.
The first thing that you can see when you go to hasura.io is Getting started with Heroku as a free option.
This is a really quick and pretty solid setup, but for the production-grade application, you should consider using a different cloud. As you can see you can choose from the various cloud options out there, but for the sake of simplicity though let’s get started with the basic setup on Heroku free tier.
The guide will show you the magic button. When you click it the engine is deployed to Heroku along with Heroku Postgres addon.
Heroku has a concept of templates which you can deploy. So when we are running deploy to Heroku what is happening is that we are hitting this link.
https://heroku.com/deploy?template=https://github.com/hasura/graphql-engine-heroku
In fact, the template that we are deploying is: https://github.com/hasura/graphql-engine-heroku
Let’s check this template app.json file (file where Heroku configuration is defined)
This json file tells Heroku to deploy web formation in free tier size with Postgres addon.
Along with app.json we also have heroku.yml which is as simple as:
As you can see we specify that there is a Dockerfile to run. So a takeaway is that deployment to Heroku is basically a syntax sugar around deploying docker container to Heroku. For Digital Ocean, one-click deploy it’s a bit different but conforms to the same idea.
Digital ocean image is just an Ubuntu + docker + Postgres already set up
AWS and Azure are a bit more tricky to set up, but the idea is the same — running engine in Docker container and connecting it to Postgres db.
So as you’ve probably figured out, running hasura.io engine is possible everywhere where you can run Docker and Postgres. AWS, Azure, Zeit, GCP, you name it.
Let’s for example setup Hasura on local environment.
Before installing engine locally you will need Docker and Docker Compose which you can install from here:
Now let’s get docker-compose file from the following repo:
https://github.com/hasura/graphql-engine/tree/master/install-manifests
This repo contains various installation manifests required to deploy Hasura anywhere.
So to get it, create a new directory and run
Now check if containers are running:
You suppose to get something like this
As you can see engine instance is running along with Postgres db.
Now the only thing that is left is to go to http://localhost:8080/console to see
It’s possible also to run Hasura engine on top of an existing Postgres database. For that, instead of getting docker-compose as we did previously, we get docker-run.sh script from install-manifests repo and edit HASURA_GRAPHQL_DATABASE_URL variable. You can read more about it here
docs.hasura.io
Totally possible. there are a couple of awesome blog posts about using PostGIS (spatial database extender for Postgres) with Hasura
blog.hasura.io
Or using Hasura with TimeScaleDB (open source time-series database with full SQL support)
blog.hasura.io
Running on Firebase? not a problem check out firebase2graphql tool. Using mongo or any other NoSQL database? You can export JSON dump and use json2graphql tool to import your data into Postgres database through Hasura engine. Using MySQL? not a problem. You can use https://www.symmetricds.org/ to migrate from MySQL to Postgres or even use Postgres FDW to make Postgres a proxy for data in MySQL
So now we know that we can run engine locally or on any cloud of our choice and not only on top of any Postgres database but also on top of Postgres extensions such as PostGIS or open source DB such as TimeScaleDB. But we haven’t talked yet about capabilities of an engine. So let’s get back to the console that we’ve already seen when running engine locally.
The console has 4 main tabs
On top of this page, you will find your API endpoint as well as Request Headers that you need to provide if you want to access GraphQL API from the client.
As you can see in our newly created example we have only Content-Type header which is not really secure since everyone can access our API. You see a notification about it on top right corner “Secure your endpoint” that will lead you to docs explaining how to secure your endpoint.
Here is a different example of API with secured access:
Here you can see that we have X-Hasura-Access-Key header that secures our endpoint. It will also secure access to our console.
GraphiQL tab has embedded GraphiQL IDE that gives you an ability to run queries/mutations or subscriptions to test your GraphQL API from the comfort of your browser. You also can explore docs of your GraphQL schema to see the shape of data and which queries, mutations and subscriptions you can execute. Hasura engine adds additional features on top of GraphiQL.
consider the following query:
When clicking on the Analyze button you will get:
Here you can analyze how your queries are executed against the database and give an indication to yourself or your DBA how to optimize database relations to be more efficient.
This tab is sort of admin for your Postgres database. Here you can define your schema structure, tables relations, set roles and permissions and even can run your own custom SQL. We will explore Data modeling in the next section.
Remote schemas tab is a tab where you can specify URL of your custom GraphQL server for your custom business logic.
Hasura engine will do schema stitching between your hasura GraphQL API and your custom GraphQL server. So for example, if you are thinking about doing custom business logic before adding something to the database, you can write a mutation on your own GraphQL server, provide hasura engine with server URL together with additional headers for security and engine will stitch schemas.
Hasura engine uses powerful eventing system. Whenever anything is inserted, updated or deleted from the database, an event can be triggered. It’s advised to connect events to serverless functions. We will talk more about it in the next section
Now after we’ve seen how to navigate the console let’s dive a bit deeper into Data tab and understand how we can model our data as well as set table relationships
when we access our Data tab we will have an option to create tables. When creating a table you need to specify it’s columns as well as types.
Hasura also gives you helper functions. In our case, it’s gen_random_uuid() for auto-generation of the unique identifier for post id primary key. Here you need to select your primary key column or several columns.
As in any database admin, you are able to set foreign key mapping to a different table as in the following example we map authorId in posts table to an id column of authors table:
As you can also see from this example, whenever we already have tables we can modify them, browse rows, insert rows or add relationships.
The cool part is that whenever we add a table, we can access the following queries/subscriptions on our table:
As you can see they are pretty powerful one. We not only can query or subscribe to our data but also order it as well as filter it. And of course, you have delete/insert/update mutations.
By accessing the Relationship tab we will be able to build two types of relationships between tables:
When we have a foreign key set up, for example in our use case posts table authorId column points to id inauthors table, we can query posts, but we won’t be able to get nested data from the authors table as we would expect in GraphQL. For that, we need to set Object relationship
It will be also suggested in various parts of UI to create it. So you can just click Add in suggested object relationships or you can create relationship manually.
Whenever you do that, you will be able to run a query like this:
In Hasura engine we can define roles and permissions and get to a really granular level. We can, for example, allow access to a specific column only if a specific rule is met. We also can pass variables to from our custom authentication webhook and define custom access based on it. In console it looks like this:
In this example, we check if provided X-HASURA-USER-ID
Hasura engine supports various types of authentication. You can use JWT Token, your custom token or Hasura-access-key. What happens under the hood is the following: Authorization layer checks for secret key token/JWT config or webhook config.
Let’s see Heroku example:
Here you see the environment variables set up in Heroku dashboard.
For example, if we use HASURA_GRAPHQL_ACCESS_KEY then we need to provide the X-Hasura-Access-Key to be able to access API or console
You can read more about different authentication options here
So what you will use Remote schemas for? Let’s think about the following example. Let’s say you want to insert a row in a database based on some custom server validation, but you still want to have a subscription to database changes. In that case, you can create your remote schema by either writing it yourself or using one of hasura boilerplates, run it on a server of your choice and connect it by providing your custom server Graphql endpoint URL.
On your server, let’s say you have a mutation defined which resolver runs some custom logic before inserting a row into the same database, a hasura engine is connected to. So what will happen when data is inserted? GraphQL subscription from hasura engine will run as expected.
As described above hasura has a powerful concept of events. Events can be triggered not only on table operations but on column changes. Whenever an event is triggered, event data is passed to webhook URL. These webhooks are advised to be serverless functions. You can check these boilerplates for creating your serverless functions.
As you can see from this overview, hasura.io platform is really flexible, can run almost anywhere and has lots of capabilities that help you create your GraphQL API in any level of complexity and almost effortless. Also, Hasura is open source and written in Haskell and JavaScript, so all contributions are welcome. You are also welcome to Join Hasura on Discord or Follow on Twitter
Anything & Everything GraphQL
673 
4
Thanks to Tanmai Gopal. 
673 claps
673 
4
Written by
Software architect & consultant, worldwide speaker, published author, workshop instructor, https://vnovick.com
Anything & Everything GraphQL
Written by
Software architect & consultant, worldwide speaker, published author, workshop instructor, https://vnovick.com
Anything & Everything GraphQL
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/coincentral-staff/grid-computing-the-powers-of-distributed-cloud-computing-7098c11e673f?source=search_post---------174,"There are currently no responses for this story.
Be the first to respond.
Grid computing, a descendant of the cloud and big brother to distributed computing.
Think of grid computing as the intersection of two core systems of organization: cloud computing and public utilities like electricity. At this intersection, grid computing is enabling you to tap into computational resources, centralized and not. Just like you would tap into the nearby energy lines for some of those glorious electrons that we rely on.
A modern power grid will have many sources of input. Power plants, for example, contribute a lot to the power grid but burgeoning technologies, such as solar panels and windmills, are democratizing power production.
Independent and artisanal power producers can contribute to the power grid and receive compensation. In some cases, this is excess energy.
Farmers, for example, may have solar panels to generate cheaper electricity locally. However, the farmer cannot store any unused electrons for future use, so they may choose to route that surplus energy back to the energy grid, where others can use it. One person’s wasted electrons are another’s fully charged Tesla.
Grid computing is much like the electricity grid. Contributors, big and small, can add to the grid. Users can tap into the computational grid and access services independent of the contributor.
To better understand what grid computing is and its nuanced differences from distributed computing, it will be easier to first understand the barrier and limitations that grid computing is able to overcome. In other words, seeing the problems grid computing can solve will help us better understand what grid computing is.
Grid computing is a subset or extension of cloud computing. In a nutshell, cloud computing is the outsourcing of computational functions. A common cloud service, like cloud data storage from Google Drive or Dropbox, lets a customer store their data with those companies.
Someone looking to use cloud data storage chooses between providers like Google Drive, Dropbox and iCloud. The company they go with would then be their provider of cloud storage. Customer support, troubleshooting, billing, networking infrastructure, and all aspects to providing the cloud service to the customer would then come directly and solely from the company they choose.
Pretty straightforward, right? One customer, one provider. However, we are looking for the limitations of cloud computing. Where do the perks of cloud computing fall short and leave room for other organizational structures like grid computing?
Common Criticisms of Cloud Computing:
Keeping in mind the parallels that grid computing has with a public utility grid, this type of computational organization can alleviate some of the common criticisms limiting cloud computing.
Let’s look over each of these claims and examine how a grid system could be more beneficial for a user over a traditional cloud service.
Cloud Limitation #1: User resources are committed to a single symmetric multiprocessing (SMP) system.
I’ll use a really basic example to showcase this pain point. There is a neural scientist looking to crunch two data sets (Set A and Set B). These data sets are huge and she’ll need to outsource the task to a cloud service.
The cloud service will have no problem running these data sets and she happily rents one machine from them to process her datasets. Remember that her datasets are exclusive to each other and need to be processed separately.
This means that the single SMP machine she leased will run Set A followed by Set B. Her single machine is unable to process both data sets simultaneously.
No big deal though, the cloud machines she leased are heavy duty and tear through the massive data sets in less than a few hours each. Processing the data will take less time than a full nights sleep for the scientist.
Now, what happens if she needs to do the same processing but for 100 data sets. Her budget still only gives her enough funding to access one cloud SMP machine. Being a person of science, she quickly does the math and discovers that it will take nearly two weeks to process all that data!
Grid Advantage: The same scientist with two data sets (Set A and Set B) could instead tap into a grid service. Instead of the scientist renting a single SMP machine from a cloud service, she would access the computing grid and rent the necessary computational power required.
The two data sets get processed at the same time. Perhaps by two machines, each dedicated to either data set, or it could be thousands of machines each fractionally processing the data sets. Regardless, the data is being processed parallel to each other. What took six hours before in two batches, now takes three hours in a single batch.
One hundred data sets? In theory, this would still only take three hours as each data set is processed side by side.
Cloud Limitation #2: Unused computing resources sit idle and are locked into a single task until it is complete.
Expanding on the above example of a neural scientist, the cloud service she leased independently processed her datasets, one after the other.
While processing either data set, the scientist noticed her rented hardware is only operating at 80 percent of it’s capacity. The remaining 20 percent is not enough to process the second data set, instead, it sits idly waiting for the next task.
Grid Advantage: The commodification of processing power allows a single task to be conducted across multiple machines. In the case of the scientist’s datasets, a grid system could process the data in a range of combinations between machines.
For example, the two datasets are allocated to two machines in the grid, each using 80 percent of the machine they are being processed on. The remaining 20 percent wouldn’t sit idly, instead, another user of the grid captures it. This use of idle capacity is an important component of the strengths of grid computing.
Cloud Limitation #3: Relatively limited scalability
There’s no denying that the capabilities of cloud computing are exponentially larger than most localized machines. The multiple layers to the cloud stack have enabled many more participants to the entire field than ever before possible.
Furthermore, cloud computing has many scaling benefits compared to self-custodianship of these same services. So to say that cloud computing is also limited in scalability may seem a touch paradoxical.
However, relative to cloud computing, scaling on a grid is even more achievable. This is in part due to the modularity of grid computing in addition to the more efficient use of idle resources.
Grid Advantage: Regardless if you are contributing to it or using it, scaling your task in a grid computing system can be as easy as installing a grid client on additional machines.
In the case of the neural scientist, she was able to scale her needs from two data sets to 100 data sets in the same timeframe, under the same budget.
Both! Well, sort of.
In conversation, it’s pretty common to use grid and distributed interchangeably. Fundamentally, both terms refer to fairly similar concepts. They are both systems for organizing and networking computational resources.
However, if you really want to split hairs, grid computing is the overall collection of distributed networks. Grid computing itself is a distributed network of distributed networks. Meta enough for you?
This has been a very macro understanding of grid computing. In actuality, is a multifaceted system for organizing a range of dynamic and individual parts, in order to get the most out them. Each component of the computing grid is layered with complexity and utility, not unlike the multiple pieces required in a public power grid.
Similar to a public utility, how it works is a beast of its own. However, the real impact is the overall accessibility. Because, like a public utility, grid computing is increasingly becoming a plug-and-play service.
The next evolution of grid computing is likely in the blockchain. Grid computing relies on multiple stakeholders trusting each other. Already, projects like Cosmos Network are creating decentralized grid systems that foster network interoperability and leverage the powers of a grid computing network.
Originally published at coincentral.com on December 4, 2018.
Follow us on twitter @realcoincentralFollow me on twitter @marshalletaylor
Curated editorials from the friendly decentralized…
844 
1
844 claps
844 
1
Curated editorials from the friendly decentralized CoinCentral journalists.
Written by
Canadian Creative and Professional Content Creator/Marketer | Armchair Economist | Travel Enthusiast
Curated editorials from the friendly decentralized CoinCentral journalists.
"
https://towardsdatascience.com/how-i-could-achieve-the-google-cloud-certification-challenge-6f07a2993197?source=search_post---------175,"Sign in
There are currently no responses for this story.
Be the first to respond.
Antonio Cachuan
Nov 5, 2019·6 min read
After following the 12 weeks of preparation recommended by Google, I passed the exam for Associate Cloud Engineer, here is what I’ve learned that could help you.
This story began 3 months ago when I was like every day checking my Linkedin feed and I saw a post from Google Cloud about the Certification Challenge. The first time I read I was considering getting…
"
https://medium.com/swlh/10-great-courses-for-aws-google-cloud-and-azure-ec89bef8a078?source=search_post---------176,"There are currently no responses for this story.
Be the first to respond.
As 2019 comes to an end, it’s that time of year when we look to setting new goals, and focusing on what we want to learn next year.
As engineers many of those goals revolve around keeping up with every new technology and framework. In particular, technologies such as AWS, Azure and Google Cloud all…
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@a.adendrata/push-notifications-with-angular-6-firebase-cloud-massaging-dbfb5fbc0eeb?source=search_post---------177,"Sign in
There are currently no responses for this story.
Be the first to respond.
Adrian Adendrata
Jul 22, 2018·4 min read
Today ‘Push Notification’ is modern way to engage our customer via browser/ application. Push Notification is something normal in native mobile application such as iOS and Android apps, when the application is closed user still received the notifications. But, can we do the same things in our web application? This is 5 Steps to implement Push Notification in our Web Apps using Angular 6 and FCM (Firebase Cloud Messaging)
Before we begin, we know there are two kind of notification:Push API : Gives web applications the ability to receive messages pushed to them from a server, whether or not the web app is in the foreground, or even currently loaded, on a user agent.Notifications API : Allows web pages to control the display of system notifications to the end user.
And in this tutorials will implement Push API using Angular 6 and FCM (Firebase Cloud Messaging). If you are lazy to read this tutorial, you may go directly to the end of page and open our GitHub.
PreparationMake sure that you are running at least Node version 8.x and npm version 5.x by running node -v or ng -v in the terminal. Older version may produces some error. You can install Angular 6 by using tutorial from angular.io .
In this project we are using firebase and @angular/fire as library. So we need to install them first.
Step 1: creating manifest.jsonPush Notifications use Service Worker browser, which must be registered in this manifest.
After create manifest.json, we need to add manifest.json in index.html.
Step 2: creating firebase-messaging-sw.js
Service Worker allows your app to detect new messages, even after the app has been closed by the user.
You need to replace YOUR-SENDER-ID with your Sender ID. You can find out your Sender ID in Firebase Console by open Setting > Cloud Messaging.
Step 3: creating provider for application
Then, we need source code to handle user permission and handle when new message has received.
requestPermission() : Browser/ device will ask to user for permission to receive notification. After permission is granted by user, firebase will return a token that can use as reference to send notification to the browser.
updateToken(): Update token based on userId to Firebase DB. You can replace this function based what need like update to your DB via back-end service. Remember, you need to change database permission to can write new/update object.
receiveMessage() : This function will triggered when new massage has received.
Step 4: configuring your application
Before can start our application need to configure our application for environment and add new assets.
You can find your configuration in Firebase Console by open Authentication > Web Setup
Now we need to add manifest.json and firebase-messaging-sw.js in angular.json. make sure these two file included when we run or deploy the application to web server.
Step 5: Implementation
Finally our code is ready to use. You can use MessagingService in your component but i recommend you to use these method in your AppComponent (root component) because normally you only need once to use MessagingService and browser will ask the permission at the time to open your application/component via browser.
And in the HTML part, we can use pipe async and json
DEMO
run application using ng serve, after compilation is complete, open can your browser and browser will ask for the permission.
You can also hit Firebase Cloud Messaging directly using cURL.
YOUR-SERVER-KEY : Find out in Firebase console by open Setting > Cloud Messaging. go to step 2.
YOUR-GENERATED-TOKEN : you can find out in browser console (because we log it) or open Firebase DB
ENJOY YOUR COFFEE…
GitHub : adrianaden/angular-push-notificationSources : Firebase Cloud Messaging , angular.io , angularfirebase.com
Software Craftsman
1.4K 
45
1.4K 
1.4K 
45
Software Craftsman
"
https://medium.com/joolsoftware/extending-firebase-with-cloud-functions-elasticsearch-129fbbb951e0?source=search_post---------178,"There are currently no responses for this story.
Be the first to respond.
Firebase is a hosted noSQL solution from Google that really focuses on the realtime aspect and the ease of use for developers. It features easy user authentication, security rules, realtime communication and blob storage. With its great features come some limitations regarding query support. Firebase only support startAt(), endAt(), and equalTo() conditions and not any form of fulltext support, let alone aggregations / facets. In this tutorial we’ll show you how to add fulltext query and aggregation support to Firebase using ElasticSearch and Firebase Cloud functions.
We won’t bother with a frontend for now, but focus on the Elasticsearch and Firebase cloud function part. We’ll use an example cars database found online and add data and queries in firebase using the Firebase cli.
If you are already experienced with Firebase or have a running Elasticsearch instance, you can skip the setup part and jump right into the Firebase cloud functions.
To get started with firebase you need to create a new Firebase project in your Firebase console, click the ‘Add project’ button, name your project and click the ‘create project’ button.
Upgrade to Firebase plan to ‘Blaze’ to allow for network connection to the Elasticsearch VM we are about to create. The Blaze plan will cost actual money. After completing the update, it might take a few minutes to be acutally completed.
Go to https://console.cloud.google.com/ and login with your Google account or use https://console.cloud.google.com/freetrial to get a free trial to Google Cloud Platform with $300 credit (but it does require a creditcard). When you are in the Google Cloud Console, go to the Cloud Launcher using the sidenav, search for ‘Elasticsearch’ (Inception!) and select the Bitnami image.
The Bitnami image is the cheap option, which is perfectly fine to get some hands-on experience. When you have selected the Bitnami Elasticsearch VM, you click on ‘Launch on Compute Engine’ button. This might trigger an ‘Enable billing’ dialog if you didn’t already have billing enabled. In the following screen you can reduce the VM machine type to ‘micro’ instead of ‘small’ as we’re not using it for any production load in this tutorial and click the ‘deploy’ button.
The deployment might take a little while but the deployment manager will eventually present you with the details we need to get data in the search index and query the search index: The IP adress (ephemeral, but usable for now) and the credentials for elasticsearch. After a successfull deployment you can find the instance in the sidenav menu under ‘Compute Engine’. Keep track of the Site address, Admin user and Admin password, you will need these later when configuring the Firebase cloud function
For a basic UI in your browser to send and receive some elasticsearch requests you can install the Chrome Sense extension. Its a bit buggy, but works good enough.
chrome.google.com
Now we need to tell elastic search what kind of data we’ll be sending and querying. The configuration of these mappings can be done sending an HTTP request using the Sense Chrome extension (or any other http client like Curl). It will require the credentials shown in the Bitnami Elasticsearch VM. The default username is always ‘user’.
Here you can find our Elasticsearch cars mapping
> npm install -g firebase-toolsThe first command installs the firebase CLI tool. This tool helps you to do import, export and deployment of firebase projects. Deployment can consist of static files, security rules and cloud functions.We’ll focus on the cloud functions.
> firebase login
The second command will authenticate you with firebase using your default browser. If all goes well, it will show you a ‘Firebase CLI Login Successful’ page and you can close the browser. These credentials are now used whenever you deploy, import or export using the Firebase CLI.
> firebase init functions
Next we initialize a firebase functions project. You will be asked to select a firebase project. A new functions directory will be bootstrapped with some npm dependencies together with a .firebaserc and firebase.json file.
Now we have all the basics setup: A firebase project in the cloud, an Elasticsearch instance and a local project folder.
Firebase Cloud functions are a kind of triggers with a callback that gets executed when one of the following happens:
For our case with Elasticsearch we’ll use the database onWrite event to monitor anything that is written to /cars/{carId} . cardId is a placeholder that can be retrieved as an event parameter.
To communicate with the Elasticsearch VM we’ll require some additional dependencies:
> npm install --save request request-promise lodash
First we’ll filter the data we receive from firebase to only include the fields used in Elasticsearch, then we post the json object to the Elasticsearch cars index. For our credentials and host configuration we use firebase config variables to seperate the code from the configuration. If we don’t receive any data it means that the data is deleted and we can safely delete it in the Elasticsearch index.
Keep in mind that the Firebase cloud function will not be called when / or /cars gets deleted or you use the firebase-admin sdk or console import!
To set the needed firebase config variables we use the Firebase CLI functions:config:set command:
> firebase functions:config:set elasticsearch.username=""user"" elasticsearch.password=""my_password"" elasticsearch.url=""http://104.154.41.53/elasticsearch/""
And now its time to deploy our first Firebase cloud function!
> firebase deploy
When all is successfull you can see the ‘indexCarsToElastic’ function in the Firebase console under ‘Functions’. Test it by manipulating the test cars data and look at the Logs tab in the Firebase Functions console.
Now it’s time to import some cars data using a node script that will read the first 1000 lines of a cars dataset. We will need a firebase service-account. You can download the keys for the service account in the Firebase console => Project settings => Service accounts page.
Place the service account key json file in a new import directory (outside the functions directory), download this file as cars.json in that same import directory and copy the following file as import.js.
Now, lets run our import using the following command
> npm install --save firebase-admin> node import.js
When all data is imported, we can query Elasticsearch with some aggregations and we would receive 1000 hits in total.
All firebase Cloud function invocations can be seen in the logs. You will see errors when for example the Elasticsearch VM was not available.
The actual code needed to run triggers based on the Firebase database writes is minimal. Just the way we like it. As said: the indexing of Elasticsearch is pretty basic. We’ll need to improve the error handling, find ways to trigger re-indexing when needed but Firebase cloud functions are definitly a better solution then the previous solutions like Flashlight and Firebase queues.
Liked it? Questions? Improvements? Let me know!
We are explorers in the world of web technology.
847 
11
Thanks to Jool Software Professionals. 
847 claps
847 
11
Written by

We are explorers in the world of web technology. We ♥ to share our discoveries with the community. We are Jool Software Professionals. https://jool.nl
Written by

We are explorers in the world of web technology. We ♥ to share our discoveries with the community. We are Jool Software Professionals. https://jool.nl
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://engineering.opsgenie.com/cloud-native-continuous-integration-and-delivery-tools-for-kubernetes-e6ea34d308c?source=search_post---------179,"Cloud Native is a new approach to build and run applications that can leverage the cloud computing delivery model. The objective is to improve speed and stability by optimizing the pipeline by leveraging Cloud Native model at each step, from coding to production, by making use of tools, such as containers and orchestrators, combined with additional tooling. As Cloud Native environments get more and more attraction, the tooling around these environments continue to evolve to fit the various needs.
A critical aspect of the modern development is Continuous Integration and Continuous Delivery. Ability to continuously push, test and deploy the code in lab, staging and production environments is an undeniable blessing for development. With the rise of containerization, the rapidness of this cycle has increased tremendously as both build and deployment stages continue to be containerized. However, the tooling for CI and CD must also keep the same pace to stay relevant in this crazy development world.
In this blog post, we will be introducing most promising CI and CD tools for Cloud-Native landscape or in other words pipeline tooling. We’ll also mention some other tools that are slightly Cloud Native compatible but promising.
First one in the list is Argo. Created by Applatix, Argo is one of the most Kubernetes compatible Cloud Native CI/CD solution, as it extends and makes use of Kubernetes APIs. Argo makes use of Custom Resource Definitions (CRD) where the pipeline itself is defined as Kubernetes objects. Custom controller listens to the changes on those objects and makes sure the pipelines are run properly. Using CRDs are the recommended way to extend Kubernetes functionality as it provides a standardized approach to define and watch the changes in arbitrary objects.
You can define pipelines as simple YAML files in Argo. Those pipelines can be used as CI, CD, Data Processing. There are many examples in the Argo repository. It also supports artifacts, templating and parameterization, variable substitution which makes pipelines reusable. As steps in a pipeline are run as Pods, you can make use of sidecar containers for services such as databases, volumes, secrets, config maps which might be helpful for defining your pipelines. Argo also features a nice UI where you can keep track of your runs.
Jenkins, formerly Hudson, is the lead open source automation software. Jenkins X is a new and opinionated Cloud Native CI/CD approach, which aims to require less intervention to pipeline configurations by providing strong defaults. It has a CLI, jx, which can create projects from templates, such as Spring Boot applications. Jenkins X also supports GitOps, where environment promotions are done via commits and environments created for each pull requests. It also comments on issues with feedback about the state of your code’s deployment. Under the hood, it uses Skaffold for image generation and CRDs of Kubernetes to manage the pipeline activity.
Although you can import existing projects, but Jenkins X seems a better fit for newly created projects, as it includes quick start templates and less documentation for importing existing projects. However it is not hard to add your own buildpacks by providing your own Jenkinsfile and Dockerfile. Jenkins X looks highly opinionated compared to regular Jenkins, which is intensely customizable. For instance, “jx import” command even creates a git repository in Github for your project, registers a Webhook to capture git push events and creates a Helm chart for your project. Although there are more than a thousand ways to accomplish the above with old Jenkins, Jenkins X seems to ease the operations for beginners and make a quick introduction to Cloud Native CI/CD but it’s default values and existing quick start templates might not be suitable for everyone, but as said before it is easy to extend it with buildpacks.
A new open source project by SAP, InfraBox is a continuous integration system based on Kubernetes. The project aims to be Cloud Native and seeks to utilize the resources of a cluster to avoid waste of resources. Unlike Argo, it does not make use of CRDs, but uses custom JSONs for defining a pipeline. InfraBox runs each step in Docker containers and allows building Docker images as well. There is also support for caching, timeouts, and secrets.
You can create dynamic pipelines, where steps are resolved while running a pipeline. As different from other solutions, InfraBox has a special support for tests, and it supports displaying them on its web UI as long as it is possible parse the output. One distinctive feature of InfraBox is ability to show not only test results but also things like code coverage, security check reports or metrics with its custom UI support.
As building images for Docker, required Docker for years, it has been a complication for most workflows. There are services such as Google Container Builder. However, these services might not be feasible for everyone. On the other hand, using regular Docker for building images requires root privileges and extended capabilities, while you might not be comfortable for granting such rights inside a container. As both the Docker image format and Registry API specification is already open source and some tools implement them to build containers without requiring Docker daemon to be run. Now, let’s list some promising ones;
Regular Jenkins, the one that most people are used to and the less opinionated one, also supports Kubernetes through the official plugin. Within the pipeline DSL, you can dynamically create pods and run your workflows inside the containers. It supports most of the features of Kubernetes, so you can make use of advanced features such as scheduling, secrets, liveness probes, which can be helpful to create isolated but robust CI and CD pipelines. The plugin can automatically auto scale the executors to your needs and according to your limits. No more waiting VMs to boot for simple Jenkins agents!
Gitlab supports a built-in container registry, Kubernetes support and even monitoring of your deployments inside the Kubernetes. Gitlab promotes a currently beta feature, Auto DevOps, which aims to handle a full cycle of a cloud-native application development. It’s CI builds, tests, asses the code quality and security for both code and container packages, and allows automatic deployment to Kubernetes. You also monitor the deployment with Prometheus and do manual review approvals, so that if you follow a git branch model, your pushes to that branch are mapped to a Deployment in your Kubernetes cluster. This helps you isolate your workflows from one another and utilize your Kubernetes to achieve a Cloud Native workflow. Although it seems complicated and currently in beta, this feature holds excellent promise for future. Gitlab itself can also be deployed to Kubernetes in a Cloud Native way too; it has a Helm chart in the development.
Concourse is a simpler automation system, which has a notion of resources, tasks, and jobs. You can use it for both CI and CD using YAML. Jobs depend on resources. Tasks inside a job make use of those resources, such as git repository source, or intermediate artifacts. Concourse can be deployed as a Helm chart in Kubernetes, and its builders make use of Docker images which makes proper task isolation. It utilizes RunC to run Docker images, without requiring Docker daemon itself. Currently, there are no autoscale mechanisms for the worker nodes as Concourse itself aims to be very simple, but that functionality can be easily added by making use of Horizontal Pod Autoscaler.
GoCD is a continuous delivery solution which can ease the modeling of complex workflows. It has a native support for Kubernetes, where all the infrastructure can be deployed with a Helm chart easily. Using GoCD, you can dynamically provision worker Pods by communicating with Kubernetes. It allows Docker in Docker and you can run customized images for different steps. There is also preliminary work for modeling the deployments as Kubernetes objects and track the state through GoCD UI easily.
An open source project by Netflix, Spinnaker, is a multi-cloud such as AWS, GCP, on-prem Kubernetes, continuous Continuous Delivery solution that handles the deployments of new codes in a controlled environment safely. It allows various deployment strategies, red/black (why not blue/green? Bonus points if you know), roll back on problems, approval, proper tearing down. Spinnaker can make use of LoadBalancer and liveness probe of deployments to ensure safe deployments. Although the same functionality can be achieved via standard Kubernetes Deployment updates even via kubectl, Spinnaker eases the management of multiple clusters and complex pipelines by allowing you to define integration tests on deployed clusters and rollback safely.
Another project from Google, Skaffold is a CLI tool for continuous development with utilizing local and remote Kubernetes clusters. It does not have a server-side component, and it can be safely used in non-interactive CI/CD systems. Skaffold in dev mode updates your application’s deployment regularly by detecting changes, pushing them to local cluster, and streaming the output. It only warns of errors, so the developer experiences a complete loop. You can also upload it to a remote Docker registry and run it within a Kubernetes cluster.
Weaveworks are originally known for creating Weave Net, a container SDN now available as both a Docker plugin and Kubernetes CNI option. Today, the company offers an extensive and eye-candy dashboard and a cloud offering, Weave Cloud. The Flux Operator for Kubernetes allows GitOps style CD & release management where operator fetches the changes from git push events and handles the deployments gracefully. It continously tries to match the declaritive configuration to the actualy deployment in Kubernetes. The cloud offering also has nice UI and Prometheus integration which aims to help your Continous Delivery pipelines.
In this blog post, we listed CI/CD tooling for Kubernetes. This is a growing interested area because as many companies try to adopt Cloud Native approaches in their development and delivery by utilizing containers, smart schedulers like Kubernetes, they also are in need of tools that work natively in environments such as Kubernetes and can build containers without interruption. As there are many new tools, and existing tools adopting the Cloud Native way, we hope to see more tools in the future.
If you liked this article, don’t forget to 👏 and follow OpsGenie Engineering!
OpsGenie is a cloud-based alerting and incident management service that could aggregate alerts from multiple IT monitoring systems and ensures the right people are notified at the right time, using e-mail, SMS, voice and mobile push notifications.
Interested in more?
engineering.opsgenie.com
engineering.opsgenie.com
Opsgenie is a cloud-based service for dev & ops teams…
833 
7
Thanks to Serhat Can. 
833 claps
833 
7
Written by
PhD Student @BilkentUniversity CS, on @OpsGenie SRE Team
Opsgenie is a cloud-based service for dev & ops teams, providing reliable alerts, on-call schedule management and escalations. OpsGenie integrates with monitoring tools & services, ensures the right people are notified.
Written by
PhD Student @BilkentUniversity CS, on @OpsGenie SRE Team
Opsgenie is a cloud-based service for dev & ops teams, providing reliable alerts, on-call schedule management and escalations. OpsGenie integrates with monitoring tools & services, ensures the right people are notified.
"
https://blog.goodaudience.com/how-to-generate-a-word-cloud-of-any-shape-in-python-7bce27a55f6e?source=search_post---------180,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
In this article, we explore how to generate a word cloud in python in any shape that you desire. We will go through an example of how to create a simple word cloud in the custom shape of a house as shown below.
If you are unfamiliar with a word cloud, it is an image of words where the size of each word indicates its frequency or importance. They are a powerful way to…
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/kubeflow/kubeflow-1-0-cloud-native-ml-for-everyone-a3950202751?source=search_post---------181,"There are currently no responses for this story.
Be the first to respond.
Coauthors: Jeremy Lewi (Google), Josh Bottum (Arrikto), Elvira Dzhuraeva (Cisco), David Aronchick (Microsoft), Amy Unruh (Google), Animesh Singh (IBM), and Ellis Bigelow (Google).
On behalf of the entire community, we are proud to announce Kubeflow 1.0, our first major release. Kubeflow was open sourced at Kubecon USA in December 2017, and during the last two years the Kubeflow Project has grown beyond our wildest expectations. There are now hundreds of contributors from over 30 participating organizations.
Kubeflow’s goal is to make it easy for machine learning (ML) engineers and data scientists to leverage cloud assets (public or on-premise) for ML workloads. You can use Kubeflow on any Kubernetes-conformant cluster.
With 1.0, we are graduating a core set of stable applications needed to develop, build, train, and deploy models on Kubernetes efficiently. (Read more in Kubeflow’s versioning policies and application requirements for graduation.)
Graduating applications include:
Hear more about Kubeflow’s mission and 1.0 release in this interview with Kubeflow founder and core contributor Jeremy Lewi on the Kubernetes Podcast.
With Kubeflow 1.0, users can use Jupyter to develop models. They can then use Kubeflow tools like fairing (Kubeflow’s python SDK) to build containers and create Kubernetes resources to train their models. Once they have a model, they can use KFServing to create and deploy a server for inference.
Kubernetes is an amazing platform for leveraging infrastructure (whether on public cloud or on-premises), but deploying Kubernetes optimized for ML and integrated with your cloud is no easy task. With 1.0 we are providing a CLI and configuration files so you can deploy Kubeflow with one command:
In Kubeflow’s user surveys, data scientists have consistently expressed the importance of Jupyter notebooks. Further, they need the ability to integrate isolated Jupyter notebooks with the efficiencies of Kubernetes on Cloud to train larger models using GPUs and run multiple experiments in parallel. Kubeflow makes it easy to leverage Kubernetes for resource management and put the full power of your datacenter at the fingertips of your data scientist.
With Kubeflow, each data scientist or team can be given their own namespace in which to run their workloads. Namespaces provide security and resource isolation. Using Kubernetes resource quotas, platform administrators can easily limit how much resources an individual or team can consume to ensure fair scheduling.
After deploying Kubeflow, users can leverage Kubeflow’s central dashboard for launching notebooks:
In the Kubeflow UI users can easily launch new notebooks by choosing one of the pre-built docker images for Jupyter or entering the URL of a custom image. Next, users can set how many CPUs and GPUs to attach to their notebook. Notebooks can also include configuration and secrets parameters which simplify access to external repositories and databases.
Distributed training is the norm at Google (blog), and one of the most exciting and requested features for deep learning frameworks like TensorFlow and PyTorch.
When we started Kubeflow, one of our key motivations was to leverage Kubernetes to simplify distributed training. Kubeflow provides Kubernetes custom resources that make distributed training with TensorFlow and PyTorch simple. All a user needs to do is define a TFJob or PyTorch resource like the one illustrated below. The custom controller takes care of spinning up and managing all of the individual processes and configuring them to talk to one another:
To train high quality models, data scientists need to debug and monitor the training process with tools like Tensorboard. With Kubernetes and Kubeflow, userscan easily deploy TensorBoard on their Kubernetes cluster by creating YAML files like the ones below. When deploying TensorBoard on Kubeflow, users can take advantage of Kubeflow’s AuthN and AuthZ integration to securely access TensorBoard behind Kubeflow’s ingress on public clouds:
No need to `kubectl port-forward` to individual pods.
KFServing is a custom resource built on top of Knative for deploying and managing ML models. KFServing offers the following capabilities not provided by lower level primitives (e.g. Deployment):
Below is an example of a KFServing spec showing how a model can be deployed. All a user has to do is provide the URI of their model file using storageUri:
Check out the samples to learn how to use the above capabilities.
A model gathering dust in object storage isn’t doing your organization any good. To put ML to work, you typically need to incorporate that model into an application – whether it’s a web application, mobile app, or part of some backend reporting pipeline.
Frameworks like flask and bootstrap make it easy for data scientists to create rich, visually appealing web applications that put their models to work. Below is a screenshot of the UI we built for Kubeflow’s mnist example.
With Kubeflow, there is no need for data scientists to learn new concepts or platforms to deploy their applications, or to deal with ingress, networking certificates, etc. They can deploy their application just like TensorBoard; the only thing that changes is the Docker image and flags.
If this sounds like just what you are looking for we recommend:
1. Visiting our docs to learn how to deploy Kubeflow on your public or private cloud.
2. Walking through the mnist tutorial to try our core applications yourself.
There’s much more to Kubeflow than what we’ve covered in this blog post. In addition to the applications listed here, we have a number of applications under development:
In future releases we will be graduating these applications to 1.0.
All this would be nothing without feedback from and collaboration with our users. Some feedback from people using Kubeflow in production include:
“The Kubeflow 1.0 release is a significant milestone as it positions Kubeflow to be a viable ML Enterprise platform. Kubeflow 1.0 delivers material productivity enhancements for ML researchers.” — Jeff Fogarty, AVP ML / Cloud Engineer, US Bank
“Kubeflow’s data and model storage allows for smooth integration into CI/CD processes, allowing for a much faster and more agile delivery of machine learning models into applications.” — Laura Schornack, Shared Services Architect, Chase Commercial Bank
“With the launch of Kubeflow 1.0 we now have a feature complete end-to-end open source machine learning platform, allowing everyone from small teams to large unicorns like Gojek to run ML at scale.” — Willem Pienaar, Engineering Lead, Data Science Platform, GoJek
“Kubeflow provides a seamless interface to a great set of tools that together manages the complexity of ML workflows and encourages best practices. The Data Science and Machine Learning teams at Volvo Cars are able to iterate and deliver reproducible, production grade services with ease.”— Leonard Aukea, Volvo Cars
“With Kubeflow at the heart of our ML platform, our small company has been able to stack models in production to improve CR, find new customers, and present the right product to the right customer at the right time.” — Senior Director, One Technologies
“Kubeflow is helping GroupBy in standardizing ML workflows and simplifying very complicated deployments!” — Mohamed Elsaied, Machine Learning Team Lead, GroupBy
None of this would have been possible without the tens of organizations and hundreds of individuals that have been developing, testing, and evangelizing Kubeflow.
We could not have achieved our milestone without an incredibly active community. Please come aboard!
Thank you all so much — onward!
Official Kubeflow Blog.
484 
2
484 claps
484 
2
Written by
open source strategy @kubeflow, formerly @apollographql @docker & @newrelic. makes a mean frittata.
Official Kubeflow Blog.
Written by
open source strategy @kubeflow, formerly @apollographql @docker & @newrelic. makes a mean frittata.
Official Kubeflow Blog.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/free-code-camp/continuous-deployment-for-node-js-on-google-cloud-platform-751a035a28d5?source=search_post---------182,"There are currently no responses for this story.
Be the first to respond.
Google Cloud Platform (GCP) provides a host of options for Node developers to easily deploy our apps. Want a managed hosting solution like Heroku? App Engine, check! Want to host a containerized app? Kubernetes Engine, check! Want to deploy serverless app? Cloud Functions, check!
Recently at work, I’ve been enjoying using our in-house continuous deployment service that quickly builds, tests, and deploys new commits pushed to GitHub. So when I read about Google’s new Cloud Build service, I wanted to take it for a spin and see if I could recreate a similar seamless deployment experience for myself. Further, in a conversation with Fransizka from the Google Cloud team, she identified this as an area where a tutorial would be helpful. So here we go…
Cloud Build is a managed build service in GCP that can pull code from a variety of sources, run a series of build steps to create a build image for the application, and then deploy this image to a fleet of servers.
Cloud Build works well with Google’s own source code repository, Bit Bucket or GitHub. It can create a build image using a Docker configuration file (Dockerfile) or Cloud Build’s own configuration file (cloudconfig.yaml). It can deploy applications (and APIs) to App Engine, Kubernetes Engine, and Cloud Functions. A really cool feature is Build Triggers. These can be setup to watch for a new commit in the code repository and trigger a new build and deploy.
This post shares the detailed steps and code to setup the continuous deployment for Node apps on GCP. It assumes that you’re familiar with developing simple Node applications, working with the command line, and have some high level understanding of deploying apps to cloud services like Heroku, AWS, Azure or GCP.
For each of the sections, a companion GitHub code repository is provided for you to follow along. Don’t sweat it though — feel free to skim over the article to learn about the high level ideas, and you can bookmark it and come to it later if you plan to set this up. The real fun of having a setup like this is that you get to deploy applications quickly.
Deploying a Node app to App Engine is quite simple. Create a new project in Google Cloud Console, add an app.yaml configuration file in our code directory (which describes the node runtime we want to use — I’ve used Node 8), and run gcloud app deploy on our terminal — and done!
If you want to try this out for yourself, here are a couple of resources:
So, what we’ve done so far by following the quickstart guide above:
….now how can we automate setup such that code changes get deployed automatically on push to GitHub?
Here is what we need to do:
2. Enable Cloud Build
3. Create a Cloud Build configuration file
This configuration has three build steps (each line starting with a hyphen is a build step) that will run npm install, then npm test and if all looks good then deploy our code to App Engine.
Each build step is just like a command we run on our machine. But in this case, since this file is in yaml and each step is split over 2 lines of name and args, it can look like a bit of a mind-bender.
Let’s try this: for the line starting with “name”, read its last word and then read the values in the “args” line. I hope this file makes more sense now!
4. Run a Build manually (optional, just for verification)
5. Create a Build Trigger
6. Run a Build automatically by pushing a commit to GitHub
Here is a screenshot for builds being triggered through a GitHub push for our app:
Too good to be true?? Run this last step a few times times to test it out a few more times. Our first application now gets deployed to App Engine on every commit to master 👏
Great, so we’ve setup our application to deploy to App Engine on GitHub push, but what if we wanted the same setup for our containerized applications? Let’s give it a spin!
At a high level, deploying a Node app to Kubernetes engine has two main tasks. First, get our app ready: Containerize the application with Docker, build it, and push the Docker image to Google Container Registry. Then, setup things on the GCP end: create a Kubernetes Cluster, create a Deployment with your application image, and then create a Service to allow access to your running application.
If you want to try this out for yourself, here are a few resources:
So, what we’ve done so far by using the guides above:
…but what we want is an continuous deployment setup such that a new commit kicks off a build and deployment.
Here is what we need to do:
2. Enable Cloud Build
3. Create a Cloud Build Configuration file
This configuration has five build steps that will run npm install and then npm test to make sure our application works, then it will create a Docker image and push to GCR and then deploy our application to our Kubernetes cluster. The values my-cluster, my-deployment and my-container in this file refer to resources in the Kubernetes cluster we have created (as per the guide we followed above). $REVISION_ID is a variable value that Cloud Build injects into the configuration based on GitHub commit that triggers this build.
4. Run a Build manually (optional, for verification)
We’re also passing the revision id in this command, since we are manually running this build vs it being triggered by GitHub.
5. Create a Build Trigger
Here is a screenshot for a build being triggered through a GitHub push for our app:
The steps in this section were pretty much the same as the App Engine section. The main differences were that we had to containerize our application with Docker, spin up our Kubernetes cluster, and then have a Cloud Build configuration with just a few more steps.
But at its core, Cloud Build and its Build Triggers work pretty much the same and give us a seamless deployment experience. Our second application now gets deployed to Kubernetes Engine on every commit to master 👏👏
Sure, App Engine and Kubernetes Engine are great, but how about automated deployments for our Serverless app? I mean, having no servers to manage at all is really the best, right? Let’s do this!
Deploying a Node app to Cloud functions will require us to create a new project. No configuration files are needed, and once GCloud functions deploy on our terminal, our functions are deployed!
If you want to try this out for yourself, here are the resources you will need:
If you’ve been following along, you can probably already picture what steps we need to do:
2. Enable Cloud Build
3. Create a Cloud Build Configuration file
Similar to the App Engine configuration, this configuration has 3 steps to install. Then test the build, and if all is good, then deploy it to Cloud Functions.
4. Run the Build manually (optional, for verification)
5. Create a Build Trigger
6. Run a Build automatically by pushing to GitHub
Here is a screenshot for build being triggered through a GitHub push for our sample app:
Cloud Functions were super easy to setup with automated builds and makes the “code → build → test → push → deploy” workflow really really fast! Our third application now gets deployed to Cloud functions on every commit to master 👏👏👏
Phew! We covered a lot of ground in this post. If this was your first time trying out GCP for Node, hopefully you got to see how easy and straightforward it is to try out the various options. If you were most eager to understand how to setup continuous deployment for apps on GCP, I hope you weren’t disappointed either!
Before you go, I just wanted to make sure that you didn’t miss the fact that all the sections had a sample app: Hello World for App Engine, Hello World for Kubernetes Engine and Hello World for Cloud Functions.
That’s it for now! Let’s go ship some code! 🚢
Thanks for following along. If you have any questions or want to report any mistakes in this post, do leave a comment.
If you found this article helpful, don’t be shy to 👏
And you can follow me on Twitter here.
We’ve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
1K 
7
1K claps
1K 
7
We’ve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
Written by
Engineering Manager at Facebook. Posts about software engineering, mobile app reliability and performance.
We’ve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
"
https://medium.com/iotforall/cloud-computing-vs-fog-computing-aa94cbc4b827?source=search_post---------183,"There are currently no responses for this story.
Be the first to respond.
Top highlight
What’s best for IoT?
The real business value enabled by the Internet of Things is derived not really from the data, but from insights that facilitate real-time actions that increase asset efficiency, reliability and utilization.
That value takes many forms with IoT use cases that range from supply chain management and manufacturing automation to parking and waste management solutions.
But, to actually save time and money with IoT, the data insight has to come from somewhere — typically centralized, scalable cloud computing platforms tailored for the device, connectivity and data management needs of the Internet of Things.
At a basic level, cloud computing is a way for businesses to use the internet to connect to off-premise storage and compute infrastructure. In the context of the Internet of Things, the cloud provides a scalable way for companies to manage all aspects of an IoT deployment including device location and management, billing, security protocols, data analysis and more.
Cloud services also allow developers to leverage powerful tools to create IoT applications and deliver services quickly. On-demand scalability is key here given the grand vision of IoT; a world saturated with smart, connected objects.
Many major technology players have brought cloud-as-a-service offerings to market for IoT. Microsoft has its Azure suite, Amazon Web Services, a giant in cloud services, has an IoT-specific play, IBM offers access to the Watson platform via its Bluemix cloud, and the list goes on and on.
Regardless of the specific product, the commonality is the ability to access flexible IT resources without having to make big investments into hardware and software and the management that comes with it.
However, for services and applications that require very low latency or have a limited “pipe” through which to pipe data, there are some downsides to the cloud that are better addressed at the edge.
Monica Paolini, president of Senza Fili Consulting, wrote on LinkedIn, “In recent years, there has been a strong push to move everything to a centralized cloud, enabled by virtualization and driven by the need to cut costs, reduce the time to market for new services, and increase flexibility. In the process, we lost sight of how important the location of functionality is to performance, efficient use of network resources and subscriber experience. Physical distance inevitably increases latency.”
The OpenFog Consortium was organized to develop a cross-industry approach to enabling end-to-end IoT deployments by creating a reference architecture to drive interoperability in connecting the edge and the cloud. The group has identified numerous IoT use cases that require edge computing including smart buildings, drone-based delivery services, real-time subsurface imaging, traffic congestion management and video surveillance. The group released a fog computing reference architecture in February 2017.
Helder Antunes, OpenFog Consortium chairman and senior director of the corporate strategic innovation group at Cisco, said the release will drive IoT adoption by providing a “universal framework. While fog computing is starting to be rolled out in smart cities, connected cars, drones and more, it needs a common, interoperable platform to turbocharge the tremendous opportunity in digital transformation.”
Another group that was formed to drive edge interoperability is the Edge X Foundry, an open source consortia approach managed by The Linux Foundation and seeded with some 125,000 lines of code developed internally by Dell Technologies.
If you want to dive deeper into how open source initiatives like the Edge X Foundry are impacting the internet of things, you can check out our primer, “Open Source and the IoT: Innovation through Collaboration.”
Let’s consider autonomous driving. Cellular networks will connect vehicles, equipped with advanced LiDAR, image processing, and other self-driving technologies with other vehicles, pedestrians, smart infrastructure, and an array of cloud-based services to support in-car entertainment, predictive maintenance, remote diagnostics, and the like.
It’s fine for your car to access your cloud-based Netflix account or maintain operational and maintenance logs, but the cloud isn’t necessarily the best place for mission critical decisions that could help a vehicle avoid a collision on the highway — given the time (latency) demands, this type of processing is best handled at the network edge.
To facilitate this type of hybrid approach, Cisco and Microsoft have integrated the former’s Fog Data Services with the latter’s Azure IoT cloud platform. The combo joins edge analytics, security, control, and data management with centralized connectivity, policy, security, analytics, app development and more.
In a recent blog post, Cisco Head of IoT Strategy Macario Namie noted, “One of the beautiful outputs of connecting ‘things’ is unlocking access to real-time data. Next is turning that data into information and, more importantly, actions that drive business value. In trying to do so, companies are finding themselves deluged with data.
So much so that demand for vast compute and storage needs have arisen, very nicely handled by public cloud providers. But the cost of transport and speed of processing has also increased, which is challenging for many uses cases such as mission-critical services…As a result, many IoT initiatives are now distributing this computing power across the edge network, data centers, and public cloud.”
Originally published at www.link-labs.com
Expert analysis, simple explanations, and the latest…
207 
4
 IoT For All's weekly round up of best reads, events and news all in one place! Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
207 claps
207 
4
Written by
Entreprenuer, wireless nerd, Making the Things of Internet of Things. Founder of Link Labs. Dad. Sailor. Former Submarine Officer, IC, USNA, Oxford.
Expert analysis, simple explanations, and the latest advances in IoT, AR/VR/MR, AI & ML and beyond! To publish with us please email: contribute@iotforall.com
Written by
Entreprenuer, wireless nerd, Making the Things of Internet of Things. Founder of Link Labs. Dad. Sailor. Former Submarine Officer, IC, USNA, Oxford.
Expert analysis, simple explanations, and the latest advances in IoT, AR/VR/MR, AI & ML and beyond! To publish with us please email: contribute@iotforall.com
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/thinking-design/the-impact-of-color-on-conversion-rates-4f2b4cce9f2a?source=search_post---------184,"There are currently no responses for this story.
Be the first to respond.
Color is one of the most powerful tools in a designer’s toolkit. It can draw attention, set a mood, and influence a users’ emotions, perceptions and actions.
Color does not add a pleasant quality to design — it reinforces it.
Pierre Bonnard
Did you know that color accounts for 90% of the reason why you purchased a specific product? Or that full-colored ads in magazines are recognized 26% more than black and white ads?
It seems quite obvious: use the right colors, and you win. But the real question that comes to mind is, how do you find the “right” color? To answer this question, we need to analyze traditional color associations, the differences between men and women’s perceptions of color schemes, and accessibility concerns.
Color does more than just give us objective information about a product, it also has a powerful psychological influence on the human brain. Certain colors can convey different feelings, depending on what part of the world your design will be viewed. Here are some feelings colors can give off in the western world:
Red is the color of fire and blood. It’s one of the most powerful colors, attributed simultaneously with love and war (the common saying, “to see red” highlights the color’s connection with anger).
Red is a very emotionally intense color. It can actually have a physical effect on people, raising blood pressure and respiration rates.
Red is also known to be eye-catching. In design, red can be a powerful accent color. Just like red carpets at awards shows and celebrity events, red is great highlight for the most important individual elements on your page.
Orange is a very vibrant color. It shares red’s stimulating aspects, but to a lesser degree: it has an energetic aura without red’s aggression and can portray a fun atmosphere.
Just like red, orange has a high visibility, so you can use it to catch attention and highlight important elements such as call to action buttons. Some research indicates that it denotes cheapness, but many apps and sites use ‘cheap’ property in a good way.
Oddly, yellow represents both happiness and anxiety. In design, yellow is very effective for attracting attention, making it a color often used for warning signs (it can be associated with danger, though not as strongly as red).
When combined with black, it can gain a lot of attention. A good example outside of digital design would be a New York taxi cab.
The most obvious association with green is nature. Since most plants are green, it is also associated with growth and health.
In design, green can have a balancing and harmonizing effect. Green’s balance lends itself to calls-to-action, but as a designer you should pay attention to color saturation. Saturated green colors are exciting and dynamic to the eyes, they grab a lot of attention. This is why they work well for call to action buttons.
Blue is the color of the sea and the sky. It’s one of the most important colors in UI design, and one of the most frequently used. In design, the exact shade of blue you select will have a huge impact on how your designs are perceived:
Purple’s rarity in nature and the expense of creating the color has given purple a special role in design. Historically linked to royalty, purple retains a feeling of luxury even today. Purple insinuates that a product or site is high-end, even if it’s not.
An interesting fact is almost 75 percent of children prefer purple to all other colors.
Black is the strongest of all colors: it attracts attention quickly, which is why it’s most commonly used for text and accents.
When used as a main component in a color scheme, such as a background, black creates its own emotional ties. It can make it easier to convey a sense of sophistication and mystery in a design.
White is often associated with purity, cleanliness, and virtue. Because of the fact that white is often connected with health services and innovation, you can use this color to suggest safety when you want to promote medical or high-tech products.
In design, white accents other colors around it, making it a popular choice for a secondary color. A proper use of white space is a powerful design feature. Take, for example, Google search homepage. It’s clear that white lets other colors in a design have a larger voice:
Gray represents neutrality. It’s a safe color to use with other colors. When used as a primary color, gray gives the impression of formality, but that doesn’t always mean a bad thing. Similar to white, you can use a gray background to make other colors stand out.
While there are no concrete rules about what colors are exclusively feminine or masculine, there have been studies conducted over the past eight decades that draw some generalizations. Although findings are ambiguous, many studies have indicated that there are differences between gender in preferences for colors:
Color takes center stage in many brands’ philosophy of design. Every color that we see implies something, either directly or indirectly, that helps drive our perception of individual brands. Some colors go beyond their brands, defining entire industries, such as blue for the travel industry, green for health, and red for fast food.
However, when it comes to selecting a color scheme for your brand there’s no hard and fast rule. While some companies choose to use their industry’s common colors, others have found that going against traditional expectations can be a very effective way to make an impression. For example, Virgin America chose to go against the grain with the design on their site and in their app. While it might not be what users would expect from an airline website, it certainly stands out.
Thus, choosing unexpected colors can be an effective way to make users remember your company.
How can you use color theory and psychology to get people to click on a button? Call to action button color is one of the longest standing debates in the world of conversion and optimization. For every expert who claims that an eye-catching red is the best color for a button, there’s another one who says that green is the best because green color is safe and means “go.”
HubSpot shared a test that showed how a change in the color of a CTA button made a drastic impact on signups.
Even though they originally predicted the green button would perform better, the red button resulted in 21% more clicks. At the same time HubSpot warned their readers that test results are subjective (their audience likely prefered the red button because it was the only color that stands-out on the page).
The color of the button has little to no effect on it’s own. What works on one site, doesn’t necessarily work on another. Saying that one color converts better than another is simply wrong because there is no universal best color. However, there are rules of thumb that can help you use color to your advantage. One of them is the psychological principle known as the isolation effect which states that an item that “stands out like a sore thumb” is more likely to be remembered.
For example, if your site or app uses a lot of green, users probably won’t notice a green button right away, no matter how well red buttons performed in another company’s A/B test.
Thus, it’s essential to changes a visual hierarchy of the whole page to make a call to action stand out. Color contrast is important because if the button color does not get the attention of the potential customer you don’t get the sale/sign-up.
Design isn’t just about looking pretty — it’s about functionality and usability, the two principles that are arguably the most important to any UX designer. Color is a tool that can help guide the eye and good UI uses color to direct not just the user’s attention, but also their interactions with the entire experience.
Applying color to a design has a lot to do with balance and the more colors you use, the more complicated it is to achieve balance. Using too many colors is a common design mistake, it can be like trying to convey a million feelings and messages at once, which can confuse the person viewing your design.
There is a simple interior design rule 60–30–10 that works well for many designs. It’s a timeless decorating technique that can help you put a color scheme together easily. 60% is your dominant hue, 30% is secondary color and 10% is for accent color. This formula works because it creates a sense of balance and allows the eye to move comfortably from one focal point to the other.
People see colors differently. Approximately 8% of men and 0.5% of women are affected by some form of color blindness. Red and green are the colors most affected by color-vision deficiency. Avoid using these color combinations as the only way to distinguish between two states or values.
Let’s examine one common case: have you ever gotten an error message when you’re filling out a form that says something like, “The fields marked in red are required”? While it might not look like a big thing, this error message combined with a form in example below can be an extremely frustrating experience for people who have color-vision deficiency.
As said in W3C guidelines, color shouldn’t be used as the only visual means of conveying information, indicating an action, prompting a response, or distinguishing a visual element. For form mentioned above designer should give more specific error messages like, “The email address that you entered is not valid” or at least provide an icon near the field which requires user attention.
Broadly speaking, there are some great tools available to help you test your UI’s accessibility:
We’ve covered several color-related factors that can affect the user experience on your site or app, but still didn’t find an answer on our question: how to find the “right” color? You likely already know the answer: there is no ‘best color’ for conversions. The real value of colour in design does not lie in single colours, but in what colours you have and how you combine them. This idea is clearly expressed in the Material Design quote:
“There are no wrong colors. What matters most is how to use them.”
If you plan to give your conversions a serious lift you need to do conversion research. Serious gains in conversions come from analyzing what your users really need, the language that works for them, and how they want to buy the product. The right design decision is the one that your users think is right.
Nick Babich is a developer, tech enthusiast, and UX lover. He has spent the last 10 years working in the software industry with a specialized focus on development. He counts advertising, psychology, and cinema among his myriad interests.
Originally published at blogs.adobe.com.
Learn about Adobe XD, our all-in-one design and prototyping tool:
Stories and insights from the design community.
206 
1
206 claps
206 
1
Stories and insights from the design community.
Written by
New Tools for New Creatives. Get all the latest creative apps plus seamless ways to share and collaborate. All right on your desktop.
Stories and insights from the design community.
"
https://medium.com/@satyenkumar/it-cloud-certification-is-greed-the-1-of-the-7-deadly-sins-f50366643138?source=search_post---------185,"Sign in
There are currently no responses for this story.
Be the first to respond.
Satyen Kumar
Aug 17, 2020·7 min read
I confess. It took 25 years (almost!) on content of this page. The article is for the future leaders of the IT industry, and for those who need motivation to deep dive into C.L.O.U.D. Being strongly opinionated on having knowledge supplemented by industry certifications, I don’t have quick-fixes. However, you can follow advices in the series , the career journey more pleasing.
Here we go, Greed to get knowledge & certifications. Welcome to the Diary in this episode!
"
https://medium.com/google-cloud/how-to-run-visual-studio-code-in-google-cloud-shell-354d125d5748?source=search_post---------186,"There are currently no responses for this story.
Be the first to respond.
Did you know you can run Visual Studio Code in a browser? It’s so cool, it even works with extensions and all that. I’ll show you an example using github.com/cdr/code-server
Note that you don’t need authentication since the Google Cloud Shell proxy already handles that for you.
If you get a 404, remove ?authuser=0 from the url. So for example, justhttps://8080-dot-YOURID-dot-devshell.appspot.com/instead ofhttps://8080-dot-YOURID-dot-devshell.appspot.com/?authuser=0
Google Cloud community articles and blogs
719 claps
719 
15
Written by
Google Cloud Customer Engineer
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Google Cloud Customer Engineer
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://towardsdatascience.com/90-second-setup-challenge-jupyter-tensorflow-in-google-cloud-93038bcac996?source=search_post---------187,"Sign in
There are currently no responses for this story.
Be the first to respond.
Cassie Kozyrkov
Apr 30, 2020·5 min read
Data science enthusiasts, how fast can you go from zero to Google Cloud Jupyter notebook? Let’s find out!
If you’re in the mood to ultra-customize your setup, Google Cloud gives you dizzying granularity. That’s a fabulous thing for teams starting an enterprise-scale project, which scales up the consequences of every corner you cut.
Do beginners have to trudge through a forest of options to get started?
But what if you’re a beginner who just wants to play around with a data science notebook? Do you have to trudge through that forest of options to get started? Great news: you don’t. You can get up and running in under 90 seconds! (Feel free to skip to the walk-through below.)
You can get up and running in under 2 minutes! (Walk-through below.)
If this is your first time, I bet you’re probably especially keen to get to hello world as quickly as possible. You’ll want to skip the control panel and use someone else’s setup solution. If so, you’ll love the Google Cloud Marketplace!
Welcome to a collection of prebaked machine images that you can use as a springboard for your own work. And yes, you can customize these third party templates later if you need to (learn more here).
A collection of prebaked templates.
Enough preamble! Let’s see if it is possible to go from zero to hero in under 2 minutes. I’ll break down the steps below — this is just the proof that it can be done. Here’s we go:
First, what didn’t we watch? Steps 0.1–0.3.
You’ll need a Google account to use Google Cloud. I’ve had mine for a while, so that’s not shown.
My video starts on the Google Cloud Dashboard screen, which you’ll get to by clicking console.cloud.google.com. First-time users will be prompted through a few housekeeping items.
I’m using a project I made called ItsTimeForDemos. To make a new project, I clicked on this part of the dashboard:
Now that we’re ready, let’s examine what we saw on my screen.
To skip reinventing your own wheel, you’ll head to Google Cloud Marketplace.
In the search bar, I’ll enter the keywords “ai cpu.”
Why AI? I felt like doing this demo with an AI-geared notebook. If you’re looking for something else, simply ask the search bar. There are almost 2000 solutions here.
Why CPU? It’s the vanilla option. More powerful hardware — like GPUs — is more powerful… which usually also means it’s more expensive and will burn through your $300 free credits faster. Unless you already know why you need the beast option, it’s probably a good idea to start with its lightweight cousin.
Among the solutions, I picked the TensorFlow Jupyter notebook. You pick whatever you like.
Why notebook? Data scientists like notebooks because they combine the best of interactive data analysis with pretty report-making.
Why Jupyter? It’s the notebook solution you’re most likely to have heard of.
Why TensorFlow? It’s a powerful deep-learning framework; get more info here.
Hit the big blue button. Repeat.
A randomly-generated password will be created for you. Copy its text while you wait patiently for a few seconds, then open the Jupyter notebook when that option becomes available.
Paste in your copied password to open Jupyter, then create a new notebook.
Once you’re in, you’ll see that TensorFlow is installed and you’re ready to start exploring.
Getting up and running took less than 90 seconds!
Is this the best data science environment you can have? No, because creating the perfect solution for you means customizing everything to your personal needs. One day, that’s probably what you’ll choose to do. But in the meantime, this was the shortest path to getting up and running that I’ve found. My goal with this blog post was simply to show you that there are super-quick options out there.
Can you do it faster than me?
Can you do it faster than me? Maybe! Consider it a challenge and let me know how many seconds it takes you.
To avoid charges, don’t forget to turn out the lights when you’re done; use the search bar to find the Deployment Manager page where you can delete/stop whatever you don’t need anymore.
This blog post was written in April 2020, so if you’re from The Future, things might look different in your world.
If you had fun here and you’re looking for an applied AI course designed to be fun for beginners and experts alike, here’s one I made for your amusement:
Head of Decision Intelligence, Google. ❤️ Stats, ML/AI, data, puns, art, theatre, decision science. All views are my own. twitter.com/quaesita
702 
6
Every Thursday, the Variable delivers the very best of Towards Data Science: from hands-on tutorials and cutting-edge research to original features you don't want to miss. Take a look.
702 claps
702 
6
Your home for data science. A Medium publication sharing concepts, ideas and codes.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/zero-equals-false/firebase-cloud-functions-part-1-introduction-to-triggers-all-you-ever-wanted-8008e50ecb70?source=search_post---------188,"There are currently no responses for this story.
Be the first to respond.
I had been questioning myself since I started using Google Firebase, that If I could manage to hook some remotely written functions to Firebase Products, like Realtime Database, Storage, etc. and trigger them on different events.
This requirement was faced firstly with Firebase Realtime Database and Firebase Authentication.
Consider below two cases:
Case 1: You have “Users” object lying in your Firebase Database as below:
And You want a Notification when there is any Write operation on this “Users” key, lets say, if an object gets inserted in “Users” key, one should get Notification.
WorkAround: One solution, I found on stackoverflow, was to keep Database Reference Live in Service and listen for changes by using onChildXXX() method, and generate Notification from there. According to me, this is not a good and efficient solution.
Case 2: You want to get an Email when any new User registers in your app.
What if you are provided with a way to write such callbacks on currently trending backend platform Node.js
You would be glad to welcome this new Product in Firebase family, Cloud Functions. Lets not beat around the bush, and lets get familiar with Cloud Functions.
From the documentation:
Well, you achieve a lot more tasks using Cloud Functions, you can just think like:
And what not…!!!
Firebase’s Repository Function Samples provides bunch of examples for you to try out.
“But.. But.. How would I be able to ? I am Android Developer (or iPhone Developer), and Node Scripts are written in JavaScript..”
Well, answer to your question is, I am too a Native Mobile Application Developer (for Android). My JavaScript skill level is Beginner’s. But when I looked at the Syntax, and tried writing it, I too became familiar with JavaScript and I am now able to write any Scripts in Node.js
Believe me, its really easy to interpret what the syntax is trying to do. Let me, help you with a code snippet I have below:
I promise I won’t go deep with concepts but merely try to help you visualize the syntax. Lets start.
You’re writing Firebase Cloud Functions, so the first line seems to be initializing this “firebase-functions”. Similarly, second line also looks like initializing some kind of admin features.
2. Then, try the next line:
const admin needs to initialize something before we start writing our actual function.
3. Finally, this is the important one:
Forget functions, what do you think this would do: auth.user().onCreate() it will give some kind of callback when a new User gets created, or something similar to it. Whenever, you think of any callback type of mechanism, most of the time callback functions gives something in arguments related to the event that happened.
In the same way, this callback function onCreate() has event object as function argument. (Syntax looks similar to Lambdas…!!??)
Next, event.data returns user object which contains User’s UID, Email, Display Name, PhotoUrl, etc.
Then, userObject is new object created using information available from user.
database.ref(‘users/’+user.uid).set(userObject); This seems to be writing some values to Firebase Database. Yes you got it correctly. This line sets this userObject object inside “Users” object in Firebase Database.
It wasn’t really that hard, right ??
The snippet listens for creation of new User using Firebase Authentication. When any new User gets created, this callback gets called and inserts a new object in “Users” object lying in Firebase Database.
Firebase Cloud Functions Samples: https://github.com/firebase/functions-samples
Thanks for visiting.
Zero Equals False delivers quality content to the Software community.
923 
10
Zero Equals False - delivering quality content to the Software community. Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
923 claps
923 
10
Written by
Technical Lead — Android at ARInspect | 8+ years of concrete experience in Android Development
Zero Equals False delivers quality content to the Software community. For more quality content, head over to https://zeroequalsfalse.com
Written by
Technical Lead — Android at ARInspect | 8+ years of concrete experience in Android Development
Zero Equals False delivers quality content to the Software community. For more quality content, head over to https://zeroequalsfalse.com
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://proandroiddev.com/cloud-firestore-android-is-easy-c940cb82715c?source=search_post---------189,NA
https://medium.com/google-cloud-jp/firestore2-920ac799345c?source=search_post---------190,"There are currently no responses for this story.
Be the first to respond.
medium.com
パート1の前置きが長くなりましたが、これからFirestoreの具体的な使い方を見ていきます。
実例があると分かりやすいかと思い、Qiita のような投稿型のブログサービスを題材に考えていきます。
https://console.firebase.google.com にて適当なサンプルプロジェクトを作ります。今回は firestore-sandbox を作りました(実アプリの場合、開発環境・テスト環境・本番環境など3つ以上用意するのが良いですがサンプルなので1つで済ませます)。
Firebaseプロジェクトはデフォルトで無料なので、個人アカウントでも安心して作れます🤗無料のままでもそれなりに本格的な利用に耐えられますし、有料プランに上げることになっても大抵は驚くほど安く済むと思います。
firebase.google.com
プロジェクト上でFirestoreを選ぶと、セキュリティルール設定の選択を迫られます。
初めての場合は Start in test mode がおすすめです。全ドキュメントに対して誰でもアクセスできるので大抵のサービスではこのままリリースはできないですが、セキュリティルール(パート3で説明します)のことを考えるのを後回しにできて楽になります。
ENABLE を押すと、次のようにFirestoreの空の画面に移ります。 RULES タブに❗️マークが付いていて、危険なセキュリティルールのままリリースしてしまうミスを防げるようになっています。
とりあえず、素朴にルートに posts コレクションと users コレクションを設けてみます。
まず、 authorId がstring 型になっていますが、Firestoreには参照型があるので、基本的にはそちらを使った方が良いです。名前は個人的には authorRef などとしています。
参照型を使うと格納されているものが明確になり、他の文字列との取り違えも防ぎやすいです。また、どの階層のコレクションのどのドキュメントを指しているのかが明確になります(文字列の authorId では「多分ルートのusersコレクションのドキュメントを指しているのかな？」程度の推測しかできません)。
その他、合計11個の種類のデータ型に対応しています。JSONより表現力豊かなので、用意されているものの中から最も用途に適したものをしっかり選んで使いましょう。
firebase.google.com
ついでに、上図のように作成日時を表す、timestamp型の createTime も追加してみました。
参照型のauthorRef ではなくオブジェクト型の author として定義して、そこにuserデータそのものを入れるのもありです。
こうすると、 posts コレクション一覧の1クエリで、 author の name など必要な情報を取れる利点があります。 参照型のauthorRef として持っていた場合は、1 postごとに author 取得の1クエリが必要になって、例えば1ページ10件の記事一覧を表示する際に1 + 10 = 11クエリが必要になります。RDBのN+1問題のように深刻に思うかもしれませんが、Firestoreはクライアントからこのように表示件数分のクエリを発行することはよくあること(クライアントサイドジョインと言います)で、あまり気にせずでOKです。とはいえ1クエリで取れる方がクライアント実装もシンプルできる・表示速度も少し上がる・課金額も抑えられる(この場合およそ半分)などのメリットなどもあり、一覧表示の都合だけ考えればオブジェクト型の author として持つことの方が優れています。
一方、オブジェクト型の author 方式では、プロフィール情報変更などで author の name などが変わってしまった場合、紐付いたpostsすべての author 情報変更が必要になります。具体的には次の操作が必要になります。
その author の既存 posts 数が多くなれば理屈的には青天井に重い処理になって行きます。とはいえ、記事数の規模としては高が知れているはずという見方もできるので、ブログサービスの場合は現実的にはこのやり方でも破綻はしないと思います。
というわけで、参照型のauthorRef 方式と、オブジェクト型の author 方式と、迷うところですが、今回は前者のやり方で進めていきます。実際には細かい要件に応じてどちらが良いかはケースバイケースになるはずです。
とりあえず、以上の状態でSwiftでクエリしてみます。まず、ログを見やすくするために、以下を定義しておきます。
次のように posts 一覧を取得してみると、
期待通りのログが出力されました。
id: HuvWyuX0NhjkNQu4LGOY, data: [“body”: Firebase🐶, “title”: Firebaseの記事です, “authorRef”: users/user2]id: aAtCN1BLstIwYpNxB8X3, data: [“body”: Firestoreの記事です。, “title”: Hello Firestore, “authorRef”: users/mono]
先ほどの結果にはmono さんだけでなく user2 さんの記事も含まれていますが、今度はクエリで mono さんだけにしてみます。 `.whereField(“authorRef”, isEqualTo: db.collection(“users”).document(“mono”))`の条件を追加すればOKです。
次に、timestamp型のcreateTimeでもソートしてみます。`.order(by: “createTime”, descending: true)`を追加します。
すると、次のエラーが発生してしまいます。
Error Domain=FIRFirestoreErrorDomain Code=9 “The query requires an index.
パート1の記事で書きましたが、デフォルトでインデックスが有効なのは単一フィールドによるクエリのみなので、2つ以上のフィールドの組み合わせによるクエリでは予めインデックスを設定しておく必要があるのです。
このままエラーログの指示通りに authorRef と createTime の複合インデックスを追加するのもありですが、そもそも posts を author でクエリする機会が多そうな場合、先ほどのように createTime など他のフィールドを組み合わせて絞り込みやソートをすることが頻発しそうなことに気づきます🤔そこで、Firestoreのサブコレクションの出番です。
users のサブコレクションに posts を置くことで、その posts がどのuserに属しているのかの情報が加わり、そもそも author フィールドが不要になります。
mono さんの posts を createTime 順で取得したい場合、次のようなクエリとなります。 .order に createTime を使っている単一フィールドによるソートのみなので、インデックスの追加作成が不要になりました🎉
ただ、users のサブコレクションに posts を移動してしまったため、今度はすべての posts を取得したい時に困ります🤔
元々は db.collection(""posts"") へのクエリで全件取れていましたが、サブコレクション化したことによって db.collection(""users"").document(""mono"").collection(""posts"") という users を介するアクセスとなり全件取得にはユーザー数分のクエリが必要となります。非効率ですしユーザー数増加とともに破綻するやり方です。
ここで、やはりサブコレクションはやめて posts をルートに配置して authorRef フィールドを持たせて、複合インデックスを必要に応じて作るやり方に戻そうかな、というのも1つの考え方です。
一方、サブコレクションは上で述べたような利点や、パート3で説明するセキュリティルールでベターな設定をしやすいなどの理由もあり、やはりサブコレクション構造を維持したいとも思います。
そこで、Cloud Functionsを使って冗長化です。次のような流れで、サブコレクションの posts をルートのコレクションにもコピーします。
こうすることで、 users ごとに分かれた posts も、全体の posts もどちらにもアクセスしやすくなります。多少更新処理が増えてデータ量も2倍になりますが、その分読み取り処理に有利になります。データは更新処理よりも読み取り処理が支配的なため、更新処理やデータの持ち方を多少犠牲にしてでも読み取り処理の都合になるべく合わせようというのはFirestoreなどNoSQLデータベースを扱う上での定石です。RDBでは非正規化は原則しない(特にこのケースのような非正規化は絶対しないレベル)ですが、Firestoreを使う上ではこのあたり柔軟に頭を切り替える必要があります。
Firestoreはクライアントから2以上(500以下)のドキュメントに対して一括書き込みする機能(Batch Write)があります。
firebase.google.com
Cloud Functionsによるコピーの代わりに、これを使ってクライアントから次のように処理することも可能です。
2・3のいずれかの処理が失敗したらすべての処理が失敗するため、片方だけしか更新されなかったという状況も起こり得ません。
一方、 users/{自分のID}/posts という自分のユーザー配下だけでなくルートの posts という全ユーザーが閲覧可能な領域の書き込み権限を直接与えることになるため、パブリックなWebサービスでこの方法を取るのは少し慎重になった方が良いと筆者は考えています。セキュリティルールで authorRef が自身のものであるものしか書き込みできないようにしたりデータ形式を検証したりで想定できる多くの攻撃は防げるとは思いますが(このあたりのセキュリティルールに関しての詳細はパート3で説明します)。
また、複数クライアント対応など考える場合も、すべてのクライアントに上記手順のような複数ドキュメント更新を強いることになる(初期実装もその後の変更も)ため、メンテナンス難易度が若干上がる懸念もあります。
Cloud Functionsでコピーするやり方では、基本的にクライアントは1ドキュメントだけの更新で済み、その後のコピーの仕方はクライアントアップデートとは関係なくいつでも好きなタイミングで自由に変更でき、柔軟性が高いです。そのため、筆者はこういった場合はCloud Functionsでコピーするやり方を取ることが多いです。
firebase.google.com
Cloud FunctionsでFirestoreトリガーを元にドキュメントコピーをするためには、諸々のセットアップがが必要です。
次の指示に従ってインストールします。
nodejs.org
現時点でのCloud FunctionsランタイムバージョンがNode v6.11.5なのでクライアントもそれに揃えることを強くお勧めします。僕は nodebrew でインストールしています。
github.com
この後、JavaScriptではなくTypeScriptで書いていきますのでインストールが必要です。
以下でインストールできます。
firebase.google.com
以上の準備が整ったら、作業ディレクトリにて、 firebase init を実行します。その時点で不要なサービスもあとから使いたくなることが良くあるので、全部有効にしておくのがお勧めです。
また、途中でTypeScriptかJavaScriptか聞かれますが、次のような理由で断然TypeScriptがおすすめです。今後の記述もTypeScript選んだ前提で進めていきます。
それ以外の選択肢は適当にデフォルトのまま進めていけばとりあえず良いです。どれも後から生成ファイルの記述を書き換えるだけで変更可能です。
色々済むと次のような状態になります。
Visual Studio Code を使っていると、Command Palletにて次のように tsc: watch コマンドを実行すると、
次のようにTypeScriptファイルが監視され、JavaScriptへのコンパイルがなされます。
これでCloud Functionsを書く環境が無事に整いました🎵
users/{userId}/posts へのドキュメント追加・更新タイミングで、 posts へ そのユーザーの authorRef 付きでコピーする処理は次のように書けます(実プロダクトでは users などコレクション名に定数を充てるタイポが発生しないように徹底しているなどより丁寧に書いていますがそれらは割愛しました)。
そしてデプロイすると無事に動くはずです🎉
うまく動かなかったら、Cloud FunctionsのLOGSタブを見るなどしてデバッグしましょう💪
console.log なども出力されるので、いわゆるprintデバッグでがんばりましょう。もう少し賢いデバッグ方法もありますが、ここでは割愛します。興味があれば、以下など見て試してみて下さい💁‍♀️
上のドキュメントコピー処理のコードは特に混みいった処理はないため、ここまで理解できていればほぼ疑問なく読めると思いますが、最後の set メソッドについて補足します。
まず、更新系のメソッドは次の3つがあります。
先ほどのコードでは set を使いつつ { merge: true } というオプションを指定してキーの被らない既存データはそのまま残すようにしています。個人的にはオプションでは無く、これに相当する merge メソッドがあっても良かったと思うくらい振る舞いが変わるなあと感じています🤔
どのメソッドが適切かどうかは仕様次第ですが、今回はとりあえず既存の他のフィールドの値を保ちつつすでに存在しているかどうか関係無く使える{ merge: true } オプション付きの set を使っています。
Cloud Functionsもまだベータであり、Firestoreトリガーについては「制約と保証」に書かれていることは強く意識することが求められます。
firebase.google.com
また、上に書いたクライアントから一括書き込みする機能(Batch Write)に比べて、データが伝播されるまで多少タイムラグが発生することに注意です。こういった不整合の瞬間が全く許されないときは、以下などの方法があります。
以上の手順で、次のようなデータ構成となりました。
次回のパート3では、セキュリティルール中心に触れていきます🐶
medium.com
Google Cloud Platform…
552 
1
552 claps
552 
1
Google Cloud Platform 製品などに関連するコミュニティが記載したテクニカル記事集。掲載された意見は著者のものであり、必ずしも Google のものを反映するものではありません。
Written by
Software Engineer(Flutter/Dart, Firebase/GCP, iOS/Swift, etc.) / Freelance / https://mono0926.com/page/job/
Google Cloud Platform 製品などに関連するコミュニティが記載したテクニカル記事集。掲載された意見は著者のものであり、必ずしも Google のものを反映するものではありません。
"
https://medium.com/step-up-labs/our-experience-with-cloud-functions-for-firebase-d206448a8850?source=search_post---------191,"There are currently no responses for this story.
Be the first to respond.
Just a few days ago, Google announced the beta launch of Cloud Functions for Firebase. In a nutshell, they are small pieces of JavaScript functions which are deployed to Firebase servers and executed in response to various events — such as a change in Firebase Database or a new user login.
We at Step Up Labs have been working with Google for the past year or so and are very excited to see the product finally make it to the public beta. As I’ve been working with the Cloud Functions for quite some time now, I thought I’d share some of the details of how we use them in our main product — Settle Up — and the experience we have gained so far. I won’t be describing the API or going over how exactly to work with the Functions, you have the official docs and samples for that.
Let me first briefly tell you what the app does. Settle Up is an app for everybody who needs to track shared expenses. It’s perfect for travelers, flatmates and friends who don’t want to argue over who should pay next or who owes to whom.
In the app, you can create separate groups (e.g. “Skiing trip 2017”). Within each group you can enter members (your friends who went skiing with you) and individual expenses (gas, tickets, hotel, etc.). Based on these expenses, the app can tell you at the end of the trip who should pay whom to break even. One useful feature is that the individual expenses can even be in different currencies, while the debts are displayed in your “home” currency.
All this information — groups, members, expenses and other data — is managed by individual client applications and stored in Firebase Database. There are however some vital parts which we decided to delegate to Cloud Functions.
One thing we want the users of Settle Up to see are the changes done within each group. For example when someone deletes an expense there should be a record about that showing who deleted it. This is a perfect use case for the Cloud Functions. The snippet below roughly shows how we do it (simplified for clarity):
Notice a couple of things here.
Another step after generating a change in the app is to send a push notification to subscribed clients. When a user logs in on their device the app requests a registration token and saves it to Firebase Database. This is what our structure looks like:
There is a nice official sample how to send push notifications with Firebase on github so you can start there. I’ll just outline a few things:
This snippet shows two types of payload — the raw one, which is sent to the web app and Android devices, and the iOS one sent to iPhone users. The raw payload contains plain JSON which the Android version of Settle Up processes on the background and groups corresponding notifications together into single notification. It works similarly with our web app.
The iOS payload on the other hand doesn’t allow any processing and is simply presented on the device as it arrives. Using body_loc and title_loc properties ensures that the notifications are correctly localized.
As Settle Up allows to enter expenses in various currencies we needed a way to work with exchange rates to allow conversion into one group currency. For that there are three pieces of puzzle required:
Having a single location for exchange rates within our database simplifies development of the client apps — to get a currency rate they only need to access single Firebase location and don’t need to worry about anything else. This location has exchange rates between any world currency and USD, gets updated daily and keeps the history for every day. There is also a special node called “latest” which has the latest exchange rates.
To fetch the latest rates every day, we use an HTTPS trigger in Cloud Functions and Google App Engine CRON job which invokes it every day.
One thing I’d like to point out is the response.end() which properly closes the incoming HTTPS requests. Without it the incoming requests will time out.
As you can see, we use Yahoo as a source of exchange rates, but this is an implementation detail which we can easily change in our Cloud Functions without having to make any changes to the client apps.
Changing the group currency is not very straight-forward. Before actually changing it we need to iterate over all expenses in the given group and update their exchange rates. Only after all expenses have the updated exchange rates can we update the group currency. It’s obvious that this needs to be a server function.
We decided to have a dedicated location in Firebase Database called “serverTasks” and have a cloud function watching this location and triggering corresponding functions. Sure, it could be just an HTTPS request (like in the case of getting the latest exchange rates every day) but that would complicate the client code. Simply writing to a Firebase location and observing it to get the result of the operation was just easier.
Here’s what the Database structure might look like:
And here’s the function to change the group currency itself along with registration:
Again a few notes about the implementation:
To join a group the user must be invited via a Dynamic link. When the user clicks the link they are redirected to the Settle Up app and they are offered to join the group. The link is generated locally by the client app and looks something like this:
That’s a pretty long and ugly link. It is however possible to shorten it using a single HTTP POST. Again, to simplify the client code, this shortening is done on the server:
There are two points of interest here. One is how we check whether the link is long (not shortened) — simply by inspecting whether it contains “?link”. This is probably over-simplified, but it works.
The other one has to do with multiple environments. You see, we actively use three environments — sandbox, alpha, and live. The endpoint for shortening links requires an API key of our app (~environment) and since we have three of them we need to make sure the correct one is used in each one of them.
To achieve that we use Firebase Config. Setting up the config is currently a set of the following commands ran once against each environment:
As you can see, we use Firebase and Cloud Functions quite heavily. We also use them in some other scenarios I haven’t mentioned — for example deleting an entire group (which deletes data from multiple locations, as well as from Firebase Storage).
Overall, we’re pleased with where the development is going and the progress the Firebase team have made. Functions are extremely useful to us for a bunch of reasons:
In terms of performance and stability, this has fluctuated during the pre-beta phase of the product but has become pretty reliable as it was approaching the beta release. As far as we can tell, there are no critical issues now that would prevent you from using the Cloud Functions effectively.
Company blog of Step Up Labs, developers of Settle Up.
868 
9
868 claps
868 
9
Written by

Company blog of Step Up Labs, developers of Settle Up.
Written by

Company blog of Step Up Labs, developers of Settle Up.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/shift-studio/flutter-redux-and-firebase-cloud-firestore-in-sync-2c1accabdac4?source=search_post---------192,"There are currently no responses for this story.
Be the first to respond.
Cloud Firestore is a great persistence option to keep your data in sync across mobile and web clients. Redux is a great solution to manage your application’s state in a way that’s easier to reason about.
Things get a bit hairy when you try to integrate an external event driven service, like Firestore or Firebase Realtime Database, into the synchronous world of redux. You need to figure out:
In this article I’ll showcase one way of solving this problem. I’m using Flutter as the context, but the ideas I’m presenting could be used in other environments as well, like react/react-native.
We’ll make a simple counter app with redux, add Cloud Firestore to it and keep the redux store in sync with data from Firestore.
Using redux.dart, flutter_redux and redux_epics.
How exactly? Keep reading… :)
We’re going to keep it simple and use the default counter app that gets created when you make a new project in Flutter.
So make a new project using either Android Studio, Intellij Idea or the command line with flutter create.
Right now, the app uses a StatefulWidget to save the counter. Let's switch it to redux!
I’ll assume you already know how redux works so I won’t go too much into detail how to implement it for the counter app.
Add redux dependencies to your pubspec.yaml file:
The redux package is a great Dart port by Brian Egan and John Ryan. It’s a plain Dart package, so you can use it for a command line application as well, not only Flutter.
The flutter_redux package (say thanks to Brian!) gives us the glue we need to marry together redux and Flutter.
First, we need to define the AppState that will be saved in the redux store.
We have one action that can change the state: a simple IncrementCounterAction.
The reducer is pretty straightforward.
We can put everything together in main.dart.
The redux implementation is pretty straightforward:
If you need help with Firebase integration, there is a great codelab that does just that. The only difference is that we’ll be using only cloud_firestore for this demo.
Add firestore dependency to your pubspec.yaml file:
If we do this right, we’ll be able to count clicks from multiple devices and see changes happening in real time.
MyHomePage is the widget that gets the counter from the store and renders it. As long as this widget is being displayed to the user we want the redux store to be in sync with Firestore.
Flutter offers a very easy way to init subscriptions when a widget is displayed for the first time and dispose them when the widget is gone: we override initState() and dispose() methods from the State class.
But we no longer have a State<MyHomePage>, you might say...
No worries, both StoreBuilder and StoreConnector have 2 callbacks, onInit and onDispose, that we can use.
From flutter_redux:
Perfect! Now we know when to start and stop the Firestore connection.
Let’s add the actions and dispatch them.
We need another action, let’s call it CounterOnDataEventAction, that will be dispatched each time Firestore fires a data change event for our counter value. This will come in handy in the next section.
Next we need to create a middleware that will watch for our request and cancel actions and manage the connection to Firestore.
With redux.dart, a middleware is just a function that receives the store, the dispatched action and a dispatcher (should you choose to let the action flow through).
What redux_epics brings to the table are Dart Streams, and Streams are awesome for event driven systems like Firestore & Firebase Realtime DB.
An Epic is a function that receives a Stream of actions, handles some of those actions and returns a new Stream of actions. That’s it. Actions go in, actions come out.
The actions you emit from your Epics are dispatched to your store, so writing an Epic that simply returns the original actions Stream will result in an infinite loop.
Do not do this!
Ok, now let’s add the redux_epics dependency to pubspec.yaml:
redux_epics comes packed with RxDart.
RxDart gives us a Stream on steroids, called Observable. Observable is a subclass of Stream, so we can use it whenever we need a Stream.
Now everything comes together nicely!
Let’s see what’s happening in counterEpic, step by step:
Now we need to update our counterReducer to handle CounterOnDataEventAction.
That’s it! We have an Epic that will keep our redux store in sync with Firestore.
There’s only one thing missing: we can no longer increment our counter!
Let’s fix that with another Epic.
Each time IncrementCounterAction is dispatched, we increment the counter value by 1 and return a new Observable that will emit CounterDataPushedAction if Firestore updated our counter successfully, or CounterOnErrorEventAction if something went wrong.
We don’t really need CounterDataPushedAction for anything, but we should fulfil the Epics contract: actions go in, actions come out, not nulls. :)
I’ll leave it up to you to handle the error action (maybe display a Toast).
The only thing left is wiring our middleware to the store.
Both EpicMiddleware and combineEpics are provided by redux_epics.
This pattern allows us to keep the widgets simple and testable and offers an efficient way to hook-up into an external event driven system like Cloud Firestore, or Firebase Realtime Database.
This was a long read, I know. Thanks for hanging in there!
You can find the entire project on github. Also, please give some love to the awesome packages we used in this article.
Leave your questions and feedback in the comments, I’ll do my best to reply.
Note: If want to use Cloud Firestore in your projects please vote for this pull request and the covered issues. I need those timestamps :).
Shift STUDIO is a development studio that works with great companies to create amazing apps. We have been building native apps for Android and iOS ever since mobile became a thing… but Flutter got as all wet and excited like never before!
If you wanna see more articles like this, follow us on twitter @shiftstudiodevs .
Contact us about your digital project.
Ideas from Shift STUDIO engineers
1.8K 
3
1.8K claps
1.8K 
3
Written by

Ideas from Shift STUDIO engineers
Written by

Ideas from Shift STUDIO engineers
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://blog.parsecgaming.com/description-of-parsec-technology-b2738dcc3842?source=search_post---------193,"Uncategorized
by Chris Dickson
Tl;dr: We just released an AWS AMI for streaming PC games at extremely low latency and 60 FPS from Amazon. Check it out here.
Last July, I read a highly upvoted article on Hacker News claiming that it was possible to play games from an AWS GPU instance. I immediately followed the steps outlined in the article, and before long I was interactively streaming Steam games over the internet. It was far from perfect, but much better than I expected; first-person-shooter games were pretty unplayable, but console style games like Witcher 3 were more forgiving with latency. While there was much room for improvement, I was convinced that some iteration of the tech described in that article would profoundly change the way we game — and I needed to get involved.
Thus began our journey to understand the latency problem of cloud gaming, and then attempt to solve that problem in the best possible way with Parsec. In cloud gaming, a 20 ms total latency vs. a 40 ms total latency can be the difference between awesome and unplayable — so every millisecond counts.
Parsec is meant to be highly optimized for streaming games, with a lean set of other features to handle things like authentication, port forwarding, and sharing for you. Beyond that, it’s meant to get the hell out of your way and let you enjoy your games. Parsec gives you full desktop access and works with any program, game or not. You can think of it like a remote desktop app on steroids.
In this article, I’ll briefly describe how our technology works (if you’re interested in more depth, let us know so we can post a follow-up) and the measures we’ve taken to eliminate latency wherever it might hide. Parsec is ready for personal use (over LAN or WAN), and we also have an EC2 AMI available for quick cloud setup.
At its core, Parsec is a high performance video streaming app. The process generally looks like this:
Windows (since 8.1) offers a very efficient API to capture desktop frames. The Desktop Duplication API essentially gives you a direct framebuffer grab and places the frame in video memory for processing. Unfortunately, until networking technology gets a hell of a lot better, these frames need some type of compression in order to be streamed over the network at any reasonable frame rate. And when it comes to compression, H.264 is really the only viable choice. Yes, there are other codecs out there, most notably H.265, but let me explain.
We’ve made the decision at Parsec to only support hardware enabled video encoding and decoding. What this means is that we always offload this video processing to special ASICs on your GPU specifically designed to encode/decode certain video codecs. H.264 has the distinction of being by far the most widely supported among hardware vendors, and is thus the default choice for Parsec. H.265 is becoming more widely supported, and will become an option in Parsec soon, but until mass distribution of H.265 happens, we decided to rely on the more widely distributed H.264.
And why only hardware support? Because performance is generally abysmal when the CPU tries to do video processing, and we’d rather not offer it as an option. Lower-end CPUs may struggle to reach 60 FPS at higher resolutions, and even if they do, there is usually a huge latency penalty. Hardware enabled H.264 devices started appearing circa 2012, so if you have a device released in the last four years, you’re probably good to go.
When it comes to GPU hardware vendors, the big three are NVIDIA, AMD, and Intel. Each vendor has a different C API for interacting with their video processing hardware, and with different APIs comes different quirks and performance tweaks. Parsec directly implements these libraries from all three vendors with no wrappers. Depending on how recent your GPUs are, we’ve seen total encode/decode latencies lower than 10 ms, much lower than we were expecting, and much better than latency has been in recent years.
OnLive was founded in 2003, and a recent independent study found that their latency was 150ms, making most twitch games unplayable. Parsec has vastly improved on that experience, but it wouldn’t have been possible without the leaps in technology made since OnLive appeared in 2003.
Dealing with H.264 is only half the battle. As mentioned above, the Desktop Duplication API captures frames in video memory. Parsec is designed from that point onward is to never let the raw frame touch system memory, which would require the CPU to copy raw frame data from the GPU. This means the raw captured frame must pass directly to the encoder, then once the frame is decoded into video memory on the client, it is rendered to the screen directly without any intermediate CPU operations. Any copy into system memory will have a noticeable latency impact.
This is where things can get tricky; if you’ve ever worked with raw video before, you’re probably well aware of color format conversions, specifically from an RGBA style format to a YUV format, and vice versa. For those that have never heard of color formats, let me fill you in.
RGBA is easy to understand. Let’s say you have a 1920×1080 pixel image in RGBA, specifically a 4 byte per pixel format. Roughly 2 million pixels, about 8 MB in size. Each pixel is 4 bytes with 1 byte dedicated to each R, G, B, and A color channel in the that order. These four channels are blended together to create a single color per pixel (also with an opacity value in the case of A), and that’s really all there is to it.
YUV color formats work differently. There are many different kinds of YUV formats, but the NV12 format is the most well represented among video processing libraries, so that’s what I’ll be referring to in this article. NV12 is a planar format, meaning that one section of the frame contains a contiguous Y “luminance” component, and a different section of the frame a UV “chrominance” component. For simplicity, you can think of the Y component as a monochrome image of the raw frame, and the UV component as the color that gets blended on top of that monochrome image. So our same 1920×1080 raw frame in NV12 would begin with a 1920×1080 Y block of 1 byte, essentially monochrome pixels. Immediately following the Y block, we have our block of alternating U and V components, the full block exactly half the height and width of the Y block.
So why is this a problem? If you’ve ever worked with OpenGL or DirectX, you know that the back buffer (the place where the rendering happens) expects some type of RGBA format. The output from the decoder in NV12 must be converted if we are to render it and display it on the screen.
Given the fundamental differences in the two color formats, converting from one to the other is a costly process when done on the CPU. This is why Parsec performs all color conversion on the GPU via pixel shaders, both with OpenGL (macOS) and DirectX (both 9 & 11 on Windows). The final “conversion” renders the raw frame directly to a back buffer, which can then be efficiently displayed on the screen. The raw frame never leaves the GPU, improving latency and saving the CPU a lot of extra work.
If you’d like more information on how we set up these shaders in OpenGL or DirectX, let us know in the comments.
So we finally got that frame as efficiently as possible to the back buffer. But now it needs to be swapped to the front buffer and displayed on the screen. This doesn’t sound hard, and if you’re OK with video tearing, it’s not. The best outcome in terms of latency is to not delay this swap at all and swap immediately after the frame is rendered. Unfortunately, the tearing that can result with this technique is unacceptable, so some kind of synchronization with the refresh rate of your monitor is required — a.k.a V-sync.
The added problem with cloud gaming is that frames arrive in unpredictable intervals. It would be nice if we received frames at precisely 60 FPS with exactly 16.66667 ms between them. But even then, you’ve got an issue, because the client screen’s refresh rate will never match up perfectly to the rate you’re receiving frames. The result is V-sync either accumulating frames, skipping frames, or if left off, causing tearing. Buffering can solve the problem, but I shouldn’t have to tell you that is out of the question 🙂
This is an area of ongoing testing and experimentation for us. Parsec by default enables V-sync, but drops accumulated frames if the frame rate on the server is slightly higher than that of the client. With OpenGL on macOS you don’t have a ton of control, but with DirectX there are a bevy of different swap effects to choose from to try to optimize this. Currently Parsec defaults to the flip-sequential effect as this seems to yield the best performance. We are considering making this an advanced option in the future, since different swap effects seem to work differently on different machines. V-sync can be turned off in the control panel for testing.
You may be thinking: “Yeah, all this is cool, but what about network latency. You can’t do anything about that.”
You’re right. We can’t do anything about that. But Parsec is meant to be a forwarding-looking product that will be able to grow into infrastructural network improvements, and from the looks of it, things are getting better rapidly as the government is pushing further investments in broadband to the point that more than 90% of the US has 25 Mbps internet.
Cloud gaming may not work for everyone now, but it will probably work for most people fairly soon. With network latencies below 20 ms, and bandwidth above 10 Mb/s, Parsec offers a near-native experience.
You can think of Parsec as a research project where the goal is to eliminate latency from cloud gaming. The first iteration may not be 100% of the way there, but you can rest assured we are working everyday to find a way to eliminate that next millisecond.
And as Al Pacino once said:
“Cloud gaming is a game of milliseconds. At Parsec, we fight for that millisecond. We claw with our fingernails for that millisecond. Cause we know when we add up all those milliseconds, that’s going to make the fucking difference between winning and losing.”
 
Parsec's new Teams API lets companies build Parsec into their existing workflows. Automate deployment, manage a team, and more.
Uncategorized
We’re thrilled to announce that we’ve raised a $25 million Series B, led by Andreessen Horowitz.
Uncategorized
All over the world, creatives are relying on Parsec and Parsec for Teams to keep media, entertainment, and, of course, games on track during the pande ...
Uncategorized
© 2016-2021 Parsec Cloud, Inc.All rights reserved.

These cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site. All information these cookies collect is aggregated and therefore anonymous. If you do not allow these cookies we will not know when you have visited our site, and will not be able to monitor its performance.
These cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising.
These cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages. If you do not allow these cookies then some or all of these services may not function properly.
These cookies are necessary for the website to function and cannot be switched off in our systems. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.
Back
Clear Filters
All Consent Allowed
By clicking “Accept All Cookies”, you agree to the storing of cookies on your device to enhance site navigation, analyze site usage, and assist in our marketing efforts. 
"
https://towardsdatascience.com/giving-vertex-ai-the-new-unified-ml-platform-on-google-cloud-a-spin-35e0f3852f25?source=search_post---------194,"Sign in
There are currently no responses for this story.
Be the first to respond.
Lak Lakshmanan
May 19, 2021·7 min read
The Google Cloud AI Platform team have been heads down the past few months building a unified view of the machine learning landscape. This was launched today at Google I/O as Vertex AI. What is it? How good is it? What does it mean for data science jobs?
The idea is that there are a few key constructs in machine learning:
Yet, depending on how you do your ETL (do you store your data in CSV files? TensorFlow records? JPEG files? In Cloud Storage? In BigQuery?), the rest of the pipeline becomes very different. Wouldn’t it be nice to have the idea of an ML dataset? That any downstream model (TensorFlow, sklearn, PyTorch) can use? That’s what it means to unify behind the concept of a dataset.
Also, the way you deploy a TensorFlow model is different from how you deploy a PyTorch model, and even TensorFlow models might differ based on whether they were created using AutoML or by means of code. In the unified set of APIs that Vertex AI provides, you can treat all these models in the same way.
Vertex AI provides unified definitions/implementations of four concepts:
The key idea is that these artifacts are the same regardless of the type of dataset or training pipeline or model or endpoint. It’s all mix and match. So, once you create a dataset, you can use it for different models. You can get Explainable AI from an endpoint regardless of how you trained your model.
This is clearly visible from the web UI:
Okay, enough of the why. Let’s take it for a spin.
It’s always good when you are training an ML model using new technology to compare it against something that you know and understand.
I’ll use the Airbnb New York City dataset on Kaggle to predict the price of an apartment rental.
First, we load the data into BigQuery. This data set is a bit unwieldy and required cleanup before I could load it into BigQuery (see this blog post for details), but I made it public, so you can simply try out the query:
Having verified that the data look reasonable now, let’s go ahead and train an xgboost model to predict the price. This is as simple as adding two lines of SQL code:
We get a model that converges quickly
…and achieves a mean absolute error of $64.88.
How good is this? What’s the mean price of a New York city apartment on Airbnb?
That is $153. So, we are within a 40% off. Maybe not a model to go marching into production with … but you wouldn’t expect a public dataset to have the more proprietary and personalized data that would help improve these predictions. Still, the availability of this data helps show us how to train an ML model to predict the price.
Next, let’s try Auto ML Tables in Vertex AI.
Let’s start the with CSV file that is on Kaggle so that we are not stuck with the data transformations I did to load the data into BigQuery. Go to https://console.cloud.google.com/vertex-ai/datasets/create
About 3 hours later, the training finished, and we can examine the model and deploy it if we’d like:
We get a Mean Absolute Error of $55, almost 20% better than what we got with xgboost. So, a more accurate model, with less code. With evaluation, feature importance, explainability, deployment, etc. all provided out-of-the-box.
Both BigQuery ML and AutoML Tables are easy-to-use and awfully good. It helps to understand the fundamentals of the domain (why are reviews missing?), and of machine learning (don’t use the id column as an input!), but they are quite approachable from the perspective of a citizen data scientist. The coding and infrastructure management overhead have been almost completely eliminated.
What does this mean for data science jobs? The move from writing Raw HTML to using weebly/wix/etc. hasn’t meant fewer web developer jobs. Instead, it has meant more people creating websites. And when everyone has a basic website, it has driven a need to differentiate, to build better websites, and so more jobs for web developers.
The democratization of machine learning will lead to the same effect. As more and more things become easy, there will be more and more machine learning models built and deployed. That will drive the need to differentiate, and build pipelines of ML models that outperform and solve increasingly complex tasks (not just estimate the price of a rental, but do dynamic pricing, for example).
Democratization of machine learning will lead to more machine learning, and more jobs for ML developers, not less. This is a good thing, and I’m excited about it.
And, oh, Vertex AI is really nice. Try it on your ML problem and let me know (you can reach me on Twitter at @lak_gcp) how it did!
Data Analytics & AI @ Google Cloud
451 
2
451 
451 
2
Your home for data science. A Medium publication sharing concepts, ideas and codes.
"
https://medium.com/@caityjohnstone/the-mushroom-cloud-angel-d98ee2a3c814?source=search_post---------195,"Sign in
There are currently no responses for this story.
Be the first to respond.
Caitlin Johnstone
Oct 23, 2017·2 min read
A mushroom cloud angel came to visit me one nightwhile I was drinking whiskey and Gatoradeand arguing with a wrong person on the internet.It smelled like ozone and tire fires.Its eyes stretched all the way back to the Big Bang.
“Uhh, look,” I said as my vase of petunias wilted in its presence.“Whatever this is, I’m really not ready for it.I haven’t showered, and my credit’s a mess,and I’ve got unresolved issues with my mother still.Emma Carmichael next door, you should try her.She’s got her shit together.She drives a Honda Odyssey.”
There was a deep rumbling in my bones,and the paint on the walls began to peel.A cockroach scuttled out backwards from its hiding place.My office supplies started to levitate,and suddenly I knew what the creature wanted.Why it had come to me.
“Oh,” I said.“Okay. Let me think.”
I told it about how I try really, really hardnot to let any moment here go to waste.How even if I’m just watching The Bacheloretteor doodling in my notepad or looking out the window,I try to really feel every part of it.I told it how the beauty of my lover’s face makes me weep,and how I cherish every time my kids include me in their things.How the galahs and magpies bicker every morningand how they make me smile while I sip my black coffee.How I know it’s spring when the magnolias blossom,and how jasmine means summer’s near.
I showed it everything I’ve come to adore about people;our frailty, our ferociousness,our relentless drive to create.The guileless symbiosis of an elderly married couple,the elegant awkwardness of teenagers,the desperation of parents trying not to screw up too bad.How thrilling it is to start again every sunrise.How terrifying it is to fall more in love every daywith someone who can’t live forever.
I invited it into my body and let it walk around in my skinso it could feel how awesome it is to be human.Still haven’t gotten the smell out of my nostrilsor the ancient eons out of my veins.
“Well?” I asked it, my face dripping sweat,when I’d run out of reasons to offer.“Do we get to stay here or not?”
The mushroom cloud angel turned and walked away,burning craters in the Carmichaels’ lawn with its feet.I don’t know who else it has talked to since,or how often we’re made to answer,but the bombs still haven’t dropped.We’re all still here,come what may.
Emma was so pissed off.
— — —
I write about the end of illusions.
1.8K 
9
1.8K 
1.8K 
9
I write about the end of illusions.
"
https://medium.datadriveninvestor.com/could-cloud-mining-services-such-as-hashmart-be-the-gateway-to-taking-bitcoin-mining-mainstream-3fa630829a93?source=search_post---------196,"There are currently no responses for this story.
Be the first to respond.
www.amazon.com
I have mined before. Yes, I have had my own fair attempt at mining gold at my father In-law’s farm in the mountains.
I can say now with an earned certainty that mining is a lot of hard work. There are also various methods and tools that one can employ to extract gold from the belly of the earth and in my case, I am more was comfortable using an old fashioned prospector’s trowel and a basic metal detector machine.
www.datadriveninvestor.com
As they say, you can never forget your first — I will never forget the day I ‘mined’ my first gold nugget. After picking out a promising patch of land to prospect, I got a couple of good signals from my metal detector in a certain area but I initially dismissed them as I was thinking that I might be picking up ‘hot' rocks. So I switched on my iron discrimination option to see if that would phase out the target, but I kept getting a signal.
I then dug the spot with a hand trowel to check it out further, but the beeping continued. “One more shovel full, then another, then just one more,” I told myself. When I finally ran a handful of material by the search coil, the detection machine sounded out a tremendously loud scream. It was a sound similar to the whistle of an old school steam locomotive!
So I thought I had found yet another steel nail or a discarded piece of junk metal since the area was littered with them. But then I began searching through the dirt in my hand. And there it was, Gold!
Bitcoin mining is a lot the same! Mining bitcoin is more or less the same as the routine that I have outlined above. Although in place of mining in the dirt this time the mining is completed online. And In place of tools like trowels, picks, and metal detectors, bitcoin miners employ Application-Specific Integrated Circuits (ASICs) or/and Graphics processing units (GPUs).
Bitcoin mining has become so competitive of late that it can only be done profitably with the most up-to-date ASICs. When using desktop computers, GPUs, or older models of ASICs, the cost of energy consumption exceeds the revenue generated that even with a newer model at your disposal, one computer is rarely enough to compete with what miners call “mining pools.”
A mining pool is a group of miners who combine their computing power and split the mined bitcoin between participants. A disproportionately large number of blocks are mined by pools rather than by individual miners. In July 2017, mining pools and companies represented roughly 80% to 90% of bitcoin computing power.
By definition, bitcoin mining is the process of securing the Bitcoin network performed by high-powered computers that solve complex computational math problems (the problems are so complex that they cannot be solved by hand — so yes, you can put away your pencil and writing pad!). The luck and work required by a computer to solve one of these problems is the equivalent of a miner (read: me) striking gold in the ground — while digging in a sandbox. The odds of a computer solving one of these problems is 1 in 6 trillion! That level is adjusted every 2016 blocks, or roughly every 2 weeks, to keep rates of mining constant.
For bitcoin miners to earn bitcoin from verifying transactions, two things have to occur. First, they must verify 1 megabyte (MB) worth of transactions, which can theoretically be as small as 1 transaction but are more often several thousand, depending on how much data each transaction stores. Then as mentioned previously, to add a block of transactions to the blockchain, miners must solve a complex computational math problem, also called a “proof of work.” What they will be doing is trying to come up with a 64-digit hexadecimal number, called a “hash,” that is less than or equal to the target hash. A miner’s computer spits out hashes at a rate of megahashes per second (MH/s), gigahashes per second (GH/s), or even terahashes per second (TH/s) depending on the unit, guessing all possible 64-digit numbers until they arrive at a solution. In other words, it’s a gamble!
Mining cryptocurrencies has become a sensation for the players in the industry as the adoption of digital assets goes global. Bitcoin mining remains one of the most profitable coins to mine given the high reward price of finding a block- but with high rewards comes high risk and costs. With over 10000 individual nodes mining the Bitcoin blocks, the odds of a miner making it while running solo are not really amazing.
As previously promised, the book exploring this topic deeper and further is now finished and ready for you.
Types of mining
solo miningThe solo mining of BTC using your Personal Computer (PC) or mining rig is highly unprofitable given the high difficulty in finding a block. Yes, the machines are yours and you can track all expenses and get all revenue from mining. But on the other hand, running your own rig is so difficult as all responsibilities such as costs of setting up hardware, technical maintenance, tracking profitability falls squarely on your shoulders. That’s why you should draw your attention to cloud mining.
Cloud miningCloud mining facilitates the process of mining cryptocurrencies via the internet. Cloud computing is one of the fastest-growing trends wherein computing services such as servers, databases, software, and storage are accessed via the cloud. Big companies with the ‘muscle’ in terms of resources to pull this off facilitate these services and such companies charge on a usage basis just like we pay for our water or electricity usage.
Cloud mining opens up the world of cryptocurrency mining to people who may be in remote locations and might have little or no technical knowledge and/or the hardware infrastructure. The process of cloud mining is very simple and only requires a person to open an account with a cloud mining company via its website and select certain things like the contract period and hashing power.
However, the presence of fraudulent companies or scammers who pose as legitimate mining companies and take prospective cloud miners for a ride cannot be ignored. Therefore one must be sure and it’s of utmost importance to carry out a thorough background check on a cloud mining service provider before parting away with hard-earned money.
How Hashmart is making cloud mining seamless for everyone Unlike many mining pools and actors in the crypto mining world who seem to try to complicate things and operate under a ‘cloud of mystery, the Siberian-based bitcoin mining company, Hashmart has sought to simplify the mining process by making its online platform simple, easy to use, with customer centred easy to understand data and statistics.
www.amazon.com
Yes, Hashmart is based in Siberia and Siberia is famous for its cold weather and low rates of electricity (the region charges 2.1 roubles ($0.04) per kilowatt-hour, compared with 5.3 roubles in Moscow) which makes it a very ideal bitcoin mining hub given that bitcoin mining equipment require adequate cooling and tends to be electricity-intensive.
Hashmart is a simple and affordable cloud mining service whose main aim is to introduce the world of Bitcoin and other cryptocurrencies to a wider audience. As of now, Hashmart only offers Bitcoin cloud mining services using the SHA256 mining algorithm, with Ethereum and other major cryptocurrencies being added soon.
As alluded to earlier, the process of cloud mining makes one a participant in a mining pool and involves buying a certain amount of “hash” power. The Hashmart cloud mining platform ensures that that the user does not need to buy any mining equipment as all you need to buy is hash power in the form of mining contracts. While everyone has his/her own strategy in mining and it is up to them to decide what type of contract is the right fits for them, Hashmart provides 2 types of contacts;
1. The 12 months’ contract which does not have maintenance fees associated with it and the holder is able to withdraw all their mined BTC at the end of the 12 months contract period.
2. The open-ended contract which literally operates in perpetuity as it does not have an end date and is more suitable for people who prefer a long-term opportunity to mine BTC as it not limited by any timeframes.
Each participant in a cloud mining pool has a rightful share of the profits in proportion to the allotted hashing power and with Hashmart payments are made daily. The first payment is credited to your account within 24 hours after you pay for the contract. Hashmart also provides simple income forecasting where the user receives precise information about his/her income in the future as soon as they make their purchase of their mining contract. All this information is presented via an easy to read and understandable user dashboard on the Hashmart website.
Make sure to check out Hashmart’s website and demo account so that you can tap into the current Bitcoin bull run!
The above article is written from an independent perspective as an entry into Hashmart’s writing competition. The above does not constitute financial advice and independent financial advice should be sought before deciding whether to invest in any financial product.
If you enjoyed this article, follow me on Twitter / Linkedin. You can also buy me a coffee HERE!
empowerment through data, knowledge, and expertise.
1.5K 
2
1.5K claps
1.5K 
2
empowerment through data, knowledge, and expertise. subscribe to DDIntel at https://ddintel.datadriveninvestor.com
Written by
Blockchain Consultant || Fintech || Author: The Rise of Blockchain for Agriculture (https://www.amazon.com/dp/B08KHC3WCF) Email: tendaitomumedium[at]gmail.com
empowerment through data, knowledge, and expertise. subscribe to DDIntel at https://ddintel.datadriveninvestor.com
"
https://medium.com/predict/a-virtual-jewish-nation-may-teach-the-world-how-to-live-on-the-cloud-95a7b60f5648?source=search_post---------197,"There are currently no responses for this story.
Be the first to respond.
We’re all familiar with “the cloud.” But have heard about “cloud nations”?
If you ask futurists and visionaries, such as Dr. Roey Tzezana, they will tell you that in the coming decades we will no longer need the territorial institutions of the world. Instead, most of the government and civil services will be provided automatically and from anywhere in the world through newly developing technologies such as Blockchain.
Essentially, “cloud nations” are a virtual way for a large number of people to unite under a common identity, and easily program their own “smart contracts” that will transparently oversee the conduct of a large number of individuals. And at the same time, enjoy all the rights and services that currently only exist in the physical world.
Cloud nations are not bounded by territory, they have no borders, and citizens can leave and enter these “states” freely. Cloud nations can replace a large number of institutions and governmental bodies, and even transfer the power of decision-making to citizens, gradually rendering governmental functions obsolete.
Cloud nations will provide immediate and decentralized systems of justice, so most of the functions of courts won’t be necessary. Citizens will receive prompt justice anywhere and at any time, and have an effective government that will act for them on their own behalf.
In addition, cloud nations will allow for a cooperative economy that doesn’t concentrate power in the hands of greedy economic tyrants. And obviously, the artificial intelligence of the cloud nation will know how to answer any question, as it will analyze all the information that exists from the dawn of humanity, and also calculate the basic needs of every human being to provide him with what he wants with a push of a button and 3D printing.
As mind bending as this future scenario sounds, Dr. Roey Tzezana doesn’t stop there. In his recent book, “Rulers of the Future,” he takes the idea of cloud nations another step further.
Together with Jewish thinkers and researchers, the futurist is working on the “Jewish Cloud Nation” project — a virtual state that will finally unite all Jews around the world, regardless of their ethnic or religious affiliation, and provide services to all of them “in a fully decentralized way with no geographical limitations.”
The core values of the Jewish Cloud Nation will be determined by a committee of Jewish sages that will include the great thinkers and social scientists who will meet physically and virtually to decipher and define common values for all Jews around the world.
According to the “Jewish Cloud nation” website, the aim is to realize the momentous Jewish value of “Tikkun Olam” — an old-new principle according to which Jews carry a responsibility for the entire world. “The code of the Jewish Cloud Nation will be open and transparent, and will be shared with everyone to allow for further creation of other cloud nations, to serve the needs of citizens of the entire world without the involvement of dysfunctional governments or corrupted regimes.”
It is nice to see that experts who recognize the imminent technological future, also envision the emergence of a cooperative society and even take steps to realize the vision. Such a vision corresponds directly with the social teachings that appear in the authentic writings of Kabbalah. About a century ago, Kabbalist Yehuda Ashlag wrote: “Do not be surprised if I mix together the well-being of a particular collective with the well-being of the whole world, because indeed, we have already come to such a degree that the whole world is considered one collective and one society.”
The process that we’re about to enter is no less than the birth of a new humanity. From day to day, we are witnessing the culmination of a multi-faceted global crisis that will necessitate us to reorganize human society. We will have to adapt to our interdependence, as well as our interconnection with the natural system. And technology experts are certainly among the pioneers to identify this trend. What’s more, they recognize the practical tools to create an infrastructure that will enable a new social order at every level: economic, social, political, educational etc.
And yet, smart contracts are not enough to bring people together. Technology does indeed upgrade our abilities, but it doesn’t upgrade our inner qualities and the way we relate to each other.
If the “Jewish Cloud Nation” aspires to be an exemplary society based on mutual concern, it must begin from an educational process that is welcomed by its initial citizens. They will have to encourage mutual consideration and build new norms and values for positive social engagement. Ultimately, they need to develop a new way to sense each other, as pieces of a single whole. This is a conscious development that requires people’s willingness to change themselves and upgrade their perception of life.
A prime example of such a change is the issue of privacy. To futurists such as Tzezana, It’s clear that no matter how much we discuss the right for privacy in our time, in the technological future, the struggle for privacy is a lost battle. Also, the renunciation of privacy will help the system benefit the individual.
Privacy in cloud nations will have to be reduced to a minimum, but the willingness to give up one’s privacy is a matter that requires great preparation and adoption of new values that will exchange the need for privacy with real benefit. This is just one example of the need to change human consciousness and nurture human connection.
As we move towards the structure of the future society that our technology is weaving before our eyes, we will have to adapt to values and concepts that today seem utopian or simply delusional. Therefore, rather than expecting our technology to direct us, we should educate ourselves towards the change we have to go through — becoming interconnected human beings in a society that promotes unity and rewards mutual concern and consideration.
If the pioneers of the Jewish Cloud Nation train themselves for this, rains of blessings will come down from the Jewish Cloud Nation to the entire world.
where the future is written
3.9K 
5
3.9K claps
3.9K 
5
Written by
PhD in Philosophy and Kabbalah. MSc in Medical Bio-Cybernetics. Founder and president of Bnei Baruch Kabbalah Education & Research Institute.
where the future is written
Written by
PhD in Philosophy and Kabbalah. MSc in Medical Bio-Cybernetics. Founder and president of Bnei Baruch Kabbalah Education & Research Institute.
where the future is written
"
https://medium.com/@charpellumeh/build-a-serverless-full-stack-app-using-firebase-cloud-functions-81afe34a64fc?source=search_post---------198,"Sign in
There are currently no responses for this story.
Be the first to respond.
Ebuka Umeh
Jun 6, 2018·12 min read
In this article, I am going to demonstrate how to build a full stack application without a server.First we are going to build the serverless part with 3 endpoints(4 actually, the 4th endpoint will be your take home assignment), then we build a minimal shopping list app with React that will make use of these endpoints and persist to a database.
You can find the project repos for both serveless-firebase and react app.
What is Serverless?According to Wikipedia, Serverless is a cloud-computing execution model in which the cloud provider dynamically manages the allocation of machine resources. Pricing is based on the actual amount of resources consumed by an application, rather than on pre-purchased units of capacity. The server management and capacity planning decisions are completely hidden from the developer.
Serverless can be explained in the perspective of humans utilizing water. Water is used on a need to need basis, when people are thirsty, they turn on the tap and drink water. Once they are done they close the tap till there is a need to use the water again. This is an abstract explanation of what serverless achieves, resources are available only when they are requested.
What is Serverless Firebase Cloud Function?In lieu of setting up a server, routing requests, defining REST resources and then creating methods for GET, POST, DELETE and PUT, one can use an already existing server made available by google.
According to the Firebase Cloud Function documentation:
Cloud Functions for Firebase let’s you automatically run backend code in response to events triggered by Firebase features and HTTPS requests. Your code is stored in Google’s cloud and runs in a managed environment. There’s no need to manage and scale your own servers.
Firebase’s goal is to help you worry less about your server implementations by abstracting it, to let you focus more on your front-end implementations. Significantly, it implies that one can build a full-stack app in a timely fashion.
They do this by taking care of your app’s back-end or infrastructure. Hence, Firebase let’s you focus on solving problems for your users.
Four reasons why you should go Serverless.
Problem Statement
Traditionally, you would have to deploy your private code to a back-end server that you manage. You’d also have to build a full fledge server using popular technology/frameworks like Node/Express, Ruby/ Ruby on Rails , Python/Django then connect to database like Postgres, MongoDb then host on heroku or other web hosting services.
The app would then use an API to communicate with the server and issue requests.
Solution
However, with firebase cloud functions, you don’t need to set up, maintain and scale your backend to get its benefit.
Instead, you can write and deploy code to firebase cloud servers that automatically responds to events in your app.
With a function, you can authenticate users, communicate with database, upload pictures and receive real time notifications.
Requirements for building this app:
As of the time of writing this post, Cloud Functions is still technically in BETA and has some areas for improvement. Secondly, it only supports Node.JS.
Cloud Functions runs Node v6.14.0, so its recommend that you develop locally with this version or greater.
To confirm your installations, run the following command:
If you get their version numbers as results, then you’re good to go.
If you’re not familiar with firebase, don’t worry — the firebase API’s are quite easy to use. Infact, this should be a good opportunity for you to learn and get your hands dirty with firebase. Tutorialspoint is another great resource you could leverage on to get you started.
This tutorial will be divided into 3 phases.- Setting up Firebase.- Creating the endpoints.- Building a minimal React App to consume the endpoints.
Phase 1
Creating a Firebase Project
4. Add project, give it a name and click on CREATE PROJECT
Once you have created the project, you will be directed to the firebase console
Setting up Firebase Locally.
On your Computer, open a terminal and create a new folder.
Let’s call this folder shop-list-server. Navigate into the folder
Once you have Node.js and npm installed, install the Firebase CLI via npm:
npm install -g firebase-tools
This installs the globally available firebase command.
If the command fails, you may need to change npm permissions.
To initialize your project:1. Run firebase login to log in and authenticate the firebase tool.
firebase login
Allow Firebase to collect anonymous CLI usage and error reporting information?
This will open your browser for you to choose an account
Once you are logged in, Run firebase init functions.
This brings a list of all your projects, use the arrow key to select shop-list and enter
What language would you like to use to write Cloud Functions?
Do you want to use ESLint to catch probable bugs and enforce style?
Do you want to install dependencies with npm now?
This will setup the project and install all your dependenciesOnce the installation is complete, open your project folder, it should look like so:
Creating first endpointWe create endpoint by defining a function which exposes a URL in the cloud.
Let’s create a simple endpoint that will display on the browser Hello user, welcome to Serverless Database! to our user.
To create our first endpoint, first change directory into the function folder
open the index.js file and insert the following code snippet
The functions.https is used to create a function that handles HTTP events. The event handler for an HTTP function listens for the onRequest() event.
The onRequest() takes arguments, the Request object gives you access to the properties of the HTTP request sent by the client, and the Response object gives you a way to send a response back to the client.
Run either of the following command on your terminal.firebase deploy or firebase deploy --only functions:helloWorld
Once the app has been successfully deployed, the URL will be generated
The URL generated for me https://us-central1-shop-list-b60aa.cloudfunctions.net/helloWorld
To test, copy YOUR URL to your browser and enter the URL generated for you, it should look like so:
https://us-central1-xxxxxxxxxxxxxxxxxxxx.cloudfunctions.net/helloWorld
Congratulations, you have created your first endpoint.
Phase 2
Creating the endpoints.
Now that we have created our helloWorld endpoint, its time to get serious.We will be creating 3 endpoints that will add a shop list, get all shop list from the database and delete an item.
However let’s set up the database first.
Pre Setup
1. Enable CORSBy default, our server will not accept request from any public content and will deny access to any universal JavaScript/browser, except we enable CORS.
CORS means Cross-Origin Resource Sharing and its a specification that enables truly open access across domain-boundaries.
CORS introduces a standard mechanism that can be used by all browsers for implementing cross-domain requests.
We would not go in depth into CORS as it is beyond the scope of this article.
To enable CORS, we install and configure it. Open your terminalnpm i -S cors
Then we import and configure it const cors = require(‘cors’)({ origin: true });
This is a basic CORS configuration, the { origin: true } command automatically allow cross-origin requests to be made
2. Setup the databaseBefore we create the url where the data will be persisted to, the database path has to be configured
Now your index.js should look like this
1 POST /your_url/addItemThis function will generate the url which would allow us create a shop item and persist to the database.
addItem(), which exposes a URL that accepts a text value and writes it to the Realtime Database.
cors is a function that takes 3 parameters; req, res and anonymous function where all of or logic would reside.
This snippet below, specifies only POST method should be used, since we are writing to the database
For debugging purpose, you can see the event logs on logs section of functions
Firebase method for saving data
This will return the list of data after it has successfully saved.
The final code should look like this
Run the following command on your terminalfirebase deploy or firebase deploy --only functions:addItem
firebase deploy deploys all the functions in your code while firebase deploy --only functions:addItemdeploys a single function specified.
Once the app has been successfully deployed, the URL will be generated.
The URL generated for you would look like so:
https://us-central1-xxxxxxxxxxxxxxxxxxxx.cloudfunctions.net/addItem
Postman will be used for testing this time.
A method other than POST, will throw an error.
Congratulations, you have successfully made a post request.
To see the item posted, log back to the console of firebase.
We have only created two functions for now, which are the helloWorld and addItem.
Click over to the LOGS tab, this is where we see all the console.log statement we create in the code. Secondly, this tab displays all the errors that may occur in the code. Take advantage of it.
Add more shop items to the List.
Check Database to Confirm the list of shop items.
2 GET /your_url/getItems
This function will generate the url which would allow us get all the shop list from the database.
Add the following code snippet;
This snippet below, specifies only GET method should be used
Run the following command on your terminal.firebase deploy or firebase deploy --only functions:getItems
Once the app has been successfully deployed, the URL will be generatedThe URL generated for you would look like so;https://us-central1-xxxxxxxxxxxxxxxxxxxx.cloudfunctions.net/getItems
Let’s test with Postman.
Before we create the last function, let’s refactor out code to follow DRY principle.
DRY means Don’t repeat yourself
As Developers, it is good practice to follow design principle where each building block, (it can be a class, a module, an object or even a function) of a system should have only a single responsibility. This is known as Single Responsibility Principle (SRP)
Refactoring Our Code
Presently our code looks like
If you look closely at the code above, we returning the result from the database in both the addItem and getItem function. This can be made easy by extracting the code into a function.
Refactoring
The getItemsFromDatabase() is a helper function which has been created to abstract repeated codes from addItem and getItems.
3 DELETE /your_url/deleteItem
This function will generate the url which would delete an item.
Add the following code snippet
Because we are deleting, the payload will be passed as a query.
A firebase endpoint for deleting resource from database.
The getItemsFromDatabase() helper method we created earlier is been reused inside the deleteItem function, thus following DRY principle.
Run the following command on your terminal.firebase deploy or firebase deploy --only functions:deleteItem
Once the app has been successfully deployed, the URL will be generatedThe URL generated for you would look like so;https://us-central1-xxxxxxxxxxxxxxxxxxxx.cloudfunctions.net/deleteItem
Testing with Postman, let’s delete Carrots item with its id.
We use the id of each item and pass it as a parameter to the header.
Use DELETE method
Use params not body
The source code is available on GitHub: HERE, feel free to fork and contribute to it.
Phase 3
Create React app to consume the endpoint.
Note: If you have not installed create-react-app, you may want to run this command first npm install -g create-react-app
On your terminal, run
After installation,
Then
NB: The React implementation used in this tutorial is modeled after the original creator, a major portion of it was used for illustration purpose. You can checkout it out here if you want to know more about the project.
Open the public/index.html file, add the following code snippet to enable bootstrap
Bootstrap CSS link in head tag
Jquery and Bootstrap js link in body tag
Source code for index.html file
Install axios
Finally we make use of the 3 endpoints we created in Phase 2.
Open your folder, in the src/App.js file, delete everything and replace with the following code snippet below.
In the constructor, we set the buyItems state to an empty array.
In componentDidMount method, we make an http request with axios to fetch the list of items with my URL https://us-central1-shop-list-b60aa.cloudfunctions.net/getItems as soon as the component has mounted, it would populate the buyItem state with the response data.
Replace the URL with yours
The addItem method adds new item to the list. First, it gets the buyItem state and the value of the newItem from the user’s input, then it checks if the newItem already exist in the list. If the item does not exist, it post the newItem with the https://us-central1-shop-list-b60aa.cloudfunctions.net/addItem endpoint and set the buyItem state to the new response from the database.
Replace the URL with yours
The removeItem method removes an item from the database.
GET
POST
There you go!
The source code for the frontend app is available on GitHub: HERE, feel free to fork and contribute to it.
Thanks for reading 😄 Please 👏 if you liked this article. If you have any questions at all, feel free to comment below and i will respond asap!
I can’t wait to see what you build. 🔥
Thanks to Eloka Chima, Fredrick Mgbeoma, Benny Ogidan, Seyi Adeleke for taking their time to review my article.
Take home Assignment for You
1 Write a function that generate’s a URL to update an item on the shopping list.
2 On the front end;
- Create an Update button on the user interface.
- Write a method called updateItem that will consume the update-URL you just created.
📝 Read this story later in Journal.
🗞 Wake up every Sunday morning to the week’s most noteworthy Tech stories, opinions, and news waiting in your inbox: Get the noteworthy newsletter >
Javascript/Node.js Developer | Tech preacher, advocator and evangelist.
1.1K 
16
1.1K 
1.1K 
16
Javascript/Node.js Developer | Tech preacher, advocator and evangelist.
"
https://medium.com/google-cloud/continuous-delivery-in-google-cloud-platform-cloud-build-with-app-engine-8355d3a11ff5?source=search_post---------199,"There are currently no responses for this story.
Be the first to respond.
Agile and DevOps continue spreading among IT projects — and I would say: at a pace that we have never seen before! As the interest in these matters increases, the set of processes and tools which make it possible to deliver better software also increases.
At the end of the software development lifecycle is the delivery “phase”. And if the delivery is not fast, the whole Agile development process may be broken. Modern software should be designed to be fast delivered, using appropriate automation tools. In this context, I’ll write about Continuous Delivery in the Google Cloud Platform. It's a series of 3 articles, covering deployment to App Engine, Compute Engine, and Kubernetes Engine.
Starting with App Engine, GCP’s fully managed serverless application platform, I will show the steps to set up a development + automated build +continuous delivery pipeline. To keep it practical, we’ll create a very simple front-end application and deploy it to App Engine Standard Environment. Please remember we will focus on delivery and not on app development itself.
Before proceeding, make sure you have created a GCP Project and installed Google Cloud SDK in your machine if you wish to run the examples. Don’t forget to run gcloud auth login and gcloud config set project <your-project-id> for proper gcloud CLI usage.
A simple way to create a front-end web application is by using Angular CLI. The steps to install this tool are out of the scope of this article and can be found here. Once installed, cd to your preferred folder and type ng new <app-name>. Wait a few seconds. After the app is created, type cd <app-name> and ng serve. Point your browser to http://localhost:4200 and make sure the app is running. As we have a fully running app, we stop the development here ;). It’s time to think about delivery!
An Angular app is a set of HTML, CSS, JS, images, and other web-related static files. So, what we need to do now is to build the app and deploy it to an HTTP server in order to make it available to the users. To build the Angular app, we run npm install and npm run build --prod in the <app-name> folder. Angular CLI then creates a new dist folder, where it places the ready-to-deploy content. We could figure lots of ways to copy these files to the webserver, but following this path we are adding complexity to our build process, don’t you agree? As we desire simplicity and automation instead of complexity and manual tasks, we’ll learn how to use Cloud Build.
Cloud Build is a Google Cloud Platform fully-managed service that lets you build software quickly across all languages, counting on containers to get the job done. Having that said, let’s prepare our project to use Cloud Build. Add a file named cloudbuild.yaml to your project’s root folder, with the following content:
This simple file instructs Cloud Build on how to build and deploy the app, similar to a Docker multi-stage build. When we invoke Cloud Build using the command gcloud builds submit --config cloudbuild.yaml ., it will compress the project’s source files (a .gitignore file, if present, will be considered in order to determine which files shall be skipped), and copy the tarball to a managed Cloud Storage Bucket. It happens in your development machine.
Then, running on GCP servers, Cloud Build will uncompress the sources and execute npm install in a container of Cloud Build’s built-in image gcr.io/cloud-builders/npm. The second step will use another npm container to run npm run build --prod. The third one will run a container of another Cloud Build’s built-in image, named gcr.io/cloud-builders/gcloud, to execute a gcloud app deploy command.
I've already talked about the 1st and 2nd steps, but not about the 3rd one, gcloud app deploy, so let me do it. This command is responsible for deploying the built content to App Engine and finish the whole delivery process. In order to succeed it requires an app.yaml file in the project’s root folder (see sample content below), the <your-project-number>@cloudbuild.gserviceaccount.com Service Account must have the App Engine Admin role, and the App Engine Admin API must be enabled for the GCP Project in which the app will be deployed. This step replaces copying the built files to an HTTP server — mentioned before — with copying them to GAE Python 2.7 Standard Environment, where they can be served as static content.
Point your browser to https://<your-project-id>.appspot.com or click a service's name in Cloud Console to see the app running on App Engine! Also, visit https://console.cloud.google.com/cloud-build/builds?project=<your-project-id> to check your project’s build history.
Ok, now we have an almost fully automated deployment process.
— Hey, Ricardo, what do you mean by “almost fully automated”?
— I mean GCP can help us doing better than this :).
We have seen how Cloud Build interoperates with Cloud Storage and App Engine, but there’s a missing piece: Source Repositories.
Git is widely used as a source control management tool nowadays. Source Repositories allow you to use GCP as a Git remote repository or to automatically mirror repositories from Github or Bitbucket. Visit https://console.cloud.google.com/code/develop/repo?project=<your-project-id>, click CREATE REPOSITORY, and follow the steps — it’s pretty straightforward.
Once the repository is set, navigate to Cloud Build’s triggers page: https://console.cloud.google.com/cloud-build/triggers?project=<your-project-id>. Click ADD TRIGGER, select a source, a repository, and proceed to trigger settings. Give a name to the trigger, let's say Push to master branch, select Branch as the Trigger type, type master as the Branch (regex), and select cloudbuild.yaml as the Build configuration. Type /cloudbuild.yaml for the cloudbuild.yaml location and save. And here we go: every time a new commit is pushed to master, the build process is automatically triggered. We don’t need to manually execute gcloud builds submit … to trigger the build anymore. The deployment process is now fully automated!
The steps described in this article can be used or customized to deploy any kind of application supported by Google Cloud Platform’s App Engine Standard Environment. Deploying to Flexible Environment requires changes in folders' structure.
The sample code is available on Github: https://github.com/ricardolsmendes/gcp-cloudbuild-gae-angular. Feel free to fork it and play.
Happy deploying :)
2019–08–04: I created a GitHub repository to demonstrate how a similar deployment might be done to App Engine Flexible Environment. Please refer to gcp-cloudbuild-gae-flex-angular for details.
This is the 1st of a 3-article series on Continuous Delivery in Google Cloud Platform:
App Engine | Compute Engine | Kubernetes Engine
Google Cloud community articles and blogs
515 
4
515 claps
515 
4
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
head of data @ ciandt.com • hobbyist tech writer • dad • birder
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/how-to-run-deep-learning-models-on-google-cloud-platform-in-6-steps-4950a57acfa5?source=search_post---------200,"There are currently no responses for this story.
Be the first to respond.
Top highlight
Using Deep Learning Virtual Machine
Google Cloud Platform is a tool provided by Google which one can leverage to build large scale solutions. This platform has recently gained a lot of popularity because of their easy access to GPUs. Additionally, they also give you $300 worth of credits for free with a year of validity which depending on the kind of processing you need to do can last up to a year.
However, some of the instances can get really challenging since there isn’t a proper GUI or many packages won’t get installed. In this blog I am going to talk of an easy way to deploy a marketplace solution for running Deep Learning model. Moreover, this would also create a Jupyter Notebook GUI that can be used to view quick results.
The first thing you need to go is setup a google cloud account.
Go to https://cloud.google.com/ and sign in using your Gmail account. If you have a school or organization account it may lead to some collaboration issues in the future so I would strongly recommend to create an account using your personal Gmail account. If you don’t have a Gmail account, now maybe a good time to create one.
While signing in, Google will ask you to share your credit card details. You can put them in but your credit card wont be charged unless you have run out of all your $300 worth of credits, so you need not worry.
Once you have signed in, it will take you to a console screen which should look something like this.
If you don’t get an automatic assignment of a project, you should go ahead and create one. Click on the “Create New Project” icon and the banner and create a new project. The project ID is assigned automatically, however you can change it if you like. I decided to stay with the default project ID assigned to me.
Now that you have an account and project, you can deploy a marketplace solution.
Your billing will only start once you deploy the solution.
To set up a deep learning marketplace solution, search for “Deep Learning VM” in the search bar. This should take you to the landing page for “Deep Learning VM”.
The advantage of using Deep Learning VM is that we don’t have to install python or tensorflow since it is a part of a pre-packaged image developed by Google. Once you’re on this page, you just hit the “Launch Compute Engine” button. This page also shows the number of past deployments you have for this engine. It is 3 in my case.
Once you launch the compute engine, you will be taken to the configuration page, where you can set a name for the environment, select the zone for the machines and select the number of CPUs and GPUs you would want.
It is important to note the Zone you select for your deployment since the machine configuration you choose will depend on it. For example, there maybe restrictions in some zones on the number of CPUs and GPUs you can access.
Depending on the kind of machine that you choose, you can see the billing amount on the right hand side change accordingly.
For example, if you choose 16 CPUs and 0 GPUs, you can see that you will be charged $392.36 per month if you use 730 hours per month. If you hit “Details” it will give you the break up for the billing. Generally, GPUs are more expensive than CPUs, so if you don’t need GPUs, it is better to skip them altogether. You also need to request for GPU quota in the zone of your deployment (which I will talk about in detail in Step 6).
For now, choose Zone: us-west1-b, 16 CPUs and GPUs as “None”. The next thing to choose is the size of your hard-drive. “Standard Persistent Disk” should be good for any project, but if you want you can expand the memory if you have a lot of data. Keep in mind larger disk sizes will lead to bigger bills, so best to be parsimonious about the requirements. They can always be modified later on if required (covered in Step 5).
Once you have selected the config hit “Deploy”. Based on your selection, it may take 5 to 10 mins for the deployment to set up. If you get an error after deploying, check to see if you selected GPUs by mistake. If you select GPUs without having an assigned quota, it may lead to an error. Just create another deployment without any GPUs and you should be good to go.
Now there are 3 different ways of running code on this VM. The easiest is using a GUI of Jupyter Notebook which runs on localhost:8080 on your machine. To access this, you need to install Google SDK to SSH this VM.
You can install Google SDK here. Initialize Google SDK and connect to your google account and project that you created. Initialization options should show up automatically after installation, if it doesn’t you can run the command : gcloud init and make sure that you connect with the same email and project ID as before.
Once you have Google SDK installed and configured, you just copy the SSH link that shows up on your deployment page and paste it on to the Google SDK. The SSH link will be under the header “ Create an SSH connection to your machine” (see image below)
If you have successfully created a SSH connection, a PuTTY screen will pop-up (image below)
Once you have your SSH setup, you are just once click away from your Jupyter GUI. Go back to the deployment manager and hit the localhost:8080 button
Voilà!! That will take you to the Jupyter Notebook instance that is deployed on 16 CPUs. You can use this like any machine.
Additionally you can also run python batch jobs on the PuTTY terminal or by hitting the SSH terminal in the Compute Engine VM. More on that in the next step.
Before we add GPUs we need to request GPU quota in the same zone our instance is deployed on.
Just search for “Quotas” in the search bar and that should take you to the Quotas page under “IAM & admin”.
Here under “Metrics” first, select “None” and then search for GPU. Based on the GPU present in your zone you can select the name of the GPU and we also need to select “GPUs (all regions)”. For example, since our zone is us-west1-b, you can select the “NVIDIA P100 GPUs” and “GPUs (all regions)”
Once you select both the GPUs, you hit the “Edit Quotas” button on top.
Make sure that the GPU you select is in the same zone as your instance, or else your deployment will not be able to access it.
This will generate a form where you need to share your personal phone number and reason for this request. As soon as you submit this form, you will get an email from Google saying that your request is under process and it will take 2–3 business days.
Although their email says 2–3 business days, the request is approved within a couple of hours. Once your quota request is approved, you can edit your virtual machine to include more GPUs.
To add the requested GPUs, you need to edit the instance that is created on the Compute Engine page. Go to the menu on your google console and then hit “Compute Engine”.
The VM instances pages gives a list of the VMs that we have installed across various solutions on google cloud platform. An important thing to note here is that we should stop all instances if we do not want to be billed for the machines. Even if we don’t run any code, google charges us for the instances.
Hence it is important to stop all instances when we aren’t running anything.
Once you have stopped the instance, you can edit it. If your quota request is approved, you should be able to add more GPUs and deploy the solution again in no time.
Another neat trick is to “Enable connection to serial ports” and “Allow full access to cloud APIs” (under Access scopes) to enable your instance to talk to buckets and vice versa.
Once your config has been modified by adding another GPU, you can either run a deep learning model on the Jupyter Lab UI or the PuTTY terminal. You will notice that it will be much faster as we have added GPUs to our system. This also means our bill is higher so make sure to keep checking the “Billing” page to ensure that you don’t run out of credits.
Below is a short video on how I accessed the Jupyter Notebook GUI on the cloud to run models.
You can copy data from your bucket to the instance that you just created using “gsutil” feature of GCP.
You can either use the PuTTY terminal or the SSH on the Compute Engine to write this command.
More details on gsutil here.
Sometimes we end up storing large files or objects on the virtual jupyter notebook. This might lead to memory constraints. However just deleting them doesn’t free up memory. Instead we need to manually go and clear the trash from the jupyter location in the VM to free up memory.
To check the available memory run, you can use du -sh * in the PuTTY terminal. Once you navigate to the jupyter folder in PuTTY, you can force delete it to free up memory. In my case the command:
rm -rf /home/jupyter/.local/share/Trash
worked. By running du -sh* again, I could observe that the available memory in dev/sda1 environment went up after running this command.
I am sure there are many other ways to run Deep learning models on Google Cloud Platform without investing too much time in setting up an environment.
What has been your experience with GCP? Please share your comments and let me know if I can help solve your queries in any way.
Google Cloud community articles and blogs
509 
7
509 claps
509 
7
Written by
End to end problem solving. Data nerd. Facebook, UT Austin
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
End to end problem solving. Data nerd. Facebook, UT Austin
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/using-google-cloud-ai-platform-notebooks-as-a-web-based-python-ide-e729e0dc6eed?source=search_post---------201,"There are currently no responses for this story.
Be the first to respond.
This post contains instructions and advice on how to set up and use Google Cloud AI Platform Notebooks as a development environment. It is targeted to Software Engineers who want to know how to use and find common IDE features, but is relevant to anyone interested in learning how to use Google Cloud AI Platform Notebooks or JupyterLab.
Google Cloud AI Platform Notebooks are built on JupyterLab. JupyterLab is web-based development environment that includes a Jupyter notebook editor, a file browser, a terminal, a text editor with code highlighting, customizable themes, a drag-and-drop tiled layout, and support for custom themes and extensions. All of these elements work side-by-side in an extremely responsive webpage.
A year ago I switched from PyCharm to JupyterLab (running on Google Cloud Deep Learning VM Images, which form the basis of AI Platform Notebooks) as my main development environment and I have not looked back.
In my day-to-day work as Machine Learning Engineer working with Google Cloud customers, I continually had challenges with my previous setup of PyCharm running on a Macbook. Most of my work requires customer data, which cannot leave the cloud. This meant I spent a lot of time in vim on a VM, relied heavily on managed services (which are great for large jobs but too slow when you just want to poke around), or struggled to create suitable synthetic data. Even when I could fully utilize PyCharm I’d run into RAM problems if a dataset was more than a couple gigabytes, or I’d encounter security problems as I traveled and made API calls to cloud projects from mysterious hotel IP addresses.
I also work a lot with data scientists, who have their own preferred tooling and workflows. Trying to get data scientists to use software engineer tools and workflows is a struggle (and hurts their productivity). But asking software engineers to switch to data science tools is absurd — you cannot build software with notebooks. And forget about trying to get a development team to unify their workflows if they are using fundamentally different tools.
There are some PyCharm features I miss, mainly advanced code navigation features. But I gained far more than I lost:
To run AI Platform Notebooks, you need a Google Cloud Platform project (with an attached billing account if you want to use GPUs) with the Compute Engine API enabled. Instructions are here. If you’re already a Google Cloud user you can probably skip this step.
Many of the commands in this post use the Google Cloud gcloud command-line tool. Most of these steps are possible with the AI Platform Notebooks UI, but using gcloud gives more control and better reproducibility.
You can install gcloud on your local machine, but if you don’t already have a machine with gcloud just use Cloud Shell, a web-based terminal session with everything you need already installed. If you already have a Google Cloud project and are logged in, click this link to launch Cloud Shell.
If you are using gcloud for the first time, run gcloud init to authorize. You can specify project/region/zone defaults if you wish, but be aware AI Platform Notebooks and GPUs are not available in all regions yet. Currently, the best way to see what zones everything is available in is to look at the regions available when creating a new Notebooks instance.
First, create an AI Platform Notebooks VM instance. You can do this in the Notebooks UI but using gcloud gives more options:
You may want to change the command to meet your specific needs.
gcloud compute instances create $INSTANCE_NAME: all VM creation gcloud commands start with this.
--zone=$ZONE: the zone you want your VM created in. Only some zones have GPUs, but any zone in a region that supports AI Platform Notebooks is fine. Currently, the best way to see what zones everything is available in is to look at the regions available when creating a new Notebooks instance.
--image_family=$IMAGE_FAMILY: specifies the image to use to create the VM. An image family is a group of related VM images; pointing to an image family instead of a specific image ensures the most up-to-date image in the family is installed. The tf-latest-gpu image family contains VM images for running TensorFlow on GPUs — all the necessary tools (including JupyterLab) are preinstalled and key binaries are custom built for increased performance. This image family is part of Google Cloud Deep Learning VM Images, the product Notebooks is based on. Deep Learning VM image families are available for popular deep learning frameworks, no deep learning framework at all, CPU-only, etc. If you want to use a specific version of an image, use --image instead of --image-family and specify a valid Deep Learning VM image version.
--machine-type=$INSTANCE_TYPE: determines the RAM and cores of your VM. Many different configurations are available. Note you can easily change this later.
--image-project=deeplearning-platform-release: in what project to find the specified --image or --image-family . The deeplearning-platform-release project holds images provided by Google Cloud Deep Learning VM Images, don’t change this if you’re creating a Notebooks VM.
--maintenance-policy=TERMINATE: what happens to your VM during a maintenance event. Most VMs can be live migrated but if GPUs are attached to your VM live migration does not work. If GPUs are not attached to your VM (and don’t intend to ever attach GPUs) you can leave this line out.
--accelerator='type=nvidia-tesla-v100,count=2':the type of GPU to attach and how many to attach, see the documentation for available GPUs/counts. If you are not using GPUs, you can leave this line out.
--no-boot-disk-auto-delete: the default behavior on VM deletion is to delete the boot disk, this overrides that default and does not delete the boot disk when the VM is deleted. This means if you accidentally delete your VM you can still recover your work. But it also means you need to delete the disk separately from your VM if you want to remove everything.
--boot-disk-device-name=$INSTANCE_NAME-disk: creates a disk based on the name of the VM.
--boot-disk-size=500GB: adjust to your taste, 100GB or above is best.
--boot-disk-type=pd-ssd: makes your disk an SSD for better performance.
--scopes=https://www.googleapis.com/auth/cloud-platform: gives your VM the ability to connect to Google Cloud Platform APIs. This means you will be able to use services like BigQuery, Cloud Storage, AI Hub, etc. from your VM. It is also required to create a direct connection URL to your VM. You can name only the scopes you need, use service accounts, or leave this line out and use the Compute Engine default service account. But be aware with reduced scopes you will not get a direct connection URL to your VM, and then you’ll need to use an SSH tunnel to connect.
--metadata=’install-nvidia-driver=True,proxy-mode=project_editors’: metadata entries are used by Deep Learning VMs to pass parameters to installation and startup scripts. If you want to use GPUs install-nvidia-driver=True installs the driver, you can leave this out if you are not using GPUs. proxy-mode=project_editors creates a direction connection URL to your VM and adds it to the Notebooks UI. You can forgo this line as well and use an SSH tunnel to connect to your VM.
One optional flag to add, especially if you want a beefy VM now or later and are not using GPUs, is --min-cpu-platform=Intel\ Skylake. This ensures your VM is on the Skylake CPU platform (if your zone supports it), which allows VMs with more cores and RAM than other platforms. You can also use other CPU platforms.
Your VM will be created a few minutes after the gcloud command finished. A few minutes after that, your VM is assigned a URL for direct connection and appears in the AI Platform Notebooks UI:
You can use the Notebooks UI to connect to your VM by clicking “OPEN JUPYTERLAB”, but you can also get the URL using gcloud:
You’ll see something like this: numbersandletters-dot-datalab-vm-region.googleusercontent.com.
This URL provides a secure web connection to your VM. Just put it in your web browser and you’ll connect to your JupyterLab session. Be aware other project users with high privileges can use this URL as well — it is not accessible only by the VM creator.
If you created your VM without proxy_mode=project_editors or did not set the https://www.googleapis.com/auth/cloud-platform scope, you need to connect to your VM via an SSH tunnel.
Once you’re connected to JupyterLab, you’ll see the default AI Platform Notebooks JupyterLab UI:
The Jupyterlab interface is powerful and highly customizable:
Out of the box, JupyterLab comes with a Jupyter notebook viewer/editor, a terminal, viewers for many file types, a text editor with syntax highlighting, and a file browser. It’s worth browsing the entire JupyterLab User Guide to learn about the features, but at least check out the basics of the interface.
AI Platform Notebooks include additional functionality beyond base JupyterLab thinks to a few pre-installed extensions: git, tensorboard, and nbdime (notebook diffs).
When you are not actively working on your VM, stop it to save money and project resources. When your VM is stopped your settings, environment, data, and work are still intact and waiting for you when you start the VM again.
You also need to stop your VM to resize it. Resizing your dev VM is a quick and dirty way to run something that would otherwise take too long or not fit in memory, or to attach more GPUs for a large training job. You can change your VM machine type with gcloud:
To attach, unattach, or change GPUs, you need to use the Notebooks UI.
Once you’ve changed your VM configuration, start your VM:
The ability to resize and respec stopped Notebooks VMs is extremely useful. You can prototype your model training on a pair of cheap GPUs and then switch to eight P100s once you know everything works. You can work effectively with datasets up to 100s of GB with standard Python packages (just make sure your packages can take advantage of multiple CPU cores; for Pandas on multiple cores check out modin). And when you no longer need the extra power you can save money by reducing the specs of your VM.
Note there are restrictions on VM size. Different CPU platforms support different machine types, and there are additional limits on cores/RAM when GPUs are attached to a VM.
Update 4/27/19: the latest VM images should allow the jupyter user to sudo from the JupyterLab terminal.
When you’re working on your VM, you are logged in as user jupyter. For better security, this user has the permissions you’ll need for day-to-day development work but has no root access to the VM or sudoer privileges.
At some point you will need sudoer privileges. You can get sudoer privileges by connecting to the VM through an SSH connection. You can do this with the Compute Engine UI, or with gcloud:
When you connect via SSH you connect as your default Google Cloud Platform user. This gives you sudoer privileges.
Depending on how your project is set up, you may not be able to SSH to your VM because port 22 is closed by default — this is a good security practice. To enable SSH, you’ll need to open port 22 by creating and applying a firewall rule.
First, create a high priority firewall rule that opens port 22:
allow-ssh is the name of the rule, --allow=tcp:22 opens TCP port 22, priority=0 makes this firewall rule take precedence over other firewall rules (lower priority=high precedence), --description is how your firewall rule is described (optional but useful), and target-tags are the VM tags that identify what VMs this rule applies to. If you’re extremely security conscious, consider using the --source-ranges parameter to only open port 22 to select IP addresses.
Next, apply the firewall rule to your instance by tagging your instance appropriately:
Now you are able to connect via SSH. Once you’re done, keep your VM more secure by untagging your VM until you need SSH access again:
Python environment management is an important consideration in a Python developer workflow. When using AI Platform Notebooks, you may not want to independently manage your environment. There are some under-the-hood optimizations you will lose (including optimized binaries of some packages) and you may run into problems with version compatibilities as you install the packages you need. You may find it easier to maintain separate VMs for each of your projects rather than separate environments.
But sometimes you need a tightly controlled environment, and the default Notebooks environment has many packages pre-installed (run pip list to see).
The most popular tools for managing Python environments are pipenv, virtualenv, and virtualenvwrapper (which extends virtualenv). If you have a preferred tool, you can use it with Notebooks (but you may require SSH to set it up properly, see Step 5 above) and you may want skip the content on virtualenv basics and skip down to “Creating a Jupyter Kernel for a Virtual Environment”.
virtualenv is the entry-level tool for environment management, and is pre-installed when a Notebooks VM is created. The general idea of virtualenv is you create a “virtual environment” with its own version of Python and its own installed packages. When you want to work in that environment you “activate” it, and then whenever you usepip only the active environment changes. When you’re done working you “deactivate” your environment. Virtual environments live on your disk in folders. You can have as many virtual environments as you want and you manage them yourself.
Creating a virtualenv virtual environment is as simple as running the virtualenv command followed by a target directory where your virtual environment will live:
This example creates a virtual environment in the venv directory.
Activate the virtual environment by running bin/activate in the environment directory:
You’ll see the shell prompt changes to have (venv) in front of it. This lets you know you’re in the virtual environment. Now anything you do with pip only changes the activated virtual environment.
When you’re done working in the environment, run the deactivate command to leave the environment:
A popularvirtualenv usage pattern is to create a virtual environment in every project you work on in a venv directory in the project root. If you do this, consider putting venv in your .gitignore file so you do not check it into a remote git repo. Another popular approach is to keep a single ~/envs directory with subdirectories for your different environments. And once you understand the basics of virtual environments, its worth checking out virtualenvwrapper which automates virtual environment management.
If you want to remove your virtual environment permanently delete the directory:
virtualenv also supports the creation of virtual environments with different Python versions:
Note that whatever comes after -p is the path to an executable Python interpreter. On a Notebooks VM python2 and python3 are in the path and point to the latest versions compatible with the pre-installed packages, but to create an environment with a very specific version of Python you may need to install that version yourself.
To make your code portable, create a requirements.txt file from inside your environment with pip freeze and share it with your code.
You may want a Jupyter kernel associated with your virtual environment. This lets you run notebooks and consoles from your virtual environment. Run this code in your virtual environment to install the ipykernel package and create the kernel for the jupyter user. KERNEL_NAME is the internal iPython name, but DISPLAY_NAME is what you’ll see in the interface:
You’ll need to refresh the JupyterLab web page before notebook and console icons for the new kernel appear in the launcher.
Be aware of your TensorFlow flavor when using virtual environments. TensorFlow has separate packages for GPU vs. non-GPU. Installing either version into your virtual environment should work, but make sure to install the right package to match your configuration (GPU vs. no GPUs). This also means you should keep separate virtual environments for both GPU and non-GPU if you plan to switch between the two. Cloud AI Platform Notebooks should handle switching automatically in the default environment (and install the correct binaries and Nvidia stack as appropriate), but you’ll need to handle it yourself if you are managing your own environments.
JupyterLab supports custom extensions (and themes) and provides detailed documentation to developers to promote the creation of new extensions. Extensions are a rapidly evolving aspect of the JupyterLab experience and are prone to instability. Other than the pre-installed extensions, they are not officially supported by AI Platform Notebooks. Also note that the JupyterLab built in extension manager is currently not available in AI Platform Notebooks
There is a simple extension worth adding called go-to-definition which jumps to the definition of a variable or function if the definition is in the same file. This is especially useful when trying to work through long or disorganized notebooks or .py files. Note this extension is not officially supported, and like all 3rd party code you should review it before installing.
You will need an SSH connection for sudoer access(see step 5). Once connected, run the following:
After the extension is installed refresh the JupyterLab webpage. Then you can hold ALT in any notebook or .py file and click to jump to a definition in the same file. ALT+O jumps backwards from the definition to where you first clicked.
There are many more extensions available, in various states of completion. There is also a tutorial and excellent documentation if you are interested in making your own extensions or fully customizing your theme.
I want to thank Viacheslav Kovalevskyi for answering my technical questions about AI Platform Notebooks and championing an excellent product, and Wesley Turner and Praneet Dutta for reviewing my post and offering excellent feedback.
Google Cloud community articles and blogs
647 
10
647 claps
647 
10
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Machine Learning Engineer at Google
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://onezero.medium.com/how-aws-and-other-cloud-providers-became-the-internets-most-powerful-moderators-9b23a954d68a?source=search_post---------202,"Sign in
There are currently no responses for this story.
Be the first to respond.
Owen Williams
Jan 15, 2021·4 min read
When Amazon Web Services decided to stop hosting the alt-right social network Parler last week following the insurrection at the Capitol, it looked like the site was doomed to go offline.
"
https://jonathancarroll.com/you-as-cloud-7e301277ab96?source=search_post---------203,"Almost everyone has some person from the past who continues to stick in their soul like a deep splinter. You know who I’m talking about: the most obvious example is the lover who dumped you years ago for reasons you still don’t understand or accept. The bully who tormented you in school, the teacher who inspired you and opened doors in your brain or soul you didn’t know existed. A neighbor who suffered and died with dignity but remained kind and thoughtful right up until the end.
Ironically in some cases if you were to go to these people now and tell them what a lasting effect they have had on you, they would be genuinely surprised or perplexed. Because in truth you were only a very small cloud on their horizon, a blip on their screen, or someone they forgot very quickly. But boy, you sure remember *them*.
art by Robert and Shana ParkeHarrison
Carroll is the author of: Bones of the Moon, The Ghost in…
223 
5
223 claps
223 
5
Written by
author • jonathancarroll.com
Carroll is the author of: Bones of the Moon, The Ghost in Love, Glass Soup White Apples, The Wooden Sea, The Marriage of Sticks, Kissing The Beehive, From the Teeth of Angels, After Silence, Outside the Dog Museum, A Child Across the Sky, Sleeping in Flame, and more
Written by
author • jonathancarroll.com
Carroll is the author of: Bones of the Moon, The Ghost in Love, Glass Soup White Apples, The Wooden Sea, The Marriage of Sticks, Kissing The Beehive, From the Teeth of Angels, After Silence, Outside the Dog Museum, A Child Across the Sky, Sleeping in Flame, and more
"
https://medium.com/google-cloud/simplifying-continuous-deployment-to-cloud-run-with-cloud-build-including-custom-domain-setup-ssl-22d23bed5cd6?source=search_post---------204,"There are currently no responses for this story.
Be the first to respond.
Cloud Run is fully managed Serverless product launched by Google Cloud for public beta availability which offers to run docker image in Serverless environment. With the introduction of Cloud Run allowing to run docker irrespective of run time environment, the limitation of Cloud Function for limited environment and version is mitigated by Cloud Run which can be run live in few minutes. Also, Cloud Run can be utilized on GKE with Istio and Knative.
In this post, we will walk through from github commit to continuous deployment to Cloud Run along with custom domain for http endpoint. The codes and configurations used in this post is on my CloudRun-Demo github repo.
We have a simple http server running on port 8080
For making our final docker image smaller and secure, we are using multi stage build with final artifact on distroless base image.
If you are interested in learning more about distroless image, here is one of my post:
medium.com
Cloudbuild.yaml for Google Cloud Build to create docker image and push to container registry(gcr)
Head over to Cloud Build Trigger and create a new trigger for github(or other source repositories) by adding authentication.
Select cloudbuild.yaml as build configuration. For now, we keep trigger for all branches but the selection can be managed with regex for both branch and tags along with files filter.
This steps adds a webhook trigger on github for all events by default.
Now, for triggering a new build, we can manually “Run Trigger” for specific branch from Code Build Trigger Page or make a change on our code and push to github. We can view the build logs from history page.
In this demo, we are using $SHORT_SHA for tagging docker image which includes the first seven characters of git commit hash. So, our image is of format gcr.io/pv-lb-test/cloudrun-demo:201c141 .
Head over to Cloud Run on the Google Cloud Console and “Create Service” with the “Container Image URL” as we got from the Cloud Build above. Give a service name and regional location as Cloud Run is regional resource. If you want to allow invocation from any user, select the “Allow Unauthenticated Invocations” checkbox.
Additional settings allows us to grant spec(Memory Allocation) for each container and maximum concurrent request a container can handle. If the concurrency is reached, Cloud Run automatically scales out new container. If you want to add more environment variables, the key-value pairs can be added.
Optionally, the service can be created with cli command:
After saving the service, in about a minute, you can get a URL for invocation in format: https://cloudrun-demo-doiyqpty6a-uc.a.run.app
Time to make the game moving !!!
For the deployment of new image to Cloud Run from Cloud Build as commit is pushed to github, we need to grant the deployment access to Cloud Build serviceaccount which is of format [id]@cloudbuild.gserviceaccount.com .
Add following permission to the IAM user from IAM & admin console.
And append the following deployment commands on cloudbuild.yaml file.
Change the [Project-Name] and region based on your favorite Cloud Run service name.
Optionally, we can trigger new version deployment with:
For confirmation, we add a change on our Go application http response and push the commit to github. After some time, we can see our changes deployed on the URL. Also, we can see from the Cloud Build log that our image on gcr is deployed:
Rather than the random URL we got from the service, we can use our own nice domain name linked with Cloud Run. For that, we need to verify the domain ownership with Webmaster Central.
On the Cloud Run page on gcloud console, click on MANAGE CUSTOM DOMAIN and add a mapping with servicename and domain.
Click on “VERIFY IN WEBMASTER CENTRAL” where you can get the domain verification options for various domain registrar or provider.
If you provider is not listed there, simply choose last option “other” which gives two options: CNAME and TXT records.
As I am using Google Cloud DNS for managing my record sets, I am going for CNAME record.
In few minutes, the record will be propagated. You can confirm by entering your domain on whatsmydns. After is succeeded, click on “Verify” on the Webmaster Central page.
On the Cloud Run page, click on “CONTINUE VERIFICATION AND CLOSE”.
Now, if you again click on “ADD MAPPING”, the verified domain appears on the list below the service name selection. If you want to use the primary domain, leave the third option as it is.
Click on “CONTINUE” and it will give few IP addresses to add on “A” and “AAAA” record set of the domain.
Then you can invoke the Cloud Run service with nice domain name.
In few minutes, the SSL certificate issued by Let’s Encrypt Authority is also provisioned for the custom domain.
By this way, we can make continuous deployment to cloud run and enjoy serverless architecture on our own container.
That’s it for now, if you are interested on getting my interesting updates, find me on Linkedin, Twitter.
Google Cloud community articles and blogs
578 
5
578 claps
578 
5
Written by
DevOps | SRE | #GDE
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
DevOps | SRE | #GDE
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://blog.doit-intl.com/dont-get-the-google-cloud-bill-shock-465907d5347c?source=search_post---------205,"With Google BigQuery ML you can now predict your Google Cloud spend in just a few minutes and without leaving your BigQuery Console UI.
Linear Regression, although very simple, can be used to generate accurate predictions for various real world problems efficiently. Due to its simplicity, linear regression training is easy to configure and benefits from fast convergence.
In this post, I will explain how to analyze Google Cloud billing data and build a simple prediction model to estimate the expected overall monthly expenditure. To make it more interesting, I will use only Google BigQuery, thereby keeping all the billing data in the data warehouse ecosystem.
In this exercise, I am going to use Google Billing Exports. Billing export to BigQuery enables customers to export daily usage and charges automatically throughout the day to a BigQuery dataset you specify. You can read about Google Billing Exports here.
The data and code samples are available here: https://github.com/doitintl/BigQueryML-Examples
The table below lists the billings for various services consumed by two Google Cloud billing accounts we own.
The goal is to estimate the current month’s total bill based on all the billings received until a given day.
The model will enable the client to not only estimate overall expenditure, but will also detect anomalies and create alerts for overcharges.
The model assumes that the monthly bill is linearly dependant on 3 variables:
The first variable represents the current daily consumption trend. The other two variables are the number of days until the end of the month and the current balance. Together, the model can use these variables to estimate the remaining monthly expenditure.
Since the model requires data at the daily resolution, we will use BigQuery to aggregate the data by day.
The resulting scheme looks like this:
account_name — the account id day — remaining days to the end of the monthmonth — billing monthyear — billing yeardaily_cost — the total cost paid for all services during the billing daymonthly_cost — the label, which is the sum all billings made during current month, including future billing
Now that the data is aggregated at daily resolution, I save it as a new table and use it to generate an ML dataset.
The next step is to calculate how much has been billed to each account from the beginning of the month until the current day. We will use an aggregate window function to do this. This function will also enable us to calculate the month’s average daily spending, until the current day.
The syntax for the aggregate window function can be found here: [1]
I use the function on our billing_daily_monthly table as follows:
Plotting the data shows that the monthly bill compounds in a somewhat linear manner:
The results above give us confidence that a linear model is an appropriate choice for the expenditure problem. In addition, the gradient has little variance between months, which suggests that the selected features are sufficient statistics with reference to the independent variable.
Obviously, more features and more complex models will likely produce more accurate predictions — those can be built with other tools like Google Cloud ML. But for now it looks like we are done preparing the data.
(WOO HOO!)
Once the data are ready, I can use the new (as of August 2018) BigQuery ML tool to fit a Linear Regression model to the data.
Fitting the model to our dataset is insanely simple !
Once saved, the model can be used to make predictions. To do so, I use the following query, which both estimates the final monthly expenditure and calculates the Relative Absolute Error of the predictions per day:
The results demonstrated in the following table can now be saved and serve other components of the system including monitoring and alerting applications. The model’s Mean Relative Absolute Error is approximately 3.0%, which isn’t bad. (Note, the data were generated especially for this demo. With real data, I achieved around 2.0% error)
Want more stories? Check our blog on Medium, or follow Gad on Twitter.
Acknowledgments: Vadim Solovey — Editingamiel m — Technical Review
Keywords: BigQueryML, BigQuery ML tutorial, BigQuery ML example
Software & Operation Engineering. Written By Engineers.
683 
3
683 claps
683 
3
Written by
ML Tech Lead @DoiT International Machine Learning Google Developer Expert.
Here, our SRE and SWE teams share the solutions to some of the most interesting challenges we encounter during our daily work at DoiT International. If you like what we do, check out our https://careers.doit-intl.com page and join the team!
Written by
ML Tech Lead @DoiT International Machine Learning Google Developer Expert.
Here, our SRE and SWE teams share the solutions to some of the most interesting challenges we encounter during our daily work at DoiT International. If you like what we do, check out our https://careers.doit-intl.com page and join the team!
"
https://medium.com/other-voices/everything-you-know-about-getting-things-done-is-wrong-3fbe6c0848a9?source=search_post---------206,"There are currently no responses for this story.
Be the first to respond.
I remember finding out early in life that there were two kinds of people in the world: list makers and not. I was always in the “not” group. I could not understand why people would have to make a list to remember what to do in the day….didn’t you know you needed a haircut by looking in the mirror? Anyhow, I just put it in the “different strokes” category and went on.
Then I became a psychologist, and a performance consultant, and I got an entirely different view. First, I found that there were actually four types of people, not two. Those who made lists and got things done, those who made lists and didn’t accomplish much, those who didn’t make lists and got things done, and those who didn’t make list and didn’t accomplish much. This led me to getting interested again into the world of “to do” lists and what they actually do for us, or not. Turns out, the list makers were right…..and not right at the same time. Lists can help, or they can actually hurt.
The reason? It has to do with your brain, and personality type combined. And, it gets complicated, but interesting. In this brief amount of time, I won’t go into all that is involved, but give you what I believe is behind the mystery, and what todo about it.
Basically, the lists provide an external tool to help your brain do what it needs in order to get things done. To accomplish anything, your brain uses something called the “executive functions.” What that means is that like a business executive, it sets a goal, brings together the resources needed to get it done, executes a plan, measures how you are doing along the way to hold you accountable (are we really getting there as desired?), and adapts to mistakes and misses along the way. That basic flow gets you from “here to there” in any task. But, in order to do that, the big categories of executive functions MUST be present…..what the brain does to make all of those things happen so you don’t get lost. To get anything done, your brain must:
You can see, then, why lists are helpful to some people. They do exactly these three things….unless they don’t. And that brings us to the problem of lists that don’t work. And also, why some people who don’t make lists actually accomplish a lot without them, and others don’t. The accomplisher’s brains are able to do those three things without lists, and the list makers are able to do them in part because of the help the list provides. What is important is not “listor not,” but whether you are able to attend to what is relevant, inhibit what is not, and keep focused on the main thing you want to do and what is relevant to doing it without getting distracted. You might need a list to do that, or you might not, but either way, that is what is going to have to happen.
Now that is where it gets interesting. For some “to do list” makers are working against themselves and their brains, and that explains why they do not get things done. The reason is that their lists are way too long for a time period to be able to focus, and the amount of things on the list begins to interfere and distract from the one they are working on or the ability to find the right one to work on. A list can do two really bad things: contain too many items so that the brain is overwhelmed just by looking at it, or fail to help the person prioritize what is really important and should be garnering all of the brain’s focus and attention.
That also explains some non-list maker’s successes. They are able to just keep what is important front of mind, and donot get distracted by other things or people. Some people who “wing it” actually do accomplish a lot. (They do bug some more organized types by ‘forgetting’ things, but for whatever reason, they just do not see those things as “key” to their life. This gets very interesting when the two types marry:)) Their brains keep the main thing the main thing. They focus and keep it present in mind. And, the non-list makers who do not do well are probably distractible by nature, and actually would do better with a “system” like a list, to help them.
So, let’s get practical. What do you do?
I have become a “to do list” convert, in a way, so here is my advice: make a list, but do it so that it aligns with how your brain works, and also what your brain needs. Remember, it needs attention on what is relevant, protection from what is not, and the ability to keep that in front of you in a “working memory” all the time. So, do lists in that way. Here are a few tips:
So, all I am saying is be careful. The more you put on there, the more you interfere with your brain. And answer this most important question: will you really, really do it? If you are not sure you will do it, do not put it on the list. It is just distraction if it is a “maybe.”
A “to do” list should be exactly that…….. To “DO!” Not “I might if I have time” list, or “I hope I get around to these things” list. It is a “TO DO,” with the emphasis on “Do.”
So, bottom line is this: a “to do” list should NOT be a collection of all the things you need to do or want to do in your near term life. Instead, it should serve you to attend to what is the MOST important priority or few, and make sure that it 1) keeps you attending to those, and 2) keeps everything else from interfering, and 3) keeps those activities in front of you all the time until done. That is why having too many items on the “real” list is the problem. If you are not going to do them, put them on the other list. That will help your brain.
In summary….here is the biggest issue: prioritization.The biggest difference between those who accomplish their goals and those who don’t is that accomplishers prioritize what is most important to getting those goals accomplished by prioritizing the actual tasks and activities that drive those goals to completion. If you are going to go on a road trip and make it to Chicago, you are going to do a lot of things, like get fuel, figure out a route,hit the accelerator, listen to the radio, get lunch, etc. But, if you do not get fuel and hit the accelerator, you are not going to get there. If music and lunch are on the list in any way appearing to be on par with fuel and driving, you are not going to get there, at least when you want to. Do not let minor things compete with the main things. That is the problem with lists that are too long. Theydo not give the proper weight to the main things, nor do they assign an absolute personal demand to get a certain activity done that day that will move the ball forward.
To-Do Lists are exactly that….activities that you are actually going to do….but for a good reason. Do not confuse your brain with other activities that you only “might do,” or are not of the highest priority. Try this, and see what happens. Your brain, and your results will thank you
Other Voices is a publication curated and edited by…
154 
5
154 claps
154 
5
Other Voices is a publication curated and edited by bestselling author and relationship expert Dr. Henry Cloud. Other Voices is a space to share impactful thoughts on the forces that affect our lives.
Written by

Other Voices is a publication curated and edited by bestselling author and relationship expert Dr. Henry Cloud. Other Voices is a space to share impactful thoughts on the forces that affect our lives.
"
https://medium.com/google-cloud/java-frameworks-performances-on-cloud-run-eb243fd84a5c?source=search_post---------207,"There are currently no responses for this story.
Be the first to respond.
In the summer 2018, at Google Next in San Francisco, my colleagues and I had the chance to meet Steren. He talked to us about a new product and allowed us to try it in the alpha program. It was the first version of Cloud Run.
Happy and enjoying the honor I packaged my favorite Spring Boot app into a container, deployed it and… WHAT ? 30 seconds of cold start…
Desperate, I continued to test the product, mainly with new apps (mainly python functions packaged into containers) and I even learned Go to obtain good cold start and processing performances.
However, I’m a Java developer since 15 years and I didn’t accept to leave this language because of a first bad experience in serverless world. Moreover, unsafe-type language, like Python, became more and more inefficient and buggy in my team with a lot of newbie developers. And Go was too “new” to hire experienced developer. Thereby, Java remains one of the best tradeoff.
If I have to choose a Java framework, which one has the best performance ?
I’m a senior Java developer, but I don’t know all the tips and optimization of all the different frameworks. By the way, to be the most neutral, I chose to write an HelloWorld HTTP GET endpoint with 2 mains Framework: Spring Boot and Micronaut.
In fact, I wrote nothing, I only took the out-of-the-box examples foreach of them. I won’t describe here how to build these hello world examples, the framework websites are really well documented! I provide the code at the end of the story
And I also wrote an HTTP endpoint in pure Java, without Framework to measure the framework impact on performance.
Each test is packaged into a container and deployed on Cloud Run fully managed.
Spring Boot is my favorite framework. I’ve developed with it for many years and I still love its handiness and its simplicity. You have a special thing to do ? No problem, a spring component exists for that ! Annotate this, and enjoy !
One of its strength is the autoloading. Add simply a dependency and, automatically at startup, Spring boot scans all the libraries and loads them! Your class is an injectable bean? No need to declare it, annotate it and it’s loaded automatically! Of course, the more dependencies you have, the more files have to be scanned and the longer is the startup time.
This strength in a non-scalable environment, like mine on premise, is perfect. I start my app once, and it stays up until the next revision. Of course, startup time is quite long (20 to 30 seconds), but once a month, it’s totally acceptable!
However, the library scan strength, with the implied extra startup time, is the main pain point in the serverless, auto-scalable and scale-to-0 environment.
I discovered Micronaut in my first session attendance at Next 19 during the spring in San Francisco. I only tried it and didn’t build real app with Micronaut, but it seems to be a good alternative to SpringBoot.
The annotation syntax is similar to Spring Boot and you can also import Spring Boot annotation wrapper in the framework. There are many of the Spring Boot equivalent functionalities. The difference with SpringBoot and the biggest strength of Micronaut is that the libraries scan is performed at the compilation time. Thus, at startup, the app knows which packages to load without having to scan all the files.
Therefore, the startup time is better than with Spring Boot but also, I guess (but I haven’t tested), the startup time should be quite stable in time, even if you add a lot of new libraries and files.
Micronaut has also the capability to be package with GraalVM. This AOT (ahead-of-time) packaging improve the startup time and the memory footprint.
I found interesting to try these performances in addition to Micronaut
Frameworks are useful, powerful and really cool for developers. But often, they are cumbersome and can consume a lot of memory and CPU (impacting startup time) for some unused functionalities.
For a simple helloWorld HTTP endpoint, I chose to develop in pure Java with Jetty webserver. Here, no annotation, just Java, inheritance and overriding! I have to do all by my own.
Again Spring Boot? Yes. As you could see in the observed results, Spring Boot out-of-the-box example has poor performances. I was annoyed about that, because I really like this framework.
I watched on Youtube a Next19 session with Ray Tsang. He explained how to use SpringBoot on GCP and he offered to reach him on Twitter if we have questions, and I did. Ray is super cool and he helped me improve my Spring Boot HelloWorld performances with webflux and pom.xml tricks. That’s why I also would like to share these improvements.
Except for GraalVM packaging, I always use a Dockerfile and JIB for packaging the container to compare the performance.
JIB is a Maven and Gradle plugin developed by Googlers to package easily and efficiently the Java code into a container. You don’t have a Dockerfile to write, only some configuration parameters if you want to customize it.
All the tests are performed on Cloud Run in the us-central1 region. Container are deployed in a secure mode (allow-unauthenticated option disabled) and with the default parameters (memory, no env var, default service account,…)
The startup time has been tested 3 times for each version. The value is these provided in the Cloud Run logs with the comment “This request caused a new container instance to be started and may thus take longer and use more CPU than a typical request.”. The average value of this 3 tests is the cold start value. 3 can seem few, but in reality, Cloud Run performances are quite stable and thus it’s enough for having a sufficient overview of the cold start.
The average response time and the memory usage are taken after a load test performed with Hey. To prevent the scale up of Cloud Run containers, I set the Hey parameters with:
The load test performs 2500 requests from the Cloud Shell environment. (As you could see, the global latency is about 115ms. Global means from Cloud Shell to Cloud Run, the processing time, and the way back. Except for the cold start, in the Cloud Run logs, the latency is stable and about 6ms. The rest of time is wasted in the network (and I’m in Europe region)).
The average response time is provided by Hey. And the memory usage is provided by Cloud Run console metrics.
Finally, the container size is provided by Google Container Registry.
The observed results can be found here
Based on those result, here are some conclusions :
First of all, containers use the same amount of memory in load tests, and the request latency is equivalent.
Second, except for GraalVM, the JIB plugin is more efficient (or equivalent for Servlet) than a Dockerfile built manually. If you haven’t special requirement in the Dockerfile, use JIB first!
I’m not an expert in Dockerfiles, I only used the “official” sample. There is maybe optimisations to improve performance with Dockerfiles.
Third, without any surprise, no-framework usage improves performances, but increases the development complexity and required skills. With frameworks, the performance of Micronaut + GraalVM are incredible for anything Java.
Finally, my fav SpringBoot is quite acceptable when optimized (startup time of 5 seconds) but it’s only a HelloWorld endpoint. I don’t know what the performance will be on a real app, with more dependencies. Not sure that fits all use cases and all requirements.
Container size is only provided for information and doesn’t affect any metrics.
My code is public on Github. You can also use it as a quick start. Dockerfile and cloudbuild.yaml file are present for building the project on Cloud Build. You can also use JIB to build the container and push it, by default to Google Container Registry.
All the build and deployment instructions are in the Readme.md file.
I finally want to thank again Ray for his help and his time in the SpringBoot improvement.
Google Cloud community articles and blogs
666 
2
666 claps
666 
2
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
GDE Google Cloud Platform, scrum master, speaker, writer and polyglot developer, Google Cloud platform 3x certified, serverless addict and Go fan.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/hackernoon/filling-cloud-firestore-with-data-3f67d26bd66e?source=search_post---------208,"There are currently no responses for this story.
Be the first to respond.
Not so long ago Firebase team announced new document database called Firestore. A new database has many improvements and new abilities. Even now in beta, it has almost the same limitation like the old brother Firebase Realtime Database (100 000 concurrent connections). From obvious advantages:
So, if you feel ready to dive into the future of serverless technologies this tutorial is for you!
During this tutorial, I will call Firebase Realtime Database as RTDB.
The first question you might ask yourself is how to push data to the new database. Old RTDB was great with the features of export/import database as the single JSON file.
For the new Cloud Firestore, we need to implement a mechanism by yourself. Let’s assume you are trying to migrate from RTDB to Firestore, so you need to export your current database snapshot. If you don’t have RDBM instance you can just use any JSON data to test this approach.
Why Firebase admin SDK instead of Firebase SDK?
At the current point, there are still problems with pushing nested arrays to firebase using Firebase SDK (tested in 4.6.1). If you try to set nested arrays using Firebase SDK you will get something like this:
Function DocumentReference.set() called with invalid data. Nested arrays are not supported.
To authorize our application we need to export service key.
In the Firebase Console click on the settings wheel next to the Overview section and choose Users and permissions option.
You’ll be landed on permissions page. Choose Service accounts tab and fill it with data as shown in the screenshot above. Fill checkbox Furnish a new private key, so you will be able to download your key and use it in the script. After creation, you will be asked for the location to save the key. Let’s save it as service-key.json
credentials — property will be filled by just created service account key
databaseURL — is the name of your Database instance + firebaseio.com
As long as Arrays are also objects this script is fully ready to use.
Keep in mind that script relies on few files:
firebase-admin — npm module, you can install it locally to the folder of the script
service-key.json — file we’ve generated in the Authentication section
data.json — actually the data we want to push to our Firestore
Cloud Firestore looks very promising. And even despite that fact that it’s still in beta doesn’t mean you should not get your hands on it. Already well known Firebase product called Cloud Functions (still in beta) have already been used by thousands of projects.
In this article, I’ve covered an easy way to submit JSON data to Cloud Firestore. If you are interested in configuring Firebase hosting with the Angular Framework Learn How Get to From Zero to Production with Angular, Firebase, and GitLab CI.
#BlackLivesMatter
722 
17
722 claps
722 
17
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
Software Architect. Help SaaS companies build their products by backing their tech development.
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
"
https://medium.com/icnh/writing-cloud-functions-in-dart-b7e62192b3bc?source=search_post---------209,"There are currently no responses for this story.
Be the first to respond.
Can you write Firebase Cloud Functions in Dart?
Yes you can and here is how.
The article has been updated to Firebase CLI version 8.4.
Install Node.js and the Firebase CLI if you haven’t done so already:
Then, log into the Firebase Console and create a new project (or pick one you want to use). Because later in this tutorial, we will demonstrate a function that accesses the Cloud Firestore, create such a database in your prefered region and remember that region. You must deploy your functions in the same region as your database if you don’t want to pay for the data traffic between different regions!
Back on the terminal, also log into Firebase here:
Then, last but not least, setup the functions project in your current directory:
Choose the Firebase project you created earlier and pick “JavaScript” as language and deselect “ESLint” (which we don’t need as we don’t write JavaScript on our own). Then allow firebase to download all dependencies via npm and wait while it downloads half of the internet.
This process created an empty firebase.json file and a directory called functions which contains a typical node project with a package.json file, a node_modules directory and an index.js file which contains a “hello world” example.
Open index.js and uncomment the example code. Then test the created function to make sure, your setup was successful, running:
The Firebase emulator should print the URL to access the cloud function. On my machine this is http://localhost:5001/<projectname>/us-central1/helloWorld. It also starts a fancy web UI for its emulator on http://localhost:4000/. It will even automatically pick up changes to index.js while running which is quite nice. Eventually, stop the server.
Next, setup the Dart project.
Install the Dart SDK if you haven’t done so already.
Create a new pubspec.yaml file in the functions directory with the following content:
Run pub get to download all dependencies.
Create a directory called node and create a file called index.dart inside that folder that looks like this:
Note: You must replace europe-west3 with your region or omit that line.
Create another file called build.yaml with this content:
Add these lines to .gitignore:
Now run
This converts node/index.dart to build/index.dart.js. The convention, to use a node directory instead of the usual bin directory has been built into the NodeJS wrapper package that is used internally. It does the heavy-lifting of translating Dart to JavaScript. You can add more Dart files to lib if you like.
Then change package.json and add:
You can remove index.js. The JavaScript reign is about to end.
Next, run npm run serve again. When accessing the shown URL, the text “Moin, moin!” should be displayed. We successfully created a cloud function in Dart! Eventually, stop the server.
Replace the content of firebase.json with this content:
Let’s now deploy the cloud function to Firebase:
Your function should be live now.
Please notice that by default, the package.json is configured to use Node.js version 8. This version will soon be disabled by Google. You must switch to version 10. However, you will have then to pay for using this version.
To develop locally, use watch instead of build like so:
and — in a second terminal — run:
Then change the Dart source and observe the build_runner to recompile the Dart source and the Firebase development server to reload the JavaScript function. Now, you’re ready to leave the JavaScript world behind and only use Dart for both your client and server development. Happy times!
Let’s define a Firestore trigger which will automatically be called every time a new document is add to the foo collection. It creates a similar document in the bar collection that has the same name property as the document in foo.
First, initialize, download and run the Firestore emulator:
A fancy UI is available on http://localhost:4000/.
Now replace index.dart with the following code and make sure that the Dart build_runner is still watching you.
Note: Make sure that the function runs in the same region as your datastore! Also notice that the store region name and the function region name might not be identical.
As both the Firebase emulator and the build watcher are running, saving the Dart file should automatically (re)reploy your function and and you’re good to go and test it buy manually creating a new document in foo, adding at least a name property to that document. Then switch to bar and observe that our function has successfully created a document.
For the last time, deploy:
Now go to Firebase console, open your Firestore database and test the trigger by creating a foo/<id> document with a name property and observe the creation of a new bar/<id> docment with a similar name property. <id> stands for the automatically created document identifier.
At ICNH, we’re building Flutter apps for fun and profit and we love to use Firebase as a simple to use backend. It is very well integrated with Flutter because Google provides ready to use packages most aspects of Firebase. For most mobile application, we also have to customize the server part. Using the same programming language for server development as for client development really helps to make this process much nicer.
We successfully created a custom backend for a Flutter app using only Dart.
github.com
ICNH is an app development company located in Kiel and…
1.1K 
12
1.1K claps
1.1K 
12
ICNH is an app development company located in Kiel and Hamburg. We develop native, x-platform and web applications for Android and iOS. For more information: http://icnh.de
Written by
App Developer · Co-Founder of I.C.N.H GmbH· Fantasy role player
ICNH is an app development company located in Kiel and Hamburg. We develop native, x-platform and web applications for Android and iOS. For more information: http://icnh.de
"
https://news.voyage.auto/classifying-vehicles-pedestrians-in-point-cloud-83db7cd6192e?source=search_post---------210,"Sign in
What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Top highlight
Michael Gump
Aug 24, 2017·6 min read
In school it is very easy to know exactly what something like Agile is and yet have absolutely no idea what it is in the real world. At Voyage I’ve not only uncovered deep insufficiencies in my education, but I’ve gotten to build a very, very real feature into the coolest product out there, a self-driving taxi. This summer has made the months of scouring AngelList well worth it.
I study computer science at MIT and I was thrilled to be working at Voyage this summer. I’m excited by machine learning, especially reinforcement learning, and I like thinking about OpenAI’s requests for research. Over the winter I took a class on self-driving cars, which hosted a competition called DeepTraffic and from there I’ve been excitedly moving to get into the growing autonomous vehicle space.
This summer I’ve been working on several problems in computer vision, which means reading a lot of papers and training a lot of models. Most generally, I’ve been working with public datasets to classify objects in LIDAR data. When we as humans see the world, we automatically perceive depth and produce information about objects like “that’s a car”, or “oh no, that’s a person in the middle of the highway, hit the brakes!” Most of my work this summer has been on bringing these types of insights to Voyage’s self-driving taxi, using a 3D view of the world (LIDAR point cloud) and deep learning.
To properly process the world, a self-driving car needs to take raw sensor information (like point cloud) and figure out what it’s seeing.
Arguably, two of the most important pieces of information are depth: “how long until I hit this object?” and category: “what kind of object is this?”
We can either try to design sensors that directly observe this information, or develop methods to extract that information from existing sensors.
In the case of depth we already have a sensor fit for the purpose, and unless we were extremely brave we wouldn’t try to develop around it. It is mostly accepted that, at least for now, LIDAR is a crucial sensor for a self-driving car. However, information like category (car, pedestrian etc.) isn’t information that can be directly sensed, and we need to develop methods to deduce it. This is the area I’ve worked on most this summer.
For humans, the problem of understanding the above scene is simple: we can recognize the car on the right and the van in the back on the left and the pedestrians milling around in the center. But we have millions of years of evolution behind us: how can we give an autonomous vehicle that same ability? If you are familiar enough with the space, the automatic answer is convolutional neural networks (CNNs). CNNs have produced incredible results on a number of difficult problems, and with some small tweaks they’re very applicable to our LIDAR data.
However, a deep convolutional neural network might not be our best bet. A crucial constraint on our system is that it must operate at a high frame rate. The final system has to be able to run in real time and because of this we need to pick a solution that can produce good results very quickly. While a CNN can recognize complex patterns in images, they are often slow, so a lot of my work this summer has been in exploring alternatives. One alternative was hand-picking informative physical features like height that were highly correlated with an object’s category. Because we have a strong understanding of the physical world (often called a prior), we can save our models a lot of work by engineering these features ourselves.
If my mentor, Tarin Ziyaee, has taught me one thing this summer it is to experiment, experiment, experiment. As I developed and tested new models having detailed breakdowns of their performance helped out tons.
One of the first things I shipped this summer was a good, quick visualizer. Using Vispy let me visualize large point clouds in sequence and debug my models in conditions as similar to the real world as possible. Another lesson from this summer is that a good visualizer should always be the first step for problems like this. It can be very difficult to figure out what the problem is from just the model’s loss curve.
To build a set of tools for training and validating my models I used PyTorch. I wasn’t familiar with PyTorch before but it’s become my new favorite framework. It loses some of the specificity of TensorFlow but it is much easier to get started.
One of the models I built was an Encoder-Decoder network that transforms several channels of input data into categorical predictions. From these noisy predictions we can infer the real categories of all the objects in front of us. This kind of model is powerful because it can become independent of certain sensor and processing errors. For example, a model relying on the size and shape of objects to predict their class can be prone to detection errors. An encoder-decoder model can sidestep issues like this by recognizing patterns in the scene and turning them into predictions directly.
In the near future Voyage will be releasing more technical information on my internship and the work I’ve done, but the good news is a lot of the resources I’ve used are available right now! Almost everything I’ve done can be recreated using open source data and accessible papers. Voyage started life as an open source self-driving car, and it still capitalizes on open source research. I’m excited to be able to help share this work!
From his very first day, Michael has embodied all of the values we hold dear at Voyage, but especially “We Win By Shipping”. Even though we hear from a lot of engineers who say they can ship, it’s easier said than done. We were super lucky to have Michael work side-by-side with us this summer, and couldn’t be happier to have his work running on our self-driving car.
I’m a summer intern at @voyage working on perception and specifically classification.
772 
5
Thanks to Luke Beard and Oliver Cameron. 
772 claps
772 
5
Voyage is delivering on the promise of self-driving cars. Voyage has built the technology and services to bring robotaxis to those who need it most, beginning in retirement communities.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/quick-code/terraform-an-answer-to-cloud-scalability-844c3182e9a?source=search_post---------211,"There are currently no responses for this story.
Be the first to respond.
It can be said with certainty that, at some point, you’ve interacted with the Cloud. If not, then you currently are! As you’re reading this article on Medium’s cloud servers, you are…
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/building-a-go-web-app-from-scratch-to-deploying-on-google-cloud-part-1-building-a-simple-go-aee452a2e654?source=search_post---------212,"There are currently no responses for this story.
Be the first to respond.
Go short for golang, is a statically compiled language. It’s language structure resembles that of C/C++ with heavy emphasis placed on terse statements and removal of boilerplate most older languages carry around. Wikipedia article describes more motivations behind go:
"
https://medium.com/codingthesmartway-com-blog/introduction-to-firebase-cloud-functions-c220613f0ef?source=search_post---------213,"There are currently no responses for this story.
Be the first to respond.
This post has been published first on CodingTheSmartWay.com.
Subscribe On YouTube
Web and mobile application often require back-end code to execute tasks like: sending out notifications or processing long running tasks (e.g. scaling images etc). In the traditional approach this back-end code is running on a server.
Recently Google’s Firebase introduces a new functionality which is called Cloud Functions. With this new service Firebase offers a scaleable solution for running back-end code in the cloud. Running code in the cloud has various advantages:
In the following tutorial you’ll get an overview of Firebase Cloud Functions. We’ll be building a sample application step-by-step so that you can follow along the way and deepen your knowledge of this Firebase feature.
The functions you write can respond to events generated by Firebase and Google Cloud features. We will call these features triggers. Before starting to develop our first cloud functions let’s explore the most common triggers you can use for Cloud Functions in the following.
With Realtime Database Triggers we’re able to respond to changes in the Firebase Realtime Database. To do so we need to register for events on a specific database path like:
The functions.database.ref function is called and the database path we would like to register for is passed in as a parameter.
It’s also possible to define a part of the path as a wildcard by surrounding this part with curly braces, like you can see in the following:
If you’re just using the ref function, your registering for all events which are write, create, update and delete. If you just want to subscribe to one event you can use the following functions in addition:
E.g. if we only want a cloud function to be executed if a new user profile unter /profiles/ is created we’re able to register an event handler with the following code:
In this case the userID value can be accessed with event.params.userID inside the event handler function.
By using authentication triggers you’re able to execute code in response to the creation and deletion of a user account via Firebase Authentication.
To create an event handler function which is executed if a new user is created you need to use the following code:
According to the Firebase documentation this trigger is invoked in the following cases:
A Cloud Functions event is not triggered when a user signs in for the first time using a custom token.
If you would like to access further information about the user profile you can use the event.data object within the event handler method. The event.data object is of type UserRecord, so that you can access properties like displayName (The user’s display name) or email (the user’s primary email).
In the opposite case you need to use the following code if you want to register an event handler for the user deletion event:
With Cloud Storage Triggers you can trigger a the execution of a Firebase Cloud Function in response to uploading, updating, or deleting of files and folders in Google Cloud Storage. To register an event handler function we need to use the functions.storage object in the following way:
Herewith we’re registering an event handler for all object changes on the default storage bucket. If you want to specifiy a specific storage bucket you need to add the call of the bucket function as well:
Within the event handler function you can make use of various storage attributes:
A typical use case for a Firebase Cloud Function which is registered for a storage trigger is a task is needed to fruther process a file, e.g. generate a thumbnail for an uploaded image file.
Another trigger type which can be used with Firebase Cloud Functions are HTTP Triggers. These triggers can be invoked through an HTTP request and can be registered by using functions.https in the following way:
As you can see the event handler functions gets two parameters: req and res. The req objects (which is a Request object from the Express framework) gives you access to the properties of the original HTTP request sent by the client. The res object can be used to send a response to back to the client.
By using Google Analytics for Firebase you’re able to understand in detail how the user interacts with your application iOS or Android app. Therefore the the Analytics API exposes various events. Events of type conversion events can be used to register Cloud Functions in the following way:
In this case we’re registering an event handler for the conversion event in_app_purchase.
Now, that you have a first overview of what Firebase Cloud Functions are and which type of triggers can be used to invoke those functions, let’s try out to implement some sample functions.
Before initiating a cloud functions project we need to make use that the Firebase Command Line Client is installed and that the latest version is available on your system. The Firebase CLI is available as an NPM package. Execute the following command to install the firebase-tools package:
$ npm install -g firebase-tools
Having installed the Firebase CLI in the last step, we’re now able to login to Firebase by using the following command:
$ firebase login
You’ll see the following response:
The browser should open up and load the URL which is displayed in the console automatically. If you have already an Firebase account you’ll be able to sign in. If you have not created a Firebase account yet, you need to do so first. Having logging in you’ll be able to see the following message:
At the same time the login is recognized by the Firebase CLI in the console:
Now you’re ready to create a new Firebase Cloud Functions project. First create a new empty project folder:
$ mkdir fb-functions
Then change into the newly created folder:
$ cd fb-functions
and execute the following command:
$ firebase init functions
You’re being asked to select the Firebase project you would like to use for the Firebase Cloud Functions project. You can also select the entry [create a new project] if you would like to add a new Firebase project to your account:
The next question you’re being asked is “Do you want to install dependencies with npm now?”. As we would like to add all necessary dependencies you need to say “Y” here or simply hit return as “Y” is the default setting.
Now you should be able to see something similar to
The Firebase CloudFunctions project has been setup successfully initiated.
Before starting with the implementation of the first Firebase Cloud Function let’s take a look at the project structure:
Here you can see that the structure of the project is quite simple:
Let’s first try out the HTTP trigger. Open file index.js and insert the following implementation:
This is the most basic form of a Firebase Cloud Function implementation based on an HTTP trigger. First we’re requiring a reference to the the firebase-functions library. The Cloud Function is implemented by calling the functions.https.onRequest method and handing over as the first parameter the function which should be registered for the HTTP trigger.
The function which is registered is very easy and consists of one line of code:
Here the Response object is used to send a text string back to the browser, so that the user gets a response and is able to see that the Cloud Function is working.
To try out the function we now need to deploy our project to Firebase. Therefore we’re making use of the Firebase CLI again:
$ firebase deploy --only functions
The deployment is started and you should receive the following response:
If the deployment has been completed successfully and you get back the function URL which now can be used to trigger the execution of the Cloud Function. Just copy and paste the URL into the browser and you should see the following output:
If you’re opening up the current Firebase project in the back-end and click on the link Functions you should be able to see the deployed helloWorld function in the Dashboard:
You can also switch to the LOGS tab to get a detailed log output:
Let’s implement another example which uses two Firebase Cloud Functions. The first function makes use of the HTTP trigger and takes a string via URL parameter and inserts that string into the Firebase Realtime Database. The second Cloud Function makes use of the Realtime Database Trigger and is listing to changes written to the database by the first function. Inside this function we’re reading out the inserted string value, convert it to uppercase and write it back to the database.
Let’s get started with the first function insertIntoDB. Insert the following code in the file index.js:
First we need to require the firebase-admin module to be able to write to the Realtime Database. In addition we need to call the method initializeApp and pass in the Firebase configuration which is available via functions.config().firebase. The implementation of the insertIntoDB Cloud Function is done by using again a HTTP trigger by calling functions.https.onRequest. The function itself is passed in as a parameter. By using the Request object req we’re able to read out the URL parameter text first. Next we’re executing
admin.database().ref('/test').push({text: text})
to write the text string to the database path /test. E.g. if the URL is accessed and the URL parameter text is set to string “My text string” a new node ID is added as a subnode to /test and text property is inserted as a child element like you can see in the following:
The call of the push method is returning a promise, so that we can use the then method to execute code after the promise has been resolved (the database operation has been performed). In this case we’re using the Response object to call the redirect method to display the Firebase Realtime Database Console View in the browser.
Next, let’s implement the convertToUppercase Cloud Function which listens for changes to the database path /test/{pushId}/text:
In this case we’re extracting the properties value with event.data.val(). Next the toUpperCase method is executed to convert the string to uppercase and finally the uppercase value is written to the property uppercaseText in the same database node:
Firebase Cloud Functions are a powerful tool. The examples from this posts have given you a basic understanding of Cloud Functions. However, there are many more use cases for Cloud Functions. Take a look at the following list of possible use cases:
This post has been published first on CodingTheSmartWay.com.
Want to dive deeper into Angular. Check out the following online courses:
Disclaimer: This post contains affiliate links, which means that if you click on one of the product links, I’ll receive a small commission. This helps support this blog!
CodingTheSmartWay.com
922 
5
922 claps
922 
5
CodingTheSmartWay.com is a blog about latest web and mobile technologies.
Written by
Full-stack Web Developer, CodingTheSmartWay.com
CodingTheSmartWay.com is a blog about latest web and mobile technologies.
"
https://medium.com/google-cloud/running-jupyter-notebooks-on-gpu-on-google-cloud-d44f57d22dbd?source=search_post---------214,"There are currently no responses for this story.
Be the first to respond.
This post is clearly inspired by this tweet from @fchollet.
Surprisingly, I have not found similar instructions on getting this set up on Google Cloud. Their Cloud Datalab product currently does not seem to run directly on a GPU enabled machine. Hope that changes soon.
Here are the steps I followed. These steps assume you have your gcloud tools already updated (or you could use Cloud Shell).
Step 1: Confirm your GPU quota and zone
Not all GCP zones currently support GPU. Also, you may have to request an increase in your GPU limit.
As per https://cloud.google.com/compute/docs/gpus/, currently these zones support GPU — us-west1-b, us-east1-d, europe-west1-b, asia-east1-a.
If the limit is <1.0, then please request an increase in the limit at https://console.cloud.google.com/compute/quotas . In my experience, Google has approved the request instantly.
Step 2: Create GPU enabled instance
This creates an instance named gpu-deep-learner in us-east1-d zone with 1 GPU and Ubuntu 16.04 (persistent disk size 50GB).
Step 3: Install CUDA
ssh to the instance (e.g. using gcloud)
Install CUDA 8.0 driver and toolkit:
Confirm:
Set environment variables:
Step 4: Install cuDNN
Register at https://developer.nvidia.com/cudnn and download cuDNN. Then, scp the file to your new instance. e.g.
Now, back on the remote instance, unzip and copy these files:
At this point, all the NVIDIA/CUDA setup is complete. You can choose your favorite way of installing Python and any deep-learning libraries that use GPU. Anaconda is one popular way to do it.
Step 5: Install Anaconda and your favorite deep-learning frameworks
Follow the prompts. I also chose “yes” to ‘adding anacoda to your PATH’. It would be good to exit and log back in.
Use conda or pip to install your library. (pip worked better for TensorFlow):
Step 6: Configure Jupyter
Edit /home/ubuntu/.jupyter/jupyter_notebook_config.py to include the following at the top:
Step 7: SSH tunnel forwarding
Set up a tunnel from your local machine to access Jupyter over ssh.
Then, on the server, start Jupyter.
Step 8: Start using Jupyter on local browser
Navigate to http://localhost:8899/ and create a new notebook. Verify by importing keras or tensorflow. The remote log will also confirm whether you are using the GPU/Cuda libraries.
Once you are done, please remember to stop your instance to save costs. These GPU instances are not cheap. Enjoy.
Notes:
Google Cloud community articles and blogs
540 
13
540 claps
540 
13
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Code, Run, Loop
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/hackernoon/aws-vs-google-cloud-platform-which-cloud-service-provider-to-choose-94a65e4ef0c5?source=search_post---------215,"There are currently no responses for this story.
Be the first to respond.
Top highlight
While AWS is undoubtedly the benchmark of cloud service quality, it has some drawbacks.Today we compare Amazon Web Services (AWS) with Google Cloud Platform (GCP).
AWS definitely is the leader of the cloud computing services, due to being the pioneer in the IaaS industry since 2006 and being 5 years ahead of other popular cloud service providers. However, this leads to certain inconveniences and drawbacks that can be exploited by the competition. Essentially, the sheer amount of AWS services is overwhelming.
While Google Cloud Platform does not boast such an ample list of services, it rapidly adds new products to the table. The important thing to note is that while AWS does offer a plethora of services, many of them are niche-oriented and only a few are essential for any project. And for these core features, we think Google cloud is a worthy competitor, even a hands-down winner sometimes, though many of essential features, like PostgreSQL support are still in beta in GCP.
Look for yourselves. Google Cloud can compete with AWS in the following areas:
Customer loyalty policies are essential as they help the customers get the most of each dollar, thus improving commitment. However, there is an important difference here: AWS provides discounts only after signing for a 1-year term and paying in advance, without the right to change the plan. This, obviously, is not the perfect choice, as many businesses adjust their requirements dynamically, not to mention paying for a year in advance is quite a significant spending.
GCP provides the same flexibility, namely the sustained-use discounts, after merely a month of usage, and the discount can be applied to any other package, should the need for configuration adjustment arise. This makes long-term discount policy from GCP a viable and feasible alternative to what AWS offers, and rather an investment, not an item of expenditure. Besides, you avoid vendor lock-in and are free to change the provider if need be, without losing all the money paid in advance.
AWS is definitely the leader for building Big Data systems, due to in-depth integration with many popular DevOps tools like Docker and Kubernetes, as well as providing a great solution for serverless computing, AWS Lambda, which is a perfect match for short-time Big Data analysis tasks.
At the same time, GCP is in possession of the world’s biggest trove of Big Data from Google Chrome, which supposedly deals with more than 2 trillion searches annually. Having access to such a goldmine of data is sure to lead to developing a great kit of products, and Bigquery is definitely such a solution. It is capable of processing huge volumes of data rapidly, and it has a really gentle learning curve for such a feature-packed tool (it even produces real-time insights on your data). The best thing about it is that Bigquery is really user-friendly and can be used with little to none technical background, not to mention $300 credit for trying out the service.
As we explained in our article on demystification of 5 popular Big Data myths, cloud computing can be more cost-efficient as compared to maintaining on-prem hardware. Essentially, this really goes down to using the resources optimally and under the best billing scheme. AWS, for example, uses prepaid hourly billing scheme, which means running a 1 hour and 5 minute-long task would cost 2 full hours.
In addition, while AWS offers a plethora of various EC2 virtual machines under several billing approaches, these configurations are not customizable. This means if your task demands 1.4GB RAM, you have to go with the 2GB package, meaning you are overpaying. Of course, there are several ways to save money with Amazon, from bidding for Spot instances to lending Reserved instances and opting for per-second billing. Unfortunately, the latter option is currently available only for Linux VM’s.
GCP, on the contrary, offers per-second billing as an option for ALL their virtual machines, regardless of the OS’s they run on, starting 26th of September 2017. What’s even more important, their instances are fully configurable, so the customers can order 1 CPU and 3.25GB RAM, or 4.5GB, or 2.5GB — you get the meaning.
As The Washington Post told us, NSA has infiltrated the data center connections and eavesdropped on Google once (many more times, supposedly). This breach has lead to Google opting for full-scale encryption of all their data and communication channels. Even the stored data is encrypted, not to mention the traffic between data centers.
AWS is still lagging in this regard. Their Relational Database Service (RDS) does provide data encryption as an option, yet it is not enabled by default and requires intense configurations if multiple availability zones are involved in the equation. The inter-data center traffic is also not encrypted by AWS as of now, which poses yet another potential security threat.
All things considered, GCP is actually a serious contestant for both AWS and MS Azure. Yes, AWS leads in terms of the numbers of customers and products, due to 5 years of head start. At the same time, GCP already provides all the needed functionality and offers competitive pricing and configuration models, backed up by serious traffic privacy and security measures. With time, as more and more businesses accept the AI-first approach to doing business, GCP’s immense power in Big Data analytics and Google Chrome’s leading position amongst the browsers will allow Google Cloud Platform become an even more serious counterpart for AWS.
Just keep in mind many of GCP features are in alpha or beta stage, so their behavior and API might change. It means that using GCP in conjunction with long-term projects may require GCP connectors upgrade during the project lifetime.
We highly recommend evaluating the actual requirements of your project and think GCP is the best choice for development and staging environments. It might turn out you will suffice to go for GCP to meet all your needs and avoid opting for multiple AWS services required to use the platform efficiently but not actually needed to run your project. Feel free to contact us with any questions regarding the project requirements, we are always ready to assist!
#BlackLivesMatter
746 
3
how hackers start their afternoons. the real shit is on hackernoon.com. Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
746 claps
746 
3
Written by
DevOps & Big Data lover
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
DevOps & Big Data lover
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/scape-technologies/building-the-ar-cloud-part-one-72a7c5cd9697?source=search_post---------216,"There are currently no responses for this story.
Be the first to respond.
Top highlight
Note: This is part one of a multi-part series on what we believe is THE biggest and most exciting challenge in computing today. Read part two here.
Over the last century, computing has transitioned from server-rooms, to desktops, to our pockets. ‘Spatial computing’ is the next step, as we move from content hidden behind screens, to a ubiquitous digital layer that exists in the world around us.
One of the first industries to tap into this ubiquitous ‘digital layer’, is augmented reality.
Why is this such a big deal?
Instead of looking at 2D diagrams in textbooks, Medical students will be able to visualise the body in A.R., allowing them to gain a deeper understanding of how the body works. Designers will be able to rapidly prototype new ideas and see those new ideas materialise before their eyes. Engineers will be visually guided through day-to-day tasks, allowing them to fix machinery that they’ve never even seen before.
A.R. will do for skills & understanding, what the internet browser did for information.
However, there are a number of challenges left to solve before this vision of the future can become a reality. First and foremost, is understanding your precise location.
Up until now, the majority of mobile-AR experiences have been so-called ‘table-top AR’ i.e. experiences designed to be viewed within space no more than a few meters in front of you. In these examples, the location you are in, has no significant impact on the content viewed.
However, to tie digital information to specific physical spaces at a global scale, it’s vital to know the precise location and orientation of the person viewing that content. This is because, unless we know exactly where you are, digital information and the physical environment won’t align.
On most mobile phones, GPS is only accurate to within 7 meters. What’s more, in urban environments, the signals sent by GPS satellites can bounce off buildings, further adding to the uncertainty of where you actually are.
If you rely on GPS for location & compass for orientation, two people in the same space will have totally different reference frames for where they are. Consequently, this means it would be impossible to have a multi-user experience, as everybody would have different frames of reference for where the AR content should appear.
If people are unable to pin content to a particular location & collaborate with others in the same shared space, A.R. will never achieve its full potential.
We need something better.
In answer to the limitations with existing technology, industry veterans and AR researchers have long proposed something called the ‘AR-Cloud’, described as “the single most important software infrastructure in computing, far more valuable than Facebook’s social graph or Google’s PageRank index”.
In the same way that Google indexes the web, the AR-Cloud (aka Machine-Perception Cloud) serves as an index of the real world, which can be used to align both content and devices to the same reference frame.
Rather than relying on GPS, the AR-Cloud uses computer-vision to provide a precise ‘anchor’ of where devices are in the world, with a higher degree of accuracy than GPS. Instead of your devices knowing roughly where they are, devices using the AR-Cloud will be able to determine exactly where where they are, down to the centimeter, allowing augmented content to have spatial persistence in space. The AR-Cloud allows people to collaborate with augmented content within the same reference frame, as they would with physical objects.
However, the AR & Machine-Perception cloud is far more than just determining your location.
After informing devices ‘where they are’, a second component of the AR-Cloud lets devices know ‘what they are looking at’. We call this second component, the ‘contextual layer’.
This second component of the AR-Cloud allows devices to understand the physical structure and semantics of what’s nearby. This ability for devices to understand what is in the scene around them, will help inform how information should be displayed.
Finally, on top of letting devices know where they are and what’s around them, a third component of the AR-Cloud keeps up-to-date with what’s changing. This is important because, while certain objects like buildings and roads don’t change often, people and things are far more dynamic. The role of this third layer is to track of these dynamic objects within the environment, predict what will happen next and ensure this information can be accessed by those who need to.
Together, the three components of the AR-Cloud allows camera-enabled internet devices to tap into a collective understanding of the world around them.
Over the next few years, we will see a shift from computing devices which we access from behind rectangular screens, to computing devices which have a physical place in the world around us. Ultimately, the AR-Cloud is the system which connects the physical world to digital infrastructure. Any machine or device that needs to understand where it is and what is around them, will benefit from the same core infrastructure.
By allowing devices to understand where they are and what’s around them, we believe machines can become more collaborative, more intelligent & ultimately, more useful. In the age of autonomous cars, advanced robotics & drones, it’s increasingly clear that this new type of cloud mapping infrastructure will be an invaluable component in helping these new industries to thrive.
At Scape Technologies, we’ve been quietly building parts of this ‘AR Cloud’.
From our offices in London, our team has been constructing a 1:1-scale mapping infrastructure for computing devices, that allows content or services to be tied to specific places in the world, outside & on a massive scale.
Over the next few months, we’ll be releasing a running series of articles designed to share our team’s thoughts, challenges-faced & progress-towards building this fundamental infrastructure.
If you are keen to learn more about the work we are doing at Scape or what you can do to partner with us, please, get in touch.
This article is part one of a series on building the AR or ‘Machine Perception’ Cloud. Read part two here.
Over the next few months, as we gear up for public release, we will touch upon what makes our technology unique and demonstrate more of what our system is capable of. If you have questions you would like answered, please leave them in the comments and I will address them in future posts.
Edward is co-founder & CEO of Scape Technologies, a computer vision startup in London, working to build a digital framework for the physical world.
Follow Edward and the company on Twitter here.
We send a newsletter every couple of months, making sense of the AR industry and sharing our progress.
Sign Up to our newsletter.
Scape Technologies is building a cloud-based ‘visual…
796 
2
796 claps
796 
2
Written by
Co-Founder Scape Technologies, building a digital framework for the physical world www.scape.io
Scape Technologies is building a cloud-based ‘visual engine’ that allows camera devices to understand their environment, using computer vision.
Written by
Co-Founder Scape Technologies, building a digital framework for the physical world www.scape.io
Scape Technologies is building a cloud-based ‘visual engine’ that allows camera devices to understand their environment, using computer vision.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/descarteslabs-team/thunder-from-the-cloud-40-000-cores-running-in-concert-on-aws-bf1610679978?source=search_post---------217,"There are currently no responses for this story.
Be the first to respond.
Back in 1998, one year after winning his second Gordon Bell prize in high-performance computing, Mike Warren and his colleagues at Los Alamos National Lab assembled the first Linux cluster to appear on the TOP500 list of the world’s fastest supercomputers. Now, leading our tech team 21 years later at Descartes Labs, he’s completed another trend-setting accomplishment by setting a new performance benchmark using virtualized resources in the public cloud, obtaining more petaflops than the fastest computer in the world could in 2010.
Mike’s use of Linux in 1998 was an outlier as it rejected the fragmented and proprietary operating systems that powered each of the other 499 machines on the list. He had grown tired of porting code between IBM, Sun, and a half-dozen flavors of UNIX, so the decoupling of hardware and software that Linux offered was positively embraced. Among other benefits like scalability, interoperability and a worldwide community, Linux allowed Mike and other HPC innovators to change the game by assembling commodity PCs into custom supercomputers of their own.
Today, what was once just a single instance, is now a de facto standard, as every single one of the TOP500 computers on the list run Linux. The adoption of the Linux operating system represented a huge leap in simplifying the design and deployment of HPC applications. It used to be that practitioners would buy a special IBM or Cray system, then it became easy to purchase mail-order PC’s and install Linux, now Amazon and other cloud providers have basically taken the hardware part out of the equation. This latest advancement severely disrupted HPC vendors starting around 2002 as usage split between “tightly-coupled” applications running on dedicated machines and “loosely-coupled” applications running in the public cloud, even though both were using essentially the same hardware underneath.
There are many well-known examples of loosely-coupled applications running successfully on the public cloud with tens or even hundreds of thousands of cores. Applications abound across drug discovery, materials science, particle physics, and the cleaning and calibration of petabytes of satellite imagery that we run here at Descartes Labs. These “massively parallel” applications are amazing in their own right but lack the “interconnect”, or core-to-core low latency network communication that’s needed to power big physics simulations like seismic processing, thermodynamics, cosmology, weather forecasting and more. These highly interconnected applications were previously thought of as only suitable for massive on-premise systems like the Summit supercomputer at Oakridge National Laboratory, or China’s Sunway TaihuLight, the latter being used to simulate the birth of the universe with a technique known as “N-body simulation.”
The two supercomputing paths may be starting to merge back together again as our team recently demonstrated on Easter weekend in April 2019. Using publicly available spot resources on AWS, we attained performance of 1.926 petaflops running the standard of HPC achievement, a giant matrix inversion called the Linpack Benchmark. Our engineering team’s goal that day was to use the Linpack Benchmark to see how far the cloud would scale. If it was capable of placing #136 on the TOP500 list then it would be capable of running global-scale customer models for Descartes Labs, including the simulations we developed for Cargill and DARPA.
One of the more interesting aspects of the story is that we didn’t ask Amazon to give our engineers any special dispensation, discount, or custom planning or setup. We wanted to see if we could do this on our own, which if completed successfully, would also be a testament to the self-service model of AWS. Our team merely followed the standard steps to request a “placement group”, or high-network throughput instance block, which is sort of like reserving a mini-Oakridge inside the AWS infrastructure. We were granted access to a group of nodes in the AWS US-East 1 region for approximately $5,000 charged to the company credit card. The potential for democratization of HPC was palpable since the cost to run custom hardware at that speed is probably closer to $20 to $30 million. Not to mention a 6–12 month wait time.
Mike believes this is the first time “virtualized” processors have been used on the TOP500 list, although AWS has been on before with a four times smaller 0.484 petaflop setup that was widely believed to have been run on bare hardware. Our system leveraged unique technical details like a fine-tuned hypervisor between the Descartes Labs code and the virtualized AWS Intel Skylake processors, as well as advanced use of MPI messaging, and Packer, a tool for creating identical machine images that manage the software configuration on each node. All of these taken together may mean the Descartes Labs system is deserving of its own software category on the TOP500 list.
Mike Warren’s vision today is a continuation of the story that started with Linux in the early days. He’s aware of the common refrain that “everyone knows the cloud is useless for tightly coupled HPC.” But he believes that just isn’t true. The cloud can definitely be used. It’s not magic, just a mix of experience, skill, and a mind for innovation. For some reason, others just haven’t really tried it yet. It’s kind of like how no one knew you could use mail order PC’s to roll your own supercomputer, or no one knew you could use Linux instead of dealing with the latest flavor of UNIX shipped on an IBM or Cray.
Back at our headquarters in Santa Fe, our team is constantly tuning the ideal architecture needed to serve our global-scale earth science projects. These include large-scale weather simulations, giant 3D Fourier transforms in seismic modeling, and greenhouse gas mixing dynamics in the atmosphere. We believe that true HPC applications will eventually migrate over to the cloud en masse. The advantages versus traditional supercomputers are hard to ignore. HPC professionals can buy their own machine at a massive cost or rent time on a highly specialized cluster somewhere that uses an old version of Linux from two years ago that needs to be brought up-to-date. But in the cloud, it’s all under your control. There can be seven different versions running different Linux kernels tuned for specific applications and it’s all easy to manage.
In summary, Big machines have historically been used for a very specific purpose, but the cloud is generalizable across purposes. The democratization of HPC is bringing the price point down to something that’s available to business, and we are well positioned to help our customers take advantage of it. We’ve built a Data Refinery that processes location-related data across the world and makes predictions about future states. These predictions are made possible only by scaling up a supercomputer in the cloud that can handle petabyte or exabyte datasets. This scaling enables our customer models to become truly global in their size and scope.
If you have a complex systems problem that you previously thought was too expensive or complicated to model, consider contacting us to learn more. Until then, keep an eye out for our tightly-coupled supercomputer in the cloud on the latest TOP500 list.
Explore posts from the Descartes Labs team
812 
4
812 claps
812 
4
Explore posts from the Descartes Labs team
Written by
Descartes Labs is building a data refinery for geospatial data.
Explore posts from the Descartes Labs team
"
https://medium.com/@hellomondaycom/how-we-built-the-google-cloud-infrastructure-webgl-experience-dec3ce7cd209?source=search_post---------218,"Sign in
There are currently no responses for this story.
Be the first to respond.
Hello Monday
Oct 5, 2018·13 min read
Google Cloud wanted to explain the vastness of their world wide infrastructure. How their data centers and regions work, and how incredibly close to end users Google Cloud has edge nodes and points-of-presence to enable fast, low-latency access to the Google Cloud backend infrastructure. Complex systems are often hard to describe with words alone, so we decided to map out the entire system and make it tangible by building an interactive world in 3D.
These are our most important learnings.
The site is built as a single page app, utilising the History API for navigation and deep linking.
Most of the site’s javascript is authored in TypeScript which, in addition to weeding out common bugs and typos, provided critical refactoring support during the development process.
Various libraries are in use, and without these it would not have been possible to build the site in a reasonable timeframe, so a big THANK YOU to the authors and contributors of these projects:
The site is hosted on Google Cloud Platform’s own App Engine (obviously): https://cloud.google.com/appengine/
All 3D models were created in Cinema4D with Blender used for importing/exporting to glTF.
The entire website clocks in at 2.9MB gzipped, 2.22MB without audio.
We knew we wanted a fairly minimal 3D look with emphasis on subtle greyscale variations as well as a few main colors from the Google Cloud color scheme. The first thing that came to mind was to try and go for a classic ray-traced clay-style with nice beautiful soft shadows, a look well known from architectural renderings. So even before we knew what the final models where gonna be, we started exploring if this was possible.
Given that render times in Cinema 4D (for this look) way exceeded anything remotely close to real-time rendering, even on a machine equipped with 4 x NVIDIA GTX 1080 Ti graphic cards, it was clear from the start that real Ray Tracing was not a viable approach for something that would eventually need to run on just a regular consumer laptop, newer smartphones etc.
From previous projects we’ve learned that this is often the case when moving from 3D software like Cinema 4D into the browser. What we often do is to try and explore different visual styles through renderings from Cinema 4D and then figure out how to recreate or mimic that look in WebGL. It’s a back-and-forth process, and while it’s time-consuming it’s also what makes it fun to develop these kinds of projects.
It’s not always clear just where we’re gonna end up.
The clay look trough Ray Tracing is mostly defined trough the soft shadows and light bouncing around multiple times to pick up the colors of the materials they bounce off of. Light bouncing multiple times is computationally very costly and so we quickly focused on getting the shadows right, foregoing true Ray Tracing.
We explored adding Ambient Occlusion using a shader pass known as Screen Space Ambient Occlusion (SSAO) — however once again it led to another trade-off. When setting up post-processing shaders in Three.js, it’s done by setting up a new offscreen render target making the browsers built in anti aliasing unavailable.
Having lost anti aliasing we looked at ways to bring back the sharp straight lines. So we explored adding custom anti aliasing shaders, like the FXAA shader suggested in this issue: https://github.com/mrdoob/three.js/issues/568.
While it enabled us to disable the browsers’ built in anti aliasing — giving quite a performance boost — the custom anti aliasing shaders weren’t quite up to par quality-wise with the browsers built in anti aliasing.
Given that the site and models are very minimalistic — almost to the point of looking like vector graphics — any low-res shadows or aliased lines stood out quite glaringly as opposed to a more complex/colorful 3D scene where the eye has a tendency not to focus so much on these little imperfections in render quality.
The first thing that was modeled (and re-modelled multiple times) was a Region from the outside and the inside of a Data Center. A Region consists of multiple Data Centers. Through many iterations and Hangouts with Google engineers we landed on a representation that fitted close enough to how an actual Google Cloud Region is laid out.
The initial export from Cinema4D was 100.4 MB OBJ-file. No way was that gonna fly for a site even though it did actually load (slowly). For quick model testing, Three.js’ editor is highly recommended: https://threejs.org/editor/
When looking at 3D model formats it can be quite a jungle to narrow down what the best option is. It’s not a one-size-fits-all, but the Khronos Group has put a lot of effort into the glTF format, striking a fine balance between low file sizes, good export options, fast browser parsing time and high consistency between 3D authoring tools and the model loaded with Three.js’ GLTFLoader. All our models where exported as glTF binaries (.glb) giving a boost in browser parse times. With other formats it’s often not just the actual network transfer that extends load times, but also the amount of time the browser uses to parse the downloaded models.
Switching to the glTF format gave us a significant reduction in file-size and browser parse time, but the file was still too big, not only in size but also in complexity. We simply had too much detail in our model. So by working in a tight iteration cycle of trying to reduce model complexity by removing vertices and testing how it looked in the browser, we finally got the model down to the size it is now: ~716 KB (gzipped). We also stripped away the normals data in the exported model, opting to calculate them at load using Three.js’ Geometry.computeVertextNormals();
The real trick though was not just to optimize the model, but to identify identical objects.
Our model consisted of many identical objects like fans, server racks, trees, cars, trucks, windmills, cooling towers etc. The only difference being each instance’s individual position, scale and rotation (PSR), the actual geometry of each instance was exactly the same, so no need to load that geometry more than once, and most importantly for render times, no need to upload the geometry to the GPU more than once. The Three.js repo has many examples of this technique, which combines rendering multiple objects with individual PSR in one draw call.
In addition to PSR instancing, we needed to maintain the ability to animate each object individually, so for example all the fans wouldn’t be rotating in unison etc. To accomplish this on the GPU involves writing a custom vertex shader and putting the individual animation information onto custom BufferGeometry attributes.
All in all not the easiest to work with solely within Three.js standard APIs, so we opted to add the THREE.BAS library to make it a bit easier to work with these GPU based animations while maintaining the ability to use Three.js materials and their support for lights.
Offloading the animation calculations from the CPU to the GPU comes with speed improvements at the cost of a little flexibility, but overall this was okay for the simple animations needed on the fans, windmills etc. However, we still needed to be able to have some over the animations, so we exposed a single uniform in the vertex shader that then acts as an overall progress variable for the whole set of animations.
Our model contains only one source geometry per object that’s instanced, so for instance for a tree, we just defined one tree as the source, and used Instance Objects in Cinema 4D to place copies of the source tree all around the scene with variations in PSR. We wanted to be able to place these objects visually and preview it within it Cinema 4D and have an easy way to export the instanced copies’ PSR information embedded within the model. This way, to preview new changes, we only needed to do one export of the model from Cinema 4D to glTF (more on that in the next section).
Turns out it’s quite easy, we simply landed on a naming convention where we name source geometry objects: source_object_name and then all the instanced objects are named: replace_object_name_X. Here’s a screenshot of how that looks for the trees:
Now there is one extra step, sadly. Because Cinema 4D can’t export directly to glTF we needed to roundtrip through Blender using FBX as an intermediary format. When exporting Instanced Objects from Cinema 4D to FBX all the Instanced Objects are replaced with actual geometry — not what we wanted. The way we solved it was to convert all the Instanced Objects to Cinema 4D Null objects, thus maintaining the Instanced Objects PSR info on export, but without replicated geometry:
Sadly there is (as of yet) no direct way to export/import glTF files in Cinema4D so we needed to go through Blender with the glTF import/export extension installed.
The flow ended up looking something like this: Cinema4D → FBX Export → Blender → Import FBX → Export glTF Binary
Here’s a short screen recording of the flow which also shows some of the import/export settings we ended up using to compensate for the differences in world-scale and world-orientation between Cinema4D and Three.js:
The above video ends up in the Three.js Editor which is a great tool for previewing models and playing around with materials, lights, geometry etc. Sadly by default there’s no way to have this “more visual approach” to playing around with the various properties of geometries, lights, materials etc. in your own project. That said, it’s very much needed, and it also what gives applications like Unity3D their great edge. In some cases you just need to play around and do tiny tweaks to lights and materials to get everything right, or to compensate for any annoying z-fighting bug etc. And while it won’t bring the power of Unity3d into the browser the Three.js Inspector Chrome Extension does add some much needed features to do just this kind of in-browser tweaking — no need to constantly change tiny code parameters and wait for a full reload of your site:
Just remember to add your Three.Scene to the window object:
At this point we had pretty much been forced to cut all realtime shadows in favor of a sharp look, but we also wanted to bring back some of the original look. A low cost way of adding shadows and other lighting effects is to bake them into a texture directly from Cinema4D and load them onto a flat plane in Three.js. We baked two textures, one for the outside of the Region and one for the inside of the Data Center. And we used two textures to have higher resolution shadows inside the Data Center.
Another goal was to be able to make smooth transitions from one scene to another. Now the most important thing to remember when dealing with GPUs and 60FPS is that any new information that needs to get uploaded to the GPU takes time and can cause frames to drop. Especially textures can be heavy to upload. There are various formats (PVRTC, DDS, ETC…) optimized for quick decompression and low memory usage on the GPU, however the native support on different platforms varies wildly and the file-size compression settings in our testing couldn’t match regular JPGs. So we decided to use JPGs because of their small network load, and sacrifice some runtime performance.
This is one of the areas we’d like to explore more, but for this project we opted to try and mitigate some of the runtime overhead by pre-uploading all textures and precompiling all materials (shaders) needed for the entire site on initial site-load. At a high level this basically means that we tried to make the cameras see all the different scenes at load by making everything visible, forcing a render once and then resetting everything to their original visibility:
Building a WebGL site that needs to run 60FPS is no small feat. It’s a constant compromise between look, feel, and responsiveness. Many factors come into play such as not knowing what the capabilities of the device it’s gonna run on is, what resolution it will run at and so on. A visitor might have a Macbook hooked up to a 4K display — that’s just never gonna fly for anything but a fairly static site. We tried to set sensible defaults, but also explored adding a quality switcher to allow visitors to choose themselves. Admittedly, this is probably not the most used feature of the site, but building it gave some insights into which parameters matter in terms of performance, and how to change these at runtime without reloading the full site. We did not want to have the user choose a quality setting when they entered the site.
To keep it simple and to avoid adding a ton of conditional code based on the settings, we chose to tweak the rendering size (pixel ratio) and toggle anti aliasing on/off.
This resulted in three quality settings:
The tricky part was to toggle anti aliasing on/off as this currently requires us to setup a new THREE.WebGLRenderer(); and clean up the old one. Pseudo code:
For further tips on how to optimize performance the responses in this Twitter thread by @mrdoob gives some good quick hints.
Since we now had a way to tweak the device requirements we wanted to explore if there was a way to automatically detect and set the quality setting based on how powerful a visitors device is. Long story short —kinda, but it’s a bit of a hack and there’s too many false positives.
EPIC Agency describes the approach in this article under “Adaptive quality”.
The approach relies on sniffing out a specific GFX card name, using the WebGL extension WEBGL_debug_renderer_info and then correlating it to a performance score.
However their project was limited to mobile devices where it may be more reasonable to link certain GFX cards to a specific performance value, since mobile devices’ GPU capabilities are usually better matched to the devices’ screen resolutions, whereas on desktop devices this may not necessarily be the case.
It is however possible to use this information if there’s a known GFX card that is underpowered for your project even at very low resolutions —think of it like minimum requirements for a game. It would have to be a GFX card that we know is used on a lot of devices to make it worthwhile the effort of testing all these GFX cards’ performance, not to mention the regular expressions needed to detect them all.
On top of that iOS 11 introduced throttling requestAnimationFrame(); to 30FPS when in Low Power mode, probably a sensible feature to save battery, but they left out a way for a site to differentiate between intentional system-wide throttling or if we are simply running on a slow device.
We decided that for the time being the best option was to not try and be too smart about auto-changing quality settings, not even based on just a simple average frame rate tracking technique.
It can be hard to track how small changes affect the performance of a WebGL project if you’re well within the limits of your GPU and browser. So let’s say you wanted to figure out how adding a new light to a scene impacts performance, but all you’re seeing is a smooth 60FPS — with and without the new light. We found it helpful to disable Chrome’s frame rate limit of 60 FPS and just let it run as fast as it can. You can do this by opening Chrome from the terminal:
open -a ""Google Chrome"" --args --disable-frame-rate-limit
Be mindful that other tasks running on your laptop or in other tabs may affect the FPS …looking at your Dropbox!
This is more of an observation than a learning, but Safari’s WebGL performance simply blows away Chrome’s on 5K displays (and probably also on lower res displays). We found that we could easily increase the pixel ratio to almost native display resolution in Safari and still have 60 FPS, sadly this is not the case with Chrome as of today.
The biggest pain point right now for us when developing a WebGL heavy site is all the intermediary steps between having an idea in Cinema 4D to actually previewing how that idea carries over into Three.js and the browser — a lot of good work has gone into solving some of these pain points, but we’d still like to find easier workflows to enable higher parity between how materials, lights and cameras look and behave in Cinema 4D vs. their Three.js counterparts. PBR materials and the work put into glTF goes a long way to smoothen these things out, but there’s still room for improvement. Biggest wish for now would be direct export to glTF from Cinema 4D.
Written by Lasse Moos (Creative Developer at Hello Monday)
A Digital Creative Agency on a mission to turn the worst day of the week into the best one. A smørrebrød of experiments, inspiration, and creativity.
See all (123)
1K 
5
Thanks to Lasse Moos. 
1K claps
1K 
5
A Digital Creative Agency on a mission to turn the worst day of the week into the best one. A smørrebrød of experiments, inspiration, and creativity.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/android-news/build-a-phonebook-with-cloud-firestore-in-10-minutes-59c65e7af4ad?source=search_post---------219,"Sign in
There are currently no responses for this story.
Be the first to respond.
Ekene Eze
Oct 12, 2017·10 min read
According to the Documentation, Cloud Firestore is a flexible, scalable database for mobile, web, and server development from Firebase and Google Cloud Platform.
Like Firebase Realtime Database it keeps your data in sync across client apps through realtime listeners and offers offline support for mobile and web so you can build responsive apps that work regardless of network latency or internet connectivity, this is just amazing if you ask me, it solves the whole essence of PWA’s. You may ask how, well
Cloud Firestore caches data that your app is actively using, so the app can write, read, listen to, and query data even if the device is offline. When the device comes back online, Cloud Firestore synchronizes any local changes back to Cloud Firestore.
Cloud Firestore also offers seamless integration with other Firebase and Google Cloud Platform products, including Cloud Functions.
However, Cloud Firestore is still in beta release as at the time of writing this article but if you’re enthusiastic (like me) and want to get hands on immediately, then this tutorial will help you get started with basic read, write, update and delete operations.
If you don’t know these yet then, you’ll
· Gain a better understanding of Firebase and Firestore
· Learn how to add firebase to your project
· Learn the FireStore database model and structure
· Learn how to write to Cloud FireStore
· Learn how to read from Cloud FireStore
· Learn how to update an existing data in Cloud FireStore
· Learn how to delete data from Cloud FireStore
· Learn how to listen for changes in realtime
· Learn how to combine all these features to create a Phonebook
As usual I like to prepare my tutorials to be very relative and not so abstract, so i won’t be talking too much, let’s start building something with FireStore.
Create a new android studio project and connect it to firebase. Once again there are two ways you can do this depending on the version of Android studio you’re on. If you’re using the latest version of Android Studio (version 2.2 or later), i recommend using the Firebase Assistant to connect your app to Firebase. The Firebase Assistant can connect your existing project or create a new one for you and automatically install any necessary gradle dependencies. Here’s how you can do it
To add firebase to your app with Firebase Assistant in Android Studio:
· Click Tools > Firebase to open the Assistant window.
· Click to expand one of the listed features (for example, Analytics), then click the provided tutorial link (for example, Log an Analytics event).
· Click the Connect to Firebase button to connect to Firebase and add the necessary code to your app.
That’s it!
If you are using the earlier versions of Android Studio or you prefer not to use the Firebase Assistant, you can still add Firebase to your app using the Firebase console.
To manually add Firebase to your app you’ll need a Firebase project and a Firebase configuration file for your app.
1. Create a Firebase project in the Firebase console, if you don’t already have one. If you already have an existing Google project associated with your mobile app, click Import Google Project. Otherwise, click Create New Project.
2. Click Add Firebase to your Android app and follow the setup steps. If you’re importing an existing Google project, this may happen automatically and you can just download the config file.
3. When prompted, enter your app’s package name. It’s important to enter the package name your app is using; this can only be set when you add an app to your Firebase project.
4. At the end, you’ll download a google-services.json file. You can download this file again at any time.
5. If you haven’t done so already, copy this into your project’s module folder, typically app/.
Then you can add any of the Firebase libraries you’d like to use in your project to your build.gradle file, it is good practice to always add com.google.firebase:firebase-core.
This guides are according to the firebase documentary however, if all of this is still not clear to you, you can always find more content on how to setup your firebase project here
Now that you have your firebase project set up, let’s add the dependency for FireStore. As at the time of writing this tutorial, the latest version is compile ‘com.google.firebase:firebase-firestore:11.4.2’ so add it to your app level build.gradle file dependency block. It might have changed depending on when you’re reading this so always check the Docs for the latest dependency and sync.
Before we proceed let’s take a moment to understand the FireStore model
Cloud Firestore is a document database; it stores all your data in documents and collections. Documents operates more like containers of fields(key-value pairs) of diverse data types however a document cannot contain other documents but can point to a sub-collection. Collections on the other hand operate also like documents but in its case, it’s a container of documents hence, it can’t contain anything else but documents.
From the above image, “Users” is a collection of 3 documents “Joe”, “Tammy” and “Vic”. Document Joe however, points to a sub-collection “Workouts” which in itself has 3 documents (“Yoga level 2”, “Beginner Jogging”, and “7-minute thighs”). Then document “7-minute thighs” further more points to a sub-collection “History” which in turn holds 3 more documents etc.
This is deep nesting and it may seem a little bit deep but it is very efficient for query performance in both simple complex query structures. With this idea of how Firestore works, let’s dive into the firebase project we created earlier and start writing some code.
For the purposes of understanding the basics of Firestore in the best simple way, we’ll be building a Phone Book database that stores contacts with 3 fields (Name, Email and Phone). We’ll demonstrate how to write, read, update and delete data from the database.
Usually we would have started with setting up EditText views in the layout files but we won’t be taking inputs from users yet so we’ll just supply all the data we want to store to the database from our java files for now. So open up MainActivity and initialize Firestore:
You may have noticed that i also initialized a TextView object and some String variables there; we’ll use the TextView to read data from the database and the String variables as “keys” for storing data into our document.
Considering the model we’ll be working with, it is best to think of the Phonebook we are building as the collection, so let’s add/save a newContact to our PhoneBook.
Here we first created a method addNewContact(), the method will create the newContact that we’d want to save in the PhoneBook and then pass the fields to it. Next we need to specify in which document we want to save the contact. We do this by extending the Firestore instance we created to accommodate both the collection and the document.
“db.set (newContact)” this creates the document if it doesn’t already exist and also creates the PhoneBook collection so as to store the data in the appropriate path according to the db path we initialized.
Then we add an OnSuccess/FailureListeners to make a Toast and keep us informed if the contact was registered successfully or if the registration failed.
Looks like we’ve got it all set up, before we run the app, we’ll need to head on over to the firebase console and tweak our RULES Tab a little bit to allow us read and write to the database because at default, nobody can read or write to it. So go to your firebase console, open the project and make sure you select Firestore in the database tab
Then open the RULES Tab, by default it looks like this
Then add this block of code to override the default security settings and gain external read and write access to your database,
Now your RULES Tab should look like this
This is a terrible idea security wise, if you’re working on a real project or something proprietary, you would definitely want to properly authenticate users and define your security measures clearly. This tweaking is only serving the purpose of this tutorial.
Haven set up the security rules in the console, publish it and wait for a few minutes after which you can now run the app and go back to the DATA Tab on the console. Hurrray!! App works…. You’ve just stored your your data to FireStore.
Now let’s try and read the data we’ve stored in the database to a TextView.
READ | First we create ReadSingleContact() method that’ll read the data from the database and pass the data to the TextView object we earlier initialized for this Purpose.
If we run the app, it populates the TextView object with the information stored on the database as is expected.
UPDATE | Next we’ll try to update the already stored Contact informations
Now running the app will update the earlier stored data with the newly provided values in the UpdateData() method.
In the DATA Tab on your firebase console, you’ll find that the changes have been updated.
And if you run the ReadSingleContact() method again, it’ll update the TextView with the current details.
REALTIME UPDATE
Next up, we listen for update on the document “Contacts”, if we update these changes without running the code to Read (fetch) data from the database (ReadSingleContact()), we might not be able to see the changes in the UI so we’ll create a method that listens for update in realtime and makes a Toast to notify users of the changes in the database.
Now let’s take input from users instead of just adding and updating the database from the studio. I’ll go ahead and create view objects on the mainActivity layout file to take Inputs for the 3 fields (name, email and phone) such that we can save whatever data the user types into any of these fields to the PhoneBook.
We’ve already done all the major work required so just create the view objects in the layout file and initialize them in the MainActivity.java file. Here’s my activity_main.xml
Next we initialize the views in the java file
Notice the additions of the view objects as compared to the code gist we had under the WRITE heading. Then to save the details coming from this Views to the database, we’ll just tweak our newContact.put() method a little bit to capture its value from the EditTextViews. So we’ll open up the addNewContact() method and edit it to capture these changes
Then we need to set the save button to call this method, so lets set an onClick listener to the button and call the method in it.
Then finally let’s update the TextView with any changes we make to the database in realtime, such that if a user fills up the input fields and clicks the save button, the TextView will update the changes in realtime wether there’s internet Connectivity or not.
And that’s it!! At this point, Users can input their details and save it to the database as well as have that same details read from the database and displayed on their screen.
DELETE | Finally lets perform the delete operation to delete the Contact from the database.
This method successfully deletes the contact from the PhoneBook database. So congratulations if you followed up to this point, this means you can now perform basic operations on one of the most sophisticated and newest online database in the world. This is just a glimpse of what you can do with Cloud FireStore, there are a lot more features and operations to perform. My intention is just to get you started with the basics.
Thanks for reading, you are welcome to make contributions and ask for clarifications anytime you deem fit. Like always, I’m available (twitter @kenny_io) to provide more clarifications and answer your questions if you have any. Always remember to hold down the clapping hands icon below the screen to appreciate the work.
The source code is also available on github if you need it.
The content I’ve written here doesn’t reflect my current realities as I’ve since moved on from Android Development. I’m Ekene Eze, and you can call me Kenny.
878 
7
878 
878 
7
The (retired) Pub(lication) for Android & Tech, focused on Development
"
https://servian.dev/exploring-beam-sql-on-google-cloud-platform-b6c77f9b4af4?source=search_post---------220,"Sign in
There are currently no responses for this story.
Be the first to respond.
Graham Polley
Jan 24, 2019·10 min read
Did you know that a new feature was recently rolled out for Apache Beam that allows you to execute SQL directly in your pipeline? Well, don’t worry folks because I missed it too. It’s called Beam SQL, and it looks pretty darn interesting.
In this article, I’ll dive into this new feature of Beam, and see how it works by using a pipeline to read a data file from GCS, transform it, and then perform a basic calculation on the values contained in the file. Far from a complex pipeline I agree, but you’ve got to start somewhere, right!
<tl;dr> You can now inject SQL directly into your Apache Beam/Dataflow pipeline (using the Java SDK only), which means having to write a lot less code. It’s still very new and probably not ready for production workloads just yet. But, it’s certainly something to keep an eye on as it quickly matures. Full source code for below example here.
After seeing Beam SQL pop up in the docs a few months ago, I’d been meaning to test drive it for a while. But, being summer here in Australia, I was finding it rather hard to pull myself away from lounging around in my hammock all day. I was also really busy baby proofing the house (again) for the arrival of our latest ankle-biter in April.
But this week I found some time between commuting to work and meetings to finally have a proper poke around with Beam SQL, and put it through its paces. So, what’s it all about it then? Well, a good place to start is the official documentation blurb:
Beam SQL allows a Beam user (currently only available in Beam Java) to query bounded and unbounded PCollections with SQL statements. Your SQL query is translated to a PTransform, an encapsulated segment of a Beam pipeline. You can freely mix SQL PTransforms and other PTransforms in your pipeline.
Let’s break that statement down y’all:
In addition to that blurb, here’s another one that tells us Beam SQL is essentially based on Apache Calcite:
Calcite provides the basic dialect underlying Beam SQL. We have added additional extensions to make it easy to leverage Beam’s unified batch/streaming model and support for complex data types.
So, you’ll need to use Calcite’s dialect of SQL when using it in Beam. Check out the Apache Calcite SQL docs here. However, for all intent and purposes, you can simply write SQL in your pipelines in order to work on your data coming in it.
Got it? Great. Let’s move on.
Now that you know what it’s all about, let’s have a look at the code needed to get a simple pipeline that uses Beam SQL up and running. To test it, I devised a cunning data pipeline:
The first thing we need is some sample data to push through the pipeline. I usually turn to the public datasets in BigQuery whenever I need some dummy data, or if I want to test something. One of the entries in the public datasets is the Wikipedia benchmark page view tables.
These tables are great tables for doing quick tests and demos on GCP. They are simple to understand and easily relatable to the audience. Also, the size of the tables range from a 1K rows, all the way up to 100B rows (6TB). Wowsers. So, you can easily scale and test your workloads with absolutely no changes needed.
To start with, we simply export the smallest of the tables (1K rows) to GCS as a CSV file from the BigQuery console. Of course, we could just read the table directly from BigQuery in the pipeline (using BigQueryIO), but I can’t think of a use case that would require using Beam/Dataflow to pull data from BigQuery in order to run SQL over it. You’d be much better off just querying it directly in BigQuery! 😀
The first part of the code constructs the pipeline and creates the read from GCS into a PCollection<String>. Simples:
To work with Beam SQL, you must convert the type of your PCollection to a Row. Don’t confuse this with a BigQuery TableRow object though. They are different things altogether. So, the next thing we nee do is to run a ParDo over the PCollection<String>, which we obtained from the initial file read from GCS, and turn it each element into a Row object and then finally set the values for each element:
Now that we have a PCollection<Row> object, we can finally run some sweet, sweet SQL over the contents of that collection. Who doesn’t love a bit of SQL, huh?
Remember, that the elements will be distributed across all the workers in the Dataflow cluster. But, we don’t need to care about that because it’s all abstracted away. Using SQL also means we don’t need write boilerplate’y GroupByKeys, Combines, etc. in Java. Using SQL is much easier to express what we’d like to do with the data:
In this example, I’m simply aggregating by language and summing the views for each one. As simple as this sounds, under the hood a lot of work needs to be done to make this happen on a distributed cluster. And it’s something I’m not smart enough to understand. But once again, the complexity of this is hidden from users like us. You don’t need to worry about it — until you have a bug and you need to debug that is! Mwaaaah!
We’ve now got the bulk of the pipeline written. The last thing we need to do is collect the results of the SQL statement, and store them somewhere. This could be anywhere really (e.g. BigQuery, Datastore, Memorystore etc.), but for brevity’s sake I simply writes the results back as a CSV file to GCS.
To do this we need to convert the SQL Row object back to String so we can write it out to GCS. Just hitting it with a simple ParDo makes this a walk in the park. Then one more apply to write the result file out (without sharding so we get just one file) to GCS.
And you’re done folks!
Putting it all together, here’s the pipeline in all its Java glory:
The final piece of the puzzle is actually running the pipeline on GCP. Now, you could run it locally using the DirectRunner, but where’s the fun in that? Actually, snark aside, executing it locally is a great way to develop and debug your Beam/Dataflow pipelines. Don’t be like me and run your pipelines on GCP just for fun — your boss won’t like you for very much longer when they get the bill.
Anyway, after kicking off the pipeline with the DataflowRunner this is what happens:
Drilling down into the transform_sql step, it’s clear to see the steps that the SQL generated for us in the pipeline. Remember, I didn’t need to code these steps. I just wrote some simple SQL to express my intent:
With the pipeline run, and the output written to GCS, it’s time to validate the results of the magical SQL code. Once I downloaded the output.csv file from the GCS bucket, I dutifully hit it with some wonderfully dodgy Bash. Yes, yes, I know I could have federated out to the file in GCS from BigQuery and, wrote some SQL to check the results. But, doing it with Bash makes me look like I know how this computering thing works.
Firstly, let’s check the 10 most popular languages line up between what the pipeline produced, and what BigQuery spits out. The left terminal is the Beam SQL results, and the right terminal in hitting the original table in BigQuery:
That looks good. Now, let’s make sure Beam SQL calculated the corrected number of aggregations for the languages field i.e. distinct values:
Finally, let’s double check the total sum of views to make sure it’s 100% correct:
Everything lines up with the original data that I exported from BigQuery into GCS.
I like it when things work.
As awesome as all that is however, 1K rows isn’t really that impressive, now is it. Instead, let’s run the exact same pipeline on 1 billion rows (~100GB) and let the Dataflow cluster scale up to 10 workers. That’s much more fun!
The pipeline took about 25 minutes to process 1B rows. That’s not bad at all. Now, we need to perform some checks on the results just like before:
Whoopsie! The calculation for the en value was much higher in BigQuery than it was in the result of my pipeline, whereas the other values were just fine. Luckily, it didn’t take me that long to figure out the problem (thanks again Stack Overflow). Looking at the number — this time with commas — it’s much easier to see why it all went pear-shaped:
4,990,236,853
Remember our pipeline was written in Java. Yes, JAVA! Got it yet? That’s right. I had bootstrapped my pipeline by copying the example from the Beam documentation, and that sample code calls addInt32Field(). Then, I had unwittingly used Integer.valueOf() when setting the value.
An Integer in Java is 2³¹-1, so the max value is 2,147,483,647. As such, because the sum for the en value was so large, the JVM was wrapping back around to -2,147,483,648 when it topped out at the max value. Ahh, gotta love Java!
Alas, the fix was simply to use addInt64Field() and instantiate aLong (2⁶³-1) object to store the values in Java instead of an Integer. This gives you just a tad more head room to play with as a Long has a max value of 9,223,372,036,854,775,807.
So, a quick fix to the code and it was time to run the pipeline again.
This time the results looked much better, and the value for en is appropriately sized number. Wonderful.
We just took a look at Beam SQL and how it works. It was relatively easy to get going, and put it through its paces. That said however, this was a very simple example using basic aggregation and summing. It would be interesting to see how it does joins and more complex operations. Finally, I only tested it using a batch workflow. I’d be curious to see how it works with a streaming source e.g. PubSub or Kafka.
It’s also import to note that it’s a very new feature and still maturing. For that reason, I ‘d be wary of using it in production workloads just yet — unless you thoroughly test it for your use case(s). Also, being so new, there’s very little material on it aside from the Beam documentation itself.
However, this is certainly something to keep on eye on if you’re using Beam/Dataflow as part of your data analytics workloads. It may help solve a lot of problems and prove useful to many developers and data engineers. It’s got a lot of potential, but most importantly, the KSQL folks can’t say Beam is missing this functionality anymore. 😀
Cause trouble on that cloud thing that everyone is talking about. I like BigQuery. Work @weareservian and tweet nonsense @polleyg. Moving blog to polleyg.dev
364 
7
364 
364 
7
At Servian, we design, deliver and manage innovative data & analytics, digital, customer engagement and cloud solutions that help you sustain competitive advantage.
"
https://medium.com/@adrianco/cloud-trends-where-have-we-come-from-and-where-are-we-headed-3d7e5e756d16?source=search_post---------221,"Sign in
There are currently no responses for this story.
Be the first to respond.
Top highlight
adrian cockcroft
Nov 23, 2016·4 min read
I gave several talks over the last few years on this subject, mostly at the Structure conference. Now that I’ve joined @AWScloud my perspective shifts somewhat, but the same trends are playing out.
The cloud ecosystem continues to mature, and customers are looking for staying power, as small vendors and small clouds from some big vendors fade away and close down. The ability to support and migrate enterprise workloads is critical for winning the biggest new cloud deals. Extremely scalable cloud capacity is critical to provide room for the largest web scale customers to keep growing. Adding new regions around the world is critical to provide local jurisdiction support and low network latency.
In 2014 we saw many enterprises sign up for AWS, start proof of concept tests and launch green-field applications. In 2015 larger scale migrations started, and plans were made for entire datacenters to be replaced by public cloud accounts. In 2016 these changes moved from early adopter markets such as media and retail, and started to take root in finance, as banks, insurance companies and their regulators figured out how to run and audit public cloud applications. Next up: early adopters in the energy, transport, government, manufacturing and healthcare markets are leading the way to cloud.
What do modern applications look like? We are seeing the combination of rapid cloud based provisioning, a DevOps culture transformation, and the journey from waterfall through agile to continuous delivery product development processes give rise to a new application architecture pattern called microservices. This shares the same principles as the service oriented architecture movement from 10–15 years ago, but in those days, machines and networks were far slower, and XML/SOAP messaging standards were inefficient. The high latency and low messaging rates meant that applications ended up composed of relatively few large complex services. With much faster hardware and more efficient messaging formats, we have low latency and high messaging rates. This makes it practical to compose applications of many simple single function microservices, independently developed and continuously deployed by cloud native automation.
What does modern hardware look like? The capacity of individual physical systems continues to increase, and is far larger than most applications require. As I write this the largest single instance type on AWS is the x1.32xlarge with almost two Terabytes of RAM, four Terabytes of solid state disk, 128 vCPUs, and a 20GBit network interface. The most powerful p2.16xlarge instance type has raw performance of 70 Teraflops from about 40,000 GPU cores. Systems are sliced up using virtual machines to make appropriately sized instances that can be provisioned and boot an operating system in a few minutes.
A few minutes to get an instance used to feel amazingly fast, but now we use containers to get applications running in a few seconds, and to pack more small applications efficiently into large instances. The early cloud native architectures such as @NetflixOSS used an instance to host each microservice, but many people are now moving to use more lightweight Docker containers to host each microservice.
We’ve shrunk the size of microservices so that they each perform a single function, one of many that make up an application. There are plenty of situations where individual microservices sit idle most of the time, but need to be ready to respond quickly when something happens, and potentially scale up to handle a lot of requests in a burst. It’s inefficient to have lots of idle containers, or to provision them when a request arrives. To meet this need AWS Lambda was launched two years ago, and has helped to create a serverless or function as a service (FaaS) programming model that is emerging as a new pattern for cloud application development. By optimizing for rapid launch of a single shot invocation, there is no need to charge when the function isn’t running. By starting exactly as many functions as are needed to process incoming events there is no need to provision extra headroom or do capacity planning. For appropriate workloads AWS Lambda is simpler to operate and a small fraction of the cost of a set of permanently running microservices.
As we look forward into 2017, there is growing interest in serverless architectures and an ecosystem is developing around tooling to build, monitor and operate serverless applications. However AWS Lambda is more than just an application architecture, it has it’s roots in S3, where it provides methods that are triggered by actions on the object store. Lambda functions can be attached to an increasing number of AWS services, and there are exciting possibilities for event driven automation of cloud infrastructure and services.
As the rate of change in technology increases there are the usual worries about skills shortages. Part of the answer is the democratization of access to technology. Some of the most advanced technology available in areas like big data, machine learning and cloud exists now as a combination of web services and open source software packages. Just a few years ago, to have access to leading edge technology you would have had to be at a top university or industry research lab and have a large budget and skilled staff to operate the systems. Today, services and infrastructure are available by the hour, software can be downloaded for free, and there is a huge amount of online information to help you learn and keep up to date. One challenge I’m particularly interested in is how to find undeveloped talent, and connect it with the right opportunities to learn. A project manager, left behind by the journey from waterfall based projects to continuous delivery of products, can discover and develop latent talents as a data scientist. Schoolgirls can build mobile applications and could even leverage machine learning with a serverless cloud back-end. Unemployed workers can retrain on the latest technologies, and build up a reputation by entering coding contests and contributing to open source projects, as a pathway to new opportunities.
I will be attending AWS re:Invent, and I’m presenting at 3:30pm on Thursday in the architecture track - ARC213 Open Source at AWS — Contributions, Support and Engagement. More on that topic in future posts.
Paraphrasing William Gibson “The future is already here — and now it’s globally distributed.”
Work: Amazon Sustainability (ex AWS, Battery Ventures, Netflix, eBay, Sun Microsystems, CCL)
173 
7
173 claps
173 
7
Work: Amazon Sustainability (ex AWS, Battery Ventures, Netflix, eBay, Sun Microsystems, CCL)
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/free-code-camp/heres-how-to-make-your-cloud-infrastructure-stable-secure-and-scalable-f9f4749697d6?source=search_post---------222,"There are currently no responses for this story.
Be the first to respond.
Top highlight
There are a lot of things to worry about as a startup. Marketing, product development, keeping your team together. Everything tends to take the “Minimum viable” pattern of getting the bare minimum up so you don’t crash and burn.
As an enterprise cloud architect, I know first hand how much work can be done in the field of DevOps. As a startup founder, I also know how little time you have to spend on things — it’s more like you have to spend time on all the things at once.
Cloud Infrastructure unfortunately also tends to follow this rule, and all the “best practices” in the field tend to follow patterns that require a large amount of time investment, something startups definitely don’t have.
With this guide, I hope to give you an overview of what a “minimum viable cloud infrastructure” can look like, with a focus on stability, security, and scalability.
When looking at the stability of your cloud infrastructure, there are a few key points to focus on when developing minimum viable cloud infrastructure. Restoring from catastrophic failure, automatic restart, and making sure there are enough resources available. If you focus on these three things, you should be in a pretty good place in terms of your uptime.
You know the worse case scenario — you bricked your server and disk. The minimum viable solution to this is to have scheduled, automated backups taken so you prevent data loss.
Depending on your cloud provider, there are a few different options you can take. Snapshotting disks is generally the simplest way to do a minimum viable backup process, but more advanced (and more stable) methods include database specific backups (dumping the database) and distributed systems.
There are two parts to automatic restarting. One, when your app crashes, does it start up again? And two, when your server reboots, does your app start up automatically?
Crontab —Crontab is a useful tool that lets you schedule jobs easily. Perhaps the simplest approach to auto-start your stack is to create a crontab job that gets run on reboot — See this guide on how to do that.
/etc/init.d — Most systems support init.d scripts. With init.d you can define scripts which can be started at boot and also support stop, start, and status commands (eg. service start myscript) to give you more control over your applications. It’s a bit more complex than a crontab, but it gives you more features — See this post to set up an init.d script.
If you are interested in the differences between these methods, check out This stack exchange post.
Applications are not always stable and can be prone to crash at awkward times. A good way to maintain stability is to have a tool which can automatically restart.
One of the most common reasons for server downtime is servers running out of resources. I’ve had SQL servers die from running out of disk space and production applications die from running out of memory. Setting up monitoring of resources is a good way to mitigate this risk.
Security is unfortunately overlooked when it comes to MVP philosophy. People just don’t see the value gained for the time investment needed. This is a form of dangerous gambling, as a security breech could cause severe loss of data, customer trust, and time. Here are some basic things you can do to get started with a security mindset.
Nowadays, SSL is basically a requirement for a modern SaaS app with many users refusing to use applications without https support. Tools like Let’s Encrypt make getting certificates easy and free.
One of the most important things when it comes to security is managing servers properly. Here are a couple basic tips you should be keeping in mind.
API keys, credentials, configurations, and all sensitive data needs to be managed. I’m always hesitant when placing this kind of data on the cloud, not only because I don’t know what the cloud provider can look at, but also because if they get my account, all my secrets become exposed.
If you have the time, the will, and the skills (or money), putting some effort into scalability could give you future benefits. If not, I’d recommend ignoring it and focusing on the previous two points.
Focus on delivering your product to your first 5 customers, not your first 1,000. The best you can do when it comes to building scalable infrastructure is think about design principals while building your app so it wont be too much work to get going when it’s finally time to scale. I should know — I’ve fallen for the over-engineering trap many, many times.
An easy win when it comes to scaling is to containerize your application. Check out Docker for a good guide. Here are some tips:
Store everything in version control: configurations, scripts, and procedures to prepare servers. This will save you when it comes to scaling. I’ve had to deal with scaling apps that require servers configured in a very particular way, and if the documentation is lacking you will be in for a hell of a time.
There is a lot of work involved with standing up and maintaining cloud infrastructure. Startups have it hardest because they have no time, and often, their skillset is lacking when it comes to DevOps. What you can do is focus on the essentials. Security, Stability, and if you have the time, Scalability.
We’ve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
830 
830 claps
830 
Written by
Founder of https://servicebot.io and Consultant for VMware
We’ve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
Written by
Founder of https://servicebot.io and Consultant for VMware
We’ve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@duhroach/improving-cloud-function-cold-start-time-2eb6f5700f6?source=search_post---------223,"Sign in
There are currently no responses for this story.
Be the first to respond.
Colt McAnlis
Mar 18, 2018·5 min read
Any time you are working with responsive cloud computing systems, cold-boot performance is something you’re going to have to consider. As we saw with Compute Engine, Container Engine, and App Engine, any time a user response comes into a cloud resource, it runs the risk of needing to do a boot-from-scratch, or cold-boot. This is problematic, since, in these scenarios, the response time is increased.
That’s why, when CAPITAL FUN approached me, asking about how to improve their cold-boot performance for Cloud Functions, I wasn’t surprised at all.
Functions are considered a “serverless” architecture, but nonetheless can still be at the mercy of cold-start times. The best way to figure out when you’re going to have to pay the overhead of a cold boot is to consider that you have one of two situations you face when calling your serverless function:
The “Hot” scenario your function has run recently, and the same machine instance on which it last ran is available to run it again. This results in the shortest round-trip time.
The “Cold” scenario the next call to your function will result in a new instance being instantiated to run your serverless function. This can happen for a handful of reasons:
In general, “Hot” functions will already exist, and be ready to handle your requests as fast as possible. “Cold” functions on the other hand, will have a longer response time due to the need to provision function resources prior to executing the code.
In order to get a baseline for the difference between these two, I created a new completely blank GCF, deployed it, and pinged it a few times. In the image below, you can see the red area is the “Cold” boot time for the function (12ms), while the blue box is the general response time for the “Hot” scenario (4ms).
Hands-down, the #1 contributor to GCF cold-boot performance is dependencies. When a module boots, node.js resolves all require() calls from the entry point using synchronous I/O, which can take a significant amount of time if the number of dependencies are large, or if the content itself requires a lot of linking.
So when I saw that CAPITAL FUN’s GCF module had 50 Dependencies (!!!!) I knew right where the problem was, and our profiling proved my point:
Getting this back down to something manageable took a few steps, and a lot of negotiation
As a first step, we worked with CAPITAL FUN to trim out dependencies. Including a module may involve a subset of other modules. Its’ critically important to understand what sub-linking is occurring, as it can vary the startup performance of your function significantly. After a lot of negotiation, digging, and spaghetti code cleaning, we were able to get down to 8 packages, including google-cloud. This resulted in ~ 2x improvement in cold-boot performance.
One of the influencers of dependency resolution is weather or not the dependency exists in the cache or not. The dependency cache is shared across all GCF dependencies, as such, the most popular versions of a module can be reused across a lot of users and deployments.
Looking at CAPITAL FUN’s dependencies, their version numbers were all over the place. In some cases, their system was changing the version number between deployments, depending on various code and staging factors.
To address this, we unified the production code to use the most popular module versions requested by external users, since, these versions are expected to be present in the dependency cache, making deployments faster.
Further digging through the Node.js code for CAPITAL FUN, we realized that not all the dependencies needed to be loaded at the boot-phase of the function. Although it’s a non-standard practice, we’ve seen before how putting requires within the function body, and pulling them out only when necessary can help the performance of cold-boot situations.
This allowed us to require only what is needed at start, and then switch to an async version of require later on for specific request handling, bringing down the cold boot times even further.
Working with CAPITAL FUN, we were able to reduce the number of dependencies, optimize the version numbers to increase dependency caching, and then lazy load certain modules. The result? Going from 20sec cold-boot time to 1.83sec, which is amazing considering their warm-response time is around 800ms or so.
DA @ Google; http://goo.gl/bbPefY | http://goo.gl/xZ4fE7 | https://goo.gl/RGsQlF | http://goo.gl/4ZJkY1 | http://goo.gl/qR5WI1
875 
11
875 
875 
11
DA @ Google; http://goo.gl/bbPefY | http://goo.gl/xZ4fE7 | https://goo.gl/RGsQlF | http://goo.gl/4ZJkY1 | http://goo.gl/qR5WI1
"
https://medium.com/swlh/create-a-private-cloud-email-file-storage-website-and-password-manager-9c02a2ce7777?source=search_post---------224,"There are currently no responses for this story.
Be the first to respond.
"
https://medium.com/javarevisited/top-10-courses-to-learn-amazon-web-services-aws-cloud-in-2020-best-and-free-317f10d7c21d?source=search_post---------225,"There are currently no responses for this story.
Be the first to respond.
Hello guys, if you want to learn Amazon Web Services, popularly known as AWS, and looking for some excellent resources like books, courses, and tutorials then you have come to the right place.
In this article, I am going to share some of the best Amazon Web Services or AWS courses which will help you to learn this revolutionary and valuable technology free of cost.
Unlike many other free courses you will find on the internet, these are genuine free AWS courses which are made free by their authors and instructors for the promotional and educational purpose.
You just need to enroll with them and then you can learn AWS at any time, at any place, and on your own schedule.
But, if you are completely new to the AWS domain or Cloud let me give you a brief overview of Amazon Web Services and its benefits over traditional infrastructure setup.
The AWS is nothing but a cloud service provider by Amazon. It’s a revolutionary change because it allows you to develop an application without worrying about hardware, network, database and other physical infrastructure you need to run your application.
For example, if you want to develop an online application for your business, you need a lot of servers, databases, and other infrastructure.
You need to rent a data center, buy servers, routers, databases, and other stuff to get the start, which is pain and pose a big hurdle for many entrepreneurs. AWS solves that problem by renting their infrastructure and servers with a nominal cost of what you incur in setting up yourself.
Amazon has built many data center around the world to support its core business i.e. the E-Commerce business which powers Amazon.com and AWS just emerge from that.
AWS allows Amazon to monetize its massive infrastructure by renting out to people and business who needs that.
It created the phenomenon of Infrastructure as Service because now you only need to pay for infrastructure you are actually using.
For example, if you set up your own data center and buy 10 servers and 10 databases but end up using only 5 of them then remaining are waste of money and they also cost in terms of maintenance. With Amazon Web Service, you can quickly get rid of them.
Similarly, you can scale pretty quickly if you are hosting your application on the cloud-like on Amazon Web Service.
If you see that your traffic is increasing then you can quickly order new servers and boom your new infrastructure is ready in hours unlike days and months with the traditional approach.
You also don’t need to hire UNIX admins, Database Administrator, Network admins, Storage guys, etc, All that is done by Amazon and because Amazon is doing it in a scale, it can offer the same service at a much lower cost.
In short, Amazon Web Service gives birth to the concept of Cloud which allows you to bring your business online without worrying about hardware and infrastructure which powers them.
Now that we know what is AWS and what are the benefits it offers in terms of Infrastructure as service, it’s time to learn different Amazon service in-depth and that’s where these courses will help you.
You can join these courses if you want to learn about AWS and Cloud in general or you are preparing for various AWS certifications like AWS Solutions Architect, AWS SysOps Admin, or AWS Developer (associate). These courses will help you to kick-start your journey to the beautiful world of AWS.
Though, if you are preparing for the AWS Solution Architect exam, I highly recommend you to join the AWS Certified Solutions Architect — Associate 2021 course on Udemy by Cloud Guru Ryan Kroonenburg, one of the best course on AWS. It’s not free but it’s completely worthy of your money.
Anyway, here is the list of free online courses to learn AWS or Amazon Web Service.
This is one of the best courses to learn Amazon Web Service and it’s FREE. It follows the principle of learning by example and real-world scenarios and that reflects in their course.
This is a short course, having just 2 hours worth of material but it’s power-packed and intense. There is no-nonsense talk or flipping, the instructor Dhruv Bias always means business.
Even if you check the preview chapter you will learn a lot about what is AWS and what problem it solves, what benefits it offers and why should you learn it.
Here is the link to join this course — Amazon Web Services — Learning and Implementing AWS Solution
The course is divided into 5 sections, in the first section, you will get an Introduction of AWS and Overview of the course while remaining section focus on different Amazon Web Service offering like Amazon S3(Simple Storage Service), Amazon AWS EC2 (Elastic Cloud Compute) and Databases like AWS DynamoDB or RDB.
Overall a great free course to learn what is AWS and its different services. I highly recommend this course to any programmer who wants to learn about Cloud and Amazon Web Service (AWS).
Btw, if you are preparing for the Amazon Web Service Solution Architect exam (code SAA- C01) I highly recommend Cloud Guru Ryan’s AWS certification course as well.
This is another awesome free course to learn Amazon Web Service on Udemy. It’s from LinuxAcademy and taught by instructor Thomas Haslet.
The series is actually divided into 2 courses: AWS Concepts and AWS Essentials.
This is the first part while the next course, which is also free is the second part of this series. In this course, you will learn the concepts of Cloud Computing and Amazon Web Service from instructor Thomas Haslet who is also a certified AWS developer.
He holds all three AWS certification for the associate level like
This course is for the absolute beginner, someone who has never heard about Cloud or AWS but understands what is hardware, server, database and why you need them. In this course, you will not only learn essential concepts but also build your vocabulary.
You will find answers to all of your basic AWS question e.g. what is Cloud? What is AWS? What is AWS Core Services? What is the benefit of AWS? Why should you use it? in this course.
In short, a perfect course if you are new to the cloud. You will learn about VPC, EC2, S3, RDS and other Cloud terminology in simple language, and if you are preparing for AWS certifications like Solution architect associate level one, I also recommend to check Whizlab’s AWS Solution Architect Practice Test + Course along with Ryan’s course, both are a very valuable resource to pass this prestigious exam.
This is the second part of the free AWS courses by LinuxAcademy on Udemy. If you have not read the first part, AWS Concepts then you should finish that first before joining this course, though it’s not mandatory.
This course goes into a little bit more details into AWS Core Services then the previous one. It also has a lot of materials with around 50 lectures covering different cloud and AWS concepts.
The course is divided into 14 section and each one covering a key AWS concept e.g. Identity and Access Management (IAM), Virtual Private Cloud (VPC), Simple Storage Service (S3), Elastic Compute Cloud (EC2), Database, Simple Notification Service (SNS), Auto Scaling, Route 53, Serverless Lambdas, etc.
In short, one of the most comprehensive AWS course which is also free. More than 70 thousand students have already enrolled in this course and learning AWS and I also highly recommend this one to anyone interested in Cloud and AWS.
This is another useful and exciting free AWS course you will love to join on Udemy. In this course instructor Mike Chambers, an early adopter of Cloud and AWS explains the basics of Amazon Web Services.
The course is also very hands-on, you will start up signing up to AWS, creating your account, and then using the command-line interface to control AWS.
You will also learn to navigate around the AWS console, build Windows and Linux Servers and create a Wordpress website in 5 minutes which demonstrates how you can leverage Cloud for your database, server, and storage requirement.
The course also teaches you how to build a simple AWS serverless system.
The course not only focuses on AWS technology and terminology but also teaches you basics e.g. the true definition of Cloud Computing and How AWS fits into the Cloud model. You will also get some real pictures to find where is AWS located in the world.
But, most importantly you will gain some hands-on experience in essential AWS services like
In short, one of the best free course to learn Amazon Web Service and Cloud Computing basics.
This is another short but truly hands-on AWS course that will teach you how to perform a common task on the AWS interface. In just 2 hours’ time, you will learn how to launch a Wordpress website based on Amazon EC2 service.
You will also learn how to create a NodeJS based web application, sending an email with AWS SES, uploading a file to AWS S3, the storage solution provided by Amazon, and finally, learn to create and connect to an AWS relational database server.
In short, a great course if you want to use AWS for hosting your application or want to learn how you can leverage Cloud to host your application and most importantly it’s FREE.
This is a free AWS course by Andrew Brown of ExamPro.com. This is a course for early startups and people who want a practical way to apply AWS, instead of going for just AWS Certification!
It covers some AWS Services people normally overlook but are incredibly useful. You will learn how to get started with AWS. This is a practical guide for early-stage startups interested in using AWS. You will learn how to create an AWS account, deploy an application, budget scaling applications, and more!
It's completely free and you can watch this on @Youtube on freeCodeCamp youtube channel.
This is another free course to learn AWS by Andrew Brown to prepare for AWS Certified Solution Architect but you can also use this to learn about AWS.
This Study Course is designed to help you pass the AWS Solutions Architect Associate Certification. It was recorded at the end of 2019 so if you’re in 2021 you can absolutely use this course for study.
All Videos are part of a 275+ Video Youtube Playlist
If you want to start earning some developer certifications, the AWS Certified Cloud Practitioner may be a good starting point for you, and this free course can help you achieve that.
For starters, it’s not super technical. You can earn this certification without knowing how to code. The official certification description recommends you have some basic IT knowledge and “six months of experience with the AWS Cloud in any role, including technical, managerial, sales, purchasing, or financial.”
But if you watch this course and follow along on your own computer, you should be OK. The course will break down most of the important concepts in detail for you.
Again this is completely free and you can watch freely on freeCodeCamp Youtube Channel.
This is an excellent course to learn AWS Fundamentals and Cloud Computing fundamentals with a focus on developing Cloud-Native applications. It will introduce you to AWS core services and infrastructure and the best thing is that its offered by the AWS itself.
It’s a hands-on course with a demonstration and you will how the AWS cloud infrastructure is built, understand Amazon Elastic Compute Cloud (Amazon EC2) and Amazon Lightsail compute services. You will also learn about networking on AWS, including how to set up Amazon Virtual Public Cloud (VPC) and different cloud storage options, including Amazon Elastic Block Storage (EBS), Amazon Simple Storage Service (S3) and Amazon Elastic File Service (EFS).
Later in the course, you’ll learn about AWS Database services, such as Amazon Relational Database Service (RDS) and Amazon DynomoDB. Your instructors will also walk you through how to monitor and scale your application on AWS using Amazon CloudWatch and Amazon EC2 Elastic Load Balancing (ELB) and Auto Scaling.
Lastly, you’ll learn about security on AWS, as well as how to manage costs when using the AWS cloud platform. While on cost, this is a free-to-audit course on Coursera which means it's free for learning but you need to pay if you need a certificate and access to quizzes and assessments.
coursera.com
This is a four-week course that focuses on migrating workloads to AWS. You will focus on analyzing your current environment, planning your migration, AWS services that are commonly used during your migration, and the actual migration steps.
There are also Hands-on labs, though not required for this class. Access to the labs is limited to paid enrolled students. You can audit this course without taking the labs.
If you are new to AWS, we strongly suggest that you take the “AWS Fundamentals: Going Cloud-Native” course available on Coursera to provide an introduction to AWS concepts and services.
coursera.com
And, if you find Coursera courses useful, which they are because they are created by reputed companies and universities around the world, I suggest you join the Coursera Plus, a subscription plan from Coursera which gives you unlimited access to their most popular courses, specialization, professional certificate, and guided projects. It cost around $399/year but its complete worth of your money as you get unlimited certificates.
This is an interactive course to learn about the core AWS’s services like compute, storage, networking services and how they work with other services like Identity, Mobile, Routing, and Security.
This course has been designed by three AWS Solution Certified Architects who have a combined industry experience of 17 years. We aim to provide you with just the right depth of knowledge you need to have.
If you don’t know Educative is another online learning platform that is gaining a lot of traction for its text-based, interactive learning courses. Reading is generally faster than watching and If you prefer reading text than watching videos then this is the platform to checkout.
It has some of the best courses to prepare for coding interviews like Grokking the Coding Interview: Patterns for Coding Questions and Grokking the system design interview.
It also has a lot of free resources like this free JavaScript tutorial to learn essential technologies. You can register for this course for free but if you like to take full advantage of the platform, I suggest you buy its membership which costs $18 (50% discount now) monthly, completely worth it for a programmer and software engineers where continuous learning is required.
www.educative.io
This is one of the best courses to learn practical AWS you will find online. This course is created by a former Amazon engineer with 15 years of experience working on AWS.
This is not your typical AWS reference course. You won’t find most of the knowledge that’s shared here in the AWS docs. The goal here is to help you realize which AWS features you’d be foolish not to use — features that have passed the test of time by being at the backbone of most things on the Internet.
In this course, you’ll learn a technique used to help make reliable technical choices without getting paralyzed in the face of so many options. You’ll start by going through the most fundamental services AWS offers such as DynamoDB, S3, EC2. Each section breaks down how it’s used, the pros and cons, why you should (or shouldn’t) be using it, and more.
Here is the link to sign up for this course — The Good Parts of AWS: Cutting Through the Clutter
You can either buy this course or you can get and Educative membership to access this course. If you ask me, I suggest you get Educative Subscription which costs $17(50% discount now) monthly, completely worth it for a programmer and software engineers where continuous learning is required. Thanks to The Educative Team for this awesome course.
This course is also available as e-Book on Gumroad, if you like to read books then you can also check out The Good Parts of AWS eBook. There is a 20% discount available if you buy the book using this link.
gumroad.com
That’s all about some of the best and free courses to learn Amazon Web Services or AWS. These are absolutely free courses on Udemy but you need to keep that in mind that sometimes instructor converts their free course to paid course once they achieve their promotional target.
This means you should check the price of the course before you join and if possible join early so that you can get the course free. Once you are enrolled in the course it’s free for life and you can learn at any time from anywhere.
I generally join the course immediately even if I am not going to learn AWS currently. This way I can get access to the course and I can start learning once I have some time or priority changes.
Other Free Programming Resources you may like to explore:
Top 5 Courses to Crack AWS Solution Architect Exam5 Free JavaScript Courses for Web Developers5 Free Courses to Learn React JS for JavaScript Developers5 Free Courses to Learn Core Spring, Spring Boot, and Spring MVC5 Free Docker Courses for Java and DevOps Engineer5 Courses to learn Maven And Jenkins for Java Developers3 Books and Courses to Learn RESTful Web Services in Java5 Courses to Learn Blockchain Technology for FREE7 Free Selenium Webdriver courses for Java and C# developers5 Free course to learn Servlet, JSP, and JDBC5 Free TypeScript Courses for Web Developers5 Free Big Data Courses to Learn Hadoop and Spark
Thanks for reading this article so far. If you like these AWS courses then please share with your friends and colleagues. If you have any questions or feedback then please drop a note.
P. S. — If you are preparing for the AWS Solution Architect — Associate Exam (SAA-C01), I strongly recommend you to go through AWS Certified Solutions Architect — Associate 2021 course on Udemy by Cloud Guru Ryan Kroonenburg. It’s not free but it’s completely worth your money.
click.linksynergy.com
Medium’s largest Java publication, followed by 14630+ programmers. Follow to join our community.
528 
4
528 claps
528 
4
A humble place to learn Java and Programming better.
Written by
I am Java programmer, blogger, working on Java, J2EE, UNIX, FIX Protocol. I share Java tips on http://javarevisited.blogspot.com and http://java67.com
A humble place to learn Java and Programming better.
"
https://serifandsemaphore.io/azure-cloud-functions-vs-aws-lambda-caf8a90605dd?source=search_post---------226,"Microsoft announced their answer to AWS Lambda at their Build 2016 conference. Brace yourself for the next cloud hosting change; because, it now seems like everyone is going “serverless.” Amazon Web Service’s Lambda, Google’s Cloud Functions, IBM’s OpenWhisk, and now Azure’s Functions.
There’s too much to compare in one article, so I’ll write about the differences in a series of articles. I’ve had the opportunity to play with some. Google’s Cloud Functions are in private alpha, so I can’t share much about them, but Microsoft’s are in public preview.
I’ve been using Lambda a lot lately and it’s been a game changer.
While Azure’s cloud function usage is billed the same way, the service is radically different than Lambda under the hood.
Before you make any major decisions, keep in mind that these services are evolving.
This will change over time for every cloud function service.
Azure Functions supports Node.js, C#, F#, Python and PHP. They also list Java, bash, and batch but it’s not clear how to use these (again, it’s in preview).
AWS Lambda supports Node.js, Python, and Java for now.
I think PHP support is a very strategic choice given its popularity, but I wish one of these services would better support Go.
Why Go? Because it runs fast and its code is easy to read. It is a language of brevity and therefore fits well for the cloud and (micro)services. Add Rust and Swift to the list too.
It’s important to note that you can execute binaries in both services through one of these languages. This is the method in which Go works in Lambda. Both Apex and also Sparta are serverless frameworks that let you use Go in Lambda as well.
It would be great for a cloud function service to support Go natively. Swift for mobile developers who often need simple back-end services would also be a smart move.
No amount of fancy devops tricks or tooling will save you from having to use the web dashboard at least every now and then.
Azure’s Function usage charts are still being developed, but they will be there and at least on par with Lambda’s.
Azure’s new dashboard is beautiful.
Once you understand the organizational structure of Azure Function it becomes a little easier. The same can be said for AWS Lambda too.
One important difference is the editor. Both are on par when it comes to the function’s settings screen. However, Azure has the more robust Visual Studio Online which can also be used.
Visual Studio Online is found under the Function’s App Service’s tools section. It’s worth noting that there are some other cool tools here too, including a console.
Most of your development time should be outside of dashboards anyway. I don’t consider them a huge factor in my decision making process but they can offer convenience.
Let’s dive into the architecture of Azure Functions compared to Lambda.
An “App Service” is a container, or environment, for a set of Azure Functions. This is much different than Lambda. In fact, the two services couldn’t be more different.
Lambdas are organizationally independent, where Azure Functions are grouped logically into an “application.”
This App Service can either by “dynamic” or “classic.” The dynamic option is where you pay only for the time and memory your functions use. This is the biggest similarity between Lambda and Azure Functions.
It’s important to note that the memory you allocate is per app service (with potentially many functions), not per function like Lambda.
I think of the “classic” App Service more like Amazon ECS where you have EC2 instances running to handle the tasks or functions. The pricing model for this is then like EC2.
Azure is actually more like a blend between ECS Tasks and Lambda. For example, you can set environment variables on App Services which are then available for your Azure Functions. AWS Lambda cannot persist environment variables, but ECS Tasks can (correction/update: Lambda now can as pointed out by Jeremy Axmacher, thank you).
The entire container architecture is different. Lambdas provision a brand new one and deploys your code (from a zip file) on a cold request. Subsequent requests can be subject to container re-use and be handled much faster. However, you need to understand there is no persistence and with Node.js Lambdas you need to watch your variable scope because the container can be re-used.
However, Azure Functions are less subject to the cold/warm request effects. Azure still provisions resources as needed, but your files aren’t “frozen” somewhere in a zip file. They run on top of Azure’s WebJobs.
Azure Functions are also supposed to be accessible via FTP by connecting to the App Service’s FTP. Though I couldn’t get it to work so far during my review (no matter how many times I reset my credentials).
Azure Functions of course is a Windows system (32bit for dynamic services) while Lambda is a Linux system. Though I do hope that changes since Ubuntu can now run on Windows. In fact, that would put Azure in a good position with cloud hosting.
Speaking of working with the files, you can deploy your Azure Functions in a variety of ways:
You have many more inherent options than AWS Lambda, but I didn’t explore any sort of CLI deployment. Lambda has some great 3rd party tools for deploying Lambdas and both cloud services have SDKs to make just about anything possible.
I do have to hand it to Microsoft here though. You can hook up GitHub, even with your favorite CI tool, and easily deploy your cloud functions to Azure. Release managers rejoice.
Though it’s important to note that Lambdas can be versioned whereas Azure Functions can not (at least not yet). So if you wanted to rollback, you’ll need to rely on Git or some other version control system.
One challenge with Lambda is development workflow. People have created tools to test your Lambda locally so you can write code, run, and then even deploy if all looks good.
There are also several “serverless frameworks” that help with this process too.
Regardless, Lambda is closed source. However, Azure Function’s runtime is open-source. I would expect some clever CI integrations in the future.
I also have to concede that Azure Functions make things a bit easier in this department too. Every function automatically maps to an HTTP endpoint if enabled. Whereas with Lambda, you must configure API Gateway separately.
API Gateway is fine, but complex and time consuming. Again, some serverless frameworks ease this pain point by automatically setting up an API for Lambdas.
Microsoft gets points for UX because you have a lot less to configure. I’m not sure you can run Azure Functions from multiple triggers though. It doesn’t appear that way, but maybe by editing the function.json directly you can.
I would say that AWS Lambda has a slight edge in flexibility here, but both services are constantly changing.
The Azure app and function names are reflected in the URLs to trigger the cloud functions. This helps avoid confusion, but might also be limiting.
Apparently all HTTP methods are supported (GET, POST, etc.) through the endpoint assigned to each cloud function. With API Gateway, you need to configure each method manually.
One thing I don’t see is path variables. It seems that everything must be passed as a querystring or in the request body.
To be fair, it’s called an “HTTP trigger” so when thinking about it in those terms, it makes sense. If you need more robust settings, then you’re talking about “API configuration and management.”
Azure Functions also have super easy authentication. You can protect the cloud function endpoints with 3rd party authentication; this includes Facebook, Twitter, Google, and of course Microsoft’s auth.
I’ve never setup Facebook auth for an HTTP endpoint so quickly and easily before in my life.
You can configure CORS and you can also manage your own domain name for the endpoints. Though I couldn’t see if there was a way to limit the HTTP request methods. I imagine with Azure’s API management service more will be possible, but then you’re going through as much work as API Gateway.
I don’t think there’s a clear “winner” here. Both Microsoft and Amazon have powerful features for triggering cloud functions with HTTP requests. Though I definitely think Amazon could learn a thing or two about UX from Microsoft.
I was thoroughly impressed by Azure and I think it’s in part due to its new dashboard. The new dashboard and the process of setting up cloud functions in a logical group called an “app” makes complete sense.
To illustrate how obvious this pain point is, look no further than Serverless (formerly JAWS) and Apex. These are two serverless frameworks that address this specific organizational concern. They help a developer logically group functions.
Another pain point with Lambda is the maximum execution time limit. It wasn’t always clear and many people never thought their functions would run that long (at least in my experience).
The time limit used to be 1 minute for Lambda, but increased to 5 to help with “ETL” operations according to Amazon. I’m happy for the time limit increase, but even 5 minutes may not be sufficient for all operations.
Iron.io pointed out that their workers had no time limits in a comparison with Lambda. Iron workers are another option, though I stayed the course with AWS Lambda. I even found a way to re-purpose your Lambda code as ECS Tasks to cope with the 5 minute limit.
While that solved my own problems with the time limit, I understand that it’s not a solution for everyone. It also requires EC2 instances which are billed in a different fashion. So it’s not a perfect solution if “pay as you go” is a top priority.
Azure’s Functions really are the perfect blend of Lambdas and ECS Tasks. You can write and execute the same exact code regardless of the environment and billing model. There are apparently no time limits either(so long as your function is not idle).
On the other hand, I imagine there could be some surprises in billing too. Lambda’s 5 minute execution time limit does serve to protect you from expense. Will Microsoft keep billing you for some accidental loop in a cloud function? Assuming the same amount of resources were used, will the cost work out to be about the same regardless of billing models? It’s perhaps too early to say since it’s still in preview and there is no pricing information available yet.
There is some chatter about this concern though if you follow the GitHub issue.
As I explored Azure Functions and it’s underlying architecture (as best I could), I realized a few other things of note.
First, since there is persistence and no execution time limits, working with large files and ETL is easier. Just watch out for how cost effective this ends up being or not being.
With Lambda + API Gateway, the content-type of the response is set via API Gateway. With Azure Functions, the content-type is set by the code. This can be very useful, but could also lead to some bloated functions.
Remember, your goal is not to build a complete application within a cloud function. It’s not really possible to do with Lambda’s limitations, but it is with Azure. So be careful not to fall into any bad practices.
Watch out for Azure Function’s limitation — only 10 concurrent executions per function.
Again, Azure Functions run on a server with a persistent filesystem and webroot. It’s just not accessible to the outside world. However, it is accessible to you while logged in.
All of your code and assets can be seen via a URL like this: https://yourAppName.scm.azurewebsites.net/dev/api/files/wwwroot/HttpTriggerNodeJS1/index.js
This maps this location on disk: /home/site/wwwroot/HttpTriggerNodeJS1
Is there a cost to request these files this way? Can you somehow open this to the public? I don’t know. It definitely might raise an eye-brow or two though.
You have more insight, access to and control over the environment running your cloud function than you do with AWS Lambda…For better or worse.
Speaking of insight, when listing the directory of D:\home\site\wwwroot where your files are, it indicates that there is over 5 terabytes of disk space available!
This is much more than Lambda’s default ephemeral disk space of 500MB. There isn’t as much info about Azure’s Cloud Functions limitations just yet, but it is important to note that the instance limit is 10 while using “dynamic” service apps (up from 4). The concurrent execution limit per instance is a bit unknown. Lambda’s limit is 100 concurrent executions total (which is a soft limit).
Is everyone’s code on the same storage volume? Or does each Azure account gets its own, very large, storage volume? I’m not familiar enough with what’s going on to know, but exploring is fun.
I would encourage everyone to explore and try each of these services. Understand why and when they they can help you. The end result can be a game changer for you too.
The cloud has powerful magic, but not all clouds are the same. Get to know the environment your code lives in as well as the capabilities of the services available to you. Read the fine print. Understand the limitations and tradeoffs.
Only with this information can you make an informed decision. Remember, your choice will be the path less taken here.
My choice? The jury is still out. I use a variety of services because I believe there is strength and opportunity in diversity.
I primarily use Lambda and ECS because it’s well established. I love AWS and will continue to use it happily, but I definitely think these companies can learn a thing or two from each other.
There’s a lot of opportunity in this fast changing landscape. There aren’t many books. Few best practices. Barely any frameworks. Less than ideal monitoring and debugging. There will be trial and error. You are subject to the possibility of having to re-build code and move your operations. If it gives you any solace, you will be in good company. Some major companies and smart developers are on board with cloud services and the (micro)service movement. This is the new frontier on the web. Buy the ticket, take the ride.
The best-laid plans of unicorns and men…
357 
8
357 claps
357 
8
Written by
Product Person
The best-laid plans of unicorns and men…
Written by
Product Person
The best-laid plans of unicorns and men…
"
https://lugassy.net/goodbye-google-cloud-hello-digital-ocean-a4ca1c8b7ac8?source=search_post---------227,"Launching cloud instances should be fun. Like invoicing customers.
GCP used to be fun too. I’ve actually written how fun it was just a year ago.
But things have changed. GCP become cumbersome and slow to manage. It is still a great provider to trust your business with, has excellent networking, unlimited compute power and of course — BigQuery, but for most of my day-to-day projects — I’ve found a new home.
If you haven’t already — meet Digital Ocean (aff link, gives $10 free).
DO is easy to fall in love with. I call it the hacker’s cloud since it doesn’t get in your way and packs serious power. In no particular order:
Good combination of RAM, CPU and Storage to choose from. SSD by default and price ranges from $5 to $1680 monthly. Really fast boot time.
US, Canada, UK, Germany, Netherlands, Singapore and India. Each zone is clearly labeled as NYC1, SFO2, LON1, etc.
Each instance includes free 1–10TB of combined in/out traffic (which alone would cost $120 to $800 at GCP). Private network is available, unmetered.
One of the fastest DNS providers, apparently. Incredibly easy and free.
Crispy fonts, big buttons and checkboxes along with smooth animations gives you a warm fuzzy feeling. Everything is a click away. I’m in control. Cloud is fun again.
Compare this instance creation wizard, for example:
To this:
Or this firewall console:
To this:
And, am I the only one freaked out by these cryptic policies, service workers and roles which GCP keeps creating automatically?
Digital Ocean is all about simplicity and basic offerings and rounded prices. This is so refreshing because cloud providers are such a Paradox of Choice. They keep asking you:
This is wrong. Cloud should be simple by default and advanced by choice.
It was trying to be AWS. Too complex and feature-rich instead of simple and feature-complete. Specifically, here’s what I would change:
Discussion: https://news.ycombinator.com/item?id=15795793
Thoughts about Startups, Development & Ops by Michael…
565 
6
565 claps
565 
6
Written by
Co-Founder, HashUnited.com. Built 7 other companies, sold 3, worked with dozens more. Writing about startups, dev & ops.
Thoughts about Startups, Development & Ops by Michael Lugassy
Written by
Co-Founder, HashUnited.com. Built 7 other companies, sold 3, worked with dozens more. Writing about startups, dev & ops.
Thoughts about Startups, Development & Ops by Michael Lugassy
"
https://medium.com/google-cloud/the-hidden-costs-of-cloud-ddb702495e93?source=search_post---------228,"There are currently no responses for this story.
Be the first to respond.
At Server Density, we just completed a 9 month project to migrate all our workloads from Softlayer to Google Cloud Platform. This started with a single service using Cloud Bigtable for large scale storage of time series monitoring data and culminated with our entire product now running on GCP.
The move went smoothly and we’re very happy to be on Google Cloud. Although they have fewer products compared to the market leader (AWS) we think that Google Cloud products are better designed, the pricing models are easier to work with and there are regular releases of innovative new features…
"
https://netflixtechblog.com/announcing-zuul-edge-service-in-the-cloud-ab3af5be08ee?source=search_post---------229,"The Netflix streaming application is a complex array of intertwined systems that work together to seamlessly provide our customers a great experience. The Netflix API is the front door to that system, supporting over 1,000 different device types and handing over 50,000 requests per second during peak hours. We are continually evolving by adding new features every day. Our user interface teams, meanwhile, continuously push changes to server-side client adapter scripts to support new features and AB tests. New AWS regions are deployed to and catalogs are added for new countries to support international expansion. To handle all of these changes, as well as other challenges in supporting a complex and high-scale system, a robust edge service that enables rapid development, great flexibility, expansive insights, and resiliency is needed.
Today, we are pleased to introduce Zuul our answer to these challenges and the latest addition to our our open source suite of software Although Zuul is an edge service originally designed to front the Netflix API, it is now being used in a variety of ways by a number of systems throughout Netflix.
At the center of Zuul is a series of filters that are capable of performing a range of actions during the routing of HTTP requests and responses. The following are the key characteristics of a Zuul filter:
Here is an example of a simple filter that delays requests from a malfunctioning device in order to distribute the load on our origin:
Zuul provides a framework to dynamically read, compile, and run these filters. Filters do not communicate with each other directly — instead they share state through a RequestContext which is unique to each request.
Filters are currently written in Groovy, although Zuul supports any JVM-based language. The source code for each filter is written to a specified set of directories on the Zuul server that are periodically polled for changes. Updated filters are read from disk, dynamically compiled into the running server, and are invoked by Zuul for each subsequent request.
There are several standard filter types that correspond to the typical lifecycle of a request:
Alongside the default filter flow, Zuul allows us to create custom filter types and execute them explicitly. For example, Zuul has a STATIC type that generates a response within Zuul instead of forwarding the request to an origin.
There are many ways in which Zuul helps us run the Netflix API and the overall Netflix streaming application. Here is a short list of some of the more common examples, and for some we will go into more detail below:
Zuul gives us a lot of insight into our systems, in part by making use of other Netflix OSS components.
Hystrix is used to wrap calls to our origins, which allows us to shed and prioritize traffic when issues occur.
Ribbon is our client for all outbound requests from Zuul, which provides detailed information into network performance and errors, as well as handles software load balancing for even load distribution.
Turbine aggregates fine-grained metrics in real-time so that we can quickly observe and react to problems.
Archaius handles configuration and gives the ability to dynamically change properties.  Because Zuul can add, change, and compile filters at run-time, system behavior can be quickly altered. We add new routes, assign authorization access rules, and categorize routes all by adding or modifying filters. And when unexpected conditions arise, Zuul has the ability to quickly intercept requests so we can explore, workaround, or fix the problem.  The dynamic filtering capability of Zuul allows us to find and isolate problems that would normally be difficult to locate among our large volume of requests. A filter can be written to route a specific customer or device to a separate API cluster for debugging. This technique was used when a new page from the website needed tuning. Performance problems, as well as unexplained errors were observed. It was difficult to debug the issues because the problems were only happening for a small set of customers. By isolating the traffic to a single instance, patterns and discrepancies in the requests could be seen in real time. Zuul has what we call a “SurgicalDebugFilter”. This is a special “pre” filter that will route a request to an isolated cluster if the patternMatches() criteria is true. Adding this filter to match for the new page allowed us to quickly identify and analyze the problem. Prior to using Zuul, Hadoop was being used to query through billions of logged requests to find the several thousand requests for the new page. We were able to reduce the problem to a search through a relatively small log file on a few servers and observe behavior in real time. The following is an example of the SurgicalDebugFilter that is used to route matched requests to a debug cluster:
In addition to dynamically re-routing requests that match a specified criteria, we have an internal system, built on top of Zuul and Turbine, that allows us to display a real-time streaming log of all matching requests/responses across our entire cluster. This internal system allows us to quickly find patterns of anomalous behavior, or simply observe that some segment of traffic is behaving as expected, (by asking questions such as “how many PS3 API requests are coming from Sao Paolo”)?
Gauging the performance and capacity limits of our systems is important for us to predict our EC2 instance demands, tune our autoscaling policies, and keep track of general performance trends as new features are added. An automated process that uses dynamic Archaius configurations within a Zuul filter steadily increases the traffic routed to a small cluster of origin servers. As the instances receive more traffic, their performance characteristics and capacity are measured. This informs us of how many EC2 instances will be needed to run at peak, whether our autoscaling policies need to be modified, and whether or not a particular build has the required performance characteristics to be pushed to production.
Zuul is central to our multi-region ELB resiliency project called Isthmus. As part of Isthmus, Zuul is used to bridge requests from the west coast cloud region to the east coast to help us have multi-region redundancy in our ELBs for our critical domains. Stay tuned for a tech blog post about our Isthmus initiative.
Today, we are open sourcing Zuul as a few different components:
zuul-core — A library containing a set of core features.
zuul-netflix — An extension library using many Netflix OSS components:
zuul-filters — Filters to work with zuul-core and zuul-netflix libraries
zuul-webapp-simple — A simple example of a web application built on zuul-core including a few basic filters
zuul-netflix-webapp — A web application putting zuul-core, zuul-netflix, and zuul-filters together.
Putting everything together, we are also providing a web application built on zuul-core and zuul-netflix. The application also provides many helpful filters for things such as:
We hope that this project will be useful for your application and will demonstrate the strength of our open source projects when using Zuul as a glue across them, and encourage you to contribute to Zuul to make it even better. Also, if this type of technology is as exciting to you as it is to us, please see current openings on our team: jobs
medium.com
Originally published at techblog.netflix.com on June 12, 2013.
Learn about Netflix’s world class engineering efforts…
773 
1
773 claps
773 
1
Written by
Learn more about how Netflix designs, builds, and operates our systems and engineering organizations
Learn about Netflix’s world class engineering efforts, company culture, product developments and more.
Written by
Learn more about how Netflix designs, builds, and operates our systems and engineering organizations
Learn about Netflix’s world class engineering efforts, company culture, product developments and more.
"
https://medium.com/@manus.can/serverless-platform-comparison-google-cloud-function-vs-aws-lambda-8e060bcc93b4?source=search_post---------230,"Sign in
There are currently no responses for this story.
Be the first to respond.
Can Tepakidareekul
Nov 26, 2018·7 min read
Serverless computing is a new trend in software development, it is used to build your application by deploying application’s functions separately. It reduces lots of steps in designing software architecture and deploying the application.
Although, you know you can take lots of advantages from the serverless computing but there is one more question that you have to answer yourself before starting to use this technology. The question is which provider you should use their service.
Google Cloud has Google Cloud Function and Amazon Web Service has AWS Lambda. So, which one is the best? Let’s find the answer in this article.
In this article, we are going to share about what we did to compare the serverless platform on both providers by comparing functionalities of them which include supported language, ease of deployment, availability, scalability, network performance & stability, CPU throughput, memory management, and pricing.
Programming language is very diverse. Many languages are used in API service development. To compare this functionality, we listed the supported languages on both providers.
We found that there are several supported languages on AWS Lambda while there is only one version of Node.js which fully supported and there are the beta versions of Node.js and Python on Google Cloud Function. So, AWS Lambda is better than Google Cloud Function, if the variety of supported language is concerned.
Ease of deployment is one of the most important functionalities because easy deployment means how fast you can deliver the product to your users.
In this part, we did an experiment by comparing the number of deployment steps which used to deploy a basic function on AWS Lambda and Google Cloud Function from the first step to the last.
From the table above, there are a lot of steps on AWS Lambda while Google Cloud Function has only one step to deploy the function. It is because AWS Lambda provides many options which the users can config their function.
Moreover, we counted the time on both providers use to deploy the functions and update the functions.
Although, Google Cloud Function has fewer steps than AWS Lambda but AWS Lambda can deploy and update the function faster than Google Cloud Function can do.
We did an availability test on both AWS Lambda and Google Cloud Function by keep sending a request to the function every 5 minutes for an entire week. We created a Microsoft Azure Instance to act as a middleman.
We found that there is no downtime in both AWS Lambda and Google Cloud Function. Maybe from there is too much gap in between test iteration, in the further experiment we should reduce the gap down to around every minute.
Scalability is an ability to scale which every cloud feature should have because the main purpose of using the cloud provider is to serve every response to every request from all of the clients.
We did an experiment on this functionality by sending the specific number of request to the functions that have the same purpose at the same time on both providers. Then, we observed the result and limitation that happen on both providers.
We used Apache Bench to send 1000 requests to AWS Lambda and Google Cloud Function at the same time.
We found that there is no failed request from our test on both providers. They can response to all of our requests. So, we can say that AWS Lambda and Google Cloud Function have the efficient scalability and no one wins for this round. It is very interesting that there is no option to config about scalability. It seems like both providers are able to do an auto-scaling without any extra settings needed.
We compared the network performance between AWS Lambda and Google Cloud Function by finding time used until the request is responded.
The average time used of AWS Lambda is 117.16 ms and Google Cloud Function is 176.80 ms. According to the result, AWS Lambda is slightly faster and more stable.
Performance is the most concerned functionality for choosing the serverless provider because no one needs slow computing performance, right?.
CPU Throughput is the number of executions per time unit without network constraints. We did an experiment on this functionality by writing a script with a loop for counting the number of iterations and limiting the execution time to the same list of specific number when running on both providers.
From the result, AWS Lambda is faster than Google Cloud Function. The throughput of AWS Lambda is 1.02 million executions per second and the throughput of Google Cloud Function is around 0.9 million executions per second.
For this functionality, we did an experiment by try to find an actual maximum memory size that we can allocate on AWS Lambda and Google Cloud Function at the same memory capacity.
Before testing memory management, let’s have a look at the memory setting on both providers.
As you can see, memory setting form of AWS Lambda is a slider which provides more options to the users to select their desired size between 128 MB to 3008 MB. On Google Cloud Function, memory setting form is just a dropdown selector which limits the available options to 128 MB, 256 MB, 512 MB, 1 GB, and 2 GB.
However, the input type is not an indicator to judge which one is better on memory management. So, we wrote a script to allocate memory on AWS Lambda and Google Cloud Function with 128 MB memory capacity for both providers, then maximized the number of memory size that they are able to allocate.
The result is we could allocate 6.29145 MB on AWS Lambda and 6.99999 MB on Google Cloud Function. So, Google Cloud Function wins on this round. Its memory management is better than AWS Lambda.
There is a slight difference in pricing between AWS Lambda and Google Cloud Function. All of the information is in the tables below.
Invocation is measured from how many requested is made to the function.
Compute time is measured from the time your function receives a request to the time it complete.
Fees for compute time are variable based on the amount of memory and CPU provisioned for the function. Units used in this calculation are:
GB-Seconds : 1 GB-second is 1 second of wall-clock time with 1GB of memory provisioned
GHz-Seconds :1 GHz-second is 1 second of wall-clock time with a 1GHz CPU provisioned (Google Cloud Function Only)
The total fee = Invocation cost + Compute time costCompute time cost = GB-Seconds + GHz-Seconds
* Note: The price above is an average price based on 128 MB memory but the real price varies by the amount allocated to the function.
To conclude, AWS lambda seems overall better than Google Cloud Function. However, choosing the right server less provider is depend on the objectives of your work. AWS Lambda works properly with the users who want flexibility and more freedom to setting their own functions. While Google Cloud Function works properly with the users who want simplicity in their work and the users who have few experience with this kind of work such as students.
** Note: The test was using Google Cloud Function on “asia-northeast1” region and AWS Lambda on “ap-northeast-1” region which are placed at Tokyo, Japan.
*** Note: This article is a part of the course named “2110498 - Cloud Computing Technologies” taught in Department of Computer Engineering, Chulalongkorn University.
Authors: Arin Trongsantipong & Manussawee Tepakidareekul
medium.com
medium.com
medium.com
a Thai software engineer working in Singapore — www.linkedin.com/in/manus-can
See all (128)
616 
5
616 claps
616 
5
a Thai software engineer working in Singapore — www.linkedin.com/in/manus-can
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/milkie-way/we-burnt-72k-testing-firebase-cloud-run-and-almost-went-bankrupt-part-2-8092a3bb773d?source=search_post---------231,"There are currently no responses for this story.
Be the first to respond.
Top highlight
One Hundred Sixteen Billion: that’s the number of times our test code read Firestore database in few hours.
This post is Part 2 in the series. If you haven’t already, go through Part 1 first so this one makes sense :)
For those who learn better with visuals, here’s a video for you:
Personally, this was the first time that I had received such a big set back. It had the potential to alter the course of our company as well as my life. There were several lessons on entrepreneurship in this incident, one important one was to stay strong.
I had a team of ~7 engineers/interns at this time, and it would take Google about 10 days to get back to us on this incident. In the meantime we had to resume development, find our way around account suspensions. Despite this thought on my mind, we had to focus on the features and our product.
Poem: Khada Himalaya bata raha hai (Standing Himalaya is telling us)
For some reason one poem from my childhood kept playing in my head. It was my favorite one, and I remembered it word for word, even though the last time I recited it was over 15 years ago.
As a very small team, we wanted to stay serverless for as long as we could. The problem with serverless solutions like Cloud Functions and Cloud Run is timeout.
One instance at any time would be serially scraping the URLs in a web page. But soon after 9 minutes, it would time out.
After discussing this problem, and powered with caffeine, within minutes I wrote some dry code on the white board which I now see had so many design issues. Back then, we were focused more on failing and learning super fast and trying new things, so we went ahead and experimented.
To overcome the timeout limitation, I suggested using POST requests (with URL as data) to send jobs to an instance, and use multiple instances in parallel instead of using one instance serially. Because each instance in Cloud Run would only be scraping one page, it would never time out, process all pages in parallel (scale), and also be highly optimized because Cloud Run usage is accurate to milliseconds.
If you look closely, the flow is missing few important pieces.
As you can imagine, this lead to 1000 instances querying, and writing to Firebase DB every few milli seconds. Looking at the data post incident, we saw that the Firebase Reads were at one point about 1 Billion Requests per minute!
Running our this version of Hello World deployment on Cloud Run, made 116 Billion reads and 33 Million Writes to Firestore. Ouch!
Read Operations Cost on Firebase:
$ (0.06 / 100,000) * 116,000,000,000 = $69,600
To put this number into perspective, Google gets around 3.5 Billion searches / day. If each search requires 30 table lookups, that would still be less than the lookups we did in few hours.
After testing, we assumed that the request died because logging stopped, but actually it went into background process. As we didn’t delete the services (this was our first time using Cloud Run, and we didn’t really understand it back then), multiple services continued to operate slowly.
In 24 hours, these service versions each scaled to 1000 instances consumed 16,022 hours.
Already discussed above. We did discover a new way to use serverless using POST requests, something I hadn’t found anywhere on the internet, but deployed it without refining the algorithm.
While creating a Cloud Run service, we chose default values in the service. The max-instances is preset to 1000, and concurrency set to 80. In the beginning we didn’t know that these values are actually worst case scenario for a test program.
Had we chosen max-instances to be “2”, our costs would’ve been 500 times less. $72,000 bill would’ve been: $144
Had we chosen concurrency of “1” request, we probably wouldn’t have even noticed the bill.
There are somethings that can only be learnt after lot of experience. Firebase isn’t a language that one can learn, it’s a containerized platform service provided by Google. It has rules defined by them, not by laws of nature or how a particular user may think they are.
Integration of Firebase and GCP is slightly tricky one. If billing is enabled in one platform, GCP assumes it’s available everywhere.
Also, while writing code in Node.js, one must take care of Background processes. If the code goes into background processes, there’s no easy way for the developer to know that the service is running, but it might be, for fairly long time. As we learnt later on, this was the reason why most of our Cloud Functions were timing out as well.
Cloud overall is like a double edged sword. When used properly, it can be of great use, but if used incorrectly, it can have consequences.
If you count the number of pages in GCP documentation, it’s probably more than pages in few novels. Understanding Pricing, Usage, is not only time consuming, but requires a deep understanding of how Cloud services work. No wonder there are full time jobs for just this purpose!
At the peak, Firebase was able to handle about one billion reads per minute. This is exceptionally powerful. We had been playing around with Firebase for 2–3 months now and still learning about it, but I had absolutely no idea how powerful it was until now.
Same goes with Cloud Run! With Concurrency == 60, max_containers == 1000 and each Request taking 400ms, number of requests Cloud Run can handle 9 million requests per minute!
For comparison, Google Search gets 3.8 million searches per minute.
This means, if set up correctly, with really fast micro-services to power the backend, entire Google Search front end could deployed on Cloud Run.
Not suggesting that Google Search is simple, but alluding to the fact that Cloud Run is very powerful.
After going through our lengthy doc on this incident sharing our side of the story, various consults, talks, and internal discussions Google let go of our bill as a one time gesture!
Thank you Google!
We got our lifeline, and got back on both our feet to build Announce. Except this time with a much better perspective, architecture, and much safer implementation.
Google, my favorite tech company, is not just a great company to work for. It’s also a great company to collaborate with. The tools provided by Google are very developer friendly, have a great documentation (for the most part), and are consistently expanding.
After writing this article, a startup reached out to me and their goal is to optimize costs for GCP. I tried their services, and highly recommend if you are short on resources of mastering GCP. Give them a try:
yrl.is
After this incident, we spent few months on understanding Cloud and our architecture. In few weeks my understanding improved so much that I approximated the cost of scraping the “entire web” using Cloud Run with improved algorithm.
This incident led me to analyze our product’s architecture in depth, and we scrapped V1 of our product, to build scalable infrastructure to power our products.
In Announce V2, we didn’t just build an MVP; we built a platform where we could iteratively develop new products rapidly, and test them thoroughly in a safe environment.
This journey took us some time… Announce was launched in November end, ~7 months later than we had decided for our V1, but it is highly scalable, gets the best of Cloud services, and is highly optimized for usage.
We also launched on all platforms, and not just web.
What’s more is that we reused the entire platform to build our second product Point Address. Not only are both the products scalable, have a great architecture, and highly efficient, they are built on a platform that allows us to rapidly build and deploy ideas into useable products.
announce.today
pointaddress.com
I will soon be writing another post on how to deploy on GCP while keeping usage, development and cost in check.
I made the post on best practices to keep usage and development in check for most cloud platforms. You can refer to the medium article below:
sudcha23.medium.com
Blog post originally published at the Milkie Way Blog: https://blog.tomilkieway.com.
Making the world a more efficient, safer and connected space.
759 
11
759 claps
759 
11
Written by
Founder of Milkie Way, Inc. https://tomilkieway.com
Milkie Way, Inc. is a technology company with the mission o make the   world a more efficient, safer and connected space.
Written by
Founder of Milkie Way, Inc. https://tomilkieway.com
Milkie Way, Inc. is a technology company with the mission o make the   world a more efficient, safer and connected space.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@philipplies/transferring-data-from-google-drive-to-google-cloud-storage-using-google-colab-96e088a8c041?source=search_post---------232,"Sign in
There are currently no responses for this story.
Be the first to respond.
Top highlight
Philipp Lies
Jul 10, 2019·3 min read
Google Colab is amazing for doing small experiments with python and machine learning. But accessing data can be tricky, especially if you need large data such as images, audio, or video files. The easiest approach is storing the data in your Google Drive and accessing it from Colab, but Google Drive tends to produce timeouts when you have a large amount of files in one folder.
More robust and scalable is Google Cloud Storage, where you can also more easily share the data with colleagues. But unfortunately there is no native way to transfer data from Google Drive to Google Cloud Storage without having to download and upload it again. However, with Google Colab we can transfer files quite easily.
Mounting your own Google Drive is fairly easy. Just import the drive tools and run the mount command. You will be asked to authenticate using a token that you create using Google Auth API. After you pasted the token your drive is mounted to the given path.
Next we need to create a Google Cloud Storage project. Go to the Resource Manager and create a new project.
After the project is created (and you need to have billing enabled, as the storage will cost you a few cents per month) click on the menu in the upper right corner and select Storage (somewhere way down the menu). Next you need to create a bucket for the data.
The name of the bucket must be globally unique, so not only for your account but for all accounts. Just be creative ;-). There you can also estimate the cost for the bucket, which is around 0.60 EUR per month for 10 GB with 10,000 uploads and 1,000,000 downloads per month.
Once your bucket is set up you can connect Colab to GCS using Google Auth API and gsutil. First you need to authenticate yourself in the same way you did for Google Drive, then you need to set your project ID before you can access your bucket(s). The project ID is shown in the Resource Manager or the URL when you manage your buckets.
This will connect to your project and list all buckets. Next you can copy data from or to GCS using gsutil cp command. Note that the content of your Google Drive is not under /content/drive directly, but in the subfolder My Drive. If you copy more than a few files, use the -m option for gsutil, as it will enable multi-threading and speed up the copy process significantly.
That’s it. Now the process is running and you can check from time to time if it’s completed. I created a Colab notebook with the example code given here: https://colab.research.google.com/drive/1Xc8E8mKC4MBvQ6Sw6akd_X5Z1cmHSNca
Further information can be found in the Colab documentation here and the gsutil documentation here.
For future projects just authenticate the Colab notebook and transfer the files from the bucket to the local file system. Then you can run all experiments on the local copy.
Machine learning and neuroscience | Coding python (and recently JS and Kotlin) | Building apps you love
See all (8)
745 
19
745 claps
745 
19
Machine learning and neuroscience | Coding python (and recently JS and Kotlin) | Building apps you love
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@garystafford/automating-multi-environment-kubernetes-virtual-clusters-with-cloud-dns-and-istio-885e598b345b?source=search_post---------233,"Sign in
There are currently no responses for this story.
Be the first to respond.
Gary A. Stafford
Jan 19, 2019·9 min read
Kubernetes supports multiple virtual clusters within the same physical cluster. These virtual clusters are called Namespaces. Namespaces are a way to divide cluster resources between multiple users. Many enterprises use Namespaces to divide the same physical Kubernetes cluster into different virtual software development environments as part of their overall Software Development Lifecycle (SDLC). This practice is commonly used in ‘lower environments’ or ‘non-prod’ (not Production)…
"
https://towardsdatascience.com/scheduling-data-ingest-using-cloud-functions-and-cloud-scheduler-b24c8b0ec0a5?source=search_post---------234,"Sign in
There are currently no responses for this story.
Be the first to respond.
Lak Lakshmanan
Nov 20, 2018·4 min read
As Google Cloud continues to evolve, some of the solutions I presented in my book “Data Science on the Google Cloud Platform” get superseded because easier, more powerful solutions become available.
For example, a few months ago, I showed how to build regression and classification models using just SQL. The availability of highly scalable machine learning without having to move your data means that we can explore the value of machine learning quite easily. Were I to write the book today, I would have inserted a section on BigQuery ML into Chapter 5 (interactive data exploration).
In this article, I will talk about a second update: a better way to do periodic data ingest than what I presented in the last section of Chapter 2.
In the last section of Chapter 2 of the book, I presented a solution to schedule monthly downloads. This consisted of five steps:
Step 1 was done in the book, and I can simply reuse that Python program. The rest of the steps have gotten easier. A lot easier.
Instead of building a Flask web app and running it in AppEngine, there is a simpler way to create a web endpoint that is accessible via Http. The new way is to use Cloud Functions. I can use the same Python code that I used for ingest, but wrap it in a file named main.py (all the code in this article is on GitHub):
Essentially, my main.py has a single function that receives a Flask request object, from which I can extract the JSON payload of the HTTP Post by which the Cloud Function will be triggered.
I get the next month by looking to see what months are already in the bucket and then ingest the necessary data using the existing code in ingest_flights.py.
Once I have the main.py written, deploying the Cloud Function can be done via gcloud:
We can test the Cloud Function by sending it a curl request:
As the code above suggests, the URL of the Cloud Function is wide open. To somewhat secure the URL against denial-of-service attacks, we should change the URL to be something unguessable.
To make the URL unguessable, use the openssl library to generate a 48-character string, remove non-alphanumeric characters and trim the result to 32 characters:
This, by itself, is insufficient. We should also insist that legitimate callers provide us a token as part of the payload. Again, we can generate a token using the openssl program and add the check to main.py:
Doing both these things — an unguessable URL and checking for a token inside the Cloud Function — help to secure the Cloud Function.
Now that the Cloud Function provides an http endpoint that will launch the ingest job, we can use Cloud Scheduler to access this endpoint once a month:
The scheduler takes many formats for the schedule, including the format of Unix’s crontab, but I find the plain-language format supported by AppEngine’s cron the most readable. So, our endpoint will be accessed on the 8th of every month at 10am US Eastern time.
If you look at ingest_flights.py, the ingest method does quite a few things. It downloads the file, unzips it, cleans it up, transforms it and then uploads the cleaned up, transformed file to Cloud Storage.
Now that we are using Cloud Functions, it might be better to redesign this to be less monolithic. Besides being triggered by http calls, Cloud Functions can also be triggered by the addition of files to a bucket.
So, we could have the first Cloud Function simply upload the unzipped files to Cloud Storage, and then have a second Cloud Function carry out the extract-transform-load (ETL) part of the ingest. This might be more maintainable especially if it turns out that we have a bug in the ETL section. The original, raw data is available to rerun the ETL job.
Data Analytics & AI @ Google Cloud
376 
12
Every Thursday, the Variable delivers the very best of Towards Data Science: from hands-on tutorials and cutting-edge research to original features you don't want to miss. Take a look.
376 claps
376 
12
Your home for data science. A Medium publication sharing concepts, ideas and codes.
About
Write
Help
Legal
Get the Medium app
"
https://netflixtechblog.com/multi-cloud-continuous-delivery-with-spinnaker-report-now-available-6040ba83b765?source=search_post---------235,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
by Emily Burns, Asher Feldman, Rob Fletcher, Tomas Lin, Justin Reynolds, Chris Sanden and Rob Zienert
We’re pleased to announce the release of our O’Reilly report, Continuous Delivery with Spinnaker. The report is available to download for free on the Spinnaker website. ( Pdf | Epub | Mobi )
At Netflix, we’ve built and use Spinnaker as a platform for continuous integration and delivery. It’s used to deploy over 95% of Netflix infrastructure in AWS, comprised of hundreds of microservices and thousands of deployments every day.
Encoded within Spinnaker are best practices for high availability, as well as integrations with Netflix tools like Chaos Monkey, ChAP Chaos Automation Platform, Archeius, Automated Canary Analysis and Titus. With Spinnaker, developers at Netflix build and manage pipelines that automate their delivery process to cloud VMs, containers, CDNs and even hardware OpenConnect devices.
We first built Spinnaker to commoditize delivery for internal teams so they can manage their deployments. Our active open source community helped validate Spinnaker’s cloud-first, application-centric view of delivery by contributing tools, stages and cloud provider integrations.
We were motivated to write this report as a high-level introduction to help engineers better understand how Netflix delivers production changes and the way Spinnaker features help simplify continuous delivery to the cloud. The report covers converting a delivery process into pipelines that can safely deploy to Kubernetes and Amazon EC2, adopting and extending Spinnaker, and ways to leverage advanced features like automated canary analysis and declarative delivery. We hope you like it.
If you would like a physical copy of the report, members of the Spinnaker team will have them on hand at the following upcoming conferences:
Learn about Netflix’s world class engineering efforts…
585 
1
585 claps
585 
1
Written by
Learn more about how Netflix designs, builds, and operates our systems and engineering organizations
Learn about Netflix’s world class engineering efforts, company culture, product developments and more.
Written by
Learn more about how Netflix designs, builds, and operates our systems and engineering organizations
Learn about Netflix’s world class engineering efforts, company culture, product developments and more.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://towardsdatascience.com/3-steps-to-get-aws-cloud-practitioner-certified-in-2-weeks-or-less-772178f48249?source=search_post---------236,"Sign in
There are currently no responses for this story.
Be the first to respond.
Kenneth Leung
Jan 2, 2021·6 min read
Introduction to AWS CertificationsStep 1: AWS E-learning ModulesStep 2: AWS Whitepapers and WebsiteStep 3: YouTube Practice QuestionsOptional ResourcesAbout the Examination
"
https://medium.com/firebase-tips-tricks/how-to-list-all-subcollections-of-a-cloud-firestore-document-17f2bb80a166?source=search_post---------237,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renaud Tarnec
Oct 25, 2019·6 min read
As detailed in the Cloud Firestore documentation, data in Firestore is stored into documents, which are “organized into collections”. Documents can contain subcollections which, in turn, can contains documents.
The documentation also indicates that:
Documents in subcollections can contain subcollections as well, allowing you to further nest data. You can nest data up to 100 levels deep.
Normally, as a Firestore database architect, while working out your data model, you decide…
"
https://coderef.com.br/arquitetura-de-microservices-com-spring-cloud-e-spring-boot-parte-1-b5c9288df66d?source=search_post---------238,"Fala pessoal, estou começando hoje uma série de Stories onde vamos abordar a construção de uma Arquitetura de MicroServices utilizando os frameworks Spring Cloud e Spring Boot da Pivotal.
Antes de começar decidi fazer uma breve introdução sobre as diferenças entre a Arquitetura de MicroServices e a Monolítica assim como reiterar alguns conceitos que surgem quando se utiliza a mesma.
Serão stories semanais e o sumário estará presente em todas para facilitar a navegação entre os tópicos. Então chega de enrolação e vamos ao que interessa!
Como agora não teremos mais só uma aplicação, mas varias espalhadas por diversos servidores é necessário centralizar a configuração de todas as aplicações em um só lugar.
A Pivotal possui um projeto chamado Spring Cloud Config que possibilita a criação de uma configuração externalizada em um sistema distribuído. Com o Config Server você tem um lugar central, um repositório git por exemplo, para gerenciar os arquivos de configuração de cada aplicativo que se encontram rodando em outros ambientes.
As aplicações consultarão o Config Server para obter suas configurações na hora da inicialização. Podem ser desde configurações de acesso ao banco de dados até mesmo a porta em que desejamos que a aplicação suba.
Quando se tem muitas aplicações rodando em diferentes ambientes é difícil controlar qual o host ou porta onde cada um se encontra e se a mesma está online ou não.
O Service Registry é um banco de dados preenchido com informações sobre como enviar pedidos para instâncias de microservice. Cada instância que sobe se cadastra no Service Registry informando que está Online para receber requisições.
Além de deter as informações de acesso as instâncias, o Service Registry também realizará checagens de saúde da aplicação e o balanceamento de carga de instâncias da mesma aplicação.
Para realizar essa função iremos utilizar o Eureka da Netflix.
Em uma aplicação distribuída é comum que haja chamadas entre diversos servidores na rede. Diferente da chamada em memória, as chamadas remotas podem falhar ou ficarem pendentes se o host destino estiver indisponível até que um tempo limite seja atingido.
Se vários clientes tentarem acessar esse mesmo recurso indisponível podemos ter uma falha crítica e isso pode afetar o funcionamento de todo o sistema.
O Circuit Break ficou popular por evitar esse tipo de cascata. Basicamente a request a um host que pode falhar é envolta em um Circuit Breaker e se essa chamada começar a falhar é retornado um erro conhecido e registrada alguma métrica informando que a request está falhando naquele local.
Tendo métricas em mãos dos locais exatos onde a aplicação está falhando, se torna menos complicada a busca pela resolução de erros ou gargalos na aplicação.
Para essa série de stories iremos utilizar o Hystrix da Netflix para tratar o Circuit Braker.
Imagine que cada instância de um microservice sobe com uma porta diferente, e se você não tem um ponto de entrada único para sua aplicação como um todo, isso pode se tornar um caos na hora fornecer os recursos para um cliente web ou mobile.
O gateway funciona como uma porta de entrada da sua aplicação, todo o trafego passa por ele antes de ser encaminhado para o microservice específico respeitando as rotas que são configuradas no mesmo.
O Gateway geralmente recebe a requisição desejada pelo cliente e consulta no Service Registry qual instância de microservice responde por aquela rota. Se for uma rota segura, ele também irá realizar a autenticação junto ao servidor de autorização antes de fazer o redirecionamento.
Para essa série de stories iremos utilizar o Zuul da Netflix como Gateway.
Configurações: https://github.com/rafaelcam/delivery-configsProjeto: https://github.com/rafaelcam/delivery
https://auth0.com/blog/an-introduction-to-microservices-part-3-the-service-registry/https://cloud.spring.io/spring-cloud-config/https://github.com/Netflix/zuulhttps://martinfowler.com/bliki/CircuitBreaker.html
Porque pensar em tecnologia é pensar no futuro
656 
3
Thanks to Diego Brener da Silva. 
656 claps
656 
3
Written by
Lead Engineer at Movile
Porque pensar em tecnologia é pensar no futuro
Written by
Lead Engineer at Movile
Porque pensar em tecnologia é pensar no futuro
"
https://medium.com/aws-enterprise-collection/an-e-book-of-cloud-best-practices-for-your-enterprise-4a211840c55b?source=search_post---------239,"There are currently no responses for this story.
Be the first to respond.
Over the last eight months, I’ve written a series of posts detailing several best practices for enterprises using (or considering) the cloud. These are practices I encourage any technology executive or professional looking to transform their business and/or career to consider. They were first inspired by my time as CIO of Dow Jones, from 2012 to 2014, where the cloud quickly became the second most impactful means by which we changed technology’s role within the company (the first most impactful being the amazing team of people that made it all happen). Since then, I’ve had the opportunity to continue refining them through my work with other technology professionals, many of whom come from the largest companies in the world and all of whom are at various stages on a similar Journey.
Over the last several weeks, I’ve found myself sharing links to individual posts as a follow up to some great conversations. I believe these best practices will lead to better results when they’re practiced in concert, so rather than continue to share links one-by-one I’m hoping that this post will serve as a useful table of contents into all the best practices I’ve written about.
(Note: these best practices, and a number of others, are now available in my book Ahead in the Cloud: Best Practices for Navigating the Future of Enterprise IT).
I wrote about meaningful cloud adoption being a Journey — an iterative process that allows organizations to move faster and devote their resources to the things that matter most in their business — in this post in September of 2014. Then, in December of 2015, I created this post as a kick-off to a more in-depth series examining some of the best practices that enterprises employ on their Journey. The “chapters” that follow contain links to all the posts I’ve authored on each of the best practices.
Before I wrote the CCoE series, I wrote a series on DevOps in your enterprise, which has a lot of relevant points to the CCoE model
I hope that you find this list useful, and I’d love to know what you think is missing!
Keep building,- Stephenorbans@amazon.com @stephenorban Read My Book: Ahead in the Cloud: Best Practices for Navigating the Future of Enterprise IT
Tales of AWS in the Enterprise
431 
2
431 claps
431 
2
Written by
Husband to Meghan, father to Harper and Finley. GM of AWS Data Exchange (formerly Head of Enterprise Strategy for AWS). Author of “Ahead in the Cloud”.
Tales of AWS in the Enterprise
Written by
Husband to Meghan, father to Harper and Finley. GM of AWS Data Exchange (formerly Head of Enterprise Strategy for AWS). Author of “Ahead in the Cloud”.
Tales of AWS in the Enterprise
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/pangeo/cloud-native-geoprocessing-of-earth-observation-satellite-data-with-pangeo-997692d91ca2?source=search_post---------240,"There are currently no responses for this story.
Be the first to respond.
If you are familiar with satellite imagery you’ve likely heard that we are entering a “golden era” of Earth Observation. It’s true! New satellites are generating Petabyte-scale publicly available archives of imagery at unprecedented rates, enabling new insights and fast global impacts.
This deluge of imagery is forcing scientists to reconsider traditional workflows of downloading thousands of image files to work on a personal computer. The alternative is a “Cloud Native” approach - forgo downloading data and operate on the data where it is stored on the Cloud. This approach has the major advantage of being able to utilize vastly scalable computing resources to crop, transform, and apply algorithms to imagery very quickly — Quickly enough to enable interactive analysis at full resolution over the entire globe.
For scientists this scalability and interactivity is fundamental to the process of discovery. A few years ago a seminal paper was published that quantified global deforestation using the entire Landsat archive using a Cloud Native approach (Hansen et. al. 2013). This paper was truly inspirational, demonstrating that questions of global scope are not limited to a select few with access to supercomputers. And in the last several years, Cloud Native tools for scientific research have been growing rapidly. In this article we will highlight a number of these tools, but focus on the Pangeo project: “A community platform for Big Data geoscience”.
We’ve developed an example Cloud Native quantitative analysis of Landsat 8 satellite imagery. What is special about this example is that the analysis is easily reproduced, scalable, and interactive: 100 Gigabytes of Landsat 8 images covering Washington State (representing the entire archive back to 2013–03–21) are found using NASA’s Common Metadata Repository (CMR). Then, using URLs instead of local file paths, the Normalized Difference Vegetation Index (NDVI), a simple landcover classification algorithm, is run in seconds on a Cloud-based cluster. Compare this to a traditional workflow, in which a scientist must wait hours or days to download and decompress 100 scenes from a USGS server, then run analysis locally.
One of the hallmark features of the Pangeo project is a community-developed JupyterHub instance running on Google Cloud with a preconfigured Python environment and Kubernetes cluster. This environment can be customized and launched with the click of a button using Binder, allowing anyone to run Python code interactively in a web browser. A more detailed blog post on the implementation of Pangeo’s Binder instance can be found can be found here. And you can interactively run the full Landsat example simply by clicking the button below!
One very special feature of the Landsat example is that computations are done in parallel and on-the-fly, made possible by several independently developed Python packages (xarray, rasterio, dask, holoviews) coming together with magical results! For example, take a look at the following screencast, which demonstrates dynamically computing NDVI for selected dates at a resolution suitable to the current zoom level.
You can also easily extract a time series for a particular pixel or patch. Interested in a different region, different index, or color scale? The example can easily be modified and run and code, graphs, and images saved to your local computer for future use.
One appealing feature of the Landsat example is that the a user needs only familiarity with Python, which has become one of the most pervasive programming languages in the scientific community. Parallel computation and memory management are taken care of by Dask behind the scenes. Rasterio and Xarray know how to pull down chunks of full resolution images, and Dask knows how to distribute computations that use those chunks. What’s more, if local memory is exceeded, data is written and read from disk, allowing for computations to be run without fear of “out of memory” errors.
However, this workflow would not be possible if the images weren’t stored in a format amenable to Cloud Native analysis. In the Earth Observation community there is a lot of excitement surrounding Cloud-Optimized Geotiffs (COGs) which are described in detail on https://www.cogeo.org, and well-advocated for in a series of blog posts by Chris Holmes (start here). In brief, COGs are Geotiff files that have internally organized overviews and image tiles and support HTTP range requests (enabling downloading of specific tiles rather than the full file). COGs are also nice because they work normally in GIS software such as QGIS. Strictly speaking the Landsat 8 images on Google Cloud are not in the COG format because they do not include built in overviews, but critically HTTP range requests still work.
Our Landsat example is not necessarily optimized in terms of computational efficiency. One simple way to speed up the analysis would be to work with “Analysis Ready Data”: At a basic level, images with the same dimensions that are aligned to the same coordinate grid, such that chunks are uniform and retrieved efficiently. The USGS has created such an archive for Landsat 8, but it is not available on a public Cloud.
For now, the reality is that most Earth Observation data is not stored on the Cloud, and of the data that is, much of it is not in a format amenable to Cloud Native workflows. There are innovative solutions using on-demand format conversion, such as the amazing GOES-16 data staging tool created by Element 84. Nevertheless, we hope that as NASA moves public archives to AWS, Cloud Native formats will be used and will lead to rapid and exciting new discoveries!
The Landsat 8 analysis described here might remind you of Google Earth Engine (GEE). GEE is a tremendous community resource and has a fantastic user interface. But what if you are not accustomed to programing in Javascript? Or, what if you want to run computations on the archive of Sentinel-1 SLC data stored only on Amazon Web Services (AWS)? Or a custom Terabyte-scale dataset on your own server?
There are many “platforms” currently under development that are designed to harness the power of commercial Cloud compute resources for scalable and fast analysis of Earth Observation data: Raster Foundry, EOBrowser, GBDX Notebooks to name a few. What distinguishes Pangeo from these platforms is that Pangeo is based purely on general purpose, open source, community based tools. Since these tools are designed well, they combine easily into something that is greater than the sum of the parts. It’s important to acknowledge that while the constituent tools are open and free, running analyses on the commercial Cloud is not. This is why some platforms charge hefty subscription fees to use their services. For now, Pangeo is generously supported by grants from the National Science Foundation and NASA which include credits on Google Cloud Platform .
In order to ensure flexibility and long term sustainability of Cloud Native tools, it is important to focus on Cloud-agnostic tools and recipes. Pangeo is tackling this issue by streamlining deployment with multiple Cloud providers. There are other great efforts on this front, generally spearheaded by academic groups. For example, OpenEO is providing “A Common, Open Source between Earth Observation Data Infrastructures and Front-End Applications”, see here for an excellent description of why this effort is so timely. And it’s worth noting that the Earth Sciences are not the only academic discipline confronting the issue of reproducing large scale analyses on Cloud infrastructure: For example, REANA comes from particle physics analyses and “… helps researchers to structure their input data, analysis code, containerised environments and computational workflows so that the analysis can be instantiated and run on remote compute clouds.”
Large archives of public satellite data on the Cloud can be a tremendous resource for the scientific community. They can also seem at first like being gifted the proverbial white elephant — how can researchers manage and conduct reproducible research with 45Tb of Landsat data? Fortunately, tools are emerging that enable researchers to make the most of Earth Observation data.
Pangeo is an exciting resource for Earth Observation because it is a collection of free and open cutting-edge computational resources geared toward Earth Scientists. It is also a collaborative community of scientists and developers who are eager to discuss these resources and continue to advance them. We hope this article and example analysis have piqued your interest in the Pangeo project. If you’d like to get involved, please visit the Pangeo website, or make direct contributions on the project GitHub repository.
This blog post and the Landsat analysis example was a team effort with important contributions from Daniel Rothenberg, Matthew Rocklin, Ryan Abernathey, Joe Hamman, Rich Signell, and Rob Fatland. Landsat data is made publicly available by U.S. Geological Survey.
The Pangeo project is currently funded through grants from the National Science Foundation and the National Aeronautics and Space Administration (NASA) . Google provides compute credits on Google Cloud Platform.
A community platform for big data geoscience
442 
1
442 claps
442 
1
A community platform for big data geoscience
Written by
Research geophysicist at University of Washington eScience Institute
A community platform for big data geoscience
"
https://medium.com/swlh/cloud-native-java-vs-golang-2a72c0531b05?source=search_post---------241,"There are currently no responses for this story.
Be the first to respond.
Java once-famous motto: “Write once and run everywhere” is pretty much obsolete these days, the only place we want to run code is inside a container. Where a “Just in time” compiler does not make any sense.
For this reason, probably, the Java ecosystem is in the midst of its transformation in order to become better suited for the cloud. Oracle’s GraalVm allows compiling byte code to Linux executables (ELF) and Rad…
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/hackernoon/welcome-to-cloud-native-development-with-aws-cloud9-aws-codestar-c7b6536afba8?source=search_post---------242,"There are currently no responses for this story.
Be the first to respond.
Top highlight
I have been experimenting with AWS Cloud9 since Werner Vogels announced it a few weeks ago at AWS re:Invent 2017 (keynote video here).
This article is the paraphrased version of my talk AWS Cloud9 & CodeStar for Serverless Apps, given at the AWS User Group in Milan on Jan 10th.
I am going to skip the “Serverless Disclaimer” section of my deck.If you are not familiar with Serverless, please have a look here, or here, or here, or here, or here, or here, or here.
If you are familiar with Serverless and you don’t like it, you may still enjoy this article and the benefits of Cloud9 and CodeStar. Just make sure you mentally replace “FaaS” with “Container”, and “SAM” with “CloudFormation” :)
AWS Cloud 9 is a “cloud IDE” for writing, running, and debugging code.
I’d start saying that most IDEs are fantastic tools to boost your productivity and the quality of your code if you can use them and invested a few months/years in learning them properly.
That being said, some IDEs offer more advanced and useful features than others (please don’t take it personally, unless you code in Word or Notepad).
AWS acquired Cloud9 in July 2016, which has now been rebranded as AWS Cloud9. Even though it looked brilliant on Werner’s browser, my first impression during the keynote was something along the lines of “Why should I pay to write code?”, immediately followed by “Does that mean I cannot code when I’m offline?”. Like most software engineers, I’ve used many IDEs in the last ten years, for free, and I’m used to coding a lot while I’m traveling.
Apparently, I was not alone, and many developers asked the same questions during my presentation. So let me briefly recap my arguments.
Regarding the cost, I believe it’s pretty much negligible for most organizations that already use AWS heavily (less than $2 per month for a t2.micro environment, 8 hours a day, 20 days a month). And without considering AWS Free Tier and automatic cost-saving settings (hibernation after 30min).
The “no coding offline” drawback is much harder to defend, but let me try.
Unless you are going for a hardcore debugging session over a well-established project, can you really code for more than 30min when you are offline? Can you git clone or git pull anything useful? Can you npm install or pip install the modules you need? Can you mock or ignore all the third-party services and APIs your application consumes? Can you avoid googling around your favorite framework’s documentation?
Sure, you could prepare for a 12h flight and download/install everything you need in advance. But how often does that happen? Simply put, I’ve seen the best developers and engineers give up and take a break when the network goes down.
On the other hand, AWS Cloud9 offers you a better alternative for when your machine gives up :) I could throw my dev machine out of the window right now, switch to my colleague’s notebook, login into my AWS Account and keep working on the very same Cloud9 session (which is saved and stored server-side). That means you could as well use a much cheaper machine such as a Chromebook or a tablet (or a phone?). Well, you could be 100% operational using the random machine of an internet cafe, or your grandmother’s computer :)
Of course, there are always exceptions, and I’ll make sure I’m ready to use my local IDE when Cloud9 is not an option. In the meantime, I hope AWS will work on some sort of client-side offline support (and maybe turn Cloud9 into a progressive web app?).
I think AWS Cloud9 solves a bunch of problems for the many organizations currently trying to set up elaborate stacks of tools on every developer’s machine, especially if the team is heterogeneous and/or distributed.
Let’s recap some of its features:
I took the following screenshot during a live-debugging session of a very simple Lambda Function (note: I also spent 3 minutes customising theme & layout, according to my taste and needs).
I do have a few wishes for AWS Cloud9, and I’ve shared a few of them on Twitter (tweets below).
Let me discuss a few of them:
AWS CodeStar (aka Code-*) is a sort of “catch-all service” for the ever-expanding suite of tools for developers.It is a free service that lets you manage and link together services such as CodeCommit, CodeBuild, CodePipeline, CodeDeploy, Lambda, EC2, Elastic Beanstalk, CloudFormation, Cloud9, etc.
One of my 2018 new year resolutions is to use more memes in my decks (until someone decides to stop me, for some reason), so here’s how I presented some of the pain points that CodeStar can solve.
Data-driven parenthesis: I can statistically confirm that the JIRA meme generated 42% more laughs than all others combined.
CodeStar may not be the best fit for every project/organization, especially the most experienced and advanced ones, but it definitely provides some very good defaults to get started with. It’s worth noting that CodeStar is 100% free, and you only pay for the resources it will spin up on your behalf.
Let’s recap its features:
CodeStar can look like magic if you’ve never played with CodePipeline and CodeBuild, but unfortunately it’s not perfect yet. I’ve shared a few “wishes” on Twitter too (tweets below), and here’s a quick recap of what I’ve found.
Cloud9 and CodeStar are pretty cool services on their own, and I was excited to see how they’ve been integrated. Or, better, how Cloud9 has been integrated into CodeStar.
You can associate AWS Cloud9 Environments to your CodeStar project natively. In case multiple developers are working on the same project, you can create and assign a Cloud9 Environment to each developer (eventually, they’ll collaborate and invite each other, if needed).
Once you open Cloud9, you’ll find your IAM credentials integrated with git (which does require some work) and your CodeCommit repository already cloned for you.
Unfortunately, this magic doesn’t happen if you choose GitHub (for now?).
As a couple of friends and colleagues pointed out, it’s not such a critical or technical complex integration, in the sense that you could have taken care of it yourself (as you’d do on your local machine). But I think it’s a great way to streamline the development experience and reduce the margin for error, especially when you work on multiple projects and multiple accounts.
For example, most developers make great use of AWS profiles when working on their local machine, and some of them also manage to remember which profile can do what, in which account, etc. With CodeStar+Cloud9 you won’t care anymore about profiles or local credentials since every Cloud9 environment is bound to a specific project and account. Also, since CI/CD is enabled by default, most of the time you will just write code, test with sam-local and git push 💛
Of course, you may also have a generic Cloud9 Environment (i.e. not related to a specific project) and use it with multiple profiles to manage unique resources or prototype new stuff.
I decided to conclude my presentation with a brief parenthesis about AWS SAM, which got a few mentions and therefore deserves some context.
** Serverless alert **
SAM stands for Serverless Application Model, and it’s an open specification whose goal is to offer a standard way to define serverless applications.
Technically speaking, it’s a CloudFormation Transform named AWS::Serverless that will convert special Serverless resources such as AWS::Serverless::Function into standard CloudFormation syntax.
You can think of Transforms as a way to augment the expressiveness of CloudFormation templates so that you can define complex resources and their relationships in a much more concise way.
If you are familiar with other tools such as the Serverless Framework, you’ll notice that the syntax is quite similar (there is even a plugin to convert your templates to SAM).
In fact, you can deploy SAM templates with AWS SAM Local, a CLI tool for local development written in Go and officially released by AWS.
You can use AWS SAM Local to test your Lambda Functions locally and emulate API Gateway endpoints too. The CLI tool is available by default on every Cloud9 EC2 Environment, and the UI already supports some of its functionalities.
I have only one wish for AWS SAM: I would love to see more transparency and documentation related to the AWS::Serverless Transform.
Apr 2018 Update: AWS SAM is now open-source on GitHub!
And since I like dreaming, why not allowing custom CloudFormation Transforms too? I am almost ready to bet they are implemented with some kind of Lambda hook, and I can’t even start to imagine how many great things the community might be able to develop and share that way.
I hope you learned something new about AWS Cloud9 and CodeStar (please don’t confuse them and create weird hybrids such as “CloudStar” as I did a few times). I would recommend building a simple prototype or a sample project on CodeStar asap. You can get started here!
If you got this far, you probably enjoyed the article or feel like sharing your thoughts. Either way, don’t forget to recommend & share, and please do not hesitate to give feedback & share your ideas =)
#BlackLivesMatter
963 
7
how hackers start their afternoons. the real shit is on hackernoon.com. Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
963 claps
963 
7
Written by
Italian | Musician | Traveler | Technical Evangelist @ AWS https://aws.amazon.com/developer/community/evangelists/alex-casalboni/ Opinions are my own.
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
Italian | Musician | Traveler | Technical Evangelist @ AWS https://aws.amazon.com/developer/community/evangelists/alex-casalboni/ Opinions are my own.
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/the-cloud-of-smart-things-dc6e04491fb3?source=search_post---------243,"There are currently no responses for this story.
Be the first to respond.
Top highlight
As we head into 2017 I reflect on the last 10 years of advances in technology, and look forward to then next 10 years — and imagine what that might look like.
The opportunity smartphone apps represent for developers and entrepreneurs is obvious today, but it was less so in 2009 when my presentations boasted to potential Android developers of the 12,000 apps then available in the Android Market.
10 years ago there were no real smartphones. The very first Android handset was released in 2008; the first iPhone in 2007. I joined Google in March of 2009, just in time to help launch Android 1.5 Cupcake (API level 3 — Widgets! Live folders! 3rd party keyboards!)
According to Wikipedia, in 2009 Android accounted for 2.8% of new smartphones, and smartphones represented less than 14% of new phone sales. Wikipedia now suggests that today Android runs on 85% of new smartphones — which in turn represent 74% of all new phones.
So what comes next?
As with everything I post here on Medium, what follows is my own, personal opinion. It does not necessarily represent the views of Google, Alphabet, or any of the people who work there. I have chosen a 10 year timeframe to avoid any suggestion that these guesses are based on any knowledge I may have because I’m a Google employee; they are not. Further, they should not be taken as an indication of anything Google is, has, or will be doing now or in the future. I’m just making this shit up people — please read accordingly.
Smartphones and laptops aren’t going anywhere anytime soon, but that doesn’t mean things aren’t changing.
I believe a new industrial revolution is underway; driven by machine intelligence that’s increasingly enabling autonomous cloud-connected devices with which we interact in natural ways.
In 2010 I got a Nexus One— a $600 smartphone with a 1GHz CPU and 512MB of RAM. Today, I can buy a Raspberry Pi Zero — with the same CPU speed and RAM — for $5.
Tech giants like Google, Amazon, and Microsoft are progressively externalizing their infrastructure — both hardware and software. This makes it possible for us as developers to build products and services that take advantage of decades of progress and 10s of billions of dollars of investments in machine intelligence, data center design, big data processing, and site reliability.
The smartphone represents the ultimate evolution of general-purpose computers. From room-sized, down to mini-computers, desktop PCs, laptops, and finally hand-held devices, we’ve finally shrunk it enough that we can take it with us.
The next step is making it disappear entirely. The technological advances that make a $5 Raspberry Pi possible, allow to integrate computer control from everything from our thermostat, to our lights, sprinklers, curtains, and mailboxes. Machine Intelligence will allow these devices to be autonomous, or controllable using conversational voice commands.
In 10 years it won’t be weird to talk to the things in your house to control them. You’ll talk conversationally and your home will recognize you (and your family members) individually and personally. Anything with a switch will be updated, and your home, car, office, and hotel room will learn your habits and preferences, acting autonomously to control everything from the thermostat, to windows, curtains, and lighting.
In 20 years: Computers become cheap enough to be disposable, allowing us to put a computer in everything. You won’t have a smart fridge, you’ll have a smart milk carton that orders more milk when it runs out. Plants will tell your reticulation how often they need to be watered. Big Data will allow you to analyze your dietary intake based on the food you’ve consumed.
The QHD AMOLED at 534ppi screen on the Pixel XL is a thing of beauty, but it’s still only images on glass — and the size of the glass is determined by the need for it be small enough to be comfortable and portable.
Google Cast lets you use any TV as a screen, and AR/VR like Daydream and Oculus can give you a personal cinema experience. Google Home and Amazon Echo eliminate the screen entirely for non-visual operations like getting answers to questions and listening to music.
In 10 years, I expect to see stand-alone devices and personal audio- and video-based personal augmentations to provide feedback for our queries and requests wherever we are, without us needing to look at a phone.
In 20 years: Rectangular glass devices as a form-factor are an anachronism, wholly replaced by in-ear headphones and retinal projectors.
The proliferation of smartphone and other connected devices has exploded in the past decade, but the availability and affordability of connectivity hasn’t enjoyed the same boost.
I believe that’s about to change. Projects like Alphabet’s Access and Facebook’s Connectivity Lab have publicly declared their desire to see affordable wireless Internet access available to everyone.
In 10 years, globally, Internet access will be more easily obtained than running water or electricity. All your devices will be able to connect at all times to fast, reliable, and cheap (if not free) Internet access.
In 20 years: People will see references to getting “4 bars” in old movies and won’t know what the hell they’re talking about. The idea that you or your belongings might not be able to connect to the Internet will seem as unlikely as it does horrifying.
Touching and swiping are a great way to interact with glass surfaces, but it’s the wrong paradigm for a future with invisible computers. The quality of voice recognition and speech synthesis have already make interacting with devices through voice a reality.
Improved voice recognition, natural language parsers, and image analysis are already available to developers. These services will continue to improve, allowing companies like Google and Apple to create increasingly intelligent AI-powered assistants, while developers create more intuitive products and services.
In 10 years physical interaction with hardware devices will be unusual, with voice and gesture control the norm — likely intermediated by an intelligent assistant able to contextualize interactions with multiple services and hardware devices.
In 20 years: Artificial intelligence will progress to the point where little interaction is required for your assistant to manage most mundane interactions with devices. Mind control will replace vocal instructions and text entry.
10 years ago, the idea of running your services on someone else’s servers was a radical concept. Today, using a public cloud provider like Google or Amazon is a given for any new company — and increasingly powerful services like BigQuery and Machine Intelligence are being externalized.
In 10 years public Cloud providers will be making advances in quantum computing, biological storage, and Machine Intelligence available to all developers. References to CPUs, cores, drives, SSDs, and Virtual Machines will disappear as out-dated metaphors for long-deprecated hardware. As Cloud resources become increasingly commoditized, they will become significantly cheaper. The idea of running your own server — even for development or debugging — will be vaguely ridiculous.
In 20 years: My generation of developers will laugh as they tell horrified new grads about the days when we ran production services on non-virtual machines with spinning disks under our desks or in our garages. Everything that doesn’t happen on-device will be processed and stored in massively redundant bio-organic data centers that generate electricity instead of consuming it.
The intersection of Cloud, machine intelligence, and connected devices feels eerily similar to the excitement of 2010 — just before Android really took off.
The active, passionate, vocal, and enthusiastic community of Android App Developers was — and continues to be — a critical factor in Android’s success and growth. Those early Android developers took advantage of what quickly become a revolution; I believe a similar opportunity exists today for Makers.
The combination of Machine Intelligence, the public Cloud, and the cheap hardware that enables the Internet of Things represents an opportunity to propel the Maker community of awesome, enthusiastic hobbyists into the forefront of the next industrial revolution.
I love learning new things and getting involved in early in potential step-changes in technology, so I’ll be keeping a close eye on these areas, and getting more involved in the communities around them.
As with Android in 2010, all the pieces you need to get a head start in this new opportunity already exist. There’s a range of providers, but I work for Google — so no surprise that I’m going to suggest you start by heading over to Google Cloud and checking out the machine learning APIs.
How will you help define the future?
Once again, this post is my personal opinion. I am not speaking on behalf of Google, Alphabet, or any of the people who work there. Any resemblance these predictions may have to actual Google plans, projects, or products is purely coincidental. Seriously.
Google Cloud community articles and blogs
158 
4
158 claps
158 
4
Written by
Developer Advocate @ Google, software engineer, and author of “Professional Android” series from Wrox. All opinions are my own.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Developer Advocate @ Google, software engineer, and author of “Professional Android” series from Wrox. All opinions are my own.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://blog.percy.io/tuning-nginx-behind-google-cloud-platform-http-s-load-balancer-305982ddb340?source=search_post---------244,"Percy is a visual testing and review platform that helps you deploy every UI change with confidence. Learn more at https://percy.io ✨
This is a consolidation of learnings from weeks of tuning and debugging NGINX behind the Google Cloud Platform HTTP(S) Load Balancer.
There is an unfortunate lack of documentation around the web for some of this, so I hope it helps you! But remember, tuning is always specific to different environments and conditions—your mileage may vary.
By default, NGINX does not compress responses to proxied requests (requests that come from the proxy server). The fact that a request comes from a proxy server is determined by the presence of the Via header field in the request.- NGINX Admin Guide: Compression and Decompression
Google’s load balancer adds the “Via: 1.1 google” header, so nginx will not gzip responses by default behind the GCP HTTP(s) Load Balancer. This happens because nginx does not think the proxy can handle the gzipped response.
To re-enable gzipped responses, configure gzip_proxied in nginx.conf (in http, server, or location blocks):
Traffic from the load balancer to your instances has an IP address in the range of 130.211.0.0/22. When viewing logs on your load balanced instances, you will not see the source address of the original client. Instead, you will see source addresses from this range.- https://cloud.google.com/compute/docs/load-balancing/http/
For security reasons, you can force all HTTP(S) traffic to flow through the load balancer and block direct access to your instances (from port scanners, for example). You just need to know this specific CIDR range:
When making a GCE firewall rule, just set Source IP ranges to this range.
Update: Google has added more ranges that load balance traffic might come from. As of Jan. 31, 2018, you’ll also need to have this range allowed:
These ranges only apply for the HTTP(S) Load Balancer and SSL Proxy. If you are using Network Load Balancing, see the docs for the applicable ranges.
This was a hard one. Many hours tuning sysctl settings, running tcpdump, re-architecting flows, and trying to recreate a rare and intrusive error before figuring out what was happening.
Summary: the default nginx keepalive_timeout is incompatible with the Google Cloud Platform HTTP(S) Load Balancer!
You must increase nginx’s keepalive_timeout, or risk intermittent and sporadic 502 Bad Gateway responses to POST requests.
The “650 seconds” here is not arbitrary, see below for justification of why we picked this specific timeout. Notably, this is opposite of the advice that most articles will give you, but most of them are configuring nginx for direct connections and not for sitting behind a global load balancer.
Several times a day, POST requests to our API would return a 502 Bad Gateway response, with no backend log of the error. Long ago we added client-side retries to our API libraries to handle these cases, but I decided to finally root-cause this bug once and for all so we didn’t have to keep patching it across libraries.
In Google Cloud Logging, you can see if you are experiencing these particular 502 responses using an advanced filter:
Now it gets tricky. There were no other logs that correlated with this rare error—no logs from nginx itself, nothing from the API app, nothing related to any syslog or kernel message—nothing, except rare 502s in load balancer logs.
That looks very much like a problem with the GCP HTTP(S) Load Balancer itself. But, as an ex-Googler who has configured services behind Google load balancers in the past, I knew it was very unlikely that my site was a special snowflake that was hitting some new bug uncaught by the billions of requests that flow through those LBs every day. It was much more likely I had a bad config somewhere.
Digging further, we see more info:
Ah ha. So, this was not a problem finding a backend machine (which would have been “failed_to_pick_backend”). The docs have a tiny description of this error:
backend_connection_closed_before_data_sent_to_client: The backend unexpectedly closed its connection to the load balancer before the response was proxied to the client.- Google Cloud Platform: Setting Up HTTP(S) Load Balancing
Meaning, a TCP connection is being established from the load balancer to the GCE instance, but the instance is terminating the connection prematurely.
Some things were common to the error:- It only happened on POST requests, never on GET requests.- It only happened on our nginx + API services, not on our nginx + static.- It only happened under moderate traffic load (but the server was not overloaded, still 2–5% CPU usage and plenty of memory). This made it feel like a race condition or timeout problem of some sort.
I finally found the right knob to turn. It turns out that there is a race condition between the Google Cloud HTTP(S) Load Balancer and NGINX’s default keep-alive timeout of 65 seconds. The NGINX timeout might be reached at the same time the load balancer tries to re-use the connection for another HTTP request, which breaks the connection and results in a 502 Bad Gateway response from the load balancer.
Let’s dig deeper.
I was actually able to reliably reproduce this error using a simple curl POST load test script, combined with an aggressive nginx timeout:
And here’s a tcpdump of what happened:
So, why doesn’t the load balancer just retry the request?
Well, that’s where the POST part of this whole thing becomes important—the Google Cloud HTTP(S) Load Balancer will retry failed GET requests, but will never retry failed POST requests. These retry behaviors are completely undocumented as far as I can tell, but relatively standard practice—GET requests are assumed to be idempotent, and POST requests are not.
So what timeout should we use to fix this? Unfortunately, the load balancer timeout is also undocumented on the actual Load Balancer docs. After some tcpdump research, I found that it matches the same ten minute TCP timeout noted elsewhere in GCE.
If you’re familiar with GCE, you might be thinking that we missed the tcp_keepalive_time tuning mentioned in Compute Engine: Tips, Troubleshooting, & Known Issues. We didn’t. :) This error happens regardless of the fact that we had tuned sysctl net.ipv4.tcp_keepalive_time long ago based on the docs. I haven’t completely figured out why, but I assume this is because the connection is inbound not outbound, and NGINX probably sets custom socket options that override the system defaults and then enforces it’s own keepalive_timeout regardless.
To fix this race condition, set “keepalive_timeout 650;” in nginx so that your timeout is longer than the 600 second timeout in the GCP HTTP(S) Load Balancer. This causes the load balancer to be the side that closes idle connections, rather than nginx, which fixes the race condition! (This is not a 100% accurate description for how closing TCP connections works, but it’s fair enough for here).
After setting this, we have not seen a single 502 Bad Gateway response from our APIs in weeks.
There are more nobs we turned and tested to support higher concurrency and number of connections, but I’ve gone on long enough. :)
Check out these additional resources:
Ultimately, Google Cloud Platform has provided us with a fantastically scalable and configurable platform, and these and other system tweaks helped us dramatically reduce our app machine footprint while making our APIs more reliable.
Percy is a visual testing platform that helps you deploy your UI with confidence. Learn more at https://percy.io ✨
Words, news, and stories from the people building Percy
801 
24
801 claps
801 
24
Written by

Words, news, and stories from the people building Percy
Written by

Words, news, and stories from the people building Percy
"
https://medium.com/google-cloud/gcp-the-google-cloud-platform-compute-stack-explained-c4ebdccd299b?source=search_post---------245,"There are currently no responses for this story.
Be the first to respond.
Google Cloud Platform (GCP) offers a myriad of services, one particular set of services is its compute stack that contains, Google Compute Engine (GCE), Google Kubernetes Engine (formerly Container Engine) (GKE), Google App Engine (GAE)and Google Cloud Functions (GCF). These services all have pretty cool names, but can get somewhat confusing with regards to their function and what makes them…
"
https://medium.com/@ben11kehoe/the-good-and-the-bad-of-google-cloud-run-34455e673ef5?source=search_post---------246,"Sign in
There are currently no responses for this story.
Be the first to respond.
Top highlight
Ben Kehoe
Apr 13, 2019·5 min read
This week, at Google Cloud Next, GCP announced an interesting new service: Cloud Run. My thoughts about the new Cloud Run service are a bit more complicated than this Twitter thread, so I’ve expanded on them in this blog and welcome your comments.
In this article, I’ll compare Google’s Cloud Run with AWS Lambda and API Gateway because I am most familiar with those services and provider. But my thoughts below are a general critique of Cloud Run relative to FaaS including Google Cloud Functions and managed API services in general — regardless of the provider.
Google’s Cloud Run allows you to hand over a container image with a web server inside, and specify some combination of memory/CPU resources and allowed concurrency. The logic inside your container must be stateless.
Cloud Run then takes care of creating an HTTP endpoint, receiving requests and routing them to containers, and making sure enough containers are running to handle the volume of requests. While your containers are handling requests, you are billed in 100 ms increments.
This sounds a lot like Lambda. How is it different? You are handling multiple requests within a single container. At a fundamental level, Cloud Run is just serving as a very fancy load balancer.
All of the web logic is inside your container; auth*, validation, error handling, the lot of it. But instead of just measuring resource utilization or other metrics that are a proxy for load, Cloud Run understands requests and uses that directly as a measure of load to know how to scale and route.
Note: if you’re using GCP IAM and you’re running on the managed version of Cloud Run, the service can do the auth for you. There’s no custom authorizers, nor do I expect there to be in the future, because why not just put it in your web server?
In Lambda, while you can set up API Gateway to do no validation or auth and pass everything through to your Lambda, you’d be missing out on a wealth of managed features that perform these functions for you.
So what’s good about Cloud Run?
So what’s bad about Cloud Run? Inside your container is a fully-featured web server doing all the work!
API Gateway allows you to use custom authentication. That means that your code, in a Lambda that does nothing else, can reject the request.
API Gateway allows you to perform schema validation on incoming requests. If the request fails validation, your Lambda doesn’t get invoked. Your code doesn’t have to worry about malformed requests.
Note: The API Gateway model validation is sadly a little more complicated than I’ve described above. Expect a post from myself and Richard Boyd on this topic in the near future.
The FaaS model is that each request is handled in isolation. Some people complain about this. I’ve even heard someone claim that AWS is pushing Lambda because users’ inability to optimize resource usage across requests is lucrative for them — which is about the most outlandish conspiracy theory I’ve heard this side of flat-earthers.
But the slightly less efficient usage model comes with benefits: you never have to worry about cross-talk effects. In Lambda, I don’t have to think about whether one request might have an impact on another. Everything’s isolated. This makes it easy to reason about, and removes one more thing I need to think about in the development process.
Security is hard, and the ability to scope your code’s involvement with it as small as possible is a huge win. Beyond the security implications, it’s also fewer moving parts that are your responsibility.
Cloud Run is also not the same as Lambda’s custom runtimes. Beyond the fact that custom runtimes should be a last resort, they don’t require running a server. Instead, you only need an HTTP client, which makes it more clear that what your code is doing is not acting as a tiny web server.
All this is to say that Cloud Run should not be seen as equivalent, or even analogous, to pure FaaS — Cloud Run fundamentally involves significantly more code ownership. Cloud Run is still a valid rung on the serverless ladder, but there are many more above this service.
And that gets to my biggest concern. Cloud Run, and GCP in general, are providing people with a system that is going to make them complacent with traditional architecture, and not push them to gain the immense benefits of shifting (however slowly) to service-full architecture that offloads as many aspects of an application as possible to fully managed services.
Google’s strategy is to push Kubernetes as the solution to cloud architecture. And for good reason: Kubernetes is really good at solving people’s pain points while staying within the familiar architecture paradigm. And Google is doing a great job creating a Kubernetes layer on top of every possible base infrastructure.
But Kubernetes keeps us running servers. It removes the infrastructure notion of server, but encourages us to keep running application servers, like the ones inside Cloud Run containers.
Google’s ability to put Kubernetes on-prem is going to satisfy developers, and this will potentially come at the cost of delaying organizational moves to the public cloud. The difference from an application development perspective will be less apparent and will hide the higher total cost of ownership for being on-prem.
While Cloud Run is going to enable better usage of existing web server infrastructure, it’s also going to provide a safety blanket for developers intimidated by the paradigm shift of FaaS and service-full architecture. This will further delay the shift to the more value-oriented approach to development.
Cloud Robotics Research Scientist at @iRobot
See all (209)
714 
13
714 claps
714 
13
Cloud Robotics Research Scientist at @iRobot
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/hackernoon/aws-vs-heroku-cloud-platform-comparison-for-2017-5f2194c0673e?source=search_post---------247,"There are currently no responses for this story.
Be the first to respond.
When you run a web project, no matter whether it is a small API solution or full-featured web application, you’ll reach the point of hosting your service and making it available to the world. Today, there is a great number of options, but we’ll drill down our favorites and also mention some alternatives to give you the general idea of the market.
AWS is a massive collection of cloud-computing services that build up a fully-fledged platform offered by Amazon.com since 2004. In fact, it can be called a powerhouse of databases, storage, management, analytics, networking and deployment/delivery options offered to developers. AWS Cloud is available in 16 geographic regions and it is still growing.
Currently, Amazon Elastic Compute Cloud (EC2) and Amazon Simple Storage Service (S3) are among the far-famed services, together with ever-expanding database library, load balancing, and ready-to-use deployment configuration. Among the core benefits of Amazon platform: level of control, comprehensive analytics, useful insights and ability to access the inner workings of your application.
Heroku is a cloud platform that offers an efficient and tuned place for building, deploying and scaling web applications. Its ecosystem is made up of over 140 add-ons, ranging from alerts and notifications to databases, analytics tools or security services to monitoring, caching, mailing or networking add-ons.
Herokucloud is designed to boost your team’s productivity, making the development and rolling-out processes more efficient and convenient. However, in all circumstance, when people compare AWS vs Heroku, they mainly talk about direct Heroku competitor, namely AWS Elastic Beanstalk. Likewise, it allows you to deploy and manage web apps in the AWS Cloud not worrying about the infrastructure.
Currently, cloud computing is one of the hottest topics in the IT industry. No matter whether you are just migrating to the cloud, storing files there or deploying software, more and more companies are concerning themselves with the benefits that it can offer to the businesses.
IDC forecasts that international spending on public cloud platforms will increase from about $70 billion in 2015 to over $141 billion in 2019. Why are companies moving to the cloud? Such platforms offer multiple advantages over local hosting and the top 6 benefits are:
Amazon is definitely among the leading providers of cloud services, but due to its complexity, cost and issues that took place in February 2017 and September 2015 more and more users tend to explore other alternatives. Currently Microsoft Azure and Google Cloud Platform are the biggest competitors and main alternatives to Amazon.
The popularity of Azure is still growing, especially since its team-up with Mesosphere and Docker. It offers quick and easy setup of VMs, auto-scaling based on live traffic, continuous functioning under maximum loads, support of various programming languages and operating systems, built-in continuous deployment and so on.
Google Cloud is considered as one of the most affordable solutions offering rich toolset for Big Data. It provides developers with live VMs migration, time-proven security system, redundant backup, swift performance and much more.
Talking about Heroku alternatives users mainly refer to Digital Ocean.
In 2015 Digital Ocean was named the 2nd largest hosting provider in the world and in 2016 Forbes included it in the world’s best 100 cloud companies list. DO focuses on high-level security, rich developer tools, reliable infrastructure, powerful SSD’s and comprehensive documentation.
However, the list isn’t complete. There are other hosting providers, like Apache CloudStack, RackSpace, Brightbox, SoftLayer, Codero, Media Temple, Linode, Vultr, Atlantic.net, and many others. All the solutions can be good choices, depending on your cost expectations, project size and future goals.
We live in a time when both big established companies and startups can access top-notch enterprise infrastructure for computing, storage, and management of their next innovative online services. Such an approach allows organizations to focus on business strategy, improvement of the operational process and so increase performance.
While choosing between Heroku vs AWS vs local machine, initially you should consider the costs and resources availability for machines’ administration. Running own datacenters, you take responsibility for getting them up and running, checking, updating and supporting 24/7 at any later dates. With Amazon Web Services and Heroku all the responsibility is shifted on cloud vendors.
Hacker Noon is how hackers start their afternoons. We’re a part of the @AMI family. We are now accepting submissions and happy to discuss advertising & sponsorship opportunities.
If you enjoyed this story, we recommend reading our latest tech stories and trending tech stories. Until next time, don’t take the realities of the world for granted!
#BlackLivesMatter
513 
2
how hackers start their afternoons. the real shit is on hackernoon.com. Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
513 claps
513 
2
Written by
Content Team Lead
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
Content Team Lead
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/deploying-a-node-js-app-on-google-cloud-8419de45e5dc?source=search_post---------248,"There are currently no responses for this story.
Be the first to respond.
For those looking for a way to deploy their app, the Google Cloud Platform (GCP) may not be the first choice that comes to mind. Heroku is simple and easy to use while Amazon’s AWS is by far the most popular platform in the industry. Nevertheless, GCP offers some unique tools that are worth exploring (and is offering a $300 trial credit in addition to its free tier).
It is in the best interest of software companies to offer a low barrier to entry for their products; the easier it is for someone to get started, the faster a user can explore what lies beyond the gate. Offering clear and easy-to-read documentation would go a long way to lowering these barriers. Google has provided excellent documentation for several of their APIs — however, the cloud department can be a bit lacking in clarity.
Deploying my first Node.js app on GCP presented significant frustrations. Part of it is my fault: I chose to play around with a lot of tools that aren’t completely developed. When they’re fully working, they will be awesome additions to the ecosystem. For now, one has to be aware of potential pitfalls, some of which I’ll cover below.
What drew me to GCP was the potential for a combination of Heroku’s ease of deployment (straight from Github with no CLI needed, even though one is provided) with AWS’s firepower. (Oh also, I used the Google Prediction API, which requires the Google Cloud SDK.)
GCP makes it possible to run a Cloud Shell CLI interface, edit files, and deploy an app with App Engine entirely in the browser — features that make it easier for someone to explore GCP without too much investment and our topic of exploration.
Go ahead, log into the GCP console, and create a new project!
On the right hand of the navigation bar that spans the page, you’ll find an icon to launch the Cloud Shell. This will provide you with a Cloud Engine virtual machine that runs Linux and is equipped with plenty of normal goodies (git, npm, and support for Node.js) and some that we’d otherwise have to install locally (Google App Engine SDK, Google Cloud SDK). A complete list of provided tools can be found here. On first appearance, the shell will appear on the bottom of your screen but there’s an option to pop it out into its own window.
Keep in mind that this VM is unique to your Google account, not your project. If you download a repo for one project and re-visit Cloud Shell for another project, you’ll see your old repos. This Compute Engine instance is also distinct and separate from the App Engine instance that will host your deployed app.
Go ahead and clone your repo into the VM. This is your chance to make sure you have everything before deploying! If you’ve forgotten anything below, feel free to launch Google’s experimental online file editor from the cloud shell toolbar (personally, I’m not a huge fan of VIM and the like).
What you’ll need:
2. Make sure that your package.json contains a “start” script. After your VM sets up your App Engine instance (e.g. installing any node modules you may need), it will run the this script. Don’t forget to start your server with something like node server.js.
3. (optional) If you choose to launch your server on port 8080 (or 8081–8084) in development, you can get a quick preview from the Cloud Shell. Run the start script and click the first icon in the toolbar to open the preview in a new window.
A word of caution here: the preview is unable to connect to any database, hosted on GCP or elsewhere. Since all my content was behind a login screen, it didn’t particularly help that the preview kept throwing an error whenever I tried to sign in or create a new account. If there are no errors locally, it should (hopefully) work in deployment.
4. If you choose to make any changes with the file editor, I recommend following this step. Go to the hamburger menu inside the GCP console, scroll down to the Tools subsection, hover over Development, and click Repositories.
The list of repositories will initially be empty and the option for Source Code will only be available once the app has been deployed. Eventually, the Source Code tab will show the code that is currently being deployed on App Engine.
For now, let’s create a new repository and title it “default”. This will be repo from which the app will be deployed. Since it’s empty, we can push our finalized code from the Cloud Shell VM after we’ve added our default repo as a remote.
Now we’re ready to deploy our app! Go ahead and run the magic command inside the repo folder: gcloud app deploy. You’ll be guided through a series of prompts. First, a location:
Then, confirm:
The build log begins to take shape:
If all goes well, this message will be displayed after a few minutes:
You can visit your app by running gcloud app browse or going to https://[PROJECT-ID].appspot.com (if you haven’t changed it).
Google Cloud community articles and blogs
662 
6
662 claps
662 
6
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/thinking-design/xd-essentials-the-art-of-minimalism-in-app-design-76b2c3f49ad0?source=search_post---------249,"There are currently no responses for this story.
Be the first to respond.
User interfaces look way different than they used to just a couple years ago — user interfaces (UIs) are becoming less cluttered and more contextual. Preferences are shifting toward a simpler interface, stripping the UI to its very basic, necessary elements. Most skeuomorphic patterns are in the past, and now users wish to see flat, simple, and minimal design solutions. Minimalism is at the forefront of this movement. All modern design guidelines, such a Material Design, iOS Human Interface Guidelines, and Metro design language take inspiration from minimalism.
In this article we’ll take a look at what minimalism is and go over important minimalist design techniques.
As Antoine de Saint-Exupéry once said, “Perfection is achieved, not when there is nothing more to add, but when there is nothing left to take away.“ Minimalism is a perfect combination of form and function. It follows the ”less is more” principle, where a design is stripped of everything except for absolutely essential components needed to complete a user’s task. Every element in a design is deliberate, and every element serves a purpose.
When you embrace minimalism, you seek to simplify interfaces by removing unnecessary elements or content that does not support user tasks. In order to achieve this goal, you need to focus on following:
Color is one of the most powerful tools in a designer’s toolkit. It can be used to create visual interest or direct attention without adding any additional design elements or graphics.
Simplifying the color scheme improves the user experience while having too many colors can have a negative impact upon it. Limiting the number of colors in your work doesn’t mean that you need to design in black and white, the idea is more about using only the colors necessary to accurately portray your design and create hierarchy. You can start with a simple color scheme such as monochromatic or analogous:
However, make sure your color scheme uses enough contrast to be legible to people with limited vision or color blindness.
You should use accent colors intentionally and consistently to highlight primary actions or important information. Using neutral colors for the general scheme and adding contrasting colors for calls to action helps the user focus on the action we want them to take.
Communication plays a vital role in design. The purpose of text in app is to establish a clear connection between the app and user and to help your users accomplish their goals. Typography plays a vital role in this interaction: great typography enables clear communication.
Mixing several different fonts can make your app seem fragmented and sloppy. Reducing the number of fonts on a screen can reveal the power of typography. When designing an app think about how can you make the typography powerful by playing with weight, style, and size, not different typefaces.
Unless your app has a compelling need for a custom font, such as for branding purposes, stick with the platform’s default font:
Use font weight and size to highlight the most important information in your app. Increased font size draws the users’ attention to a particular area of the screen without additional visual hints. As a result, users can access information more quickly and more easily. However keep in mind that drawing attention to bold typography is only useful when that text communicates meaningful information.
Traditional dividers such as horizontal or vertical lines might break up content well on a desktop screen, but they have one big disadvantage for mobile apps — they take up valuable space on mobile screen. A heavy use of dividers can also lead to visual noise and dense, crowded interfaces.
Less lines and dividers will give your interface a cleaner, modern and more functional feel. There are other ways to separate content with methods such as using whitespacing or elevation.
Whitespace is the areas of a design where there is no element placed by a designer. Whitespace has been called “the backbone of” minimalist interfaces. Generous whitespace can make some of the messiest interfaces look inviting and simple — it creates the spaces around elements in the design to help them stand out or separate from the other elements.
Shadows and elevation aren’t only able to create some depth in the UI, but they also can visually separate content sections. Calendar App from Google is a good example of how leveraging space and using shadows instead of drawing lines helps to define different sections in a non-obtrusive manner.
Iconography is a visual language used to represent functionality or content. Icons are meant to be simple, visual elements that are recognized and understood immediately.
In minimalism, icons are creating by reducing any elements that don’t contribute meaningful information. That’s why most icons are flat textures. However, the primary purpose of an icon is to be understandable, it’s really important to be sure that removing even a tiny specific detail won’t hurt usability. For example, this is how the Home icon on Kindle devices has evolved recently:
Amazon decided to simplify this icon and remove some graphical details step-by-step, making the third version hard to recognize at first glance.
Since iOS 7 many minimalist UI has stroke and filled icons. A single icon can have both solid and hollow characteristics.
This pair of icons works great as navigation tab bar icons: these are the icons that you usually see in a row of four or five at the bottom of the screen in mobile apps. Because bar icons serve as navigation to other sections of the app, it’s important to indicate which section is currently active by highlighting its icon in some way: a solid version to show an active/selected state and a hollow version to show an inactive/unselected state. This makes recognition of active tabs and controls more straightforward.
Minimalist interfaces design techniques are certainly a way to achieve good design, but they are not the goal. The ultimate goal is to simplify our interfaces and make them more functional and usable at the same time. Design, like language, is defined by the way people use it. That’s why minimalist design can be a powerful tool, but only when it’s framed by the needs of your users.
Your app should prioritize function over the form, and aim to address problems for your users through clear visual communication. Beautiful minimalist apps combined with great usability create a seamless interaction.
Nick Babich is a developer, tech enthusiast, and UX lover. He has spent the last 10 years working in the software industry with a specialized focus on development. He counts advertising, psychology, and cinema among his myriad interests.
Originally published at blogs.adobe.com.
More about Adobe XD:
Stories and insights from the design community.
169 
6
169 claps
169 
6
Stories and insights from the design community.
Written by
New Tools for New Creatives. Get all the latest creative apps plus seamless ways to share and collaborate. All right on your desktop.
Stories and insights from the design community.
"
https://medium.com/@sathishvj/notes-from-my-google-cloud-professional-data-engineer-exam-530d11966aa0?source=search_post---------250,"Sign in
There are currently no responses for this story.
Be the first to respond.
sathish vj
Jan 8, 2019·6 min read
Subscribe to my YouTube channel that teaches you to apply Google Cloud to your projects and also prepare for the certifications: youtube.com/AwesomeGCP. Check out the playlists I currently have for Associate Cloud Engineer, Professional Architect, Professional Data Engineer, Professional Cloud Developer, Professional Cloud DevOps Engineer, Professional Cloud Network Engineer, and Professional Cloud Security Engineer.
Immediately after the exam I do a memory dump as notes. Hence it is also quite unordered. This is a sanitized list that gives general topics and questions I encountered. The intention is not to give you the questions, but to give you topics that you can be prepared for. I was often stumped by some questions; hopefully you can be more prepared based on my experience. Wish you the very best!
Tough exam. I assumed this one would be easier because I spent more time preparing and I had the experience of the previous certifications. After the exam I went over the questions again to remind myself later what areas were covered — the answer is, everything. Zero direct questions. Every question was embedded in a situation/use case.
The Data Engineer exam was refreshed on March 29th. These are some extracted key points and links that others have posted. From what I am reading of others’ notes
Notes
Posts
Google Cloud Certified — Professional Data Engineer
For those appearing for the various certification exams, here is a list of sanitized notes (no direct question, only general topics) about the exam.
Overall notes across all GCP certification exams
Notes from the Professional Cloud Architect exam
Notes from the beta Professional Cloud Developer exam
Notes from the Professional Data Engineer exam
Notes from the Associate Cloud Engineer exam
Notes from the beta Professional Cloud Network Engineer Exam
Notes from the beta Professional Cloud Security Engineer Exam
Notes from the Professional Collaboration Engineer Exam
Notes from the G Suite Exam
Notes from the Professional DevOps Engineer Exam
Notes from the Professional Machine Learning Engineer Exam
Main Link — https://cloud.google.com/certification/data-engineer
Topics Outline — https://cloud.google.com/certification/guides/data-engineer/
Practice Exam —https://cloud.google.com/certification/practice-exam/data-engineer
A collection of posts, videos, courses, qwiklabs, and other exam details for all exams: https://github.com/sathishvj/awesome-gcp-certifications
I’ve collected here a bunch of free Qwiklabs codes which are awesome to get lots of hands-on practice. Use them well.
medium.com
Check the FAQs here: https://medium.com/@sathishvj/frequently-asked-follow-up-questions-on-google-cloud-gcp-certifications-438e1addb91d.
Wish you the very best with your GCP certifications. You can reach me at LinkedIn and Twitter. If you can support my work creating videos on my YouTube channel AwesomeGCP, you can do so on Patreon or BuyMeACoffee.
tech architect, tutor, investor | GCP 12x certified | youtube/AwesomeGCP | Google Developer Expert | Go
449 
3
449 
449 
3
tech architect, tutor, investor | GCP 12x certified | youtube/AwesomeGCP | Google Developer Expert | Go
"
https://read.acloud.guru/the-true-cost-of-cloud-a-comparison-of-two-development-teams-edc77d3dc6dc?source=search_post---------251,"Over the last few months, I’ve talked with a number people who are concerned about the average cost of public cloud services in comparison to the price of traditional on-premises infrastructure. To provide some insights from the discussion, let’s follow two development teams within an enterprise — and compare how they would approach building a similar service.
The first team will deploy their application using traditional on-premises infrastructure, while the second will leverage some of the public cloud services available on AWS.
The two teams are being asked to develop a new service for a global enterprise company that currently serves millions of consumers worldwide. This new service will need to meet theses basic requirements:
As far as new services go, this seems to be a fairly standard set of requirements — nothing that would intrinsically favor either traditional on-premises infrastructure over the public cloud.
When it comes to scalability, this new service needs to scale to meet the variable demands of consumers. We can’t build a service that might drop requests and cost our company money and cause reputational risk.
The Traditional TeamWith on-premises infrastructure, the architectural approach dictates that compute capacity needs to be sized to match your peak data demands. For services that features a variable workload, this will leave you with a lot of excess and expensive compute capacity in times of low utilization.
This approach is wasteful — and the capital expense will eat into your profits. In addition, there is a heavy operational costs with maintaining your fleet of underutilized servers. This is a cost that is often overlooked — and I cannot emphasize enough how much money and time will be wasted supporting a rack of bare-metal servers for a single service.
The Cloud TeamWith a cloud-based autoscaling solution, your application scales up and down in-line with demand. This means that you’re only paying for the compute resources that you consume.
A well-architected cloud-based application enables the act of scaling up and down to be seamless — and automatic. The development team defines auto-scaling groups that spin up more instances of your application based on high-CPU utilization, or a large number of requests per second, and you can customize these rules to your heart’s content.
When it comes to resiliency, hosting a service on infrastructure that resides within the same four walls isn’t an option. If your application resides within a single datacenter — then you are stuffed when (not if) something fails.
The Traditional TeamTo meet basic resiliency criteria for an on-premises solution, this team would a minimum of two servers for local resiliency — replicated in a second data center for geographic redundancy.
The development team will need to identify a load balancing solution that automatically redirects traffic between sites in the event of saturation or failure — and ensure that the mirror site is continually synchronized with the entire stack.
The Cloud TeamWithin each of their 50 regions worldwide, AWS provides multiple availability zones. Each zone consists of one of more fault-tolerant data centers — with automated failover capabilities that can seamlessly transition AWS services to other zones within the region.
Defining your infrastructure as code within a CloudFormation template ensures your infrastructure resources remain consistent across the zones during autoscaling events — and the AWS load balancer service requires minimal effort to setup and manage the traffic flow.
Security is be a fundamental requirement of any system being developed within an organization. You really don’t want to be one of the unfortunate companies dealing with fallout from a security breach.
The Traditional TeamThe traditional team will incur the ongoing costs of ensuring that the bare-metal servers that’s running their services is secure. This means investing in a team that is trying to monitor, identify, and patch security threats across multiple vendor solutions from a variety of unique data sources.
The Cloud TeamLeveraging the public cloud does not exempt yourself from security. The cloud team still has to remain vigilant, but doesn’t have to worry about patching the underlying infrastructure. AWS actively works combat against zero-day exploits — most recently with Spectre and Meltdown.
Leveraging the identify management and encryption security services from AWS allows the cloud team to focus on their application — and not the undifferentiated security management. The API calls to AWS services are fully audited using CloudTrail, which provides transparent monitoring.
Every infrastructure and application service being deployed need to be closely monitored with aggregated realtime data. The teams should have access to dashboards that provide alerts when thresholds are exceeded, and offer the ability to leverage logs for event correlation for troubleshooting.
The Traditional TeamFor traditional infrastructure, you will have to set up monitoring and alerting solutions across disparate vendors and snowflake solutions. Setting this up takes a hell of a lot of time — and effort and getting it right is incredibly difficult.
For many applications deployed on-premises, you may find yourself searching through log files stored on your server’s file-system in order to make sense of why your application is crashing. A lot of time will be wasted as teams need to ssh into the server, navigate to the unique directory of log files, and then grep through potentially hundreds of files. Having done this for an application that was deployed across 60 servers — I can tell you that this isn’t a pretty solution.
The Cloud TeamNative AWS services such as CloudWatch and CloudTrail make monitoring cloud applications an absolute breeze. Without much setup, the development team can monitor a wide variety of different metrics for each of the deployed services — making the process of debugging issues an absolute dream.
With traditional infrastructure, teams need build their own solution, and configure their REST API or service to push log files to an aggregator. Getting this ‘out-of-the-box’ is an insane improvement fo productivity.
The ability to accelerate time-to-market is increasingly important in today’s business environment. The lost opportunity costs for delayed implementations can have a major impact on bottom-line profits.
The Traditional TeamFor most organizations, it takes a long time to purchase, configure and deploy hardware needed for new projects — and procuring extra capacity in advance leads to massive waste due to poor forecasting.
Most likely, the traditional development team will spend months working across the myriad of silos and hand-offs to create the services. Each step of the project will require a distinct work request to database, system, security, and network administrators.
The Cloud TeamWhen it comes to developing new features in a timely manner, having a massive suite of production-ready services at the disposal of your keyboard is a developer’s paradise. Each of the AWS services is typically well documented and can be accessed programmatically via your language of choice.
With new cloud architectures such a serverless, development teams can build and deploy a scalable solution with minimal friction. For example, in just a couple of days I recently built a serverless clone of Imgur that features image recognition, a production-ready monitoring/logging solution built-in, and is incredibly resilient.
If I had to engineer the resiliency and scalability myself, I can guarantee you that I would still be developing this project — and the final product would have been nowhere near as good as it currently is.
Using serverless architecture, I experimented and delivered the solution in a less time that it takes to provision hardware in most companies. I simply glued together a series of AWS services with Lambda functions — and ta-da! I focused on developing the solution, while the undifferentiated scalability and resiliency is handled for me by AWS.
When it comes to scalability, the cloud team is the clear winner with demand-drive elasticity — thus only paying for compute power that they need. The team no longer needs dedicated resources devoted to maintaining and patching the underlying physical infrastructure.
The cloud also provides development teams a resilient architecture with multiple availability zones, security features built into each service, consistent tools for logging and monitoring, pay-as-you-go services, and low-cost experimentation for accelerated delivery.
More often than not, the absolute cost of cloud will amount to less than the cost of buying, supporting, maintaining, and designing the on-premises infrastructure needed for your application to run — with minimal fuss.
By leveraging cloud we can move faster and with the minimal upfront capital investment needed. Overall, the economics of the cloud will actively work in your favor when developing and deploying your business services.
There will always be a couple of niche examples where the cost of cloud is prohibitively more expensive than traditional infrastructure, and a few situations where you end up forgetting that you have left some incredibly expensive test boxes running over the weekend.
Dropbox saved almost $75 million over two years by building its own tech infrastructure — After making the decision to roll its own infrastructure and reduce its dependence on Amazon Web Services, Dropbox… — www.geekwire.com
However, these cases remain few and far between. Not to mention that Dropbox initially started out life on AWS — and it was only after they had reached a critical mass that they decided to migrate off the platform. Even now, they are overflowing into the cloud and retain 40% of their infrastructure on AWS and GCP.
The idea of comparing cloud services to traditional infrastructure-based of a single “cost” metric is incredibly naive — it blatantly disregards some of the major advantages the cloud offers development teams and your business.
In the rare cases when cloud services result in a greater absolute cost than your more traditional infrastructure offering — it still represents a better value in terms of developer productivity, speed, and innovation.
Master modern tech skills, get certified, and level up your career. Whether you’re starting out or a seasoned pro, you can learn by doing and advance your career in cloud with ACG.
I’m very interested in hearing your own experiences and feedback related to the true cost of developing in cloud! Please drop a comment below, on Twitter at @Elliot_F, or connect with me directly on LinkedIn.
Get more insights, news, and assorted awesomeness around all things cloud learning.
"
https://medium.com/@imrenagi/ekstraksi-informasi-e-ktp-dengan-google-cloud-function-dan-cloud-vision-api-4655db21d084?source=search_post---------252,"Sign in
There are currently no responses for this story.
Be the first to respond.
Imre Nagi
Dec 26, 2018·6 min read
Semenjak satu atau dua tahun terakhir, alat pembayaran digital seperti seperti Go-pay, OVO, Traveloka PayLater, Kredivo dan lain-lain mulai digandrungi oleh banyak pengguna smartphone di Indonesia. Alasannya jelas karena lebih simple <coret> dan banyak promo-promo menarik </coret>. Namun sayangnya untuk mendapatkan fitur-fitur premium dari metode pembayaran ini pengguna harus melakukan verifikasi dengan cara mengunggah identitas diri mereka ke layanan-layanan tersebut.
Verifikasi ini pada umumnya dilakukan dengan menguggah identitias diri seperi foto Kartu Tanda Penduduk Elektronik (e-KTP), Surat Izin Mengemudi (SIM), atau passport. Jika seorang pengguna ingin melakukan verifikasi, tentu perusahaan pembayaran digital tersebut harus memastikan apakah data yang diunggah benar atau salah. Jika benar, maka pengguna akan mendapatkan fitur premium dan jika salah, mohon maaf Anda belum beruntung dan silahkan coba lagi. :p
Kini bayangkan bahwa dalam satu hari ada ratusan ribu pengguna yang ingin melakukan verifikasi dan mengunggah identitasnya. Jika dibalik layar ada seseorang atau beberapa orang yang harus melakukan verifikasi dan mencocokan data yang terdaftar dengan data identitas tersebut, bayangkan berapa man-hour yang dibutuhkan untuk menuntaskan semua verifikasi tersebut? Bisakah kita mengautomasi pekerjaan ini?
Tulisan ini adalah ringkasan demo yang saya lakukan di acara Google DevFest 2018 di Jakarta dan Semarang. Demo yang dilakukan adalah bagaimana cara mengekstrak informasi e-KTP seperti Nomor Induk Kependudukan (NIK), nama, alamat, nama provinsi dan kabupaten atau kota, dari gambar e-KTP yang diunggah ke sebuah sistem.
Slide presentasi bisa diunduh dari Slideshare. Presentasi bisa ditonton di Youtube.
Dalam demonstrasi ini dua teknologi utama yang saya gunakan adalah serverless dengan Google Cloud Function dan Optical Character Recognition dengan Google Cloud Vision API. Mari kita bahas satu persatu.
Berdasarkan definisi Martin Fowler,
Serverless architectures are application designs that incorporate third-party “Backend as a Service” (BaaS) services, and/or that include custom code run in managed, ephemeral containers on a “Functions as a Service” (FaaS) platform. By using these ideas, and related ones like single-page applications, such architectures remove much of the need for a traditional always-on server component
Dari definisi diatas, kita bisa mengambil intisari bahwa arsitektur serverless pada dasarnya memungkinkan kita untuk dapat menjalankan baris kode tanpa harus peduli dengan kebutuhan akan sebuah server. Jika sebelumnya, sebuah aplikasi berbasis web-server harus dijalankan di sebuah server yang dikelola oleh seorang system administrator, kini dengan teknologi serverless, seorang pengembang piranti lunak dapat melakukan deployment secara langsung tanpa harus menyewa sebuah server di salah satu penyedia layanan cloud. Yang perlu dilakukan hanyalah membuat sebuah fungsi dan menjalankan sebuah perintah untuk menjalankan fungsi tersebut di cloud (Functions as a Service).
Sementara itu, Cloud Function merupakan salah satu layanan FaaS yang disediakan oleh Google Cloud Platform. Saat ini Google Cloud Function baru mendukung bahasa pemograman Javascript (Node.JS) dan Python (versi beta). Disamping itu, Cloud Function juga dapat diintegrasikan dengan berbagai macam layanan lain milik GCP seperti Google Cloud Storage, Cloud Pub/Sub, dan lain-lain. Pada demonstrasi kali ini, kita akan menggunakan Google Cloud Function untuk membuat beberapa fungsi, antara lain:
Percayalah bahwa sebenarnya hal-hal tersebut dapat dilakukan hanya dengan menggunakan satu fungsi. Tapi untuk membatasi cakupan dan tanggung jawab dari masing-masing fungsi, maka TS berpikir akan lebih baik jika fungsi-fungsi tersebut dibuat terpisah.
Optical Character Recognition atau OCR merupakan salah satu teknik pengolahan citra digital yang sudah umum digunakan untuk mendapatkan teks yang terkandung dalam sebuah citra atau gambar. Namun, sayangnya OCR bukanlah sebuah teknik atau algoritma yang dapat dipahami dalam satu malam.
Namun, beruntunglah kita karena GCP memiliki Cloud Vision API yang memiliki fitur untuk melakukan OCR. Selain OCR, Google Cloud Vision API juga memiliki fitur lain seperti Label Detection, Multiple Object Detection, dan lain-lain. 😊
Berikut adalah diagram rancangan sistem yang digunakan untuk mengekstrak informasi e-KTP:
Dalam rancangan diatas, ada beberapa komponen dengan peran yang beerbeda-beda seperti Cloud Function, Cloud Storage dan Cloud Pub/Sub.
Fungsi untuk mengunggah e-KTP ini adalah satu-satunya tatap muka yang digunakan langsung oleh pengguna. Fungsi ini menggunakan sistem http-trigger yang berarti bahwa eksekusi dari fungsi ini akan terjadi jika pengguna melakukan request call ke http endpoint fungsi tersebut.
Fungsi ini memiliki tugas sebagai berikut:
Berikut baris kode yang digunakan:
Untuk melakukan deployment fungsi berbasis http-trigger, kita cukup menambahkan argument --trigger-http ketika fungsi tersebut di deploy ke google cloud. Sebagai contoh:
Apabila deployment berhasil, maka Google Cloud Function akan menghasilkan sebuah URL (e.g. “https://REGION-PROJECT_ID.cloudfunctions.net/uploadFile”) yang dapat digunakan untuk mengunggah foto e-KTP.
Fungsi kedua sedikit berbeda dibandingkan dengan fungsi sebelumnya. Fungsi ini memiliki dua tugas utama:
Berikut baris kode yang digunakan:
Fungsi diatas dapat di deploy dengan menggunakan perintah berikut:
Dengan perintah tersebut, fungsi ini akan dieksekusi ketika ada penambahan berkas baru di dalam GCS (event google.storage.object.finalize). Jika kita tertarik dengan event-event lain, berikut beberapa opsi lain yang bisa digunakan:
Untuk lebih detail, silahkan baca disini.
Sayangnya hasil pemrosesan dari Cloud Vision API belum bisa menentukan teks mana yang merupakan data penting dari e-KTP. Berikut hasil pemrosesan yang dilakukan oleh Cloud Vision API:
Dari hasil diatas, bisa dilihat bahwa hasil ekstraksi hanyalah berupa sebuah string dengan separator tertentu seperti line break. Oleh karena itu, fungsi ini memiliki tugas utama untuk mengambil informasi seperti nama, tempat tanggal lahir, NIK dll dari hasil pemrosesan Cloud Vision API yang dikirim melalui Google Cloud Pub/Sub.
Pada fungsi diatas, ekstraksi informasi dilakukan dengan cara membuang semua label e-KTP dengan regular expression. Setelah label dihapus, maka nama provinsi, kota, NIK, nama dan tanggal ulang tahun akan berada pada posisi yang berututan. Simpel?
Deploy fungsi dengan perintah berikut:
Sample e-KTP yang kita gunakan 😂
Hasil pemrosesan:
TIL:
Google Developer Expert, Cloud Platform Engineer @gojek
See all (253)
473 
6
473 claps
473 
6
Google Developer Expert, Cloud Platform Engineer @gojek
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/6d-ai/dawn-of-the-ar-cloud-1b31eb4b52ac?source=search_post---------253,"There are currently no responses for this story.
Be the first to respond.
Top highlight
Since Apple’s WWDC conference this time last year, which fired the starting gun for consumer AR with the launch of ARKit, we’ve seen every big platform announce an AR strategy: Google’s ARCore; Facebook’s camera platform; Amazon Sumerian; and Microsoft continuing to build out its Mixed Reality ecosystem. We’ve also seen thousands of developers experiment with AR Apps but very little uptake with consumers. Back in Sept 2017, I predicted that AR Apps will struggle for engagement without the AR Cloud, and this has certainly turned out to be the case. However we are now witnessing the dawn of the cloud services which will unlock compelling capabilities for AR developers, but only if cloud providers get *their* UX right. It’s not about being first to market, but first to achieving a consumer grade UX.
Does anyone remember AR before ARKit & ARCore? It technically worked, but the UX was clunky. You needed a printed marker or to hold & move the phone carefully to get started, then it worked pretty well. Nice demo videos were made showing the final working experience which wow’ed people. The result…. zero uptake. Solving the technical problem (even if quite a hard technical problem) turned out to be very different to achieving a UX that consumers could use. It wasn’t until ARKit was launched that a “just works” UX for basic AR was available (and this was 10 years after Mobile SLAM was invented in the Oxford Active Vision Lab which Victor Prisacariu, my 6D.ai cofounder, leads).
We are entering a similar time with the AR Cloud. The term came about in a September 2017 conversation between Ori Inbar and I as a way to describe a set of computer vision infrastructure problems that needed to be solved in order for AR Apps to become compelling. After a number of early startups saw the value in the term (and more importantly the value of solving these problems), we are now seeing the largest AR platforms start to adopt this language in recognition of the problems being critically important. I’m hearing solid rumors Google won’t be the last $multi-billion company to adopt AR Cloud language in 2018.
Multi-player AR (and AR Cloud features in general) has the same challenges as basic 6DoF AR. unless the UX is nailed, early enthusiast developers will have fun building & making demo videos, but users won’t be bothered to use it. I’ve built multi-player AR systems several times over the last 10 years, and worked with UX designers on my teams to user test the SLAM aspects of the UX quite extensively. It wasn’t that hard to figure out what the UX needed to deliver:
So… putting aside all the application-level aspects of a multi-player UI (such as the lobby buttons & selector list to choose to join the game), the SLAM-synch piece isn’t just a checkbox, it’s a UX in and of itself. If that UX doesn’t deliver on “just works” then users won’t even bother to get to the app-level a second time. They will try once out of curiosity, though….Which means that market observers shouldn’t pay attention to AR app downloads or registered users, but to repeat usage.
Enabling developers to build engaging AR Apps is where AR Cloud companies need to focus, by solving the hard technical problems to enable AR-First apps that are native to AR. This means (as I have learnt painfully several times) that UX comes first. Even though we are a deep-tech computer vision company, the UX of the way those computer vision systems works is what matters, not whether they work at all.
At Google I/O last week, Google announced an update to ARCore 1.2, which included a handful of new features, the most notable was support for AR Multiplayer via technology called “Cloud Anchors”. Everyone registered the headlines… “Multiplayer” and “IOS support” but there wasn’t any discussion (or even any real demo) of the Cloud Anchor UX (there were some nice looking demos of multiplayer running *after* the setup UX was completed… funny that).
So how does Google do AR Mutiplayer? (Note: I don’t have any special insight into the algorithms Google is using, however I’ve built AR multiplayer systems in the past, and at a high level the steps are clear & well known. It’s deep in the relocalization algorithms themselves where the advances are taking place). These are the high level steps involved:
So far so good. There’s nothing really surprising or technically impressive here. In fact, Google has been able to do this for several years via Tango’s ADF files (which are a form of Anchor), though it was a manual process.
One thing that really stood out to me, having worked on these problems for so many years, is that Google didn’t talk about the Multiplayer UX at all. Only that the technology for Multiplayer exists. Demonstrating an understanding of why devs & end users have struggled with multi-player would have allowed them to show how those UX problems have been solved vs the technology problems.
It’s important to point out that I can’t claim to be completely impartial wrt this write up as my startup 6D.ai is building a similar service .
After spending a couple of days using Google’s Cloud Anchors, and talking with other experts who did their own testing, we were able to get a good handle on the UX, and the limits of the system
The first UX challenge, and by far the biggest one for anyone working on building solutions to these problems, is that there is no pre-existing map data for the scene, and thus “Player 1” needs to pre-scan the scene to gather image data in order to build an anchor.
Here’s what we learned:
Even after this experimenting, there’s still a couple of things we don’t know:- exactly what data is passed up, and why does it need to be discarded? Google is carefully vague here. A journalist friend described it as “twisting themselves into a privacy pretzel”.- what about China or private premises (like a military base)? Google cloud services are unavailable in China, and Cloud Anchors seem to depend completely on accessing Google’s cloud (ie no offline mode).
I’m also curious around what this “100% cloud” approach portends for the future direction of AR Core, as persistence & occlusion & semantics move closer towards public release in the next couple of years.
When it comes to Google’s Cloud Anchors, visual image data is sent up to Google’s servers. It’s a reasonably safe assumption that this can potentially be reverse engineered back into personally identifiable images (Google was carefully vague in their description, so I’m assuming that’s because if it was truly anonymous they would have said so clearly)
For the future of the AR Cloud’s ability to deliver persistence & relocalization, visual image data should never leave the phone, and in fact never even be stored on the phone. My opinion is that all the necessary processing should be executed on-device in real-time. With the users permission, all that should be uploaded is the post-processed sparse point map & feature descriptors which cannot be reverse engineered. An interesting challenge that we (and others) are working through is that as devices develop the ability to capture, aggregate & save dense point clouds, meshes and photorealistic textures, there is more & more value in the product the more “recognizable” the captured data is. We believe this will require new semantic approaches to 3D data segmentation & spatial identification, in order to give users appropriate levels of control over their data, and is an area our Oxford research group is exploring.
Here’s what a sparse point map looks like for the scene above (note our system selects semi-random sparse points, not geometric corners & edges, which cannot be meshed into a recognizable geometric space)
The second piece of the puzzle is the “feature descriptors” which are saved by us & also Google in the cloud. Google has previously said that the Tango ADF files, which ARCore is based on, can have their visual feature descriptors reverse engineered with deep learning back into a human-recognizable image (From Tango’s ADF documentation — “it is in principle possible to write an algorithm that can reconstruct a viewable image”). Note I have no idea if ARCore changed the Anchor spec from Tango’s ADF enough to change this fact, but Google has been clear that ARCore is based upon Tango, and changing the feature descriptor data structure is a pretty fundamental change to the algorithm.
This is critical because for AR content to be truly persistent, there needs to be a persistent cloud-hosted data model of the real-world. And the only way to achieve this commercially is for end-users to know that that description of the real world is private and anonymous. Additionally I believe access to the cloud data should be restricted by requiring the user to be physically standing in the place the data mathematically describes, before applying the map to the application.
This reality regarding AR Cloud data creates a structural market problem for all of today’s major AR platform companies, as Google and Facebook’s (and others) business models are built on applying the data they collect to better serve you ads. The platforms such as Apple & Microsoft are silos, so won’t offer a cross-platform solution, and also won’t prioritize cloud solutions where a proprietary on-device P2P solution is possible.
The one factor that I had underestimated is that large developers & partners clearly understand the value of the data generated by their apps, and they do not want to give that data away to a big platform for them to monetize. They either want to bring everything in house (like Niantic is doing) or work with a smaller partner who can deliver technology parity with the big platforms (no small ask) and who also can guarantee privacy and business model alignment. AR is seen as too important to give away the data foundations. This is a structural market advantage that AR Cloud startups have, and is an encouraging sign for our forseeable future.
As ARKit announced the dawn of AR last year, we believe Google’s Cloud Anchors are announcing the dawn of the AR Cloud. AR Apps will become far more engaging, but only if AR Cloud providers deliver a “just works” computer vision UX and address some challenging & unique privacy problems.
At 6D.ai we are thinking slightly differently than Google (and everyone else to be honest). We believe that persistence is foundational, and you can’t have persistence without treating privacy seriously. And to treat privacy seriously it means that personally identifying information cannot leave the device (unless explicitly allowed by the user). This creates a much harder technical problem to solve, as it means building & searching a large SLAM map on device, and in real-time. This is technically easy-ish to do with small maps/anchors, but very very hard to do with large maps. Where small means 1/2 a room, and large means bigger than a big house.
Fortunately we have the top AR research group from the Oxford Active Vision Lab behind 6D.ai, and we built our system on a next-generation relocalizer algorithm, taking advantage of some as-yet unpublished research. The goal for all of this was to get multi-player and persistent AR as close as possible to a “just works” user experience, where nothing needs to be explained, and an end-users intuition about how the AR content should behave is correct. There’s no special “Host/Resolve” steps or “manually enter a room number” or “trust us, your data is personally identifiable, so we throw it away…. but we need it for the system to work…”
Here’s what’s special about how 6D.ai supports maps/anchors for multi-player and persistence:
6D.ai’s relocalization algorithm, to the best of our knowledge from our Oxford Research Lab, cannot have its map or feature descriptors reverse engineered into identifiable visual data based on any research available today. We will continue to ensure our data structures are updated as new research comes to light. Even if you hacked into our system and figured out our algorithm then applied huge compute resources to reverse engineer the data, the best you would get would be the image way above of the circular feature descriptors (which correspond to the desk scene also above). In addition, the source wifi network ID is encrypted on the phone and only the encrypted info is stored, so the hacker at best gets the point cloud & these feature descriptors but has no way of telling where on earth it corresponds to. This also means that we can’t determine the images used to construct the 3D cloud data, even if we wanted to or were requested to by a govt etc, as the 6D cloud never sees the source images or has any way to reconstruct them.
I’ll be honest, when I heard that Google would be launching multiplayer, I had some fears (founder paranoia). It was the first-time some of the hypotheses we had founded 6D.ai upon were going to be tested. Would we have better technology, better focus on developers needs and a better understanding of the desired end-user experience? I’m pleased to say on all those fronts we are looking good.
Obviously, Google has distribution power & it’s a default option for Android devs. But the biggest market problem right now is that developers don’t know about AR multi-player at all, and Google has an ability to invest in educating the market that no startup can beat. We expect that once devs start testing multi-player and start investing real time & money into multi-player apps, then they will see how Cloud Anchors come up short, and see how 6D.ai solves their problems.
6D.ai is building advanced APIs for the AR Cloud. Our closed beta is rapidly taking on new developers.
Building the 3D Map of the World
790 
6
790 claps
790 
6
Written by
CEO @6D_ai, @Super_Ventures. Building the AR Cloud
Building the 3D Map of the World
Written by
CEO @6D_ai, @Super_Ventures. Building the AR Cloud
Building the 3D Map of the World
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@jun.chenying/flutter-tutorial-part3-push-notification-with-firebase-cloud-messaging-fcm-2fbdd84d3a5e?source=search_post---------254,"Sign in
There are currently no responses for this story.
Be the first to respond.
Ying Chen
Apr 20, 2019·5 min read
In this tutorial, I will show you how to integrate Flutter with Firebase Cloud Messaging.
Create a new flutter project
Install the flutter firebase cloud messaging plugin firebase_messaging 4.0.0+3
pub.dartlang.org
Add this to your package’s pubspec.yaml file:
Then get the package
For iOS integration, we need to create a certificate required by Apple.
Login into your Apple developer account, go to Certificates, Identifiers & Profiles to create the authentication key.
Click continue and confirm button.
Download your key.
Create a new Firebase app and add an iOS app.
Download google service json file GoogleService-Info.plist
Open ios/Runner.xcworkspace
Select automatic signing.
Copy GoogleService-Info.plist into project
In Xcode, select Runner in the Project Navigator. In the Capabilities Tab turn on Push Notifications and Background Modes, and enable Background fetch and Remote notifications under Background Modes.
Upload your APNs authentication key to Firebase. If you don’t already have an APNs authentication key, see Configuring APNs with FCM.
The FCM SDK performs method swizzling in two key areas: mapping your APNs token to the FCM registration token and capturing analytics data during downstream message callback handling. Developers who prefer not to use swizzling can disable it by adding the flag FirebaseAppDelegateProxyEnabled in the app’s Info.plist file and setting it to NO (boolean value). Relevant areas of the guides provide code examples, both with and without method swizzling enabled.
Github source code:
github.com
You might not get a notification in a simulator. You might get the following error. So let’s test it on a physical device.
5.17.0 — [Firebase/Messaging][I-FCM012002] Error in application:didFailToRegisterForRemoteNotificationsWithError: remote notifications are not supported in the simulator
Run Xcode to deploy the app to the physical device.
Refer to the Firebase documentation about FCM for all the details about sending messages to your app. When sending a notification message to an Android device, you need to make sure to set the click_action property of the message to FLUTTER_NOTIFICATION_CLICK. Otherwise the plugin will be unable to deliver the notification to your app when the users clicks on it in the system tray.
For testing purposes, the simplest way to send a notification is via the Firebase Console. Make sure to include click_action: FLUTTER_NOTIFICATION_CLICK as a ""Custom data"" key-value-pair (under ""Advanced options"") when targeting an Android device. The Firebase Console does not support sending data messages.
Alternatively, a notification or data message can be sent from a terminal:
Remove the notification property in DATA to send a data message.
You will find the message displays on the screen.
Download Source code:
github.com
References:
github.com
firebase.google.com
medium.com
medium.com
Thanks for reading!
Software Engineer
597 
6
597 
597 
6
Software Engineer
"
https://medium.com/backticks-tildes/react-native-image-upload-to-firebase-cloud-storage-ios-24b8dc0a9c8b?source=search_post---------255,"There are currently no responses for this story.
Be the first to respond.
In this tutorial, we will go through how to upload images to Firebase Cloud Storage in React Native applications.
React Native
React Native is a Javascript framework by Facebook used to build cross mobile native applications(iOS, Android, and Windows). We will build an example app for iOS. You can read more about React Native here.
"
https://expertise.jetruby.com/firebase-cloud-storage-how-to-and-use-case-dd9810fe041b?source=search_post---------256,"Firebase Cloud Storage is a modern technology that allows for storing and managing various media content generated by mobile app users.
One of its biggest advantages is reliability. Firebase SDK for Cloud Storage works regardless of the network quality. in other words, if a file stops uploading because of slow Internet connection, the process will be automatically restarted. For regular users, this feature saves minutes of waiting.
From a security viewpoint, Firebase Cloud Storage is doing great. Firebase SDK for Cloud Storage can be integrated with Firebase Authentication which provides a simple and user-friendly mechanism of authentication for developers. It allows you to use a declarative security model to enable access based on the name of a file, its size, content type, and other kinds of metadata.
Firebase Cloud Storage SDK also uses a declarative data protection language which allows you to manage the access to the files in the storage and make them public or private.
In plain English, Cloud Storage takes full care of protecting the files you upload.
First, you need to register your application in Firebase Console. When that’s done, add the generated file “google-services.json” to the app.
Go to the “Rules” section and enable the ability to read from and write into the storage. In future, you should provide access only to authorized users.
firebase.google.com
Now you need to create Storage. All the files will be stored in Bucket and organized in the form of a tree structure (just like the file system on a hard drive). To get access to a specific file, you need to create a link to it that can be used for uploading or editing the metadata and removing the files.
Now let’s connect Google Services to build.gradle(Project):
After that, we need to connect Storage SDK to build.gradle(Module):
In the app, we’ll pick am image and send it to Storage. To pick an image, we’ll use Intent. The Uri of the image will be stored in onActivityResult.
Now let’s create Firebase Storage:
To upload the file to Firebase Storage, we need to create a link to the file “img/fileUri”. This will create a child object “child(“img/${fileUri.lastPathSegment}”)” in the Storage. The “img” folder is where the file “fileUri.lastPathSegment” will be uploaded.
We can also create the metadata for the file:
Let’s create a Task that will upload the image with the metadata to Storage putFile(fileUri, metadata):
To listen on a state of the file upload, we need to add OnProgressListener:
Also, to listen on the state of successful upload or upload failure, we need to add OnCompleteListener, OnFailureListener:
Now we need to make sure the file has been created in the storage:
and it has all the necessary metadata:
Now it’s time to create a link to the file in the storage to upload it to the device. Another thing that we’ll need is calling the getFile() method:
To delete the file, we can use the delete() method:
Firebase Cloud Storage is a powerful yet easy-to-use tool for storing all sorts of objects. With its help, you can store images, audio, video, and other types of user content.
Firebase Cloud Storage allows you to smoothly switch from an app prototype to a full-fledged product thanks to being highly-scalable and the ability to process exabytes of data.
If you want to have a more detailed look at the capabilities of Firebase Cloud Storage, you can refer to the official documentation by this link:
https://firebase.google.com/docs/storage
We are a team of web and mobile development jedi.
1.1K 
1
1.1K claps
1.1K 
1
Written by
JetRuby is a Digital Agency that doesn’t stop moving. We expound on subjects as varied as developing a mobile app and through to disruptive technologies.
We are a team of web and mobile development jedi. We believe sharing knowledge pushes the industry forward and creates communication bridges. Our technical expertise includes Ruby on Rails, ReactJS, Elixir, Swift, Kotlin, React Native, and many more.
Written by
JetRuby is a Digital Agency that doesn’t stop moving. We expound on subjects as varied as developing a mobile app and through to disruptive technologies.
We are a team of web and mobile development jedi. We believe sharing knowledge pushes the industry forward and creates communication bridges. Our technical expertise includes Ruby on Rails, ReactJS, Elixir, Swift, Kotlin, React Native, and many more.
"
https://medium.com/ibm-garage/deploy-swagger-apis-to-ibm-cloud-private-using-ibm-cloud-developer-tools-7eb2bf55dad0?source=search_post---------257,"There are currently no responses for this story.
Be the first to respond.
While working on a project, my client asked if I could quickly demo how to take their Swagger API file and deploy this into IBM Cloud Private (an application platform for developing and managing on-premises, containerized applications using Kubernetes as an orchestrator) also called ICP, with minimal…
"
https://medium.com/google-developers/building-a-smart-home-cloud-service-with-google-1ee436ac5a03?source=search_post---------258,"There are currently no responses for this story.
Be the first to respond.
Recently, my colleague Dan Myers wrote a great piece on IoT & Google Assistant, introducing the core concepts of the Smart Home Actions API. This API enables developers to report device state to the Home Graph from their existing cloud service infrastructure and execute commands sent from Assistant-enabled devices.
In the article, Dan mentions that in order to integrate your devices with Google Assistant, “you, as the device creator, develop your own cloud service, which includes its own dashboard, device registration, and device management that functions independently of the Assistant.”
In this post, I’d like to explore what that cloud service might look like if you’re a developer who hasn’t already invested the time and resources into building your own device cloud — or simply don’t want to manage the cloud infrastructure yourself as your device fleet scales. Maybe you’re just looking into getting your existing products connected, and wondering what it takes to build a cloud service for the smart home.
We’re going to discuss a working example of a smart home cloud using components of Firebase and the Google Cloud Platform. You will see how to securely connect and provision devices, build a user-authenticated device model that enables easy front-end user application development for both mobile and the web, and set up the structure necessary to integrate with the Google Assistant.
You can find the sample code described in this post on GitHub.
Expanding a bit on what a smart home device cloud should look like, here are some typical requirements you might have:
Not to mention that you want all of this to happen securely as data travels between users, the cloud, and devices. We can satisfy all of these requirements using services from the Google Cloud Platform and Firebase to create a scalable serverless solution for home devices.
Cloud IoT Core is a fully managed service for securely connecting and managing IoT devices. Using the MQTT or HTTP bridge, IoT devices can connect to Google Cloud using per-device public/private key authentication and exchange data. Incoming device data is published to a Cloud Pub/Sub event stream.
Cloud Firestore is a flexible, scalable NoSQL cloud database to store and sync data for client- and server-side development. It keeps your data in sync across client apps through realtime listeners and offers offline support for mobile and web through their native SDKs. Firestore also pairs with Firebase Authentication to control access to data through built-in security rules.
Cloud Functions enable backend code to run in response to events triggered by Firebase and Google Cloud features. We can use this to marshal device data between IoT Core and Firestore. Using this architecture, Cloud Firestore becomes the central data hub and source of truth for the state of all connected devices and exposes that state to authenticated client applications.
The Firestore model represents data as key-value pairs stored in documents, which are then organized into collections. We will represent each smart home device in Firestore using two documents:
Changes to the device’s config document trigger a cloud function to update the device. By separating the configuration and state into two documents, we ensure that successful updates received from the device don’t trigger the same logic. This simplifies the function logic and creates a nice separation outgoing and incoming data flows.
We need to map changes in state to messages that we can exchange with the device. Cloud IoT Core supports the following message types:
We want user commands to turn on the lights or adjust the temperature to be guaranteed to arrive at the device, even if that device happens to be offline. With configuration messages, this is handled for us by IoT Core. The device will receive the latest configuration each time it connects to the gateway, removing the need for our application logic to handle this.
Using cloud functions, we can deploy code that triggers when a device config document changes in Firestore, and publish those to the corresponding device in IoT Core. The document path used as a trigger contains a wildcard for the device id. This allows the function to trigger for every device, and captures the device id value in the function’s context.
When the device reports its state back to IoT Core, the message gets published to a Cloud Pub/Sub topic that we choose. We can then use another function to capture those messages and write the updated state into Firestore.
Our devices could publish this data as either state updates or telemetry events, and the code would behave the same way. However, because Firestore does the work of persisting device state in our cloud service, we don’t need IoT Core to do the same. Therefore, it makes the most sense to have the device send data using telemetry events.
NOTE: The sample-device project on GitHub implements a virtual device using Node.js that connects to the MQTT gateway, subscribes to configuration changes, and publishes updates back as telemetry events.
Because all of the device data resides in Firestore, building user applications for the web and mobile devices is very straightforward using the native Firebase SDKs. To make things even simpler, the Firebase team provides additional libraries that integrate these SDKs with popular frameworks.
This example uses Angular and AngularFire for the web interface. The mobile application was built in Flutter with FlutterFire, which has the added benefit of being cross-platform between iOS and Android devices.
Security rules define access control for the data housed in Firestore, ensuring only authorized users have permission to view and manage the devices they own. The rule set below defines the following controls:
Since Firestore represents the source of truth for device data, client applications only need to interface with the Firebase SDK to authenticate users and manage devices. Users can view their devices by listing the documents in the devices collection where their account matches the owner field. The following Dart code lists the documents for the current user’s devices using the FlutterFire plugin.
Each user command to change the state of the device updates the corresponding document in the device-configs collection. This triggers an update to the device’s IoT Core configuration using the cloud function described in the previous section.
Registering a new device to the user’s account requires a few additional steps for security purposes, so let’s look in more detail at that process.
Cloud IoT Core uses public/private key pairs to authenticate devices. We won’t discuss the process in detail here, but see device security for more information on how it works. For consumer devices, we will split the provisioning process into two stages: factory provisioning and end-user provisioning.
The factory assigns a private key to the physical device, and registers the corresponding public key and device identifier with Cloud IoT Core. By doing this at the factory, we ensure only devices we manufacture have credentials to access the cloud. The device is shipped to the end user with a copy of the public key, which the user application must provide in order to register that device to their account.
In our example, the public key and device metadata are bound to a QR code that the user can scan from their mobile device during the device registration process. The example QR code contains the following information as a JSON blob:
The user application writes this data into Firestore as a pending device registration, which triggers a cloud function to verify that the public key from the physical device matches the value provisioned in Cloud IoT Core for the corresponding device identifier and that the device is not already registered to a different account.
Once the device data has been verified, the server can create the new device entry for the user and remove the pending entry from Firestore.
In this post, we’ve explored what it takes to get started building a cloud service for smart home devices using the Google Cloud Platform and Firebase, and how these services make developing front-end applications easy while keeping everything secure end-to-end.
In the next post of this series, you will see how the groundwork we have laid enables us to quickly add the account linking and intent fulfillment necessary to connect your new device cloud with Smart Home Actions and the Google Assistant.
Engineering and technology articles for developers, written…
554 
11
554 claps
554 
11
Written by
Android+Embedded. Developer Advocate, IoT @ Google.
Engineering and technology articles for developers, written and curated by Googlers. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Android+Embedded. Developer Advocate, IoT @ Google.
Engineering and technology articles for developers, written and curated by Googlers. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://blog.kovalevskyi.com/deep-learning-images-for-google-cloud-engine-the-definitive-guide-bc74f5fb02bc?source=search_post---------259,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Co-author of the article: Mike Cheng
Google Cloud Platform now provides machine learning images designed for deep learning practitioners. This article will cover the fundamentals of the Google Deep Learning Images, how they benefit the developer, creating a deep learning instance, and common workflows.
Disclaimers:
* At the time of writing, the product is still in Beta, therefore it is not covered by any SLAs.
* We will update this guide as new features are available. Please see the bottom of the page to see when the article was updated.
The Google Deep Learning images are a set of prepackaged VM images with a deep learning framework ready to be run out of the box. Currently, there are images supporting TensorFlow, PyTorch, and generic high-performance computing, with versions for both CPU-only and GPU-enabled workflows. To understand which set of images is right for your use case, consult the graph below.
We also have experimental families:
All of the images are based on Debian 9.
All images come with python 2.7/3.5 with pre-installed core packages:
Jupyter environments (Lab and Notebook) for doing quick prototyping.
Nvidia packages (GPU images only):
The list is constantly growing, so I would advise the reader to keep an eye out for any updates.
Let’s say that you want to train some models with Keras and TensorFlow. You care about performance, so you want to attach GPUs. To see any benefit from the GPUs, you will need to install the Nvidia stack (Nvidia driver + CUDA + CuDNN). Not only is this tricky in and of itself, but you will also need to consider compatibility with the ML framework binaries. For example, the official TensorFlow 1.10 binary is compiled with CUDA 9.0, therefore a machine with CUDA 9.2 or 10.0 will NOT work with the official TensorFlow binary. As anyone who has set up this stack can tell you, matching dependencies is a nontrivial pain.
Now, let say that after some sleepless nights you’ve finally installed required CUDA stack. Now we can consider the question of optimization. Performance is key, since it means less time to convergence (less thumb-twiddling and a lower total cost). Is the CUDA + framework combination the fastest stack for the hardware that one will be using? For example, will a GCE instance with a SkyLake CPU, one Volta V100 GPU, and CUDA 9.0 show the highest possible performance for TensorFlow 1.10? With improvements coming in constantly from Nvidia, the framework, and even the platform itself, it’s hard to know for certain.
In order to be sure, you will have to compile TensorFlow yourself with different compilation keys and different versions of the Nvidia stack, run measurements, and pick the best one. All of this trial and error will require GPU instances, which are quite pricey. To top it off, you will have to do this all over again as each new version of Nvidia stack or TensorFlow is released. Safe to say, this process is not something you really want to handle yourself.
This is where the Google Deep Learning images come into play. The TensorFlow images have a custom build of TensorFlow that is optimized precisely for the hardware that we have on Google Cloud Engine. We’ve also tested each configuration of Nvidia stack and packaged the one that has the best possible speed. And, on top of this, almost all of the important packages you will need over the course of your research come pre-baked in the image
There are two ways of creating an instance with our image:
Our UI is very simple, so I would just show to you how it looks like:
You can start using it right away by going here: https://console.cloud.google.com/mlengine/notebooks/instances
Since this part is simple I will leave you with the official documentation for the UI and we will move for the CLI part for the users who either need to do an automation or more flexibility.
Before starting, be sure that you have installed the gcloud CLI on your local machine. Alternatively you can also use Google Cloud Shell; however, beware that WebPreview from Google Cloud Shell is currently not supported.
Now, you will need to pick the family of images you want for your VM instance. For ease of reference, I’ve duplicated the non-experimental images families graph:
Let’s say you want a TensorFlow GPU image. You would select the family tf-latest-gpu, which will reference an up-to-date image with the most recent release of TensorFlow. We also have family tf-latest-cu100 that will have latest TF with the CUDA 10.0. We will be using this family later on across the article. However, beware that after migration to the newest CUDA (like CUDA 10) this group will not be supported, so I would suggest to use tf-latest-gpu.
Let’s say you have a project that requires TensorFlow 1.11, but TensorFlow 1.12 has already come out, and the tf-latest images have already moved to 1.12. For this scenario, we provide image families that refer directly to the framework version (in this case, tf-1–11-cpu and tf-1–11-gpu). These images will still be updated with the necessary security patches, but the framework is fixed.
I do understand that there might be plenty of cases where you might want to reuse the exact same image again and again. There are many cases where this is actually preferred. For example, if you are spinning up a cluster it is NOT recommended to reference the images by image family in your scripts, because if the family is updated while your script is working, you will end up having different images (therefore different version of the software) on some instances in the cluster. In these cases, you’ll want to get the name of the latest image in the family directly:
Unfortunately, I do not have a fancy bash function for you for this one, simply because it is not something that I use very often. All the deep learning images are public and can be listed from the project “deeplearning-platform-release” where they are hosted.
Now you have the EXACT image name: e.g. tf-latest-gpu-20181023 that you can reuse wherever you want.
To create an instance from an image family:
If using an image name, you should replace “ — image-family=image-family” with “ — image=image-name” in the command.
Several things to note here:
Pick right instance type. Example of the command is using “n1-standard-8”, which has 8 vCPUs and 30 GB of RAM. You might want a cheaper instance, or more powerful. All available instance types can be found here.
Pick right disk size. You probably do not want to find out that disk is not big enough to host all your data for the training so make a good decision about the size upfront.
If you are using an instance with GPUs, there are a few points that you should be aware of:
Pick a valid zone. If you are creating an instance with a certain GPU, be sure that the GPU is available in the zone you’ve selected. See the documentation that gives a list of zones with GPUs. As you can see us-west1-b is the ONLY zone that have all 3 different GPUs.
Verify that you have quota to create an instance with GPUs. Even if you have picked the right region it does not mean that you have quota to create a GPU instance. By default, quotas for GPU are zero, therefore any attempts to create an instance with GPUs would result in a failure. A good explanation how to request increase of the quotas can be found here.
Verify that the zone has enough GPUs to fulfill your request. Even if you have picked the right region and you have quotas, it does not mean that there available GPUs in the zone that you have picked. Unfortunately, I’m not aware of any simple way to check the availability of the resource in any other way other than actually trying and creating the resource.
Pick the right GPU type and amount. The “accelerator” flag in the command controls the GPU type and the amount of GPUs that will be attached to the instance: e.g “— accelerator=’type=nvidia-tesla-v100,count=8'”. Each GPU has certain valid counts. Here are the supported types and counts that can be used with them (in order of least to most powerful):
Give permission for Google Cloud to install the Nvidia driver on your behalf. The Nvidia driver is required for your instance to interface with the GPUs correctly. Due to reasons that are out of the scope of this article, images do NOT come preinstalled with the Nvidia driver. However, it is possible to authorize Google Cloud to install this driver on your behalf. This is done via the flag “ — metadata=’install-nvidia-driver=True’”. If you do not opt into the automatic install using the metadata flag, the VM will prompt you to install the driver on your first SSH.
Unfortunately, the process of installing the driver affects the startup time for the first boot, since it must download the files from Nvidia, install the driver. This should take no more than 1 minute, but it might affect certain use cases. We will discuss how to reduce time to boot time later in this article.
If you are not planning to use GPU you might want to add the following key to your command:
this will guarantee that you will have the instance with the fastest possible CPU by the time for writing. However, you need also be sure that this CPU is available in your region. You can check it here. So the overall command for creating a CPU-only instance would be:
There is a way to create Deep Learning VM with the special HTTPS link. The link will give you access to the Jupyter Lab that is running on the VM. With the link, you will not have to use SSH (unless you want to) and port mapping.
This feature is currently in Beta and is supported ONLY for instances in US/EU and Asia.
In order to use this feature, you need to change the following line:
to
So the final command will look like this:
This will show your instance in the new Notebooks UI that we have shown before now you either go directly to the UI to start using your instance here: https://console.cloud.google.com/mlengine/notebooks/instances or if you want you can can get the url for accessing Jupyter on the VM by calling the following command:
If everything is ok you will see URL that can be used in the browser to access Jupyter Lab.
It might take a minute for the URL to show up
This can be done via simple command:
gcloud will propagate your SSH keys and create your user, so you don’t have to worry about that part. If you want to make the process even easier, I actually have several bash helper functions that simplify things for me. I prefer to ssh like this:
BTW, you can find many of my gcloud related functions here. Before we jump to questions like how fast is the image, or what can be done with it, let me address one last question related to image start time.
This is very simple. You can do the following:
We already have covered how to create the instance, ssh to it, and verify that driver is installed. Now, let me tell you how to stop instance from CLI and how to create your own image.
To stop your instance, you can use the following command (from your local machine, not on the instance):
To create your own image:
After this command finishes, you will have your own image with Nvidia drivers pre-installed that can be used to start new VMs.
First of all, if you do not know what is “preemptible”, first familiarize yourself with the official documentation here. And now the good part, in order to create a preemptible instance the ONLY change you need to do to the creation command is to add the following flag in the end: “preemptible”.
I would strongly advise you to use the access method from the section “Create Instance With Simplified Jupyter Access Feature. Beta.”. Use this section only if that method did not work or can NOT be used.
As soon as your VM is up next step probably would be to start the Jupyter Lab and do some DL :) This is very simple with images. Jupyter Lab is actually already running (also there is a user “jupyter” that is created in the system and is used by the Jupyter Lab), you just need to connect to the instance with port mapping. By default, it will be on port 8080.
Now you can open your browser on your local machine on http://localhost:8080 and this is it!
This is a very important question. However, the answer to this question probably would be even longer than everything written here up to this point. Therefore, my friends, you will have to wait for the next article :)
However, just a very quick estimation shows a speed of training on ImageNet around 6100 images/sec (ResNet-50). I did not have a personal budget in order to finish the training but I guess it is safe to assume that with such speed one can train the model to 75% accuracy in slightly more than 5 hours.
If you need anything, please do not hesitate to file question on the stack overflow with the tag google-dl-platform. Even if you think that the question is small, just file it:)
Also, you can now write to the public Google Group.
If you have any feedback, please do not hesitate to contact me by any possible means. In fact, if you want to say thank you for the article, the best way is to play with the images and give your feedback! It would be really-really nice to hear what can be improved!
These new images are bridging the gap between the software stack for deep learning (TensorFlow) and the GCE hardware (like Nvidia Volta). And since Google is behind TensorFlow they definitely have the expertise required to provide the best possible performance on the hardware.
Last updated: Mar 6 2019
follow me on twitter
Articles about the #DeepLearning (mostly Google Cloud with…
645 
7
645 claps
645 
7
Written by
#ML/#NLP researcher. Making Things Work! #TensorFlow. In the past #MXNet. All opinions are my own.
Articles about the #DeepLearning (mostly Google Cloud with #TF).
Written by
#ML/#NLP researcher. Making Things Work! #TensorFlow. In the past #MXNet. All opinions are my own.
Articles about the #DeepLearning (mostly Google Cloud with #TF).
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/thinking-design/30-ux-design-inspiration-resources-7a65fe9e29a4?source=search_post---------260,"There are currently no responses for this story.
Be the first to respond.
The world of user experience has never stopped evolving and it’s essential to stay up-to-date on the latest ideas and conversations happening in the industry. However, being overloaded with content it can be hard to find the truly helpful materials.
Since you’ve already found your way here to the Creative Cloud blog, you’re off to a great start! If you’re looking to add more resources to your list though, this post is here to help. I’ve put together a rundown of 30 of my other favorite UX design blogs and resources (in no particular order) to help you better understand UX and stay on top of current trends.
Behance is the world’s largest online creative community where users share their work. Users represent a wide array of creatives including graphic designers, illustrators, typographers, user interfaces designers, web designers and more.
Smashing Magazine is one of the world’s most popular and highly regarded magazines in the area of web design and web development. The User Experience category features great articles on usability, information architecture, interaction design and other UX-related topics both for digital (web, mobile, desktop apps) and physical products.
UX Matters provides inspiration and insights for both professionals who are working in all aspects of UX and newbies who are just stepping into the field.
Awwwards is the site that awards the best of the best in web design and development. It’s a place where creative professionals from all over the world come to find inspiration, share knowledges and receive constructive critiques.
Designer News is a global community where design and technology professionals share interesting links and timely events.
Nielsen Norman Group is trusted authority in all things related to UX. Eliminate guesswork and rely on NNG’s fundamental reports, research and articles regarding user experience.
UX Magazine is an online UX publication discussing the latest trends in UX design and product strategy. Here you can also find information about upcoming UX events (conferences, workshops, informal meetups, and classes).
The main real difference between UI Movement and other design inspiration sites, is that it’s focused exclusively on user interface designs. Each design example in a collection is tagged so that you can easily find whatever design element you’re interested in.
This site is dedicated specifically to user onboarding — the process of increasing the likelihood that new users become successful when adopting a product. Samuel Hulick, the author of this site, presents each case study in fun and engaging way.
UX Myths is a popular source of information to debunk common myths in UX design. The site collects a lot of research data and facts from well-known UX experts in order to reveal the common design misconception. This resource is especially helpful for UX beginners.
Creative Bloq contains a lot of useful information and fresh ideas for your future design project. The information shared via this publication varies from design ideas to project management methodology.
Dribbble is a designers’ community, where designers demonstrate their works and get inspiration from each other. Designers use Dribbble to showcase creativity without any goal besides that, but it still serves as a good resource for inspiration.
UX mastery helps UX professionals get started in the field and get better at what they do. The site contains a lot of helpful articles, tutorials, and resources.
Little Big Details is a curated collection of design inspiration. This resources aims to demonstrate why little details such as form labels, notification messages, and subtle animation effects are just as important as the more obvious elements of your design.
Design Shack showcases inspiring examples of design, alongside resources and helpful tutorials. Regular articles are highly focussed on teaching readers new techniques for creating their own designs, and daily community news ensures that everyone who follows this resource is up to date with the latest developments.
A List Apart explores the design, development, and meaning of web content, with a special focus on web standards and best practices. The site features a lot of helpful long-form articles from the field professionals covering topics around design methodology.
Web Designer Depot makes posts daily about design trends, best practices, great web design, and more.
UI Patterns is a user interface design pattern library. It makes it easy to find patterns that you need for your project. UI Patterns doesn’t only highlight design patterns, it goes a step ahead to discuss the problems typically associated with each pattern and effective methods to solve common problems related to them.The site also contains a bunch of interesting articles on UI design which can help extend the way you approach using patterns in your designs.
From Up North is an online magazine which publishes content on the latest trends in creative industry. It explores different categories providing a variety of design perspectives.
The UX Booth is a publication by and for the user experience community. Their articles are strongly focussed on beginner-to-intermediate user experience and interaction designers.
Medium is online publishing platform, with thousands of new stories updated daily. It’s a great place to read, write and interact with the stories of fellow UX designers.
Site Inspire is a gallery which showcase the best web design projects. It features over 2500 real-world websites and makes it possible to search for a specific project by type, subject, and style. Thus, when you want to get very specific with your searches related exclusively to web design, Site Inspire is a good plate to go.
Mobile Patterns focuses solely on the UI of mobile apps. This website is good for getting inspiration on iOS and Android app design. From the sidebar, you can narrow down screenshots by several different UI elements (e.g. Calendar, Camera, Maps, etc).
Pttrns provides holistic collection of design patterns, resources and inspiration. Unlike previously mentioned Mobile Patterns, Pttrns contains not only Mobile patterns, but also patterns for smartwatch apps.
UX expert Luke Wroblewski publishes articles and reviews about product design, and provides helpful materials for mobile and web design strategy.
Informative content which is often based on user research makes this blog a great source to understand all aspects for creating a great user experience.
Designmodo is a great resource of informative material for designers and web developers. They also have a dedicated category for UX Design.
52 Weeks of UX is a discourse on the process of designing for real people. The site contains dozens of interesting topics related to the user experience and human behaviors.
Here you can find a lot of helpful articles about design principles, processes and methods. There are also a number of case studies on UI design, interviews and product reviews that makes it a truly valuable resources for UX designers.
UX Stack Exchange is a question-and-answer website for UX design. If you have a specific question and would like to hear a frank opinions from UX experts you can ask it there.
Designing a great UX is challenging, even for the most seasoned designer. Countless factors need to be taken into account. Hopefully the resources mentioned in this article will help you come out of an ideation dilemma and inspire you to design user experiences that are simple, intuitive and delightful.
Do you have any resources you’d add to the list? Let us know in the comments!
Nick Babich is a developer, tech enthusiast, and UX lover. He has spent the last 10 years working in the software industry with a specialized focus on development. He counts advertising, psychology, and cinema among his myriad interests.
Originally published at blogs.adobe.com.
Learn about Adobe XD, our all-in-one design and prototyping tool:
Stories and insights from the design community.
256 
1
256 claps
256 
1
Written by
New Tools for New Creatives. Get all the latest creative apps plus seamless ways to share and collaborate. All right on your desktop.
Stories and insights from the design community.
Written by
New Tools for New Creatives. Get all the latest creative apps plus seamless ways to share and collaborate. All right on your desktop.
Stories and insights from the design community.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/cloud-run-and-cloud-function-what-i-use-and-why-12bb5d3798e1?source=search_post---------261,"There are currently no responses for this story.
Be the first to respond.
In April 2019, at Google Next event, Google announced a new serverless product: Cloud Run. I’m a Cloud Run alpha tester and I experimented this awesome product on many use cases. I was also honored to be on stage in San Francisco and in Paris (for Google Cloud Summit) to present my experiences on this product.
Cloud Run comes in addition of other Google Serverless products, especially Cloud Function. Even if each product has a typical use case and recommendations, some use cases can be implemented in several ways with several products. So, What I use in my current developments? And Why do I make this choice?
Before going further, here’s a quick overview of each product
Cloud Functions are serverless, single purpose and event driven solution. They can be also triggered on HTTP request. Each request/event are processed by a new instance of the function and provide a strong isolation between each invocation.
Today in Generally Available (GA). Several languages are handled like NodeJS8, Python3.7 and Go 1.11; NodeJS10 in Beta ; In Alpha, Go 1.12 and Java
Only the code and the dependencies list are provided, the platform does all the rest: package, deploy and run the code.
Typical Use case
Single purpose workload, especially event driven (PubSub, Storage, Firestore/Firebase, …) but also HTTP triggered. Possible languages are limited to NodeJS 8/10, Go 1.11/1.12 and Python3.7 with only dependencies publicly reachable (not true with Java). Image transformation, micro-batching (duration less than 9 minutes), streaming processing,… are common use cases
Cloud Run is build on top of Knative, that allows to serve stateless containers in a serverless way and it’s natively portable. Only stateless containers with HTTP endpoints are required to be run. There is no limitation in languages, binaries and dependencies.
By this way, the container has to integrate a webserver and can handle several requests in the same time and thus, can be multi-purpose.
Container packaging has to be done before deployment. Cloud Build can be used for this, or locally with Docker or other tools (like Kaniko or Jib)
Typical Use case
All workloads working in stateless mode, with a processing time less than 15 minutes and can be triggered by one or several HTTP end-points, like CRUD REST API. Languages, libraries and binaries don’t matter, only a container is required. Website backend, micro-batching, ML inference,… are possible use cases
However, when I want to implement, in Python, Go or NodeJS, a workload triggered by an HTTP request, like a batch processing triggering by a Cloud Scheduler or a single purpose HTTP webhook, What I should use ?
It’s possible to use a Cloud Function to address this problem. Single purpose, http triggered and language in GA. But it’s also possible to build a HTTP container and to deploy it with Cloud Run.
When I have to cope with this question, I always use Cloud Run. Why ? Indeed, this choice could seem overkill for a single purpose usage. However, I have 2 main reasons
When you develop for Cloud Run, you have to build a container. This container have the ability to be deployed anywhere: on premise, on Kubernetes/GKE, on Cloud Run (serverless and GKE), on a VM,… anywhere.
On the other side, code deployed on Cloud Function implicitly assumes that there is a webserver, somewhere, able to route the request to the code. For changing the execution environment, you have to repackage the code, maybe in a container, but at least with a webserver in front of it.
Cloud Run has here this strong advantage to reduce the rework and the refactoring when you want to change the environment. And, you are less locked in to Google Cloud.
In mission critical development, the testability and the reliability are paramount. I’m not talking about unit test here, but about end-to-end tests also called “integration” tests. In a production environment in real time IoT application, we can’t be approximative or accept to lose messages because of a bad version.
With Cloud Function, the code handles the request forwarded by the webserver of the execution environment.
Thereby, for testing your function you have to simulate the webserver request and to fill correctly the request object passed in parameter of the code. Not sure that is the best way, but, even in the GCP doc, there is no example in Python; I did what could with my (low) level in Python -> I’m open to suggestions!
Here is the problem: How can you be sure that the simulated object is correct ? Do you know all the headers passed in this object ? How arebasic authentication, Bearer or API keys set ? Do you know the version of the request object used in Cloud Function environment ?
Personally, I’m not able to answer these questions. And the new developer that joins your team, is he able ? Moreover, I’m not confident in my test code, and it’s a problem not to be confident in tests !
My solution for solving this issue is to create a test file, with a webserver that imports the function code.
To launch a test, simply calls the webserver with CURL command. This way, the webserver will format correctly your request object, with all the rules and the parameters, and forward it to your code. The additional advantage is that you can change your webserver, and even your dev language, without changing your CURL test requests.
And, it’s at this moment that Cloud Run make all its sense. In this context, Cloud Run is no more than Cloud Function code + simple webserver that forwards requests. Simply add a standard Dockerfile for your language. And it’s over - you don’t have a Cloud Function with a test server, but a packaged container ready to be deployed on Cloud Run.
So, Cloud Run is the good mix of simplicity of Cloud Function, end-to-end testability and portability. Many pain-points are solved, many advantages are gained, that’s why I use and I recommend Cloud Run for all your stateless HTTP container, even for events like PubSub (create a push subscription for calling your Cloud Run) or Storage (create a PubSub notification on GCS). For other events, Cloud Functions are, until now, unavoidable.
In addition, you can consider several other advantages of Cloud Run like the capacity to handle several entry-points in only one deployment. Or also, the capability to run a workload up to 15 minutes (only 9 minutes with Cloud Function). But also, the capability to be charged only once and to be impacted by the cold-start once when several requests are handled concurrently (up to 80), but with the trade-off of a loss of isolation/memory sharing between all requests (also possible with Cloud Run if you set the concurrency param to 1).
Finally, for me, Cloud Run is the best solution for a majority of use cases. The development team is very active and new features are constantly added: Metrics, Cloud SQL connectivity, Service Account Identity,… I strongly recommend you to stay tuned of this product, it will become essential for all purposes!
Cloud Run on Google Cloud Blog Cloud Run product pageCloud Function product page
Google Cloud community articles and blogs
309 
11
309 claps
309 
11
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
GDE Google Cloud Platform, scrum master, speaker, writer and polyglot developer, Google Cloud platform 3x certified, serverless addict and Go fan.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/flutter-community/flutter-crud-operations-using-firebase-cloud-firestore-a7ef38bbf027?source=search_post---------262,"There are currently no responses for this story.
Be the first to respond.
Cloud Firestore provides you an easily manageable pre-configured NoSQL database. It helps in storing & syncing data for both client and server-side development; it also supports automatic caching of data for using it even offline. Google Cloud is the driving platform behind Cloud Firestore that can be scaled easily.
In this article, you will learn to integrate Cloud Firestore with Flutter, and perform CRUD (create, read, update and delete) operations.
So, let’s get started.
You will need to create a new Firebase project in order to integrate it with your application.
Navigate to the Firebase console and click Add project.
Enter a project name and click Continue.
You’ll be asked whether you want to enable Google Analytics for the project. We won’t need analytics since this is just a sample project, just click Create project.
If you do want to enable Google Analytics, you’ll be prompted on the next screen to select a Google Analytics account:
Wait for the project to be created, and you’ll be navigated to the project’s Firebase dashboard.
Though we’re using Flutter, which is a cross-platform framework, we still need to integrate the Firebase project separately for each platform.
Flutter 2.0 has support for Android, iOS, and web in its stable channel, so we’ll configure Firebase for all three platforms.
Let’s start by configuring for the Android platform.
You have successfully configured Firebase for Android. On the final step, click on Continue to console to go back to the dashboard.
Follow the steps below to complete the configuration on the iOS platform.
Configure the Firebase project for the web by following the steps given below.
Then, click on Continue to console to navigate back to the dashboard.
Finally, you have completed the Firebase configuration for all the three platforms.
Let’s take a look at the sample project that we will be building, and then start integrating Cloud Firestore with the Flutter app.
We will be building a simple notes management app where you can see the list of saved notes, save new notes, and also update & delete them.
This sample app will consist of four screens; the first one will be a very simple login screen for authenticating a user. But as the main focus of this article is the database, we won’t go and set up a full-fledged authentication logic. The login screen will simply accept a unique identifier which can be later used to customize and store data privately for each user.
The other three screens will be a DashboardScreen (displaying the list of notes), AddScreen (for adding a new note item), and EditScreen (for editing an already saved note item).
You can enable the Firestore database by selecting Firestore from the left menu and then clicking on Create database.
Now, you will be prompted to select a type of security rule. Here, we will choose test mode (that is, the database is open to all and it doesn’t check for any kind of authentication) because we will not set up any kind of strict authentication for this sample app. Click on Next.
If you are working on a production app, make sure you define an appropriate security rule. You can learn more here.
Then, you have to select a Cloud Firestore location and click on Enable.
You will be taken to an empty Firestore database structure.
We will start adding data to the database using the Flutter app.
You can create a new Flutter project using the following command:
Then open the project using your favorite code editor. For opening with VS Code you can use:
Flutter 2.0 has support for null safety in the stable channel, but in order to use it inside the app, you have to run a command for migrating the project to null safety.
Before running the migration command, check if all your current project dependencies support null safety by using:
Then, run the following command to migrate:
You can follow the migration guide here.
We will be using the following plugins in this project:
The latest version of both these plugins support null safety.
Add these to your pubspec.yaml file:
Install the packages from the command line using:
Go to your main.dart file present in the lib folder of the root directory. Replace the contents with the following:
LoginScreen will be a StatefulWidget. Define a method inside it called _initializeFirebase() that will initialize the firebaseApp.
Inside a FutureBuilder call the _initializeFirebase() method which will show the LoginForm widget as the initialization is complete.
Before moving on further with the UI, let’s take a look at how the Firestore database is structured, and then define methods for performing the CRUD operations.
It's important to understand how data is structured in the Firestore database before we start defining our CRUD methods.
Cloud Firestore mainly consists of collections, documents, and fields (key-value pairs). Collections can contain a number of documents, which in turn can consist of sub-collections and key-value pairs.
References are used for pointing to a particular location in the database, that information can be used for storing, retrieving, updating, and deleting data from the database.
You can learn more about the Cloud Firestore data model here.
Our database structure will be like this:
Let’s start defining the CRUD operations inside our Flutter app’s Dart code. Create a new class Database inside a file called database.dart.
Here, you will first need to initialize the FirebaseFirestore and define the main collection where all of the database information will be stored.
Now, we can start defining the CRUD methods in this class. We will start with the write operation.
The write operation will be used for adding a new note item to the Firestore. Define it inside a method called addItem() by passing a title and description of the note.
We use the set() method on the documentReferencer to write a new data to the Firestore, and we have passed the data as a map.
We can read data from Firestore in two ways, as a Future or as a Stream. You can use Future if you want to read the data for a single time. But in our case we need the latest data available in the database, so we will use Stream as it will automatically synchronize data whenever it gets modified on the database.
To update data of the database, you can use the update() method on the documentReferencer object by passing the new data as a map. To update a particular document of the database, you will need to use its unique document ID.
To delete a note from the database, you can use its particular document ID and remove it using the delete() method on the documentReferencer object.
Congratulations 🥳, you have successfully defined the CRUD methods in Flutter to interact with the Cloud Firestore database.
A brief demo of the app is shown here:
Cloud Firestore also provides a number of filters that can be used while querying the database in order to sort the responses, search for responses having a particular format, etc. You can learn more about it here.
FlutterFire handles the official documentation of the Firebase plugins for Flutter, you can find them here.
You can get the entire code of this app from this GitHub repository:
github.com
Originally published on Codemagic Blog.
medium.com
medium.com
medium.com
If you want to support me and want me to continue writing Flutter articles and building some interesting Flutter projects, please contribute to my Patreon page below:
www.patreon.com
You can follow me on Twitter and find some of my projects on GitHub. Also, don’t forget to checkout my Website. Thank you for reading, if you enjoyed the article make sure to show me some love by hitting that clap (👏) button!
Happy coding…
Souvik Biswas
Articles and Stories from the Flutter Community
594 
3
594 claps
594 
3
Written by
Mobile Developer (Android, iOS & Flutter) | Technical Writer (Nevercode) | IoT Enthusiast | Avid video game player
Articles and Stories from the Flutter Community
Written by
Mobile Developer (Android, iOS & Flutter) | Technical Writer (Nevercode) | IoT Enthusiast | Avid video game player
Articles and Stories from the Flutter Community
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/continuous-delivery-in-google-cloud-platform-cloud-build-with-compute-engine-a95bf4fd1821?source=search_post---------263,"There are currently no responses for this story.
Be the first to respond.
This article is the 2nd part of a series written to tackle Continuous Delivery in Google Cloud Platform. In the 1st article, Google App Engine was the main actor. In this one, Google Compute Engine — or simply GCE — will come into the scene. GCE is the Infrastructure as a Service component of GCP, which is built on Google’s global infrastructure and allows its users to launch virtual machines on demand.
As we had already seen, GAE fits perfectly for continuously delivering projects. However, there are some limitations to use the platform: the code must be written in specific languages/versions, mainly if your team aims to use the Standard Environment, which is less expensive (more info at https://cloud.google.com/appengine/docs/the-appengine-environments); and surely will be fully compatible if the project was designed to run on GAE, but may have troubles if trying to migrate legacy code to the platform, even if choosing the Flexible Environment.
After a pros and cons analysis, your team may conclude that GAE is not the best option for a project. Maybe because the project uses some language or tool that is not supported by the platform; maybe because they want more control or customizability over the execution environment; maybe because they want just to migrate their workload from existing servers to GCP, to take immediate advantage of scaling up the app without changing so much the codebase. For all of these cases, consider Google Compute Engine.
What we will see in the next lines is how to use a set of tools available in GCP that allows your team to set up a development + automated build + continuous delivery pipeline using GCE. Also, using an example Angular app, it will be possible to compare the solution with the one proposed to use with GAE in the 1st article. The key point here is that the application must be shipped as a Docker image. If you are able to do it in your project, you have great chances to automate the whole deployment process as described in this article.
Before proceeding, make sure you have created a GCP Project and installed Google Cloud SDK in your machine if you wish to run the examples. Don’t forget to run gcloud auth login and gcloud config set project <your-project-id> for proper gcloud CLI usage.
For the sake of simplicity, the Angular app will be served by Nginx. So, create your new Angular app (follow the steps described here) and cd <app-name>. In order to pack the app plus the Nginx server as a Docker image, let’s create a file named Dockerfile in its root folder, with the following content:
Basically, it’s a multi-stage container build. The lines from 1 to 5 (stage 1) use a NodeJS 8 image just to build the app. The lines from 7 to 11(stage 2) copy the result of the build process — a set of HTML, CSS, and JS files — to an Nginx image and replace the default server’s home page with app’s content. Finally, line 12 starts the server.
Let’s also create a second file,.dockerignore, in the same folder, with the below content. It will prevent Docker from copying unnecessary files to the built images, decreasing their sizes.
If you trigger a docker build -t <app-name> . command followed by docker run -d --name <app-name>-container -p 80:80 <app-name>, and point your browser to http://localhost:4200, you’ll see the application running with Docker.
What we have seen so far is just Angular and Docker setup stuff. From now on GCP will be in action, making things more interesting!
In typical non-automated or semi-automated deployment scenario, an Engineer could (1) set up a CI tool such as Jenkins to monitor a Git repository for new pushes, (2) trigger the build process when new content is pushed and (3) find a mystic way to update all VMs that are running the Docker containers to update the new version. Steps 1 and 2 are simple. Step 3 maybe not... At this point GCP offers a much more sophisticated approach: instead of creating VMs, installing Docker on them, and manually managing the containers, what about creating VMs that exclusively run a specific container and automatically update themselves when new versions of the images are published? Sounds good, right? So, let’s see how to do it.
First of all, we need to add one more small file to the project root folder, named cloudbuild.yaml, as follows:
This file will be used by Cloud Build to build the Docker image and create a repository to store it in the Container Registry, using the specified name — at build time, Cloud Build automatically replaces $PROJECT_ID with your project ID.
Ops… a new GCP component was mentioned in the previous paragraph: Container Registry. According to the official documentation, it’s more than a private container repository: is a single place for your team to manage container images, perform vulnerability analysis, and decide who can access what with fine-grained access control.
Coming back to the code: after adding the file, run gcloud builds submit --config cloudbuild.yaml . in the project’s root folder, wait a little bit, and access https://console.cloud.google.com/gcr/images/<your-project-id> after the building has finished. You’ll see the new repository there (similar to the picture below). This manual step is required only once. And you may copy the full repository name, we will need it later ;).
We're almost done with Docker image building. Just need to set up a trigger that will start the build process automatically every time new code is pushed to a monitored Git repository, and this is a Source Repositories job, as we saw in the first article of this series. It will be set up exactly as it was for delivery with GAE. Bear in mind the main difference will be the result of Cloud Build processing: for GAE, it publishes a new version of the app; for the current process, it only pushes an image in the Container Registry.
Now, let’s create some VMs to run the containers. In GCP Navigation Menu, click Compute Engine > Instance Templates. Click on CREATE INSTANCE TEMPLATE. In the next screen, select Deploy a container image to this VM Instance, type (or paste) the full Container image — aka Repository — name, select Allow HTTP traffic, and click on Create.
In the Navigation Menu, select Compute Engine > Instance Groups. Click on Create instance group. In the next screen, select the Instance template you have just created and set the Minimum number of instances to 3. Click on Create.
After the Instance Group has been created, select Compute Engine > VM Instances in the Navigation menu. Click on instances’ External IP links to make sure the application is running in each of them. It may take a while to return the home page for the first time…
Finally, grant the Compute Instance Admin (v1) and Service Account User roles to the <your-project-number>@cloudbuild.gserviceaccount.com Service Account (Navigation menu > IAM & admin > IAM).
So, what if we push a new version of the application to the Git repository? Which steps do we need to repeat in order to have it running in production? Just restart the VMs! There are many ways to do it in GCP, including publishing a message to some PubSub topic to invoke a Cloud Function that restarts the VMs, or use advanced options for update managed instance groups (https://cloud.google.com/compute/docs/instance-groups/updating-managed-instance-groups). But, to show a simple example, let’s just add the following lines to cloudbuild.yaml:
They will include a new step in our build process that consists of running the gcloud compute instance-groups managed rolling-action restart command. The instances that belong to the group we created a few steps before will be restarted after the build, one by one, in a process that ensures there will always be available machines at any given time. It may take some minutes depending on how many instances the group has, but works perfectly. To see it in action, change something in your code, the title in app.component.ts for example, and push the new code to a Git repository that is monitored by Cloud Build. Wait a few minutes and refresh the HTML pages served by each instance (external IPs may change after restart).
Well, that’s pretty much it for this topic. As demonstrated here, setting up a CI environment to GCE is usually more complex than to GAE, but is a valid option if your project requirements don’t fit GAE. And sure, this is one solution, but there are others.
Below picture shows the main GCP components mentioned in the article:
The sample code is available on Github: https://github.com/ricardolsmendes/gcp-cloudbuild-gce-angular. Feel free to fork it and play.
Hope it helps!
This is the 2nd of a 3-article series on Continuous Delivery in Google Cloud Platform:
App Engine | Compute Engine | Kubernetes Engine
Google Cloud community articles and blogs
586 
9
586 claps
586 
9
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
head of data @ ciandt.com • hobbyist tech writer • dad • birder
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/built-to-adapt/how-to-save-a-fortune-on-cloud-infrastructure-5ff418c7658c?source=search_post---------264,"There are currently no responses for this story.
Be the first to respond.
The Pivotal R&D organization has grown rapidly over the last several years and with it, our IaaS costs. If your business is growing, then you’re adding new customers and services every day and there’s a good chance your R&D organization is in the same boat.
This isn’t necessarily a bad thing, of course. Elastic infrastructure helps us build and test great products! But thrift is a virtue. So we decided to look for ways to cut wasteful usage, and reduce spending with our internal cloud provider, Google Cloud Platform (GCP). Our goal was to save money, without impacting our pace of delivery.
Here’s a look at some simple tactics that lead to big savings. We hope our experience gives you some ideas about how to trim your IaaS spending without limiting innovation. You should be able to apply these ideas for all the public clouds you use—none are specific to GCP.
The first step? Getting a better understanding of where our money was going. It turns out, 90% of our spending with GCP went to virtual machines. We asked ourselves, “What are all these VMs doing, and why are they so expensive?”
To answer these questions, we looked at how long R&D teams used environments for our myriad of continuous integration (CI) tests. We discovered that some environments are used for a few hours, while others are used for days or weeks. After interviewing teams, we decided that a test cycle shouldn’t be longer than one day. It’s just CI right? There’s no need for infrastructure to run longer than this!
So we started deleting environments after one day. Our user research showed that individual R&D teams actually liked this new rigor. It’s viewed as a garbage collection feature. This simple action reduced the average “time-to-live” for environments by five hours. That adds up to $100,000 per year savings!
Next, we discovered that we deployed an HAProxy VM for each Pivotal Application Service tile. This surprised us, since we use GCP’s built-in load balancers. But someone, sometime, figured that we might want to test load balancing using an HAProxy. After interviewing a few teams, we found no one was using that VM. So we killed it. There’s another $70,000 per year saved just by eliminating all the HAProxy’s from our PAS tiles and modifying a few .yml files. Not bad for a Tuesday!
GCP gives recommendations for VM instance sizes based on the characteristics of your workloads. So many suggestions, so little time! We picked the biggest instances for the components that really need it — Diego Cells, UAA, Cloud Controller, and a few others. Then we adopted the recommended smaller instances for other parts of PCF. From there, we set each job to use its new, right-sized instance type. Then we used the Ops Manager API to implement the new lean-and-mean configuration. The result is a reduction of about $250,000 per year from our total spend. If you use AWS or Azure, you’ll have to do some number crunching to run these optimizations yourself. Google makes this optimization very easy.
Then it was Friday, and we went home. It turns out that pretty much everybody goes home for the weekend. Bizarre, I know, especially given Pivotal’s rapid product release schedule. Our PCF environments are pre-created, and left running for R&D teams to start using whenever they may need them. No one really wants our PCF environments on the weekends. But they do want them to be available Monday morning. So we decided to take all of our PCF environments offline on weekends (except for one, just in case). We’re also spinning up new environments in the morning, before the teams get into the office. The result is another $130,000 in cost reduction.
Did you know that a typical FileServer VM costs $1.14 per day on GCP while 100GB of Blob Storage is only $0.07? Unless you’re a GCP pricing nerd like me, you probably didn’t. That’s another $125,000 a year in savings by turning off a VM and switching to built in IaaS Storage. (This savings calculation assumes you’re using the default configuration for the VM in the US-Central location, where the prices are the lowest.)
It’s been a week since we made these configuration changes, and we haven’t impacted our users. The net cost savings is around $675,000 per year, and our user stories aren’t any harder than normal to implement.
Your most difficult task may be building the cost model, and determining the value of an individual change. As long as the same assumptions are made for each scenario, though, the relative values will be correct. You’ll soon notice that a few of the most important stories will result in savings that are orders of magnitude larger than the others.
We hope we’ve inspired you to take a look for ways that you can trim IaaS costs! The savings can be put towards building even more great products and services, or jump-starting innovation at your organization. Take a stab at it and let us know how it goes! If you have any other tips on how to reduce IaaS spending, please leave them as a response to this post.
Change is the only constant, so individuals, institutions, and businesses must be Built to Adapt. At Pivotal, we believe change should be expected, embraced, and incorporated continuously through development and innovation, because good software is never finished.
How software is changing the way society and businesses are…
463 
5
463 claps
463 
5
Written by
Professional Tech Worker & Amateur Car Reviewer
How software is changing the way society and businesses are built.
Written by
Professional Tech Worker & Amateur Car Reviewer
How software is changing the way society and businesses are built.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/free-code-camp/a-brief-history-of-serverless-or-how-i-learned-to-stop-worrying-and-start-loving-the-cloud-7e2fc633310d?source=search_post---------265,"There are currently no responses for this story.
Be the first to respond.
Top highlight
In the good ol’ days of the 1950s when Elvis was ruling the charts with hits like Jailhouse Rock, a computing paradigm came out called mainframes a.k.a Big Iron (no relation to Arnold Schwarzenegger).
IBM was the main mover and shaker in this space, with a sprinkling of other small providers sometimes giving competition to Big Blue.
These early beasts were purely batch processing oriented — Mr John Appleseed could punch in his instructions(read programs) on a punch card (see below), pass it to the operations team, and go off for a coffee, or even leave for the day.
The operator would schedule and execute the previously submitted punch cards.
Fun fact: the above picture is of a HUGE 5Mb of of data on 62,500 punched cards. So just imagine the number of cards required to capture the code base of Win 10 or MacOS if we still used this technology!
Once the output was generated/printed, the operator would physically collect it and deposit the same to Mr. Appleseed’s cubicle or physical mailbox. Online access was primarily reserved for resources requiring it, like, say, staff at an airline’s booking counter.
So to all those crying in spite of having GUI-based IDEs to do their software development, if you think your job is hard — think of these pioneers.
Needless to say, the above process was not just slow but also extremely expensive. So much so that system usage was measured and charged in seconds towards the client’s account.
Code re-writes were extremely cumbersome and time-consuming. In fact, come to think of it, this may be the reason for the exceptionally good coding skills of the developers of that era, because as one Mr. Real slim shady has rightly said:
Next came the era of the PC, which was the democratization of technology in the form of the personal computer.
The processing model started moving from mainframe-based, centralized systems to the client-server-based model. Infrastructure planning, procurement, and maintenance became a vital part of software development, as the pace of business started to increase exponentially.
So it was not unusual to see software deployment on a client-server-based system accompanied by projections for future volume growth. Provisions had to be made for that. As it was, the typical engineer’s job was tough — but this also required him to be a soothsayer.
So now, our poor Mr. Appleseed had a lot of things to figure out. He not only had to figure out what the business teams were saying in the form of their requirements, but he also had to learn all the coding tools, standards, best practices, and innovations to follow.
He also had to take into account parameters like what the usage patterns of the software might be, how fast or slow it may grow in future, and what the optimum hardware may be so as to strike the right balance between performance and cost efficiency.
The bureaucratic hurdles in any given organization ensured that the hapless engineer was always stuck between the devil and the deep blue sea.
In addition to having sleepless nights over the ever-changing requirements and shifting deadlines, he also had to make do with the vagaries of infrastructure procurement. In the mainframe world, the situation was a bit better, as hardware procurement was not that commonly done. But resource allocation was still a chore, which led to a lot of heartburn.
The mid-2000s saw the advent of a new paradigm in computing: “the cloud.” It changed the nature of computing as it was known up to that point.
Before we delve deeper into it, let’s take a minute to refresh the definition of “the cloud” as given by National Institute of Standards and Technology (NIST):
Cloud computing is a model for enabling convenient, on-demand network access to a shared pool of configurable computing resources (e.g. networks, servers, storage, applications and services) that can be rapidly provisioned and released with minimal management effort or service provider interaction. Five major attributes define a cloud based system viz-
a) On Demand self service
b) Broad network access
c) Resource pooling
d) Rapid elasticity
e) Measurement of service
The key enablers for this paradigm are:-
a) Fast WANs
b) powerful commodity servers
c) high performance visualization for commodity hardware
Our beloved Mr. Appleseed was now happy — after all, he was no longer at the mercy of his infrastructure engineer for getting servers or storage. He could, at the push of a button, get compute power, storage, a queue, and any other such service.
All was well in the computing world. But Mr. Appleseed, true to his human nature and being an enterprising species, started pondering how he could make his life easier.
Moreover, the world was becoming digital at a great pace with unheard-of scales of enterprises. This was making the practice of pre-allocating hardware a self defeating purpose. All the interactions with the digital world were becoming majorly event-driven.
2015 (or some say 2012) was the time when this computing paradigm came into being. There have been a number of interpretations proposed for this term (serverless) and its implications.
One school of thought attributes it to Backend as a Service (BaaS). For example, authentication services offered by third party providers like Google or Facebook.
The other school of thought links it to a concept wherein applications with business (read: server side) logic are run over stateless containers, managed by a third party provider in its entirety, which is known as Function as a Service (Faas).
This article focuses on the second definition of the concept, as it has interesting implications for the way web applications are being architected.
At its core, FaaS is a simple concept which means that:
It does not mean that there are no servers anywhere in the whole scheme of things. All it means is that the servers and their maintenance is now hidden from the developer.
Also, since the physical unit of the compute is now a container, there is no need to have long running servers with event listeners running on them to carry out any processing. Qualified event sources could be plugged into the system and the service would take over.
“What’s in a name? That which we call a rose by any other name would smell as sweet”. — Juliet, Romeo and Juliet
As it is with any new innovation, more often than not glitzy names are used by vendors to re-package old wine in a new bottle, so to speak, to gain more eyeballs.
In case of serverless, this confusion is more wide-spread. This is because the concept has developed in a very short period of time and in very close proximity to the other concepts with which it can be confused.
So here I take a shot at trying to clarify what serverless usually is (or can be) confused with. I’ll also share my two cents.
“A container image is a lightweight, stand-alone, executable package of a piece of software that includes everything needed to run it: code, runtime, system tools, system libraries, settings. Available for both Linux and Windows based apps, containerized software will always run the same, regardless of the environment. Containers isolate software from its surroundings, for example differences between development and staging environments and help reduce conflicts between teams running different software on the same infrastructure.” — Docker.com
The container is a recent innovation in computing. Google popularized it by running Gmail on it. Containers are useful for maintaining the reliability and homogeneity of software running across various computing environments.
For example, the code may have been developed in Java 8, but production may be on Java 9 — so the code which was running fine in dev may start throwing weird errors in production.
Roughly speaking, the container is the erstwhile VM (virtual machine) on steroids. It has none of the excess flab of individual OS versions. Unlike VMs, where there used to be a separate copy of the guest OS (thereby making the VM resource heavy), containers share the underlying OS kernel. This allows multiple containers to be run on that particular machine compared to VMs.
Containers remove one part of the problem: they provide a homogenous runtime production environment, albeit at the cost of increased maintenance effort.
However, to be fair, containers are typically better-placed for different kinds of work-load (like those that are inherently more complex). So they find favor in enterprise IT landscapes where there is already a monolith up and running and the organization may be wanting to port it quickly to the cloud.
Horizontal scaling is a vector wherein serverless steals the show over containers. Modern cloud vendors “theoretically” provide unlimited scaling capability for their serverless offerings. And best of all, this scaling is totally transparent to the user.
Serverless computing is better aligned to event-driven, asynchronous operations, while containers appear to be better aligned to the synchronous REQ/RESP workloads.
But given the pace at which things are changing, the differences can get diluted to a great extent over a short period of time.
“Platform as a service” (PaaS) is a cloud computing model in which a thirdparty provider delivers hardware and software tools — usually those needed for application development — to users over the internet. A PaaS provider hosts the hardware and software on its own infrastructure. As a result, PaaS frees users from having to install in-house hardware and software to develop or run a new application.” — Techtarget
Just like containers , PaaS also differs mainly concerning the vector of scaling. However hands off the PaaS vendors may claim their offering is, it still requires some admin maintenance effort, unlike serverless.
With PaaS, there is always a minimum running footprint in the system which will be up and incurring costs. However with serverless, it can be brought down to absolute zero.
PaaS may still be a good choice given its more advanced ecosystem and tooling and language support. However this should not be a show-stopper given the high pace of advancements in the field of serverless.
We’ve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
919 
5
919 claps
919 
5
Written by
Techie working for a MNC bank and striving to be a polymath. Interested in all things tech , finance and self improvement
We’ve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
Written by
Techie working for a MNC bank and striving to be a polymath. Interested in all things tech , finance and self improvement
We’ve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/self-attention-mechanisms-in-natural-language-processing-9f28315ff905?source=search_post---------266,"Sign in
There are currently no responses for this story.
Be the first to respond.
Top highlight
Alibaba Cloud
Sep 12, 2018·10 min read
Over the last few years, Attention Mechanisms have found broad application in all kinds of natural language processing (NLP) tasks based on deep learning.
With more in-depth research into Attention Mechanisms, the types of “attention” raised by researchers multiplied. In June 2017, Google’s machine translation team published a paper at arXiv entitled “Attention is All You Need,” which attracted the attention of the industry. Self-attention mechanisms became a hot topic in neural network attention research and proved useful in a wide variety of tasks.
In this article, we will explore various forms of Attention Mechanisms in NLP and their associated applications. We will look at works by Dr. Zhang Junlin’s blog “Self-Attention Mechanisms in Deep Learning” (2017 v.), and the article written by Su Jianlin, “Understanding Attention is All You Need” along with the attached code examples.
In their earliest days, Attention Mechanisms were used primarily in the field of visual imaging, beginning in about the 1990s. However, they didn’t become trendy until Google Mind team issued the paper “Recurrent Models of Visual Attention” in 2014. In the paper, they applied Attention Mechanisms to the RNN model for image classification.
Later, researchers experimented with Attention Mechanisms for machine translation tasks. Dzmitry Bahdanau et al. used this method in the paper “Neural Machine Translation by Jointly Learning to Align and Translate” to execute translations concurrently. Their work was the first to apply Attention Mechanisms to the field of Natural Language Processing. This paper caught on, and Attention Mechanisms then became common in NLP tasks based on neural networks such as RNN/CNN.
In 2017, Google’s machine translation team used self-attention mechanisms heavily to learn text representation in the essay “Attention is All You Need.” Self-attention mechanisms have also become a hot research topic, and its use is getting explored in all kinds of NLP tasks.
The image below displays the general trend of Attention Mechanism research:
The essence of Attention Mechanisms is that they are an imitation of the human sight mechanism. When the human sight mechanism detects an item, it will typically not scan the entire scene end to end; rather it will always focus on a specific portion according to the person’s needs. When a person notices that the object they want to pay attention to typically appears in a particular part of a scene, they will learn that in the future, the object will look in that portion and tend to focus their attention on that area.
Next, I will discuss the Attention Computation method typically used in NLP, hinging in large part on some of the images found in Dr. Zhang Junlin’s Attention Mechanisms in Deep Learning (2017 v.).
In essence, I will describe Attention Parameters as a projection of a query for a series of key-value pairs as in the below image:
Calculating attention comes primarily in three steps. First, we take the query and each key and compute the similarity between the two to obtain a weight. Frequently used similarity functions include dot product, splice, detector, etc. The second step is typically to use a softmax function to normalize these weights, and finally to weight these weights in conjunction with the corresponding values and obtain the final Attention.
In current NLP work, the key and value are frequently the same, therefore key=value.
The paper “Attention is All You Need” was submitted at the 2017 arXiv by the Google machine translation team, and finally at the 2017 NIPS.
The major points of this article are:
The structure of the model is shown in the image below. The entire decoder stack constructs N blocks formed by a Multi-Headed Attention child layer and feed-forward neural child network in a network block in the encoder built by a group of encoder and decoder. However, similar to an encoder, one of the network blocks in the decoder has one more Multi-Headed Attention layer.
To better optimize the deep network, the entire network uses a residual connection and applies Add & Norm to the layer.
Before we get into Multi-Headed Attention, we should first look at the concept of Scaled Dot-Product Attention raised in the article.
Compared to the standard form of Attention described in the background knowledge above, Scaled Dot-Product Attention is a type of Attention that utilizes Scaled Dot-Product to calculate similarity. The difference is that it has an extra dimension (K dimension) for adjustment that prevents the inner product from becoming too large.
Multi-Head Attention structure is outlined in the image below, where the query, key, and value first go through a linear transformation and then enters into Scaled-Dot Attention. Here, the attention is calculated h times, making it so-called Multi-Headed, where each iteration computes ahead. The W parameter is different each time Q, K, and V undergo a linear transformation. Subsequently, the result from the h iterations of Scaled-Dot Transformation Attention is spliced, and the value obtained by further linear transformation is the result of the Multi-Head Attention.
We can see that the thing that makes the Multi-Head Attention proposed by Google is that the calculation is performed h times instead of once. The essay mentions that this allows the model to learn relevant information in different representative child spaces. Then it uses the Attention visualization as verification.
As shown in the image below, we first use Multi-Head Attention as a connection between the encoder and decoder. The key, value, and query respectively are the layer output of the encoder (key=value here), and the output of the Multi-Head Attention in the decoder.
It’s the same as the Attention used in the popular machine translation model, however, in that it uses decoder and encoder Attention to align translations. It uses Multi-Headed Self-Attention between the encoder and decoder to learn the representatives of the text.
In Self-Attention or K=V=Q, if the input is, for example, a sentence, then each word in the sentence needs to undergo Attention computation. The goal is to learn the dependencies between the words in the sentence and use that information to capture the internal structure of the sentence.
As for a reason behind using Self-Attention mechanisms, the paper brings up three main points (complexity of each layer, run-ability, distant dependency learning) and gives comparisons to the complexity of RNN and CNN computations.
We can see that if the input sequence n is smaller than the representative dimension d, then Self-Attention is advantageous regarding the time complexity of each layer.
When n is larger, the author provides a solution in Self-Attention (restricted), where not every word undergoes Attention calculation, instead only r words undergo the calculation.
Regarding concurrency, Multi-Head Attention and CNN both depend on calculations from the previous instance and features better concurrency than RNN.
In distant dependencies, since self-attention is applied to both each word and all words together, no matter how distant they are, the longest possible path is one so that the system can capture distant dependency relationships.
Finally, let’s take a look at some experiment results working with WMT2014 English-German and English-French machine translation tasks where results were far ahead of the pack and training speed was faster than other models.
We can see from the hyperparameter experiment on the model that the h hyperparameter in Multi-Head Attention cannot be too small, and will decline if it’s too large. Overall it is better than model be larger than smaller, and dropout can help with the issue of overfitting.
The author also applies this model to grammar analysis tasks where it performed well.
Last, let’s take a look at the effects of Attention visualization (here different colors represent different Attention results, the deeper the color, the larger the Attention value). We can see that Self-Attention is capable of learning the distant dependencies within the phrase “making… More difficult.”
Comparing two heads and one head, we can see that the single head for “its” is only capable of learning the dependent relationship to “law. However, two heads not only found the dependent relationship between “its” and “law” but between “its” and “application.” Multiple heads can learn related information from representative spaces.
Paper: https://www.paperweekly.site/papers/1786
Source code: https://github.com/XMUNLP/Tagger
This essay comes from the work done by Tan and associates from Xiamen University in AAAI2018. They applied Self-Attention to Semantic Role Labeling tasks with impressive results.
In this essay, the authors treat SRL as an issue of sequence labeling and use BIO tags for the labeling process. They then raised the idea of using a Deep Attentional Neural Network for labeling. The structure is as follows:
In each network block, there is a combination of RNN/CNN/FNN child layer and a Self-Attention child layer. Finally, they used softmax as a method of label classification for sequence labeling.
This model has produced terrific results in both the CoNLL-2005 and CoN11–2012 SRL datasets. We are aware that in the matter of sequence labeling, labels have a dependent relationship. For example, label I should appear after label B and not after label O.
The currently favored model of sequence labeling is the BiLSTM-CRF model which uses CRF for full label optimization. In comparative experiments, the He et al and Zhou and Xu models used CRF and constrained decoding respectively to handle this issue.
We can see that this thesis uses only Self-Attention. The authors believe that the Attention at the top layer of the model is capable of learning the implicit dependent information between the tags.
Paper: https://www.paperweekly.site/papers/1787
Authors: Patrick Verga / Emma Strubell / Andrew McCallum
This essay by Andrew McCallum’s team outlines their application of Self-Attention to relationship extraction tasks in the field of biology and should get accepted by the NAACL 2018. The authors of this essay proposed a biological relationship extraction model on the document level and presented no small amount of work that interested readers are welcome to explore in more detail in the essay.
Here we will simplify the portion where they applied Self-Attention. The model outlined in the paper is shown in the image below. The authors used Google to extract transformers containing Self-Attention and enact representation learning on the input text. The transformer is slightly different from the original. In that, they used a CNN with a window of 5 to replace the original FNN.
Let’s focus on the results of the experiments concerning Attention for a moment. They showed excellent results on a Chemical Disease Relations dataset. After removing the Self-Attention layer we can see a massive drop in results, and using a CNN with a window of 5 instead of the original FNN makes results on this dataset even more apparent.
As a final summary, Self-Attention can be a special situation for typical Attention. In Self-Attention, Q=K=V and Attention is applied to the unit of each sequence and the unit of all sequences.
The Multi-Head Attention method raised by Google uses multiple iterations of computation to capture relevant information from different child spaces. What makes Self-Attention unique is that it ignores the distance between words and directly computes dependency relationships, making it capable of learning the internal structure of a sentence and more merely calculating in parallel.
Read similar articles and learn more about Alibaba Cloud’s products and solutions at www.alibabacloud.com/blog.
Reference:
https://www.alibabacloud.com/blog/self-attention-mechanisms-in-natural-language-processing_593968?spm=a2c4.11999894.0.0
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
484 
1
484 claps
484 
1
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/airbnb-engineering/smartstack-service-discovery-in-the-cloud-4b8a080de619?source=search_post---------267,"There are currently no responses for this story.
Be the first to respond.
By Igor Serebryany & Martin Rhoads
SmartStack is an automated service discovery and registration framework. It makes the lives of engineers easier by transparently handling creation, deletion, failure, and maintenance work of the machines running code within your organization. We believe that our approach to this problem is among the best possible: simpler conceptually, easier to operate, more configurable, and providing more introspection than any of its kind. The SmartStack way has been battle-tested at Airbnb over the past year, and has broad applicability in many organizations, large and small.
SmartStack’s components — Nerve and Synapse — are available on GitHub! Read on to learn more about the magic under the hood.
Companies like Airbnb often start out as monolithic applications — a kind of swiss army knife which performs all of the functions of the organization. As traffic (and the number of engineers working on the product) grows, this approach doesn’t scale. The code base becomes too complicated, concerns are not cleanly separated, changes from many engineers touching many different parts of the codebase go out together, and performance is determined by the worst-performing sections in the application.
The solution to this problem is services: individual, smaller code bases, running on separate machines with separate deployment cycles, that more cleanly address more targeted problem domains. This is called a services-oriented architecture: SOA.
As you build out services in your architecture, you will notice that instead of maintaining a single pool of general-purpose application servers, you are now maintaining many smaller pools. This leads to a number of problems. How do you direct traffic to each machine in the pool? How do you add new machines and remove broken or retired ones? What is the impact of a single broken machine on the rest of the application?
Dealing with these questions, across a collection of several services, can quickly grow to be a full-time job for several engineers, engaging in an arduous, manual, error-prone process fraught with peril and the potential for downtime.
The answer to the problem of services is automation — let computers do the dirty work of maintaining your backend pools. But there are many ways to implement such automation. Let’s first outline the properties of an ideal implementation:
We can use these criteria to evaluate potential solutions. For instance, manually changing a list of backends which has been hard-coded in a client, and then deploying the client, immediately fails many of these criteria by wide margins. How do some other approaches stack up?
Many commonly-used approaches to service discovery don’t actually work very well in practice. To understand why SmartStack works so well, it can be helpful to first understand why other solutions do not.
The simplest solution to registration and discovery is to just put all of your backends behind a single DNS name. To address a service, you contact it by DNS name and the request should get to a random backend.
The registration component of this is fairly well-understood. On your own infrastructure, you can use dynamic DNS backends like BIND-DLZ for registration. In the cloud, with hosted DNS like Route53, simple API calls suffice. In AWS, if you use round-robin CNAME records you would even get split horizon for free, so the same records would work from both inside and outside AWS.
However, using DNS for service discovery is fraught with peril. First, consumers have to poll for all changes — there’s no way to push state. Also, DNS suffers from propagation delays; even after your monitoring detects a failure and issues a de-registration command to DNS, there will be at least a few seconds before this information gets to the consumers. Worse, because of the various layers of caching in the DNS infrastructure, the exact propagation delay is often non-deterministic.
When using the naive approach, in which you just address your service by name, there’s no way to determine which boxes get traffic. You get the equivalent of random routing, with loads chaotically piling up behind some backends while others are left idle.
Worst of all, many applications cache DNS resolution once, at startup. For instance, Nginx will cache the results of the initial name resolution unless you use a configuration file hack. The same is true of HAProxy. Figuring out that your application is vulnerable can be costly, and fixing the problem harder still.
Note that we are here referring to native use of DNS by client libraries. There are ways to use the DNS system more intelligently to do service discovery — where we use Zookeeper in SmartStack, we could use DNS too without losing too much functionality. But simply making an HTTP request to myservice.mydomain.com via an HTTP library does not work well.
If you’re convinced that DNS is the wrong approach for service discovery, you might decide to take the route of centralizing your service routing. In this approach, if service a wants to talk to service b, it should talk to a load balancer, which will properly route the request. All services are configured with the method of finding the load balancer, and the load balancer is the only thing that needs to know about all of the backends.
This approach sounds promising, but in reality it doesn’t buy you much. First, how do your services discover the load balancer? Often, the answer is DNS, but now you’ve introduced more problems over the DNS approach than you’ve solved — if your load balancer goes down, you are still faced with all of the problems of service discovery over DNS. Also, a centralized routing layer is a big fat point of failure. If that layer goes down, everything else goes with it. Reconfiguring the load balancer with new backends — a routine operation — becomes fraught with peril.
Next, what load balancer do you choose? In a traditional data center, you might be tempted to reach for a couple of hardware devices like an F5. But in the cloud this is not an option. On AWS, you might be tempted to use ELB, but ELBs are terrible at internal load balancing because they only have public IPs. Traffic from one of your servers to another would have to leave your private network and re-enter it again. Besides introducing latency, it wreaks havoc on security groups. If you decide to run your own load balancing layer on EC2 instances, you will end up just pushing the problem of service discovery one layer up. Your load balancer is now no longer a special, hardened device; it’s just another instance, just as prone to failure but just especially critical to your operations.
Given the problems of DNS and central load balancing, you might decide to just solve the problem with code. Instead of using DNS inside your app to discover a dependency, why not use some different, more specialized mechanism?
This is a popular approach, which is found in many software stacks. For instance, Airbnb initially used this model with our Twitter commons services. Our java services running on Twitter commons automatically registered themselves with zookeeper, a central point of configuration information which replaces DNS. Apps that wanted to talk to those services would ask Zookeeper for a list of available backends, with periodic refresh or a subscription via zookeeper watches to learn about changes in the list.
However, even this approach suffers from a number of limitations. First, it works best if you are running on a unified software stack. At Twitter, where most services run on the JVM, this is easy. However, at Airbnb, we had to implement our own zookeeper client pool in ruby in order to communicate with ZK-registered services. The final implementation was not well-hardened, and resulted in service outages whenever Zookeeper was down.
Worse, as we began to develop more services for stacks like Node.js, we foresaw ourselves being forced to implement the same registration and discovery logic in language after language. Sometimes, the lack or immaturity of relevant libraries would further hamper the effort. Finally, sometimes you would like to run apps which you did not write yourself but still have them consume your infrastructure: Nginx, a CI server, rsyslog or other systems software. In these cases, in-app discovery is completely impossible.
Finally, this approach is very difficult operationally. Debugging an app that registers itself is impossible without stopping the app — we’ve had to resort to using iptables to block the Zookeeper port on machines we were investigating. Special provisions have to be made to examine the list of backends that a particular app instance is currently communicating with. And again, intelligent load balancing is complicated to implement.
SmartStack is our solution to the problem of SOA. SmartStack works by taking the whole problem out-of-band from your application, in effect abstracting it away. We do this with two separate applications that run on the same machine as your app: Nerve, for service registration, and Synapse, for service discovery.
Nerve does nothing more complicated than what Twitter commons already did. It creates ephemeral nodes in Zookeeper which contain information about the address/port combos for a backend available to serve requests for a particular service.
In order to know whether a particular backend can be registered, Nerve performs health checks. Every service that you want to register has a list of health checks, and if any of them fail the backend is de-registered.
Although a health check can be as simple as “is this app reachable over TCP or HTTP”, properly integrating with Nerve means implementing and exposing a health check via your app. For instance, at Airbnb, every app that speaks HTTP exposes a /health endpoint, which returns a 200 OK if the app is healthy and a different status code otherwise. This is true across our infrastructure; for instance, try visiting https://www.airbnb.com/health in your browser!
For non-HTTP apps, we’ve written checks that speak the particular protocol in question. A redis check, for instance, might try to write and then read a simple key. A rabbitmq check might try to publish and then consume a message.
Synapse is the magic behind service discovery at Airbnb. It runs beside your service, and handles making your service dependencies available to use, transparently to your app. Synapse reads the information in Zookeeper for available backends, and then uses that information to configure a local HAProxy process. When a client wants to talk to a service, it just talks to the local HAProxy, which takes care of properly routing the request.
This is, in essence, a decentralized approach to the centralized load load balancing solution discussed above. There’s no bootstrapping problem because there’s no need to discover the load balancing layer — it’s always present on localhost.
The real work is performed by HAProxy, which is extremely stable and battle-tested. The Synapse process itself is just an intermediary — if it goes down, you will not get notification about changes but everything else will still function. Using HAProxy gives us a whole host of advantages. We get all of the powerful logging and introspection. We get access to very advanced load-balancing algorithms, queueing controls, retries, and timeouts. There is built-in health checking which we can use to guard against network partitions (where we learn about an available backend from Nerve, but can’t actually talk to it because of the network). In fact, we could probably fill an entire additional blog post just raving about HAProxy and how we configure it.
Although Synapse writes out the entire HAProxy config file every time it gets notified about changes in the list of backends, we try to use HAProxy’s stats socket to make changes when we can. Backends that already exist in HAProxy are put into maintenance mode via the stats socket when they go down, and then recovered when they return. We only have to restart HAProxy to make it re-read its config file when we have to add an entirely new backend.
SmartStack has worked amazingly well here at Airbnb. To return to our previous checklist, here is how we stack up:
After using SmartStack in production at Airbnb for the past year, we remain totally in love with it. It’s simple to use, frees us from having to think about many problems, and easy to debug when something goes wrong.
Besides the basic functionality of SmartStack — allowing services to talk to one another — we’ve built some additional tooling directly on top of it.
Charon is Airbnb’s front-facing load balancer. Previous to Charon, we used Amazon’s ELB. However, ELB did not provide us with good introspection into our production traffic. Also, the process of adding and removing instances to the ELB was clunky — we had a powerful service discovery framework, and then we had a second one just for front-facing traffic. Finally, because ELB is based on EBS, we were worried about the impact of another EBS outage.
We were already putting Nginx as the first thing behind the ELB. This is because our web traffic is split between several user-facing apps — https://www.airbnb.com is not served by the same servers as https://www.airbnb.com/help, for example. With Charon, traffic from Akamai hits one of a few Nginx servers directly. Each of these servers is running Synapse, with the user-facing services enabled. As we match the URI against locations, traffic is sent, via the correct HAProxy for that service, to the optimal backend to handle that request.
In effect, Charon makes routing from users to our user-facing services just like routing between backend services. We get all of the same benefits — health checks, the instant addition and removal of instances, and correct queueing behavior via HAProxy.
What we do for user-facing services, we also do for internal services like dashboards or monitors. Wild card DNS sends all *.dyno requests to a few dyno boxes. Nginx extracts the service name from the Host header and then directs you to that service via Synapse and HAProxy.
The achilles heel of SmartStack is Zookeeper. Currently, the failure of our Zookeeper cluster will take out our entire infrastructure. Also, because of edge cases in the zk library we use, we may not even be handling the failure of a single ZK node properly at the moment.
Most recent work has been around building better testing infrastructure around SmartStack. We can now spin up an integration test for the entire platform easily with our Chef cookbook and Vagrant. We hope that with better testing, we can make headway on making SmartStack even more resilient.
We are also considering adding dynamic service registration for Nerve. Currently, the discovery that Nerve performs is hardcoded into Nerve via its config file. In practice, because we manage our infrastructure with configuration management (Chef) this is not a huge problem for us. If we deploy a new service on a machine, the Chef code that deploys the service also updates the Nerve config.
However, in environments like Mesos, where a service might be dynamically scheduled to run on a machine at any time, it would be awesome to have the service easily register itself with Nerve for health checking. This introduces tighter coupling between the services and SmartStack than we currently have, so we are thinking carefully through this approach to make sure we build the correct API.
We would love for SmartStack to prove it’s value in other organizations. Give it a shot, and tell us how it works for you!
Probably the easiest way to get started is by using the Chef cookbook. The code there can create a comprehensive test environment for SmartStack and run it through automated integration tests. It is also the code we use to run SmartStack in production, and provides what we think is a solid approach to managing SmartStack configuration. There is also excellent documentation on running and debugging SmartStack included with the cookbook.
If you would like to configure Nerve and Synapse yourself, check out documentation in their respective repositories. We’ve included example config files to get you started with each.
Scaling a web infrastructure requires services, and building a service-oriented infrastructure is hard. Make it EASY, with SmartStack’s automated, transparent service discovery and registration: cruise control for your distributed infrastructure.
Originally published at nerds.airbnb.com on October 23, 2013.
Creative engineers and data scientists building a world…
397 
5
397 claps
397 
5
Creative engineers and data scientists building a world where you can belong anywhere. http://airbnb.io
Written by
Creative engineers and data scientists building a world where you can belong anywhere. http://airbnb.io
Creative engineers and data scientists building a world where you can belong anywhere. http://airbnb.io
"
https://medium.com/google-cloud/airflow-for-google-cloud-part-1-d7da9a048aa4?source=search_post---------268,"There are currently no responses for this story.
Be the first to respond.
You know Big Data… it’s a dirty business. All the literature shows you how powerful all those data crunchers and query engines are, but it all assumes that all the data is ready to be consumed. In reality a lot of automated Dataflow, Spark and BigQuery ETL processes are glued together with bash or Python.
Well it’s time to change that… and to take a look at Apache Airflow. Airflow is a workflow engine that will make sure that all your transform-, crunch- and query jobs will run at the correct time, order and when the data they need are ready for consumption. No more writing a lot of fragile boilerplate code to schedule, retry and wait: just focus on the workflow.
A lot of work has been purred into Airflow in 2016 to make it a first class workflow engine for Google Cloud. This Medium series will explain how you can use Airflow to automate a lot of Google Cloud products and make it so that your data transitions smoothly from one engine into another. I won’t dive into how to construct DAG’s into Airflow, you should read the docs on that. Basically all DAG’s are build up though Python objects… I will be focusing on the Google Cloud integration.
In this first part we’ll explain how you can automate BigQuery tasks from Airflow.
BigQuery is a very popular for interactive querying very large datasets, but I also love to use it for storing a lot of temporary data. I do prefer it over files on Cloud Storage because you can do some ad-hoc exploratory queries on it. So let’s get start using Airflow to get data in and out of BigQuery.
The first BigQuery integration is executing a query and having the output stored in a new table, this is done with the BigQueryOperator. The operator takes a query (or a reference to a query file) and an output table.
If you look at the destination_dataset_table you will notice the template parameter. Airflow uses the power of jinja templates for making your workflow more dynamic and context aware. In our example it will fill in the ds_nodash with the current execution_date. Execution date in Airflow is the contextual date of your data. For example if your calculation some metrics for the 4th of July the execution_date will be 2017–07–04T00:00:00+0.
You can even use jinja templates in your referenced queries. This is powerful tool because in general the BigQuery API doesn’t support parameters, but with the template engine you can simulate this.
For a full list of whats possible and what kind of parameters are available in the context have a look at the Airflow macro documentation.
In our example the above query file lives on the filesystem next to the example DAG.
Next to querying you want to get your data in and out of BigQuery. Let’s start with extracting to Cloud Storage.
Extracting is a very common scenario and done with BigQueryToCloudStorageOperator. People familiar with the BigQuery API will recognise a lot of the parameter, but basically you have to specify the source table and the destination storage object pattern. It’s important that you specify a pattern though (example: …/part-.avro). The pattern will make sure if you have huge tables that you have multiple storage objects per extract.
Notice again the use of the jinja template parameter. In this case we read variables defined in Airflow, very useful for differentiating between different environments like production or test.
But before doing queries and extraction you need to get the data into BigQuery. That’s done with the GoogleCloudStorageToBigQueryOperator.
Compared to extraction, the load operator does have a few more parameters. The reason is that you need to tell BigQuery a bit of metadata of the imported object are, like the schema and the format. You do this by specifying the schema_fields and source_format.
Another important parameter to specify is the create_disposition and write_disposition. I like my operations repeatable so you can run them again and again, so CREATE_IF_NEEDED and WRITE_TRUNCATE are good defaults. Just make sure your tables can handle it, so use partitioned tables. For now you’ll have to use the classic partitioning with the date suffixed in the name. In a next later release of Airflow we will add full support for the new BigQuery partitioning.
Next in this series we’ll have a look a Airflow driving Dataproc, see you then.
The examples are taken from the Airflow for Google Cloud integration test and examples: https://github.com/alexvanboxel/airflow-gcp-examples
Google Cloud community articles and blogs
303 
8
303 claps
303 
8
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Google Developer Expert for Cloud
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://blog.paytm.com/we-have-launched-an-ai-cloud-for-india-816f85434281?source=search_post---------269,"Download Paytm App
"
https://medium.com/google-cloud/top-13-google-cloud-reference-architectures-c8a697546505?source=search_post---------270,"There are currently no responses for this story.
Be the first to respond.
👋 Hi Cloud Devs!!Last year I created #13DaysOfGCP mini series on Twitter which you all loved. So, here I compiled 13 more common Google Cloud reference architectures. If you were not able to catch it, or if you missed a few days, here I bring to you the summary!
Those the 13 hand picked Google Cloud architectures. I am thankful and really humbled by your participation, engagement and topic suggestions that made this series so much fun, not just for me but also for the rest of the Google Cloud Community members!
Thanks again and 👋 until later 🙂
Google Cloud community articles and blogs
294 
2
294 claps
294 
2
Written by
Developer Advocate @Google, Artist & Traveler! Twitter @pvergadia
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Developer Advocate @Google, Artist & Traveler! Twitter @pvergadia
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@waxzce/joue-la-comme-clever-cloud-comment-nous-avons-surv%C3%A9cu-%C3%A0-un-redressement-judiciaire-68a4b79c902?source=search_post---------271,"Sign in
There are currently no responses for this story.
Be the first to respond.
Quentin ADAM
Dec 13, 2016·9 min read
tl;dr C’est fou comme le temps et le google homepage ranking passent vite : ce billet date de 2016, donc causons si tu as des questions plus actuelles. Clever Cloud va super bien, avec des centaines de clients dans 50 pays, et il n’y a plus aucun risque sur notre stabilité (en clair, le montant total du plan de l’époque à payer sur dix ans est inférieur à notre revenu mensuel d’aujourd’hui et on est en procédure pour sortir en anticipation de notre plan et finaliser toute cette affaire). Le Tribunal de Commerce, c’est cool. Notre avocate roxxe.
Il y a 4 ans chez Clever Cloud, on était ravis de vous parler de notre présence au Web Summit dans la piétaille du monde merveilleux des startups, et de notre passion pour Scala. On kiffe toujours autant Scala, notez.
Aujourd’hui, on a plus intéressant à partager.
Nous sommes très heureux de vous annoncer que Clever Cloud est aujourd’hui sortie d’une procédure de redressement judiciaire entamée en 2015.
Le fait de n’être pas morts est une bonne nouvelle en soi, méritant déjà largement d’être célébrée, me direz-vous, et vous avez bien raison.
Mais au-delà de ça, on se dit qu’il est important d’expliquer en quoi précisément cette procédure de redressement judiciaire nous a aidé, comment nous l’avons préparée, et pourquoi elle est intéressante. Une entreprise, en particulier une startup tech, ne doit pas avoir peur du Tribunal de Commerce.
Nous avons créé Clever Cloud parce que nous sommes convaincus que l’industrialisation de l’hébergement et du déploiement du code informatique permet aux entreprises de travailler plus vite, d’être plus agiles dans leurs marchés, de se concentrer sur leur valeur ajoutée et leurs forces, et d’arrêter de se préoccuper inutilement de leur technologie d’hébergement.
Notre boulot consiste à radicalement changer un processus de déploiement du code informatique long et chiant, pour rendre les développeurs à nouveau heureux.
Au passage du coup, on se fâche souvent avec une vision comptable de l’entreprise, du logiciel, et de sa valeur, vision complètement inadaptée au monde réel : amortir du code comme on amorti l’investissement dans une grosse machine, sur 3 ans, au mieux ça épuise des équipes à maintenir un code obsolète, là où tout réécrire est souvent la bonne mesure à prendre vite, une fois qu’on connaît parfaitement les problèmes de terrain.
C’est assez simple, voire même affligeant de banalité. Aux débuts de Clever Cloud, pendant notre première année (2012–2013), on a essayé de vendre de l’hébergement à des grands groupes, alors que ce n’est pas notre cœur de métier. A cette erreur profonde de positionnement commercial, s’est assez rapidement ajouté le constat que les créateurs de Clever Cloud, moi inclus donc, n’étaient pas de bons gestionnaires (douce litote).
En 2013, nous avons décidé de changer drastiquement notre approche commerciale et de devenir “full developer evangelist”. C’était bien mieux, ça nous correspondait beaucoup plus, mais ce n’était pas suffisant. C’est le moment où les coutures de gestion ont commencé à craquer. Des erreurs de casting sur 2 recrutements nous ont coûté beaucoup trop cher, jusqu’à un 3e, qui est devenu celui de trop.
C’est évidemment précisément l’instant où “le gros projet stratégique pour la boîte” (lire : “celui où on construit nous-mêmes notre propre dépendance malsaine envers un seul gros client”) devient “le gros projet de trop où le gros client oublie malencontreusement de nous payer”. Saupoudrez un gros peu de désalignement sur le fond entre actionnaires et fondateurs, et on a le terrain glissant parfait pour un bon fuck-up des familles. Couronnée en 2014–2015 par un rachat avorté, cette première période de la vie de Clever Cloud ne pouvait que se terminer dans la douleur et la violence. Ou pas.
“Quand la madame de l’URSSAF me dit qu’elle veut couler ma boîte, c’est gênant.” (ça se tweete, non ?)
Dans tous les bons thrillers, il faut un méchant. En l’espèce, on va être obligés de refiler ce rôle à l’URSSAF, le réseau français d’organismes privés à mission de service public, en charge de la collecte des cotisations salariales et patronales destinées à financer le régime général de la Sécurité sociale, ainsi que d’autres organismes ou institutions (le régime de l’Assurance-chômage par exemple). C’est cadeau, ça nous fait plaisir.
Quand l’URSSAF du coin a unilatéralement décidé que Clever Cloud n’était pas une « jeune entreprise innovante », ce qui donne normalement droit à une réduction de charges substantielle et vitale pour nous, on tenait notre gros drama.
Sûrs de notre bon droit, nous décidons donc de porter l’affaire devant le Tribunal de Commerce, avec cette logique simple : “on a un problème qu’on ne peut pas résoudre seuls, aidez-nous à sortir de l’impasse que constitue l’interprétation de l’administration, largement en notre défaveur, et ce n’est pas trop le moment en plus d’ailleurs.”
Deux éléments sont alors déterminants : le partage par un autre entrepreneur en situation délicate de l’info de l’existence du “CCSF” (qui permet un plan d’étalement des dettes fiscales et sociales), qui nous a aidé à commencer à entrevoir les avantages de la démarche (plus d’infos ici et là), et la rencontre avec Marie Robineau, notre avocate spécialisée en procédure collective, présentée par notre CFO en temps partagé, Rodolphe Cadio.
Dès juin 2015, nous nous attachons à restructurer notre dette pour qu’elle soit acceptable dans le cadre d’un Redressement Judiciaire. Concrètement, cela signifie faire des choix dans les paiements que nous réalisons, et dans quel ordre. Si notre dette avait été trop élevée, nous prenions le risque de passer directement par la case liquidation, merci d’avoir joué, bonsoir.
Plusieurs astuces :
Dès celle-ci prononcée, tu n’as plus le droit de payer tes dettes. Tout ton passif est gelé ; une annonce officielle est faite pour que tous les créanciers potentiels se déclarent.
On le répète, la liquidation a été évitée à ce stade parce que notre dette a été efficacement et intelligemment restructurée en amont, grâce aux conseils de notre avocate et au travail efficace de notre directeur financier.
Il existe deux types de redressement judiciaire : la procédure simplifiée, et le redressement judiciaire normal. C’est la première qui a été appliquée à Clever Cloud. Concrètement, cela signifie qu’un mandataire, et non un administrateur, est là pour t’accompagner mais tu continues de gérer seul ton entreprise. Durant le premier mois de procédure, son rôle est de vérifier ton passif.
Posons un conseil d’ami tout de suite : tu ne bullshit JAMAIS le Tribunal de Commerce.
On parle donc d’une collaboration pleine et entière des dirigeants de l’entreprise redressée.
Dès ce moment, les anciens comptes en banque de l’entreprise redressée sont clôturés, de nouveaux comptes spéciaux de redressement judiciaire sont ouverts. On s’amusera avec un détachement certain de la taquinerie chafouine de certaines banques ne respectant pas la loi, qui profitent de ces joyeuses circonstances pour changer arbitrairement conditions et tarifs (et pas exactement à la baisse), ou de certains prestataires comptables qui se sentent soudain pris d’un irrésistible besoin de doubler leurs tarifs de prestation. Elégance quand tu nous tiens.
Un des super-pouvoirs impressionnants du mandataire : écrire au juge-commissaire, qui peut, lui, donner un coup de téléphone bien senti ou émettre une ordonnance en cas d’urgence (et d’abus).
Les créanciers potentiels ont donc deux mois pour déclarer leurs créances auprès du mandataire, suite à l’annonce officielle.
Soudain, des créances imaginaires apparaissent (soyez-y préparé) : point “business model chacal”, il existe des boîtes dont l’unique activité est de déclarer des créances imaginaires auprès de boîtes en redressement judiciaire. Pour des montants ridicules, dans la masse ça passe. Sur le volume, apparemment, ça paie les faux-frais et plus si affinités.
Le dirigeant de l’entreprise redressée fait alors le tri des créances. Chaque créance contestée donne lieu à une ordonnance (on peut contester tout ou partie des créances déclarées) prononcée par le juge-commissaire, qui, si tout se passe bien, a plutôt un regard bienveillant envers l’entreprise redressée (cf. la “collaboration pleine et entière” initiale. On construit une relation de confiance).
L’entreprise redressée et le mandataire proposent un plan d’apurement de la dette n’excédant pas 10 ans. Les mensualités peuvent être progressives, et une année de franchise peut s’appliquer (tu peux commencer à rembourser un an après le jugement). C’est la décision qui vient d’être prise pour Clever Cloud, et qui marque officiellement la sortie du redressement judiciaire.
Arrêtons-nous un moment sur la notion de “Cockroach startup” (ne traduisons pas ça par “jeune pousse de cafard”, résistons au jeu de mot pourri, restez avec moi).
Ok, il faut aimer les coquillettes et la frugalité pour être entrepreneur. Mais il s’agit de bien plus que ça. Il s’agit d’être capable de résister à une bombe atomique. Ça tombe bien, notre métier c’est d’être “always up” ; c’est profondément dans l’ADN de Clever Cloud et dans celui de nos technologies d’être indestructibles, en fait. Alors c’est ce que nous avons fait. Rester debout, par tous les moyens, à outrance.
Il faut aussi rendre un hommage appuyé à la loyauté des employés de Clever Cloud. Quand on sait que la période qui va suivre risque d’être difficile, ce n’est pas la peine d’essayer de le planquer : ça se sait et puis ça se voit. Alors autant être transparent, et expliquer clairement la situation aux employés. C’est le seul moyen de les garder sur le pont. Donner du sens à ce que l’on fait, rappeler la mission de la boîte. C’est la seule chose qui permet d’impliquer les gens, malgré la délicate situation dans laquelle tu les mets. Des centaines de clients sont plus productifs grâce à nos services, et c’est quelque chose dont nous sommes fiers. C’est ça qui rend notre équipe loyale, une mission qui a du sens et qui continue de résonner avec ce que ses membres pensent important. Pas le “fake startup mode, fatboys and free waffles”.
Pendant un redressement judiciaire, tu en profites aussi pour jouer l’écureuil : le moindre centime est économisé au profit de tes fournisseurs “core-services”. Mois après mois, nous avons réussi à mettre quelques noisettes de côté pour pouvoir parer à toute éventualité. Aujourd’hui Clever Cloud est incapable de fermer du jour au lendemain : nous avons suffisamment de trésorerie pour voir venir sereinement les mois à suivre, tout en continuant de faire des investissements dans l’infrastructure et en assumant nos dettes.
A titre personnel, j’ai aussi dû beaucoup apprendre : je suis aujourd’hui bien meilleur en gestion financière, et je comprends également bien mieux les enjeux légaux de mon core business.
Et maintenant ? Avec pas mal de dur travail, auprès de clients sérieux, Clever Cloud a paradoxalement prospéré pendant cette période d’un an de redressement judiciaire, avec une croissance du chiffre d’affaires mensuel récurrent d’environ 10 à 15%. Au point aujourd’hui, à la sortie officielle et définitive de cette procédure, de ne s’être jamais si bien portée.
On peut enfin avancer plus vite : signer avec de gros clients qui nous attendaient, répondre enfin “oui !” aux e-mails des amis qui piaffent d’impatience pour entrer au capital de Clever Cloud.
On est toujours là, plus vivants que jamais, prêts à vous aider à vous concentrer sur ce que vous faites de mieux, et nous laisser gérer pour vous l’automatisation de votre IT. Pour le plus grand bonheur de notre communauté de développeurs, qui ont des trucs plus importants à faire pour supporter la vision liée à leur produit et à leur entreprise.
Cela va sans dire, mais cela ira encore mieux en le disant : je suis disponible pour répondre en privé (quentin.adam@clever-cloud.com) aux questions précises de tout entrepreneur se trouvant en ce moment dans une situation aussi délicate que nous il y a une grosse année.
Merci, bisous.
Et merci 1000 fois à Yann Heurtaux qui a accouché cet article :-)
Quentin ADAM est le CEO de Clever Cloud.
Vous Développez.Nous Déployons.
Clever Cloud est un service d’automatisation de votre usine logicielle : toutes les opérations d’infrastructure ne sont plus à votre charge, au profit de votre cœur de métier.
CEO @clever_cloud PaaS cloud computing company. We industrialize IT management to help developer to be happy and efficient and make organizations move fast.
226 
17
226 
226 
17
CEO @clever_cloud PaaS cloud computing company. We industrialize IT management to help developer to be happy and efficient and make organizations move fast.
"
https://medium.com/pytorch/get-started-with-pytorch-cloud-tpus-and-colab-a24757b8f7fc?source=search_post---------272,"There are currently no responses for this story.
Be the first to respond.
Top highlight
Author: Joe Spisak (PyTorch Product Lead)
PyTorch aims to make machine learning research fun and interactive by supporting all kinds of cutting-edge hardware accelerators. We announced support for Cloud TPUs at the 2019 PyTorch Developer Conference, and this blog post shows you how to use a Cloud TPU for free via Colab to speed up your PyTorch programs right in your browser.
All of the code we’ll walk through below is available in this Colab notebook — we recommend opening it up and selecting “Runtime > Run all” right now before you read the rest of this post! Then you can visit the notebook to edit code and re-run cells interactively after you’ve gotten an overview below. (Many thanks to Mike Ruberry from Facebook and Daniel Sohn from Google for putting this notebook together!)
When you select a TPU backend in Colab, which the notebook above does automatically, Colab currently provides you with access to a full Cloud TPU v2 device, which consists of a network-attached CPU host plus four TPU v2 chips with two cores each. You are encouraged to use all eight of these TPU cores, but the introductory notebook above only drives a single core for maximum simplicity. Additional Colab notebooks are available that demonstrate how to use multiple TPU cores, including this one which trains a network on the MNIST dataset and this one which trains a ResNet-18 architecture on the CIFAR-10 dataset. You can also find additional Colabs and links to PyTorch Cloud TPU tutorials here. All of these Colab notebooks are intended for people who are already familiar with PyTorch. If you haven’t used PyTorch before, we recommend that you check out the tutorials at https://pytorch.org/ before continuing.
The PyTorch support for Cloud TPUs is achieved via an integration with XLA, a compiler for linear algebra that can target multiple types of hardware, including CPU, GPU, and TPU. You can follow the ongoing development of the PyTorch/XLA integration on GitHub here.
In addition to supporting individual Cloud TPU devices, PyTorch/XLA supports “slices” of Cloud TPU Pods, which are multi-rack supercomputers that can deliver more than 100 petaflops of sustained performance. You can learn more about scaling up PyTorch training jobs on Cloud TPU Pods here.
And now, let’s get started with PyTorch on a Cloud TPU via Colab!
The PyTorch/XLA package lets PyTorch connect to Cloud TPUs. In particular, PyTorch/XLA makes TPU cores available as PyTorch devices. This lets PyTorch create and manipulate tensors on TPUs.
PyTorch uses Cloud TPUs just like it uses CPU or CUDA devices, as the next few cells will show. Each core of a Cloud TPU is treated as a different PyTorch device.
See the documentation at http://pytorch.org/xla/ for a description of all public PyTorch/XLA functions. Here xm.xla_device() acquired the first Cloud TPU core ('xla:1'). Other cores can also be directly acquired:
It is recommended that you use functions like xm.xla_device() over directly specifying TPU cores. A future Colab tutorial will show how to easily train a network using multiple cores (or you can look at an example). Tensors on TPUs can be manipulated like any other PyTorch tensor. The following cell adds, multiplies, and matrix multiplies two tensors on a TPU core:
This next cell runs a 1D convolution on a TPU core:
And tensors can be transferred between CPU and TPU. In the following cell, a tensor on the CPU is copied to a TPU core, and then copied back to the CPU again. Note that PyTorch makes copies of tensors when transferring them across devices, so t_cpu and t_cpu_again are different tensors.
Modules and autograd are fundamental PyTorch components. In PyTorch, every stateful function is a module. Modules are Python classes augmented with metadata that lets PyTorch understand how to use them in a neural network. For example, linear layers are modules, as are entire networks. Since modules are stateful, they can also be placed onto devices. PyTorch/XLA lets us place them on TPU cores:
Autograd is the system PyTorch uses to populate the gradients of weights in a neural network. See here for details about PyTorch’s autograd. When a module is run on a TPU core, its gradients are also populated on the same TPU core by autograd. The following cell demonstrates this:
As mentioned above, PyTorch networks are also modules, and so they’re run in the same way. The following cell runs a relatively simple PyTorch network from the PyTorch tutorial docs on a TPU core:
As in the previous snippets, running PyTorch on a TPU just requires specifying a TPU core as a device.
Thank you for reading! If you run into any issues with the Colabs above or have ideas on how to improve PyTorch support for Cloud TPUs and Cloud TPU Pods, please file an issue in our Github repo. As a reminder, you can also try out the more advanced Colab notebooks here. Stay tuned for future posts about accelerating PyTorch with Cloud TPUs!
An open source machine learning framework that accelerates…
533 
2
533 claps
533 
2
Written by
PyTorch is an open source machine learning platform that provides a seamless path from research prototyping to production deployment.
An open source machine learning framework that accelerates the path from research prototyping to production deployment
Written by
PyTorch is an open source machine learning platform that provides a seamless path from research prototyping to production deployment.
An open source machine learning framework that accelerates the path from research prototyping to production deployment
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/intive-developers/approach-to-clean-architecture-in-angular-applications-hands-on-35145ceadc98?source=search_post---------273,"There are currently no responses for this story.
Be the first to respond.
After we have seen in theory how a web application project can be structured according to Clean Architecture, let us see how we can implement this pattern in practice. If you have missed the introduction article, then you can find it here. We will go through all layers and see what’s implemented there. You will find the whole code here.
The sample application is a birthday calendar for elephants. It is kept very simple to clarify the usage of Clean Architecture. It takes data from an API or a MockRepository included within the app and displays all Elephants and their birthdays in a table.
An Angular project could be structured in the following way, starting off with the known structure generated by the angular-cli.
At first, let us have a look at our Core layer. As we know, we should define our core entities, usecases, repository interfaces, and mappers here. To keep the architecture clean and reusable, consider adding inheritance for the usecases and mappers.
The entities of this application are kept very simple, so an ElephantModel contains the elephant's name, its family status (mother, father, baby…) and a Date for its birthday. These are all information that’s our core application needs.
A Typescript interface is sufficient to keep the mapping process of all entities through the whole project consistent. It has only two functions, one to map from the core entity layer, and one to map to the core entity layer.
The usecase consists of one main function, that is called when we run our usecase and returns a RxJS observable. We also define an input parameter S to pass parameters during the usecase execution.
Read and write operations are handled in this application through repositories. Please notice, that only the interfaces are specified there for each repository and that a repository interface does not have to be an actual repository. This means that these interfaces do not need to talk to a relational database or NoSQL store, but to a restful API for instance. As already mentioned earlier, using repository interfaces for querying APIs is a perfect fit, because a lot of APIs are based on CRUD operations which can be perfectly represented as a repository.
Our actual interface for our simple elephants birthday API provides queries to search for an elephant by its ID and list all elephants which are stored in the repository.
Note: Because we will later use this class as a base class for dependency injection with Angular, our repository must be an abstract class. This is caused by Typescript, that does not know interfaces at runtime and dependency injection will fail. Interfaces in Typescript are just present in static code checking but are removed during compiling.
Finally, let us have a look at the core of our architecture pattern — the usecases. In our sample application, our usecases more or less duplicate the functionality of the repository but adding some level of abstraction in between. Remember, due to the dependency rule, usecases can only use layers which are lower than their current layer — in this case, that’s very easy because we only have our core layer and nothing underneath.
But does our usecase need to know where it can find the data? Not necessarily. The only way the usecase can talk to the data source is through the repository interface, which we will provide as a dependency like this:
Note: As an attentive reader you may wonder why there is an Angular annotation on a core layer where theoretically should only be plain Typescript without any external dependencies to frameworks. The reason is, that Angular only has this @Injectable annotation to provide a module via dependency injection. As you may remember, we talked about a fourth layer that was called configuration. If Angular would have such a functionality like Spring Boot or Dagger does, then the configuration layer could take care of our dependency injection. But for now, we have to stick to this solution as long as we do not want to hack the dependency injection mechanism.
Moving on to the data layer, we start implementing the actual repository. To show the usage of the Clean Architecture approach, we implement the repository twice. First, a mock repository, secondly with a REST client that talks to a small API hosted mockAPI.
Both implementations are very similar and consist of three classes:
This approach creates some overhead through the duplicate code and might seem a bit weird at first. Nevertheless, decoupling business logic entities, data layer entities and presentation layer entities can be very useful, because they often have different or additional fields caused by their usage.
There are a lot of scenarios in which the abstraction layer can be handy. In our application, the API, for example, is delivering the birthday of an elephant as milliseconds, but our core logic or data structure is more convenient and suitable with the Date format, so using one entity for both could be problematic. So our mapper simply converts the time formats back and forth.
The repository implementation uses the standard Angular http client and maps the entities from the API format with the help of the mapper class into our applications core entity format.
Since we are now finished with our core business logic, usecases and the repository implementations, our application is ready to run — we just have to show that the application works. This is handled by the presentation layer. This layer can be as Angular as you want because you only make use of all underlying layers and call the usecases here. Since we defined our repositories as injectables, our usescases automatically know where to search for the right repository and— in addition — the repository can be easily exchanged through our interface!
But how does Angular know which repository we want to use? As we saw, we have two repositories implemented in this project — mock and web. We simply have to add one line to the module where we want to provide:
The “useClass” parameter offers the possibility to specify any class that implements the ElephantRepository interface. So just replace “ElephantMockRepository” with “ElephantWebRepository” and our app is ready to go online!
The rest of the presentation layer is really straightforward: Each component that has to access our elephant's list needs to get the right usecase injected and is ready to use the business logic just like it would use a service. Theoretically, the presentation layer should also have its own entity classes to show data on the UI.
Finally, I want to sum up with the benefits and drawbacks that Clean Architecture has to offer:
Pros:
Cons:
I hope you enjoyed my little introduction into the world of Clean Architecture and that it helps you at least with the last mentioned point in cons: Minimize the learning curve! 😊
At intive we’re building great digital products for our…
909 
14
909 claps
909 
14
Written by
Junior Fullstack Developer on his journey 🌎 currently in love with progressive web apps, flutter and micronaut 🤗
At intive we’re building great digital products for our customers. Day by day. We want to share with you our way of doing things, the challenges we face, the tricks and shortcuts we discover. A little peek behind the scenes — welcome to our intive_dev blog!
Written by
Junior Fullstack Developer on his journey 🌎 currently in love with progressive web apps, flutter and micronaut 🤗
At intive we’re building great digital products for our customers. Day by day. We want to share with you our way of doing things, the challenges we face, the tricks and shortcuts we discover. A little peek behind the scenes — welcome to our intive_dev blog!
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://read.acloud.guru/the-cloud-scales-faster-than-your-budget-4460e328edd9?source=search_post---------274,NA
https://medium.com/hackernoon/can-middleware-survive-the-serverless-enabled-cloud-f1b2a28ebf78?source=search_post---------275,"There are currently no responses for this story.
Be the first to respond.
Would you join me in a thought experiment, to explore the possibilities?
Here are our assumptions.
Having accepted the assumptions, Imagine Bezo’s smile, Microsoft following closely, Google hanging on with their vast coffers, and finally, a smile of a kid who got an app running in Serverless in an hour.
How would middleware look like then, and what will be its future? Let’s us divide and conquer; Let’s explore by different types of middleware
A bottom layer of a typical application would include several services and a database. Service might also depend on core middleware services such as message brokers, cache, and stream processors, etc.
The next layer will include integration middleware such as ESBs, workflow engine, and API management tools. They would compose multiple services to deliver specific business use cases.
The next layers provide the end user experience, often via a Web app that runs in the browser or a mobile application.
Finally, there are helper services that will enhance the experience and environment the application operates in and make the make service lifecycle as painless as possible.
Middleware supports each of these layers. Let us explore how each of these is affected by a Serverless-enabled cloud.
This category includes middleware that helps writing and hosting services and applications (e.g., application servers, web servers) and middleware directly used by those applications (e.g., databases, message brokers, cache, and stream processors, etc.).
The core middleware will be where the pitched battle will be fought. Due to the high latencies of wide-area networks, the rest of middleware will be co-located with core middleware.
The application servers are in trouble. Serverless is a direct competitor to application servers. Most of the new apps likely follow Microservice architecture while Serverless is a natural extension to the Microservices. Microservices already have broken down monoliths to loosely coupled services. Moving such applications to Serverless is easy.
Serverless-enabled Cloud will absorb databases, message brokers, stream processors into the PaaS(Platform as a Services). We have two questions to answer. First, will there be enough standalone use cases left for middleware? Second, if Serverless takes over most of the development, will they choose to take over the middleware layer as well?
Serverless is opinionated. Hence, users would time to time run into scenarios that do not match their expectations. On the one hand, this might be a reason enough for middleware to survive. On the other hand, to handle this scenario, well-orchestrated customer service is needed. Moreover, the middleware market depends on relationships and customer service where the vendor and organization both view it as a partnership, not as a take-it-or-leave-it black box. However, customer service and relationships have never been part of mega-clouds DNA. There will be a customer service gap, a gap that is going against mega-cloud’s business model, a gap that is hard to fill. If mega-clouds did not fix this, they would effectively ignore the long tail of applications. This gap can give enough room for middleware companies to survive, to scuff away at mega-clouds, and to even win the long game. Instead, mega-clouds can choose to share some revenue and let middleware vendors handle customer service. So there is hope.
Secondly, even if Serverless controls the bulk of the market, it is not clear whether mega-clouds will absorb all middleware killing independent projects. Middleware is a hard problem, which took decades of time, hordes of best minds, furious disagreements to arrive at the status quo. Without maintenance, middleware code rusts, where most of its endpoints, hardware, and business they represent evolve. Taking them over may be too large an undertaking even to mega-clouds.
Most of these middleware projects have vibrant opensource communities. In which case mega-clouds could go in, contribute, and guide those projects to be the market leader. Even do some marketing on their behalf. In which case, independent middleware that is not part of the opensource project will be in trouble and wither away. Either because it is better or because the real competition is elsewhere, multiple mega-clouds may even cooperate to build one project, as in the case of Kubernetes.
When non-opensource players lead the market, Mega-clouds will pay a licensing fee or do a revenue share like currently done with Oracle databases and Microsoft Windows. One one hand, markets for independent developments of middleware will contract with an ascent of Serverless. On the other hand, middleware companies will have new revenue streams by licensing to Serverless. However, in this relationship mega-clouds would have an advantage is pricing negotiations. Middleware companies will see a revenue increase and margin erosion. It is not clear which one will win.
If the licensing fee is too large and the law does not protect middleware, mega-clouds might choose to write a new version. The outcome will depend on the complexity of the middleware. Rewriting is no simple undertaking. They will find that they lack talent and understanding of the problem after burning millions. Complexity is enemy of mega-clouds; if they try rewriting, they might break under their size like ancient empires.
When law protects middleware, or when they are complicated to build, mega-clouds can buy them. Mega-cloud could operate those companies while controlling “the margin for their use ” and while selling for standalone use cases at higher margins.
Integration middleware creates the next layer composing and integrating functions, services, and APIs. Among examples are ESBs, workflow engine, and API management tools.
You might argue that integration middleware is not Serverless. That is the reason the post is titled “Serverless-enabled middleware” not Serverless. The competition is between iPaaS and on-premise integration middleware.
However, the battle is likely won or lost elsewhere. Performance of integration middleware depends on the latency to reach services they integrate. If most services, APIs, and functions are in the cloud, then integration middleware in the cloud has a significant advantage over their on-premise counterparts and vice versa. If both the integration middleware and services they integrate are in the Cloud, they would have much better network connections connecting them, hence would have better latency. Moreover, mega-clouds might be able to do optimizations to co-locate dependent parts of the same application.
For limited cases where latency does not matter, the outcome will depend on feature gaps between iPaaS and on-premise middleware. Historically, on-premise middleware stayed ahead in-terms of features. However, they will have an uphill battle to flight as iPaaS matures.
Helper services include security, observability, tracing and debugging, operations, anomaly and Fraud detection, Application lifecycle management (e.g., CI/CD, Versioning, and different load balancing schemas such as canaries, blue-green deployments, etc.).
In my opinion, this is the strength of mega-clouds. Since they control the environments they run in; they can invest to provide exceptional helper services that a developer would get by default. This process has already begun. Although the same is possible in on-premise deployments, setting up everything and make it work takes significant time, energy, and vision. So such environments are only available for developers in large organizations.
Mega-clouds will not directly affect on-premise helper middleware. Instead, mage-clouds will use the helper middleware themselves. However, helper middleware might find their user base increasingly moving to the cloud. This will leads to similar dynamics as we discussed in the core middleware section.
Serverless platforms will also challenge editors and tools. They will need to build a close integration with Serverless. Mega-clouds will encourage editors and tools to support Serverless. However, unlike other middleware, they are less challenged by the Serverless. For example, developers may build their applications on their laptops and push them into the Serverless cloud without any runtime latency differences. It is likely that mega-clouds will be happy to let editors alone.
Concerns about “vendor lock-in” due to lack of standards is the Biggest risk faced by Serverless. The real concern is not serverless functions, but the helper and platform services such as security, observability, required by those functions, without which it is impossible to build meaningful applications. It is hard to abstract away those services efficiently in the absence of standards like SQL for API calls.
Current Serverless market leaders are resisting standardization. It is not clear what mega-clouds should worry about more, other mega-clouds or antipathy to vendor locking. If mega-clouds cooperation to make applications portable, it could expand the pie to the extent so that everyone is better off. It will take time for those scenarios to play out. Another possibility is standardizations enforced by the government. Although chances are small, it is not impossible in a world where GDPR is possible. Standardization, in any form, will significantly hasten the adoption of serverless.
Private Serverless Platforms (PSPs), like Apache OpenWhisk, tries to exploit this concern. The argument is that an organization can get most of the advantages of Serverless, by running a PSP without depending on mega-clouds. This is a strategic answer by middleware companies like IBM to combat Serverless threat faced by middleware.
Private Serverless platforms, however, faces two challenges.
First, without platform services, PSPs loses most of its vitality. PSPs must add databases, without which it is a deal breaker due to stateless nature Serverless. Effect of lack of other platform services remains to be seen. My bets, sadly, is not with PSPs.
Second, PSPs will provide cost savings only if the organization can run a large enough Serverless platform that can offer economies of scale. Running PSPs in mega-cloud IaaS offerings would fail. If it is working, mage-clouds can, in response, make their IaaS expensive. Instead, it is interesting to explore the possibility of multiple small organizations to pool their resources into one Serverless platform securely.
By open-sourcing most of their Serverless platform implementation, Microsoft has also taken an interesting position where companies can choose to run the same tools on-premise as well.
Machine learning algorithms are already moving towards specialized-hardware, towards GPU, and towards systems that are crafted for performance.
It is unlikely that Serverless would replace machine learning algorithms. The Same is true for low latency applications such as algorithmic trading, systems managing utilities, and AR and VR applications.
Serverless can fight back by offering support to co-located related applications physically in the same machine, using optimization techniques such as machine learning or based on human discretion, just like what Kubernetes did with pods. That would handle some use cases, but not all.
Serverless can also grab a part of the market by providing pre-canned versions of well-known Machine Learning algorithms.
Even if this market segment resisted Serverless, in the grand schema of things, it would only be a small part of the market. It will shrink with time and only prolong the inevitable.
Middleware shall live in interesting times. However, all is not lost. A lot will depend on core middleware. On-premise middleware can’t afford to let service hosting move to cloud (e.g., via Serverless). If service hosting is lost, dominos will start to fall. All else will be lost. In my opinion, on this battleground patents and trade secrets will serve well rather than opensource software as they play right into the hands of the mega-clouds.
Hope this was useful if you enjoyed this post you might also find the following interesting.
medium.com
hackernoon.com
#BlackLivesMatter
545 
4
Some rights reserved

how hackers start their afternoons. the real shit is on hackernoon.com. Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
545 claps
545 
4
Written by
A scientist, software architect, author, Apache member and distributed systems programmer for 15y. Designed Apache Axis2, WSO2 Stream Processor... Works @WSO2
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
A scientist, software architect, author, Apache member and distributed systems programmer for 15y. Designed Apache Axis2, WSO2 Stream Processor... Works @WSO2
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/aws-enterprise-collection/cloud-native-or-lift-and-shift-99970053b25b?source=search_post---------276,"There are currently no responses for this story.
Be the first to respond.
“There is always tension between the possibilities we aspire to and our wounded memories and past mistakes.” — Sean Brady
I talk to a lot of executives who are debating different migration approaches for the applications in their IT portfolio. While there’s no one-size-fits-all answer to this question, we spend a good deal of time building migration plans with enterprises using a rubric that takes into account their objectives, the age/architecture of their applications, and their constraints. The goal is to help them bucket the applications in their portfolio into one of 6 Migration Strategies.
In some cases, the choices are obvious. We see a lot of organizations migrating their back-office technology and end-user computing applications to an as-a-service model (“re-purchasing” toward vendors like Salesforce and Workday); a number of organizations will look for opportunities to retire systems that are no longer in use; and some organizations will choose to later revisit systems that they don’t feel they have the appetite or capabilities to migrate yet (i.e. the Mainframe, though You Can Migrate Your Mainframe to the Cloud).
In other cases, the approach isn’t so obvious. In my previous post, I touched on the tension between re-architecting and rehosting (a.k.a. “lift-and-shift”). I’ve heard a lot of executives — including myself, before I learned better — suggest that they’re only moving to the cloud if they “do it right,” which usually means migrating to a cloud-native architecture. Other executives are biased toward a rehosting strategy because they have a compelling reason to migrate quickly (for example, a data center lease expiry), want to avoid a costly refresh cycle, or simply need a quick budget win, which tends to be in the neighborhood of 30% when you’re honest about your on-premises TCO.
Somewhere in the middle of rehosting and re-architecting is what we call re-platforming, where you’re not spending the time on a complete re-architecture, but, rather, making some adjustments to take advantage of cloud-native features or otherwise optimize the application. This common middle ground includes right-sizing instances using realistic capacity scenarios that can easily be scaled up rather than overbought, or moving from a pay-for product like WebLogic to an open-source alternative like Tomcat.
So which approach is more often right for your organization?
Without talking to you about your specific opportunities and constraints (which I’m happy to do; just drop me a note), it’s hard to give a definitive answer, but I can highlight a few anecdotes that should help shape your perspective.
The first is a quote from Yury Izrailevsky’s blog. Yury is the Vice President of Cloud and Platform Engineering at Netflix, and is a well-respected thought leader in our industry.
“Our journey to the cloud at Netflix began in August of 2008, when we experienced a major database corruption and for three days could not ship DVDs to our members. That is when we realized that we had to move away from vertically scaled single points of failure, like relational databases in our datacenter, towards highly reliable, horizontally scalable, distributed systems in the cloud. We chose Amazon Web Services (AWS) as our cloud provider because it provided us with the greatest scale and the broadest set of services and features. The majority of our systems, including all customer-facing services, had been migrated to the cloud prior to 2015. Since then, we’ve been taking the time necessary to figure out a secure and durable cloud path for our billing infrastructure as well as all aspects of our customer and employee data management. We are happy to report that in early January, 2016, after seven years of diligent effort, we have finally completed our cloud migration and shut down the last remaining data center bits used by our streaming service!
Given the obvious benefits of the cloud, why did it take us a full seven years to complete the migration? The truth is, moving to the cloud was a lot of hard work, and we had to make a number of difficult choices along the way. Arguably, the easiest way to move to the cloud is to forklift all of the systems, unchanged, out of the data center and drop them in AWS. But in doing so, you end up moving all the problems and limitations of the data center along with it. Instead, we chose the cloud-native approach, rebuilding virtually all of our technology and fundamentally changing the way we operate the company … Many new systems had to be built, and new skills learned. It took time and effort to transform Netflix into a cloud-native company, but it put us in a much better position to continue to grow and become a global TV network.”
Yury’s experience is both instructive and inspirational, and I’m certain that Netflix’s re-architecting approach was right for them.
But most enterprises aren’t Netflix, and many will have different drivers for their migration.
When I was the CIO at Dow Jones several years ago, we initially subscribed to the ivory tower attitude that everything we migrated needed to be re-architected, and we had a relentless focus on automation and cloud-native features. That worked fine until we had to vacate one of our data centers in less than 2 months. We re-hosted most of what was in that data center into AWS, and sprinkled in a little re-platforming where we could to make some small optimizations but still meet our time constraint. One could argue that we would not have been able to do this migration that quickly if we didn’t already have the experience leading up to it, but no one could argue with the results. We reduced our costs by more than 25%. This experience led to a business case to save or reallocate more than $100 million in costs across all of News Corp (our parent company) by migrating 75% of our applications to the cloud as we consolidated 56 data centers into 6.
GE Oil & Gas rehosted hundreds of applications to the cloud as part of a major digital overhaul. In the process, they reduced their TCO by 52%. Ben Cabanas, one of GE’s most forward-thinking technology executives, told me a story that was similar to mine — they initially thought they’d re-architect everything, but soon realized that would take too long, and that they could learn and save a lot by rehosting first.
One of my favorite pun-intended quotes comes from Nike’s Global CIO, Jim Scholefield, who told us that “Sometimes, I tell the team to just move it.”
Cynics might say that rehosting is simply “your mess for less,” but I think there’s more to it than that. I’d boil the advantage of rehosting down to 2 key points (I’m sure there are others; please write about them and we’ll post your story…) —
First, rehosting takes a lot less time, particularly when automated, and typically yields a TCO savings in the neighborhood of 30%. As you learn from experience, you’ll be able to increase that savings through simple replatforming techniques, like instance right-sizing and open source alternatives. Your mileage on the savings may vary, depending on your internal IT costs and how honest you are about them.
Second, it becomes easier to re-architect and constantly reinvent your applications once they’re running in the cloud. This is partly because of the obvious toolchain integration, and partly because your people will learn an awful lot about what cloud-native architectures should look like through rehosting. One customer we worked with rehosted one of its primary customer-facing applications in a few months to achieve a 30% TCO reduction, then re-architected to a serverless architecture to gain another 80% TCO reduction!
Re-architecting takes longer, but it can be a very effective way for an enterprise to re-boot its culture and, if your application is a good product-market fit, can lead to a healthy ROI. Most importantly, however, re-architecting can set the stage for years and years of continual reinvention that boosts business performance in even the most competitive markets.
While I still believe there’s no one-size-fits-all answer, I’d summarize by suggesting that you look to re-architect the applications where you know you need to add business capabilities that a cloud-native architecture can help you achieve (performance, scalability, globality, moving to a DevOps or agile model), and that you look to rehost or re-platform the steady-state applications that you aren’t otherwise going to repurchase, retire, or revisit. Either migration path paves the way for constant reinvention.
What’s your experience been?
Keep building,- Stephenorbans@amazon.com@stephenorbanRead My Book: Ahead in the Cloud: Best Practices for Navigating the Future of Enterprise IT
Note: “Reinvention” is the fourth (and never-ending) stage of adoption I’m writing about in the Journey to Cloud-First series. The first stage is “Project,” the second stage is “Foundation,” and the third is “Migration.” This series follows the best practices I’ve outlined in An E-Book of Cloud Best Practices for Your Enterprise. Stay tuned for more posts in this series.
Both of these series are now available in my book Ahead in the Cloud: Best Practices for Navigating the Future of Enterprise IT.
Tales of AWS in the Enterprise
226 
2
226 claps
226 
2
Written by
Husband to Meghan, father to Harper and Finley. GM of AWS Data Exchange (formerly Head of Enterprise Strategy for AWS). Author of “Ahead in the Cloud”.
Tales of AWS in the Enterprise
Written by
Husband to Meghan, father to Harper and Finley. GM of AWS Data Exchange (formerly Head of Enterprise Strategy for AWS). Author of “Ahead in the Cloud”.
Tales of AWS in the Enterprise
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/evenbit/on-collision-course-with-cloud-firestore-7af26242bc2d?source=search_post---------277,"There are currently no responses for this story.
Be the first to respond.
This is the opening line for a concept called “distributed counters” in the Cloud Firestore documentation, which goes on to explain a fairly complicated random distribution of shard counters to reduce the probability of collisions upon document write. But “reduce the probability” are the words to pay attention to.
So, what’s the problem here? What happens if you ignore the warnings and decide to try your luck?
This example will use Firebase cloud functions to simulate aggressively simultaneous actors to our highly popular (and also fictive) movie review application.
First we’ll make a naive example of just boldly going for it. No transactions, no regrets.
The cloud function is triggered on each new review, counting and aggregating the number of reviews to get an average review score. But it’s not hard to imagine what kind of problems that will arise as soon as the pace of writes are picking up.
As expected, we can see that there are several simultaneous cloud function executions that all read an old state of the counter, hence creating inconsistency in the counter. We did indeed fail to count all reviews.
Perhaps we’re having better luck if we’re wrapping it in a transaction so that the operation will retry until we can get our write operation through?
Unfortunately, as you can see in the logs below, the operation is indeed retrying as we want and expect (see the highlighted #16). But the result is less satisfying as it pushes the Firestore to a congested state where an exception is thrown.
It’s starting to be clear that we can rule out any simple and magical solution for this. The distributed counter is starting to sound like a good idea to try out. The Google Cloud Datastore (as in Google App Engine) is the very same technology on which Firestore is build upon, and the shard counter concept has been around for quite a long time. Long before Firebase realtime database existed.
cloud.google.com
Cloud Firestore is sitting conveniently close to the Firebase Realtime Database, and the two are easily available to use, mix and match within an application. You can freely choose to store data in both places for your project, if that serves your needs.
So, why not use the Realtime database for one of its strengths: to manage fast data streams from distributed clients. Which is the one problem that arises when trying to aggregate and count data in the Firestore.
First we add a little helper cloud function that scaffolds a default state of aggregated counter and average review score. Please note that this is written to the realtime Database.
After that, we’ll modify the cloud function that was making a transaction on the Firestore, above. Change the transaction to run on the realtime database instead.
Once again, pay attention to that we’re running the transaction against the realtime database to handle the high volume of writes better. And as you can see in the log below, the RTDB handles it with grace.
The use of realtime database in this case is much easier to manage and setup than the shard counter. And we also don’t need to worry about having to “reduce the probability” of document write collisions. We simply leave that to the RTDB implementation, which is designed to handle exactly this kind of high pace concurrent writes.
For further exercise, you might want to find a way to mirror the counters and aggregated values back to Firestore in a controlled way. I’d be happy to see any actual implementation that uses this method and also mirrors the values back to a Firestore document in a smart way.
The odd bit of technology
670 
6
Some rights reserved

670 claps
670 
6
The odd bit of technology
Written by
Google Developer Expert for Firebase, nerd and passionate problem solver | Founder of Kumpul coworking space in Bali
The odd bit of technology
"
https://medium.com/sicara/build-own-cloud-kubernetes-raspberry-pi-9e5a98741b49?source=search_post---------278,"There are currently no responses for this story.
Be the first to respond.
Read the original article on Sicara’s blog here.
Managing several Raspberry Pi can be a lot of work. This article will teach you how Kubernetes and Docker will make your life easier.
I own 4 Raspberry Pi and I got interested in Kubernetes when I was tired of managing my Raspberry Pi and keeping track of what was installed and running on which machine.
Using Docker containers allows me to make sure my applications are packaged with their dependencies. Kubernetes is a platform that manages containers on hosts. It allocates the containers on the available Raspberry Pi. I can pull out one of them to do something else, and when I’m done, I add it back into the cluster.
For this step, I won’t reinvent the wheel. A tutorial has been made to install Kubernetes on the Raspberry Pi. This gist is updated on a regular basis to keep up with any breaking changes.
This can be tedious because you have to repeat some steps (like burning the SD cards, install Docker, etc.) for each Raspberry Pi. It took around an hour for four machines.
Read carefully, some steps are to be performed on the master node only and others are to be performed on all the nodes (or all the slave nodes).
The only thing that didn’t work as mentioned in the tutorial is getting the 3/3 Running on the kube-dns pods. It only showed 3/3 Running after I launched the command.
Take your time to get it working. I’ll wait for you.
If you don’t have a cluster of Raspberry Pi available but you still want to try Kubernetes on one machine, you can use Minikube.
Maybe you didn’t notice but if you followed the tutorial to install Kubernetes on Raspbian, you ran an elaborate Hello world example when you executed
Take a look at the function.yml file. There are two files in the file. You created a service which maps the port 8080 of your pod to the port 31118 of your cluster. You also created a deployment of one pod from the Docker image functions/markdownrender:latest-armhf exposing an API on port 8080. Your API is now available on port 31118 from outside of the cluster.
You interact with Kubernetes with the kubectl command. Here you read a configuration file and create the objects described in the file.
Pods are the base unit of Kubernetes. It can contain containers and volumes. All containers within a pod share the same port space and IP address (they can find each other on localhost).
Deployments are a way to manage multiple pods together (for example replications of the same pod). They are the preferred way of managing the state of the cluster (e.g. “I want 3 replicas of this pod from that image).
Services are the network interfaces of the pods. It allows mapping the port of the pods to external port. They also act as load balancers when you use replicas.There are a lot of different objects corresponding to different functionalities (like jobs, configuration, replication, etc.).
I use my cluster as a home server to host applications. For example, it serves as a backend for my humidity and temperature monitoring sensors, following a water damage in my apartment. I log my data in InfluxDB and plot them with Grafana. Kubernetes answers my problem because:
I also have deployed some applications like my burndown chart app that I use to track my goals. Before that, I was using Heroku on the free tier but the application was slow to start and it was public. Now:
My other use case with my cluster is the experimentation with distributed systems. What happens if I launch two MySQL pods on the same data volume? How many messages/second can I send through RabbitMQ? Is it easy to set up Consul? How fast is eventual consistency in Cassandra?
You can deploy an image in a few lines of configuration and set up your experiments.
Overall, Kubernetes answers my needs: I can host my applications without having to manage individual machines. There are also some bonuses:
It’s a bit over-engineered but as it’s designed to work on a huge number of hosts with a huge number of pods, it works really well on my small setup.
Want more articles like this one? Don’t forget to hit the Follow button!
We build tailor-made AI and Big Data solutions for amazing…
576 
5
Thanks to Alexandre Sapet, and Vincent Quagliaro. 
576 claps
576 
5
Written by
Alexandre Chaintreuil, Software Engineer @datadoghq
We build tailor-made AI and Big Data solutions for amazing clients
Written by
Alexandre Chaintreuil, Software Engineer @datadoghq
We build tailor-made AI and Big Data solutions for amazing clients
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/rigetti/introducing-rigetti-quantum-cloud-services-c6005729768c?source=search_post---------279,"There are currently no responses for this story.
Be the first to respond.
By Chad Rigetti
Quantum computing is approaching a pivotal milestone called quantum advantage. This is the inflection point where quantum computers first begin to solve practical problems faster, better, or cheaper than otherwise possible. The first demonstration of quantum advantage will be an extraordinary achievement, but it will only be the beginning. Ultimately, quantum advantage will be reached over and over again in new markets and new domains, changing the ways in which problems are solved across industries.
Three key capabilities are essential to achieving quantum advantage. First, users need more qubits with lower error rates. Second, users need computing systems designed to run the hybrid quantum-classical algorithms that offer the shortest path to quantum advantage. Finally, these capabilities must be delivered alongside a real programming environment so users can build and run true quantum software applications.
In August, we announced that we are building 128-qubit quantum computers with the low error rates needed to achieve advantage. These systems are based on our scalable 16, 32, and 128-qubit Aspen quantum processors. And today, to deliver the final key capabilities, we are excited to introduce Quantum Cloud Services.
Quantum Cloud Services is the only quantum-first cloud computing platform. With QCS, for the first time, quantum processors are tightly integrated with classical computing infrastructure to deliver the application-level performance needed to achieve quantum advantage.
Users access these integrated systems through their dedicated Quantum Machine Image. The QMI is a virtualized programming and execution environment designed for users to develop and run quantum software applications. Every QMI comes pre-configured with Forest 2.0, the latest version of our industry-leading software development kit.
We will be granting early access to Quantum Cloud Services in the coming weeks. You can sign up to reserve a QMI today at rigetti.com.
Partnerships with leading startups We’re partnering with visionary startups who are building the first generation of practical quantum applications. These companies include 1QBit, Entropica Labs, Heisenberg Quantum Simulation, Horizon Quantum Computing, OTI Lumionics, ProteinQure, QC Ware, Qulab, QxBranch, Riverlane Research, Strangeworks, and Zapata Computing. They will use QCS to develop groundbreaking applications and as a channel to distribute these applications to the broader community, putting even more tools into the hands of developers and researchers.
The Quantum Advantage Prize Quantum Cloud Services has been designed from the bottom-up to accelerate the pursuit of quantum advantage. We don’t know when the first demonstration of quantum advantage will be achieved, or what shape it will take, but one thing is certain: it will dramatically accelerate progress in unlocking the power of quantum computing for everyone. Recognizing the significance of this achievement, Rigetti Computing is offering a $1 million prize for the first conclusive demonstration of quantum advantage on QCS. More details of the prize will be announced on October 30th, 2018. Stay tuned!
Rigetti Computing
532 
532 claps
532 
Rigetti Computing
Written by
On a mission to build the world’s most powerful computer.
Rigetti Computing
"
https://medium.com/firebasethailand/%E0%B8%AA%E0%B8%A3%E0%B9%89%E0%B8%B2%E0%B8%87-cron-job-%E0%B8%94%E0%B9%89%E0%B8%A7%E0%B8%A2-cloud-functions-for-firebase-%E0%B8%87%E0%B9%88%E0%B8%B2%E0%B8%A2%E0%B8%99%E0%B8%B4%E0%B8%94%E0%B9%80%E0%B8%94%E0%B8%B5%E0%B8%A2%E0%B8%A7-fe8cb64cec52?source=search_post---------280,"There are currently no responses for this story.
Be the first to respond.
Top highlight
หนึ่งในความสามารถของ Cloud Functions for Firebase คือการสร้าง API ให้นักพัฒนาสามารถเรียกใช้จากภายนอกได้ โดยคำถามหนึ่งที่ทีม Firebase ได้รับบ่อยๆคือ จะ schedule ฟังก์ชันเหล่านั้นแบบ Cron Job ได้อย่างไรบ้าง
ก่อนหน้านี้ทีม Firebase ก็แนะนำว่าให้ไปใช้ App Engine, Cloud Pub/Sub หรือ thrid-party อย่าง cron-job.org ในการ schedule เพื่อเรียก API เหล่านั้น
แต่การไปสร้าง scheduler ใน App Engine หรือ Cloud Pub/Sub ก็จะมี learning curve สำหรับคนที่ยังไม่เคยไปสัมผัสมาก่อน หรือแม้กระทั่งการใช้งาน thrid-party ก็ถือว่าไม่ปลอดภัยซะทีเดียวเนื่องจาก Endpoint URL ของเรา ได้ถูก publish ออกไป ซึ่งแปลว่าใครรู้ URL ก็สามารถ request ได้นั้นเอง
และเมื่อไม่นานมานี้เองทีม GCP ได้ออกบริการใหม่ชื่อ Cloud Scheduler ซึ่งเป็นบริการให้เรา schedule ตัว API และ Cloud Pub/Sub ได้ ทีม Firebase จึงไม่รอช้าที่จะพัฒนาโซลูชั่นแบบครบวงจรใน Cloud Functions for Firebase บน Cloud Scheduler เพื่อความปลอดภัยในการ schedule ฟังก์ชัน ให้ง่ายกว่าทุกทางที่เคยแนะนำไป และทำงานได้ 24 x 7 แถม recurring ได้ด้วย
ก่อนจะไปดูวิธีการ ใครที่ยังไม่รู้จัก Cloud Functions for Firebase จงไปทำความรู้จัก และติดตั้งให้พร้อมกับการพัฒนากันก่อนที่บทความนี้
medium.com
สิ่งแรกที่ทุกคนต้องทำ ก็คือตรวจสอบดูก่อนว่าเครื่องมือเราอัพเดทล่าสุดหรือยัง โดย Firebase CLI ต้องเป็นเวอร์ชันตั้งแต่ 6.7.1 ขึ้นไป และ firebase-functions ต้องเป็นเวอร์ชันตั้งแต่ 2.3.0 ขึ้นไป จึงจะใช้งานได้ หากเก่ากว่านี้ให้อัพเดทตามนี้ที่โฟลเดอร์ functions ของตะเองนะ
เมื่อพร้อมแล้ว มาดูตัวอย่างกัน โดยผมจะตั้งเวลาให้ฟังก์ชันพ่น log ทุกนาที
ดูผลลัพธ์ที่ได้ใน Firebase Console เพื่อยืนยันความสำเร็จกันหน่อย
เฮ้ย! แล้วโค้ดมันสั้นแค่นี้เหรอ? …เออ แค่นี้แหละ จริงๆ :)แต่ผู้อ่านคงงงว่า ดอกไม้จันทร์ 5 ดอก(“* * * * *”) มันคืออะไรช่ายมะ งั้นมาดูรูปกัน
แต่ละหลัก แทนที่เรื่องของวันเวลาที่เราจะ schedule
ตัวอย่างเช่น
นอกจากนี้เรายังสามารถเปลี่ยน “* * * * *” ไปเป็นประโยคที่บอกเวลา(ภาษาอังกฤษ) ได้
Hooray! มีการรายงานสภาพอากาศมาให้ทุกนาทีเรียบร้อย
หวังว่าบทความนี้จะเป็นประโยชน์กับผู้ที่ต้องการตั้ง Cron Job ให้ปลอดภัยและง่ายในการพัฒนา อีกทั้งยืดหยุ่นมากๆสำหรับการตั้งเวลา ซึ่ง use case หนึ่งที่ผมนึกออกก็คือ Cron Job วันหวยออก ทุกวันที่ 1 และ 16 ของทุกเดือนช่วงเวลา 16:30 เราก็สามารถทำได้อย่างง่ายดายละ สำหรับวันนี้ขอตัวลาไปก่อน จนกว่าจะพบกันใหม่บทความหน้า ราตรีสวัสดิ์พี่น้องนักพัฒนาชาวไทย
Let you learn and share your Firebase experiences with each…
387 
387 claps
387 
Written by
Technology Evangelist at LINE Thailand / Google Developer Expert - 🔥Firebase
Let you learn and share your Firebase experiences with each other.
Written by
Technology Evangelist at LINE Thailand / Google Developer Expert - 🔥Firebase
Let you learn and share your Firebase experiences with each other.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://faun.pub/the-dos-and-don-ts-of-cloud-devops-62ff8904e4ac?source=search_post---------281,"There are currently no responses for this story.
Be the first to respond.
If you’re in software development, or you’re in IT Operations and support software developers, you’re probably familiar with DevOps and cloud. The two are quickly becoming intertwined, leaving folks scrambling to figure out how to make them work together.
As organizations implement DevOps best practices and improve how they build software, the need for better and faster ways to deploy their…
"
https://itnext.io/google-cloud-advantages-over-aws-28751469e570?source=search_post---------282,"This post is not about GCP vs. AWS. There are a lot of articles comparing both cloud providers. I’m mainly an AWS user but recently I have been working with GCP and, although AWS is much more mature and has a lot of more services, GCP has a couple of services and few advantages that make it a better provider for certain use cases. This post is a short summary of the advantages of GCP over AWS.
If you are an AWS user, Google has a nice article explaining the similarities and differences, so I will focus on where I think GCP…
"
https://netflixtechblog.com/netflix-cloud-packaging-in-the-terabyte-era-d6869b4b84ae?source=search_post---------283,"By Xiaomei Liu, Rosanna Lee, Cyril Concolato
Behind the scenes of the beloved Netflix streaming service and content, there are many technology innovations in media processing. Packaging has always been an important step in media processing. After content ingestion, inspection and encoding, the packaging step encapsulates encoded video and audio in codec agnostic container formats and provides features such as audio video synchronization, random access and DRM protection. Our previous tech blog Packaging award-winning shows with award-winning technology detailed our packaging technology deployed on the streaming side.
As Netflix becomes a producer of award winning content, the studio and content creation needs are also pushing the envelope of technology advancements. As an example, cloud-based post-production editing and collaboration pipelines demand a complex set of functionalities, including the generation and hosting of high quality proxy content. Supporting those workflows poses new challenges to our packaging service.
Apple ProRes encoded video and PCM audio with Quicktime container are one of the most popular formats in professional post-production editing. The ProRes codec family provides great editing performance and image quality. ProRes 422 HQ offers visually lossless preservation of the highest quality professional HD video and is a great video format choice for high quality editing proxy.
As described by the white paper Apple ProRes (link), the target data rate of the Apple ProRes HQ for 1920x1080 at 29.97 is 220 Mbps. With the wide adoption of 4K content across the production pipeline, the generation of ProRes 422 HQ at the resolution of 3840x2160 requires our processing pipeline to encode and package content in the order of terabytes. The following table gives us an example of file sizes for 4K ProRes 422 HQ proxies.
A simplified view of our initial cloud video processing pipeline is illustrated in the following diagram. The inspection stage examines the input media for compliance with Netflix’s delivery specifications and generates rich metadata. This metadata includes both file level information such as video encoding format, video frame rate, and resolution, as well as frame level information such as frame offset, frame dependency, and frame active region to facilitate downstream processing stages. After the inspection stage, we leverage the cloud scaling functionality to slice the video into chunks for the encoding to expedite this computationally intensive process (more details in High Quality Video Encoding at Scale) with parallel chunk encoding in multiple cloud instances. Once all the chunks are encoded, they are physically stitched back into a final encoded bitstream. Lastly, the packager kicks in, adding a system layer to the asset, making it ready to be consumed by the clients.
With this architecture, chunk encoding is very efficient and processed in distributed cloud computing instances. However, assembly and packaging become the processing bottleneck, especially when the file size increases to the terabyte range. From chunk encoding to assembly and packaging, the result of each previous processing step must be uploaded to cloud storage and then downloaded by the next processing step.
Uploading and downloading data always come with a penalty, namely latency. While the input to our encoders, assemblers and packager instances is mounted using MezzFS and therefore read in parallel to be processed, output data is uploaded only after all processing is complete. The following table breaks down the various processing (including download) and uploading phases within an assembler and packager instance operating on large media files. It is worth pointing out that cloud processing is always subject to variable network conditions.
Additionally, in this architecture, the packager still needs access to local storage for its packaged output (before uploading it to the final cloud destination) and any intermediate output if there are multiple passes in the processing. Since not all projects are terabytes projects, allocating the largest cloud storage to all packager instances is not an efficient use of cloud resources. We took the approach of allocating small or large cloud storage space depending on the actual packaging input size (Figure 2). Jobs processing large files were directed to instances with cloud storage large enough to hold intermediate results and packaged output.
This initial architecture was designed at a time when packaging from a list of chunks was not possible and terabyte-sized files were not considered. It is very clear now that there will be significant processing savings if the physical assembly of the encoded chunks can be avoided. Also, the use of different sizes of local storage is not optimal, given that we can only support a small number of storage configurations, and given that the configurations need periodic updates as the maximum file size in the catalog inevitably grows.
In order to address the limitations of our initial architecture, we proceeded to make some optimizations.
Virtual Assembly
Figure 3 describes how a virtual assembly of the encoded chunks replaces the physical assembly used in our previous architecture. In this approach, an index assembler generates an index file, maintaining the temporal order of the encoded chunks. Care has been taken to ensure all chunks are accounted for and are in the right order during the virtual assembly to ensure the consistency of the final packaged stream and the original source. The index file keeps track of the physical location (URL) of each chunk and also keeps track of the physical location (URL + byte offset + size) of each video frame to facilitate downstream processing. The main advantage of using an assembled index is that any processing downstream of the video encoding can be abstracted away from the physical storage of the encoded video. Media processing services downstream of video encoding have intelligent downloaders that consume the assembled index file in order to mount the encoded video as video frames or encoded chunks. This is also the case with the packager, which reads and writes the encoded chunks only when it is generating the packaged output.
Using virtual assembly greatly improves the latency performance of the ProRes 422 HQ proxy generation by removing one round trip of cloud downloading and cloud uploading by the physical assembler.
Writable MezzFS
As described in a previous blog post, MezzFS is a tool developed by Netflix that allows cloud storage objects to be mounted as local files via FUSE. It allows our encoders and packagers to do random access reads of cloud storage objects without having to download an entire object before beginning their processing.
With similar goals in mind for write operations, we set about supporting storage of objects in the cloud without incurring any local storage and before the entire object has been created so that data generation and uploading can occur simultaneously. There are existing distributed file systems for the cloud as well as off-the-shelf FUSE modules for S3. We chose to enhance MezzFS instead of using these other solutions because the cloud storage system where packager stores its output is a custom object store service built on top of S3 with additional security features. Doing so has the added advantage of being able to design and tune the enhancement to suit the requirements of packager and our other encoding applications.
The requirements and challenges for supporting write operations are different from those for read operations. Our previous blog post described how MezzFS addresses the challenges for reads using various techniques, such as adaptive buffering and regional caches, to make the system performant and to lower costs. For write operations, those challenges do not apply. Furthermore, the goal for writes is not to build a general purpose system that supports arbitrary writers, but rather one that maximizes potential packager performance. In order to do so, we started by analyzing the packager’s IO patterns and configuring the packager to make its patterns more friendly to cloud writes.
The problematic pattern of packagers is that they do not always generate data linearly. They sometimes update parts of the file that had been written earlier for various reasons. For example, both ISOBMFF (ISO/IEC 14496–12) and Apple Quicktime use box structures to represent packaged media. The ‘moov’ box represents the metadata header describing the media while the ‘mdat’ box encapsulates the media content. Boxes start with a header which gives size and type of the box before the box content. When a packager is encapsulating the media content into the ‘mdat’ box, the size of the box is not known until all the media data are processed. To optimize the packager for writable MezzFS, we did not utilize the packager features that require multi-pass processing, intermediate storage and frequent update of headers.
With this packager constraint, there are a number of ways to design a writable MezzFS feature, but we wanted a solution that best fit the IO patterns of the packager in terms of latency, network utilization, and memory usage. In order to do that, the storage cloud object is modeled as a number of fixed size parts. MezzFS maintains a pool of upload buffers that correspond to a subset of these parts. As the packager creates and writes data to the object, data fills up the buffers, which are automatically uploaded asynchronously to the cloud. You can think of packaging as creating a stream of output that is stored directly in the cloud.
What happens when the packager references bytes that have already been uploaded (e.g. when it updates the ‘mdat’ size)? Any single read or write operation may involve a mix of previously uploaded and yet-to-be uploaded bytes. MezzFS borrows from how operating systems handle page faults. It downloads the part(s) that contain the referenced, uploaded bytes and keeps them in an LRU active cache. Packager’s read/write operations are then translated into operations on the upload buffers and/or buffers in the active cache. The buffers in the active cache are uploaded to the cloud as the cache becomes full. Just as with virtual memory management systems, locality of reference is important in determining the active cache size and performance. If the packager updates random, unrelated parts of the file within short periods of time, thrashing would occur and performance would be degraded. Our analysis of packager’s IO patterns determined that the packager makes updates with close proximity to each other — at most few parts at a time — thus making this design viable.
Use of MezzFS is not without its cost in terms of performance. Use of FUSE means that file operations must go through MezzFS instead of directly to the kernel. As faster disk technology such as NVMe SSD are adopted, this overhead becomes increasingly noticeable and the time saved by uploading while packaging is counterbalanced by this overhead. As shown in Figure 5, packaging locally using non-NVMe local storage such as AWS Elastic Block Store (EBS) and then uploading takes more time than using MezzFS, but doing the same with NVMe SSD takes less time.
However, time savings is not the only factor to consider when choosing between different upload techniques. Use of writable MezzFS offers the advantage of not requiring a large disk — larger and faster disks incur higher monetary costs and multiple disk size configurations make resource scheduling and sharing more challenging.
Supporting packaging of media content at terabytes scale is challenging. With innovation from system architecture, platform engineering and underlying packaging tools, processing terabyte-sized media files is now supported with greater efficiency.
The overall ProRes video processing speed is increased from 50GB/Hour to 300GB/Hour. From a different perspective, the processing time to movie runtime ratio is reduced from 6:1 to about 1:1. This significantly improves the Studio high quality proxy generation efficiency and workflow latency.
In addition to speed improvements, there is no longer a need to keep different configurations of local storage for cloud packagers. All the cloud packager instances now share a single scheduling queue with optimized compute resource utilization. There is also no need for multi-terabyte local storage, and no more unexpected out-of-disk processing failures. The single-sized local disk storage is future-proof for movies with longer runtime and higher resolution.
We would like to thank Anush Moorthy, Subbu Venkatrav and Chao Chen for their contribution to virtual assembly, Zoran Simic and Barak Alon for their contribution to writable MezzFS.
If you are passionate about media processing and platform engineering, come join us at Netflix! The Media Systems team is hiring. Please contact Flavio Ribeiro for more information.
Learn about Netflix’s world class engineering efforts…
420 
3
420 claps
420 
3
Written by
Learn more about how Netflix designs, builds, and operates our systems and engineering organizations
Learn about Netflix’s world class engineering efforts, company culture, product developments and more.
Written by
Learn more about how Netflix designs, builds, and operates our systems and engineering organizations
Learn about Netflix’s world class engineering efforts, company culture, product developments and more.
"
https://medium.com/sturfee/supercharge-arkit-core-apps-with-world-scale-ar-cloud-32d35124acfd?source=search_post---------284,"There are currently no responses for this story.
Be the first to respond.
Top highlight
The mobile era put the power of PCs and the Internet into our pockets. We were suddenly gifted with the ability to search for answers and consume information on the go. Augmented reality has been heralded by many as the next major computing platform shift.
But for AR to reach this stage, it cannot just be a marginal improvement over mobile. In fact, if it were, AR would not gain enough velocity to escape the orbit of the mobile paradigm and cross into mainstream adoption.
AR in its essence allows us to access knowledge at its source — timely, frictionless, and screenless.
Since snazzy AR glasses aren’t available yet to solve the screenless part, we can at least address the first two factors. To that end, Apple and Google’s ARKit and ARCore respectively are steps in the right direction, but they only allow the creation of novelty features that add marginal value to apps native to the mobile era. However, we’re still in the early days of the AR platform creation phase, so there’s plenty of work to be done. The goal of this post is to illustrate one possible AR user journey, why ARKit alone isn’t enough, and what’s needed to rocket boost us to AR’s true promise.
It’s a bustling Saturday and you’re emerging from the subway to meet your best friend for lunch. As soon as you reach your stop, a green line appears before you leading you through the station, onto the streets all the way to the restaurant. As you walk through the station, the blank walls come alive with virtual storefronts of brands that cater to your interests. You notice a collection of jackets inside the H&M section and ask your AR glasses how much they are and save them in your wishlist to revisit later.
Soon you’re at street level and digital overlays highlighted outlines appear of points of interest that your friends have mentioned to you on Facebook or reviewed on Yelp. A tourist cable train passes by and you see a map of its route on the side facing you. Suddenly, you backtrack a few steps upon realizing that you had just passed by the Pokemon egg your team had left for you behind the fountain. You leave behind a big heart emoji to show your thanks. By now, the green line is flashing yellow, reminding you to hurry and not be late.
That’s the vision of an augmented city, where the digital, both useful and fantastical, blend seamlessly with the real. In the words of Tim Cook, “augmented reality is going to change everything”. However, with today’s tools, we’re just not quite there yet. And to understand why, we first have to examine ARKit’s capabilities and limitations.
Let’s start with the same scenario above except this time you’re walking down the street with your ARKit-based app on your phone. The app internally starts detecting horizontal planes like the ground or the tops of cars that are up to 15 feet in front of you. The app uses these detected planes as anchor points to place 3D digital objects so that the object stays roughly in place even if you’re moving your phone around it.
However, you’ll notice a few things about this ARKit app:
Update (thanks to Patrick Metcalfe for test driving ARKit 1.5!): Apple released ARKit 1.5 on Jan 24, 2018, which added support for detecting vertical planes as well as marker-based tracking using reference images. However, during our tests, it’s pretty clear that this Beta has some way to go before it’s ready for the masses though not all the problems lay with ARKit. Walls rarely have good enough texture to find points for the algorithm to track. Therefore, the user has to move around with their device pointed at the wall for extended periods of time. The shaking motion can (as happened with our tests) lead to dropping your device and garnering various stares from people around you.
Moreover, while ARKit has incredible light estimation (where it can follow planes even when lighting condition change), support outside is not good enough to follow the shiny Spanish stone walls so common on California buildings. Coupled with ARKit’s roughly 2 meter plane detection limit; ARKit is no where near being able to follow 3D objects like buildings in order to identify where they are located.
Because of these limitations, most AR apps are limited to indoors and flat surfaces. These are coined as “tabletop AR experiences”, which for the most part have proven to be novelty products. If they do fulfill a niche, the use cases don’t drive daily usage — after all, would you really use the IKEA app to visualize where to put your furniture every day?
In order to make the jump to the mainstream, AR needs to be contextual, timely, persistent, scalable, and power efficient. The breakthrough solution needed is known amongst the AR community as the AR Cloud, which industry veteran Matt Miesnieks defines as “a machine-readable 1:1 scale model of the real world. Our AR devices are the real-time interface to this parallel virtual world which is perfectly overlaid onto the physical world”.
Once this technology is here, a lot of magical things can happen. At Sturfee, we are building a version of the AR Cloud on the world-scale.
In a nutshell, Sturfee StreetAR’s Visual Positioning System (VPS) makes ARKit/Core much more robust for location-based AR applications. Here’s how we tackle some of the limitations of the current platforms:
We map out an exact 3D model of the real world using satellite imagery (view from the sky) and continue to monitor and convert geospatial features like buildings and cityscapes into a machine readable format. The Sturfee engine then uses visual processing to analyze the features around the you (ground view) to instantaneously and accurately locate where you are and where you’re facing. This is done through any camera, whether it’s a smartphone, headset, or even drones.
Now, ARKit can use these Sturfee-derived geospatial measurements to reduce the positioning errors introduced by the phone’s GPS and internal sensors, and accurately detect street-level 3D features like changes in terrain elevation, building surfaces (vertical planes), and even trees. As an added benefit, by limiting ARKit’s mesh computation to a few limited objects in the scene (i.e. people and cars) the app can make huge savings on battery power.
So in brief, Sturfee enables that same ARKit app to fully exploit the potential of the your surroundings and unlock a host of creative design and social engagement applications. So now instead of Pikachu just walking across the ground, imagine it jumping off walls and hiding behind statues. Or a Snapchat Star Wars filter that causes porgs to dash across the park, jumping over benches rather than just fluttering around on your table. Below is some footage from apps we’ve built in-house:
As we race towards our Q1 2018 SDK launch, we’ll share a series of posts that’ll reveal our vision for an AR future and highlight some cool things that are possible with Sturfee’s tech to get your creative juices flowing. Here are some examples:
If you want to build amazing world-scale AR experiences (and beyond), partner with us, or join our team, we should connect.
Special thanks to Anil Cheriyadat, Matt Miesnieks, and Michael Ludden for your feedback on this post!
Insights and updates from the team behind Sturfee, the…
1.2K 
1
1.2K claps
1.2K 
1
Written by
Head of Biz Ops @ Sturfee. Ex-Niantic Labs + Google. Learn, plan, execute. Reflect and repeat.
Insights and updates from the team behind Sturfee, the computer vision company giving cameras spatial intelligence & enabling world-scale AR
Written by
Head of Biz Ops @ Sturfee. Ex-Niantic Labs + Google. Learn, plan, execute. Reflect and repeat.
Insights and updates from the team behind Sturfee, the computer vision company giving cameras spatial intelligence & enabling world-scale AR
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/design-and-tech-co/creating-a-configuration-microservice-using-spring-cloud-config-800d08fa596a?source=search_post---------285,"There are currently no responses for this story.
Be the first to respond.
The advent of microservice architecture has alleviated a lot of pain points for developers that were present in the monolith architecture. However, it also has introduced some complications as a result of its distributed nature. One such complication is maintaining property files for the system.
In a monolith there is one place where the property files are stored and when changes are needed, there is only one place that they need to be updated. In a microservice architecture, each microservice owns its own properties. This…
"
https://medium.com/@tatianaensslin/how-to-create-a-free-personal-vpn-in-the-cloud-using-ec2-openvpn-626c40e96dab?source=search_post---------286,"Sign in
There are currently no responses for this story.
Be the first to respond.
Tatiana Ensslin
May 6, 2018·13 min read
Considering the massive iceberg of privacy issues finally surfacing through the media, its time to teach people how to be safe and secure when utilizing the internet, especially public networks.
To do this, we need to make technology feasible!
Thankfully, Virtual Private Networks (VPN) exist and I’m about to show you how simple it is to make one using Amazon Web Services (AWS) and an OpenVPN image!
Think of a Virtual Private Network like a layer that sits below your connection to a network. By connecting your device to the VPN, data is sent and received privately through a different IP address. Connecting to a VPN allows for a multitude of benefits, like functionality, security (data encryption), and if you run your own VPN, private network management. Yesssss. Score.
Alright, bare with me here because I know that this tutorial *looks* long — but thats only because I added a ton of pictures to make sure things were as straightforward and clear as possible.
Overall, I was able to set up my own VPN in just 15 minutes.
If you haven’t yet signed up for an AWS developer account, you can do so here for free. AWS offers free instance tiers for the first 12 months, which you’re welcome to use for your VPN if you so wish.
EC2, also known as Elastic Cloud Compute, can be described as a chunk of server space on a rack sitting somewhere in Amazon’s 54 availability zones — a.k.a. data-centers. Once you’re logged into your developer account, navigate to the EC2 service from the AWS management console. You can select which zone you want your server space to be in on the top right bar of the console. For myself, I chose N. Virginia.
Instances are created by loading that chunk of server with an operating system/image at initialization time. Since different operating systems are utilized for different operations and applications, this allows for a plethora of choices for specific optimizations and use cases.
For the VPN, were going to bootstrap our little chunk of server with OpenVPN’s open source image located in the marketplace.
This means OpenVPN is already packaged along with the operating system and were going to spin both up together on our machine at the same time. Get started by hitting the big, blue launch instance button seen below.
After selecting “Launch Instance,” navigate to the AWS Marketplace using the left side navigation bar. In the search bar, type in OpenVPN so that you receive filtered results.
I decided to use OpenVPN for my server because it has the added benefits of open source software, as well as being free for the average user. Additionally, it utilizes its own custom security protocol that utilizes SSL/TLS for key exchange and its capable of traversing network address translators (NATs) and firewalls. Note that this image has a license for 2 devices.
For Step 1 “Select” the OpenVPN Access Server image, seen first below.
Once you select the image, you will be prompted to ensure you’re selecting the right option. There will be a list of hourly fees for the service, mainly for those users with larger networks or enterprise customers. You can go ahead and click “Continue”.
Next, for Step 2, you will choose your EC2 instance type. Since I am the only person using my VPN, I selected the t2.micro instance, which is the free 12 month tier offered to new users.
If you’re interested in learning about the differences of each instance type, you can do so here. After selecting the type, continue by hitting the “Next: Configure Instance Details” button.
In Step 3, we make some modifications to the instance that guard our VPN against accidental deletion. To do this, select the “Protect against accidental termination” option in the 5th section of this step. When that is done, hit the “Next: Add Storage” button.
After selecting the protection and navigating to the next page, you should see Step 4 — storage.
I prefer adding storage to my server since most operations that exist within a system utilize memory to function and you can still utilize the EC2 instance for other purposes if you wish, as long as there is enough storage. For this reason, I changed the size to 50Gb, as the default size is 8Gb.
Next, select a different volume type using the drop down, choosing “General Purpose SSD” since were doing average operations on the machine. Lastly, uncheck the delete on termination box, meaning that to delete the snapshot (backup) of storage, you must manually delete it.
Note: Changing the storage properties can affect the price of the selected tier, although AWS is still incredibly inexpensive.
Okay okay, were almost done creating the instance for our VPN to run on in the cloud. Awesome. So let’s finish by adding a tag to our instance in Step 5 so that we can find it among our other instances if we need to.
You can tag it however you’d like, but I’d recommend keeping OpenVPN in the tag value. YourNameOpenVPN, MyOpenVPN, or OpenVPN all should work fine.
Lastly, go ahead and navigate to Step 6 “Configure Security Group.” Since we booted a pre-made image of OpenVPN onto the EC2 instance, they already established a recommended security group. Go ahead and select the existing group.
Then, select “Review and Launch.”
Yay! We practically did it! Go ahead and scroll through the configuration changes that you’ve made and ensure everything is correct. When you’re done looking things over. Hit “Launch.” It will ask you to create or select a key for your instance, if you haven’t yet created a key — PLEASE DO. The purpose of the instance key (.pem) is to ensure that no-one can SSH into your EC2 instance without it. Name it as you’d like, then hit “Launch Instances.”
Congratulations! You just created your very own EC2 Instance with OpenVPN installed!
At the moment, your VPN is public — No. Bueno. This means that any one is able to access your server through the IP address.
Were going to make some changes now that add some layers of security to your instance and the VPN you just installed. These changes will be creating a permanent IP and private IP address, creating a user account to manage and access the VPN with, and turning off settings on the server which disable public connections.
Upon launching an EC2 instance, a Public IP address is assigned so that that instance is available. As soon as the instance is shut down, a new public IP gets assigned for the same instance. This means if we set up the VPN server with the default IP, we wont be able to access the VPN if the instance is shut down. Elastic IP solves this issue and assigns a permanent IP address.
Once your instance starts up, you should see a list of instances associated to your account. Select the OpenVPN instance and on the left navigation window, scroll down to “Network & Security” and select Elastic IPs.
Inside Elastic IPs, we are going to “Allocate A New Address” and select this our OpenVPN instance. Once you’ve finished this, hit “Associate.”
Sweet! Success is upon us. Hopefully, your permanent public IP address is now listed as one of the properties of your instance. Note that you can also view it from selecting the instance and viewing its description.
Now that we have created a permanent IP address, we are going to make some changes to our OpenVPN configurations, disable public access and create a truly private network.
To complete these next steps, we use a protocol called SSH to log into our instance. SSH, or Secure Shell, is a cryptographically secure way to access and preform network operations over an un-secure network.
We will use use this, along with our private instance key (.pem) to initialize our OpenVPN configuration. To use our key right, we must first confirm that the key (.pem) was saved to the root user’s folder in your local computer’s file directory.
If your computer is set to default settings, the key most likely downloaded to the Downloads folder. Move the file to your user’s root directory. On my Mac it was MacintoshHD/Users/Tatiana. If necessary, you can open the root directory by searching the file path in finder.
Once the key is in your user’s root folder, launch a terminal window. You can do this by using finder and typing in terminal.
Once the window opens type the following command to log into the server using your instances Elastic IP. Replace the bold parts below with your instance’s information.
If your situation is anything like mine, your key file has unlimited ownership access privileges and you’ll receive a nicely printed WARNING. This means you’ll have to run an extra command, which I’ve listed below. You will be asked for your password to complete the command.
Chmod 600 changes privileges of the file to “rw- — —” or in other words, the user only can read and write to this file. This is good, because you should be the only user with access to this file as admin.
After running those commands, you should successfully be able to login. When you do, you should see this OpenVPN License Agreement.
Scroll all the way down and hit “Enter” to agree.
This step is easy, just hit “Enter” to all of them until you’re done. After completing this step, you should see a initialization complete message. Woohoo!
In the terminal, type in the following command to create a user with your name. This is great practice, because you never want to manage a system as the root user all the time, as that can lead to excess access privileges and accidental system corruption.
It will prompt you for a password, and then prompt you again to confirm the password. Once this is finished type exit to close the connection and logout.
Yay. Most the hard part is over, and now we can focus on the good stuff.
To actually use to the VPN server you just made, you need a client that lets you establish a connection to it. The client is an OpenVPN program which you can install on your computer like any other application. To do this, use your public IP to access the installer link.
In your browser, open a new tab and type http://YourPublicIP and hit enter. This should bring you to the following page which warns about a public connection. Hit advanced and select the link at the bottom to proceed.
Now we are directed to the login page of the OpenVPN server. Log into the server with the username and password you just made in the terminal.
In my case, I named my user openvpn. When you’re done, hit “Go”.
At this point, you will be asked to click a link to download the installer for OpenVPN. Click the link, and when the installer is done downloading, double click to open it.
It should bring you to an installation wizard. Hit continue each time and install the package.
When you’re done, you should see a new icon at the top of your desktop bar. In my case, its the orange icon across from the Help tab.
Yay, the VPN is up and running and we downloaded the application that lets us connect to it. Let’s log in!
Click this new icon to start a connection to the OpenVPN we made. Under the IP address, select connect. Log in with your username and password you created. After this, you should be connected to your very own VPN!
You have now successfully logged in and the end of this tutorial is nigh. Lets finish strong by adding the last security touches!
Log into the server again using your browser with the following URL:
Once again, hit advanced and proceed to direct to the login page. Login with your username and password. You should see this notification. Hit accept to access the admin portal for the VPN.
Now that we are in the admin portal, we want to disable access to the portal from your public/elastic IP, and only allow usage through the private IP that was assigned. To do this, on the left side navigation panel, under “Configuration” select “Server network settings.”
Scroll down the bottom of the page and toggle off the admin and client web server options, seen below.
After this is done, hit save at the bottom. The page will update at the top to include this message. Hit “Update Running Server.”
When you hit the button, the page will break. This is a good sign, because we just successfully disabled the usage thru the public IP.
Just like in Step 9, you can still access the portal with the same method, however, the public IP no longer works. This means that you can only access it through the private IP. The URL is given below for clarity.
Returning to your AWS EC2 Console, select your instance. On the left side navigation panel, under “Network & Security,” select “Security Groups.”
Under the instance, there now should be a tab labeled “Inbound.” Select this tab and hit the edit button. You should now be able to delete SSH as a type by clicking the X on the right. Hit save to keep the changes.
Be sure to manually disconnect the client connection through the desktop icon when you are closing your computer or putting it to sleep! Failure to terminate the connection can cause daemon ports to establish and can break the application.
Woohoo! Thanks for following along! Now you can reap the benefits of having your own private VPN!! You can even extend your VPN to your cell phone by downloading the OpenVPN application from the app store — cool!
I hope you learned from this tutorial and if you enjoyed it please follow me on twitter @tensslin or here on Medium.
My articles and tutorials can’t get better if I don’t know how they are to others. I’d love to hear your feedback, so leave a comment, let me know how long it took you and what your tech experience was going into the tutorial. Thanks for helping me constantly grow and improve.
If you’re having a problem re-connecting to your VPN though the client application, you might see this error message when you try to connect:
If thats the case, don’t fret. For some reason, one of the ports on your client is disconnected, and for that reason, you can’t get the access verification you need to connect. Just a guess but assuming something like this happens if you close your computer without disconnecting from the VPN, or if you’re at a network that has a proxy set up.
To resolve this issue, there is a hack. Re-download the client installation package. This means you would delete/uninstall your current installation and then begin again at Step 8. Instead of turning off the web server client — enable it! Then, log on again like in step 6. After you re-establish your connection, follow step 7 and 8 to turn off the web server again to public access.
This should fix the problem!
Loves blockchain engineering, copious amounts of coffee, metacognitive conversations & dancing to Berlin techno. Software Engineer @Zillow
See all (83)
519 
26
519 claps
519 
26
Loves blockchain engineering, copious amounts of coffee, metacognitive conversations & dancing to Berlin techno. Software Engineer @Zillow
About
Write
Help
Legal
Get the Medium app
"
https://howtofirebase.com/firebase-cloud-functions-753935e80323?source=search_post---------287,"Cloud Functions for Firebase are Google’s serverless solution for Firebase apps. They can be used as the (R)eactor functions for FIRE Stack app architecture. If you’ve developed with firebase-queue, AWS Lambda or some other Functions-as-a-Service architecture, Cloud Functions for Firebase should feel natural… just a lot slicker and easier to use :)
If you’re wondering where to start… well, read on my friend.
FIRE Stack architecture replaces the typical REST API, with its endpoints and HTTP calls, with standalone functions — written by you and running on Google’s infrastructure — that react to changes in your app and can run any Node.js or Java that your heart desires.
As of this writing, there are six types of triggers:
1. Firebase Realtime Database triggers
2. Firebase Authentication triggers
3. Firebase Analytics triggers
4. Cloud Storage triggers
5. HTTP triggers
6. Cloud Pub/Sub triggers
You can read the docs on each of those triggers for the full rundown. They’re not hard to use, although they can be tricky to test. I’m going to start with Authentication and Firebase Realtime Database triggers. If you can get those working, you shouldn’t have trouble with the other event types.
Cloud Functions supports Node.js LTS releases. The current release is v6.9.1, but check the docs to make sure that you’re developing against the freshest-possible version of Node.js.
If you need help jumping between Node.js versions, check out n for fast version switching.
To get started, I’m running $: n 6.9.1 to switch to v6.9.1.
Authentication triggers track creation and deletion of Firebase Authentication users. That’s about it. Here are the examples from the docs.
That’s really all there is to it. event.data is the user data from your newly minted or deleted currentUser JWT (JavaScript Web Token).
If you want to access your Realtime Database, you can’t get it from the event :( Fortunately, functions.config().firebase contains your initialization details, so you can use firebase-admin to create whatever refs you need.
This is the trigger that you need for FIRE Stack architecture
It’s also the most complicated trigger, because of all of the attributes on the event itself. You need to read up on these attributes. I’ll only be summarizing a few here.
First, scan the docs about event properties. Focus on the following:
- event.data
- event.params
Next, read a bit more carefully through the docs on DataSnapshot, a.k.a. event.data. This is the crux of Realtime Database events. Pay attention to everything, but read the following word-for-word.
- DeltaSnapshot.adminRef
- DeltaSnapshot.current
- DeltaSnapshot.key
- DeltaSnapshot.previous
- DeltaSnapshot.ref
- DeltaSnapshot.val()
- DeltaSnapshot.toJSON()
- DeltaSnapshot.numChildren()
You’re back already? Have you understood the docs? If so, GREAT! If not, for shame! Go back and read ’em :)
Once you understand the event object, database triggers aren’t tough to figure out. I have an architectural pattern where I like to track user logins. It’s kind of an important metric, and it’s a good opportunity to update the user’s account. I have my client app push data to /queues/current-user/{uid}, something like this…
Then I register a Firebase Function to listen to queues/login/{uid} and do whatever I need.
event.data.adminRef is a ref with full admin privileges over your entire database. event.data.ref is a ref as well, but is has the same authentication permissions as the user who triggered the event. This is super useful. It enables you to impersonate a user from within your function, security-rules restrictions and all!
Also, if you try to do something “admin-like” from event.data.ref, don’t be surprised when you get weird permission denied errors in your Cloud Functions logs.
In the earlier example I registered my onWrite event like so:
Notice the wildcard {uid}. That wildcard ends up as event.params.uid. So if write something to /queues/login/fake-uid-123, then event.params.uid will be ’fake-uid-123'. Pretty simple.
But the plot thickens, because you can do crazy stuff like
See that! You can use multiple wildcards in a path! I don’t know how many wildcards you can use, and I’ve never used more than two… but go hog-wild and let me know if you find the limits!
The docs on environment variables show how you can use firebase-tools to set your functions environment variables from the command line. Basically, you install firebase-tools globally with yarn global add firebase-tools (or npm install -g firebase-tools). Then you use the CLI to do the work.
Notice that I used kebab case instead of camel case. Functions config does not allow uppercase characters in attribute keys. So accessControlLists.adminUsers, which is what I’d like to type here, will not work!!! 👹
Also notice that I passed in a comma-delimited string. Stick to strings. You can coerce these strings to other data types once you’re in the function… but don’t try to get cute with booleans, numbers or arrays. They’ll all get wrapped in double-quotes.
You’ll want to define most of your require statements, i.e., const quiverFunctions = require(‘quiver-functions’), within the function itself. This will run the require function every time the function is run.
This won’t bite you often, but I’ve run into a few situations where my functions wouldn’t deploy until I moved a require call into the function. I suspect it has something to do with how the functions are triggered on Google’s end. If you see error messages when attempting to deploy functions, this tip might help.
I’m using Jasmine for my Node.js tests. It’s easy to get started on your own projects. Just do the following:
I’m using Yarn instead of NPM these days. NPM works almost the same, but let’s be honest, Yarn’s better :)
Here are the docs on installing Yarn.
If you’re on OS X, you should be using Homebrew for your own sanity.
If you’re serious about dev on OS X, and you’re not using Homebrew, take a few minutes to get it set up. Thank me later.
Installing Yarn with Homebrew is as easy as brew update && brew install yarn.
1. Install Jasmine globally: yarn global add jasmine
2. Install Jasmine in your project: yarn add jasmine
3. Initialize Jasmin: jasmine init
4. Edit ./spec/support/jasmine.json to make sure that the spec_dir and spec_files will pick up your tests.
You can run the test in this repo by simply installing and running jasmine at the top-level of the repo. Otherwise, you can cd functions && yarn test to get the same result. Note that you’ll need to copy /functions/config.json.dist to /functions/config.json and edit the file so that it contains your Firebase details. I’m not checking my service-account.json file into source control :)
Firebase tutorials and tips
318 
4
318 claps
318 
4
Written by
Front-End Web Developer; Google Developer Expert: Firebase; Calligraphy.org
Firebase tutorials and tips
Written by
Front-End Web Developer; Google Developer Expert: Firebase; Calligraphy.org
Firebase tutorials and tips
"
https://medium.com/@jamsawamsa/running-a-google-cloud-gpu-for-fast-ai-for-free-5f89c707bae6?source=search_post---------288,"Sign in
There are currently no responses for this story.
Be the first to respond.
James Lee
Dec 13, 2017·10 min read
EDIT* This guide was written for fastai version 1, which at the current date and time (Jan 2018) is in the midst of transitioning to the a newer version, dubbed fastai v2. An updated guide will be coming soon.
As a deep learning enthusiast in Malaysia, one of the biggest issues I have is securing a cheap GPU option to run my models on. If you’re like me and come from a country where paying $80-$100 every month for AWS GPUs is too expensive, I’ll show you how I set up a my GPU on GCP without incurring any cost at all.
In this article I’ll walk you through setting up a google cloud computing instance with a 500gb SSD, a 3.75gb ram Broadwell CPU and a Nvidia Tesla K80 GPU. All of this can be done for free at the start.
I’ll be setting up my instance in the Asia(Taiwan) servers since I’m pretty sick of the high latency I get when using servers in America or Europe, but you can change this on your own later.
Google Cloud Platform is a cloud computing infrastructure which provides secure, powerful, high-performance and cost-effective frameworks. It’s not just for data analytics and machine learning, but that’s for another time. Check it out over here.
Google is promoting usage of their platform right now and are giving away $300 dollars of credit and 12 months as a free tier user.
Pssss…you get $300 for each google account you have 😉 😉
So what are you waiting for?
Seriously go get them though, you’ll need these credits to continue.
To start off you’ll need to upgrade your account to a paid account. Free trial accounts aren’t allowed any quota for GPUs so this is a mandatory step.
Don’t worry you won’t be charged anything as of yet. You’ll only start getting charged once you run out of credits and $300 can last you up to a month if you’re wise about it.
Go to your GCP console here and click on the menu button on the top left.
Look for the option billing and click it. If you haven’t upgraded to a paid account yet, you should see a small button near the top that says upgrade to a paid account. Go ahead and do that.
Smash that menu button again. This time you’ll be looking for the Compute Engine option instead, smash that too. 😃
Next look for three hexagon-ish dots at the taskbar on top.
Smash it and it should bring up a window that lists all your projects. Hit the + button to add a new one. Name your project and move on.
First off you’re gonna have to do a little reading and deciding. You’ll need to figure out which region and zone you want your cloud computer to be at.
Basically each region has a bunch of zones and not all zones offer the same services. Some have SSDs and GPUs, some don’t.
Pay attention to which zones have Broadwell CPUs, as GPUs can only be attached to Broadwell generation or higher CPUs.
This link shows you what each zone offers and this link shows you which zones GPUs are available in.
Since I’m based in Southeast Asia, the nearest region with GPUs is asia-east1-a so I’ll be using that for my set up.
The names of regions and zones are concatenated in GCP. For example in asia-east1-a, the region is asia-east1 and the zone is a. Pretty confusing huh. 😓
Next you’ll have to request for an increase of our GPU quota. Most projects start with a quota of 0 GPUs available so you’ll have to get on our knees and ask for some love from big daddy G.
Smash that menu button once more and look for IAM Admin. Smash.
Once you’re at the IAM & admin page, look for the option Quotas on the side menu. Smash.
Okay it’s gonna get a little tricky here. You’ll be brought to a page filled things that are all called Google Compute Engine API. It looks like this:
Remember that region-zone you decided on earlier? You’ll need to use it now. Smash the regions drop down and select your region only. You’ll now see the Google Compute Engine APIs for that region only.
Ctrl+F and search for “k80”, check it and smash Edit Quotas on top. Fill up your details, the number of GPUs you want to request for and a justification. I only requested for 1 but you can ask for more if you want. Just be aware that more GPUs will cost more $$$ per hour so be prepared for that. We’re done here, moving on.
Requested for one? Good. It should take under 5 minutes for an email to pop up in your inbox saying your request has been approved. Give thanks to big daddy G.
Next you’ll have to create the VM instance. I did it through my terminal on a Ubuntu computer, so that’s what I’ll be using in this article. You can do it from the console just as easily too. To use the console to, go to Compute Engine from the menu button and click on VM Instances. Be sure to set up your instance with a Broadwell CPU. Read the docs on setting it up here.
Anyway, you’ll have to install google-cloud-sdk on your local machine. The docs have a pretty good guide on setting it up for different OS’ so check it out.
Once you’re done, run this snippet on your terminal:
You can get the snippet here. “jamsa-dl-fastai” in the first line is the name of the instance, change it to whatever you fancy.
Once you’re instance is set up, head to your GCP console >> Compute Engine >> VM Instances.
You’re new instance should be listed there, along with it’s IP.
Before you can go into the instance you’ll need to edit some settings. Click on the instance name and you’ll be brought to the instance settings.
Click edit at the top and scroll down to the Firewalls. Check both “Allow HTTP traffic” and “Allow HTTPS traffic”. This is so that you can connect to Jupyter Notebook which you’ll be using in the course.
Next add the tag “jupyter” without the double quotes into the Network tags. Save and go back.
Every time you spin up your instance, a new IP address will be assigned to it. This gets pretty annoying if you’re a a frequent user. A static external IP address is an external IP address that is reserved for your project until you decide to release it. Let’s go ahead and request for one. Go back to the main button, look for VPC Engine >> External IP addresses.
Click reserve a static address at the top. Name it, check IPv4, Regional, select your region and attach it to a project. It should look something like this:
Go ahead and reserve it.
Back at your VM Instance console. Check your instance and click on start at the top. Once it’s done starting up you can ssh into your instance from your local machine. Fire up your terminal and enter:
Make sure to change username to your GCP account username and instance_name that you chose for yourself.
So if your username is “i-is-smarts” and the instance is called “smarts-pants-land”, you’ll enter this:
Now that your’re in your instance, you’ve got a little bit more of housekeeping to do.
First run:
If your GPU was set up properly during instance creation it should look like this:
If not, you can reinstall it with:
Next you’ll be getting all the course files from fast.ai, enter:
Make sure it’s all in one line, the command is too long for the Medium’s code snippet block.
The file you just got is a nifty little script that you can use to install all the dependencies like anaconda2, keras, configures your jupyter notebook and git clones the fast.ai lesson materials. Run it with:
The password for sudo on your instance is an empty field, unless you set one up already. Reboot your instance either through the GCP console or:
Jupyter Notebook is the IDE you’ll be using throughout the course so we’ll need to configure it beforehand. It was installed earlier when you ran the install-gpu.sh script with anaconda. If it’s not go ahead and install anaconda again.
On your local machine (not the VM instance), add a firewall rule to allow access to port 8888, which is what you’ll be using for Jupyter:
Be sure to edit “project_name” and “external_ip_of_your_local_machine” to the corresponding values. Make sure that “project_name” is the project and not instance name (I admit I mixed that up at first 😅).
Once your instance has restarted, ssh into it again. Then start up Jupyter Notebook:
EDIT: Some people have been getting the ERR_CONNECTION_TIMEOUT whilst trying to connect to your jupyter notebook. Make sure that you give yourself ownership of your .jupyter and your anaconda directories with:
Where username is your GCP username that appears in your alias when you ssh into the instance (username@instance_name; so the part before the ‘@’).
END EDIT
You may be prompted to set up a password for Jupyter if it’s your first time. Do so if you wish to, you can always set it later. But for now you should see a token when you fire it up. Something like:
Remember it for now.
Go back to your GCP console >> VM Instances. Get the external IP of your instance.
To connect to Jupyter Notebook on your local machine’s browser, go to external_ip:8888 and you should be in. You’ll need to use the token you got earlier here to login if you didn’t set up a password.
IMPORTANT — ALWAYS, ALWAYS TURN OFF YOUR VM INSTANCE WHEN YOU ARE NOT USING IT. IF YOU DON’T YOU’LL CONTINUE BEING CHARGED ON AN HOURLY BASIS IT IS UP.
Congrats! You’re all down with setting up your GPU cloud instance on GCP. Kudos goes to Nok for beating me to the punch and writing an article about this first (I burned a whole day reading the GCP docs before finding that article), the folks at the fast.ai forums for all the resources and eshvk for coming up with a similar method too.
Alright folks, that’s it for now. If there’s any questions you’ve got you can ping me on twitter James Lee or drop me a response down here and I’ll get back to you ASAP.
— — — — — — — — — — — — — — — — — — — — — — — — — — — — — — —
Find this useful? Feel free to smash that clap and check out my other works. 😄
James Lee is an AI Research Fellow at Nurture.AI. A recent graduate from Monash University in Computer Science, he writes about interesting papers on Artificial Intelligence and Deep Learning. Find him on Twitter at @jamsawamsa.
Future Tech. Ai, Blockchain and game design enthusiast. AI Research Fellow at Nurture.Ai & moderator of the FB group Awesome AI Papers
606 
15
606 
606 
15
Future Tech. Ai, Blockchain and game design enthusiast. AI Research Fellow at Nurture.Ai & moderator of the FB group Awesome AI Papers
"
https://medium.com/@cloud9consultive/over-40-use-this-loophole-to-sleep-with-women-half-your-age-69ccec69f93d?source=search_post---------289,"Sign in
There are currently no responses for this story.
Be the first to respond.
Cloud 9 Consultive
Dec 2, 2021·6 min read
According to a 2019 study published by the Journal of Personality, women around the globe put kindness at the top of the list when ranking characteristics they would value most in a partner. [1]
In short, women rank kindness above physical attractiveness.
I’m in my 50’s and I have been using this “kindness” loophole for years to attract women of all ages.
I’m not talking about acts of kindness, like holding the door for the person behind you.
I’m talking about emanating kindness from the deepest core of your being.
That’s what makes it so powerful.
People can feel it (especially women) and you can’t fake it.
As a bonus, since you’re attracting women you will never be seen as “the creepy old guy”.
Here’s how it’s done…
To eliminate negativity, you’ll need a Release Technique (RT).
There are many RTs, but they all lead to the same result.
They liberate your natural ability to let go of negative emotions on the spot.
Quantum physics teaches us that everything is energy. Even our emotions.
What we think of as emotion is the experience of energy moving through the body.
Negative emotions are uncomfortable, so when we experience them, we suppress these feelings instead of allowing them to completely pass through the body.
Negative emotions are just energy wanting to be released.
RTs allow for this release to happen so you no longer experience fear, worry, anxiety, hate, anger, jealousy, sadness, or any other negative emotion.
Science has verified the effectiveness of Release Techniques to eliminate negative emotions.
Numerous research studies were conducted by Dr. Richard Davidson of State University in New York, and Dr. David McClelland of Harvard University.
The doctors concluded that Release Techniques are simple, efficient, absent of questionable concepts, and produce rapid observable results. [2]
Here’s a quick RT to get you started:
Whenever you experience a negative emotion, allow yourself to feel it as fully as you can until the feeling completely goes away on its own.
Negative emotions can’t stand the clear light of awareness.
So give the feeling your undivided attention.
Let the feeling rise up, expand, and dissipate.
Do this for every negative emotion you experience and you will no longer feel negativity.
To learn more check out “The Sedona Method” published in 2003 by Hale Dwoskin.
Most people have a feeling, a sense, or belief we are connected to something greater than ourselves…
God, the Universe, Nature, a Higher Power… It doesn’t matter what you call it.
The fact is, we are connected to something divine in nature.
For now, I’ll refer to it as your True Self.
There’s an ancient saying that goes like this:
“When the world is seen the True Self is hidden. When the True Self is seen the world is not.”
When you are Mindful long enough you will see your True Self.
It’s like being captivated by a TV show.
You get so caught up in the story, the sides of the TV and the wall behind the TV fall away.
All you see is the TV screen.
In the same way, when you are Mindful long enough, the world falls away so you can see your True Self.
When this happens you will experience the deepest joy you can imagine.
In the Buddhist tradition, this state of mind is called the first Jhana: Detachment from the external world and a consciousness of joy and ease.
You can’t break the connection to your True Self but you can increase the strength of your connection.
How? By spending time with your True Self.
When you are Mindful you are spending time with your True Self.
As a result, you will cultivate Metta.
Metta is loving-kindness, benevolence, goodwill.
Here’s a Mindfulness exercise to increase Metta quickly:
As often as you can and for as long as you can, sit comfortably and bring your awareness to the present moment.
Pay attention to every moment of your breath:
– Notice the moment when your INHALE ends and your EXHALE begins.
– Notice the moment when your EXHALE ends and your INHALE begins, and
– Notice all the moments in between.
While paying attention to the breath, repeat a mantra, like…
– May all beings be happy and well
– May all beings be free from suffering
– May I be happy
– May I be peaceful, or
– Repeat the same word over and over, like love or peace
If you say repeatedly to yourself: Peace… Peace… Peace…
Eventually, you will feel peace.
Keep breathing and saying the words until you FEEL the MEANING of the words.
Then focus on the FEELING.
Use whatever words you like to generate positive emotion.
When the emotion becomes strong, focus on the EMOTION and let go of the words.
The words light the match that ignites the Metta.
The feelings that come AFTER the words… that’s Metta.
Cultivating Metta is like lighting a fire. Once the fire gets going, it just takes off.
It’s a pleasant experience. You’ll love it!
For immediate results, increase Metta right before prospecting.
To learn more check out “Bear Awareness” published in 2017 by Ajahn Brahm.
PLEASE NOTE: The intent of this article is not to encourage predatory actions or treat women as sex objects. The intent is to revolutionize the dating industry by encouraging people to make loving-kindness a part of their dating routine. It would be difficult if not impossible for a person to complete the steps given in this article and then become a predator, objectify women, or fake true kindness. Please complete steps 1 and 2 above for yourself so you can make an educated assessment of the content and its true purpose.
Some women you attract will be skilled at starting conversations, but most will just make eye contact and smile.
So you will need to initiate the conversation.
As you know, women are incredibly intuitive.
If your sole intention is to get laid, she’ll know it.
Instead, your intention should be to connect emotionally and spiritually… to give… to lift her up… to brighten her day… without wanting or needing anything in return.
An amazing conversation consists of three parts: An Open, a Conversation, and a Close (OCC for short).
Here’s a quick example of each…
OPEN: Compliment-Expand (CE)
Give her a sincere compliment. Then expand on the compliment with more detail:
– I love that dress. Red is a beautiful color on you.
– That is an amazing outfit. I love women with class and style.
– That is a beautiful top. It looks incredible on you.
CONVERSATION: Q&A
Ask questions and be genuinely interested in her answers.
With one simple question you can start a fun conversation, flirt like a pro, and connect with her as no one has before:
– If you could name 3 things about yourself that are uniquely attractive — that have nothing to do with your physical appearance — what would you say they would be?
– What’s the one thing you can’t say no to?
– What qualities do you find attractive in guys?
– Occasionally, I think a woman just needs an evening of passion. If that were to happen, how do you think it would start?
CLOSE
You have attracted her, created an emotional connection, and built sexual tension.
Now it’s time to close…
Number Close:
– You’re the most interesting person I’ve met in a while. How can we continue this conversation?
Meetup Close:
– I would love to get to know you better. I’m having a get-together on Sunday. You should come.
Immediate Date Close:
– I have a meeting in 30 minutes, but I would love to get to know you better. Would you like to join me for a quick coffee?
Your Seduction Plan will dictate the appropriate close.
If you don’t have a Seduction Plan I would be happy to create one for you.
I’ll help you fix it for free — no strings attached…
Schedule a free call here
It’s just a casual phone call to immediately increase your success with women…
– NO sales pitch
– NO follow up calls or emails
– NO obligation of any kind
Free sessions are subject to availability and are awarded on a first come — first serve basis.
Because of the intensely personal nature and time commitment, I reserve the right to discontinue this free offer at any time — without notice.
So if you’re interested sign up now
Resources
[1] psychologytoday.com/us/blog/the-athletes-way/201909/the-trait-people-desire-most-in-partner
[2] sedona.com/Scientific-Evidence.asp
What’s Your Biggest Dating Challenge? I will help you overcome it For Free. Men & women schedule a free call here: https://Cloud9Consultive.com/free
644 
41
644 
644 
41
What’s Your Biggest Dating Challenge? I will help you overcome it For Free. Men & women schedule a free call here: https://Cloud9Consultive.com/free
"
https://netflixtechblog.com/a-brief-history-of-open-source-from-the-netflix-cloud-security-team-412b5d4f1e0c?source=search_post---------290,"by Jason Chan
This summer marks three years of releasing open source software for the Netflix Cloud Security team. It’s been a busy three years — our most recent release marks 15 open source projects — so we figured a roundup and recap would be useful.
Penetration testing tools, vulnerabilities, and offensive security techniques have dominated security conferences and security-related open source for some time. However, in recent years, more individuals and organizations have been publishing “blue team” and defensive security tools and talks. We’re thrilled that the security industry has become more supportive of sharing these tools and techniques, and we’re more than happy to participate through the release of open source.
Our security-related OSS tends to be reflective of the unique Netflix culture. Many of the tools we’ve released are aimed at facilitating security in high-velocity and distributed software development organizations. Automation is a big part of our approach, and we seek to keep our members, employees, data, and systems safe and secure while enabling innovation. For our team, scale, speed, and integration with the culture are the keys to enabling the business to move fast.
Without further ado, here’s a look back at the OSS we’ve released.
We’ve enjoyed contributing to the OSS security community and have learned a lot from the feedback and collaboration. It’s always instructive to see how software evolves over its lifecycle and to see how others extend it in novel and creative ways. And going forward, we’ll look to make more use of our Skunkworks project to share projects that are experimental or that we don’t necessarily envision supporting long term. We have a few projects we’re considering open sourcing in the near future — if you’re interested, keep an eye on this space, our GitHub site, and @NetflixOSS on Twitter, and check out our YouTube channel for more talks from our team.
Learn about Netflix’s world class engineering efforts…
533 
2
533 claps
533 
2
Written by
Learn more about how Netflix designs, builds, and operates our systems and engineering organizations
Learn about Netflix’s world class engineering efforts, company culture, product developments and more.
Written by
Learn more about how Netflix designs, builds, and operates our systems and engineering organizations
Learn about Netflix’s world class engineering efforts, company culture, product developments and more.
"
https://medium.com/google-cloud/how-to-deploy-a-static-react-site-to-google-cloud-platform-55ff0bd0f509?source=search_post---------291,"There are currently no responses for this story.
Be the first to respond.
Top highlight
Looking at the few guides out there, I was pretty confused when trying to find out how to deploy my app.
I looked at a few guides from Google’s site and some Stack Overflow posts but didn’t find them too helpful.
I found a great resource which got me 70% there on MDN:
developer.mozilla.org
Filling in a few gaps I was able to deploy my static React site. So here’s my guide, in a few easy steps.
Note: we’ll be deploying a static site which uses React to render the Front End. No backend/server is hooked up in this example.
3. Create a bucket in GCP. You can keep the default values, just click create.
We’ll be using this bucket to easily upload our build folder files into GCP. Afterwards we’ll transfer the files to our GCP project. Take note of the bucket name as we’ll be using this later. This bucket’s name is straight-veld-8658
4. Once the bucket has been created, click into it and select upload files. Browse to your project directory and upload the entirebuild folder.
5. Another thing we need is an app.yaml file. This file is a config file that tells the App Engine how to map the URLs to static files. I’ve used the same app.yaml file as provided in the sample-app of MDN’s tutorial (but have changed the website directory to build. It looks like this:
Upload this file to the bucket as well.
Your bucket should now be populated with the following files:
6. On the same page find the icon that lets you open a Google Cloud Shell to your app instance. Click on it and open the shell.
Now we’ll upload the build directory and app.yaml file that we uploaded into the straight-veld-8658 bucket into our instance so we can launch the app. Use the following commands:
Note: the format of the gsutil rsync command is as follows : gsutil rsync -r [source] [destination] (-r means sync files recursively)
You can make sure that the files are there. When you cd test-app and ls you should see both app.yaml and build
7. Deploy the app by running gcloud app deploy in the shell. You should see some sort of success message indicating the app is served. It should also provide you with the url you can visit to see the app.
Generally this url is in the following format unless you’ve linked a custom domain name to it: https://[app_name].appspot.com
Google Cloud community articles and blogs
694 
25
694 claps
694 
25
Written by
Learn more about me here: https://dddotcom.github.io/ !
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Learn more about me here: https://dddotcom.github.io/ !
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://towardsdatascience.com/how-to-use-jupyter-on-a-google-cloud-vm-5ba1b473f4c2?source=search_post---------292,"Sign in
There are currently no responses for this story.
Be the first to respond.
Lak Lakshmanan
Apr 11, 2019·8 min read
Note: The recipes in this article will still work, but I recommend that you use the notebook API now. Do:
gcloud beta notebooks --help
The simplest way to launch a notebook on GCP is to go through the workflow from the GCP console. Go to AI Platform and click on Notebook Instances. You can create a new instance from the user interface:
Once the instance is launched, you can click on a link to open JupyterLab:
When the instance is launched, it has a persistent disk. That disk will hold your notebooks. You can stop and restart the VM (from the GCP web console) without losing those notebooks.
Note that you can attach a GPU to a notebook instance from the user interface:
Enjoy!
This article is a collection of a few of my “recipes” for working with Notebook instances.
The instance is a Compute Engine image, so if you want to script things out, customize the machine, change its firewall rule, etc. you can use Compute Engine capabilities. The notebook instance is a Deep Learning VM, which is a family of images that provides a convenient way to launch a virtual machine with/without a GPU on Google Cloud. It has Jupyter Lab already installed on it and you can access it without the need for proxies or ssh.
The simplest approach is to specify an image family (see the docs for what the image families are available). For example, you can get the latest image in the tensorflow-gpu family with a P100 GPU attached using:
The URL to access Jupyter Lab is part of the metadata of the VM that you just launched. You can get it using:
Here’s a script that will do steps #1 and #2, waiting until the Jupyter notebook server has started:
Simply navigate to that URL and you’ll be in JupyterLab.
Click on the last icon in the ribbon of icons in the left-hand pane and you will be able to git clone a repository. Use the one for my book:
https://github.com/GoogleCloudPlatform/data-science-on-gcp
Navigate to updates/cloudml and open flights_model.ipynb. You should be able to run through the notebook.
You can also open up a Terminal and use git clone, git checkout, git push, etc. I tend to find it easier than using the built-in Git UI. But your mileage may vary!
You can specify a set of operations to run after Jupyter launches. These will be run as root.
In general, use the image-family approach for development (so that you are always developing with the latest of everything), but pin down to a specific image once you move things to production. The reason you want to pin down to a specific image in production is that you want to run on a version that you have actually tested your code with.
Get the list of images and find the one you were using (the latest in the image family you specified above):
Then, specify it when creating the Deep Learning VM (lines you might want to change are bolded):
The key aspect here is to launch papermill with a startup script and exit the notebook VM using TERMINATE without a restart-on-failure once papermill is done. Then, delete the VM. See this blog post for more details.
To create a Deep Learning VM attached to a TPU, first create a Deep Learning VM and then create a TPU with the same TensorFlow version:
The only difference when creating the Deep Learning VM is that you are specifying the TPU_NAME in the startup script.
If you create a Deep Learning VM and you specified a GCP login name (all my examples above, except for the production one did so), then only you (and project admins) will be able to ssh into the VM.
All Jupyter notebooks will run under a service account. For the most part, this will be fine, but if you need to run operations that the service account doesn’t have permission to do, you can have code in Jupyter run as you by doing the following:
Note: Do not use end-user credentials unless you started the machine in ‘single user mode’.
Creating an image in the tf-latest family uses the latest stable TensorFlow version. To work with TF-nightly (e.g. this is how to get TensorFlow 2.0-alpha), use:
Restarting Jupyter: Usually, all you need to do is to restart the kernel by clicking on the icon in the notebook menu. But once in a long while, you might completely hose the environment and want to restart Jupyter. To do that, go to the Compute Instances section of the GCP Console and click on the SSH button corresponding to your Notebooks instance. In the SSH window, type:
Startup logs: If Jupyter failed to start, or you don’t get a notebook link, you might want to look at the complete logs (including startup logs). Do that using:
The TensorFlow images use pip, but the PyTorch images use conda. So, if you want to use conda, the PyTorch images are a better starting point.
If you want to develop using the Deep Learning VM container image on your local machine, you can do that using Docker:
If you have a GPU on your local machine, change the image name from tf-latest-cpu to tf-latest-cu100.
__________________________________________________
I’m interested in expanding on these recipes. Contact me if you have a suggestion on a question/answer that I should add. For your convenience, here’s a gist with all the code.
Data Analytics & AI @ Google Cloud
See all (63)
417 
9
Every Thursday, the Variable delivers the very best of Towards Data Science: from hands-on tutorials and cutting-edge research to original features you don't want to miss. Take a look.
417 claps
417 
9
Your home for data science. A Medium publication sharing concepts, ideas and codes.
About
Write
Help
Legal
Get the Medium app
"
https://koukia.ca/build-your-first-web-api-with-f-giraffe-and-host-it-on-azure-cloud-1d9dc07dc248?source=search_post---------293,"Today I am going to talk to you about programming Web APIs using F# programming language.
F# has been around for a while now, but what I would like to talk about is F# in .Net Core because it has a lot of cool features specially for Web Programming and building Web APIs and with playing around with it building some Web API, so far I personally…
"
https://medium.com/s-c-a-l-e/talking-microservices-with-the-man-who-made-netflix-s-cloud-famous-1032689afed3?source=search_post---------294,"There are currently no responses for this story.
Be the first to respond.
Adrian Cockcroft is currently a technology fellow at venture capital firm Battery Ventures, but he is best known for his stint as cloud architect at Netflix. He was the company’s public face as it grew into one of the world’s foremost users of cloud computing and microservices, and developers of open source technology. He also speaks at a lot of conferences.
In this edited interview, Cockcroft discusses his path to Netflix, the lessons he has learned about scalability over the years, and why microservices are arguably more about business concerns than about architectural best practices.
SCALE: Can you give a recap of your career? I think most people know you from Netflix, and now with Battery Ventures, but you were with Sun Microsystems in the early days.
ADRIAN COCKCROFT: The foundation is I grew up in the U.K. playing around with computers in high school in the 1970s when that was a very rare thing. I decided to go and learn something to do with computers rather than do computing directly, so I did physics and electronics at university. The first job I had was an engineering R&D company where we basically built all kinds of embedded systems, and eventually was one of the first customers of Sun in the U.K.
After about three years of that, Sun opened an office across the street, and I persuaded them to hire me as a field systems engineer. I went from being scraggly looking, T-shirt-and-jeans developer, age 20-something, to having to wear a suit everyday and drive around with sales reps and try to persuade other people to buy computers. A little bit of a culture shock. I did that for five or six years.
…
Just as the internet was starting to happen and we were making this transition from Sun being a workstation company to a server company. One of the first things that happened was that we got this 20-processor machine, and all the software was designed to run maybe one or two processors, so there was a huge scalability performance problem. Then web servers came a long and they didn’t scale either. We had all of that interesting stuff in the lab, so I spent several years figuring out scalability performance problems.
I eventually became a distinguished engineer at Sun for capacity planning and performance-related things. Largely because I wrote a book about it in in the ‘90s. … Eventually, I became a high-performance computing architect for Sun for a year or two, and then Sun laid off that entire team as it shrank.
I went to eBay to sort of lick my wounds and get out of enterprise IT for a bit. Two or three years at eBay, helped form eBay Research Labs and played around. I had a lot of fun building mobile apps, and building datacenter capacity-planning models and things like that.
That was up to ‘07. … It wasn’t working out for me to be in a research lab.
Netflix in ’07 … was trying to hire people who knew about scalability. That was really my specialty at the time, so I managed a team there that was building the home page for Netflix for the DVD business. Another guy, Mark White, was building the streaming home page at the time.He had recently been hired from PayPal and is still there as the VP running all of the personalization stuff there.
Eventually, we shifted around a bit, since I’m not a front-end developer and I don’t really get all of that stuff still. I started doing more backend things and built the services that supported personalization. Eventually that had to move into the cloud, so we figured out how to do that. Then we formed a cloud team, and then I joined that as the overall architect in about 2009 or 2010.
“I think there’s a breed of people that are performance-engineering people. … If you scratch the surface, you usually find they have a physics degree instead of a computer science degree.”
Scalability, as a term, seems to have morphed in ways since your career started, where you’re talking about scaling up on these big 20-core boxes. How do you look at scalability differently today than you did when you were at Sun?
There’s the same set of problems: You’re starving some resource, which means you can’t scale. You’ve either got too much locking, or a lack of concurrency, not enough threads or something. The same basic principles and behaviors apply if you have a big machine with lots of CPUs in it, or a rack full of lots of separate machines. There’s some slight differences in the way they behave, but the basic problems and patterns look the same. It’s mostly about the tooling that changes.
I think there’s a breed of people that are performance-engineering people. You can see them scattered across the industry, and they don’t quite think the same way that normal developers do. If you scratch the surface, you usually find they have a physics degree instead of a computer science degree. That’s extremely common for people, even people who have been working in computers for a long time, that they actually have a physics degree.
Kyle Kingsbury, for example, who has been doing all of the Jepsen torture-testing of NoSQL things. He’s a theoretical physicist by training. … How stuff works, and how to decompose things and figure out how things behave is kind of the core of physics — experimental method.
What was the rationale 2009 and 2010 to move Netflix into the cloud? What did you see while so many other people were still debating whether cloud was a real thing?
Netflix was growing so fast that it couldn’t build datacenters fast enough. So they would have had to get very good very quickly at building datacenters in ridiculously short time spans — and thrown a huge pile of money at it, because you have to pay money up front for building out datacenter capacity. You see that with something like Zynga, where they put in $100 million or more in one year to just go build their datacenters.
Netflix wanted to spend that money on content. Every time you have $100 million, you go, “I could build a datacenter …” — because that’s the size of datacenter that would make any sense at all for Netflix — “or I could buy another season of House of Cards. Which moves the business forward? I think we’ll spend money on House of Cards, and we’ll just keep paying for datacenters as we go.”
You pay a month after you use it with Amazon Web Services, so the long-term investments are in content, and the expenses are in delivering the content. The cost of the content itself is vastly more than the cost of delivering the content for Netflix by huge amounts. If you spend a little bit more on computers it’s not material … it’s more about when you spend money in what investments and what you’re focused on.
This came from [CEO] Reed [Hastings] downwards, and there was a large team of people. I get more credit than I should for the Netflix cloud stuff, because I was the main person out there talking about it, so I kind of became the public face of it. That wasn’t deliberate.
“The kind of thing where you go in architect the version 2.0 thing for two or three years, and then deliver it with a pink bow on the top and say, ‘Everyone will move now’ — that never works.”
What was the learning process like as you went on? Eventually you started building all these internal tools, and there’s this big service-oriented architecture. I’m curious about the cloud effort grew.
Whenever you do a transition, the way you do it is you prioritize the things that need to be understood, and you do the smallest thing that teaches you the most, and you do that over and over again. It was very much pathfinder projects — very, very surgically we’re going to try this thing out. We’re going to go as deep as we can, and learn as much as we can with the smallest amount of risk. If you structure everything that way, then you discover deep things about the way things behave, and then you come back and change your plans a bit.
For example, the very first piece of Netflix that was running in the cloud was the search auto-complete service. As you start typing in “Ratatouille” and you can’t remember how to spell it, you type “rat” and it says, “You probably meant Ratatouille.” That ran as a service, there was no graphics around it. All of the website that was supporting that was still running in the datacenter. It’s just that as you type that word in, it was sent off to a search index in the cloud.
It’s a trivial piece of technology, but it taught us everything about pushing production systems to the cloud, hooking them up to a load balancer and the tooling we needed to do it. Two or three engineers, I think, worked on getting that built in a month or so maybe. It was a very small piece of work, plus the tooling, but it proved certain things worked. Then, we got the first bits and pieces up and running in the cloud one piece at a time.
That incremental approach works very well for basically taking risk out of the thing. The same thing when we switched from Oracle to Cassandra as well, where we went from a mixture of Oracle and SimpleDB to Cassandra. Again, you stand up one server, you take one backend dataset, you tinker with it, keep tinkering with it until you figure out the recipe that works, and then you duplicate it. And then if that works, it keeps duplicating itself until you’ve got thousands of nodes of Cassandra in 50 or 100 different clusters.
So it wasn’t just a mass migration all at one time?
No. The kind of thing where you go in architect the version 2.0 thing for two or three years, and then deliver it with a pink bow on the top and say, “Everyone will move now” — that never works. I saw that fail at eBay, as well. It would just take a long, long, long time to get the next version.
You have to incrementally build things, so it’s very organic, and it’s an emergent architecture. It’s not designed centrally. It’s whatever anyone needed to do at the time. And a lot of talking about thing so that bad ideas get rooted out and become understood as “avoid this.”
“You asked me what I’m most proud of. I think it’s, basically, that you can’t mention ‘cloud’ or ‘microservices’ or whatever without mentioning Netflix at some point now.”
If you look back at your time at Netflix, is there something you’re particularly proud of that you left there?
The thing that I did specifically, when I went into the architect role for the cloud team, I was talking to Yury Izrailevsky, who is the VP for cloud there, and said, “I want to go out and talk about what Netflix is doing.”
…
I worked with our public relations team and explicitly went to create a Netflix technology brand. Because Netflix is a consumer brand. When you hear Netflix, you should think about movies.
This is something you have to deeply think about. Because at eBay, they really wanted eBay to be a retail brand. They didn’t want the technology brand to play to eBay, so we didn’t do very many talks at eBay — because they wanted to keep the brand pure. With Netflix, they sort of went, “Well, there’s an opportunity here.”
And one of the problems Netflix has is, “How do you attract the very best engineers in the industry?” If you’re going for talent density, you’ve got to create some attractants. To create the attractants, you’ve got to talk about what we’re doing. We’d already put the culture deck out, which was attracting people in general, but we wanted to talk about the technology.
…
Then I said, “We can do that, and I can go out and talk about Netflix. And then as we’re developing all this code, we should also back that up with a resource.” We had all the pieces to release a platform. I was pushing the idea that we should release a platform and got other people to buy into it, and then collectively we agreed, “Yeah, we should just go ahead and release this thing as a platform.”
I effectively product-managed that. I wrote, basically, none of the code, but I named a few of the projects somewhere along the way.
…
You asked me what I’m most proud of. I think it’s, basically, that you can’t mention “cloud” or “microservices” or whatever without mentioning Netflix at some point now. We really did put Netflix on the map as a technology place. The effect of that is they’ve been able to hire some amazing engineers over the years, people that would not otherwise have decided to go to Netflix.
“It’s very hard to find well-written monoliths. Most of them are tangled balls of mud with all kinds of disgusting things going on inside that are broken in very odd ways that are hard to debug.”
You mentioned microservices. One of the things I saw covering the tech side of Netflix was this evolution from talking just about cloud to also talking about microservices. Can you explain that transition?
These newer architectures are built to be dynamic, and designed to be broken into small chunks so you can update them independently. As you try and go fast, and you try and do continuous delivery, it’s just harder to do continuous delivery if you don’t have API-driven infrastructure and self-service.
Cloud, for me, means self-service infrastructure, API-driven self-service. I don’t really care whether it’s public or private. We used to call the things we were building on the cloud “cloud-native” or “fine-grained SOA,” and then the ThoughtWorks people came up with the word “microservices.” It’s just another name for what we were doing anyways, so we just started calling it microservices, as well. … You’re trying to find words for things that resonate
…
Right now, “cloud” sort of implies you’ve got to migrate to cloud, and people that aren’t migrating to cloud are going, “OK, yeah, but maybe I don’t want to do that.” It’s sort of an operations-y thing, whether you’re running on cloud or not. Whereas “microservices” is really a developer term. It’s developer-driven architecture and you can deploy it on anything you want.
The second reason microservices has gotten really big right now is that it’s the terminology Docker’s been using since the beginning, and Docker has both benefited from this microservice architecture being a thing and also helped enhance it. There’s an obvious way to deliver microservices using containers, but also, containers arrived because we were trying to do microservices. We start with one chicken and one egg, and you end up with a whole chicken farm, or something like that.
There’s a cooperative runaway effect where part of the reason microservices are hot right now is because of containers and Docker, and they were also part of the reason that Docker became of interest.
When you look at this evolution to embracing microservices, how important of a shift in architectures do you think this is? Eric Brewer told me recently it will be bigger than the movement to cloud in the first place. Do you get that same sense?
My pitch, if you’ve seen my slide decks, is that you start of with the business need, which is moving faster, and you need to move faster than your competition. If everyone’s doing waterfall, then you can keep bumbling along. As soon as somebody does agile, you go, “Oh crap, we need to do agile too.” They move up, and now everyone’s releasing code every two weeks.
Then, someone figures out how to do continuous delivery, and it’s putting code out multiple times a day. Then, there’s another one of those “Oh, crap” moments. “We’re getting left behind.”
This is what’s really driving it at the business level. That’s what the CIO and the product people care about. They’re all trying to get product out faster than their competitors. But when you look at what it takes to do continuous delivery, you just end up with something that looks like microservices because you have to be able to break things into small chunks and get them out very fast.
There’s a limit on how big a team you can have to be agile. The limit is probably defined by whatever Etsy is currently doing, because they’re the best people at running a very, very agile monolithic app. It’s an amazing feat, but most people are saying, “It’s amazing, but it’s easier to just do things in smaller chunks. It’s less efficient, but I care more about speed than efficiency.”
“Oh yeah, the last bunch of machines we got you to provision — we stuck Docker on them, and now we’re just doing our own deploys as often as we like.”
There’s definitely a sense that a monolith is a more efficient way — a well-written monolith, anyway, is a very efficient way to do things. But it’s very hard to find well-written monoliths. Most of them are tangled balls of mud with all kinds of disgusting things going on inside that are broken in very odd ways that are hard to debug.
That’s part of the problem: as you get a large team of developers on a monolithic app, it gets harder and harder to build. You want to make it more tractable and, effectively, less complex and surface some of the complexity. I think the overall complexity goes down.
It sounds like almost a business decision to some degree more than a decision that a bunch of architects got together and said, “You know, we’re going to rebuild these in different ways.”
One of the reasons I get really good traction in the talk I’ve been giving, from everyone from management down to the developers, is because I start with connecting it to the real goal here: which is to just go faster, and what are all the different ways you can get friction out of the developer experience and the product release experience. As you speed up your delivery process, you’re reducing the risk by doing smaller and smaller chunks that have lower and lower risk, and you end up going faster and faster. Basically, you want to change one thing at a time.
This ties back into why Docker is interesting. It only takes seconds to deploy something with a containerized production thing like Docker. If it takes seconds to deliver, why are you doing it only once every two weeks to do something that takes a second? You could do it a thousand times a day and it would still not be overhead. Whereas, in the old days, it used to take weeks or months to deliver something because it took forever to test it, and it took forever to procure the hardware. All that stuff’s instant now.
“When your Amazon bill is about the same as what it costs to hire an engineer you go, ‘You know what? Before that doubles I want to just shrink it down and hire another engineer instead of spending the money on infrastructure.’”
That makes sense. One of the things I keep hearing as I’m talking to Mesos users is that development is so much faster now, they’re deploying stuff so much faster as a result of Docker on Mesos, let’s say.
Yeah, there’s lots of experiences like that. The speed is the thing that people want. No one wants their products to take longer to develop.
I’ve heard these kinds of things as well. One anecdote was an ops team that were reviewing their monthly review of their performance of their tickets and were they closing them on time. They looked at their things, and the number of provisioning tickets that they had had basically trended from whatever it was to a tiny fraction of it. It basically disappeared, and they went, “Oh, it must be a bug. Something’s gone wrong with the way we collect this data.”
So they went back and they said, “Yeah, but I can’t remember the last time I deployed a machine.” This is clicking-buttons-on-VMware-off-a-trouble-ticket kind of deploys.
So they bent back and talked to the developers and they said, “Oh yeah, the last bunch of machines we got you to provision — we stuck Docker on them, and now we’re just doing our own deploys as often as we like.” And, “Yeah, thanks. Every now and again we’ll need another one so don’t forget how to provision machines, but we’re not asking for 10 new machines a day. We’re doing hundreds of deploys a day without involving you.”
They were kind of going, “Well, I guess that’s good, but what am I here for now?” There’s that sense of automating things out of existence as a good thing, but you have to deal with the way that the workflows move around.
You’ve been away from Netflix for a while, and you’re at Battery now. What’s are you seeing as you’re engaging with what Battery’s portfolio companies?
One of the things I do is act as a consultant to the CTO of companies, and a lot of these companies are SaaS across a bunch of different industries. They go through this thing where you start off building, a Ruby on Rails backend or something, and eventually it gets a bit bigger, and they need to scale it. So some of them I’m helping scale off of a single backend database.
Or they’ve realized that their AWS bill is finally getting to be big enough that they should pay attention to tuning down a bit. My gut feel there is when your Amazon bill is about the same as what it costs to hire an engineer you go, “You know what? Before that doubles I want to just shrink it down and hire another engineer instead of spending the money on infrastructure.”
Sometimes I’m helping them with speeding up development, sometimes with migrations to cloud or between cloud vendors, and sometimes scalability and cost reduction. I’ve got some slides I’ve been using on how to optimize your build-out. Those are, generally, me acting as a technical consultant across the field.
“There’s lots and lots of pieces of our daily lives, and of industries’ daily lives, that are done with horrible, boring software that’s going to be replaced by a SaaS provider at some point.”
Are you seeing now in companies that that the technology is fundamentally different or better? What’s the shift in what’s happening in startups today compared to 10 years ago?
One of the things is that everything is open source. Your hardware is all cloud. Your cost of building something is really tiny now.
There was a conference for children called HackKidCon last year. I did a talk called, “How to be a data scientist for a dollar.” My point was to teach these kids that they could actually go, for less than a dollar spent, they could actually go and create an account on a cloud vendor, and provision Hadoop clusters, and learn to write MapReduce, or learn to use Spark, or learn to do things which a few years before would’ve been a hundred-person team and millions of dollars to just think about. All this stuff is available instantly, and I talked them through some of that.
…
The fun thing was that I actually had Google Glass and I put it on the head of this 12-year-old boy, and the video is bascially him watching me give the talk, recorded on Google Glass. It’s a kid’s eye view of the event.
Is the moral here, “If you’re a startup coming to pitch, the bar higher in terms of what’s expected of the backend or the product than it might have been before the cloud?”
No, it’s just the constant building. It does make it harder to do hardware. Hardware is disproportionately more expensive now than doing software startups. There are just a lot more SaaS-based services, because you need this continuous delivery, it’s very hard to do it with packaged software.
Basically, everything is moving out of packaged software, and there’s lots and lots of pieces of our daily lives, and of industries’ daily lives, that are done with horrible, boring software that’s going to be replaced by a SaaS provider at some point. All of the interesting things kind of fit in that space.
What’s next in computing, told by the people behind the…
155 
1
155 claps
155 
1
What’s next in computing, told by the people behind the software
Written by
Founder/editor/writer of ARCHITECHT. Day job is at Pivotal. You might know me from Gigaom - way back in the day, now.
What’s next in computing, told by the people behind the software
"
https://medium.com/avmconsulting-blog/kubernetes-ci-cd-using-jenkins-on-google-cloud-5b10da6147a6?source=search_post---------295,"There are currently no responses for this story.
Be the first to respond.
This is our new blog and its about setting up CI/CD pipeline for Kubernetes using Jenkins in Google Cloud .
Sample code GitHub link . This is simple python application with backend PostgreSQL as database. This is just sample code we use this as reference and later you guys can modify same for your code. In repository we have Dockerfile and Kube YAML scripts as part of main…
"
https://medium.com/hackernoon/make-predictions-on-a-custom-automl-model-with-cloud-functions-and-firebase-ec3868a1671b?source=search_post---------296,"There are currently no responses for this story.
Be the first to respond.
If you haven’t heard about AutoML yet, it‘s the newest ML offering on Google Cloud and lets you build custom ML models trained on your own data — no model code required. It’s currently available for images, text, and translation models. There are lots of resources out there to help you prepare your data and train models in AutoML, so in this post I want to focus on the prediction (or serving) part of AutoML.
I’ll walk you through building a simple web app to generate predictions on your trained model. It makes use of Firebase and Cloud Functions so it’s entirely serverless (yes, I put serverless and ML in the same blog post 🙄). Here’s the app architecture:
Want to skip to the code? It’s all available in this GitHub repo.
I was particularly excited to discover that in addition to providing an entire UI for building and training models, AutoML has an API for adding training data, deploying models, generating predictions, and more. Let’s say you’re crowdsourcing training data for your model: with the AutoML API you could dynamically add new data to your project’s dataset and regularly train updated versions of your model. I’ll cover that in a future post, here I’ll focus on the prediction piece.
For this demo we’ll build a web app for generating predictions on a trained AutoML Vision model (though it could easily be adapted to AutoML NL since they use the same API). The particular model I’ll be querying can detect the type of cloud in an image. On the frontend, users will be able to upload an image for prediction. Our app will upload that image to Firebase Storage, which will kick off a Cloud Function. Inside the function we’ll call the AutoML API and return the prediction data to our frontend client. The finished product looks like this:
Firebase is a great way to get apps up and running quickly without worrying about managing servers. It provides a variety of SDKs that make it easy to do things like upload images, save data, and authenticate users directly from client-side JavaScript.
For this blog post I’ll assume you already have a trained AutoML Vision model that’s ready for predictions. The next step is to associate this project with Firebase. Head over to the Firebase console and click Add project. Then click on the dropdown and select the Cloud project where you’ve created your AutoML model. If you’ve never used Firebase before, you’ll also need to install the CLI.
Next, clone the code from this GitHub repo and cd into the directory where you’ve downloaded it. To initialize Firebase in that directory run firebase init and select Firestore, Functions, Hosting, and Storage when prompted (this demo uses all four):
Now we’re ready to go. In the next step we’ll set up and deploy the Cloud Function that calls AutoML.
You can use Cloud Functions independently of Firebase, but since I’m using so many Firebase features in my app already, I’ll make use of the handy Firebase SDK for Cloud Functions. Take a look at the functions/index.js file and update the 3 variables at the top to reflect the info for your project:
Our Cloud Function is defined in exports.callCustomModel. To trigger this function whenever a file is added to our Storage bucket we use: functions.storage.object().onFinalize(). Here’s what’s happening in the function:
We can create an AutoML prediction client with 2 lines of code:
The request JSON to make an AutoML prediction looks like this:
All we need to do to send this to the AutoML API is created a prediction client and call predict():
Time to deploy the function. From the root directory of this project, run firebase deploy --only functions. When the deploy completes you can test it out by navigating to the Storage section of your Firebase console and uploading an image:
Then, head over to the Functions part of the console to look at the logs. If the prediction request completed successfully, you should see the prediction response JSON in the logs:
Inside the function, we also write the prediction metadata to Firestore so that our app can display this data on the client. In the Firestore console, you should see the metadata saved in a images/ collection:
With the function working, it’s time to set up the app frontend.
To test the frontend locally, run the command firebase serve from the root directory of your project and navigate to localhost:5000. Click on the Upload a cloud photo button in the top right. If the image you uploaded returned a prediction from your model, you should see that displayed in the app. Remember that this app is configured for my cloud detector model, but you can easily modify the code to make it work for your own domain. When you upload a photo, check your Functions, Firestore, and Storage dashboards to ensure everything is working.
Finally, let’s make use of Firebase Hosting to deploy the frontend so we can share it with others! Deploying the app is as simple as running the command firebase deploy --only hosting. When the deploy finishes your app will be live at your own firebaseapp.com domain.
That’s it! We’re getting predictions from a custom ML model with entirely serverless technology. To dive into the details of everything covered in this post, check out these resources:
Let me know what you think in the comments or find me on Twitter at @SRobTweets.
#BlackLivesMatter
656 
2
how hackers start their afternoons. the real shit is on hackernoon.com. Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
656 claps
656 
2
Written by
Connoisseur of code, country music, and homemade ice cream. Helping developers build awesome apps @googlecloud. Opinions = my own, not that of my company.
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
Connoisseur of code, country music, and homemade ice cream. Helping developers build awesome apps @googlecloud. Opinions = my own, not that of my company.
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/swlh/multi-cloud-architectures-for-the-enterprise-part-1-623530b6b4c4?source=search_post---------297,"There are currently no responses for this story.
Be the first to respond.
These days, many enterprises are opting for a multi-cloud strategy. At my previous employer, I’d worked with AWS for 10 years, I’ve just recently moved into a new role with a Microsoft partner who naturally recommends Azure for a lot of workloads (even though AWS claim they are the better platform for Windows workloads) — I still love and prefer AWS.
"
https://medium.com/firebasethailand/%E0%B8%AD%E0%B8%AD%E0%B8%81%E0%B9%81%E0%B8%9A%E0%B8%9A%E0%B9%82%E0%B8%84%E0%B8%A3%E0%B8%87%E0%B8%AA%E0%B8%A3%E0%B9%89%E0%B8%B2%E0%B8%87%E0%B8%90%E0%B8%B2%E0%B8%99%E0%B8%82%E0%B9%89%E0%B8%AD%E0%B8%A1%E0%B8%B9%E0%B8%A5%E0%B8%82%E0%B8%AD%E0%B8%87-instagram-%E0%B9%82%E0%B8%94%E0%B8%A2%E0%B9%83%E0%B8%8A%E0%B9%89-cloud-firestore-%E0%B8%95%E0%B8%B1%E0%B9%89%E0%B8%87%E0%B9%81%E0%B8%95%E0%B9%88%E0%B9%80%E0%B8%A3%E0%B8%B4%E0%B9%88%E0%B8%A1%E0%B8%95%E0%B9%89%E0%B8%99-5f74ec088bce?source=search_post---------298,"There are currently no responses for this story.
Be the first to respond.
ต่อจากที่เคยพูดไว้ในบทความที่แล้วว่าเราจะมาลองออกแบบโครงสร้างของฐานข้อมูลโดยใช้ Cloud Firestore กันซึ่งเป็นฐานข้อมูลแบบ NoSQL โดยตัวอย่างของสิ่งที่เราจะลองออกแบบกัน เราก็จะเลือกสิ่งที่ใกล้ๆตัวเราเพื่อที่จะทำให้ทุกคนได้เห็นภาพนั้นคือ Instagram นั้นเองซึ่งเราเกือบทุกคนก็คุ้นชินกับมันและก็ยังมี Scope ที่ไม่ใหญ่จนเกินไป ทำให้ทุกๆคนสามารถมองเห็นภาพรวมและนำไปประยุกต์กับการออกแบบฐานข้อมูลของตนเองได้
ย้อนกลับไปพื้นฐานของโครงสร้างของฐานข้อมูลใน Cloud Firestore กัน ใครที่ยังไม่รู้ว่า Cloud Firestore นั้นเก็บข้อมูลในรูปแบบไหนบ้าง เช่น Key-Value , Field , Document , Collection , Subcollection ให้ลองกลับไปอ่านบทความ เข้มข้นกับ Firebase Cloud Firestore ระบบฐานข้อมูลที่เปิดตัวใหม่ล่าสุดจาก Firebase แบบจัดเต็ม เป็นพื้นฐานก่อนนะครับ เพราะในบทความนี้เราจะเข้าเรื่องโครงสร้างของฐานข้อมูลที่เคยอธิบาย ประเภทโครงสร้างของฐานข้อมูล กันคร่าวๆไปในบทความที่แล้วกันเลย
ในโครงสร้างของฐานข้อมูลของ Cloud Firestore จะแบ่งออกเป็น 3 ประเภทใหญ่ๆ ได้แก่ Nested data in documents , Subcollections , Root-level collections โดยที่แต่ละชนิดก็จะมีข้อดีข้อจำกัดแตกต่างกันไป เราควรที่เลือกเพื่อให้เหมาะกับสถานการณ์การใช้งานของเรา
การออกแบบโดยใช้ Document เพียงอย่างเดียว ซึ่งถ้าหากข้อมูลมีความซับซ้อนก็จะใช้วิธีสร้าง Objects หรือ Array
การออกแบบโดยการสร้าง Collection ไว้ใน Document ซึ่งเรียกว่า Subcollection เมื่อเรามีข้อมูลที่ต้องการขยายมากขึ้นเรื่อยๆในอนาคต
การสร้าง Collection หลายๆอันไว้ที่ระดับบนสุด ถ้าหากเรามีส่วนของชุดข้อมูลที่แตกต่างกัน
หลังจากเรารู้ว่าแต่ละชนิดของการออกแบบมีข้อดีข้อจำกัดอย่างไรบ้าง ? ที่นี้เราลองมาดูขั้นตอนการออกแบบฐานข้อมูลกัน
ในขั้นตอนนี้เราจะมาลองดูกันว่ารายละเอียดต่างๆของแต่ละหน้าหลักๆ ของแอพ Instagram มีอะไรกันบ้าง
หน้า Home
หน้า Search
หน้า Post
หน้า Notification
หน้า Profile
ต่อมาเราจะรวบรวมรายละเอียดต่างๆในส่วนย่อยๆ เช่น Story , Post , Comment , Reply , Notification , Profile
รายละเอียดของ Story ประกอบไปด้วย
รายละเอียดของ Post ประกอบไปด้วย
รายละเอียดของ Comment ประกอบไปด้วย
รายละเอียดของ Reply ประกอบไปด้วย
รายละเอียดของ Notification ประกอบไปด้วย
รายละเอียดของ Profile ประกอบไปด้วย
มาถึงขั้นตอนที่เราจะนำเอาข้อมูลที่เราศึกษาและเก็บรวบรวมรายละเอียดมาวางเป็นโครงสร้างของฐานข้อมูลกันดู โดยที่เราต้องเข้าใจก่อนว่า การออกแบบโครงสร้างของฐานข้อมูลนั้นไม่มีถูก ไม่มีผิด แต่จะอยู่ที่ว่าเหมาะสมกับการใช้งานของเราหรือไม่และความสะดวกในการ เพิ่ม , ลบ , นำข้อมูลออกมาแสดงผล , การสอบถามข้อมูลต่างๆ นั้นสะดวกเพียงใด เพราะอย่างที่ได้เคยกล่าวไปในบทความที่แล้วว่า
ถ้าหากเราออกแบบโครงสร้างไม่ดีนั้นก็จะทำการทำงานของเรายากขึ้นตามไปด้วย
โดยการออกแบบโครงสร้างนั้นจะมีทั้งส่วนที่เรามองเห็นจากภายนอกเป็นรูปธรรมเช่น ข้อมูลของ Post หรือ จะเป็นการออกแบบที่จะเอาไว้ใช้ประโยชน์ในด้านอื่นๆ เช่น ในด้านความสัมพันธ์ของข้อมูล , ในด้านการตรวจสอบความถูกต้องของข้อมูล เป็นต้น
อย่างไรก็ตามทาง Firebase ก็ได้คำแนะนำให้เราทำการออกแบบให้ Flat และ Denormalize คือ เวลาที่เราไปร้องขอข้อมูลจากที่ใดๆ เราควรจะได้ข้อมูลที่พร้อมนำไปแสดงผลทั้งหมด ไม่ใช่กรณีที่เราต้องรอข้อมูลในผลลัพธ์จากการร้องขอครั้งที่ 1 และนำข้อมูลบางส่วนของผลลัพธ์นั้นไปเป็นข้อมูลที่จะต้องเรียกใช้ในการร้องขอครั้งที่ 2 ถึงจะได้ข้อมูลไปแสดงผลครบถ้วน และไม่ต้องกลัวที่จะมีข้อมูลซ้ำๆกันภายในฐานข้อมูล เพราะทาง Firebase นั้นต้องการให้เราสามารถดึงข้อมูลออกไปใช้ให้ง่ายที่สุด โดยในคำแนะนำนี้ก็เป็นคำแนะนำรวมทั้งฐานข้อมูลแบบ Realtime Database และ Cloud Firestore ด้วยแต่ใน Cloud Firestore จะสามารถซ้อนทับของข้อมูลลงไปได้ (Nested) แต่ยังไงเราไม่ต้องทำตามคำแนะนำนี้เสมอไปก็ได้ ขอให้เราออกแบบฐานข้อมูลให้เหมาะสมกับการใช้งานของเรามากที่สุดก็พอ
ทีนี้รูปแบบการออกแบบโครงสร้างของเราที่จะใช้ เราจะใช้ทั้ง 3 รูปแบบเลย บางจุดก็จะเป็น Nested data in documents บางจุดก็ใช้ Subcollections หรือบางจุด Root-level collections โดยจะอธิบายเหตุผลที่จะใช้ในแต่ละส่วนกันไป
มาเริ่มจากส่วนเล็กๆกันก่อนดีกว่า
Story
โดยผมจะสมมุติ ลักษณ์ “+” เป็นตัวแทนของ Collection และ “>” เป็นตัวแทนของ Document ส่วนที่ไม่มีอะไรข้างหน้าเลยก็คือชื่อ Field นะครับ โดยในวงเล็บก็จะเป็น Type ของแต่ละ Field
ซึ่งอันนี้เป็นเพียงโครงสร้าง Story เพียงอันเดียว แต่ละคนอาจจะมีหลายๆอัน เราจะค่อยไปจัดการความสัมพันธ์ทีหลัง ซึ่งแต่ละ Story จะมีอายุ 24 ชั่วโมง โดยเราก็สามารถเขียน Cloud Funtion มาตรวจสอบได้จาก timestamp ถ้าเกินเราก็ลบ storykey นี้ทิ้ง
Watch Story
ซึ่งอันนี้เป็นเพียงโครงสร้าง Watch Story เพียงคนเดียว แต่ละ Story อาจจะมีคนมาดูหลายคน เราจะค่อยนำไปจัดการความสัมพันธ์ทีหลัง
Post
ซึ่งอันนี้เป็นเพียงโครงสร้าง Post เพียงอันเดียว แต่ละคนอาจจะมีหลาย post เราจะค่อยนำไปจัดการความสัมพันธ์ทีหลัง โดยแต่ละ post อาจจะมีข้อมูลภายใน ไม่เหมือนกัน เช่น บาง Post เป็นรูปก็จะมี imagePost หรือหากเป็นวิดีโอก็จะมี videoPost และ watchVideoPostCount เป็นต้น
Like Post
ซึ่งอันนี้เป็นเพียงโครงสร้าง Like Post เพียงคนเดียว แต่ละ post อาจจะมีคนมากดหลายคน เราจะค่อยนำไปจัดการความสัมพันธ์ทีหลัง
Comment
หลายละเอียดต่างๆ คล้ายๆกับ Post แต่อาจจะมีรายละเอียดน้อยกว่า ซึ่งนี้เป็นเพียงโครงสร้างของ Comment เดียว แต่ในแต่ละ Post อาจจะมี Comment หลายอัน
Like Comment
เป็นโครงสร้างคล้ายๆของ Like Post แต่เปลี่ยนชื่อมาเป็นของ Like Comment โดยมีจุดประสงค์เหมือนกันเป๊ะ
Reply
ไม่มีไรมาก Copy จาก Comment มาแปะแล้วเปลี่ยนชื่อ
Like Reply
เช่นเดิมครับ Copy แปะแล้วเปลี่ยนชื่อ
Notification
ซึ่งอันนี้เป็นเพียงโครงสร้าง Notification เพียงอันเดียว แต่ละคนอาจจะมีหลาย Notification เราจะค่อยนำไปจัดการความสัมพันธ์ทีหลัง โดยแต่ละ Notification อาจจะมีข้อมูลภายใน ไม่เหมือนกัน เช่น บาง Notification เป็นการแจ้งเตือนกด Like Post ก็จะมี postKeyPath หรือหากการแจ้งเตือน Comment ก็จะมี commentKeyPath เป็นต้น โดยใน Cloud Firestore จะมี Type ของตัวแปรชนิด Reference ให้เราสามารถระบุ Path ของ Document อื่นๆ ได้โดยง่าย โดย Reference Path ต่างๆ ในการออกแบบของเรา ก็จะมีประโยชน์ตอนที่ผู้ใช้กดจะไปดูรายละเอียดจากการแจ้งเตือนอีกทีก็จะทำให้เราจะนำพาผู้ใช้ไปเข้าถึงข้อมูลที่แท้จริงได้
Search History
การเก็บข้อมูลของผู้ที่เราทำการค้นหา ซึ่งเป็นเพียงโครงสร้างของประวัติการค้นหาเพียงบุคคลเดียว
Following
การเก็บข้อมูลของผู้ติดตามของเรา ก็คล้ายๆ กับกรณีอื่นๆ
Follower
การเก็บข้อมูลของผู้ที่เราติดตามของเร า Copy มาแปะแล้วเปลี่ยนชื่อครับผม
User
มาถึงตรงนี้ส่วนที่สำคัญและค่อนข้างที่ต้องอธิบายยาว นั้นคือโครงสร้างของ User โดยส่วนด้านบนตั้งแต่ profileImage ถึง biography ก็เป็น field ทั่วไป แต่จะมีส่วนที่พิเศษออกมาก็คือ
หลังจากที่เราออกแบบข้อมูลจากส่วนย่อยๆกันแล้วก็มีบางข้อมูลที่มีความสัมพันธ์กัน เช่น Post กับ Like Post , Post กับ Comment , User กับ Following เป็นต้น
ทีนี้ลองมาดูโครงสร้างของฐานข้อมูลหลังจากที่เราทำการความสัมพันธ์เรียบร้อยแล้วกัน ซึ่งในระดับบนสุดจะเป็นรูปแบบ Root-level collections โดยจะอธิบายเป็นจุดๆไป
Post
Story & Notification
User
userStory , userNewfeed , userNotification และ survey
userStoryจะเป็นส่วนเชื่อมความสัมพันธ์ของระหว่าง User และ Story โดยสาเหตุที่เราต้องแยกออกมาเป็น Root-level collections ใหม่
จะมีกรณีที่น่าคิดอยู่ 2 กรณี คือ
นั่นก็เพราะว่าหากเรานำไปทำเป็นโครงสร้างชนิด Nested data in documents ภายใน user นั่นจะหมายถึงข้อมูลของเราจะถูกอ่านพร้อมกับการเข้าถึงข้อมูลของ user คนนั้น แต่ข้อมูลที่จะบอกว่า User ของคนใดคนหนึ่งจะสามารถเห็น Story ของใครได้บ้าง เป็นข้อมูลส่วนตัวของเฉพาะแต่ละ User เราจึงไม่ควรที่จะให้ผู้ใช้คนอื่นสามารถเห็นข้อมูลตรงส่วนนี้ขณะที่เข้าถึง Document ของ User พร้อมกับข้อมูลใน Profile user คนนั้นๆ
2. ทำไมไม่ทำเป็น Subcollection เหมือนอย่างเช่น Following หรือ searchHistory
นั่นก็เพราะโครงสร้างภายในของ userStory ที่เราได้ทำการออกแบบจะประกอบไปด้วย 1 User สามารถเข้าถึง Story ของคนที่เราไปติดตามได้หลายคนและแต่ละคนอาจจะมี Story อยู่หลายๆอัน ซึ่งใน following user key เราจะเก็บ Reference Path แต่ละ Story ไว้เป็นข้อมูลชนิด Array
ซึ่งหากเราทำดังรูป จะทำให้เกิดข้อผิดพลาดขึ้นมา เพราะ ชื่อของ Document ไม่สามารถสร้างเป็น Array ได้ และ ชื่อของ Field ไม่สามารถมี ‘/’ ภายในชื่อได้ โดยก็มีวิธีแก้ปัญหาเหมือนกัน เช่นทำดังนี้
ซึ่งหากเรายังยืนยันว่าจะต้องนำ userStory ไปเป็น Subcollection ให้ได้ ก็จะต้องทำการออกแบบดังรูป แต่ Document ก็จะมีเพียง userkey ของเราเพียง Document เดียว ก็จะเกิดการซ้ำซ้อนของการออกแบบโครงสร้างจนเกินไป เพราะต้องมีการอ้างอิงถึง userkey ของเราถึง 2 ครั้ง โดยที่เราก็รู้ทั้งรู้ว่าเป็น Document ของเรา
วิธีที่แนะนำก็คือเราควรที่จะต้องแยกความสัมพันธ์ระหว่าง User และ Story ออกมาเป็น Root-level collections ใหม่นั้นเอง
userNewfeedก็มีสาเหตุที่จะต้องทำ Root-level collections ใหม่ ก็มีเหตุผลเหมือนกับ userStory ทุกประการ โดยที่โครงสร้างภายในก็จะเป็นการแบ่ง Document ไปตาม userkey ของผู้ใช้แต่ละคน และข้างในก็ใช้ postkey เป็นชื่อของ Field โดยจะมี Type เป็น null ซึ่งเราจะนำชื่อของ Field ที่ได้ไปหาข้อมูลรายละเอียดของ Post ที่ Path post/postkey
userNotificationก็มีสาเหตุที่จะต้องทำ Root-level collections ใหม่ ก็มีเหตุผลเหมือนกับ userStory ทุกประการ โดยที่โครงสร้างภายในก็จะเป็นการแบ่ง Document ไปตาม userkey ของผู้ใช้แต่ละคน ข้างในจะแบ่งเป็น Field ที่เก็บการแจ้งเตือนส่วนตัวของเราไว้เป็นชนิด Array แต่ละ Index จะเก็บ Reference Path ของ notification ไว้ และอีกส่วนจะเป็นการบอกว่าผู้ที่เราได้ทำการติดตามได้ไปทำอะไรบ้าง โดยจะแบ่งเป็น 2 ส่วน คือ หากผู้ที่เราติดตามหลายคนไปกระทำการใดๆ ที่ Post เดียวกัน และ หากผู้ที่เราติดตามคนเดียวไปกระทำการใดๆ ที่หลายๆ Post ซึ่งจะการเก็บข้อมูลเป็นชนิด Array แต่ละ Index จะเก็บ Reference Path ของ activityLog ของผู้ที่เราได้ไปติดตามไว้
surveyส่วนสุดท้ายของการออกแบบก็จะเป็นที่ส่วนที่จะให้ทุกๆคนเห็นเหมือนกันทั้งหมด คือหน้าที่ก่อนที่เราจะเข้าไป Search ที่จะมีการแนะนำรูปภาพหรือวิดีโอต่างๆ ซึ่งไม่ได้มีความสัมพันธ์ใดๆกับใคร ก็จะแบ่งเป็น Document เป็นประเทศๆ ไป Field ภายในก็เป็นประเภท Array ที่จะเก็บ Reference path ของ Post
นี้ก็เป็นไอเดียในการออกแบบโครงสร้างของฐานข้อมูลโดยใช้ Cloud Firestore เพื่อเป็นแนวทางให้กับแต่ละคนได้นำไปปรับใช้ในการออกแบบของตนเอง ซึ่งก็อย่างที่บอกไปข้างบนไว้ว่า ไม่มีถูก ไม่มีผิด มีเพียงเหมาะสมกับการใช้งานของเราหรือไม่ ก็ใช้เราเลือกใช้การออกแบบตามสถานการณ์กันไป อย่างไรก็ดีก็ควรคำนึงถึงความยาก-ง่ายในการสอบถามข้อมูลและความปลอดภัยของข้อมูลกันด้วย
โดยบทความต่อไปเราก็จะมาสร้าง Security & rule ให้กับฐานข้อมูลของเราโดยใช้ตัวอย่างจากฐานข้อมูลที่เราได้ออกแบบในบทความนี้กันต่อ ยังไงก็ฝากติดตามและกดปรบมือเป็นกำลังใจให้ด้วยนะครับ :)
Let you learn and share your Firebase experiences with each…
580 
4
580 claps
580 
4
Let you learn and share your Firebase experiences with each other.
Written by
CTO @ Flagfrog # Full-stack Developer # Everything i can do , but it maybe not cool
Let you learn and share your Firebase experiences with each other.
"
https://medium.com/sentinel-hub/improving-cloud-detection-with-machine-learning-c09dc5d7cf13?source=search_post---------299,"There are currently no responses for this story.
Be the first to respond.
Top highlight
This is a story about clouds. Real clouds in the sky. Sometimes you love them and sometimes you wish they weren’t there. If you work in remote sensing with satellite images you probably hate them. You want them gone. Always and completely. Unfortunately, the existing cloud masking algorithms that work with Sentinel-2 images are either too complicated, expensive to run, or simply don’t produce satisfactory results. They miss to identify clouds too often and/or like to identify clear sky over land as clouds.
The lack of a good cloud masking algorithm is also our problem at Sentinel Hub, therefore we’ve decided to tackle this challenge with our research team. We hope that we’ve managed to develop a fast machine learning-based algorithm that detects clouds in real time and gives state-of-the-art results among single-scene algorithms. We wish to share our cloud detector and how we’ve developed it with you.
Everything else from here on gives a more detailed description of the problem and our solution, which you may skip if you’re not interested in technicalities. But make sure not to miss the validation results and comparison with other algorithms on the market at the very end. And of course, all the pretty pictures.
Cloud detection is the most crucial step during the pre-processing of optical satellite images. Failure to mask out the clouds from the image will have a significant negative impact on any subsequent analyses such as change detection, land-use classification, crop monitoring in agriculture, etc. Multiple algorithms are currently in use to identify pixels contaminated by clouds. Here we list only a few, which are to our knowledge also most widely used and known. They can be roughly divided into two groups, depending on whether they use a single acquisition or multiple acquisitions recorded by the satellite at different dates:
Under the hood, all three algorithms implement a set of static or dynamic thresholds for various Sentinel-2’s spectral bands and apply them for the detection of clouds. We opt for a different approach — the machine learning approach.
Our aim is to develop a single-scene cloud detection algorithm that works on a global scale and heavily relies on machine learning techniques. We’ve started with the most simple approach — so called pixel-based classification — where we assign each image pixel a probability being covered with a cloud solely based on satellite’s spectral response for that pixel. Calculation of a pixel’s cloud probability therefore doesn’t depend on its neighborhood. We only take the broader context (pixel’s neighborhood) into account when we construct the cloud mask of a given scene from its cloud probability map by performing so called morphological operations, such as convolution. We believe that machine learning algorithms which can take into account the context such as semantic segmentation using convolutional neural networks (see this preprint for overview) will ultimately achieve the best results among single-scene cloud detection algorithms. However, due to the high computational complexity of such models and related costs they are not yet used in large scale production. We’ve decided to develop first the best possible pixel-based cloud detector, see how it compares to other existing algorithms, and last but not least learn in the process. In the future, we will also use deep learning techniques to detect clouds. But this is something for the future.
The success of any machine learning application depends heavily not only on the quality and size of the training data but also on the usage of an appropriate validation set.
To our knowledge, there exists only one publicly available data set of manually labeled Sentinel-2 scenes. Over the last couple of years, Hollstein et al. curated a data set consisting of around 6.4 million hand labeled pixels from 108 Sentinel-2 scenes sampled roughly evenly from around the globe and throughout the year to ensure full climatic coverage.
Each pixel is labeled with one of six classes: clear (land), cloud, shadow, snow, cirrus, or water. Authors used this data set to train and validate different machine learning algorithms to detect clouds. They report good results using a random and independent subset of their training set. The raw number of pixels in their data set being huge is in our opinion misleading. The pixels are sampled from larger labeled polygons drawn by human labeler and are thus by construction very correlated. The Hollstein data sample is effectively much smaller and any classifier trained on it will not generalize well. Therefore we do not use it as our training set but rather as a validation set.
The lack of large and high quality labeled data sets of Sentinel-2 data is a real problem in our opinion. It is slowing down the progress in the development, validation and comparison of cloud detection algorithms for Sentinel-2 imagery. We are actively tackling this problem in another project:
medium.com
Not having a large high quality labeled data set that we could use as our training data sample we turned to a second best option — to use the best currently available cloud detector and its cloud masks as a proxy for ground truth. We chose MAJA multi-temporal processor since it has, based on our experiences, a high cloud detection rate and a very low misclassification rate of no cloudy pixels. Usage of a machine labeled data set unavoidably introduces noise in labels which is not ideal, but on the other hand a machine curated data set can be produced very fast with no or very little costs.
When we started this project, there were over 40,000 MAJA products available to download each of them containing the raster cloud mask. To speed up the download process, we decided to download only the tiff files containing masks and not the entire products. In addition, we selected only products with image cover above 90% and cloud cover between 5% and 90%. In total, we have downloaded masks for 14,140 Sentinel-2 tiles, out of which 596 are geographically unique (tiles can cover the same location but are recorded at different dates). These 596 unique tiles include 77 different countries from Europe, Asia, Africa, North and South Americas, and Oceania. This is the closest that we could get to a truly global data set.
Each of the 14,140 Sentinel-2 tiles contains over 120 million pixels at 10 meter resolution, which is way too much to be used in training. Instead, we sampled randomly 1000 pixels from each tile and downloaded reflectances for all thirteen Sentinel-2’s bands using our Sentinel Hub services. In total, we have made over 14 million requests in a short period of time and we didn’t notice even a glitch in our Sentinel Hub services. Our training sample at the end consists of around 14 million pixels, 47% of them being classified by MAJA as cloudy pixels.
The lack of a validation set or a poorly chosen validation set can lead to a complete failure of a seemingly impressive machine learning model when implemented in production (check this blog for very nice discussion of the issue). Cloud detection or any other remote sensing application is very susceptible to this problem. Simply taking a random and independent subset of the training data to validate a model is not enough. The validation set must be representative of the new unseen data, which in the field of satellite imagery means it has to come from geographically scattered data covering all possible land cover types, such as water, bare land, forests, snow, grass, cropland, urban areas, etc. The Hollstein data set fits the bill of validation set perfectly. It consists of diverse geographic areas not covered by our training set and it’s of high quality since it was curated by a human labeler.
In addition we use as validation set also over thousand 64x64 patches labeled in-house with our own ClassificationApp. We used it for the feature and model selection process, while keeping the Hollstein data set for the final validation and comparison with other algorithms.
Once we had the training and validation sets in place, we started experimenting with input features, models, hyper-parameters, and whatnot in order to see what works best. Here’s the executive summary:
In order to have better visual comparison of cloud masks produced with our classifiers and MAJA, we randomly sampled one single 512x512 patch from each Sentinel-2 tile processed by MAJA that we have downloaded. We’ve calculated an intersection over union of the two masks and visually inspected those patches where the disagreement was found to be large. A pattern emerged: the disagreement was large on patches over water and bare land (sand, mountains and alike). In the former case, MAJA mask was often identified to be wrong, which is not surprising given the fact that MAJA algorithm is optimized for cloud classification over land and not water. In the case of bare land our classifier was wrong all the time.
To circumvent the issue of systematic misclassification of bare land as clouds, we’ve augmented the training sample with misclassified pixels from around fifty handpicked 512x512 patches. Our final classifier performs much better on bare land than our earlier versions, but it still fails from time to time. We have also added pixels from our in-house labled 64x64 patches to the training sample of our final classifier.
Uff. Congratulations. You made it until the most interesting part. Here’s the reward for sticking around until the very end: the comparison of performance of popular cloud detection algorithms including our own on Sentinel-2 imagery:
As mentioned above, we use Hollstein et al. hand labeled data set for validation and comparison. The results for single-scene algorithms on 108 hand labeled Sentinel-2 scenes with more than 6 million pixels are:
To the best of our knowledge, this is the first comparison of the most popular cloud masking algorithms on a large human labeled data set of Sentinel-2 imagery. As can be seen, our cloud detector developed for Sentinel Hub performs better than current state-of-the-art single-scene algorithms, yielding higher cloud detection rate while at the same time having much lower misclassification rate of land and snow as clouds. The misclassification of shadow as clouds is higher in the case of our algorithm on the account of dilation, which enlarges the cloudy regions. A cloud shadow is almost always next to a cloud in a satellite image therefore inflated cloudy regions will most often include cloud shadow regions as well. This is not really an issue since shadow masking usually follows cloud masking in a pre-processing chain.
The single-scene algorithms can be compared with the MAJA multi-temporal algorithm on a subset of Hollstein’s data set containing only 15 Sentinel-2 scenes from western Europe and northern Africa (see figure above). The results are:
Before we comment the above results, we would like to stress out again that this subset is clearly not representative of global land cover types and variations in climate. They should not be used to make any conclusions about the performance of all four classifiers on a global scale. The MAJA multi-temporal algorithm clearly outperforms single-scene algorithms in terms of land misclassification as clouds, while in the case of cloud and cirrus detection rates our classifier is better. The limitations of this subset are also visible in the fact that our classifier correctly identifies all cloudy pixels. It correctly identifies all cloudy pixels on this subset, but when subjected to a larger data set with more variation its cloud detection rate is no longer 100% (see the table with results using 108 Sentinel-2 scenes). The high misclassification rates for single-scene algorithms on this subset of 15 tiles compared to the misclassification rate shown above for 108 tiles can be explained by the fact that a large fraction of these 15 tiles include bare land with sand that looks very bright in images and is problematic to correctly classify by single-scene algorithms. It would be very interesting to have MAJA classification masks for all 108 tiles from Hollstein’s data set for a more fair comparison.
Machine learning approach can give state-of-the-art results, if the training and validation sets are both of good enough quality and representative of the unseen data. Procurement of labeled samples in remote sensing suitable for development of models that perform well on a global scale is particularly challenging. We’re fortunate to be living on a planet with such versatile landscapes. But this versatility is a curse for machine learning. Models based on machine learning have, however, the ability to constantly evolve and improve. With the new labeled data sets that we will curate with the help of community and feedback we hope to get from our users, we are certain that the performance of our cloud detector can only improve. Stayed tuned for future developments!
Contact us if you’re interested in using Sentinel Hub Cloud Detector. We will wait a bit to get additional feedback on our findings that confirm or challange our results. Once we are confident in our algorithm, we plan to process the entire Sentinel-2 archive and provide cloud masks for our Sentinel Hub users as an additional layer to be used in Custom scripts.
The Sentinel Hub Cloud Detector is available as python package s2cloudless. See
medium.com
for more details.
Further reading about this topic:
Stories from the next generation satellite imagery platform
716 
12
Thanks to Grega Milcinski, Miha Kadunc, Matej Aleksandrov, and Devis Peressutti. 
Some rights reserved

716 claps
716 
12
Written by
Physicist | Data Scientist | Problems Solver
Stories from the next generation satellite imagery platform
Written by
Physicist | Data Scientist | Problems Solver
Stories from the next generation satellite imagery platform
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/firebase-developers/organize-cloud-functions-for-max-cold-start-performance-and-readability-with-typescript-and-9261ee8450f0?source=search_post---------300,"There are currently no responses for this story.
Be the first to respond.
A while back I made a video about minimizing cold starts for functions deployed with the Firebase CLI using TypeScript:
It sums up the issue like this:
"
https://medium.com/@jgwest/microclimate-a-new-container-based-multi-language-cloud-friendly-development-tool-98a2d03326a9?source=search_post---------301,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jonathan West
Oct 1, 2018·10 min read
Microclimate is a new free-of-charge development tool from IBM that makes it easy to create Java/Node/Spring/Swift cloud-native microservices. But, unlike traditional IDEs, with Microclimate your full development environment (including the IDE itself) is entirely contained within lightweight Docker containers that are running on your local machine (or optionally, in Kubernetes). Now, you may ask, why have we at IBM created yet another development tool? Excellent question… read on to find out!
Microclimate is our team’s vision for the future of microservice/cloud-native application development tools, targeting a full range of cloud environments. This include Docker-based containers on IaaS VMs, hybrid cloud environments through IBM Cloud Private, public cloud Kubernetes hosted on the IBM Cloud Kubernetes service, as well as general support for similar environments in the clouds of other major providers.
As of this writing, the Microclimate project is still new and growing, with it’s first public release dating back to only February 2018 (and it’s first internal release not far behind that).You can learn more by visiting our product page at https://microclimate-dev2ops.github.io/.
There are many features that set Microclimate apart from other development environments. Here are just some of the highlights:
Microclimate is built from the ground up to support the polyglot development model, where small independent applications (ie microservices) written in different languages/runtimes combine to form a single large application. This allows you to choose the best language/runtime combination for the job at hand: for example, a Node.js-based application may prove most adept at implementing a Web frontend that interacts with the browser, while Java may be better suited to backend business logic, or Swift better suited to a mobile applications frontend.
This multi-language multi-runtime world requires a development tool that treats all environments as equal first-class citizens: these different environments should run side-by-side in the UI, while still providing the features that developers expect from an IDE for that language (for example, content assist/intellisense). Microclimate lives in this brave new polyglot world, allowing you to create Java/Node/Spring/Swift applications that work together and exist side-by-side within a single consistent UI. Within this environment we include full real time code intelligence powered by the Language Server Protocol (LSP) technology. Microclimate is one tool to rule them all!
When you download the Microclimate installer, you will see it is a tiny file: only about 750KB. This is because when you extract and run it, the full contents of Microclimate will download as a self-contained set of Docker images. Microclimate exists entirely within these pre-built Docker images, and the footprint on your machine that is outside these containers is limited to the source code of your Java/Spring/Swift/Node projects.
Microclimate’s container-based architecture enables the polyglot development model described above by automatically downloading the required language/runtime development tools when you first begin developing in a new language.
For example, when you create or import a Java application, Microclimate will automatically download the Java JDK (the compiler and Java APIs), Maven (build system), and WebSphere Liberty Docker images (web container). Unlike traditional IDEs that require you to download/install these language-specific packages on your own, Microclimate grabs them on your behalf both automatically and seamlessly. And it’s not just Java: the Node, Spring, and Swift development/ecosystem tools are fully supported as well.
Microclimate gives you the choice of which editor or IDE you wish to use to develop. Included as part of the Microclimate browser-based UI is a code editor based on Eclipse Theia, which is a open source project designed from the ground up to provide a rich development environment delivered through web technologies.
In addition to developing using Microclimate’s integrated Theia editor, you can also develop using the IDE of your choice, whether it be Eclipse, VSCode, IntelliJ Idea, Sublime/Atom, etc. Code that is generated or imported into Microclimate is available outside the container as well, using a Docker bind mount, which means that code can easily be imported into your own IDE as easily as importing code from another local directory.
All your of Microclimate-based project files are accessible as a folder on your local machine under the ‘microclimate-workspace’ directory. Any changes you make here will be visible both inside and outside the Microclimate UI, and from here you may access the other Microclimate functionality. More information about using an IDE is here.
Even if you are not using the Microclimate code editor you can still take advantage of Microclimate’s many other features, including self-contained application containers, internal build system, build and application logs, integrated performance metrics, and Jenkins-based delivery pipeline.
As developers, our general day-to-day development workflow looks like this: we write some code, we wait for that code to build, we deploy that build, and then we run our application on that deployment. Hopefully our new code made our application better rather than worse, but using feedback from our changes we then do some more coding. Next we wait for that code to build, deploy it, and run it. That loop, what we refer to as the development inner loop — code, build, deploy, run — must be as fast possible, in order to keep your developers both happy and productive.
Personally as a developer I like to be as happy and productive as possible 😄, and as such in Microclimate this fast turnaround time has been a core focus. We want to avoid the scenario you may see in some other container-based development tools where the build and deploy steps take much longer than they should, leaving the programmer to get distracted by nearby shiny objects.
In Microclimate, using various clever Docker/Kubernetes tactics, we have achieved that fast turnaround time, reducing our waiting time to as slim a slice as possible.
Microservice projects that are generated by Microclimate are container native. That is, they are hosted in, built by, and deployed to, Docker containers, with the container images themselves being as close as possible to the Java/Node/Spring/Swift production containers you’ll find on Docker Hub. All of this combines to ensure that not only is it easy to build and run your project, but also that it will be seamless to transition that container into production.
Behind the scenes, when you create a new project or import an existing one, we automatically create a Dockerfile for that project as well as configuring an automated build (for example, a Maven build for Java). Microclimate then manages the lifecyle of that application, building and hosting it as a containerized application.
At any time you can see your application happily running along if you run the docker ps command (to list all running containers), or from within the deployment resource list in the Web UI of IBM Cloud Private (where containers are run at ‘enterprise scale’).
And to double-down on our ease of deployment, with just a few clicks Microclimate will create a Jenkins-based delivery pipeline to automatically deploy your containers to IBM Cloud Private or the IBM Cloud Kubernetes Service.
So you’ve developed an application in Microclimate, and now you need somewhere to host it: Microclimate makes it easy to create a deployment pipeline between your microservice project and the target IBM Cloud Kubernetes server.
With just a few clicks inside the Microclimate UI, a Jenkins-based delivery/deployment pipeline is configured in your target cloud environment. From that point on, any commits made to your projects will trigger a build that will then be packaged into a Docker image and uploaded to the target Docker image registry. The applications deployed by the Jenkins delivery pipeline are managed as Helm charts (linked to the Jenkins-built image in your Docker registry), and thus are visible with the other Helm releases from within the IBM Cloud Private or IBM Kubernetes Service user interfaces.
In less than a minute and with only a small handful of steps, you can go from starting the Microclimate environment for the first time, to creating and running your first Java/Node/Spring/Swift microservice.
When designing our user interface we exercised careful, considered focus to ensure that it is intuitive to new users, thus minimizing the barrier-of-entry to getting a real application up and running in the language and runtime of your choice.
You can throw out the excessively long step-by-step guides required by other IDEs in order to create and deploy a new project, because with Microclimate each next step is obvious and intuitive. Skeptical, or, curious to know more? Browse through the screenshots in this article and then try it for yourself!
If you’ve ever hit a Java OutOfMemoryException, or been baffled by why your supposedly lightweight code is eagerly consuming a full CPU, then you have been “lucky” enough to work with code that strains the limits of its virtual environment. Finding and fixing performance issues such as these can be tricky, and having the right performance tool to point you in the correct direction is key.
That’s why every application running in MC is automatically configured to run with performance monitoring by default, which includes CPU usage, heap usage, % of time spent garbage collecting, HTTP Incoming Requests, HTTP Throughput, and maximum heap used after GC.
Also included is response time monitoring for individual HTTP endpoints, which includes total endpoint hits, average response time (ms), and longest response time (ms).
In addition we include the ability to further drive application load to test performance using JMeter, which you can kick off by clicking Run load in the App monitor tab.
In the past, the user interface of an IDE would traditionally have been coded using a native widget toolkit specific to a particular operating system, in tools like XCode (Cocoa), Visual Studio (Windows Forms/WPF/MFC/etc), or Eclipse (SWT, which interfaces with GTK/Cocoa/Windows).
In recent years, however, Web technologies like HTML, CSS, JavaScript, the HTML Canvas, and WebGL have been used to create richer user interfaces, as seen in tools like Visual Studio Code, Slack, and Atom. Applications based on Web technologies mostly maintain the overall feel of a native application, though not necessarily the specific look of one.
Microclimate is based on those same web technologies, both through our browser-based code editor Eclipse Theia, but also through the suite of tools that are integrated into Microclimate at the browser level. This ensures that Microclimate provides the same rich UI of other competing tools, while enabling fluid animation, vibrant colours, sophisticated layouts, and custom widgets.
Our UI is based on IBM’s Carbon Design System, which is built to provide beautiful and intuitive designs that are logically oriented around consistent guidelines. Not only is the Microclimate UI attractive and modern, but it also shares a unified design experience with our sister products in IBM Cloud and IBM Cloud Private.
Microclimate can be hosted either locally or on IBM Cloud Private (based on Kubernetes). This allow you to host multiple self-contained developer environments in a private cloud environment. Microclimate containers hosted on IBM Cloud Private can be installed through Helm, either through the catalog UI or the Helm CLI tool. Ultimately hosting on IBM Cloud Private means that the resources of the cluster are distributed across developers, and are fully managed inside the cloud, with developers requiring only a thin client (the web browser) to connect.
Hosting your development environment in the cloud has the advantage of persisting your workflow across individual machines and across physical contexts: if you start developing your code, but turn off your machine and head out of the office, when you later return to your cloud-based environment it will be exactly as you left it. Cloud-based development environments are likewise not tied to a specific machine, but are accessible from anywhere.
The best part of Microclimate is that you don’t have to take my word on all of its great features… you can try it out for yourself right now! Microclimate is available for download from our website.
The installer is less than a single megabyte, and running the installer will automatically download Microclimate into self-contained Docker images hosted on your local machine.
So try it out, and let us know what you think!
315 
2
315 claps
315 
2
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@sonradata/comparing-snowflake-cloud-data-warehouse-to-aws-athena-query-service-4f0ea32ef6db?source=search_post---------302,"Sign in
There are currently no responses for this story.
Be the first to respond.
Uli Bethke
Nov 27, 2018·11 min read
In this blog post we look at the commonalities and differences between the Snowflake cloud data warehouse and the AWS Athena query service. AWS Athena is built on top of open source technology Presto DB. As of the day of publishing, it does not support all of the features of Presto.
Both platforms implement a design that separates compute from storage. One can be scaled without having to scale the other. This is very different to a traditional MPP database such as Redshift, Teradata, Vertica etc. I have discussed the differences between the two approaches in detail in my post SQL on Hadoop, BigQuery, or Exadata. Please don’t call them MPP.
Both services follow a pay as you go model. You are only billed for the resources you use up. It’s like your electricity bill. This pricing model for data analysis was pioneered by Google (BigQuery) and Snowflake. Amazon then adopted the model and released AWS Athena in early 2017. It is now also implemented by Oracle for their autonomous data warehouse.
Snowflake charges you for storage and compute. Pricing starts at $2 per hour (minimum of 1 minute billed; by the second thereafter. Pricing for regions and editions differs). With Athena you pay for storage and the amount of data that is scanned during a query. For Athena you are charged $5 per TB scanned with a minimum scanning of 10 MB ($0.00004768). In other words you are punished for running queries over small data sets. Google BigQuery offers a similar price tag: $5 per Terabyte of scanned data. However, with BigQuery you are charged for the raw/uncompressed data whereas for Athena you pay for the data (either compressed or in raw format depending on your scenario). BigQuery gives you 1 TB for free and there is no minimum query charge. Assuming 5x compression, Google BigQuery on the surface is 5 times more expensive than Athena.
To forecast your annual consumption for both Athena and Snowflake you need to have a fair understanding of your use case, the workload and the number of users.
But how can we compare pricing between the Snowflake model and the Athena model?
We ran a quick test fully scanning a 392 GB table on Snowflake with SELECT * using a Medium sized virtual warehouse. The query executed in 316s. Extrapolated to 1 TB that gives us 827s or ~14 minutes. Translated into USD that gives us $1.4 to scan 1 TB of data (Medium instance costs are $6 for one hour).
Any pricing comparison should also take into account factors such as performance, management overhead, features etc.
If you are struggling with pricing feel free to reach out to us for help. We also have a set of guidelines on how to bring down the costs of your cloud data warehouse.
Both Snowflake and Athena claim to follow a serverless model. In other words it’s a self managed service. Everything is taken care of for you. There are no knobs to tune or administer. Compare this to other data warehouse platforms. Just scrolling through the list of parameters you can set in Hive (a popular data warehouse on Hadoop) takes a few minutes 🙂
With Snowflake there isn’t really a single parameter you have to tune or any maintenance you have to perform. No vacuum (re-sorting your data), no distribution keys, no stats collection, no data redistribution, no upgrades. You don’t need any DBAs either. There are no backups to manage as you are continously protected.
Let’s have a look at Athena. Upgrades, backups etc. are all taken care off. Combined with AWS Glue, Athena is even able to auto-generate schemas for you. You can achieve something similar on Snowflake using Fivetran and similar tools. There aren’t too many knobs or parameters to set or tune. However, as Athena does not come with a cost based optimizer there is a good bit of manual effort required to get good performance for your SQL queries. We also need to write ETL to convert our data to Parquet or ORC and to create partitions etc. We discuss the limitations of this further down in the post.
Snowflake is a proper relational data warehouse platform and supports transactions, different isolation levels, ACID, read consistency and multi version concurrency control (MVCC). In other words the stuff you would expect from a mature data warehouse platform. Snowflake has its roots in the world of relational databases.
Athena is positioned as a query service for running queries against data that already sits on S3. The use case is very limited. It shouldn’t come as a surprise then that Athena does not have any of the mature features you would expect from a relational data warehouse platform such as ACID, transactions etc. Athena is just an SQL query engine. It is based on Presto DB, a SQL engine with its roots in the Hadoop world.
Storage on Athena is immutable, which means that you can’t update, upsert, or delete individual records. This may be considered as a huge limitation. However, as I have outlined in my article on dimensional modeling in the era of big data there are ways to work around this limitation. It just adds extra effort and overhead 🙁
SQL
Both engines offer a mature set of SQL features. We see Snowflake slightly ahead, e.g. it offers User Defined Functions (UDF) and User Defined Aggregate Functions. We have a comparison of SQL support of various vendors on our website for download.
On the other hand, Athena supports geo spatial data types, which makes it convenient to work with maps and geo data. Currently, there is no support for geo-spatial data in Snowflake.
You scale Snowflake by selecting an instance type aka virtual warehouse. Instance types come in different sizes starting with XS and ranging to 4XL. The virtual warehouse represents the compute component of the platform. If you want to cut your execution time in half just double the size of your warehouse by the click of a button. The cost stays constant. Quite amazing. You get stuff done 2x, 4x, 8x, 16x etc. faster and don’t pay a cent more. Optionally you can set your warehouse to auto-scale and Snowflake automatically increases the warehouse size when there is increased demand on the platform.
Apart from scaling an individual virtual warehouse you can scale Snowflake by adding additional warehouses to the mix. This is particularly useful for physically separating different types of warehouse consumers from each other, e.g. you could spin up a virtual warehouse for a demanding ETL job or another one for your power users. It’s a bit like on the schoolyard where you separate kids by age and maturity.
You don’t have any of these options in Athena. Athena is a blackbox when it comes to scaling and concurrency. We could not find any way to scale a query by throwing hardware/money at the problem. Based on the documentation it seems to autoscale with an increase in the concurrent load. This is quite similar to the BigQuery model where the user does not have any control over scaling individual queries.
Snowflake has unlimited concurrency. You can run as many sessions in parallel as you would like by either scaling the size of a virtual warehouse or by adding additional warehouse instances.
Athena’s architecture also allows for unlimited concurrency. However, in practice AWS limits the concurrency to 20 parallel SQL queries (of type DML). You can request an increase of this limit by contacting AWS support.
Snowflake comes with a mature cost based optimizer (CBO). As per Snowflake philosophy everything is taken care off for you. You don’t collect statistics after data has been loaded or significant changes have been made to data in a table. The result of CBO planning can be examined using query profiles.
You don’t need to gather statistics in Athena either. But for a different reason. It is hard to believe, but Athena does not come with a CBO. In this article on Athena best practices I have come across stuff like this:
“When you join two tables, specify the larger table on the left side of join and the smaller table on the right side of the join. Presto distributes the table on the right to worker nodes, and then streams the table on the left to do the join. If the table on the right is smaller, then there is less memory used and the query runs faster”.
LOL. It’s reminiscient of the 1990ies. It’s probably best to avoid table joins altogether, in particular joining two large tables.
A third party CBO developed by Starburst in co-operation with Teradata is available for Presto DB but has not made its way into Athena (yet?).
Snowflake caches data you query on SSDs on the compute nodes. If the same data or a subset of the data is needed for a different query then the data is retrieved from cache. If you suspend your virtual warehouse, the data continues to reside on the compute nodes for some time. There is a good chance that the data is still cached when you spin up your virtual warehouse at a later stage.
Athena does not seem to have a data caching feature. At least we could not find any mention of such a feature.
The other type of caching is result caching. Both Snowflake and Athena (to a much lesser degree) support it. For each query the result is physically set down to storage and if the same query is run again is satisfied from the cache. For Snowflake this happens automatically. For Athena it is a manual process. Query results are stored in buckets on S3. Re-using the result set for another query requires us to create DDL against that table. We also need to point the query to it. In reality all this effort makes this feature unusable on Athena. However, if you just want to download the results of a query then this may help you.
Partitioning and clustering (similar in concept is bucketing on Athena) are manual processes on Athena. Both are fully automated on Snowflake.
Both Snowflake and Athena offer the usual security features such as encryption, authentication and authorisation.
Snowflake has a security feature that allows you to secure data at the column and row level.
With Snowflake you can set up a multi-tenant environment and only expose certain data sets to some of your users. Secure views can also be used with Snowflake’s data sharing feature. Athena does not have any similar features. You can secure objects at the less granular table level though.
Combining AWS Glue crawlers with Athena is a nice feature to auto generate a schema for querying your data on S3 as it takes away the pain of defining DDL for your data sets. You also have this option in Snowflake using third party tools such as Fivetran.
Paradoxically, data loading is Athena’s biggest weakness and in my opinion its ultimate downfall as a query service (nevermind a data warehouse). We have already learned that Athena is positioned as a query service with schema on read and a NoETL engine. In theory you just dump your data on S3 and query away. In practice it is more difficult. For efficient querying we want our data in Athena partitioned, bucketed, compressed, and in columnar format (Parquet or ORC). As per the Athena best practices guide you can achieve cost savings of 99%+ following these steps. So on the one hand you save a lot of money by applying these steps, but on the other all the nice benefits of schema on read and NoETL go out the window. You have to write a lot of code and run lengthy ETL pipelines. Sorry to burst the schema on read bubble. And did I tell you about optimal file sizes and the small files problem? Well, if your files are smaller than the blocksize (Athena documentation mentions 128 MB, which seems quite low) you will need to run some extra code to merge your files. The last problem you will encounter with schema on read and Athena is data quality. If your files do not fully conform to what you specify then querying will fail :-(.
Even though Snowflake is a schema on write engine it gets a lot closer to the NoETL ideal than Athena. Yes, in Snowflake you have to load your data and create a schema, but the platform takes care of everything else for you. You don’t have to create partitions or buckets. You don’t have to worry about ORC or Parquet, or compression. It is all done for you. Tools such as Fivetran or our own tool Flexter for XML and JSON also take care of target schema creation.
Snowflake also handles data quality issues by validating your data upload, optionally writing rogue errors to an error hospital during bulk loads.
Look beyond the hype. Learn the tools and techniques that work
Teach me Big Data
There are fewer tools that allow you to work with Athena than with Snowflake. Athena is mainly supported by AWS tools such as Quicksight etc.
Both platforms support JSON. Snowflake supports XML as a preview feature. Using SQL for querying semi-structured data works reasonably well for simple scenarios with low or medium data volumes.
Support for XML/JSON as data types and XQuery etc. to convert semi-structured data to tables is nice. But it will not get you very far in terms of automating the whole conversion process from XML/JSON to relational data.
We have developed Flexterto address all of these limitations. Flexter automatically converts JSON/XML to a relational format in Snowflake or any other relational database. It also supports Hadoop (ORC, Parquet, Avro) and text (CSV etc.).
Athena is built on top of Presto DB and could in theory be installed in your own data centre. Snowflake is only available in the cloud on AWS and Azure.
Both Snowflake and Athena come with SDKs. They both support JDBC and ODBC. Snowflake also ships connectors for Spark and Python and drivers for Node.js, .Net, and Go.
Snowflake has some cool features such as point in time queries, no cost data cloning, data sharing, materialised views that don’t have an equivalent feature in Athena.
As we have seen, Athena does not compare favourably as a data warehouse platform to Snowflake. I think we can all agree on this point. As a matter of fact, AWS don’t position it as a data warehouse. They market it as a query service for data on S3. It tightly integrates with the AWS Glue Catalog to detect and create schemas (DDL). Nice! In theory you should be able to query away to your heart’s content. In practice however, you first need to convert your data to Parquet or ORC, partition, bucket, compress, adapt its file size etc. before you are ready to rock. And even then you better be careful with your queries. It’s probably best to avoid joins of large tables altogether.
Is Athena useful as a query service? Personally, I would use Snowflake any time for this use case until AWS Athena adds more automation and lives up to the promises of schema on read. With Snowflake you have to perform considerably less ETL work. At the same time you get superior functionality and performance for a similar or even lower price tag.
Originally published at sonra.io on November 27, 2018.
CEO Sonra. Big Data and Cloud for Data Warehouse and BI Professionals http://tinyurl.com/yalvyfgy. Connect with me on LinkedIn https://www.linkedin.com/in/ulibe
330 
7
330 
330 
7
CEO Sonra. Big Data and Cloud for Data Warehouse and BI Professionals http://tinyurl.com/yalvyfgy. Connect with me on LinkedIn https://www.linkedin.com/in/ulibe
"
https://blog.graph.cool/prisma-cloud-preview-invite-only-c251bebf5670?source=search_post---------303,
https://javascript.plainenglish.io/unikernel-vs-container-vs-operating-system-side-by-side-comparison-3b8d6d93665d?source=search_post---------304,"There are currently no responses for this story.
Be the first to respond.
The process of deploying software to production evolves constantly. Just a few decades ago, everyone used virtual machines to host and manage the infrastructure. Lately, the industry shifted towards using containers with systems such as Docker and Kubernetes. The next logical step in this…
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/aelfblockchain/aelf-a-decentralized-platform-for-cloud-computing-78165a5ec79d?source=search_post---------305,"There are currently no responses for this story.
Be the first to respond.
aelf has been going strong since being introduced into the crypto world in the second half of 2017. Over the last three months this project has undergone some astounding growth, major updates, partnerships and a strong following spreading across multiple social media platforms. As such, it is high time for a review overview. This is aelf — Year in review.
Project Summary — What is aelf?
Before I go any further, let me give a brief introduction to the aelf project. aelf, for all intents and purposes, is creating a new blockchain which will resolve 3 main blockchain problems which has been restricting this technology from being adopted in more mainstream uses. These problems are limited performance, lack of resource segregation, and a properly working governance model. By resolving these issues, aelf will be bringing blockchain technology into business scenarios with full force.
In response to this, they have created a blockchain which can be integrated to side chain specific business scenarios. The aelf blockchain and sidechains will possess the following qualities:
Rebrand — Out with the Old, In with the New
aelf was originally launched as Grid, however due to confusion with a similar project called Grid+, it was rebranded as aelf late November 2017. Another confusion which was cleared up early in aelf’s life was the complete separation from the Polkadot project.
From Presentations to Investors
Throughout the Christmas and New Year season, Co-Founder and COO, Zhuling Chen led the worldwide charge of aelf presentations at multiple conferences. One of the first of these was the presentation at Consensus Invest 2017. aelf’s vision presented by Zhuling was to establish a ‘Central Business District’ where different industry applications have dedicated chains to serve their needs. Thanks to these impressive presentations; aelf’s private sale, to a focused investor group, oversubscribed by more than 6 times their cap within 2 weeks. These investors included 1kx Capital, Alphabit, BlockTower, FBG Capital, Galaxy Partners, Hashed, Hyperchain, LinkVC, and Signam Capital. aelf has attracted investors from all around the world which indicates that it’s looking to branch it’s operations and outreach globally instead of focusing on one region.
Candy, Candy for Everyone!
An exceptional marketing scheme developed by the aelf team was the Candy Airdrop Program. This airdrop project rewards users for completing simple tasks like re-tweeting an aelf tweet or logging onto the Candy dashboard daily. As of March 4th 2018, the Candy system’s success is demonstrated below:
Partnerships — They all want to be One
Another example of the combined marketing success and clear real world application for the aelf project is the continuous partnership announcements. aelf is targeting existing companies that are looking for Blockchain solutions. Companies that already have a customer base which will help increase awareness among customers that are not yet familiar with Blockchain. Over the last two months, partnerships and agreements have come from Decent, Youlive, Merry Merkle Tree, Wachsman, Theta, U Network, DATx and CNN.
Social Media — An Army has been Raised
Similarly to the business partnerships, the social media and community component to this project has not been overlooked either. Within a few months:
Behind the scenes — What Keeps this Machine Running
Such huge efforts on the frontline at aelf is just an indication to the work ethic of the whole team. Behind the front lines, the development team have been working tirelessly to improve and refine the overall aelf system. This includes things like SDK encapsulation and test work for asset management in relation to the Asset Chain, Smart Contract invoking tests and Chain initialization tests.
One Small Step for Man, One Giant Leap for Mankind
2018 is proving to be a busy year for the aelf team. Not only are they ramping up their marketing and online presence, they are also aiming to release minor/major upgrades every few months. By the start of 2019, aelf plans on officially launching their main product into multiple business scenarios.
I’m hooked, how do I get involved?
If you would like to hear the aelf team present, their next calendar event will be on Friday 23rd of March in Berlin, Germany. aelf will be presenting at ZK0x01 — The Zero Knowledge Summit at 9:30am. Alternatively, if you would like to invest in aelf, the tokens are currently listed on multiple exchanges including Huobi, Binance, OKEx, Bibox, Bancor Network, Kucoin and Gate.io. You can also get involved by following aelf on twitter and facebook, and joining the Telegram and WeChat groups.
Mappo has been investing and trading in fiat currencies since 2013. He has recently moved into the crypto world spreading his portfolio over long term coin investments, ICOs and day trading.
#Cryptocurrency #ICO #exchange #bitcoin #Crypto #blockchain #aelf #Mappo
Tomorrow is running on ælf.
904 
8
904 claps
904 
8
ælf, the next breakthrough in Blockchain.
Written by
ælf, the next breakthrough in Blockchain.
ælf, the next breakthrough in Blockchain.
"
https://towardsdatascience.com/how-to-host-a-r-shiny-app-on-aws-cloud-in-7-simple-steps-5595e7885722?source=search_post---------306,"Sign in
There are currently no responses for this story.
Be the first to respond.
Top highlight
Venkat Raman
Nov 29, 2017·7 min read
R shiny app is an interactive web interface. R shiny app has two components user interface object (UI.R) and server function (Server .R). The two components are passed as arguments to the shiny app function that creates a shiny app object. For more info on how to build Shiny apps please refer this link. Basics of R Shiny App
Log in to AWS account, click on EC2 under the ‘Compute’ header or click on EC2 under ‘Recently visited services’
Click on Launch Instance
Choose a Machine Image of your choice, here I have chosen Ubuntu server 16.04 LTS (HVM)
Choose an Instance type; one can start out with t2.micro, t2.small or t2.medium instances. For larger apps one can use t2.large and beyond.
Then click on launch instance, you will then be directed to the below page
Click on Edit security groups, you will be directed to below page (Configure security group).
In the SSH row, change source to ‘My IP’
Click on add a Rule, custom TCP Rule would be added. Under the ‘Port range’ enter 3838. This is the port for R shiny server.
Click ‘Review and Launch’ and then click ‘Launch’. Upon doing this you will get a dialogue box like below. The dialogue box helps in creating a private key which will enable us to ssh into the EC2 instance. Give a name to the key and click ‘Download Key Pair’. You will get .pem file. Save the .pem file/key securely.
Press Launch Instances. You will get a screen like below
If the instance is created successfully, the instance state will show ‘running’
Copy the IP address under public DNS (IPv4), this will be form the basis of our URL to host the R shiny app later.
Download putty, after downloading, convert the .pem file into ppk file.
To convert .pem file to ppk, type puttygen in the windows start dialog box. Click on ‘puttygen’. The below box should appear.
Click on File tab and click on load ‘private key’.
Navigate to folder or path where you have saved the .pem file and select it. The .pem file will be imported and you should see a screen like below.
Now save the key as ‘save private key’, give a name to key and save it in your desired location. Upon saving the key the below icon should appear.
Open putty and in the host name box enter the IP of EC2 instance i.e. one adjacent IPv4 Public IP (54.148.189.55) as shown in Fig a
Next navigate to ‘Auth’ on the left hand side panel and browse for the ppk key that you had saved earlier.
After browsing the ppk key click open. If your key is a valid one you will get a command prompt screen like below. Enter your log in credentials and then press enter
Enter the IP address of EC2 instance in the host name box. The click on ‘Advanced’.
Navigate to the left hand side panel and under SSH click on ‘Authentication’, enter the private key as shown below
After entering the private key click ok. You will get the below screen. Click on ‘Login’.
You will get a dialog box like below. Just click ‘Yes’.
The final result will be the screen below
The first prerequisite to run R shiny app is to install r base, shiny server, shiny package and associated packages.
To install the above, the first step is to go to the root and install them. The reason being if you are logged in as non root user in Ec2, you will have your own library path and probably the R packages, r base, shiny server may not get installed system wide. To install it system wide, go to root and install the above
Steps to go to root:
In the prompt type the below
sudo –i
You should then get a # symbol like below
Now run the following commands
sudo apt-get update
sudo apt-get install r-base
sudo apt-get install r-base-dev
The below command installs R shiny package
sudo su — -c “R -e \”install.packages(‘shiny’, repos = ‘http://cran.rstudio.com/')\""""
The below command installs shiny server
wget https://download3.rstudio.org/ubuntu-12.04/x86_64/shiny-server-1.4.4.807-amd64.deb
sudo dpkg -i shiny-server-1.4.4.807-amd64.deb
After execution of above steps a directory(folder) by the name ‘shiny-server’ would have been created in the path /srv/shiny-server/
The next step is to create a folder inside the directory shiny-server where we can place our R shiny app components (UI.R, Server. R, configuration file, R workspace, data files or R programs).
At first we may not be able to create folder inside the shiny-server folder, to do this execute the below commands first
sudo chmod 777 /srv/shiny-server
sudo mkdir /srv/shiny-server/myapp
In the above command I have created a folder ‘myapp’ to place all the R shiny app components.
Now copy the R shiny app components from your local machine to Ec2 instance under the following path /srv/shiny-server/myapp/
One important thing to be taken into consideration is to configure the shiny-server.conf
The shiny-server.conf is available in the location /etc/shiny-server/
Again you may not be able to access the shiny-server directory under /etc/.
Hence run the below command
sudo chmod 777 /etc/shiny-server
After executing the above command you can copy the configuration file into local system, edit it and then transfer back the edited configuration file in the location /etc/shiny-server.
The edition to be made is as follows; please note the words after # are comments.
The result below shows shiny components copied in the path /srv/shiny-server/myapp
Now the final step. In the amazon console, go to your running EC2 instance. Copy the Public DNS (Ipv4) e.g. : ec2–34–215–115–68.us-west-2.compute.amazonaws.com.
Copy this in the browser and suffix: 3838/myapp like below and press enter.
Your R shiny app is hosted successfully!!!!
Important Notes to consider:
· It is important to copy the R workspace as it will contain the R objects and data files in the folder created i.e. myapp. In the above example we are using a simple shiny app ‘a hello world equivalent’ hence we do not have any R workspace or data files.
· Sometimes the app may not load for some idiopathic reasons or the screen could get ‘greyed’ out. Please refresh and try again.
Sources:
Running R on AWS
Hosting shiny on amazon EC2
Shiny server troubleshoots
If you liked my article, give it a few claps.
You can reach out to me on Linkedin
Data scientist/Statistician with business acumen. Hoping to amass knowledge and share it throughout my life. Rafa Nadal Fan, Love to read non fiction books.
See all (292)
420 
10
Every Thursday, the Variable delivers the very best of Towards Data Science: from hands-on tutorials and cutting-edge research to original features you don't want to miss. Take a look.
420 claps
420 
10
Your home for data science. A Medium publication sharing concepts, ideas and codes.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/byteridge/building-a-voice-enabled-chat-bot-for-a-website-using-dialogflow-firebase-jquery-3a10a3a36e2?source=search_post---------307,"There are currently no responses for this story.
Be the first to respond.
The aim of this article is to build the following chat dialog with said technology stack.
At a high level, our chatbot will be addressing users who want to register online for a programming course. The ideal conversation would look like below.
User : Hi
Agent : I can help you with registering to our programming language courses, let’s start by knowing your email and phone numberUser : Email is test@test.com and phone is 9999999999
Agent : Thanks for sharing the details, Which programming language course do you wish to join?User : Java/.Net/Python
If course available,Agent : Sure, would you like to register for a Java class?User : Yes
Agent(yes) : Great, registration complete. You will get a call back. Thanks.Agent(no) : No problem, do you want to try another course?
If the course is not available,Agent : Oops, we don’t offer this course at this time, let us know if you want to try some other course!User : Java/.Net/Python
We will start by logging into Dialogflow console using Gmail account @ www.dialogflow.com
After logging in, navigate to the top left side panel to create a new Agent
Give it a name, say course_registration_bot and click on create. Within a few moments, you will have an empty bot created.! And it will come with a couple of default intents ‘welcome’ and ‘fallback’.
The moment you click CREATE button your page will look as below,
Default Fallback Intent will be matched only when no other intent is matched
Default Welcome Intent is matched when a user gives any greetings message say hi, hello etc.,
In our case when a user greets, the agent needs to respond saying
I can help you with registering to our programming language courses, let's start by knowing your email and phone number
So we have to edit the responses section under Default Welcome Intent. Remove all the default responses and add the response as stated above. And then click the SAVE button.
The user is now expected to give their email and phone, the agent has to make sure that it will always pick both parameters otherwise prompt the user until the user shares these details. We can use the slot filling technique to achieve this.
Create a new intent which can be matched when a user shares email, phone. Let's name it as ‘take_user_details’
Now we will add training phrases to match to this intent when a user shares email, phone details.
The moment a training phrase is added Dialogflow detects the possible extractable values - in our case email, phone and assigns to them a default system entity @sys.email, @sys.phone-number. In case these are not automatically detected we can just select the part of the text we wanted to extract and assign our desired entity type. Then SAVE.
Since we wanted our agent to make sure to collect both email, phone details as required fields we should define it under ‘Action and parameters’ section. Select the check boxes under the REQUIRED column and define the prompts that an agent will use to ask the user when either of the parameters is not yet captured.
You would want this user data to be saved in a database for further communication, so we have to enable fulfillment on this intent and then create a cloud function which can perform this work.
After the intent captures both the email and phone details, instead of setting the response in Dialogflow Responses section, we will let fulfillment do it. Below is what we will set as a response,
Thanks for sharing the details, Which programming language course do you wish to join?
After enabling fulfillment for this intent, navigate to Fulfillment menu in the left panel. Webhook is used to point to your existing API. But for this demo let’s use Inline Editor as we have very limited functionality requirement.
There is some template code in place to help us get started, we will just say DEPLOY. This will take a moment to complete as its creating an application in Firebase and deploying our code as a cloud function. Clicking on ‘View execution logs in the Firebase console’ link will navigate to the newly created app in Firebase.
Under the same Firebase console, navigate to Database menu in the left panel and create a new Realtime Database where we will save user details using Dialogflow Fulfillment code.
Chose test mode for demo purposes. And then navigate to Project settings under Project Overview in the left menu to figure out the connection string for this database.
Click on </> symbol (web), to get the connection details. Copy the entire config object and replace in the Fulfillment code.
Fulfillment code — Saving user details to Realtime Database,
The moment user details are picked, we can see the data pushed to Realtime Database.
While all this is happening under the hood, the user will be responding with their preferred course.
We need another intent that can match when a user mentions his preference of programming language course. Let's call it ‘take_course_preference’, then we shall train the intent to match for messages with programming languages.
Since Dialogflow doesn’t have a default entity viz., email or phone which can identify a programming language, we have to create one of our own. So we will create a new entity called ‘ProgrammingLanguages’
Now we can go ahead and create ‘take_course_preference’ intent along with training phrases those will match to a programming language.
We will have to check if we have the training course user requested in our database and then respond accordingly.
Hence we have to enable fulfillment for this intent also and then create and map to a respective function inside the fulfillment code which can check and respond.
We will also have to fill our Database manually with some sample data as shown below,
All good except that if a user gives a programming language that is not matched to ‘take_course_preference’ intent. If this happens then Default Fallback intent is matched and our fulfillment is not executed. An easy way to handle this use case is to give maximum training to ProgrammingLanguage entity with all the possible values.
Considering ‘take_course_preference’ intent is matched but that course is not available in our database, fulfillment code will respond by saying,
Oops, we don’t offer this course at this time, let us know if you want to try some other course!
And then when the user gives another preference again ‘take_course_preference’ intent is matched and responds accordingly.
If the fulfillment code finds a course is available then it will respond by saying,
Sure, would you like to register for a Java class?
The user may say Yes or No, we need to create follow-up intents for both cases.
For Yes fallback, the response section should have,
Great, registration complete. You will get a call back. Thanks.
For No fallback, the response section should have,
No problem, do you want to try another course?
That’s it..! We have completed all our backend setup.
There are a couple of ways to integrate this setup and give it a UI,
2. Creating a customized web client setup (chat widget), typically by means of HTML and JavaScript frameworks like jQuery or Angular.
3. We can also easily integrate this into a mobile app!
You can make use of the jQuery client-side code which I have published to GitHub (refer to the end of the article), make sure to change the ‘Developer access token’ value and point to your agent.
That’s all, Hope it’s a worthy read, Thank you!
P.S. Make sure to NOT use capital letters while naming ‘context’, In such cases inside fulfillment code agent.getContext(‘context-name’) will return null.
Chat widget source code, Dialogflow agent backup, and Realtime Database backup is available at GitHub here
I have also presented a webinar on this topic, recording available here
Crafting Ideas To Life - Technology company with proven…
544 
1
544 claps
544 
1
Written by
Lead Software Engineer
Crafting Ideas To Life - Technology company with proven track record of partnering with   enterprises and startups
Written by
Lead Software Engineer
Crafting Ideas To Life - Technology company with proven track record of partnering with   enterprises and startups
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@cloud9consultive/how-a-fat-ugly-looking-50-year-old-dude-pulls-gorgeous-hot-10s-on-the-regular-b2c39c6bfe57?source=search_post---------308,"Sign in
There are currently no responses for this story.
Be the first to respond.
Cloud 9 Consultive
Nov 4, 2021·4 min read
I’m not an attractive guy but every time I leave the house, at least one woman will either come up to me and start a conversation or give me a strong I.O.I. (indicator of interest) letting me know she wants me to approach her.
By the way, I’m in my 50’s and these ladies are typically in their 20’s.
Here’s what I did to start attracting women…
I created a vibe of swag and confidence women can’t resist.
As you will see, this is easy to do. It’s a simple 2 step process.
First, I dropped my Backpack. Then I started practicing Off Time.
Let me explain…
We all have what I call a “Backpack”. It’s the place we store every negative, hurtful experience we have ever had in our entire life.
The stuff in your Backpack is the root cause of every negative emotion you experience… Including fear, anxiety, worry, hate, anger, jealousy, and sadness.
Some people have a small Backpack. I had one of those super huge “I’m going into the wilderness for a month like a crazy person” Backpacks.
My parents were physically and verbally abusive. My Backpack was huge.
So even if you have a GIGANTIC backpack, you can still let go of all that hurt and be free.
Here’s how to drop your Backpack:
(1) Make a list of all the negative emotions you experience.
(2) Schedule alone time and turn off your phone. You will not want to be disturbed.
(3) Select the first emotion you would like to release and think of something that causes you to feel the emotion.
(4) Allow yourself to experience the emotion as fully as you can.
(5) Notice what the emotion feels like.
(6) Notice where the emotion is located in your body.
(7) Continue to feel the emotion until it completely goes away on its own.
(8) If the emotion transforms into another emotion before going away completely, keep feeling whatever emotion comes up until you don’t feel it anymore.
Negative emotions can’t stand the clear light of awareness.
Giving the emotion your full attention by feeling it completely and noticing where it is in the body will cause the emotion to disappear.
(9) Repeat the process for every negative emotion on your list.
(10) In the future the second you feel a negative emotion use this exercise to remove it.
To learn more check out “The Untethered Soul” published in 2007 by New York Times best-selling author Michael Singer.
Now that you know how to drop your Backpack (release negative emotions), let’s talk about Off Time…
Off Time is the practice of bringing your awareness to the present moment. It’s a form of mindful meditation.
To get a strong body you must exercise. The opposite is true of the mind. To get a strong mind, keep it peaceful and still.
Practice Off Time and you will experience unconditional happiness. True happiness comes from within… Not from anyone or anything outside of yourself.
As if unconditional happiness isn’t enough, here are 3 more reasons to add Off Time to your daily routine:
1) You deserve a break from thinking. According to the National Science Foundation, an average person has about 12,000 to 60,000 thoughts per day. Of those, 80% are negative and 95% are repetitive thoughts.
2) Women will find you more attractive. Science Direct published a research study that concluded women tend to be more attracted to men who are high in mindfulness.
3) Findings reported by the EOC Institute regarding the benefits of mindfulness and meditation confirm what I already know to be true…
If you practice mindfulness meditation regularly, you will experience a sharp increase in joy, confidence, and peace.
Here’s a quick Off Time Technique to get you started…
As often as you can and for as long as you can, sit somewhere comfortably, bring your awareness to the present moment, and listen to the sounds around you.
As you listen, you can identify the loudest and softest sounds or just listen to sounds as they come and go.
Here’s my secret…
I dropped my Backpack so I no longer experience fear, hate, anger, worry, or any other negative emotion.
Then I started practicing Off Time to amplify my positive energy.
I attract women because I literally radiate joy, confidence, or peace all the time.
Drop your Backpack, practice Off Time, and you will do the same!
Now that you know how to attract women, I have a question for you…
I will help you fix it For Free in exchange for answering a few market research questions.
Schedule a free call here
If you want to be more successful with women, you can have results or excuses… not both.
Excuses will ALWAYS be there for you.
This opportunity won’t…
Free sessions are subject to availability and are awarded on a first come — first serve basis.
Because of the intensely personal nature and time commitment, I reserve the right to discontinue this free offer at any time — without notice.
So if you’re interested sign up now
What’s Your Biggest Dating Challenge? I will help you overcome it For Free. Men & women schedule a free call here: https://Cloud9Consultive.com/free
687 
22
687 
687 
22
What’s Your Biggest Dating Challenge? I will help you overcome it For Free. Men & women schedule a free call here: https://Cloud9Consultive.com/free
"
https://medium.com/milkie-way/we-burnt-72k-testing-firebase-cloud-run-and-almost-went-bankrupt-part-1-703bb3052fae?source=search_post---------309,"There are currently no responses for this story.
Be the first to respond.
This is the story of how close we came to shutting down before even launching our first product, how we survived, and the lessons we learnt.
In March, 2020, when COVID hit the world, our startup Milkie Way too was hit with a big blow and we almost shut down.
We burnt $72,000 while exploring and internally testing Cloud Run with Firebase within a few hours.
This is the story of the incident, how we messed up, and what happened afterwards.
If you’re like me and would rather watch a video here it is:
Not one for videos?
Let’s continue!
In early 2020, after having the idea, I started developing Announce https://announce.today. The goal was to create an “MVP”, a functional V1 of the product, and for this reason our code was based on a simple stack. We used JS, Python and deployed our product on Google App engine.
Having a very small team, our focus was on writing code, designing the UI and getting product ready. I spent minimal time in Cloud management, just enough to make us go live, and have basic development flow (cicd) going.
In the V1 web application, user experience was not the smoothest, but we just wanted to make a product that some of our users could experiment with, while we built a better version of Announce. With Covid hitting the world, we thought it was the best time to make a difference as Announce could be used by the governments to make announcements world wide.
Wouldn’t it be cool to have some rich data on the platform even if users don’t create content to begin with? This thought that led to another project, called Announce-AI. Its purpose was to create rich content for Announce automatically. Rich data == events, safety warnings like earthquakes, and possibly local relevant news.
To begin developing Announce-AI, we used Cloud Functions. As our bot scraping the web was fairly young, we believed light weight Cloud functions were the way to go. However, as we decided to scale, we ran into troubles because Cloud Functions have a timeout of ~9 minutes.
At this time we learn about Cloud Run, which then had a big free usage tier! Without understanding it completely, I asked my team to deploy a “test” Announce AI function on Cloud Run, and see it’s performance. The goal was to play around with Cloud Run, so we can learn and explore it really fast.
To keep it simple, as our experiment was for a very small site, we used Firebase for database, as Cloud Run doesn’t have any storage, and deploying on SQL server, or any other DB for a test run would have been an over kill.
I created a new GCP project ANC-AI Dev, set up $7 Cloud Billing budget, kept Firebase Project on the Free (Spark) plan. The worst case we imagined was exceeding the daily free Firestore limits if we faltered.
After some code modifications, we deployed the code, ran it by making few requests in middle of the day manually and then left it.
Everything went fine on the day of test, and we got back to developing Announce. Next day after working, I went for a quick nap in late afternoon. On waking up I read few emails from Google Cloud, all sent within few minutes of each other.
Luckily my card had a spending limit of $100 preset. This led to declining the charges, and Google suspending all our accounts with it.
I jumped out of the bed, logged into Google Cloud Billing, and saw a bill for ~$5,000. Super stressed, and not sure what happened, I clicked around, trying to figure out what was happening. I also started thinking of what may have happened, and how we could “possibly” pay the $5K bill.
The problem was, every minute the bill kept going up.
After 5 minutes, the bill read $15,000, in 20 mins, it said $25,000. I wasn’t sure where it would stop. Perhaps it won’t stop?
After two hours, it settled at a little short of $72,000.
By this time, my team and I were on a call, I was in a state of complete shock and had absolutely no clue about what we would do next. We disabled billing, closed all services.
Because we used same company card across all our GCP Projects, all our accounts and projects were suspended by Google.
This happened on Friday evening, March 27th, 3 days before we had planned V1 of Announce to go live. Our product development was dead as Google suspended all our projects as they were tied to same credit card. My morale was as low as it could be, and the future of our company was unsure.
Once my mind made peace with this new reality, at midnight I sat down to actually investigate what happened. I started writing a document detailing all the investigations… I called this document: “Chapter 11”.
Two of my team members who were in this experiment also stayed up all night investigating and trying to make sense of what had happened.
The next morning on Saturday, March 28th, I called and emailed over a dozen law firms to book an appointment / have a chat with some attorney. All of them were away, but I was able to get response from one of them over email. Because the details of the incident are so complicated even for engineers, explaining this to an attorney in plain english was a challenge of its own.
As a bootstrapped company, there was no way for us to come up with $72K.
By this time, I was well versed with Chapter 7 and Chapter 11 of Bankruptcy and mentally prepared of what could come next.
On the Saturday after sending emails to lawyers, I started reading more and going through every single page in GCP Documentation. We did make mistakes, but it didn’t make sense that Google let us spend $72K without even making a payment on the project before!
We never anticipated this, nor was this ever displayed while signing up for Firebase. Our GCP project had billing connected to have Cloud Run execute, but Firebase was under Free plan (Spark). GCP just out of the blue upgraded it, and charged us for the amount it needed to.
It turns out this is their process as “Firebase and GCP are deeply integrated”.
GCP Billing is actually delayed by at least a day. In most of their documentation Google suggests using Budgets and auto shut-off cloud function. Well guess what, by the time the cut off function would trigger, or the Cloud Users be notified, the damage would’ve probably been done.
Billing takes about a day to be synced, and that’s why we noticed the charges the next day.
As our account had not made any payment thus far, GCP should’ve first made charge for $100 as per billing info, and on non-payment, stopped the services. But it didn’t. I understood the reason later, but it’s still not the user’s fault!
The first billing charge made to our account was ~ $5,000. The next one for $72,000.
Not just Billing, but even Firebase Dashboard took more than 24 hours to update.
As per Firebase Console documentation, the Firebase console dashboard numbers may differ ‘slightly’ from Billing reports.
In our case, it differed by 86,585,365.85 %, or 86 million percentage points. Even when the bill was notified to us, Firebase Console dashboard still said 42,000 read+writes for the month (below the daily limit).
Having been a Googler for ~6.5 years and written dozens of project documents, incident reports, and what not, I knew how to put the case for Google team when they would come back to work in 2 days.
[EDIT: Clearing confusion here. I didn’t have any connections and only communicated with Google via consults / chat and emails. By “putting the case” I mean putting together a doc that had all the details, and highlighting all the loopholes for Google engineers. This doc gave me a breather as we had some grounds to call it foul play.]
Another task was to understand our mistake, and devise our product development strategy. Not everyone on the team knew what was going on, but it was quite clear that we were in some big trouble.
As a Googler I had experienced teams making mistakes costing Google millions of dollars, but the Google culture saves the employees (except engineers have to write a long incident report). This time, there was no Google. Our own limited capital and our hard work, was at complete stake.
This post is already getting long, so I’ll continue the details of how we managed to make this blunder, how we survived, and what did we learn.
See you in Part 2:
medium.com
This post was originally published on our company blog: https://blog.tomilkieway.com.
Making the world a more efficient, safer and connected space.
566 
1
566 claps
566 
1
Written by
Founder of Milkie Way, Inc. https://tomilkieway.com
Milkie Way, Inc. is a technology company with the mission o make the   world a more efficient, safer and connected space.
Written by
Founder of Milkie Way, Inc. https://tomilkieway.com
Milkie Way, Inc. is a technology company with the mission o make the   world a more efficient, safer and connected space.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/pinterest-engineering/optimizing-kafka-for-the-cloud-4e936643fde0?source=search_post---------310,"There are currently no responses for this story.
Be the first to respond.
By Ambud Sharma | Software Engineer, Logging team
One of the fundamental principles when operating in the cloud is to ensure applications safeguard against outages. A common way to achieve this is to spread an application’s deployment across multiple failure domains. In a public cloud environment, Availability Zone (AZ) can serve as a failure domain. We can use multiple AZs to provide fault tolerance for an application.
Distributed systems like HDFS are traditionally made rack aware to improve fault tolerance by spreading replicas across multiple racks within a datacenter. However, using AZs as rack information is a common practice when running in cloud environments. This enables spreading data copies across multiple AZs, thereby providing fault tolerance in case one fails. While replicating data across AZs provides fault tolerance, it does come at a premium in form of AZ transfer costs.
At Pinterest, we use Kafka extensively as a scalable, fault tolerant distributed message bus to power several key services like user action counting and change data capture (CDC). Since we have Kafka running at a very large scale, we need to be mindful of AZ transfer costs and run as efficiently as possible, and so we focused on reducing the amount of data transferred across AZ.
When a Kafka cluster has brokers spread across multiple AZs, it results in three types of cross AZ network traffic:
Out of the above traffic types we need 1 for fault tolerance. However, 2 and 3 are unwanted side effects that cause additional cost which, in theory, can be eliminated.
There are two potential ways to solve this problem.
Approach 1
We can make our Producers and Consumers write/read data only for partitions whose leaders share the same AZ to make them more cost efficient.
Approach 2
Alternatively we could deploy AZ specific Kafka clusters, but in order to achieve this any other real-time consumers would need to make their business logic AZ aware.
In the interest of simplicity, we chose to go with Approach 1 since it minimized code and stack changes. Producer / Consumer AZ awareness can be achieved by looking up the rack information for the leader broker of the partition we are trying to read/write to, and change the partitioning logic for producers and assignments for consumer.
In Kafka, the broker’s rack information is part of the PartitionInfo metadata object that is shared with Kafka clients (both consumers and producers). Therefore, we deployed rack awareness to our Kafka clusters, where each broker publishes the AZ it’s in as node rack info.
We started this initiative with our biggest producer and consumer applications for Kafka, logging agent and S3 transporter.
Our logging agent is responsible for reading data from log files and shipping them to Kafka in microbatches. The agent also lets users configure how logs are partitioned across a topic’s partitions.
One key design of our logging agent is the ability to pre-partition the data before calling Kafka’s producer.send(). This allows us to add more advanced routing. To make it AZ aware, we added ability for the logging agent to look up AZ info for the node it’s running on using the EC2 Metadata API. Next, we enhanced our partitioner to leverage rack information in Kafka’s producer metadata to limit writes to only partitions for which leaders are in the same AZ as the logging agent. (This change was only applicable to topics that didn’t use key based partitioning since ordering couldn’t be guaranteed after AZ awareness change, as a partition switches AZ.)
S3 transporter is responsible for reading logs from Kafka and persisting them to S3 for further processing. We tried something similar to the producer for S3 transporter.
Our S3 transporter doesn’t use Kafka consumer assignments. Rather, it uses its own partition assignment system. This allows us to preserve locality in case of node restarts or has temporary network isolation, thus reducing the amount of data that needs to be replayed for a given batch.
We make each S3 transporter worker look up and publish its AZ info to Zookeeper, which helps the S3 transporter master assign Kafka partitions to the workers based on their rack (AZ). If we are unable to look up the (rack) AZ information of a partition, we degrade to assignment across all available workers.
We rolled out AZ aware S3 transporter to production, which resulted in more than 25% savings in AZ transfer cost for Logging. We’re currently in the process of slowly rolling out AZ aware logging agent to further reduce our AZ transfer costs.
We’re also working to extend this design to standard Kafka Producers and Consumers to help us extend our savings to other applications as well, which may include KIP-392 once it’s implemented.
Acknowledgements: Huge thanks to Eric Lopez, Henry Cai, Heng Zhang, Ping-Min Lin, Vahid Hashemian, Yi Yin, Yinian Qi and Yu Yang who helped with improving Logging at Pinterest. Also, thanks to Kalim Moghul for helping us with this effort.
Inventive engineers building the first visual discovery engine, 200 billion ideas and counting.
297 
2
297 claps
297 
2
Written by
https://medium.com/pinterest-engineering | Inventive engineers building the first visual discovery engine https://careers.pinterest.com/
Inventive engineers building the first visual discovery engine, 200 billion ideas and counting.
Written by
https://medium.com/pinterest-engineering | Inventive engineers building the first visual discovery engine https://careers.pinterest.com/
Inventive engineers building the first visual discovery engine, 200 billion ideas and counting.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@selvaganesh93/firebase-cloud-messaging-important-rest-apis-be79260022b5?source=search_post---------311,"Sign in
There are currently no responses for this story.
Be the first to respond.
Selvaganesh
Jan 14, 2018·5 min read
Firebase Cloud Messaging (FCM) is a cross-platform messaging solution that lets you reliably deliver messages at no cost.
For use cases such as instant messaging, a message can transfer a payload of up to 4KB to a client app.
PS: FCM is free. The limit is not mentioned anywhere in the docs.
We will cover 5 important API’s
Postman Collection Linkhttps://github.com/ganny26/firebase-notification-postman-collection
https://fcm.googleapis.com/fcm/send (POST)
Use below API and pass valid API_KEY in authorization field.
Request Body
Request body should consist of “data” and “to” below format i use frequently. there is no limit in adding attributes to data json pass here and control like however you want.
it is possible to send nested objects also like below i have attached “meta”
Response
https://iid.googleapis.com/iid/info/<VALID_REGISTRATION_ID> (GET)
Response:
With device group messaging, you can send a single message to multiple instances of an app running on devices belonging to a group. Typically, “group” refers a set of different devices that belong to a single user. All devices in a group share a common notification key, which is the token that FCM uses to fan out messages to all devices in the group.
PS: The maximum number of members allowed for a notification key is 20.
https://android.googleapis.com/gcm/notification(POST)
Headers:
Request Body
To create a group you can pass array of FCM registered token keys on “registration_ids” attribute
The notification_key_name is a name or identifier (e.g., it can be a username) that is unique to a given group. Thenotification_key_name and notification_key are unique to a group of registration tokens. It is important that notification_key_name is unique per client app if you have multiple client apps for the same sender ID. This ensures that messages only go to the intended target app.
Response
A successful request returns a notification_key like the following:
To add or remove devices from an existing group, send a POST request with the operation parameter set to add or remove, and provide the registration tokens for addition or removal.
Request:
Response:
Based on the publish/subscribe model, FCM topic messaging allows you to send a message to multiple devices that have opted in to a particular topic. You compose topic messages as needed, and FCM handles routing and delivering the message reliably to the right devices.
For example, users of a local weather forecasting app could opt in to a “severe weather alerts” topic and receive notifications of storms threatening specified areas. Users of a sports app could subscribe to automatic updates in live game scores for their favorite teams.
Some things to keep in mind about topics:
https://iid.googleapis.com/iid/v1:batchAdd(POST)
Headers
Note: topic must be prefixed with “topic/<any name>”
https://developers.google.com/instance-id/reference/server
Once topic create send notification using topic name
“to”: “/topics/test2”
To identify how many topics assigned to particular use below endpoint
https://iid.googleapis.com/iid/info/<fcm_token>?details=true(POST)
Response
https://iid.googleapis.com/iid/v1:batchRemove(POST)
Request
Response
https://iid.googleapis.com/iid/v1:batchImport(POST)
batchImport method, you can bulk import existing iOS APNs tokens to Google Cloud Messaging or Firebase Cloud Messaging, mapping them to valid registration tokens. Call the Instance ID service at this endpoint, providing a list of APNs tokens in the JSON body
Request:
Response:
I try to learn and help others by sharing what I find | https://www.youtube.com/user/ganeshrsg | https://github.com/ganny26
687 
14
687 
687 
14
I try to learn and help others by sharing what I find | https://www.youtube.com/user/ganeshrsg | https://github.com/ganny26
"
https://medium.com/@Electronic_Arts/announcing-project-atlas-a-vision-for-a-cloud-native-gaming-future-d58ef77f74db?source=search_post---------312,"Sign in
There are currently no responses for this story.
Be the first to respond.
Electronic Arts
Oct 30, 2018·15 min read
By Ken Moss, Chief Technology Officer at Electronic Arts
I think I have the best job in the world. Growing up, I spent my spare time and spare change at my neighborhood arcade. That’s also where I found a great group of friends. At 13, I programmed my first video game. At 15, I sold my first game on a cassette tape on a Commodore 64. Four years ago, I joined Electronic Arts as the CTO, and two years ago, I took over leadership of our Frostbite engine. My job is to create the technology that powers the games that…
"
https://medium.com/google-cloud/serverless-continuous-integration-and-ota-update-flow-using-google-cloud-build-and-arduino-d5e1cda504bf?source=search_post---------313,"There are currently no responses for this story.
Be the first to respond.
Adding Over The Air (OTA) updates is an important factor for IoT applications to succeed. It’s a mechanism to ensure that devices are always up to date with new settings, security fixes and also adding new features to the hardware, making the customer who brought the device happy with the hardware improvements and at the same time feeling safer.
There are two important parts on an OTA architecture:
Here I’ll show how to setup an initial OTA mechanism using some Google Cloud tools, deploying the updates to ESP8266 and ESP32 board using the Arduino platform.
PlatformIO will be used for building the images, as it has a set of command line tools that enables us to automate the process of generating binary images for the devices. On Google Cloud we are going to use Google Cloud Build, that is a managed Continuous Integration environment, Google Cloud Storage for storing the binary images on the cloud and Cloud Functions to handle HTTP requests querying for current firmware versions and managing them.
PlatformIO is a set of cross-platform tools for developing for embedded devices. It supports a lot of different platforms and frameworks for IoT development and also a huge set of libraries made by the community that can be easily used on your project.
I recommend installing the Visual Studio Code (VSCode) IDE and the PlatformIO plugin to get started using it. Just follow the step on the link below:
platformio.org
The code for this project is available on the following Github link. Clone or download the project the code and open it on the IDE.
github.com
The platformio.inifile contains all the configuration to build the project on the ESP32 and ESP8266 boards. Also, the project dependencies are listed here. An important configuration is the build flag VERSION, that is compiled on the project code to mark which version the device is running currently. So, every time that we create a new firmware version, this code should be bumped so the device will be able to properly check if it needs to download a new version.
The device code makes an HTTP query to the backend, sending the current version, to check if it should download a new one. Also, there is a device internal HTTP handler to display the current version. To handle Wifi connectivity, the project uses the WifiManager library, that creates an access point to setup WiFi on the device.
To deploy to the board you can use the “Build” and “Upload” buttons on PlatformIO Toolbar:
To get started with Google Cloud you can do all on the Cloud Console web interface, but the command line tools is a more powerful tool and we’ll need later to deploy the cloud function. To use the gcloud command line tools, follow the instructions here to download and install it.
console.cloud.google.com
cloud.google.com
Also after this you should authenticate and create a project to use in this tutorial, exchange YOUR_PROJECT_NAMEwith a name that you want for this project:
Now let’s create the Cloud Build configuration and also the Bucket to store the binaries. Follow the steps:
Cloud Build Setup :
Cloud Storage Setup :
Our repository contains a cloudbuild.yaml file, that contains all the configuration to build the firmware and push to Cloud Storage. Cloud Build uses Docker for building artifacts, so I used an image that contains all the PlatformIO tools for building embedded projects using our platformio.ini file.
Now, every time that you push a new tag to your repository, it will trigger a build on Cloud Build. You can create the tag on the UI of your git provider or you can do this using the following git commands.
And if everything is working correctly you should start seeing some builds on the History tag on Cloud Build page when you push a new tag. We’ll revisit this at the end of the post to see how to push new versions.
To control the OTA process, we basically need two things: Store firmware metadata in a database, so we can query later for the latest version and a way to check if given the current device version, check if there is the need to update the device. For achieving this I built two cloud functions:
Now you will need the gcloudtool that we installed in the beginning to deploy the functions. Alter the project ID on the file deploy-prod.sh and run it to deploy both functions. On the first time, it will probably ask to enable the cloud functions API. Just confirm that and continue with the process.
With this command, all the functions are deploying and reacting to events in our architecture.
To push and build a new version that can be download by the devices is simple, with just a couple of git commands we can trigger a new Continuous Integration build. So, let’s say that you add a new feature to the device, like a blink on our loop function.
Now to create a new release, change the version on platformio.inifile, from v1.1.0 to v1.2.0 for example, commit the changed files, tag the new commit and push all to the repository. Here are the commands:
This will trigger all of our processes and when it’s all done, you can reset your device with an older version and see in the logs that it will download the new version and reset itself. If everything is right, you should start seeing you a LED blinking.
I hope that this tutorial gave you an overview of what can be done to automate this process for achieving better IoT deployments. We went though a lot of pieces involved in an OTA deployment process, but of course, is too simple yet. We can improve by adding much more features, like :
References :
Google Cloud community articles and blogs
505 
14
505 claps
505 
14
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Product Engineer @Leverege & Google Developer Expert for IoT. When I'm not cooking, I'm building fun stuff or helping my local dev community. GDG organizer.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/google-cloud-pub-sub-ordered-delivery-1e4181f60bc8?source=search_post---------314,"There are currently no responses for this story.
Be the first to respond.
The Google Cloud Pub/Sub team is happy to announce that ordered delivery is now generally available. This new feature allows subscribers to receive messages in the order they were published without sacrificing scale. This article discusses the details of how the feature works and talks about some common gotchas when trying to process messages in order in distributed systems.
Ordering in Cloud Pub/Sub consists of two properties. The first is the ordering key set on a message when publishing. This string — which can be up to 1KB — represents the entity for which messages should be ordered. For example, it could be a user ID or the primary key of a row in a database. The second property is the enable_message_ordering property on a subscription. When this property is true, subscribers receive messages for an ordering key in the order in which they were received by the service.
These two properties allow publishers and subscribers to decide independently if messages are ordered. If the publisher does not specify ordering keys with messages or the subscriber does not enable ordered delivery, then message delivery is not in order and behaves just like Cloud Pub/Sub without the ordered delivery feature. Not all subscriptions on a topic need to have the same setting for enable_message_ordering. Therefore, different use cases that receive the same messages can determine if they need ordered delivery without impacting each other.
The number of ordering keys is limited only by what can be represented by the 1KB string. The publish throughput on each ordering key is limited to 1MB/s. The throughput across all ordering keys on a topic is limited to the quota available in a publish region. This limit can be increased to many GBs/s.
All the Cloud Pub/Sub client libraries have rich support for ordered delivery. They are the best way to take advantage of this feature, as they take care of a lot of the details necessary to ensure that messages are processed in order. Ordered delivery works with all three types of subscribers: streaming pull, pull, and push.
Ordered delivery has three main properties:
Let’s examine what these properties mean with an example. Imagine we have two ordering keys, A and B. For key A, we publish the messages 1, 2, and 3, in that order. For key B, we publish the messages 4, 5, and 6, in that order. With the ordering property, we guarantee that 1 is delivered before 2 and 2 is delivered before 3. We also guarantee that 4 is delivered before 5, which is delivered before 6. Note that there are no guarantees about the order of messages across different ordering keys. For example, message 1 could arrive before or after message 4.
The second property explains what happens when messages are redelivered. In general, Cloud Pub/Sub offers at-least-once delivery. That means messages may be sent to subscribers multiple times, even if those messages have been acknowledged. With the consistent redelivery guarantee, when a message is redelivered, the entire sequence of subsequent messages for the same ordering key that were received after the redelivered message will also be redelivered. In the above example, imagine a subscriber receives messages 1, 2, and 3. If message 2 is redelivered (because the ack deadline expired or because the best-effort ack was not persisted in Cloud Pub/Sub), then message 3 is guaranteed to be redelivered as well.
The last property defines where messages for the same ordering key are delivered. It applies only to streaming pull subscribers, since they are the only ones that have a long-standing connection that can be used for affinity. This property has two parts. First, when messages are outstanding to a streaming pull subscriber — meaning the ack deadline has not yet passed and the messages have not been acknowledged — then if there are more messages to deliver for the ordering key, they go to that same subscriber.
The second part pertains to what happens when no messages are outstanding. Ideally, one wants the same subscribers to handle all of the messages for an ordering key. Cloud Pub/Sub tries to do this, but there are cases where it cannot guarantee that it will continue to deliver messages to the same subscriber. In other words, the affinity of a key could change over time. Usually this is done for load-balancing purposes. For example, if there is only one subscriber, all messages must be delivered to it. If another subscriber starts, one would generally want it to start to receive half of the load. Therefore, the affinity of some of the ordering keys must move from the first subscriber to this new subscriber. Cloud Pub/Sub waits until there are no more messages outstanding on an ordering key before changing the affinity of the key.
One of the most difficult problems with ordered delivery is doing it at scale. It usually requires an understanding of the scaling characteristics of the topic in advance. When a topic extends beyond that scale, maintaining order becomes extremely difficult. Cloud Pub/Sub’s ordered delivery is designed to scale with usage without the user having to think about it.
The most common way to do ordering at scale is with partitions. A topic can be made up of many partitions, where each stores a subset of the messages published to the topic. When a message gets published, a partition is chosen for that message, either explicitly or by hashing the message’s key or value to a partition. The “key” in this case is what Cloud Pub/Sub calls the ordering key.
Subscribers connect to one or more partitions and receive messages from those partitions. Much like the publish side, subscribers can choose partitions explicitly or rely on the messaging service to assign subscribers to partitions. Partition-based messaging services guarantee that messages within the same partition are delivered in order.
A typical partition setup would look like this:
The green boxes represent the partitions that store messages. They would be owned by the messaging servers (often called “brokers”), but we have omitted those servers for simplicity. The circles represent messages, with the color indicating the message key and the number indicating the relative order for the messages of that color.
One usually has a lot fewer partitions than there are keys. In the example above, there are four message colors but only three partitions, so the second partition contains both blue and red messages. There are two subscribers, one that consumes from the first partition and one that consumes from the second and third partitions.
There are three major issues a user may have to deal with when using partitions: subscriber scaling limitations, hot shards, and head-of-line blocking. Let’s look at each in detail.
Within a set of subscribers across which delivery of messages is load balanced (often called a “consumer group”), only one subscriber can be assigned to a partition at any time. Therefore, the maximum amount of parallel processing that can occur is min(# of partitions, # of subscribers). In the example above, we could load balance across no more than three subscribers:
If processing messages suddenly became more expensive or — more likely — a new consumer group was added to receive messages in a new pipeline that requires longer processing of the messages, it may not be possible to gain enough parallelism to process all the published messages. One solution would be to have a subscriber whose job is to republish the messages on a topic with more shards, which the original subscribers could consume instead:
The downside is that now both of these topics must be maintained or a careful migration must be done to change the original publisher to publish to the new topic. If both topics are maintained, then messages are stored twice. It might be possible to delete the messages from the first topic once they are published to the second topic, but this would require the migration of any subscribers receiving messages from the original topic to the new topic.
The next issue is a hot shard — the overloading of a single partition. Ideally, traffic patterns across partitions are relatively similar. However, it is possible that there are a lot more messages or much larger messages hashing to one partition in comparison to messages hashing to other partitions. As a result, a single partition can become overloaded:
What can be done to deal with this hot shard? Typically, the solution is to add partitions. However, maintaining order during a repartitioning can be very difficult. For example, if we add a new partition in the case above, it could result in related messages going to completely different partitions:
With this new set of partitions, purple messages now publish to the first partition, blue messages to the third partition, and yellow and red messages to the fourth partition. This repartitioning causes several problems. First of all, the fourth partition now contains messages for keys that were previously split among the two subscribers. That means the affinity of keys to subscribers must change.
Even more difficult is the fact that if subscribers want all messages in order, they must carefully coordinate from which partitions they receive messages when. The subscribers would have to be aware of the last offset in each partition that was for a message before adding more partitions. Then, they need to consume messages up to those offsets. After they have processed messages up to the offsets in all the partitions, then the subscribers can start to consume messages beyond that last offset.
The last difficult issue is head-of-line blocking, or the inability to process messages due to the slow processing of messages that must be consumed first. Let’s go back to the original scenario:
Imagine that the red messages require a lot more time to process than the blue ones. When reading messages from the second partition, the processing of the blue message 2 could be needlessly delayed due to the slow processing of red message 1. Since the unit of ordering is a partition, there is no way to process the blue messages without processing the red messages. One could try to solve this by repartitioning in the hopes that the red and blue messages end up in different partitions. However, the processing of the red messages will block the processing of others in whichever partition they end up. The repartitioning also results in the same issues discussed in the Hot Shards section.
Alternatively, the publisher could explicitly assign the red messages to their own partition, but it breaks the decoupling of publishers and subscribers if the publisher has to make decisions based on the way subscribers process messages. It may also be that the extra processing time for the red messages is temporary and doesn’t warrant large-scale changes to the system. The user has to decide if the delayed processing of some messages or the arduous process of changing the partitions is better.
Cloud Pub/Sub’s ordered delivery implementation is designed so users do not need to be subject to such limitations. It can scale to billions of keys without subscriber scaling limitations, hot shards or head-of-line blocking. As one may expect with a high-throughput pub/sub system, messages are split up into underlying partitions in Cloud Pub/Sub. However, there are two main properties of the service that allow it to overcome the issues commonly associated with ordered delivery:
By taking advantage of these properties, Cloud Pub/Sub brokers have three useful behaviors:
These behaviors allow Cloud Pub/Sub to avoid all three major issues with ordered delivery at scale!
Ordered delivery doesn’t come for free, of course. Compared with unordered delivery, the ordered delivery of messages may slightly decrease publish availability and increase end-to-end message delivery latency. Unlike the unordered case, where delivery can fail over to any broker without any delay, failover in the ordered case requires coordination across brokers to ensure the messages are written to and read from the correct partitions.
Even with Cloud Pub/Sub’s ability to deliver messages in order at scale, there are still subtleties that exist when relying on ordered delivery. This section details the things to keep in mind when building an ordered pipeline. Some of these things apply when using other messaging systems with ordered delivery, too. In order to provide a good example of how to use ordering keys effectively, the Cloud Pub/Sub team has released an open-source version of its ordering keys prober. This prober is almost identical to the one run by the team continuously to verify the correct behavior of this new feature.
On the surface, publishing in order seems like it should be very easy: Just call publish for each message. If we could guarantee that publishes never fail, then it would be that simple. However, transient or permanent failures can happen with publish at any time, and a publisher must understand the implications of those failures.
Let’s take the simple example of trying to publish three messages for the same ordering keys A: 1, 2, and 3. The Java code to publish these messages could be the following:
If there were no failures, then each publish call would succeed and the message ID would be returned in the future. We’d expect the subscriber to receive messages 1, 2, and 3 in that order. However, there are a lot of things that could happen. If a publish fails, it likely needs to be attempted again. The Cloud Pub/Sub client library internally retries requests on retriable errors. Errors such as deadline exceeded do not indicate whether or not the publish actually succeeded. It is possible that the publish did succeed, but the publish response wasn’t received by the client in time for the deadline, in which case the client may have attempted the publish again. In such cases, the sequence of messages could have repeats, e.g., 1, 1, 2, 3. Each published message would have its own message ID, so from the subscriber’s perspective, it would look like four messages were published, with the first two having identical content.
Retrying publish requests is complicated even more by batching. The client library may batch messages together when it sends them to the server for more efficient publishing. This is particularly important for high-throughput topics. In the case above, it could be that messages 1 and 2 are batched together and sent to the server as a single request. If the server fails to return a response in time, the client will retry this batch of two messages. Therefore, it is possible the subscriber could see the sequence of messages 1, 2, 1, 2, 3. If one wants to avoid these batched republishes, it is best to set the batch settings to allow only a single message in each batch.
There is one additional case with publishing that could cause issues. Imagine that in running the above code, the following sequence of events happens:
The result could be that messages 2 and/or 3 are successfully published and sent to subscribers without 1 having been sent, which would result in out-of-order delivery. A simple solution may be to make the calls to publish synchronous:
While this change would guarantee that messages are published in order, it would make it much more difficult to publish at scale, as every publish operation would block a thread. The Cloud Pub/Sub client libraries overcome this problem in two ways. First, if a publish fails and there are other messages for the same ordering key queued up in the library’s message buffer, it fails the publishes for all those messages as well. Secondly, the library immediately fails any subsequent publish calls made for messages with the same ordering key.
How does one get back to a state of being able to publish on an ordering key when this happens? The client library exposes a method, resumePublish(String orderingKey). A publisher should call resumePublish when it has handled the failed publishes, determined what it wants to do, and is ready to publish messages for the ordering key again. The publisher may decide to republish all the failed messages in order, publish a subset of the messages, or publish an entirely new set of messages. No matter how the publisher wants to handle this edge case, the client library provides resumePublish as a means to do so without losing the scaling advantages of asynchronous publishing. Take a look at the ordering key prober’s publish error logic for an example of how to use resumePublish.
All of the above issues deal with publishing from a single publisher. However, there is also the question of how to publish messages for the same ordering key from different publishers. Cloud Pub/Sub allows this and guarantees that for publishes in the same region, the order of messages that subscribers see is consistent with the order in which the publishes were received by the broker. As an example, let’s say that both publishers X and Y publish a message for ordering key A. If X’s message is received by Cloud Pub/Sub before Y’s, then all subscribers will see the messages in that order. However, publishers do not have a way to know in which order the messages were received by the service. If the order of messages across different publishers must be maintained, then the publishers need to use some other mechanism to coordinate their publishes, e.g., some kind of locking service to maintain ownership of an ordering key while publishing.
It is important to remember that ordering guarantees are only for messages published in the same region. Therefore, it is highly recommended that all publishers use regional service endpoints to ensure they publish messages to the same region for the same ordering key. This is particularly important for publishers hosted outside of GCP; if requests are routed to GCP from another place, it is always possible that the routing could change if using the global endpoint, which could disrupt the order of messages.
Subscribers receive messages in the order they were published. What it means to “receive messages in order” varies based on the type of subscriber. Cloud Pub/Sub supports three ways of receiving messages: streaming pull, pull, and push. The client libraries use streaming pull (with the exception of PHP), and we talk about receiving messages via streaming pull in terms of using the client library. No matter what method is used for receiving messages, it is important to remember that Cloud Pub/Sub offers at-least-once delivery. That means subscribers must be resilient to receiving sequences of messages again, as discussed in the Ordering Properties section. Let’s look at what receiving messages in order means for each type of subscriber.
When using the client libraries, one specifies a user callback that should be run whenever a message is received. The client libraries guarantee that for any given ordering key, the callback is run to completion on messages in the correct order. If the messages are acked within that callback, then it means all computation on a message occurs in order. However, if the user callback schedules other asynchronous work on messages, the subscriber must ensure that the asynchronous work is done in order. One option is to add messages to a local work queue that is processed in order.
It is worth noting that because of asynchronous processing in a subscriber like this, ordered delivery in Cloud Pub/Sub does not work with Cloud Dataflow at this time. The nature of Dataflow’s parallelized execution means it does not maintain the order of messages after they are received. Therefore, a user’s pipeline would not be able to rely on messages being delivered in order. To ensure that one does not use Pub/Sub in Dataflow and expect ordered delivery, Dataflow pipelines that use a subscription with ordering keys enabled fail on startup.
For subscribers that use the pull method directly, Cloud Pub/Sub makes two guarantees:
The requirement that only one batch of messages can be outstanding at a time is necessary to maintain ordered delivery. The Cloud Pub/Sub service can’t guarantee the success or latency of the response it sends for a subscriber’s pull request. If a response fails and a subsequent pull request is fulfilled with a response containing subsequent messages for the same ordering key, it is possible those subsequent messages could arrive to the subscriber before the messages in the failed response. It also can’t guarantee that the subsequent pull request comes from the same subscriber.
The restrictions on push are even tighter than those on pull. For a push subscription, Cloud Pub/Sub allows only one message to be outstanding per ordering key at a time. Since each message is sent to a push subscriber via its own request, sending such requests out in parallel would have the same issue as delivering multiple batches of messages for the same ordering key to pull subscribers simultaneously. Therefore, push subscribers may not be a good choice for topics where messages are frequently published with the same ordering key or latency is extremely important, as the restrictions could prevent the subscriber from keeping up with the published messages.
In summary, ordered delivery at scale usually requires one to be very careful with the capacity and setup of their messaging system. When that capacity is exceeded or message processing characteristics change, adding capacity while maintaining order is a time-consuming and difficult process. With the introduction of ordered delivery into Cloud Pub/Sub, users can rely on order in ways they are accustomed to in a system that still automatically scales with their usage.
Google Cloud community articles and blogs
451 
5
451 claps
451 
5
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Technical Lead on Google Cloud Pub/Sub
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/thinking-design/mobile-design-best-practices-abfc8899b9ed?source=search_post---------315,"There are currently no responses for this story.
Be the first to respond.
Good UX is what separates successful apps from unsuccessful ones. Customers are won and lost every day because of good or bad user experience design. The most important thing to keep in mind when designing a mobile app is to make sure it is both useful and intuitive. Obviously, if the app is not useful, it will have no practical value for the user, and no one will have any reason to use it. And even if the app is useful but requires a lot of effort, people won’t bother learning how to use it.
Good design addresses both problems: It has a clear focus on key user goals, and it removes all obstacles from the user’s way by bringing clarity to the interface. In this article, I’ll share seven UX design tips that I think are key for creating really great mobile user experiences.
Reduce the effort users have to put in to get what they want.
Users often have to quickly accomplish one core function in a mobile app: make a payment, check for new messages, etc. Focus on users’ key goals, and remove all obstacles from their way:
Divide complex tasks into smaller activities. These smaller tasks may better meet user goals. Take Lyft. It knows that the goal of its user is to get a ride somewhere. The app does not overwhelm the user with much of information: It automatically detects the user’s location based on geolocation data, and the only thing the user has to do is select a pickup location.
Login walls are pages that ask the user to log in or register in order to proceed. A login wall is commonly shown when an app launches for the first time or when a web page is first accessed. Keep in mind that forcing registration too early can cause more than 85% of users to abandon the product.
In the example below, Soundcloud requires its users to log in to access the app’s content.
Login walls are encountered quite often in store checkouts. Designers of e-commerce websites and apps think that, by logging in, users will be able to take advantage of previously saved account information and thus won’t need to type in information such as their shipping address and credit card number. Even Amazon has this problem — the service doesn’t provide a guest checkout option.
The registration option could probably be safely replaced by a guest checkout option. Gather data slowly as the user moves through the checkout, asking for a password with the incentive of a coupon code after purchase, like Smashing Magazine does.
The term “information overload” was coined by Bertram Gross, professor of political science at Hunter College, in his 1964 work The Managing of Organizations. Gross defined information overload as follows:
Information overload occurs when the amount of input to a system exceeds its processing capacity. Decision makers have fairly limited cognitive processing capacity. Consequently, when information overload occurs, it is likely that a reduction in decision quality will occur.
Information overload is a serious problem, preventing users from making decisions or taking action because they feel they have too much information to consume. There are some simple ways to minimize information overload. One common technique is chunking. A checkout form is a perfect example. A rule of thumb is to display at most five to seven input fields at a time and break down the checkout into pages, progressively disclosing fields as necessary.
Make the content the interface.
Content is what provides value in most apps. Whether it’s a social feed, news updates, a to-do list or something more technical, like a system dashboard, content is why people are there! This is why it’s critical to focus on the content and remove unnecessary elements that do not support user tasks. Given their reduced attention span, users should be guided to the content they’re looking for, quickly. Content should be the interface.
Google Maps is a great example. For its redesign, Google removed all unnecessary panels and buttons, saying that the map is the interface.
Another pattern that can be used as an interface is card-style design. Cards are great way to display actionable content, because they allow content to naturally reveal itself.
An example of a common approach would be Swiss Air’s app. In the fare tables, every row becomes a collapsed accordion, which can be opened if needed.
Flipboard is another good example of cards in an interface. Flipboard is a personalized magazine that aggregates content from news feeds and social media networks, enabling you to discover and share stories.
Use white space to draw attention to important content.
White space (or negative space) is the empty area between and around elements of a design, and it is often neglected. Though some designers consider it a waste of valuable space, it is an essential element of mobile design. Jan Tschichold says:
White space is to be regarded as an active element, not a passive background.
Clutter overloads an interface with too much information. Every added button, image and line of text makes the screen more complicated.
As Antoine de Saint-Exupéry famously said:
“Perfection is achieved when there is nothing left to take away.”
Get rid of anything in a mobile design that isn’t absolutely necessary, because reducing clutter will improve comprehension. A simple rule of thumb is one primary action per screen. Every screen you design for the app should support a single action of real value to the person using it. This makes it easier to learn, easier to use and easier to add to or build on when necessary.
White space largely contributes not only to readability and content prioritization, but also to visual layout. Thus, it can simplify the UI and improve the UX.
4. Navigation Made Simple
Navigation should inspire users to engage and interact with the content you are delivering.
Helping users navigate should be a high priority for every app. Mobile navigation must be discoverable and accessible and must occupy little screen space. However, making navigation accessible on mobile is a challenge due to the limitations of the small screen and the need to prioritize content over chrome.
Take a few general rules of thumb into account when designing a navigation system for a mobile app:
Even designers who are familiar with all of these rules still end up creating menus that are confusing, difficult to manipulate or simply hard to find.
Navigation UI patterns are a shortcut to good usability. Let’s see some examples.
It’s tempting to rely on menu controls in order to simplify a mobile interface — especially on a small screen. But hiding critical parts of an application behind these kinds of menus could impair usage. Hidden navigation drives down engagement, slows down exploration and confuses people.
Exposing menu options in a more visible way increases engagement and satisfaction.
Tab bars and navigation bars are well suited to apps with relatively few navigation options. The pattern is adopted on both iOS and Android. Tabs are great because they display all major navigation options up front, and with one simple tap, the user can instantly go from one view to another. For this type of navigation, I strongly recommend using labels for navigation options. Don’t make navigation a guessing game.
If there are only a couple of destinations, you can use segmented control. As with a tab bar, all options are visible at once and can be accessed with a single tap.
It might sound contradictory to what I said about saving screen space, but full-screen navigation might be a good choice. Basically, it’s a page (usually the home page) that lists all navigation options. Though you won’t be able to display any content, the full-screen navigation pattern is good for simplicity and coherence. Once the user decides where to go, then you can dedicate the entire screen space to the content. This type of navigation works well in task-based websites and apps, where users are squarely focused on accomplishing a very specific task (for example, checking in for a flight or changing the settings on their phone) or where they limit themselves to one branch of the navigation hierarchy during a single session (for example, if they’re intrested in a particular service or product).
This type of navigation is good when you have hierarchical tree navigation — for example, when you have a menu with seven primary options and each option contains layers of subcategories.
If search is a key function of your app, it needs to be in front of people. Don’t hide it. Either display it at the top of the screen, or have a visible reference (a magnifying glass icon) that activates search mode.
Adapt your design to big screens.
With the release of the iPhone 6 and 6 Plus, it’s become clear that screen sizes are going to keep expanding.
Here are three basic ways of how people hold their phone:
According to research by Steven Hoober, 85% of users use their phone with one hand. The following heat map shows the thumb zones for every iPhone display size since 2007. You can see that the bigger the display is, the more of the screen is less easily accessible.
Adapt your design to improve the user experience. Make sure that your app can be easily (and fully) used on a large screen (such as an iPhone 6 or 7) with one hand.
Place the top-level menu, frequently used controls and common action items in the green zone of the screen, which is comfortably reached with one thumb.
Place destructive actions (such as delete and erase) in the hard-to-reach red zone, because you don’t want users to accidentally tap them.
The “Edit” button in Apple’s Mail app is in the hard-to-reach zone.
Don’t make users wait for content.
While an instant response is best, there are times when your app won’t be able to meet the standard guidelines for speed. A slow response could be caused by a bad Internet connection, or an operation could be taking a long time (for example, installation of an update for the operating system). As much as possible, make the app fast and responsive.
Let people know that things are going to take a while by using a progress indicator. That being said, while the intention behind a progress indicator is good, the result can be bad. As Luke Wroblewski mentions, “Progress indicators by definition call attention to the fact that someone needs to wait. It’s like watching the clock tick down — when you do, time seems to go slower.” There is a good alternative to progress indicators: skeleton screens. These containers are essentially a temporarily blank version of the page, into which information is gradually loaded. Rather than show a loading indicator, use a skeleton screen to focus on actual progress and create anticipation for what is to come. This creates a sense that things are happening immediately, as information is incrementally displayed on the screen and people see that the application is acting while they wait.
Keep in mind that perception can be just as important as raw speed. If an app gives users something interesting to look at while waiting, they will pay less attention to the wait itself. Thus, to ensure that people don’t get bored while waiting for something to happen, offer a distraction.
Do things in the background to make imminent actions appear fast. Actions that are packed into background operations have two benefits: They are invisible to the user, and they happen before the user asks for them.
A good example of this technique is the process of uploading pictures in Instagram. The app uploads photos early. As soon as the user chooses a picture to share, the app starts uploading; and by the time the user is ready to press the “Share” button, uploading is complete, and the user can share their picture instantly.
Think twice before sending a message.
Every day, users are bombarded with useless notifications that distract them from their day-to-day activities, and it gets downright annoying. Annoying notifications are the number one reason people uninstall mobile apps (according to 71% of survey respondents).
Below are four principles to remember when crafting user-centric notifications.
The most common mistake you can make when sending push notifications, and the most damaging in the long term, is to send users more notifications than they can handle. Don’t overwhelm users with push messages, or they might end up deleting your app altogether.
When a user starts using your app, they won’t mind getting notifications, as long as the value they get is sufficiently greater than the interruption. Don’t send push notifications just for the sake of “engaging users.” For example, Facebook routinely sends notifications inviting users to connect to randomly suggested people or to “Find more of your friends on Facebook.” This is a poor attempt to direct users back to the app. Also, it interrupts users with irrelevant alerts.
Personalizing content to inspire and delight is critical. Netflix carefully uses viewing data to present recommendations that feel tailor-made.
Tailoring your notifications to users isn’t just about what you say, but about when you say it. Don’t send push notifications at odd hours. An ill-timed notification sent between 12:00 am and 6:00 am risks waking up or disrupting users:
Of course, users could always disable notifications while they’re sleeping, but that’s not a good solution. A real solution would be to send notifications at times that would be most convenient for users, unless it’s critical to inform them of something right away. According to comScore, a good time for push notifications is between 6:00 pm and 10:00 pm. Always push notifications according to the user’s time zone.
The most effective mobile messaging strategy is to use different message types: push notifications, email, in-app notifications and news feed updates. Diversify your messaging — your messages should work together in perfect harmony to create a great user experience.
As with any other design element, the tips shared here are just a start. Make sure to mix and match these ideas with your own for best results. Remember that design isn’t just for designers — it’s for users. Treat your app as a continually evolving project, and use data from analytics and user feedback to constantly improve the experience.
Nick Babich is a developer, tech enthusiast, and UX lover. He has spent the last 10 years working in the software industry with a specialized focus on development. He counts advertising, psychology, and cinema among his myriad interests.
Originally published at blogs.adobe.com.
Learn about Adobe XD, our all-in-one design and prototyping tool:
Stories and insights from the design community.
181 
1
181 claps
181 
1
Stories and insights from the design community.
Written by
New Tools for New Creatives. Get all the latest creative apps plus seamless ways to share and collaborate. All right on your desktop.
Stories and insights from the design community.
"
https://medium.com/@hbmy289/how-to-set-up-a-free-micro-vps-on-google-cloud-platform-bddee893ac09?source=search_post---------316,"Sign in
There are currently no responses for this story.
Be the first to respond.
HBMY 289
Nov 21, 2018·5 min read
While there are a lot of providers that offer smaller VPS you can test for free for a certain time range, there are not many that are permanently free. This article will focus on setting up an f1-micro VPS on Google Cloud Platform that is advertised as “always free”. Google offers a very broad range of possible VPS configurations and in contrast to other providers, the free tier is rather hidden in all the available options. This article will guide you through the setup process step by step.
When testing Google cloud computing for the first time you will currently also get a voucher that allows renting a wide range of non-free VPS configurations up to a total amount of 300$ in the first 12 months.
If you already have a valid account at Google Cloud Platform (either in test phase or upgraded) you can directly go to Set up an f1-micro instance. Otherwise, you first need to get a new test account.
Visit the Google cloud console (https://console.cloud.google.com/getting-started) and press Try For Free.
Fill out all necessary information on the displayed dialog. You will be required to enter valid credit card information. Google will not charge your credit card even if you use up all of the 300$ that is part of the test phase.
After entering all the required information, the free-trial phase is started.
The f1-micro VPS is part of Google’s “always free” products. If the 12 month test period has ended or if you used up all the credit of your voucher, you will not be able to use these free products anymore. In this case, you will have to upgrade your account, enabling Google to charge your credit card. However, the f1-micro VPS will still be free. Be aware that you might be credited in this case if you use more than the free network egress traffic of 1 GB per month (excluding Australia and China).
Although setting up an f1-micro VPS as shown in this article will not generate any cost for you, it could happen that due to some misconfiguration you are actually using a non-free product.
I cannot take responsibility for any cost that might be generated on your account, especially when exceeding the allowed traffic volume of 1 GB per month. Be sure to use the correct configuration and check your balance repeatedly to avoid any unwanted charges!
Visit the compute engine section of the Google Cloud Platform (https://console.cloud.google.com/compute/) and create a new instance.
The next dialog will allow you to define the configuration of your VPS. You can also choose a much more powerful setup, but only the correct F1-micro settings will result in a free VPS.
Choose a meaningful name for your new VPS and select micro (1 shared CPU) as the machine type for the N1 series. Only instances located in certain US regions will be part of the “always free” deal. At the time of writing this included the regions us-central1 (Iowa), us-east1 (South Carolina) and us-west1 (Oregon).
IMPORTANT!
Check the text next to your settings. It has to look like below and show a free usage time that is equivalent to the number of days in the current month times 24 hours (720 hours for November with 30 days).
Only if this text is displayed your VPS instance will be free of charge!
Click on Change in the Boot disk section. Optionally you can change the pre-installed operating system you want to use. More importantly, you can increase the amount of persistent disk space in this dialog. Increase the value to 30 (included in the “always free” package) and leave the dialog using the Select button.
The firewall section allows to automatically add exceptions for port 80 and 443 if you plan to use your VPS for a web server. Firewall rules can also be added and edited later.
Finally, finish the process by hitting Create.
After a few seconds, you will have a running f1-micro instance. You can access it via the SSH drop-down menu.
Use the first option in the menu to open an ssh-shell in a new browser window to access the VPS.
I hope you liked this short article and now can start using your micro-VPS instance for a lot of cool stuff.
HBMY289
607 
4
607 claps
607 
4
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/google-cloud-products-described-in-4-words-or-less-7776af0552cd?source=search_post---------317,"There are currently no responses for this story.
Be the first to respond.
Download PDFs, text, and hi-res PNGs from https://github.com/gregsramblings/google-cloud-4-words
‪Includes Google Cloud, Firebase, Google Maps Platform, G Suite APIs
Welcome Looker!
Also tweeted at https://twitter.com/gregsramblings/status/1248268470308851712
Check my blog for other resources — https://gregsramblings.com
Google Cloud community articles and blogs
227 
3
227 claps
227 
3
Written by
Google Developer Relations — Living in San Francisco https://gregsramblings.com | https://cloud.google.com | http://instagram.com/gregsimages
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Google Developer Relations — Living in San Francisco https://gregsramblings.com | https://cloud.google.com | http://instagram.com/gregsimages
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/super-ventures-blog/ar-and-blockchain-a-match-made-in-the-ar-cloud-7b10c52faddb?source=search_post---------318,"There are currently no responses for this story.
Be the first to respond.
Top highlight
In my introduction to the AR Cloud I argued that in order to reach mass adoption, AR experiences need to persist in the real world across space, time, and devices.
To achieve that, we will need a persistent realtime spatial map of the world that enables sharing and collaboration of AR Experiences among many users.
And according to AR industry insiders, it’s poised to become:
“the most important software infrastructure in computing”
aka: The AR Cloud.
This simplified yet powerful perspective generated renewed interest in the potential of AR beyond entertainment, and dozens of startups, investors and corporations, came out of the woodwork to thank us for “spelling out what we were thinking about and working on” and started using the term in pitches, press announcements and even job descriptions.
Nevertheless, that intro just scratched the surface on the deep impact the AR Cloud will have on the economy. In contrast to the web era where information was organized in interlinked web pages, the AR Cloud enables a new way to organize the word’s information on physical things, places and people themselves. With it, information is intuitively found at its origin, in situ, without the need to search.
That would allow to access the how-to-use of any object, the history of any place, and the background of any person, by just glancing at it. In situ.
Changing how information is organized is poised to profoundly disrupt the Web economy. This post is about that shakeup. A handful of companies on the chart below became giants thanks to the current model. No wonder they are all contenders in the battle for AR Cloud dominance.
The web economy was primarily based on “clicks on links”: calculating the cost for advertisers based on page impressions (CPMs) and clicks on links (CPCs).
Can that model continue to thrive in the spatial economy? Unlikely.
What can replace it?
I like to call it Clicks on Bricks — a punning rhyme that captures a new world where everything is driven by digital interaction with the physical world.
In the early web days (90’s) companies were identified as either a “bricks” company with physical presence, or as a “clicks” company with online shopping only.
The strategy du-jour for retailers has obviously evolved to seek BOTH “clicks” AND “bricks” as exemplified by Amazon.
But the challenge to bring these two discrete worlds together to reduce the friction is still keeping many CEOs up at night.
Why is that so important? How much value can be generated by closing this gap?
When you perform a task in the real world, no matter where you are or what technology you use, you constantly switch your attention between looking at information required to perform that task and focusing on the actual task in front of you.
How many times do you switch attention per day?
How do you quantify the worldwide cost associated with the need for workers to switch attention back and forth between real world tasks and the information essential to perform those tasks? What value can you generate when you remove that friction?
I will go on a limb and throw a number:
Eliminating the attention switch between online content and the real world is worth over 20% of GDP
Well, the value is actually proven
Consider this study by GE and Upskill comparing wind turbine wiring productivity between the old way on the left (shifting attention between task and step-by-step manual) and the Augmented Reality way on the right — which synchronizes the information with the user’s view of the physical world.
The worker on the left constantly switches attention from the manuals that guide him what to do — to the task at hand.
The worker on the right, uses smartglasses and AR overlays (by Upskill Skylight software) so he doesn’t need to switch his attention back and forth. The information he needs is always aligned with the real world, at a glance.
The results here are indisputable: sync’ing the documentation with the real world view improves productivity by 34% for a first time user!
Harvard Business Review covered a study conducted by Boeing that showed improved productivity in wiring harness assembly by 25%. And at GE Healthcare with Upskill software, a warehouse worker receiving a new pick-list order through AR (on the right) completed the task 46% faster compared to using the standard process (on the left), which relies on a paper list and item searches on a work station.
Additional studies from other firms show an average productivity improvement of 32%.
So what’s the upside when you remove that friction for a whole nation? How much value can be generated when the virtual and the physical worlds are synch’ed for…a billion people?
Take the GDP associated with all jobs that require to shift focus back and forth between real-world tasks to digital data (or printed documentation). Now, apply the most conservative estimate — 20% time/cost savings: it adds up to a very large number!
How does it apply to Clicks on Bricks?
What does this all have to do with the consumer and retail experience?
Once the real and virtual are synch’ed with the AR Cloud — you have the prerequisite for “Clicks on Bricks” — a digital economy driven by interaction with the physical world.
Here’s one of the first “Clicks on Bricks” examples — a project built by Metaio for Lego stores way back in 2010.
The kid looks at a physical product and gets crucial information for his buying decision — right on the product: How does this toy looks inside the box? What does it do? How hard is it to assemble?
The marketer in return can track which boxes where looked at, for how long, and can analyze the overall interaction with the toy.
And BTW — this doesn’t require smartglasses or even smartphones — just a camera, computer and screen (magic mirror).
Here’s a dystopian example from the short film Hyper-Reality.
The product information is customized for your gender and displays personalized information from the advertiser; in this instance a “glitch” in recognizing the player’s gender confuses the system and it briefly switches the display from a female targeted layer to a male oriented one.
Here’s another futuristic example illustrating a product interaction scenario at home — from the short film Sight.
Marketers could learn a great deal about customer interaction with their products while at home: how do they use it? IN what context? how often?
This information collected is so much more valuable as feedback for product improvement than any ad could ever be.
Since the subtle information delivery and gathering of “Clicks on Bricks” offers a far better user experience (who likes ads, really?) and is far more valuable to marketers (Nielsen for the real world!) — it will eventually wipe out the current advertising convention “repeat ad nauseam” — bombarding us with ads anywhere we look.
The good news: the dystopian prophecy depicting ads all around us in the short film Hyper-Reality will never materialize. Like any good prophecy — it’s a warning to focus on the important things: user experience and value creation in order to avert that disturbing future.
When every interaction counts
When every interaction in the real world can be tracked and become valuable to someone: you, a friend, an advertiser, or perhaps a competitor — how do you determine the value of each action?
How do you create an economy that can serve as the foundation for this new world?
You are the product
Before we find the answer, I have to bring up this warning by cyber security guru Bruce Schneier who said:
“Don’t make the mistake of thinking you’re Facebook’s customer, you’re not — you’re the product.”
How do you alleviate and balance the situation? How do you give all sides a measurable and fair stake?
Gamification and reputation models?
— Not granular enough. Not enough incentive.
App stores?
— Let me make a bold prediction: in a world were the AR cloud is predominant app stores are dead!
What if we match the AR cloud with a crypto-currency based on blockchain?
If you are new to blockchain — don’t worry, we all are— here is how to think about its benefits.
A blockchain is a public, decentralized ledger of encrypted transactions
How does it help an AR Cloud-based economy?
Who in XR (short for AR,VR,MR) is already using the blockchain?
Apparently a lot of people!
Here are some of the already published initiatives. I am aware of at least a dozen more that will launch later this year.
High Fidelity
An open, user-driven market for the buying and selling of virtual goods. From the creator of Second Life it offers a better alternative to virtual currency (used in virtual worlds such as Second life). The system is already in beta.
Lucyd
Using Blockchain and a LCD crypto currency to nurture an ecosystem and app store around a new smartglasses.
Lapmix
Blockchain for machine Learning data
It’s envisioned as an ‘image mining’ network for augmented reality or any other computer vision system, such as its own Lampix product.
A new blockchain that will hold {image,description} data-sets of real world objects to democratize image recognition.
Varcrypt
The world’s first blockchain for content distribution designed specifically for Virtual Reality, Augmented Reality, and legacy content.
A Digital Rights Management system for the spatial computing era.
RNDR
Led by OTOY — a light field rendering software company)— blockchain for a distributed GPU rendering economy.
It’s aimed to make it possible for any 3D object or environment to be authored, shared, and monetized through the Ethereum blockchain protocol.
The RNDR token is powered by breakthrough cloud rendering technology, creating a distributed global network of millions of peer GPU devices. The first network to transform the power of GPU compute into a decentralized economy of connected 3D assets.
Cappasity utility token (CAPP)
Focused on retail where there is massive need for 3D content — will be issuing a digital payment vehicle that facilitates AR/VR/3D content exchange among the ecosystem participants from all over the globe
Gazecoin
Get paid to look at things…a blockchain platform measured by gaze control/eye tracking.
Claims to be the killer app for VR/AR advertising, copyright tracking and user rewards. Attempts to fix the main problem with monetizing VR/AR.
Recently completed an ICO! And as of this month is listed on Cryptos (a trading platform)
Arcona
Launched a Blockchain powered AR Cloud and already sold over a million dollar worth of crypto currency.
The blockchain manages sale of digital lands for airrights — the ability to place AR content on physical places.
These initiatives and many others that popped up recently — could become necessary services for the AR Cloud. Although many will need to consolidate into fewer services and tokens — to simplify life for users.
But what’s missing here?
In my AR Cloud intro, I listed its 3 main ingredients:
The first — a scalable and shareable point cloud — is the hardest to build because of its unmatched magnitude.
A blockchain-based economy could help accelerate its creation in crowd-sourced style.
Get thousands and millions of people to help scan the world anywhere they live or work and upload it to the AR Cloud. In addition, incentivize people and especially businesses to contribute their existing content: videos, photos, scans, 3D content, etc.
They would all get some fractions of tokens for contributing to the creation and on-going maintenance of the AR Cloud. Users of the AR Cloud would be charged their respective fractions– depending on the value of the particular part of the cloud being consumed.
With this approach, a crypto-currency could offer the incentive to accelerate the creation of the massive AR Cloud. It could provide the foundation for the Clicks on Bricks economy, allow to eliminate the attention switch by sync’ing the virtual and the physical worlds, and perhaps increase GDP by 20% or more.
Sounds crazy?
Well, a few companies have already started working on this….expect to hear about it any day now. There is no time to waste —
Whether you are a startup, an investor or a corporation, come partner with us in building the AR Cloud!
***
References and further reading
Augmented Reality Is Already Improving Worker Performance by Harvard Business Review
More case studies by AREA.org
The Global AR Cloud Platform — by YouAR
6D.ai is building AR Tech — 3D mesh of the world — TechCrunch
Building the AR Cloud — by Scape
The AR Cloud will be bigger than search — Forbes
Gree VR Capital Closes First Fund — with focus on AR Cloud
Unlocking data with the AR Cloud — by ARtillry
The Augmented Reality Cloud and the Future of Information — By Rackspace
The official blog for Super Ventures — an early stage fund…
772 
4
772 claps
772 
4
Written by
In pursuit of the ultimate Augmented Reality experience. CoFounder of Ogmento, AWE, Super Ventures
The official blog for Super Ventures — an early stage fund dedicated to augmented reality. Learn more at www.superventures.com
Written by
In pursuit of the ultimate Augmented Reality experience. CoFounder of Ogmento, AWE, Super Ventures
The official blog for Super Ventures — an early stage fund dedicated to augmented reality. Learn more at www.superventures.com
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/exploring-sobriety/understanding-the-pink-cloud-phase-of-sobriety-becee56c239d?source=search_post---------319,"There are currently no responses for this story.
Be the first to respond.
Most alcoholics know that when they quit drinking, they’re likely to suffer from serious withdrawal symptoms.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://eng.lifion.com/going-cloud-native-2dc748c0fbcf?source=search_post---------320,"It may come as no surprise for any software engineer to hear this, but managed services offered by today’s cloud providers render significant benefits to engineering organizations. These cloud providers have developed several different solutions to help organizations scale both vertically and/or horizontally across the globe, while in several cases, being at a lower cost to any institution managing their own infrastructure. On top of lower cost and overall ease of scalability, cloud providers guarantee several out-of-the-box freebies that would normally need to be discussed, addressed, and actively monitored if an organization managed their servers solely on their own.
While the benefits are clear, the plethora of Cloud-Native database options makes it difficult to select the right one for your organization’s use case. Here at Lifion by ADP we’re building a global scale platform with a massive suite of products. We use multiple types of data stores dependent on the data and its use case in our platform, such as key-value, relational, graph, document stores, etc.... As we continue to scale our platform and our customer base we found ourselves with a business need to move to a Cloud-Native solution for a core data store that is heavily and frequently used whenever end-users interact with the system. We strive to be thoughtful and systematic about our decisions and designs, especially when changing such a critical piece of our architecture. This meant carefully identifying our requirements, enumerating our possible solution options, and making sure our approach to selecting the correct one was diligent, strategic, and evidence-based. We’re firm believers in using “the right tool for the job” when it comes to building our technology stack. Given we strategically use Amazon Web Services for hosting our platform we intentionally did not consider alternative non-AWS cloud-native options. In this article, my team, who’s responsible for the Metadata Engine and Architecture of our platform, shares how we went about this exercise and explains our findings.
After having several stakeholder discussions and identifying areas that could be improved upon within our current paradigm, we came up with the following overarching requirements.
The team identified a strategy to consistently test across the board with a goal of benchmarking only read speeds of JSON payloads per data store. We developed a proof of concept, and ran those equivalent tests against each database conforming to the following parameters:
Based on the results of our benchmarking strategy outlined above, we compiled a pros and cons list for each solution, which we’ve captured below. Expanded graphs with full document sizes and all requests, specified in our benchmarking strategy above, can be seen in “For The Curious” below. The graphs in “Final Outcome” are zoomed for ease of visualization.
https://aws.amazon.com/s3/
No, Amazon S3 does not meet our requirements, due to its slowness and variability of response time. However, we have several internal networking layers to control traffic in our VPCs, which could have potentially hurt response times during our testing. Since it can give the most stable cache by its high durability and availability, it is possible it could address our use case in conjunction with another caching layer.
https://aws.amazon.com/rds/aurora/details/mysql-details/
Yes, Aurora (MySQL) does meet our requirements, in most cases. There is a speed penalty for using features such as persistent storage, atomic operations, flexible query capabilities, etc. however we may not need all of these features for our use case.
https://aws.amazon.com/rds/aurora/details/postgresql-details/
Yes, Aurora (PostgreSQL) does meet our requirements. Amazon Aurora (PostgreSQL), like the MySQL implementation, has a speed penalty for using features such as persistent storage, atomic operations, flexible query capabilities and so on, but again we may not need all of these features for our use case.
https://aws.amazon.com/elasticache/redis/
Yes, ElastiCache for Redis meets our requirements. ElastiCache For Redis does not persist documents by default, however, automatic daily backups can be enabled. This automatic backup stores snapshots within S3, up to 35 days. With Redis, we can get extremely fast response times, making this one of the best options for our use case.
https://aws.amazon.com/elasticache/memcached/
Yes, ElastiCache for Memcached meets our requirements in all the same ways in which Redis does. The Memcached implementation is missing several out-of-the-box features that the Redis implementation offers. Some features missing include snapshots, replications, lua scripting and advanced data structure support.
Below are additional graphs as measured during our tests: color indicates JSON payload size, Y-Axis measures time, and X-Axis represents number of requests. Each graph is the expanded version of those graphs highlighted above. Again, the following benchmarks only tested the read speeds of JSON payloads per data store.
Based on our tests/learnings from developing our proof of concept, we concluded that S3 is too slow for our use case, confirming our theory that S3 fits better for latency-insensitive, high-reliability needs such as for file storage or serving. Both implementations of Aurora seem like good choices because they provide persistent storage by default, and may be good options for some other use cases we have internally. However, we may not need to pay the extra cost of persistent storage and large scale transactions. Upon further investigation we also found with the Aurora options we’d need to run a larger instance size than we would if using ElastiCache, which would cost considerably more. If you could avoid paying extra cost, on top of using a faster solution, wouldn’t you? Hello, ElastiCache.
Once we had narrowed down to ElastiCache for our approach the next question we had to ask ourselves was which implementation? This is a question we debated internally, and it ultimately boiled down to the following differences we saw between Redis and Memcached. With that said, here are some of the differences we identified that were key to our use case between Redis and Memcached:
ElastiCache for Memcached
ElastiCache for Redis
Knowing the result of our findings would become a very important piece of the Lifion platform undoubtedly fueled us to take several extra steps in our due diligence, while enjoying the impact of our work. Keep in mind these findings were pivotal for our specific use case, and might not hold true in all scenarios, as most questions posed within Computer Science are rebutted with: “it depends”.
Plain and simple, our final decision boiled down to our global scalability requirements. Built-in replication, as well as snapshots in cases of disaster recovery, were what eliminated the Memcached implementation. Sure, we could have implemented our own solutions to fill the void, but why not take advantage of already optimized systems? Our needs are global, and having these optimized solutions provided by Amazon were truly the differentiator. Not to mention, Redis read speeds, based on most of our benchmarks, were also slightly faster. So, at the end of the day, it’s all about choosing the “right tool for the right job”.
Credited Platform Engineers
All things tech at Lifion by ADP
4.1K 
Thanks to Tom Rogers and Aubrie-Ann Jones. 
4.1K claps
4.1K 
Written by
Lead Technical Product Manager @ Lifion by ADP. https://goo.gl/BZS1am
All things tech at Lifion by ADP
Written by
Lead Technical Product Manager @ Lifion by ADP. https://goo.gl/BZS1am
All things tech at Lifion by ADP
"
https://medium.com/@savinihemachandra/creating-rest-api-using-express-on-cloud-functions-for-firebase-53f33f22979c?source=search_post---------321,"Sign in
There are currently no responses for this story.
Be the first to respond.
Savini Hemachandra
Oct 8, 2019·3 min read
First, you have to create a project in the firebase through this link by simply clicking on the get started button appear on the page.
Then go to Develop -> Database and create a test mode database. Here we are going to work with Cloud Firestore. Therefore select Cloud Firestore to see changes in the database with API calls.
Before beginning we have to set up our project locally
To install firebase tools,
Then you have to log in to the firebase account and initialize it.
You need “functions” to use cloud functions and it creates a new folder called “functions” inside the project folder. As well as it is the place where we are going to write our TypeScript code. You can see two dependencies have been added to the package.json file called “firebase-admin” and “firebase-functions”. Firebase-functions allow us to trigger cloud functions based on cloud Firestore, Real-Time Database, Firebase Authentication, Google analytics ..etc. Firebase admin allows us to interact with Firebase from privileged environments to perform actions like, read and write from the real-time database, generate and verify Firebase auth tokens, etc.
Then you can install other dependencies. Go inside the functions folder and install express and body-parser.
Then we can start writing our cloud functions in our express.js server. First, go to the functions/src/index.ts
Let’s start to create first CRUD operation- CREATE
First I have created a user interface with necessary variables and their types and then created an API that posts a new user record to the database collection as POST “../users”.
we can create GET “../users” endpoint to Read all users in the database collection. Here we get the collection snapshot. We iterate through it and push each document and document-id into “users” array.
We can create GET “../users/{userId}” to get a single user, UPDATE “../users/{userId}” to change an existing record, DELETE “../users/{userId}” to delete an existing record from the database collection.
Now you can deploy to the firebase
By using Postman you can test your APIs,
Software Engineer at Axiata Digital Labs
See all (43)
706 
11
706 claps
706 
11
Software Engineer at Axiata Digital Labs
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@austinhale/building-a-node-api-with-express-and-google-cloud-sql-9bda260b040f?source=search_post---------322,"Sign in
There are currently no responses for this story.
Be the first to respond.
Austin Hale
Jul 28, 2018·5 min read
Note: This is an ongoing series of blog posts detailing how we made an iOS & Android mobile app in 10 days. Feel free to follow me here or on Twitter to get updated when the other articles are published.
"
https://medium.com/google-cloud/13-most-common-google-cloud-reference-architectures-23630b46326d?source=search_post---------323,"There are currently no responses for this story.
Be the first to respond.
👋 Hi Google Cloud Devs!!
I am asked multiple times to compile a list of most common Google Cloud reference architectures. That got me thinking and I started #13DaysOfGCP mini series on Twitter. If you were not able to catch it, or if you missed a few days, here I bring to you the summary!
Thank you to all who provided the topic ideas and shared their thoughts. This has been instrumental in helping me pick the topics that you all were interested in seeing the over the course of 13 days.
I am thankful and really humbled by your participation, engagement and topic suggestions that made this series so much fun, not just for me but also for the rest of the Google Cloud Community members!
Thanks again and 👋 until later 🙂
Google Cloud community articles and blogs
393 
1
393 claps
393 
1
Written by
Developer Advocate @Google, Artist & Traveler! Twitter @pvergadia
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Developer Advocate @Google, Artist & Traveler! Twitter @pvergadia
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud-jp/firestore1-a62405a7cd82?source=search_post---------324,"There are currently no responses for this story.
Be the first to respond.
Google製、モバイル・ウェブ・サーバー開発に対応したCloud Firestoreを使いこなすための勘所をつらつらと書いていきます。パート1の本記事ではFirestoreの全体観を掴んでいただけたらと思います。
Cloud Firestoreは、Firebaseのサービス群の1つという捉え方と、GCP(Google Cloud Platform)のサービス群の1つという捉え方がありますが、本記事ではFirebase経由で使うことを前提に書いていきます。Cloud Functionsなど関連サービスとして言及するものも同様です。Firebaseはざっくり言うとGCPのクライアントアプリ向けサービスを使いやすく提供しているラッパーサービスなので、それで足りるものはFirebase経由での利用をお勧めします。
ドキュメントも次のように2箇所にあるのですが、Firebase経由で使う場合は基本的に前者の方を見るだけで充分です。
まず、そもそもFirestoreとは何？ということに関しては、次の記事の前半に書いたので、ご覧ください。
medium.com
まずはFirestoreの利点をざっと紹介します。
Firestoreに限らずFirebase配下のサービスすべてに言えることですが、サーバーの管理が不要で、主に次の恩恵を受けられます。
2019年2月1日に晴れてGA(General Availability)版としてリリースされました🎉
cloud.google.com
firebase.googleblog.com
これによる最も大きな恩恵は、Service Level Agreement (SLA)が定義されたことだと思います。品質要件が厳しいサービスでも選択しやすくなったはずです。
cloud.google.com
また、これに合わせて、2019年3月3日から非マルチリージョンの料金が半額くらいになります。
— 追記終わり。以下のベータ版はそれ以前の状態でしたがしばらく残しておきます。 —
ただ、Firestoreは現状ベータ版なので、以下の注意が必要です。
特に、SLA(サービス品質保証)適用外ということが気になると思いますが、筆者が使っている限りは特に最近はとても安定して動いています。2017年まではたまに挙動不審になったり遅延が大きくなる感じもありましたが、最近は全然体感できないです。
また、過去1週間のサービスステータスは以下から確認できますが、ご覧の通り障害ゼロです。
status.firebase.google.com
FirestoreはまだSLA適用外なものの、筆者はプロダクション利用に充分耐え得ると感じて使っていますが、これは個人の感覚や運用しているサービスポリシーにもよるので、ベータ版な時点で使いものにならないと見なされることもあると思います。早くベータ版を脱して欲しいですね🐶
公式ドキュメントに、世界規模の大きなアプリにも耐え得るスケーリング性能を持つと明記されています。普通に書けば普通に実質青天井にスケールするサービスを作れます。
Cloud Firestore brings you the best of Google Cloud Platform’s powerful infrastructure: automatic multi-region data replication, strong consistency guarantees, atomic batch operations, and real transaction support. We’ve designed Cloud Firestore to handle the toughest database workloads from the world’s biggest apps.https://firebase.google.com/docs/firestore/
次の記事も参考になります。
qiita.com
このあたりから、実利用する時の観点になってきます。Firestoreはその登場以前からある Firebase Realtime Database と同じく、従来からよくある構成のデータベースを持つ自前のWeb APIを介するサーバー・クライアント間の処理を、クライアントコードだけで簡単に実現できます。
ローカルデータベースに近い感覚でデータの取得・更新処理を書くだけで、クラウド上のデータベース(内部的には Google Cloud Datastore の模様)と同期されます。オフライン時は、データ取得の場合キャッシュ済みのデータを返してくれたり、データ更新の場合は一旦ローカルに保持されオンラインになり次第クラウド同期、というケアもFirestore SDKが自動的にやってくれます。
これを従来のやり方でWeb API含めて自力で正確かつ高速な実装をしようとすると大きな労力がかかりますし、スケーリング性能を考慮するとさらに難易度が上がります。これまでの少人数開発ではそのあたりの何かしらを妥協しながら開発進めざるを得ないことが多かったですが、Firestore(や類似技術)の登場により短期間で極めて品質の高いアプリを作ることが可能となりました。
### クラウドのロジックを挟むことも可能
また、「クライアントコードのみで同期処理を書かなくてはならない」という制約があるわけではなく、要件によってはCloud Functionsを併用することで次のようにクラウド上のロジックを組み合わせることも容易にできるという柔軟性も持ち合わせています。
ここまで良い点ばかり挙げていましたが、もちろん不利な点もあります。主に、NoSQLデータベースであることに起因するものが挙げられます。
まず、Firestoreでは「クエリがまったくできない」というわけではないです。これはRealtime Databaseから大きく改善された点でもあります。
firebase.google.com
ざっくり次のことは対応されています。
デフォルトでは、すべての単一フィールドにインデックスが貼られているため前者のクエリはできるものの、後者の複数のフィールドの組み合わせによるクエリをしたい場合、その組み合わせによってその都度あらかじめインデックスの作成が必要なことにも注意です。
firebase.google.com
インデックス未定義のフィールドの組み合わせのクエリを投げると、次のような実行時エラーが発生してインデックス定義を促されます。
Error Domain=FIRFirestoreErrorDomain Code=9 “The query requires an index. You can create it here: https://console.firebase.google.com/project/PROJECT_ID/database/firestore/indexes?create_index=EghtZXNzYWdlcxoNCgljcmVhdGVkQXQQAxoQCgxzZW5kZXJVc2VySWQQAxoMCghfX25hbWVfXxAD""
記載のURLを踏むと、次のように定義の足りないインデックスの作成画面に飛びます。
ちなみに、インデックス管理はこのままサクッとWebコンソールから手動作成もできますが、実アプリ開発ではFirebase CLIを利用することをおすすめします。インデックスをコード管理できたり、複数環境のインデックス更新をミス無く行えるようになります。
上のダイアログで促されたインデックス作成を次のようなインデックス定義JSONファイルに落とし込んでデプロイ( firebase deploy --only firestore:indexes )すると、手動操作した場合と同じ結果になります。
2つ以上のキーによるクエリ・ソートの度にインデックス定義が強制されるのはやや面倒と思うかもしれませんが、これによりすべてのクエリにはインデックスが使われることになり、つまり基本的に重いクエリを投げることが不可能となっていて、むしろ優れた点だと捉えています。
単純な絞り込み・ソート
それぞれ「単純な」と注釈付きなのは、一般的なRDB(リレーショナルデータベース)では当たり前にできることも、FIrestoreではできないことが多いからです。ドキュメントの「クエリの制限事項」に5つほど記載されているものを眺めるとイメージが湧くはずです。
また、FirestoreではSQLのJOIN や GROUP BY などの集計操作などは概念さえ存在しません。つまり、次のような考え方の変化が必要です。
最後に述べたFirestoreドキュメントのデータ分析のしにくさは、Firestore → Google Cloud Storage(GCS) export → BigQueryへロード・クエリー、というやり方が公式にサポートされたので、かなり解消されました。
詳しくは以下の一連のツイートなどご覧ください。
cloud.google.com
ただ、RDBはエクスポート・ロードなどの手順無しにすぐにクエリーできるので、その手軽さには劣るはずですし少しコストが嵩むのも気になります。とはいえ、RDBでもデータ分析用にリードレプリカ追加したり分析に適したデータベースに適宜移すなどして別途分析基盤を整えることも普通なので、それと同等のことを簡単にできるようになったと言えます。
2018年8月頃からインポート・エクスポートのREST APIおよびgcloudコマンドが提供されました:
そのため、次のようにそれらを組み合わせて、バックアップ・リストアの仕組みを構築可能です。
medium.com
medium.com
以前と比べると大きく改善されはしましたが、いわゆるフルマネージドなバックアップサービスはまだ提供されていません。
Firebase Realtime Database では次のように、バックアップ・リストアのサービスが提供されていて、簡単な設定をするだけで、毎日Google Cloud Storage(GCS)にバックアップされます。
firebase.google.com
Firestoreでも、ベータ版が外れる頃には同様な仕組みが提供されそうな気がしています🧐
2018年7月までの状況:
Stack Overflowなどでも質問があり、その回答の通り、現時点ではFirestoreにマネージドなバックアップサービスが存在しません。
stackoverflow.com
ただ、node-firestore-backup というOSSは存在し、これを定期的にCron Jobなどで実行することでバックアップは可能です(もちろん自前で別のバックアップ処理を書いてもOKです)。
github.com
本記事で述べたFirestoreの特性をまとめると、次のような感じでしょうか。
NoSQLデータベースであることに起因する不利な点は、強力なスケーリング性能とのトレードオフとして割り切るしかないですね。個人的には、単純なクエリができるだけでもありがたいと感じています。
本記事のパート1ではFirestoreの概要説明だけとなりましたが、パート2では実践的な使い方をなぞっていきます。
medium.com
Google Cloud Platform…
272 
272 claps
272 
Google Cloud Platform 製品などに関連するコミュニティが記載したテクニカル記事集。掲載された意見は著者のものであり、必ずしも Google のものを反映するものではありません。
Written by
Software Engineer(Flutter/Dart, Firebase/GCP, iOS/Swift, etc.) / Freelance / https://mono0926.com/page/job/
Google Cloud Platform 製品などに関連するコミュニティが記載したテクニカル記事集。掲載された意見は著者のものであり、必ずしも Google のものを反映するものではありません。
"
https://medium.com/analytics-vidhya/tpu-training-made-easy-with-colab-3b73b920878f?source=search_post---------325,"There are currently no responses for this story.
Be the first to respond.
You have a plain old TensorFlow model that’s too computationally expensive to train on your standard-issue work laptop. I get it. I’ve been there too, and if I’m being honest, seeing my laptop crash twice in a row after trying to train a model on it is painful to watch.
In this article, I’ll be breaking down the steps on how to train any model on a TPU in the cloud using Google Colab. After this, you’ll never want to touch your clunky CPU ever again, believe me.
TL;DR: This article shows you how easy it is to train any TensorFlow model on a TPU with very few changes to your code.
The Tensor Processing Unit (TPU) is an accelerator — custom-made by Google Brain hardware engineers — that specializes in training deep and computationally expensive ML models.
Let’s put things into perspective just to give you an idea of how awesome and powerful a TPU is. A standard MacBook Pro Intel CPU can perform a few operations per clock cycle. A standard off-the-shelf GPU can perform tens of thousands of operations per cycle. A state-of-the-art TPU can perform hundreds of thousands of operations per cycle (sometimes up to 128K OPS).
To understand the scale, imagine using these devices to print a book. A CPU can print character-by-character. A GPU can print a few words at a time. A TPU? Well, it can print a whole page at a time. That’s some amazing speed and power that we now have at our disposal; shoutout to Google for giving everyone access to high-performance hardware.
If you’re interested in the inner workings of a TPU and what makes it so amazing, go check out the Google Cloud blog article where they discuss everything from hardware to software here. You can find information regarding mechanisms, hardware specs, benefits, limits, constraints, and much more to help you with your projects!
For the sake of this tutorial, I’ll be running through a quick and easy MNIST model. Do note that this model can be whatever you want it to be. To better help you visualize what’s going on, I’ve chosen good old MNIST (again, a dataset of your choosing).
Let’s first begin by extracting and preprocessing our dataset. This shouldn’t be much of a problem:
I’ve tried to make this tutorial as comprehensive as possible so that you can train your models at unfathomable speeds and feel like you’re on top of the world. There are 4 steps we will be taking to train our model on a TPU:
Let’s dive right into it!
Note: Feel free to preprocess your datasets using tf.data or convert them into TFRecords if that catches your fancy. These steps are not necessary but may be useful in handling out-of-memory instances in the case that you are working with very large datasets.
When I was messing around with TPUs on Colab, connecting to one was the most tedious. It took quite a few hours of searching online and looking through tutorials, but I was finally able to get it done.
We first need to connect our Colab notebook running locally to the TPU runtime. To change runtime, simply click the Runtime tab in the navigation bar. A dropdown menu will be displayed from which you can select the Change runtime type option. A popup window will show up where you can select the TPU option from the dropdown selector.
To connect to a TPU instance, we need to create a TPUClusterResolver that takes in the available device and provisions one for us:
This should connect our current Colab session to an available TPU instance. Let’s move on to initializing the strategy and finally invoking the TPU.
Note: If you are experiencing difficulties in connecting to a TPU instance even after several attempts and re-runs, you can change the runtime type to GPU instead. The code still compiles and runs efficiently.
Now that we’ve changed the runtime and have acquired a list of available TPUs, we need to invoke the TPU by creating a distributed training strategy — a wrapper around the model that makes it compatible with multi-core training.
The resolver gives us access to the TPU such that we can finally build a parallelly-distributed pipeline on it. This is a necessary step because a TPU is a distributed training processor and is not single-core like a traditional CPU. With this strategy method, jumping on board a TPU is very simple! It should give you a list of available instances:
With a training strategy now at hand, we can proceed to build our model using that strategy as such:
This looks like a lot of jargon, but all it does is take a regulartf.keras model (that is typically run on a CPU) and places it on the TPU and automatically distributes it across all the available TPU cores.
This is the exciting part of the process. We can finally train our model on a Cloud TPU for free. With so much power in our hands, let’s make good use of it and train on MNIST (I know…very anti-climatic).
Ensure that the number of instances is perfectly divisible by the steps_per_epoch parameter so that all the instances are used duringtraining. For example, we have 60000 instances in our training set. 60000 is divisible by 50 so that means all our instances are fed into the model without any leftovers. If you hit run, it should start training on a TPU instance after a short while:
With that, training should commence soon. Colab will boot up a TPU and upload the model architecture on it. You should soon see the classic Keras progress bar style layout in the terminal output. Congrats! You’ve successfully just used a TPU.
Retrospectively, I always thought training models on TPUs were a thing only ML Wizards — those with years of experience and skill — could handle. TPU training was always that one huge thing out of my grasp simply because of the hype around it (making it seem very difficult to use). Never in my wildest dreams did I think it’d be as simple as adding less than 10 lines of code to my preexisting models.
After reading this article, I want you to know that anyone can train on accelerated hardware regardless of experience. This is just one of many steps undertaken by Google AI to democratize Machine Learning and Artificial Intelligence amongst the masses.
The Neural Machine Translation model I had written (for the TensorFlow Docs) took me less than a minute to train on a TPU. I was astonished because the same model took more than 4 hours to train on a CPU (which probably explains why my laptop crashed and ran out of memory twice).
My model hit a very high accuracy — higher than what I achieved training on a CPU. With performance and speed, the Cloud TPU is second to none when it comes to quality training!
Note: You can find my code here. Have fun!
Google always comes bearing gifts when it launches new Machine Learning toys that we can tinker and play around with. The TPU processor is certainly a boon to the ML community as it plays a major role in the democratization of AI — it gives everyone a chance to use accelerated hardware regardless of demographic. With TPUs, research and experimentation are hitting all-time highs and people are now engaging in Machine Learning like never before!
I hope this article has helped you train your models in a much more efficient and elegant manner. If you have any questions about the use of TPUs or want to chat in general about Tech and ML, feel free drop them down below in the comments or catch me on Twitter or LinkedIn. I usually reply within a day.
Until then, I’ll catch you in the next one!
Article by Rishabh Anand
Interested in reading about the latest-and-greatest technologies and getting your hands dirty with the recent happenings in Machine Learning, Data Science, and Technology? Do catch my other articles or give me a follow! Your feedback and constant support mean a lot and encourage me to continue writing high-quality content for your learning!
towardsdatascience.com
medium.com
hackernoon.com
towardsdatascience.com
hackernoon.com
towardsdatascience.com
Analytics Vidhya is a community of Analytics and Data…
411 
2
Latest news from Analytics Vidhya on our Hackathons and some of our best articles! Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
411 claps
411 
2
Written by
CS + Math @ NUS • ML Research @ A*STAR • Writer • Open-source Jedi • https://rish-16.github.io
Analytics Vidhya is a community of Analytics and Data Science professionals. We are building the next-gen data science ecosystem https://www.analyticsvidhya.com
Written by
CS + Math @ NUS • ML Research @ A*STAR • Writer • Open-source Jedi • https://rish-16.github.io
Analytics Vidhya is a community of Analytics and Data Science professionals. We are building the next-gen data science ecosystem https://www.analyticsvidhya.com
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/hackernoon/making-audio-searchable-with-cloud-speech-36ce63b6b4d3?source=search_post---------326,"There are currently no responses for this story.
Be the first to respond.
Last month Cloud Speech introduced a new word-level timestamps feature: audio transcriptions now include the start and end timestamp for each word. This opens up tons of possibilities: developers can now skip to the exact moment in an audio file where a word was spoken, display the relevant text while audio is playing, or search a library of audio for a specific term.
With the ability to search an audio file I wanted to try this feature out on videos. To do this I extracted the audio track from a video file, sent it to Cloud Speech, and built a frontend for searching the audio transcription JSON. The result is the following demo (it’s best viewed with sound in my recent ML API presentation):
In addition to the Speech API, the demo also uses Cloud Functions, Cloud Storage, and App Engine for hosting. Here’s a diagram of how the backend works:
Because Cloud Speech lets you provide the Cloud Storage URL for an audio file to transcribe, I decided to store all of my video and audio content in Cloud Storage. Then in my App Engine frontend I could get the video and associated transcription JSON directly from Cloud Storage.
I wanted to be able to drop a video file into Cloud Storage and automatically have the transcription show up in another storage bucket. Sounds magical, right? I implemented this with Cloud Functions: a compute solution for writing functions that are automatically triggered by certain cloud events. Functions are written in Node.js, and you specify the type of event that will trigger each function. In this case I triggered my function every time a new file was added to my video bucket. I split the transcription process into two functions:
2. transcribeAudio: Send the FLAC file to the Speech API and upload transcriptions to Cloud Storage
The extractAudio function uses the google-cloud Node module for accessing Cloud Storage and fluent-ffmpeg for extracting and transcoding audio. To get ffmpeg working in my Cloud Functions environment I needed to upload the ffmpeg binaries when I deployed my function and tell fluent-ffmpeg the path to those binaries.
Here’s the full list of npm dependencies:
We’ll also define variables for each of our Cloud Storage buckets: one for videos, one for the FLAC audio files, and one for the transcription JSON:
The function will do the following:
The function receives an event parameter, which will give us data on the file that triggered the event. Here’s an outline of our function:
Next we’ll write the function to download our video to Cloud Storage. We can save the file to a local disk in our Cloud Function environment by writing it to the /tmp directory:
Once we’ve got the video file available locally in Cloud Functions, we’re ready to extract and transcode the audio with ffmpeg:
The last step is to upload the flac file to a new Cloud Storage bucket. You can find the code for uploading files in the google-cloud documentation.
To get our audio transcription and timestamp data we’ll write a Cloud Function called transcribeAudio which will be triggered whenever a flac file is added to our audio bucket. For this function we’ll need to instantiate a speech client with google-cloud Node and then write our transcription function:
We just need to call longRunningRecognize() to make a request to Cloud Speech with our client. This will kick off a long running speech operation and return the final transcription results when it finishes:
We can then write the transcriptions to a local JSON file:
The last step is uploading our JSON file to Cloud Storage in the same way we did in the first function.
Woohoo! Now we’ve got an entirely serverless solution that generates timestamp transcriptions from a video. Note that you’ll want to periodically delete the contents of tmp/ from your Cloud Functions file system to avoid hitting a memory limit. You can do this with the rimraf npm module.
To start using the timestamps functionality in your own apps, head over to the Speech API timestamp docs. For details on Cloud Functions check out the docs here or watch my teammate Bret’s awesome talk on Cloud Functions.
I’d love to see what you build with the Speech API and Cloud Functions. Let me know what you think in the comments or find me on Twitter @SRobTweets.
#BlackLivesMatter
590 
6
how hackers start their afternoons. the real shit is on hackernoon.com. Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
590 claps
590 
6
Written by
Connoisseur of code, country music, and homemade ice cream. Helping developers build awesome apps @googlecloud. Opinions = my own, not that of my company.
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
Connoisseur of code, country music, and homemade ice cream. Helping developers build awesome apps @googlecloud. Opinions = my own, not that of my company.
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://howtofirebase.com/cloud-functions-migrating-to-node-8-9640731a8acc?source=search_post---------327,"Google Cloud Functions has been stuck on Node 6 since launch.
But Google Cloud’s Next conference is this week, and they just announced the new Node 8 runtime.
I miss my async/await from Node 8, so I spent the morning upgrading my Cloud Functions.
Here’s what you need to do to get on the Node 8 train:
I had some difficulty figuring out these steps, so I manually deleted a Node 6 version of my function using the GCP console. That may have helped… but I have no way of testing that now.
Node 8 has async/await and the spread operator.
There’s probably more… but I don’t care. That’s all that I need right now.
Both of these features have been around on the front end for a while now. Developing in Node 6 has been sad.
Oh yeah… and you can use Node-8-only packages… for whatever that’s worth!
Here’s the repo that I’m working on right now: firebase-ssr-starter
And there’s a new Node 8 branch of the firebase/functions-samples repo.
Firebase tutorials and tips
687 
13
687 claps
687 
13
Written by
Front-End Web Developer; Google Developer Expert: Firebase; Calligraphy.org
Firebase tutorials and tips
Written by
Front-End Web Developer; Google Developer Expert: Firebase; Calligraphy.org
Firebase tutorials and tips
"
https://towardsdatascience.com/how-to-deploy-machine-learning-models-with-tensorflow-part-3-into-the-cloud-7115ff774bb6?source=search_post---------328,"Sign in
There are currently no responses for this story.
Be the first to respond.
Vitaly Bezgachev
Jul 1, 2017·9 min read
In Part 1 and Part 2 we created a GAN model to predict the Street View House Numbers and hosted it with TensorFlow Serving locally in a Docker container. Now it is a time to bring it into the Cloud!
When you implement a web service (such as prediction service) and want that other people use it, you publish it at a Cloud Hosting provider. Usually you do not want to take care of such things as availability of your service, scalability and so on; you want to concentrate on developing of your models and algorithms.
Nowadays you have a huge selection of cloud platform providers. Most prominent are Amazon AWS, Microsoft Azure, Google Cloud Platform, IBM Bluemix. They provide resources and take care of availability of a service once it is deployed on their servers.
The second important task is an automation. You want automate deployment, scaling and management of you service. Personally I want to deploy per mouse click or running a simple script, I want that my service scales automatically if it gets more requests, I want that in a case of crash it restores without manual intervention and so on. Here a tool Kubernetes comes into play.
Kubernetes is an open source system for automating deployment, scaling, and management of containerized applications.
Kubernetes is developed and supported by Google. So you can definitely rely on it. The cloud providers, listed above, have a built-in support of it. So we can relatively easy setup our deployment into the Cloud.
For this article I use Microsoft Azure. Mainly for one reason — they provide 30-days trial with 200$ credit.
I also looked at other providers. At AWS I didn’t find a possibility to get a small Kubernetes cluster for free. At Google you can test their Cloud Platform for free, but they scared me with “Account Type = Business” (I wanted just a small test environment for myself). IBM Bluemix was not very intuitive.
So I went with Microsoft Azure and they have nice User Interface too :-)
Before we step into the Kubernetes, we need to save our Docker container with all changes, we made, as an image. First we need container ID:
You’ll get a list of all Docker containers — select ones you have created. For example:
I marked in bold the ID we need.
HINT: Since I do not want to have an expensive GPU-powered virtual machine in the cloud, I went with TensorFlow Serving CPU Docker container.
Now we can create Docker image from our container:
If you look at the image list (docker images command), you should find a newly created image <your user name>/tensorflow-serving-gan tagged with v1.0. The image is approx. 5 times bigger than the original ones, since we downloaded and built a lot of new things.
I encourage you to check Kubernetes documentation for its capabilities, concepts and tutorials. Here I give just a very rough idea, how to work with it.
We deploy our Docker image into Kubernetes cluster. Kubernetes cluster consists of, at least, one master and one or more worker Nodes.
Node is a worker machine in the Kubernetes cluster (virtual machine or bare metal). It is managed by the master component and has all services to run Pods. Those services include, for example, Docker, which is important for us.
We will create one master and one worker Node. Master Node does not run any Pod, its responsibility is a cluster management.
Pod is a group of one or more containers, the shared storage for those containers, and options about how to run the containers. So Pod is a logical host for logically dependent containers.
In our Pod we have only one Docker container, namely, our TensorFlow GAN Serving. We will create two Pods, though they will run on the same node.
Pods are mortal. They are born and die. Replication controllers in particular create and destroy Pods dynamically. While each Pod gets its own IP address, even those IP addresses are not reliable. So if we have Pods that need talk to each other (e.g. frontend to backend) or we want to access some Pods externally (in our case), then we have a problem.
Kubernetes Service solves it. This is an abstraction, which defines a logical set of Pods and policy to access them. In our case we will create one service that abstracts two pods.
Microsoft Azure provides a convenient way to setup and operate a Kubernetes cluster. First of all, you need Azure account.
Then check installed version with:
It should be equal to or greater than 2.0.x.
You need to go through simple steps to create a Kubernetes cluster.
Login into Azure
and follow instructions in the Terminal and in browser. At the end you should see a JSON document with a cloud information.
Create resource group
Resource group is a logical group where you deploy your resources (e.g. virtual machine) and manage them.
You should see a JSON document with the resource group information.
Create cluster
Now it is time to create Kubernetes cluster. In Azure free trial we can use up to 4 cores, so we can create only one Master and one Worker; each of them has 2 cores. Master Node is created automatically, agent-count specifies number of worker (or agent) Nodes.
Hint: do not use very long names, I got an error by creation of Kubernetes Services, which complained about too long names (Azure adds pretty long suffixes to a cluster name).
You will receive also generated SSH keys in the default location if they don’t already exist. Wait a couple of minutes… If everything went right, then you should see pretty long and detailed JSON document with cluster information. Convince yourself that a cluster has been created in your resource group
You can also check in Azure Portal that we have 2 virtual machines — k8s-master-… and k8s-agent-… in the resource group ganRG.
CAUTION
Be aware to stop or deallocate virtual machines if you do not use them to avoid extra costs:
You can start them again with:
Now I make a short break with Kubernetes and upload our Docker image into Azure Container Registry. It is our Docker private registry in the Cloud. We need this Registry to pull the Docker image into our Kubernetes cluster.
creates a private registry ganEcr. As a response you should get a JSON document with registry information. Interesting fields are:
The first tells you, there is no administrator for the Registry and the second gives you a name of the server in your resource group. For the upload of a Docker image we need an administrator — we must enable him:
First, we must provide credentials for the upload:
You should receive a response similar to:
You use a username and one of passwords to login into registry container from Docker:
Then tag the GAN Docker image in that way:
And now you can push it into Azure Container Registry!
Be patient, it takes a while :-) Remember, this operation uploads the Docker image from your PC to Microsoft servers.
Back to Kubernetes. For operations on a cluster we need a tool named kubectl. It is a command line interface for running commands against Kubernetes clusters.
I encourage you to check kubectl documentation for a list of available commands. I use here:
to get the information about Kubernetes Nodes, Pods and Services respectively and
for creation of Pods and Services from a configuration file.
First get credentials from Azure:
This commands gets credentials from Azure and saves them locally into ~/.kube/config. So you do not need to ask for them again later.
Now verify connection to the cluster with kubectl:
You should see master and worker Nodes:
I have created a configuration file (YAML format) for the deployment into Kubernetes cluster. There I defined a deployment controller and a service. I encourage you to read Kubernetes documentation for details.
Here is just an explanation, what I did. Actually, you can deploy Pods and Services with kubectl commands, but it is much more convenient to write desired deployment configuration into such YAML file once and use it later.
In our case it defines a deployment and a service:
Deployment
I want to deploy my Docker image into 2 Pods:
And pull it from my Azure Container Registry:
After deployment a Pod should start the Shell and start TensorFlow, serving a GAN model, in the Docker container:
On the port 9000:
Service
Service must accept external requests on port 9000 and forward them to a container port 9000 in a Pod:
And provide load balancing between 2 underlying Pods:
Now deploy the Docker image and let Kubernetes manage it!
You should see:
Now check that Pods are running:
Hint: it can take some time before the status changes to Running. Only then you can be sure that Pods are ready to serve.
And the service is ready:
Hint: External IP for gan-service must be a valid IP (not <pending> or <node>). Otherwise the service is not operational.
So now we can check that our efforts were worthwhile!
You should see:
If you get this result, congratulations! You deployed the GAN model in the Cloud in a Kubernetes cluster. And Kubernetes scales, load balances and manages a GAN model in reliable way.
In Part 1 and Part 2 we created a GAN model, prepared it for serving by TensorFlow and put it into a Docker container. That was a lot of work, but we made everything locally.
Now we made it publicly available and did that in reliable and scalable way, using state-of-the-art technologies.
For sure, the model is quite simple and it gives results in user-unfriendly form. There is a lot of automation potential, and I did a lot of manual steps to explain the mechanics. All the steps can be packed into scripts for “one-click” deployment or be a part of Continuous Integration/Continuous Deployment pipeline.
I hope you enjoyed this tutorial and find its useful. In case of any questions or problems do not hesitate to contact me.
I extended a tutorial with instructions, how to create REST API to models, hosted by TensorFlow Serving.
Engineering Manager @Klarna, lifelong learner
See all (31)
686 
7
Every Thursday, the Variable delivers the very best of Towards Data Science: from hands-on tutorials and cutting-edge research to original features you don't want to miss. Take a look.
686 claps
686 
7
Your home for data science. A Medium publication sharing concepts, ideas and codes.
About
Write
Help
Legal
Get the Medium app
"
https://m.subbu.org/cloud-optimization-circus-65f3b47d0c79?source=search_post---------329,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Top highlight
If you are a cloud adopter rapidly adopting cloud services, but not developing the finance governance muscle, you will certainly be visiting the cloud optimization circus frequently.
I compare cloud optimization exercises to going to a circus because those exercises invite all the same characters and emotions that you find in a circus. There is fear (of wasting money), trickery (by folks showing you how much you could be saving), illusion (of savings that don’t exist where you’re told they exist), excitement (of finding savings), and drama (of playing heroics). It may be fun and entertaining once or twice. Not so when you’ve a mission to accomplish, unless, of course, the mission is going to the circus.
Security and costs are the two biggest risks of cloud adoption. Security is a risk because, teams that optimize for agility on cloud tend to ignore security initially only to realize later. Cloud certainly gives the building blocks for security, but it is up to you to use the building blocks in the intended manner. Cost is the second on my list. Cloud is cheaper once you understand how cloud costs work, and develop the governance muscle. Cloud can be very expensive otherwise.
Back in February 2018, I gave a talk at the Container World Conference on Are We Ready for Serverless. One of the key themes of my talk was that serverless frameworks like AWS Lambda are the closest available today to ensure required supply of resources follow the demand for resources. Here is a hypothetical supply-demand chart.
Curve A shows the resource demand. This is the sum total of all resources required to run the business, which in this example varies during the day and the week. Curve B is the ideal supply and spend. In the best case, supply, and hence spend, closely follows the demand. This is possible with serverless frameworks. Curve C is what usually happens in cloud environments. Though supply varies due to auto-scaling and ephemeral usage, such as dev/test activities during the day tapering off over nights and weekends, it usually stays above the resource demand. Curve D shows the supply in data centers where it typically stays flat.
Let’s ignore serverless here. Though it is the most efficient and requires no effort to maintain the spend to tightly follow the demand, only a tiny fraction of total cloud workloads today run on serverless frameworks like Lambda. Serverless potential is yet to be realized at large, and each enterprise will have to carve out its own journey in the coming years.
Majority of cloud workloads today run on virtual machines followed by multi-tenant managed services including network and storage services. Though some managed services bill you for what you need and use, for the vast majority, the task of making the supply (C) to efficiently follow the demand (A) falls on development teams, an assortment of nascent tools, and mostly reactive practices.
However, the task of making the spend efficiently follow the demand is easier said than done. Cost consideration is usually an after thought as most cloud adopters’ early focus remains on speed of delivery and not cost efficiency.
Unfortunately, this topic does not get much attention in the cloud community. Cost worries are usually brushed aside with suggestions like “use auto-scaling”, “use spot instances”, “fix your automation to clean up”, or “turn off your machines when you leave work”. Look at conference talks, meetups and blogs — you will rarely hear about spend management practices, how to project costs, how to understand detailed billing data, how to maintain efficiency, best practices, failures, lessons learned etc. Consequently, most cloud adopters fail to realize the strongest lever that cloud offers — which is to manage the spend to vary with the demand. But such a lever won’t exercise by itself. You need to equip the organization with tools, practices and processes to actually do the work.
For enterprises migrating from traditional data centers to the cloud, spend management is a lever that they don’t have in the data center. In the data center world, you do your best to estimate what you need in a year or so from now, spend all that, and hope it meets the need. There is no turning back if you find yourself with spare capacity. This is why most tech teams operating in traditional data center environments consider data center resources as free for all practical purposes. It would be a great missed opportunity to not deliberately practice efficient spend management as you ramp up on the cloud.
Over the last two+ years of leading cloud migration at work, I’ve had a chance to look at this area very closely, and take part in building a successful cloud finance governance engine to increase cloud spend efficiency. Let me share my observations and experience.
A RightScale post from November 2017 states that about $10 billion is wasted each year across AWS, Azure and Google. Another report by BusinessInsider from Dec 2017 proclaims that “companies waste $62 billion on the cloud by paying for capacity they don’t need”.
These are staggering numbers for sure. In my experience, we can’t quickly project such numbers at the enterprise level without producing a bottom up baseline through exercises like zero-based budgeting applied to every workload. These are time consuming activities involving testing every workload for the best price-performance ratio. Even predictions produced by tools like AWS Trusted Advisor and cloud cost dashboarding tools like CloudHealth fall short in reality as these tools lack the context of the workload. Consequently, most dev teams don’t often pay enough attention to these predictions.
Furthermore, detailed billing reports like the AWS Cost and Usage Report provide a wealth of detailed billing records. Here a few sample billing records.
Depending on your scale and activity, you might see millions of records like these every day. These records span over tens of resource types and product families, and tens of thousands of usage types. As new features are introduced, and as your adoption grows, the volume and detail, and hence the complexity of these records, also grows.
On one hand, having such a wealth of data shows the true power and potential of on-demand pay-per-use model of the cloud. This data can help you understand the implications of your architecture choices, be able to correlate workload patterns with costs, and make price-performance trade-offs. Insights from this data, when gained, can help bring cost awareness to the engineering culture.
On the other hand, most billing tools available today mainly focus on providing dashboards with high level metrics, but not many insights. For example, in one particular case, recommendations from Trusted Advisor showed significant potential savings in certain areas, while analysis of the raw billing data revealed much bigger opportunities elsewhere. The latter required deeper understanding of the billing data to spot inefficiencies.
Developing a deep and solid understanding of billing records is an engineering problem that consumes time and investments. It’s like understanding operating system level metrics. It is not optional to not understand such metrics. You’ve to build tools to process the data, visualize, and then derive insights. You can not also centralize all this to one particular tech team or a finance team, as you need every team spending on the cloud learn to gain their own insights. Such insights need to complement performance metrics to gain awareness of price for performance. This is why I believe that cost awareness must be part of the engineering culture, and it starts with developing an understanding of billing data.
Unlike other drivers like dev agility, availability, and security; cost related practices often tend to be reactionary. Cost concerns come to the front seat only when there is a sense of urgency to reduce cloud costs. Otherwise, cost concerns get left in the garage back home. Whenever there is a realization of cost increases beyond budgets, organizations scramble to conduct optimization exercises, and when the dust settles, go back to the business as usual.
Part of this is due to Problem 1 above, which is not looking at the billing data, and/or not gaining enough insights from the data, and thus not being able to incorporate cost awareness into the engineering culture. The remaining of it is due to the holy trinity of cost, speed, and quality.
In order to produce an outcome of a certain quality, at any given time, you can either move fast while spending more, or move slow and be efficient. You can’t maximize all the three at the same time. The key question to ask therefore is, how much cost inefficiency are you willing to tolerate for a given amount of quality and speed.
Though we hear about wastage on the cloud from reports like those I cited above, apart from a few “how we saved such and such by doing so and so” blog posts, we don’t hear much about building sustainable practices of spend management and governance; and most importantly real stories about failures.
There is a reason why. Most of us mentally equate having to optimize cloud spend to the business not being healthy. We compare it to other usual cost cutting measures that most companies take at various points in their cycles, such as letting people go, avoiding business travel, reducing discretionary spending etc, shutting down offices etc. Though more experienced managers and leaders see these as natural acts of cost governance, common perception remains otherwise. A shadow of stigma follows cost optimization.
However, most successful companies build cost governance into everything they do, whether it is hiring, business travel, discretionary spending, or technology related spending. Cloud costs are no different. Acknowledging that cloud spend is a variable that you can manage, that you must maintain the spend at a certain efficiency, and removing the stigma from cost optimization are essential to building a culture of cloud cost awareness.
Wherever there is a stigma, there is FUD. I’ve heard stories of cost optimization practitioners in the wild that make claims like “we will show you how to save $XXX, just give us $Y”. One friend once shared a story of a consulting team proposing to optimize for a percentage cut of the savings realized. Do you remember “termination assistance” from Up in the Air (pun intended)?
Such approaches might make sense in places where cloud adoption is not strategic, and is treated as a utility, like a third party maintaining your corporate media web site. However, these approaches don’t produce sustainable results for anyone running serious workloads on the cloud. While not rejecting the need to seek help, you’ve to equip yourself with tools, automation and cultural changes. This is the philosophy behind DevOps — you make teams autonomous and hence accountable for development and operations for higher team performance. The same goes for cloud costs too.
This brings me to commonly dreaded term “governance”. Instead of treating cost optimization as a necessary evil, what we need is a practice of cloud finance governance and operations. Optimization is a part and parcel of governance. Here is how I describe cloud finance governance.
Cloud finance governance is pushing for responsible spending practices, and introducing checks and balances. It is about learning to operate spend management levers to trade between between speed, cost efficiency, and sometimes even quality.
Governance is not a bad word. Governance is not bureaucracy. Governance is not introducing roadblocks. When done right, governance is empowering, rewarding, and helps us exercise new muscles. Governance is what responsible families, cultures, societies and businesses must do in order to be adaptable and be resilient.
While prescribing a general purpose blueprint for how to practice cloud finance governance is tricky as each organization needs to determine what works best for them, below are some of the essential building blocks.
You can’t govern and optimize what you can’t measure. I was in situations with charts showing large unallocated cloud spend on a big screen in front, and struggling to explain where that money was going. You can’t optimize, let alone govern, if you don’t know who is spending what. Resource attribution to people and teams is fundamental to operating successfully on the cloud for cost as well security reasons.
There are several techniques to consider to maintain high percentage of attributed costs:
The next step is to gain insights from the billing data. Insights aren’t easy and automatic. I approach this step by first observing cost and usage data across different dimensions (time, regions, accounts, dev/test/production environments, resource types, usage types, instance types, allocated vs unallocated costs, etc), asking questions, making hypothesis, and validating those hypothesis. This is an iterative process over time.
In order to do these, you need access to the raw billing data, stored and indexed in a form that allows fast and easy queries. At work, we built a data warehouse using Redshift and ElasticSearch for billing data. This system loads raw billing data as soon as it lands in S3, merges it with the metadata of people and applications, and loads into an ElasticSearch cluster for queries and visualizations. This process helped us several times to improve our overall understanding of costs, efficiencies and inefficiencies, and areas of improvement.
While we like to automate everything, in reality, automation is never complete, and the degree of completeness varies by what you’re optimizing your automation for.
You may, for example, optimize for speed and availability, and decide to leave older deployments for a week or to allow for rollbacks. You may optimize for performance for your analytics workloads and decide to keep all your offline data in the S3 standard access class, and run the compute on pricey instance types. You may have a bug in your automation that forgets to propagate tags from EC2 to EBS, thus increasing unattributed costs. Your teams may have forgotten to upgrade some legacy EC2 instances that may be pricier for the same performance. I’ve seen all such scenarios and more that lead to waste.
You can improve hygiene by crafting policies (such as “all unattached EBS volumes shall be deleted after 48 hours”), and then automating those policies.
Remember that cloud spend is not a fixed sunk cost. It is a variable expense that can you manage. There are several levers possible:
Furthermore, if you’re still running in the hybrid mode with some apps serving traffic both in your data centers and the cloud, another lever may be to shift traffic one way or the other to balance between variable cloud costs and fixed data center costs.
Forecasting is another important aspect of developing a finance governance process, particularly for those moving workloads from the data center to the cloud, or those building new systems. During such phases, cloud spend tends to increase at a higher rate than in the steady state. Forecasting is less reliable during such phases as you may not have past data to build forecasting models. Regardless, you can correct for this by forecasting more frequently — ramp up some volume of traffic, build models for forecasting, forecast, then ramp up more. Also read Cloud and Finance — Lessons learned from a year ago on this topic.
Forecasting is what you expect to spend in future based on your cloud adoption plans, your team velocity, and the architectures your team is building. The budget tells you how much is being set aside for that area of spend. The difference should tell you how to tweak the plans, architecture, and levers you can exercise to meet the budgetary goal. Usually finance teams determine your budget.
Lastly, incorporate all cost related metrics, and insights into your periodic operational reviews. Most teams use such rituals to review overall KPIs of applications, and the status of projects the teams are working. Add cost related topics to the same. This is a place to observe billing data, to ask questions to develop better understanding of the data, to identify ambiguous areas, and to keep on improving the governance muscle.
To reiterate, cloud gives you many levers to manage costs. Discovering and exercising those levers requires thinking of how to govern cloud costs, and building the automation and processes to develop insights into costs, creating spend management levers, and knowing how to make cost vs speed vs quality tradeoffs.
A blog on tech and leadership
468 
6
Some rights reserved

468 claps
468 
6
Written by
See https://www.Subbu.org
A blog on tech and leadership
Written by
See https://www.Subbu.org
A blog on tech and leadership
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alfianlosari/serverless-node-js-rest-api-with-google-cloud-function-firestore-d7b422f58511?source=search_post---------330,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alfian Losari
Jun 6, 2018·5 min read
You can also read this article in my Xcoding With Alfian blog website using the link below.
www.alfianlosari.com
Serverless Application Architecture has been gaining massive popularity in recent years because as a software engineer we do not have to worry about configuration, deployment, and scalability of the application. Instead of spending time doing those activities, we can focus our time on building and testing the application logic for better user experience.
In this article, i am going to focus on demonstrating how to build a simple CRUD node.js Express REST API using Google Cloud Function and store the data using Firestore NoSQL database. Both of them are fully serverless and provided by Google Cloud as service with free tier.
What we will build:
Our note API is a straightforwared Express router object with methods for GET all notes, GET single note by ID, POST to create note, PUT to update note by ID, and DELETE to delete note by ID.
To access Firestore we import admin from firebase-admin module that we add in our package.json NPM dependency. Because our app runs on Google Cloud Platform with the same project as our Firestore, we can just initialize the admin with default credential. If we run our app in other server, we can create and import JSON credential and pass it to initalize the Firestore. We store the firestore db object in db constant to access later.
How do Firestore model and store data?, according to Google Firestore documentation:
Following Cloud Firestore’s NoSQL data model, you store data in documents that contain fields mapping to values. These documents are stored in collections, which are containers for your documents that you can use to organize your data and build queries.
To sums it up, basically Firestore stores JSON document inside a collections, the JSON type can be String, Number, Nested objects. The cool things about Firestore we can also store subcollections within documents to build hierarchical data structures and it scales as the database grows.
For our note app we will store the note inside collection we called “notes” and our document JSON will only contain text with the value of String.
We can access the collection by passing the name of the collection, then we can use the add method passing the JSON data we want to add. This automatically generates unique ID to our document.
To query all our notes document inside the notes collection we simply call get() on collection object. It returns a snapshot of document reference that we can iterate.
To query a single note document inside the collection we can use the doc method passing the ID of the document.
While the query is not as powerful as mongoDB, Firestore provide many quite powerful complex queries such as compound queries and many others. See the documentation about querying complex data.
To update note we can use the doc method passing the ID and set method passing the data. We are passing the merge true options so if the document has another fields we are not overriding another fields, just the text field.
To delete note we can use the doc method passing the ID and delete method.
Here is the snippet of all the code inside our Router:
The Express app is very simple and straightforward, it assign some middlewares like cors, express 4.1+ json parser, and error handler. We also import our note Router and assign it to the /api route.
To deploy our Express app to Google Cloud Platform, we need to create an index.js file that will be used by the Google Cloud CLI as an entrypoint when deploying the function to GCP. We need to export all the function we want to expose, the function must accept 2 parameters, which are Request and Response object, they are compatible with Express request and response object so we can import our Express app and use it right away. It is assigned to a note constant because each function deployed must have a name.
One important note, Google Cloud Function currently only runs on Node .JS 6.1+ environment so we can’t deploy ES7 based source because Node 6 does not support many features like async await. Make sure to transpile all of the sources using Babel transpiler before deploying.
Make sure to Install Google Cloud CLI and beta components because Cloud Function is still in Beta, then type:
It will take about 2 minutes to deploy, after it completes, the url to access the function will be provided or you can access it from the Google Cloud Console. In this case to access the API we call https://us-central1-alfianlosari-cd236.cloudfunctions.net/note/api. We can also access the Google StackDriver Logger and Error Reporting from the Console to see all the log activities and error traces on our API.
There are many ways that we can use to deploy our functions, that you can refer in the deploying documentation.
I hope this article serves as a guide for all of you that want to deploy a simple backend service without knowledge of dev-ops and its complexity. You can access all the source code in the project Github Repository.
Serverless and Microservices are really the future for backend development because of the simplicity and rapid development productivity. There are still many improvements that need to be addressed, but i really believe Serverless is the future!.
Mobile Developer and Lifelong Learner. Currently building super app @ Go-Jek. Xcoding with Alfian at https://alfianlosari.com
532 
4
532 
532 
4
Mobile Developer and Lifelong Learner. Currently building super app @ Go-Jek. Xcoding with Alfian at https://alfianlosari.com
"
https://medium.com/google-cloud/professional-cloud-architect-certification-6a6dfa5c6ff5?source=search_post---------331,"There are currently no responses for this story.
Be the first to respond.
TL;DR: I recently went through the preparation and exam of Google Cloud’s Professional Cloud Architect Certification. It was great learning experience and I highly recommend it. You can register here to get certified yourself!
As you might know, I’m a Googler and a Developer Advocate for Google Cloud. Why do I want to be certified by Google Cloud when I already work at Google and know a great deal about Google Cloud? I had 2 main motivations:
I also learned that Cloud Professional Certification recently ranked #1 in 15 Top-Paying IT Certifications for 2019. Good to know!
Professional Cloud Engineer Certification page has a details page. In this page, there’s a Path to Success section that lists exam guide, training courses on Coursera and Instructor-led and practice challenges with Qwiklabs. It’s great that all the resources are listed in this single page in a clear format.
I decided to go with online courses for more flexibility. Coursera has a Specialization on Architecting with Google Cloud Platform. It has 6 courses. My plan was to finish these 6 courses first and then go through exam guides before registering for the exam.
Later, I realized that there’s an additional course specifically on the exam itself. I went through that as well. In total, I completed 7 courses over roughly 7 weeks to prepare for the exam.
I usually get bored of online tutorial videos because they’re either too long or too hands-off to keep me focused. Going through the courses on Coursera was quite fun. Videos were the right length and had enough level of detail. They were not only informative but they also contained hands-on exercises with Qwiklabs. That made videos more relevant and courses more engaging for me.
I wasn’t expecting to learn much new information due to my current role at Google but I was pleasantly surprised how informative some videos were for me, especially the ones in networking and general architecture.
Hands-on exercises with Qwiklabs were great too. The only slight glitch was that Coursera sometimes had difficulty in registering that I finished the lab. As a result, I had to do some labs twice and that was somewhat annoying. Also, having to re-login with new credentials for every lab gets a little tedious but not a big issue.
Let me go through courses briefly, so you can get an idea on what to expect.
Start date: 2019–01–13, End date: 2019–01–20
This was the first course I went through. It was a good introduction to Google Cloud, explaining all the different services and options. I liked the videos overall. They weren’t too long or short. I also liked the fact that they had quizzes and Qwiklabs hands-on exercises.
Start date: 2019–01–20, End date: 2019–01–26
This course was much shorter than Course1. I found some of the content in Module 1 redundant because they were already covered in Course1 but good to have some repetition, I suppose.
Some of Module 2 (VPCs and subnetworks) is already been covered in Course 1, however, later parts of the module had good info that I didn’t know about.
Start date: 2019–01–27, End date: 2019–02–02
Module 1 of this course was all about IAM. This was already covered in Course 1 but it was more detailed, so I learned quite a bit. The lab exercise was very nice as well. It was long enough to help me understand IAM but not too long to make me bored.
Module 2 was all about storage. It started with Cloud Storage. It was a review of Course 1 with additional information and exercises.
Module 3 was about billing and Module 4 was about Stackdriver. Not most exciting topics for me but useful nevertheless.
Start date: 2019–02–04, End date: 2019–02–09
This was probably my favorite course because it contained a lot of networking topics that I didn’t know in detail.
Module 1 started with VPNs and Module 2 was about HTTPs load balancing. Videos were very nice and diagrams explaining how it works was very useful. I also liked their best practices section. This whole section was very useful and probably the best part of the course for me.
The next three modules were mostly review of what was already covered in Course 1. Module 3 about autoscaling, Module 4 about infrastructure automation with Cloud API and Module 5 about Deployment Manager.
The last module, Module 6, is about Managed Services such as Dataflow, Dataproc and BigQuery at a high level.
Start date: 2019–02–10, End date: 2019–02–10
This course got into more application development side of things. Module 1 was about Cloud Pub/Sub, Cloud Endpoints and Cloud Functions. Module 2 was about App Engine Standard and Flex. They were both good overviews but there was no hands-on lab.
Module 3 was about Kubernetes Engine and Container Registry. It was a good module with a hands-on lab.
Start date: 2019–02–14, End date: 2019–02–17
This course was the only course in the series that is about software architecture rather than specific Google Cloud products. I liked the first part of the course but later when it started talking about data layer, it got repetitive because I already heard about Cloud Storage, BigQuery, etc. many times before in the previous courses.
Start date: 2019–02–23, End date: 2019–02–24
This course wasn’t part of the specialization but more about the logistics of the exam and going through the case studies one by one to figure out solutions. I liked it quite a lot. Lots of useful information that you wouldn’t normally get from documentation. It turned out to be much more relevant for the exam than I thought (more on this in the next section).
Towards the later part of the course, it turned into more of a lecture than case studies. At this point, it got a little repetitive for me because it started talking about Kubernetes Engine, App Engine etc. or things like BigQuery, Dataflow. All of these were covered in other courses.
I took the exam on 2019–02–28. The exam consists of 50 multiple choice questions and you have 2 hours to answer them. I thought I’d be done in 1 hour but to my surprise, it took me 1 hour 45 minutes to finish the exam and it was harder than I expected.
There were questions from the case studies. I spent quite a bit of time reading the case studies. In hindsight, I should have read the case studies more carefully while studying for Course 7. In general, I should have spent more time on Course 7, it seemed the most relevant course for the exam.
I finished the initial pass of the questions in about 1 hour and I marked about 20 questions as unsure. I did a full second pass in 35 minutes and I still had about 10 questions as unsure. I finally spent 10 more minutes to review the unsure questions one last time and I submitted. I got a provisional result that I passed.
On the exam itself, I think it’s a moderately difficult exam. There were about 5 or 6 questions that were quite easy. The rest, you either had to read the case study well or know the material inside out. I realized that I didn’t remember some of the networking stuff (Direct Peering, CloudVPN etc.) and also some BigQuery details like federated vs. partitioned tables. As a result, I wasn’t sure of the answers.
I found some questions quite tricky. For example, I felt that there was a question where normal or preemptible VMs would be OK in the solution, but I had to choose one. Not sure in the end what I missed.
Overall, I’m very happy I went through the process of getting certified. It was definitely more challenging than I thought it would be. It’s a time commitment with all the courses and the actual exam itself. However, you end up learning a lot, even if you already know a lot about Google Cloud.
I noted some of the improvements we can make in courses and certification exam and I will follow up with our certifications team to make sure the experience gets even better going forward.
Google Cloud community articles and blogs
233 
6
233 claps
233 
6
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Developer Advocate at Google
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://itnext.io/introduction-to-quarkus-cloud-native-java-apps-e205ae702762?source=search_post---------332,"Java and the JVM are still the most popular programming languages but for Serverless and Cloud Native Microservices, its usage is declining due to the heavy memory footprint and slow boot time required for short lived containers; this is now about to change thanks to Quarkus.
As I get more into DevOps,Containers and Serverless; I find myself writing my containerized code in lightweight containers or FaaS using Python or JavaScript. Java it is just too slow to boot up to use in a Serverless framework and…
"
https://medium.com/@olamilekan001/image-upload-with-google-cloud-storage-and-node-js-a1cf9baa1876?source=search_post---------333,"Sign in
There are currently no responses for this story.
Be the first to respond.
Olalekan Odukoya
Oct 30, 2019·11 min read
In this relatively short tutorial, I’ll be showing you how to upload an image to Google Cloud Storage and also how you can resize the image.
As developers, one of the things we shouldn’t do is to always store an Image to our database as this could open up our application to all sort security vulnerabilities, it also eats up unnecessary space in our database since they are stored as a blob file, and finally, it could lead to an increase in the cost of backing up data. Solutions to these problems could either be to Store our images in the file system, and create links/pointers to these images in the database but this could be slow when we need to fetch those images. Another way this problem can be solved is to serve our files with a CDN (Content Delivery Network). A Content Delivery network is often used to accelerate the effectiveness and efficiency of serving contents from a server to the clients by caching them and making them available to the clients as quick as possible. However, serving just images with a CDN can be very expensive, and the thing is, since CDN tends to thrive by caching contents, you might have uploaded a new image or file but it wouldn’t be available to the users until you clear the cache. The best option is to use an online file storage web service on the cloud.
The Cloud is just a remote server sitting somewhere, where you can store your content and access them quickly, easily, securely, and from anywhere in the world. One of the advantages of using the Cloud is because of how elastic, accessible, and secure it can be. A lot of companies offer these cloud-based services including Tech Giants like Amazon and Google.
In this tutorial, we will be using Google’s cloud-based service known as Google Cloud Platform. Google Cloud platforms offer a range of cloud-based services but the one we are going to be focusing on is going to be Google Cloud Storage. Google cloud storage is a service on GCP that allows you to store static files like images and files. The advantage of using this is that instead of storing say an image in your database, you instead, upload the image to GCS and get a link/url to that file in which you can then store into your database hence, saving disk space in your database and reducing time spent in trying to backup your data.
Before we can start uploading images or any files, we need to first create an account, to do so, copy and paste the URL below into your browser and click Enter. This takes you the Google Cloud platform’s website.
After the page loads, you should see something similar to what I have below.
Click on the “Go To Console” Button. This takes you the Cloud Console. Which looks something similar to what we have below.
You’ll notice that I painted a part of the picture, click on that part on your screen. This redirects us to a page where we are being asked to create a project. Give it a name and click on the create button. This action creates a new project for us in which we can associate our GCS project with.
Yours might be way different from mine if it’s your first using GCP.
In the search bar, type Cloud Storage.
After doing so, you should be redirected to a page similar to what we have below.
You’ll notice that the “Create Bucket” button isn’t active, and also there is a button that is asking us to “Signup for free”. Click on the “Sign Up Button” and fill all the necessary information. You might also be asked to enter your Credit/Debit Card Details before you can be able to complete the “Sign Up exercise”.
After you are done, the “Create Bucket” button should be active, click on it, you should see a modal similar to what we have below.
If not, just give your bucket a name. Then, keep hitting the continue button until you get to the last select box. Finally, click on the create button.
After, completing the clicking exercise, you should be redirected to a page similar to what we have below.
Click on the create button to finally create a Bucket. A Bucket is basically a place on the cloud where all of your data are being stored. According to the Google Cloud documentation, a Bucket is said to be a “Container that holds your data.” To read more about Buckets visit the link below.
That being done, we can now move to our development environment and start writing some code!
To set up your dev environment for this little project, open your terminal or console and type in the commands below.
if you are using Visual Studio Code as your Code Editor you can type the command below after the initial ones to open up the Editor.
Once you are in your directory and have confirmed, type in the command below to initialize your project.
The command below is often used whenever a new project is to be started in Node.js. This allows us to keep track of all install dependencies when have used in developing the project. The result of running the command is shown below. A json file with some meta-data about our project.
The next thing we need to do is to install all necessary dependencies our app needs for it to function. In your terminal, enter the command below to install a couple of dependencies we need to start our app’s server.
We need express for creating a server that is going to power our application, body-parser for parsing requests whenever it hits our server, and multer for parsing files which must have been part of the request object.
After successfully completing the action, a node_modules directory is being included in your project directory. This folder contains all the dependencies our application needs for it to be able to run.
After you are done, in your root directory create a file and name it index.js. This is where we are going to put the code which is going to create a server for our application. At this rate, your application’s project structure should look similar to the picture below.
In your index.js file, paste the code below into it.
After you’re done with that, run the command below to start your server.
This would create a server that listens for requests on port 9001. To kill the process, click on ctrl + c at the same time.
Starting our app with node directly would not be efficient as when we make changes to the app when we have to manually kill the server, and then restart before it can be able to notice any changes in our application. A solution to this problem is to install nodemon which we can use to automatically start our project anytime we make a change to any file in our project. To install it run the command below in your terminal.
This installs it as a dev-dependency in the project. Now, go to your package.json file and change it’s contents to what is in the snippet below.
You’ll notice that in the script key, a new set of key-value pair has been added to it. Now, go to your terminal and run the command below to start your application server with nodemon.
You should see something similar to what we have below.
Now that we have a server running, we can now start making requests in order to push an image or file into our bucket. But to do so, we need a dependency and also some certain credentials from our bucket to do so.
In your terminal, type in the command to install the google cloud storage dependency which is going to help us with the image upload.
Create a folder in the root of your project and name it assets. This is where we are going to put the files and images we want to push into our bucket. Once you are done with that, put a random picture in it and go back to the GCP console. We need to create a service account key which gcs is going to use to authorize us to put a file in our bucket.
On the GCP console, search for API, click on the api & services option. You should be redirected to a new page. Scroll down and click on the storage link
After clicking on the link, you will be redirected to a new page. At the left section of the page, click on the credentials button, afterward, click on the add credentials button as shown below.
Then, click on the service account option. Fill in the forms and then click on the create button as shown below.
You should be redirected to the create role section. Scroll down or search for storage. Select it and also select API KEYS ADMIN.
Once that’s done, you are once again being redirected to another page. Just click on the create key button as shown below.
Click on create
After it is being created, an automatic download of the account key is being automatically downloaded. You need to keep this file very safe.
In the previous section, we created a service account key and also downloaded it on our machine. So, we need to make use of this file before we can be able to do anything. In your root directory create a folder and name it config. Copy and then paste the JSON file we downloaded into it. Also, in the config directory, create an index.js file and paste the code below into it.
What we have done here is basically a configuration of the @googale-cloud/storage dependency we installed earlier on in the project.
After doing this we can now start with the core functionality of our application, which is image/file upload.
In the root directory create a folder and name it helpers. Then, create a helpers.js file in it. We are going to use this file to create some helper functions that would help us with the image upload. However, replace the index.js file in your root directory with the code snippet below.
You’ll notice that our app has changed. We actually haven’t done anything much, all we have done is just to use the other dependencies as a middleware in our application.
Proceed to the helpers.js file and paste the code snippet into it.
What we have done here is to first import the @google-cloud/storage we initially configured and then we proceeded in linking it to our bucket. Afterward, we created a function that returns a Promise. The function expects an object which contains the blob object. We then extracted the file name and the file (buffer) from the object. We clean it up, then, create a read stream. Once it is done writing the stream, it returns a URL to the image.
Once that is done, go back to your index.js file and replace the existing code with the code snippet below.
Open up POSTMAN, click on form-data, then, type file (it is important that the key is named file since we already configured multer to grab our file in the files key) under the key column, and select a random file on your system then, finally, make a POST request to…
if that’s successful, you should see a link to the image.
However, at this rate, the image would not be publicly available. You need to go back to the gcs console and edit permission of the file.
Go to permissions => Add Members => storage object viewer.
Once that is complete, the file can now be publicly available.
In this relatively short tutorial (lol) we can see that gcs have made file upload easier. With few lines of code, you already have your file uploaded. It also has a very secure system for authorized users who can have access to the files in your bucket.
The application’s source code can be found…
Also, if you’d like to ask me further questions, feel free to hit me up on Twitter.
To learn more about gcs, check out the docs here
You can also try this awesome library for the file upload in case you do not want to go through the stress of writing the code all over again.
Arigato!
Data Science, Tech, and Finance.
902 
15
902 
902 
15
Data Science, Tech, and Finance.
"
https://medium.com/glucosio-project/whats-new-in-firebase-cloud-messaging-and-how-to-migrate-from-gcm-578019c2167d?source=search_post---------334,"There are currently no responses for this story.
Be the first to respond.
At I/O 2016, Google turned Firebase into an unified mobile platform and migrated to it some existing services service like Google Cloud Platform, adding some new interesting features.
Firebase covers all that mobile developers need to build, maintain and scale a mobile app on all platforms (even on iOS): from Storage and databases to innovative tools like Remote Config and Test Lab.
Today we’re going to explore Firebase’s Cloud Messaging Platform, based on the precedent Google Cloud Messaging service.
Firebase Cloud Platform inherits GCM’s core infrastructure but simplifies the client development. Developers no longer needs to write their own registration or subscription retry logic. Now, you can integrate FCM in your app with just a few lines of code.
Plus, FCM includes a web console called Firebase Notifications that reminds me of Parse Push Console (RIP, :/).
However, GCM is not deprecated: Google will continue to support it but all new client-side features will be available on FCM SDK only. The protocol remains the same server-side though.
Adding FCM to your app is very easy. First add FCM gradle dependence:
Then you need to write a service that extends FirebaseMessagingService that will handle notifications when the app is in the background. Another service, FirebaseInstanceIdService, will handle the creation and updating of registration tokens.
All of this goes in AndroidManifest.xml of course.
As I said before, Firebase creates a token for each client app instance. You’ll need this token if you want to target and send customized messages to specific devices from Firebase’s console. To get this token, you can simply override the method onTokenRefresh and send the updated token to your server each time.
Be careful because FirebaseInstanceID.getToken() returns null if the token has not yet been generated.
Open your Firebase Console and click on the Notifications tab.
Select Send your first message (\o/). You’ll now be able to edit message title and label and select the target, as well as the delivery date. In fact, you can even pick a time and date and send the notification automatically (you can also send the message according to the receiver’s timezone).
You can target your users by language (you’ll also get the estimated number of users for each language) or/and for app version. Selecting audience you’ll be able to target only the users who purchased something on your app or the ones who didn’t.
Finally, you may want to send a message to a specific device: click on “Single Device” and insert the token you received with the method I wrote before.
Then, in the Conversion tab you can track the users that purchased something right after they read the message. As you can see, Cloud Messaging is perfectly integrated with Firebase Analytics to help you monetize.
Other Advanced options include setting the notification priority, the expiration period, and if you want to play a sound when the notification is received :)
We managed to integrate some Firebase services like FCM in Glucosio in just one day, so I’m sure you can do it too in the same time for your apps.
If you’re interested in Glucosio, a free and opensource tool for diabetics, visit this page. We’re always looking for new contributors.
Happy coding and have fun!
A free and open source project, Glucosio aims to help…
249 
2
Some rights reserved

249 claps
249 
2
A free and open source project, Glucosio aims to help people with diabetes.
Written by
Android Engineer @ Nextome. Lead @GDG Bari. Pursuing Master in Computer Engineering at PoliBa. Big #OpenSource supporter and #Kotlin fan.
A free and open source project, Glucosio aims to help people with diabetes.
"
https://medium.com/@GaryHarrower/working-with-stripe-webhooks-firebase-cloud-functions-5366c206c6c?source=search_post---------335,"Sign in
There are currently no responses for this story.
Be the first to respond.
Gary Harrower
Jun 25, 2018·7 min read
What is Stripe?
Stripe is a modern payments platform that allows you to easily accept payment on your web services. This tutorial is aimed at people who either already have Stripe integrated or are in the process of integrating Stripe and are curious about using the webhooks.
We’re going to look at setting up Stripe Webhooks and Firebase Cloud Functions, saving our Stripe events into the Firebase Realtime Database with a quick example of how you could take things further and trigger Firebase Cloud Function database triggers.
Why use webhooks?
Stripe webhooks can be used for many different reasons. You could use them for populating real-time reports/dashboards, monitoring, updating customer information in your database, alerting users on declined subscription renewals and a whole lot more.
What is Firebase?
Firebase is part of Google Cloud Platform and gives you access to several useful tools and services such as hosting, databases, cloud functions, analytics, storage and more.
In this tutorial, we’re going to use Firebase Functions and Firebase Realtime Database, but check out the Firebase website to see what else they have to offer.
The Free plan on Firebase gives you access to 125K Cloud Function invocations per month and 1GB of storage in the Realtime Database which is great — and even if you need more than that, the prices are still incredible.
Let’s get started
To get started, we first need to head over to our Firebase console and create a new project. I’m going to name mine Stripe Webhooks Tutorial, but you can name yours whatever you like.
Now lets head over to our terminal and create a new directory for our project.
We then need to install the firebase-tools package globally using NPM.
Now we have firebase-tools installed, we should login to our firebase account and initiate a new project in our project directory. You will get a choice of which services you would like to use — you only need to select ‘Functions’ here for now.
Let’s move into the functions folder (created with firebase init) and make sure we have the latest version of firebase-functions and firebase-admin installed. firebase-functions is used for running the actual cloud functions and firebase-admin is used to access the Realtime Database.
Let’s create a new function called events to handle our endpoint. We’ll start off simple with a basic function that returns a string to let us generate the URL we need to supply our Stripe account with.
We can then deploy our function and get our endpoint (Function URL in output in screenshot).
Now we have our Cloud Function URL, we can head over to the webhooks section of the Stripe dashboard. We then want to + Add Endpoint, and enter our URL into the URL field. You can select the types of events to be sent, but for now we will just stick to all event types.
Once you create the endpoint, take note of your ‘Signing secret’ — we’ll need that to verify the request send to our function.
While we’re in our Stripe Dashboard, let’s head over to the API Keys section to generate and take not of the API key we’re going to use.
You should create a restricted API key and only assign permissions you’re going to need in your firebase project. For example, if you’re only going to read customer objects, you can specify only Customers when creating the key.
Now we have our signing secret and our API key, let’s add them to our Firebase project as environment variables so we don’t need to check them in to any source control.
That’s our setup complete — We’re now ready to write some code! I’m going to be using examples from Firebase and Stripe, so if there’s anything you would like to dive deeper into, you can use the following links:
To start with, we’re going to need the Stripe NPM package, so let’s go ahead and install that:
We added our API keys to our Firebase config, so we can access them using functions.config() (For example: functions.config().keys.webhooks will return our keys.webhooks string we added).
We will then require the Stripe package in our functions index.js. We will also bring in our Signing key to our application (endpointSecret).
Note: Stripe marks a webhook as successful only when your function returns a success (2xx) response. If it receives anything else, such as a 400 or 500, then it marks it as failed, and will try again.
We can use our signing key to verify that a request has actually come from Stripe, and not an unauthorized attacker. The stripe package has a method (stripe.webhooks.constructEvent) which we can use to verify the request. We can also use a Try Catch to return an error if the request fails verification.
Note: We need to use the original request body otherwise the verification will fail, so we must use Firebase Function’s request.rawBody, instead of the usual request.body.
As mentioned, we can wrap this in a Try Catch to catch any failed requests.
Now we have our valid events, let’s save them to our Firebase Realtime Database.
We can do this by using the firebase-admin database methods. We’re going to be using the .push() method to create a new entry in our database.
Let’s break this code down a bit.
Now we should deploy our function again, then we can test it out.
Let’s head back over to the Stripe Dashboard Webhooks area, select our endpoint where we can now ‘Send test webhook’.
Select an event type and hit ‘Send test webhook’ — all going well, we get a successful response, and our event is now saved in our database!
That’s it in terms of saving. Now you have endless possibilities of cool things to do with your data. For further reading, you could explore the different triggers Cloud Functions can use. You can run another function whenever anything is added to your database. The example below would run any time a new event is saved to our database. We could now check the event type, and if it’s a successful charge, update our Realtime database to increase our daily revenue on our live dashboard…
You can read more about database events here: https://firebase.google.com/docs/functions/database-events
I hope this was useful. Please leave any feedback or questions - I’m always learning and really appreciate any comments you may have.
You can find the completed code over on Github — https://github.com/GaryH21/Stripe-Webhooks-Tutorial
Happy building!
Senior Software Engineering Manager at Edinburgh Airport. I love making things, learning and sharing knowledge.
See all (777)
889 
3
889 claps
889 
3
Senior Software Engineering Manager at Edinburgh Airport. I love making things, learning and sharing knowledge.
About
Write
Help
Legal
Get the Medium app
"
https://faun.pub/approaching-cloud-native-2903a253b8f9?source=search_post---------336,"There are currently no responses for this story.
Be the first to respond.
Continuous development, testing, integration, and delivery are, for sure, amongst the important pillars of DevOps, but if you want to create a healthy DevOps strategy, one of the necessary and vital things to do is using the power of IaC and Cloud Computing.
In other words, becoming “Cloud Native”. This is what we are going to see in the following analysis.
IaC is simply managing hardware and physical resources using code.
IaC is not cloud computing; both concepts have different meanings, even if there is a deep link between them.
The elasticity of cloud resources and the disposability of cloud machines make IaC meaningful. In a single press of a button, you can create hundreds of provisioned machines. The same thing would need a team of system administrators some years ago.
Note that cloud computing does not necessarily mean AWS or GCP; you can build your own cloud too.
If we look into the simplest concepts in DevOps like continuous integration and delivery, we automatically think of delivering small chunks of software to be built, tested, and deployed.
Tests are run against an evolved version of the software that is not really very different from the previous one. In other words, “the diff is not huge”.
Some companies with technical maturity tend to deploy hundreds of times a week. Github deploys dozens of times a day.
SlideShare deploys up to 20 times a day.
Other companies like Amazon make more than 20k deployments per day (source ITRevolution.com)
This means every 4 seconds there is a new deployment.
Don’t be upset when you compare your company deployment frequency to the frequencies above; it is not just about maximizing the number of daily deployments, not a race.Whether you deploy 10 times per day or 100 times per second, cloud computing is a must because it can be leveraged using code.
It allows you to “rent” the computation, storage, networking, and the necessary resources to run ephemeral testing and staging environments that disappear when you don’t need them. You can interface with your cloud provider using your favorite language (or DSL).
The Things You Own End up Owning You ~ Tyler Durden
I don’t know if you watched Fight Club, but this quote is one of the things that marked me in that great movie.
We are neither talking about the “having vs. the being” nor seeking the paths of wisdom here, but if you apply the same quote to your on-premises infrastructure, you will understand that “The Server You Own Ends up Owning You”.
The advantage of cloud computing is that we never have machines, we always supply resources on demand.
If I summarize the last paragraph, I’d say two things:
The second option, which is building your own, has two drawbacks:
Most businesses will choose using an existing cloud computing provider.In both cases, using the cloud does not mean that you are cloud native. More than that, if you are using the cloud and your software engineering approaches and methodologies are not following the cloud native best practices, you are probably wasting money, time, and effort.
In 2011, Gartner identified 5 migration strategies. AWS identified 6:
Rehosting is about the redeployment of application and data in the cloud without modification (IaaS). Thinking about re-architecting the application is the next step here.
Your organization will develop better skills in using your cloud provider and can re-architect your application in an easier way, however, you will not be able to leverage the full power of the cloud
Replatforming is about moving to the cloud with a small amount of up-versioning. It is done by upgrading your existing on-premise applications and implementing cloud paradigms and best practices like multi-tenancy or auto scalability and the 12-factors app.
Replatforming is faster than refactoring, it allows taking immediate, but modest advantage of the cloud.
Repurchasing is replacing the legacy on-premise application by a commercial SaaS platform.
This solution reduces the development efforts but has some disadvantages like vendor lock-in and interoperability issues.
This is when you re-architect your legacy applications and replace them with modern cloud-native apps.
Refactoring allows companies to benefit from the full power of the cloud but this approach is riskier and requires more knowledge and work.
By looking at the whole IT system of an organization, we usually realize that there are some applications that are either no longer used or with low business value.
Retiring is phasing out these applications.
In some cases, moving to the cloud is not the best solution to your problem. Not moving to the cloud and retaining is also a strategy.
The main role of a “DevOps engineer” or a “DevOps team” is implementing DevOps within an organization until it becomes evidence.
Since DevOps is also a philosophy, implementations differ in function of one’s understanding and interpretation.
When implementing a DevOps strategy, we usually think about automation, however, we might forget that automating the wrong process is multiplying faults. For that very reason, thinking about the process should be upstream of any implementation.
To find the best process you must find the development, production and deployment bottlenecks and move them to the beginning of your process.
On the other hand, because DevOps is not a framework or a set of instructions to follow, and because if we keep things at their philosophical level, everyone will have a different opinion and the DevOps implementation approach diverges in several directions; we need standards.
There were several initiatives to create standardized technical approaches such as the Webscale, Twelve-Factor Apps, 13 Factor Apps, The Reactive Manifesto or the work done by the OCI and the CNCF.
Driven by passion, some developers and engineers think of code as the goal, however, it is not.
The code is the tool and the goal is solving business and real-world problems.
Unless you are coding a side project for fun or for learning purposes, coding should not be treated as a goal.
Adding more code is always adding entropy to your code, which means more tests, more maintenance, and most probably more bugs, so think before you code.
Code Less, Think More
A soon as there is a new project, a new problem to solve, some will jump up and create a REST API or a CRUD application, deploy it to production until the first incident happens and the product limitations start to appear: e.g not scalable, vulnerable, everybody thought about deploying and no one thought about rollback ..etc
Sometime, when you ignore traditional thinking and revise old processes, they will see positive change.
In this universe, there are laws that can be noticed everywhere … like entropy.
Entropy is “the measurement of disorder”.
The entropy of the universe increases with time, the more time passes, the more there is disorder increases if you compare it to the initial state.
The heat of a coffee left on the table will be transmitted to the cup: We say that the entropy of the cup increases.
There is plenty of entropy manifestation around us, but what is important for us is about the entropy in software and IT systems.
Software entropy refers to the tendency for software, over time, to become difficult and costly to maintain. A software system that undergoes continuous change, such as having new functionality added to its original design, will eventually become more complex and can become disorganized as it grows, losing its original design structure.
In theory, it may be better to redesign the software in order to support the changes rather than building on the existing program, but redesigning the software is more work because redesigning the existing software will introduce new bugs and problems. ~ source: Webopedia
We can outline two important concepts here:
Immutability and self-healing platforms are some of the new ways of thinking and designing applications that solve this problem. These paradigms showed us their strengths in creating stable and scalable applications at a faster pace.
Immutable: Cloud virtual machines and/or containers are never changed after deployments. When there is a new deployment, everything is re-created.
Self Healing: The system can identify its problems then resolves them, usually, by returning to an initial state.
Microservices, containers, and orchestration are the technical tools to implement these paradigms.
They also have other advantages, for example, containers coupled to microservices allow deploying a single service instead of re-deploying the whole monolithic application and containers orchestration frameworks allow autoscaling a microservice under load, instead of scaling the whole monolithic application.
When we approach containers, microservices, and orchestration, there is always an important aspect, common to the three concepts: Event-driven approaches.
Microservices, containers, and orchestration are reactive to change and act in function of events.
Cloud Native is reactive, that’s why we will discover what is the Reactive Manifesto.
This schema explains a lot about the philosophy of the “Reactive Manifesto”:
The goal of the “Reactive Manifesto” is creating responsive applications.
Regardless of the conditions, a responsive application must always have the same behavior.
To achieve this responsivity, the application must be elastic and resilient.
Without being elastic and resilient, we can not speak of a responsive application.
Finally, building message-driven applications is the key to elasticity and resiliency.
There is something at the center of everything we develop: data. There is always data input or output.
Databases were very involved in solving the data problem, but they created a lot of other problems, in particular, problems of performance and scalability.
To optimize our use of databases, we tried several solutions, such as code optimization, which is limited.
We tried caching techniques which are also limited because we always cache a small part of the whole data. This can be limiting in the case when data demand is dynamic.
We tried materialized view to solve caching problems but it adds load to the database.
We tried to use cloud databases, and they are expensive.
We tried vertical scaling and replication, and they are also expensive and have some performance limits.
We tried NoSQL databases which are not suitable for all scenarios and use cases. They may also be expensive.
Message-driven applications solve many of the above problems. In short, they are based on message exchanges. To dive into this concept, let’s get back to the history of databases.
If we take one of the most used databases, MySql (or any other alternative technology like MariaDB), we can notice that the transaction log is the heart of the database.
It records all the events that change the state of the data and it contains events: delete, create, update, insert .. etc
The database tables hold the current state representation of these events, replication is a replay of the transaction logs and even the cache layer is a partial copy of this data.
The transactions log can be seen as a proof of concept of a message-driven system.
If this log’s events are streamed by publisher components and consumed by consumer components of the same application, we can create optimized polyglot persistence stores for each data consumer. Each service can use the most suitable storage technology (SQL, NoSQL, SQLite, raw files, graph database ..etc)
We can also avoid storing unnecessary data and add a load to the database.
The good thing about message-driven applications is that it completely fits the microservices development, production and data model.
Let’s take the example of Uber, do you think that the same database model and technology are suitable for all of these three services?
Uber uses Cassandra, MySQL, PostgreSQL, and other home-grown solutions to manage their data and by using microservices they can choose the right database and data model to manage passengers without impacting how drivers’ and trips’ data is managed.
To maintain the reactivity and especially the coherence of a system composed of microservices. One could use transaction logs to inform other services of changes to a particular service.
The event sourcing is another practice that we can find in applications based on messages exchange.
When updating your Facebook profile from your smartphone, there are several bricks that need to be notified.
The monitoring application needs to detect fraud.
There is at least one database that needs to store the changes that happened.
The indexing and search datastores (like Solr and ElasticSearch) need also to know about it.
The newsfeed service needs to publish your update so it should be notified.
In short, there are multiple data stores and services that need to be notified about a change in your profile.
Imagine the case where the mobile application must notify all of these applications as soon as you change a single letter in your first name.
The Facebook mobile application will probably take a few minutes to update all these services at once.
The solution is to write the update on a message and send it once to a streaming system like Kafka. Each other service will consume the data that interests him.
Change is published once and consumed several times.
We have already seen that one of the advantages of cloud computing is providing IaC tools and environment that can be used in your cloud-native journey. The other advantage is the reliability of its services. Most cloud services have an SLA of 99.99 %.
We have also seen how polyglot microservices, reactive programming, and message-based services fit together perfectly like puzzle pieces. However, there are some disadvantages in relying on such architectures.
Everything will be down and dysfunctional when the streaming system is down. This is when cloud computing is a savior.
Since all of your application blocks rely on streaming data, the use of a system that has high availability is advisable.
e.g: Amazon Kinesis stream can be used for event sourcing.
John and Jane are preparing some cheese pancakes. This is what John followed as steps:
..and this is what Jane did:
There is a difference between the first way of getting things done: The second way is faster because of its asynchronicity.
To create cloud native applications, we should eliminate all synchronous inter-component communication.
Event streaming helps not only in creating message-driven architectures but also in implementing asynchronously within your application.
Event streaming is the basic pattern. We always have a producer and a consumer.
The first publishes a message and the consumer subscribes to a topic to receive these messages.
It is advisable to use a fully managed streaming service.
When you leave a message on the streaming system, you do not have to wait for the application that consumes that message. Therefore, the communication is asynchronous.
Event sourcing uses the first pattern (event streaming) except that there is a database that plays a role in this model.
When we talk about event sourcing, we call up the process of transforming a series of events from our streaming system to a persistent data store.
There are 2 approaches to doing this.
Event First: The publisher sends its message to the event stream, the consumer react to this event and it records it in the data store.
Database First: The user first records the state in the database, then the database propagate this change to the entire application. This can be done with CDC databases. This is not the case for all databases, generally, cloud databases are equipped with this mechanism.
Wikipedia defines the CDC as follows:
In databases, change data capture (CDC) is a set of software design patterns used to determine (and track) the data that has changed so that action can be taken using the changed data.
Also, Change data capture (CDC) is an approach to data integration that is based on the identification, capture, and delivery of the changes made to enterprise data sources.
The CQRS is based on the event sourcing pattern but separates the reading of writing.
CQRS separates reading (query) from writing (command).
The publisher records each change, usually as an event. Processing is then performed, often asynchronously, to generate denormalized data models.
The consumer simply queries these templates in order to minimize the querying of the database.
In order to better understand, let’s take the example of a user who, using a web browser, made a change in the user interface.
Any change produces an event that is sent to the event stream and is consumed by a subscriber that creates a materialized view for the consumer (reading).
In other words, the event is transformed into a state in the database of the reading.
The event is also written to the (writing) data store.
The advantage is that we can, for instance, write in a Mysql dat store and read from a NoSQL data store .
The title is a quote said by Tim Berners-Lee, the inventor of the World Wide Web.
Indeed, IT systems and architectural evolution show us today that the data didn't”t change but we changed the way we consume and produce it.
(Orchestrated) microservices is the best example that shows us how far software and infrastructure architecture has changed.
Microservices has the advantage of allowing developers to choose freely the database technology to use with a microservice or a specific operation (eg. reading vs writing in CQRS).
This solves many problems related to data like performance, replication, scalability, and complexity.
Choosing the right data store technology brings icing on the cake. With more than 300 database technology, this can be intimidating:
So, start by picking up your choice criteria like read/write performance and latency. This will get you closer to the right choice.
These are some good articles that may help you make a better choice:
thenewstack.io
www.dataversity.net
We have seen 3 patterns of cloud-native architectural patterns, there are more to see
A book that I recommend, if you want to read more about these patterns, is Cloud Native Development Patterns and Best Practices: Practical architectural patterns for building modern, distributed cloud-native systems:
www.amazon.com
These are some patterns:
Designing cloud-native applications and systems is not only about architecture and technical patterns but also about how processes are implemented and managed.
The challenge of microservice is also human.
In fact, The Conway Law is among the known laws in leadership.
This law assumes that the architecture of a system will be a copy of the communication scheme within an organization.
Besides, there are some caricatures that can be found on Intenet.
This law can be applied in software architecture to create microservices. The organization of teams and the interaction between them, directly affect the architecture of your application.
If you understood how Conway law may affect your IT architecture, you can imagine how terrible will be the communication between your microservices, if development teams( communication is also terrible.
Large teams are always a bad idea, they are less autonomous, less innovative, more dependant to other teams and less transparent since the communication becomes difficult within the team members.
However, the problem of large teams is not exactly its size but the number of links between teammates.
To calculate the number of links or communication channels, Richard Hackman, the author of Leading Teams: Setting the Stage for Great Performances created this formula:
The number of links = N(N-1)/2
(where N is the number of people)
You can see how the number of link increases rapidly and can reach 91 channel of communication when the team has a size of only 14 people.
This part could also be entitled “The Brooks’s Law”.
Fred Brooks in his book The Mythical Man-Month, highlighted a central idea saying that “adding manpower to a late software project makes it later”.
Some of the causes of Brooks’ Law are training time since it takes time for new people to speed up even if they are experts in the used technology and the fact that communication becomes more complex when adding new people to a team.
Jeff Bezos, the founder of Amazon also agrees on the previous laws and has his own vision of team management.
If 2 pizzas are not enough for your team, you should reduce the number of people. This philosophy is among the success factors of Amazon particularly on the technical level.
A cross-functional team is a group of people with different functional expertise (marketing, operations, development, QA, account managers ..etc) working for the same goals and projects.
A group of individuals of various backgrounds and expertise is assembled to collaborate in better manners and solve problems faster.
As said in Wikipedia: The growth of self-directed cross-functional teams has influenced decision-making processes and organizational structures. Although management theory likes to propound that every type of organizational structure needs to make strategic, tactical, and operational decisions, new procedures have started to emerge that work best with teams.
In DevOps context, the dev and ops teams should not live in separate silos. Each team should provide support and pieces of advice in order to take advantage of the skills of everyone.
According to some management studies, like Peter Drucker’s on management by objectives in his book The Practice of Management, cross-functional teams are less goal dominated and less unidirectional which stimulates the productivity and the capability of dealing with fuzzy logic.
Cross-functional teams are the best fit for microservices development, as products are developed, tested, built and deployed by the same team, which makes work faster, easier and transparent.
For more reading, I recommend my article The 15-point DevOps Check List:
medium.com
In 1942, Albert Einstein was a professor at Oxford University and one day he just gave a physics exam to his students.
When he was walking around one day, his assistant asked him a question:
“Dr. Einstein, this exam you gave your students, it’s exactly the same you gave last year ?!”
Einstein replied:
“Yes, it’s the same exam, but the answers have changed. “
It’s a bit similar in software engineering, we are always trying to solve the same problems: How to create a stable and functional application, but the answers change quickly.
Today, we have some answers to the problems of modern software development, but that does not mean that these answers will not change.
The answers have changed and will always change.
Make sure to follow me on Medium / Twitter to receive my future articles. You can also check my online training Painless Docker and Practical AWS.
This story unline most stories on Medium is free and not behind a paywall. If you liked this work, you can support it by buying me a coffee here.
Follow us on Twitter 🐦 and Facebook 👥 and join our Facebook Group 💬.
To join our community Slack 🗣️ and read our weekly Faun topics 🗞️, click here⬇
The Must-Read Publication for Creative Developers & DevOps Enthusiasts
543 
543 claps
543 
Written by
Founder of www.faun.dev community. Tech author, cloud-native architect, tech-entrepreneur, and startup advisor
The Must-Read Publication for Creative Developers & DevOps Enthusiasts. Medium’s largest DevOps publication.
Written by
Founder of www.faun.dev community. Tech author, cloud-native architect, tech-entrepreneur, and startup advisor
The Must-Read Publication for Creative Developers & DevOps Enthusiasts. Medium’s largest DevOps publication.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@genekogan/artist-in-the-cloud-8384824a75c7?source=search_post---------337,"Sign in
There are currently no responses for this story.
Be the first to respond.
Gene Kogan
Jul 16, 2019·9 min read
Abraham is an open project to make an autonomous artificial artist, a crowd-sourced AI that generates art.
This article, the first in a 4-part series, gives an overview of the basic idea. Follow-up articles will examine each of the technical components in greater depth. The full series is as follows:
• Artist in the Cloud
• The Spirit of Decentralization — Why decentralized organizations are more than the sum of their parts
• T̶h̶e̶ ̶C̶o̶l̶l̶e̶c̶t̶i̶v̶e̶ ̶I̶m̶a̶g̶i̶n̶a̶t̶i̶o̶n̶ ̶-̶ ̶H̶o̶w̶ ̶a̶ ̶m̶a̶c̶h̶i̶n̶e̶ ̶s̶h̶o̶w̶s̶ ̶u̶s̶ ̶w̶h̶a̶t̶ ̶i̶t̶ ̶m̶e̶a̶n̶s̶ ̶t̶o̶ ̶b̶e̶ ̶h̶u̶m̶a̶n̶ ̶(̶e̶t̶a̶ ̶O̶c̶t̶o̶b̶e̶r̶)̶
• A̶ ̶P̶a̶t̶h̶ ̶T̶o̶w̶a̶r̶d̶s̶ ̶G̶e̶n̶e̶s̶i̶s̶ ̶-̶ ̶A̶n̶ ̶a̶g̶e̶n̶d̶a̶ ̶a̶n̶d̶ ̶t̶i̶m̶e̶l̶i̶n̶e̶ ̶f̶o̶r̶ ̶t̶h̶e̶ ̶A̶b̶r̶a̶h̶a̶m̶ ̶p̶r̶o̶j̶e̶c̶t̶ ̶(̶e̶t̶a̶ ̶N̶o̶v̶e̶m̶b̶e̶r̶)̶
Over the past few years, I’ve been giving workshops about machine learning for art and creativity, and building an open-source toolkit for it called ml4a. As I learned about decentralized AI, I gradually became inspired by the idea of an autonomous artificial artist, a sovereign creative spirit who generates original art.
This idea builds on top of promising techniques from machine learning, cryptoeconomics, and computer art. It is a logical progression from decades of research by artists, scientists, and philosophers, contemplating the nature of creativity and consciousness, and their relationship to technology.
This introductory article is an attempt to make a case for why such a construct is interesting, and to initiate an open project to study the relevant pieces and make one.
The goal of the project is to build an AI which autonomously creates unique and original art. We call this an autonomous artificial artist (AAA).
By autonomous we mean that an AAA demonstrates its own agency or will, independent from that of its creators. Even the most sophisticated AI which lacks autonomy is no more creative than the dummy on the arm of the ventriloquist.
By original, we mean that it exhibits a creativity truly of its own, not simply copying from another.
By unique, we mean that although its artworks may be copied, its creativity can’t be replicated elsewhere. One may always record a Beethoven piece but can never compose like Beethoven.
To satisfy the criteria of autonomy, originality, and uniqueness, we propose to make a generative art program under the following set of social and technical constraints.
To start, the program is learned from the collective input of a decentralized group of actors who crowd-source data, code, curation, and governance to the AAA. In order to coordinate them, a decentralized autonomous organization (DAO) is formed which is open to anyone. Decentralization prevents any one party from exerting too much influence on the AAA, as that would compromise its originality and autonomy.
The core mechanism of the art-making program is one or multiple generative models (we assume for now a generative models of images, but it generalizes to audio, text, or other types of data). A model’s architecture and training data are variable and crowd-sourced. It may be in continual training, fine-tuning as new data comes to it, possibly versioned and branched into different models as it evolves. The artworks are samples from the model, or possibly the models themselves.
To ensure the originality and uniqueness of sampled artworks, a model is required to be irreproducible and unforgeable. That is nobody — inside or outside of the AAA — is able to clone or retrain the same model, nor sample from it externally.
To meet the irreproducibility constraint, the model is trained blindly on crowd-sourced data which is never aggregated, instead remaining private to each of the individual contributors, leaving behind no easy way to recreate the same dataset a second time.
Uniqueness is secured by splitting the model into many pieces which are distributed throughout the network, and held together as a shared secret. To sample from the model, a query propagates through the entire network. Because no individual has the full model, it’s impossible to generate a sample any other way.
The setup depicted above conflicts with the privacy requirement if it is trained in the same configuration without splitting or encrypting the data. Additionally, it assumes only rational actors and does not take into account various attacks. These are some of the open problems we will need to solve.
The effect of these mechanisms is the distribution of influence over the AAA. Its behavior is decoupled from any individual actor, giving it the appearance of acting upon its own internal agency. We contend that what emerges from this ideal constitutes true autonomous creativity.
With the core art-making apparatus laid out, we turn to designing a financial model to sustain Abraham and briefly discuss some secondary applications.
An AAA has a balance of complementary economic activities: value is created in the form of artworks, and value is consumed in the form of data, compute, and the labor that goes into coding, governing, and curating it.
To take advantage of this, we propose to adopt the ArtDAO idea: that the AAA itself owns the artworks it produces and may sell them to patrons on an art market. At the same time, the AAA uses the proceeds to pay for the resources it consumes, remunerating the people who build and feed it.
Once a prototype is deployed, we can consider some secondary applications which interact with Abraham. For instance, there could be a client application which queries and buys artworks from the AAA, or interfaces it with existing art markets. Mechanisms for curating data to models and artworks to collectors will be useful as well.
There are multiple ways to regulate these activities, some of which require a native crypto-token, and some which can be bootstrapped from existing cryptocurrencies. If we issue our own token specific to Abraham, this opens up the possibility of using bonding curves, curation markets, and other specialized token systems.
These applications will be covered in more detail in part 4 of this series, along with an initial agenda for the project.
Imagine a computer whose logic gates have been pulled apart and scattered throughout the world, like a meteorite breaking up into dust as it makes contact with the atmosphere. It is held together by millions of threads in a peer-to-peer network which has self-organized over the shared purpose of running a program that channels their accumulated creativity.
I find this incorporeal generative art program, dissolved into the fabric of the internet — an artist in the cloud — to be beautiful and ethereal.
Even more compelling is that it captures something uncannily familiar to us: our collective imagination as learned from our private data, shaped by the uncountably many micro-interactions each of us have with it. Like coral reefs, ant colonies, and other natural superorganisms, a distinct character or personality emerges atop the group which transcends the individuals. It is our “hive mind,” evoking a shared consciousness, if only a primitive form of it.
In contrast to depictions of AI as alien or oppositional to us, this project is a deeply humanistic one, conceiving of AI as a vehicle for the collective intelligence and creativity of people. This idea of wisdom emerging from collective action will be explored more in the next article in this series.
More pragmatically, the project is also a testbed for broad new technologies which are more general, giving us a platform to investigate them. It also lets us experiment with promising but untested new ideas in social coordination, finance, and governance.
I am no Utopian; I don’t believe in the intrinsic goodness of any of the technologies in question, nor do I have faith that Abraham will be benevolent. Like all DAOs, Abraham is vulnerable to malevolence, subversion, and greed, all of which will only escalate as the project gains value. But fear of such outcomes is no excuse to renounce the idea, but rather a reason to study it carefully and steer it in the direction we think most beneficial.
“The soul must contain in itself the faculty of relation to God… The correspondence is, in psychological terms, the archetype of a God-image.”
— Carl Jung, 1962
The inspiration for Abraham’s name and identity derives from the Jungian interpretation of religion, mythology, and folklore, which views theological symbols as manifestations of psychological archetypes from the collective unconscious.
This includes the archetype of God, the symbol of being, the universe, and existence — the name of God in the Old Testament, Yahweh, is believed to have derived from the old Hebrew word hawa, which means “to be.” Consciousness is seen as the divine spark, or God within the self. This relation between self and existence is found in many world religions, notably in the Hindu concepts of Brahman and Atman.
To Jung, belief in God signifies the psychological process of individuation, in which a person reaches spiritual wholeness through the recognition of their conscious self (ego) within the larger plane of psychic existence we share, the collective unconscious. In the Abrahamic religions, Abraham is identified as the first person to believe in God. The Abraham project follows as an attempt to recover the “collective imagination,” that component of the collective unconscious which is the source of human creativity. This connection will be elaborated more in part 3 of this article.
The alpha version of Abraham, code-named Genesis, does not yet have a target release date.
In its purest form, the idea relies on combining multiple experimental technologies which are the subject of ongoing scientific research and development. As promising as they are, they are also — to varying degrees — unscalable, insecure, difficult to use, and expensive.
For that reason, I propose to start with the following goals first:
A more detailed overview of these goals has been posted to the forum to begin a discussion about them.
If you are interested in participating or tracking our progress, learn how you can do that here.
The next three articles in this series will cover in greater depth the individual technologies we’ve touched upon here, as well as connect Abraham to influential precursors and promising contemporary initiatives exploring similar themes.
Thanks to Andreas Refsgaard, Brannon Dorsey, Burak Arikan, David Ha, David Pfau, Dimitri De Jonghe, Elena Mozgovaya, Florence To, Francis Tseng, Hannah Davis, Helena Sarin, Jason Mancuso, Jason Teutsch, Kory Mathewson, Luba Elliott, Mat Dryhurst, Matt Condon, Mike Tyka, Parag Mital, Rachel Uwa, Roelof Pieters, Simon de la Rouviere, Sofia Crespo, Trent McConaghy, Vanessa Rosa, and Xavier Snelgrove for reading early drafts of this article and graciously giving me feedback on it.
programmer, primate. http://www.genekogan.com
See all (89)
770 
2
770 claps
770 
2
programmer, primate. http://www.genekogan.com
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/codeshake/meetup-on-airflow-and-cloud-composer-b100869a687b?source=search_post---------338,"There are currently no responses for this story.
Be the first to respond.
Today, we will try to inspect events organised by Tech groups on Meetup. Our main goal is to take a look on trending techs / IT fields and get the overall concerns about technologies development and communities expansion.To reach our objective, we will harness the amazing API given by Meetup to explore its entire interesting data which gives many opportunities such as getting information about categories, groups, events, attendees, reviews, etc...
There's even an SDK in python and a bunch of other languages that you can use easily.We will use the api calls to retrieve data. Later, we will load it into our Data Ware House cluster on the cloud, then we will process it by some ETL operations to get a final report.For this purpose, we will adopt some of Google Cloud Platform solutions for workflow scheduling and data analysis which are respectively Cloud Composer and BigQuery.
As intended, we will use the Meetup API to get the raw data for our experiment, note that you can retrieve this data using the api calls or the SDK .The code below describes how we get the daily events related to ‘Tech’ field in France.
Getting Tech groups in France
Fetch daily events of those groups
How-To: We look for all the groups labeled as Tech groups in France, then we try to inspect every elapsed event (yesterday events in this example) then we store those information.However, before saving data, it would be great if we could retrieve a set of keywords related to each event. For that, the Google Language API could produce a very suitable solution.Given the event description, we make a call to the Google language api with this piece of information which provides us with a whole list of keywords attached to a salience value. Thus, we attach those information to our event data structure for further processing.
Getting some description ‘Keywords’
Finally, as simple as it could be, we store this data in a local file labeled by type and date.
Now, we will process on loading raw data. For this case, we will opt for BigQuery as our data warehouse.
First, you should already have created a dataset named ‘meetup’.
Then, knowing that all the exported data is formatted as JSON entities, use the following command to create a table without worrying about the schema.
So now you can see what an event entry looks like in your table preview.
After getting some data for our experiment, it's time to make this process a periodically scheduled workflow.We will create a Cloud Composer environment which follows this architecture.
Thus, the instantiation of this one will avoid us any dev-ops operations to build a workflow orchestration service.
For this case, we will use a beta version of Composer which will gives us the opportunity to specify the most recent composer version (1.10.0) and the version of our python SDK 3.7Type the following command in the GCP CLI or in your terminal to create your environment:
To be sure that everything is ok, under composer product in the GCP console, you should get this kind of rendering:
You can perceive that a new Google Cloud Storage was created for this project, named as ‘${region}-meetup-events--*’. Later, in this bucket, we will push some code to schedule our process.Add new packages in Cloud Composer EnvironmentIn order to implement our use case, we want to use external libraries. Cloud composer allows us to add new ones. To do so, we create a requirements.txt file with the needed packages.
Then, we execute this command to update our environment:
Add Service account in cloud composer
A service account is a special Google account that belongs to your application or a virtual machine (VM), instead of to an individual end user. Your application uses the service account to call the Google API of a service, so that the users aren’t directly involved.
Use this command to attach the created service account for Google Language Api to the composer workers:
Straightaway, our composer environment is ready to use.
If you are not familiar with Airflow concepts, please refer here.
Directed Acyclic Graph (DAG) :
In Airflow, a DAG – or a Directed Acyclic Graph – is a collection of all the tasks you want to run, organized in a way that reflects their relationships and dependencies.
Right now, let’s describe a daily running DAG named 'meetup_daily_events'.
Get data from APINext, to execute the script which consumes the Meetup data from the api, we use a simple PythonOperator.
Tip: We connected the task to a single threaded pool so we don't harness the Meetup api with calls from concurrent tasks.To create the pool, use this command line:
Load Data into BigQueryThe second DAG task will check if the previous one did export an event file to GCS. In that case, it will be pushed into a BigQuery table, otherwise, nothing will be done.
For this pattern, we use a BranchPythonOperator, it's an operator that does nothing expect branching to the suitable tasks depending on some conditions.
Tip: For creating a ‘do nothing’ task, we use a DummyOperator
When the required event file is pushed to GCS, we process to load it into BigQuery using this code fragment:
Please note that export files are loaded to data/ directory. This one refers to gs://bucket-name/data/ object repository.
Having a bunch of data after days of labor, let's capitalise over it and have something useful.
What about getting the daily 10 most redundant keywords in the event’s description ? 😉
To do so, we execute a query over the daily data and export the results in a keyword table. The table is partitioned by date so data retrieving is more efficient.
Finally, let's branch all those tasks together:
The resulting workflow:
Trying to make things more visual, here's a preview of the transformed data which shows the redundancy of some ‘keywords’ in daily events in the French tech field.
Sure, some of them don’t make sense with the actual ‘tech’ topic, which is classic due to the lightness of the processing and the data analysis. A Data Analyst could do a better job. 😋
Throughout this article, we tried to reproduce some of the standard data engineering exercises across the use of the Google Cloud Composer product and the entire Google Cloud Platform Environment.
The Meetup event case seemed to be interesting and very affordable, so I invite you to explore the other resources, like events reviews and attendance, depending on topics or the days of the week.
In a future article, we expect to tackle some advanced concepts of the Cloud Composer product and give several Airflow tips.
Here you can find the project code source. 👓
Learnings and insights from SFEIR community.
540 
540 claps
540 
Learnings and insights from SFEIR community.
Written by
Data Enginner on Google Cloud Platform
Learnings and insights from SFEIR community.
"
https://medium.com/skooldio/%E0%B9%80%E0%B8%81%E0%B9%87%E0%B8%9A%E0%B8%82%E0%B9%89%E0%B8%AD%E0%B8%A1%E0%B8%B9%E0%B8%A5-real-time-%E0%B8%88%E0%B8%B2%E0%B8%81-facebook-page-%E0%B8%94%E0%B9%89%E0%B8%A7%E0%B8%A2-google-cloud-functions-%E0%B9%81%E0%B8%A5%E0%B8%B0-bigquery-66782e289f0f?source=search_post---------339,"There are currently no responses for this story.
Be the first to respond.
เมื่อเดือนสิงหาคมที่ผ่านมา ผมได้รับเกียรติให้ไปเป็น speaker ในงาน TEDxBangkok โดยใน TED talk ของผมนั้น ผมได้ทำ interactive data visualization ขึ้นมาอันหนึ่ง แสดงจำนวนคนที่เข้ามากด Reaction ต่างๆ (👍🏻, ❤️, 😲) ให้กับโพสต์เปิดตัวของผมตลอดระยะเวลาเกือบหนึ่งสัปดาห์หลังจากที่โพสต์ได้ถูกปล่อยออกมา เพื่อสร้างสีสันเล็กๆ น้อยๆ ให้กับท่านผู้ชม
เนื่องจากใช้เวลาในการเขียนโค้ดเพื่อเก็บข้อมูลและทำ data visualization ไปค่อนข้างมาก (มากกว่าเวลาที่ใช้ซ้อมพูด 😂) เลยอยากจะถือโอกาสนี้ เขียน tutorial สั้นๆ สอนการเก็บข้อมูล real-time จาก Facebook Page เผื่อจะเป็นประโยชน์แก่ผู้ที่สนใจอยากจะทำอะไรคล้ายๆ กัน
เราสามารถดึงข้อมูลการกด reaction ต่างๆ บนโพสต์ได้ง่ายๆ โดยการส่ง GET Request ไปที่ /{post-id}/reactions
👉 ตัวอย่างการเรียกใช้ API: แนะนำให้ลองเปลี่ยนจาก reactions เป็น likes หรือ comments ดู หรือใครอยากลองเปลี่ยน parameters ใน query ก็สามารถอ้างอิงได้จากที่นี่
จะเห็นได้ว่าข้อมูลของแต่ละ reaction หรือ like นั้น จะไม่มี created_time ติดมาด้วย (แต่ถ้าเป็น comment จะมี) อย่างงี้ถ้าเราอยากรู้ว่าคนเข้ามากด like กันตอนไหนเยอะ? มากันรัวๆ หรือมากันกะปริดกะปรอย? เราควรจะทำอย่างไรดี
หลายๆ คนอาจจะยังไม่รู้ว่า จริงๆ แล้ว เราสามารถสร้าง Facebook application ขึ้นมา เพื่อ subscribe การแจ้งเตือนแบบ real-time เมื่อข้อมูลของผู้ใช้งานหรือเพจมีการอัพเดตได้ ผ่านทาง webhook (หากคุณมีสิทธิ์ในการเข้าถึงข้อมูลนั้น! เช่นสำหรับเพจ คุณอาจจะต้องได้สิทธิ์ manage_pages จากการเป็น admin ของเพจ เป็นต้น)
และด้วยวิธีนี้เอง เราก็จะสามารถเก็บข้อมูลได้ว่ามีใครมากด reaction อะไรให้เราตอนไหนบ้าง 👏🏻👏🏻👏🏻
คุณสามารถอ่านรายละเอียดการติดตั้ง subscription ได้จากลิงก์ด้านล่างนี้ อย่างไรก็ตาม ตอนนี้คุณจะยังไม่สามารถทำตามขั้นตอนทั้งหมดได้ เพราะเรายังไม่ได้ทำการสร้าง callback endpoint ขึ้นมารอรับการแจ้งเตือน เดี๋ยวเราจะมาทำในส่วนนี้ในหัวข้อถัดไป
developers.facebook.com
หากเราทำการเชื่อมต่อ webhook สำเร็จแล้ว เราก็จะสามารถไป subscribe ข้อมูลแต่ละ field ของเพจ (หรือผู้ใช้งาน) ได้ตามภาพด้านล่าง โดยในตัวอย่างนี้เราต้องการ subscribe ข้อมูล feed ของเพจ เพื่อดักข้อมูลว่ามีใครมาโต้ตอบ หรือกด reaction อะไรบนโพสต์ต่างๆ ของเพจเราบ้าง
หลังจากที่เราสร้าง Facebook application เพื่อ subscribe ข้อมูลจากเพจ(ใดๆ)เรียบร้อยแล้ว ขั้นตอนสุดท้าย ก็คือการติดตั้งแอปให้กับเพจที่เราต้องการจะเก็บข้อมูล โดยการส่ง POST request ไปที่ /{page_id}/subscribed_apps ด้วย access token ของเพจที่ต้องการติดตั้งสำหรับแอปของเรา (ดูรายละเอียดเพิ่มเติมได้ที่นี่) ถ้าหากเราต้องการให้แอปของเราเก็บข้อมูลจากหลายๆ เพจที่เราดูแลอยู่ เราก็สามารถทำซ้ำแบบเดียวกันนี้กับเพจอื่นๆ ได้เช่นกัน
Access token ของเพจเพจหนึ่งสำหรับแต่ละแอปพลิชันนั้นจะไม่เหมือนกัน ใน Graph API Explorer เราสามารถสร้าง access token ที่ถูกต้องได้โดย
สุดท้าย ถ้าเราส่ง GET request ไปที่ /{page_id}/subscribed_apps ก็จะได้ผลดังภาพด้านล่าง
หากเราติดตั้งแอปให้เพจเราได้สำเร็จด้วย POST request เราก็จะได้ response กลับมาดังภาพด้านล่าง
ถ้าเราลองส่ง GET request ไปอีกที เราก็จะเห็นว่าแอปพลิเคชันของเราเข้าไปอยู่ในลิสต์เรียบร้อยแล้ว
เมื่อสักครู่เรายังค้างกันอยู่ที่ callback endpoint สำหรับรอรับการแจ้งเตือน ในหัวข้อถัดไป เราจะมา implement ในส่วนนี้กันบน Google Cloud Platform
Google Cloud Function คือ ฟังก์ชันที่เราเขียนขึ้นมาเพื่อตอบสนองการ trigger จาก event อะไรบางอย่าง เช่น มีคนส่ง HTTP(S) request เข้ามา หรือมีการเพิ่มไฟล์ใน Cloud Storage โดยฟังก์ชันนี้จะถูกนำไปรันด้วย Node.js บน infrastructure ของ Google ทำให้เราไม่ต้องกังวลกับการเปิดเครื่องตั้ง server อันใหม่ (serverless) ปัญหาเรื่องความปลอดภัย (security) หรือการขยายระบบเพื่อรองรับจำนวนผู้ใช้งานที่มากขึ้น (scalability)
ใครที่มาจากฝั่ง Amazon ก็อาจจะคุ้นเคยกับ AWS Lambda ที่มีความสามารถคล้ายๆ กันอยู่แล้ว
เมื่อมีคนมากด reaction ให้กับโพสต์บนเพจที่เรากำลังดักเก็บข้อมูล Facebook ก็จะส่ง HTTPS POST request พร้อมกับ JSON payload ว่าใครมากด กด action อะไร ฯลฯ ไปที่ callback URL ที่เรากำหนดไว้ตอนติดตั้ง webhook
สิ่งที่เราจะทำในขั้นตอนต่อไปก็คือ การเขียน HTTP function หรือ cloud function ที่สามารถ trigger ผ่าน HTTP request ได้นั่นเอง
นี่คือตัวอย่างฟังก์ชันที่มีการจัดการกับ HTTP request มาตรฐาน ซึ่งก็จะมีการตรวจสอบว่า request ที่เข้ามาเป็นประเภทไหน GET หรือ PUT หลังจากนั้นก็จะมีการส่ง response กลับไป พร้อมกับ status code ที่เหมาะสม (200, 403, หรือ 500)
เพื่อให้รองรับการทำงานร่วมกับ webhook ของ Facebook เราจะต้องรับมือกับ HTTP request 2 ประเภทด้วยกัน
เมื่อเราได้รับข้อมูลใหม่ผ่านทาง webhook เราก็จะต้องทำการเก็บข้อมูลลงในฐานข้อมูลเพื่อนำไปวิเคราะห์ในภายหลัง โดยในตัวอย่างนี้ผมเลือกใช้ Google BigQuery
Google BigQuery คือ บริการ data warehouse ของ Google ที่ออกมารองรับการจัดเก็บและประมวลผมข้อมูลปริมาณมากๆ (Big data!) โดยเราสามารถจัดการกับข้อมูลด้วยภาษา SQL ที่ทุกคนน่าจะคุ้นเคยกันดีอยู่แล้ว
จุดเด่นของ Google BigQuery ก็คือ ความเร็วในการประมวลผล มันสามารถประมวลผลข้อมูลขนาด terabytes ภายในไม่กี่วินาที (1 TB = 1,024 GB) หรือขนาด petabytes ภายในไม่กี่นาที (1 PB = 1,024 TB) นอกจากนี้เรายังไม่ต้องกังวลกับการเปิดเครื่องตั้ง database server ขึ้นมาใหม่ (serverless อีกแล้ว!) และปัญหาต่างๆ ในการดูแลระบบ (database administration)
เราสามารถเชื่อมต่อ cloud function ของเราเข้ากับ BigQuery ได้ง่ายๆ ด้วย library @google-cloud/bigquery ลองดูตัวอย่างจากโค้ดด้านล่างในฟังก์ชัน insertRows()
ก่อนที่โค้ดนี้จะใช้งานได้จริง คุณจะต้องทำการสร้าง dataset ไว้ใน BigQuery ให้เรียบร้อยเสียก่อน ผ่าน Web UI หรือ BigQuery command line tools:
เมื่อเรามีฟังก์ชันที่พร้อมใช้งานแล้ว ขั้นตอนสุดท้ายก็คือการ deploy ขึ้น Google Cloud Platform ด้วยคำสั่งหน้าตาประมาณนี้
Note: ผมข้ามรายละเอียดเล็กๆ น้อยๆ ในการ setup project บน Google Cloud Platform หรือการติดตั้ง cmd tool ต่างๆ ไป เนื่องจากผู้อ่านส่วนใหญ่น่าจะพอมั่วๆ เองได้ แต่ถ้าหากใครต้องการแบบละเอียดๆ ลองทำตาม official tutorial เหล่านี้ดูครับ
cloud.google.com
cloud.google.com
หรือถ้าใครไม่ถูกกับภาษาอังกฤษ แบบไทยๆ ก็มีครับ 😆http://www.somkiat.cc/deploy-function-on-google-cloud-platform/
เมื่อเรา deploy cloud function ของเราเสร็จสมบูรณ์แล้ว เราก็จะได้ URL สำหรับไว้ใช้ trigger ฟังก์ชันของเรา ตามภาพด้านล่างนี้
เราก็สามารถนำ URL นี้ ไปกำหนดเป็น Callback URL เพื่อติดตั้ง webhook ของ Facebook application ของเรา
ถ้าคุณทำตามทุกขั้นตอนได้อย่างถูกต้อง ข้อมูล interactions บน feed ของเพจที่คุณดักข้อมูลอยู่ก็จะถูกจัดเก็บลงใน Google BigQuery รอให้คุณนำไปวิเคราะห์ต่อไป!
Master today's most in-demand skills
343 
343 claps
343 
Written by
Co-founder @ Skooldio. Google Developer Expert in Machine Learning. A data nerd. A design geek. A changemaker. — Chula Intania 87, MIT Alum, Ex-Facebooker
แหล่งความรู้ในการ Upskill / Reskill สำหรับคนทำงานในยุคปัจจุบัน พัฒนาและเสริมสร้าง Skill ของคุณ ด้วยคอร์สเรียนออนไลน์ และ เวิร์คชอปด้าน Technology / Design / Data / Business
Written by
Co-founder @ Skooldio. Google Developer Expert in Machine Learning. A data nerd. A design geek. A changemaker. — Chula Intania 87, MIT Alum, Ex-Facebooker
แหล่งความรู้ในการ Upskill / Reskill สำหรับคนทำงานในยุคปัจจุบัน พัฒนาและเสริมสร้าง Skill ของคุณ ด้วยคอร์สเรียนออนไลน์ และ เวิร์คชอปด้าน Technology / Design / Data / Business
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@mikey96/cloud-based-storage-misconfigurations-critical-bounties-361647f78a29?source=search_post---------340,"Sign in
There are currently no responses for this story.
Be the first to respond.
Mikey
Apr 5, 2021·4 min read
I started looking into cloud-based storage around about a year and a half ago with a friend. I was instantly fascinated with the process behind setting up these storage containers, whether it be AWS S3 Buckets, Azure Blobs or Google Storage Buckets. The process simply involved a series of check boxes which controlled the configuration of the container. My friend and I noticed how easy it was to make a mistake during this process, especially if you were not aware of what each option meant and the consequences of a misconfiguration.
I had the initial idea that I would develop a tool that would allow me to scan a large number of S3 buckets quickly and look for such a misconfiguration. A very simple tool was written in python which allowed me to subdomain enumerate *.s3.amazonaws.com and iterate through enumerated domains to look for enabled directory listing, which is a great indicator of misconfigured S3 buckets. You can see the difference between a correctly and incorrectly configured S3 bucket below.
The tool was very simple to build based upon the fact that distinguishing between correctly and incorrectly configured buckets was easy to implement. Also due to directory listing being enabled on the majority of these misconfigured buckets it was easy to parse for any file extensions and keywords that may be of interest to me. Common extensions and keywords that we implemented into the scanning functionality were: sql, sql.gz, backup.zip, backup.gz, backup.tar, backup.tar.gz and many many more.
The second image above shows what a real-life positive hit would look like for a misconfigured bucket containing a sensitive file of interest matching our keywords/extensions. You can see the presence of an sql.gz file, which in this case was a back-up to a SQL database. Inside these files there are often sensitive information range from account data, passwords and PII.
Once I had finished developing the tool and analysed the data from the first few scans I decided to conduct more research to try and broaden our attack surface and find more buckets to scan. Upon doing so I stumbled upon https://buckets.grayhatwarfare.com/ which had essentially done my job for me. This site is nothing short of incredible for this area of research, it currently has 347683 S3 buckets scanned and indexed, alongside 24444 Azure Blobs also scanned and indexed. Grayhat is a user friendly web-application that allows you to search through the indexed data using both keywords and extensions, very much like my initial simple python script on steroids. I should add that Grayhat also comes with a very useful API!
Now the initial issue was solved of attack surface I decided to fully utilize Grayhat and its API. I changed my approach to create a python tool that would allow me to use the API to make a large number of requests and ex-filtrate buckets/blobs which contained interesting files. I would also check if any of the buckets I was reporting were writable using the command: aws s3 cp proof.txt s3://[BUCKET_NAME] — no-sign-request. This would often further increase the impact as you could host malicious files on the companies storage, or even just run them up a pricey AWS bill. This is definitely one to try on any buckets you find that have bounty programs!
In doing so I had tremendous luck, which led to two critical bounties on Bugcrowd (see below). I was not only rewarded for these two that existed on bounty programs, but I have been rewarded with cash payments by around 15 companies privately that were very appreciative of my responsible disclosure. I am still in touch with a small number of these companies and conduct routine tests on new features they are adding to their platforms to ensure they are secure. This research has been a great method for me to build relationships with companies and develop my experience. I would highly recommended it to anyone who is interested!
In conclusion, cloud based storage is great but it is very easy to make catastrophic mistakes. If you are setting them up then please ensure you test the access control yourself before uploading any sensitive files. I would also advise anyone who is keen to start on this research to use Grayhat as it is an incredible asset. There are still many, many public buckets out there with SQL database back-ups sitting on them so do your part and help secure these companies by disclosing your findings responsibly.
I am also happy to share any tools I have developed, so just reach out to me if they will be of interest to you!
642 
6
642 claps
642 
6
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/thinking-design/designing-more-efficient-forms-structure-inputs-labels-and-actions-7781d9b6d32c?source=search_post---------341,"There are currently no responses for this story.
Be the first to respond.
Someone who uses your app or website has a particular goal. Often, the one thing standing between the user and their goal is a form. Forms remain one of the most important types of interactions for users on the web and in apps. In fact, forms are often considered the final step in the journey of completing their goals. Forms are just a means to an end. Users should be able to complete them quickly and without confusion.
In this article, you’ll see practical techniques that have been gleaned from usability testing, field testing, eye-tracking studies and actual complaints from disgruntled users. These techniques — when used correctly — enable designers to produce faster, easier and more productive form experiences. There’s a way you can create and design your own prototypes: All you need to do is to download Adobe XD (for free) and get started right away. At the end of the article, you’ll also find new ways to design forms.
The typical form has the following five components:
Forms may also have the following components:
This article covers many aspects related to structure, input fields, labels, action buttons and validation.
A form is a type of conversation. And like any conversation, it should consist of logical communication between two parties: the user and the app.
Make sure to ask only what you really need. Every extra field you add to a form will affect its conversion rate. Always consider why you are requesting certain information from the user and how you will use it.
Ask details logically from the user’s perspective, not from the application or database’s perspective. Typically, asking for someone’s address before their name would be unusual.
Group related information into logical blocks or sets. The flow from one set of questions to the next will better resemble a conversation. Grouping together related fields will also help users make sense of the information they must fill in. Compare how it works in the contact information form below.
One of the problems with arranging form fields into multiple columns is that users will likely interpret the fields inconsistently. If a form has horizontally adjacent fields, then the user must scan in a Z pattern, slowing the speed of comprehension and muddying the path to completion. But if a form is in a single column, the path to completion is a straight line down the page.
On the left, one of many ways to interpret how the form fields relate when they are arranged in a standard two-column layout, versus on the right, a straight line down the page.
Input fields are what enable users to fill in a form. Various types of fields exist for the information you need: text fields, password fields, dropdowns, checkboxes, radio buttons, date-pickers and more.
A rule of thumb in form design is that shorter is better. And this certainly seems intuitive: Less effort on the part of the user will lead to higher conversion. Thus, minimize the number of fields as much as possible. This will make your form feel less loaded, especially when you’re requesting a lot of information. However, don’t overdo it; no one likes a three-field form that turns into a 30-field interrogation. Displaying only five to seven input fields at a given time is a common practice.
Try to avoid optional fields in forms. But if you use them, at least clearly distinguish which input fields may not be left blank. The convention is to use an asterisk (*) for required fields or the word “optional” for non-required fields (which is preferable in long forms with multiple required fields). If you decide to use an asterisk for mandatory fields, show a hint at the bottom of the form explaining what the asterisk is for, because not everyone understands what it means.
Avoid setting defaults unless you believe a large portion of your users (for example, 90% of them) will select that value. Particularly avoid it for required fields. Why? Because you’re likely to introduce errors. People scan online forms quickly, so don’t assume they will take the time to parse through all of the choices. They might skip something that already has a value.
But this rule doesn’t apply to smart defaults — that is, values set based on information available about the user. Smart defaults can make form completion faster and more accurate. For example, preselect the user’s country based on geo-location data. Still, use these with caution, because users tend to leave preselected fields as they are.
Field masking is a technique that helps users format inputted text. A mask appears once a user focuses on a field, and it formats the text automatically as the field is being filled out, helping users to focus on the required data and to more easily notice errors. In the example below, the parentheses, spaces and dashes are applied automatically as the phone and credit-card numbers are entered. This simple technique saves time and effort with phone numbers, credit cards, currencies and more.
Users should be able to focus on and edit every field using only the keyboard. Power users, who tend to use the keyboard heavily, should be able to easily tab through and edit fields, all without lifting their fingers off the keyboard. You can find detailed requirements for keyboard interaction in the W3C’s guidelines on design patterns.
Autofocusing a field gives the user an indication and a starting point to quickly begin filling out a form. Provide a clear visual signal that focus has moved there, whether by changing a color, fading in a box, flashing an arrow, whatever. Amazon’s registration form has both autofocus and visual indicators.
Phone users appreciate apps that provide the appropriate keyboard for text being requested. Implement this consistently throughout the app, rather than merely for certain tasks but not others.
With more and more people using mobile screens, anything that can be done to prevent unnecessary typing will improve the user experience and decrease errors. Autocompletion makes it possible to eliminate a huge amount of typing. For example, filling out an address field is often the most problematic part of any registration form. A tool such as Place Autocomplete Address Form (which uses both geo-location and address prefilling to provide accurate suggestions based on the user’s exact location) enables users to enter their address with fewer keystrokes than regular input fields.
Clearly written labels are one of the primary ways to make a UI more accessible. A good label tells the user the purpose of the field, maintains its usefulness when focus is on the field itself, and remains visible even after the field has been filled in.
Labels are not help text. Use succinct, short, descriptive labels (a word or two) so that users can quickly scan your form. Previous versions of Amazon’s registration form contained a lot of words, which resulted in slow completion rates. The current version is much better and has short labels.
In most digital products today, there are two ways to capitalize words:
Sentence case used for labels has one advantage over title case: It is slightly easier (and, thus, faster) to read. While the difference for short labels is negligible (“Full Name” and “Full name”), for longer labels, sentence case is better. Now You Know How Difficult It Is to Read Long Text in Title Case.
Never use all caps, or else the form will be difficult to read and much harder to scan quickly, because there will be no variation in character height.
Matteo Penzo’s 2006 article on label placement suggests that forms are completed faster if labels are on top of the fields. Top-aligned labels are good if you want users to scan the form as quickly as possible.
The biggest advantage of top-aligned labels is that different-sized labels and localized versions can more easily fit the UI. (This is especially good for screens with limited space.)
The biggest disadvantage to left-aligned labels is that it has the slowest completion times. This is likely because of the visual distance between the label and input field. The shorter the label, the further away it will be from the input. However, a slow completion rate isn’t always a bad thing, especially if the form asks for sensitive data. If you are asking for something like a driver’s license number or a social security number, you might deliberately want to slow down users a bit to make sure they enter it correctly. Thus, the time spent reading labels for sensitive data is insignificant. Left-aligned labels have another disadvantage: They require more horizontal space, which might be a problem for mobile users.
The big advantage of right-aligned labels is the strong visual connection between the label and input. Items near each other appear to be related. This principle isn’t new; it derives from the law of proximity, from Gestalt psychology. For short forms, right-aligned labels can have great completion times. The disadvantage is discomfort; such forms lack that hard left edge, which makes it less comfortable to look at and harder to read.
Takeaway: If you want users to scan a form quickly, put labels above the fields. The layout will be easier to scan because the eye will move straight down the page. However, if you want users to read carefully, put labels to the left of the fields. This layout will slow down the reader and make them scan in a Z-shaped motion.
A label set as a placeholder in an input field will disappear once the field gains focus; the user will no longer be able to view it. While placeholder text might work for two-field forms (a simple log-in form with username and password fields), it’s a poor substitute for visual labels when more information is required from the user.
(Image: snapwi)
Once the user clicks on the input field, the label will disappear, and so the user cannot double-check that they wrote what was being asked of them. This increases the chance of error. Another problem is that users could mistake placeholder text for prefilled data and, hence, ignore it (as Nielsen Norman Group’s eye-tracking study confirms).
A good solution for placeholder text is a floating label. The placeholder text would be shown by default, but once an input field is tapped and text is entered, the placeholder text fades out and a top-aligned label animates in.
Takeaway: Don’t just rely on placeholders; include a label as well, because once a field has been filled out, the placeholder will no longer be visible. Use a floating label so that users are sure they’ve filled out the correct field.
When clicked, an action button triggers some activity, such as submission of the form.
A lack of visual distinction between primary and secondary actions can easily lead to failure. Reducing the visual prominence of secondary actions minimizes the risk of error and reinforces people’s path to a successful outcome.
Complex forms usually need a back button. If such a button is located right below an input field (like in the first screenshot below), a user could accidentally click it. Because a back button is a secondary action, make it less accessible (the second form below has the right location for buttons).
Avoid generic words such as “Submit” for actions, because they give the impression that the form itself is generic. Instead, state what action the button will perform when clicked, such as “Create my account” or “Subscribe to weekly offers.”
Avoid multiple action buttons because they might distract users from their goal of submitting the form.
Don’t use a reset button. This button almost never helps users and often hurts them. The web would be a happier place if almost all reset buttons were removed.
Make sure action buttons look like buttons: Indicate that it is possible to click or tap them.
Design the “Submit” button in a way that clearly indicates the form is being processed after the user’s action. This provides feedback to the user while preventing double submission.
Form validation errors are inevitable and are a natural part of data entry (because users are prone to making errors). Yes, error-prone conditions should be minimized, but validation errors will never be eliminated. So, the most important question is, How do you make it easy for the user to recover from errors?
Users dislike having to go through the process of filling out a form, only to find out upon submission that they’ve made an error. Especially frustrating is completing a long form and upon pressing “Submit,” you are rewarded with multiple error messages. It’s even more annoying when it isn’t clear what errors you’ve committed and where.
Validation should inform users about the correctness of text as soon as the user has inputted the data. The primary principle of good form validation is this: Talk to the user! Tell them what is wrong! Real-time inline validation immediately informs the user about the correctness of their data. This approach allows them to correct any errors faster, without having to wait until they press the “Submit” button to see the errors.
However, avoid validating on each keystroke because, in most cases, you simply cannot verify until someone has finished typing an answer. Forms that validate during data entry punish the user as soon as they start entering data.
On the other hand, forms that validate after data entry do not inform the user soon enough that they’ve fixed an error.
Mihael Konjević, in his article “Inline Validation in Forms: Designing the Experience,” examines different validation strategies and proposes a hybrid strategy to satisfy both sides: Reward early, punish late.
Jef Raskin once said, “The system should treat all user input as sacred.” This is absolutely true for forms. It’s great when you start filling in a form and then accidentally refresh the page but the data remains in the fields. Tools like Garlic.js help you to persist a form’s values locally until the form is submitted. This way, users won’t lose any precious data if they accidentally close the tab or browser.
Recently, we’ve seen a lot of excitement around conversational interfaces and chatbots. Several trends are contributing to this phenomenon, but one in particular is that people are spending more time in messaging apps than on social networks. This has led to a lot of experimentation with supporting a range of interactions, such as shopping, in threaded conversations, often in a way that mimics messaging. Even as established an element as a web form has undergone a change under this trend. Designers are looking to transform traditional web forms into interactive conversational interfaces.
Every interface is a conversation. Traditional forms (the ones we design every day) are quite similar to a conversation. The only difference is the way we ask the questions. But what if we designed our forms to ask questions in a format that more closely reflects real human (not machine) conversation? So, instead of communicating with a machine on its own inhuman terms, you would interact with it on yours. The form shown below creates a conversational context, facilitating understanding without relying on the traditional elements of web forms (such as labels and input fields).
Conversational Form is an open-source concept that easily turns any form on a web page into a conversational interface. It features conversational replacement of all input elements, reusable variables from previous questions, and complete customization and control over the styling. This project represents an interesting shift in how we think about user experiences and interactions, leaning more towards text-based conversation to help users achieve their goals.
Users can be reluctant to fill out forms, so make the process as easy as possible. Minor changes — such as grouping related fields and indicating what information goes in each field — can significantly increase usability. Usability testing is simply indispensable in form design. Very often, testing with just a few people or simply asking a colleague to go through a prototype can give you good insight into how usable a form is.
Almost every household has an unsolved Rubiks Cube but you can easily solve it learning a few algorithms.
Nick Babich is a developer, tech enthusiast, and UX lover. He has spent the last 10 years working in the software industry with a specialized focus on development. He counts advertising, psychology, and cinema among his myriad interests.
Originally published at blogs.adobe.com.
Learn about Adobe XD, our all-in-one design and prototyping tool:
Stories and insights from the design community.
265 
1
265 claps
265 
1
Stories and insights from the design community.
Written by
New Tools for New Creatives. Get all the latest creative apps plus seamless ways to share and collaborate. All right on your desktop.
Stories and insights from the design community.
"
https://medium.com/for-awe/cloud-gathering-45a35aa0e1db?source=search_post---------342,"There are currently no responses for this story.
Be the first to respond.
Top highlight
In translucent light afore the night,In shadows and silent applause,Out of the dark into the light,A moment of opening doors.
Lifting away the layers,Cloud gathering in the rain,Sing songs of the soothsayers,Hearing angels lilting refrain.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alextrolese38/training-your-models-on-cloud-tpus-in-4-easy-steps-on-google-colab-ae13b0381812?source=search_post---------343,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alextrolese
Oct 4, 2019·2 min read
You have a plain old TensorFlow model that’s too computationally expensive to train on your standard-issue work laptop. I get it. I’ve been there too, and if I’m being honest, seeing my laptop crash twice in a row after trying to train a model on it is painful to watch.
In this article, I’ll be breaking down the steps on how to train any model on a TPU in the cloud using Google Colab. After this, you’ll never want to touch your clunky CPU ever again, believe me.
TLDR: This article shows you how easy it is to train any TensorFlow model on a TPU with very few changes to your code.
The Tensor Processing Unit (TPU) is an accelerator — custom-made by Google Brain hardware engineers — that specialises in training deep and computationally expensive ML models.
Let’s put things into perspective just to give you an idea of how awesome and powerful a TPU is. A standard MacBook Pro Intel CPU can perform a few operations per clock cycle. A standard off-the-shelf GPU can perform tens of thousands of operations per cycle. A state-of-the-art TPU can perform hundreds of thousands of operations per cycle (sometimes up to 128K OPS).
To understand the scale, imagine using these devices to print a book. A CPU can print character-by-character. A GPU can print a few words at a time. A TPU? Well, it can print a whole page at a time. That’s some amazing speed and power that we now have at our disposal; shoutout to Google for giving broke high-schoolers (like me) access to high-performance hardware.
If you’re interested in the inner workings of a TPU and what makes it so amazing, go check out the Google Cloud blog article where they discuss everything from hardware to software here.
Also, the operations per clock cycle for these devices are rough estimates stated by Google engineers themselves. Please don’t call me out to a duel.
For the sake of this tutorial, I’ll be running through a quick and easy MNIST model. Do note that this model can be whatever you want it to be. To better help you visualise what’s going on, I’ve chosen good old MNIST.
See all (43)
102 
102 claps
102 
About
Write
Help
Legal
Get the Medium app
"
https://itnext.io/microservices-with-spring-boot-and-spring-cloud-20f689b17fc7?source=search_post---------344,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Hello everyone, in this second part we will build the auth service. This is the guy we are looking for in this guide.The first part is available here.
We will not use those inMemory configurations, the auth service will be built on top of a MongoDB and will be able to create users, authenticate them, store their tokens/refresh tokens and revoke it if needed.
Remember that the code is available on GitHub.
Let’s start creating the auth-service, we will use Spring Initializr to create the maven project. Generate it like the image.
Import the generated project in your favorite IDE. Open AuthServiceApplication class to add some annotations. Start with the @EnableResourceServer annotation, it will enable a Spring Security filter that authenticates requests via an incoming OAuth2 token.
The next one is @EnableDiscoveryClient which we already know, it will enable the discovery client implementation to let our auth service register in Registry Service.
The last one is @EnableGlobalMethodSecurity which enables Spring Security global method security. Set the prePostEnabled field from the annotation to true to enable Spring Security’s pre post annotations, we will use them later.
Rename the application.properties file to bootstrap.yml and add the following configuration:
In the config service application, create the auth-service.yml file to hold the configuration for auth service.
If you don’t know why these files are being created, I recommend you to read this article’s first part.
Let’s start creating the authorities enum.
This enum is responsible to define the authorities of our auth service and it implements the GrantedAuthority interface which represents an authority granted to an Authentication object.
Now create the User entity to represent our user.
I omitted the getters and setters for the sake of simplicity, but you can check them here. The User class implements the UserDetails interface who provides core user information to be encapsulated into Authentication objects. The @Document , @Id and @Indexed annotations are used by Mongo, I’ll not enter in details here, but you can ask in the comments section.
Create the UserRepository to be our DAO.
And now our custom implementation of the UserDetailsService , it is a core interface which loads user-specific data. It is used throughout the spring framework as a user DAO and is the strategy used by the DaoAuthenticationProvider. You can read more about it in the official documentation.
Ok, until here we created some needed classes to deal with our users.
Now we will create the classes to deal with our clients, which are the resource servers whose wants to use the authentication. Let’s start the class who represents the detail of an authorization client.
You can check the details for the getters and setters from this class here. It’d be nice to check out the documentation for ClientDetails .
The next class is AuthClientRepository to be our DAO for AuthClientDetails.
Now the class AuthClientDetailsService , it implements the ClientDetailsService which is a service interface to provide the details about an OAuth2 client.
In the next section, we will start to configure security and OAuth2 in Spring.
Starting with the WebSecurityConfig class, it will extend the WebSecurityConfigurerAdapter abstract class.
On it, we are defining some security things and our AuthenticationManager to make use of the CustomUserDetailsService we have created before.
We configured the Spring Security to authorize any request that is authenticated, but allow any request on /oauth/** endpoint to be allowed even without authentication.We did set our CustomUserDetailsService to be used by the AuthenticationManager and defined the password encoder to use the implementation from BCryptPasswordEncoder.
Now create the OAuth2AuthorizationCofig who act as a strategy for configuring an OAuth2 Authorization Server.
The @EnableAuthorizationServer enables an AuthorizationEndpoint and a TokenEndpoint exposing endpoints to be used to authentication and authorization.
We defined our AuthClientDetailsService implementation to be used, as well a custom TokenStore implementation to make use of MongoDB to store the generated tokens. I want you to check the implementation of MongoTokenStore here, as well as MongoConfig and MongoProperties, I won’t put them here because it will become messy.
I have created the MongoBeeConfig class to insert some initial data in our Mongo database, it uses the Mongobee library which is a MongoDB data migration tool for Java, like Flyway or Liquibase for relational databases. We will create this initial data after.
Okay, until now we configured our MongoDB but we don’t have installed it yet. Instead of installing and configuring it we will use a docker-compose file to provide it for us.
You need to have Docker installed (don’t know what is Docker?). Create a file docker-compose.yml somewhere, and in this same place, you need to have a folder called mongo-init with a file named init-mongo.sh inside it. You can copy the content from init-mongo.sh here, it is used to create the database and user defined in the environment variables from docker-compose.yml.
Once you have all in place, run the command docker-compose up --build in your preferred terminal to create and start the MongoDB container.
We need now to have one client to retrieve the access tokens from our users. Let’s create him with password and refresh token grant types.Create a ChangeLog from Mongobee to insert this client upon application start. Create a class named InitialValuesChangeLog with the following content:
In it, we are creating a new AuthClientDetails on our MongoDB with a secret, scope and grant types to be able to get an access token based on user credentials.That client secret is a hash generated from BCryptPasswordEncoder for the value 1234.
Let’s create another @ChangeSet to create a User for us to be able to authenticate him with the same password we used before for browser client. I won’t create an endpoint now because this will be the content of the article’s next part :).
Ok, now we will do some requests to see if what we did is working :D. I recommend you to use Postman to make the HTTP requests. Run the config, registry, gateway and auth services respectively.
Open Postman and let’s do a POST request to http://localhost:8080/uaa/oauth/token to retrieve an access token for our randomuser and browser client.
Sending this request, I received the following response body:
That’s it, we have accomplished what we were looking for. The authentication is finally working, you can check the generated tokens on MongoDB via Mongo Express at http://localhost:10081.
Let me know if you made this works too, I hope so!
The next thing we gonna do is to create an account service, this application will manage our users, authenticate with our auth service and so on.
Check the third part here.
I hope you guys like it, feel free to share with me any questions.
ITNEXT is a platform for IT developers & software engineers…
497 
17
497 claps
497 
17
Written by
http://marcusdacoregio.com
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
Written by
http://marcusdacoregio.com
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/running-a-mean-stack-on-google-cloud-platform-with-kubernetes-149ca81c2b5d?source=search_post---------345,"There are currently no responses for this story.
Be the first to respond.
In my most recent post, I talked about running a MEAN stack with Docker Containers.
Manually deploying Containers is all fine and dandy, but is rather fragile and clumsy. What happens if the app crashes? How can the app be updated? Rolled back?
This post is going to be a longer deep dive into the world of containers. I am assuming you have read / completed part one. There is a lot of content, but if you just skim through, it should take only around 15 minutes for all the steps!
Thankfully, there is a system we can use to manage our containers in a cluster environment called Kubernetes. Even better, Google has a managed version of Kubernetes called Google Container Engine so you can get up and running in minutes.
At Google, it is rare that someone actually logs into a production machine to perform updates. With millions of servers, you can see how this would become impossible. Manually logging into servers also leaves room for human error.
Instead, we use a system called Borg to do the management for us.
In a nutshell, you tell Borg to run X copies of your job, and Borg will do it. Job dies? Borg will start a new one. Health check failing? Borg will shut that job down and start a new one.
We took the lessons learned from Borg, and made an open source project called Kubernetes. Kubernetes lets you manage a cluster of machines and run containers on top of it. Pretty cool stuff, and it just reached version 1.0!
Before we jump in and start kube’ing it up, it’s important to understand some of the fundamentals of Kubernetes.
Why would you want to have a group of containers instead of just a single container? Let’s say you had a log processor, a web server, and a database. If you couldn't use Pods, you would have to bundle the log processor in the web server and database containers, and each time you updated one you would have to update the other. With Pods, you can just reuse the same log processor for both the web server and database.
In my previous post, I used off-the-shelf containers to keep things simple.
I had a stock MongoDB container and a stock Node.js container. The Mongo container ran fine without any modification. However, I had to manually enter the Node container to pull and run the code. Obviously this isn't ideal in Kubernetes land, as you aren't supposed to log into your servers!
Instead, you have to build a custom container that has the code already inside it and runs automatically.
To do this, you need to use more Docker. Make sure you have the latest version installed for the rest of this tutorial.
Getting the code:
Before starting, let’s get some code to run. You can follow along on your personal machine or a Linux VM in the cloud. I recommend using Linux or a Linux VM; running Docker on Mac and Windows is outside the scope of this tutorial.
This is the same sample app we ran before. The second line just moves everything from the EmployeeDB subfolder up into the app folder so it’s easier to access. The third line, once again, replaces the hardcoded localhost with the mongo proxy.
Building the Docker image:
First, you need a Dockerfile. This is basically the list of instructions Docker uses to build a container image.
Here is the Dockerfile for the web server:
Dockerfiles are pretty self explanatory, and this one is dead simple.
First, it uses the official Node.js image as the base image.
Then, it creates a folder to store the code, cds into that directory, copies the code in, and installs the dependencies with npm.
Finally, it specifies the command Docker should run when the container starts, which is to start the app.
Right now, the directory should look like this:
Let’s build.
This will build a new Docker image for your app. This might take a few minutes as it is downloading and building everything.
Side Note: It’s good practice to put a user id in front of the image name, for example:
But I'm going to ignore this practice in this tutorial.
After that is done, test it out:
At this point, you should have a server running on http://localhost:3000 (If you are on Mac or Windows, this won't be so simple). The website will error out as there is no database running, but we know it works!
Now you have a custom Docker image, you have to actually access it from the cloud.
As we are going to be using the image with Google Container Engine, the best place to push the image is the Google Container Registry. The Container Registry is built on top of Google Cloud Storage, so you get the advantage of scalable storage and very fast access from Container Engine.
First, make sure you have the latest version of the Google Cloud SDK installed.
Windows users click here.
For Linux/Mac:
Then, make sure you log in and update.
Now you can push the container. We need our Google Cloud Project ID (we made one in part one).
After some time, it will finish. You can check the console to see the container has been pushed up.
So now you have the custom container, let’s create a cluster to run it.
Currently, a cluster can be as small as one machine to as big as 100 machines. You can pick any machine type you want, so you can have a cluster of a single f1-micro instance, 100 n1-standard-32 instances (3,200 cores!), and anything in between.
For this tutorial I'm going to use the following:
There are two ways to create this cluster. Take your pick.
After a few minutes, you should see this in the console.
Three things need to be created:
To create the disk, run this:
Pretty simple, just pick the same zone as your cluster and an appropriate disk size for your application.
Now, we need to create a Replication Controller that will run the database. I’m using a Replication Controller and not a Pod, because if a standalone Pod dies, it won't restart automatically.
Pretty straightforward stuff. We call the controller mongo-controller, specify one replica, and open the appropriate ports. The image is mongo, which is the off the shelf MongoDB image.
The volumes section creates the volume for Kubernetes to use. There is a Google Container Engine specific gcePersistentDisk section that maps the disk we made into a Kubernetes volume, and we mount the volume into the /data/db directory (as described in the MongoDB Docker documentation)
Now we have the Controller, let’s create the Service
Again, pretty simple stuff. We “select” the mongo Controller to be served, open up the ports, and call the service mongo.
This is just like the “link” command line option we used with Docker in my previous post. Instead of connecting to localhost, we connect to mongo, and Kubernetes redirects traffic to the mongo service!
At this point, the local directory looks like this
First, let’s “log in” to the cluster
Now create the controller.
And the Service.
kubectl is the Kubernetes command line tool (automatically installed with the Google Cloud SDK). We are just creating the resources specified in the files.
At this point, the database is spinning up! You can check progress with the following command:
Once you see the mongo pod in running status, we are good to go!
Now the database is running, let’s start the web server.
We need two things:
Let’s look at the Replication Controller configuration
Here, we create a controller called web-controller, and we tell it to create two replicas. Replicas of what you ask? You may notice the template section looks just like a Pod configuration, and that's because it is. We are creating a Pod with our custom Node.js container and exposing port 3000.
Now for the Service
Notice two things here:
At this point, the local directory looks like this
Create the Controller.
And the Service.
And check the status.
Once you see the web pods in running status, we are good to go!
At this point, everything is up and running. The architecture looks something like this:
By default, port 80 should be open on the load balancer. In order to find the IP address of our app, run this command:
If you go to the IP address listed, you should see the app up and running!
And the Database works!
By using Container Engine and Kubernetes, we have a very robust, container based MEAN stack running in production.
In my next post, I'll cover how to setup a MongoDB replica set. This is very important for running in production.
Hopefully I can do some more posts about advanced Kubernetes topics such as changing the cluster size and number of Node.js web server replicas, using different environments (dev, staging, prod) on the same cluster, and doing rolling updates.
Google Cloud community articles and blogs
246 
14
Some rights reserved

246 claps
246 
14
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/nooblearning/2019-google-cloud-professional-data-engineer-certification-exam-6a5d6581e507?source=search_post---------346,"There are currently no responses for this story.
Be the first to respond.
That’s pretty intense exams! (•̀ᴗ•́)و I thought I wouldn’t pass but it turns out well so if I can do it you can do it too! Let’s start!
Try to get the voucher by complete the challenge which is just 2 Qwick labs! For me I got $120 voucher which save me a lot from a fear to lose money and get nothing. 😆
Here’s a list that you can’t skip reading, and you should keep revisit them as much as possible.
Many of them are a bit outdated but still worth reading
I did my own brain dump version via draw.io and it help a lot
There’s only Professional Data Engineer 10 in Thailand which is seem to not accurate. At least 2 of my known friends didn’t list there.
The exam just get updated so other blog I read is somehow outdated already I would recommend this one for your preparation.
medium.com
Good luck!
UPDATE 2020 : Data Engineering Learning Path now include Data Fusion and Cloud Composer. More labs on advanced BigQuery, BigQuery ML, and Bigtable streaming
cloud.google.com
It’s good to be noob so we can learn and have fun ;)
305 
3
305 claps
305 
3
Written by
DLT & ML Debugger
It’s good to be noob so we can learn and have fun ;)
Written by
DLT & ML Debugger
It’s good to be noob so we can learn and have fun ;)
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/gowombat/iot-simple-iot-with-free-cloud-solutions-tutorial-76ac5cf6e5a0?source=search_post---------347,"There are currently no responses for this story.
Be the first to respond.
The resulting dashboard can be found here ;)
I love automation. And I always wanted to automatize the watering of my plants (because I always forget about them). And yes, there are plenty of different turnkey solutions, but that’s not a developer way.
As a first step, I decided to implement the monitoring of soil moisture. But to monitor some value — I need to see that value somehow. And also important to see the historical change of that value. Therefore I need some dashboard and server to host it and some data storage. And that’s cost money. But I don’t need the full power and storage of even the smallest and cheapest server on Digital Ocean for example. Ideally, I don’t want to spend money each month at all for that. So, why not to just use a free plan of plenty of different cloud solutions for everything?
And I was able to achieve that and now I’m ready to share it with you.
MQTT broker — CloudMQTTVery straight and really simple to set up cloud MQTT solution. Has plenty of plans for different purposes and in case you will need to scale. Also has a free plan named “Cute Cat” that allows you to have 5 devices and supports 10 Kbit/s. Pretty enough for an experiment or PoC.
[Update] Currently CloudMQTT says that free plan is “out of stock”. You can try it on Heroku or use an alternative like MyQttHub.
Backend and Database — Back4AppBackend as a Service with a pretty user-friendly interface. Provides you a storage and REST API. It will give you 10 requests per second and 10 000 requests per month. It gives you around 13 requests per hour. So be careful with testing ;)
Dashboard — FreeboardSimple but powerful dashboard. It provides you a cloud solution but there are no free plans. But Freeboard is open-source so we can host it by ourselves. Since it will be self-hosted — no limitations on what you can do.
Server — HerokuPopular Platform as a Service. Has a free plan and gives you the possibility to scale up later with what you need. Great way to host something simple like freeboard.
So, the device with a sensor (soil moisture sensor in our case) will publish its measurements using the MQTT protocol. MQTT to HTTP server will listen to the topic where the measurements are published and will resend them through HTTP API to the Back4App. Back4App will store all the history of measurements and provide us an endpoint to retrieve that data. Freeboard will use that API to load the last 50 measurements and build sparkline based on it.
Since our device will work with MQTT and Back4App has only REST API, we need one more layer between them — a simple MQTT to HTTP translator that will listen our MQTT topic with sensors data and send it to the Back4App API.
In that particular case using MQTT is not required, the device simply can send an HTTP request directly to Back4App. But it will be simpler to add more devices and implement communication between devices using MQTT broker.
Setting it up using CloudMQTT is as simple as possible. Go to the web page, click on the button “Get a managed IoT broker today”. Choose a free plan “Cute Cat” and Sign Up. After that, you will need to choose a name and region for your project and then you will have all the credentials you need to work with it.
Here is a small example of how to connect and publish a message using MicroPython:
Back4App gives you a way to rapidly set up your database and start working with it. Just sign up, click on Build new app and choose a name. After that, you will see a dashboard and a small tutorial will be started. When you pass the tutorial you can create a new clear App or use that one if you want.
In the App, you’ll need to create a new class named moisture (actually you can name it whatever you want but in next steps, I will use that name).
By default, any class you create has the next columns: objectId, updatedAt, createdAt and ACL. It’s useful for as to have a column so we don’t need to take care of the time of the measurement.
We need to add one more column named value with type Number to store the actual value of soil moisture. And we’re mostly done after that!
It’s not so important for some personal experiment to set up the security, so you can skip that part if you don’t want to make your dashboard public to show it to someone.
So, if you don’t want a stranger to be able using your API key from Freeboard (where you can’t hide it) to mess with your data (e.g. deleting everything just because he can). You’ll need to restrict write rights for the moisture table and create a User that will have write privileges.
The class for the users already exists in App by default. So we need to create a new record here. Click on “Add a row” and fill out next fields: email, password, username, emailVerified. Just like that:
After filling out all of the required fields with valid values object must be created automatically and objectId will be assigned to it.
When the user created you need to set up security on the moisture table. Click on the icon with shield and in the dialog window remove the checkmark from theWrite column, in the field below enter the username and press enter. The checkmark on the Write field will be already set and the user’s ID will be shown instead of the user’s name.
The last step is to Log In with the created user to retrieve its session token. Just send the GET request with username and password of the user specified in parameters. From the response, you will need a sessionToken. You can find an example here.
For that part, I decided to make a simple python script using Paho MQTT and Requests libraries. It’s simply listening to thesensors/moisture topic and pushes each value that is published there.
Let’s begin with object creation with Back4App API. Using requests it’s super simple:
All of the keys can be found in the Back4App dashboard, in the App Settings, Security & Keys.
After we ready to create new objects the next step is to subscribe to our MQTT topic with measurements data.
In the example above when any new message will be published to the topic sensors/moisture the function on_message will be called. You can try it out by publishing messages using WebSocket UI in your CloudMQTT console.
So, after we’ve figured out how to work with both parts all we need is to add an object creation to the on_message function:
Last part of that step is to deploy our small app on Heroku. First of all, you need to create an account on Heroku (if you don’t have one already). After that simply create a new app, chose a name and region for it.
Also, we will need to specify a Procfile for Heroku so it will know how too run our application. Simply create a file named Procfile (no extension needed) and put it there:
After it’s done we’re ready to deploy our app:
Basically, almost the same instructions can be found in the “Deploy” tab of your app dashboard.
After deployment is done you can test it using WebSocket UI in your CloudMQTT console.
We will use Freeboard to simply create a dashboard for our small system. First of all, clone it from Freeboard’s GitHub. Then from the folder Git just created run npm install and then run grunt.
Unfortunately, for our setup, we can’t simply use Freeboard as is. That’s because of the default sparkline widget in it designed to work with real-time data, but we need to draw a historical data from our database.
Therefore we will need to make a simple plugin with sparkline that able to show an array of historical data. You can find my simple implementation of it in GitHub. You can simply download a file, add it to the folder plugins in your Freeboard clone and then add the path to the plugin in the index.html :
Important: don’t refresh the page because all of the changes exist only in your browser until you save them.
Now you can simply open index.html using your browser. There you will find an empty and ready to go Freeboard interface. You can play with it as you want, while I will describe to you how to prepare it for our system.
And the first step here is to add a data source. In the “Datasources” panel click on ADD. In the pop up select a type JSON and choose the name you like. In the URL field specify Back4App endpoint to retrieve moisture.
I also added here parameters to retrieve only the last 50 records. order=-createdAt — order rows by creation date in reversed order (the last one will go first).limit=50 — retrieve only 50 rows.
Method must be GET and two headers need to be added:X-Parse-Application-Id: [Application ID key for Back4App]X-Parse-JavaScript-Key: [JavaScript key for Back4App]
All of the keys can be found in the Back4App dashboard, in the App Settings, Security & Keys.
The next thing we need to add is out historical sparkline. Click on ADD PANE to add a pane (wow). Those panes basically a “shelves” for your widgets. After pane is added — click on the plus icon in its corner. In type select the Historical Sparkline. Choose the title for it and add our data source datasources[“moisture”][“results”].
If everything goes well you must be able to see a small graph with your data (hope you have some random numbers in the database, right?).
I also wanted to add an average value for moisture. Here comes another cool part of the Freeboard — you can use JavaScript function as a value for the widget.
To make it happen, create a new widget with type Text. And near the value field — click on the .JS EDITOR. Put that simple code in there:
We don’t want to lose all our hard work here just after we close the browser. So we need to save our dashboard.
Click on the SAVE FREEBOARD in the heading panel and chose the way of JSON file formatting (I prefer PRETTY version). Save your dashboard to the root directory of your clone of Freeboard, we will need it later to be accessible on the server.
After we are done with setting up the dashboard — we might be wanting to make it accessible from anywhere and not just locally. Deploying Freeboard on the Heroku from scratch for the first time can be tricky. But I already figured it out and will give you some simple instructions ;)
You will need to create a new Heroku app for that part.
First of all, we need something to serve ours index.html. For that, we need to add some packages to the NPM: npm install connect serve-static --save.
After that, we need to create a simple server.js file with the next content:
The last step is to specify Procfile:
And that’s all. After it’s done commit all the changes and push it to the Heroku remote (the same process that was done in the MQTT to HTTP setup).
That’s why we needed to add the dashboard JSON to the repository. To load some JSON with dashboard you just need to add that small part to the end of the URL to your Freeboard #source=dashboard.json. E.g.
If everything was smooth you now must have a working prototype of your cloud-based IoT infrastructure. The one I ended up with you can find here:
http://freeboard-cucumbers.s3-website.eu-north-1.amazonaws.com/#source=cucumbers.json
Thanks for reading! Hope you liked it.
German Gensetskiy under the support of Go Wombat Team.
P.S. In case of any problems with following the tutorial I will be happy to hear your feedback. You can reach me out by the email: Ignis2497@gmail.com
Go Wombat Team Blog
306 
2
306 claps
306 
2
Go Wombat Team Blog
Written by

Go Wombat Team Blog
"
https://medium.com/coinmonks/polkadot-hello-world-1-cost-effective-cloud-deployment-of-a-validator-node-d3c1bbdb9200?source=search_post---------348,"There are currently no responses for this story.
Be the first to respond.
This tutorial is based on Polkadot POC2.
The following article series “Polkadot Hello World” (describes the steps and findings during my personal kickoff phase in the area of the Polkadot multi-chain framework. The full article series can be found here.
In the last months, I was extensively researching projects in the area of blockchain and crypto with the goal to find a challenging visionary…
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://towardsdatascience.com/google-colab-your-python-workspace-on-cloud-c3aed424de0d?source=search_post---------349,"Sign in
There are currently no responses for this story.
Be the first to respond.
Karan Bhanot
Jul 8, 2019·6 min read
Today, I switched to a new machine for my development. While there is a learning curve attached to get aligned with the new machine, I can still continue to work on my projects and pursue my online courses through Google Colab. It’s practically as simple as logging into my Google account and I’m all set.
"
https://betterprogramming.pub/the-cloud-isnt-developer-friendly-anymore-9f57ad55d6be?source=search_post---------350,"Sign in
There are currently no responses for this story.
Be the first to respond.
Tyler Finethy
Mar 11, 2021·5 min read
When I mention configuration management, I know the battle-worn developers out there shiver. I can’t count the number of failed releases I’ve seen as a result of a seemingly innocuous config change. Even Google has published seven postmortem reports related to config errors. During one incident, they…
About
Write
Help
Legal
Get the Medium app
"
https://towardsdatascience.com/building-your-own-deep-learning-computer-and-saving-money-on-cloud-services-c9797261077d?source=search_post---------351,"Sign in
There are currently no responses for this story.
Be the first to respond.
Chris Fotache
Oct 11, 2019·8 min read
After struggling with Microsoft Azure’s GPU VM’s for a few years, and hearing that Amazon’s AWS is not much better, I decided it’s time to have my own local deep learning machine.
One of my main reasons was that the cloud VM doesn’t have a display, therefore you can’t do anything visually. No big deal if you just train there and then run the model on a local computer but if you need to work on simulation-based robotics projects, those won’t run at all in a virtual environment.
I later found that not only building an almost state-of-the-art machine pays for itself in about 4 months, but it is significantly faster than a cloud server (mainly because of local data transfer speed, since everything is in the same box on the same bus, while the cloud service might have the compute units and storage in different racks — so even if the GPU is faster, it can’t get the data fast enough to benefit from that speed).
My system ended costing just under $3K (compare that to about $800/month you’d pay for an entry-level cloud GPU from AWS or Azure). That was in May 2019, and prices tend to vary a lot so it can be 10% lower and higher at any time. Also, by the time you read this technology might’ve already evolved.
You might ask why go to the pain of building the computer yourself instead of buying a high-end super-computer. It’s because ready-built deep learning systems are insanely expensive. But if you are still afraid to tinker with expensive components and are interested in a pre-built system, I found that Exxact sells some of the most affordable deep learning systems starting at $5,899 (2x NVIDIA RTX 2080 Ti + Intel Core i9) which also includes a 3 year warranty and deep learning stack. They are also recommended by another AI engineer, Jeff Chen.
To make sure everything works together, I recommend using PC Part Picker. It will both show you the lowest price for each component, and ensure that you don’t pick incompatible parts. As for putting things together, YouTube rules. Just type the name of the component and you’ll find several very explanatory videos on how to install it. So now lets go over the components that are required:
CPU
Here you’ll have to make a big choice: AMD or Intel. For my entire life I’ve been an Intel fan, but for this machine the CPU is not the most important part. That is the GPU. And Intel CPU’s can cost twice as much as the AMD counterpart. AMD’s new Ryzen line has very good reviews, and I don’t need to overclock it since I’m not playing video games with it. Therefore I went for the AMD Threadripper 1920x, which has 12 cores and 24 threads, more than enough for my case. It was reasonable priced around $350 but prices were dropping. The alternative would be the 10-core Intel i9–7900 at over $900.
CPU Cooler
AMD CPUs have always ran very hot (one of the main reasons they weren’t that reliable). They still are, so you definitely need a liquid cooler. I went with the Fractal S24 which has 2 fans, at about $115. An alternative is the Corsair H100i.
Motherboard
The main choice about the motherboard is the chipset. The simple rule is: For AMD Threadripper, use X399. For Intel 7900, use X299.
Based on reviews, I went with the MSI X399 Gaming Pro Carbon AC, which has everything I needed for deep learning. You’ll find it at just over $300. Other good alternatives are the Asus ROG, Gigabyte Aorus and Asrock Taichi (just make sure it has at least 40 PCIe lanes). You have to make sure the board design accomodates the size of the GPU, and maybe adding multiple GPU’s. The MSI one has plenty of room, and everything is well placed.
GPU
Now this is the most important component of your deep learning system. You have to go with an Nvidia GPU, and the minimum recommended is the GTX 1080 Ti. Unfortunately, when I was looking, that was impossible to find at its regular price of about $800 (blame gamers? crypto miners?). So I had to go to the next level, the RTX 2080 Ti, which is not easy to find either, but I was lucky to get on an excellent $1,187 deal from EVGA. RTX is the newer generation, with one of the best performance among early 2019 consumer GPU’s. I’m glad I was “forced” to make that choice. If you look around, you might still find deals around $1,200. I think EVGA and Gigabyte are the top manufacturers, and the choices you make are about the cooling system. The EVGA RTX 2080 Ti XC Ultra has dual air coolers and that proved enough so far, it never got to critical overheating.
Memory
For the configuration above, DDR4 is the best choice. Corsair is probably the main manufacturer. And it’s 2019, you need 64Gb. So I ended up with 4x16Gb Corsair Vengeance LPX DDR4. I paid $399 but prices are dropping dramatically, they’re well under $300 by now.
Hard-Drive
SSD is old tech by now. The state-of-the-art is the M.2 standard, and that drive plugs right into the motherboard into a PCIe slot. Going at the main bus speed, this is basically a high-capacity, persistent memory chip. I really liked the 1Tb Samsung EVO SSD M.2. I paid $241 but prices for this also went down towards $200. If you need more storage, you can add a regular SSD drive which should be less than $100.
Power Supply
PCPartPicker will make sure you pick a power supply big enough for your system. There are also other online wattage calculators. With one GPU you probably won’t get close to 1,000W but if you plan to add a second GPU, then you need 1,200W to be safe. EVGA is a solid manufacturer and I picked the EVGA SuperNOVA P2 Platinum 1200 which is around $250.
Case
There are plenty of options here, and it might come down to personal preferences and design, but it’s important to make sure it’s big enough to fit all the components in without being cramped, and to have good air circulation. I went with the Lian-Li PC-O11AIR at $114 because it fit those requirements. It’s very roomy, everything is well placed inside, and there’s good cooling.
Additional Cooling
After you’re done with your build, you might want to add additional fans to improve the air flow. My case came with several fans, but I got additional ones, to fill almost every mounting location. It can never get too cold in a GPU machine that’s gonna crank up convolutional networks. I got an 80mm Noctua for the back, and also a regular 120mm Corsair that I added on top. And yes, I got theRGB one. I didn’t care much about bright shiny colors in my case (since it’s under the desk anyway), but in the end I gave in and bought a cool fan.
Assembly
Like I said, search for each component on YouTube and you’re sure to find detailed walkthroughs about installation. As an example, here are a few that I followed: a build similar to mine, a walkthrough of the MSI X399 motherboard and its components, and a focus on the Threadripper mounting. And read all the installation instructions in the manuals. For example, be careful about the slot locations of the memory units.
Basically, the order of operations is this:
First, prep the case, install the power supply and pull the power cables. Then prep the motherboard, install the CPU and then the M.2 drive. Mount the motherboard in the case and add the CPU cooler. Then add the other fans and connect the power and button / lights wires. Finally install the memory modules and the GPU.
After you’re done and you power up the system, finish with cable management and optimize cooling. For example, I ended up removing most dust filters that were covering the fans. I made an intense GPU-heavy test protocol (training a Yolo model) and kept moving fans around until I got the lowest temperatures.
Software Installation
That’s where the fun really begins, but it’s not the focus of this story. Spring of 2019 — you’ll probably go with Ubuntu 18.04, the Nvidia drivers for your GPU version (do that quick, or the display will pretty much suck), CUDA 10, and then whatever frameworks you use (PyTorch, Tensorflow, etc). And enjoy higher speeds than any cloud GPU you’ve tried at a one time price that pays off in a few months.
Component List
Here’s my parts list, with the prices from April 2019. You can also see updated prices on my PCPartPicker list.
Here are a few other alternative builds, that I’ve used for inspiration and education: Jeff Chen’s, Colin Shaw’s and Wayde Gilliam’s.
Chris Fotache is an AI researcher with CYNET.ai based in New Jersey. He covers topics related to artificial intelligence in our life, Python programming, machine learning, computer vision, natural language processing, robotics and more.
AI researcher at CYNET.ai, writing about artificial intelligence, Python programming, machine learning, computer vision, robotics, natural language processing
670 
3
Every Thursday, the Variable delivers the very best of Towards Data Science: from hands-on tutorials and cutting-edge research to original features you don't want to miss. Take a look.
670 claps
670 
3
Your home for data science. A Medium publication sharing concepts, ideas and codes.
About
Write
Help
Legal
Get the Medium app
"
https://levelup.gitconnected.com/dockerizing-and-autoscaling-node-js-on-google-cloud-ef8db3b99486?source=search_post---------352,NA
https://medium.com/@adamelmore/descent-into-cloud-madness-12-aws-certifications-in-6-weeks-965de12c626d?source=search_post---------353,"Sign in
There are currently no responses for this story.
Be the first to respond.
Adam Elmore
Feb 24, 2021·5 min read
This morning I passed the AWS Certified Cloud Practitioner exam (CLF-C01) to earn my final remaining AWS certification. I’m not the first to accomplish this feat—earning all twelve of the available AWS cloud certifications—but I may have been the fastest. In total, I spent forty-one days—a little less than six weeks—preparing for and taking the twelve required tests.
Below, I’ll answer some of the common questions I’ve received and share some tips in hopes that I might inspire others to dive deeper into the depths of AWS for fun and profit.
I initially knocked out the three associate level exams during a week in July of last year. Six months later, I got the itch to dive back into these and see how quickly I could collect them all.
Here’s the final timeline:
Why put myself through this knowledge gauntlet? There were several reasons:
The resources you leverage for study can make a huge impact on your comfort level heading into an exam. I’ve used some fantastic free and paid materials over these six weeks, and some less-than-fantastic ones as well.
Generally, there are two buckets of study materials: guided courses and practice tests. I recommend settling on a single guided course per certification, as it can be overwhelming if you’re staring at 40+ hours of content spread across multiple courses.
Paid resources I purchased:
Free resources I took advantage of:
Overall, I enjoyed this journey immensely. A steady stream of learning objectives and the pressure of test days made for an intense six weeks that I’ll look back on fondly.
I do have one suggestion for the AWS training team. These exams skew pretty heavily towards legacy and hybrid architectures, which makes sense, I suppose. However, I’d love to see an exam focused on modern day web and “serverless” infrastructure. After taking twelve AWS exams, I didn’t answer a single question around AppSync, for instance, which is a shame.
My final (and most important) piece of advice for anyone on the AWS certification path: book the exam! There’s nothing that can hone your focus like a looming deadline. Pick a date that makes you comfortable, and then move it up until you’re not. You’ll be amazed at what you’re capable of with a proper nudge! 🚀
If this post inspired you to take your next step, I’d love to hear about it! You can follow me on Twitter where I’ll be sharing more certification tips and resources!
Building things in the (AWS) cloud and helping others do the same. Co-founder @statmuse. 12x AWS Certified.
391 
5
391 claps
391 
5
Building things in the (AWS) cloud and helping others do the same. Co-founder @statmuse. 12x AWS Certified.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/hackernoon/how-to-choose-a-cloud-computing-technology-for-your-startup-2527c0d68ce5?source=search_post---------354,"There are currently no responses for this story.
Be the first to respond.
Cloud computing technology becomes a standard when talking about developing applications nowadays. A few years ago, companies were enforced to have dedicated teams for configuring, running and maintaining server rooms which made it extremely difficult to scale up easily and offer a sustainable product. For small startups, it was even more difficult due to the lack of human resource as well as funding.
In present days, not only there are cloud computing technologies for almost every architecture you might imagine, but the cloud vendors also compete nonstop about our (the developers) attention. Most of the largest tech companies, like Google, Amazon and IBM launched cloud services in the past few years. They advertise, offer free tiers, present in tech conferences and conduct free-of-charge workshops for experiencing with their cloud solutions. They are aware that once you fall in love with their services, it will most likely be your favorite choice in every project for years to come.
So what is a cloud provider anyway? A cloud provider is an entity that offers cloud services for operating your application. Operating may include running servers, serving your application, hosting static files, providing database solutions, handling networking between servers, managing DNS and much more. Different cloud vendors offer different levels of abstractions in their services, usually defined as IaaS vs. PaaS.
IaaS, or infrastructure-as-a-service, refers to a low-level solution, like providing a Linux Ubuntu server with nothing installed on it. This kind of solutions is suitable for more advanced developers who have experience with designing, configuring and securing servers infrastructure in all aspects. IaaS services provide you with flexibility and scalability down the road, and this will most likely be the way to go when designing application for scale. This approach requires, as already mentioned before, at least one developer in your startup who has this skill-set, otherwise, your product will turn into a big mess sooner than later.
PaaS, or platform-as-a-service, refers to a fully-maintained and managed environment that is hidden under a layer of abstraction you should not even care of. The cloud vendor takes care of maintaining the servers needed for the operations for you, and you get high-level databases for storing your data, services for user authentication, endpoints for client side applications etc. This approach is much easier and faster to get up and running with, and typically satisfies most of the basic applications. You should take into consideration though, that for more complex architectures it might not be enough.
Generally speaking, both IaaS and PaaS are huge time-savers when dealing with deploying and serving applications. You are able to run a server with a click-of-a-button and usually pay per use. Scaling your servers can be done manually or even automatically using APIs when a peak in traffic suddenly occurs. You can be sure that you’re in a good company (as long as you choose wisely) and whatever you can imagine, you can basically create.
In early-stage startups, using cloud computing technologies became a standard because of the flexibility, the pricing models and the accessibility. Choosing the best cloud service for your startup is an essential task every technological entrepreneur must perform. As the head of development in your company, you should know the differences between the main alternatives, and choose the one that suits your product best.
Technical debts may stack up in a case of a bad decision. In addition, migrating an entire architecture from one cloud provider to another is not considered to be a trivial task at all. Therefore, you should be able to know the differences, experiment with each of the main alternatives and make a wise decision.
After examining and experiencing the best cloud providers out there:
and using them in a wide variety of project, I’ll take my top two: AWS and DigitalOcean and compare them using a set of parameters.
I’ve chosen these two cloud providers to be my best choice after grading each of them using the most important parameters when building a startup from the ground up:
How wide is the range of services offered?
Amazon Web Services: AWS has by far the widest range of services and it comes to offerings. If you don’t find a cloud computing technology under AWS manifest, you’ll most likely not find it anywhere else. AWS has many different IaaS and PaaS services dedicated to every task needed to be performed by a server, divided into organized categories. When using AWS you can be sure that your startup scalability is potentially endless. On the other hand, the offering might sometimes be confusing for beginners as it makes the getting started process a little longer. If your application has many custom components that are not trivial, AWS might be the cloud provider you should consider.
Grade: 5/5
DigitalOcean: DigitalOcean offers a relatively narrow range of services. As for IaaS, you can find droplets (servers), data storage units, networking and monitoring services. As for PaaS, you can easily deploy apps with zero configuration needed, like Node.js, Redis, Docker etc. Although the offering is very concise, I find it to be exactly what you need for more than 80% of the applications. In addition to the standard droplets, high CPU and high memory droplets are available for custom use, as well as backups and snapshots for each droplet. DigitalOcean team is working nonstop on increasing their offering based on the community requests. As a developer who uses DigitalOcean for quite a lot of time now, I can admit that their desire to satisfy their community is highly appreciated.
Grade: 4/5
Pricing models available and transparency
Amazon Web Services: AWS is based on a pay per use pricing model. Every cloud computing technology has its own unique pricing and a pricing calculator is available for trying to estimate your costs upfront. You might find this calculator a bit complex if you haven’t used AWS before. In order to estimate your costs up front you need to translate your servers architecture design into AWS terms, and then try and estimate by choosing the appropriate services from the sidebar. I find the wide range of offering sometimes overshadows the costs estimations, so I find it useful sometimes to start firing up services and tracking the costs inside the dashboard using pricing alerts. On the other hand, AWS offers a very useful free tier for 12 months that can help early-stage startups get up and running.
Grade: 3.5/5
DigitalOcean: DigitalOcean extremely transparent pricing models exist in two different yet similar approaches: pay per hour and pay per month. When using DigitalOcean you have no surprises. You can calculate the exact amount that will be charged, due to fixed prices for each droplet unit. Starting at $5/month for a 512MB droplet, DigitalOcean is suitable also for tiny side projects. Besides droplets and data storage units that are charged according to the resources allocated to you, networking, monitoring, alerts, DNS management and more, are completely free of charge. Bottom line, you pay only for the allocated resources, and you get a lot of useful extra components as a free of charge service.
Grade: 4.5/5
How easy is it to get up and running as well as to iterate
Amazon Web Services: AWS dashboard is quite comfortable once you get used to it. Because of the large amounts of services, you might find it a bit overcrowded in comparison to the other alternatives presented here. You can use the default settings for your services and then to get up and running relatively quickly, but if you’d like to dive deeper into details (also for reducing costs) you might find yourself spending quite a lot of time on configurations using AWS dashboard. On the other hand, in large-scaled applications, you can find the additional features available for each service extremely useful and necessary.
Grade: 4/5
DigitalOcean: DigitalOcean is branded for a reason as “Cloud computing, designed for developers”. As developers, we have so many things to take care of, especially when in charge of the end-to-end technological stack of our startup. Therefore, we need our cloud provider to be as simple as possible to setup. DigitalOcean’s user interface is the best I’ve used. It’s intuitive and let you get up and running in minutes even when using it for the first time. You don’t need to explore and scroll over too many features and options, just choose your Linux distribution, plan and geographic location, and you’re up and running in no time.
Grade: 5/5
Available resources and support team
Amazon Web Services: AWS has a very useful tutorials library. There are many tutorials, but ones sometimes seem to be less detailed and user-friendly than others. You need to be experienced with servers infrastructure design before accessing many of the AWS tutorials. So, it might take you some time to explore their library before you’ll be able to actually find what you’re looking for. On the other hand, their customer support team is extraordinary. AWS support agents are super responsive and sensitive and will answer your questions in a professional way.
Grade: 4/5
DigitalOcean: The tutorials library of DigitalOcean is endless. In almost every Google search about a topic related to servers or cloud infrastructure, you’ll find results from DigitalOcean tutorials library. The tutorials are well-written and cover important principals alongside with the technicalities of how to achieve your goal. In addition to accomplishing your task, you’re actually learning new things when following DigitalOcean’s tutorials. The support team is very responsive and professional, and free of charge virtual meetings are available with cloud specialists to help you design the architecture of your server.
Grade: 5/5
Amazon Web Services: AWS is by far the leading cloud provider when it comes to offering, scalability and features. On the other hand, its learning curve is moderate, so if you haven’t experienced with AWS before, it might take you some time to get up and running with properly.
Final startup grade: 4.5/5
DigitalOcean: I like comparing DigitalOcean to a boutique hotel. When using their cloud computing technologies you feel like you’re part of a family and treated like one. DigitalOcean covers everything you need as an early-stage startup, it is easy to use and provides expected convenient pricing models.
Final startup grade: 5/5
The most important thing about cloud provider is to have one. In our world, it’s much better to have your application deployed in a little smaller cloud provider than keep arguing about which cloud provider is better when you have no idea where your application will be 6 months from now.
If you’re familiar with one of the cloud vendors, use it for your main startup unless you’re sure it will not meet your requirements.
When developing side projects, I highly encourage you to try and play with new cloud providers. Who knows, maybe you’ll fall in love with another.
Try DigitalOcean with $10 credit | Try AWS free tier
Find more great tips for technological entrepreneurs at CodingStartups.
Published originally here.
#BlackLivesMatter
501 
9
501 claps
501 
9
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
CEO, Co-founder @ DataGen | https://datagen.tech/
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
"
https://medium.com/nft-cloud/cloud-nft-cloud-pre-sale-details-9266215ae697?source=search_post---------355,"There are currently no responses for this story.
Be the first to respond.
Cloud NFT is a cross-chain ecosystem that aggregate NFTs from all marketplaces on different blockchains, mint and exchange NFTs with zero fees and flawless user onboarding. The whole ecosystem operates on the native API that maintains the entire processes within the platform.
Learn more about Cloud NFT ecosystem and join our Telegram group 👉🏼https://t.me/NFT_Cloud_chat
Dear community, we are glad that you have shown so much interest in the Cloud NFT project and we will be pleased to see all of you as a part of our great NFT ecosystem. Join our waitlist to stay tuned with the latest updates!
Today, we’d like to share all the details of the Cloud NFT pre-sale round and disclose the participation procedure.
Be among the first $CLOUD holders and grow the ecosystem together with us!
📅PRE-SALE date: November 25th, start 10:00 АM UTC
🧮QUANTITY: 7,000,000 CLOUD tokens
💰PRE-SALE PRICE: 0.32 USD
🔗SUPPORTED CHAINS: Ethereum, BSC
💎ACCEPTED CURRENCIES: Ethereum network — ETH, USDT, USDC; BSC — BNB, BUSD, USDT
The Cloud NFT pre-sale round will be supported by two chains — Ethereum and Binance Smart Chain. This tokensale round will be managed by smart contract and will take place on the Cloud NFT website [the appropriate interface for each chain will be adjusted to the tokensale webpage].
We understand how volatile the market is right now and we want to secure our first adopters. Therefore, the Cloud NFT dev team decided to implement an additional security mechanism for investors using ETH as an enter currency. The mechanism will be supported in the Ethereum mainnet only and will work as follows:
💎before pre-sale starts, the smart contract will fix the current ETH price
💎the price will remain fixed within the Cloud NFT tokensale page interface for all participants
💎if the actual price of ETH grows during the pre-sale, the participants will get the additional amount of $CLOUD tokens in the form of an airdrop [corresponding to the difference from the actual price and the fixed price for this round].
For more info, please contact us in the Telegram chat 👉🏼https://t.me/NFT_Cloud_chat
Cloud NFT is building a great environment for the best NFTing experience and flawless onboarding for everyone, and we would like to attract as many users as we can. Yet, unfortunately there are some significant legal limitations that we cannot disregard. Therefore, we would like to inform you that the US and Republic of China citizens are NOT allowed to participate in this sale round. See the full list of restricted countries here.
The maximum personal cap will reach 15,000 USD in equivalent.
ALL investments above 15,000 have to be whitelisted via Cloud NFT webpage through the following interface👇🏻
We’ll publish more information regarding $CLOUD token cliff and vesting details as well as step-by-step instructions for pre-sale participants soon. So if you don’t want to miss it, follow our social media channels:
💎Website:https://cloudnft.io/
💎Twitter: https://twitter.com/Cloud_NFT_api
💎Telegram news: https://t.me/NFT_Cloud_chat
💎Telegram Announcements: https://t.me/NFTCloud_api
Create.Mint.Trade with Cloud NFT.
4.1K 
4.1K claps
4.1K 
Written by

Cloud NFT is a place where you can easily design, create and trade unique NFTs with the minimum expenses!
Written by

Cloud NFT is a place where you can easily design, create and trade unique NFTs with the minimum expenses!
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-developers/firebase-google-cloud-whats-different-with-cloud-firestore-40f1fc3e6d1e?source=search_post---------356,"There are currently no responses for this story.
Be the first to respond.
Top highlight
Previously in this blog series, I talked about the relationship between Firebase and Google Cloud Platform (GCP). In the last post, you learned about some of the specific differences with Cloud Functions. In this post, I’ll do the same with Cloud Firestore (Firebase, GCP). It’s a really good time to talk about Cloud Firestore, since it’s now generally available (Firebase, GCP) with a production quality service level agreement. I’m sure many of you have tried out Cloud Firestore already and are eager to use it in your apps!
Cloud Firestore is a massively scalable, cloud-hosted, NoSQL, realtime database. In order to structure your data, you define collections (similar to tables in SQL) which contain documents (similar to rows). Each document contains fields that contain the actual data. You can reference an individual document using its unique path, or you can query a collection for documents whose fields contain the data you’re looking for.
Cloud Firestore is unlike some of the other Cloud products that have a Firebase SDK for mobile development (such as Cloud Functions and Cloud Storage). Instead, the Cloud Firestore product is a collaboration between the Cloud and Firebase teams, combining the expertise with realtime data, learned from Firebase Realtime Database, and the scalability of Google Cloud Platform.
Cloud Firestore has SDKs for mobile apps and server components (which I will discuss later), but the underlying database product is the same. If you’re a Google Cloud developer using Firestore as part of your backend architecture, or you’re a Firebase developer whose users access it directly from your client app, Cloud Firestore stores data and scales in exactly the same way. However, there are some differences you should be aware of.
Using the Cloud console, you can browse data in the Cloud Firestore database in your project. It looks like this, where the left column lists all the top-level collections. When you select a collection, the middle column lists the collection’s documents by their unique IDs, and finally the right column shows the fields in a selected document.
This is similar to the view of the same data in the Firebase console:
Both consoles have a way to switch between showing data and showing indexes. In the Cloud console, you switch between the two in the navigation panel on the far left, whereas in the Firebase console, you switch between tabs above the content area.
Unique to the Firebase console, you’ll notice additional tabs for Rules and Usage. Usage shows a graph of reads, writes, and deletes for the current quota period:
This is convenient for Firebase developers, who might try to get a sense of what the per-user usage is like for the app that they’re working on.
If you look carefully, you can also see where your usage stands against the free quota per day. Also notice the link at the very bottom, which sends you off to the Cloud console for more detailed billing metrics:
Data, indexes, and billing are all essentially the same in the consoles for both Firebase and Cloud. But the tab labeled “Rules” is special to Firebase. I’ll get to that in a bit. First, let’s talk about the client libraries that Firebase adds to Cloud Firestore.
GCP provides SDKs for working with Cloud Firestore in server-side code. There is support for Python, Node.js, Java, C# (.NET), Go, PHP, and Ruby. These SDKs are expected to run in a trusted environment, where you have control over execution. For web and mobile apps, where you must ship the code to the end user to run directly, Firebase provides additional libraries for iOS, Android, web (and likely, in the future, for games running on Unity and C++).
One important optimization that the client SDKs give you is the ability to cache documents on the mobile device. As long as offline persistence is enabled, all documents previously read are automatically cached on the device. The next time the client queries for a document, if the cached version is up to date with the version on the server, the cached version will be used, saving you money and your users time. It also works while the device is offline. This caching feature is available only with the client SDKs provided by Firebase, but not with the server SDKs.
Another difference between the mobile and server SDKs for Cloud Firestore is the assumption about where the code is executing. Server code typically runs in a low latency environment, especially within other Google Cloud products. In this situation, other optimizations are in place to help database operations (especially transactions, which must round-trip between the caller and the database) complete more quickly.
Firebase also provides the Firebase Admin SDK (for Java, Python, Node.js, and Go), which is often used to implement some of the backend of your mobile app. Like the Cloud SDKs, the Admin SDK is meant for use exclusively on the backend and never for use in code that ships to end users. When working with Cloud Firestore using the Admin SDK, you should know that it effectively repackages, or wraps, the underlying Cloud SDK. The Admin SDK doesn’t offer any new APIs for Firestore — it simply gives you access to the existing Cloud API. As such, if you’re using the Firebase Admin SDK, you will end up consulting Cloud API documentation to understand how things work.
Backend code is typically considered to be “privileged” and normally operates without restrictions, because it’s assumed that you (the developer) have full control over the execution environment. When you initialize one of the Cloud Firestore server SDKs, you must provide a service account that gives permission to the SDK to perform actions in your project. So, if you need code that credits virtual money to your game’s players, your backend code can (and should) do that with no problems because your backend is authorized by a service account with access to your project. However, it’s highly unlikely that you want your end users to be able to modify the document that contains the record of their credits, intentionally or not! When you ship client code that accesses your database, you should assume that the code can’t be trusted, nor can the data it produces. Code running on end user devices can be modified and compromised to do whatever the attacker wants (after all, they may control the end user’s device entirely!). Since you can’t trust the end user and their device, you should implement appropriate security rules to protect your database. And it’s not even just malicious users — it’s also important to protect yourself from your own buggy code (we’re all guilty of this one)!
Cloud Firestore security rules are used to limit read and write access to Cloud Firestore coming from your mobile clients that are using one of the Firebase SDKs. This is unique to Firebase, and there is no equivalent on the Google Cloud side. The rules are enforced on the server, so they’re impossible for a client to bypass them.
By default, all access to Cloud Firestore coming from one of the mobile SDKs, or the Cloud Firestore REST API (when it’s presented with a Firebase Authentication user ID token) is rejected. To allow access to document, security rules typically employ some combination of:
So, you could easily write a rule that only allows access to a document if it has the same ID as the user’s authenticated user ID. And you could check to see if the fields being written have valid values. Note though, that security rules don’t implement a full programming language. You’ll notice that there are no loops, assignments, or procedural statements, but you have the ability to write complex expressions that evaluate to a final true or false value.
Bear in mind, however, when you access Cloud Firestore using one of the server SDKs, it always bypasses the rules. This goes back to the expectation that server code always runs in a trusted environment. If you fully control the code and the environment, then it’s assumed that you aren’t going to do anything harmful (again, excluding any bugs on your end!).
Lastly, if you don’t have end users working directly with Cloud Firestore in a mobile or web app, then you don’t need to be concerned about security rules. Firestore is perfectly usable on the backend without a client app. But if you are shipping an app, I’d strongly recommend integrating Firebase Authentication first, and you should think carefully about structuring your data and rules from there. There can be times when your data structure must follow security restrictions, such as when you want to store both public and private data for a user.
I’m curious to know how you’re making use of Cloud Firestore! Feel free to tweet the team @Firebase and ask questions to us on firebase-talk. And if you aren’t using Firestore yet, perhaps you could try one of the codelabs for Android, iOS, or the web. There’s no charge to get started with the free daily quotas.
Engineering and technology articles for developers, written…
568 
10
568 claps
568 
10
Written by
firebase-consultant.com, Firebase GDE, engineer, developer advocate, Xoogler
Engineering and technology articles for developers, written and curated by Googlers. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
firebase-consultant.com, Firebase GDE, engineer, developer advocate, Xoogler
Engineering and technology articles for developers, written and curated by Googlers. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/swlh/you-need-a-paper-to-cloud-based-notebook-system-7fcb5674c789?source=search_post---------357,"There are currently no responses for this story.
Be the first to respond.
Notebooks are extensions of our minds, a place to explore thoughts and to preserve them. Like minds, notebooks are not created equally.
"
https://medium.com/@oriinbar/the-ar-cloud-is-making-it-rain-66c4c0f164f9?source=search_post---------358,"Sign in
There are currently no responses for this story.
Be the first to respond.
Ori Inbar
Apr 10, 2018·6 min read
It’s not as big as dancing hotdogs, but the AR industry is obsessed with it — and it’s raining green.
Just 7 quick months passed since I coined the term, and I am ecstatic to report: the AR Cloud is fully born.
For those who missed my partner’s and my previous posts — no, it doesn’t stand for “Account Receivable in the Cloud”. Nor does it relate to Arkansa’s weather.
The concept dubbed “Augmented Reality Cloud” — provides something sorely missing with current AR Apps: a persistent 3D digital copy of the real world to enable sharing of AR experiences across multiple users and devices.
Just a few months ago it sounded far fetched; it’s incredible to see how much this concept has already bloomed. This post is a progress report for everything happening in the AR Cloud.
Just in the past month, 3 new investment rounds led by top tier VCs were announced by fantastic AR Cloud companies that emerged from stealth.
Vision: “Blue Vision — build the future of AR”
Money: $14.5M lead by Google ventures
Best for: city scale augmented reality experiences from millions of pictures
Vision: Edit Reality Together
Money: $10.5M led by Index Ventures, with participation from First Round Capital, Kleiner Perkins, Google’s Gradient Ventures, LDVP, A+E and WndrCo
Best for: shared experiences in a confined area
6d.ai — Introduction and Master Plan
Money: an undisclosed amount led by General Catalyst, Super Ventures among others. Conceived while at Super Ventures and led by my partner - the brilliant Matt Miesnieks.
Best for: creating an infinitely large scale mesh from a handheld smartphone with RGB camera, with a goal to support crowd-sourced AR Cloud creation.
These 3 companies join over a dozen companies that already came out as AR Cloud companies — see at the bottom.
With a strong show of confidence, a handful of investors announced a (re)focus on investing in the AR Cloud:
General Catalyst — “to build the AR Cloud”
Shasta Camera fund — “drive the entire AR world forward.”
GFR — “particular focus on mobile AR platforms, AR Cloud”
Presence Capital —” interested in companies that are building AR infrastructure (such as the AR cloud)”
…and of course, yours truly Super Ventures is the fuel behind the AR Cloud.
Forbes article series helped spread the word with a series of articles lead by Charlie Fink:
Who’s seeding the AR Cloud
The AR Cloud will be bigger than search
The Search Engine of AR
Charlie Fink’s Metaverse book made it official — the AR Cloud is now in print. Actually according to Amazon — it’s already out of print!
A series of great explainer articles followed and helped popularized the concept:
Building the AR Cloud by Scape
Why GPS won’t be enough for the AR era by Sturfee
The AR Cloud Explained by Artillry
How We Get to AR Future by Wareable
What’s the point of Augmented Reality by exaquark
AR Cloud is Augmented Reality’s Next Big Evolution by ARCritic
and from GDC… The AR Cloud: Why the future of MR is not a device, it’s all of them
These articles have been translated into many languages such as French (Tout savoir sur l’AR Cloud), German (AR-Cloud für Augmented Reality?, and Augmented Reality ohne AR Cloud ist “nutzlos”), Chinese, and many others.
Since the beginning of 2018, 2 foundations dedicated to building a decentralized open source-style AR Cloud were announced (more to come soon):
On Github: https://github.com/OpenArCloud
https://open-arcloud.org
2. YouAR announced a call for a decentralized, open AR Cloud foundation
https://www.arcloud.world/
This is a true testimonial:
“finally my wife understands what I have been doing for so many years and why it’s so damn important!”
— CEO of an AR Cloud company
You are welcome.
We’re investing heavily in this concept of AR Cloud
— CEO Niantic Labs, John Hanke
During these past few months, we have have seen the big players such as Google, Apple, Microsoft, Facebook, Snap, Niantic Labs, Amazon,… paying close attention to the AR Cloud concept and engaging on multiple levels. It resonated with their vision (and perhaps their FOMO) and might have something to do with accelerating their planning. Two key aspects of the AR Cloud: Persistence & Sharing — may soon be provided by Google and Apple and announced at I/O and WWDC respectively, according to sources familiar with the matter.
These new capabilities will be a significant boost to the global AR Cloud and may make some startups obsolete, but they won’t quite fulfill the complete vision of the AR Cloud and that leaves plenty of room for startups to spearhead the concept and take leadership positions.
AR Cloud is becoming a topic at meetups and events around the globe and will naturally be a big draw at the industry’s largest gathering at AWE USA on May 30-June 1, 2018 in Santa Clara California — don’t miss: Building the AR Cloud panel, and the Open AR Cloud Roundtable.
Blue vision (London) — Bringing new digital experiences to the real world
Ubiquity6 (San Francisco) — Edit Reality Together
6d.ai (San Francisco)- Occlusion, Persistence, Multi-Player, Cross-platform, At Outdoor Scale
YOUar (Portland) —Delivering a World of Shared Augmented Reality
Scape (London) — A digital framework for the physical world
Escher Reality (Boston) [acquired by Niantic Labs]
Escher Reality marks the first AR Cloud company to be acquired this year . I predict it’ll be one of many in 2018.
Sturfee (Santa Clara) — Using satellite imagery to create 3D point cloud
Fantasmo (Los Angeles) — Point clouds for retail and more
Insider Navigation (Vienna, Austria) — Navigation for industrial spaces
Postar.io (San Francisco) — Pin secret digital notes in the real world for your friends to find later
www.arcona.io (Saint Petersburg) — buy virtual land on the blockchain
Vertical (San Francisco)— try their app Placenote!
Recently joined the list, some with flashy press releases:
Immersal — “A new challenger emerges and reaches for the sky and the AR Cloud!”
Miralupa — announcing blockchain-based AR Cloud
Jido Maps — Y combinator recent graduate
0terra — AR Network for multiple users to collaborate
Bubbled — Decentralised virtual spaces and governance for AR content
Mixed Place — the greatest digital revolution in human history
Aromni — The AR Platform of the future
Dent — Retail Reimagined
Xperiel — creating the Real World Web
Is your company missing?
**************
If you are a startup, investor or corporation interested in the AR Cloud- talk to us.
Let’s partner in building the AR Cloud !
To wrap it up on a sweet note: a 3D cake with AR Cloud on top! (sketchfab)
In pursuit of the ultimate Augmented Reality experience. CoFounder of Ogmento, AWE, Super Ventures
See all (22)
571 
6
571 claps
571 
6
In pursuit of the ultimate Augmented Reality experience. CoFounder of Ogmento, AWE, Super Ventures
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@mkahnucf/i-highly-recommend-establishing-an-enterprise-relationship-with-google-cloud-e97f4ac5d69?source=search_post---------359,"Sign in
There are currently no responses for this story.
Be the first to respond.
Mike Kahn
Jun 30, 2018·1 min read
Punch a Server
I highly recommend establishing an enterprise relationship with Google Cloud. It seems you are running a mission critical application on a consumer account and this issue could have been avoided. Reach out to the support team and let them know you want to discuss enterprise options to ensure you have done everything possible to ensure your account is never impacted like this in the future. Ping me if you have any trouble getting through.
Customer Engineer, Google Cloud. All views and opinions are my own. @mkahn5
759 
9
759 
759 
9
Customer Engineer, Google Cloud. All views and opinions are my own. @mkahn5
"
https://medium.com/@grapesfrog/so-youre-going-to-take-the-gcp-cloud-architect-s-certification-1ce8504c029e?source=search_post---------360,"Sign in
There are currently no responses for this story.
Be the first to respond.
Grace
Feb 4, 2017·7 min read
I am not one to put too much reliance on exams and certs to ascertain a person’s capabilities. Yes a university degree kinda proves you have the ability to concentrate, retain facts , study and pass exams but not everyone has the chance in life to attend University yet are perfectly capable of doing all the things that going to university proves one is capable of .
But a university education is fast becoming such an expensive endeavour that I am pretty sure at some point as it was in the “good ole” days a university education will be something for the elite few who are willing to face years of debt or come from a background where they can afford it.
The world does seem to be taking so many steps backwards this century .
I personally tend to be heavily biased towards your actual experience and general cognitive ability. That aside I totally understand there being some sort of benchmark employers need to help them sift CV’s, consultants, partners etc. So having some sort of certification in the area of expertise the candidate or consultant you wish to hire is super useful.
GCP have been hard at work developing a set of professional certifications
I work with GCP as you may have gathered if you’ve read any of my medium posts and as I help customers with using GCP I felt that I really had no choice but to obtain the GCP Cloud Architecture professional certification as soon as I could so I took the beta exam.
I have to admit I was very happy that GCP is not just focused on how adept you are at python or Java as the previous set of training and tests associated with them seemed to be. imho they didn’t actually help roles other than developers to hone their skills for GCP. As an architect I was nonplussed about those . I wanted to see exams and certs that would help folks with architecting solutions for GCP; hone their sysadmin skills for GCP or have the appropriate big data skills to fully exploit those offerings on GCP .
I’ve held both the AWS assoc and the AWS pro Solution Architects certs in the past and I mistakenly assumed that this cert would be like the AWS associate exam and thus I wouldn’t really need to do any studying .
Shortly before the exam I suddenly realised that wasn’t the case and this was going to be a professional level exam .
This was good and bad!
None of this over proliferation of certs at various levels . A good solid well thought through cert for each specialisation so it did make me think about the areas of GCP I needed to double down on.
As I had one day I could carve out before I sat down to take the cert to do any kind of focused study ( bear in mind what my job is !) . This did mean my co-workers had to put up with me grumbling as I realised the undue pressure I’d put myself under. I couldn’t defer taking the cert either as this would mean deferring till April when I could then get round to it . Do NOT do this study properly set up a study plan read , attend relevant conferences , watch videos , do some of the basic coursera training or an in person training course. This is not a tick in the box exercise it is aimed at making sure that anyone who holds this cert actually does have an appropriate level of knowledge on GCP.
The problem with it being a beta exam and I found this particular issue slightly annoying on behalf of candidates is that there was no reading list to help make sure you had good coverage. Reading stuff is my go to way to up ramp on anything.
There were also a distinct lack of direction towards “useful” videos .
The sample questions are not really representative of the questions you get in the exam and I feel some that are more reflective would be helpful . Two example questions are not enough at all . So I really had no practice questions to help me at all ! I am hopeful this will be remedied
The number of questions was insane but as pointed out here I was helping with the quality of the certification to make sure the final certification was solid . I did enjoy the questions that made me think I was working with customers they were actually fun. I can’t give anything away about the actual exam questions but I can say if you’re an architect you should enjoy those questions as much as I did.
The assessment centre I attended was cold so sat there for just over 3 hours ( you get 4 hours to do the beta ) was pretty uncomfortable . Wish I’d not put my coat in a locker ☹️ . I would advise wearing layers of clothing if taking the exam while the weather is dodgy just in case the assessment centre you choose was as pants as the one I selected on the comfort factor.
So what’s the point of this post I hear you ask ?
It’s not just to hear me lament about my lack of prep or my personal opinions on the usefulness of certs. I just felt some helpful pointers on preparing for the GCP Cloud Architecture professional certification would be useful until more prescriptive guidance is provided by GCP:
Actual hands on experience — This is ultimately the best way to prepare and is probably the only reason I managed to pass despite my woeful lack of preparation!
Attend In person training courses
No time to actually attend an in person training course then do some online training.
I went through the coursera sysops online course earlier last month ( January) after I’d taken the beta cert just to see what that was like . It’s actually pretty reasonable imho as a baseline to build upon so I would strongly advise doing this early in your study plan. It also would have been useful for me to get me back into practice into taking an online exam. I hadn’t done one for years so super rusty . Note not all the questions are of the same format you will find in the actual cert.
Read the cert guide but don’t be put off by some of the language as it is crucial in helping you create your study plan as the cert exam does indeed cover all the topics. imho this needs a serious rewrite to not put off folks who are perfectly ready to take the cert from doing so.
This extract from section 3 for example is just weird it’s almost as if it’s deterring non US folks from taking the cert
” 3.2. Designing for legal compliance. Considerations include: • Health Insurance Portability and Accountability Act (HIPAA), Children’s Online • Privacy Protection Act (COPPA), etc. • audits • certification “
Why not generalise and say being able to design for legal compliance requirements and choose standards that are universally in use such as PCI DSS as examples.
Conferences and Videos your thing then start by attending GCP NEXT or watching the sessions from GCP NEXT . Here are the sessions from 2016 The sessions from 2017 look fantastic imho!. Even if videos are not your thing you should watch the ones from the GCP NEXT conference as they do have plenty of information to help you prepare for the certification.
If you’re attending GCP NEXT 17 then why not sign up for a bootcamp or two and do some of the codelabs and while you’re there talk to some of the GCP Cloud Solutions Architects who will be attending awaiting your questions .
Reading list:
Hopefully this prep list helps you to get ready and not do as I did . It should be a fun experience preparing for the certification .. Don’t do what I did !
Just a quick update to say some of my gripes have been answered so your prep should be a lot easier now.
Sample questions — Okay this may not be quite as I hoped but what you do get is a look at some of the sample customer use cases
Those are great , read them carefully.
Don’t be fooled by the title “ cloud infrastructure” track this is where the instructor led training for the Architect’s cert starts
Watch the GCP NEXT 2017 videos . Plenty of great material for you to watch
The coursera architects track starts here it consists of 4 courses . The Sysops track has been deprecated . I haven’t gone through this updated track though but I’m sure it’ll be of the same quality as the sysops course so definitely worth considering if you can’t do the instructor led training.
The docs are constantly being updated and new solutions and blog posts added all the time so I can’t stress enough that you should keep up to date with your reading. I love the flow charts to help customers navigate their way through choices that have been produced for storage & compute ( I leave it as an exercise for the reader to find those!)
Above all as well as studying you need hands on and there are a bunch of cloud focused codelabs to assist
I had to renew my Cert as it was about to run out and I have more words related to that experience here
Chocolate addict - I have it under control really I do. I do stuff involving cloudy tech. Tweets my own so only me to blame, except for retweets.
332 
5
332 
332 
5
Chocolate addict - I have it under control really I do. I do stuff involving cloudy tech. Tweets my own so only me to blame, except for retweets.
"
https://medium.com/google-developers/reducing-the-cost-of-my-developer-workstation-with-cloud-functions-and-pub-sub-d9b9550bf397?source=search_post---------361,"There are currently no responses for this story.
Be the first to respond.
Posted by Matt Cowger, Global Head of Startup Architects
I really don’t love developing on my local machine. Compared to what’s available on a cloud provider like Google Cloud, it’s not as fast, drains the battery more quickly, and generally has lesser network connectivity / performance. Historically I’ve accepted those limitations because developing on a remote machine meant a bunch of tradeoffs around my IDE of choice, etc. However, the recent advances that Visual Studio Code has made with Remote Development Extension means that much of that pain is mitigated. As a result, I wanted to find a way to run my development environment on fast machine in Google Cloud, but not waste a bunch of money running said machine when I’m asleep or it’s the weekend :).
I had some requirements and constraints going into this:
In short, I made the decision to use on demand VMs (which meets items 1, 3, 4), but find a way to schedule them (#2).
To schedule them, I wanted to avoid using tools that run all the time (thus causing an increase in cost), and wanted to avoid running tools on my laptop (which could be off, and I might be using a different terminal device). Ultimately, I decided the best scenario to be using Cloud Scheduler to send a message via Pub/Sub at the correct time to a Cloud Function that starts the appropriate VM.
From the diagram, we can see that Cloud Scheduler is configured to push a Pub/Sub message on the ‘autoonoff’ topic with the ‘start’ command every weekday at 8am, along with some other metadata about my project name, zone, etc. It also includes keys for the Pushover API, so I get a notification when it starts:
Next, I configured a Cloud Function that takes the message from PubSub, and uses the Google Cloud APIs to start the VM:
Because this function is very small, we allocate only 128MB of memory, have it listen to the same topic (autoonoff), and run the short code pasted below. You can get a full copy of the code on my GitHub. Whenever a correctly formatted Pub/Sub message is sent, this code will execute and start my VM.
Change the Cloud Scheduler options and the Pub/Sub message for the stop side (I stop the VM at 6pm local), and you have a fully automated, low(er) cost dev machine!
An exercise left to the reader: there’s no reason you’d have to specify machine names. It would be entirely possible to select compute resources by tag instead of name, and start/stop entire fleets of systems (say a LAMP stack) only during working hours.
Engineering and technology articles for developers, written…
393 
4
393 claps
393 
4
Written by
News about and from Google developers.
Engineering and technology articles for developers, written and curated by Googlers. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
News about and from Google developers.
Engineering and technology articles for developers, written and curated by Googlers. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/exploring-sobriety/how-to-stay-sober-after-the-pink-cloud-wears-off-d0def54e637f?source=search_post---------362,"There are currently no responses for this story.
Be the first to respond.
Before I quit drinking, I imagined that being sober would feel like walking straight along a path. Each day without drinking would be another step forward: I’d feel better, be healthier, and be happier than the day before.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/5-principles-you-need-to-know-before-using-google-cloud-dataprep-for-data-preparation-8b639bd844b5?source=search_post---------363,"There are currently no responses for this story.
Be the first to respond.
Google Cloud Dataprep is an intelligent data service on Google Cloud Platform for exploring, cleaning, and preparing structured and unstructured data.
There are 5 principles important to know before your data preparation with Dataprep.
Before you get started cleaning your dataset, it is helpful to create a virtual profile of the source data…
"
https://heartbeat.comet.ml/using-firebases-cloud-firestore-in-flutter-79a79ec5303a?source=search_post---------364,"Sign in
Sameeha Rahman
Mar 22, 2019·7 min read
Very rarely do we find mobile apps that have no connection to a backend of any sort. There are many services and platforms like Azure or AWS, but one Google product stands out as well: Firebase. It’s easy to use, scalable and—most importantly—free. In this tutorial, we’ll explore how to connect our Flutter mobile app to Firebase.
To demonstrate, we will use an app I have created previously, which can be found in the following tutorial:
medium.freecodecamp.org
All we need to create a Firebase project is a Google account with which to access Firebase. So head on over to the Firebase console to get started. Add a new project, give it a name, agree to all they ask and hit ‘Create Project’. Once the project is successfully created, the Project Dashboard can be accessed.
Firebase has two types of databases:
Firestore is the better option, as it has a better structure than Realtime Database. In Realtime Database, rules cascade. This allows easy data manipulation if the reference and permission to the database are both available. This affects all data stored. And for that reason, I picked Firestore as the database.
I created a collection called tasks that contains the title and description of a list of tasks. I also included a few documents as shown below:
What will be displayed for this tutorial is a list of task titles, each in a separate card. Upon clicking a button on the card, it will direct the user to a new page that displays the description.
Now that we have set up the Firebase dashboard, let’s move on to integrating Firebase into the app.
We must now register both the iOS and Android app in the project overview dashboard.
To register the iOS app, we will need the Bundle Identifier, which is in ios.Runner/Infoplist, with the key CFBundleIdentifier.
Another way to find it is to search the project.pbxproj for the variable ‘PRODUCT_BUNDLE_IDENTIFIER’. Fill in the remaining information (optional) to register the app.
Once registered, we can download the config file GoogleService-Info.plist and move it into the ios/Runner folder.
To register the Android app, we willl need the Application ID, which is in android/app/build.gradle, inside the tag defaultConfig. Fill in the remaining information (optional) to register the app.
Once registered, we can download the config file google-services.json and move it into the android/app folder.
In comparison to setting up iOS, Android has 3 extra steps. We now need to add the Google Services plugin to the android/build.gradle and check to see if the file looks like this:
Then add the line below after the dependencies object in the android/app/build.gradle.
The final step for the Android setup is to run the command flutter packages get, to install new dependencies.
One last step to configure Firebase for both platforms is to install FlutterFire. FlutterFire is a collection of plugins for each different Firebase service. FlutterFire can be used by both platforms, as it depends on the multi-platform Flutter SDK.
Find the different plugins for each service here:
github.com
Since I’m using the Cloud Firestore Database service, I’ll need to install cloud_firestore, the plugin for the Cloud Firestore API.
Update the pubspec.yaml file to include the plugin dependency below, inside dependencies:
Run flutter packages get once more to install the required packages.
To start using the cloud_firestore package, import it at the top of the page like so:
In this page, the collection tasks is queried to return all its values. The titles of each document are then displayed in a list of cards.
To achieve this, the Home page is updated to look as below:
StreamBuilder is a widget that builds itself depending on the Stream of data it gets. A Stream is a data source that receives a series of asynchronous events. In this case, an asynchronous event is considered as one document in the task collection, making the collection itself the Stream.
This data stream is passed as the data source to the StreamBuilder. The builder returns a widget depending on the data in the asynchronous stream.
In the above case, it returns:
The ListView is a list of widgets arranged as a scrollable list. The data sent to it needs to be in the format of a List. To achieve that, documents of the resolved snapshot are mapped to return a CustomCard element. Each CustomCard element returned contains the mapped documents’ title. All the returned objects are then aggregated into a List.
Now a quick look at this ListView implementation;
I’ve also changed the functionality of the floating action button on the home page. It now displays an alert that allows the user to enter a task title and description. Clicking the add button adds a new document to the tasks collection. It checks if both text fields contain values before inserting the document. Clicking cancel dismisses the alert and clears the text fields.
AlertDialog is the widget that displays a Material Design Alert. Within the content is the Column containing the TextFields that allow the user to enter the task title and description. Each TextField has its own controller that has access to the text entered into it.
Each TextField also includes an InputDecoration Class. This adds styling to the input; a label in the above code, for example. We also need to pass a list of possible actions (buttons) to the dialog. One such action is ‘cancel’, which closes the dialog and clears the TextFields. The other is ‘add’ to add the document into the collection.
The TextEditingControllers are first queried to check if the TextFields are empty. If both contain a value, the data is inserted into Firestore. To insert a new document, an instance of the Firestore collection is created. Data is then passed to the method as a JSON object. Upon a successful insert, the dialog is dismissed and the TextFields cleared.
A quick view of the alert dialog implemented;
This element is used to display the title of the document inside a Card Widget. It also contains a button that navigates the user to another page, which contains the task description.
This component displays the title of the task in the header bar and description in the body. Both pieces of data are passed to the page when navigating to it.
The mainAxisAlignment and crossAxisAlignment tags are used to specify the exact location of the elements on the page. In this example, I’m trying to display the elements both vertically and horizontally centered. By default, the Column widget arranges its elements starting at the top of its page.
Now to have a look at all the features newly implemented:
In this tutorial, we were able to;
In a similar way, other Firebase services can be used, after installing the corresponding plugin as specified above. Join me next time on introducing Firebase Authentication for Flutter.
Find the code repo, here.
Find the commit for this post, here.
Editor’s Note: Heartbeat is a contributor-driven online publication and community dedicated to providing premier educational resources for data science, machine learning, and deep learning practitioners. We’re committed to supporting and inspiring developers and engineers from all walks of life.
Editorially independent, Heartbeat is sponsored and published by Comet, an MLOps platform that enables data scientists & ML teams to track, compare, explain, & optimize their experiments. We pay our contributors, and we don’t sell ads.
If you’d like to contribute, head on over to our call for contributors. You can also sign up to receive our weekly newsletters (Deep Learning Weekly and the Comet Newsletter), join us on Slack, and follow Comet on Twitter and LinkedIn for resources, events, and much more that will help you build better ML models, faster.
Front End Developer at Switchd Ltd. | MSc in Computer Science @ QMUL (2022)
See all (6)
626 
3
Thanks to Austin Kodra. 
626 claps
626 
3
Helping data scientists, ML engineers, and deep learning engineers build better models faster
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@madhupathy/simplified-microservices-building-with-spring-cloud-netflix-oss-eureka-zuul-hystrix-ribbon-2faa9046d054?source=search_post---------365,"Sign in
There are currently no responses for this story.
Be the first to respond.
Madhu Pathy
May 30, 2018·11 min read
As Microservice Architecture gained huge popularity the last few years, if you are not using it already, you are most likely to work on such projects soon.
About
Write
Help
Legal
Get the Medium app
"
https://rominirani.com/google-cloud-functions-tutorial-series-f04b2db739cd?source=search_post---------366,"Update : November 8, 2018
— Added new tutorial on Cloud Scheduler, an Enterprise grade Job Scheduler available in Beta now. The scheduler can be used to trigger your function at specific regular intervals.
Update: October 23–24, 2018 — Updated series for Node.js v6 and Node.js v8 runtimes.  — Added new tutorial on running headless Chrome via Puppeteer in Google Cloud Functions. — Added new tutorial on deploying Cloud Functions from Cloud Source Repositories — Added new tutorial on using Environment variables in Cloud Functions
This is a tutorial series on Google Cloud Functions, an Event-driven Serverless Compute Platform.
This offering lies in the Functions as a Service (FaaS) computing offering.
The series will cover the following topics:
All companion code for the tutorial series is available here. Go ahead and clone the repository or better still, Open it up in Google Cloud Shell via the button provided on the Github project page below. Do look at the README.md file in the root folder, it contains instructions on how to run tutorials in Google Cloud Shell itself.
github.com
As Google Cloud Functions continues to add more event providers and language runtimes, I hope to keep this tutorial tutorial series updated to reflect the changes and the new features.
If you have any specific feedback on the existing content and/or would like me to possibly explore covering a topic around Google Cloud Functions, do let me know in the comments or email me.
Next Part : Overview of Computing Options
Technical Tutorials, APIs, Cloud, Books and more.
640 
4
640 claps
640 
4
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
Technical Tutorials, APIs, Cloud, Books and more.
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
Technical Tutorials, APIs, Cloud, Books and more.
"
https://medium.com/free-code-camp/train-like-an-aws-certified-cloud-ninja-534f1770eba2?source=search_post---------367,"There are currently no responses for this story.
Be the first to respond.
“See first with your mind, then with your eyes, and finally with your body.” — Yagyu Munenori
During my journey across the flow of a large-scale enterprise cloud adoption, I discovered that the greatest barrier to mass migrations isn’t the engineering challenges — it’s the short supply of developers with cloud-related skills.
Cloud has gone mainstream, and large companies are now scrambling for trained developers. Cloud computing skills are essential for modern software shops.
This article will show you the way of a cloud guru, and the path toward enlightenment. Your journey toward securing these sought-after skills begins where everyone starts — as a white belt in training.
The White Belt signifies the birth of a seed as it lies beneath the snow in the winter.
An Intro to Cloud Computing is a great place for the student who is thirsting for knowledge and seeking answers to “what is cloud computing?”.
This free course is full of excellent material to get you headed in the right direction. The introduction guides you through the fundamentals of cloud computing, and explore the buzzwords to help demystify the cloud.
The Yellow Belt signifies the first beams of sunlight which shine upon the seed giving it new strength.
Exploring the AWS services will help the yellow belt student obtain the first ray of knowledge and open your mind. The free AWS Technical Essentials course teaches everything you need to know about the “how” and “why” of AWS.
This is great for beginners — no AWS or programming experience is required. The 1-hour course guide you through the fundamentals of cloud computing, until you become more confident with the AWS concepts and terminology.
The Orange Belt represents the growing power of the sun as it warms the earth to prepare for new growth.
As the orange belt student starts to feel the body and mind open and develop, you should sign-up for an AWS Free Tier account. Once you register and create an AWS account, you can use any of the AWS services for free within certain usage limits.
The free tier offers hands-on experience with AWS Services, without the safety net of the lab environment or instruction. Consider applying the learnings from the lab training to create your own personal website or an Alexa skill using multiple services.
The Green Belt signifies the growth of the seed as it sprouts from the earth reaching toward the sun.
To establish strong roots and a firm foundation, the green belt student should look for opportunities to apply their early learnings. A fun and simple way is by using your free-tier account to create a basic Alexa skill.
medium.freecodecamp.org
The free Intro to Alexa training series is designed to help anyone learn how to design and program an Alexa skill.
Not only will you learn the fundamentals of creating Alexa skills, you will learn the basics of using Node.JS and fundamental Amazon Web Services (AWS) services such as Lambda, DynamoDB, and Simple Storage Service (S3).
The Blue Belt signifies the blue sky as the plant continues to grow upward. Just as a plant grows taller, a blue belt student moves up higher in rank as knowledge and experience grows.
For students interested in growing their knowledge of the cloud, the associate-level AWS Certified Solutions Architect is the #1 cloud certification for 2 years in a row. The cloud architect certification training is for anyone seeking to learn the major components of Amazon Web Services (AWS).
Given the high rate of speed that AWS delivers new features and services, it’s important that the instructors are subject matter experts and keep their content up-to-date. The certification training provided by Ryan Kroonenburg reflects his expertise as an AWS Community Hero and passion for teaching others about cloud computing.
The Purple Belt represents the changing sky of dawn. As the student undergoes the transition to an advanced level, a purple belt begins to understand the meaning of the black belt.
The AWS Whitepapers and AWS FAQs offer the purple belt student an opportunity to fill in the details and solidify their knowledge before scheduling the certification exam.
The ‘must read’ whitepapers include:
The Brown Belt represents the ripening of the seed. A brown belt is an advanced student whose techniques are beginning to mature.
To achieve a fuller appreciation of cloud computing, the brown belt student understands the importance of exploring the entire ecosystem of services.
For those seeking to expand their changing horizons, A Cloud Guru offers a series of courses on a variety of cloud computing services. AWS is constantly evolving their platform, with over 1,000 product release each year.
To stay current, explore deep-dives into AWS services such CloudFormation, DynamoDB, S3, Application Load Balancers, Cost Control, Security, and CodeDeploy.
You can easily keep up with the latest releases by listening to a weekly five-minute episode of AWS This Week.
acloud.guru
The Red Belt signifies the red-hot heat of the Sun as the plant continues to thrive. Red is a sign of danger, and the red belt is starting to become dangerous with knowledge and abilities.
After passing the AWS Certification exam, a cloud guru is obliged to share learnings with others in the community. As cloud gurus “pay it forward”, their students blossom and grow through the ranks.
Join an AWS User Group in your area to explain and explore emerging trends, while always seeking opportunities to mentor others in your local community.
The Black Belt signifies the darkness beyond the Sun as the student seeks more profound knowledge. As the student begins to teach others, new seeds are planted to grow and mature.
At this stage of enlightenment, a cloud guru continually thirsts for new knowledge. Explore free introductions to other cloud providers including Microsoft Azure and Google Cloud Platform — and it’s never too late to start learning how to code in the cloud!
A Cloud Guru also supports a community-sourced publication with articles from engineers and leaders in the industry. It’s another free source of cloud insights — with a lot of hands-on tutorials and fun projects.
The journey of a cloud guru is a never-ending process of self-growth, knowledge, and enlightenment. Now that you know the path to enlightenment, take your first step forward today.
I help migrate talent to the cloud. Follow me on Twitter @drewfirment.
We’ve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
241 
2
241 claps
241 
2
We’ve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
Written by
migrating talent to the cloud at acloud.guru
We’ve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
"
https://faun.pub/is-going-serverless-worth-it-pros-and-cons-of-server-less-a7631d13f666?source=search_post---------368,"There are currently no responses for this story.
Be the first to respond.
The process is like a child playing with shape sorter, although sometimes you might need to do modifications either to the shape or to the hole that fits that shape. So, as a DevOps professional, it’s important to do a personal evaluation of any technology. Without further ado, here are my thoughts on serverless.
"
https://medium.com/firebase-developers/patterns-for-security-with-firebase-group-based-permissions-for-cloud-firestore-72859cdec8f6?source=search_post---------369,"There are currently no responses for this story.
Be the first to respond.
In my last post, I discussed a couple straightforward ways to use security rules to protect user-owned documents in Cloud Firestore. The key is to put the UID for the user assigned by Firebase Authentication into either the document ID, or a field in the document, then write the rule to compare that string against the UID provided by request.auth.uid when evaluating the rule. But when you need to give access to documents based on a user’s role in your app, it gets more complicated.
Last time, I showed an example of a chat room implemented by a collection called “messages” where anyone could create or update a document with their own UID in a field called “uid”. The rules to allow access looked like this:
Now let’s say we want to add an “admin” role to the app, where users should be granted the ability to update the contents of existing message documents (in order to correct mistakes to mark them as spam, or as inappropriate). We need some way to store a list of all the users who are in this admin group, and check if the current user is in that list at the time security rules are evaluated. There are a few ways to do this.
Since Cloud Firestore security rules have the ability to get() another document and use its field values, we can pretty easily set up a document dedicated to storing a list of UIDs of admin users. Let’s call it “/config/roles”. We’ll use the “admins” field in that document to store an array of UID strings.
Assuming this document is set up, here’s how it can be used in the rule. First, here’s a standalone rule for checking admin access when updating any document in the messages collection:
This rule is doing the following for each update of any message document:
This works fine when the list of admins has a predictable, limited size. However, if the list has to scale up in size, it’s possible that a single document might not be able to hold them all, since the maximum size of a document is 1MB. You could then try to shard the admin list across two documents, and use both of them in the rule, but this doesn’t really scale. There is a hard limit of 10 document get operations per rule evaluation. So, what if we need the list of admins to scale massively just like Cloud Firestore itself?
When modeling data in Cloud Firestore, if a list of data can get too big for a single document, you should break up that list into one document per item. There are a couple sub-options here.
The first sub-option is to create dedicated per-user documents for admin access where the presence of the document for the user indicates admin access. Let’s create an “admins” collection for this, using the UID as the document ID.
Note that the per-user document doesn’t contain any fields. It just needs to exist with the correct ID.
Now, to find out if a user is an admin in security rules, all we have to do in the rule is check if that document exists:
Here, the full path to the document is being built with the request.auth.uid variable inserted as the document ID in the admins collection.
Alternatively, if there is an existing collection that stores some per-user data, and it’s OK to add another field to that document to indicate group membership, we could use a new boolean field called “isAdmin”. Suppose we have an existing collection called “users” whose document IDs are UIDs, we can just add an isAdmin field to the per-user document:
The rule to check a user’s document for admin access looks like this, which verifies that the user document’s isAdmin field is true:
IMPORTANT: For all of the above cases that rely on the contents of other documents to grant special permission, you should be very careful about allowing users to modify those documents containing the permissions. If your security rules don’t correctly limit access to the documents that assign group roles, you run the risk of users granting themselves admin access simply by creating or updating a document. Bear in mind that if a document doesn’t match any rule, then no user can access it via a client SDK. But you can still use server SDKs to make changes, and the document can still be read by security rules.
It’s worth noting that both exists() and get() in security rules have the same monetary cost as a normal document read. So, just be aware of that when it comes time to estimate billing. But if you’d rather not incur these costs, there is one other option for group permissions.
Firebase Authentication allows for a small amount of JSON data to be stored for each user account: these are called custom claims. They can be set using the Firebase Admin SDK on a backend you control (including Cloud Functions), then used in security rules to check for access to documents.
Here’s a bit of JavaScript code that uses the Admin SDK for node.js to set a custom claims object for a user:
If this completes successfully, the “isAdmin” boolean property of the object can be used in security rules, found in request.auth.token like this:
The limit for custom claims is 1000 bytes of JSON, so if you need a lot of groups, that might cause a problem. In that case, you’re back to using the contents of other documents and paying the cost of document reads. The upside to custom claims is that they can also be used in Realtime Database and Cloud Storage security rules. So if you’re working across products, this is one way to share per-user permissions between each of them.
There’s one other caveat to using custom claims. The API call to set the claims takes effect immediately, however, the new claims don’t get propagated to the client app immediately. If you want that, you’ll have to arrange for it yourself (by having the client call getIdToken after the claims are updated), or the user will have to wait until their current token expires (one hour max).
So, you have a few options to implement group or role permission, each with their pros and cons:
Tutorials, deep-dives, and random musings from Firebase…
505 
3
A summary of what has happened on the Firebase Developers publication in the past quarter. Sent once a quarter. Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
505 claps
505 
3
Written by
firebase-consultant.com, Firebase GDE, engineer, developer advocate, Xoogler
Tutorials, deep-dives, and random musings from Firebase developers all around the world. Views expressed are those of the authors and don’t necessarily reflect those of Firebase or its parent companies.
Written by
firebase-consultant.com, Firebase GDE, engineer, developer advocate, Xoogler
Tutorials, deep-dives, and random musings from Firebase developers all around the world. Views expressed are those of the authors and don’t necessarily reflect those of Firebase or its parent companies.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://betterprogramming.pub/architecting-for-the-cloud-ad3c4505e053?source=search_post---------370,"Sign in
There are currently no responses for this story.
Be the first to respond.
Bob Roebling
Feb 7, 2020·16 min read
The image above might be rocket science, but changing how you develop programs for the public cloud isn’t!
There are many ways to successfully write code that works in Microsoft Azure, Amazon Web Services, Google Cloud Platform, or IBM Cloud. I’m not here to promote one platform over the other —so…
"
https://medium.com/firebase-developers/migrate-your-firebase-cloud-functions-to-node-js-10-d9c677933ee?source=search_post---------371,"There are currently no responses for this story.
Be the first to respond.
You might have heard that Cloud Functions supports Node.js 10 since May 18, 2020 and has deprecated Node.js 8 since June 5, 2020. And in the Firebase console, you’ll notice this message:
Node.js 8 has been deprecated.
- Starting Feb 15, 2021, we’ll no longer support new deploys or updates of Node.js 8 functions.- Starting Mar 15, 2021, we’ll no…
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://towardsdatascience.com/how-to-automate-lidar-point-cloud-processing-with-python-a027454a536c?source=search_post---------372,"Sign in
There are currently no responses for this story.
Be the first to respond.
Florent Poux, Ph.D.
Nov 22, 2020·13 min read
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/free-code-camp/how-to-perform-crud-operations-using-blazor-and-google-cloud-firestore-52890b06e2f8?source=search_post---------373,"There are currently no responses for this story.
Be the first to respond.
In this article, we will create a Blazor application using Google Firstore as database provider. We will create a Single Page Application (SPA) and perform CRUD operations on it. We will use Bootstrap 4 to display a modal popup for handling user inputs. The form also has a dropdown list, which will bind to a collection in our database. We will also implement a client-side search functionality to search the employee list by employee name.
Take a look at the final application.
Get the source code from GitHub.
The first step is to create a project in google Firebase console. Navigate to https://console.firebase.google.com and sign-in with your google account. Click on Add Project link. A pop up window will open as shown in the image below. Provide your project name and click on Create project button at the bottom.
Note the project id here. Firebase project ids are globally unique. You can edit your project id while creating a new project. Once the project is created you cannot change your project id. We will use this project id in next section while initializing our application.
Click on the project you just created. A project overview page will open. Select “Database” from left menu. Then click on “Create database” button. A popup window will open asking you to select the “Security rules for Cloud Firestore”. Select “Start in locked mode” and click on enable.
Refer to the image below:
This will enable the database for your project. Firebase project have two options for database — Realtime Database and Cloud Firestore. For this application, we will use “Cloud Firestore” database. Click on “Database” dropdown at the top of the page and select “Cloud Firestore”.
Refer to the image below:
We will create cities collection to store the city name for employees. We will also bind this collection to a dropdown list in our web application from which the user will select the desired city. Click on “Add collection”. Set the collection ID as “cities”. Click on “Next”. Refer to the image below:
Put the Field value as “CityName”, Select string from the Type dropdown and fill the value with city name as “Mumbai”. Click on Save. Refer to the image below:
This will create the “cities” collection and insert the first document in it. Similarly, create four more documents inside this collection and put the “CityName” value as Chennai, New Delhi, Bengaluru and Hyderabad.
We will use “employees” collection to store employee data, but we will not create it manually. We will create “employees” collection while adding the first employee data from the application.
To access database from our project, we need to set the GOOGLE_APPLICATION_CREDENTIALS environment variable to point to a JSON service account key file. This will set an authentication pipeline from our application to cloud Firestore.
To generate the service account key file follow the steps mentioned below:
Step 1: Navigate to https://console.cloud.google.com/iam-admin/. Login with the same google account, you have used to create Firestore DB.
Step 2: Select Project from the drop down in menu bar at the top.
Step 3: Select “Service accounts” from the left menu. Select the service account for which you want to create the key. Click on more button in the “Actions” column in that row, and then click Create key.
Refer to the image below:
Step 4: A popup modal will open asking you to select the key type. Select “JSON” and click on create button. This will create private key for accessing your Firestore account and downloads a JSON key file to your machine. We will use this file to set GOOGLE_APPLICATION_CREDENTIALS environment variable in later part of this article.
Open Visual Studio and select File >> New >> Project. After selecting the project, a “New Project” dialog will open. Select .NET Core inside Visual C# menu from the left panel. Then, select “ASP.NET Core Web Application” from available project types. Put the name of the project as BlazorWithFirestore and press OK.
After clicking on OK, a new dialog will open asking you to select the project template. You can observe two drop-down menus at the top left of the template window. Select “.NET Core” and “ASP.NET Core 2.1” from these dropdowns. Then, select “Blazor (ASP .NET Core hosted)” template and press OK.
Now, our Blazor solution will be created. You can observe that we have three project files created in this solution.
We need to add the package reference for Google cloud Firestore, which will allow us to access our DB from the Blazor application. Right click on BlazorWithFirestore.Shared project.
Select “Edit BlazorWithFirestore.Shared.csproj”. It will open the BlazorWithFirestore.Shared.csproj file. Add the following lines inside it.
Similarly add these lines to BlazorWithFirestore.Server.csproj file also.
We will create our model class in BlazorWithFirestore.Shared project. Right click on BlazorWithFirestore.Shared and select Add >> New Folder. Name the folder as Models. Again, right click on Models folder and select Add >> Class to add a new class file. Put the name of you class as Employee.cs and click Add.
Open the Employee.cs class and put the following code into it.
We have decorated the class with [FirestoreData] attribute. This will allow us to map this class object to Firestore collection. Only those class properties, which are marked with [FirestoreProperty] attribute, are considered when we are saving the document to our collection. We do not need to save EmployeeId to our database as it is generated automatically.
While fetching the data, we will bind the auto generated document id to the EmployeeId property. Similarly, we will use date property to bind the created date of collection while fetching the record. We will use this date property to sort the list of employees by created date. Hence, we have not applied [FirestoreProperty] attribute to these two properties.
Similarly, create a class file Cities.cs and put the following code into it.
Right-click on BlazorWithFirestore.Server project and then select Add >> New Folder and name the folder as DataAccess. We will be adding our class to handle database related operations inside this folder only. Right click on DataAccess folder and select Add >> Class. Name your class EmployeeDataAccessLayer.cs.
Put the following code inside this class.
In the constructor of this class we are setting the GOOGLE_APPLICATION_CREDENTIALS environment variable. You need to set the value of filepath variable to the path where the JSON service account key file is located in your machine. Remember we downloaded this file in the previous section. The projectId variable should be set to the project id of your Firebase project.
We have also defined the methods for performing CRUD operations. The GetAllEmployees method will fetch the list of all employee document from our “employees” collection. It will return the employee list sorted by document creation date.
The AddEmployee method will add a new employee document to our “employees” collection. If the collection does not exist, it will create the collection first then insert a new document in it.
The UpdateEmployee method will update the field values of an already existing employee document, based on the employee id passed to it. We are binding the document id to employeeId property, hence we can easily manipulate the documents.
The GetEmployeeData method will fetch a single employee document from our “employees” collection based on the employee id.
DeleteEmployee method will delete the document for a particular employee from the “employees” collection.
GetCityData method will return the list of cities from “cities” collection.
Right-click on BlazorWithFirestore.Server/Controllers folder and select Add >> New Item. An “Add New Item” dialog box will open. Select Web from the left panel, then select “API Controller Class” from templates panel and put the name as EmployeeController.cs. Click Add.
This will create our API EmployeeController class. We will call the methods of EmployeeDataAccessLayer class to fetch data and pass on the data to the client side.
Open EmployeeController.cs file and put the following code into it.
We will create the component in the BlazorWithFirestore.Client/Pages folder. The application template provides the Counter and Fetch Data files by default in this folder. Before adding our own component file, we will delete these two default files to make our solution cleaner. Right-click on BlazorWithFirestore.Client/Pages folder and then select Add >> New Item. An “Add New Item” dialog box will open, select “ASP.NET Core” from the left panel, then select “Razor Page” from templates panel and name it EmployeeData.cshtml. Click Add. Refer to the image below:
This will add an EmployeeData.cshtml page to our BlazorSPA.Client/Pages folder. This razor page will have two files – EmployeeData.cshtml and EmployeeData.cshtml.cs.
We will be using a bootstrap modal dialog in our application. We will also include a few Font Awesome icons for styling in the application. To be able to use these two libraries, we need to add the CDN references to allow the JS interop.
Here, we have included the CDN references, which will allow us to use the bootstrap modal dialog and Font Awesome icons in our applications. Now, we will add codes to our view files.
Open EmployeeData.cshtml.cs and put the following code into it.
Here, we have defined the EmployeeDataModel class, which is inheriting from BlazorComponent. This allows the EmployeeDataModel class to act as a Blazor component.
We are also injecting the HttpClient service to enable the web API calls to our EmployeeController API.
We will use the two variables — empList and cityList — to hold the data of our Employee and Cities collections respectively. The modalTitle property, which is of type string, is used to hold the title that will be displayed in the modal dialog. The value provided in the search box is stored in the searchString property which is also of type string.
The GetCityList method will make a call to our web API GetCities method to fetch the list of city data from the cities collection. The GetEmployeeList method will send a GET request to our web API to fetch the list of Employee Data from the Employee table.
We are invoking these two methods inside the OnInitAsync method, to ensure that the Employee Data and the cities data will be available as the page loads.
The AddEmployee method will initialize an empty instance of the Employee object and set the modalTitle property, which will display the title message on the Add modal popup.
The EditEmployee method will accept the employee ID as the parameter. It will send a GET request to our web API to fetch the record of the employee corresponding to the employee ID supplied to it.
We will use the SaveEmployee method to save the record of the employee for both the Add request and Edit request. To differentiate between the Add and the Edit requests, we will use the EmployeeId property of the Employee object. If an Edit request is made, then the EmployeeId property contains a string value, and we will send a PUT request to our web API, which will update the record of the employee. Otherwise, if we make an Add request, then the EmployeeId property is not initialized, and hence it will be null. In this case, we need to send a POST request to our web API, which will create a new employee record.
The DeleteConfirm method will accept the employee ID as the parameter. It will fetch the Employee Data corresponding to the employee ID supplied to it.
The DeleteEmployee method will send a delete request to our API and pass the employee ID as the parameter. It will then call the GetEmployeeList method to refresh the view with the updated list of Employee Data.
The SearchEmployee method is used to implement the search by the employee name functionality. We will return all the records of the employee, which will match the search criteria either fully or partially. To make the search more effective, we will ignore the text case of the search string. This means the search result will be same whether the search text is in uppercase or in lowercase.
Open EmployeeData.cshtml page and put the following code into it.
The route for our component is defined at the top as “/employeerecords”. To use the methods defined in the EmployeeDataModel class, we will inherit it using the @inherits directive.
We have defined an Add Employee button. Upon clicking, this button will invoke the AddEmployee method and open a modal dialog, which allows the user to fill out the new Employee Data in a form.
We have also defined our search box and a corresponding search button. The search box will bind the value to searchString property. On clicking the search button, SearchEmployee method will be invoked, which will return the filtered list of data as per the search text. If the empList property is not null, we will bind the Employee Data to a table to display it on the web page. Each employee record has the following two action buttons corresponding to it:
We have defined a form inside the bootstrap modal to accept user inputs for the employee records. The input fields of this form will bind to the properties of the employee class. The City field is a drop-down list, which will bind to the cities collection of the database with the help of the cityList variable. When we click on the save button, the SaveEmployee method will be invoked and the modal dialog will be closed.
When user click on the Delete button corresponding to an employee record, another bootstrap modal dialog will be displayed. This modal will show the Employee Data in a table and ask the user to confirm the deletion. Clicking on the Delete button inside this modal dialog will invoke the DeleteEmployee method and close the modal. Clicking on the Cancel button will close the modal without performing any action on the data.
Before executing the application, we will add the navigation link to our component in the navigation menu.
Open the BlazorWithFirestore.Client/Shared/NavMenu.cshtml page and add the following navigation link:
Hence, we have successfully created a Single Page Application (SPA) using Blazor with the help of cloud Firestore as database provider.
Press F5 to launch the application.
A web page will open as shown in the image below. The navigation menu on the left is showing navigation link for Employee data page.
You can perform the CRUD operations on this application as shown in the GIF image at the start of this article.
We have created a Single Page Application (SPA) using Blazor with the help of Google cloud Firestore as database provider. We have created a sample employee record management system and performed CRUD operations on it. Firestore is a NoSQL database, which allows us to store data in form of collections and documents. We have also used a bootstrap modal popup to handle user inputs. We have also implemented a search box to search the employee list by employee name.
Please get the source code from GitHub and play around to get a better understanding.
Get my book Blazor Quick Start Guide to learn more about Blazor.
Preparing for interviews? Read my article on C# Coding Questions For Technical Interviews
Originally published at https://ankitsharmablogs.com/
We’ve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
304 
1
304 claps
304 
1
Written by
SDE III @cisco | GDE for Angular | Microsoft MVP | Author | Speaker | Passionate Programmer https://ankitsharmablogs.com/
We’ve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
Written by
SDE III @cisco | GDE for Angular | Microsoft MVP | Author | Speaker | Passionate Programmer https://ankitsharmablogs.com/
We’ve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://betterprogramming.pub/how-to-become-aws-cloud-developer-certified-7318a67f7085?source=search_post---------374,"Sign in
There are currently no responses for this story.
Be the first to respond.
Ryan Gleason
May 6, 2020·7 min read
If you clicked on the link for this article, you are most likely thinking about taking the AWS Cloud Developer Certification exam. If you are not thinking about it, it’s time you thought about it!
About
Write
Help
Legal
Get the Medium app
"
https://scientya.com/a-beginners-guide-to-the-basics-of-what-cloud-computing-is-about-e8b3b7f25a30?source=search_post---------375,"Forget about definitions and technical explanations. Not everyone is a highly skilled DevOps Engineer and Cloud Architect. Salesforce CEO and Chairman Marc Benioff brings it straight to the point. Cloud computing is a better way to run your business according to him. The possibility of improving your business is something which is appealing to every decision maker across the globe. Who thought that the cloud would one day be considered as the digitization tool par excellence as n’cloud.swiss Founder and Chairman André Matter emphasizes again and again. The year 2000 will always be remembered as the milenium of the more and more oustanding pace of technological progress. New and old technologies are as close together as never before. The almost uncontrollable increase in human knowledge leads to endless incremental innovations. The hunt for the next big thing seems to be never ending.
Even though cloud computing still plays that role, one might think that it is being pushed aside by the likes of artificial intelligence and blockchain. But wait… most new technologies are distinguished by a huge number of data and intelligence. In other words, “the many disparate servers which are part of cloud technology hold the data which an AI can access and use to make decisions and learn things like how to hold a conversation. But as the AI learns this, it can impart this new data back to the cloud, which can thus help other AIs learn as well” as noticed Gary Eastwood from the IDG Contributor Network. Same goes for blockchain and other data intensive technology. Cloud computing is not only the digitization tool par excellence, it is omnipresent and plays undoubtedly a key role in today’s technological progress.
Providing IT resources and on-demand applications over the Internet at usage-based prices
“Simply put, cloud computing is the delivery of computing services — servers, storage, databases, networking, software, analytics and more — over the Internet (“the cloud”). Companies offering these computing services are called cloud providers and typically charge for cloud computing services based on usage, similar to how you are billed for water or electricity at home.” (Microsoft Azure)
Whether you run apps that share photos with millions of mobile users or support critical business operations in your organization, the cloud is a technology providing quick access to flexible and cost-effective IT resources. When it comes to cloud computing, you do not have to invest in hardware in advance or spend a lot of time managing it. Instead, you can provide the exact type and size of computing resources you need to implement your latest breakthrough idea or operate your IT department. You can access as many resources as you need almost immediately by paying only for what you use. Cloud computing provides an easy way to access servers, storage, databases and a full range of application services over the Internet. Cloud providers such as Amazon Web Services, Microsoft Azure, Google Cloud Platform or “Swiss made” n’cloud.swiss operate and manage the network-attached hardware needed for these application services, providing and using the resources you need through a web application.
Advantages of cloud computing
The cloud has become a technology that influences everyone’s daily life. The adoption of solutions and services in the cloud present a number of advantages and benefits, among others:
Types of cloud services: IaaS, PaaS, SaaS
Cloud computing consists of three main types, commonly referred to as Infrastructure as a Service (IaaS), Platform as a Service (PaaS), and Software as a Service (SaaS). Choosing the right cloud computing type consists in knowing your needs to achieve an optimal level of control with worrying about unnecessary tasks. Microsoft defines these types as follows:
Infrastructure-as-a-service (IaaS): The most basic category of cloud computing services. With IaaS, you rent IT infrastructure — servers and virtual machines (VMs), storage, networks, operating systems — from a cloud provider on a pay-as-you-go basis.
Platform as a service (PaaS): Platform-as-a-service (PaaS) refers to cloud computing services that supply an on-demand environment for developing, testing, delivering and managing software applications. PaaS is designed to make it easier for developers to quickly create web or mobile apps, without worrying about setting up or managing the underlying infrastructure of servers, storage, network and databases needed for development.
Software as a service (SaaS): Software-as-a-service (SaaS) is a method for delivering software applications over the Internet, on demand and typically on a subscription basis. With SaaS, cloud providers host and manage the software application and underlying infrastructure and handle any maintenance, like software upgrades and security patching. Users connect to the application over the Internet, usually with a web browser on their phone, tablet or PC.
Cloud deployments: public, private, hybrid
There are three different ways to deploy cloud computing resources. These are public cloud, private cloud and hybrid cloud.
Public clouds are owned and operated by a third-party cloud service provider, and deliver computing resources like servers and storage over the Internet using a web browser. Today’s leading cloud providers Amazon AWS or Microsoft Azure are examples of a public cloud.
A private or on-prem cloud refers to cloud computing resources used in-house and exclusively by a single business or organisation. The particularity here is that a private cloud can be physically located on the company’s on-site datacenter.
A combination of both public and private clouds leads to what we call hybrid cloud. The advantage here is that a hybrid cloud allows data and applications to be shared between them. By allowing data and applications to move between private and public clouds, customers enjoy greater flexibility and more deployment options.
After which criteria do customers decide for a cloud provider?
Simply put, customers do not decide for one single cloud provider. Many companies pursue a multi cloud strategy to maintain the ability and flexibility to select different cloud services from different providers. Single multi-cloud infrastructure vendors like n’cloud.swiss are rare. Praised as a Swiss and European alternative to the major cloud providers, n’cloud.swiss is a cloud platform running in one of the world’s most secure data centers in Switzerland. The idea is to enable customers to design a cloud according to their specific requirements with the same product, either as a service model, an on-prem version in existing IT environments or as a hybrid variant. In addition, all cloud service models from Infrastructure as a Service (IaaS), over Platform as a Service (PaaS) to Software as a Service (SaaS) are part of the platform. All this is surrounded by an innovative internal n’cloud.swiss application catalogue. Within the latter, n’cloud.swiss offers more than 142 applications from 30 different IT categories “free and ready to go” as well as the opportunity to upload also other development applications and tools easily. In addition, personal support and competitive pricing models, API connectivity for easy and fast transfers of existing developments from or to other major cloud platforms award this Swiss cloud platform a unique selling point and a competitive advantage.
Overview of n’cloud.swiss products und services
UNMANAGED PRODUCTS
MANAGED PRODUCTS
A cloud solution in minutesTry n’cloud.swiss for free1. Create an n’cloud.swiss account2. Start a virtual machine3. Join the world of n’cloud
The digital world publication
2.8K 
3
2.8K claps
2.8K 
3
Written by
Dynamic and data-driven marketing leader. Author, Mentor & Speaker. Top 50 Global Thought Leader. See www.yahyamohamedmao.com
Scientya is all about creating and sharing knowledge in the digital world. Switzerland’s fastest growing Medium publication  covers various topics including Blockchain, Artificial Intelligence, Cloud Computing, Health Care, Marketing and more.
Written by
Dynamic and data-driven marketing leader. Author, Mentor & Speaker. Top 50 Global Thought Leader. See www.yahyamohamedmao.com
Scientya is all about creating and sharing knowledge in the digital world. Switzerland’s fastest growing Medium publication  covers various topics including Blockchain, Artificial Intelligence, Cloud Computing, Health Care, Marketing and more.
"
https://medium.com/google-cloud/going-serverless-on-google-cloud-platform-b770803c5ee7?source=search_post---------376,"There are currently no responses for this story.
Be the first to respond.
Calling something serverless is so hot right now. Every developer tools company found out and suddenly everything is #serverless.
In fact, in the Google Cloud Next 17 Keynotes, so many things were called serverless. These products were amazing in their own right, but the word serverless was thrown around so loosely that it was cringeworthy at times.
As expected, people are upset about this.
So what the hell is serverless? Again, everyone has their own opinions on what serverless is.
So let me define my definition.
I’m going to break down “cloud” products into four different categories. The names are basically made up, but this is what makes sense to me.
SaaS products are ready to use out of the box by end users. There is nothing to provision. You either pay for what you use, or purchase bulk credits of some sort.
Serverless products are used by developers to build higher order products. There is nothing to provision, but there may be some knobs to tune the infrastructure. You pay for exactly what you use, nothing more.
Managed products are used by developers to build higher order products. Developers need to provision capacity and tune things, but don’t need to maintain the infrastructure. You pay for excess infrastructure when it is not being used.
IaaS products mirror on-prem solutions, except you can provision resources in seconds/minutes instead of days/months. They require explicit provisioning and maintenance. You pay for excess infrastructure when it is not being used.
Do you agree with this categories? Am I TOTALLY WRONG IN EVERY WAY!!?Let me know in the comments!
I am going to map most of Google Cloud’s products to one of these four categories. Of course, arguments can be made to fit products in multiple categories, but I’m going to make the tough calls and put them in only one. I think this will make it very clear how I think about these categories.
G SuiteThe obvious entry into this category. G Suite is ready to be used by the end user, no need for developers to do anything. Of course, Apps Script can be considered serverless, but as it depends so much on the rest of G Suite, it feels right in this category.
Data StudioData studio lets you build dashboards for your Google Analytics, Cloud SQL, BigQuery, etc data. There is a lot of drag and drop involved, and end users can be productive without developer support.
Cloud FunctionsThe obvious entry into this category. AWS Lambda started the FaaS revolution, and Cloud Functions follows a similar pattern. You write your functions, specify when they trigger, tune how much memory/cpu is allocated per call, and deploy. Everything else is handled for you. You pay per function invocation, so it is exactly matched to your traffic.
Cloud RunGoogle’s serverless container offering, Cloud Run is similar to Cloud Functions or App Engine, except you deploy a container instead of code. Everything else is handled for you. Cloud Run charges per incoming request, so you pay for exactly what you use.
You can also use Cloud Run on GKE, and in that case it becomes more managed than serverless. See the GKE section for more details.
Cloud StorageCloud Storage is similar to Google Drive, but isn’t usable by end users. It requires a developer to build higher order systems on top. You don’t provision anything, and pay for exactly what you use. You can tune the latency and price by choosing the bucket type (Nearline, Multi-Regional, etc).
App Engine StandardThe first Google Cloud product. Write your code, tune how much memory/cpu you want, and deploy. Everything else is handled for you. App Engine Standard scales to exactly match your traffic and scales to zero when you have none, so you pay for exactly what you use.
Cloud DatastoreThe first NoSQL database at Google Cloud. You don’t provision anything, just read and write data. You pay per read/write and for the exact storage you use, and it scales automatically.
Firebase Realtime Database / HostingSimilar to Cloud Datastore, but used directly on the frontend. You don’t provision anything, just read and write data. You pay for data transfer and for the exact storage you use, and it scales automatically.
Cloud FirestoreThis next generation database brings the best from Datastore and Firebase Realtime Database together. You can use it directly from the frontend like the Realtime Database and from the server like Datastore, and get advanced query support and strong consistency. You don’t provision anything, just read and write data. You pay per read/write and for the exact storage you use, and it scales automatically.
BigQueryBigQuery is Google’s data warehouse service, letting users query TB of data with SQL. There is nothing to manage, just dump/stream your data in and query. You pay to store data and query data, no need to provision machines or storage or set up indexes.
Pub/SubPub/Sub is Google’s many-to-many async messaging bus (think Apache Kafka or RabbitMQ). No need to provision anything, and it scales to millions of messages a second instantly. Pub/Sub charges per message, so you pay for exactly what you use.
Stackdriver (Logs, Monitoring, Debug, Tracing)The Stackdriver tools let you access powerful tools without needs to set anything up. They are all have a free tier, but you can pay to monitor more resources and other clouds like AWS. There is nothing to provision or tune and you pay per resource monitored so there is no extra spending.
Cloud DataflowCloud Dataflow uses Apache Beam to create fully managed ETL, batch processing, and streaming analytics pipelines. It autoscales to process your pipeline, and scales back to zero when there is no more work left.
Kubernetes EngineGoogle Kubernetes Engine creates a Kubernetes Cluster for you in one click. Google manages the Master and Nodes and auto-updates them for you. You have to provision a cluster ahead of time(though there are some auto-scaling capabilities) so you are paying for unused resources.
App Engine FlexibleApp Engine Flexible is similar to Standard, but runs on VMs instead of a sandbox. While this gives it more “flexibility” over what it can run, you lose the “serverless” magic of standard. Auto-scaling and deploying is not as fast, but the biggest difference is you have to always have one VM instance running, meaning you are paying for unused resources.
Cloud SQLCloud SQL gives you managed MySQL and PostgreSQL instances. Google worries about backups, failover replication, etc, which reduces the operational overhead of running a database. While there are some auto-scaling capabilities, you need to provision a machine and disk ahead of time, which means paying for unused resources.
Cloud BigtableCloud Bigtable is Google’s HBase compatible NoSQL database. While storage scales automatically (you pick SSD or HDD), you need to choose how many nodes you want to tune performance. More nodes = more performance, but of course you can over/under provision.
Cloud SpannerCloud Spanner is similar to BigTable except it is globally consistent and relational instead of NoSQL. This is pretty magical! You need to choose how many nodes you want to tune performance. More nodes = more performance, but of course you can over/under provision.
Cloud Load Balancing (L4/L7)Both the L4 and L7 load balancers offered by Google Cloud are fully managed services that require no provisioning, pre-warming, or tuning. The only reason this is not in the Serverless category is you have to pay a flat rate to launch the load balancer regardless of how much traffic it handles.
Cloud DataprocCloud Dataproc launches a managed Spark/Hadoop cluster. You need to specify how many VMs you want and tune them, but after that you can use the cluster without any additional setup. You pay for the VMs even when they are not being used.
Cloud Machine Learning EngineCloud Machine Learning Engine creates and manages a distributed TensorFlow cluster for you to train and serve your models on. You have to provision a cluster, but the cluster is fully managed and you can just submit jobs to it.
Compute EngineThese are Linux and Windows VMs. While you can autoscale them using Managed Instance Groups, and you can do cool things like have the disks automatically grow, at the end of the day you are 100% responsible.
Cloud LauncherCloud Launcher lets you deploy pre-configured apps on Compute Engine. Though the initial setup is automatic, you are responsible for maintaining the servers and pay for unused resources.
What do you think? Does this make sense? Is this whole debate worth it? Is there something I missed? Please let me know!
Google Cloud community articles and blogs
206 
8
206 claps
206 
8
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://towardsdatascience.com/why-i-moved-from-google-colab-and-amazon-sagemaker-to-saturn-cloud-675f0a51ece1?source=search_post---------377,"Sign in
There are currently no responses for this story.
Be the first to respond.
James Le
Oct 24, 2019·8 min read
When it comes to scientific experimentation and collaboration, people tend to need the same thing: an easy-to-use interface to hack and optimize their algorithms, a data input-output system, and the support for their preferred programming language. A natural solution to these problems emerged in 2011 with the release of Jupyter, an extremely versatile web application that allows you to create a notebook file that serves as an interactive code interface, a data visualization tool, and a markdown editor.
There are many ways to share a static Jupyter notebook with others, such as posting it on GitHub or sharing an nbviewer link. However, the recipient can only interact with the notebook file if they already have the Jupyter Notebook environment installed. But what if you want to share a fully interactive Jupyter notebook that doesn’t require any installation? Or, you want to create your own Jupyter notebooks without installing anything on your local machine?
The realization that Jupyter notebooks have become the standard gateway to machine learning modeling and analytics amongst data scientists has fueled a surge in software products marketed as “Jupyter notebooks on the cloud (plus new stuff!)”. Just from memory, here’s a few company offerings and startup products that fit this description in whole or in part: Kaggle Kernels, Google Colab, AWS SageMaker, Google Cloud Datalab, Domino Data Lab, DataBrick Notebooks, Azure Notebooks…the list goes on and on. Based on my conversations with fellow data science novices, the 2 most popular Jypyter cloud platforms seem to be Google Colab and Amazon SageMaker.
Google Colab is ideal for everything from improving your Python coding skills to working with deep learning libraries, like PyTorch, Keras, TensorFlow, and OpenCV. You can create notebooks in Colab, upload notebooks, store notebooks, share notebooks, mount your Google Drive and use whatever you’ve got stored in there, import most of your favorite directories, upload your personal Jupyter Notebooks, upload notebooks directly from GitHub, upload Kaggle files, download your notebooks, and do just about everything else that you might want to be able to do.
Visually, the Colab interface looks quite similar to the Jupyter interface. However, working in Colab actually feels very dissimilar to working in the Jupyter Notebook:
A lot has been written about Google Colab troubleshooting, so without going down that rabbit hole, here are a few things that are less than ideal. Because the Colab menu bar is missing some items and the toolbar is kept very simple, some actions can only be done using keyboard shortcuts. You can’t download your notebook into other useful formats such as an HTML webpage or a Markdown file (though you can download it as a Python script). You can upload a dataset to use within a Colab notebook, but it will automatically be deleted once you end your session.
In terms of the ability to share publicly, if you choose to make your notebook public and you share the link, anyone can access it without creating a Google account, and anyone with a Google account can copy it to their own account. Additionally, you can authorize Colab to save a copy of your notebook to GitHub or Gist and then share it from there.
In terms of the ability to collaborate, you can keep your notebook private but invite specific people to view or edit it (using Google’s familiar sharing interface). You and your collaborator(s) can edit the notebook and see each other’s changes, as well as add comments for each other (similar to Google Docs). However, your edits are not visible to your collaborators in real-time (there’s a delay of up to 30 seconds), and there’s a potential for your edits to get lost if multiple people are editing the notebook at the same time. Also, you are not actually sharing your environment with your collaborators (meaning there is no syncing of what code has been run), which significantly limits the usefulness of the collaboration functionality.
Colab does give you access to a GPU or a TPU. Otherwise, Google does not provide any specifications for their environments. If you connect Colab to Google Drive, that will give you up to 15 GB of disk space for storing your datasets. Sessions will shut down after 60 minutes of inactivity, though they can run for up to 12 hours.
The greatest strength of Colab is that it’s easy to get started, since most people already have a Google account, and it’s easy to share notebooks since the sharing functionality works the same as Google Docs. However, the cumbersome keyboard shortcuts and the difficulty of working with datasets are significant drawbacks. The ability to collaborate on the same notebook is useful; but less useful than it could be since you’re not sharing an environment and you can’t collaborate in real-time.
Amazon SageMaker is a fully managed machine learning service that helps data scientists and developers to quickly and easily build & train models, then directly deploy them into a production-ready hosted environment. It provides an integrated Jupyter authoring notebook instance for easy access to your datasets for exploration/analysis, so you don’t have to manage servers. It also provides common ML algorithms that are optimized to run efficiently against extremely larger data in a distributed environment. With native support for bring-your-own algorithms and frameworks, Amazon SageMaker offers flexible distributed training options that adjust to your specific workflows.
First, you spin up a so-called “notebook instance” which will host the Jupyter Notebook application itself, all the notebooks, auxiliary scripts, and other files. No need to connect to that instance (actually you cannot, even if wanted to) or set it up in any way. Everything is already prepared for you to create a new notebook and use it to collect and prepare some data, to define a model, and to start the learning process. All the configuration, provisioning of compute instances, moving of the data, etc. will be triggered literally with a single function call. This nifty process dictates a certain approach to defining the models and organizing the data.
SageMaker is built on top of other AWS services. Notebook, training, and deployment machines are just ordinary EC2 instances running specific Amazon Machine Images (AMI). And data (also results, checkpoints, logs, etc.) is stored in S3 object storage. This might be troubling if you are working with images, videos or any huge datasets. The fact is that you have to upload all your data to S3. When you configure the training you tell SageMaker where to find your data. SageMaker then automatically downloads the data from S3 to every training instance before starting the training. Every time. For a reference, it takes around 20 minutes to download 100Gb worth of images. Which means that you have to wait at least 25 minutes before the training begins. Good luck debugging your model! On the other hand, when all preliminary trials are done elsewhere and your model is already polished, the training experience is very smooth. Simply upload your data to S3 and get interim results from there as well.
Another aspect for consideration is pricing. Notebook instances can be very cheap, especially when there is no need to pre-process the data. Training instances, on the other hand, may easily burn a hole in your pocket. Check here all the prices, as well as the list of regions where SageMaker has already been launched.
Saturn Cloud is a new kid on the block that provides data scientists who are not interested in setting up infrastructure but care more about doing data science easily. More specifically, the platform helps manage Python environments in the cloud.
You can get started with the free tier after signing up for an account. In the dashboard, you can create a Jupyter Notebook for a project by choosing the disk space and the size of your machine. The configurations cover requirements for a lot of practical data science projects. Furthermore, you can define auto-shutoff duration for your project, which would keep your project from shutting down because of inactivity.
It’s extremely easy to share notebooks via Saturn Cloud. I did a previous project that explores the Instacart Market Basket Analysis challenge, and you can view the public notebook here: https://www.saturncloud.io/yourpub/khanhnamle1994/instacart-notebooks/notebooks/Association-Rule-Mining.ipynb. I especially enjoyed how the code blocks and the visualization get rendered without any clutter as we see with the Google Colab notebooks. It looks just like a report as it is intended to be. I also like the “Run in Saturn” option provided where users can just go and click over to run this code by themselves without any need for an explicit login.
Overall, using Saturn Cloud makes it easy to share your notebooks with other teammates without having to deal with the hassle of making sure they have all the correct libraries installed. This sharing capability is superior compared to Google Colab.
Furthermore, for those of you who have had the issue of running notebooks locally and running out of memory, it also allows you to spin up Virtual Machines with the memory and RAM required and only pay for what you use. This cost-association is a tremendous advantage compared to Amazon SageMaker.
There are also a few other bells and whistles that really mitigate some of the complexities of work typically done from a DevOps perspective. That’s what makes tools like this great. In many ways, until you work in a corporate environment where even sharing a basic Excel document can become cumbersome, it is almost hard to explain to younger data scientists why this is so great.
Saturn Cloud is still far from being production-ready, but it’s a very promising solution. I personally think that it is the only solution so far that comes close to the ease-of-use of a local Jupyter server with the added benefits of cloud hosting (CVS, scaling, sharing, etc.).
The proof of concept is indeed there, and it merely lacks some polish around the angles (more language support, better version control, and easier point-and-click interface). I am excited to see the future version(s) of the Saturn platform!
Data >< Product >< Community | https://jameskle.com/ | @le_james94
382 
8
382 
382 
8
Your home for data science. A Medium publication sharing concepts, ideas and codes.
"
https://developers.ascendcorp.com/%E0%B8%AA%E0%B8%A3%E0%B9%89%E0%B8%B2%E0%B8%87-microservices-%E0%B8%94%E0%B9%89%E0%B8%A7%E0%B8%A2-netflix-oss-%E0%B9%81%E0%B8%A5%E0%B8%B0-spring-cloud-2678667d9dbc?source=search_post---------378,"ปัจจุบันนี้แทบทุกบริษัทล้วนใช้ microservices architecture แทนที่แบบ full monolithic หรือว่าการรวมทั้งระบบใหญ่ๆไว้ใน code base เดียวกัน และ microservices ก็ได้เข้ามาทำหน้าที่การจำแนกแบ่ง monolithic ออกตาม business systems ในการสร้าง application หรือระบบที่มีขนาดใหญ่ การใช้ microservices architecture ทำให้พัฒนาระบบได้เร็วขึ้น เนื่องจากแต่ละ Service มีความเป็นอิสระและขึ้นต่อกันแบบหลวมๆทำให้สามารถแยกทำ continuously delivered เป็นอิสระของแต่ละทีมแต่ละ Service ไปได้
Netflix เป็นบริษัท media ขนาดใหญ่ที่ให้บริการ streaming media และ video แบบ online โดยในปัจจุบัน Netflix มีลูกค้ากว่า 100 ล้าน users ทั่วโลก Netflix เริ่มใช้ cloud และ microservices มาตั้งแต่ปี 2008 โดย microservices ที่ Netflix สร้างตั้งอยู่บนพื้นฐานของ Amazon Web Services และ Netflix ได้ทำการ open source software และ libraries ที่ Netflix สร้างและใช้งานจริงๆเป็น production ออกมามากมาย โดยเรียกแบบรวมๆว่า Netflix OSS สามารถเข้าไปดูรายละเอียดเพิ่มเติมได้ที่ https://netflix.github.io / วันนี้เราจะมาทำการลองสร้าง microservices ด้วย Spring Boot และ Netflix OSS กันครับ
Spring Cloud เป็นกลุ่ม tools จาก Pivotal ที่ช่วยในการสร้าง microservices บน cloud โดย Spring cloud ได้ทำการ release เวอร์ชั่น 1 ตั้งแต่ปี 2015 โดยได้ทำการเพิ่ม Spring cloud Netflix ซึ่งมาจาก Netflix OSS เข้ามาด้วย เช่น Eureka, Hystrix, Zuul และ Ribbon และ Spring Cloud ก็เข้ามาช่วยการจัดการและแก้ปัญหาต่างๆ ดังนี้
สำหรับการทำ Microservices ในบทความนี้ เราจะสร้าง Microservices แบบง่ายๆโดยจะประกอบด้วย pricing service และ product service โดย product service จะมีการยิง API เพื่อไปถามราคาของ product จาก pricing service
Microservices ของเราจะมีการใช้ Netflix Eureka ในการทำ Service Discovery และใช้ Spring Cloud Config ในการทำ centralized configuration หรือว่า Configuration Server
ในการทำ Service Discovery มีหลากหลายรูปแบบในการทำ แต่ในบทความนี้ ผมขอใช้ Eureka ที่สร้างโดย Netflix และ Netflix Eureka เป็นระบบพื้นฐานในการสร้าง client-server model โดยเราสามารถมี Eureka Servers ได้หลายตัวและสามารถมี Eureka Clients ได้หลายตัวเช่นกัน โดย client จะมาทำการ registry กับ Servers และ Servers ก็จะทำหน้าที่ในการหาตำแหน่งของ Clients ให้เอง นอกจากนั้น Eureka ยังช่วยแก้ปัญหา dynamic ของ service addresses และในบางกรณีที่มี services เดี้ยงไป Service Discovery ยังช่วย client request ถึง service ที่ยังทำงานดีอยู่เท่านั้นลองพิจารณารูปด้านล่างนี้
ทีนี้เราจะมาทำการสร้าง Eureka Server กันครับ เหมือนเดิมเราก็จะใช้ https://start.spring.io/ ทำการสร้าง Project และเลือก dependencies ดังนี้
จากนั้นก็ทำการ import project แล้วก็ทำการเพิ่ม Java config @EnableEurekaServer โดย @EnableEurekaServer จะทำหน้าที่ configure ให้ Service นี้เป็น discovery server โดยใช้ Netflix Eureka
Properties
จากนั้นทำการเพิ่ม configuration ของเรา โดยเราจะมี Eureka Discovery Service ตัวเดียวให้ clients หรือว่า Services มา connect เพิ่ม bootstrap.yml ใน src/main/resources และทำการ config ดังนี้
เกือบทุก Services ใช้ configurations file ใน src/main/resources และบางครั้ง configurations อาจจะมีการเปลี่ยนแปลงบ่อยกว่าตัว Code ซะอีก Spring Cloud Config มีความสามารถในการทำ centralized configuration โดยสามารถ config ทุกๆ Service ในที่เดียว และสามารถ config ได้จาก Git และแก้ไขค่า config เมื่อไรก็ได้ โดยไม่ต้องทำการ Restart Service โดยเราสามารถเปลี่ยนค่า configurations ได้แบบ on the fly
ทำการสร้าง Project และเลือก dependencies ดังนี้
จากนั้นก็ทำการ import project แล้วก็ทำการเพิ่ม Java config @EnableConfigServer
จากนั้นเราต้องทำการ config server port และ Git-url ในการ configuration ของแต่ละ Services ของเรา
Properties
เพิ่ม bootstrap.yml ใน src/main/resources และทำการ config ดังนี้
Git Repository Configuration
ทำการสร้าง Git repository ในที่นี้ผมขอใช้ local git นะครับ โดยทำการ initialize ดังนี้
จาดนั้นทำการพิมพ์คำสั่ง
แล้ว copy location ของ local git ของเราไปใส่ใน bootstrap.yml ซะ
เสร็จแล้วก็ทำการ run application ด้วยคำสั่ง mvn spring-boot:run แล้วทำการ request ที่ http://localhost:8585/product-service/default ถ้าไม่มีอะไรผิดพลาด เราก็จะได้ response เป็น product-service configuration ดังนี้
ทำการเรียก pricing-service configuration ที่ http://localhost:8585/pricing-service/default เราก็จะได้ response ดังนี้
ทำการสร้าง Project และเลือก dependencies ดังนี้
จากนั้นก็ทำการ import project แล้วก็ทำการเพิ่ม Java config @EnableDiscoveryClient
จากนั้นทำการสร้าง ProductController
Note: PRICING-SERVICE ที่เห็นอยู่ที่ url คือชื่อ Pricing service ที่ได้ทำการ register กับ Eureka service ไปแล้ว โดยทาง Eureka service จะทำหน้าที่ในการหา url location ของ pricing-service ให้เอง
Properties
จากนั้นทำการเพิ่ม configuration ของเรา โดยเพิ่ม bootstrap.yml ใน src/main/resources และทำการ config ดังนี้
ทำการสร้าง Project และเลือก dependencies ดังนี้
จากนั้นก็ทำการ import project แล้วก็ทำการเพิ่ม Java config @EnableDiscoveryClient
จากนั้นทำการสร้าง PricingController
Note: @RefreshScope สำคัญมากนะครับ เวลาเราแก้ไขค่า configuration ใน git แล้ว เราจำเป็นต้อง POST request ไปที่ endpoint “/refresh” เช่น curl -d{} http://localhost:8000/refresh และ class หรือ method ไหนที่ได้ทำการ annotated ด้วย @RefreshScope จะทำการ refresh ค่า configurationให้เรา
Properties
จากนั้นทำการเพิ่ม configuration โดยเพิ่ม bootstrap.yml ใน src/main/resources และทำการ config ดังนี้
หลังจากที่เราได้ทำการ imprement clients ของเราทั้ง 2 ตัวแล้ว เราก็จะมาทำการทดสอบระบบของเรากัน
อันดับแรก run eurika-service ด้วยคำสั่ง mvn spring-boot:run จากนั้นเรียกไปที่ http://localhost:8888/ จะเห็นว่า ยังไม่มี clients มา register กับ eurika-service ของเราดังภาพด้านล่าง
จากนั้นทำการ run configuration-service และ clients ทั้ง 2 ของเรา ด้วยคำสั่ง mvn spring-boot:run จากนั้นเรียกไปที่ http://localhost:8888/ อีกครั้ง จะเห็นว่าทั้ง pricing-service และ product-service ได้ทำการ registered กับ eurika-service ของเราแล้ว ดังภาพด้านล่าง
จากนั้นเราก็ทำการทดสอบ Services ของเรา โดยเรียก API ของ Product-service ของเรา http://localhost:8000/products/search?sku=sku-bnk48
จากนั้นทำการแก้ไข configuration ที่ Git-backed configuration ของเรา โดยแก้ pricing-service.yml โดยการเปลี่ยน price ที่ product.price จาก 99.99 เป็น 100.00 ดังนี้
จากนั้นทำการ commit git ซะ
หลังจาก commit เรียบร้อยแล้วก็ทำการ POST request ไปที่ endpoint “/refresh” ของ pricing-service ดังนี้
หลังจากที่ refresh แล้วก็ทำการ request http://localhost:8000/products/search?sku=sku-bnk48 อีกทีจะเห็นว่า product price ได้เปลี่ยนเป็น 100.00 ไปแล้ว โดยที่เราไม่จำเป็นต้อง restart service !
สำหรับบทความนี้เราก็ได้ทำการสร้าง microservices ด้วย spring cloud ซึ่งเราได้ใช้ technology ของ spring cloud แค่บางส่วนเท่านั้น เช่น Config Server และ Discovery Server เท่านั้น ยังมีอีกหลายๆส่วนที่เราไม่ได้ลงรายเอียดและนำมาใช้ในบทความนี้ เช่น Netflix Hystrix, Zuul และ Ribbon เป็นต้น จริงๆถ้าจะให้ครบๆเราต้องทำ API Gateway และเพิ่มระบบ monitoring และใช้ Circuit breakers ในการ handle failure ขอระบบเราด้วย ซึ่งผมไม่ได้ลงรายละเอียดในบทความนี้ เนื่องจากเกรงว่าจะยาวไป สำหรับ source code สามารถหาได้จาก GitHub
Creating opportunities for all through world-class digital…
197 
3
197 claps
197 
3
Written by
Software Engineer at LINE Thailand | Learning is a journey, Let's learn together.
Creating opportunities for all through world-class digital platforms & services
Written by
Software Engineer at LINE Thailand | Learning is a journey, Let's learn together.
Creating opportunities for all through world-class digital platforms & services
"
https://towardsdatascience.com/easy-devops-for-data-science-with-saturn-cloud-notebooks-d19e8c4d1772?source=search_post---------379,"Sign in
There are currently no responses for this story.
Be the first to respond.
George Seif
Jul 8, 2019·4 min read
I write a newsletter for learners called Mighty Knowledge. Each new issue contains links and key lessons from the very best content including quotes, books, articles, podcasts, and videos. Each and every one is picked out specifically for learning how to live a wiser, happier, and fuller life. Sign up for it here.
Data Science can be a fun thing to do!
"
https://medium.com/accelerate-good/a-crash-course-in-the-cloud-with-a-google-sre-60497e9e2f55?source=search_post---------380,"There are currently no responses for this story.
Be the first to respond.
The age old adage “If it ain’t broke, don’t fix it” does not apply to Emmanuel Klu. In fact, Emmanuel, an amazing Google Site Reliability Engineer, is an expert at breaking things on purpose so he can figure out how to prevent those failures in the future.
We launched our 2018 Accelerator with Emmanuel’s talk on the cloud, because according to Emmanuel, it’s extra critical for tech nonprofits to build strong infrastructure. When things break in a tech nonprofit’s product, vulnerable populations are at risk, so mitigating failure is key.
That’s right. Sorry to break it to you, but nothing is free. That “free trial” you’ve been promised is laden with human and financial capital. Free credits incentivize you to build with one cloud company over another, but once your credits run out you need to be cognizant of how this will impact your organization financially. If you didn’t build things right from the beginning, you could have a ridiculously high monthly cloud spend once your credits run dry.
The cloud platform decisions you make now will have a long-term impact on your organization.
You need to be able to tie your impact unit to a cost. This impact unit will serve you any time you’re reporting impact, and is critical in fundraising conversations. The cost includes staff time, free licenses, and even the in-kind costs of volunteers. If you don’t bake in the full cost, you can’t show the full impact.
Here’s a simple way to calculate your impact:
Before building anything, carefully consider the cost in human and financial capital. Even though you have discounted product licenses or cloud credits, it’s imperative to think about how the features you plan to build will translate into future infrastructure costs.
You need to make sure that as you scale, your cost per impact unit goes down. Did you hear that? YOU WANT YOUR IMPACT TO INCREASE AND THE COST PER IMPACT TO DECREASE AS YOU SCALE.
By infrastructure cost, we mean everything it costs to support and maintain your product’s software. This mostly translates to storage costs, processing costs, and the engineering time it takes to maintain your infrastructure.
Engineering time is expensive, so it’s critical to work with your tech leads to determine how much your infrastructure will cost in maintenance. Understanding this cost will keep you more informed on the technical side of your product. To do this, require that your engineers (volunteer or hired) document their code and roadmaps in a way that’s readable and understandable.
The decision about whether to build or to buy depends on what stage your product is in. For example, imagine you’re using someone else’s API to complete a function, like a phone call. If that function costs 1 cent per call and you plan to eventually support 100,000 calls per day, your costs will be high.
According to Emmanuel, if you can buy infrastructure tools in the beginning, you probably should. Maintaining something you’ve built can be expensive and difficult. Even though buying usually costs more money up front, it may save you in the long run. If you do decide to buy, the first step is to decide between service providers, and you’ll need to get an engineer to review your options. They should understand how high your usage will be, and how set up your integrations with the right interfaces so if you ever find a cheaper option, you can swap dependencies.
Pro Tip? Focus on building an adapter for one specific API, so that at some point in the future if you need to change it, you just have to write a new adapter instead of rewriting everything.
There are four main types of compute products that you can build your software on. They get easier to develop on as you go down the list.
Virtual machines: This means you own the operating system of your environment. It’s yours alone and nobody else is running software on it. Consider a virtual machine if you run a consistent load, like processing 10 requests/second or for batch processes. This is because the costs are fixed, so you’ll have to determine the resources you’ll need upfront, and pay for the lifetime of ownership. Examples include Amazon EC2 and Google Compute Engine.
Container Engine: This type of compute product provides a thin container around things you’re running. Your code is isolated from other products, but other people are running code on the same operating system. The risk is that you may be affected by neighbors. Examples include Amazon Elastic Container Service and Google Container Engine (GKE).
Hosted apps: Running on this type of compute product means you share more resources with other products. After you build your app, you hand it over to the cloud provider and they’re responsible for scaling it. Placing that control in the hands of the service provider reduces costs for you. Examples include Amazon Elastic Beanstalk, Google App Engine, and Heroku. Pro Tip? Don’t accept huge bills.
Serverless: Within your product, if you have certain tasks that require event-driven behavior, consider outsourcing those tasks to a serverless computing service. You can rest assured that the computing service should reliably complete the task, and will be able to scale with load. Examples include Amazon Lambda and Google Cloud Functions.
Emmanuel’s final piece of advice was to require your engineers to document all of their code. Really, it’s advice worth mentioning twice. It’s imperative to have technical and architectural documentation showing how things work together so other people can help you! And, so you can understand the map of what your product is doing.
Surprising but true — having your site architecture clearly mapped is helpful when fundraising. Most funders are not technical, so an infrastructure map is a great storytelling trick. Funders can generally understand a map much more easily than code.
So, what are you waiting for? Time to get building! Thanks again to the incredible Emmanuel Klu, and the Google.org team for supporting our Accelerator year after year. Your dedication to Fast Forward helps our teams level up and we’re so grateful to benefit from your wisdom and support.
Fast Forward’s Look at the Intersection of Tech and…
308 
308 claps
308 
Written by
Improving the world by accelerating tech nonprofits
Fast Forward’s Look at the Intersection of Tech and Nonprofit
Written by
Improving the world by accelerating tech nonprofits
Fast Forward’s Look at the Intersection of Tech and Nonprofit
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/hackernoon/cloud-agnostic-architecture-is-a-myth-53eac80be85d?source=search_post---------381,"There are currently no responses for this story.
Be the first to respond.
Vendor lock-in — being locked into a specific cloud provider through the use of services specific to that cloud provider
I’ve been working with my head in the cloud for the past 2 years now and one thing is pretty certain — Cloud-agnostic Architecture is a Myth, at least in a conventional sense.
In this article, I’ll be discussing the current state of cloud services and how being truly cloud-agnostic is not only something we can’t quite achieve, but why it isn’t necessarily something you may want to achieve.
Our first port of call is Docker. I am a seriously massive advocate for containerization technologies such as Docker and have recently given talks demonstrating how these technologies could massively improve the way we build and deploy software.
One of the key advantages of Docker that I like to highlight, is its ability to reduce the friction when it comes to migrating from one cloud service provider to another. You specify everything that your application needs to run in terms of runtime, environment variables and any setup scripts and you are able to start said container almost everywhere. I say *almost* as there are some very minor exceptions such as Windows-only images etc.
This essentially means that the Python application we need to run can be containerized and dropped into any managed container service such as ECS. The underlying container platform will then manage the container lifecycle and provide additional benefits such as logging and monitoring should we wish.
The underlying OCI-compliant container runtime will then handle any discrepancies between hosts and run your application as you intended it to run with minimal fuss.
When it comes to trying to avoid vendor lock-in, containerization is an incredibly low-hanging fruit.
Terraform is the latest “shiny thing” that I’ve had the pleasure of playing with in my travels and I’m very much a fan of the CLI and how it lets you define your infrastructure as code.
You can specify the provider you wish to use as well as the access keys and secret keys and when you run terraform apply , the CLI will the go away and spin up the appropriate infrastructure for you to then deploy on top of:
If you subsequently need to tear down your infrastructure, you can with ease using the terraform destroy command, which is a command that sounds freaking awesome.
If you are looking for a good reference manual on Terraform, then I highly recommend the Yevgeniy’s book:
amzn.to
Now, this is great for spinning up your system’s required infrastructure on a specific cloud provider, but this means we will have to write multiple Terraform scripts for each of the distinct cloud providers.
This isn’t entirely out of the question, but we then have to maintain these scripts going forward in such a manner that we can ensure their correctness should we need to fail over to a given provider. There also needs to be a certain level of knowledge regarding the underlying platform in order to be explicit about what instance types you are spinning up or what size these instances need to be.
From an application developer’s point-of-view, you may not necessarily have to, or want to, care about what AMI you are creating your t2.micro instances from, you may just care about developing your application and having somewhere stable to run said application so that it can serve your paying customers.
One of the potential solutions to this that I have seen mentioned is to implement a layer of abstraction above all cloud service providers and essentially define your application’s underlying infrastructure as a series of blocks. Essentially an extension on top of Terraform that would have the ability to spin up cloud-platform agnostic infrastructure.
This Terraform-esque abstraction layer would then have the job of interfacing with whichever cloud platform it so chooses and spinning up said infrastructure. This would let you define say, a simple web app, as 1x “Load Balancer” and 2x “Small Server”.
To some, this seems like the obvious solution, but when it comes to implementing this abstraction layer and dealing with the nuances between how different services are exposed, this becomes next to impossible.
Trying to handle this at an application level would be next to impossible and your codebase would be riddled with hundreds, if not thousands of conditional flags in order to cover a large number of services. Could we abstract this into a service-broker or SDK? Maybe, but it just shifts our initial problem left.
So, Kubernetes is incredible and I’ve discussed some of its potential benefits for Enterprise environments here:
medium.com
Kubernetes, in combination with Docker and Terraform, may actually be somewhat of a good answer when it comes to attempting to be agnostic. It’s also something that has been explored by a number of different companies and individuals and we are starting to see managed solutions come about such as Triton.
www.joyent.com
These federated kube-clusters (klusters?) can then be treated as an amalgamous lump of compute that can then run and manage any container-based applications you may wish to run.
The biggest advantage of this is that application developers need just worry about how they want their application to look and does abstract away the underlying infrastructure.
But then this isn’t perfect, these services are still very young and have yet to hit the mainstream. It’ll most likely be a while before these services become mature enough to be picked up by larger enterprises.
Vendor lock-in — The art of leveraging specific services from a cloud provider in order to provide value to clients
One of the biggest things I’ve seen people disregard when talking about vendor lock-in, is the fact that there is a huge advantage in leveraging these managed services for your own gain.
When considering the use of these managed services, development teams need to weigh up how much of an advantage these services provide and consider how much of a disadvantage they are subsequently put at by locking themselves into said service.
If service X from AWS, or Azure, or GCP can provide you a massive advantage over your competition then it surely doesn’t make sense to avoid using said service because of vendor lock-in. If anything, by actively leveraging these services you may find yourself able to iterate faster with new ideas and push the bounds of what we are currently capable of.
If we consider the likes of AWS’ RDS, if we were to attempt to host our own database instance on an EC2 instance and manage this ourselves, we would end up spending hundreds of hours attempting to achieve the same level of resiliency, or performance that an out-of-the-box RDS offering provides.
This article goes into a more detailed comparison for self-hosted vs RDS:
cloud.netapp.com
Whilst vendor lock-in is something you should somewhat consider when designing and developing your applications, I would put far more stock in the advantage of using said managed service as long as you understand its constraints fully. Being mindful of the fact that you are relying heavily on these services is far more valuable in the long run and can give you an edge should you leverage these in appropriate ways.
As it stands however, being able to remain fully agnostic of cloud provider is just not possible. More work will have to be done in order to provide that level of abstraction above said provider and at this point it’s just turtles all the way down.
Hopefully, you enjoyed these ramblings, if you don’t agree with anything or would like to add to this then please feel free to let me know in the comments section below!
If you’d like to support me then feel free to follow me on twitter: @Elliot_f or subscribe to my YouTube channel!
youtube.com
N.B — This article was peer-reviewed by my good friend Alan Reid so please @ him if you see any mistakes!
#BlackLivesMatter
318 
2
how hackers start their afternoons. the real shit is on hackernoon.com. Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
318 claps
318 
2
Written by
Professional Software Engineer.
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
Professional Software Engineer.
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/dfinity/the-decentralized-cloud-vision-of-the-dfinity-blockchain-f68449c49be2?source=search_post---------382,"There are currently no responses for this story.
Be the first to respond.
This post explores the decentralized cloud vision of the DFINITY team and how it relates to traditional blockchain and existing cloud providers such as Amazon Web Services. Demonstrations of DFINITY technology applied by a large-scale network will be made in the fall of 2017 that will be followed by a Main fundraising for the supporting not-for-profit foundation, with the “open cloud” network expected to launch early summer 2018. DFINITY is a realization of dedicated blockchain research ongoing since 2014.
Today people mean many different things by the word “blockchain”. The word is often used to describe networks similar to Bitcoin designed to allow tokens to be transferred from one address to another in a verifiable and tamperproof way. But increasingly, the meaning relates to “blockchain computer” functionality provided by networks such as Ethereum. This is most similar to how those working on the DFINITY project understand blockchain. When DFINITY researchers talk about blockchain, they are imagining a super massive and highly performant blockchain computer in cyberspace that scales up its capacity as needed to provide a mainframe computer the world can share — ultimately, the world’s first blockchain that runs at web speed and scales without bound, enabling smart contracts to securely serve interactive web content directly into the browsers of end users. It will be underpinned by a decentralized network that will be an alternative to the centralized systems run by technology giants such as Google, Amazon, and Microsoft.
If you are non-technical, and aren’t that familiar with cloud computing, you can takeaway that the DFINITY network will create a giant virtual computer in cyberspace that will support enterprise IT systems (that is, backends for websites, HR systems, supply chain management systems and other things companies and other organizations run) and eventually mass market open source autonomous businesses and decentralized finance. Additionally, since DFINITY is a blockchain computer, it will also support all of the applications of blockchain discussed in the media today, but with far better performance and greater capacity than currently possible. This is achieved by applying numerous novel crypto innovations and technologies produced by our stellar team. This post examines the general perspective and explains product-market-fit.
DFINITY represents an Internet 3.0 paradigm in which the Internet acts as a computational resource with new properties that can run business systems, applications, and a new kind of “open source autonomous business”. This is very different from today’s pure cryptocurrency blockchains, private blockchain systems that Amazon will offer to host, or indeed today’s centralized cloud platforms such as Amazon Web Services. Those clouds will be able to offer DFINITY technology in service form to provide some of its special properties, but Internet 3.0 will be decentralized and open. For a start, many fully peer-to-peer applications such as file sharing (e.g. BitTorrent) or cryptocurrencies (e.g. Bitcoin) could not be run upon these centralized clouds as threatened industries and suspicious authorities might petition for them to be shutdown. But more fundamentally, businesses, application developers, and people generally will strongly prefer to build their systems on large open cloud networks for similar reasons they preferred the open Internet to proprietary curated walled garden networks such as AOL or Compuserve in the 1990s. Once this new paradigm takes root, network effects will quickly propel open scalable blockchain computers such as DFINITY ahead with explosive growth.
The first objective pursued by DFINITY is not particularly abstract and involves the mass hosting of enterprise IT systems. On first glance this seems a strange idea, since the complex protocols, cryptography, and high degree of data replication inherent in its workings will make raw computation much more expensive than on, say, Amazon Web Services. That is until you realize that the costs of enterprise IT are almost all related to the human capital involved in R&D, system maintenance, and administration. It is in dramatically reducing exactly this supporting human capital and thus overall costs that DFINITY will excel. The platform is unstoppable and data inside software persists automatically, often removing the need for continuity planning, backup and restore and traditional database management systems. Since the system is tamperproof, and there are no backends for hackers to gain access to, the need for security administration is reduced as well. DFINITY is also introducing a new R&D concept called “Liquid Development” (to be explained in a later post) that will reduce the cost of developing new systems by orders of magnitude.
The potential for cost savings is related to the special properties that the decentralized cloud provides through its high capacity blockchain computer underpinnings. These properties also make it much easier to create a single shared source of truth that can be verified by multiple parties and create systems that are inherently tamperproof among other things. Using these properties, enterprise IT systems can not only be reengineered to save money, they can also be fully reimagined to generate new value.
If businesses relocate their enterprise IT to the DFINITY network en masse, hundreds of billions of dollars in value will be generated each year as costs are cut and the benefits of its blockchain-derived properties are exploited. The migration in turn will fuel the creation of Open Decentralized Business Infrastructures (ODBIs). These can be thought of much like shared business infrastructure components that can be included into business systems rather like reusable software components. For example, in the past, if a developer created an HR system, say, he would program the forms that display employee data such as name, DOB, role and salary, but he would not program the software to decode and display staff photographs himself since this would involve years of work. Instead, this would simply be included in component form from some preexisting software library, which would be shared and reused by thousands of businesses. In the future, those encapsulating business logic inside enterprise software on the DFINITY blockchain computer will include ODBIs rather like developers include such software components, quickly extending their business operations in ways that would otherwise be too difficult and expensive.
Supply chains, which move goods and services, nicely illustrate how this will work. The DFINTY team is particularly familiar with this use case as it is supporting the development of Fortune 500 supply chain blockchain systems that will move many billions of dollars in value annually (note the terms supply “chain” and “blockchain” are unrelated, if you are unfamiliar with this area of business). Using DFINITY, it will be relatively easy to create a tamperproof supply chain system that tracks the movement of large volumes of goods and services that is shared amongst many industry participants, and once these systems are in production, additional exciting opportunities will be unlocked. For example, the 90 day payment terms of many invoices tie up billions of dollars in liquidity in many of the supply chains we have looked at (since vendors must wait for their payments and therefore cannot reapply the cash in their own operations immediately).
One way to address this is to create open markets where investors can finance invoices, paying say 97% of their value to the vendor immediately and then collecting the full 100% from the payer later to make a profit. Such functionality can be obtained via a market ODBI included as a component, but to resolve disputes occurring in the real world, for example relating to the non-performance of contracts, another ODBI providing arbitration functionality must also be included to resolve them. Since most lorry drivers also own their vehicles, another market ODBI can be used to automatically engage haulage services on demand where appropriate. Such ODBIs can be included like software libraries by any new supply chain system to greatly increase their impact while reducing the costs of development and maintenance. DFINITY Stiftung is currently working with BCG DV (Boston Consulting Group Digital Ventures) and its spinout DVolution to create ODBIs for supply chains and other types of business infrastructure responsible for billions of dollars in annual merchandise volume.
While reducing enterprise IT costs and enabling the creation of new business infrastructures is a key initial objective for the project, there are other equally important prongs to the DFINITY vision. In fact, originally the project was spun out of 2014 research into scalable blockchain technologies with the primary aim of enabling the creation of mass market “open source businesses” implemented as autonomous self-updating software systems (software independently resident on chain with an inbuilt governance system that enables it to upgrade and tune itself). We look at a company such as eBay, whose website has hardly changed in 20 years, which can simply extract rent by virtue of its monopolistic position as an absurdity, and believe the fabric of the Internet should play the role instead. This thinking can be extended to many intermediary systems from social media, through resource sharing systems such as Uber, to fundamental services such as messaging and, even, Web search. Search is particularly interesting technically since a good user experience depends upon provision of results almost instantly. Remarkably, relatively simple crypto techniques such as lottery charging and lazy validation can take you a long way toward this goal. Such services will themselves also become a kind of ODBI, since people will be able to easily integrate their colocated systems.
The final prong relates to decentralized finance. Currently, we believe that the term cryptocurrency is a misnomer. The problem is that money, or currency, performs three roles as a unit of account, medium of exchange and store of value. Unless its value is stable, it cannot perform the first two roles very well at all. The problems are obvious. If I borrow a bitcoin today, I might have to pay back twice the value tomorrow. If a farmer wishes to hedge the weather using the Gnosis prediction market on Ethereum, he will have a problem because the ETH he uses will be more volatile than the effects of the weather. DFINITY plans to address this by helping to launch the PHI system, which generates “cryptofiat” tokens that securely mirror local fiat currencies. An explanation of the workings are beyond this post, but it involves the issuance of loans using randomly selected sequences of validators, such that tokens are created that are transitively backed by aggregate liens on assets, cash flows and personal guarantees (in turn backed by income). As it turns out, PHI requires not only a blockchain computer with good performance and huge capacity, but also sources of unmanipulable and unpredictable randomness that DFINITY generates using special cryptography. We believe that it is this final step that will truly uncork the decentralization genie.
In summary then, it is perfectly fine to understand DFINITY as a decentralized cloud that will begin by reducing enterprise IT costs. This alone can generate hundreds of billions in value every year, and takes us nearer to the original Internet vision in which it was never intended that data and computational capacity would be concentrated in a tiny handful of huge data centers as they are today (after all, the Internet was conceived to help us withstand nuclear war). But this step quickly leads to the development of ODBIs, something that is truly revolutionary, where business infrastructure can be included like software in a new paradigm where businesses can be built from components. This, remarkably, is already in motion via business partners. These first steps precede much more disruptive phases in the development and application of the decentralized cloud, in which critical infrastructure and monopolistic intermediary businesses are decentralized and open sourced, accelerated by the advent of cryptofiat and the emergence of viable advanced decentralized financial systems in cyberspace. The aims of DFINITY are ambitious, which is reflected in the stellar team we are building. Whether the DFINITY project will succeed cannot be known, but any blockchain computer network that successfully delivers on even a fraction of this vision will become very valuable to humanity indeed.
A DFINITY Foundation Resource
591 
5
Thanks to Artia. 
591 claps
591 
5
Written by
President/Chief Scientist DFINITY
A DFINITY Foundation Resource
Written by
President/Chief Scientist DFINITY
A DFINITY Foundation Resource
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@pauldjohnston/cloud-2-0-code-is-no-longer-king-serverless-has-dethroned-it-c6dc955db9d5?source=search_post---------383,"Sign in
There are currently no responses for this story.
Be the first to respond.
Paul Johnston
Feb 22, 2019·6 min read
In recent months I’ve stopped blogging about serverless quite so much. There have been many reasons for this, but a lot of it has been down to the fact that I don’t have a lot to add to the conversation. Not because I don’t have anything new to say, but because I have already said a lot of it in previous blog posts or twitter threads or recorded talks. The conversations and points aren’t new to me and after a while, it becomes really hard and a little pointless to say the same things over and over.
But what has become clear to me is that there are some fundamental areas that still need clarification both for individuals using serverless as a term, and vendors who are building services and calling them serverless.
So, let me take you on a journey to why I think Serverless is the start of Cloud 2.0.
For a very long time in technology, building networked applications relied on software. Software that you had to understand intimately to know what was going on, or software that you downloaded from somewhere, and deployed.
The Open Source movement was in part a reaction to major software companies trying to control the supply of software for the internet and that led to an amazing explosion of some incredible tools and services.
In the late 1990s and early 2000s we saw some open source technologies emerge, most notably the LAMP stack, that laid the foundations for an incredible burst of innovation around internet technologies.
With a seeming inevitability, alongside virtualisation technologies, this led to cloud services and the behemoths that are the cloud vendors now who have transformed technology.
Cloud services became the norm, and even though many companies are still stuck in the era of on-prem, it is certainly becoming the de facto standard in enterprise.
And all this was predicated on the premise of being able to upload your own code to the cloud.
Code was king and Developers became the New Kingmakers — https://thenewkingmakers.com/ — as stephen o'grady of Redmonk has said for a very long time.
And that is how it’s been for a long time. Developers developing software, and Operations teams managing infrastructure, and while the DevOps movement has provided better ways for these teams to work together — CI/CD, process automation, improvements in understanding — the separation has still been there.
Then along comes Serverless. While a lot of people will argue about when Serverless came along (and what it is!), I am absolutely going to pin the birth of Serverless to the launch of AWS Lambda at re:Invent in 2014 (I will not go into why I am doing that here, it’s not totally relevant). While Serverless != FaaS, FaaS is a major enabler of Serverless, and this was the breakthrough that meant it became mainstream.
The key for me is that Serverless relies on the Developer to understand Ops to a much higher level than ever before. That is to say, the Developer needs to understand things like scaling behaviour, both how their uploaded logic will scale to 100 and scale to zero, and things like data access behaviours, how the impact of a spike in usage will affect data access or similar.
My definition of Serverless also is an economic one:
“A serverless application is one that costs you nothing to run when nobody is using it, excluding data storage costs.”
So this relies on a Developer understanding at least in part that they are part of a business, and not isolated from business decisions. Something that is often more a part of an Ops team’s remit when it comes to cloud.
When you take all of these elements, the Developer takes on much more responsibility in the new serverless world than under previous paradigms.
It is much easier for a Developer to not understand the impact of a change within a serverless solution, and unintended consequences to occur, having a financial impact.
The serverless shift is one that relies on their understanding of how their infrastructure works and actually less on how good or bad their code is.
Which brings us to an interesting point.
When you are building in a serverless way, you are often more reliant on the services provided by a cloud provider. As has often been suggested, serverless is not a good name, and “service-ful” may have been a better name.
And that does make sense. Understanding what services to use and when is a key element in the building of a good serverless application.
But that is only a part of the paradigm shift with serverless. There is a second part that isn’t always so obvious.
You actually want as little code as possible.
Because code is a liability.
I’m not the only one that thinks this as can be seen from the above tweet. Others such as Joe Emison talk about LOC (Lines of Code) being kept to an absolute minimum within a serverless paradigm.
Because if you have less code, you have less complexity.
www.infoq.com
And that includes your infrastructure which should be some form of defined language that allows you to deploy your application.
Because when you’re building a serverless application, you’re creating infrastructure and business logic to connect the services, and some “code” where the code is only what you write to achieve what the services cannot.
In fact, your code is far less important than how you architect your application. It is still important, but much less so than previously.
How you utilise the cloud services becomes your application.
I’d suggest this:
The ultimate serverless applications have zero code in them at all
Why am I calling this “Cloud 2.0” and making it a “new thing” as if it is somehow different?
It’s really very simple.
Up to now, the idea of cloud native has been about building applications and deploying onto servers, instances containers.
Serverless, and “Cloud 2.0” is no longer about that.
Serverless is about aiming to reduce your code to zero and use the cloud vendors services to do as much of the work as possible.
Which is why when someone talks to me about “running FaaS on Kubernetes” as being serverless, I find that baffling. That is, to me, seriously increasing the amount of code, and decreasing the number of services being used, and so completely opposite to the idea of serverless. Yes, it uses “Functions"" and if that was the only definition of what makes something serverless, then fine, but if you take a look at the above, this approach becomes ridiculous.
I started my technology career in a University Data Centre in the late 1990s, and was a systems administrator. I know what it means to run actual machines, and deal with all the problems that entails. I have spent my entire career trying to avoid all those problems.
The cloud vendors have spent over a decade building services so that you don’t have to build complex systems.
Why would you run your own Functions as a Service on top of Kubernetes or similar on a cloud provider if you can simply purchase the service from the cloud provider? It genuinely doesn’t make sense from an LOC/maintenance/time/effort point of view and unless you have to run on-prem or similar, then I don’t understand.
If you still want to, then go for it.
But you almost certainly don’t have to any more.
Taking on stephen o'grady and the phrase from his book, I wonder who the New Kingmakers will be?
I have a few ideas.
I think it will be those who grasp this new “Infrastructure is King” paradigm.
I think it will be those who grasp that perfect code is no longer a requirement for success.
I think it will be those who grasp that whatever code you write today is always tomorrow’s technical debt.
I think it will be those who grasp the business imperative behind their work is more important than their technical achievement.
I think it will be the Serverless Architects…
…and these will be people who understand high scaling and availability cloud services, and know how to write highly optimised and minimal code to connect them together, with a very pragmatic view of technical debt.
This new world lacks the tools and the stories and the heroes.
At present we’re using the tools of Cloud 1.0 to try to deliver serverless solutions.
And at present, the tech world isn’t quite ready for those new tools.
We’re not far off though.
Cloud 2.0 is Serverless
And that’s where we’re all going to end up. Of that I’m sure.
ServerlessDays CoFounder (Jeff), ex AWS Serverless Snr DA, experienced CTO/Interim, Startups, Entrepreneur, Techie, Geek and Christian
553 
5
553 
553 
5
ServerlessDays CoFounder (Jeff), ex AWS Serverless Snr DA, experienced CTO/Interim, Startups, Entrepreneur, Techie, Geek and Christian
"
https://blog.sia.tech/cloud-storage-for-2-tb-mo-8a34043e93bb?source=search_post---------384,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
One of the frequent claims of the Sia network is that over the long term, storage will be cheaper than $2 / TB / Mo, assuming that storage economics do not change. Though we’ve claimed this many times, we’ve never published a detailed model explaining where this number comes from. Until now.
For the purposes of this model, we are going to be assuming an endgame where Sia has substantially outgrown all of the latent / unused storage in the world, where the only way Sia can continue to grow is by having new datacenters established for the sole purpose of profiting from selling data to the Sia network. You can follow along with the math using this spreadsheet.
One of the key ways that the Sia network distinguishes itself from traditional cloud storage is its datacenter architecture and requirements. The Sia network only expects hosts to have a 95–98% uptime. Despite this, the Sia network is able to achieve 99.9999% uptime for files. This is because each piece of data on the Sia network is stored on many hosts, requiring only a subset of them to be online in order for the file to remain available.
Today, data is typically stored on the Sia network with a 10-of-30 redundancy scheme. This means that there’s a 3x overhead, and that as long as any 10 hosts are online, the file itself is still retrievable. Once the Sia network is more mature, we will likely be switching to 64-of-96 hosts.
If we assume that the 30 hosts go offline independently, and each host has a 95% chance of being online over a given time interval, the equation to determine the probability that a file is unavailable looks like this, giving a result of 10^-19, or 18 nines of uptime. Practically speaking host failures are not truly independent and you have to account for black swan situations like world war three. The true reality is that no system actually hits 18 nines of uptime (nor 11 nines of reliability).
Amazingly, even though 64-of-96 is only 1.5x redundancy, exactly half of the total copies of 10-of-30, the uptime equation has a nearly identical result — 17 nines of uptime.
In Sia’s endgame, 1.5x redundancy is the number that is most likely to be used in production contexts, so that is the number we are going to be using when we model the long term cost of storage.
An efficient storage rig is going to need the following parts:
What’s going here is we’re buying 32 HDDs, and then finding a low cost way to put them together. This is a technique that was championed by the Bitcoin mining industry, with unprecedented levels of corner cutting, bitcoin mining farms were able to get prices to be absurdly low. We can employ the same strategies here. With 95% uptime requirements, we don’t need to splurge on expensive parts like a full computer tower. The mobo selected above supports 48 gbps in data transfer, which means we still end up with over 1gbps per HDD even though we are doing a ton of splitting. It also turns out that 32 HDDs only consume 200w, so the 750w PSU we picked is more than sufficient.
In total, we’re spending $4945 to get a rig with 192 TB, or about $26/TB. And this is all paying consumer prices. This rig was assembled using consumer parts, when buying a large number of rigs at scale, a datacenter should be able to get much better pricing. So we will be using a rig cost of $4500 in our spreadsheet.
Often times for datacenter buildout, you budget around $1mm per megawatt in costs, or about $1 per watt. These rigs are going to consume about 400w each (6 watts per HDD, 65 for the CPU, 70 for RAM and mobo, and then loss from the PSU and datacenter PUE). In total, that means budgeting about $500 in capex for datacenter buildout.
Another cost that needs to be considered is networking equipment. If we’re assuming that you can fit about 8 machines per rack, and you need $2,000 of networking equipment in the datacenter per rack, you get $250 per rig.
Finally, everything is going to have to be assembled and constructed. Including screws and such, each rig has in the ballpark of 400 parts. That means about 2 hours of labor per rig. We’ll call that $50, bringing our total buildout expenses to $800 per rig.
The whole purpose of understanding capex for the purposes of a profitable datacenter build is to model depreciation. We need to understand how much value we are losing on our hardware every year that the datacenter is in operation. Hard drives last on average about 7 years. Hard drives also go down in price over time, meaning that you lose value a little faster than hard drives break. For this example, we are going to assume a 15% depreciation rate on our storage rigs, and a 5% depreciation on our build-out. That means our total depreciation costs are $785 per year.
Investors putting money into datacenter buildout are going to expect to make more revenue than just operational costs and depreciation costs. I think a reasonable profit expectation for a datacenter built to support a mature Sia ecosystem is around 10%, because the risk of building storage for a mature platform should be relatively low. Earlier in the life of the Sia network, risk may be higher, therefore profit requirements may be higher. At 10% profit expectation, the rigs will need to earn $570 per year in profit to be considered a good investment.
Our rig takes up about 2 square feet. The shelf that I linked should actually be able to hold about 3 of these rigs, and should be able to stack 3 or more of these shelves together. If you account for leaving space for airflow and aisles, you end up with about 1 square foot per rig, meaning a 2,000 square foot datacenter can hold 200,000 TB. The cost of rent is going to be negligible compared to other expenses, and is therefore excluded. Electricity aside, utilities should be approximately negligible as well.
If we assume a PSU efficiency of 93% (this is easily attained at datacenters) and a PUE of 1.4 (generally attainable for this type of setup), you get about 400w per rig. Mining farms often have electricity costs at low as 4 cents, but Sia datacenters are restricted by the fact that they need to have access to good network connections, so we will budget 10 cents per kilowatt hour for electricity. At 500 watts per rig, this comes out to $450 per year in electricity expenses.
On Sia, bandwidth is priced separately, renters pay for upload bandwidth and download bandwidth independently from storage, which means that we can exclude all ISP costs from this equation. Those expenses are covered as renters utilize the bandwidth.
Typical datacenters often have multiple redundant power agreements, backup power setups, batteries, etc, so that they can maintain 99.99% uptime. They will also have technical staff at the datacenter 24/7 to react quickly to any failures or outages. None of these things are required when your uptime target is 95%, and therefore this is a huge set of costs we can ignore when designing Sia datacenters.
Sia datacenters will however need at least a bit of maintenance. For a 32 HDD system, you expect about 5 drives to fail per year. This takes time to repair and you will need on-site staff (just not 24/7). To account for these costs, we will budget $50 per year per rig.
All told, we are at about $1850 per year in expected revenue. This combines the depreciation costs, the electricity costs, the utility costs, and the profit requirements.
A mature datacenter should be able to maintain a utilization of 90% or more by purchasing equipment on an as-needed basis. Assuming that a datacenter does achieve 90% utilization, a 192 TB storage rig is going to need to earn all of its revenue on only 172 TB.
Something that is unique to the Sia network is collateral lockup. When hosting someone’s data, you need to lock up collateral to retain that data. When storing 172 TB of data, a host should expect to have about 6 months worth of revenue locked up. At a 10% profit expectation, that comes down to losing about 1 month worth of revenue to capital expenses per year on actively used storage. So really, we should only be counting 11 months of revenue, as that 12th month is going to pay for our collateral lockup expenses. We compute the collateral expenses as a number of months lost because it can be computed independently of other values this way and simplifies the math.
The final expense is the siafund fee. All storage on the network incurs a roughly 10% fee, which means the renter is going to be paying more than the host is earning.
With a total revenue requirement of about $1750, an effective utilization of about 172 TB, and an effective earnings period of 11 months, we compute that the host needs to be earning about 90 cents per TB per month in revenue for an investment to make sense. Accounting for siafund fees, the renter needs to pay about $1.00 per TB per month. And then accounting for 1.5x redundancy, the final cost of redundant storage is about $1.50 per TB per month.
This is possible because the Sia network is designed so that datacenters can take shortcuts while building. The traditional model for the cloud assumes that datacenters need to be ultra high reliability, and often also assumes certain performance requirements on the internal network and servers of the datacenter. Sia is much more relaxed, everything needs to happen on the order of milliseconds as opposed to microseconds, and is perfectly comfortable with datacenters and storage rigs that have over a day of downtime per month on average.
More to the point, as a whole the Sia network has been carefully designed to be optimal as a decentralized system. Many of the fundamental design choices for Sia had to be different to achieve decentralization, and we have been able to leverage the strengths and weaknesses of these requirements to create something entirely new, and ultimately far superior to the traditional cloud.
Try out Sia’s personal storage solution at https://sia.tech or Sia’s content publishing and distribution solution at https://siasky.net
Decentralized storage — Sia, Skynet, and cryptocurrency.
499 
3
499 claps
499 
3
Written by

Decentralized storage — Sia, Skynet, and cryptocurrency.
Written by

Decentralized storage — Sia, Skynet, and cryptocurrency.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/theta-network/theta-labs-announces-google-cloud-as-enterprise-validator-and-launch-partner-for-theta-mainnet-2-0-8f765096f2a9?source=search_post---------385,"There are currently no responses for this story.
Be the first to respond.
San Jose, CA, May 27, 2020 — Theta Labs, Inc., a leading video delivery network powered by an innovative new blockchain and distributed ledger technology, announced today that Google Cloud has joined its Enterprise Validator Program along with Binance, Blockchain Ventures, and gumi. Theta’s Enterprise Validator Node program allows enterprises to validate transactions in accordance with Theta’s underlying consensus protocol. Google Cloud is also now the company’s preferred cloud provider, and users around the world will have the ability to deploy and run Theta nodes directly from Google Cloud Marketplace (GCP Marketplace) with just a few clicks. The turn-key cloud solution went live on GCP Marketplace today, coinciding with the launch of Theta Mainnet 2.0.
Theta Network is quickly advancing towards full decentralization of its security and governance. Google has joined the Theta enterprise validator program by running a validator node, and is providing the stability, reliability and security offered by the Google Cloud Platform to the Theta Network. Theta and its partners in the media & entertainment, telecom, technology and gaming industries thus provide a high-performance decentralized micropayment network that scales to millions of concurrent video viewers.
“Distributed ledger technology enables new business models that potentially transform the global digital economy, including the media & entertainment industry,” said Allen Day, Developer Advocate for Google Cloud. “We’re impressed by Theta’s achievements in blockchain video and data delivery. We look forward to participating as an enterprise validator, and to providing Google Cloud infrastructure in support of Theta’s long-term mission and future growth.”
Theta Mainnet 2.0 and the Theta Guardian network, a unique decentralized layer of security and consensus run by community members, is now available for users to access via Google Cloud Marketplace. As part of the partnership, Google Cloud will also power the cloud infrastructure for THETA.tv video platform, a fast-growing first-party esports streaming site built on the Theta Network.
“We’re thrilled to deepen our partnership with Google Cloud across a number of key strategic areas to accelerate Theta adoption across industries,” said Mitch Liu, co-founder and CEO of Theta Labs. “We welcome Google Cloud as an enterprise validator along with our other global partners to further strengthen the security and decentralization of our protocol. As we continue to build our network and streaming business, Google Cloud is the perfect partner to help us scale globally, with extensive geographical coverage offering ease-of-use, networking advantages and platform performance.”
Next generation video delivery powered by you
1.6K 
3
1.6K claps
1.6K 
3
Written by
Creators of the Theta Network and THETA.tv — see www.ThetaLabs.org for more info!
Next generation video delivery powered by you
Written by
Creators of the Theta Network and THETA.tv — see www.ThetaLabs.org for more info!
Next generation video delivery powered by you
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/seedify/seedify-integrates-phala-network-for-trustless-and-secure-cloud-computing-power-to-its-customers-5bf6bd814ad7?source=search_post---------386,"There are currently no responses for this story.
Be the first to respond.
Hello, Seedify Community!
Today’s announcement has us up on ‘cloud nine’. We are proud and thrilled to share our new partnership with Phala Network!
Cloud computing has played a transformational role in the development of new tech projects. Having access to existing computer system resources, such as; data storage, computing power and speed, and analytics, has afforded developers the ability to build and launch applications and software with reduced time to market. Further benefits also include less initial capital requirement and having the ability to scale without the extensive cost of upgrading infrastructure. Although this innovation has been undeniably advantageous, it also comes at a price, with large cloud services providers like AWS and Google reaping significant financial benefits through selling users’ private data. This is an issue that Phala Network’s forward-thinking technology solves.
Phala Network is a Web 3.0 cloud computing platform that supports data privacy while remaining trustless. Unlike centralized cloud services, Phala doesn’t own any server or data center. Anyone can provide permissionless servers into the Phala Network, and because of a clever combination of blockchain and trusted execution environments (TEE), Phala can ensure that there are no ‘evil’ servers in the network, even when they are in an edge network situation. Together, this creates the infrastructure for a robust, secure, and scalable trustless computing cloud. Phala Network is also developing high-performance smart contracts that will offer a decentralized alternative to Google Analytics.
Through this strategic alliance, Seedify will direct new blockchain game projects towards utilizing Phala’s decentralized cloud computing resources. At the same time, Phala Network will introduce GameFi projects using its services to Seedify’s incubator and launchpad platform.
Phala Network is a marriage between the best of Web 2.0 and Web 3.0 and will allow new GameFi projects to gain access to crucial infrastructure in a decentralized setting. Through Phala’s platform, gaming projects will have the ability to employ cloud resources with computing speeds comparable to large centralized cloud services providers like AWS and Alibaba Cloud without compromising the privacy and security of their community’s private data.
Phala Network also features interoperability. Built on Substrate and being a Polkadot para chain, it has the ability to serve the entire Polkadot para chain ecosystem while also serving other blockchain developers.
GameFi projects introduced to Seedify through Phala Network will have access to the full benefits of the Seedify Gaming Incubator and Launchpad ecosystem. This includes; various early-stage funding opportunities, incubation, solutions partners, launchpad and all other platform services, which will be made available, depending on each project’s specific needs.
Through Phala Network’s game-changing ability to separate computation and consensus by utilizing TEE tech and blockchain, they can provide a 2-fold solution, bringing decentralization and user privacy to cloud computing, a space that has offered neither of these features up until now. We are excited to partner with Phala, which will allow us to provide cloud computing solutions to the projects we work with while simultaneously protecting our community’s private data.
Levent Cem Aydan — CEO and Founder, Seedify
We are glad to partner with Seedify to explore the potential integration of gaming projects into the Phala universe. We hope to provide a powerful, secure, and scalable trustless computing cloud to all the Seedify users as well as enhance the private data security for GameFi projects.
Marvin Tong — Co-founder, Phala Network
Through this partnership, Phala Network and Seedify have the ability to work together, engaging GameFi and decentralization to usher in a new era in cloud computing, where the integrity of users’ private data is a vital part of the resources and solutions that cloud computing platforms provide.
To learn more about Phala Network, follow the links:
About Seedify
Seedify is a blockchain gaming-focused incubator and launchpad ecosystem, empowering innovators and project developers through access to funding, community and partnership building, and a full support system to help drive the future of gaming and blockchain.
To learn more about Seedify, follow the links:
Blockchain Gaming Incubator & Launchpad
310 
310 claps
310 
Seedify is an incubator & launchpad platform for blockchain games and gamers. Through holding SFUND, become eligible to participate in Initial Game Offerings.
Written by
Digital Marketing Manager at Seedify
Seedify is an incubator & launchpad platform for blockchain games and gamers. Through holding SFUND, become eligible to participate in Initial Game Offerings.
"
https://medium.com/design-ibm/ibm-cloud-continues-to-demonstrate-design-excellence-7a5150124136?source=search_post---------387,"There are currently no responses for this story.
Be the first to respond.
The IBM Cloud design team is thrilled to announce that two of our products, IBM Data Science Experience (DSX) and IBM App Connect, have received an iF Design Award 2018 in the Communications — Software Application category. Each year, the world’s oldest independent design organization, iF International Forum Design GmbH, organizes the iF Design Award. For 65 years, the iF Design Award has been recognized as an arbiter of quality for exceptional design.
IBM DSX and IBM App Connect won over the 63-member jury, made up of independent experts from all over the world, with their user-focused experience design for enterprise software. The competition was intense: over 6,400 entries were submitted from 54 countries in hopes of receiving the seal of quality.
This award is an additional achievement to two other design awards recently given to IBM Cloud, the Red Dot Award and the Spark Award, and serves as an unequivocal indicator of the quality of work produced by IBM Design and how much design has come to impact the entire organization.
IBM App Connect is a cloud-based solution for business users that quickly connects apps to automate tasks and boost productivity. Instead of manually performing high-touch, repetitive integration tasks, it gives users a simple, business-oriented interface to create workflows and get notifications, sync data, track outcomes and more. Business decision makers that are looking to integrate their business apps and combine their data often don’t have an easy way to do so. They either write custom code or use complex integration platforms, which is time consuming, involves lots of software and requires deep middleware expertise. IBM App Connect was designed to enable users to seamlessly connect and integrate with back-end systems, web, and mobile applications wherever they reside — whether on-premises or in a private or public cloud.
We are passionate about creating amazing experiences for our users, and it’s brilliant being part of an eclectic team working together to make this a reality!” — Sarah Burwood, Design Lead & UX Designer
From a design perspective, the IBM App Connect project provided an exciting opportunity to re-imagine the integration experience for new users, and apply design thinking to this business context.
“My proudest memories of our App Connect journey naturally centre around the designers. As customers reacted to what we’d made, they observed and listened; then the whole IBM team moved as one to iterate and enhance that user experience.” — Brian Peaston, IBM Design Manager, Application Integration
IBM Data Science Experience (DSX) is a cloud-based, innovative, end-to-end experience for data scientists to research, create, and collaborate across multidisciplinary teams. DSX merges a powerful set of open source and legacy tools with an online community for data scientists. At the time of its inception, there wasn’t a unified platform available to data practitioners. Our primary mission was to make data simple and accessible through our user-centered design process. The data science field is complex and constantly changing. DSX also continues to expand with the field, incorporating new, richer technologies as data scientists’ workflow and tools shift.
“Data Science Experience is a true innovation, merging a powerful set of tools with an online community. There are no barriers between creating, learning and sharing.” — David Townsend, Director of Design (IBM Analytics)
Enterprise software is often looked at as the roach motel of complicated and clunky user experiences, far removed from the modern, elegant and intuitive user experiences available in many consumer apps. However, “the times they are a-changin” as businesses are starting to realize how UX can truly be a key differentiator in the enterprise software industry. There’s a growing realization that organizations need to offer enterprise and business software that works as well and as intuitively as the other apps that their employees use in their daily lives.
Enterprise software needs product experiences that increase employee engagement, simplify complex workflows, reduce turnover rates, eliminate the need for training, and most importantly, increase user satisfaction and employee productivity.
As a leader in the enterprise software field, IBM has the ability to set a global precedent of how design can impact products, users, and businesses on a large scale. At IBM Design, we are delivering on that promise to bring sexy back to enterprise software.
The 4 design awards for IBM Cloud products in the past year, including these two from iF Design, display the incredible strides that IBM has made as a design-driven organization, as well as the integral role that design plays in IBM’s cloud products. We believe that continued and unwavering focus on our users, and on the design quality and user experience of our products will enable IBM Cloud to continue to excel, set precedents and differentiate by design.
IBM DSXDirector of Design: David Townsend (IBM Analytics, San Francisco)DSX Design team: Caroline Law, Renee Mascarinas, Jessica Gore, Leila Johannesen (IBM Analytics, San Francisco)
IBM App ConnectDesign Manager: Brian PeastonApp Connect Design Team: Sarah Burwood, Gary Thornton, Rob Breeds, Danny Skinner, Lee Chase and Steve Haskey.
Arin Bhowmick (@arinbhowmick) is Vice President, Design at IBM based in San Francisco, California. The above article is personal and does not necessarily represent IBM’s positions, strategies or opinions.
Stories from the practice of design at IBM
417 
2
417 claps
417 
2
Stories from the practice of design at IBM
Written by
Global VP Design | Chief Design Officer, @IBM |Cloud, Data and AI I UX Leadership| UX Strategy| Usability & User Research| Product Design
Stories from the practice of design at IBM
"
https://medium.com/thinking-design/master-your-ux-vocabulary-with-these-50-must-know-terms-d840cf5fb502?source=search_post---------388,"There are currently no responses for this story.
Be the first to respond.
Designing is a complex process in and by itself. Add a little confusion caused by industry jargon, and you’re in deep waters.
Mastering the UX terms and tech words that often surface in design conversations is a surefire way to keep your head in the game and avoid awkward situations. I’ve gathered the 50 must-know UX terms that will give you the confidence to engage in design talks and get the projects moving.
A/B testing, also known as split testing, is a method of comparing two versions of online content against each other to determine which one drives more conversions and revenue. It is a great method for figuring out the most effective promotional and marketing strategies.
API stands for Application Programming Interface and is basically code that allows apps and websites to communicate with each other. The API defines the correct way for a developer to write a program that requests information from another application. A good example would be an app like CityMapper that retrieves information from city transports systems and provides real-time travel updates and advice.
Accessibility is a function of access that enables people with disabilities to understand, navigate and interact with a website or application. Designing for accessibility means designing for a very diverse set of users, which might include users who are color blind, blind, visually impaired, deaf, people with cognitive disabilities, and so on.
An avatar is an image that represents an online user on the screen. Avatars are typically used in gaming, online forums, and chatrooms and can be represented in either three-dimensional or two-dimensional forms. They can often be customized and personalized by users.
Breadcrumb or breadcrumb trail is a secondary navigation aid that helps users understand and keep track of their location on a web page.
A software bug is a program error that causes it to crash or produce incorrect and unexpected results. Bugs prevent applications from functioning as they should and are most commonly a human mistake.
A backlog is a list of tasks and requirements that need to be completed within a sprint. The agile product backlog represents a prioritized list of tasks that is derived from the roadmap.
Back-end development is simply writing the code that is not seen by the end user but essentially powers the application. Back-end developers are responsible for building the logic behind the application and organizing the site’s structure.
A chatbot is a computer program that uses textual and auditory methods to conduct conversations with human users online. It is powered by artificial intelligence and enables human-computer interactions via a chat interface.
CRM (Customer Relationship Management) Software refers to a category of software products that help businesses manage a diverse range of business processes, such as sales automation, customer data, customer interactions, and so on.
Clickstream is a record of a user’s activities online, including every website or page the user visits. It’s represented as a path that the visitor takes through the website.
Cache is a temporary storage location on your computer that stores commonly accessed data to shorten data access times and reduce latency.
CSS (Cascading Style Sheets) is a language used to add style, such as fonts, colors, spacing, layout and other, to web documents.
Conversion Rate refers to a percentage of website visitors who complete a desired action on the site.
DevOps is the combination of people, practices, cultural philosophies and products that enable organizations to deliver applications at high velocity. It is often achieved through better communication and collaboration between software developers and IT professionals.
In product development, an end user represents the person for whom the product is designed/created and who is ultimately intended to use it.
Engaged time is a metric that tracks the amount of time a user spends on a specific page on a site. This helps businesses determine what users want as well as identify any friction points that could be removed.
Experience architecture combines multiple user experience and design processes, like information architecture, experience design, and interaction design, to articulate a clear user journey.
Eye tracking is the process of measuring eye activity. It uses tools like special glasses to record what users look at and in what order.
An emoticon is a sequence of printable characters that represent a facial expression and are used to convey emotions in a text medium.
Fishbone Diagram is designed to demonstrate the cause and effect relationship and can help to visually display the various potential causes for a problem or effect. It is known as Fishbone Diagram because of its shape resembling a fish skeleton.
Front-end development is a combination of programming and layout that powers the interactions and visuals of a site. Front-end developers use CSS, HTML and presentational JavaScript code to implement web app designs created by web designers.
Focus Group is a research method that uses a group of target users to identify information required to develop or improve a product. A focus group is usually led by a moderator that guides the discussion to obtain feedback about products, features, users, strategies, and so on.
Flat design is a minimalistic style of interface design that emphasizes usability and uses a minimum of stylistic elements, typography, and flat colors to create the illusion of three dimensions.
GitHub is a web-based development platform that developers use to share and review code, collaborate on projects, and learn from their peers.
Gamification is the process of making the most boring actions funnier and more entertaining to execute in an attempt to engage and retain the user’s attention.
Geolocation indicators make it possible to tailor the experience to every user based on where they are. They are especially popular on mobile apps that require location-based design, such as mapping or travel services.
Heart Framework is Google’s user experience tool that combines traditional metrics with emotion-led indicators to learn more about their customers, their emotions and user experience on a larger scale.
A hack is a quick, unconventional solution to solve a problem.
Every device connected to a network has a unique identifier known as its IP address.
Iterative development is a development approach that breaks down the development process into smaller cycles. Usually, a set period of time is allocated for every cycle, such as data gathering, design, development, and testing.
iOS is a mobile operating system created by and used exclusively on Apple products.
Javascript is a programming language of HTML and the web. It is often used to create interactive effects and dynamic pages.
A landing page is basically any web page on a site that a visitor “lands” on. Landing pages often act as extensions of ads, search results or links that led a visitor to click through and are designed to drive higher conversions.
KPIs stand for Key Performance Indicators and are used to measure the usability and effectiveness of a site or digital design.
MVP, or Minimal Viable Product, is a version of a product that has the minimum amount of features but can be used for user testing or demonstration. This approach allows for a more elaborate, in-depth testing phase before its potential is determined.
A mental model is an explanation of a person’s thought process about how something works in real life. The closer a user’s mental model is to the functionality of a site, the more intuitive user experience.
A mockup is a detailed static representation of the design that clearly displays the information structure, renders the content and demonstrates the basic functionality.
The term “open source” refers to something that is publicly accessible and can be used or modified by anyone.
A path is the sequence of steps a user takes when navigating through a site and can often be shown by breadcrumbs.
A user persona is a representation of a target user that is created based on research findings and available data. User personas are typically fictional, but the information used to create them is not.
A prototype is a highly detailed representation of the final product that can simulate user interactions with the interface.
Code refactoring is a technique for restructuring and improving code hygiene as well as optimizing code design for agile programming.
Responsive design is an approach to web design aimed at allowing a website to adapt to the device a user is viewing it on.
A sprint is a set period of time during which a chosen task must be completed and made ready for a review. Commonly, a sprint lasts between one and three weeks.
Slack is a real-time messaging and archiving app that helps teams collaborate and share ideas more effectively.
Usability testing is a research method used to measure the accessibility and ease of use of a product or design by testing it with real users.
A user journey is a sequence of steps a user takes to reach their goal when navigating a website. It is simply an experience a user has while interacting with software.
A widget is an on-screen element that users interact with. Common examples of widgets are contact forms, calendar tools, sliders, and so on.
You’re quite right in thinking that it’s almost impossible to know all the lingo. But maintaining an up-to-date understanding of the change and evolution of design terminology is the only way for designers and developers to speak the same language and work together effectively.
Tomas Laurinavicius is a lifestyle entrepreneur and blogger from Lithuania. He writes about habits, lifestyle design, and entrepreneurship. Right now, he’s traveling the world with a mission to empower 1 million people to change their lifestyle for good.
Originally published at blogs.adobe.com.
Learn about Adobe XD, our all-in-one design and prototyping tool:
Stories and insights from the design community.
127 
2
127 claps
127 
2
Stories and insights from the design community.
Written by
New Tools for New Creatives. Get all the latest creative apps plus seamless ways to share and collaborate. All right on your desktop.
Stories and insights from the design community.
"
https://medium.com/thinking-design/5-tips-for-graphic-designers-switching-to-ux-design-452f56e2a40f?source=search_post---------389,"There are currently no responses for this story.
Be the first to respond.
Technology continues to open up exciting career paths for design professionals. One of the hottest jobs right now is user experience (UX) designer, and companies are willing to pay top salaries for people with experience: the median salary for a UX Designer in the U.S. is $70,000/year for entry-level, and $100,00/year for experienced professionals.
With demand at a high and lots of transferable skills between professions, graphic design might see UX design as a move worth making. If you’re a graphic designer looking to become a UX designer, but not sure how to start the transition, this article is for you.
Before we dive into details on how to make the transition to UX design, it’s essential to define what UX design is all about and how it’s different from graphic design.
While graphic design and UX design do have some commonalities (they both require creative thinking), there’s a major difference between two — responsibilities and end-goals:
Another important difference between graphic design and UX design is design process. While for a graphic designer the design process finishes once the product is launched; for a UX designer the product launch is just a step in design process. A UX design should be continually tested and adjusted based on user feedback. Thus, UX designers should be ready to rework their prototype and correct their hypothesis based on user’s needs.
Many people believe that UX is an exclusive club that only those with the right talent and extensive training can join. It’s not true. In fact, the career shift may come naturally to those who already possess strong design skills. Here are five things to remember when moving from graphic design to UX design:
One way to make sure that you’re ready to transition into a career in UX design is by investing some time and effort in learning UX skills. While graphic design is a specialized discipline, and there is a certain set of specialized skills (such as typography and color theory) required to produce great visuals, UX design is much more multi-disciplinary. UX design sits at the crossroads of a lot of fields and designers have to constantly learn about human psychology, visual design, interaction design, information architecture and user research techniques in order to create the right solutions to user problems.
While it’s impossible to learn all disciplines right away, it’s still possible to provide a few recommendations on how to get started:
What do employers look for when hiring UX designers? Two factors they consider are relevant professional experience and designportfolio. If you don’t have the former, focus on the latter. Career-switchers often face the same dilemma as recent graduates looking for their first jobs: to get hired for a UX design job you need UX experience. But how can you get that experience? It’s recommended to show your potential in any way that you can:
When you have a graphic design background, creating a pixel-perfect design is likely the aspect you enjoy most. Ensuring text has perfect kerning and colors are selected according to brand guidelines often takes up a significant portion of a graphic designer’s time. This isn’t the way things work for UX design.
UX designers are primarily focused on users and strongly concerned with whether they are able to achieve their goal. To create user-focused design you need to keep following things in mind:
A UX designer’s job is to create a product that provides the best possible user experience. How does that happen? It starts with a lot of research. Research is an essential part of the UX design process, as it informs the product’s design. You can’t create a valuable product for your users unless you understand the problems they face and how you can solve those problems via design. Graphic designers looking to switch career tracks will need to invest time into learning how to conduct user research.
Don’t be afraid of showing your work to others and let people test your thinking — you learn a lot from knowing what worked and what didn’t work.
Once you’ve got practical UX design skills and created your UX portfolio, you’ll need to focus on networking. Networking is essential for UX designers since the best opportunities are often found when someone already in the field recommends you for a position. One of the best places to start networking are LinkedIn and Medium. Join local UX-related groups, start asking and answering the questions, and you’ll eventually build all important network with your peers. But take it slow — don’t just show up and start asking for a job, you need to build relationships with people first.
One other useful way to start networking is to follow UX experts on Twitter. Here are just a few names: Don Norman, Luke Wroblewski, Steve Krug. Not only will you learn a lot from them, you can also interact with their followers who are mostly designers like you.
Is there a gap between graphic design skills and UX design skills? Yes, but not an insurmountable one. Graphic designers already speak the language of design. The world of user experience design is full of opportunities to expand your creative career. Good luck!
The Rubix Cube is is not the only twisty puzzle. Learn about Pyraminx, the 2×2 and 4×4 cubes, the Megaminx on Ruwix.
Nick Babich is a developer, tech enthusiast, and UX lover. He has spent the last 10 years working in the software industry with a specialized focus on development. He counts advertising, psychology, and cinema among his myriad interests.
Originally published at blogs.adobe.com.
Learn about Adobe XD, our all-in-one design and prototyping tool:
Stories and insights from the design community.
181 
2
181 claps
181 
2
Written by
New Tools for New Creatives. Get all the latest creative apps plus seamless ways to share and collaborate. All right on your desktop.
Stories and insights from the design community.
Written by
New Tools for New Creatives. Get all the latest creative apps plus seamless ways to share and collaborate. All right on your desktop.
Stories and insights from the design community.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/codeduck/cloud-run-vs-app-engine-dc1871abedca?source=search_post---------390,"There are currently no responses for this story.
Be the first to respond.
In a nutshell, you give Google’s Cloud Run a Docker container containing a webserver. Google will run this container and create an HTTP endpoint.
"
https://rominirani.com/tutorial-flutter-app-powered-by-google-cloud-functions-3eab0df5f957?source=search_post---------391,"The Beta release of Flutter from Google has caught the attention of several developers and that includes me. I am always interested in trying out developer tools and understanding how companies are pushing the envelope in terms of productivity.
This blog post is by no means a detailed tutorial on Flutter and for that matter Google Cloud Functions, but aims to demonstrate how straightforward it was for me to put together the two.
We all love inspirational quotes .. don’t we? This is a simple “Famous Quotes” app that has just single screen and shows a random famous quote. We shall use Flutter to create our mobile application and we will deliver a random quote to the Flutter mobile application via Google Cloud Functions, which is the Serverless FaaS (Functions as a Service) available on GCP (Google Cloud Platform).
Here is a screenshot of the Famous Quotes application in action:
The Quotes are completely managed by a Google Cloud Function that we shall deploy. For the sake of keeping things simple, we will simply have about 3–4 quotes available as sample data in the function itself and it will return us a random quote.
So, when we click the Get a Random Quote button that you see in the screen, it will directly invoke the Google Cloud Function that we have deployed and then render the data.
Sounds good? Let’s get going.
Here are the prerequisites before you get going with the rest of the tutorial:
Click on that and then Enable the API.
Now, that we have the prerequisites in place, let us do this in 2 steps:
I am going to demonstrate developing our function completely in the cloud via the Google Cloud console. The only reason I am doing this is to keep things simple and our function is very compact and there is nothing much to it.
In a real world scenario, I expect that you do your development locally and either deploy it from a repository or from your local machine via the gCloud SDK.
Assuming that you are logged into to Google Cloud Console and selected your specific Google Cloud Platform project, navigate to Cloud Functions.
Click on CREATE FUNCTION as shown below:
This will bring up a form where you can provide the details for your function as follows:
7. Function to execute : In this field, the value will be getRandomQuote as you can see from the exports field in the above code.
8. Go ahead and click on the Create function. This will create the GCF function and you should see it deployed and ready in the list as shown below:
9. Let us test out our function by clicking on the function above. It will lead you to a details page for the function, where you can not just track the metrics, source code but also test out your function. You should see a screen similar to the one shown below:
Simply click on the Test the function button and you should get the output.
Please make a note of the FUNCTION_URL as mentioned earlier. You will need this in your Flutter App. You can also test out the FUNCTION_URL directly in your browser and you should get a similar JSON output for a particular quote.
Great! Now that we are done with developing our GCF, it is time to take a look at the Flutter App.
Since I have just started with Flutter, I heavily relied on the documentation to help me get started. Here are a couple of specific links that helped me understand the basics of a simple Flutter App.
I strongly recommend that you go through the above documentation links. They are worth your time to understand what is going on.
Assuming that you have setup the Flutter plugin in Android Studio, you can create a new Flutter project. You will see some sample code present in the lib/main.dart file. Simply replace it with the source code given below:
Famous Quotes Flutter App source code:
Here are some key points from the above source code:
If you have followed the previous 2 links that I provided to the documentation, this code is heavily borrowed from there, so I suggest that you study that.
You could run the application directly from Android Studio or from the Terminal, go to the root folder of the application and give flutter run command.
If all goes well, you will see the Famous Quotes App working for you, all powered by Flutter and Google Cloud Functions.
Hope you enjoyed the tutorial.
Technical Tutorials, APIs, Cloud, Books and more.
427 
4
427 claps
427 
4
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
Technical Tutorials, APIs, Cloud, Books and more.
Written by
My passion is to help developers succeed. ¯\_(ツ)_/¯
Technical Tutorials, APIs, Cloud, Books and more.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/daitan-tech/what-we-learned-by-serving-machine-learning-models-at-scale-using-google-cloud-ml-faea5010d29a?source=search_post---------392,"There are currently no responses for this story.
Be the first to respond.
By Bruno Schionato, Diego Domingos, Fernando Moraes, Gustavo Rozato, Isac Souza, Marciano Nardi, Thalles Silva — Daitan Group
Following our series of articles about cloud infrastructures for solving the Machine Learning (ML) pipeline problem, this time we gave Google Cloud ML a try. We’ll also provide a comparison between…
"
https://medium.com/developermind/using-firebase-cloud-messaging-for-remote-notifications-in-ios-d35de1dc67b2?source=search_post---------393,"There are currently no responses for this story.
Be the first to respond.
Read this post in a better format on my website.
Let’s talk about Firebase a little bit first.
On the Firebase website, it says “Firebase gives you the tools and infrastructure you need to build better apps and grow successful businesses.”. It provides several features divided into three groups develop, grow, and earn and all of them…
"
https://medium.com/@davidmytton/aws-vs-google-cloud-flexibility-vs-operational-simplicity-dca4324b03d4?source=search_post---------394,"Sign in
There are currently no responses for this story.
Be the first to respond.
David Mytton
Sep 26, 2015·5 min read
I wanted to revisit a theory I first wrote about in my 15 April 2015 e-mail newsletter — how the approach of Amazon Web Services is different from Google Cloud Platform — and add that to my theory on how containers are core to Google’s Cloud Platform strategy.
On the surface, AWS and GCP are very similar, but their approach to product design is actually quite different:
"
https://medium.com/google-cloud/building-a-personal-genome-data-warehouse-with-google-cloud-23andme-and-family-tree-dna-e9869d8dc7a0?source=search_post---------395,"There are currently no responses for this story.
Be the first to respond.
This is a followup to my post from May 2017 Interpreting 23andMe Raw Genome data with Google Genomics and BigQuery.
TLDR: In this post I will use Cloud Dataprep to clean up my Family Tree DNA and 23andMe raw data, import that raw data into BigQuery for extended genome SNP identification and Cloud Datalab for working with my genomic data. Ideally I will gain a better understanding of my genotyping raw data and I will have a stronger dataset for learning more about my DNA in the future.
Our genomic data can be used to predict disease risk, side effects to pharmaceuticals and tell us more about our who we are and what our future may hold for us. After using 23andMe to explore my raw data with Google BigQuery last year I wanted more data and to be able to do more with it. I imagined a data warehouse of my DNA with multiple raw data sources. This way in the future when we know more about DNA markers (SNPs) I can easily check my genetic data taken from multiple services to verify any suspicions, risks, or concerns I may have about my health.
In my previous post interpreting 23andMe raw Genome data with Google Genomics and BigQuery I took the 23andMe raw data txt file, converted to vcf, and used the Google Genomics load variants pipeline to load into BigQuery for analysis. In this post I will prepare 2 different raw data genomes (23andMe and FTDNA) and copy directly to BigQuery for analysis.
There is a service called Promethease that will take your raw genome data and match it with SNPs found on SNPedia for $12. If you do not want to pay the $12 and the idea of having a personal data warehouse of your genome data appeals to you, you can set one up with Google Cloud quite easily.
If was going to build a personal genome data warehouse I would need more data on my DNA. This time I tried Family Tree DNA’s Family Finder service. After having a second set of raw data I would be able to cross reference any data I found in my 23andMe raw data with another data source, hopefully giving me increased confidence in anything that I found. In the future I will add Ancestry raw data, covering all 3 consumer priced genetic testing services. Ideally this data would allow me to possibly better anticipate or plan for any health issues that may occur in my future. Would this second source give me a more accurate dataset? I think so. Both sources use different sequencing:
23andMe — Uses a customized Illumina Omniexpress+ array that includes about 600,000 SNPs.
Family Tree DNA Family finder — Uses Illumina OmniExpress microarray chip. It includes 711,424 total SNPs. Only about 13,913 of these have annotations in SNPedia.
More on SNP coverage analysis/comparisons on /r/23andme
A nice write up and charge to the many options for DNA analysis Neo.life Your Guide to Getting Sequenced
FamilyTreeDNa family finder raw data download
Family Tree DNA gives you your raw data in a few forms. It can be a bit confusing.
Every cell in the human body has DNA that is tightly packed into structures called chromosomes. There are 23 pairs of chromosomes of which 22 pairs are called autosomes and the 23rd pair is called allosome or sex chromosomes. The X chromosome spans about 155 million DNA base pairs and represents approximately 5 percent of total DNA in cells (source).
The concatenated file from FamilyTree DNA combines the autosomal and X raw data file so if you are looking to gain insight into your raw data you’ll want to use the concatenated file.
A build is a reference system used to describe a kind of typical human genome fully sequenced. Build 37 is supposed to be more accurate in terms of where SNPs actually are located than build 36 (source). A build is a Genome assembly, as more is learned about the human genome, new Genome assemblies are released.
For comparing FTDNA with another raw data source like 23andMe, Build 37 Raw Data Concatenated is the one you want to use.
More on reference genome and build 37 here.
Family Tree DNA gives you a gzip with a CSV so it’s easy to load into BigQuery for analysis. My Family Tree DNA raw data was about 6.5MB compressed and about 22MB uncompressed (CSV). My 23andMe raw data was 5MB compressed and about 15MB uncompressed (txt).
Download your raw data from 23andMe and Family Tree DNA here:
Once you have the CSV of your raw data you could really import right into BigQuery. I’ll start with importing the Family Tree DNA raw data.
You can do this either via the BQ UI or via the bq CLI tool
Result: Unable to import the FTDNA CSV dataset with the position field as an integer. So this led me to start thinking that something was wrong with my data, the POSITION rows should be all numbers.
I went ahead and imported with all fields as STRING data type to move forward and see whats going on here.
Success, but ultimately I do not want all columns to be STRING data type. So let’s explore the data a bit in BigQuery.
I found some issues with the CSV that was provided from Family Tree DNA making it not fully cleansed and in need of some changes.
2. Last 2 rows in the dataset had the column names
To clean this dataset and standardize fields and columns across my 2 different genome datasets I’ll use Cloud Dataprep.
Since some duplicate rows needed to be removed as well as some other changes in this dataset I thought this would be a great time to use Google Cloud Dataprep. Since I am using two different raw data sources I should make sure they have the same columns and column types and formatting so querying both will be easier. I can easily take care of this work in Dataprep.
The first step in in Dataprep is creating a flow and adding a dataset. This is pretty simple, give it a name, then either upload a dataset or import one in GCS or an existing table in BigQuery. I will add datasets for both my FTDNA and 23andMe raw data in the same flow.
And now my second dataset (23andMe)
After the flow is created and datasets are imported add a new recipe to one of your datasets.
Now you can edit your recipe
Great. It will take a moment to load your dataset. Now that your dataset is imported and your recipe is started, you can explore your dataset a bit and add steps or transformations to the recipe.
For this first dataset, I need to make sure the position columns are INT data type. I can also explore my dataset by mousing over the categories displayed in Dataprep.
Here is the work that I will do on the first FTDNA dataset.
Here’s how the recipe looks:
Browsing the dataset with the recipe it looks clean:
Download the FTDNA wrangle file here.
Now it’s time to Run the job and apply these transformations.
You’ll want to modify some of the default configuration here if you plan to take this cleansed dataset right over to BigQuery.
Lets see what needs to be changed. Click the pencil and lets edit this job. Lets change the following:
And under more options underneath replace the file every run
Single file so we can easily import the new .csv into BQ.
Save settings and you are taken back to the run job dialog box. Lets run job.
This transform should take a few minutes to spin up a dataflow job and output our new CSV.
The nice thing here is that we did not have to write any apache beam code at all to clean up this dataset. Dataprep is basically a layer ontop of Cloud Dataflow. While the job is running you can take a look at it in Dataflow if you would like.
A few minutes after running the job (about 7 min for this one) the transformations should be complete. View results to make sure your data is clean.
Its okay to have a 1% mismatch on the CHROMOSOME field in this table as the raw data we are using here has our x chromosome (X) and autosomal values (1–22).
Looks good to me.
Now export your results.
Validate the locations you set before, and click Create
Check your cloud storage bucket and the new .csv should be there.
Now you can either download the .csv or just import to BQ right from the GCS bucket.
Success! Now my FTDNA dataset is imported into BigQuery. Next, lets get the 23andMe dataset prepared and ready for BQ.
Since the 23andMe raw data columns do not match FTDNA columns, you will want to clean the 23andMe raw data genome TXT file in Dataprep as well. Import it as a data source and make the following transformations in your recipe:
Download the 23andMe Cloud Dataprep wrangle file here.
Run the job like we did on the FTDNA dataset, no compression and single file.
Let’s view the results of our completed job to see how it came out:
This dataset should now look similar to your Dataprep cleaned FTDNA dataset and is ready to be exported to CSV then loaded into BigQuery alongside the FTDNA table. Export results the same way that I did for the FTDNA earlier.
Super easy to import to BQ after you run the job and export the results.
My BQ personalgenome dataset includes 2 tables and 2 different genomes. I have a test with Ancestry awaiting results so eventually I’ll have 3 different copies of my genome from unique personal genetic providers. This is my personal DNA dataset:
In BQ to find the total number of rows and matching rows use the following query:
Both genomes together have 1,322,236 rows, 1,009,997 of which are unique. So that gives me 312,259 matching rows. That’s a 23.6% match between my 23andMe and FTDNA genomes.
Also, try a nested wildcard query to find the total number of matching rows between the tables in my dataset
As of June 24, 2018, SNPedia has detail on only 108,485 SNPs. So do not expect to have some strong answers to your DNA, but instead some possible insights.
I had a carrier test done before I got married from a company called Counsyl. Unfortunatly Counsyl does not provide comparable data to 23andMe and FTDNA as they primarily provide a report matching your DNA with your spouse. I attempted to obtain my raw data from Counsyl and they did send me a vcf but it was considerably smaller and did not include reference genome variants. More on Counsyl vs 23andMe:
The Counsyl Foresight screen performs full next generation sequencing across hundreds of genes that can cause inherited genetic disease. This constitutes approximately a million base pair positions which are inspected for novel variation. The VCF primarily indicates positions with rare non-reference variation which gives a VCF of approximately 1,200 entries.
23andme by comparison performs genotyping with a microarray at approximately 600,000 positions with known common ancestral variation. Their VCF files likely contain the genotypes at every position so it’s expected that the file would be 500x larger.
My Counsyl test came back that I was a carrier for something called congenital amegakaryocytic thrombocytopenia. I wanted to verify the SNP for this disease found in my genome by Counsyl was also in my genome taken by 23andMe and FTDNA.
So lets query those tables in BigQuery.
Google search for congenital amegakaryocytic thrombocytopenia snp returns an SNPedia result:
rs12731981 — SNPedia
So now that we have both 23andMe and ftdna datasets in BigQuery, lets check for this SNP in my personal genomes
Since I have made sure both tables have the same column names using Dataprep, I can use a wildcard table in my query to query all tables in the dataset for the SNP. I’ll use _TABLE_SUFFIX to let me know which raw data source is displaying the SNP.
That SNP is matched on all 3 raw data sources I’ve had, Counsyl (via report), 23andMe and ftdna. No worries, my wife had the Counsyl test done as well and she is not a carrier for this disease so our future children will be fine 🙏🏼.
What I just did wasvalidate a genetic marker across 3 different DNA testing services. This gives me a good level of confidence on this genetic marker on my DNA. Pretty cool.
Randomly checking the SNPs for cancer on SNPedia
Prostate Cancer has an identified SNP with aggressive prostate cancer here:
One SNP has been found to be associated not only with prostate cancer in general, but also specifically with aggressive prostate cancer [PMID 18073375]:
Checking for this SNP across my FTDNA and 23andMe genome
Shows up on both of my raw data sources:
Some research has been done to back this up:
We performed an exploratory genome-wide association scan in 498 men with aggressive prostate cancer and 494 control subjects selected from a population-based case-control study in Sweden. We combined the results of this scan with those for aggressive prostate cancer from the publicly available Cancer Genetic Markers of Susceptibility (CGEMS) Study. Single-nucleotide polymorphisms (SNPs) that showed statistically significant associations with the risk of aggressive prostate cancer based on two-sided allele tests were tested for their association with aggressive prostate cancer in two independent study populations composed of individuals of European or African American descent using one-sided tests and the genetic model (dominant or additive) associated with the lowest value in the exploratory study.
Fortunately the allele (variant of the gene) for this marker on SNPedia is A;A with the highest risk and my allele seems to be G;T, so hopefully I may be in the clear here. But this shows you how much we know about DNA. The research that was done was only on about 500 men and SNPedia says the A;A variant has >1.36x risk for prostate cancer. So its not very strong or conclusive but it may mean something in another case to someone.
Next I’ll use Google Cloud Datalab to create notebooks for my genome data warehouse exploration.
Quickstart for Cloud Datalab
Using Cloud Datalab I can run BigQuery queries inside a notebook with %%bq query
In Datalab I can write python code to do analysis. For example in my notebook I am scraping snpedia.com for the popular SNPs using BeautifulSoup.
Once I add more raw data sources (Ancestry), I’ll write something to give me a score for matches found across multiple tables.
Download this Cloud Datalab ipython notebook here.
Today we can have genotyping done from multiple sources at a relatively reasonable cost. Services such as 23andMe, Family Tree DNA, and Ancestry the most popular today for people to have genotyping done for ancestry and health data exploration. This data can help us better understand our genetic makeup, understand disease risk, and possibly allow people to better plan their lifestyle. SNP markers are still being identified and may not be accurate for certain cases. With your genotyping raw data and Google Cloud Platform you can easily build a personal data warehouse and gain a better understanding of large datasets such as your own DNA.
Google Cloud community articles and blogs
335 
335 claps
335 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Customer Engineer, Google Cloud. All views and opinions are my own. @mkahn5
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://netflixtechblog.com/production-media-management-transforming-media-workflows-by-leveraging-the-cloud-1174699e4a08?source=search_post---------396,"Written by Anton Margoline, Avinash Dathathri, Devang Shah and Murthy Parthasarathi. Credit to Netflix Studio’s Product, Design, Content Hub Engineering teams along with all of the supporting partner and platform teams.
In this post, we will share a behind-the-scenes look at how Netflix delivers technology and infrastructure to help production crews create and exchange media during production and post production stages. We’ll also cover how our Studio Engineering efforts are helping Netflix productions to spend less time on media logistics by utilizing our cloud based services.
In a typical live action production, after media is offloaded from the camera and sound recorders on set, it is operated on as files on disk using various tools between departments, like Editorial, Sound and Music, Visual Effects (VFX), Picture Finishing and teams at Netflix. Increasingly, the teams are globally distributed, and each stage of the process generates many terabytes of data.
Media exchanges between different departments constitute a media workflow, and no two productions share the same workflow, known in the industry by the term ‘snowflake workflow’. The stories demand different technical approaches to production, which is why a media workflow for a multi-camera show with visual effects such as Stranger Things, has a different workflow to Formula 1: Drive to Survive with an extensive amount of footage.
Media workflows are always evolving and adapting; driven by changes in production technology (new cameras and formats), post production technology (tools used by Sound, Music, VFX, and Picture Finishing) and consumer technology (adoption of 4K, HDR, and Atmos). It would be impossible to describe all of the complexities and the history of the industry in a single post. For a more comprehensive overview, please refer to Scott Arundale and Tashi Trieu’s book, Modern Post: Workflows and Techniques for Digital Filmmakers.
Now that we understand what media workflows are, let’s take a look at some of the workflows we’ve enabled.
Collect Camera Media (On-Set/Near-Set)
We enable camera and sound media imports via our partner API integrations or via Netflix media import UIs. Along with the files, metadata plays an important role in downstream workflows, so we make significant efforts to categorize all media into respective assets with the help of the metadata we collect from our partner API integrations as well as our internal video inspection services. Media Workflows:
Iterate on a Movie Timeline (Editorial)
We enable Editorial workflows to drive media interchange between Editorial and VFX, Sound & Music, Picture Finishing facility and Netflix. Most of the workflows start with an Editor providing an edit decision list timeline with a playable reference (.mov file). Depending on the type of the workflow, this timeline can be shared as is, or transformed into alternative formats required by the tools used in other areas of production. Media Workflows:
Produce Visual Effects (VFX)
We enable VFX via several media workflows, starting from the initial request from an Editorial department to facilitate the visual effects work, iterating on the produced VFX shots using Media Review workflows, delivering back the finished product by VFX Shot Delivery and, at the very end, archiving everything for safekeeping. Media Workflows:
Picture Finishing (Picture Finishing Facility)
We enable Picture Finishing facilities to get all of the ingredients needed to do the conform, where all media used in the timeline is verified and made available for color grading. If the facility also helps with media management on a given production, we have workflows where the Picture Finishing facility would manage VFX Plate delivery to the VFX facility. Media workflows: VFX Plate Delivery: Provides means to procure VFX Plates (ACES EXR images + other files) used by VFX in the process of creating visual effects.
Sound, Music
We enable Editorial to share their versions of the timeline (cuts) in the form of playable timeline references (.mov files) with Sound/Music. We also enable Sound/Music to deliver their final products as Stems and Mixes so they can be used further into the production cycle, such as for mixing, dubbing and safekeeping. Media Workflows: Studio Archival and its variants.
Localization, Marketing/PR, Streaming (Netflix)
We enable our production partners to deliver media from many different aspects of production, some of which are mentioned in the areas above, with many more. In addition to safekeeping the media, Studio Archival media workflows empower media used during production for Marketing, PR and other workflows. Media Workflows: Studio Archival and its variants.
Lets dive deeper into VFX Plate Generation & Delivery media workflow to demonstrate the steps required within this media exchange. While describing the details we’ll use the opportunity to refer to how our technology infrastructure enables this workflow among many others.
The VFX Plate Generation & Delivery workflow is a process by which an Editor provides the necessary media to a Visual Effects team, with metadata and raw ingredients necessary to begin their work. This workflow is enabled by camera media workflows, which would have been done earlier to make the camera media and its metadata available.
The VFX Plate Generation & Delivery workflow is started by an Editorial team with an edit decision list timeline file (.edl, .xml) exported from a Non Linear Editing tool. This timeline file contains only the references to media with additional information about time, color, markers and more, but not any of the actual media files. In addition to the timeline, the Editor chooses whether they would want the resulting media to be rescaled to UHD and how many extra frames they would like to have added for each event referenced in the timeline.
After processing the timeline file, each individual media reference is extracted with relevant timecode, media reference, color decisions and markers. To support different editorial tools, each having its own edit decision list timeline format, our Video Encoding platform interprets the timeline into a standardized interchange format called OpenTimelineIO.
Media reference, color decisions and markers are linked with the original camera media and transcoded from raw camera formats onto ACES EXR. Most Visual Effects tools are not able to process raw camera files directly. Along with image media, color metadata is extracted from the timeline to generate Color Decision List files (.cdl, .xml) which are used to communicate color decisions made by an Editor. All of the media transformations and metadata are then persisted as VFX Plate assets.
The Editor then reviews VFX Plate Generation & Delivery details, with all of the timeline events clearly identified and any inconsistencies spotted, such as if raw camera media is not found or there are any challenges with transcoding media. If all looks good, an Editor is able to submit this workflow onto the final step where results are packaged and shared with the Visual Effects team.
To share results with Visual Effects artists, we’re transforming all of the VFX Plate assets and media created earlier and sharing with the recipients, who can either download the files via browser, or use our auto-downloader tools for additional convenience. Concluding this workflow is an email, sharing all the relevant information with the Editorial and VFX teams.
We’re leveraging the VFX Plate Generation & Delivery workflow (among others) on shows including the next installments of our amazing series like Money Heist, Selena and others. We’re excited to help even more productions this year, as we’re continuing to build support for more use cases and polish the experiences.
Let’s now zoom out and take a look at the foundation that supports the 20+ unique media workflows that we’ve enabled in the last two years, with more being added at an accelerating rate.
No single monolithic service would scale to support the various demands of this platform. Many teams at Netflix contribute to the success of Media Workflows Platform, by providing the foundations we rely on for many of the steps taken.
Media Workflows Platform (also known as “Content Hub”): a component that powers all of our media workflows. At a very high level it is composed of the Platform, UI and Partner APIs.
— Resource Management to associate files, assets and other workflows — Robust Execution Engine execution engine, powered by Conductor — State Machine defining user and system interaction — Reusable Steps enabling component reuse across different workflows
Media Inspection and Encoding: scalable media services that are able to handle various media types, including raw camera media. Use cases range from gathering metadata to transforming (change format) or trans-wrapping (trim media).
Universal Asset Management: all media with its metadata maintained in a common asset management system enabling a common framework for consuming media assets in a microservice environment.
Global Storage: global, fault tolerant storage solution that supports file-based workflows in the cloud.
Data Science Platform: all of the layers feed into the data science platform, enabling insights to help us iterate on improving our services using data-driven metrics.
Netflix Platform Tools: paved path services provided in building, deploying and orchestrating our services together.
We’ve helped productions manage and exchange many petabytes of media which is only accelerating with more usage of the platform. Some of our recent workflows in Editorial are in pilot on a handful of productions, our VFX workflows helped dozens of shows, Media Review assisted hundreds of shows and, our Archival workflows are used on all of our shows. While we’ve innovated on many workflows, we’re continuing to add support for more workflows and are refining existing ones to be more helpful. Our media workflows platform is a robust, scalable and easy to customize solution that helps us create great content!
Thanks for getting this far! If you are just learning about how production media management works, we hope this sparks an interest in our problem space. If you are designing tools to empower media workflows, we hope that by sharing our challenges and approaches to solving them, we can all learn from each other. Realizing common challenges inspires more openness and the standardization we crave. We’re really excited to see the proliferation of open APIs and industry standards for media transformation and interchange such as OpenTimelineIO, OpenColorIO, ACES and more.
If you’re passionate about building production media workflows, or any of the foundational services, we’re always looking for talented engineers. Please check out our job listings for Studio Engineering, Production Media Engineering, Product Management, Content Engineering, Data Science Engineering and many more.
Learn about Netflix’s world class engineering efforts…
204 
1
204 claps
204 
1
Written by
Learn more about how Netflix designs, builds, and operates our systems and engineering organizations
Learn about Netflix’s world class engineering efforts, company culture, product developments and more.
Written by
Learn more about how Netflix designs, builds, and operates our systems and engineering organizations
Learn about Netflix’s world class engineering efforts, company culture, product developments and more.
"
https://medium.com/thinking-design/collaborative-design-with-adobe-xd-c1ac5d5db74d?source=search_post---------397,"There are currently no responses for this story.
Be the first to respond.
One of the core motivations for creating Adobe XD was to enable designers to design at the speed of thought — removing friction in the design process wherever possible.
With this in mind, we started our journey by delivering XD on Mac, bringing together design, prototyping and sharing tools, while also focusing relentlessly on performance and quality. We then added support for real-time design and prototype preview on iOS and Android, making it easier than ever to craft beautiful mobile experiences.
We’re extremely close to releasing the first beta of XD on Windows, bringing the same speed and efficiency to the large number of designers using PCs. With the availability of XD on Mac and Windows, we hope to improve interoperability for design teams in mixed environments — something that is a real issue when using software that is only available on a single platform.
While we’re excited to see how Adobe XD will make a positive impact to your efficiency and creativity as a designer, we understand that designing compelling experiences involves a much larger group of people, including other designers on your team, developers and additional project stakeholders.
At Adobe MAX, our annual creativity conference, and in this blog post, we’re previewing the next set of capabilities that we’ll be adding to Adobe XD throughout 2017, all of which are centered around collaborative design — with the goal of helping you design, prototype and iterate faster than ever before.
Note: Like everything we work on for XD, we spend lots of time getting feedback from customers and iterating on designs and functionality, so everything described here is subject to change before release.
Publishing prototypes for review by stakeholders has been a core capability since we first released XD. With our MAX release, we’ve added support for commenting on prototypes. Looking beyond 2016, expect continued improvements to both the commenting and design signoff experience.
Another critical step in the stakeholder process is the handoff between the designer and the developer. From talking to our customers, we’ve heard that when working with developers to implement the design, you want to provide as much information and as much interpretation of the design in one place. While you could create a PDF and add annotations to provide additional context and guidance, it is a slow and tedious process of exporting and updating. To simplify this process, we will be adding the ability to publish and share design specs and style guides directly from your XD document.
With the addition of design specifications in XD, you’ll be able to provide developers with a link to a version of your design document that they can view and interact with directly inside the browser. In this view, they’ll get access to artboard and object level annotations you’ve made, be able to download assets, get code snippets for the target platforms, inspect animations and transitions, as well as take measurements so as to get layouts just right.
While the spec provides a view on the XD document, a link to a style guide will provide an easy to consume view of the assets used within your project. Whether you’re a developer who needs an asset for implementation or a marketer who wants a logo for use in campaign, the published style guide will provide an up-to-date repository of colors, fonts and assets for download and re-use.
Across the prototyping, document specs and style guide experiences, stakeholders will be able to review and comment so as to provide feedback on the design, as well as use the assets you’ve created for production purposes. We believe these tools will dramatically improve how you collaborate with stakeholders and are excited to add them to XD in the coming months.
As a designer, you’re not just collaborating with stakeholders, especially on larger projects, you work with other designers to divide and conquer to meet deadlines.
Existing approaches for collaboration among multiple designers require splitting work into separate design documents, which ultimately means needing to merge multiple versions of the documents and sharing assets across documents. With the added pain of assembling everything together for final review and delivery, managing a single source of truth is time-consuming and difficult.
With our goal of reducing friction and ensuring focus on designing, we’re excited to be adding the ability for multiple designers to collaborate in real time on the same document in XD.
This will be as simple as inviting your co-designers into the XD document and then working on your artboards. While you’re editing an artboard, others can see what you’re doing and use your assets, but they can’t make edits to your work. When you’re done and move onto editing a different artboard, the artboard automatically becomes available for another designer to work on, allowing you to focus on design rather than managing files and assets.
As a cloud-connected native desktop app, XD will support multiple designers working on hundreds of artboards in a single document, with the amazing performance and fidelity that you expect as designer.
Of course, with multiple designers working together, you might find that an incorrect design change happens inadvertently — that’s why we are adding visual versioning to XD.
With visual versioning, you can access automatically saved revisions, as well as create milestone versions of the document. Directly within XD, you can now quickly go back to any point in your design, find the version of an artboard or design asset that you need, and bring it forward to the latest version of the document in no time at all.
Since the versioning capability is cloud-based, the capability will be available by default when you save your document to the Creative Cloud. Once you’re cloud-enabled, you’ll be able to publish prototypes, specifications and style guides, as well as co-edit in real-time, secure in the knowledge that your document versions are automatically managed and available to you.
We’re actively working on the collaborative workflows highlighted here and will be exposing these gradually to selected pre-release customers before making them incrementally available through our public beta releases starting in early 2017.
We hope you are as excited as we are about the opportunity to collaborate more effectively with other designers, developers and project stakeholders as you work on mobile and web projects.
You can reach out to our team here in the comments or @AdobeXD on Twitter — we look forward to hearing from you.
Andrew Shorten is Director of Product Management for UX Design at Adobe. He is passionate about improving the quality, richness, and value of digital experiences. Andrew previously developed user interfaces for government and enterprise customers while working at Fujitsu. He has since worked for Macromedia, Microsoft, and Adobe, where he has engaged with designers, developers, digital agencies, and organizations to help them deliver engaging web, mobile and desktop experiences.
Originally published at blogs.adobe.com.
More about Adobe XD:
Stories and insights from the design community.
93 
4
93 claps
93 
4
Stories and insights from the design community.
Written by
New Tools for New Creatives. Get all the latest creative apps plus seamless ways to share and collaborate. All right on your desktop.
Stories and insights from the design community.
"
https://blog.heptio.com/cloud-native-part-1-definition-716ed30e9193?source=search_post---------398,"As Craig and I start our journey toward building Heptio, we have been doing a lot of thinking around where we see the industry going. We spent quite a bit of time at Google (16 years between the two of us) and have a good understanding how Google builds and manages systems. But chances are you don’t work at Google. So how do all of these evolving new concepts apply a typical company/developer/operator?
This is the first part in a multi-part series that examines multiple angles of how to think about and apply “cloud native” thinking.
There is no hard and fast definition for what Cloud Native means. In fact there are other overlapping terms and ideologies. At its root, Cloud Native is structuring teams, culture and technology to utilize automation and architectures to manage complexity and unlock velocity. Operating in this mode is as much a way to scale the people side of the equation as much as the technology side.
Cloud Native is structuring teams, culture and technology to utilize automation and architectures to manage complexity and unlock velocity.
One important note: you don’t have to run in the cloud to be “Cloud Native”. These techniques can be applied incrementally as appropriate and should help smooth any transition to the cloud.
The real value from Cloud Native goes far beyond the basket of technologies that are closely associated with it. To really understand where our industry is going, we need to examine where and how we can make companies, teams and people more successful.
At this point, these techniques have been proven at technology centric forward looking companies that have dedicated large amounts of resources to the effort. Think Google or Netflix or Facebook. Smaller, more flexible, companies are also realizing value here. However, there are very few examples of this philosophy being applied outside of technology early adopters. We are still at the beginning of this journey when viewed across the wider IT world.
We are still at the beginning of this journey.
With some of the early experiences being proven out and shared, what themes are emerging?
At Heptio, we are incredibly excited to help bring the benefits of Cloud Native to the wider IT industry. In the rest of this series we’ll be looking at integrating with existing systems, DevOps, containers and orchestration, microservices, and security. Please stay tuned and let us know what is most interesting to you.
Continue with part 2 as I cover practical considerations of applying Cloud Native.
Heptio
238 
4
238 claps
238 
4
Written by
Dad of two. CTO of Heptio. Started Google Compute Engine, Kubernetes and Google Container Engine.
Heptio
Written by
Dad of two. CTO of Heptio. Started Google Compute Engine, Kubernetes and Google Container Engine.
Heptio
"
https://medium.com/planet-stories/cloud-native-geospatial-part-2-the-cloud-optimized-geotiff-6b3f15c696ed?source=search_post---------399,"There are currently no responses for this story.
Be the first to respond.
Top highlight
This article is part of a ‘Cloud Native Geospatial’ series exploring how the geospatial world looks different when systems and workflows are built from the ground up for the cloud. This time we are going to take a deep look at arguably the most important enabling technology for truly cloud native geospatial: the Cloud Optimized GeoTIFF.
Cloud Optimized GeoTIFF’s, also known as COG’s, are a specially formatted GeoTIFF file that leverages a newer feature of HTTP called Byte Serving. You can learn much more about Cloud Optimized GeoTIFF’s at cogeo.org. Byte serving is the technology that lets you stream a video or music file online and skip forward or backwards through the content. Instead of having to download the full video file, you can tell the server that you want to start at a particular point. The COG format works the same way by allowing users and processes to access just the portion of a raster file that they need.
Jumping to a desired portion of a raster file opens up varied new workflows, as data can be ‘streamed’ like a video instead of being transferred whole across networks. In the geospatial world users can access online web tiles in a streaming manner, but to do actual analysis requires source raster files. Traditionally that has meant long download times to acquire files that are hundreds of megabytes and larger. This is because the source raster files are distributed online and on the cloud, but they aren’t formatted for streaming, so users must fully download the files before processing and visualization could start.
The Cloud Optimized GeoTIFF format began as a collaboration between Amazon, Planet Labs, MapBox, ESRI and USGS to put the Landsat Archive onto AWS in a more accessible way. The GeoTIFF is the most widely used imagery file format, but there was extensive discussion on the Landsat-pds mailing list on how to best format the data so that it could be streamed and processed on the fly. A good solution for formatting Landsat data enabled companies to all leverage the archive in their existing workflows on Amazon Web Services without duplication and reprocessing. Once this pattern of access was established, new software started leveraging the data in the same way, greatly increasing the use of the data.
From there the practice of formatting GeoTIFF to be optimized for cloud workflows has evolved to a documented best practice, with a full implementation in GDAL (the most widely used geospatial library) including documentation and performance testing results published on the GDAL Wiki. Planet Labs transitioned to producing all of the data going through its processing pipeline as Cloud Optimized GeoTIFF’s, with partners like FarmShots and Santiago & Cintra re-architecting their domain specific applications to leverage it.
Reflecting industry adoption, leading open source projects like GDAL, QGIS and GeoServer can already read the format (though QGIS and GeoServer take advanced configuration). DigitalGlobe recently shifted their IDAHO system to leverage COG’s, while reprocessing a significant amount of data to make use of it. OpenAerialMap built their whole architecture on turning user uploaded data into web accessible GeoTIFF’s and then streaming tiles directly from that data. GeoTrellis projects supporting COG on their short term roadmap, and a number of others, including similar cluster computing geospatial processing systems have indicated an intent to support it.
These newer on-the-fly processing systems underscore the power of cloud native geospatial architecture. Such systems can simultaneously process imagery on hundreds and even thousands of computers, returning in seconds analyses that previously would take days or weeks. Despite these modern advances, because the core of the standard is GeoTIFF, any software can read the data, even older desktop applications.
Though the core concept is quite simple — put imagery online and streamable — it is a fundamental building block of a truly cloud native geospatial ecosystem. Data can live on the cloud and numerous software systems can run next to the data to derive value from it without anyone having to incur additional download and storage costs. A full exploration of that ecosystem is a topic for future posts, but the core COG building block will be a foundation to enable users to spend their time actually using data and gaining insights in near real time instead of finding data and relying on a small number of experts who have expert geospatial processing skills.
Although cloud optimized GeoTIFFs are still a relatively new format, backward compatibility and ease of implementation make the format a compelling next step, and the founding group of organizations aim to encourage more software implementers and data providers to adopt it. If you are interested in helping out and learning more, as a software implementer, user or data provider, check out cogeo.org.
Up next in this series: a couple posts taking deeper looks in to actual Cloud Native Geospatial architectures.
Using space imagery to tell stories about our changing…
340 
4
340 claps
340 
4
Written by
Product Architect @ Planet, Board Member @ Open Geospatial Consortium, Technical Fellow @ Radiant.Earth
Using space imagery to tell stories about our changing planet.
Written by
Product Architect @ Planet, Board Member @ Open Geospatial Consortium, Technical Fellow @ Radiant.Earth
Using space imagery to tell stories about our changing planet.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
